{"id": "0705.0017", "contents": "Title: Checking Equivalence of Quantum Circuits and States Abstract: Quantum computing promises exponential speed-ups for important simulation and\noptimization problems. It also poses new CAD problems that are similar to, but\nmore challenging, than the related problems in classical (non-quantum) CAD,\nsuch as determining if two states or circuits are functionally equivalent.\nWhile differences in classical states are easy to detect, quantum states, which\nare represented by complex-valued vectors, exhibit subtle differences leading\nto several notions of equivalence. This provides flexibility in optimizing\nquantum circuits, but leads to difficult new equivalence-checking issues for\nsimulation and synthesis. We identify several different equivalence-checking\nproblems and present algorithms for practical benchmarks, including quantum\ncommunication and search circuits, which are shown to be very fast and robust\nfor hundreds of qubits. \n\n"}
{"id": "0901.3202", "contents": "Title: Model-Consistent Sparse Estimation through the Bootstrap Abstract: We consider the least-square linear regression problem with regularization by\nthe $\\ell^1$-norm, a problem usually referred to as the Lasso. In this paper,\nwe first present a detailed asymptotic analysis of model consistency of the\nLasso in low-dimensional settings. For various decays of the regularization\nparameter, we compute asymptotic equivalents of the probability of correct\nmodel selection. For a specific rate decay, we show that the Lasso selects all\nthe variables that should enter the model with probability tending to one\nexponentially fast, while it selects all other variables with strictly positive\nprobability. We show that this property implies that if we run the Lasso for\nseveral bootstrapped replications of a given sample, then intersecting the\nsupports of the Lasso bootstrap estimates leads to consistent model selection.\nThis novel variable selection procedure, referred to as the Bolasso, is\nextended to high-dimensional settings by a provably consistent two-step\nprocedure. \n\n"}
{"id": "0909.4119", "contents": "Title: Fast Equivalence-checking for Quantum Circuits Abstract: We perform formal verification of quantum circuits by integrating several\ntechniques specialized to particular classes of circuits. Our verification\nmethodology is based on the new notion of a reversible miter that allows one to\nleverage existing techniques for circuit simplification of quantum circuits.\nFor reversible circuits which arise as runtime bottlenecks of key quantum\nalgorithms, we develop several verification techniques and empirically compare\nthem.\n  We also combine existing quantum verification tools with the use of\nSAT-solvers. Experiments with circuits for Shor's number-factoring algorithm,\ncontaining thousands of gates, show improvements in efficiency by 3-4 orders of\nmagnitude. \n\n"}
{"id": "0910.0610", "contents": "Title: Regularization Techniques for Learning with Matrices Abstract: There is growing body of learning problems for which it is natural to\norganize the parameters into matrix, so as to appropriately regularize the\nparameters under some matrix norm (in order to impose some more sophisticated\nprior knowledge). This work describes and analyzes a systematic method for\nconstructing such matrix-based, regularization methods. In particular, we focus\non how the underlying statistical properties of a given problem can help us\ndecide which regularization function is appropriate.\n  Our methodology is based on the known duality fact: that a function is\nstrongly convex with respect to some norm if and only if its conjugate function\nis strongly smooth with respect to the dual norm. This result has already been\nfound to be a key component in deriving and analyzing several learning\nalgorithms. We demonstrate the potential of this framework by deriving novel\ngeneralization and regret bounds for multi-task learning, multi-class learning,\nand kernel learning. \n\n"}
{"id": "1004.1697", "contents": "Title: A Library-Based Synthesis Methodology for Reversible Logic Abstract: In this paper, a library-based synthesis methodology for reversible circuits\nis proposed where a reversible specification is considered as a permutation\ncomprising a set of cycles. To this end, a pre-synthesis optimization step is\nintroduced to construct a reversible specification from an irreversible\nfunction. In addition, a cycle-based representation model is presented to be\nused as an intermediate format in the proposed synthesis methodology. The\nselected intermediate format serves as a focal point for all potential\nrepresentation models. In order to synthesize a given function, a library\ncontaining seven building blocks is used where each building block is a cycle\nof length less than 6. To synthesize large cycles, we also propose a\ndecomposition algorithm which produces all possible minimal and inequivalent\nfactorizations for a given cycle of length greater than 5. All decompositions\ncontain the maximum number of disjoint cycles. The generated decompositions are\nused in conjunction with a novel cycle assignment algorithm which is proposed\nbased on the graph matching problem to select the best possible cycle pairs.\nThen, each pair is synthesized by using the available components of the\nlibrary. The decomposition algorithm together with the cycle assignment method\nare considered as a binding method which selects a building block from the\nlibrary for each cycle. Finally, a post-synthesis optimization step is\nintroduced to optimize the synthesis results in terms of different costs. \n\n"}
{"id": "1103.0215", "contents": "Title: Reversible Circuit Optimization via Leaving the Boolean Domain Abstract: For years, the quantum/reversible circuit community has been convinced that:\na) the addition of auxiliary qubits is instrumental in constructing a smaller\nquantum circuit; and, b) the introduction of quantum gates inside reversible\ncircuits may result in more efficient designs. This paper presents a systematic\napproach to optimizing reversible (and quantum) circuits via the introduction\nof auxiliary qubits and quantum gates inside circuit designs. This advances our\nunderstanding of what may be achieved with a) and b). \n\n"}
{"id": "1103.2686", "contents": "Title: A Study of Optimal 4-bit Reversible Toffoli Circuits and Their Synthesis Abstract: Optimal synthesis of reversible functions is a non-trivial problem. One of\nthe major limiting factors in computing such circuits is the sheer number of\nreversible functions. Even restricting synthesis to 4-bit reversible functions\nresults in a huge search space (16! {\\approx} 2^{44} functions). The output of\nsuch a search alone, counting only the space required to list Toffoli gates for\nevery function, would require over 100 terabytes of storage. In this paper, we\npresent two algorithms: one, that synthesizes an optimal circuit for any 4-bit\nreversible specification, and another that synthesizes all optimal\nimplementations. We employ several techniques to make the problem tractable. We\nreport results from several experiments, including synthesis of all optimal\n4-bit permutations, synthesis of random 4-bit permutations, optimal synthesis\nof all 4-bit linear reversible circuits, synthesis of existing benchmark\nfunctions; we compose a list of the hardest permutations to synthesize, and\nshow distribution of optimal circuits. We further illustrate that our proposed\napproach may be extended to accommodate physical constraints via reporting\nLNN-optimal reversible circuits. Our results have important implications in the\ndesign and optimization of reversible and quantum circuits, testing circuit\nsynthesis heuristics, and performing experiments in the area of quantum\ninformation processing. \n\n"}
{"id": "1104.0729", "contents": "Title: Online and Batch Learning Algorithms for Data with Missing Features Abstract: We introduce new online and batch algorithms that are robust to data with\nmissing features, a situation that arises in many practical applications. In\nthe online setup, we allow for the comparison hypothesis to change as a\nfunction of the subset of features that is observed on any given round,\nextending the standard setting where the comparison hypothesis is fixed\nthroughout. In the batch setup, we present a convex relation of a non-convex\nproblem to jointly estimate an imputation function, used to fill in the values\nof missing features, along with the classification hypothesis. We prove regret\nbounds in the online setting and Rademacher complexity bounds for the batch\ni.i.d. setting. The algorithms are tested on several UCI datasets, showing\nsuperior performance over baselines. \n\n"}
{"id": "1106.1622", "contents": "Title: Large-Scale Convex Minimization with a Low-Rank Constraint Abstract: We address the problem of minimizing a convex function over the space of\nlarge matrices with low rank. While this optimization problem is hard in\ngeneral, we propose an efficient greedy algorithm and derive its formal\napproximation guarantees. Each iteration of the algorithm involves\n(approximately) finding the left and right singular vectors corresponding to\nthe largest singular value of a certain matrix, which can be calculated in\nlinear time. This leads to an algorithm which can scale to large matrices\narising in several applications such as matrix completion for collaborative\nfiltering and robust low rank matrix approximation. \n\n"}
{"id": "1107.2021", "contents": "Title: Multi-Instance Learning with Any Hypothesis Class Abstract: In the supervised learning setting termed Multiple-Instance Learning (MIL),\nthe examples are bags of instances, and the bag label is a function of the\nlabels of its instances. Typically, this function is the Boolean OR. The\nlearner observes a sample of bags and the bag labels, but not the instance\nlabels that determine the bag labels. The learner is then required to emit a\nclassification rule for bags based on the sample. MIL has numerous\napplications, and many heuristic algorithms have been used successfully on this\nproblem, each adapted to specific settings or applications. In this work we\nprovide a unified theoretical analysis for MIL, which holds for any underlying\nhypothesis class, regardless of a specific application or problem domain. We\nshow that the sample complexity of MIL is only poly-logarithmically dependent\non the size of the bag, for any underlying hypothesis class. In addition, we\nintroduce a new PAC-learning algorithm for MIL, which uses a regular supervised\nlearning algorithm as an oracle. We prove that efficient PAC-learning for MIL\ncan be generated from any efficient non-MIL supervised learning algorithm that\nhandles one-sided error. The computational complexity of the resulting\nalgorithm is only polynomially dependent on the bag size. \n\n"}
{"id": "1109.2397", "contents": "Title: Structured sparsity through convex optimization Abstract: Sparse estimation methods are aimed at using or obtaining parsimonious\nrepresentations of data or models. While naturally cast as a combinatorial\noptimization problem, variable or feature selection admits a convex relaxation\nthrough the regularization by the $\\ell_1$-norm. In this paper, we consider\nsituations where we are not only interested in sparsity, but where some\nstructural prior knowledge is available as well. We show that the $\\ell_1$-norm\ncan then be extended to structured norms built on either disjoint or\noverlapping groups of variables, leading to a flexible framework that can deal\nwith various structures. We present applications to unsupervised learning, for\nstructured sparse principal component analysis and hierarchical dictionary\nlearning, and to supervised learning in the context of non-linear variable\nselection. \n\n"}
{"id": "1110.5892", "contents": "Title: Semi-optimal Practicable Algorithmic Cooling Abstract: Algorithmic Cooling (AC) of spins applies entropy manipulation algorithms in\nopen spin-systems in order to cool spins far beyond Shannon's entropy bound. AC\nof nuclear spins was demonstrated experimentally, and may contribute to nuclear\nmagnetic resonance (NMR) spectroscopy. Several cooling algorithms were\nsuggested in recent years, including practicable algorithmic cooling (PAC) and\nexhaustive AC. Practicable algorithms have simple implementations, yet their\nlevel of cooling is far from optimal; Exhaustive algorithms, on the other hand,\ncool much better, and some even reach (asymptotically) an optimal level of\ncooling, but they are not practicable. We introduce here semi-optimal\npracticable AC (SOPAC), wherein few cycles (typically 2-6) are performed at\neach recursive level. Two classes of SOPAC algorithms are proposed and\nanalyzed. Both attain cooling levels significantly better than PAC, and are\nmuch more efficient than the exhaustive algorithms. The new algorithms are\nshown to bridge the gap between PAC and exhaustive AC. In addition, we\ncalculated the number of spins required by SOPAC in order to purify qubits for\nquantum computation. As few as 12 and 7 spins are required (in an ideal\nscenario) to yield a mildly pure spin (60% polarized) from initial\npolarizations of 1% and 10%, respectively. In the latter case, about five more\nspins are sufficient to produce a highly pure spin (99.99% polarized), which\ncould be relevant for fault-tolerant quantum computing. \n\n"}
{"id": "1205.5075", "contents": "Title: Efficient Sparse Group Feature Selection via Nonconvex Optimization Abstract: Sparse feature selection has been demonstrated to be effective in handling\nhigh-dimensional data. While promising, most of the existing works use convex\nmethods, which may be suboptimal in terms of the accuracy of feature selection\nand parameter estimation. In this paper, we expand a nonconvex paradigm to\nsparse group feature selection, which is motivated by applications that require\nidentifying the underlying group structure and performing feature selection\nsimultaneously. The main contributions of this article are twofold: (1)\nstatistically, we introduce a nonconvex sparse group feature selection model\nwhich can reconstruct the oracle estimator. Therefore, consistent feature\nselection and parameter estimation can be achieved; (2) computationally, we\npropose an efficient algorithm that is applicable to large-scale problems.\nNumerical results suggest that the proposed nonconvex method compares favorably\nagainst its competitors on synthetic data and real-world applications, thus\nachieving desired goal of delivering high performance. \n\n"}
{"id": "1206.1270", "contents": "Title: Factoring nonnegative matrices with linear programs Abstract: This paper describes a new approach, based on linear programming, for\ncomputing nonnegative matrix factorizations (NMFs). The key idea is a\ndata-driven model for the factorization where the most salient features in the\ndata are used to express the remaining features. More precisely, given a data\nmatrix X, the algorithm identifies a matrix C such that X approximately equals\nCX and some linear constraints. The constraints are chosen to ensure that the\nmatrix C selects features; these features can then be used to find a low-rank\nNMF of X. A theoretical analysis demonstrates that this approach has guarantees\nsimilar to those of the recent NMF algorithm of Arora et al. (2012). In\ncontrast with this earlier work, the proposed method extends to more general\nnoise models and leads to efficient, scalable algorithms. Experiments with\nsynthetic and real datasets provide evidence that the new approach is also\nsuperior in practice. An optimized C++ implementation can factor a\nmultigigabyte matrix in a matter of minutes. \n\n"}
{"id": "1206.5236", "contents": "Title: Fast and efficient exact synthesis of single qubit unitaries generated\n  by Clifford and T gates Abstract: In this paper, we show the equivalence of the set of unitaries computable by\nthe circuits over the Clifford and T library and the set of unitaries over the\nring $\\mathbb{Z}[\\frac{1}{\\sqrt{2}},i]$, in the single-qubit case. We report an\nefficient synthesis algorithm, with an exact optimality guarantee on the number\nof Hadamard and T gates used. We conjecture that the equivalence of the sets of\nunitaries implementable by circuits over the Clifford and T library and\nunitaries over the ring $\\mathbb{Z}[\\frac{1}{\\sqrt{2}},i]$ holds in the\n$n$-qubit case. \n\n"}
{"id": "1206.5655", "contents": "Title: Path Selection for Quantum Repeater Networks Abstract: Quantum networks will support long-distance quantum key distribution (QKD)\nand distributed quantum computation, and are an active area of both\nexperimental and theoretical research. Here, we present an analysis of\ntopologically complex networks of quantum repeaters composed of heterogeneous\nlinks. Quantum networks have fundamental behavioral differences from classical\nnetworks; the delicacy of quantum states makes a practical path selection\nalgorithm imperative, but classical notions of resource utilization are not\ndirectly applicable, rendering known path selection mechanisms inadequate. To\nadapt Dijkstra's algorithm for quantum repeater networks that generate\nentangled Bell pairs, we quantify the key differences and define a link cost\nmetric, seconds per Bell pair of a particular fidelity, where a single Bell\npair is the resource consumed to perform one quantum teleportation. Simulations\nthat include both the physical interactions and the extensive classical\nmessaging confirm that Dijkstra's algorithm works well in a quantum context.\nSimulating about three hundred heterogeneous paths, comparing our path cost and\nthe total work along the path gives a coefficient of determination of 0.88 or\nbetter. \n\n"}
{"id": "1206.6381", "contents": "Title: Shortest path distance in random k-nearest neighbor graphs Abstract: Consider a weighted or unweighted k-nearest neighbor graph that has been\nbuilt on n data points drawn randomly according to some density p on R^d. We\nstudy the convergence of the shortest path distance in such graphs as the\nsample size tends to infinity. We prove that for unweighted kNN graphs, this\ndistance converges to an unpleasant distance function on the underlying space\nwhose properties are detrimental to machine learning. We also study the\nbehavior of the shortest path distance in weighted kNN graphs. \n\n"}
{"id": "1207.3031", "contents": "Title: Distributed Strongly Convex Optimization Abstract: A lot of effort has been invested into characterizing the convergence rates\nof gradient based algorithms for non-linear convex optimization. Recently,\nmotivated by large datasets and problems in machine learning, the interest has\nshifted towards distributed optimization. In this work we present a distributed\nalgorithm for strongly convex constrained optimization. Each node in a network\nof n computers converges to the optimum of a strongly convex, L-Lipchitz\ncontinuous, separable objective at a rate O(log (sqrt(n) T) / T) where T is the\nnumber of iterations. This rate is achieved in the online setting where the\ndata is revealed one at a time to the nodes, and in the batch setting where\neach node has access to its full local dataset from the start. The same\nconvergence rate is achieved in expectation when the subgradients used at each\nnode are corrupted with additive zero-mean noise. \n\n"}
{"id": "1208.5425", "contents": "Title: Depth-Optimized Reversible Circuit Synthesis Abstract: In this paper, simultaneous reduction of circuit depth and synthesis cost of\nreversible circuits in quantum technologies with limited interaction is\naddressed. We developed a cycle-based synthesis algorithm which uses negative\ncontrols and limited distance between gate lines. To improve circuit depth, a\nnew parallel structure is introduced in which before synthesis a set of\ndisjoint cycles are extracted from the input specification and distributed into\nsome subsets. The cycles of each subset are synthesized independently on\ndifferent sets of ancillae. Accordingly, each disjoint set can be synthesized\nby different synthesis methods. Our analysis shows that the best worst-case\nsynthesis cost of reversible circuits in the linear nearest neighbor\narchitecture is improved by the proposed approach. Our experimental results\nreveal the effectiveness of the proposed approach to reduce cost and circuit\ndepth for several benchmarks. \n\n"}
{"id": "1209.1688", "contents": "Title: Rank Centrality: Ranking from Pair-wise Comparisons Abstract: The question of aggregating pair-wise comparisons to obtain a global ranking\nover a collection of objects has been of interest for a very long time: be it\nranking of online gamers (e.g. MSR's TrueSkill system) and chess players,\naggregating social opinions, or deciding which product to sell based on\ntransactions. In most settings, in addition to obtaining a ranking, finding\n`scores' for each object (e.g. player's rating) is of interest for\nunderstanding the intensity of the preferences.\n  In this paper, we propose Rank Centrality, an iterative rank aggregation\nalgorithm for discovering scores for objects (or items) from pair-wise\ncomparisons. The algorithm has a natural random walk interpretation over the\ngraph of objects with an edge present between a pair of objects if they are\ncompared; the score, which we call Rank Centrality, of an object turns out to\nbe its stationary probability under this random walk. To study the efficacy of\nthe algorithm, we consider the popular Bradley-Terry-Luce (BTL) model\n(equivalent to the Multinomial Logit (MNL) for pair-wise comparisons) in which\neach object has an associated score which determines the probabilistic outcomes\nof pair-wise comparisons between objects. In terms of the pair-wise marginal\nprobabilities, which is the main subject of this paper, the MNL model and the\nBTL model are identical. We bound the finite sample error rates between the\nscores assumed by the BTL model and those estimated by our algorithm. In\nparticular, the number of samples required to learn the score well with high\nprobability depends on the structure of the comparison graph. When the\nLaplacian of the comparison graph has a strictly positive spectral gap, e.g.\neach item is compared to a subset of randomly chosen items, this leads to\ndependence on the number of samples that is nearly order-optimal. \n\n"}
{"id": "1210.0974", "contents": "Title: Quantum circuits of T-depth one Abstract: We give a Clifford+T representation of the Toffoli gate of T-depth 1, using\nfour ancillas. More generally, we describe a class of circuits whose T-depth\ncan be reduced to 1 by using sufficiently many ancillas. We show that the cost\nof adding an additional control to any controlled gate is at most 8 additional\nT-gates, and T-depth 2. We also show that the circuit THT does not possess a\nT-depth 1 representation with an arbitrary number of ancillas initialized to 0. \n\n"}
{"id": "1210.1980", "contents": "Title: A State Distillation Protocol to Implement Arbitrary Single-qubit\n  Rotations Abstract: An important task required to build a scalable, fault-tolerant quantum\ncomputer is to efficiently represent an arbitrary single-qubit rotation by\nfault-tolerant quantum operations. Traditionally, the method for decomposing a\nsingle-qubit unitary into a discrete set of gates is Solovay-Kitaev\ndecomposition, which in practice produces a sequence of depth\nO(\\log^c(1/\\epsilon)), where c~3.97 is the state-of-the-art. The proven lower\nbound is c=1, however an efficient algorithm that saturates this bound is\nunknown. In this paper, we present an alternative to Solovay-Kitaev\ndecomposition employing state distillation techniques which reduces c to\nbetween 1.12 and 2.27, depending on the setting. For a given single-qubit\nrotation, our protocol significantly lowers the length of the approximating\nsequence and the number of required resource states (ancillary qubits). In\naddition, our protocol is robust to noise in the resource states. \n\n"}
{"id": "1211.0025", "contents": "Title: Venn-Abers predictors Abstract: This paper continues study, both theoretical and empirical, of the method of\nVenn prediction, concentrating on binary prediction problems. Venn predictors\nproduce probability-type predictions for the labels of test objects which are\nguaranteed to be well calibrated under the standard assumption that the\nobservations are generated independently from the same distribution. We give a\nsimple formalization and proof of this property. We also introduce Venn-Abers\npredictors, a new class of Venn predictors based on the idea of isotonic\nregression, and report promising empirical results both for Venn-Abers\npredictors and for their more computationally efficient simplified version. \n\n"}
{"id": "1211.1043", "contents": "Title: Soft (Gaussian CDE) regression models and loss functions Abstract: Regression, unlike classification, has lacked a comprehensive and effective\napproach to deal with cost-sensitive problems by the reuse (and not a\nre-training) of general regression models. In this paper, a wide variety of\ncost-sensitive problems in regression (such as bids, asymmetric losses and\nrejection rules) can be solved effectively by a lightweight but powerful\napproach, consisting of: (1) the conversion of any traditional one-parameter\ncrisp regression model into a two-parameter soft regression model, seen as a\nnormal conditional density estimator, by the use of newly-introduced enrichment\nmethods; and (2) the reframing of an enriched soft regression model to new\ncontexts by an instance-dependent optimisation of the expected loss derived\nfrom the conditional normal distribution. \n\n"}
{"id": "1211.6085", "contents": "Title: Random Projections for Linear Support Vector Machines Abstract: Let X be a data matrix of rank \\rho, whose rows represent n points in\nd-dimensional space. The linear support vector machine constructs a hyperplane\nseparator that maximizes the 1-norm soft margin. We develop a new oblivious\ndimension reduction technique which is precomputed and can be applied to any\ninput matrix X. We prove that, with high probability, the margin and minimum\nenclosing ball in the feature space are preserved to within \\epsilon-relative\nerror, ensuring comparable generalization as in the original space in the case\nof classification. For regression, we show that the margin is preserved to\n\\epsilon-relative error with high probability. We present extensive experiments\nwith real and synthetic data to support our theory. \n\n"}
{"id": "1212.0506", "contents": "Title: Exact synthesis of multiqubit Clifford+T circuits Abstract: We prove that a unitary matrix has an exact representation over the\nClifford+T gate set with local ancillas if and only if its entries are in the\nring Z[1/sqrt(2),i]. Moreover, we show that one ancilla always suffices. These\nfacts were conjectured by Kliuchnikov, Maslov, and Mosca. We obtain an\nalgorithm for synthesizing a exact Clifford+T circuit from any such n-qubit\noperator. We also characterize the Clifford+T operators that can be represented\nwithout ancillas. \n\n"}
{"id": "1212.0822", "contents": "Title: Asymptotically optimal approximation of single qubit unitaries by\n  Clifford and T circuits using a constant number of ancillary qubits Abstract: We present an algorithm for building a circuit that approximates single qubit\nunitaries with precision {\\epsilon} using O(log(1/{\\epsilon})) Clifford and T\ngates and employing up to two ancillary qubits. The algorithm for computing our\napproximating circuit requires an average of O(log^2(1/{\\epsilon})log\nlog(1/{\\epsilon})) operations. We prove that the number of gates in our circuit\nsaturates the lower bound on the number of gates required in the scenario when\na constant number of ancillae are supplied, and as such, our circuits are\nasymptotically optimal. This results in significant improvement over the\ncurrent state of the art for finding an approximation of a unitary, including\nthe Solovay-Kitaev algorithm that requires O(log^{3+{\\delta}}(1/{\\epsilon}))\ngates and does not use ancillae and the phase kickback approach that requires\nO(log^2(1/{\\epsilon})log log(1/{\\epsilon})) gates, but uses\nO(log^2(1/{\\epsilon})) ancillae. \n\n"}
{"id": "1212.5156", "contents": "Title: Nonparametric ridge estimation Abstract: We study the problem of estimating the ridges of a density function. Ridge\nestimation is an extension of mode finding and is useful for understanding the\nstructure of a density. It can also be used to find hidden structure in point\ncloud data. We show that, under mild regularity conditions, the ridges of the\nkernel density estimator consistently estimate the ridges of the true density.\nWhen the data are noisy measurements of a manifold, we show that the ridges are\nclose and topologically similar to the hidden manifold. To find the estimated\nridges in practice, we adapt the modified mean-shift algorithm proposed by\nOzertem and Erdogmus [J. Mach. Learn. Res. 12 (2011) 1249-1286]. Some numerical\nexperiments verify that the algorithm is accurate. \n\n"}
{"id": "1212.6964", "contents": "Title: Practical approximation of single-qubit unitaries by single-qubit\n  quantum Clifford and T circuits Abstract: We present an algorithm, along with its implementation that finds T-optimal\napproximations of single-qubit Z-rotations using quantum circuits consisting of\nClifford and T gates. Our algorithm is capable of handling errors in\napproximation down to size $10^{-15}$, resulting in optimal single-qubit\ncircuit designs required for implementation of scalable quantum algorithms. Our\nimplementation along with the experimental results are available in the public\ndomain. \n\n"}
{"id": "1302.0315", "contents": "Title: Sparse Multiple Kernel Learning with Geometric Convergence Rate Abstract: In this paper, we study the problem of sparse multiple kernel learning (MKL),\nwhere the goal is to efficiently learn a combination of a fixed small number of\nkernels from a large pool that could lead to a kernel classifier with a small\nprediction error. We develop an efficient algorithm based on the greedy\ncoordinate descent algorithm, that is able to achieve a geometric convergence\nrate under appropriate conditions. The convergence rate is achieved by\nmeasuring the size of functional gradients by an empirical $\\ell_2$ norm that\ndepends on the empirical data distribution. This is in contrast to previous\nalgorithms that use a functional norm to measure the size of gradients, which\nis independent from the data samples. We also establish a generalization error\nbound of the learned sparse kernel classifier using the technique of local\nRademacher complexity. \n\n"}
{"id": "1302.1611", "contents": "Title: Bounded regret in stochastic multi-armed bandits Abstract: We study the stochastic multi-armed bandit problem when one knows the value\n$\\mu^{(\\star)}$ of an optimal arm, as a well as a positive lower bound on the\nsmallest positive gap $\\Delta$. We propose a new randomized policy that attains\na regret {\\em uniformly bounded over time} in this setting. We also prove\nseveral lower bounds, which show in particular that bounded regret is not\npossible if one only knows $\\Delta$, and bounded regret of order $1/\\Delta$ is\nnot possible if one only knows $\\mu^{(\\star)}$ \n\n"}
{"id": "1302.2576", "contents": "Title: The trace norm constrained matrix-variate Gaussian process for multitask\n  bipartite ranking Abstract: We propose a novel hierarchical model for multitask bipartite ranking. The\nproposed approach combines a matrix-variate Gaussian process with a generative\nmodel for task-wise bipartite ranking. In addition, we employ a novel trace\nconstrained variational inference approach to impose low rank structure on the\nposterior matrix-variate Gaussian process. The resulting posterior covariance\nfunction is derived in closed form, and the posterior mean function is the\nsolution to a matrix-variate regression with a novel spectral elastic net\nregularizer. Further, we show that variational inference for the trace\nconstrained matrix-variate Gaussian process combined with maximum likelihood\nparameter estimation for the bipartite ranking model is jointly convex. Our\nmotivating application is the prioritization of candidate disease genes. The\ngoal of this task is to aid the identification of unobserved associations\nbetween human genes and diseases using a small set of observed associations as\nwell as kernels induced by gene-gene interaction networks and disease\nontologies. Our experimental results illustrate the performance of the proposed\nmodel on real world datasets. Moreover, we find that the resulting low rank\nsolution improves the computational scalability of training and testing as\ncompared to baseline models. \n\n"}
{"id": "1302.2671", "contents": "Title: Latent Self-Exciting Point Process Model for Spatial-Temporal Networks Abstract: We propose a latent self-exciting point process model that describes\ngeographically distributed interactions between pairs of entities. In contrast\nto most existing approaches that assume fully observable interactions, here we\nconsider a scenario where certain interaction events lack information about\nparticipants. Instead, this information needs to be inferred from the available\nobservations. We develop an efficient approximate algorithm based on\nvariational expectation-maximization to infer unknown participants in an event\ngiven the location and the time of the event. We validate the model on\nsynthetic as well as real-world data, and obtain very promising results on the\nidentity-inference task. We also use our model to predict the timing and\nparticipants of future events, and demonstrate that it compares favorably with\nbaseline approaches. \n\n"}
{"id": "1302.3931", "contents": "Title: Understanding Boltzmann Machine and Deep Learning via A Confident\n  Information First Principle Abstract: Typical dimensionality reduction methods focus on directly reducing the\nnumber of random variables while retaining maximal variations in the data. In\nthis paper, we consider the dimensionality reduction in parameter spaces of\nbinary multivariate distributions. We propose a general\nConfident-Information-First (CIF) principle to maximally preserve parameters\nwith confident estimates and rule out unreliable or noisy parameters. Formally,\nthe confidence of a parameter can be assessed by its Fisher information, which\nestablishes a connection with the inverse variance of any unbiased estimate for\nthe parameter via the Cram\\'{e}r-Rao bound. We then revisit Boltzmann machines\n(BM) and theoretically show that both single-layer BM without hidden units\n(SBM) and restricted BM (RBM) can be solidly derived using the CIF principle.\nThis can not only help us uncover and formalize the essential parts of the\ntarget density that SBM and RBM capture, but also suggest that the deep neural\nnetwork consisting of several layers of RBM can be seen as the layer-wise\napplication of CIF. Guided by the theoretical analysis, we develop a\nsample-specific CIF-based contrastive divergence (CD-CIF) algorithm for SBM and\na CIF-based iterative projection procedure (IP) for RBM. Both CD-CIF and IP are\nstudied in a series of density estimation experiments. \n\n"}
{"id": "1303.1280", "contents": "Title: Large-Margin Metric Learning for Partitioning Problems Abstract: In this paper, we consider unsupervised partitioning problems, such as\nclustering, image segmentation, video segmentation and other change-point\ndetection problems. We focus on partitioning problems based explicitly or\nimplicitly on the minimization of Euclidean distortions, which include\nmean-based change-point detection, K-means, spectral clustering and normalized\ncuts. Our main goal is to learn a Mahalanobis metric for these unsupervised\nproblems, leading to feature weighting and/or selection. This is done in a\nsupervised way by assuming the availability of several potentially partially\nlabelled datasets that share the same metric. We cast the metric learning\nproblem as a large-margin structured prediction problem, with proper definition\nof regularizers and losses, leading to a convex optimization problem which can\nbe solved efficiently with iterative techniques. We provide experiments where\nwe show how learning the metric may significantly improve the partitioning\nperformance in synthetic examples, bioinformatics, video segmentation and image\nsegmentation problems. \n\n"}
{"id": "1303.2042", "contents": "Title: Polynomial-time T-depth Optimization of Clifford+T circuits via Matroid\n  Partitioning Abstract: Most work in quantum circuit optimization has been performed in isolation\nfrom the results of quantum fault-tolerance. Here we present a polynomial-time\nalgorithm for optimizing quantum circuits that takes the actual implementation\nof fault-tolerant logical gates into consideration. Our algorithm\nre-synthesizes quantum circuits composed of Clifford group and T gates, the\nlatter being typically the most costly gate in fault-tolerant models, e.g.,\nthose based on the Steane or surface codes, with the purpose of minimizing both\nT-count and T-depth. A major feature of the algorithm is the ability to\nre-synthesize circuits with additional ancillae to reduce T-depth at\neffectively no cost. The tested benchmarks show up to 65.7% reduction in\nT-count and up to 87.6% reduction in T-depth without ancillae, or 99.7%\nreduction in T-depth using ancillae. \n\n"}
{"id": "1303.3557", "contents": "Title: Linear-Depth Quantum Circuits for n-qubit Toffoli gates with no Ancilla Abstract: We design a circuit structure with linear depth to implement an $n$-qubit\nToffoli gate. The proposed construction uses a quadratic-size circuit consists\nof elementary 2-qubit controlled-rotation gates around the x axis and uses no\nancilla qubit. Circuit depth remains linear in quantum technologies with\nfinite-distance interactions between qubits. The suggested construction is\nrelated to the long-standing construction by Barenco et al. (Phys. Rev. A, 52:\n3457-3467, 1995, arXiv:quant-ph/9503016), which uses a quadratic-size,\nquadratic-depth quantum circuit for an $n$-qubit Toffoli gate. \n\n"}
{"id": "1304.0432", "contents": "Title: Constant-Factor Optimization of Quantum Adders on 2D Quantum\n  Architectures Abstract: Quantum arithmetic circuits have practical applications in various quantum\nalgorithms. In this paper, we address quantum addition on 2-dimensional\nnearest-neighbor architectures based on the work presented by Choi and Van\nMeter (JETC 2012). To this end, we propose new circuit structures for some\nbasic blocks in the adder, and reduce communication overhead by adding\nconcurrency to consecutive blocks and also by parallel execution of expensive\nToffoli gates. The proposed optimizations reduce total depth from $140\\sqrt\nn+k_1$ to $92\\sqrt n+k_2$ for constants $k_1,k_2$ and affect the computation\nfidelity considerably. \n\n"}
{"id": "1304.5583", "contents": "Title: Distributed Low-rank Subspace Segmentation Abstract: Vision problems ranging from image clustering to motion segmentation to\nsemi-supervised learning can naturally be framed as subspace segmentation\nproblems, in which one aims to recover multiple low-dimensional subspaces from\nnoisy and corrupted input data. Low-Rank Representation (LRR), a convex\nformulation of the subspace segmentation problem, is provably and empirically\naccurate on small problems but does not scale to the massive sizes of modern\nvision datasets. Moreover, past work aimed at scaling up low-rank matrix\nfactorization is not applicable to LRR given its non-decomposable constraints.\nIn this work, we propose a novel divide-and-conquer algorithm for large-scale\nsubspace segmentation that can cope with LRR's non-decomposable constraints and\nmaintains LRR's strong recovery guarantees. This has immediate implications for\nthe scalability of subspace segmentation, which we demonstrate on a benchmark\nface recognition dataset and in simulations. We then introduce novel\napplications of LRR-based subspace segmentation to large-scale semi-supervised\nlearning for multimedia event detection, concept detection, and image tagging.\nIn each case, we obtain state-of-the-art results and order-of-magnitude speed\nups. \n\n"}
{"id": "1304.6487", "contents": "Title: Locally linear representation for image clustering Abstract: It is a key to construct a similarity graph in graph-oriented subspace\nlearning and clustering. In a similarity graph, each vertex denotes a data\npoint and the edge weight represents the similarity between two points. There\nare two popular schemes to construct a similarity graph, i.e., pairwise\ndistance based scheme and linear representation based scheme. Most existing\nworks have only involved one of the above schemes and suffered from some\nlimitations. Specifically, pairwise distance based methods are sensitive to the\nnoises and outliers compared with linear representation based methods. On the\nother hand, there is the possibility that linear representation based\nalgorithms wrongly select inter-subspaces points to represent a point, which\nwill degrade the performance. In this paper, we propose an algorithm, called\nLocally Linear Representation (LLR), which integrates pairwise distance with\nlinear representation together to address the problems. The proposed algorithm\ncan automatically encode each data point over a set of points that not only\ncould denote the objective point with less residual error, but also are close\nto the point in Euclidean space. The experimental results show that our\napproach is promising in subspace learning and subspace clustering. \n\n"}
{"id": "1305.0810", "contents": "Title: Optimization of Clifford Circuits Abstract: We study optimal synthesis of Clifford circuits, and apply the results to\npeep-hole optimization of quantum circuits. We report optimal circuits for all\nClifford operations with up to four inputs. We perform peep-hole optimization\nof Clifford circuits with up to 40 inputs found in the literature, and\ndemonstrate the reduction in the number of gates by about 50%. We extend our\nmethods to the optimal synthesis of linear reversible circuits, partially\nspecified Clifford functions, and optimal Clifford circuits with five inputs up\nto input/output permutation. The results find their application in randomized\nbenchmarking protocols, quantum error correction, and quantum circuit\noptimization. \n\n"}
{"id": "1305.1396", "contents": "Title: A new framework for optimal classifier design Abstract: The use of alternative measures to evaluate classifier performance is gaining\nattention, specially for imbalanced problems. However, the use of these\nmeasures in the classifier design process is still unsolved. In this work we\npropose a classifier designed specifically to optimize one of these alternative\nmeasures, namely, the so-called F-measure. Nevertheless, the technique is\ngeneral, and it can be used to optimize other evaluation measures. An algorithm\nto train the novel classifier is proposed, and the numerical scheme is tested\nwith several databases, showing the optimality and robustness of the presented\nclassifier. \n\n"}
{"id": "1305.5306", "contents": "Title: A Supervised Neural Autoregressive Topic Model for Simultaneous Image\n  Classification and Annotation Abstract: Topic modeling based on latent Dirichlet allocation (LDA) has been a\nframework of choice to perform scene recognition and annotation. Recently, a\nnew type of topic model called the Document Neural Autoregressive Distribution\nEstimator (DocNADE) was proposed and demonstrated state-of-the-art performance\nfor document modeling. In this work, we show how to successfully apply and\nextend this model to the context of visual scene modeling. Specifically, we\npropose SupDocNADE, a supervised extension of DocNADE, that increases the\ndiscriminative power of the hidden topic features by incorporating label\ninformation into the training objective of the model. We also describe how to\nleverage information about the spatial position of the visual words and how to\nembed additional image annotations, so as to simultaneously perform image\nclassification and annotation. We test our model on the Scene15, LabelMe and\nUIUC-Sports datasets and show that it compares favorably to other topic models\nsuch as the supervised variant of LDA. \n\n"}
{"id": "1306.0239", "contents": "Title: Deep Learning using Linear Support Vector Machines Abstract: Recently, fully-connected and convolutional neural networks have been trained\nto achieve state-of-the-art performance on a wide variety of tasks such as\nspeech recognition, image classification, natural language processing, and\nbioinformatics. For classification tasks, most of these \"deep learning\" models\nemploy the softmax activation function for prediction and minimize\ncross-entropy loss. In this paper, we demonstrate a small but consistent\nadvantage of replacing the softmax layer with a linear support vector machine.\nLearning minimizes a margin-based loss instead of the cross-entropy loss. While\nthere have been various combinations of neural nets and SVMs in prior art, our\nresults using L2-SVMs show that by simply replacing softmax with linear SVMs\ngives significant gains on popular deep learning datasets MNIST, CIFAR-10, and\nthe ICML 2013 Representation Learning Workshop's face expression recognition\nchallenge. \n\n"}
{"id": "1306.3200", "contents": "Title: Synthesis of unitaries with Clifford+T circuits Abstract: We describe a new method for approximating an arbitrary $n$ qubit unitary\nwith precision $\\varepsilon$ using a Clifford and T circuit with\n$O(4^{n}n(\\log(1/\\varepsilon)+n))$ gates. The method is based on rounding off a\nunitary to a unitary over the ring $\\mathbb{Z}[i,1/\\sqrt{2}]$ and employing\nexact synthesis. We also show that any $n$ qubit unitary over the ring\n$\\mathbb{Z}[i,1/\\sqrt{2}]$ with entries of the form\n$(a+b\\sqrt{2}+ic+id\\sqrt{2})/2^{k}$ can be exactly synthesized using\n$O(4^{n}nk)$ Clifford and T gates using two ancillary qubits. This new exact\nsynthesis algorithm is an improvement over the best known exact synthesis\nmethod by B. Giles and P. Selinger requiring $O(3^{2^{n}}nk)$ elementary gates. \n\n"}
{"id": "1306.3664", "contents": "Title: Fault-tolerant Operations for Universal Blind Quantum Computation Abstract: Blind quantum computation is an appealing use of quantum information\ntechnology because it can conceal both the client's data and the algorithm\nitself from the server. However, problems need to be solved in the practical\nuse of blind quantum computation and fault-tolerance is a major challenge. On\nan example circuit, the computational cost measured in T gates executed by the\nclient is 97 times more than performing the original computation directly,\nwithout using the server, even before applying error correction. (The client\nstill benefits due to drastically reduced memory requirements.) Broadbent et\nal. proposed running error correction over blind computation, but our first\nprotocol applies one layer of Steane's [[7,1,3]] code underneath instead. This\nprotocol has better fault tolerance, but still results in a substantial\noverhead. We propose another protocol to reduce the client's computational load\nby transferring the qubit preparation to the server. For each logical qubit\nused in the computation, the client is only required to receive eight logical\nqubits via teleportation then buffer two logical qubits before returning one.\nThis protocol also protects the client's fault-tolerant preparation of logical\nqubits from a side-channel attack. \n\n"}
{"id": "1306.3760", "contents": "Title: A Space-Efficient Design for Reversible Floating Point Adder in Quantum\n  Computing Abstract: Reversible logic has applications in low-power computing and quantum\ncomputing. However, there are few existing designs for reversible\nfloating-point adders and none suitable for quantum computation. In this paper\nwe propose a space-efficient reversible floating-point adder, suitable for\nbinary quantum computation, improving the design of Nachtigal et al. Our work\nfocuses on improving the reversible designs of the alignment unit and the\nnormalization unit, which are the most expensive parts. By changing a few\nelements of the existing algorithm, including the circuit designs of the RLZC\n(reversible leading zero counter) and converter, we have reduced the cost about\n68%. We also propose fault-tolerant designs for the circuits. The KQ for our\nfault-tolerant design is almost sixty times as expensive as for a 32-bit\nfixed-point addition. We note that the floating-point representation makes\nin-place, truly reversible arithmetic impossible, requiring us to retain both\ninputs, which limits the sustainability of its use for quantum computation. \n\n"}
{"id": "1308.2493", "contents": "Title: On quantum circuits employing roots of the Pauli matrices Abstract: The Pauli matrices are a set of three 2x2 complex Hermitian, unitary\nmatrices. In this article, we investigate the relationships between certain\nroots of the Pauli matrices and how gates implementing those roots are used in\nquantum circuits. Techniques for simplifying such circuits are given. In\nparticular, we show how those techniques can be used to find a circuit of\nClifford+T gates starting from a circuit composed of gates from the well\nstudied NCV library. \n\n"}
{"id": "1308.3432", "contents": "Title: Estimating or Propagating Gradients Through Stochastic Neurons for\n  Conditional Computation Abstract: Stochastic neurons and hard non-linearities can be useful for a number of\nreasons in deep learning models, but in many cases they pose a challenging\nproblem: how to estimate the gradient of a loss function with respect to the\ninput of such stochastic or non-smooth neurons? I.e., can we \"back-propagate\"\nthrough these stochastic neurons? We examine this question, existing\napproaches, and compare four families of solutions, applicable in different\nsettings. One of them is the minimum variance unbiased gradient estimator for\nstochatic binary neurons (a special case of the REINFORCE algorithm). A second\napproach, introduced here, decomposes the operation of a binary stochastic\nneuron into a stochastic binary part and a smooth differentiable part, which\napproximates the expected effect of the pure stochatic binary neuron to first\norder. A third approach involves the injection of additive or multiplicative\nnoise in a computational graph that is otherwise differentiable. A fourth\napproach heuristically copies the gradient with respect to the stochastic\noutput directly as an estimator of the gradient with respect to the sigmoid\nargument (we call this the straight-through estimator). To explore a context\nwhere these estimators are useful, we consider a small-scale version of {\\em\nconditional computation}, where sparse stochastic units form a distributed\nrepresentation of gaters that can turn off in combinatorially many ways large\nchunks of the computation performed in the rest of the neural network. In this\ncase, it is important that the gating units produce an actual 0 most of the\ntime. The resulting sparsity can be potentially be exploited to greatly reduce\nthe computational cost of large deep networks for which conditional computation\nwould be useful. \n\n"}
{"id": "1309.3533", "contents": "Title: Mixed Membership Models for Time Series Abstract: In this article we discuss some of the consequences of the mixed membership\nperspective on time series analysis. In its most abstract form, a mixed\nmembership model aims to associate an individual entity with some set of\nattributes based on a collection of observed data. Although much of the\nliterature on mixed membership models considers the setting in which\nexchangeable collections of data are associated with each member of a set of\nentities, it is equally natural to consider problems in which an entire time\nseries is viewed as an entity and the goal is to characterize the time series\nin terms of a set of underlying dynamic attributes or \"dynamic regimes\".\nIndeed, this perspective is already present in the classical hidden Markov\nmodel, where the dynamic regimes are referred to as \"states\", and the\ncollection of states realized in a sample path of the underlying process can be\nviewed as a mixed membership characterization of the observed time series. Our\ngoal here is to review some of the richer modeling possibilities for time\nseries that are provided by recent developments in the mixed membership\nframework. \n\n"}
{"id": "1309.6707", "contents": "Title: Distributed Online Learning in Social Recommender Systems Abstract: In this paper, we consider decentralized sequential decision making in\ndistributed online recommender systems, where items are recommended to users\nbased on their search query as well as their specific background including\nhistory of bought items, gender and age, all of which comprise the context\ninformation of the user. In contrast to centralized recommender systems, in\nwhich there is a single centralized seller who has access to the complete\ninventory of items as well as the complete record of sales and user\ninformation, in decentralized recommender systems each seller/learner only has\naccess to the inventory of items and user information for its own products and\nnot the products and user information of other sellers, but can get commission\nif it sells an item of another seller. Therefore the sellers must distributedly\nfind out for an incoming user which items to recommend (from the set of own\nitems or items of another seller), in order to maximize the revenue from own\nsales and commissions. We formulate this problem as a cooperative contextual\nbandit problem, analytically bound the performance of the sellers compared to\nthe best recommendation strategy given the complete realization of user\narrivals and the inventory of items, as well as the context-dependent purchase\nprobabilities of each item, and verify our results via numerical examples on a\ndistributed data set adapted based on Amazon data. We evaluate the dependence\nof the performance of a seller on the inventory of items the seller has, the\nnumber of connections it has with the other sellers, and the commissions which\nthe seller gets by selling items of other sellers to its users. \n\n"}
{"id": "1310.1533", "contents": "Title: CAM: Causal additive models, high-dimensional order search and penalized\n  regression Abstract: We develop estimation for potentially high-dimensional additive structural\nequation models. A key component of our approach is to decouple order search\namong the variables from feature or edge selection in a directed acyclic graph\nencoding the causal structure. We show that the former can be done with\nnonregularized (restricted) maximum likelihood estimation while the latter can\nbe efficiently addressed using sparse regression techniques. Thus, we\nsubstantially simplify the problem of structure search and estimation for an\nimportant class of causal models. We establish consistency of the (restricted)\nmaximum likelihood estimator for low- and high-dimensional scenarios, and we\nalso allow for misspecification of the error distribution. Furthermore, we\ndevelop an efficient computational algorithm which can deal with many\nvariables, and the new method's accuracy and performance is illustrated on\nsimulated and real data. \n\n"}
{"id": "1310.1934", "contents": "Title: Discriminative Features via Generalized Eigenvectors Abstract: Representing examples in a way that is compatible with the underlying\nclassifier can greatly enhance the performance of a learning system. In this\npaper we investigate scalable techniques for inducing discriminative features\nby taking advantage of simple second order structure in the data. We focus on\nmulticlass classification and show that features extracted from the generalized\neigenvectors of the class conditional second moments lead to classifiers with\nexcellent empirical performance. Moreover, these features have attractive\ntheoretical properties, such as inducing representations that are invariant to\nlinear transformations of the input. We evaluate classifiers built from these\nfeatures on three different tasks, obtaining state of the art results. \n\n"}
{"id": "1310.2040", "contents": "Title: Quantum Computing's Classical Problem, Classical Computing's Quantum\n  Problem Abstract: Tasked with the challenge to build better and better computers, quantum\ncomputing and classical computing face the same conundrum: the success of\nclassical computing systems. Small quantum computing systems have been\ndemonstrated, and intermediate-scale systems are on the horizon, capable of\ncalculating numeric results or simulating physical systems far beyond what\nhumans can do by hand. However, to be commercially viable, they must surpass\nwhat our wildly successful, highly advanced classical computers can already do.\nAt the same time, those classical computers continue to advance, but those\nadvances are now constrained by thermodynamics, and will soon be limited by the\ndiscrete nature of atomic matter and ultimately quantum effects. Technological\nadvances benefit both quantum and classical machinery, altering the competitive\nlandscape. Can we build quantum computing systems that out-compute classical\nsystems capable of some $10^{30}$ logic gates per month? This article will\ndiscuss the interplay in these competing and cooperating technological trends. \n\n"}
{"id": "1310.4150", "contents": "Title: Asymptotically Optimal Topological Quantum Compiling Abstract: In a topological quantum computer, universality is achieved by braiding and\nquantum information is natively protected from small local errors. We address\nthe problem of compiling single-qubit quantum operations into braid\nrepresentations for non-abelian quasiparticles described by the Fibonacci anyon\nmodel. We develop a probabilistically polynomial algorithm that outputs a braid\npattern to approximate a given single-qubit unitary to a desired precision. We\nalso classify the single-qubit unitaries that can be implemented exactly by a\nFibonacci anyon braid pattern and present an efficient algorithm to produce\ntheir braid patterns. Our techniques produce braid patterns that meet the\nuniform asymptotic lower bound on the compiled circuit depth and thus are\ndepth-optimal asymptotically. Our compiled circuits are significantly shorter\nthan those output by prior state-of-the-art methods, resulting in improvements\nin depth by factors ranging from 20 to 1000 for precisions ranging between\n$10^{-10}$ and $10^{-30}$. \n\n"}
{"id": "1310.6731", "contents": "Title: Zermelo Navigation and a Speed Limit to Quantum Information Processing Abstract: We use a specific geometric method to determine speed limits to the\nimplementation of quantum gates in controlled quantum systems that have a\nspecific class of constrained control functions. We achieve this by applying a\nrecent theorem of Shen, which provides a connection between time optimal\nnavigation on Riemannian manifolds and the geodesics of a certain Finsler\nmetric of Randers type. We use the lengths of these geodesics to derive the\noptimal implementation times (under the assumption of constant control fields)\nfor an arbitrary quantum operation (on a finite dimensional Hilbert space), and\nexplicitly calculate the result for the case of a controlled single spin system\nin a magnetic field, and a swap gate in a Heisenberg spin chain. \n\n"}
{"id": "1310.6813", "contents": "Title: Generators and relations for n-qubit Clifford operators Abstract: We define a normal form for Clifford circuits, and we prove that every\nClifford operator has a unique normal form. Moreover, we present a rewrite\nsystem by which any Clifford circuit can be reduced to normal form. This yields\na presentation of Clifford operators in terms of generators and relations. \n\n"}
{"id": "1310.8499", "contents": "Title: Deep AutoRegressive Networks Abstract: We introduce a deep, generative autoencoder capable of learning hierarchies\nof distributed representations from data. Successive deep stochastic hidden\nlayers are equipped with autoregressive connections, which enable the model to\nbe sampled from quickly and exactly via ancestral sampling. We derive an\nefficient approximate parameter estimation method based on the minimum\ndescription length (MDL) principle, which can be seen as maximising a\nvariational lower bound on the log-likelihood, with a feedforward neural\nnetwork implementing approximate inference. We demonstrate state-of-the-art\ngenerative performance on a number of classic data sets: several UCI data sets,\nMNIST and Atari 2600 games. \n\n"}
{"id": "1311.1780", "contents": "Title: Learned-Norm Pooling for Deep Feedforward and Recurrent Neural Networks Abstract: In this paper we propose and investigate a novel nonlinear unit, called $L_p$\nunit, for deep neural networks. The proposed $L_p$ unit receives signals from\nseveral projections of a subset of units in the layer below and computes a\nnormalized $L_p$ norm. We notice two interesting interpretations of the $L_p$\nunit. First, the proposed unit can be understood as a generalization of a\nnumber of conventional pooling operators such as average, root-mean-square and\nmax pooling widely used in, for instance, convolutional neural networks (CNN),\nHMAX models and neocognitrons. Furthermore, the $L_p$ unit is, to a certain\ndegree, similar to the recently proposed maxout unit (Goodfellow et al., 2013)\nwhich achieved the state-of-the-art object recognition results on a number of\nbenchmark datasets. Secondly, we provide a geometrical interpretation of the\nactivation function based on which we argue that the $L_p$ unit is more\nefficient at representing complex, nonlinear separating boundaries. Each $L_p$\nunit defines a superelliptic boundary, with its exact shape defined by the\norder $p$. We claim that this makes it possible to model arbitrarily shaped,\ncurved boundaries more efficiently by combining a few $L_p$ units of different\norders. This insight justifies the need for learning different orders for each\nunit in the model. We empirically evaluate the proposed $L_p$ units on a number\nof datasets and show that multilayer perceptrons (MLP) consisting of the $L_p$\nunits achieve the state-of-the-art results on a number of benchmark datasets.\nFurthermore, we evaluate the proposed $L_p$ unit on the recently proposed deep\nrecurrent neural networks (RNN). \n\n"}
{"id": "1312.4400", "contents": "Title: Network In Network Abstract: We propose a novel deep network structure called \"Network In Network\" (NIN)\nto enhance model discriminability for local patches within the receptive field.\nThe conventional convolutional layer uses linear filters followed by a\nnonlinear activation function to scan the input. Instead, we build micro neural\nnetworks with more complex structures to abstract the data within the receptive\nfield. We instantiate the micro neural network with a multilayer perceptron,\nwhich is a potent function approximator. The feature maps are obtained by\nsliding the micro networks over the input in a similar manner as CNN; they are\nthen fed into the next layer. Deep NIN can be implemented by stacking mutiple\nof the above described structure. With enhanced local modeling via the micro\nnetwork, we are able to utilize global average pooling over feature maps in the\nclassification layer, which is easier to interpret and less prone to\noverfitting than traditional fully connected layers. We demonstrated the\nstate-of-the-art classification performances with NIN on CIFAR-10 and\nCIFAR-100, and reasonable performances on SVHN and MNIST datasets. \n\n"}
{"id": "1312.5604", "contents": "Title: Learning Transformations for Classification Forests Abstract: This work introduces a transformation-based learner model for classification\nforests. The weak learner at each split node plays a crucial role in a\nclassification tree. We propose to optimize the splitting objective by learning\na linear transformation on subspaces using nuclear norm as the optimization\ncriteria. The learned linear transformation restores a low-rank structure for\ndata from the same class, and, at the same time, maximizes the separation\nbetween different classes, thereby improving the performance of the split\nfunction. Theoretical and experimental results support the proposed framework. \n\n"}
{"id": "1312.5753", "contents": "Title: SOMz: photometric redshift PDFs with self organizing maps and random\n  atlas Abstract: In this paper we explore the applicability of the unsupervised machine\nlearning technique of Self Organizing Maps (SOM) to estimate galaxy photometric\nredshift probability density functions (PDFs). This technique takes a\nspectroscopic training set, and maps the photometric attributes, but not the\nredshifts, to a two dimensional surface by using a process of competitive\nlearning where neurons compete to more closely resemble the training data\nmultidimensional space. The key feature of a SOM is that it retains the\ntopology of the input set, revealing correlations between the attributes that\nare not easily identified. We test three different 2D topological mapping:\nrectangular, hexagonal, and spherical, by using data from the DEEP2 survey. We\nalso explore different implementations and boundary conditions on the map and\nalso introduce the idea of a random atlas where a large number of different\nmaps are created and their individual predictions are aggregated to produce a\nmore robust photometric redshift PDF. We also introduced a new metric, the\n$I$-score, which efficiently incorporates different metrics, making it easier\nto compare different results (from different parameters or different\nphotometric redshift codes). We find that by using a spherical topology mapping\nwe obtain a better representation of the underlying multidimensional topology,\nwhich provides more accurate results that are comparable to other,\nstate-of-the-art machine learning algorithms. Our results illustrate that\nunsupervised approaches have great potential for many astronomical problems,\nand in particular for the computation of photometric redshifts. \n\n"}
{"id": "1312.6229", "contents": "Title: OverFeat: Integrated Recognition, Localization and Detection using\n  Convolutional Networks Abstract: We present an integrated framework for using Convolutional Networks for\nclassification, localization and detection. We show how a multiscale and\nsliding window approach can be efficiently implemented within a ConvNet. We\nalso introduce a novel deep learning approach to localization by learning to\npredict object boundaries. Bounding boxes are then accumulated rather than\nsuppressed in order to increase detection confidence. We show that different\ntasks can be learned simultaneously using a single shared network. This\nintegrated framework is the winner of the localization task of the ImageNet\nLarge Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very\ncompetitive results for the detection and classifications tasks. In\npost-competition work, we establish a new state of the art for the detection\ntask. Finally, we release a feature extractor from our best model called\nOverFeat. \n\n"}
{"id": "1312.6584", "contents": "Title: Remarks on Matsumoto and Amano's normal form for single-qubit Clifford+T\n  operators Abstract: Matsumoto and Amano (2008) showed that every single-qubit Clifford+T operator\ncan be uniquely written of a particular form, which we call the Matsumoto-Amano\nnormal form. In this mostly expository paper, we give a detailed and\nstreamlined presentation of Matsumoto and Amano's results, simplifying some\nproofs along the way. We also point out some corollaries to Matsumoto and\nAmano's work, including an intrinsic characterization of the Clifford+T\nsubgroup of SO(3), which also yields an efficient T-optimal exact single-qubit\nsynthesis algorithm. Interestingly, this also gives an alternative proof of\nKliuchnikov, Maslov, and Mosca's exact synthesis result for the Clifford+T\nsubgroup of U(2). \n\n"}
{"id": "1401.0304", "contents": "Title: Learning without Concentration Abstract: We obtain sharp bounds on the performance of Empirical Risk Minimization\nperformed in a convex class and with respect to the squared loss, without\nassuming that class members and the target are bounded functions or have\nrapidly decaying tails.\n  Rather than resorting to a concentration-based argument, the method used here\nrelies on a `small-ball' assumption and thus holds for classes consisting of\nheavy-tailed functions and for heavy-tailed targets.\n  The resulting estimates scale correctly with the `noise level' of the\nproblem, and when applied to the classical, bounded scenario, always improve\nthe known bounds. \n\n"}
{"id": "1401.0579", "contents": "Title: More Algorithms for Provable Dictionary Learning Abstract: In dictionary learning, also known as sparse coding, the algorithm is given\nsamples of the form $y = Ax$ where $x\\in \\mathbb{R}^m$ is an unknown random\nsparse vector and $A$ is an unknown dictionary matrix in $\\mathbb{R}^{n\\times\nm}$ (usually $m > n$, which is the overcomplete case). The goal is to learn $A$\nand $x$. This problem has been studied in neuroscience, machine learning,\nvisions, and image processing. In practice it is solved by heuristic algorithms\nand provable algorithms seemed hard to find. Recently, provable algorithms were\nfound that work if the unknown feature vector $x$ is $\\sqrt{n}$-sparse or even\nsparser. Spielman et al. \\cite{DBLP:journals/jmlr/SpielmanWW12} did this for\ndictionaries where $m=n$; Arora et al. \\cite{AGM} gave an algorithm for\novercomplete ($m >n$) and incoherent matrices $A$; and Agarwal et al.\n\\cite{DBLP:journals/corr/AgarwalAN13} handled a similar case but with weaker\nguarantees.\n  This raised the problem of designing provable algorithms that allow sparsity\n$\\gg \\sqrt{n}$ in the hidden vector $x$. The current paper designs algorithms\nthat allow sparsity up to $n/poly(\\log n)$. It works for a class of matrices\nwhere features are individually recoverable, a new notion identified in this\npaper that may motivate further work.\n  The algorithm runs in quasipolynomial time because they use limited\nenumeration. \n\n"}
{"id": "1401.3409", "contents": "Title: Low-Rank Modeling and Its Applications in Image Analysis Abstract: Low-rank modeling generally refers to a class of methods that solve problems\nby representing variables of interest as low-rank matrices. It has achieved\ngreat success in various fields including computer vision, data mining, signal\nprocessing and bioinformatics. Recently, much progress has been made in\ntheories, algorithms and applications of low-rank modeling, such as exact\nlow-rank matrix recovery via convex programming and matrix completion applied\nto collaborative filtering. These advances have brought more and more\nattentions to this topic. In this paper, we review the recent advance of\nlow-rank modeling, the state-of-the-art algorithms, and related applications in\nimage analysis. We first give an overview to the concept of low-rank modeling\nand challenging problems in this area. Then, we summarize the models and\nalgorithms for low-rank matrix recovery and illustrate their advantages and\nlimitations with numerical experiments. Next, we introduce a few applications\nof low-rank modeling in the context of image analysis. Finally, we conclude\nthis paper with some discussions. \n\n"}
{"id": "1402.1958", "contents": "Title: Better Optimism By Bayes: Adaptive Planning with Rich Models Abstract: The computational costs of inference and planning have confined Bayesian\nmodel-based reinforcement learning to one of two dismal fates: powerful\nBayes-adaptive planning but only for simplistic models, or powerful, Bayesian\nnon-parametric models but using simple, myopic planning strategies such as\nThompson sampling. We ask whether it is feasible and truly beneficial to\ncombine rich probabilistic models with a closer approximation to fully Bayesian\nplanning. First, we use a collection of counterexamples to show formal problems\nwith the over-optimism inherent in Thompson sampling. Then we leverage\nstate-of-the-art techniques in efficient Bayes-adaptive planning and\nnon-parametric Bayesian methods to perform qualitatively better than both\nexisting conventional algorithms and Thompson sampling on two contextual\nbandit-like problems. \n\n"}
{"id": "1402.4102", "contents": "Title: Stochastic Gradient Hamiltonian Monte Carlo Abstract: Hamiltonian Monte Carlo (HMC) sampling methods provide a mechanism for\ndefining distant proposals with high acceptance probabilities in a\nMetropolis-Hastings framework, enabling more efficient exploration of the state\nspace than standard random-walk proposals. The popularity of such methods has\ngrown significantly in recent years. However, a limitation of HMC methods is\nthe required gradient computation for simulation of the Hamiltonian dynamical\nsystem-such computation is infeasible in problems involving a large sample size\nor streaming data. Instead, we must rely on a noisy gradient estimate computed\nfrom a subset of the data. In this paper, we explore the properties of such a\nstochastic gradient HMC approach. Surprisingly, the natural implementation of\nthe stochastic approximation can be arbitrarily bad. To address this problem we\nintroduce a variant that uses second-order Langevin dynamics with a friction\nterm that counteracts the effects of the noisy gradient, maintaining the\ndesired target distribution as the invariant distribution. Results on simulated\ndata validate our theory. We also provide an application of our methods to a\nclassification task using neural networks and to online Bayesian matrix\nfactorization. \n\n"}
{"id": "1402.5176", "contents": "Title: Pareto-depth for Multiple-query Image Retrieval Abstract: Most content-based image retrieval systems consider either one single query,\nor multiple queries that include the same object or represent the same semantic\ninformation. In this paper we consider the content-based image retrieval\nproblem for multiple query images corresponding to different image semantics.\nWe propose a novel multiple-query information retrieval algorithm that combines\nthe Pareto front method (PFM) with efficient manifold ranking (EMR). We show\nthat our proposed algorithm outperforms state of the art multiple-query\nretrieval algorithms on real-world image databases. We attribute this\nperformance improvement to concavity properties of the Pareto fronts, and prove\na theoretical result that characterizes the asymptotic concavity of the fronts. \n\n"}
{"id": "1403.2975", "contents": "Title: Optimal ancilla-free Clifford+T approximation of z-rotations Abstract: We consider the problem of approximating arbitrary single-qubit z-rotations\nby ancilla-free Clifford+T circuits, up to given epsilon. We present a fast new\nprobabilistic algorithm for solving this problem optimally, i.e., for finding\nthe shortest possible circuit whatsoever for the given problem instance. The\nalgorithm requires a factoring oracle (such as a quantum computer). Even in the\nabsence of a factoring oracle, the algorithm is still near-optimal under a mild\nnumber-theoretic hypothesis. In this case, the algorithm finds a solution of\nT-count m + O(log(log(1/epsilon))), where m is the T-count of the\nsecond-to-optimal solution. In the typical case, this yields circuit\napproximations of T-count 3log_2(1/epsilon) + O(log(log(1/epsilon))). Our\nalgorithm is efficient in practice, and provably efficient under the\nabove-mentioned number-theoretic hypothesis, in the sense that its expected\nruntime is O(polylog(1/epsilon)). \n\n"}
{"id": "1403.4781", "contents": "Title: A Split-and-Merge Dictionary Learning Algorithm for Sparse\n  Representation Abstract: In big data image/video analytics, we encounter the problem of learning an\novercomplete dictionary for sparse representation from a large training\ndataset, which can not be processed at once because of storage and\ncomputational constraints. To tackle the problem of dictionary learning in such\nscenarios, we propose an algorithm for parallel dictionary learning. The\nfundamental idea behind the algorithm is to learn a sparse representation in\ntwo phases. In the first phase, the whole training dataset is partitioned into\nsmall non-overlapping subsets, and a dictionary is trained independently on\neach small database. In the second phase, the dictionaries are merged to form a\nglobal dictionary. We show that the proposed algorithm is efficient in its\nusage of memory and computational complexity, and performs on par with the\nstandard learning strategy operating on the entire data at a time. As an\napplication, we consider the problem of image denoising. We present a\ncomparative analysis of our algorithm with the standard learning techniques,\nthat use the entire database at a time, in terms of training and denoising\nperformance. We observe that the split-and-merge algorithm results in a\nremarkable reduction of training time, without significantly affecting the\ndenoising performance. \n\n"}
{"id": "1404.2353", "contents": "Title: Power System Parameters Forecasting Using Hilbert-Huang Transform and\n  Machine Learning Abstract: A novel hybrid data-driven approach is developed for forecasting power system\nparameters with the goal of increasing the efficiency of short-term forecasting\nstudies for non-stationary time-series. The proposed approach is based on mode\ndecomposition and a feature analysis of initial retrospective data using the\nHilbert-Huang transform and machine learning algorithms. The random forests and\ngradient boosting trees learning techniques were examined. The decision tree\ntechniques were used to rank the importance of variables employed in the\nforecasting models. The Mean Decrease Gini index is employed as an impurity\nfunction. The resulting hybrid forecasting models employ the radial basis\nfunction neural network and support vector regression. Apart from introduction\nand references the paper is organized as follows. The section 2 presents the\nbackground and the review of several approaches for short-term forecasting of\npower system parameters. In the third section a hybrid machine learning-based\nalgorithm using Hilbert-Huang transform is developed for short-term forecasting\nof power system parameters. Fourth section describes the decision tree learning\nalgorithms used for the issue of variables importance. Finally in section six\nthe experimental results in the following electric power problems are\npresented: active power flow forecasting, electricity price forecasting and for\nthe wind speed and direction forecasting. \n\n"}
{"id": "1404.2644", "contents": "Title: A Distributed Frank-Wolfe Algorithm for Communication-Efficient Sparse\n  Learning Abstract: Learning sparse combinations is a frequent theme in machine learning. In this\npaper, we study its associated optimization problem in the distributed setting\nwhere the elements to be combined are not centrally located but spread over a\nnetwork. We address the key challenges of balancing communication costs and\noptimization errors. To this end, we propose a distributed Frank-Wolfe (dFW)\nalgorithm. We obtain theoretical guarantees on the optimization error\n$\\epsilon$ and communication cost that do not depend on the total number of\ncombining elements. We further show that the communication cost of dFW is\noptimal by deriving a lower-bound on the communication cost required to\nconstruct an $\\epsilon$-approximate solution. We validate our theoretical\nanalysis with empirical studies on synthetic and real-world data, which\ndemonstrate that dFW outperforms both baselines and competing methods. We also\nstudy the performance of dFW when the conditions of our analysis are relaxed,\nand show that dFW is fairly robust. \n\n"}
{"id": "1404.3747", "contents": "Title: Low-distance Surface Codes under Realistic Quantum Noise Abstract: We study the performance of distance-three surface code layouts under\nrealistic multi-parameter noise models. We first calculate their thresholds\nunder depolarizing noise. We then compare a Pauli-twirl approximation of\namplitude and phase damping to amplitude and phase damping. We find the\napproximate channel results in a pessimistic estimate of the logical error\nrate, indicating the realistic threshold may be higher than previously\nestimated. From Monte-Carlo simulations, we identify experimental parameters\nfor which these layouts admit reliable computation. Due to its low resource\ncost and superior performance, we conclude that the 17-qubit layout should be\ntargeted in early experimental implementations of the surface code. We find\nthat architectures with gate times in the 5-40 ns range and T1 times of at\nleast 1-2 us range will exhibit improved logical error rates with a 17-qubit\nsurface code encoding. \n\n"}
{"id": "1404.5320", "contents": "Title: Efficient synthesis of universal Repeat-Until-Success circuits Abstract: Recently, it was shown that Repeat-Until-Success (RUS) circuits can achieve a\n$2.5$ times reduction in expected $T$-count over ancilla-free techniques for\nsingle-qubit unitary decomposition. However, the previously best known\nalgorithm to synthesize RUS circuits requires exponential classical runtime. In\nthis paper we present an algorithm to synthesize an RUS circuit to approximate\nany given single-qubit unitary within precision $\\varepsilon$ in\nprobabilistically polynomial classical runtime. Our synthesis approach uses the\nClifford+$T$ basis, plus one ancilla qubit and measurement. We provide\nnumerical evidence that our RUS circuits have an expected $T$-count on average\n$2.5$ times lower than the theoretical lower bound of $3 \\log_2\n(1/\\varepsilon)$ for ancilla-free single-qubit circuit decomposition. \n\n"}
{"id": "1405.3726", "contents": "Title: Topic words analysis based on LDA model Abstract: Social network analysis (SNA), which is a research field describing and\nmodeling the social connection of a certain group of people, is popular among\nnetwork services. Our topic words analysis project is a SNA method to visualize\nthe topic words among emails from Obama.com to accounts registered in Columbus,\nOhio. Based on Latent Dirichlet Allocation (LDA) model, a popular topic model\nof SNA, our project characterizes the preference of senders for target group of\nreceptors. Gibbs sampling is used to estimate topic and word distribution. Our\ntraining and testing data are emails from the carbon-free server\nDatagreening.com. We use parallel computing tool BashReduce for word processing\nand generate related words under each latent topic to discovers typical\ninformation of political news sending specially to local Columbus receptors.\nRunning on two instances using paralleling tool BashReduce, our project\ncontributes almost 30% speedup processing the raw contents, comparing with\nprocessing contents on one instance locally. Also, the experimental result\nshows that the LDA model applied in our project provides precision rate 53.96%\nhigher than TF-IDF model finding target words, on the condition that\nappropriate size of topic words list is selected. \n\n"}
{"id": "1405.4897", "contents": "Title: Screening Tests for Lasso Problems Abstract: This paper is a survey of dictionary screening for the lasso problem. The\nlasso problem seeks a sparse linear combination of the columns of a dictionary\nto best match a given target vector. This sparse representation has proven\nuseful in a variety of subsequent processing and decision tasks. For a given\ntarget vector, dictionary screening quickly identifies a subset of dictionary\ncolumns that will receive zero weight in a solution of the corresponding lasso\nproblem. These columns can be removed from the dictionary prior to solving the\nlasso problem without impacting the optimality of the solution obtained. This\nhas two potential advantages: it reduces the size of the dictionary, allowing\nthe lasso problem to be solved with less resources, and it may speed up\nobtaining a solution. Using a geometrically intuitive framework, we provide\nbasic insights for understanding useful lasso screening tests and their\nlimitations. We also provide illustrative numerical studies on several\ndatasets. \n\n"}
{"id": "1406.4566", "contents": "Title: Guaranteed Scalable Learning of Latent Tree Models Abstract: We present an integrated approach for structure and parameter estimation in\nlatent tree graphical models. Our overall approach follows a\n\"divide-and-conquer\" strategy that learns models over small groups of variables\nand iteratively merges onto a global solution. The structure learning involves\ncombinatorial operations such as minimum spanning tree construction and local\nrecursive grouping; the parameter learning is based on the method of moments\nand on tensor decompositions. Our method is guaranteed to correctly recover the\nunknown tree structure and the model parameters with low sample complexity for\nthe class of linear multivariate latent tree models which includes discrete and\nGaussian distributions, and Gaussian mixtures. Our bulk asynchronous parallel\nalgorithm is implemented in parallel and the parallel computation complexity\nincreases only logarithmically with the number of variables and linearly with\ndimensionality of each variable. \n\n"}
{"id": "1407.0286", "contents": "Title: DC approximation approaches for sparse optimization Abstract: Sparse optimization refers to an optimization problem involving the zero-norm\nin objective or constraints. In this paper, nonconvex approximation approaches\nfor sparse optimization have been studied with a unifying point of view in DC\n(Difference of Convex functions) programming framework. Considering a common DC\napproximation of the zero-norm including all standard sparse inducing penalty\nfunctions, we studied the consistency between global minimums (resp. local\nminimums) of approximate and original problems. We showed that, in several\ncases, some global minimizers (resp. local minimizers) of the approximate\nproblem are also those of the original problem. Using exact penalty techniques\nin DC programming, we proved stronger results for some particular\napproximations, namely, the approximate problem, with suitable parameters, is\nequivalent to the original problem. The efficiency of several sparse inducing\npenalty functions have been fully analyzed. Four DCA (DC Algorithm) schemes\nwere developed that cover all standard algorithms in nonconvex sparse\napproximation approaches as special versions. They can be viewed as, an $\\ell\n_{1}$-perturbed algorithm / reweighted-$\\ell _{1}$ algorithm / reweighted-$\\ell\n_{1}$ algorithm. We offer a unifying nonconvex approximation approach, with\nsolid theoretical tools as well as efficient algorithms based on DC programming\nand DCA, to tackle the zero-norm and sparse optimization. As an application, we\nimplemented our methods for the feature selection in SVM (Support Vector\nMachine) problem and performed empirical comparative numerical experiments on\nthe proposed algorithms with various approximation functions. \n\n"}
{"id": "1407.4729", "contents": "Title: Sparse Partially Linear Additive Models Abstract: The generalized partially linear additive model (GPLAM) is a flexible and\ninterpretable approach to building predictive models. It combines features in\nan additive manner, allowing each to have either a linear or nonlinear effect\non the response. However, the choice of which features to treat as linear or\nnonlinear is typically assumed known. Thus, to make a GPLAM a viable approach\nin situations in which little is known $a~priori$ about the features, one must\novercome two primary model selection challenges: deciding which features to\ninclude in the model and determining which of these features to treat\nnonlinearly. We introduce the sparse partially linear additive model (SPLAM),\nwhich combines model fitting and $both$ of these model selection challenges\ninto a single convex optimization problem. SPLAM provides a bridge between the\nlasso and sparse additive models. Through a statistical oracle inequality and\nthorough simulation, we demonstrate that SPLAM can outperform other methods\nacross a broad spectrum of statistical regimes, including the high-dimensional\n($p\\gg N$) setting. We develop efficient algorithms that are applied to real\ndata sets with half a million samples and over 45,000 features with excellent\npredictive performance. \n\n"}
{"id": "1408.2552", "contents": "Title: Comparing Nonparametric Bayesian Tree Priors for Clonal Reconstruction\n  of Tumors Abstract: Statistical machine learning methods, especially nonparametric Bayesian\nmethods, have become increasingly popular to infer clonal population structure\nof tumors. Here we describe the treeCRP, an extension of the Chinese restaurant\nprocess (CRP), a popular construction used in nonparametric mixture models, to\ninfer the phylogeny and genotype of major subclonal lineages represented in the\npopulation of cancer cells. We also propose new split-merge updates tailored to\nthe subclonal reconstruction problem that improve the mixing time of Markov\nchains. In comparisons with the tree-structured stick breaking prior used in\nPhyloSub, we demonstrate superior mixing and running time using the treeCRP\nwith our new split-merge procedures. We also show that given the same number of\nsamples, TSSB and treeCRP have similar ability to recover the subclonal\nstructure of a tumor. \n\n"}
{"id": "1408.3060", "contents": "Title: Fastfood: Approximate Kernel Expansions in Loglinear Time Abstract: Despite their successes, what makes kernel methods difficult to use in many\nlarge scale problems is the fact that storing and computing the decision\nfunction is typically expensive, especially at prediction time. In this paper,\nwe overcome this difficulty by proposing Fastfood, an approximation that\naccelerates such computation significantly. Key to Fastfood is the observation\nthat Hadamard matrices, when combined with diagonal Gaussian matrices, exhibit\nproperties similar to dense Gaussian random matrices. Yet unlike the latter,\nHadamard and diagonal matrices are inexpensive to multiply and store. These two\nmatrices can be used in lieu of Gaussian matrices in Random Kitchen Sinks\nproposed by Rahimi and Recht (2009) and thereby speeding up the computation for\na large range of kernel functions. Specifically, Fastfood requires O(n log d)\ntime and O(n) storage to compute n non-linear basis functions in d dimensions,\na significant improvement from O(nd) computation and storage, without\nsacrificing accuracy.\n  Our method applies to any translation invariant and any dot-product kernel,\nsuch as the popular RBF kernels and polynomial kernels. We prove that the\napproximation is unbiased and has low variance. Experiments show that we\nachieve similar accuracy to full kernel expansions and Random Kitchen Sinks\nwhile being 100x faster and using 1000x less memory. These improvements,\nespecially in terms of memory usage, make kernel methods more practical for\napplications that have large training sets and/or require real-time prediction. \n\n"}
{"id": "1408.3467", "contents": "Title: Evaluating Visual Properties via Robust HodgeRank Abstract: Nowadays, how to effectively evaluate visual properties has become a popular\ntopic for fine-grained visual comprehension. In this paper we study the problem\nof how to estimate such visual properties from a ranking perspective with the\nhelp of the annotators from online crowdsourcing platforms. The main challenges\nof our task are two-fold. On one hand, the annotations often contain\ncontaminated information, where a small fraction of label flips might ruin the\nglobal ranking of the whole dataset. On the other hand, considering the large\ndata capacity, the annotations are often far from being complete. What is\nworse, there might even exist imbalanced annotations where a small subset of\nsamples are frequently annotated. Facing such challenges, we propose a robust\nranking framework based on the principle of Hodge decomposition of imbalanced\nand incomplete ranking data. According to the HodgeRank theory, we find that\nthe major source of the contamination comes from the cyclic ranking component\nof the Hodge decomposition. This leads us to an outlier detection formulation\nas sparse approximations of the cyclic ranking projection. Taking a step\nfurther, it facilitates a novel outlier detection model as Huber's LASSO in\nrobust statistics. Moreover, simple yet scalable algorithms are developed based\non Linearized Bregman Iteration to achieve an even less biased estimator.\nStatistical consistency of outlier detection is established in both cases under\nnearly the same conditions. Our studies are supported by experiments with both\nsimulated examples and real-world data. The proposed framework provides us a\npromising tool for robust ranking with large scale crowdsourcing data arising\nfrom computer vision. \n\n"}
{"id": "1408.6202", "contents": "Title: The exact synthesis of 1- and 2-qubit Clifford+T circuits Abstract: We describe a new method for the decomposition of an arbitrary $n$ qubit\noperator with entries in $\\mathbb{Z}[i,\\frac{1}{\\sqrt{2}}]$, i.e., of the form\n$(a+b\\sqrt{2}+i(c+d\\sqrt{2}))/{\\sqrt{2}^{k}}$, into Clifford+$T$ operators\nwhere $n\\le 2$. This method achieves a bound of $O(k)$ gates using at most one\nancilla using decomposition into $1$- and $2$-level matrices which was first\nproposed by Giles and Selinger. \n\n"}
{"id": "1409.3552", "contents": "Title: Efficient synthesis of probabilistic quantum circuits with fallback Abstract: Recently it has been shown that Repeat-Until-Success (RUS) circuits can\napproximate a given single-qubit unitary with an expected number of $T$ gates\nof about $1/3$ of what is required by optimal, deterministic, ancilla-free\ndecompositions over the Clifford+$T$ gate set. In this work, we introduce a\nmore general and conceptually simpler circuit decomposition method that allows\nfor synthesis into protocols that probabilistically implement quantum circuits\nover several universal gate sets including, but not restricted to, the\nClifford+$T$ gate set. The protocol, which we call Probabilistic Quantum\nCircuits with Fallback (PQF), implements a walk on a discrete Markov chain in\nwhich the target unitary is an absorbing state and in which transitions are\ninduced by multi-qubit unitaries followed by measurements. In contrast to RUS\nprotocols, the presented PQF protocols terminate after a finite number of\nsteps. Specifically, we apply our method to the Clifford+$T$, Clifford+$V$, and\nClifford+$\\pi/12$ gate sets to achieve decompositions with expected gate counts\nof $\\log_b(1/\\varepsilon)+O(\\log(\\log(1/\\varepsilon)))$, where $b$ is a\nquantity related to the expansion property of the underlying universal gate\nset. \n\n"}
{"id": "1409.4355", "contents": "Title: Optimal ancilla-free Clifford+V approximation of z-rotations Abstract: We describe a new efficient algorithm to approximate z-rotations by\nancilla-free Clifford+V circuits, up to a given precision epsilon. Our\nalgorithm is optimal in the presence of an oracle for integer factoring: it\noutputs the shortest Clifford+V circuit solving the given problem instance. In\nthe absence of such an oracle, our algorithm is still near-optimal, producing\ncircuits of V-count m + O(log(log(1/epsilon))), where m is the V-count of the\nthird-to-optimal solution. A restricted version of the algorithm approximates\nz-rotations in the Pauli+V gate set. Our method is based on previous work by\nthe author and Selinger on the optimal ancilla-free approximation of\nz-rotations using Clifford+T gates and on previous work by Bocharov, Gurevich,\nand Svore on the asymptotically optimal ancilla-free approximation of\nz-rotations using Clifford+V gates. \n\n"}
{"id": "1410.0759", "contents": "Title: cuDNN: Efficient Primitives for Deep Learning Abstract: We present a library of efficient implementations of deep learning\nprimitives. Deep learning workloads are computationally intensive, and\noptimizing their kernels is difficult and time-consuming. As parallel\narchitectures evolve, kernels must be reoptimized, which makes maintaining\ncodebases difficult over time. Similar issues have long been addressed in the\nHPC community by libraries such as the Basic Linear Algebra Subroutines (BLAS).\nHowever, there is no analogous library for deep learning. Without such a\nlibrary, researchers implementing deep learning workloads on parallel\nprocessors must create and optimize their own implementations of the main\ncomputational kernels, and this work must be repeated as new parallel\nprocessors emerge. To address this problem, we have created a library similar\nin intent to BLAS, with optimized routines for deep learning workloads. Our\nimplementation contains routines for GPUs, although similarly to the BLAS\nlibrary, these routines could be implemented for other platforms. The library\nis easy to integrate into existing frameworks, and provides optimized\nperformance and memory usage. For example, integrating cuDNN into Caffe, a\npopular framework for convolutional networks, improves performance by 36% on a\nstandard model while also reducing memory consumption. \n\n"}
{"id": "1410.5884", "contents": "Title: Mean-Field Networks Abstract: The mean field algorithm is a widely used approximate inference algorithm for\ngraphical models whose exact inference is intractable. In each iteration of\nmean field, the approximate marginals for each variable are updated by getting\ninformation from the neighbors. This process can be equivalently converted into\na feedforward network, with each layer representing one iteration of mean field\nand with tied weights on all layers. This conversion enables a few natural\nextensions, e.g. untying the weights in the network. In this paper, we study\nthese mean field networks (MFNs), and use them as inference tools as well as\ndiscriminative models. Preliminary experiment results show that MFNs can learn\nto do inference very efficiently and perform significantly better than mean\nfield as discriminative models. \n\n"}
{"id": "1410.6776", "contents": "Title: Online and Stochastic Gradient Methods for Non-decomposable Loss\n  Functions Abstract: Modern applications in sensitive domains such as biometrics and medicine\nfrequently require the use of non-decomposable loss functions such as\nprecision@k, F-measure etc. Compared to point loss functions such as\nhinge-loss, these offer much more fine grained control over prediction, but at\nthe same time present novel challenges in terms of algorithm design and\nanalysis. In this work we initiate a study of online learning techniques for\nsuch non-decomposable loss functions with an aim to enable incremental learning\nas well as design scalable solvers for batch problems. To this end, we propose\nan online learning framework for such loss functions. Our model enjoys several\nnice properties, chief amongst them being the existence of efficient online\nlearning algorithms with sublinear regret and online to batch conversion\nbounds. Our model is a provable extension of existing online learning models\nfor point loss functions. We instantiate two popular losses, prec@k and pAUC,\nin our model and prove sublinear regret bounds for both of them. Our proofs\nrequire a novel structural lemma over ranked lists which may be of independent\ninterest. We then develop scalable stochastic gradient descent solvers for\nnon-decomposable loss functions. We show that for a large family of loss\nfunctions satisfying a certain uniform convergence property (that includes\nprec@k, pAUC, and F-measure), our methods provably converge to the empirical\nrisk minimizer. Such uniform convergence results were not known for these\nlosses and we establish these using novel proof techniques. We then use\nextensive experimentation on real life and benchmark datasets to establish that\nour method can be orders of magnitude faster than a recently proposed cutting\nplane method. \n\n"}
{"id": "1411.0023", "contents": "Title: Validation of Matching Abstract: We introduce a technique to compute probably approximately correct (PAC)\nbounds on precision and recall for matching algorithms. The bounds require some\nverified matches, but those matches may be used to develop the algorithms. The\nbounds can be applied to network reconciliation or entity resolution\nalgorithms, which identify nodes in different networks or values in a data set\nthat correspond to the same entity. For network reconciliation, the bounds do\nnot require knowledge of the network generation process. \n\n"}
{"id": "1411.0972", "contents": "Title: Convex Optimization for Big Data Abstract: This article reviews recent advances in convex optimization algorithms for\nBig Data, which aim to reduce the computational, storage, and communications\nbottlenecks. We provide an overview of this emerging field, describe\ncontemporary approximation techniques like first-order methods and\nrandomization for scalability, and survey the important role of parallel and\ndistributed computation. The new Big Data algorithms are based on surprisingly\nsimple principles and attain staggering accelerations even on classical\nproblems. \n\n"}
{"id": "1411.1158", "contents": "Title: On the Complexity of Learning with Kernels Abstract: A well-recognized limitation of kernel learning is the requirement to handle\na kernel matrix, whose size is quadratic in the number of training examples.\nMany methods have been proposed to reduce this computational cost, mostly by\nusing a subset of the kernel matrix entries, or some form of low-rank matrix\napproximation, or a random projection method. In this paper, we study lower\nbounds on the error attainable by such methods as a function of the number of\nentries observed in the kernel matrix or the rank of an approximate kernel\nmatrix. We show that there are kernel learning problems where no such method\nwill lead to non-trivial computational savings. Our results also quantify how\nthe problem difficulty depends on parameters such as the nature of the loss\nfunction, the regularization parameter, the norm of the desired predictor, and\nthe kernel matrix rank. Our results also suggest cases where more efficient\nkernel learning might be possible. \n\n"}
{"id": "1411.1792", "contents": "Title: How transferable are features in deep neural networks? Abstract: Many deep neural networks trained on natural images exhibit a curious\nphenomenon in common: on the first layer they learn features similar to Gabor\nfilters and color blobs. Such first-layer features appear not to be specific to\na particular dataset or task, but general in that they are applicable to many\ndatasets and tasks. Features must eventually transition from general to\nspecific by the last layer of the network, but this transition has not been\nstudied extensively. In this paper we experimentally quantify the generality\nversus specificity of neurons in each layer of a deep convolutional neural\nnetwork and report a few surprising results. Transferability is negatively\naffected by two distinct issues: (1) the specialization of higher layer neurons\nto their original task at the expense of performance on the target task, which\nwas expected, and (2) optimization difficulties related to splitting networks\nbetween co-adapted neurons, which was not expected. In an example network\ntrained on ImageNet, we demonstrate that either of these two issues may\ndominate, depending on whether features are transferred from the bottom,\nmiddle, or top of the network. We also document that the transferability of\nfeatures decreases as the distance between the base task and target task\nincreases, but that transferring features even from distant tasks can be better\nthan using random features. A final surprising result is that initializing a\nnetwork with transferred features from almost any number of layers can produce\na boost to generalization that lingers even after fine-tuning to the target\ndataset. \n\n"}
{"id": "1411.1990", "contents": "Title: A totally unimodular view of structured sparsity Abstract: This paper describes a simple framework for structured sparse recovery based\non convex optimization. We show that many structured sparsity models can be\nnaturally represented by linear matrix inequalities on the support of the\nunknown parameters, where the constraint matrix has a totally unimodular (TU)\nstructure. For such structured models, tight convex relaxations can be obtained\nin polynomial time via linear programming. Our modeling framework unifies the\nprevalent structured sparsity norms in the literature, introduces new\ninteresting ones, and renders their tightness and tractability arguments\ntransparent. \n\n"}
{"id": "1411.5928", "contents": "Title: Learning to Generate Chairs, Tables and Cars with Convolutional Networks Abstract: We train generative 'up-convolutional' neural networks which are able to\ngenerate images of objects given object style, viewpoint, and color. We train\nthe networks on rendered 3D models of chairs, tables, and cars. Our experiments\nshow that the networks do not merely learn all images by heart, but rather find\na meaningful representation of 3D models allowing them to assess the similarity\nof different models, interpolate between given views to generate the missing\nones, extrapolate views, and invent new objects not present in the training set\nby recombining training instances, or even two different object classes.\nMoreover, we show that such generative networks can be used to find\ncorrespondences between different objects from the dataset, outperforming\nexisting approaches on this task. \n\n"}
{"id": "1412.2863", "contents": "Title: Score Function Features for Discriminative Learning: Matrix and Tensor\n  Framework Abstract: Feature learning forms the cornerstone for tackling challenging learning\nproblems in domains such as speech, computer vision and natural language\nprocessing. In this paper, we consider a novel class of matrix and\ntensor-valued features, which can be pre-trained using unlabeled samples. We\npresent efficient algorithms for extracting discriminative information, given\nthese pre-trained features and labeled samples for any related task. Our class\nof features are based on higher-order score functions, which capture local\nvariations in the probability density function of the input. We establish a\ntheoretical framework to characterize the nature of discriminative information\nthat can be extracted from score-function features, when used in conjunction\nwith labeled samples. We employ efficient spectral decomposition algorithms (on\nmatrices and tensors) for extracting discriminative components. The advantage\nof employing tensor-valued features is that we can extract richer\ndiscriminative information in the form of an overcomplete representations.\nThus, we present a novel framework for employing generative models of the input\nfor discriminative learning. \n\n"}
{"id": "1412.2954", "contents": "Title: Max vs Min: Tensor Decomposition and ICA with nearly Linear Sample\n  Complexity Abstract: We present a simple, general technique for reducing the sample complexity of\nmatrix and tensor decomposition algorithms applied to distributions. We use the\ntechnique to give a polynomial-time algorithm for standard ICA with sample\ncomplexity nearly linear in the dimension, thereby improving substantially on\nprevious bounds. The analysis is based on properties of random polynomials,\nnamely the spacings of an ensemble of polynomials. Our technique also applies\nto other applications of tensor decompositions, including spherical Gaussian\nmixture models. \n\n"}
{"id": "1412.4182", "contents": "Title: The Statistics of Streaming Sparse Regression Abstract: We present a sparse analogue to stochastic gradient descent that is\nguaranteed to perform well under similar conditions to the lasso. In the linear\nregression setup with irrepresentable noise features, our algorithm recovers\nthe support set of the optimal parameter vector with high probability, and\nachieves a statistically quasi-optimal rate of convergence of Op(k log(d)/T),\nwhere k is the sparsity of the solution, d is the number of features, and T is\nthe number of training examples. Meanwhile, our algorithm does not require any\nmore computational resources than stochastic gradient descent. In our\nexperiments, we find that our method substantially out-performs existing\nstreaming algorithms on both real and simulated data. \n\n"}
{"id": "1412.5567", "contents": "Title: Deep Speech: Scaling up end-to-end speech recognition Abstract: We present a state-of-the-art speech recognition system developed using\nend-to-end deep learning. Our architecture is significantly simpler than\ntraditional speech systems, which rely on laboriously engineered processing\npipelines; these traditional systems also tend to perform poorly when used in\nnoisy environments. In contrast, our system does not need hand-designed\ncomponents to model background noise, reverberation, or speaker variation, but\ninstead directly learns a function that is robust to such effects. We do not\nneed a phoneme dictionary, nor even the concept of a \"phoneme.\" Key to our\napproach is a well-optimized RNN training system that uses multiple GPUs, as\nwell as a set of novel data synthesis techniques that allow us to efficiently\nobtain a large amount of varied data for training. Our system, called Deep\nSpeech, outperforms previously published results on the widely studied\nSwitchboard Hub5'00, achieving 16.0% error on the full test set. Deep Speech\nalso handles challenging noisy environments better than widely used,\nstate-of-the-art commercial speech systems. \n\n"}
{"id": "1412.6544", "contents": "Title: Qualitatively characterizing neural network optimization problems Abstract: Training neural networks involves solving large-scale non-convex optimization\nproblems. This task has long been believed to be extremely difficult, with fear\nof local minima and other obstacles motivating a variety of schemes to improve\noptimization, such as unsupervised pretraining. However, modern neural networks\nare able to achieve negligible training error on complex tasks, using only\ndirect training with stochastic gradient descent. We introduce a simple\nanalysis technique to look for evidence that such networks are overcoming local\noptima. We find that, in fact, on a straight path from initialization to\nsolution, a variety of state of the art neural networks never encounter any\nsignificant obstacles. \n\n"}
{"id": "1412.6558", "contents": "Title: Random Walk Initialization for Training Very Deep Feedforward Networks Abstract: Training very deep networks is an important open problem in machine learning.\nOne of many difficulties is that the norm of the back-propagated error gradient\ncan grow or decay exponentially. Here we show that training very deep\nfeed-forward networks (FFNs) is not as difficult as previously thought. Unlike\nwhen back-propagation is applied to a recurrent network, application to an FFN\namounts to multiplying the error gradient by a different random matrix at each\nlayer. We show that the successive application of correctly scaled random\nmatrices to an initial vector results in a random walk of the log of the norm\nof the resulting vectors, and we compute the scaling that makes this walk\nunbiased. The variance of the random walk grows only linearly with network\ndepth and is inversely proportional to the size of each layer. Practically,\nthis implies a gradient whose log-norm scales with the square root of the\nnetwork depth and shows that the vanishing gradient problem can be mitigated by\nincreasing the width of the layers. Mathematical analyses and experimental\nresults using stochastic gradient descent to optimize tasks related to the\nMNIST and TIMIT datasets are provided to support these claims. Equations for\nthe optimal matrix scaling are provided for the linear and ReLU cases. \n\n"}
{"id": "1412.6980", "contents": "Title: Adam: A Method for Stochastic Optimization Abstract: We introduce Adam, an algorithm for first-order gradient-based optimization\nof stochastic objective functions, based on adaptive estimates of lower-order\nmoments. The method is straightforward to implement, is computationally\nefficient, has little memory requirements, is invariant to diagonal rescaling\nof the gradients, and is well suited for problems that are large in terms of\ndata and/or parameters. The method is also appropriate for non-stationary\nobjectives and problems with very noisy and/or sparse gradients. The\nhyper-parameters have intuitive interpretations and typically require little\ntuning. Some connections to related algorithms, on which Adam was inspired, are\ndiscussed. We also analyze the theoretical convergence properties of the\nalgorithm and provide a regret bound on the convergence rate that is comparable\nto the best known results under the online convex optimization framework.\nEmpirical results demonstrate that Adam works well in practice and compares\nfavorably to other stochastic optimization methods. Finally, we discuss AdaMax,\na variant of Adam based on the infinity norm. \n\n"}
{"id": "1412.7210", "contents": "Title: Denoising autoencoder with modulated lateral connections learns\n  invariant representations of natural images Abstract: Suitable lateral connections between encoder and decoder are shown to allow\nhigher layers of a denoising autoencoder (dAE) to focus on invariant\nrepresentations. In regular autoencoders, detailed information needs to be\ncarried through the highest layers but lateral connections from encoder to\ndecoder relieve this pressure. It is shown that abstract invariant features can\nbe translated to detailed reconstructions when invariant features are allowed\nto modulate the strength of the lateral connection. Three dAE structures with\nmodulated and additive lateral connections, and without lateral connections\nwere compared in experiments using real-world images. The experiments verify\nthat adding modulated lateral connections to the model 1) improves the accuracy\nof the probability model for inputs, as measured by denoising performance; 2)\nresults in representations whose degree of invariance grows faster towards the\nhigher layers; and 3) supports the formation of diverse invariant poolings. \n\n"}
{"id": "1501.00263", "contents": "Title: Communication-Efficient Distributed Optimization of Self-Concordant\n  Empirical Loss Abstract: We consider distributed convex optimization problems originated from sample\naverage approximation of stochastic optimization, or empirical risk\nminimization in machine learning. We assume that each machine in the\ndistributed computing system has access to a local empirical loss function,\nconstructed with i.i.d. data sampled from a common distribution. We propose a\ncommunication-efficient distributed algorithm to minimize the overall empirical\nloss, which is the average of the local empirical losses. The algorithm is\nbased on an inexact damped Newton method, where the inexact Newton steps are\ncomputed by a distributed preconditioned conjugate gradient method. We analyze\nits iteration complexity and communication efficiency for minimizing\nself-concordant empirical loss functions, and discuss the results for\ndistributed ridge regression, logistic regression and binary classification\nwith a smoothed hinge loss. In a standard setting for supervised learning, the\nrequired number of communication rounds of the algorithm does not increase with\nthe sample size, and only grows slowly with the number of machines. \n\n"}
{"id": "1501.00559", "contents": "Title: The Learnability of Unknown Quantum Measurements Abstract: Quantum machine learning has received significant attention in recent years,\nand promising progress has been made in the development of quantum algorithms\nto speed up traditional machine learning tasks. In this work, however, we focus\non investigating the information-theoretic upper bounds of sample complexity -\nhow many training samples are sufficient to predict the future behaviour of an\nunknown target function. This kind of problem is, arguably, one of the most\nfundamental problems in statistical learning theory and the bounds for\npractical settings can be completely characterised by a simple measure of\ncomplexity.\n  Our main result in the paper is that, for learning an unknown quantum\nmeasurement, the upper bound, given by the fat-shattering dimension, is\nlinearly proportional to the dimension of the underlying Hilbert space.\nLearning an unknown quantum state becomes a dual problem to ours, and as a\nbyproduct, we can recover Aaronson's famous result [Proc. R. Soc. A\n463:3089-3144 (2007)] solely using a classical machine learning technique. In\naddition, other famous complexity measures like covering numbers and Rademacher\ncomplexities are derived explicitly. We are able to connect measures of sample\ncomplexity with various areas in quantum information science, e.g. quantum\nstate/measurement tomography, quantum state discrimination and quantum random\naccess codes, which may be of independent interest. Lastly, with the assistance\nof general Bloch-sphere representation, we show that learning quantum\nmeasurements/states can be mathematically formulated as a neural network.\nConsequently, classical ML algorithms can be applied to efficiently accomplish\nthe two quantum learning tasks. \n\n"}
{"id": "1501.02524", "contents": "Title: Design of a Universal Logic Block for Fault-Tolerant Realization of any\n  Logic Operation in Trapped-Ion Quantum Circuits Abstract: This paper presents a physical mapping tool for quantum circuits, which\ngenerates the optimal Universal Logic Block (ULB) that can perform any logical\nfault-tolerant (FT) quantum operations with the minimum latency. The operation\nscheduling, placement, and qubit routing problems tackled by the quantum\nphysical mapper are highly dependent on one another. More precisely, the\nscheduling solution affects the quality of the achievable placement solution\ndue to resource pressures that may be created as a result of operation\nscheduling whereas the operation placement and qubit routing solutions\ninfluence the scheduling solution due to resulting distances between\npredecessor and current operations, which in turn determines routing latencies.\nThe proposed flow for the quantum physical mapper captures these dependencies\nby applying (i) a loose scheduling step, which transforms an initial quantum\ndata flow graph into one that explicitly captures the no-cloning theorem of the\nquantum computing and then performs instruction scheduling based on a modified\nforce-directed scheduling approach to minimize the resource contention and\nquantum circuit latency, (ii) a placement step, which uses timing-driven\ninstruction placement to minimize the approximate routing latencies while\nmaking iterative calls to the aforesaid force-directed scheduler to correct\nscheduling levels of quantum operations as needed, and (iii) a routing step\nthat finds dynamic values of routing latencies for the qubits. In addition to\nthe quantum physical mapper, an approach is presented to determine the single\nbest ULB size for a target quantum circuit by examining the latency of\ndifferent FT quantum operations mapped onto different ULB sizes and using\ninformation about the occurrence frequency of operations on critical paths of\nthe target quantum algorithm to weigh these latencies. \n\n"}
{"id": "1501.03854", "contents": "Title: Understanding Kernel Ridge Regression: Common behaviors from simple\n  functions to density functionals Abstract: Accurate approximations to density functionals have recently been obtained\nvia machine learning (ML). By applying ML to a simple function of one variable\nwithout any random sampling, we extract the qualitative dependence of errors on\nhyperparameters. We find universal features of the behavior in extreme limits,\nincluding both very small and very large length scales, and the noise-free\nlimit. We show how such features arise in ML models of density functionals. \n\n"}
{"id": "1502.02362", "contents": "Title: Counterfactual Risk Minimization: Learning from Logged Bandit Feedback Abstract: We develop a learning principle and an efficient algorithm for batch learning\nfrom logged bandit feedback. This learning setting is ubiquitous in online\nsystems (e.g., ad placement, web search, recommendation), where an algorithm\nmakes a prediction (e.g., ad ranking) for a given input (e.g., query) and\nobserves bandit feedback (e.g., user clicks on presented ads). We first address\nthe counterfactual nature of the learning problem through propensity scoring.\nNext, we prove generalization error bounds that account for the variance of the\npropensity-weighted empirical risk estimator. These constructive bounds give\nrise to the Counterfactual Risk Minimization (CRM) principle. We show how CRM\ncan be used to derive a new learning method -- called Policy Optimizer for\nExponential Models (POEM) -- for learning stochastic linear rules for\nstructured output prediction. We present a decomposition of the POEM objective\nthat enables efficient stochastic gradient optimization. POEM is evaluated on\nseveral multi-label classification problems showing substantially improved\nrobustness and generalization performance compared to the state-of-the-art. \n\n"}
{"id": "1502.04635", "contents": "Title: Parameter estimation in softmax decision-making models with linear\n  objective functions Abstract: With an eye towards human-centered automation, we contribute to the\ndevelopment of a systematic means to infer features of human decision-making\nfrom behavioral data. Motivated by the common use of softmax selection in\nmodels of human decision-making, we study the maximum likelihood parameter\nestimation problem for softmax decision-making models with linear objective\nfunctions. We present conditions under which the likelihood function is convex.\nThese allow us to provide sufficient conditions for convergence of the\nresulting maximum likelihood estimator and to construct its asymptotic\ndistribution. In the case of models with nonlinear objective functions, we show\nhow the estimator can be applied by linearizing about a nominal parameter\nvalue. We apply the estimator to fit the stochastic UCL (Upper Credible Limit)\nmodel of human decision-making to human subject data. We show statistically\nsignificant differences in behavior across related, but distinct, tasks. \n\n"}
{"id": "1502.05890", "contents": "Title: Contextual Semibandits via Supervised Learning Oracles Abstract: We study an online decision making problem where on each round a learner\nchooses a list of items based on some side information, receives a scalar\nfeedback value for each individual item, and a reward that is linearly related\nto this feedback. These problems, known as contextual semibandits, arise in\ncrowdsourcing, recommendation, and many other domains. This paper reduces\ncontextual semibandits to supervised learning, allowing us to leverage powerful\nsupervised learning methods in this partial-feedback setting. Our first\nreduction applies when the mapping from feedback to reward is known and leads\nto a computationally efficient algorithm with near-optimal regret. We show that\nthis algorithm outperforms state-of-the-art approaches on real-world\nlearning-to-rank datasets, demonstrating the advantage of oracle-based\nalgorithms. Our second reduction applies to the previously unstudied setting\nwhen the linear mapping from feedback to reward is unknown. Our regret\nguarantees are superior to prior techniques that ignore the feedback. \n\n"}
{"id": "1502.08053", "contents": "Title: Stochastic Dual Coordinate Ascent with Adaptive Probabilities Abstract: This paper introduces AdaSDCA: an adaptive variant of stochastic dual\ncoordinate ascent (SDCA) for solving the regularized empirical risk\nminimization problems. Our modification consists in allowing the method\nadaptively change the probability distribution over the dual variables\nthroughout the iterative process. AdaSDCA achieves provably better complexity\nbound than SDCA with the best fixed probability distribution, known as\nimportance sampling. However, it is of a theoretical character as it is\nexpensive to implement. We also propose AdaSDCA+: a practical variant which in\nour experiments outperforms existing non-adaptive methods. \n\n"}
{"id": "1503.00024", "contents": "Title: Influence Maximization with Bandits Abstract: We consider the problem of \\emph{influence maximization}, the problem of\nmaximizing the number of people that become aware of a product by finding the\n`best' set of `seed' users to expose the product to. Most prior work on this\ntopic assumes that we know the probability of each user influencing each other\nuser, or we have data that lets us estimate these influences. However, this\ninformation is typically not initially available or is difficult to obtain. To\navoid this assumption, we adopt a combinatorial multi-armed bandit paradigm\nthat estimates the influence probabilities as we sequentially try different\nseed sets. We establish bounds on the performance of this procedure under the\nexisting edge-level feedback as well as a novel and more realistic node-level\nfeedback. Beyond our theoretical results, we describe a practical\nimplementation and experimentally demonstrate its efficiency and effectiveness\non four real datasets. \n\n"}
{"id": "1503.02531", "contents": "Title: Distilling the Knowledge in a Neural Network Abstract: A very simple way to improve the performance of almost any machine learning\nalgorithm is to train many different models on the same data and then to\naverage their predictions. Unfortunately, making predictions using a whole\nensemble of models is cumbersome and may be too computationally expensive to\nallow deployment to a large number of users, especially if the individual\nmodels are large neural nets. Caruana and his collaborators have shown that it\nis possible to compress the knowledge in an ensemble into a single model which\nis much easier to deploy and we develop this approach further using a different\ncompression technique. We achieve some surprising results on MNIST and we show\nthat we can significantly improve the acoustic model of a heavily used\ncommercial system by distilling the knowledge in an ensemble of models into a\nsingle model. We also introduce a new type of ensemble composed of one or more\nfull models and many specialist models which learn to distinguish fine-grained\nclasses that the full models confuse. Unlike a mixture of experts, these\nspecialist models can be trained rapidly and in parallel. \n\n"}
{"id": "1503.06453", "contents": "Title: Experimental quantum annealing: case study involving the graph\n  isomorphism problem Abstract: Quantum annealing is a proposed combinatorial optimization technique meant to\nexploit quantum mechanical effects such as tunneling and entanglement.\nReal-world quantum annealing-based solvers require a combination of annealing\nand classical pre- and post-processing; at this early stage, little is known\nabout how to partition and optimize the processing. This article presents an\nexperimental case study of quantum annealing and some of the factors involved\nin real-world solvers, using a 504-qubit D-Wave Two machine and the graph\nisomorphism problem. To illustrate the role of classical pre-processing, a\ncompact Hamiltonian is presented that enables a reduced Ising model for each\nproblem instance. On random N-vertex graphs, the median number of variables is\nreduced from N^2 to fewer than N lg N and solvable graph sizes increase from N\n= 5 to N = 13. Additionally, a type of classical post-processing error\ncorrection is evaluated. While the solution times are not competitive with\nclassical approaches to graph isomorphism, the enhanced solver ultimately\nclassified correctly every problem that was mapped to the processor and\ndemonstrated clear advantages over the baseline approach. The results shed some\nlight on the nature of real-world quantum annealing and the associated hybrid\nclassical-quantum solvers. \n\n"}
{"id": "1504.04350", "contents": "Title: A framework for exact synthesis Abstract: Exact synthesis is a tool used in algorithms for approximating an arbitrary\nqubit unitary with a sequence of quantum gates from some finite set. These\napproximation algorithms find asymptotically optimal approximations in\nprobabilistic polynomial time, in some cases even finding the optimal solution\nin probabilistic polynomial time given access to an oracle for factoring\nintegers. In this paper, we present a common mathematical structure underlying\nall results related to the exact synthesis of qubit unitaries known to date,\nincluding Clifford+T, Clifford-cyclotomic and V-basis gate sets, as well as\ngates sets induced by the braiding of Fibonacci anyons in topological quantum\ncomputing. The framework presented here also provides a means to answer\nquestions related to the exact synthesis of unitaries for wide classes of other\ngate sets, such as Clifford+T+V and SU(2) level k anyons. \n\n"}
{"id": "1504.08291", "contents": "Title: Deep Neural Networks with Random Gaussian Weights: A Universal\n  Classification Strategy? Abstract: Three important properties of a classification machinery are: (i) the system\npreserves the core information of the input data; (ii) the training examples\nconvey information about unseen data; and (iii) the system is able to treat\ndifferently points from different classes. In this work we show that these\nfundamental properties are satisfied by the architecture of deep neural\nnetworks. We formally prove that these networks with random Gaussian weights\nperform a distance-preserving embedding of the data, with a special treatment\nfor in-class and out-of-class data. Similar points at the input of the network\nare likely to have a similar output. The theoretical analysis of deep networks\nhere presented exploits tools used in the compressed sensing and dictionary\nlearning literature, thereby making a formal connection between these important\ntopics. The derived results allow drawing conclusions on the metric learning\nproperties of the network and their relation to its structure, as well as\nproviding bounds on the required size of the training set such that the\ntraining examples would represent faithfully the unseen data. The results are\nvalidated with state-of-the-art trained networks. \n\n"}
{"id": "1505.00482", "contents": "Title: Risk Bounds For Mode Clustering Abstract: Density mode clustering is a nonparametric clustering method. The clusters\nare the basins of attraction of the modes of a density estimator. We study the\nrisk of mode-based clustering. We show that the clustering risk over the\ncluster cores --- the regions where the density is high --- is very small even\nin high dimensions. And under a low noise condition, the overall cluster risk\nis small even beyond the cores, in high dimensions. \n\n"}
{"id": "1505.01866", "contents": "Title: DART: Dropouts meet Multiple Additive Regression Trees Abstract: Multiple Additive Regression Trees (MART), an ensemble model of boosted\nregression trees, is known to deliver high prediction accuracy for diverse\ntasks, and it is widely used in practice. However, it suffers an issue which we\ncall over-specialization, wherein trees added at later iterations tend to\nimpact the prediction of only a few instances, and make negligible contribution\ntowards the remaining instances. This negatively affects the performance of the\nmodel on unseen data, and also makes the model over-sensitive to the\ncontributions of the few, initially added tress. We show that the commonly used\ntool to address this issue, that of shrinkage, alleviates the problem only to a\ncertain extent and the fundamental issue of over-specialization still remains.\nIn this work, we explore a different approach to address the problem that of\nemploying dropouts, a tool that has been recently proposed in the context of\nlearning deep neural networks. We propose a novel way of employing dropouts in\nMART, resulting in the DART algorithm. We evaluate DART on ranking, regression\nand classification tasks, using large scale, publicly available datasets, and\nshow that DART outperforms MART in each of the tasks, with a significant\nmargin. We also show that DART overcomes the issue of over-specialization to a\nconsiderable extent. \n\n"}
{"id": "1505.02250", "contents": "Title: Newton Sketch: A Linear-time Optimization Algorithm with\n  Linear-Quadratic Convergence Abstract: We propose a randomized second-order method for optimization known as the\nNewton Sketch: it is based on performing an approximate Newton step using a\nrandomly projected or sub-sampled Hessian. For self-concordant functions, we\nprove that the algorithm has super-linear convergence with exponentially high\nprobability, with convergence and complexity guarantees that are independent of\ncondition numbers and related problem-dependent quantities. Given a suitable\ninitialization, similar guarantees also hold for strongly convex and smooth\nobjectives without self-concordance. When implemented using randomized\nprojections based on a sub-sampled Hadamard basis, the algorithm typically has\nsubstantially lower complexity than Newton's method. We also describe\nextensions of our methods to programs involving convex constraints that are\nequipped with self-concordant barriers. We discuss and illustrate applications\nto linear programs, quadratic programs with convex constraints, logistic\nregression and other generalized linear models, as well as semidefinite\nprograms. \n\n"}
{"id": "1505.04252", "contents": "Title: Global Convergence of Unmodified 3-Block ADMM for a Class of Convex\n  Minimization Problems Abstract: The alternating direction method of multipliers (ADMM) has been successfully\napplied to solve structured convex optimization problems due to its superior\npractical performance. The convergence properties of the 2-block ADMM have been\nstudied extensively in the literature. Specifically, it has been proven that\nthe 2-block ADMM globally converges for any penalty parameter $\\gamma>0$. In\nthis sense, the 2-block ADMM allows the parameter to be free, i.e., there is no\nneed to restrict the value for the parameter when implementing this algorithm\nin order to ensure convergence. However, for the 3-block ADMM, Chen \\etal\n\\cite{Chen-admm-failure-2013} recently constructed a counter-example showing\nthat it can diverge if no further condition is imposed. The existing results on\nstudying further sufficient conditions on guaranteeing the convergence of the\n3-block ADMM usually require $\\gamma$ to be smaller than a certain bound, which\nis usually either difficult to compute or too small to make it a practical\nalgorithm. In this paper, we show that the 3-block ADMM still globally\nconverges with any penalty parameter $\\gamma>0$ if the third function $f_3$ in\nthe objective is smooth and strongly convex, and its condition number is in\n$[1,1.0798)$, besides some other mild conditions. This requirement covers an\nimportant class of problems to be called regularized least squares\ndecomposition (RLSD) in this paper. \n\n"}
{"id": "1505.04650", "contents": "Title: Compressed Nonnegative Matrix Factorization is Fast and Accurate Abstract: Nonnegative matrix factorization (NMF) has an established reputation as a\nuseful data analysis technique in numerous applications. However, its usage in\npractical situations is undergoing challenges in recent years. The fundamental\nfactor to this is the increasingly growing size of the datasets available and\nneeded in the information sciences. To address this, in this work we propose to\nuse structured random compression, that is, random projections that exploit the\ndata structure, for two NMF variants: classical and separable. In separable NMF\n(SNMF) the left factors are a subset of the columns of the input matrix. We\npresent suitable formulations for each problem, dealing with different\nrepresentative algorithms within each one. We show that the resulting\ncompressed techniques are faster than their uncompressed variants, vastly\nreduce memory demands, and do not encompass any significant deterioration in\nperformance. The proposed structured random projections for SNMF allow to deal\nwith arbitrarily shaped large matrices, beyond the standard limit of\ntall-and-skinny matrices, granting access to very efficient computations in\nthis general setting. We accompany the algorithmic presentation with\ntheoretical foundations and numerous and diverse examples, showing the\nsuitability of the proposed approaches. \n\n"}
{"id": "1505.04732", "contents": "Title: Layered Adaptive Importance Sampling Abstract: Monte Carlo methods represent the \"de facto\" standard for approximating\ncomplicated integrals involving multidimensional target distributions. In order\nto generate random realizations from the target distribution, Monte Carlo\ntechniques use simpler proposal probability densities to draw candidate\nsamples. The performance of any such method is strictly related to the\nspecification of the proposal distribution, such that unfortunate choices\neasily wreak havoc on the resulting estimators. In this work, we introduce a\nlayered (i.e., hierarchical) procedure to generate samples employed within a\nMonte Carlo scheme. This approach ensures that an appropriate equivalent\nproposal density is always obtained automatically (thus eliminating the risk of\na catastrophic performance), although at the expense of a moderate increase in\nthe complexity. Furthermore, we provide a general unified importance sampling\n(IS) framework, where multiple proposal densities are employed and several IS\nschemes are introduced by applying the so-called deterministic mixture\napproach. Finally, given these schemes, we also propose a novel class of\nadaptive importance samplers using a population of proposals, where the\nadaptation is driven by independent parallel or interacting Markov Chain Monte\nCarlo (MCMC) chains. The resulting algorithms efficiently combine the benefits\nof both IS and MCMC methods. \n\n"}
{"id": "1506.02785", "contents": "Title: On the Error of Random Fourier Features Abstract: Kernel methods give powerful, flexible, and theoretically grounded approaches\nto solving many problems in machine learning. The standard approach, however,\nrequires pairwise evaluations of a kernel function, which can lead to\nscalability issues for very large datasets. Rahimi and Recht (2007) suggested a\npopular approach to handling this problem, known as random Fourier features.\nThe quality of this approximation, however, is not well understood. We improve\nthe uniform error bound of that paper, as well as giving novel understandings\nof the embedding's variance, approximation error, and use in some machine\nlearning methods. We also point out that surprisingly, of the two main variants\nof those features, the more widely used is strictly higher-variance for the\nGaussian kernel and has worse bounds. \n\n"}
{"id": "1506.03338", "contents": "Title: Neural Adaptive Sequential Monte Carlo Abstract: Sequential Monte Carlo (SMC), or particle filtering, is a popular class of\nmethods for sampling from an intractable target distribution using a sequence\nof simpler intermediate distributions. Like other importance sampling-based\nmethods, performance is critically dependent on the proposal distribution: a\nbad proposal can lead to arbitrarily inaccurate estimates of the target\ndistribution. This paper presents a new method for automatically adapting the\nproposal using an approximation of the Kullback-Leibler divergence between the\ntrue posterior and the proposal distribution. The method is very flexible,\napplicable to any parameterized proposal distribution and it supports online\nand batch variants. We use the new framework to adapt powerful proposal\ndistributions with rich parameterizations based upon neural networks leading to\nNeural Adaptive Sequential Monte Carlo (NASMC). Experiments indicate that NASMC\nsignificantly improves inference in a non-linear state space model\noutperforming adaptive proposal methods including the Extended Kalman and\nUnscented Particle Filters. Experiments also indicate that improved inference\ntranslates into improved parameter learning when NASMC is used as a subroutine\nof Particle Marginal Metropolis Hastings. Finally we show that NASMC is able to\ntrain a latent variable recurrent neural network (LV-RNN) achieving results\nthat compete with the state-of-the-art for polymorphic music modelling. NASMC\ncan be seen as bridging the gap between adaptive SMC methods and the recent\nwork in scalable, black-box variational inference. \n\n"}
{"id": "1506.03693", "contents": "Title: Optimization Monte Carlo: Efficient and Embarrassingly Parallel\n  Likelihood-Free Inference Abstract: We describe an embarrassingly parallel, anytime Monte Carlo method for\nlikelihood-free models. The algorithm starts with the view that the\nstochasticity of the pseudo-samples generated by the simulator can be\ncontrolled externally by a vector of random numbers u, in such a way that the\noutcome, knowing u, is deterministic. For each instantiation of u we run an\noptimization procedure to minimize the distance between summary statistics of\nthe simulator and the data. After reweighing these samples using the prior and\nthe Jacobian (accounting for the change of volume in transforming from the\nspace of summary statistics to the space of parameters) we show that this\nweighted ensemble represents a Monte Carlo estimate of the posterior\ndistribution. The procedure can be run embarrassingly parallel (each node\nhandling one sample) and anytime (by allocating resources to the worst\nperforming sample). The procedure is validated on six experiments. \n\n"}
{"id": "1506.03705", "contents": "Title: Random Maxout Features Abstract: In this paper, we propose and study random maxout features, which are\nconstructed by first projecting the input data onto sets of randomly generated\nvectors with Gaussian elements, and then outputing the maximum projection value\nfor each set. We show that the resulting random feature map, when used in\nconjunction with linear models, allows for the locally linear estimation of the\nfunction of interest in classification tasks, and for the locally linear\nembedding of points when used for dimensionality reduction or data\nvisualization. We derive generalization bounds for learning that assess the\nerror in approximating locally linear functions by linear functions in the\nmaxout feature space, and empirically evaluate the efficacy of the approach on\nthe MNIST and TIMIT classification tasks. \n\n"}
{"id": "1506.03877", "contents": "Title: Bidirectional Helmholtz Machines Abstract: Efficient unsupervised training and inference in deep generative models\nremains a challenging problem. One basic approach, called Helmholtz machine,\ninvolves training a top-down directed generative model together with a\nbottom-up auxiliary model used for approximate inference. Recent results\nindicate that better generative models can be obtained with better approximate\ninference procedures. Instead of improving the inference procedure, we here\npropose a new model which guarantees that the top-down and bottom-up\ndistributions can efficiently invert each other. We achieve this by\ninterpreting both the top-down and the bottom-up directed models as approximate\ninference distributions and by defining the model distribution to be the\ngeometric mean of these two. We present a lower-bound for the likelihood of\nthis model and we show that optimizing this bound regularizes the model so that\nthe Bhattacharyya distance between the bottom-up and top-down approximate\ndistributions is minimized. This approach results in state of the art\ngenerative models which prefer significantly deeper architectures while it\nallows for orders of magnitude more efficient approximate inference. \n\n"}
{"id": "1506.07540", "contents": "Title: Global Optimality in Tensor Factorization, Deep Learning, and Beyond Abstract: Techniques involving factorization are found in a wide range of applications\nand have enjoyed significant empirical success in many fields. However, common\nto a vast majority of these problems is the significant disadvantage that the\nassociated optimization problems are typically non-convex due to a multilinear\nform or other convexity destroying transformation. Here we build on ideas from\nconvex relaxations of matrix factorizations and present a very general\nframework which allows for the analysis of a wide range of non-convex\nfactorization problems - including matrix factorization, tensor factorization,\nand deep neural network training formulations. We derive sufficient conditions\nto guarantee that a local minimum of the non-convex optimization problem is a\nglobal minimum and show that if the size of the factorized variables is large\nenough then from any initialization it is possible to find a global minimizer\nusing a purely local descent algorithm. Our framework also provides a partial\ntheoretical justification for the increasingly common use of Rectified Linear\nUnits (ReLUs) in deep neural networks and offers guidance on deep network\narchitectures and regularization strategies to facilitate efficient\noptimization. \n\n"}
{"id": "1506.08105", "contents": "Title: Modelling of directional data using Kent distributions Abstract: The modelling of data on a spherical surface requires the consideration of\ndirectional probability distributions. To model asymmetrically distributed data\non a three-dimensional sphere, Kent distributions are often used. The moment\nestimates of the parameters are typically used in modelling tasks involving\nKent distributions. However, these lack a rigorous statistical treatment. The\nfocus of the paper is to introduce a Bayesian estimation of the parameters of\nthe Kent distribution which has not been carried out in the literature, partly\nbecause of its complex mathematical form. We employ the Bayesian\ninformation-theoretic paradigm of Minimum Message Length (MML) to bridge this\ngap and derive reliable estimators. The inferred parameters are subsequently\nused in mixture modelling of Kent distributions. The problem of inferring the\nsuitable number of mixture components is also addressed using the MML\ncriterion. We demonstrate the superior performance of the derived MML-based\nparameter estimates against the traditional estimators. We apply the MML\nprinciple to infer mixtures of Kent distributions to model empirical data\ncorresponding to protein conformations. We demonstrate the effectiveness of\nKent models to act as improved descriptors of protein structural data as\ncompared to commonly used von Mises-Fisher distributions. \n\n"}
{"id": "1507.00473", "contents": "Title: The Optimal Sample Complexity of PAC Learning Abstract: This work establishes a new upper bound on the number of samples sufficient\nfor PAC learning in the realizable case. The bound matches known lower bounds\nup to numerical constant factors. This solves a long-standing open problem on\nthe sample complexity of PAC learning. The technique and analysis build on a\nrecent breakthrough by Hans Simon. \n\n"}
{"id": "1507.00814", "contents": "Title: Incentivizing Exploration In Reinforcement Learning With Deep Predictive\n  Models Abstract: Achieving efficient and scalable exploration in complex domains poses a major\nchallenge in reinforcement learning. While Bayesian and PAC-MDP approaches to\nthe exploration problem offer strong formal guarantees, they are often\nimpractical in higher dimensions due to their reliance on enumerating the\nstate-action space. Hence, exploration in complex domains is often performed\nwith simple epsilon-greedy methods. In this paper, we consider the challenging\nAtari games domain, which requires processing raw pixel inputs and delayed\nrewards. We evaluate several more sophisticated exploration strategies,\nincluding Thompson sampling and Boltzman exploration, and propose a new\nexploration method based on assigning exploration bonuses from a concurrently\nlearned model of the system dynamics. By parameterizing our learned model with\na neural network, we are able to develop a scalable and efficient approach to\nexploration bonuses that can be applied to tasks with complex, high-dimensional\nstate spaces. In the Atari domain, our method provides the most consistent\nimprovement across a range of games that pose a major challenge for prior\nmethods. In addition to raw game-scores, we also develop an AUC-100 metric for\nthe Atari Learning domain to evaluate the impact of exploration on this\nbenchmark. \n\n"}
{"id": "1507.02189", "contents": "Title: Intersecting Faces: Non-negative Matrix Factorization With New\n  Guarantees Abstract: Non-negative matrix factorization (NMF) is a natural model of admixture and\nis widely used in science and engineering. A plethora of algorithms have been\ndeveloped to tackle NMF, but due to the non-convex nature of the problem, there\nis little guarantee on how well these methods work. Recently a surge of\nresearch have focused on a very restricted class of NMFs, called separable NMF,\nwhere provably correct algorithms have been developed. In this paper, we\npropose the notion of subset-separable NMF, which substantially generalizes the\nproperty of separability. We show that subset-separability is a natural\nnecessary condition for the factorization to be unique or to have minimum\nvolume. We developed the Face-Intersect algorithm which provably and\nefficiently solves subset-separable NMF under natural conditions, and we prove\nthat our algorithm is robust to small noise. We explored the performance of\nFace-Intersect on simulations and discuss settings where it empirically\noutperformed the state-of-art methods. Our work is a step towards finding\nprovably correct algorithms that solve large classes of NMF problems. \n\n"}
{"id": "1507.02268", "contents": "Title: Optimal approximate matrix product in terms of stable rank Abstract: We prove, using the subspace embedding guarantee in a black box way, that one\ncan achieve the spectral norm guarantee for approximate matrix multiplication\nwith a dimensionality-reducing map having $m = O(\\tilde{r}/\\varepsilon^2)$\nrows. Here $\\tilde{r}$ is the maximum stable rank, i.e. squared ratio of\nFrobenius and operator norms, of the two matrices being multiplied. This is a\nquantitative improvement over previous work of [MZ11, KVZ14], and is also\noptimal for any oblivious dimensionality-reducing map. Furthermore, due to the\nblack box reliance on the subspace embedding property in our proofs, our\ntheorem can be applied to a much more general class of sketching matrices than\nwhat was known before, in addition to achieving better bounds. For example, one\ncan apply our theorem to efficient subspace embeddings such as the Subsampled\nRandomized Hadamard Transform or sparse subspace embeddings, or even with\nsubspace embedding constructions that may be developed in the future.\n  Our main theorem, via connections with spectral error matrix multiplication\nshown in prior work, implies quantitative improvements for approximate least\nsquares regression and low rank approximation. Our main result has also already\nbeen applied to improve dimensionality reduction guarantees for $k$-means\nclustering [CEMMP14], and implies new results for nonparametric regression\n[YPW15].\n  We also separately point out that the proof of the \"BSS\" deterministic\nrow-sampling result of [BSS12] can be modified to show that for any matrices\n$A, B$ of stable rank at most $\\tilde{r}$, one can achieve the spectral norm\nguarantee for approximate matrix multiplication of $A^T B$ by deterministically\nsampling $O(\\tilde{r}/\\varepsilon^2)$ rows that can be found in polynomial\ntime. The original result of [BSS12] was for rank instead of stable rank. Our\nobservation leads to a stronger version of a main theorem of [KMST10]. \n\n"}
{"id": "1508.00842", "contents": "Title: Perceptron like Algorithms for Online Learning to Rank Abstract: Perceptron is a classic online algorithm for learning a classification\nfunction. In this paper, we provide a novel extension of the perceptron\nalgorithm to the learning to rank problem in information retrieval. We consider\npopular listwise performance measures such as Normalized Discounted Cumulative\nGain (NDCG) and Average Precision (AP). A modern perspective on perceptron for\nclassification is that it is simply an instance of online gradient descent\n(OGD), during mistake rounds, using the hinge loss function. Motivated by this\ninterpretation, we propose a novel family of listwise, large margin ranking\nsurrogates. Members of this family can be thought of as analogs of the hinge\nloss. Exploiting a certain self-bounding property of the proposed family, we\nprovide a guarantee on the cumulative NDCG (or AP) induced loss incurred by our\nperceptron-like algorithm. We show that, if there exists a perfect oracle\nranker which can correctly rank each instance in an online sequence of ranking\ndata, with some margin, the cumulative loss of perceptron algorithm on that\nsequence is bounded by a constant, irrespective of the length of the sequence.\nThis result is reminiscent of Novikoff's convergence theorem for the\nclassification perceptron. Moreover, we prove a lower bound on the cumulative\nloss achievable by any deterministic algorithm, under the assumption of\nexistence of perfect oracle ranker. The lower bound shows that our perceptron\nbound is not tight, and we propose another, \\emph{purely online}, algorithm\nwhich achieves the lower bound. We provide empirical results on simulated and\nlarge commercial datasets to corroborate our theoretical results. \n\n"}
{"id": "1508.01746", "contents": "Title: Using Deep Learning for Detecting Spoofing Attacks on Speech Signals Abstract: It is well known that speaker verification systems are subject to spoofing\nattacks. The Automatic Speaker Verification Spoofing and Countermeasures\nChallenge -- ASVSpoof2015 -- provides a standard spoofing database, containing\nattacks based on synthetic speech, along with a protocol for experiments. This\npaper describes CPqD's systems submitted to the ASVSpoof2015 Challenge, based\non deep neural networks, working both as a classifier and as a feature\nextraction module for a GMM and a SVM classifier. Results show the validity of\nthis approach, achieving less than 0.5\\% EER for known attacks. \n\n"}
{"id": "1508.03390", "contents": "Title: Doubly Stochastic Primal-Dual Coordinate Method for Bilinear\n  Saddle-Point Problem Abstract: We propose a doubly stochastic primal-dual coordinate optimization algorithm\nfor empirical risk minimization, which can be formulated as a bilinear\nsaddle-point problem. In each iteration, our method randomly samples a block of\ncoordinates of the primal and dual solutions to update. The linear convergence\nof our method could be established in terms of 1) the distance from the current\niterate to the optimal solution and 2) the primal-dual objective gap. We show\nthat the proposed method has a lower overall complexity than existing\ncoordinate methods when either the data matrix has a factorized structure or\nthe proximal mapping on each block is computationally expensive, e.g.,\ninvolving an eigenvalue decomposition. The efficiency of the proposed method is\nconfirmed by empirical studies on several real applications, such as the\nmulti-task large margin nearest neighbor problem. \n\n"}
{"id": "1509.01770", "contents": "Title: Theoretical and Experimental Analyses of Tensor-Based Regression and\n  Classification Abstract: We theoretically and experimentally investigate tensor-based regression and\nclassification. Our focus is regularization with various tensor norms,\nincluding the overlapped trace norm, the latent trace norm, and the scaled\nlatent trace norm. We first give dual optimization methods using the\nalternating direction method of multipliers, which is computationally efficient\nwhen the number of training samples is moderate. We then theoretically derive\nan excess risk bound for each tensor norm and clarify their behavior. Finally,\nwe perform extensive experiments using simulated and real data and demonstrate\nthe superiority of tensor-based learning methods over vector- and matrix-based\nlearning methods. \n\n"}
{"id": "1509.02971", "contents": "Title: Continuous control with deep reinforcement learning Abstract: We adapt the ideas underlying the success of Deep Q-Learning to the\ncontinuous action domain. We present an actor-critic, model-free algorithm\nbased on the deterministic policy gradient that can operate over continuous\naction spaces. Using the same learning algorithm, network architecture and\nhyper-parameters, our algorithm robustly solves more than 20 simulated physics\ntasks, including classic problems such as cartpole swing-up, dexterous\nmanipulation, legged locomotion and car driving. Our algorithm is able to find\npolicies whose performance is competitive with those found by a planning\nalgorithm with full access to the dynamics of the domain and its derivatives.\nWe further demonstrate that for many of the tasks the algorithm can learn\npolicies end-to-end: directly from raw pixel inputs. \n\n"}
{"id": "1509.04612", "contents": "Title: Adapting Resilient Propagation for Deep Learning Abstract: The Resilient Propagation (Rprop) algorithm has been very popular for\nbackpropagation training of multilayer feed-forward neural networks in various\napplications. The standard Rprop however encounters difficulties in the context\nof deep neural networks as typically happens with gradient-based learning\nalgorithms. In this paper, we propose a modification of the Rprop that combines\nstandard Rprop steps with a special drop out technique. We apply the method for\ntraining Deep Neural Networks as standalone components and in ensemble\nformulations. Results on the MNIST dataset show that the proposed modification\nalleviates standard Rprop's problems demonstrating improved learning speed and\naccuracy. \n\n"}
{"id": "1509.05789", "contents": "Title: BLC: Private Matrix Factorization Recommenders via Automatic Group\n  Learning Abstract: We propose a privacy-enhanced matrix factorization recommender that exploits\nthe fact that users can often be grouped together by interest. This allows a\nform of \"hiding in the crowd\" privacy. We introduce a novel matrix\nfactorization approach suited to making recommendations in a shared group (or\nnym) setting and the BLC algorithm for carrying out this matrix factorization\nin a privacy-enhanced manner. We demonstrate that the increased privacy does\nnot come at the cost of reduced recommendation accuracy. \n\n"}
{"id": "1509.05808", "contents": "Title: Word, graph and manifold embedding from Markov processes Abstract: Continuous vector representations of words and objects appear to carry\nsurprisingly rich semantic content. In this paper, we advance both the\nconceptual and theoretical understanding of word embeddings in three ways.\nFirst, we ground embeddings in semantic spaces studied in\ncognitive-psychometric literature and introduce new evaluation tasks. Second,\nin contrast to prior work, we take metric recovery as the key object of study,\nunify existing algorithms as consistent metric recovery methods based on\nco-occurrence counts from simple Markov random walks, and propose a new\nrecovery algorithm. Third, we generalize metric recovery to graphs and\nmanifolds, relating co-occurence counts on random walks in graphs and random\nprocesses on manifolds to the underlying metric to be recovered, thereby\nreconciling manifold estimation and embedding algorithms. We compare embedding\nalgorithms across a range of tasks, from nonlinear dimensionality reduction to\nthree semantic language tasks, including analogies, sequence completion, and\nclassification. \n\n"}
{"id": "1509.08144", "contents": "Title: Optimal Copula Transport for Clustering Multivariate Time Series Abstract: This paper presents a new methodology for clustering multivariate time series\nleveraging optimal transport between copulas. Copulas are used to encode both\n(i) intra-dependence of a multivariate time series, and (ii) inter-dependence\nbetween two time series. Then, optimal copula transport allows us to define two\ndistances between multivariate time series: (i) one for measuring\nintra-dependence dissimilarity, (ii) another one for measuring inter-dependence\ndissimilarity based on a new multivariate dependence coefficient which is\nrobust to noise, deterministic, and which can target specified dependencies. \n\n"}
{"id": "1510.00377", "contents": "Title: Reversible circuit compilation with space constraints Abstract: We develop a framework for resource efficient compilation of higher-level\nprograms into lower-level reversible circuits. Our main focus is on optimizing\nthe memory footprint of the resulting reversible networks. This is motivated by\nthe limited availability of qubits for the foreseeable future. We apply three\nmain techniques to keep the number of required qubits small when computing\nclassical, irreversible computations by means of reversible networks: first,\nwherever possible we allow the compiler to make use of in-place functions to\nmodify some of the variables. Second, an intermediate representation is\nintroduced that allows to trace data dependencies within the program, allowing\nto clean up qubits early. This realizes an analog to \"garbage collection\" for\nreversible circuits. Third, we use the concept of so-called pebble games to\ntransform irreversible programs into reversible programs under space\nconstraints, allowing for data to be erased and recomputed if needed.\n  We introduce REVS, a compiler for reversible circuits that can translate a\nsubset of the functional programming language F# into Toffoli networks which\ncan then be further interpreted for instance in LIQui|>, a domain-specific\nlanguage for quantum computing and which is also embedded into F#. We discuss a\nnumber of test cases that illustrate the advantages of our approach including\nreversible implementations of SHA-2 and other cryptographic hash-functions,\nreversible integer arithmetic, as well as a test-bench of combinational\ncircuits used in classical circuit synthesis. Compared to Bennett's method,\nREVS can reduce space complexity by a factor of $4$ or more, while having an\nonly moderate increase in circuit size as well as in the time it takes to\ncompile the reversible networks. \n\n"}
{"id": "1510.03888", "contents": "Title: A Framework for Approximating Qubit Unitaries Abstract: We present an algorithm for efficiently approximating of qubit unitaries over\ngate sets derived from totally definite quaternion algebras. It achieves\n$\\varepsilon$-approximations using circuits of length $O(\\log(1/\\varepsilon))$,\nwhich is asymptotically optimal. The algorithm achieves the same quality of\napproximation as previously-known algorithms for Clifford+T [arXiv:1212.6253],\nV-basis [arXiv:1303.1411] and Clifford+$\\pi/12$ [arXiv:1409.3552], running on\naverage in time polynomial in $O(\\log(1/\\varepsilon))$ (conditional on a\nnumber-theoretic conjecture). Ours is the first such algorithm that works for a\nwide range of gate sets and provides insight into what should constitute a\n\"good\" gate set for a fault-tolerant quantum computer. \n\n"}
{"id": "1510.04935", "contents": "Title: Holographic Embeddings of Knowledge Graphs Abstract: Learning embeddings of entities and relations is an efficient and versatile\nmethod to perform machine learning on relational data such as knowledge graphs.\nIn this work, we propose holographic embeddings (HolE) to learn compositional\nvector space representations of entire knowledge graphs. The proposed method is\nrelated to holographic models of associative memory in that it employs circular\ncorrelation to create compositional representations. By using correlation as\nthe compositional operator HolE can capture rich interactions but\nsimultaneously remains efficient to compute, easy to train, and scalable to\nvery large datasets. In extensive experiments we show that holographic\nembeddings are able to outperform state-of-the-art methods for link prediction\nin knowledge graphs and relational learning benchmark datasets. \n\n"}
{"id": "1510.05214", "contents": "Title: Clustering Noisy Signals with Structured Sparsity Using Time-Frequency\n  Representation Abstract: We propose a simple and efficient time-series clustering framework\nparticularly suited for low Signal-to-Noise Ratio (SNR), by simultaneous\nsmoothing and dimensionality reduction aimed at preserving clustering\ninformation. We extend the sparse K-means algorithm by incorporating structured\nsparsity, and use it to exploit the multi-scale property of wavelets and group\nstructure in multivariate signals. Finally, we extract features invariant to\ntranslation and scaling with the scattering transform, which corresponds to a\nconvolutional network with filters given by a wavelet operator, and use the\nnetwork's structure in sparse clustering. By promoting sparsity, this transform\ncan yield a low-dimensional representation of signals that gives improved\nclustering results on several real datasets. \n\n"}
{"id": "1510.08231", "contents": "Title: Operator-valued Kernels for Learning from Functional Response Data Abstract: In this paper we consider the problems of supervised classification and\nregression in the case where attributes and labels are functions: a data is\nrepresented by a set of functions, and the label is also a function. We focus\non the use of reproducing kernel Hilbert space theory to learn from such\nfunctional data. Basic concepts and properties of kernel-based learning are\nextended to include the estimation of function-valued functions. In this\nsetting, the representer theorem is restated, a set of rigorously defined\ninfinite-dimensional operator-valued kernels that can be valuably applied when\nthe data are functions is described, and a learning algorithm for nonlinear\nfunctional data analysis is introduced. The methodology is illustrated through\nspeech and audio signal processing experiments. \n\n"}
{"id": "1511.03163", "contents": "Title: Semi-supervised Tuning from Temporal Coherence Abstract: Recent works demonstrated the usefulness of temporal coherence to regularize\nsupervised training or to learn invariant features with deep architectures. In\nparticular, enforcing smooth output changes while presenting temporally-closed\nframes from video sequences, proved to be an effective strategy. In this paper\nwe prove the efficacy of temporal coherence for semi-supervised incremental\ntuning. We show that a deep architecture, just mildly trained in a supervised\nmanner, can progressively improve its classification accuracy, if exposed to\nvideo sequences of unlabeled data. The extent to which, in some cases, a\nsemi-supervised tuning allows to improve classification accuracy (approaching\nthe supervised one) is somewhat surprising. A number of control experiments\npointed out the fundamental role of temporal coherence. \n\n"}
{"id": "1511.05236", "contents": "Title: Reduced-Precision Strategies for Bounded Memory in Deep Neural Nets Abstract: This work investigates how using reduced precision data in Convolutional\nNeural Networks (CNNs) affects network accuracy during classification. More\nspecifically, this study considers networks where each layer may use different\nprecision data. Our key result is the observation that the tolerance of CNNs to\nreduced precision data not only varies across networks, a well established\nobservation, but also within networks. Tuning precision per layer is appealing\nas it could enable energy and performance improvements. In this paper we study\nhow error tolerance across layers varies and propose a method for finding a low\nprecision configuration for a network while maintaining high accuracy. A\ndiverse set of CNNs is analyzed showing that compared to a conventional\nimplementation using a 32-bit floating-point representation for all layers, and\nwith less than 1% loss in relative accuracy, the data footprint required by\nthese networks can be reduced by an average of 74% and up to 92%. \n\n"}
{"id": "1511.06382", "contents": "Title: Iterative Refinement of the Approximate Posterior for Directed Belief\n  Networks Abstract: Variational methods that rely on a recognition network to approximate the\nposterior of directed graphical models offer better inference and learning than\nprevious methods. Recent advances that exploit the capacity and flexibility in\nthis approach have expanded what kinds of models can be trained. However, as a\nproposal for the posterior, the capacity of the recognition network is limited,\nwhich can constrain the representational power of the generative model and\nincrease the variance of Monte Carlo estimates. To address these issues, we\nintroduce an iterative refinement procedure for improving the approximate\nposterior of the recognition network and show that training with the refined\nposterior is competitive with state-of-the-art methods. The advantages of\nrefinement are further evident in an increased effective sample size, which\nimplies a lower variance of gradient estimates. \n\n"}
{"id": "1511.06443", "contents": "Title: Neural Network Matrix Factorization Abstract: Data often comes in the form of an array or matrix. Matrix factorization\ntechniques attempt to recover missing or corrupted entries by assuming that the\nmatrix can be written as the product of two low-rank matrices. In other words,\nmatrix factorization approximates the entries of the matrix by a simple, fixed\nfunction---namely, the inner product---acting on the latent feature vectors for\nthe corresponding row and column. Here we consider replacing the inner product\nby an arbitrary function that we learn from the data at the same time as we\nlearn the latent feature vectors. In particular, we replace the inner product\nby a multi-layer feed-forward neural network, and learn by alternating between\noptimizing the network for fixed latent features, and optimizing the latent\nfeatures for a fixed network. The resulting approach---which we call neural\nnetwork matrix factorization or NNMF, for short---dominates standard low-rank\ntechniques on a suite of benchmark but is dominated by some recent proposals\nthat take advantage of the graph features. Given the vast range of\narchitectures, activation functions, regularizers, and optimization techniques\nthat could be used within the NNMF framework, it seems likely the true\npotential of the approach has yet to be reached. \n\n"}
{"id": "1511.08551", "contents": "Title: Regularized EM Algorithms: A Unified Framework and Statistical\n  Guarantees Abstract: Latent variable models are a fundamental modeling tool in machine learning\napplications, but they present significant computational and analytical\nchallenges. The popular EM algorithm and its variants, is a much used\nalgorithmic tool; yet our rigorous understanding of its performance is highly\nincomplete. Recently, work in Balakrishnan et al. (2014) has demonstrated that\nfor an important class of problems, EM exhibits linear local convergence. In\nthe high-dimensional setting, however, the M-step may not be well defined. We\naddress precisely this setting through a unified treatment using\nregularization. While regularization for high-dimensional problems is by now\nwell understood, the iterative EM algorithm requires a careful balancing of\nmaking progress towards the solution while identifying the right structure\n(e.g., sparsity or low-rank). In particular, regularizing the M-step using the\nstate-of-the-art high-dimensional prescriptions (e.g., Wainwright (2014)) is\nnot guaranteed to provide this balance. Our algorithm and analysis are linked\nin a way that reveals the balance between optimization and statistical errors.\nWe specialize our general framework to sparse gaussian mixture models,\nhigh-dimensional mixed regression, and regression with missing variables,\nobtaining statistical guarantees for each of these examples. \n\n"}
{"id": "1512.02337", "contents": "Title: Fast spectral algorithms from sum-of-squares proofs: tensor\n  decomposition and planted sparse vectors Abstract: We consider two problems that arise in machine learning applications: the\nproblem of recovering a planted sparse vector in a random linear subspace and\nthe problem of decomposing a random low-rank overcomplete 3-tensor. For both\nproblems, the best known guarantees are based on the sum-of-squares method. We\ndevelop new algorithms inspired by analyses of the sum-of-squares method. Our\nalgorithms achieve the same or similar guarantees as sum-of-squares for these\nproblems but the running time is significantly faster.\n  For the planted sparse vector problem, we give an algorithm with running time\nnearly linear in the input size that approximately recovers a planted sparse\nvector with up to constant relative sparsity in a random subspace of $\\mathbb\nR^n$ of dimension up to $\\tilde \\Omega(\\sqrt n)$. These recovery guarantees\nmatch the best known ones of Barak, Kelner, and Steurer (STOC 2014) up to\nlogarithmic factors.\n  For tensor decomposition, we give an algorithm with running time close to\nlinear in the input size (with exponent $\\approx 1.086$) that approximately\nrecovers a component of a random 3-tensor over $\\mathbb R^n$ of rank up to\n$\\tilde \\Omega(n^{4/3})$. The best previous algorithm for this problem due to\nGe and Ma (RANDOM 2015) works up to rank $\\tilde \\Omega(n^{3/2})$ but requires\nquasipolynomial time. \n\n"}
{"id": "1512.02752", "contents": "Title: A Novel Regularized Principal Graph Learning Framework on Explicit Graph\n  Representation Abstract: Many scientific datasets are of high dimension, and the analysis usually\nrequires visual manipulation by retaining the most important structures of\ndata. Principal curve is a widely used approach for this purpose. However, many\nexisting methods work only for data with structures that are not\nself-intersected, which is quite restrictive for real applications. A few\nmethods can overcome the above problem, but they either require complicated\nhuman-made rules for a specific task with lack of convergence guarantee and\nadaption flexibility to different tasks, or cannot obtain explicit structures\nof data. To address these issues, we develop a new regularized principal graph\nlearning framework that captures the local information of the underlying graph\nstructure based on reversed graph embedding. As showcases, models that can\nlearn a spanning tree or a weighted undirected $\\ell_1$ graph are proposed, and\na new learning algorithm is developed that learns a set of principal points and\na graph structure from data, simultaneously. The new algorithm is simple with\nguaranteed convergence. We then extend the proposed framework to deal with\nlarge-scale data. Experimental results on various synthetic and six real world\ndatasets show that the proposed method compares favorably with baselines and\ncan uncover the underlying structure correctly. \n\n"}
{"id": "1512.02802", "contents": "Title: Lively quantum walks on cycles Abstract: We introduce a family of quantum walks on cycles parametrized by their\nliveliness, defined by the ability to execute a long-range move. We investigate\nthe behaviour of the probability distribution and time-averaged probability\ndistribution. We show that the liveliness parameter, controlling the magnitude\nof the additional long-range move, has a direct impact on the periodicity of\nthe limiting distribution. We also show that the introduced model provides a\nmethod for network exploration which is robust against trapping. \n\n"}
{"id": "1512.02866", "contents": "Title: Multi-Player Bandits -- a Musical Chairs Approach Abstract: We consider a variant of the stochastic multi-armed bandit problem, where\nmultiple players simultaneously choose from the same set of arms and may\ncollide, receiving no reward. This setting has been motivated by problems\narising in cognitive radio networks, and is especially challenging under the\nrealistic assumption that communication between players is limited. We provide\na communication-free algorithm (Musical Chairs) which attains constant regret\nwith high probability, as well as a sublinear-regret, communication-free\nalgorithm (Dynamic Musical Chairs) for the more difficult setting of players\ndynamically entering and leaving throughout the game. Moreover, both algorithms\ndo not require prior knowledge of the number of players. To the best of our\nknowledge, these are the first communication-free algorithms with these types\nof formal guarantees. We also rigorously compare our algorithms to previous\nworks, and complement our theoretical findings with experiments. \n\n"}
{"id": "1512.03385", "contents": "Title: Deep Residual Learning for Image Recognition Abstract: Deeper neural networks are more difficult to train. We present a residual\nlearning framework to ease the training of networks that are substantially\ndeeper than those used previously. We explicitly reformulate the layers as\nlearning residual functions with reference to the layer inputs, instead of\nlearning unreferenced functions. We provide comprehensive empirical evidence\nshowing that these residual networks are easier to optimize, and can gain\naccuracy from considerably increased depth. On the ImageNet dataset we evaluate\nresidual nets with a depth of up to 152 layers---8x deeper than VGG nets but\nstill having lower complexity. An ensemble of these residual nets achieves\n3.57% error on the ImageNet test set. This result won the 1st place on the\nILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100\nand 1000 layers.\n  The depth of representations is of central importance for many visual\nrecognition tasks. Solely due to our extremely deep representations, we obtain\na 28% relative improvement on the COCO object detection dataset. Deep residual\nnets are foundations of our submissions to ILSVRC & COCO 2015 competitions,\nwhere we also won the 1st places on the tasks of ImageNet detection, ImageNet\nlocalization, COCO detection, and COCO segmentation. \n\n"}
{"id": "1512.03824", "contents": "Title: Improved Quantum Ternary Arithmetics Abstract: Qutrit (or ternary) structures arise naturally in many quantum systems,\nparticularly in certain non-abelian anyon systems. We present efficient\ncircuits for ternary reversible and quantum arithmetics. Our main result is the\nderivation of circuits for two families of ternary quantum adders, namely\nripple carry adders and carry look-ahead adders. The main difference to the\nbinary case is the more complicated form of the ternary carry, which leads to\nhigher resource counts for implementations over a universal ternary gate set.\nOur ternary ripple adder circuit has a circuit depth of $O(n)$ and uses only\n$1$ ancilla, making it more efficient in both, circuit depth and width than\nprevious constructions. Our ternary carry lookahead circuit has a circuit depth\nof only $O(\\log\\,n)$, while using with $O(n)$ ancillas. Our approach works on\ntwo levels of abstraction: at the first level, descriptions of arithmetic\ncircuits are given in terms of gates sequences that use various types of\nnon-Clifford reflections. At the second level, we break down these reflections\nfurther by deriving them either from the two-qutrit Clifford gates and the\nnon-Clifford gate $C(X): |i,j\\rangle \\mapsto |i, j + \\delta_{i,2} \\mod\n3\\rangle$ or from the two-qutrit Clifford gates and the non-Clifford gate\n$P_9=\\mbox{diag}(e^{-2 \\pi \\, i/9},1,e^{2 \\pi \\, i/9})$. The two choices of\nelementary gate sets correspond to two possible mappings onto two different\nprospective quantum computing architectures which we call the metaplectic and\nthe supermetaplectic basis, respectively. Finally, we develop a method to\nfactor diagonal unitaries using multi-variate polynomial over the ternary\nfinite field which allows to characterize classes of gates that can be\nimplemented exactly over the supermetaplectic basis. \n\n"}
{"id": "1512.08204", "contents": "Title: New Perspectives on $k$-Support and Cluster Norms Abstract: We study a regularizer which is defined as a parameterized infimum of\nquadratics, and which we call the box-norm. We show that the k-support norm, a\nregularizer proposed by [Argyriou et al, 2012] for sparse vector prediction\nproblems, belongs to this family, and the box-norm can be generated as a\nperturbation of the former. We derive an improved algorithm to compute the\nproximity operator of the squared box-norm, and we provide a method to compute\nthe norm. We extend the norms to matrices, introducing the spectral k-support\nnorm and spectral box-norm. We note that the spectral box-norm is essentially\nequivalent to the cluster norm, a multitask learning regularizer introduced by\n[Jacob et al. 2009a], and which in turn can be interpreted as a perturbation of\nthe spectral k-support norm. Centering the norm is important for multitask\nlearning and we also provide a method to use centered versions of the norms as\nregularizers. Numerical experiments indicate that the spectral k-support and\nbox-norms and their centered variants provide state of the art performance in\nmatrix completion and multitask learning problems respectively. \n\n"}
{"id": "1601.03945", "contents": "Title: Improved graph-based SFA: Information preservation complements the\n  slowness principle Abstract: Slow feature analysis (SFA) is an unsupervised-learning algorithm that\nextracts slowly varying features from a multi-dimensional time series. A\nsupervised extension to SFA for classification and regression is graph-based\nSFA (GSFA). GSFA is based on the preservation of similarities, which are\nspecified by a graph structure derived from the labels. It has been shown that\nhierarchical GSFA (HGSFA) allows learning from images and other\nhigh-dimensional data. The feature space spanned by HGSFA is complex due to the\ncomposition of the nonlinearities of the nodes in the network. However, we show\nthat the network discards useful information prematurely before it reaches\nhigher nodes, resulting in suboptimal global slowness and an under-exploited\nfeature space.\n  To counteract these problems, we propose an extension called hierarchical\ninformation-preserving GSFA (HiGSFA), where information preservation\ncomplements the slowness-maximization goal. We build a 10-layer HiGSFA network\nto estimate human age from facial photographs of the MORPH-II database,\nachieving a mean absolute error of 3.50 years, improving the state-of-the-art\nperformance. HiGSFA and HGSFA support multiple-labels and offer a rich feature\nspace, feed-forward training, and linear complexity in the number of samples\nand dimensions. Furthermore, HiGSFA outperforms HGSFA in terms of feature\nslowness, estimation accuracy and input reconstruction, giving rise to a\npromising hierarchical supervised-learning approach. \n\n"}
{"id": "1601.05495", "contents": "Title: Data-driven Rank Breaking for Efficient Rank Aggregation Abstract: Rank aggregation systems collect ordinal preferences from individuals to\nproduce a global ranking that represents the social preference. Rank-breaking\nis a common practice to reduce the computational complexity of learning the\nglobal ranking. The individual preferences are broken into pairwise comparisons\nand applied to efficient algorithms tailored for independent paired\ncomparisons. However, due to the ignored dependencies in the data, naive\nrank-breaking approaches can result in inconsistent estimates. The key idea to\nproduce accurate and consistent estimates is to treat the pairwise comparisons\nunequally, depending on the topology of the collected data. In this paper, we\nprovide the optimal rank-breaking estimator, which not only achieves\nconsistency but also achieves the best error bound. This allows us to\ncharacterize the fundamental tradeoff between accuracy and complexity. Further,\nthe analysis identifies how the accuracy depends on the spectral gap of a\ncorresponding comparison graph. \n\n"}
{"id": "1601.07137", "contents": "Title: Posner computing: a quantum neural network model Abstract: We present a construction, rendered in Quipper, of a quantum algorithm which\nprobabilistically computes a classical function from n bits to n bits. The\nconstruction is intended to be of interest primarily for the features of\nQuipper it highlights. However, intrigued by the utility of quantum information\nprocessing in the context of neural networks, we present the algorithm as a\nsimplest example of a particular quantum neural network which we first define.\nAs the definition is inspired by recent work of Fisher concerning possible\nquantum substrates to cognition, we precede it with a short description of that\nwork. \n\n"}
{"id": "1602.00061", "contents": "Title: Spectrum Estimation from Samples Abstract: We consider the problem of approximating the set of eigenvalues of the\ncovariance matrix of a multivariate distribution (equivalently, the problem of\napproximating the \"population spectrum\"), given access to samples drawn from\nthe distribution. The eigenvalues of the covariance of a distribution contain\nbasic information about the distribution, including the presence or lack of\nstructure in the distribution, the effective dimensionality of the\ndistribution, and the applicability of higher-level machine learning and\nmultivariate statistical tools. We consider this fundamental recovery problem\nin the regime where the number of samples is comparable, or even sublinear in\nthe dimensionality of the distribution in question. First, we propose a\ntheoretically optimal and computationally efficient algorithm for recovering\nthe moments of the eigenvalues of the population covariance matrix. We then\nleverage this accurate moment recovery, via a Wasserstein distance argument, to\nshow that the vector of eigenvalues can be accurately recovered. We provide\nfinite--sample bounds on the expected error of the recovered eigenvalues, which\nimply that our estimator is asymptotically consistent as the dimensionality of\nthe distribution and sample size tend towards infinity, even in the sublinear\nsample regime where the ratio of the sample size to the dimensionality tends to\nzero. In addition to our theoretical results, we show that our approach\nperforms well in practice for a broad range of distributions and sample sizes. \n\n"}
{"id": "1602.02196", "contents": "Title: BISTRO: An Efficient Relaxation-Based Method for Contextual Bandits Abstract: We present efficient algorithms for the problem of contextual bandits with\ni.i.d. covariates, an arbitrary sequence of rewards, and an arbitrary class of\npolicies. Our algorithm BISTRO requires d calls to the empirical risk\nminimization (ERM) oracle per round, where d is the number of actions. The\nmethod uses unlabeled data to make the problem computationally simple. When the\nERM problem itself is computationally hard, we extend the approach by employing\nmultiplicative approximation algorithms for the ERM. The integrality gap of the\nrelaxation only enters in the regret bound rather than the benchmark. Finally,\nwe show that the adversarial version of the contextual bandit problem is\nlearnable (and efficient) whenever the full-information supervised online\nlearning problem has a non-trivial regret guarantee (and efficient). \n\n"}
{"id": "1602.02220", "contents": "Title: Improved Dropout for Shallow and Deep Learning Abstract: Dropout has been witnessed with great success in training deep neural\nnetworks by independently zeroing out the outputs of neurons at random. It has\nalso received a surge of interest for shallow learning, e.g., logistic\nregression. However, the independent sampling for dropout could be suboptimal\nfor the sake of convergence. In this paper, we propose to use multinomial\nsampling for dropout, i.e., sampling features or neurons according to a\nmultinomial distribution with different probabilities for different\nfeatures/neurons. To exhibit the optimal dropout probabilities, we analyze the\nshallow learning with multinomial dropout and establish the risk bound for\nstochastic optimization. By minimizing a sampling dependent factor in the risk\nbound, we obtain a distribution-dependent dropout with sampling probabilities\ndependent on the second order statistics of the data distribution. To tackle\nthe issue of evolving distribution of neurons in deep learning, we propose an\nefficient adaptive dropout (named \\textbf{evolutional dropout}) that computes\nthe sampling probabilities on-the-fly from a mini-batch of examples. Empirical\nstudies on several benchmark datasets demonstrate that the proposed dropouts\nachieve not only much faster convergence and but also a smaller testing error\nthan the standard dropout. For example, on the CIFAR-100 data, the evolutional\ndropout achieves relative improvements over 10\\% on the prediction performance\nand over 50\\% on the convergence speed compared to the standard dropout. \n\n"}
{"id": "1602.02627", "contents": "Title: Optimal and asymptotically optimal NCT reversible circuits by the gate\n  types Abstract: We report optimal and asymptotically optimal reversible circuits composed of\nNOT, CNOT, and Toffoli (NCT) gates, keeping the count by the subsets of the\ngate types used. This study fine tunes the circuit complexity figures for the\nrealization of reversible functions via reversible NCT circuits. An important\nconsequence is a result on the limitation of the use of the $T$-count quantum\ncircuit metric popular in applications. \n\n"}
{"id": "1602.04283", "contents": "Title: Deep Learning on FPGAs: Past, Present, and Future Abstract: The rapid growth of data size and accessibility in recent years has\ninstigated a shift of philosophy in algorithm design for artificial\nintelligence. Instead of engineering algorithms by hand, the ability to learn\ncomposable systems automatically from massive amounts of data has led to\nground-breaking performance in important domains such as computer vision,\nspeech recognition, and natural language processing. The most popular class of\ntechniques used in these domains is called deep learning, and is seeing\nsignificant attention from industry. However, these models require incredible\namounts of data and compute power to train, and are limited by the need for\nbetter hardware acceleration to accommodate scaling beyond current data and\nmodel sizes. While the current solution has been to use clusters of graphics\nprocessing units (GPU) as general purpose processors (GPGPU), the use of field\nprogrammable gate arrays (FPGA) provide an interesting alternative. Current\ntrends in design tools for FPGAs have made them more compatible with the\nhigh-level software practices typically practiced in the deep learning\ncommunity, making FPGAs more accessible to those who build and deploy models.\nSince FPGA architectures are flexible, this could also allow researchers the\nability to explore model-level optimizations beyond what is possible on fixed\narchitectures such as GPUs. As well, FPGAs tend to provide high performance per\nwatt of power consumption, which is of particular importance for application\nscientists interested in large scale server-based deployment or\nresource-limited embedded applications. This review takes a look at deep\nlearning and FPGAs from a hardware acceleration perspective, identifying trends\nand innovations that make these technologies a natural fit, and motivates a\ndiscussion on how FPGAs may best serve the needs of the deep learning community\nmoving forward. \n\n"}
{"id": "1602.04951", "contents": "Title: Q($\\lambda$) with Off-Policy Corrections Abstract: We propose and analyze an alternate approach to off-policy multi-step\ntemporal difference learning, in which off-policy returns are corrected with\nthe current Q-function in terms of rewards, rather than with the target policy\nin terms of transition probabilities. We prove that such approximate\ncorrections are sufficient for off-policy convergence both in policy evaluation\nand control, provided certain conditions. These conditions relate the distance\nbetween the target and behavior policies, the eligibility trace parameter and\nthe discount factor, and formalize an underlying tradeoff in off-policy\nTD($\\lambda$). We illustrate this theoretical relationship empirically on a\ncontinuous-state control task. \n\n"}
{"id": "1602.06053", "contents": "Title: First-order Methods for Geodesically Convex Optimization Abstract: Geodesic convexity generalizes the notion of (vector space) convexity to\nnonlinear metric spaces. But unlike convex optimization, geodesically convex\n(g-convex) optimization is much less developed. In this paper we contribute to\nthe understanding of g-convex optimization by developing iteration complexity\nanalysis for several first-order algorithms on Hadamard manifolds.\nSpecifically, we prove upper bounds for the global complexity of deterministic\nand stochastic (sub)gradient methods for optimizing smooth and nonsmooth\ng-convex functions, both with and without strong g-convexity. Our analysis also\nreveals how the manifold geometry, especially \\emph{sectional curvature},\nimpacts convergence rates. To the best of our knowledge, our work is the first\nto provide global complexity analysis for first-order algorithms for general\ng-convex optimization. \n\n"}
{"id": "1602.06516", "contents": "Title: Uniform Hypergraph Partitioning: Provable Tensor Methods and Sampling\n  Techniques Abstract: In a series of recent works, we have generalised the consistency results in\nthe stochastic block model literature to the case of uniform and non-uniform\nhypergraphs. The present paper continues the same line of study, where we focus\non partitioning weighted uniform hypergraphs---a problem often encountered in\ncomputer vision. This work is motivated by two issues that arise when a\nhypergraph partitioning approach is used to tackle computer vision problems:\n(i) The uniform hypergraphs constructed for higher-order learning contain all\nedges, but most have negligible weights. Thus, the adjacency tensor is nearly\nsparse, and yet, not binary. (ii) A more serious concern is that standard\npartitioning algorithms need to compute all edge weights, which is\ncomputationally expensive for hypergraphs. This is usually resolved in practice\nby merging the clustering algorithm with a tensor sampling strategy---an\napproach that is yet to be analysed rigorously. We build on our earlier work on\npartitioning dense unweighted uniform hypergraphs (Ghoshdastidar and Dukkipati,\nICML, 2015), and address the aforementioned issues by proposing provable and\nefficient partitioning algorithms. Our analysis justifies the empirical success\nof practical sampling techniques. We also complement our theoretical findings\nby elaborate empirical comparison of various hypergraph partitioning schemes. \n\n"}
{"id": "1602.06566", "contents": "Title: Interactive Storytelling over Document Collections Abstract: Storytelling algorithms aim to 'connect the dots' between disparate documents\nby linking starting and ending documents through a series of intermediate\ndocuments. Existing storytelling algorithms are based on notions of coherence\nand connectivity, and thus the primary way by which users can steer the story\nconstruction is via design of suitable similarity functions. We present an\nalternative approach to storytelling wherein the user can interactively and\niteratively provide 'must use' constraints to preferentially support the\nconstruction of some stories over others. The three innovations in our approach\nare distance measures based on (inferred) topic distributions, the use of\nconstraints to define sets of linear inequalities over paths, and the\nintroduction of slack and surplus variables to condition the topic distribution\nto preferentially emphasize desired terms over others. We describe experimental\nresults to illustrate the effectiveness of our interactive storytelling\napproach over multiple text datasets. \n\n"}
{"id": "1602.07265", "contents": "Title: Search Improves Label for Active Learning Abstract: We investigate active learning with access to two distinct oracles: Label\n(which is standard) and Search (which is not). The Search oracle models the\nsituation where a human searches a database to seed or counterexample an\nexisting solution. Search is stronger than Label while being natural to\nimplement in many situations. We show that an algorithm using both oracles can\nprovide exponentially large problem-dependent improvements over Label alone. \n\n"}
{"id": "1603.00531", "contents": "Title: LOFS: Library of Online Streaming Feature Selection Abstract: As an emerging research direction, online streaming feature selection deals\nwith sequentially added dimensions in a feature space while the number of data\ninstances is fixed. Online streaming feature selection provides a new,\ncomplementary algorithmic methodology to enrich online feature selection,\nespecially targets to high dimensionality in big data analytics. This paper\nintroduces the first comprehensive open-source library for use in MATLAB that\nimplements the state-of-the-art algorithms of online streaming feature\nselection. The library is designed to facilitate the development of new\nalgorithms in this exciting research direction and make comparisons between the\nnew methods and existing ones available. \n\n"}
{"id": "1603.01025", "contents": "Title: Convolutional Neural Networks using Logarithmic Data Representation Abstract: Recent advances in convolutional neural networks have considered model\ncomplexity and hardware efficiency to enable deployment onto embedded systems\nand mobile devices. For example, it is now well-known that the arithmetic\noperations of deep networks can be encoded down to 8-bit fixed-point without\nsignificant deterioration in performance. However, further reduction in\nprecision down to as low as 3-bit fixed-point results in significant losses in\nperformance. In this paper we propose a new data representation that enables\nstate-of-the-art networks to be encoded to 3 bits with negligible loss in\nclassification performance. To perform this, we take advantage of the fact that\nthe weights and activations in a trained network naturally have non-uniform\ndistributions. Using non-uniform, base-2 logarithmic representation to encode\nweights, communicate activations, and perform dot-products enables networks to\n1) achieve higher classification accuracies than fixed-point at the same\nresolution and 2) eliminate bulky digital multipliers. Finally, we propose an\nend-to-end training procedure that uses log representation at 5-bits, which\nachieves higher final test accuracy than linear at 5-bits. \n\n"}
{"id": "1603.01635", "contents": "Title: Verified compilation of space-efficient reversible circuits Abstract: The generation of reversible circuits from high-level code is an important\nproblem in several application domains, including low-power electronics and\nquantum computing. Existing tools compile and optimize reversible circuits for\nvarious metrics, such as the overall circuit size or the total amount of space\nrequired to implement a given function reversibly. However, little effort has\nbeen spent on verifying the correctness of the results, an issue of particular\nimportance in quantum computing. There, compilation allows not only mapping to\nhardware, but also the estimation of resources required to implement a given\nquantum algorithm, a process that is crucial for identifying which algorithms\nwill outperform their classical counterparts. We present a reversible circuit\ncompiler called ReVerC, which has been formally verified in F* and compiles\ncircuits that operate correctly with respect to the input program. Our compiler\ncompiles the Revs language to combinational reversible circuits with as few\nancillary bits as possible, and provably cleans temporary values. \n\n"}
{"id": "1603.02644", "contents": "Title: Online but Accurate Inference for Latent Variable Models with Local\n  Gibbs Sampling Abstract: We study parameter inference in large-scale latent variable models. We first\npropose an unified treatment of online inference for latent variable models\nfrom a non-canonical exponential family, and draw explicit links between\nseveral previously proposed frequentist or Bayesian methods. We then propose a\nnovel inference method for the frequentist estimation of parameters, that\nadapts MCMC methods to online inference of latent variable models with the\nproper use of local Gibbs sampling. Then, for latent Dirich-let allocation,we\nprovide an extensive set of experiments and comparisons with existing work,\nwhere our new approach outperforms all previously proposed methods. In\nparticular, using Gibbs sampling for latent variable inference is superior to\nvariational inference in terms of test log-likelihoods. Moreover, Bayesian\ninference through variational methods perform poorly, sometimes leading to\nworse fits with latent variables of higher dimensionality. \n\n"}
{"id": "1603.04119", "contents": "Title: Exploratory Gradient Boosting for Reinforcement Learning in Complex\n  Domains Abstract: High-dimensional observations and complex real-world dynamics present major\nchallenges in reinforcement learning for both function approximation and\nexploration. We address both of these challenges with two complementary\ntechniques: First, we develop a gradient-boosting style, non-parametric\nfunction approximator for learning on $Q$-function residuals. And second, we\npropose an exploration strategy inspired by the principles of state abstraction\nand information acquisition under uncertainty. We demonstrate the empirical\neffectiveness of these techniques, first, as a preliminary check, on two\nstandard tasks (Blackjack and $n$-Chain), and then on two much larger and more\nrealistic tasks with high-dimensional observation spaces. Specifically, we\nintroduce two benchmarks built within the game Minecraft where the observations\nare pixel arrays of the agent's visual field. A combination of our two\nalgorithmic techniques performs competitively on the standard\nreinforcement-learning tasks while consistently and substantially outperforming\nbaselines on the two tasks with high-dimensional observation spaces. The new\nfunction approximator, exploration strategy, and evaluation benchmarks are each\nof independent interest in the pursuit of reinforcement-learning methods that\nscale to real-world domains. \n\n"}
{"id": "1603.07678", "contents": "Title: Basic circuit compilation techniques for an ion-trap quantum machine Abstract: We study the problem of compilation of quantum algorithms into optimized\nphysical-level circuits executable in a quantum information processing (QIP)\nexperiment based on trapped atomic ions. We report a complete strategy:\nstarting with an algorithm in the form of a quantum computer program, we\ncompile it into a high-level logical circuit that goes through multiple stages\nof decomposition into progressively lower-level circuits until we reach the\nphysical execution-level specification. We skip the fault-tolerance layer, as\nit is not within the scope of this work. The different stages are structured so\nas to best assist with the overall optimization while taking into account\nnumerous optimization criteria, including minimizing the number of expensive\ntwo-qubit gates, minimizing the number of less expensive single-qubit gates,\noptimizing the runtime, minimizing the overall circuit error, and optimizing\nclassical control sequences. Our approach allows a trade-off between circuit\nruntime and quantum error, as well as to accommodate future changes in the\noptimization criteria that may likely arise as a result of the anticipated\nimprovements in the physical-level control of the experiment. \n\n"}
{"id": "1603.08988", "contents": "Title: Towards Practical Bayesian Parameter and State Estimation Abstract: Joint state and parameter estimation is a core problem for dynamic Bayesian\nnetworks. Although modern probabilistic inference toolkits make it relatively\neasy to specify large and practically relevant probabilistic models, the silver\nbullet---an efficient and general online inference algorithm for such\nproblems---remains elusive, forcing users to write special-purpose code for\neach application. We propose a novel blackbox algorithm -- a hybrid of particle\nfiltering for state variables and assumed density filtering for parameter\nvariables. It has following advantages: (a) it is efficient due to its online\nnature, and (b) it is applicable to both discrete and continuous parameter\nspaces . On a variety of toy and real models, our system is able to generate\nmore accurate results within a fixed computation budget. This preliminary\nevidence indicates that the proposed approach is likely to be of practical use. \n\n"}
{"id": "1604.02218", "contents": "Title: A Low Complexity Algorithm with $O(\\sqrt{T})$ Regret and $O(1)$\n  Constraint Violations for Online Convex Optimization with Long Term\n  Constraints Abstract: This paper considers online convex optimization over a complicated constraint\nset, which typically consists of multiple functional constraints and a set\nconstraint. The conventional online projection algorithm (Zinkevich, 2003) can\nbe difficult to implement due to the potentially high computation complexity of\nthe projection operation. In this paper, we relax the functional constraints by\nallowing them to be violated at each round but still requiring them to be\nsatisfied in the long term. This type of relaxed online convex optimization\n(with long term constraints) was first considered in Mahdavi et al. (2012).\nThat prior work proposes an algorithm to achieve $O(\\sqrt{T})$ regret and\n$O(T^{3/4})$ constraint violations for general problems and another algorithm\nto achieve an $O(T^{2/3})$ bound for both regret and constraint violations when\nthe constraint set can be described by a finite number of linear constraints. A\nrecent extension in \\citet{Jenatton16ICML} can achieve\n$O(T^{\\max\\{\\theta,1-\\theta\\}})$ regret and $O(T^{1-\\theta/2})$ constraint\nviolations where $\\theta\\in (0,1)$. The current paper proposes a new simple\nalgorithm that yields improved performance in comparison to prior works. The\nnew algorithm achieves an $O(\\sqrt{T})$ regret bound with $O(1)$ constraint\nviolations. \n\n"}
{"id": "1604.03736", "contents": "Title: A Differentiable Transition Between Additive and Multiplicative Neurons Abstract: Existing approaches to combine both additive and multiplicative neural units\neither use a fixed assignment of operations or require discrete optimization to\ndetermine what function a neuron should perform. However, this leads to an\nextensive increase in the computational complexity of the training procedure.\n  We present a novel, parameterizable transfer function based on the\nmathematical concept of non-integer functional iteration that allows the\noperation each neuron performs to be smoothly and, most importantly,\ndifferentiablely adjusted between addition and multiplication. This allows the\ndecision between addition and multiplication to be integrated into the standard\nbackpropagation training procedure. \n\n"}
{"id": "1604.04706", "contents": "Title: DS-MLR: Exploiting Double Separability for Scaling up Distributed\n  Multinomial Logistic Regression Abstract: Scaling multinomial logistic regression to datasets with very large number of\ndata points and classes is challenging. This is primarily because one needs to\ncompute the log-partition function on every data point. This makes distributing\nthe computation hard. In this paper, we present a distributed stochastic\ngradient descent based optimization method (DS-MLR) for scaling up multinomial\nlogistic regression problems to massive scale datasets without hitting any\nstorage constraints on the data and model parameters. Our algorithm exploits\ndouble-separability, an attractive property that allows us to achieve both data\nas well as model parallelism simultaneously. In addition, we introduce a\nnon-blocking and asynchronous variant of our algorithm that avoids\nbulk-synchronization. We demonstrate the versatility of DS-MLR to various\nscenarios in data and model parallelism, through an extensive empirical study\nusing several real-world datasets. In particular, we demonstrate the\nscalability of DS-MLR by solving an extreme multi-class classification problem\non the Reddit dataset (159 GB data, 358 GB parameters) where, to the best of\nour knowledge, no other existing methods apply. \n\n"}
{"id": "1605.01749", "contents": "Title: Rank Ordered Autoencoders Abstract: A new method for the unsupervised learning of sparse representations using\nautoencoders is proposed and implemented by ordering the output of the hidden\nunits by their activation value and progressively reconstructing the input in\nthis order. This can be done efficiently in parallel with the use of cumulative\nsums and sorting only slightly increasing the computational costs. Minimizing\nthe difference of this progressive reconstruction with respect to the input can\nbe seen as minimizing the number of active output units required for the\nreconstruction of the input. The model thus learns to reconstruct optimally\nusing the least number of active output units. This leads to high sparsity\nwithout the need for extra hyperparameters, the amount of sparsity is instead\nimplicitly learned by minimizing this progressive reconstruction error. Results\nof the trained model are given for patches of the CIFAR10 dataset, showing\nrapid convergence of features and extremely sparse output activations while\nmaintaining a minimal reconstruction error and showing extreme robustness to\noverfitting. Additionally the reconstruction as function of number of active\nunits is presented which shows the autoencoder learns a rank order code over\nthe input where the highest ranked units correspond to the highest decrease in\nreconstruction error. \n\n"}
{"id": "1605.02470", "contents": "Title: Randomized Kaczmarz for Rank Aggregation from Pairwise Comparisons Abstract: We revisit the problem of inferring the overall ranking among entities in the\nframework of Bradley-Terry-Luce (BTL) model, based on available empirical data\non pairwise preferences. By a simple transformation, we can cast the problem as\nthat of solving a noisy linear system, for which a ready algorithm is available\nin the form of the randomized Kaczmarz method. This scheme is provably\nconvergent, has excellent empirical performance, and is amenable to on-line,\ndistributed and asynchronous variants. Convergence, convergence rate, and error\nanalysis of the proposed algorithm are presented and several numerical\nexperiments are conducted whose results validate our theoretical findings. \n\n"}
{"id": "1605.02756", "contents": "Title: Factoring with Qutrits: Shor's Algorithm on Ternary and Metaplectic\n  Quantum Architectures Abstract: We determine the cost of performing Shor's algorithm for integer\nfactorization on a ternary quantum computer, using two natural models of\nuniversal fault-tolerant computing:\n  (i) a model based on magic state distillation that assumes the availability\nof the ternary Clifford gates, projective measurements, classical control as\nits natural instrumentation set; (ii) a model based on a metaplectic\ntopological quantum computer (MTQC). A natural choice to implement Shor's\nalgorithm on a ternary quantum computer is to translate the entire arithmetic\ninto a ternary form. However, it is also possible to emulate the standard\nbinary version of the algorithm by encoding each qubit in a three-level system.\nWe compare the two approaches and analyze the complexity of implementing Shor's\nperiod finding function in the two models. We also highlight the fact that the\ncost of achieving universality through magic states in MTQC architecture is\nasymptotically lower than in generic ternary case. \n\n"}
{"id": "1605.03835", "contents": "Title: Noisy Parallel Approximate Decoding for Conditional Recurrent Language\n  Model Abstract: Recent advances in conditional recurrent language modelling have mainly\nfocused on network architectures (e.g., attention mechanism), learning\nalgorithms (e.g., scheduled sampling and sequence-level training) and novel\napplications (e.g., image/video description generation, speech recognition,\netc.) On the other hand, we notice that decoding algorithms/strategies have not\nbeen investigated as much, and it has become standard to use greedy or beam\nsearch. In this paper, we propose a novel decoding strategy motivated by an\nearlier observation that nonlinear hidden layers of a deep neural network\nstretch the data manifold. The proposed strategy is embarrassingly\nparallelizable without any communication overhead, while improving an existing\ndecoding algorithm. We extensively evaluate it with attention-based neural\nmachine translation on the task of En->Cz translation. \n\n"}
{"id": "1605.04711", "contents": "Title: Ternary Weight Networks Abstract: We present a memory and computation efficient ternary weight networks (TWNs)\n- with weights constrained to +1, 0 and -1. The Euclidian distance between full\n(float or double) precision weights and the ternary weights along with a\nscaling factor is minimized in training stage. Besides, a threshold-based\nternary function is optimized to get an approximated solution which can be fast\nand easily computed. TWNs have shown better expressive abilities than binary\nprecision counterparts. Meanwhile, TWNs achieve up to 16$\\times$ model\ncompression rate and need fewer multiplications compared with the float32\nprecision counterparts. Extensive experiments on MNIST, CIFAR-10, and ImageNet\ndatasets show that the TWNs achieve much better result than the\nBinary-Weight-Networks (BWNs) and the classification performance on MNIST and\nCIFAR-10 is very close to the full precision networks. We also verify our\nmethod on object detection task and show that TWNs significantly outperforms\nBWN by more than 10\\% mAP on PASCAL VOC dataset. The pytorch version of source\ncode is available at: https://github.com/Thinklab-SJTU/twns. \n\n"}
{"id": "1605.06636", "contents": "Title: Deep Transfer Learning with Joint Adaptation Networks Abstract: Deep networks have been successfully applied to learn transferable features\nfor adapting models from a source domain to a different target domain. In this\npaper, we present joint adaptation networks (JAN), which learn a transfer\nnetwork by aligning the joint distributions of multiple domain-specific layers\nacross domains based on a joint maximum mean discrepancy (JMMD) criterion.\nAdversarial training strategy is adopted to maximize JMMD such that the\ndistributions of the source and target domains are made more distinguishable.\nLearning can be performed by stochastic gradient descent with the gradients\ncomputed by back-propagation in linear-time. Experiments testify that our model\nyields state of the art results on standard datasets. \n\n"}
{"id": "1605.06711", "contents": "Title: Learning From Hidden Traits: Joint Factor Analysis and Latent Clustering Abstract: Dimensionality reduction techniques play an essential role in data analytics,\nsignal processing and machine learning. Dimensionality reduction is usually\nperformed in a preprocessing stage that is separate from subsequent data\nanalysis, such as clustering or classification. Finding reduced-dimension\nrepresentations that are well-suited for the intended task is more appealing.\nThis paper proposes a joint factor analysis and latent clustering framework,\nwhich aims at learning cluster-aware low-dimensional representations of matrix\nand tensor data. The proposed approach leverages matrix and tensor\nfactorization models that produce essentially unique latent representations of\nthe data to unravel latent cluster structure -- which is otherwise obscured\nbecause of the freedom to apply an oblique transformation in latent space. At\nthe same time, latent cluster structure is used as prior information to enhance\nthe performance of factorization. Specific contributions include several\ncustom-built problem formulations, corresponding algorithms, and discussion of\nassociated convergence properties. Besides extensive simulations, real-world\ndatasets such as Reuters document data and MNIST image data are also employed\nto showcase the effectiveness of the proposed approaches. \n\n"}
{"id": "1605.07139", "contents": "Title: Fairness in Learning: Classic and Contextual Bandits Abstract: We introduce the study of fairness in multi-armed bandit problems. Our\nfairness definition can be interpreted as demanding that given a pool of\napplicants (say, for college admission or mortgages), a worse applicant is\nnever favored over a better one, despite a learning algorithm's uncertainty\nover the true payoffs. We prove results of two types.\n  First, in the important special case of the classic stochastic bandits\nproblem (i.e., in which there are no contexts), we provide a provably fair\nalgorithm based on \"chained\" confidence intervals, and provide a cumulative\nregret bound with a cubic dependence on the number of arms. We further show\nthat any fair algorithm must have such a dependence. When combined with regret\nbounds for standard non-fair algorithms such as UCB, this proves a strong\nseparation between fair and unfair learning, which extends to the general\ncontextual case.\n  In the general contextual case, we prove a tight connection between fairness\nand the KWIK (Knows What It Knows) learning model: a KWIK algorithm for a class\nof functions can be transformed into a provably fair contextual bandit\nalgorithm, and conversely any fair contextual bandit algorithm can be\ntransformed into a KWIK learning algorithm. This tight connection allows us to\nprovide a provably fair algorithm for the linear contextual bandit problem with\na polynomial dependence on the dimension, and to show (for a different class of\nfunctions) a worst-case exponential gap in regret between fair and non-fair\nlearning algorithms \n\n"}
{"id": "1605.07422", "contents": "Title: Computing Web-scale Topic Models using an Asynchronous Parameter Server Abstract: Topic models such as Latent Dirichlet Allocation (LDA) have been widely used\nin information retrieval for tasks ranging from smoothing and feedback methods\nto tools for exploratory search and discovery. However, classical methods for\ninferring topic models do not scale up to the massive size of today's publicly\navailable Web-scale data sets. The state-of-the-art approaches rely on custom\nstrategies, implementations and hardware to facilitate their asynchronous,\ncommunication-intensive workloads.\n  We present APS-LDA, which integrates state-of-the-art topic modeling with\ncluster computing frameworks such as Spark using a novel asynchronous parameter\nserver. Advantages of this integration include convenient usage of existing\ndata processing pipelines and eliminating the need for disk writes as data can\nbe kept in memory from start to finish. Our goal is not to outperform highly\ncustomized implementations, but to propose a general high-performance topic\nmodeling framework that can easily be used in today's data processing\npipelines. We compare APS-LDA to the existing Spark LDA implementations and\nshow that our system can, on a 480-core cluster, process up to 135 times more\ndata and 10 times more topics without sacrificing model quality. \n\n"}
{"id": "1605.07717", "contents": "Title: Deep Structured Energy Based Models for Anomaly Detection Abstract: In this paper, we attack the anomaly detection problem by directly modeling\nthe data distribution with deep architectures. We propose deep structured\nenergy based models (DSEBMs), where the energy function is the output of a\ndeterministic deep neural network with structure. We develop novel model\narchitectures to integrate EBMs with different types of data such as static\ndata, sequential data, and spatial data, and apply appropriate model\narchitectures to adapt to the data structure. Our training algorithm is built\nupon the recent development of score matching \\cite{sm}, which connects an EBM\nwith a regularized autoencoder, eliminating the need for complicated sampling\nmethod. Statistically sound decision criterion can be derived for anomaly\ndetection purpose from the perspective of the energy landscape of the data\ndistribution. We investigate two decision criteria for performing anomaly\ndetection: the energy score and the reconstruction error. Extensive empirical\nstudies on benchmark tasks demonstrate that our proposed model consistently\nmatches or outperforms all the competing methods. \n\n"}
{"id": "1605.08003", "contents": "Title: Tight Complexity Bounds for Optimizing Composite Objectives Abstract: We provide tight upper and lower bounds on the complexity of minimizing the\naverage of $m$ convex functions using gradient and prox oracles of the\ncomponent functions. We show a significant gap between the complexity of\ndeterministic vs randomized optimization. For smooth functions, we show that\naccelerated gradient descent (AGD) and an accelerated variant of SVRG are\noptimal in the deterministic and randomized settings respectively, and that a\ngradient oracle is sufficient for the optimal rate. For non-smooth functions,\nhaving access to prox oracles reduces the complexity and we present optimal\nmethods based on smoothing that improve over methods using just gradient\naccesses. \n\n"}
{"id": "1605.08233", "contents": "Title: Stochastic Variance Reduced Riemannian Eigensolver Abstract: We study the stochastic Riemannian gradient algorithm for matrix\neigen-decomposition. The state-of-the-art stochastic Riemannian algorithm\nrequires the learning rate to decay to zero and thus suffers from slow\nconvergence and sub-optimal solutions. In this paper, we address this issue by\ndeploying the variance reduction (VR) technique of stochastic gradient descent\n(SGD). The technique was originally developed to solve convex problems in the\nEuclidean space. We generalize it to Riemannian manifolds and realize it to\nsolve the non-convex eigen-decomposition problem. We are the first to propose\nand analyze the generalization of SVRG to Riemannian manifolds. Specifically,\nwe propose the general variance reduction form, SVRRG, in the framework of the\nstochastic Riemannian gradient optimization. It's then specialized to the\nproblem with eigensolvers and induces the SVRRG-EIGS algorithm. We provide a\nnovel and elegant theoretical analysis on this algorithm. The theory shows that\na fixed learning rate can be used in the Riemannian setting with an exponential\nglobal convergence rate guaranteed. The theoretical results make a significant\nimprovement over existing studies, with the effectiveness empirically verified. \n\n"}
{"id": "1605.08618", "contents": "Title: Variational Bayesian Inference for Hidden Markov Models With\n  Multivariate Gaussian Output Distributions Abstract: Hidden Markov Models (HMM) have been used for several years in many time\nseries analysis or pattern recognitions tasks. HMM are often trained by means\nof the Baum-Welch algorithm which can be seen as a special variant of an\nexpectation maximization (EM) algorithm. Second-order training techniques such\nas Variational Bayesian Inference (VI) for probabilistic models regard the\nparameters of the probabilistic models as random variables and define\ndistributions over these distribution parameters, hence the name of this\ntechnique. VI can also bee regarded as a special case of an EM algorithm. In\nthis article, we bring both together and train HMM with multivariate Gaussian\noutput distributions with VI. The article defines the new training technique\nfor HMM. An evaluation based on some case studies and a comparison to related\napproaches is part of our ongoing work. \n\n"}
{"id": "1605.08833", "contents": "Title: Muffled Semi-Supervised Learning Abstract: We explore a novel approach to semi-supervised learning. This approach is\ncontrary to the common approach in that the unlabeled examples serve to\n\"muffle,\" rather than enhance, the guidance provided by the labeled examples.\nWe provide several variants of the basic algorithm and show experimentally that\nthey can achieve significantly higher AUC than boosted trees, random forests\nand logistic regression when unlabeled examples are available. \n\n"}
{"id": "1605.09046", "contents": "Title: TripleSpin - a generic compact paradigm for fast machine learning\n  computations Abstract: We present a generic compact computational framework relying on structured\nrandom matrices that can be applied to speed up several machine learning\nalgorithms with almost no loss of accuracy. The applications include new fast\nLSH-based algorithms, efficient kernel computations via random feature maps,\nconvex optimization algorithms, quantization techniques and many more. Certain\nmodels of the presented paradigm are even more compressible since they apply\nonly bit matrices. This makes them suitable for deploying on mobile devices.\nAll our findings come with strong theoretical guarantees. In particular, as a\nbyproduct of the presented techniques and by using relatively new\nBerry-Esseen-type CLT for random vectors, we give the first theoretical\nguarantees for one of the most efficient existing LSH algorithms based on the\n$\\textbf{HD}_{3}\\textbf{HD}_{2}\\textbf{HD}_{1}$ structured matrix (\"Practical\nand Optimal LSH for Angular Distance\"). These guarantees as well as theoretical\nresults for other aforementioned applications follow from the same general\ntheoretical principle that we present in the paper. Our structured family\ncontains as special cases all previously considered structured schemes,\nincluding the recently introduced $P$-model. Experimental evaluation confirms\nthe accuracy and efficiency of TripleSpin matrices. \n\n"}
{"id": "1606.00925", "contents": "Title: Convolutional Imputation of Matrix Networks Abstract: A matrix network is a family of matrices, with relatedness modeled by a\nweighted graph. We consider the task of completing a partially observed matrix\nnetwork. We assume a novel sampling scheme where a fraction of matrices might\nbe completely unobserved. How can we recover the entire matrix network from\nincomplete observations? This mathematical problem arises in many applications\nincluding medical imaging and social networks.\n  To recover the matrix network, we propose a structural assumption that the\nmatrices have a graph Fourier transform which is low-rank. We formulate a\nconvex optimization problem and prove an exact recovery guarantee for the\noptimization problem. Furthermore, we numerically characterize the exact\nrecovery regime for varying rank and sampling rate and discover a new phase\ntransition phenomenon. Then we give an iterative imputation algorithm to\nefficiently solve the optimization problem and complete large scale matrix\nnetworks. We demonstrate the algorithm with a variety of applications such as\nMRI and Facebook user network. \n\n"}
{"id": "1606.04080", "contents": "Title: Matching Networks for One Shot Learning Abstract: Learning from a few examples remains a key challenge in machine learning.\nDespite recent advances in important domains such as vision and language, the\nstandard supervised deep learning paradigm does not offer a satisfactory\nsolution for learning new concepts rapidly from little data. In this work, we\nemploy ideas from metric learning based on deep neural features and from recent\nadvances that augment neural networks with external memories. Our framework\nlearns a network that maps a small labelled support set and an unlabelled\nexample to its label, obviating the need for fine-tuning to adapt to new class\ntypes. We then define one-shot learning problems on vision (using Omniglot,\nImageNet) and language tasks. Our algorithm improves one-shot accuracy on\nImageNet from 87.6% to 93.2% and from 88.0% to 93.8% on Omniglot compared to\ncompeting approaches. We also demonstrate the usefulness of the same model on\nlanguage modeling by introducing a one-shot task on the Penn Treebank. \n\n"}
{"id": "1606.04080", "contents": "Title: Matching Networks for One Shot Learning Abstract: Learning from a few examples remains a key challenge in machine learning.\nDespite recent advances in important domains such as vision and language, the\nstandard supervised deep learning paradigm does not offer a satisfactory\nsolution for learning new concepts rapidly from little data. In this work, we\nemploy ideas from metric learning based on deep neural features and from recent\nadvances that augment neural networks with external memories. Our framework\nlearns a network that maps a small labelled support set and an unlabelled\nexample to its label, obviating the need for fine-tuning to adapt to new class\ntypes. We then define one-shot learning problems on vision (using Omniglot,\nImageNet) and language tasks. Our algorithm improves one-shot accuracy on\nImageNet from 87.6% to 93.2% and from 88.0% to 93.8% on Omniglot compared to\ncompeting approaches. We also demonstrate the usefulness of the same model on\nlanguage modeling by introducing a one-shot task on the Penn Treebank. \n\n"}
{"id": "1606.04268", "contents": "Title: Local Canonical Correlation Analysis for Nonlinear Common Variables\n  Discovery Abstract: In this paper, we address the problem of hidden common variables discovery\nfrom multimodal data sets of nonlinear high-dimensional observations. We\npresent a metric based on local applications of canonical correlation analysis\n(CCA) and incorporate it in a kernel-based manifold learning technique.We show\nthat this metric discovers the hidden common variables underlying the\nmultimodal observations by estimating the Euclidean distance between them. Our\napproach can be viewed both as an extension of CCA to a nonlinear setting as\nwell as an extension of manifold learning to multiple data sets. Experimental\nresults show that our method indeed discovers the common variables underlying\nhigh-dimensional nonlinear observations without assuming prior rigid model\nassumptions. \n\n"}
{"id": "1606.04934", "contents": "Title: Improving Variational Inference with Inverse Autoregressive Flow Abstract: The framework of normalizing flows provides a general strategy for flexible\nvariational inference of posteriors over latent variables. We propose a new\ntype of normalizing flow, inverse autoregressive flow (IAF), that, in contrast\nto earlier published flows, scales well to high-dimensional latent spaces. The\nproposed flow consists of a chain of invertible transformations, where each\ntransformation is based on an autoregressive neural network. In experiments, we\nshow that IAF significantly improves upon diagonal Gaussian approximate\nposteriors. In addition, we demonstrate that a novel type of variational\nautoencoder, coupled with IAF, is competitive with neural autoregressive models\nin terms of attained log-likelihood on natural images, while allowing\nsignificantly faster synthesis. \n\n"}
{"id": "1606.06121", "contents": "Title: Quantifying and Reducing Stereotypes in Word Embeddings Abstract: Machine learning algorithms are optimized to model statistical properties of\nthe training data. If the input data reflects stereotypes and biases of the\nbroader society, then the output of the learning algorithm also captures these\nstereotypes. In this paper, we initiate the study of gender stereotypes in {\\em\nword embedding}, a popular framework to represent text data. As their use\nbecomes increasingly common, applications can inadvertently amplify unwanted\nstereotypes. We show across multiple datasets that the embeddings contain\nsignificant gender stereotypes, especially with regard to professions. We\ncreated a novel gender analogy task and combined it with crowdsourcing to\nsystematically quantify the gender bias in a given embedding. We developed an\nefficient algorithm that reduces gender stereotype using just a handful of\ntraining examples while preserving the useful geometric properties of the\nembedding. We evaluated our algorithm on several metrics. While we focus on\nmale/female stereotypes, our framework may be applicable to other types of\nembedding biases. \n\n"}
{"id": "1606.06160", "contents": "Title: DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low\n  Bitwidth Gradients Abstract: We propose DoReFa-Net, a method to train convolutional neural networks that\nhave low bitwidth weights and activations using low bitwidth parameter\ngradients. In particular, during backward pass, parameter gradients are\nstochastically quantized to low bitwidth numbers before being propagated to\nconvolutional layers. As convolutions during forward/backward passes can now\noperate on low bitwidth weights and activations/gradients respectively,\nDoReFa-Net can use bit convolution kernels to accelerate both training and\ninference. Moreover, as bit convolutions can be efficiently implemented on CPU,\nFPGA, ASIC and GPU, DoReFa-Net opens the way to accelerate training of low\nbitwidth neural network on these hardware. Our experiments on SVHN and ImageNet\ndatasets prove that DoReFa-Net can achieve comparable prediction accuracy as\n32-bit counterparts. For example, a DoReFa-Net derived from AlexNet that has\n1-bit weights, 2-bit activations, can be trained from scratch using 6-bit\ngradients to get 46.1\\% top-1 accuracy on ImageNet validation set. The\nDoReFa-Net AlexNet model is released publicly. \n\n"}
{"id": "1606.06361", "contents": "Title: A Probabilistic Generative Grammar for Semantic Parsing Abstract: Domain-general semantic parsing is a long-standing goal in natural language\nprocessing, where the semantic parser is capable of robustly parsing sentences\nfrom domains outside of which it was trained. Current approaches largely rely\non additional supervision from new domains in order to generalize to those\ndomains. We present a generative model of natural language utterances and\nlogical forms and demonstrate its application to semantic parsing. Our approach\nrelies on domain-independent supervision to generalize to new domains. We\nderive and implement efficient algorithms for training, parsing, and sentence\ngeneration. The work relies on a novel application of hierarchical Dirichlet\nprocesses (HDPs) for structured prediction, which we also present in this\nmanuscript.\n  This manuscript is an excerpt of chapter 4 from the Ph.D. thesis of Saparov\n(2022), where the model plays a central role in a larger natural language\nunderstanding system.\n  This manuscript provides a new simplified and more complete presentation of\nthe work first introduced in Saparov, Saraswat, and Mitchell (2017). The\ndescription and proofs of correctness of the training algorithm, parsing\nalgorithm, and sentence generation algorithm are much simplified in this new\npresentation. We also describe the novel application of hierarchical Dirichlet\nprocesses for structured prediction. In addition, we extend the earlier work\nwith a new model of word morphology, which utilizes the comprehensive\nmorphological data from Wiktionary. \n\n"}
{"id": "1607.00076", "contents": "Title: Multi-class classification: mirror descent approach Abstract: We consider the problem of multi-class classification and a stochastic opti-\nmization approach to it. We derive risk bounds for stochastic mirror descent\nalgorithm and provide examples of set geometries that make the use of the\nalgorithm efficient in terms of error in k. \n\n"}
{"id": "1607.02959", "contents": "Title: From Behavior to Sparse Graphical Games: Efficient Recovery of\n  Equilibria Abstract: In this paper we study the problem of exact recovery of the pure-strategy\nNash equilibria (PSNE) set of a graphical game from noisy observations of joint\nactions of the players alone. We consider sparse linear influence games --- a\nparametric class of graphical games with linear payoffs, and represented by\ndirected graphs of n nodes (players) and in-degree of at most k. We present an\n$\\ell_1$-regularized logistic regression based algorithm for recovering the\nPSNE set exactly, that is both computationally efficient --- i.e. runs in\npolynomial time --- and statistically efficient --- i.e. has logarithmic sample\ncomplexity. Specifically, we show that the sufficient number of samples\nrequired for exact PSNE recovery scales as $\\mathcal{O}(\\mathrm{poly}(k) \\log\nn)$. We also validate our theoretical results using synthetic experiments. \n\n"}
{"id": "1608.00860", "contents": "Title: Hierarchically Compositional Kernels for Scalable Nonparametric Learning Abstract: We propose a novel class of kernels to alleviate the high computational cost\nof large-scale nonparametric learning with kernel methods. The proposed kernel\nis defined based on a hierarchical partitioning of the underlying data domain,\nwhere the Nystr\\\"om method (a globally low-rank approximation) is married with\na locally lossless approximation in a hierarchical fashion. The kernel\nmaintains (strict) positive-definiteness. The corresponding kernel matrix\nadmits a recursively off-diagonal low-rank structure, which allows for fast\nlinear algebra computations. Suppressing the factor of data dimension, the\nmemory and arithmetic complexities for training a regression or a classifier\nare reduced from $O(n^2)$ and $O(n^3)$ to $O(nr)$ and $O(nr^2)$, respectively,\nwhere $n$ is the number of training examples and $r$ is the rank on each level\nof the hierarchy. Although other randomized approximate kernels entail a\nsimilar complexity, empirical results show that the proposed kernel achieves a\nmatching performance with a smaller $r$. We demonstrate comprehensive\nexperiments to show the effective use of the proposed kernel on data sizes up\nto the order of millions. \n\n"}
{"id": "1608.01228", "contents": "Title: Ancilla-Input and Garbage-Output Optimized Design of a Reversible\n  Quantum Integer Multiplier Abstract: A reversible logic has application in quantum computing. A reversible logic\ndesign needs resources such as ancilla and garbage qubits to reconfigure\ncircuit functions or gate functions. The removal of garbage qubits and ancilla\nqubits are essential in designing an efficient quantum circuit. In the\nliterature, there are multiple designs that have been proposed for a reversible\nmultiplication operation. A multiplication hardware is essential for the\ncircuit design of quantum algorithms, quantum cryptanalysis, and digital signal\nprocessing (DSP) applications. The existing designs of reversible quantum\ninteger multipliers suffer from redundant garbage qubits. In this work, we\npropose a reversible logic based, garbage-free and ancilla qubit optimized\ndesign of a quantum integer multiplier. The proposed quantum integer multiplier\nutilizes a novel add and rotate methodology that is specially suitable for a\nreversible computing paradigm. The proposed design methodology is the modified\nversion of a conventional shift and add method. The proposed design of the\nquantum integer multiplier incorporates add or no operation based on multiplier\nqubits and followed by a rotate right operation. The proposed design of the\nquantum integer multiplier produces zero garbage qubits and shows an\nimprovement ranging from 60% to 90% in ancilla qubits count over the existing\nwork on reversible quantum integer multipliers. \n\n"}
{"id": "1608.01230", "contents": "Title: Learning a Driving Simulator Abstract: Comma.ai's approach to Artificial Intelligence for self-driving cars is based\non an agent that learns to clone driver behaviors and plans maneuvers by\nsimulating future events in the road. This paper illustrates one of our\nresearch approaches for driving simulation. One where we learn to simulate.\nHere we investigate variational autoencoders with classical and learned cost\nfunctions using generative adversarial networks for embedding road frames.\nAfterwards, we learn a transition model in the embedded space using action\nconditioned Recurrent Neural Networks. We show that our approach can keep\npredicting realistic looking video for several frames despite the transition\nmodel being optimized without a cost function in the pixel space. \n\n"}
{"id": "1608.01747", "contents": "Title: A Distance for HMMs based on Aggregated Wasserstein Metric and State\n  Registration Abstract: We propose a framework, named Aggregated Wasserstein, for computing a\ndissimilarity measure or distance between two Hidden Markov Models with state\nconditional distributions being Gaussian. For such HMMs, the marginal\ndistribution at any time spot follows a Gaussian mixture distribution, a fact\nexploited to softly match, aka register, the states in two HMMs. We refer to\nsuch HMMs as Gaussian mixture model-HMM (GMM-HMM). The registration of states\nis inspired by the intrinsic relationship of optimal transport and the\nWasserstein metric between distributions. Specifically, the components of the\nmarginal GMMs are matched by solving an optimal transport problem where the\ncost between components is the Wasserstein metric for Gaussian distributions.\nThe solution of the optimization problem is a fast approximation to the\nWasserstein metric between two GMMs. The new Aggregated Wasserstein distance is\na semi-metric and can be computed without generating Monte Carlo samples. It is\ninvariant to relabeling or permutation of the states. This distance quantifies\nthe dissimilarity of GMM-HMMs by measuring both the difference between the two\nmarginal GMMs and the difference between the two transition matrices. Our new\ndistance is tested on the tasks of retrieval and classification of time series.\nExperiments on both synthetic data and real data have demonstrated its\nadvantages in terms of accuracy as well as efficiency in comparison with\nexisting distances based on the Kullback-Leibler divergence. \n\n"}
{"id": "1608.02005", "contents": "Title: Quantum algorithms for abelian difference sets and applications to\n  dihedral hidden subgroups Abstract: Difference sets are basic combinatorial structures that have applications in\nsignal processing, coding theory, and cryptography. We consider the problem of\nidentifying a shifted version of the characteristic function of a (known)\ndifference set. We present a generic quantum algorithm that can be used to\ntackle any hidden shift problem for any difference set in any abelian group. We\ndiscuss special cases of this framework where the resulting quantum algorithm\nis efficient. This includes: a) Paley difference sets based on quadratic\nresidues in finite fields, which allows to recover the shifted Legendre\nfunction quantum algorithm, b) Hadamard difference sets, which allows to\nrecover the shifted bent function quantum algorithm, and c) Singer difference\nsets based on finite geometries. The latter class allows us to define instances\nof the dihedral hidden subgroup problem that can be efficiently solved on a\nquantum computer. \n\n"}
{"id": "1608.03355", "contents": "Title: A Practical Quantum Instruction Set Architecture Abstract: We introduce an abstract machine architecture for classical/quantum\ncomputations---including compilation---along with a quantum instruction\nlanguage called Quil for explicitly writing these computations. With this\nformalism, we discuss concrete implementations of the machine and non-trivial\nalgorithms targeting them. The introduction of this machine dovetails with\nongoing development of quantum computing technology, and makes possible\nportable descriptions of recent classical/quantum algorithms. \n\n"}
{"id": "1608.03665", "contents": "Title: Learning Structured Sparsity in Deep Neural Networks Abstract: High demand for computation resources severely hinders deployment of\nlarge-scale Deep Neural Networks (DNN) in resource constrained devices. In this\nwork, we propose a Structured Sparsity Learning (SSL) method to regularize the\nstructures (i.e., filters, channels, filter shapes, and layer depth) of DNNs.\nSSL can: (1) learn a compact structure from a bigger DNN to reduce computation\ncost; (2) obtain a hardware-friendly structured sparsity of DNN to efficiently\naccelerate the DNNs evaluation. Experimental results show that SSL achieves on\naverage 5.1x and 3.1x speedups of convolutional layer computation of AlexNet\nagainst CPU and GPU, respectively, with off-the-shelf libraries. These speedups\nare about twice speedups of non-structured sparsity; (3) regularize the DNN\nstructure to improve classification accuracy. The results show that for\nCIFAR-10, regularization on layer depth can reduce 20 layers of a Deep Residual\nNetwork (ResNet) to 18 layers while improve the accuracy from 91.25% to 92.60%,\nwhich is still slightly higher than that of original ResNet with 32 layers. For\nAlexNet, structure regularization by SSL also reduces the error by around ~1%.\nOpen source code is in https://github.com/wenwei202/caffe/tree/scnn \n\n"}
{"id": "1608.04236", "contents": "Title: Generative and Discriminative Voxel Modeling with Convolutional Neural\n  Networks Abstract: When working with three-dimensional data, choice of representation is key. We\nexplore voxel-based models, and present evidence for the viability of\nvoxellated representations in applications including shape modeling and object\nclassification. Our key contributions are methods for training voxel-based\nvariational autoencoders, a user interface for exploring the latent space\nlearned by the autoencoder, and a deep convolutional neural network\narchitecture for object classification. We address challenges unique to\nvoxel-based representations, and empirically evaluate our models on the\nModelNet benchmark, where we demonstrate a 51.5% relative improvement in the\nstate of the art for object classification. \n\n"}
{"id": "1608.04414", "contents": "Title: Generalization of ERM in Stochastic Convex Optimization: The Dimension\n  Strikes Back Abstract: In stochastic convex optimization the goal is to minimize a convex function\n$F(x) \\doteq {\\mathbf E}_{{\\mathbf f}\\sim D}[{\\mathbf f}(x)]$ over a convex set\n$\\cal K \\subset {\\mathbb R}^d$ where $D$ is some unknown distribution and each\n$f(\\cdot)$ in the support of $D$ is convex over $\\cal K$. The optimization is\ncommonly based on i.i.d.~samples $f^1,f^2,\\ldots,f^n$ from $D$. A standard\napproach to such problems is empirical risk minimization (ERM) that optimizes\n$F_S(x) \\doteq \\frac{1}{n}\\sum_{i\\leq n} f^i(x)$. Here we consider the question\nof how many samples are necessary for ERM to succeed and the closely related\nquestion of uniform convergence of $F_S$ to $F$ over $\\cal K$. We demonstrate\nthat in the standard $\\ell_p/\\ell_q$ setting of Lipschitz-bounded functions\nover a $\\cal K$ of bounded radius, ERM requires sample size that scales\nlinearly with the dimension $d$. This nearly matches standard upper bounds and\nimproves on $\\Omega(\\log d)$ dependence proved for $\\ell_2/\\ell_2$ setting by\nShalev-Shwartz et al. (2009). In stark contrast, these problems can be solved\nusing dimension-independent number of samples for $\\ell_2/\\ell_2$ setting and\n$\\log d$ dependence for $\\ell_1/\\ell_\\infty$ setting using other approaches. We\nfurther show that our lower bound applies even if the functions in the support\nof $D$ are smooth and efficiently computable and even if an $\\ell_1$\nregularization term is added. Finally, we demonstrate that for a more general\nclass of bounded-range (but not Lipschitz-bounded) stochastic convex programs\nan infinite gap appears already in dimension 2. \n\n"}
{"id": "1608.04478", "contents": "Title: A Geometrical Approach to Topic Model Estimation Abstract: In the probabilistic topic models, the quantity of interest---a low-rank\nmatrix consisting of topic vectors---is hidden in the text corpus matrix,\nmasked by noise, and the Singular Value Decomposition (SVD) is a potentially\nuseful tool for learning such a low-rank matrix. However, the connection\nbetween this low-rank matrix and the singular vectors of the text corpus matrix\nare usually complicated and hard to spell out, so how to use SVD for learning\ntopic models faces challenges. In this paper, we overcome the challenge by\nrevealing a surprising insight: there is a low-dimensional simplex structure\nwhich can be viewed as a bridge between the low-rank matrix of interest and the\nSVD of the text corpus matrix, and allows us to conveniently reconstruct the\nformer using the latter. Such an insight motivates a new SVD approach to\nlearning topic models, which we analyze with delicate random matrix theory and\nderive the rate of convergence. We support our methods and theory numerically,\nusing both simulated data and real data. \n\n"}
{"id": "1608.05182", "contents": "Title: A Bayesian Nonparametric Approach for Estimating Individualized\n  Treatment-Response Curves Abstract: We study the problem of estimating the continuous response over time to\ninterventions using observational time series---a retrospective dataset where\nthe policy by which the data are generated is unknown to the learner. We are\nmotivated by applications where response varies by individuals and therefore,\nestimating responses at the individual-level is valuable for personalizing\ndecision-making. We refer to this as the problem of estimating individualized\ntreatment response (ITR) curves. In statistics, G-computation formula (Robins,\n1986) has been commonly used for estimating treatment responses from\nobservational data containing sequential treatment assignments. However, past\nstudies have focused predominantly on obtaining point-in-time estimates at the\npopulation level. We leverage the G-computation formula and develop a novel\nBayesian nonparametric (BNP) method that can flexibly model functional data and\nprovide posterior inference over the treatment response curves at both the\nindividual and population level. On a challenging dataset containing time\nseries from patients admitted to a hospital, we estimate responses to\ntreatments used in managing kidney function and show that the resulting fits\nare more accurate than alternative approaches. Accurate methods for obtaining\nITRs from observational data can dramatically accelerate the pace at which\npersonalized treatment plans become possible. \n\n"}
{"id": "1608.05275", "contents": "Title: A Tight Convex Upper Bound on the Likelihood of a Finite Mixture Abstract: The likelihood function of a finite mixture model is a non-convex function\nwith multiple local maxima and commonly used iterative algorithms such as EM\nwill converge to different solutions depending on initial conditions. In this\npaper we ask: is it possible to assess how far we are from the global maximum\nof the likelihood? Since the likelihood of a finite mixture model can grow\nunboundedly by centering a Gaussian on a single datapoint and shrinking the\ncovariance, we constrain the problem by assuming that the parameters of the\nindividual models are members of a large discrete set (e.g. estimating a\nmixture of two Gaussians where the means and variances of both Gaussians are\nmembers of a set of a million possible means and variances). For this setting\nwe show that a simple upper bound on the likelihood can be computed using\nconvex optimization and we analyze conditions under which the bound is\nguaranteed to be tight. This bound can then be used to assess the quality of\nsolutions found by EM (where the final result is projected on the discrete set)\nor any other mixture estimation algorithm. For any dataset our method allows us\nto find a finite mixture model together with a dataset-specific bound on how\nfar the likelihood of this mixture is from the global optimum of the likelihood \n\n"}
{"id": "1608.05889", "contents": "Title: Online Feature Selection with Group Structure Analysis Abstract: Online selection of dynamic features has attracted intensive interest in\nrecent years. However, existing online feature selection methods evaluate\nfeatures individually and ignore the underlying structure of feature stream.\nFor instance, in image analysis, features are generated in groups which\nrepresent color, texture and other visual information. Simply breaking the\ngroup structure in feature selection may degrade performance. Motivated by this\nfact, we formulate the problem as an online group feature selection. The\nproblem assumes that features are generated individually but there are group\nstructure in the feature stream. To the best of our knowledge, this is the\nfirst time that the correlation among feature stream has been considered in the\nonline feature selection process. To solve this problem, we develop a novel\nonline group feature selection method named OGFS. Our proposed approach\nconsists of two stages: online intra-group selection and online inter-group\nselection. In the intra-group selection, we design a criterion based on\nspectral analysis to select discriminative features in each group. In the\ninter-group selection, we utilize a linear regression model to select an\noptimal subset. This two-stage procedure continues until there are no more\nfeatures arriving or some predefined stopping conditions are met. %Our method\nhas been applied Finally, we apply our method to multiple tasks including image\nclassification %, face verification and face verification. Extensive empirical\nstudies performed on real-world and benchmark data sets demonstrate that our\nmethod outperforms other state-of-the-art online feature selection %method\nmethods. \n\n"}
{"id": "1608.06253", "contents": "Title: Multi-Dueling Bandits and Their Application to Online Ranker Evaluation Abstract: New ranking algorithms are continually being developed and refined,\nnecessitating the development of efficient methods for evaluating these\nrankers. Online ranker evaluation focuses on the challenge of efficiently\ndetermining, from implicit user feedback, which ranker out of a finite set of\nrankers is the best. Online ranker evaluation can be modeled by dueling ban-\ndits, a mathematical model for online learning under limited feedback from\npairwise comparisons. Comparisons of pairs of rankers is performed by\ninterleaving their result sets and examining which documents users click on.\nThe dueling bandits model addresses the key issue of which pair of rankers to\ncompare at each iteration, thereby providing a solution to the\nexploration-exploitation trade-off. Recently, methods for simultaneously\ncomparing more than two rankers have been developed. However, the question of\nwhich rankers to compare at each iteration was left open. We address this\nquestion by proposing a generalization of the dueling bandits model that uses\nsimultaneous comparisons of an unrestricted number of rankers. We evaluate our\nalgorithm on synthetic data and several standard large-scale online ranker\nevaluation datasets. Our experimental results show that the algorithm yields\norders of magnitude improvement in performance compared to stateof- the-art\ndueling bandit algorithms. \n\n"}
{"id": "1609.00489", "contents": "Title: A deep learning model for estimating story points Abstract: Although there has been substantial research in software analytics for effort\nestimation in traditional software projects, little work has been done for\nestimation in agile projects, especially estimating user stories or issues.\nStory points are the most common unit of measure used for estimating the effort\ninvolved in implementing a user story or resolving an issue. In this paper, we\noffer for the \\emph{first} time a comprehensive dataset for story points-based\nestimation that contains 23,313 issues from 16 open source projects. We also\npropose a prediction model for estimating story points based on a novel\ncombination of two powerful deep learning architectures: long short-term memory\nand recurrent highway network. Our prediction system is \\emph{end-to-end}\ntrainable from raw input data to prediction outcomes without any manual feature\nengineering. An empirical evaluation demonstrates that our approach\nconsistently outperforms three common effort estimation baselines and two\nalternatives in both Mean Absolute Error and the Standardized Accuracy. \n\n"}
{"id": "1609.00629", "contents": "Title: SEBOOST - Boosting Stochastic Learning Using Subspace Optimization\n  Techniques Abstract: We present SEBOOST, a technique for boosting the performance of existing\nstochastic optimization methods. SEBOOST applies a secondary optimization\nprocess in the subspace spanned by the last steps and descent directions. The\nmethod was inspired by the SESOP optimization method for large-scale problems,\nand has been adapted for the stochastic learning framework. It can be applied\non top of any existing optimization method with no need to tweak the internal\nalgorithm. We show that the method is able to boost the performance of\ndifferent algorithms, and make them more robust to changes in their\nhyper-parameters. As the boosting steps of SEBOOST are applied between large\nsets of descent steps, the additional subspace optimization hardly increases\nthe overall computational burden. We introduce two hyper-parameters that\ncontrol the balance between the baseline method and the secondary optimization\nprocess. The method was evaluated on several deep learning tasks, demonstrating\npromising results. \n\n"}
{"id": "1609.00904", "contents": "Title: High Dimensional Human Guided Machine Learning Abstract: Have you ever looked at a machine learning classification model and thought,\nI could have made that? Well, that is what we test in this project, comparing\nXGBoost trained on human engineered features to training directly on data. The\nhuman engineered features do not outperform XGBoost trained di- rectly on the\ndata, but they are comparable. This project con- tributes a novel method for\nutilizing human created classifi- cation models on high dimensional datasets. \n\n"}
{"id": "1609.01226", "contents": "Title: The Robustness of Estimator Composition Abstract: We formalize notions of robustness for composite estimators via the notion of\na breakdown point. A composite estimator successively applies two (or more)\nestimators: on data decomposed into disjoint parts, it applies the first\nestimator on each part, then the second estimator on the outputs of the first\nestimator. And so on, if the composition is of more than two estimators.\nInformally, the breakdown point is the minimum fraction of data points which if\nsignificantly modified will also significantly modify the output of the\nestimator, so it is typically desirable to have a large breakdown point. Our\nmain result shows that, under mild conditions on the individual estimators, the\nbreakdown point of the composite estimator is the product of the breakdown\npoints of the individual estimators. We also demonstrate several scenarios,\nranging from regression to statistical testing, where this analysis is easy to\napply, useful in understanding worst case robustness, and sheds powerful\ninsights onto the associated data analysis. \n\n"}
{"id": "1609.02943", "contents": "Title: Stealing Machine Learning Models via Prediction APIs Abstract: Machine learning (ML) models may be deemed confidential due to their\nsensitive training data, commercial value, or use in security applications.\nIncreasingly often, confidential ML models are being deployed with publicly\naccessible query interfaces. ML-as-a-service (\"predictive analytics\") systems\nare an example: Some allow users to train models on potentially sensitive data\nand charge others for access on a pay-per-query basis.\n  The tension between model confidentiality and public access motivates our\ninvestigation of model extraction attacks. In such attacks, an adversary with\nblack-box access, but no prior knowledge of an ML model's parameters or\ntraining data, aims to duplicate the functionality of (i.e., \"steal\") the\nmodel. Unlike in classical learning theory settings, ML-as-a-service offerings\nmay accept partial feature vectors as inputs and include confidence values with\npredictions. Given these practices, we show simple, efficient attacks that\nextract target ML models with near-perfect fidelity for popular model classes\nincluding logistic regression, neural networks, and decision trees. We\ndemonstrate these attacks against the online services of BigML and Amazon\nMachine Learning. We further show that the natural countermeasure of omitting\nconfidence values from model outputs still admits potentially harmful model\nextraction attacks. Our results highlight the need for careful ML model\ndeployment and new model extraction countermeasures. \n\n"}
{"id": "1609.04789", "contents": "Title: Coherence Pursuit: Fast, Simple, and Robust Principal Component Analysis Abstract: This paper presents a remarkably simple, yet powerful, algorithm termed\nCoherence Pursuit (CoP) to robust Principal Component Analysis (PCA). As\ninliers lie in a low dimensional subspace and are mostly correlated, an inlier\nis likely to have strong mutual coherence with a large number of data points.\nBy contrast, outliers either do not admit low dimensional structures or form\nsmall clusters. In either case, an outlier is unlikely to bear strong\nresemblance to a large number of data points. Given that, CoP sets an outlier\napart from an inlier by comparing their coherence with the rest of the data\npoints. The mutual coherences are computed by forming the Gram matrix of the\nnormalized data points. Subsequently, the sought subspace is recovered from the\nspan of the subset of the data points that exhibit strong coherence with the\nrest of the data. As CoP only involves one simple matrix multiplication, it is\nsignificantly faster than the state-of-the-art robust PCA algorithms. We derive\nanalytical performance guarantees for CoP under different models for the\ndistributions of inliers and outliers in both noise-free and noisy settings.\nCoP is the first robust PCA algorithm that is simultaneously non-iterative,\nprovably robust to both unstructured and structured outliers, and can tolerate\na large number of unstructured outliers. \n\n"}
{"id": "1609.05536", "contents": "Title: Learning Personalized Optimal Control for Repeatedly Operated Systems Abstract: We consider the problem of online learning of optimal control for repeatedly\noperated systems in the presence of parametric uncertainty. During each round\nof operation, environment selects system parameters according to a fixed but\nunknown probability distribution. These parameters govern the dynamics of a\nplant. An agent chooses a control input to the plant and is then revealed the\ncost of the choice. In this setting, we design an agent that personalizes the\ncontrol input to this plant taking into account the stochasticity involved. We\ndemonstrate the effectiveness of our approach on a simulated system. \n\n"}
{"id": "1609.06831", "contents": "Title: Hawkes Processes with Stochastic Excitations Abstract: We propose an extension to Hawkes processes by treating the levels of\nself-excitation as a stochastic differential equation. Our new point process\nallows better approximation in application domains where events and intensities\naccelerate each other with correlated levels of contagion. We generalize a\nrecent algorithm for simulating draws from Hawkes processes whose levels of\nexcitation are stochastic processes, and propose a hybrid Markov chain Monte\nCarlo approach for model fitting. Our sampling procedure scales linearly with\nthe number of required events and does not require stationarity of the point\nprocess. A modular inference procedure consisting of a combination between\nGibbs and Metropolis Hastings steps is put forward. We recover expectation\nmaximization as a special case. Our general approach is illustrated for\ncontagion following geometric Brownian motion and exponential Langevin\ndynamics. \n\n"}
{"id": "1609.07087", "contents": "Title: (Bandit) Convex Optimization with Biased Noisy Gradient Oracles Abstract: Algorithms for bandit convex optimization and online learning often rely on\nconstructing noisy gradient estimates, which are then used in appropriately\nadjusted first-order algorithms, replacing actual gradients. Depending on the\nproperties of the function to be optimized and the nature of ``noise'' in the\nbandit feedback, the bias and variance of gradient estimates exhibit various\ntradeoffs. In this paper we propose a novel framework that replaces the\nspecific gradient estimation methods with an abstract oracle. With the help of\nthe new framework we unify previous works, reproducing their results in a clean\nand concise fashion, while, perhaps more importantly, the framework also allows\nus to formally show that to achieve the optimal root-$n$ rate either the\nalgorithms that use existing gradient estimators, or the proof techniques used\nto analyze them have to go beyond what exists today. \n\n"}
{"id": "1609.07257", "contents": "Title: Using Neural Network Formalism to Solve Multiple-Instance Problems Abstract: Many objects in the real world are difficult to describe by a single\nnumerical vector of a fixed length, whereas describing them by a set of vectors\nis more natural. Therefore, Multiple instance learning (MIL) techniques have\nbeen constantly gaining on importance throughout last years. MIL formalism\nrepresents each object (sample) by a set (bag) of feature vectors (instances)\nof fixed length where knowledge about objects (e.g., class label) is available\non bag level but not necessarily on instance level. Many standard tools\nincluding supervised classifiers have been already adapted to MIL setting since\nthe problem got formalized in late nineties. In this work we propose a neural\nnetwork (NN) based formalism that intuitively bridges the gap between MIL\nproblem definition and the vast existing knowledge-base of standard models and\nclassifiers. We show that the proposed NN formalism is effectively optimizable\nby a modified back-propagation algorithm and can reveal unknown patterns inside\nbags. Comparison to eight types of classifiers from the prior art on a set of\n14 publicly available benchmark datasets confirms the advantages and accuracy\nof the proposed solution. \n\n"}
{"id": "1610.00696", "contents": "Title: Deep Visual Foresight for Planning Robot Motion Abstract: A key challenge in scaling up robot learning to many skills and environments\nis removing the need for human supervision, so that robots can collect their\nown data and improve their own performance without being limited by the cost of\nrequesting human feedback. Model-based reinforcement learning holds the promise\nof enabling an agent to learn to predict the effects of its actions, which\ncould provide flexible predictive models for a wide range of tasks and\nenvironments, without detailed human supervision. We develop a method for\ncombining deep action-conditioned video prediction models with model-predictive\ncontrol that uses entirely unlabeled training data. Our approach does not\nrequire a calibrated camera, an instrumented training set-up, nor precise\nsensing and actuation. Our results show that our method enables a real robot to\nperform nonprehensile manipulation -- pushing objects -- and can handle novel\nobjects not seen during training. \n\n"}
{"id": "1610.01675", "contents": "Title: Generalized Inverse Classification Abstract: Inverse classification is the process of perturbing an instance in a\nmeaningful way such that it is more likely to conform to a specific class.\nHistorical methods that address such a problem are often framed to leverage\nonly a single classifier, or specific set of classifiers. These works are often\naccompanied by naive assumptions. In this work we propose generalized inverse\nclassification (GIC), which avoids restricting the classification model that\ncan be used. We incorporate this formulation into a refined framework in which\nGIC takes place. Under this framework, GIC operates on features that are\nimmediately actionable. Each change incurs an individual cost, either linear or\nnon-linear. Such changes are subjected to occur within a specified level of\ncumulative change (budget). Furthermore, our framework incorporates the\nestimation of features that change as a consequence of direct actions taken\n(indirectly changeable features). To solve such a problem, we propose three\nreal-valued heuristic-based methods and two sensitivity analysis-based\ncomparison methods, each of which is evaluated on two freely available\nreal-world datasets. Our results demonstrate the validity and benefits of our\nformulation, framework, and methods. \n\n"}
{"id": "1610.02143", "contents": "Title: Stochastic Averaging for Constrained Optimization with Application to\n  Online Resource Allocation Abstract: Existing approaches to resource allocation for nowadays stochastic networks\nare challenged to meet fast convergence and tolerable delay requirements. The\npresent paper leverages online learning advances to facilitate stochastic\nresource allocation tasks. By recognizing the central role of Lagrange\nmultipliers, the underlying constrained optimization problem is formulated as a\nmachine learning task involving both training and operational modes, with the\ngoal of learning the sought multipliers in a fast and efficient manner. To this\nend, an order-optimal offline learning approach is developed first for batch\ntraining, and it is then generalized to the online setting with a procedure\ntermed learn-and-adapt. The novel resource allocation protocol permeates\nbenefits of stochastic approximation and statistical learning to obtain\nlow-complexity online updates with learning errors close to the statistical\naccuracy limits, while still preserving adaptation performance, which in the\nstochastic network optimization context guarantees queue stability. Analysis\nand simulated tests demonstrate that the proposed data-driven approach improves\nthe delay and convergence performance of existing resource allocation schemes. \n\n"}
{"id": "1610.05083", "contents": "Title: Efficient Metric Learning for the Analysis of Motion Data Abstract: We investigate metric learning in the context of dynamic time warping (DTW),\nthe by far most popular dissimilarity measure used for the comparison and\nanalysis of motion capture data. While metric learning enables a\nproblem-adapted representation of data, the majority of methods has been\nproposed for vectorial data only. In this contribution, we extend the popular\nprinciple offered by the large margin nearest neighbors learner (LMNN) to DTW\nby treating the resulting component-wise dissimilarity values as features. We\ndemonstrate that this principle greatly enhances the classification accuracy in\nseveral benchmarks. Further, we show that recent auxiliary concepts such as\nmetric regularization can be transferred from the vectorial case to\ncomponent-wise DTW in a similar way. We illustrate that metric regularization\nconstitutes a crucial prerequisite for the interpretation of the resulting\nrelevance profiles. \n\n"}
{"id": "1610.06453", "contents": "Title: Change-point Detection Methods for Body-Worn Video Abstract: Body-worn video (BWV) cameras are increasingly utilized by police departments\nto provide a record of police-public interactions. However, large-scale BWV\ndeployment produces terabytes of data per week, necessitating the development\nof effective computational methods to identify salient changes in video. In\nwork carried out at the 2016 RIPS program at IPAM, UCLA, we present a novel\ntwo-stage framework for video change-point detection. First, we employ\nstate-of-the-art machine learning methods including convolutional neural\nnetworks and support vector machines for scene classification. We then develop\nand compare change-point detection algorithms utilizing mean squared-error\nminimization, forecasting methods, hidden Markov models, and maximum likelihood\nestimation to identify noteworthy changes. We test our framework on detection\nof vehicle exits and entrances in a BWV data set provided by the Los Angeles\nPolice Department and achieve over 90% recall and nearly 70% precision --\ndemonstrating robustness to rapid scene changes, extreme luminance differences,\nand frequent camera occlusions. \n\n"}
{"id": "1610.07797", "contents": "Title: Frank-Wolfe Algorithms for Saddle Point Problems Abstract: We extend the Frank-Wolfe (FW) optimization algorithm to solve constrained\nsmooth convex-concave saddle point (SP) problems. Remarkably, the method only\nrequires access to linear minimization oracles. Leveraging recent advances in\nFW optimization, we provide the first proof of convergence of a FW-type saddle\npoint solver over polytopes, thereby partially answering a 30 year-old\nconjecture. We also survey other convergence results and highlight gaps in the\ntheoretical underpinnings of FW-style algorithms. Motivating applications\nwithout known efficient alternatives are explored through structured prediction\nwith combinatorial penalties as well as games over matching polytopes involving\nan exponential number of constraints. \n\n"}
{"id": "1611.01236", "contents": "Title: Adversarial Machine Learning at Scale Abstract: Adversarial examples are malicious inputs designed to fool machine learning\nmodels. They often transfer from one model to another, allowing attackers to\nmount black box attacks without knowledge of the target model's parameters.\nAdversarial training is the process of explicitly training a model on\nadversarial examples, in order to make it more robust to attack or to reduce\nits test error on clean inputs. So far, adversarial training has primarily been\napplied to small problems. In this research, we apply adversarial training to\nImageNet. Our contributions include: (1) recommendations for how to succesfully\nscale adversarial training to large models and datasets, (2) the observation\nthat adversarial training confers robustness to single-step attack methods, (3)\nthe finding that multi-step attack methods are somewhat less transferable than\nsingle-step attack methods, so single-step attacks are the best for mounting\nblack-box attacks, and (4) resolution of a \"label leaking\" effect that causes\nadversarially trained models to perform better on adversarial examples than on\nclean examples, because the adversarial example construction process uses the\ntrue label and the model can learn to exploit regularities in the construction\nprocess. \n\n"}
{"id": "1611.03898", "contents": "Title: Low Latency Anomaly Detection and Bayesian Network Prediction of Anomaly\n  Likelihood Abstract: We develop a supervised machine learning model that detects anomalies in\nsystems in real time. Our model processes unbounded streams of data into time\nseries which then form the basis of a low-latency anomaly detection model.\nMoreover, we extend our preliminary goal of just anomaly detection to\nsimultaneous anomaly prediction. We approach this very challenging problem by\ndeveloping a Bayesian Network framework that captures the information about the\nparameters of the lagged regressors calibrated in the first part of our\napproach and use this structure to learn local conditional probability\ndistributions. \n\n"}
{"id": "1611.04528", "contents": "Title: Benchmarking Quantum Hardware for Training of Fully Visible Boltzmann\n  Machines Abstract: Quantum annealing (QA) is a hardware-based heuristic optimization and\nsampling method applicable to discrete undirected graphical models. While\nsimilar to simulated annealing, QA relies on quantum, rather than thermal,\neffects to explore complex search spaces. For many classes of problems, QA is\nknown to offer computational advantages over simulated annealing. Here we\nreport on the ability of recent QA hardware to accelerate training of fully\nvisible Boltzmann machines. We characterize the sampling distribution of QA\nhardware, and show that in many cases, the quantum distributions differ\nsignificantly from classical Boltzmann distributions. In spite of this\ndifference, training (which seeks to match data and model statistics) using\nstandard classical gradient updates is still effective. We investigate the use\nof QA for seeding Markov chains as an alternative to contrastive divergence\n(CD) and persistent contrastive divergence (PCD). Using $k=50$ Gibbs steps, we\nshow that for problems with high-energy barriers between modes, QA-based seeds\ncan improve upon chains with CD and PCD initializations. For these hard\nproblems, QA gradient estimates are more accurate, and allow for faster\nlearning. Furthermore, and interestingly, even the case of raw QA samples (that\nis, $k=0$) achieved similar improvements. We argue that this relates to the\nfact that we are training a quantum rather than classical Boltzmann\ndistribution in this case. The learned parameters give rise to hardware QA\ndistributions closely approximating classical Boltzmann distributions that are\nhard to train with CD/PCD. \n\n"}
{"id": "1611.04967", "contents": "Title: Iterative Orthogonal Feature Projection for Diagnosing Bias in Black-Box\n  Models Abstract: Predictive models are increasingly deployed for the purpose of determining\naccess to services such as credit, insurance, and employment. Despite potential\ngains in productivity and efficiency, several potential problems have yet to be\naddressed, particularly the potential for unintentional discrimination. We\npresent an iterative procedure, based on orthogonal projection of input\nattributes, for enabling interpretability of black-box predictive models.\nThrough our iterative procedure, one can quantify the relative dependence of a\nblack-box model on its input attributes.The relative significance of the inputs\nto a predictive model can then be used to assess the fairness (or\ndiscriminatory extent) of such a model. \n\n"}
{"id": "1611.06475", "contents": "Title: Dealing with Range Anxiety in Mean Estimation via Statistical Queries Abstract: We give algorithms for estimating the expectation of a given real-valued\nfunction $\\phi:X\\to {\\bf R}$ on a sample drawn randomly from some unknown\ndistribution $D$ over domain $X$, namely ${\\bf E}_{{\\bf x}\\sim D}[\\phi({\\bf\nx})]$. Our algorithms work in two well-studied models of restricted access to\ndata samples. The first one is the statistical query (SQ) model in which an\nalgorithm has access to an SQ oracle for the input distribution $D$ over $X$\ninstead of i.i.d. samples from $D$. Given a query function $\\phi:X \\to [0,1]$,\nthe oracle returns an estimate of ${\\bf E}_{{\\bf x}\\sim D}[\\phi({\\bf x})]$\nwithin some tolerance $\\tau$. The second, is a model in which only a single bit\nis communicated from each sample. In both of these models the error obtained\nusing a naive implementation would scale polynomially with the range of the\nrandom variable $\\phi({\\bf x})$ (which might even be infinite). In contrast,\nwithout restrictions on access to data the expected error scales with the\nstandard deviation of $\\phi({\\bf x})$. Here we give a simple algorithm whose\nerror scales linearly in standard deviation of $\\phi({\\bf x})$ and\nlogarithmically with an upper bound on the second moment of $\\phi({\\bf x})$.\n  As corollaries, we obtain algorithms for high dimensional mean estimation and\nstochastic convex optimization in these models that work in more general\nsettings than previously known solutions. \n\n"}
{"id": "1611.07004", "contents": "Title: Image-to-Image Translation with Conditional Adversarial Networks Abstract: We investigate conditional adversarial networks as a general-purpose solution\nto image-to-image translation problems. These networks not only learn the\nmapping from input image to output image, but also learn a loss function to\ntrain this mapping. This makes it possible to apply the same generic approach\nto problems that traditionally would require very different loss formulations.\nWe demonstrate that this approach is effective at synthesizing photos from\nlabel maps, reconstructing objects from edge maps, and colorizing images, among\nother tasks. Indeed, since the release of the pix2pix software associated with\nthis paper, a large number of internet users (many of them artists) have posted\ntheir own experiments with our system, further demonstrating its wide\napplicability and ease of adoption without the need for parameter tweaking. As\na community, we no longer hand-engineer our mapping functions, and this work\nsuggests we can achieve reasonable results without hand-engineering our loss\nfunctions either. \n\n"}
{"id": "1611.07056", "contents": "Title: The Recycling Gibbs Sampler for Efficient Learning Abstract: Monte Carlo methods are essential tools for Bayesian inference. Gibbs\nsampling is a well-known Markov chain Monte Carlo (MCMC) algorithm, extensively\nused in signal processing, machine learning, and statistics, employed to draw\nsamples from complicated high-dimensional posterior distributions. The key\npoint for the successful application of the Gibbs sampler is the ability to\ndraw efficiently samples from the full-conditional probability density\nfunctions. Since in the general case this is not possible, in order to speed up\nthe convergence of the chain, it is required to generate auxiliary samples\nwhose information is eventually disregarded. In this work, we show that these\nauxiliary samples can be recycled within the Gibbs estimators, improving their\nefficiency with no extra cost. This novel scheme arises naturally after\npointing out the relationship between the standard Gibbs sampler and the chain\nrule used for sampling purposes. Numerical simulations involving simple and\nreal inference problems confirm the excellent performance of the proposed\nscheme in terms of accuracy and computational efficiency. In particular we give\nempirical evidence of performance in a toy example, inference of Gaussian\nprocesses hyperparameters, and learning dependence graphs through regression. \n\n"}
{"id": "1611.07078", "contents": "Title: A Deep Learning Approach for Joint Video Frame and Reward Prediction in\n  Atari Games Abstract: Reinforcement learning is concerned with identifying reward-maximizing\nbehaviour policies in environments that are initially unknown. State-of-the-art\nreinforcement learning approaches, such as deep Q-networks, are model-free and\nlearn to act effectively across a wide range of environments such as Atari\ngames, but require huge amounts of data. Model-based techniques are more\ndata-efficient, but need to acquire explicit knowledge about the environment.\n  In this paper, we take a step towards using model-based techniques in\nenvironments with a high-dimensional visual state space by demonstrating that\nit is possible to learn system dynamics and the reward structure jointly. Our\ncontribution is to extend a recently developed deep neural network for video\nframe prediction in Atari games to enable reward prediction as well. To this\nend, we phrase a joint optimization problem for minimizing both video frame and\nreward reconstruction loss, and adapt network parameters accordingly. Empirical\nevaluations on five Atari games demonstrate accurate cumulative reward\nprediction of up to 200 frames. We consider these results as opening up\nimportant directions for model-based reinforcement learning in complex,\ninitially unknown environments. \n\n"}
{"id": "1611.07995", "contents": "Title: Factoring using 2n+2 qubits with Toffoli based modular multiplication Abstract: We describe an implementation of Shor's quantum algorithm to factor n-bit\nintegers using only 2n+2 qubits. In contrast to previous space-optimized\nimplementations, ours features a purely Toffoli based modular multiplication\ncircuit. The circuit depth and the overall gate count are in O(n^3) and O(n^3\nlog(n)), respectively. We thus achieve the same space and time costs as\nTakahashi et al., while using a purely classical modular multiplication\ncircuit. As a consequence, our approach evades most of the cost overheads\noriginating from rotation synthesis and enables testing and localization of\nfaults in both, the logical level circuit and an actual quantum hardware\nimplementation. Our new (in-place) constant-adder, which is used to construct\nthe modular multiplication circuit, uses only dirty ancilla qubits and features\na circuit size and depth in O(n log(n)) and O(n), respectively. \n\n"}
{"id": "1611.08331", "contents": "Title: An Overview on Data Representation Learning: From Traditional Feature\n  Learning to Recent Deep Learning Abstract: Since about 100 years ago, to learn the intrinsic structure of data, many\nrepresentation learning approaches have been proposed, including both linear\nones and nonlinear ones, supervised ones and unsupervised ones. Particularly,\ndeep architectures are widely applied for representation learning in recent\nyears, and have delivered top results in many tasks, such as image\nclassification, object detection and speech recognition. In this paper, we\nreview the development of data representation learning methods. Specifically,\nwe investigate both traditional feature learning algorithms and\nstate-of-the-art deep learning models. The history of data representation\nlearning is introduced, while available resources (e.g. online course, tutorial\nand book information) and toolboxes are provided. Finally, we conclude this\npaper with remarks and some interesting research directions on data\nrepresentation learning. \n\n"}
{"id": "1611.08648", "contents": "Title: Patient-Driven Privacy Control through Generalized Distillation Abstract: The introduction of data analytics into medicine has changed the nature of\npatient treatment. In this, patients are asked to disclose personal information\nsuch as genetic markers, lifestyle habits, and clinical history. This data is\nthen used by statistical models to predict personalized treatments. However,\ndue to privacy concerns, patients often desire to withhold sensitive\ninformation. This self-censorship can impede proper diagnosis and treatment,\nwhich may lead to serious health complications and even death over time. In\nthis paper, we present privacy distillation, a mechanism which allows patients\nto control the type and amount of information they wish to disclose to the\nhealthcare providers for use in statistical models. Meanwhile, it retains the\naccuracy of models that have access to all patient data under a sufficient but\nnot full set of privacy-relevant information. We validate privacy distillation\nusing a corpus of patients prescribed to warfarin for a personalized dosage. We\nuse a deep neural network to implement privacy distillation for training and\nmaking dose predictions. We find that privacy distillation with sufficient\nprivacy-relevant information i) retains accuracy almost as good as having all\npatient data (only 3\\% worse), and ii) is effective at preventing errors that\nintroduce health-related risks (only 3.9\\% worse under- or over-prescriptions). \n\n"}
{"id": "1611.09207", "contents": "Title: AutoMOS: Learning a non-intrusive assessor of naturalness-of-speech Abstract: Developers of text-to-speech synthesizers (TTS) often make use of human\nraters to assess the quality of synthesized speech. We demonstrate that we can\nmodel human raters' mean opinion scores (MOS) of synthesized speech using a\ndeep recurrent neural network whose inputs consist solely of a raw waveform.\nOur best models provide utterance-level estimates of MOS only moderately\ninferior to sampled human ratings, as shown by Pearson and Spearman\ncorrelations. When multiple utterances are scored and averaged, a scenario\ncommon in synthesizer quality assessment, AutoMOS achieves correlations\napproaching those of human raters. The AutoMOS model has a number of\napplications, such as the ability to explore the parameter space of a speech\nsynthesizer without requiring a human-in-the-loop. \n\n"}
{"id": "1611.09957", "contents": "Title: Low-dimensional Data Embedding via Robust Ranking Abstract: We describe a new method called t-ETE for finding a low-dimensional embedding\nof a set of objects in Euclidean space. We formulate the embedding problem as a\njoint ranking problem over a set of triplets, where each triplet captures the\nrelative similarities between three objects in the set. By exploiting recent\nadvances in robust ranking, t-ETE produces high-quality embeddings even in the\npresence of a significant amount of noise and better preserves local scale than\nknown methods, such as t-STE and t-SNE. In particular, our method produces\nsignificantly better results than t-SNE on signature datasets while also being\nfaster to compute. \n\n"}
{"id": "1612.00631", "contents": "Title: Design Automation and Design Space Exploration for Quantum Computers Abstract: A major hurdle to the deployment of quantum linear systems algorithms and\nrecent quantum simulation algorithms lies in the difficulty to find inexpensive\nreversible circuits for arithmetic using existing hand coded methods. Motivated\nby recent advances in reversible logic synthesis, we synthesize arithmetic\ncircuits using classical design automation flows and tools. The combination of\nclassical and reversible logic synthesis enables the automatic design of large\ncomponents in reversible logic starting from well-known hardware description\nlanguages such as Verilog. As a prototype example for our approach we\nautomatically generate high quality networks for the reciprocal $1/x$, which is\nnecessary for quantum linear systems algorithms. \n\n"}
{"id": "1612.01988", "contents": "Title: Local Group Invariant Representations via Orbit Embeddings Abstract: Invariance to nuisance transformations is one of the desirable properties of\neffective representations. We consider transformations that form a \\emph{group}\nand propose an approach based on kernel methods to derive local group invariant\nrepresentations. Locality is achieved by defining a suitable probability\ndistribution over the group which in turn induces distributions in the input\nfeature space. We learn a decision function over these distributions by\nappealing to the powerful framework of kernel methods and generate local\ninvariant random feature maps via kernel approximations. We show uniform\nconvergence bounds for kernel approximation and provide excess risk bounds for\nlearning with these features. We evaluate our method on three real datasets,\nincluding Rotated MNIST and CIFAR-10, and observe that it outperforms competing\nkernel based approaches. The proposed method also outperforms deep CNN on\nRotated-MNIST and performs comparably to the recently proposed\ngroup-equivariant CNN. \n\n"}
{"id": "1612.02695", "contents": "Title: Towards better decoding and language model integration in sequence to\n  sequence models Abstract: The recently proposed Sequence-to-Sequence (seq2seq) framework advocates\nreplacing complex data processing pipelines, such as an entire automatic speech\nrecognition system, with a single neural network trained in an end-to-end\nfashion. In this contribution, we analyse an attention-based seq2seq speech\nrecognition system that directly transcribes recordings into characters. We\nobserve two shortcomings: overconfidence in its predictions and a tendency to\nproduce incomplete transcriptions when language models are used. We propose\npractical solutions to both problems achieving competitive speaker independent\nword error rates on the Wall Street Journal dataset: without separate language\nmodels we reach 10.6% WER, while together with a trigram language model, we\nreach 6.7% WER. \n\n"}
{"id": "1612.03350", "contents": "Title: Non-negative Factorization of the Occurrence Tensor from Financial\n  Contracts Abstract: We propose an algorithm for the non-negative factorization of an occurrence\ntensor built from heterogeneous networks. We use l0 norm to model sparse errors\nover discrete values (occurrences), and use decomposed factors to model the\nembedded groups of nodes. An efficient splitting method is developed to\noptimize the nonconvex and nonsmooth objective. We study both synthetic\nproblems and a new dataset built from financial documents, resMBS. \n\n"}
{"id": "1612.04022", "contents": "Title: Distributed Multi-Task Relationship Learning Abstract: Multi-task learning aims to learn multiple tasks jointly by exploiting their\nrelatedness to improve the generalization performance for each task.\nTraditionally, to perform multi-task learning, one needs to centralize data\nfrom all the tasks to a single machine. However, in many real-world\napplications, data of different tasks may be geo-distributed over different\nlocal machines. Due to heavy communication caused by transmitting the data and\nthe issue of data privacy and security, it is impossible to send data of\ndifferent task to a master machine to perform multi-task learning. Therefore,\nin this paper, we propose a distributed multi-task learning framework that\nsimultaneously learns predictive models for each task as well as task\nrelationships between tasks alternatingly in the parameter server paradigm. In\nour framework, we first offer a general dual form for a family of regularized\nmulti-task relationship learning methods. Subsequently, we propose a\ncommunication-efficient primal-dual distributed optimization algorithm to solve\nthe dual problem by carefully designing local subproblems to make the dual\nproblem decomposable. Moreover, we provide a theoretical convergence analysis\nfor the proposed algorithm, which is specific for distributed multi-task\nrelationship learning. We conduct extensive experiments on both synthetic and\nreal-world datasets to evaluate our proposed framework in terms of\neffectiveness and convergence. \n\n"}
{"id": "1612.05628", "contents": "Title: An Alternative Softmax Operator for Reinforcement Learning Abstract: A softmax operator applied to a set of values acts somewhat like the\nmaximization function and somewhat like an average. In sequential decision\nmaking, softmax is often used in settings where it is necessary to maximize\nutility but also to hedge against problems that arise from putting all of one's\nweight behind a single maximum utility decision. The Boltzmann softmax operator\nis the most commonly used softmax operator in this setting, but we show that\nthis operator is prone to misbehavior. In this work, we study a differentiable\nsoftmax operator that, among other properties, is a non-expansion ensuring a\nconvergent behavior in learning and planning. We introduce a variant of SARSA\nalgorithm that, by utilizing the new operator, computes a Boltzmann policy with\na state-dependent temperature parameter. We show that the algorithm is\nconvergent and that it performs favorably in practice. \n\n"}
{"id": "1612.07640", "contents": "Title: Deep Learning and Its Applications to Machine Health Monitoring: A\n  Survey Abstract: Since 2006, deep learning (DL) has become a rapidly growing research\ndirection, redefining state-of-the-art performances in a wide range of areas\nsuch as object recognition, image segmentation, speech recognition and machine\ntranslation. In modern manufacturing systems, data-driven machine health\nmonitoring is gaining in popularity due to the widespread deployment of\nlow-cost sensors and their connection to the Internet. Meanwhile, deep learning\nprovides useful tools for processing and analyzing these big machinery data.\nThe main purpose of this paper is to review and summarize the emerging research\nwork of deep learning on machine health monitoring. After the brief\nintroduction of deep learning techniques, the applications of deep learning in\nmachine health monitoring systems are reviewed mainly from the following\naspects: Auto-encoder (AE) and its variants, Restricted Boltzmann Machines and\nits variants including Deep Belief Network (DBN) and Deep Boltzmann Machines\n(DBM), Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN).\nFinally, some new trends of DL-based machine health monitoring methods are\ndiscussed. \n\n"}
{"id": "1612.07843", "contents": "Title: \"What is Relevant in a Text Document?\": An Interpretable Machine\n  Learning Approach Abstract: Text documents can be described by a number of abstract concepts such as\nsemantic category, writing style, or sentiment. Machine learning (ML) models\nhave been trained to automatically map documents to these abstract concepts,\nallowing to annotate very large text collections, more than could be processed\nby a human in a lifetime. Besides predicting the text's category very\naccurately, it is also highly desirable to understand how and why the\ncategorization process takes place. In this paper, we demonstrate that such\nunderstanding can be achieved by tracing the classification decision back to\nindividual words using layer-wise relevance propagation (LRP), a recently\ndeveloped technique for explaining predictions of complex non-linear\nclassifiers. We train two word-based ML models, a convolutional neural network\n(CNN) and a bag-of-words SVM classifier, on a topic categorization task and\nadapt the LRP method to decompose the predictions of these models onto words.\nResulting scores indicate how much individual words contribute to the overall\nclassification decision. This enables one to distill relevant information from\ntext documents without an explicit semantic information extraction step. We\nfurther use the word-wise relevance scores for generating novel vector-based\ndocument representations which capture semantic information. Based on these\ndocument vectors, we introduce a measure of model explanatory power and show\nthat, although the SVM and CNN models perform similarly in terms of\nclassification accuracy, the latter exhibits a higher level of explainability\nwhich makes it more comprehensible for humans and potentially more useful for\nother applications. \n\n"}
{"id": "1612.08044", "contents": "Title: Error tracing in linear and concatenated quantum circuits Abstract: Descriptions of quantum algorithms, communication etc. protocols assume the\nexistence of closed quantum system. However, real life quantum systems are open\nand are highly sensitive to errors. Hence error correction is of utmost\nimportance if quantum computation is to be carried out in reality. Ideally, an\nerror correction block should be placed after every gate operation in a quantum\ncircuit. This increases the overhead and reduced the speedup of the quantum\ncircuit. Moreover, the error correction blocks themselves may induce errors as\nthe gates used for error correction may be noisy. In this paper, we have\nproposed a procedure to trace error probability due to noisy gates and\ndecoherence in quantum circuit and place an error correcting block only when\nthe error probability exceeds a certain threshold. This procedure shows a\ndrastic reduction in the required number of error correcting blocks.\nFurthermore, we have considered concatenated codes with tile structure layout\nlattice architecture[25][21],[24] and SWAP gate based qubit transport\nmechanism. Tracing errors in higher levels of concatenation shows that, in most\ncases, after 1 or 2 levels of concatenation, the number of QECC blocks required\nbecome static. However, since the gate count increases with increasing\nconcatenation, the percentage saving in gate count is considerably high. \n\n"}
{"id": "1612.08091", "contents": "Title: ProjectQ: An Open Source Software Framework for Quantum Computing Abstract: We introduce ProjectQ, an open source software effort for quantum computing.\nThe first release features a compiler framework capable of targeting various\ntypes of hardware, a high-performance simulator with emulation capabilities,\nand compiler plug-ins for circuit drawing and resource estimation. We introduce\nour Python-embedded domain-specific language, present the features, and provide\nexample implementations for quantum algorithms. The framework allows testing of\nquantum algorithms through simulation and enables running them on actual\nquantum hardware using a back-end connecting to the IBM Quantum Experience\ncloud service. Through extension mechanisms, users can provide back-ends to\nfurther quantum hardware, and scientists working on quantum compilation can\nprovide plug-ins for additional compilation, optimization, gate synthesis, and\nlayout strategies. \n\n"}
{"id": "1701.00140", "contents": "Title: A Finite Presentation of CNOT-Dihedral Operators Abstract: We give a finite presentation by generators and relations of the unitary\noperators expressible over the {CNOT, T, X} gate set, also known as\nCNOT-dihedral operators. To this end, we introduce a notion of normal form for\nCNOT-dihedral circuits and prove that every CNOT-dihedral operator admits a\nunique normal form. Moreover, we show that in the presence of certain\nstructural rules only finitely many circuit identities are required to reduce\nan arbitrary CNOT-dihedral circuit to its normal form.\n  By appropriately restricting our relations, we obtain a finite presentation\nof unitary operators expressible over the {CNOT, T} gate set as a corollary. \n\n"}
{"id": "1701.00874", "contents": "Title: Neural Probabilistic Model for Non-projective MST Parsing Abstract: In this paper, we propose a probabilistic parsing model, which defines a\nproper conditional probability distribution over non-projective dependency\ntrees for a given sentence, using neural representations as inputs. The neural\nnetwork architecture is based on bi-directional LSTM-CNNs which benefits from\nboth word- and character-level representations automatically, by using\ncombination of bidirectional LSTM and CNN. On top of the neural network, we\nintroduce a probabilistic structured layer, defining a conditional log-linear\nmodel over non-projective trees. We evaluate our model on 17 different\ndatasets, across 14 different languages. By exploiting Kirchhoff's Matrix-Tree\nTheorem (Tutte, 1984), the partition functions and marginals can be computed\nefficiently, leading to a straight-forward end-to-end model training procedure\nvia back-propagation. Our parser achieves state-of-the-art parsing performance\non nine datasets. \n\n"}
{"id": "1701.01325", "contents": "Title: Outlier Detection for Text Data : An Extended Version Abstract: The problem of outlier detection is extremely challenging in many domains\nsuch as text, in which the attribute values are typically non-negative, and\nmost values are zero. In such cases, it often becomes difficult to separate the\noutliers from the natural variations in the patterns in the underlying data. In\nthis paper, we present a matrix factorization method, which is naturally able\nto distinguish the anomalies with the use of low rank approximations of the\nunderlying data. Our iterative algorithm TONMF is based on block coordinate\ndescent (BCD) framework. We define blocks over the term-document matrix such\nthat the function becomes solvable. Given most recently updated values of other\nmatrix blocks, we always update one block at a time to its optimal. Our\napproach has significant advantages over traditional methods for text outlier\ndetection. Finally, we present experimental results illustrating the\neffectiveness of our method over competing methods. \n\n"}
{"id": "1701.03655", "contents": "Title: Dictionary Learning from Incomplete Data Abstract: This paper extends the recently proposed and theoretically justified\niterative thresholding and $K$ residual means algorithm ITKrM to learning\ndicionaries from incomplete/masked training data (ITKrMM). It further adapts\nthe algorithm to the presence of a low rank component in the data and provides\na strategy for recovering this low rank component again from incomplete data.\nSeveral synthetic experiments show the advantages of incorporating information\nabout the corruption into the algorithm. Finally, image inpainting is\nconsidered as application example, which demonstrates the superior performance\nof ITKrMM in terms of speed at similar or better reconstruction quality\ncompared to its closest dictionary learning counterpart. \n\n"}
{"id": "1701.06279", "contents": "Title: dna2vec: Consistent vector representations of variable-length k-mers Abstract: One of the ubiquitous representation of long DNA sequence is dividing it into\nshorter k-mer components. Unfortunately, the straightforward vector encoding of\nk-mer as a one-hot vector is vulnerable to the curse of dimensionality. Worse\nyet, the distance between any pair of one-hot vectors is equidistant. This is\nparticularly problematic when applying the latest machine learning algorithms\nto solve problems in biological sequence analysis. In this paper, we propose a\nnovel method to train distributed representations of variable-length k-mers.\nOur method is based on the popular word embedding model word2vec, which is\ntrained on a shallow two-layer neural network. Our experiments provide evidence\nthat the summing of dna2vec vectors is akin to nucleotides concatenation. We\nalso demonstrate that there is correlation between Needleman-Wunsch similarity\nscore and cosine similarity of dna2vec vectors. \n\n"}
{"id": "1701.08978", "contents": "Title: Mixed Low-precision Deep Learning Inference using Dynamic Fixed Point Abstract: We propose a cluster-based quantization method to convert pre-trained full\nprecision weights into ternary weights with minimal impact on the accuracy. In\naddition, we also constrain the activations to 8-bits thus enabling sub 8-bit\nfull integer inference pipeline. Our method uses smaller clusters of N filters\nwith a common scaling factor to minimize the quantization loss, while also\nmaximizing the number of ternary operations. We show that with a cluster size\nof N=4 on Resnet-101, can achieve 71.8% TOP-1 accuracy, within 6% of the best\nfull precision results while replacing ~85% of all multiplications with 8-bit\naccumulations. Using the same method with 4-bit weights achieves 76.3% TOP-1\naccuracy which within 2% of the full precision result. We also study the impact\nof the size of the cluster on both performance and accuracy, larger cluster\nsizes N=64 can replace ~98% of the multiplications with ternary operations but\nintroduces significant drop in accuracy which necessitates fine tuning the\nparameters with retraining the network at lower precision. To address this we\nhave also trained low-precision Resnet-50 with 8-bit activations and ternary\nweights by pre-initializing the network with full precision weights and achieve\n68.9% TOP-1 accuracy within 4 additional epochs. Our final quantized model can\nrun on a full 8-bit compute pipeline, with a potential 16x improvement in\nperformance compared to baseline full-precision models. \n\n"}
{"id": "1702.00403", "contents": "Title: Generative Adversarial Networks recover features in astrophysical images\n  of galaxies beyond the deconvolution limit Abstract: Observations of astrophysical objects such as galaxies are limited by various\nsources of random and systematic noise from the sky background, the optical\nsystem of the telescope and the detector used to record the data. Conventional\ndeconvolution techniques are limited in their ability to recover features in\nimaging data by the Shannon-Nyquist sampling theorem. Here we train a\ngenerative adversarial network (GAN) on a sample of $4,550$ images of nearby\ngalaxies at $0.01<z<0.02$ from the Sloan Digital Sky Survey and conduct\n$10\\times$ cross validation to evaluate the results. We present a method using\na GAN trained on galaxy images that can recover features from artificially\ndegraded images with worse seeing and higher noise than the original with a\nperformance which far exceeds simple deconvolution. The ability to better\nrecover detailed features such as galaxy morphology from low-signal-to-noise\nand low angular resolution imaging data significantly increases our ability to\nstudy existing data sets of astrophysical objects as well as future\nobservations with observatories such as the Large Synoptic Sky Telescope (LSST)\nand the Hubble and James Webb space telescopes. \n\n"}
{"id": "1702.01229", "contents": "Title: Simple to Complex Cross-modal Learning to Rank Abstract: The heterogeneity-gap between different modalities brings a significant\nchallenge to multimedia information retrieval. Some studies formalize the\ncross-modal retrieval tasks as a ranking problem and learn a shared multi-modal\nembedding space to measure the cross-modality similarity. However, previous\nmethods often establish the shared embedding space based on linear mapping\nfunctions which might not be sophisticated enough to reveal more complicated\ninter-modal correspondences. Additionally, current studies assume that the\nrankings are of equal importance, and thus all rankings are used\nsimultaneously, or a small number of rankings are selected randomly to train\nthe embedding space at each iteration. Such strategies, however, always suffer\nfrom outliers as well as reduced generalization capability due to their lack of\ninsightful understanding of procedure of human cognition. In this paper, we\ninvolve the self-paced learning theory with diversity into the cross-modal\nlearning to rank and learn an optimal multi-modal embedding space based on\nnon-linear mapping functions. This strategy enhances the model's robustness to\noutliers and achieves better generalization via training the model gradually\nfrom easy rankings by diverse queries to more complex ones. An efficient\nalternative algorithm is exploited to solve the proposed challenging problem\nwith fast convergence in practice. Extensive experimental results on several\nbenchmark datasets indicate that the proposed method achieves significant\nimprovements over the state-of-the-arts in this literature. \n\n"}
{"id": "1702.01852", "contents": "Title: Experimental Comparison of Two Quantum Computing Architectures Abstract: We run a selection of algorithms on two state-of-the-art 5-qubit quantum\ncomputers that are based on different technology platforms. One is a publicly\naccessible superconducting transmon device with limited connectivity, and the\nother is a fully connected trapped-ion system. Even though the two systems have\ndifferent native quantum interactions, both can be programmed in a way that is\nblind to the underlying hardware, thus allowing the first comparison of\nidentical quantum algorithms between different physical systems. We show that\nquantum algorithms and circuits that employ more connectivity clearly benefit\nfrom a better connected system of qubits. While the quantum systems here are\nnot yet large enough to eclipse classical computers, this experiment exposes\ncritical factors of scaling quantum computers, such as qubit connectivity and\ngate expressivity. In addition, the results suggest that co-designing\nparticular quantum applications with the hardware itself will be paramount in\nsuccessfully using quantum computers in the future. \n\n"}
{"id": "1702.04459", "contents": "Title: Robust Stochastic Configuration Networks with Kernel Density Estimation Abstract: Neural networks have been widely used as predictive models to fit data\ndistribution, and they could be implemented through learning a collection of\nsamples. In many applications, however, the given dataset may contain noisy\nsamples or outliers which may result in a poor learner model in terms of\ngeneralization. This paper contributes to a development of robust stochastic\nconfiguration networks (RSCNs) for resolving uncertain data regression\nproblems. RSCNs are built on original stochastic configuration networks with\nweighted least squares method for evaluating the output weights, and the input\nweights and biases are incrementally and randomly generated by satisfying with\na set of inequality constrains. The kernel density estimation (KDE) method is\nemployed to set the penalty weights for each training samples, so that some\nnegative impacts, caused by noisy data or outliers, on the resulting learner\nmodel can be reduced. The alternating optimization technique is applied for\nupdating a RSCN model with improved penalty weights computed from the kernel\ndensity estimation function. Performance evaluation is carried out by a\nfunction approximation, four benchmark datasets and a case study on engineering\napplication. Comparisons to other robust randomised neural modelling\ntechniques, including the probabilistic robust learning algorithm for neural\nnetworks with random weights and improved RVFL networks, indicate that the\nproposed RSCNs with KDE perform favourably and demonstrate good potential for\nreal-world applications. \n\n"}
{"id": "1702.06280", "contents": "Title: On the (Statistical) Detection of Adversarial Examples Abstract: Machine Learning (ML) models are applied in a variety of tasks such as\nnetwork intrusion detection or Malware classification. Yet, these models are\nvulnerable to a class of malicious inputs known as adversarial examples. These\nare slightly perturbed inputs that are classified incorrectly by the ML model.\nThe mitigation of these adversarial inputs remains an open problem. As a step\ntowards understanding adversarial examples, we show that they are not drawn\nfrom the same distribution than the original data, and can thus be detected\nusing statistical tests. Using thus knowledge, we introduce a complimentary\napproach to identify specific inputs that are adversarial. Specifically, we\naugment our ML model with an additional output, in which the model is trained\nto classify all adversarial inputs. We evaluate our approach on multiple\nadversarial example crafting methods (including the fast gradient sign and\nsaliency map methods) with several datasets. The statistical test flags sample\nsets containing adversarial inputs confidently at sample sizes between 10 and\n100 data points. Furthermore, our augmented model either detects adversarial\nexamples as outliers with high accuracy (> 80%) or increases the adversary's\ncost - the perturbation added - by more than 150%. In this way, we show that\nstatistical properties of adversarial examples are essential to their\ndetection. \n\n"}
{"id": "1702.07464", "contents": "Title: Deep Models Under the GAN: Information Leakage from Collaborative Deep\n  Learning Abstract: Deep Learning has recently become hugely popular in machine learning,\nproviding significant improvements in classification accuracy in the presence\nof highly-structured and large databases.\n  Researchers have also considered privacy implications of deep learning.\nModels are typically trained in a centralized manner with all the data being\nprocessed by the same training algorithm. If the data is a collection of users'\nprivate data, including habits, personal pictures, geographical positions,\ninterests, and more, the centralized server will have access to sensitive\ninformation that could potentially be mishandled. To tackle this problem,\ncollaborative deep learning models have recently been proposed where parties\nlocally train their deep learning structures and only share a subset of the\nparameters in the attempt to keep their respective training sets private.\nParameters can also be obfuscated via differential privacy (DP) to make\ninformation extraction even more challenging, as proposed by Shokri and\nShmatikov at CCS'15.\n  Unfortunately, we show that any privacy-preserving collaborative deep\nlearning is susceptible to a powerful attack that we devise in this paper. In\nparticular, we show that a distributed, federated, or decentralized deep\nlearning approach is fundamentally broken and does not protect the training\nsets of honest participants. The attack we developed exploits the real-time\nnature of the learning process that allows the adversary to train a Generative\nAdversarial Network (GAN) that generates prototypical samples of the targeted\ntraining set that was meant to be private (the samples generated by the GAN are\nintended to come from the same distribution as the training data).\nInterestingly, we show that record-level DP applied to the shared parameters of\nthe model, as suggested in previous work, is ineffective (i.e., record-level DP\nis not designed to address our attack). \n\n"}
{"id": "1702.07652", "contents": "Title: Control of Gene Regulatory Networks with Noisy Measurements and\n  Uncertain Inputs Abstract: This paper is concerned with the problem of stochastic control of gene\nregulatory networks (GRNs) observed indirectly through noisy measurements and\nwith uncertainty in the intervention inputs. The partial observability of the\ngene states and uncertainty in the intervention process are accounted for by\nmodeling GRNs using the partially-observed Boolean dynamical system (POBDS)\nsignal model with noisy gene expression measurements. Obtaining the optimal\ninfinite-horizon control strategy for this problem is not attainable in\ngeneral, and we apply reinforcement learning and Gaussian process techniques to\nfind a near-optimal solution. The POBDS is first transformed to a\ndirectly-observed Markov Decision Process in a continuous belief space, and the\nGaussian process is used for modeling the cost function over the belief and\nintervention spaces. Reinforcement learning then is used to learn the cost\nfunction from the available gene expression data. In addition, we employ\nsparsification, which enables the control of large partially-observed GRNs. The\nperformance of the resulting algorithm is studied through a comprehensive set\nof numerical experiments using synthetic gene expression data generated from a\nmelanoma gene regulatory network. \n\n"}
{"id": "1702.07834", "contents": "Title: Efficient coordinate-wise leading eigenvector computation Abstract: We develop and analyze efficient \"coordinate-wise\" methods for finding the\nleading eigenvector, where each step involves only a vector-vector product. We\nestablish global convergence with overall runtime guarantees that are at least\nas good as Lanczos's method and dominate it for slowly decaying spectrum. Our\nmethods are based on combining a shift-and-invert approach with coordinate-wise\nalgorithms for linear regression. \n\n"}
{"id": "1702.07958", "contents": "Title: Efficient Online Bandit Multiclass Learning with $\\tilde{O}(\\sqrt{T})$\n  Regret Abstract: We present an efficient second-order algorithm with\n$\\tilde{O}(\\frac{1}{\\eta}\\sqrt{T})$ regret for the bandit online multiclass\nproblem. The regret bound holds simultaneously with respect to a family of loss\nfunctions parameterized by $\\eta$, for a range of $\\eta$ restricted by the norm\nof the competitor. The family of loss functions ranges from hinge loss\n($\\eta=0$) to squared hinge loss ($\\eta=1$). This provides a solution to the\nopen problem of (J. Abernethy and A. Rakhlin. An efficient bandit algorithm for\n$\\sqrt{T}$-regret in online multiclass prediction? In COLT, 2009). We test our\nalgorithm experimentally, showing that it also performs favorably against\nearlier algorithms. \n\n"}
{"id": "1702.08658", "contents": "Title: Towards Deeper Understanding of Variational Autoencoding Models Abstract: We propose a new family of optimization criteria for variational\nauto-encoding models, generalizing the standard evidence lower bound. We\nprovide conditions under which they recover the data distribution and learn\nlatent features, and formally show that common issues such as blurry samples\nand uninformative latent features arise when these conditions are not met.\nBased on these new insights, we propose a new sequential VAE model that can\ngenerate sharp samples on the LSUN image dataset based on pixel-wise\nreconstruction loss, and propose an optimization criterion that encourages\nunsupervised learning of informative latent features. \n\n"}
{"id": "1702.08715", "contents": "Title: Building a Completely Reversible Computer Abstract: A critical analysis of the feasibility of reversible computing is performed.\nThe key question is: Is it possible to build a completely reversible computer?\nA closer look into the internal aspects of the reversible computing as well as\nthe external constraints such as the second law of thermodynamics has\ndemonstrated that several difficulties would have to be solved before\nreversible computer is being built. It is shown that a conventional reversible\ncomputer would require energy for setting up the reversible inputs from\nirreversible signals, for the reading out of the reversible outputs, for the\ntransport of the information between logic elements and finally for the control\nsignals that will require more energy dissipating into the environment. A loose\nbound on the minimum amount of energy required to be dissipated during the\nphysical implementation of a reversible computer is obtained and a\ngeneralization of the principles for reversible computing is provided. \n\n"}
{"id": "1702.08835", "contents": "Title: Deep Forest Abstract: Current deep learning models are mostly build upon neural networks, i.e.,\nmultiple layers of parameterized differentiable nonlinear modules that can be\ntrained by backpropagation. In this paper, we explore the possibility of\nbuilding deep models based on non-differentiable modules. We conjecture that\nthe mystery behind the success of deep neural networks owes much to three\ncharacteristics, i.e., layer-by-layer processing, in-model feature\ntransformation and sufficient model complexity. We propose the gcForest\napproach, which generates \\textit{deep forest} holding these characteristics.\nThis is a decision tree ensemble approach, with much less hyper-parameters than\ndeep neural networks, and its model complexity can be automatically determined\nin a data-dependent way. Experiments show that its performance is quite robust\nto hyper-parameter settings, such that in most cases, even across different\ndata from different domains, it is able to get excellent performance by using\nthe same default setting. This study opens the door of deep learning based on\nnon-differentiable modules, and exhibits the possibility of constructing deep\nmodels without using backpropagation. \n\n"}
{"id": "1703.00593", "contents": "Title: Positive-Unlabeled Learning with Non-Negative Risk Estimator Abstract: From only positive (P) and unlabeled (U) data, a binary classifier could be\ntrained with PU learning, in which the state of the art is unbiased PU\nlearning. However, if its model is very flexible, empirical risks on training\ndata will go negative, and we will suffer from serious overfitting. In this\npaper, we propose a non-negative risk estimator for PU learning: when getting\nminimized, it is more robust against overfitting, and thus we are able to use\nvery flexible models (such as deep neural networks) given limited P data.\nMoreover, we analyze the bias, consistency, and mean-squared-error reduction of\nthe proposed risk estimator, and bound the estimation error of the resulting\nempirical risk minimizer. Experiments demonstrate that our risk estimator fixes\nthe overfitting problem of its unbiased counterparts. \n\n"}
{"id": "1703.00676", "contents": "Title: A Unifying View of Explicit and Implicit Feature Maps of Graph Kernels Abstract: Non-linear kernel methods can be approximated by fast linear ones using\nsuitable explicit feature maps allowing their application to large scale\nproblems. We investigate how convolution kernels for structured data are\ncomposed from base kernels and construct corresponding feature maps. On this\nbasis we propose exact and approximative feature maps for widely used graph\nkernels based on the kernel trick. We analyze for which kernels and graph\nproperties computation by explicit feature maps is feasible and actually more\nefficient. In particular, we derive approximative, explicit feature maps for\nstate-of-the-art kernels supporting real-valued attributes including the\nGraphHopper and graph invariant kernels. In extensive experiments we show that\nour approaches often achieve a classification accuracy close to the exact\nmethods based on the kernel trick, but require only a fraction of their running\ntime. Moreover, we propose and analyze algorithms for computing random walk,\nshortest-path and subgraph matching kernels by explicit and implicit feature\nmaps. Our theoretical results are confirmed experimentally by observing a phase\ntransition when comparing running time with respect to label diversity, walk\nlengths and subgraph size, respectively. \n\n"}
{"id": "1703.00874", "contents": "Title: Optimized Aaronson-Gottesman stabilizer circuit simulation through\n  quantum circuit transformations Abstract: In this paper we improve the layered implementation of arbitrary stabilizer\ncircuits introduced by Aaronson and Gottesman in {\\it Phys. Rev. A 70(052328)},\n2004. In particular, we reduce their 11-stage computation\n-H-C-P-C-P-C-H-P-C-P-C- into an 8-stage computation of the form\n-H-C-CZ-P-H-P-CZ-C-. We show arguments in support of using -CZ- stages over the\n-C- stages: not only the use of -CZ- stages allows a shorter layered\nexpression, but -CZ- stages are simpler and appear to be easier to implement\ncompared to the -C- stages. Relying on the 8-stage decomposition we develop a\ntwo-qubit depth-$(14n-4)$ implementation of stabilizer circuits over the gate\nlibrary {P,H,CNOT}, executable in the LNN architecture, improving best\npreviously known depth-$25n$ circuit, also executable in the LNN architecture.\nOur constructions rely on folding arbitrarily long sequences $($-P-C-$)^m$ into\na 3-stage computation -P-CZ-C-, as well as efficient implementation of the -CZ-\nstage circuits. \n\n"}
{"id": "1703.01014", "contents": "Title: Active Learning for Cost-Sensitive Classification Abstract: We design an active learning algorithm for cost-sensitive multiclass\nclassification: problems where different errors have different costs. Our\nalgorithm, COAL, makes predictions by regressing to each label's cost and\npredicting the smallest. On a new example, it uses a set of regressors that\nperform well on past data to estimate possible costs for each label. It queries\nonly the labels that could be the best, ignoring the sure losers. We prove COAL\ncan be efficiently implemented for any regression family that admits squared\nloss optimization; it also enjoys strong guarantees with respect to predictive\nperformance and labeling effort. We empirically compare COAL to passive\nlearning and several active learning baselines, showing significant\nimprovements in labeling effort and test cost on real-world datasets. \n\n"}
{"id": "1703.03073", "contents": "Title: Deep Convolutional Neural Network Inference with Floating-point Weights\n  and Fixed-point Activations Abstract: Deep convolutional neural network (CNN) inference requires significant amount\nof memory and computation, which limits its deployment on embedded devices. To\nalleviate these problems to some extent, prior research utilize low precision\nfixed-point numbers to represent the CNN weights and activations. However, the\nminimum required data precision of fixed-point weights varies across different\nnetworks and also across different layers of the same network. In this work, we\npropose using floating-point numbers for representing the weights and\nfixed-point numbers for representing the activations. We show that using\nfloating-point representation for weights is more efficient than fixed-point\nrepresentation for the same bit-width and demonstrate it on popular large-scale\nCNNs such as AlexNet, SqueezeNet, GoogLeNet and VGG-16. We also show that such\na representation scheme enables compact hardware multiply-and-accumulate (MAC)\nunit design. Experimental results show that the proposed scheme reduces the\nweight storage by up to 36% and power consumption of the hardware multiplier by\nup to 50%. \n\n"}
{"id": "1703.03400", "contents": "Title: Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks Abstract: We propose an algorithm for meta-learning that is model-agnostic, in the\nsense that it is compatible with any model trained with gradient descent and\napplicable to a variety of different learning problems, including\nclassification, regression, and reinforcement learning. The goal of\nmeta-learning is to train a model on a variety of learning tasks, such that it\ncan solve new learning tasks using only a small number of training samples. In\nour approach, the parameters of the model are explicitly trained such that a\nsmall number of gradient steps with a small amount of training data from a new\ntask will produce good generalization performance on that task. In effect, our\nmethod trains the model to be easy to fine-tune. We demonstrate that this\napproach leads to state-of-the-art performance on two few-shot image\nclassification benchmarks, produces good results on few-shot regression, and\naccelerates fine-tuning for policy gradient reinforcement learning with neural\nnetwork policies. \n\n"}
{"id": "1703.03862", "contents": "Title: Joint Embedding of Graphs Abstract: Feature extraction and dimension reduction for networks is critical in a wide\nvariety of domains. Efficiently and accurately learning features for multiple\ngraphs has important applications in statistical inference on graphs. We\npropose a method to jointly embed multiple undirected graphs. Given a set of\ngraphs, the joint embedding method identifies a linear subspace spanned by rank\none symmetric matrices and projects adjacency matrices of graphs into this\nsubspace. The projection coefficients can be treated as features of the graphs,\nwhile the embedding components can represent vertex features. We also propose a\nrandom graph model for multiple graphs that generalizes other classical models\nfor graphs. We show through theory and numerical experiments that under the\nmodel, the joint embedding method produces estimates of parameters with small\nerrors. Via simulation experiments, we demonstrate that the joint embedding\nmethod produces features which lead to state of the art performance in\nclassifying graphs. Applying the joint embedding method to human brain graphs,\nwe find it extracts interpretable features with good prediction accuracy in\ndifferent tasks. \n\n"}
{"id": "1703.04782", "contents": "Title: Online Learning Rate Adaptation with Hypergradient Descent Abstract: We introduce a general method for improving the convergence rate of\ngradient-based optimizers that is easy to implement and works well in practice.\nWe demonstrate the effectiveness of the method in a range of optimization\nproblems by applying it to stochastic gradient descent, stochastic gradient\ndescent with Nesterov momentum, and Adam, showing that it significantly reduces\nthe need for the manual tuning of the initial learning rate for these commonly\nused algorithms. Our method works by dynamically updating the learning rate\nduring optimization using the gradient with respect to the learning rate of the\nupdate rule itself. Computing this \"hypergradient\" needs little additional\ncomputation, requires only one extra copy of the original gradient to be stored\nin memory, and relies upon nothing more than what is provided by reverse-mode\nautomatic differentiation. \n\n"}
{"id": "1703.05175", "contents": "Title: Prototypical Networks for Few-shot Learning Abstract: We propose prototypical networks for the problem of few-shot classification,\nwhere a classifier must generalize to new classes not seen in the training set,\ngiven only a small number of examples of each new class. Prototypical networks\nlearn a metric space in which classification can be performed by computing\ndistances to prototype representations of each class. Compared to recent\napproaches for few-shot learning, they reflect a simpler inductive bias that is\nbeneficial in this limited-data regime, and achieve excellent results. We\nprovide an analysis showing that some simple design decisions can yield\nsubstantial improvements over recent approaches involving complicated\narchitectural choices and meta-learning. We further extend prototypical\nnetworks to zero-shot learning and achieve state-of-the-art results on the\nCU-Birds dataset. \n\n"}
{"id": "1703.08383", "contents": "Title: Smart Augmentation - Learning an Optimal Data Augmentation Strategy Abstract: A recurring problem faced when training neural networks is that there is\ntypically not enough data to maximize the generalization capability of deep\nneural networks(DNN). There are many techniques to address this, including data\naugmentation, dropout, and transfer learning. In this paper, we introduce an\nadditional method which we call Smart Augmentation and we show how to use it to\nincrease the accuracy and reduce overfitting on a target network. Smart\nAugmentation works by creating a network that learns how to generate augmented\ndata during the training process of a target network in a way that reduces that\nnetworks loss. This allows us to learn augmentations that minimize the error of\nthat network.\n  Smart Augmentation has shown the potential to increase accuracy by\ndemonstrably significant measures on all datasets tested. In addition, it has\nshown potential to achieve similar or improved performance levels with\nsignificantly smaller network sizes in a number of tested cases. \n\n"}
{"id": "1703.09039", "contents": "Title: Efficient Processing of Deep Neural Networks: A Tutorial and Survey Abstract: Deep neural networks (DNNs) are currently widely used for many artificial\nintelligence (AI) applications including computer vision, speech recognition,\nand robotics. While DNNs deliver state-of-the-art accuracy on many AI tasks, it\ncomes at the cost of high computational complexity. Accordingly, techniques\nthat enable efficient processing of DNNs to improve energy efficiency and\nthroughput without sacrificing application accuracy or increasing hardware cost\nare critical to the wide deployment of DNNs in AI systems.\n  This article aims to provide a comprehensive tutorial and survey about the\nrecent advances towards the goal of enabling efficient processing of DNNs.\nSpecifically, it will provide an overview of DNNs, discuss various hardware\nplatforms and architectures that support DNNs, and highlight key trends in\nreducing the computation cost of DNNs either solely via hardware design changes\nor via joint hardware design and DNN algorithm changes. It will also summarize\nvarious development resources that enable researchers and practitioners to\nquickly get started in this field, and highlight important benchmarking metrics\nand design considerations that should be used for evaluating the rapidly\ngrowing number of DNN hardware designs, optionally including algorithmic\nco-designs, being proposed in academia and industry.\n  The reader will take away the following concepts from this article:\nunderstand the key design considerations for DNNs; be able to evaluate\ndifferent DNN hardware implementations with benchmarks and comparison metrics;\nunderstand the trade-offs between various hardware architectures and platforms;\nbe able to evaluate the utility of various DNN design techniques for efficient\nprocessing; and understand recent implementation trends and opportunities. \n\n"}
{"id": "1703.10663", "contents": "Title: Near Perfect Protein Multi-Label Classification with Deep Neural\n  Networks Abstract: Artificial neural networks (ANNs) have gained a well-deserved popularity\namong machine learning tools upon their recent successful applications in\nimage- and sound processing and classification problems. ANNs have also been\napplied for predicting the family or function of a protein, knowing its residue\nsequence. Here we present two new ANNs with multi-label classification ability,\nshowing impressive accuracy when classifying protein sequences into 698 UniProt\nfamilies (AUC=99.99%) and 983 Gene Ontology classes (AUC=99.45%). \n\n"}
{"id": "1703.10717", "contents": "Title: BEGAN: Boundary Equilibrium Generative Adversarial Networks Abstract: We propose a new equilibrium enforcing method paired with a loss derived from\nthe Wasserstein distance for training auto-encoder based Generative Adversarial\nNetworks. This method balances the generator and discriminator during training.\nAdditionally, it provides a new approximate convergence measure, fast and\nstable training and high visual quality. We also derive a way of controlling\nthe trade-off between image diversity and visual quality. We focus on the image\ngeneration task, setting a new milestone in visual quality, even at higher\nresolutions. This is achieved while using a relatively simple model\narchitecture and a standard training procedure. \n\n"}
{"id": "1703.10951", "contents": "Title: Comparison of multi-task convolutional neural network (MT-CNN) and a few\n  other methods for toxicity prediction Abstract: Toxicity analysis and prediction are of paramount importance to human health\nand environmental protection. Existing computational methods are built from a\nwide variety of descriptors and regressors, which makes their performance\nanalysis difficult. For example, deep neural network (DNN), a successful\napproach in many occasions, acts like a black box and offers little conceptual\nelegance or physical understanding. The present work constructs a common set of\nmicroscopic descriptors based on established physical models for charges,\nsurface areas and free energies to assess the performance of multi-task\nconvolutional neural network (MT-CNN) architectures and a few other approaches,\nincluding random forest (RF) and gradient boosting decision tree (GBDT), on an\nequal footing. Comparison is also given to convolutional neural network (CNN)\nand non-convolutional deep neural network (DNN) algorithms. Four benchmark\ntoxicity data sets (i.e., endpoints) are used to evaluate various approaches.\nExtensive numerical studies indicate that the present MT-CNN architecture is\nable to outperform the state-of-the-art methods. \n\n"}
{"id": "1704.00003", "contents": "Title: Spectral Methods for Nonparametric Models Abstract: Nonparametric models are versatile, albeit computationally expensive, tool\nfor modeling mixture models. In this paper, we introduce spectral methods for\nthe two most popular nonparametric models: the Indian Buffet Process (IBP) and\nthe Hierarchical Dirichlet Process (HDP). We show that using spectral methods\nfor the inference of nonparametric models are computationally and statistically\nefficient. In particular, we derive the lower-order moments of the IBP and the\nHDP, propose spectral algorithms for both models, and provide reconstruction\nguarantees for the algorithms. For the HDP, we further show that applying\nhierarchical models on dataset with hierarchical structure, which can be solved\nwith the generalized spectral HDP, produces better solutions to that of flat\nmodels regarding likelihood performance. \n\n"}
{"id": "1704.00217", "contents": "Title: Adversarial Connective-exploiting Networks for Implicit Discourse\n  Relation Classification Abstract: Implicit discourse relation classification is of great challenge due to the\nlack of connectives as strong linguistic cues, which motivates the use of\nannotated implicit connectives to improve the recognition. We propose a feature\nimitation framework in which an implicit relation network is driven to learn\nfrom another neural network with access to connectives, and thus encouraged to\nextract similarly salient features for accurate classification. We develop an\nadversarial model to enable an adaptive imitation scheme through competition\nbetween the implicit network and a rival feature discriminator. Our method\neffectively transfers discriminability of connectives to the implicit features,\nand achieves state-of-the-art performance on the PDTB benchmark. \n\n"}
{"id": "1704.01427", "contents": "Title: AMIDST: a Java Toolbox for Scalable Probabilistic Machine Learning Abstract: The AMIDST Toolbox is a software for scalable probabilistic machine learning\nwith a spe- cial focus on (massive) streaming data. The toolbox supports a\nflexible modeling language based on probabilistic graphical models with latent\nvariables and temporal dependencies. The specified models can be learnt from\nlarge data sets using parallel or distributed implementa- tions of Bayesian\nlearning algorithms for either streaming or batch data. These algorithms are\nbased on a flexible variational message passing scheme, which supports discrete\nand continu- ous variables from a wide range of probability distributions.\nAMIDST also leverages existing functionality and algorithms by interfacing to\nsoftware tools such as Flink, Spark, MOA, Weka, R and HUGIN. AMIDST is an open\nsource toolbox written in Java and available at http://www.amidsttoolbox.com\nunder the Apache Software License version 2.0. \n\n"}
{"id": "1704.01858", "contents": "Title: An Online Hierarchical Algorithm for Extreme Clustering Abstract: Many modern clustering methods scale well to a large number of data items, N,\nbut not to a large number of clusters, K. This paper introduces PERCH, a new\nnon-greedy algorithm for online hierarchical clustering that scales to both\nmassive N and K--a problem setting we term extreme clustering. Our algorithm\nefficiently routes new data points to the leaves of an incrementally-built\ntree. Motivated by the desire for both accuracy and speed, our approach\nperforms tree rotations for the sake of enhancing subtree purity and\nencouraging balancedness. We prove that, under a natural separability\nassumption, our non-greedy algorithm will produce trees with perfect dendrogram\npurity regardless of online data arrival order. Our experiments demonstrate\nthat PERCH constructs more accurate trees than other tree-building clustering\nalgorithms and scales well with both N and K, achieving a higher quality\nclustering than the strongest flat clustering competitor in nearly half the\ntime. \n\n"}
{"id": "1704.03296", "contents": "Title: Interpretable Explanations of Black Boxes by Meaningful Perturbation Abstract: As machine learning algorithms are increasingly applied to high impact yet\nhigh risk tasks, such as medical diagnosis or autonomous driving, it is\ncritical that researchers can explain how such algorithms arrived at their\npredictions. In recent years, a number of image saliency methods have been\ndeveloped to summarize where highly complex neural networks \"look\" in an image\nfor evidence for their predictions. However, these techniques are limited by\ntheir heuristic nature and architectural constraints. In this paper, we make\ntwo main contributions: First, we propose a general framework for learning\ndifferent kinds of explanations for any black box algorithm. Second, we\nspecialise the framework to find the part of an image most responsible for a\nclassifier decision. Unlike previous works, our method is model-agnostic and\ntestable because it is grounded in explicit and interpretable image\nperturbations. \n\n"}
{"id": "1704.05409", "contents": "Title: Ranking to Learn: Feature Ranking and Selection via Eigenvector\n  Centrality Abstract: In an era where accumulating data is easy and storing it inexpensive, feature\nselection plays a central role in helping to reduce the high-dimensionality of\nhuge amounts of otherwise meaningless data. In this paper, we propose a\ngraph-based method for feature selection that ranks features by identifying the\nmost important ones into arbitrary set of cues. Mapping the problem on an\naffinity graph-where features are the nodes-the solution is given by assessing\nthe importance of nodes through some indicators of centrality, in particular,\nthe Eigen-vector Centrality (EC). The gist of EC is to estimate the importance\nof a feature as a function of the importance of its neighbors. Ranking central\nnodes individuates candidate features, which turn out to be effective from a\nclassification point of view, as proved by a thoroughly experimental section.\nOur approach has been tested on 7 diverse datasets from recent literature\n(e.g., biological data and object recognition, among others), and compared\nagainst filter, embedded and wrappers methods. The results are remarkable in\nterms of accuracy, stability and low execution time. \n\n"}
{"id": "1704.06084", "contents": "Title: Knowledge Fusion via Embeddings from Text, Knowledge Graphs, and Images Abstract: We present a baseline approach for cross-modal knowledge fusion. Different\nbasic fusion methods are evaluated on existing embedding approaches to show the\npotential of joining knowledge about certain concepts across modalities in a\nfused concept representation. \n\n"}
{"id": "1704.07378", "contents": "Title: Geometry-Based Optimization of One-Way Quantum Computation Measurement\n  Patterns Abstract: In one-way quantum computation (1WQC) model, an initial highly entangled\nstate called a graph state is used to perform universal quantum computations by\na sequence of adaptive single-qubit measurements and post-measurement Pauli-X\nand Pauli-Z corrections. The needed computations are organized as measurement\npatterns, or simply patterns, in the 1WQC model. The entanglement operations in\na pattern can be shown by a graph which together with the set of its input and\noutput qubits is called the geometry of the pattern. Since a one-way quantum\ncomputation pattern is based on quantum measurements, which are fundamentally\nnondeterministic evolutions, there must be conditions over geometries to\nguarantee determinism. Causal flow is a sufficient and generalized flow (gflow)\nis a necessary and sufficient condition over geometries to identify a\ndependency structure for the measurement sequences in order to achieve\ndeterminism. Previously, three optimization methods have been proposed to\nsimplify 1WQC patterns which are called standardization, signal shifting and\nPauli simplification. These optimizations can be performed using measurement\ncalculus formalism by rewriting rules. However, maintaining and searching these\nrules in the library can be complicated with respect to implementation.\nMoreover, serial execution of these rules is time consuming due to executing\nmany ineffective commutation rules. To overcome this problem, in this paper, a\nnew scheme is proposed to perform optimization techniques on patterns with flow\nor gflow only based on their geometries instead of using rewriting rules.\nFurthermore, the proposed scheme obtains the maximally delayed gflow order for\ngeometries with flow. It is shown that the time complexity of the proposed\napproach is improved over the previous ones. \n\n"}
{"id": "1704.07943", "contents": "Title: Reward Maximization Under Uncertainty: Leveraging Side-Observations on\n  Networks Abstract: We study the stochastic multi-armed bandit (MAB) problem in the presence of\nside-observations across actions that occur as a result of an underlying\nnetwork structure. In our model, a bipartite graph captures the relationship\nbetween actions and a common set of unknowns such that choosing an action\nreveals observations for the unknowns that it is connected to. This models a\ncommon scenario in online social networks where users respond to their friends'\nactivity, thus providing side information about each other's preferences. Our\ncontributions are as follows: 1) We derive an asymptotic lower bound (with\nrespect to time) as a function of the bi-partite network structure on the\nregret of any uniformly good policy that achieves the maximum long-term average\nreward. 2) We propose two policies - a randomized policy; and a policy based on\nthe well-known upper confidence bound (UCB) policies - both of which explore\neach action at a rate that is a function of its network position. We show,\nunder mild assumptions, that these policies achieve the asymptotic lower bound\non the regret up to a multiplicative factor, independent of the network\nstructure. Finally, we use numerical examples on a real-world social network\nand a routing example network to demonstrate the benefits obtained by our\npolicies over other existing policies. \n\n"}
{"id": "1705.01942", "contents": "Title: Improving the Accuracy of an Adiabatic Quantum Computer Abstract: The purpose of the D-Wave adiabatic quantum computer is to find a set of\nqubit values that minimize its objective function. For various reasons, the set\nof qubit values returned by the D-Wave has errors. This paper presents a method\nof improving the results returned by the D-Wave. The method individually\nmodifies the qubit values returned by the D-Wave to find a set of values which\nis a minimum of the objective function. That set however is not necessarily\nguaranteed to be a global minimum. The method is simple and easily incorporated\ninto any algorithm that has direct access to the sets of values returned by the\nD-Wave. Examples are also presented that demonstrate the merit of using such a\nsample improvement method. \n\n"}
{"id": "1705.02908", "contents": "Title: Machine Learning with World Knowledge: The Position and Survey Abstract: Machine learning has become pervasive in multiple domains, impacting a wide\nvariety of applications, such as knowledge discovery and data mining, natural\nlanguage processing, information retrieval, computer vision, social and health\ninformatics, ubiquitous computing, etc. Two essential problems of machine\nlearning are how to generate features and how to acquire labels for machines to\nlearn. Particularly, labeling large amount of data for each domain-specific\nproblem can be very time consuming and costly. It has become a key obstacle in\nmaking learning protocols realistic in applications. In this paper, we will\ndiscuss how to use the existing general-purpose world knowledge to enhance\nmachine learning processes, by enriching the features or reducing the labeling\nwork. We start from the comparison of world knowledge with domain-specific\nknowledge, and then introduce three key problems in using world knowledge in\nlearning processes, i.e., explicit and implicit feature representation,\ninference for knowledge linking and disambiguation, and learning with direct or\nindirect supervision. Finally we discuss the future directions of this research\ntopic. \n\n"}
{"id": "1705.05491", "contents": "Title: Distributed Statistical Machine Learning in Adversarial Settings:\n  Byzantine Gradient Descent Abstract: We consider the problem of distributed statistical machine learning in\nadversarial settings, where some unknown and time-varying subset of working\nmachines may be compromised and behave arbitrarily to prevent an accurate model\nfrom being learned. This setting captures the potential adversarial attacks\nfaced by Federated Learning -- a modern machine learning paradigm that is\nproposed by Google researchers and has been intensively studied for ensuring\nuser privacy. Formally, we focus on a distributed system consisting of a\nparameter server and $m$ working machines. Each working machine keeps $N/m$\ndata samples, where $N$ is the total number of samples. The goal is to\ncollectively learn the underlying true model parameter of dimension $d$.\n  In classical batch gradient descent methods, the gradients reported to the\nserver by the working machines are aggregated via simple averaging, which is\nvulnerable to a single Byzantine failure. In this paper, we propose a Byzantine\ngradient descent method based on the geometric median of means of the\ngradients. We show that our method can tolerate $q \\le (m-1)/2$ Byzantine\nfailures, and the parameter estimate converges in $O(\\log N)$ rounds with an\nestimation error of $\\sqrt{d(2q+1)/N}$, hence approaching the optimal error\nrate $\\sqrt{d/N}$ in the centralized and failure-free setting. The total\ncomputational complexity of our algorithm is of $O((Nd/m) \\log N)$ at each\nworking machine and $O(md + kd \\log^3 N)$ at the central server, and the total\ncommunication cost is of $O(m d \\log N)$. We further provide an application of\nour general results to the linear regression problem.\n  A key challenge arises in the above problem is that Byzantine failures create\narbitrary and unspecified dependency among the iterations and the aggregated\ngradients. We prove that the aggregated gradient converges uniformly to the\ntrue gradient function. \n\n"}
{"id": "1705.05591", "contents": "Title: Learning Convex Regularizers for Optimal Bayesian Denoising Abstract: We propose a data-driven algorithm for the maximum a posteriori (MAP)\nestimation of stochastic processes from noisy observations. The primary\nstatistical properties of the sought signal is specified by the penalty\nfunction (i.e., negative logarithm of the prior probability density function).\nOur alternating direction method of multipliers (ADMM)-based approach\ntranslates the estimation task into successive applications of the proximal\nmapping of the penalty function. Capitalizing on this direct link, we define\nthe proximal operator as a parametric spline curve and optimize the spline\ncoefficients by minimizing the average reconstruction error for a given\ntraining set. The key aspects of our learning method are that the associated\npenalty function is constrained to be convex and the convergence of the ADMM\niterations is proven. As a result of these theoretical guarantees, adaptation\nof the proposed framework to different levels of measurement noise is extremely\nsimple and does not require any retraining. We apply our method to estimation\nof both sparse and non-sparse models of L\\'{e}vy processes for which the\nminimum mean square error (MMSE) estimators are available. We carry out a\nsingle training session and perform comparisons at various signal-to-noise\nratio (SNR) values. Simulations illustrate that the performance of our\nalgorithm is practically identical to the one of the MMSE estimator\nirrespective of the noise power. \n\n"}
{"id": "1705.07210", "contents": "Title: Two-temperature logistic regression based on the Tsallis divergence Abstract: We develop a variant of multiclass logistic regression that is significantly\nmore robust to noise. The algorithm has one weight vector per class and the\nsurrogate loss is a function of the linear activations (one per class). The\nsurrogate loss of an example with linear activation vector $\\mathbf{a}$ and\nclass $c$ has the form $-\\log_{t_1} \\exp_{t_2} (a_c - G_{t_2}(\\mathbf{a}))$\nwhere the two temperatures $t_1$ and $t_2$ ''temper'' the $\\log$ and $\\exp$,\nrespectively, and $G_{t_2}(\\mathbf{a})$ is a scalar value that generalizes the\nlog-partition function. We motivate this loss using the Tsallis divergence. Our\nmethod allows transitioning between non-convex and convex losses by the choice\nof the temperature parameters. As the temperature $t_1$ of the logarithm\nbecomes smaller than the temperature $t_2$ of the exponential, the surrogate\nloss becomes ''quasi convex''. Various tunings of the temperatures recover\nprevious methods and tuning the degree of non-convexity is crucial in the\nexperiments. In particular, quasi-convexity and boundedness of the loss provide\nsignificant robustness to the outliers. We explain this by showing that $t_1 <\n1$ caps the surrogate loss and $t_2 >1$ makes the predictive distribution have\na heavy tail.\n  We show that the surrogate loss is Bayes-consistent, even in the non-convex\ncase. Additionally, we provide efficient iterative algorithms for calculating\nthe log-partition value only in a few number of iterations. Our compelling\nexperimental results on large real-world datasets show the advantage of using\nthe two-temperature variant in the noisy as well as the noise free case. \n\n"}
{"id": "1705.08051", "contents": "Title: Wasserstein Learning of Deep Generative Point Process Models Abstract: Point processes are becoming very popular in modeling asynchronous sequential\ndata due to their sound mathematical foundation and strength in modeling a\nvariety of real-world phenomena. Currently, they are often characterized via\nintensity function which limits model's expressiveness due to unrealistic\nassumptions on its parametric form used in practice. Furthermore, they are\nlearned via maximum likelihood approach which is prone to failure in\nmulti-modal distributions of sequences. In this paper, we propose an\nintensity-free approach for point processes modeling that transforms nuisance\nprocesses to a target one. Furthermore, we train the model using a\nlikelihood-free leveraging Wasserstein distance between point processes.\nExperiments on various synthetic and real-world data substantiate the\nsuperiority of the proposed point process model over conventional ones. \n\n"}
{"id": "1705.08841", "contents": "Title: Multi-Level Variational Autoencoder: Learning Disentangled\n  Representations from Grouped Observations Abstract: We would like to learn a representation of the data which decomposes an\nobservation into factors of variation which we can independently control.\nSpecifically, we want to use minimal supervision to learn a latent\nrepresentation that reflects the semantics behind a specific grouping of the\ndata, where within a group the samples share a common factor of variation. For\nexample, consider a collection of face images grouped by identity. We wish to\nanchor the semantics of the grouping into a relevant and disentangled\nrepresentation that we can easily exploit. However, existing deep probabilistic\nmodels often assume that the observations are independent and identically\ndistributed. We present the Multi-Level Variational Autoencoder (ML-VAE), a new\ndeep probabilistic model for learning a disentangled representation of a set of\ngrouped observations. The ML-VAE separates the latent representation into\nsemantically meaningful parts by working both at the group level and the\nobservation level, while retaining efficient test-time inference. Quantitative\nand qualitative evaluations show that the ML-VAE model (i) learns a\nsemantically meaningful disentanglement of grouped data, (ii) enables\nmanipulation of the latent representation, and (iii) generalises to unseen\ngroups. \n\n"}
{"id": "1705.08865", "contents": "Title: Anti-spoofing Methods for Automatic SpeakerVerification System Abstract: Growing interest in automatic speaker verification (ASV)systems has lead to\nsignificant quality improvement of spoofing attackson them. Many research works\nconfirm that despite the low equal er-ror rate (EER) ASV systems are still\nvulnerable to spoofing attacks. Inthis work we overview different acoustic\nfeature spaces and classifiersto determine reliable and robust countermeasures\nagainst spoofing at-tacks. We compared several spoofing detection systems,\npresented so far,on the development and evaluation datasets of the Automatic\nSpeakerVerification Spoofing and Countermeasures (ASVspoof) Challenge\n2015.Experimental results presented in this paper demonstrate that the useof\nmagnitude and phase information combination provides a substantialinput into\nthe efficiency of the spoofing detection systems. Also wavelet-based features\nshow impressive results in terms of equal error rate. Inour overview we compare\nspoofing performance for systems based on dif-ferent classifiers. Comparison\nresults demonstrate that the linear SVMclassifier outperforms the conventional\nGMM approach. However, manyresearchers inspired by the great success of deep\nneural networks (DNN)approaches in the automatic speech recognition, applied\nDNN in thespoofing detection task and obtained quite low EER for known and\nun-known type of spoofing attacks. \n\n"}
{"id": "1705.09176", "contents": "Title: Shorter stabilizer circuits via Bruhat decomposition and quantum circuit\n  transformations Abstract: In this paper we improve the layered implementation of arbitrary stabilizer\ncircuits introduced by Aaronson and Gottesman in Phys. Rev. A 70(052328), 2004:\nto obtain a general stabilizer circuit, we reduce their $11$-stage computation\n-H-C-P-C-P-C-H-P-C-P-C- over the gate set consisting of Hadamard,\nControlled-NOT, and Phase gates, into a $7$-stage computation of the form\n-C-CZ-P-H-P-CZ-C-. We show arguments in support of using -CZ- stages over the\n-C- stages: not only the use of -CZ- stages allows a shorter layered\nexpression, but -CZ- stages are simpler and appear to be easier to implement\ncompared to the -C- stages. Based on this decomposition, we develop a two-qubit\ngate depth-$(14n{-}4)$ implementation of stabilizer circuits over the gate\nlibrary $\\{$H, P, CNOT$\\}$, executable in the Linear Nearest Neighbor (LNN)\narchitecture, improving best previously known depth-$25n$ circuit, also\nexecutable in the LNN architecture. Our constructions rely on Bruhat\ndecomposition of the symplectic group and on folding arbitrarily long sequences\nof the form $($-P-C-$)^m$ into a 3-stage computation -P-CZ-C-. Our results\ninclude the reduction of the $11$-stage decomposition -H-C-P-C-P-C-H-P-C-P-C-\ninto a $9$-stage decomposition of the form -C-P-C-P-H-C-P-C-P-. This reduction\nis based on the Bruhat decomposition of the symplectic group. This result also\nimplies a new normal form for stabilizer circuits. We show that a circuit in\nthis normal form is optimal in the number of Hadamard gates used. We also show\nthat the normal form has an asymptotically optimal number of parameters. \n\n"}
{"id": "1705.09303", "contents": "Title: Latent Geometry and Memorization in Generative Models Abstract: It can be difficult to tell whether a trained generative model has learned to\ngenerate novel examples or has simply memorized a specific set of outputs. In\npublished work, it is common to attempt to address this visually, for example\nby displaying a generated example and its nearest neighbor(s) in the training\nset (in, for example, the L2 metric). As any generative model induces a\nprobability density on its output domain, we propose studying this density\ndirectly. We first study the geometry of the latent representation and\ngenerator, relate this to the output density, and then develop techniques to\ncompute and inspect the output density. As an application, we demonstrate that\n\"memorization\" tends to a density made of delta functions concentrated on the\nmemorized examples. We note that without first understanding the geometry, the\nmeasurement would be essentially impossible to make. \n\n"}
{"id": "1705.09367", "contents": "Title: Stabilizing Training of Generative Adversarial Networks through\n  Regularization Abstract: Deep generative models based on Generative Adversarial Networks (GANs) have\ndemonstrated impressive sample quality but in order to work they require a\ncareful choice of architecture, parameter initialization, and selection of\nhyper-parameters. This fragility is in part due to a dimensional mismatch or\nnon-overlapping support between the model distribution and the data\ndistribution, causing their density ratio and the associated f-divergence to be\nundefined. We overcome this fundamental limitation and propose a new\nregularization approach with low computational cost that yields a stable GAN\ntraining procedure. We demonstrate the effectiveness of this regularizer across\nseveral architectures trained on common benchmark image generation tasks. Our\nregularization turns GAN models into reliable building blocks for deep\nlearning. \n\n"}
{"id": "1705.09605", "contents": "Title: Combinatorial Multi-Armed Bandits with Filtered Feedback Abstract: Motivated by problems in search and detection we present a solution to a\nCombinatorial Multi-Armed Bandit (CMAB) problem with both heavy-tailed reward\ndistributions and a new class of feedback, filtered semibandit feedback. In a\nCMAB problem an agent pulls a combination of arms from a set $\\{1,...,k\\}$ in\neach round, generating random outcomes from probability distributions\nassociated with these arms and receiving an overall reward. Under semibandit\nfeedback it is assumed that the random outcomes generated are all observed.\nFiltered semibandit feedback allows the outcomes that are observed to be\nsampled from a second distribution conditioned on the initial random outcomes.\nThis feedback mechanism is valuable as it allows CMAB methods to be applied to\nsequential search and detection problems where combinatorial actions are made,\nbut the true rewards (number of objects of interest appearing in the round) are\nnot observed, rather a filtered reward (the number of objects the searcher\nsuccessfully finds, which must by definition be less than the number that\nappear). We present an upper confidence bound type algorithm, Robust-F-CUCB,\nand associated regret bound of order $\\mathcal{O}(\\ln(n))$ to balance\nexploration and exploitation in the face of both filtering of reward and heavy\ntailed reward distributions. \n\n"}
{"id": "1705.10257", "contents": "Title: Boltzmann Exploration Done Right Abstract: Boltzmann exploration is a classic strategy for sequential decision-making\nunder uncertainty, and is one of the most standard tools in Reinforcement\nLearning (RL). Despite its widespread use, there is virtually no theoretical\nunderstanding about the limitations or the actual benefits of this exploration\nscheme. Does it drive exploration in a meaningful way? Is it prone to\nmisidentifying the optimal actions or spending too much time exploring the\nsuboptimal ones? What is the right tuning for the learning rate? In this paper,\nwe address several of these questions in the classic setup of stochastic\nmulti-armed bandits. One of our main results is showing that the Boltzmann\nexploration strategy with any monotone learning-rate sequence will induce\nsuboptimal behavior. As a remedy, we offer a simple non-monotone schedule that\nguarantees near-optimal performance, albeit only when given prior access to key\nproblem parameters that are typically not available in practical situations\n(like the time horizon $T$ and the suboptimality gap $\\Delta$). More\nimportantly, we propose a novel variant that uses different learning rates for\ndifferent arms, and achieves a distribution-dependent regret bound of order\n$\\frac{K\\log^2 T}{\\Delta}$ and a distribution-independent bound of order\n$\\sqrt{KT}\\log K$ without requiring such prior knowledge. To demonstrate the\nflexibility of our technique, we also propose a variant that guarantees the\nsame performance bounds even if the rewards are heavy-tailed. \n\n"}
{"id": "1706.00476", "contents": "Title: The Mixing method: low-rank coordinate descent for semidefinite\n  programming with diagonal constraints Abstract: In this paper, we propose a low-rank coordinate descent approach to\nstructured semidefinite programming with diagonal constraints. The approach,\nwhich we call the Mixing method, is extremely simple to implement, has no free\nparameters, and typically attains an order of magnitude or better improvement\nin optimization performance over the current state of the art. We show that the\nmethod is strictly decreasing, converges to a critical point, and further that\nfor sufficient rank all non-optimal critical points are unstable. Moreover, we\nprove that with a step size, the Mixing method converges to the global optimum\nof the semidefinite program almost surely in a locally linear rate under random\ninitialization. This is the first low-rank semidefinite programming method that\nhas been shown to achieve a global optimum on the spherical manifold without\nassumption. We apply our algorithm to two related domains: solving the maximum\ncut semidefinite relaxation, and solving a maximum satisfiability relaxation\n(we also briefly consider additional applications such as learning word\nembeddings). In all settings, we demonstrate substantial improvement over the\nexisting state of the art along various dimensions, and in total, this work\nexpands the scope and scale of problems that can be solved using semidefinite\nprogramming methods. \n\n"}
{"id": "1706.00820", "contents": "Title: Information, Privacy and Stability in Adaptive Data Analysis Abstract: Traditional statistical theory assumes that the analysis to be performed on a\ngiven data set is selected independently of the data themselves. This\nassumption breaks downs when data are re-used across analyses and the analysis\nto be performed at a given stage depends on the results of earlier stages. Such\ndependency can arise when the same data are used by several scientific studies,\nor when a single analysis consists of multiple stages.\n  How can we draw statistically valid conclusions when data are re-used? This\nis the focus of a recent and active line of work. At a high level, these\nresults show that limiting the information revealed by earlier stages of\nanalysis controls the bias introduced in later stages by adaptivity.\n  Here we review some known results in this area and highlight the role of\ninformation-theoretic concepts, notably several one-shot notions of mutual\ninformation. \n\n"}
{"id": "1706.01824", "contents": "Title: Robust Online Multi-Task Learning with Correlative and Personalized\n  Structures Abstract: Multi-Task Learning (MTL) can enhance a classifier's generalization\nperformance by learning multiple related tasks simultaneously. Conventional MTL\nworks under the offline or batch setting, and suffers from expensive training\ncost and poor scalability. To address such inefficiency issues, online learning\ntechniques have been applied to solve MTL problems. However, most existing\nalgorithms of online MTL constrain task relatedness into a presumed structure\nvia a single weight matrix, which is a strict restriction that does not always\nhold in practice. In this paper, we propose a robust online MTL framework that\novercomes this restriction by decomposing the weight matrix into two\ncomponents: the first one captures the low-rank common structure among tasks\nvia a nuclear norm and the second one identifies the personalized patterns of\noutlier tasks via a group lasso. Theoretical analysis shows the proposed\nalgorithm can achieve a sub-linear regret with respect to the best linear model\nin hindsight. Even though the above framework achieves good performance, the\nnuclear norm that simply adds all nonzero singular values together may not be a\ngood low-rank approximation. To improve the results, we use a log-determinant\nfunction as a non-convex rank approximation. The gradient scheme is applied to\noptimize log-determinant function and can obtain a closed-form solution for\nthis refined problem. Experimental results on a number of real-world\napplications verify the efficacy of our method. \n\n"}
{"id": "1706.01945", "contents": "Title: Practical Integer-to-Binary Mapping for Quantum Annealers Abstract: Recent advancements in quantum annealing hardware and numerous studies in\nthis area suggests that quantum annealers have the potential to be effective in\nsolving unconstrained binary quadratic programming problems. Naturally, one may\ndesire to expand the application domain of these machines to problems with\ngeneral discrete variables. In this paper, we explore the possibility of\nemploying quantum annealers to solve unconstrained quadratic programming\nproblems over a bounded integer domain. We present an approach for encoding\ninteger variables into binary ones, thereby representing unconstrained integer\nquadratic programming problems as unconstrained binary quadratic programming\nproblems. To respect some of the limitations of the currently developed quantum\nannealers, we propose an integer encoding, named bounded- coefficient encoding,\nin which we limit the size of the coefficients that appear in the encoding.\nFurthermore, we propose an algorithm for finding the upper bound on the\ncoefficients of the encoding using the precision of the machine and the\ncoefficients of the original integer problem. Finally, we experimentally show\nthat this approach is far more resilient to the noise of the quantum annealers\ncompared to traditional approaches for the encoding of integers in base two. \n\n"}
{"id": "1706.01983", "contents": "Title: Deep Learning: Generalization Requires Deep Compositional Feature Space\n  Design Abstract: Generalization error defines the discriminability and the representation\npower of a deep model. In this work, we claim that feature space design using\ndeep compositional function plays a significant role in generalization along\nwith explicit and implicit regularizations. Our claims are being established\nwith several image classification experiments. We show that the information\nloss due to convolution and max pooling can be marginalized with the\ncompositional design, improving generalization performance. Also, we will show\nthat learning rate decay acts as an implicit regularizer in deep model\ntraining. \n\n"}
{"id": "1706.02237", "contents": "Title: Efficient Reinforcement Learning via Initial Pure Exploration Abstract: In several realistic situations, an interactive learning agent can practice\nand refine its strategy before going on to be evaluated. For instance, consider\na student preparing for a series of tests. She would typically take a few\npractice tests to know which areas she needs to improve upon. Based of the\nscores she obtains in these practice tests, she would formulate a strategy for\nmaximizing her scores in the actual tests. We treat this scenario in the\ncontext of an agent exploring a fixed-horizon episodic Markov Decision Process\n(MDP), where the agent can practice on the MDP for some number of episodes (not\nnecessarily known in advance) before starting to incur regret for its actions.\n  During practice, the agent's goal must be to maximize the probability of\nfollowing an optimal policy. This is akin to the problem of Pure Exploration\n(PE). We extend the PE problem of Multi Armed Bandits (MAB) to MDPs and propose\na Bayesian algorithm called Posterior Sampling for Pure Exploration (PSPE),\nwhich is similar to its bandit counterpart. We show that the Bayesian simple\nregret converges at an optimal exponential rate when using PSPE.\n  When the agent starts being evaluated, its goal would be to minimize the\ncumulative regret incurred. This is akin to the problem of Reinforcement\nLearning (RL). The agent uses the Posterior Sampling for Reinforcement Learning\nalgorithm (PSRL) initialized with the posteriors of the practice phase. We\nhypothesize that this PSPE + PSRL combination is an optimal strategy for\nminimizing regret in RL problems with an initial practice phase. We show\nempirical results which prove that having a lower simple regret at the end of\nthe practice phase results in having lower cumulative regret during evaluation. \n\n"}
{"id": "1706.02721", "contents": "Title: Logic Synthesis for Quantum Computing Abstract: We present a synthesis framework to map logic networks into quantum circuits\nfor quantum computing. The synthesis framework is based on LUT networks\n(lookup-table networks), which play a key role in conventional logic synthesis.\nEstablishing a connection between LUTs in a LUT network and reversible\nsingle-target gates in a reversible network allows us to bridge conventional\nlogic synthesis with logic synthesis for quantum computing, despite several\nfundamental differences. We call our synthesis framework LUT-based Hierarchical\nReversible Logic Synthesis (LHRS). Input to LHRS is a classical logic network;\noutput is a quantum network (realized in terms of Clifford+$T$ gates). The\nframework offers to trade-off the number of qubits for the number of quantum\ngates. In a first step, an initial network is derived that only consists of\nsingle-target gates and already completely determines the number of qubits in\nthe final quantum network. Different methods are then used to map each\nsingle-target gate into Clifford+$T$ gates, while aiming at optimally using\navailable resources. We demonstrate the effectiveness of our method in\nautomatically synthesizing IEEE compliant floating point networks up to double\nprecision. As many quantum algorithms target scientific simulation\napplications, they can make rich use of floating point arithmetic components.\nBut due to the lack of quantum circuit descriptions for those components, it\ncan be difficult to find a realistic cost estimation for the algorithms. Our\nsynthesized benchmarks provide cost estimates that allow quantum algorithm\ndesigners to provide the first complete cost estimates for a host of quantum\nalgorithms. Thus, the benchmarks and, more generally, the LHRS framework are an\nessential step towards the goal of understanding which quantum algorithms will\nbe practical in the first generations of quantum computers. \n\n"}
{"id": "1706.03419", "contents": "Title: Improved reversible and quantum circuits for Karatsuba-based integer\n  multiplication Abstract: Integer arithmetic is the underpinning of many quantum algorithms, with\napplications ranging from Shor's algorithm over HHL for matrix inversion to\nHamiltonian simulation algorithms. A basic objective is to keep the required\nresources to implement arithmetic as low as possible. This applies in\nparticular to the number of qubits required in the implementation as for the\nforeseeable future this number is expected to be small. We present a reversible\ncircuit for integer multiplication that is inspired by Karatsuba's recursive\nmethod. The main improvement over circuits that have been previously reported\nin the literature is an asymptotic reduction of the amount of space required\nfrom $O(n^{1.585})$ to $O(n^{1.427})$. This improvement is obtained in exchange\nfor a small constant increase in the number of operations by a factor less than\n$2$ and a small asymptotic increase in depth for the parallel version. The\nasymptotic improvement are obtained from analyzing pebble games on complete\nternary trees. \n\n"}
{"id": "1706.05113", "contents": "Title: T-count Optimized Design of Quantum Integer Multiplication Abstract: Quantum circuits of many qubits are extremely difficult to realize; thus, the\nnumber of qubits is an important metric in a quantum circuit design. Further,\nscalable and reliable quantum circuits are based on Clifford + T gates. An\nefficient quantum circuit saves quantum hardware resources by reducing the\nnumber of T gates without substantially increasing the number of qubits.\nRecently, the design of a quantum multiplier is presented by Babu [1] which\nimproves the existing works in terms of number of quantum gates, number of\nqubits, and delay. However, the recent design is not based on fault-tolerant\nClifford + T gates. Also, it has large number of qubits and garbage outputs.\nTherefore, this work presents a T-count optimized quantum circuit for integer\nmultiplication with only $4 \\cdot n + 1$ qubits and no garbage outputs. The\nproposed quantum multiplier design saves the T-count by using a novel quantum\nconditional adder circuit. Also, where one operand to the controlled adder is\nzero, the conditional adder is replaced with a Toffoli gate array to further\nsave the T gates. To have fair comparison with the recent design by Babu and\nget an actual estimate of the T-count, it is made garbageless by using\nBennett's garbage removal scheme. The proposed design achieves an average\nT-count savings of $47.55\\%$ compared to the recent work by Babu. Further,\ncomparison is also performed with other recent works by Lin et. al. [2], and\nJayashree et. al.[3]. Average T-count savings of $62.71\\%$ and $26.30\\%$ are\nachieved compared to the recent works by Lin et. al., and Jayashree et. al.,\nrespectively. \n\n"}
{"id": "1706.05439", "contents": "Title: Control Variates for Stochastic Gradient MCMC Abstract: It is well known that Markov chain Monte Carlo (MCMC) methods scale poorly\nwith dataset size. A popular class of methods for solving this issue is\nstochastic gradient MCMC. These methods use a noisy estimate of the gradient of\nthe log posterior, which reduces the per iteration computational cost of the\nalgorithm. Despite this, there are a number of results suggesting that\nstochastic gradient Langevin dynamics (SGLD), probably the most popular of\nthese methods, still has computational cost proportional to the dataset size.\nWe suggest an alternative log posterior gradient estimate for stochastic\ngradient MCMC, which uses control variates to reduce the variance. We analyse\nSGLD using this gradient estimate, and show that, under log-concavity\nassumptions on the target distribution, the computational cost required for a\ngiven level of accuracy is independent of the dataset size. Next we show that a\ndifferent control variate technique, known as zero variance control variates\ncan be applied to SGMCMC algorithms for free. This post-processing step\nimproves the inference of the algorithm by reducing the variance of the MCMC\noutput. Zero variance control variates rely on the gradient of the log\nposterior; we explore how the variance reduction is affected by replacing this\nwith the noisy gradient estimate calculated by SGMCMC. \n\n"}
{"id": "1706.06428", "contents": "Title: An online sequence-to-sequence model for noisy speech recognition Abstract: Generative models have long been the dominant approach for speech\nrecognition. The success of these models however relies on the use of\nsophisticated recipes and complicated machinery that is not easily accessible\nto non-practitioners. Recent innovations in Deep Learning have given rise to an\nalternative - discriminative models called Sequence-to-Sequence models, that\ncan almost match the accuracy of state of the art generative models. While\nthese models are easy to train as they can be trained end-to-end in a single\nstep, they have a practical limitation that they can only be used for offline\nrecognition. This is because the models require that the entirety of the input\nsequence be available at the beginning of inference, an assumption that is not\nvalid for instantaneous speech recognition. To address this problem, online\nsequence-to-sequence models were recently introduced. These models are able to\nstart producing outputs as data arrives, and the model feels confident enough\nto output partial transcripts. These models, like sequence-to-sequence are\ncausal - the output produced by the model until any time, $t$, affects the\nfeatures that are computed subsequently. This makes the model inherently more\npowerful than generative models that are unable to change features that are\ncomputed from the data. This paper highlights two main contributions - an\nimprovement to online sequence-to-sequence model training, and its application\nto noisy settings with mixed speech from two speakers. \n\n"}
{"id": "1706.06752", "contents": "Title: Quantum resource estimates for computing elliptic curve discrete\n  logarithms Abstract: We give precise quantum resource estimates for Shor's algorithm to compute\ndiscrete logarithms on elliptic curves over prime fields. The estimates are\nderived from a simulation of a Toffoli gate network for controlled elliptic\ncurve point addition, implemented within the framework of the quantum computing\nsoftware tool suite LIQ$Ui|\\rangle$. We determine circuit implementations for\nreversible modular arithmetic, including modular addition, multiplication and\ninversion, as well as reversible elliptic curve point addition. We conclude\nthat elliptic curve discrete logarithms on an elliptic curve defined over an\n$n$-bit prime field can be computed on a quantum computer with at most $9n +\n2\\lceil\\log_2(n)\\rceil+10$ qubits using a quantum circuit of at most $448 n^3\n\\log_2(n) + 4090 n^3$ Toffoli gates. We are able to classically simulate the\nToffoli networks corresponding to the controlled elliptic curve point addition\nas the core piece of Shor's algorithm for the NIST standard curves P-192,\nP-224, P-256, P-384 and P-521. Our approach allows gate-level comparisons to\nrecent resource estimates for Shor's factoring algorithm. The results also\nsupport estimates given earlier by Proos and Zalka and indicate that, for\ncurrent parameters at comparable classical security levels, the number of\nqubits required to tackle elliptic curves is less than for attacking RSA,\nsuggesting that indeed ECC is an easier target than RSA. \n\n"}
{"id": "1706.06859", "contents": "Title: Analysis of dropout learning regarded as ensemble learning Abstract: Deep learning is the state-of-the-art in fields such as visual object\nrecognition and speech recognition. This learning uses a large number of\nlayers, huge number of units, and connections. Therefore, overfitting is a\nserious problem. To avoid this problem, dropout learning is proposed. Dropout\nlearning neglects some inputs and hidden units in the learning process with a\nprobability, p, and then, the neglected inputs and hidden units are combined\nwith the learned network to express the final output. We find that the process\nof combining the neglected hidden units with the learned network can be\nregarded as ensemble learning, so we analyze dropout learning from this point\nof view. \n\n"}
{"id": "1706.07979", "contents": "Title: Methods for Interpreting and Understanding Deep Neural Networks Abstract: This paper provides an entry point to the problem of interpreting a deep\nneural network model and explaining its predictions. It is based on a tutorial\ngiven at ICASSP 2017. It introduces some recently proposed techniques of\ninterpretation, along with theory, tricks and recommendations, to make most\nefficient use of these techniques on real data. It also discusses a number of\npractical applications. \n\n"}
{"id": "1706.08470", "contents": "Title: Efficiency of quantum versus classical annealing in non-convex learning\n  problems Abstract: Quantum annealers aim at solving non-convex optimization problems by\nexploiting cooperative tunneling effects to escape local minima. The underlying\nidea consists in designing a classical energy function whose ground states are\nthe sought optimal solutions of the original optimization problem and add a\ncontrollable quantum transverse field to generate tunneling processes. A key\nchallenge is to identify classes of non-convex optimization problems for which\nquantum annealing remains efficient while thermal annealing fails. We show that\nthis happens for a wide class of problems which are central to machine\nlearning. Their energy landscapes is dominated by local minima that cause\nexponential slow down of classical thermal annealers while simulated quantum\nannealing converges efficiently to rare dense regions of optimal solutions. \n\n"}
{"id": "1706.09200", "contents": "Title: Energy-Based Sequence GANs for Recommendation and Their Connection to\n  Imitation Learning Abstract: Recommender systems aim to find an accurate and efficient mapping from\nhistoric data of user-preferred items to a new item that is to be liked by a\nuser. Towards this goal, energy-based sequence generative adversarial nets\n(EB-SeqGANs) are adopted for recommendation by learning a generative model for\nthe time series of user-preferred items. By recasting the energy function as\nthe feature function, the proposed EB-SeqGANs is interpreted as an instance of\nmaximum-entropy imitation learning. \n\n"}
{"id": "1707.00391", "contents": "Title: Fair Pipelines Abstract: This work facilitates ensuring fairness of machine learning in the real world\nby decoupling fairness considerations in compound decisions. In particular,\nthis work studies how fairness propagates through a compound decision-making\nprocesses, which we call a pipeline. Prior work in algorithmic fairness only\nfocuses on fairness with respect to one decision. However, many decision-making\nprocesses require more than one decision. For instance, hiring is at least a\ntwo stage model: deciding who to interview from the applicant pool and then\ndeciding who to hire from the interview pool. Perhaps surprisingly, we show\nthat the composition of fair components may not guarantee a fair pipeline under\na $(1+\\varepsilon)$-equal opportunity definition of fair. However, we identify\ncircumstances that do provide that guarantee. We also propose numerous\ndirections for future work on more general compound machine learning decisions. \n\n"}
{"id": "1707.00724", "contents": "Title: Efficient Probabilistic Performance Bounds for Inverse Reinforcement\n  Learning Abstract: In the field of reinforcement learning there has been recent progress towards\nsafety and high-confidence bounds on policy performance. However, to our\nknowledge, no practical methods exist for determining high-confidence policy\nperformance bounds in the inverse reinforcement learning setting---where the\ntrue reward function is unknown and only samples of expert behavior are given.\nWe propose a sampling method based on Bayesian inverse reinforcement learning\nthat uses demonstrations to determine practical high-confidence upper bounds on\nthe $\\alpha$-worst-case difference in expected return between any evaluation\npolicy and the optimal policy under the expert's unknown reward function. We\nevaluate our proposed bound on both a standard grid navigation task and a\nsimulated driving task and achieve tighter and more accurate bounds than a\nfeature count-based baseline. We also give examples of how our proposed bound\ncan be utilized to perform risk-aware policy selection and risk-aware policy\nimprovement. Because our proposed bound requires several orders of magnitude\nfewer demonstrations than existing high-confidence bounds, it is the first\npractical method that allows agents that learn from demonstration to express\nconfidence in the quality of their learned policy. \n\n"}
{"id": "1707.00865", "contents": "Title: Advanced Simulation of Quantum Computations Abstract: Quantum computation is a promising emerging technology which, compared to\nconventional computation, allows for substantial speed-ups e.g. for integer\nfactorization or database search. However, since physical realizations of\nquantum computers are in their infancy, a significant amount of research in\nthis domain still relies on simulations of quantum computations on conventional\nmachines. This causes a significant complexity which current state-of-the-art\nsimulators try to tackle with a rather straight forward array-based\nrepresentation and by applying massive hardware power. There also exist\nsolutions based on decision diagrams (i.e. graph-based approaches) that try to\ntackle the exponential complexity by exploiting redundancies in quantum states\nand operations. However, these existing approaches do not fully exploit\nredundancies that are actually present. In this work, we revisit the basics of\nquantum computation, investigate how corresponding quantum states and quantum\noperations can be represented even more compactly, and, eventually, simulated\nin a more efficient fashion. This leads to a new graph-based simulation\napproach which outperforms state-of-the-art simulators (array-based as well as\ngraph-based). Experimental evaluations show that the proposed solution is\ncapable of simulating quantum computations for more qubits than before, and in\nsignificantly less run-time (several magnitudes faster compared to previously\nproposed simulators). An implementation of the proposed simulator is publicly\navailable online at http://iic.jku.at/eda/research/quantum_simulation. \n\n"}
{"id": "1707.01939", "contents": "Title: High-Performance FPGA Implementation of Equivariant Adaptive Separation\n  via Independence Algorithm for Independent Component Analysis Abstract: Independent Component Analysis (ICA) is a dimensionality reduction technique\nthat can boost efficiency of machine learning models that deal with probability\ndensity functions, e.g. Bayesian neural networks. Algorithms that implement\nadaptive ICA converge slower than their nonadaptive counterparts, however, they\nare capable of tracking changes in underlying distributions of input features.\nThis intrinsically slow convergence of adaptive methods combined with existing\nhardware implementations that operate at very low clock frequencies necessitate\nfundamental improvements in both algorithm and hardware design. This paper\npresents an algorithm that allows efficient hardware implementation of ICA.\nCompared to previous work, our FPGA implementation of adaptive ICA improves\nclock frequency by at least one order of magnitude and throughput by at least\ntwo orders of magnitude. Our proposed algorithm is not limited to ICA and can\nbe used in various machine learning problems that use stochastic gradient\ndescent optimization. \n\n"}
{"id": "1707.06356", "contents": "Title: Use of global interactions in efficient quantum circuit constructions Abstract: In this paper we study the ways to use a global entangling operator to\nefficiently implement circuitry common to a selection of important quantum\nalgorithms. In particular, we focus on the circuits composed with global Ising\nentangling gates and arbitrary addressable single-qubit gates. We show that\nunder certain circumstances the use of global operations can substantially\nimprove the entangling gate count. \n\n"}
{"id": "1707.07012", "contents": "Title: Learning Transferable Architectures for Scalable Image Recognition Abstract: Developing neural network image classification models often requires\nsignificant architecture engineering. In this paper, we study a method to learn\nthe model architectures directly on the dataset of interest. As this approach\nis expensive when the dataset is large, we propose to search for an\narchitectural building block on a small dataset and then transfer the block to\na larger dataset. The key contribution of this work is the design of a new\nsearch space (the \"NASNet search space\") which enables transferability. In our\nexperiments, we search for the best convolutional layer (or \"cell\") on the\nCIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking\ntogether more copies of this cell, each with their own parameters to design a\nconvolutional architecture, named \"NASNet architecture\". We also introduce a\nnew regularization technique called ScheduledDropPath that significantly\nimproves generalization in the NASNet models. On CIFAR-10 itself, NASNet\nachieves 2.4% error rate, which is state-of-the-art. On ImageNet, NASNet\nachieves, among the published works, state-of-the-art accuracy of 82.7% top-1\nand 96.2% top-5 on ImageNet. Our model is 1.2% better in top-1 accuracy than\nthe best human-invented architectures while having 9 billion fewer FLOPS - a\nreduction of 28% in computational demand from the previous state-of-the-art\nmodel. When evaluated at different levels of computational cost, accuracies of\nNASNets exceed those of the state-of-the-art human-designed models. For\ninstance, a small version of NASNet also achieves 74% top-1 accuracy, which is\n3.1% better than equivalently-sized, state-of-the-art models for mobile\nplatforms. Finally, the learned features by NASNet used with the Faster-RCNN\nframework surpass state-of-the-art by 4.0% achieving 43.1% mAP on the COCO\ndataset. \n\n"}
{"id": "1707.08834", "contents": "Title: Arithmetic Circuits for Multilevel Qudits Based on Quantum Fourier\n  Transform Abstract: We present some basic integer arithmetic quantum circuits, such as adders and\nmultipliers-accumulators of various forms, as well as diagonal operators, which\noperate on multilevel qudits. The integers to be processed are represented in\nan alternative basis after they have been Fourier transformed. Several\narithmetic circuits operating on Fourier transformed integers have appeared in\nthe literature for two level qubits. Here we extend these techniques on\nmultilevel qudits, as they may offer some advantages relative to qubits\nimplementations. The arithmetic circuits presented can be used as basic\nbuilding blocks for higher level algorithms such as quantum phase estimation,\nquantum simulation, quantum optimization etc., but they can also be used in the\nimplementation of a quantum fractional Fourier transform as it is shown in a\ncompanion work presented separately. \n\n"}
{"id": "1707.09157", "contents": "Title: Efficient Algorithms for Non-convex Isotonic Regression through\n  Submodular Optimization Abstract: We consider the minimization of submodular functions subject to ordering\nconstraints. We show that this optimization problem can be cast as a convex\noptimization problem on a space of uni-dimensional measures, with ordering\nconstraints corresponding to first-order stochastic dominance. We propose new\ndiscretization schemes that lead to simple and efficient algorithms based on\nzero-th, first, or higher order oracles; these algorithms also lead to\nimprovements without isotonic constraints. Finally, our experiments show that\nnon-convex loss functions can be much more robust to outliers for isotonic\nregression, while still leading to an efficient optimization problem. \n\n"}
{"id": "1707.09641", "contents": "Title: Towards Visual Explanations for Convolutional Neural Networks via Input\n  Resampling Abstract: The predictive power of neural networks often costs model interpretability.\nSeveral techniques have been developed for explaining model outputs in terms of\ninput features; however, it is difficult to translate such interpretations into\nactionable insight. Here, we propose a framework to analyze predictions in\nterms of the model's internal features by inspecting information flow through\nthe network. Given a trained network and a test image, we select neurons by two\nmetrics, both measured over a set of images created by perturbations to the\ninput image: (1) magnitude of the correlation between the neuron activation and\nthe network output and (2) precision of the neuron activation. We show that the\nformer metric selects neurons that exert large influence over the network\noutput while the latter metric selects neurons that activate on generalizable\nfeatures. By comparing the sets of neurons selected by these two metrics, our\nframework suggests a way to investigate the internal attention mechanisms of\nconvolutional neural networks. \n\n"}
{"id": "1708.00023", "contents": "Title: Two-step approach to scheduling quantum circuits Abstract: As the effort to scale up existing quantum hardware proceeds, it becomes\nnecessary to schedule quantum gates in a way that minimizes the number of\noperations. There are three constraints that have to be satisfied: the order or\ndependency of the quantum gates in the specific algorithm, the fact that any\nqubit may be involved in at most one gate at a time, and the restriction that\ntwo-qubit gates are implementable only between connected qubits. The last\naspect implies that the compilation depends not only on the algorithm, but also\non hardware properties like connectivity. Here we suggest a two-step approach\nin which logical gates are initially scheduled neglecting connectivity\nconsiderations, while routing operations are added at a later step in a way\nthat minimizes their overhead. We rephrase the subtasks of gate scheduling in\nterms of graph problems like edge-coloring and maximum subgraph isomorphism.\nWhile this approach is general, we specialize to a one dimensional array of\nqubits to propose a routing scheme that is minimal in the number of exchange\noperations. As a practical application, we schedule the Quantum Approximate\nOptimization Algorithm in a linear geometry and quantify the reduction in the\nnumber of gates and circuit depth that results from increasing the efficacy of\nthe scheduling strategies. \n\n"}
{"id": "1708.00102", "contents": "Title: Advantages and Limitations of using Successor Features for Transfer in\n  Reinforcement Learning Abstract: One question central to Reinforcement Learning is how to learn a feature\nrepresentation that supports algorithm scaling and re-use of learned\ninformation from different tasks. Successor Features approach this problem by\nlearning a feature representation that satisfies a temporal constraint. We\npresent an implementation of an approach that decouples the feature\nrepresentation from the reward function, making it suitable for transferring\nknowledge between domains. We then assess the advantages and limitations of\nusing Successor Features for transfer. \n\n"}
{"id": "1708.00306", "contents": "Title: New Design of Reversible Full Adder/Subtractor using $R$ gate Abstract: Quantum computers require quantum processors. An important part of the\nprocessor of any computer is the arithmetic unit, which performs binary\naddition, subtraction, division and multiplication, however multiplication can\nbe performed using repeated addition, while division can be performed using\nrepeated subtraction. In this paper we present two designs using the reversible\n$R^3$ gate to perform the quantum half adder/ subtractor and the quantum full\nadder/subtractor. The proposed half adder/subtractor design can be used to\nperform different logical operations, such as $AND$, $XOR$, $NAND$, $XNOR$,\n$NOT$ and copy of basis. The proposed design is compared with the other\nprevious designs in terms of the number of gates used, the number of constant\nbits, the garbage bits, the quantum cost and the delay. The proposed designs\nare implemented and tested using GAP software. \n\n"}
{"id": "1708.00308", "contents": "Title: SenGen: Sentence Generating Neural Variational Topic Model Abstract: We present a new topic model that generates documents by sampling a topic for\none whole sentence at a time, and generating the words in the sentence using an\nRNN decoder that is conditioned on the topic of the sentence. We argue that\nthis novel formalism will help us not only visualize and model the topical\ndiscourse structure in a document better, but also potentially lead to more\ninterpretable topics since we can now illustrate topics by sampling\nrepresentative sentences instead of bag of words or phrases. We present a\nvariational auto-encoder approach for learning in which we use a factorized\nvariational encoder that independently models the posterior over topical\nmixture vectors of documents using a feed-forward network, and the posterior\nover topic assignments to sentences using an RNN. Our preliminary experiments\non two different datasets indicate early promise, but also expose many\nchallenges that remain to be addressed. \n\n"}
{"id": "1708.04622", "contents": "Title: Deep Learning the Ising Model Near Criticality Abstract: It is well established that neural networks with deep architectures perform\nbetter than shallow networks for many tasks in machine learning. In statistical\nphysics, while there has been recent interest in representing physical data\nwith generative modelling, the focus has been on shallow neural networks. A\nnatural question to ask is whether deep neural networks hold any advantage over\nshallow networks in representing such data. We investigate this question by\nusing unsupervised, generative graphical models to learn the probability\ndistribution of a two-dimensional Ising system. Deep Boltzmann machines, deep\nbelief networks, and deep restricted Boltzmann networks are trained on thermal\nspin configurations from this system, and compared to the shallow architecture\nof the restricted Boltzmann machine. We benchmark the models, focussing on the\naccuracy of generating energetic observables near the phase transition, where\nthese quantities are most difficult to approximate. Interestingly, after\ntraining the generative networks, we observe that the accuracy essentially\ndepends only on the number of neurons in the first hidden layer of the network,\nand not on other model details such as network depth or model type. This is\nevidence that shallow networks are more efficient than deep networks at\nrepresenting physical probability distributions associated with Ising systems\nnear criticality. \n\n"}
{"id": "1708.04781", "contents": "Title: Racing Thompson: an Efficient Algorithm for Thompson Sampling with\n  Non-conjugate Priors Abstract: Thompson sampling has impressive empirical performance for many multi-armed\nbandit problems. But current algorithms for Thompson sampling only work for the\ncase of conjugate priors since these algorithms require to infer the posterior,\nwhich is often computationally intractable when the prior is not conjugate. In\nthis paper, we propose a novel algorithm for Thompson sampling which only\nrequires to draw samples from a tractable distribution, so our algorithm is\nefficient even when the prior is non-conjugate. To do this, we reformulate\nThompson sampling as an optimization problem via the Gumbel-Max trick. After\nthat we construct a set of random variables and our goal is to identify the one\nwith highest mean. Finally, we solve it with techniques in best arm\nidentification. \n\n"}
{"id": "1708.04788", "contents": "Title: BitNet: Bit-Regularized Deep Neural Networks Abstract: We present a novel optimization strategy for training neural networks which\nwe call \"BitNet\". The parameters of neural networks are usually unconstrained\nand have a dynamic range dispersed over all real values. Our key idea is to\nlimit the expressive power of the network by dynamically controlling the range\nand set of values that the parameters can take. We formulate this idea using a\nnovel end-to-end approach that circumvents the discrete parameter space by\noptimizing a relaxed continuous and differentiable upper bound of the typical\nclassification loss function. The approach can be interpreted as a\nregularization inspired by the Minimum Description Length (MDL) principle. For\neach layer of the network, our approach optimizes real-valued translation and\nscaling factors and arbitrary precision integer-valued parameters (weights). We\nempirically compare BitNet to an equivalent unregularized model on the MNIST\nand CIFAR-10 datasets. We show that BitNet converges faster to a superior\nquality solution. Additionally, the resulting model has significant savings in\nmemory due to the use of integer-valued parameters. \n\n"}
{"id": "1708.05033", "contents": "Title: Corrupt Bandits for Preserving Local Privacy Abstract: We study a variant of the stochastic multi-armed bandit (MAB) problem in\nwhich the rewards are corrupted. In this framework, motivated by privacy\npreservation in online recommender systems, the goal is to maximize the sum of\nthe (unobserved) rewards, based on the observation of transformation of these\nrewards through a stochastic corruption process with known parameters. We\nprovide a lower bound on the expected regret of any bandit algorithm in this\ncorrupted setting. We devise a frequentist algorithm, KLUCB-CF, and a Bayesian\nalgorithm, TS-CF and give upper bounds on their regret. We also provide the\nappropriate corruption parameters to guarantee a desired level of local privacy\nand analyze how this impacts the regret. Finally, we present some experimental\nresults that confirm our analysis. \n\n"}
{"id": "1708.05123", "contents": "Title: Deep & Cross Network for Ad Click Predictions Abstract: Feature engineering has been the key to the success of many prediction\nmodels. However, the process is non-trivial and often requires manual feature\nengineering or exhaustive searching. DNNs are able to automatically learn\nfeature interactions; however, they generate all the interactions implicitly,\nand are not necessarily efficient in learning all types of cross features. In\nthis paper, we propose the Deep & Cross Network (DCN) which keeps the benefits\nof a DNN model, and beyond that, it introduces a novel cross network that is\nmore efficient in learning certain bounded-degree feature interactions. In\nparticular, DCN explicitly applies feature crossing at each layer, requires no\nmanual feature engineering, and adds negligible extra complexity to the DNN\nmodel. Our experimental results have demonstrated its superiority over the\nstate-of-art algorithms on the CTR prediction dataset and dense classification\ndataset, in terms of both model accuracy and memory usage. \n\n"}
{"id": "1708.06020", "contents": "Title: Improving Deep Learning using Generic Data Augmentation Abstract: Deep artificial neural networks require a large corpus of training data in\norder to effectively learn, where collection of such training data is often\nexpensive and laborious. Data augmentation overcomes this issue by artificially\ninflating the training set with label preserving transformations. Recently\nthere has been extensive use of generic data augmentation to improve\nConvolutional Neural Network (CNN) task performance. This study benchmarks\nvarious popular data augmentation schemes to allow researchers to make informed\ndecisions as to which training methods are most appropriate for their data\nsets. Various geometric and photometric schemes are evaluated on a\ncoarse-grained data set using a relatively simple CNN. Experimental results,\nrun using 4-fold cross-validation and reported in terms of Top-1 and Top-5\naccuracy, indicate that cropping in geometric augmentation significantly\nincreases CNN task performance. \n\n"}
{"id": "1708.07164", "contents": "Title: Newton-Type Methods for Non-Convex Optimization Under Inexact Hessian\n  Information Abstract: We consider variants of trust-region and cubic regularization methods for\nnon-convex optimization, in which the Hessian matrix is approximated. Under\nmild conditions on the inexact Hessian, and using approximate solution of the\ncorresponding sub-problems, we provide iteration complexity to achieve $\n\\epsilon $-approximate second-order optimality which have shown to be tight.\nOur Hessian approximation conditions constitute a major relaxation over the\nexisting ones in the literature. Consequently, we are able to show that such\nmild conditions allow for the construction of the approximate Hessian through\nvarious random sampling methods. In this light, we consider the canonical\nproblem of finite-sum minimization, provide appropriate uniform and non-uniform\nsub-sampling strategies to construct such Hessian approximations, and obtain\noptimal iteration complexity for the corresponding sub-sampled trust-region and\ncubic regularization methods. \n\n"}
{"id": "1708.08917", "contents": "Title: CirCNN: Accelerating and Compressing Deep Neural Networks Using\n  Block-CirculantWeight Matrices Abstract: Large-scale deep neural networks (DNNs) are both compute and memory\nintensive. As the size of DNNs continues to grow, it is critical to improve the\nenergy efficiency and performance while maintaining accuracy. For DNNs, the\nmodel size is an important factor affecting performance, scalability and energy\nefficiency. Weight pruning achieves good compression ratios but suffers from\nthree drawbacks: 1) the irregular network structure after pruning; 2) the\nincreased training complexity; and 3) the lack of rigorous guarantee of\ncompression ratio and inference accuracy. To overcome these limitations, this\npaper proposes CirCNN, a principled approach to represent weights and process\nneural networks using block-circulant matrices. CirCNN utilizes the Fast\nFourier Transform (FFT)-based fast multiplication, simultaneously reducing the\ncomputational complexity (both in inference and training) from O(n2) to\nO(nlogn) and the storage complexity from O(n2) to O(n), with negligible\naccuracy loss. Compared to other approaches, CirCNN is distinct due to its\nmathematical rigor: it can converge to the same effectiveness as DNNs without\ncompression. The CirCNN architecture, a universal DNN inference engine that can\nbe implemented on various hardware/software platforms with configurable network\narchitecture. To demonstrate the performance and energy efficiency, we test\nCirCNN in FPGA, ASIC and embedded processors. Our results show that CirCNN\narchitecture achieves very high energy efficiency and performance with a small\nhardware footprint. Based on the FPGA implementation and ASIC synthesis\nresults, CirCNN achieves 6-102X energy efficiency improvements compared with\nthe best state-of-the-art results. \n\n"}
{"id": "1708.09757", "contents": "Title: Opportunities and challenges for quantum-assisted machine learning in\n  near-term quantum computers Abstract: With quantum computing technologies nearing the era of commercialization and\nquantum supremacy, machine learning (ML) appears as one of the promising\n\"killer\" applications. Despite significant effort, there has been a disconnect\nbetween most quantum ML proposals, the needs of ML practitioners, and the\ncapabilities of near-term quantum devices to demonstrate quantum enhancement in\nthe near future. In this contribution to the focus collection on \"What would\nyou do with 1000 qubits?\", we provide concrete examples of intractable ML tasks\nthat could be enhanced with near-term devices. We argue that to reach this\ntarget, the focus should be on areas where ML researchers are struggling, such\nas generative models in unsupervised and semi-supervised learning, instead of\nthe popular and more tractable supervised learning techniques. We also\nhighlight the case of classical datasets with potential quantum-like\nstatistical correlations where quantum models could be more suitable. We focus\non hybrid quantum-classical approaches and illustrate some of the key\nchallenges we foresee for near-term implementations. Finally, we introduce the\nquantum-assisted Helmholtz machine (QAHM), an attempt to use near-term quantum\ndevices to tackle high-dimensional datasets of continuous variables. Instead of\nusing quantum computers to assist deep learning, as previous approaches do, the\nQAHM uses deep learning to extract a low-dimensional binary representation of\ndata, suitable for relatively small quantum processors which can assist the\ntraining of an unsupervised generative model. Although we illustrate this\nconcept on a quantum annealer, other quantum platforms could benefit as well\nfrom this hybrid quantum-classical framework. \n\n"}
{"id": "1708.09784", "contents": "Title: Quantum-assisted Helmholtz machines: A quantum-classical deep learning\n  framework for industrial datasets in near-term devices Abstract: Machine learning has been presented as one of the key applications for\nnear-term quantum technologies, given its high commercial value and wide range\nof applicability. In this work, we introduce the \\textit{quantum-assisted\nHelmholtz machine:} a hybrid quantum-classical framework with the potential of\ntackling high-dimensional real-world machine learning datasets on continuous\nvariables. Instead of using quantum computers only to assist deep learning, as\nprevious approaches have suggested, we use deep learning to extract a\nlow-dimensional binary representation of data, suitable for processing on\nrelatively small quantum computers. Then, the quantum hardware and deep\nlearning architecture work together to train an unsupervised generative model.\nWe demonstrate this concept using 1644 quantum bits of a D-Wave 2000Q quantum\ndevice to model a sub-sampled version of the MNIST handwritten digit dataset\nwith 16x16 continuous valued pixels. Although we illustrate this concept on a\nquantum annealer, adaptations to other quantum platforms, such as ion-trap\ntechnologies or superconducting gate-model architectures, could be explored\nwithin this flexible framework. \n\n"}
{"id": "1709.01867", "contents": "Title: Neural Networks Regularization Through Class-wise Invariant\n  Representation Learning Abstract: Training deep neural networks is known to require a large number of training\nsamples. However, in many applications only few training samples are available.\nIn this work, we tackle the issue of training neural networks for\nclassification task when few training samples are available. We attempt to\nsolve this issue by proposing a new regularization term that constrains the\nhidden layers of a network to learn class-wise invariant representations. In\nour regularization framework, learning invariant representations is generalized\nto the class membership where samples with the same class should have the same\nrepresentation. Numerical experiments over MNIST and its variants showed that\nour proposal helps improving the generalization of neural network particularly\nwhen trained with few samples. We provide the source code of our framework\nhttps://github.com/sbelharbi/learning-class-invariant-features . \n\n"}
{"id": "1709.01870", "contents": "Title: Clustering of Data with Missing Entries using Non-convex Fusion\n  Penalties Abstract: The presence of missing entries in data often creates challenges for pattern\nrecognition algorithms. Traditional algorithms for clustering data assume that\nall the feature values are known for every data point. We propose a method to\ncluster data in the presence of missing information. Unlike conventional\nclustering techniques where every feature is known for each point, our\nalgorithm can handle cases where a few feature values are unknown for every\npoint. For this more challenging problem, we provide theoretical guarantees for\nclustering using a $\\ell_0$ fusion penalty based optimization problem.\nFurthermore, we propose an algorithm to solve a relaxation of this problem\nusing saturating non-convex fusion penalties. It is observed that this\nalgorithm produces solutions that degrade gradually with an increase in the\nfraction of missing feature values. We demonstrate the utility of the proposed\nmethod using a simulated dataset, the Wine dataset and also an under-sampled\ncardiac MRI dataset. It is shown that the proposed method is a promising\nclustering technique for datasets with large fractions of missing entries. \n\n"}
{"id": "1709.01921", "contents": "Title: Distributed Deep Neural Networks over the Cloud, the Edge and End\n  Devices Abstract: We propose distributed deep neural networks (DDNNs) over distributed\ncomputing hierarchies, consisting of the cloud, the edge (fog) and end devices.\nWhile being able to accommodate inference of a deep neural network (DNN) in the\ncloud, a DDNN also allows fast and localized inference using shallow portions\nof the neural network at the edge and end devices. When supported by a scalable\ndistributed computing hierarchy, a DDNN can scale up in neural network size and\nscale out in geographical span. Due to its distributed nature, DDNNs enhance\nsensor fusion, system fault tolerance and data privacy for DNN applications. In\nimplementing a DDNN, we map sections of a DNN onto a distributed computing\nhierarchy. By jointly training these sections, we minimize communication and\nresource usage for devices and maximize usefulness of extracted features which\nare utilized in the cloud. The resulting system has built-in support for\nautomatic sensor fusion and fault tolerance. As a proof of concept, we show a\nDDNN can exploit geographical diversity of sensors to improve object\nrecognition accuracy and reduce communication cost. In our experiment, compared\nwith the traditional method of offloading raw sensor data to be processed in\nthe cloud, DDNN locally processes most sensor data on end devices while\nachieving high accuracy and is able to reduce the communication cost by a\nfactor of over 20x. \n\n"}
{"id": "1709.04301", "contents": "Title: Quantum error correction for non-maximally entangled states Abstract: Quantum states have high affinity for errors and hence error correction is of\nutmost importance to realise a quantum computer. Laflamme showed that 5 qubits\nare necessary to correct a single error on a qubit. In a Pauli error model,\nfour different types of errors can occur on a qubit. Maximally entangled states\nare orthogonal to each other and hence can be uniquely distinguished by a\nmeasurement in the Bell basis. Thus a measurement in Bell basis and a unitary\ntransformation is sufficient to correct error in Bell states. However, such a\nmeasurement is not possible for non-maximally entangled states. In this work we\nshow that the 16 possible errors for a non-maximally entangled two qubit system\nmap to only 8 distinct error states. Hence, it is possible to correct the error\nwithout perfect knowledge of the type of error. Furthermore, we show that the\npossible errors can be grouped in such a way that all 4 errors can occur on one\nqubit, whereas only bit flip error can occur on the second qubit. As a\nconsequence, instead of 10, only 8 qubits are sufficient to correct a single\nerror. We propose an 8-qubit error correcting code to correct a single error in\na non-maximally entangled state. We further argue that for an $n$-qubit\nnon-maximally entangled state of the form $a|0>^{n} + b|1>^{n}$, it is always\npossible to correct a single error with fewer than $5n$ qubits, in fact only\n$3n+2$ qubits suffice. \n\n"}
{"id": "1709.06030", "contents": "Title: N2N Learning: Network to Network Compression via Policy Gradient\n  Reinforcement Learning Abstract: While bigger and deeper neural network architectures continue to advance the\nstate-of-the-art for many computer vision tasks, real-world adoption of these\nnetworks is impeded by hardware and speed constraints. Conventional model\ncompression methods attempt to address this problem by modifying the\narchitecture manually or using pre-defined heuristics. Since the space of all\nreduced architectures is very large, modifying the architecture of a deep\nneural network in this way is a difficult task. In this paper, we tackle this\nissue by introducing a principled method for learning reduced network\narchitectures in a data-driven way using reinforcement learning. Our approach\ntakes a larger `teacher' network as input and outputs a compressed `student'\nnetwork derived from the `teacher' network. In the first stage of our method, a\nrecurrent policy network aggressively removes layers from the large `teacher'\nmodel. In the second stage, another recurrent policy network carefully reduces\nthe size of each remaining layer. The resulting network is then evaluated to\nobtain a reward -- a score based on the accuracy and compression of the\nnetwork. Our approach uses this reward signal with policy gradients to train\nthe policies to find a locally optimal student network. Our experiments show\nthat we can achieve compression rates of more than 10x for models such as\nResNet-34 while maintaining similar performance to the input `teacher' network.\nWe also present a valuable transfer learning result which shows that policies\nwhich are pre-trained on smaller `teacher' networks can be used to rapidly\nspeed up training on larger `teacher' networks. \n\n"}
{"id": "1709.06390", "contents": "Title: Analogical-based Bayesian Optimization Abstract: Some real-world problems revolve to solve the optimization problem\n\\max_{x\\in\\mathcal{X}}f\\left(x\\right) where f\\left(.\\right) is a black-box\nfunction and X might be the set of non-vectorial objects (e.g., distributions)\nwhere we can only define a symmetric and non-negative similarity score on it.\nThis setting requires a novel view for the standard framework of Bayesian\nOptimization that generalizes the core insightful spirit of this framework.\nWith this spirit, in this paper, we propose Analogical-based Bayesian\nOptimization that can maximize black-box function over a domain where only a\nsimilarity score can be defined. Our pathway is as follows: we first base on\nthe geometric view of Gaussian Processes (GP) to define the concept of\ninfluence level that allows us to analytically represent predictive means and\nvariances of GP posteriors and base on that view to enable replacing kernel\nsimilarity by a more genetic similarity score. Furthermore, we also propose two\nstrategies to find a batch of query points that can efficiently handle high\ndimensional data. \n\n"}
{"id": "1709.06404", "contents": "Title: Interactive Music Generation with Positional Constraints using\n  Anticipation-RNNs Abstract: Recurrent Neural Networks (RNNS) are now widely used on sequence generation\ntasks due to their ability to learn long-range dependencies and to generate\nsequences of arbitrary length. However, their left-to-right generation\nprocedure only allows a limited control from a potential user which makes them\nunsuitable for interactive and creative usages such as interactive music\ngeneration. This paper introduces a novel architecture called Anticipation-RNN\nwhich possesses the assets of the RNN-based generative models while allowing to\nenforce user-defined positional constraints. We demonstrate its efficiency on\nthe task of generating melodies satisfying positional constraints in the style\nof the soprano parts of the J.S. Bach chorale harmonizations. Sampling using\nthe Anticipation-RNN is of the same order of complexity than sampling from the\ntraditional RNN model. This fast and interactive generation of musical\nsequences opens ways to devise real-time systems that could be used for\ncreative purposes. \n\n"}
{"id": "1709.07124", "contents": "Title: Deep Recurrent NMF for Speech Separation by Unfolding Iterative\n  Thresholding Abstract: In this paper, we propose a novel recurrent neural network architecture for\nspeech separation. This architecture is constructed by unfolding the iterations\nof a sequential iterative soft-thresholding algorithm (ISTA) that solves the\noptimization problem for sparse nonnegative matrix factorization (NMF) of\nspectrograms. We name this network architecture deep recurrent NMF (DR-NMF).\nThe proposed DR-NMF network has three distinct advantages. First, DR-NMF\nprovides better interpretability than other deep architectures, since the\nweights correspond to NMF model parameters, even after training. This\ninterpretability also provides principled initializations that enable faster\ntraining and convergence to better solutions compared to conventional random\ninitialization. Second, like many deep networks, DR-NMF is an order of\nmagnitude faster at test time than NMF, since computation of the network output\nonly requires evaluating a few layers at each time step. Third, when a limited\namount of training data is available, DR-NMF exhibits stronger generalization\nand separation performance compared to sparse NMF and state-of-the-art\nlong-short term memory (LSTM) networks. When a large amount of training data is\navailable, DR-NMF achieves lower yet competitive separation performance\ncompared to LSTM networks. \n\n"}
{"id": "1709.07172", "contents": "Title: SpectralLeader: Online Spectral Learning for Single Topic Models Abstract: We study the problem of learning a latent variable model from a stream of\ndata. Latent variable models are popular in practice because they can explain\nobserved data in terms of unobserved concepts. These models have been\ntraditionally studied in the offline setting. In the online setting, on the\nother hand, the online EM is arguably the most popular algorithm for learning\nlatent variable models. Although the online EM is computationally efficient, it\ntypically converges to a local optimum. In this work, we develop a new online\nlearning algorithm for latent variable models, which we call SpectralLeader.\nSpectralLeader always converges to the global optimum, and we derive a\nsublinear upper bound on its $n$-step regret in the bag-of-words model. In both\nsynthetic and real-world experiments, we show that SpectralLeader performs\nsimilarly to or better than the online EM with tuned hyper-parameters. \n\n"}
{"id": "1709.07174", "contents": "Title: Agile Autonomous Driving using End-to-End Deep Imitation Learning Abstract: We present an end-to-end imitation learning system for agile, off-road\nautonomous driving using only low-cost sensors. By imitating a model predictive\ncontroller equipped with advanced sensors, we train a deep neural network\ncontrol policy to map raw, high-dimensional observations to continuous steering\nand throttle commands. Compared with recent approaches to similar tasks, our\nmethod requires neither state estimation nor on-the-fly planning to navigate\nthe vehicle. Our approach relies on, and experimentally validates, recent\nimitation learning theory. Empirically, we show that policies trained with\nonline imitation learning overcome well-known challenges related to covariate\nshift and generalize better than policies trained with batch imitation\nlearning. Built on these insights, our autonomous driving system demonstrates\nsuccessful high-speed off-road driving, matching the state-of-the-art\nperformance. \n\n"}
{"id": "1709.08201", "contents": "Title: An Optimal Online Method of Selecting Source Policies for Reinforcement\n  Learning Abstract: Transfer learning significantly accelerates the reinforcement learning\nprocess by exploiting relevant knowledge from previous experiences. The problem\nof optimally selecting source policies during the learning process is of great\nimportance yet challenging. There has been little theoretical analysis of this\nproblem. In this paper, we develop an optimal online method to select source\npolicies for reinforcement learning. This method formulates online source\npolicy selection as a multi-armed bandit problem and augments Q-learning with\npolicy reuse. We provide theoretical guarantees of the optimal selection\nprocess and convergence to the optimal policy. In addition, we conduct\nexperiments on a grid-based robot navigation domain to demonstrate its\nefficiency and robustness by comparing to the state-of-the-art transfer\nlearning method. \n\n"}
{"id": "1709.09844", "contents": "Title: Distance-based Confidence Score for Neural Network Classifiers Abstract: The reliable measurement of confidence in classifiers' predictions is very\nimportant for many applications and is, therefore, an important part of\nclassifier design. Yet, although deep learning has received tremendous\nattention in recent years, not much progress has been made in quantifying the\nprediction confidence of neural network classifiers. Bayesian models offer a\nmathematically grounded framework to reason about model uncertainty, but\nusually come with prohibitive computational costs. In this paper we propose a\nsimple, scalable method to achieve a reliable confidence score, based on the\ndata embedding derived from the penultimate layer of the network. We\ninvestigate two ways to achieve desirable embeddings, by using either a\ndistance-based loss or Adversarial Training. We then test the benefits of our\nmethod when used for classification error prediction, weighting an ensemble of\nclassifiers, and novelty detection. In all tasks we show significant\nimprovement over traditional, commonly used confidence scores. \n\n"}
{"id": "1709.09929", "contents": "Title: SUBIC: A Supervised Bi-Clustering Approach for Precision Medicine Abstract: Traditional medicine typically applies one-size-fits-all treatment for the\nentire patient population whereas precision medicine develops tailored\ntreatment schemes for different patient subgroups. The fact that some factors\nmay be more significant for a specific patient subgroup motivates clinicians\nand medical researchers to develop new approaches to subgroup detection and\nanalysis, which is an effective strategy to personalize treatment. In this\nstudy, we propose a novel patient subgroup detection method, called Supervised\nBiclustring (SUBIC) using convex optimization and apply our approach to detect\npatient subgroups and prioritize risk factors for hypertension (HTN) in a\nvulnerable demographic subgroup (African-American). Our approach not only finds\npatient subgroups with guidance of a clinically relevant target variable but\nalso identifies and prioritizes risk factors by pursuing sparsity of the input\nvariables and encouraging similarity among the input variables and between the\ninput and target variables \n\n"}
{"id": "1710.01408", "contents": "Title: A Fully Convolutional Network for Semantic Labeling of 3D Point Clouds Abstract: When classifying point clouds, a large amount of time is devoted to the\nprocess of engineering a reliable set of features which are then passed to a\nclassifier of choice. Generally, such features - usually derived from the\n3D-covariance matrix - are computed using the surrounding neighborhood of\npoints. While these features capture local information, the process is usually\ntime-consuming, and requires the application at multiple scales combined with\ncontextual methods in order to adequately describe the diversity of objects\nwithin a scene. In this paper we present a 1D-fully convolutional network that\nconsumes terrain-normalized points directly with the corresponding spectral\ndata,if available, to generate point-wise labeling while implicitly learning\ncontextual features in an end-to-end fashion. Our method uses only the\n3D-coordinates and three corresponding spectral features for each point.\nSpectral features may either be extracted from 2D-georeferenced images, as\nshown here for Light Detection and Ranging (LiDAR) point clouds, or extracted\ndirectly for passive-derived point clouds,i.e. from muliple-view imagery. We\ntrain our network by splitting the data into square regions, and use a pooling\nlayer that respects the permutation-invariance of the input points. Evaluated\nusing the ISPRS 3D Semantic Labeling Contest, our method scored second place\nwith an overall accuracy of 81.6%. We ranked third place with a mean F1-score\nof 63.32%, surpassing the F1-score of the method with highest accuracy by\n1.69%. In addition to labeling 3D-point clouds, we also show that our method\ncan be easily extended to 2D-semantic segmentation tasks, with promising\ninitial results. \n\n"}
{"id": "1710.02277", "contents": "Title: Efficient K-Shot Learning with Regularized Deep Networks Abstract: Feature representations from pre-trained deep neural networks have been known\nto exhibit excellent generalization and utility across a variety of related\ntasks. Fine-tuning is by far the simplest and most widely used approach that\nseeks to exploit and adapt these feature representations to novel tasks with\nlimited data. Despite the effectiveness of fine-tuning, itis often sub-optimal\nand requires very careful optimization to prevent severe over-fitting to small\ndatasets. The problem of sub-optimality and over-fitting, is due in part to the\nlarge number of parameters used in a typical deep convolutional neural network.\nTo address these problems, we propose a simple yet effective regularization\nmethod for fine-tuning pre-trained deep networks for the task of k-shot\nlearning. To prevent overfitting, our key strategy is to cluster the model\nparameters while ensuring intra-cluster similarity and inter-cluster diversity\nof the parameters, effectively regularizing the dimensionality of the parameter\nsearch space. In particular, we identify groups of neurons within each layer of\na deep network that shares similar activation patterns. When the network is to\nbe fine-tuned for a classification task using only k examples, we propagate a\nsingle gradient to all of the neuron parameters that belong to the same group.\nThe grouping of neurons is non-trivial as neuron activations depend on the\ndistribution of the input data. To efficiently search for optimal groupings\nconditioned on the input data, we propose a reinforcement learning search\nstrategy using recurrent networks to learn the optimal group assignments for\neach network layer. Experimental results show that our method can be easily\napplied to several popular convolutional neural networks and improve upon other\nstate-of-the-art fine-tuning based k-shot learning strategies by more than10% \n\n"}
{"id": "1710.02971", "contents": "Title: Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE,\n  and node2vec Abstract: Since the invention of word2vec, the skip-gram model has significantly\nadvanced the research of network embedding, such as the recent emergence of the\nDeepWalk, LINE, PTE, and node2vec approaches. In this work, we show that all of\nthe aforementioned models with negative sampling can be unified into the matrix\nfactorization framework with closed forms. Our analysis and proofs reveal that:\n(1) DeepWalk empirically produces a low-rank transformation of a network's\nnormalized Laplacian matrix; (2) LINE, in theory, is a special case of DeepWalk\nwhen the size of vertices' context is set to one; (3) As an extension of LINE,\nPTE can be viewed as the joint factorization of multiple networks' Laplacians;\n(4) node2vec is factorizing a matrix related to the stationary distribution and\ntransition probability tensor of a 2nd-order random walk. We further provide\nthe theoretical connections between skip-gram based network embedding\nalgorithms and the theory of graph Laplacian. Finally, we present the NetMF\nmethod as well as its approximation algorithm for computing network embedding.\nOur method offers significant improvements over DeepWalk and LINE for\nconventional network mining tasks. This work lays the theoretical foundation\nfor skip-gram based network embedding methods, leading to a better\nunderstanding of latent network representation learning. \n\n"}
{"id": "1710.03285", "contents": "Title: Coresets for Dependency Networks Abstract: Many applications infer the structure of a probabilistic graphical model from\ndata to elucidate the relationships between variables. But how can we train\ngraphical models on a massive data set? In this paper, we show how to construct\ncoresets -compressed data sets which can be used as proxy for the original data\nand have provably bounded worst case error- for Gaussian dependency networks\n(DNs), i.e., cyclic directed graphical models over Gaussians, where the parents\nof each variable are its Markov blanket. Specifically, we prove that Gaussian\nDNs admit coresets of size independent of the size of the data set.\nUnfortunately, this does not extend to DNs over members of the exponential\nfamily in general. As we will prove, Poisson DNs do not admit small coresets.\nDespite this worst-case result, we will provide an argument why our coreset\nconstruction for DNs can still work well in practice on count data. To\ncorroborate our theoretical results, we empirically evaluated the resulting\nCore DNs on real data sets. The results \n\n"}
{"id": "1710.03297", "contents": "Title: Sum-Product Networks for Hybrid Domains Abstract: While all kinds of mixed data -from personal data, over panel and scientific\ndata, to public and commercial data- are collected and stored, building\nprobabilistic graphical models for these hybrid domains becomes more difficult.\nUsers spend significant amounts of time in identifying the parametric form of\nthe random variables (Gaussian, Poisson, Logit, etc.) involved and learning the\nmixed models. To make this difficult task easier, we propose the first\ntrainable probabilistic deep architecture for hybrid domains that features\ntractable queries. It is based on Sum-Product Networks (SPNs) with piecewise\npolynomial leave distributions together with novel nonparametric decomposition\nand conditioning steps using the Hirschfeld-Gebelein-R\\'enyi Maximum\nCorrelation Coefficient. This relieves the user from deciding a-priori the\nparametric form of the random variables but is still expressive enough to\neffectively approximate any continuous distribution and permits efficient\nlearning and inference. Our empirical evidence shows that the architecture,\ncalled Mixed SPNs, can indeed capture complex distributions across a wide range\nof hybrid domains. \n\n"}
{"id": "1710.03487", "contents": "Title: An Analysis of Dropout for Matrix Factorization Abstract: Dropout is a simple yet effective algorithm for regularizing neural networks\nby randomly dropping out units through Bernoulli multiplicative noise, and for\nsome restricted problem classes, such as linear or logistic regression, several\ntheoretical studies have demonstrated the equivalence between dropout and a\nfully deterministic optimization problem with data-dependent Tikhonov\nregularization. This work presents a theoretical analysis of dropout for matrix\nfactorization, where Bernoulli random variables are used to drop a factor,\nthereby attempting to control the size of the factorization. While recent work\nhas demonstrated the empirical effectiveness of dropout for matrix\nfactorization, a theoretical understanding of the regularization properties of\ndropout in this context remains elusive. This work demonstrates the equivalence\nbetween dropout and a fully deterministic model for matrix factorization in\nwhich the factors are regularized by the sum of the product of the norms of the\ncolumns. While the resulting regularizer is closely related to a variational\nform of the nuclear norm, suggesting that dropout may limit the size of the\nfactorization, we show that it is possible to trivially lower the objective\nvalue by doubling the size of the factorization. We show that this problem is\ncaused by the use of a fixed dropout rate, which motivates the use of a rate\nthat increases with the size of the factorization. Synthetic experiments\nvalidate our theoretical findings. \n\n"}
{"id": "1710.03600", "contents": "Title: Fast and Strong Convergence of Online Learning Algorithms Abstract: In this paper, we study the online learning algorithm without explicit\nregularization terms. This algorithm is essentially a stochastic gradient\ndescent scheme in a reproducing kernel Hilbert space (RKHS). The polynomially\ndecaying step size in each iteration can play a role of regularization to\nensure the generalization ability of online learning algorithm. We develop a\nnovel capacity dependent analysis on the performance of the last iterate of\nonline learning algorithm. The contribution of this paper is two-fold. First,\nour nice analysis can lead to the convergence rate in the standard mean square\ndistance which is the best so far. Second, we establish, for the first time,\nthe strong convergence of the last iterate with polynomially decaying step\nsizes in the RKHS norm. We demonstrate that the theoretical analysis\nestablished in this paper fully exploits the fine structure of the underlying\nRKHS, and thus can lead to sharp error estimates of online learning algorithm. \n\n"}
{"id": "1710.04881", "contents": "Title: User Modelling for Avoiding Overfitting in Interactive Knowledge\n  Elicitation for Prediction Abstract: In human-in-the-loop machine learning, the user provides information beyond\nthat in the training data. Many algorithms and user interfaces have been\ndesigned to optimize and facilitate this human--machine interaction; however,\nfewer studies have addressed the potential defects the designs can cause.\nEffective interaction often requires exposing the user to the training data or\nits statistics. The design of the system is then critical, as this can lead to\ndouble use of data and overfitting, if the user reinforces noisy patterns in\nthe data. We propose a user modelling methodology, by assuming simple rational\nbehaviour, to correct the problem. We show, in a user study with 48\nparticipants, that the method improves predictive performance in a sparse\nlinear regression sentiment analysis task, where graded user knowledge on\nfeature relevance is elicited. We believe that the key idea of inferring user\nknowledge with probabilistic user models has general applicability in guarding\nagainst overfitting and improving interactive machine learning. \n\n"}
{"id": "1710.05092", "contents": "Title: Dropout as a Low-Rank Regularizer for Matrix Factorization Abstract: Regularization for matrix factorization (MF) and approximation problems has\nbeen carried out in many different ways. Due to its popularity in deep\nlearning, dropout has been applied also for this class of problems. Despite its\nsolid empirical performance, the theoretical properties of dropout as a\nregularizer remain quite elusive for this class of problems. In this paper, we\npresent a theoretical analysis of dropout for MF, where Bernoulli random\nvariables are used to drop columns of the factors. We demonstrate the\nequivalence between dropout and a fully deterministic model for MF in which the\nfactors are regularized by the sum of the product of squared Euclidean norms of\nthe columns. Additionally, we inspect the case of a variable sized\nfactorization and we prove that dropout achieves the global minimum of a convex\napproximation problem with (squared) nuclear norm regularization. As a result,\nwe conclude that dropout can be used as a low-rank regularizer with data\ndependent singular-value thresholding. \n\n"}
{"id": "1710.05488", "contents": "Title: A Geometric View of Optimal Transportation and Generative Model Abstract: In this work, we show the intrinsic relations between optimal transportation\nand convex geometry, especially the variational approach to solve Alexandrov\nproblem: constructing a convex polytope with prescribed face normals and\nvolumes. This leads to a geometric interpretation to generative models, and\nleads to a novel framework for generative models. By using the optimal\ntransportation view of GAN model, we show that the discriminator computes the\nKantorovich potential, the generator calculates the transportation map. For a\nlarge class of transportation costs, the Kantorovich potential can give the\noptimal transportation map by a close-form formula. Therefore, it is sufficient\nto solely optimize the discriminator. This shows the adversarial competition\ncan be avoided, and the computational architecture can be simplified.\nPreliminary experimental results show the geometric method outperforms WGAN for\napproximating probability measures with multiple clusters in low dimensional\nspace. \n\n"}
{"id": "1710.06703", "contents": "Title: Function Norms and Regularization in Deep Networks Abstract: Deep neural networks (DNNs) have become increasingly important due to their\nexcellent empirical performance on a wide range of problems. However,\nregularization is generally achieved by indirect means, largely due to the\ncomplex set of functions defined by a network and the difficulty in measuring\nfunction complexity. There exists no method in the literature for additive\nregularization based on a norm of the function, as is classically considered in\nstatistical learning theory. In this work, we propose sampling-based\napproximations to weighted function norms as regularizers for deep neural\nnetworks. We provide, to the best of our knowledge, the first proof in the\nliterature of the NP-hardness of computing function norms of DNNs, motivating\nthe necessity of an approximate approach. We then derive a generalization bound\nfor functions trained with weighted norms and prove that a natural stochastic\noptimization strategy minimizes the bound. Finally, we empirically validate the\nimproved performance of the proposed regularization strategies for both convex\nfunction sets as well as DNNs on real-world classification and image\nsegmentation tasks demonstrating improved performance over weight decay,\ndropout, and batch normalization. Source code will be released at the time of\npublication. \n\n"}
{"id": "1710.06900", "contents": "Title: Temporally-Reweighted Chinese Restaurant Process Mixtures for\n  Clustering, Imputing, and Forecasting Multivariate Time Series Abstract: This article proposes a Bayesian nonparametric method for forecasting,\nimputation, and clustering in sparsely observed, multivariate time series data.\nThe method is appropriate for jointly modeling hundreds of time series with\nwidely varying, non-stationary dynamics. Given a collection of $N$ time series,\nthe Bayesian model first partitions them into independent clusters using a\nChinese restaurant process prior. Within a cluster, all time series are modeled\njointly using a novel \"temporally-reweighted\" extension of the Chinese\nrestaurant process mixture. Markov chain Monte Carlo techniques are used to\nobtain samples from the posterior distribution, which are then used to form\npredictive inferences. We apply the technique to challenging forecasting and\nimputation tasks using seasonal flu data from the US Center for Disease Control\nand Prevention, demonstrating superior forecasting accuracy and competitive\nimputation accuracy as compared to multiple widely used baselines. We further\nshow that the model discovers interpretable clusters in datasets with hundreds\nof time series, using macroeconomic data from the Gapminder Foundation. \n\n"}
{"id": "1710.07324", "contents": "Title: Scalable Gaussian Processes with Billions of Inducing Inputs via Tensor\n  Train Decomposition Abstract: We propose a method (TT-GP) for approximate inference in Gaussian Process\n(GP) models. We build on previous scalable GP research including stochastic\nvariational inference based on inducing inputs, kernel interpolation, and\nstructure exploiting algebra. The key idea of our method is to use Tensor Train\ndecomposition for variational parameters, which allows us to train GPs with\nbillions of inducing inputs and achieve state-of-the-art results on several\nbenchmarks. Further, our approach allows for training kernels based on deep\nneural networks without any modifications to the underlying GP model. A neural\nnetwork learns a multidimensional embedding for the data, which is used by the\nGP to make the final prediction. We train GP and neural network parameters\nend-to-end without pretraining, through maximization of GP marginal likelihood.\nWe show the efficiency of the proposed approach on several regression and\nclassification benchmark datasets including MNIST, CIFAR-10, and Airline. \n\n"}
{"id": "1710.07345", "contents": "Title: Automated optimization of large quantum circuits with continuous\n  parameters Abstract: We develop and implement automated methods for optimizing quantum circuits of\nthe size and type expected in quantum computations that outperform classical\ncomputers. We show how to handle continuous gate parameters and report a\ncollection of fast algorithms capable of optimizing large-scale quantum\ncircuits. For the suite of benchmarks considered, we obtain substantial\nreductions in gate counts. In particular, we provide better optimization in\nsignificantly less time than previous approaches, while making minimal\nstructural changes so as to preserve the basic layout of the underlying quantum\nalgorithms. Our results help bridge the gap between the computations that can\nbe run on existing hardware and those that are expected to outperform classical\ncomputers. \n\n"}
{"id": "1710.07547", "contents": "Title: Learning compressed representations of blood samples time series with\n  missing data Abstract: Clinical measurements collected over time are naturally represented as\nmultivariate time series (MTS), which often contain missing data. An\nautoencoder can learn low dimensional vectorial representations of MTS that\npreserve important data characteristics, but cannot deal explicitly with\nmissing data. In this work, we propose a new framework that combines an\nautoencoder with the Time series Cluster Kernel (TCK), a kernel that accounts\nfor missingness patterns in MTS. Via kernel alignment, we incorporate TCK in\nthe autoencoder to improve the learned representations in presence of missing\ndata. We consider a classification problem of MTS with missing values,\nrepresenting blood samples of patients with surgical site infection. With our\napproach, rather than with a standard autoencoder, we learn representations in\nlow dimensions that can be classified better. \n\n"}
{"id": "1710.07732", "contents": "Title: A Tight Excess Risk Bound via a Unified\n  PAC-Bayesian-Rademacher-Shtarkov-MDL Complexity Abstract: We present a novel notion of complexity that interpolates between and\ngeneralizes some classic existing complexity notions in learning theory: for\nestimators like empirical risk minimization (ERM) with arbitrary bounded\nlosses, it is upper bounded in terms of data-independent Rademacher complexity;\nfor generalized Bayesian estimators, it is upper bounded by the data-dependent\ninformation complexity (also known as stochastic or PAC-Bayesian,\n$\\mathrm{KL}(\\text{posterior} \\operatorname{\\|} \\text{prior})$ complexity. For\n(penalized) ERM, the new complexity reduces to (generalized) normalized maximum\nlikelihood (NML) complexity, i.e. a minimax log-loss individual-sequence\nregret. Our first main result bounds excess risk in terms of the new\ncomplexity. Our second main result links the new complexity via Rademacher\ncomplexity to $L_2(P)$ entropy, thereby generalizing earlier results of Opper,\nHaussler, Lugosi, and Cesa-Bianchi who did the log-loss case with $L_\\infty$.\nTogether, these results recover optimal bounds for VC- and large (polynomial\nentropy) classes, replacing localized Rademacher complexity by a simpler\nanalysis which almost completely separates the two aspects that determine the\nachievable rates: 'easiness' (Bernstein) conditions and model complexity. \n\n"}
{"id": "1710.07746", "contents": "Title: Stochastic Backward Euler: An Implicit Gradient Descent Algorithm for\n  $k$-means Clustering Abstract: In this paper, we propose an implicit gradient descent algorithm for the\nclassic $k$-means problem. The implicit gradient step or backward Euler is\nsolved via stochastic fixed-point iteration, in which we randomly sample a\nmini-batch gradient in every iteration. It is the average of the fixed-point\ntrajectory that is carried over to the next gradient step. We draw connections\nbetween the proposed stochastic backward Euler and the recent entropy\nstochastic gradient descent (Entropy-SGD) for improving the training of deep\nneural networks. Numerical experiments on various synthetic and real datasets\nshow that the proposed algorithm provides better clustering results compared to\n$k$-means algorithms in the sense that it decreased the objective function (the\ncluster) and is much more robust to initialization. \n\n"}
{"id": "1710.09513", "contents": "Title: Maximum Principle Based Algorithms for Deep Learning Abstract: The continuous dynamical system approach to deep learning is explored in\norder to devise alternative frameworks for training algorithms. Training is\nrecast as a control problem and this allows us to formulate necessary\noptimality conditions in continuous time using the Pontryagin's maximum\nprinciple (PMP). A modification of the method of successive approximations is\nthen used to solve the PMP, giving rise to an alternative training algorithm\nfor deep learning. This approach has the advantage that rigorous error\nestimates and convergence results can be established. We also show that it may\navoid some pitfalls of gradient-based methods, such as slow convergence on flat\nlandscapes near saddle points. Furthermore, we demonstrate that it obtains\nfavorable initial convergence rate per-iteration, provided Hamiltonian\nmaximization can be efficiently carried out - a step which is still in need of\nimprovement. Overall, the approach opens up new avenues to attack problems\nassociated with deep learning, such as trapping in slow manifolds and\ninapplicability of gradient-based methods for discrete trainable variables. \n\n"}
{"id": "1710.10016", "contents": "Title: Regularization via Mass Transportation Abstract: The goal of regression and classification methods in supervised learning is\nto minimize the empirical risk, that is, the expectation of some loss function\nquantifying the prediction error under the empirical distribution. When facing\nscarce training data, overfitting is typically mitigated by adding\nregularization terms to the objective that penalize hypothesis complexity. In\nthis paper we introduce new regularization techniques using ideas from\ndistributionally robust optimization, and we give new probabilistic\ninterpretations to existing techniques. Specifically, we propose to minimize\nthe worst-case expected loss, where the worst case is taken over the ball of\nall (continuous or discrete) distributions that have a bounded transportation\ndistance from the (discrete) empirical distribution. By choosing the radius of\nthis ball judiciously, we can guarantee that the worst-case expected loss\nprovides an upper confidence bound on the loss on test data, thus offering new\ngeneralization bounds. We prove that the resulting regularized learning\nproblems are tractable and can be tractably kernelized for many popular loss\nfunctions. We validate our theoretical out-of-sample guarantees through\nsimulated and empirical experiments. \n\n"}
{"id": "1710.10230", "contents": "Title: Not-So-Random Features Abstract: We propose a principled method for kernel learning, which relies on a\nFourier-analytic characterization of translation-invariant or\nrotation-invariant kernels. Our method produces a sequence of feature maps,\niteratively refining the SVM margin. We provide rigorous guarantees for\noptimality and generalization, interpreting our algorithm as online\nequilibrium-finding dynamics in a certain two-player min-max game. Evaluations\non synthetic and real-world datasets demonstrate scalability and consistent\nimprovements over related random features-based methods. \n\n"}
{"id": "1710.10313", "contents": "Title: A Self-Training Method for Semi-Supervised GANs Abstract: Since the creation of Generative Adversarial Networks (GANs), much work has\nbeen done to improve their training stability, their generated image quality,\ntheir range of application but nearly none of them explored their self-training\npotential. Self-training has been used before the advent of deep learning in\norder to allow training on limited labelled training data and has shown\nimpressive results in semi-supervised learning. In this work, we combine these\ntwo ideas and make GANs self-trainable for semi-supervised learning tasks by\nexploiting their infinite data generation potential. Results show that using\neven the simplest form of self-training yields an improvement. We also show\nresults for a more complex self-training scheme that performs at least as well\nas the basic self-training scheme but with significantly less data\naugmentation. \n\n"}
{"id": "1710.10403", "contents": "Title: Trainable back-propagated functional transfer matrices Abstract: Connections between nodes of fully connected neural networks are usually\nrepresented by weight matrices. In this article, functional transfer matrices\nare introduced as alternatives to the weight matrices: Instead of using real\nweights, a functional transfer matrix uses real functions with trainable\nparameters to represent connections between nodes. Multiple functional transfer\nmatrices are then stacked together with bias vectors and activations to form\ndeep functional transfer neural networks. These neural networks can be trained\nwithin the framework of back-propagation, based on a revision of the delta\nrules and the error transmission rule for functional connections. In\nexperiments, it is demonstrated that the revised rules can be used to train a\nrange of functional connections: 20 different functions are applied to neural\nnetworks with up to 10 hidden layers, and most of them gain high test\naccuracies on the MNIST database. It is also demonstrated that a functional\ntransfer matrix with a memory function can roughly memorise a non-cyclical\nsequence of 400 digits. \n\n"}
{"id": "1710.11253", "contents": "Title: Approximation Algorithms for $\\ell_0$-Low Rank Approximation Abstract: We study the $\\ell_0$-Low Rank Approximation Problem, where the goal is,\ngiven an $m \\times n$ matrix $A$, to output a rank-$k$ matrix $A'$ for which\n$\\|A'-A\\|_0$ is minimized. Here, for a matrix $B$, $\\|B\\|_0$ denotes the number\nof its non-zero entries. This NP-hard variant of low rank approximation is\nnatural for problems with no underlying metric, and its goal is to minimize the\nnumber of disagreeing data positions. We provide approximation algorithms which\nsignificantly improve the running time and approximation factor of previous\nwork. For $k > 1$, we show how to find, in poly$(mn)$ time for every $k$, a\nrank $O(k \\log(n/k))$ matrix $A'$ for which $\\|A'-A\\|_0 \\leq O(k^2 \\log(n/k))\n\\mathrm{OPT}$. To the best of our knowledge, this is the first algorithm with\nprovable guarantees for the $\\ell_0$-Low Rank Approximation Problem for $k >\n1$, even for bicriteria algorithms. For the well-studied case when $k = 1$, we\ngive a $(2+\\epsilon)$-approximation in {\\it sublinear time}, which is\nimpossible for other variants of low rank approximation such as for the\nFrobenius norm. We strengthen this for the well-studied case of binary matrices\nto obtain a $(1+O(\\psi))$-approximation in sublinear time, where $\\psi =\n\\mathrm{OPT}/\\lVert A\\rVert_0$. For small $\\psi$, our approximation factor is\n$1+o(1)$. \n\n"}
{"id": "1710.11303", "contents": "Title: Algorithmic learning of probability distributions from random data in\n  the limit Abstract: We study the problem of identifying a probability distribution for some given\nrandomly sampled data in the limit, in the context of algorithmic learning\ntheory as proposed recently by Vinanyi and Chater. We show that there exists a\ncomputable partial learner for the computable probability measures, while by\nBienvenu, Monin and Shen it is known that there is no computable learner for\nthe computable probability measures. Our main result is the characterization of\nthe oracles that compute explanatory learners for the computable (continuous)\nprobability measures as the high oracles. This provides an analogue of a\nwell-known result of Adleman and Blum in the context of learning computable\nprobability distributions. We also discuss related learning notions such as\nbehaviorally correct learning and orther variations of explanatory learning, in\nthe context of learning probability distributions from data. \n\n"}
{"id": "1711.01243", "contents": "Title: ReBNet: Residual Binarized Neural Network Abstract: This paper proposes ReBNet, an end-to-end framework for training\nreconfigurable binary neural networks on software and developing efficient\naccelerators for execution on FPGA. Binary neural networks offer an intriguing\nopportunity for deploying large-scale deep learning models on\nresource-constrained devices. Binarization reduces the memory footprint and\nreplaces the power-hungry matrix-multiplication with light-weight XnorPopcount\noperations. However, binary networks suffer from a degraded accuracy compared\nto their fixed-point counterparts. We show that the state-of-the-art methods\nfor optimizing binary networks accuracy, significantly increase the\nimplementation cost and complexity. To compensate for the degraded accuracy\nwhile adhering to the simplicity of binary networks, we devise the first\nreconfigurable scheme that can adjust the classification accuracy based on the\napplication. Our proposition improves the classification accuracy by\nrepresenting features with multiple levels of residual binarization. Unlike\nprevious methods, our approach does not exacerbate the area cost of the\nhardware accelerator. Instead, it provides a tradeoff between throughput and\naccuracy while the area overhead of multi-level binarization is negligible. \n\n"}
{"id": "1711.01569", "contents": "Title: Double Q($\\sigma$) and Q($\\sigma, \\lambda$): Unifying Reinforcement\n  Learning Control Algorithms Abstract: Temporal-difference (TD) learning is an important field in reinforcement\nlearning. Sarsa and Q-Learning are among the most used TD algorithms. The\nQ($\\sigma$) algorithm (Sutton and Barto (2017)) unifies both. This paper\nextends the Q($\\sigma$) algorithm to an online multi-step algorithm Q($\\sigma,\n\\lambda$) using eligibility traces and introduces Double Q($\\sigma$) as the\nextension of Q($\\sigma$) to double learning. Experiments suggest that the new\nQ($\\sigma, \\lambda$) algorithm can outperform the classical TD control methods\nSarsa($\\lambda$), Q($\\lambda$) and Q($\\sigma$). \n\n"}
{"id": "1711.02033", "contents": "Title: Estimating Cosmological Parameters from the Dark Matter Distribution Abstract: A grand challenge of the 21st century cosmology is to accurately estimate the\ncosmological parameters of our Universe. A major approach to estimating the\ncosmological parameters is to use the large-scale matter distribution of the\nUniverse. Galaxy surveys provide the means to map out cosmic large-scale\nstructure in three dimensions. Information about galaxy locations is typically\nsummarized in a \"single\" function of scale, such as the galaxy correlation\nfunction or power-spectrum. We show that it is possible to estimate these\ncosmological parameters directly from the distribution of matter. This paper\npresents the application of deep 3D convolutional networks to volumetric\nrepresentation of dark-matter simulations as well as the results obtained using\na recently proposed distribution regression framework, showing that machine\nlearning techniques are comparable to, and can sometimes outperform,\nmaximum-likelihood point estimates using \"cosmological models\". This opens the\nway to estimating the parameters of our Universe with higher accuracy. \n\n"}
{"id": "1711.02771", "contents": "Title: On the Discrimination-Generalization Tradeoff in GANs Abstract: Generative adversarial training can be generally understood as minimizing\ncertain moment matching loss defined by a set of discriminator functions,\ntypically neural networks. The discriminator set should be large enough to be\nable to uniquely identify the true distribution (discriminative), and also be\nsmall enough to go beyond memorizing samples (generalizable). In this paper, we\nshow that a discriminator set is guaranteed to be discriminative whenever its\nlinear span is dense in the set of bounded continuous functions. This is a very\nmild condition satisfied even by neural networks with a single neuron. Further,\nwe develop generalization bounds between the learned distribution and true\ndistribution under different evaluation metrics. When evaluated with neural\ndistance, our bounds show that generalization is guaranteed as long as the\ndiscriminator set is small enough, regardless of the size of the generator or\nhypothesis set. When evaluated with KL divergence, our bound provides an\nexplanation on the counter-intuitive behaviors of testing likelihood in GAN\ntraining. Our analysis sheds lights on understanding the practical performance\nof GANs. \n\n"}
{"id": "1711.03038", "contents": "Title: Recency-weighted Markovian inference Abstract: We describe a Markov latent state space (MLSS) model, where the latent state\ndistribution is a decaying mixture over multiple past states. We present a\nsimple sampling algorithm that allows to approximate such high-order MLSS with\nfixed time and memory costs. \n\n"}
{"id": "1711.03712", "contents": "Title: Quantized Memory-Augmented Neural Networks Abstract: Memory-augmented neural networks (MANNs) refer to a class of neural network\nmodels equipped with external memory (such as neural Turing machines and memory\nnetworks). These neural networks outperform conventional recurrent neural\nnetworks (RNNs) in terms of learning long-term dependency, allowing them to\nsolve intriguing AI tasks that would otherwise be hard to address. This paper\nconcerns the problem of quantizing MANNs. Quantization is known to be effective\nwhen we deploy deep models on embedded systems with limited resources.\nFurthermore, quantization can substantially reduce the energy consumption of\nthe inference procedure. These benefits justify recent developments of\nquantized multi layer perceptrons, convolutional networks, and RNNs. However,\nno prior work has reported the successful quantization of MANNs. The in-depth\nanalysis presented here reveals various challenges that do not appear in the\nquantization of the other networks. Without addressing them properly, quantized\nMANNs would normally suffer from excessive quantization error which leads to\ndegraded performance. In this paper, we identify memory addressing\n(specifically, content-based addressing) as the main reason for the performance\ndegradation and propose a robust quantization method for MANNs to address the\nchallenge. In our experiments, we achieved a computation-energy gain of 22x\nwith 8-bit fixed-point and binary quantization compared to the floating-point\nimplementation. Measured on the bAbI dataset, the resulting model, named the\nquantized MANN (Q-MANN), improved the error rate by 46% and 30% with 8-bit\nfixed-point and binary quantization, respectively, compared to the MANN\nquantized using conventional techniques. \n\n"}
{"id": "1711.04366", "contents": "Title: A unified framework for hard and soft clustering with regularized\n  optimal transport Abstract: In this paper, we formulate the problem of inferring a Finite Mixture Model\nfrom discrete data as an optimal transport problem with entropic regularization\nof parameter $\\lambda\\geq 0$. Our method unifies hard and soft clustering, the\nExpectation-Maximization (EM) algorithm being exactly recovered for\n$\\lambda=1$. The family of clustering algorithm we propose rely on the\nresolution of nonconvex problems using alternating minimization. We study the\nconvergence property of our generalized $\\lambda-$EM algorithms and show that\neach step in the minimization process has a closed form solution when inferring\nfinite mixture models of exponential families. Experiments highlight the\nbenefits of taking a parameter $\\lambda>1$ to improve the inference performance\nand $\\lambda\\to 0$ for classification. \n\n"}
{"id": "1711.04855", "contents": "Title: Modeling Human Categorization of Natural Images Using Deep Feature\n  Representations Abstract: Over the last few decades, psychologists have developed sophisticated formal\nmodels of human categorization using simple artificial stimuli. In this paper,\nwe use modern machine learning methods to extend this work into the realm of\nnaturalistic stimuli, enabling human categorization to be studied over the\ncomplex visual domain in which it evolved and developed. We show that\nrepresentations derived from a convolutional neural network can be used to\nmodel behavior over a database of >300,000 human natural image classifications,\nand find that a group of models based on these representations perform well,\nnear the reliability of human judgments. Interestingly, this group includes\nboth exemplar and prototype models, contrasting with the dominance of exemplar\nmodels in previous work. We are able to improve the performance of the\nremaining models by preprocessing neural network representations to more\nclosely capture human similarity judgments. \n\n"}
{"id": "1711.04894", "contents": "Title: Sobolev GAN Abstract: We propose a new Integral Probability Metric (IPM) between distributions: the\nSobolev IPM. The Sobolev IPM compares the mean discrepancy of two distributions\nfor functions (critic) restricted to a Sobolev ball defined with respect to a\ndominant measure $\\mu$. We show that the Sobolev IPM compares two distributions\nin high dimensions based on weighted conditional Cumulative Distribution\nFunctions (CDF) of each coordinate on a leave one out basis. The Dominant\nmeasure $\\mu$ plays a crucial role as it defines the support on which\nconditional CDFs are compared. Sobolev IPM can be seen as an extension of the\none dimensional Von-Mises Cram\\'er statistics to high dimensional\ndistributions. We show how Sobolev IPM can be used to train Generative\nAdversarial Networks (GANs). We then exploit the intrinsic conditioning implied\nby Sobolev IPM in text generation. Finally we show that a variant of Sobolev\nGAN achieves competitive results in semi-supervised learning on CIFAR-10,\nthanks to the smoothness enforced on the critic by Sobolev GAN which relates to\nLaplacian regularization. \n\n"}
{"id": "1711.05068", "contents": "Title: Robust Matrix Elastic Net based Canonical Correlation Analysis: An\n  Effective Algorithm for Multi-View Unsupervised Learning Abstract: This paper presents a robust matrix elastic net based canonical correlation\nanalysis (RMEN-CCA) for multiple view unsupervised learning problems, which\nemphasizes the combination of CCA and the robust matrix elastic net (RMEN) used\nas coupled feature selection. The RMEN-CCA leverages the strength of the RMEN\nto distill naturally meaningful features without any prior assumption and to\nmeasure effectively correlations between different 'views'. We can further\nemploy directly the kernel trick to extend the RMEN-CCA to the kernel scenario\nwith theoretical guarantees, which takes advantage of the kernel trick for\nhighly complicated nonlinear feature learning. Rather than simply incorporating\nexisting regularization minimization terms into CCA, this paper provides a new\nlearning paradigm for CCA and is the first to derive a coupled feature\nselection based CCA algorithm that guarantees convergence. More significantly,\nfor CCA, the newly-derived RMEN-CCA bridges the gap between measurement of\nrelevance and coupled feature selection. Moreover, it is nontrivial to tackle\ndirectly the RMEN-CCA by previous optimization approaches derived from its\nsophisticated model architecture. Therefore, this paper further offers a bridge\nbetween a new optimization problem and an existing efficient iterative\napproach. As a consequence, the RMEN-CCA can overcome the limitation of CCA and\naddress large-scale and streaming data problems. Experimental results on four\npopular competing datasets illustrate that the RMEN-CCA performs more\neffectively and efficiently than do state-of-the-art approaches. \n\n"}
{"id": "1711.05136", "contents": "Title: Deep Rewiring: Training very sparse deep networks Abstract: Neuromorphic hardware tends to pose limits on the connectivity of deep\nnetworks that one can run on them. But also generic hardware and software\nimplementations of deep learning run more efficiently for sparse networks.\nSeveral methods exist for pruning connections of a neural network after it was\ntrained without connectivity constraints. We present an algorithm, DEEP R, that\nenables us to train directly a sparsely connected neural network. DEEP R\nautomatically rewires the network during supervised training so that\nconnections are there where they are most needed for the task, while its total\nnumber is all the time strictly bounded. We demonstrate that DEEP R can be used\nto train very sparse feedforward and recurrent neural networks on standard\nbenchmark tasks with just a minor loss in performance. DEEP R is based on a\nrigorous theoretical foundation that views rewiring as stochastic sampling of\nnetwork configurations from a posterior. \n\n"}
{"id": "1711.05225", "contents": "Title: CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep\n  Learning Abstract: We develop an algorithm that can detect pneumonia from chest X-rays at a\nlevel exceeding practicing radiologists. Our algorithm, CheXNet, is a 121-layer\nconvolutional neural network trained on ChestX-ray14, currently the largest\npublicly available chest X-ray dataset, containing over 100,000 frontal-view\nX-ray images with 14 diseases. Four practicing academic radiologists annotate a\ntest set, on which we compare the performance of CheXNet to that of\nradiologists. We find that CheXNet exceeds average radiologist performance on\nthe F1 metric. We extend CheXNet to detect all 14 diseases in ChestX-ray14 and\nachieve state of the art results on all 14 diseases. \n\n"}
{"id": "1711.05401", "contents": "Title: Revisiting Simple Neural Networks for Learning Representations of\n  Knowledge Graphs Abstract: We address the problem of learning vector representations for entities and\nrelations in Knowledge Graphs (KGs) for Knowledge Base Completion (KBC). This\nproblem has received significant attention in the past few years and multiple\nmethods have been proposed. Most of the existing methods in the literature use\na predefined characteristic scoring function for evaluating the correctness of\nKG triples. These scoring functions distinguish correct triples (high score)\nfrom incorrect ones (low score). However, their performance vary across\ndifferent datasets. In this work, we demonstrate that a simple neural network\nbased score function can consistently achieve near start-of-the-art performance\non multiple datasets. We also quantitatively demonstrate biases in standard\nbenchmark datasets, and highlight the need to perform evaluation spanning\nvarious datasets. \n\n"}
{"id": "1711.05828", "contents": "Title: BoostJet: Towards Combining Statistical Aggregates with Neural\n  Embeddings for Recommendations Abstract: Recommenders have become widely popular in recent years because of their\nbroader applicability in many e-commerce applications. These applications rely\non recommenders for generating advertisements for various offers or providing\ncontent recommendations. However, the quality of the generated recommendations\ndepends on user features (like demography, temporality), offer features (like\npopularity, price), and user-offer features (like implicit or explicit\nfeedback). Current state-of-the-art recommenders do not explore such diverse\nfeatures concurrently while generating the recommendations.\n  In this paper, we first introduce the notion of Trackers which enables us to\ncapture the above-mentioned features and thus incorporate users' online\nbehaviour through statistical aggregates of different features (demography,\ntemporality, popularity, price). We also show how to capture offer-to-offer\nrelations, based on their consumption sequence, leveraging neural embeddings\nfor offers in our Offer2Vec algorithm. We then introduce BoostJet, a novel\nrecommender which integrates the Trackers along with the neural embeddings\nusing MatrixNet, an efficient distributed implementation of gradient boosted\ndecision tree, to improve the recommendation quality significantly. We provide\nan in-depth evaluation of BoostJet on Yandex's dataset, collecting online\nbehaviour from tens of millions of online users, to demonstrate the\npracticality of BoostJet in terms of recommendation quality as well as\nscalability. \n\n"}
{"id": "1711.06922", "contents": "Title: Run, skeleton, run: skeletal model in a physics-based simulation Abstract: In this paper, we present our approach to solve a physics-based reinforcement\nlearning challenge \"Learning to Run\" with objective to train\nphysiologically-based human model to navigate a complex obstacle course as\nquickly as possible. The environment is computationally expensive, has a\nhigh-dimensional continuous action space and is stochastic. We benchmark state\nof the art policy-gradient methods and test several improvements, such as layer\nnormalization, parameter noise, action and state reflecting, to stabilize\ntraining and improve its sample-efficiency. We found that the Deep\nDeterministic Policy Gradient method is the most efficient method for this\nenvironment and the improvements we have introduced help to stabilize training.\nLearned models are able to generalize to new physical scenarios, e.g. different\nobstacle courses. \n\n"}
{"id": "1711.07461", "contents": "Title: Bidirectional Conditional Generative Adversarial Networks Abstract: Conditional Generative Adversarial Networks (cGANs) are generative models\nthat can produce data samples ($x$) conditioned on both latent variables ($z$)\nand known auxiliary information ($c$). We propose the Bidirectional cGAN\n(BiCoGAN), which effectively disentangles $z$ and $c$ in the generation process\nand provides an encoder that learns inverse mappings from $x$ to both $z$ and\n$c$, trained jointly with the generator and the discriminator. We present\ncrucial techniques for training BiCoGANs, which involve an extrinsic factor\nloss along with an associated dynamically-tuned importance weight. As compared\nto other encoder-based cGANs, BiCoGANs encode $c$ more accurately, and utilize\n$z$ and $c$ more effectively and in a more disentangled way to generate\nsamples. \n\n"}
{"id": "1711.07970", "contents": "Title: Deep Learning for Physical Processes: Incorporating Prior Scientific\n  Knowledge Abstract: We consider the use of Deep Learning methods for modeling complex phenomena\nlike those occurring in natural physical processes. With the large amount of\ndata gathered on these phenomena the data intensive paradigm could begin to\nchallenge more traditional approaches elaborated over the years in fields like\nmaths or physics. However, despite considerable successes in a variety of\napplication domains, the machine learning field is not yet ready to handle the\nlevel of complexity required by such problems. Using an example application,\nnamely Sea Surface Temperature Prediction, we show how general background\nknowledge gained from physics could be used as a guideline for designing\nefficient Deep Learning models. In order to motivate the approach and to assess\nits generality we demonstrate a formal link between the solution of a class of\ndifferential equations underlying a large family of physical phenomena and the\nproposed model. Experiments and comparison with series of baselines including a\nstate of the art numerical approach is then provided. \n\n"}
{"id": "1711.08442", "contents": "Title: From Monte Carlo to Las Vegas: Improving Restricted Boltzmann Machine\n  Training Through Stopping Sets Abstract: We propose a Las Vegas transformation of Markov Chain Monte Carlo (MCMC)\nestimators of Restricted Boltzmann Machines (RBMs). We denote our approach\nMarkov Chain Las Vegas (MCLV). MCLV gives statistical guarantees in exchange\nfor random running times. MCLV uses a stopping set built from the training data\nand has maximum number of Markov chain steps K (referred as MCLV-K). We present\na MCLV-K gradient estimator (LVS-K) for RBMs and explore the correspondence and\ndifferences between LVS-K and Contrastive Divergence (CD-K), with LVS-K\nsignificantly outperforming CD-K training RBMs over the MNIST dataset,\nindicating MCLV to be a promising direction in learning generative models. \n\n"}
{"id": "1711.09279", "contents": "Title: A Big Data Analysis Framework Using Apache Spark and Deep Learning Abstract: With the spreading prevalence of Big Data, many advances have recently been\nmade in this field. Frameworks such as Apache Hadoop and Apache Spark have\ngained a lot of traction over the past decades and have become massively\npopular, especially in industries. It is becoming increasingly evident that\neffective big data analysis is key to solving artificial intelligence problems.\nThus, a multi-algorithm library was implemented in the Spark framework, called\nMLlib. While this library supports multiple machine learning algorithms, there\nis still scope to use the Spark setup efficiently for highly time-intensive and\ncomputationally expensive procedures like deep learning. In this paper, we\npropose a novel framework that combines the distributive computational\nabilities of Apache Spark and the advanced machine learning architecture of a\ndeep multi-layer perceptron (MLP), using the popular concept of Cascade\nLearning. We conduct empirical analysis of our framework on two real world\ndatasets. The results are encouraging and corroborate our proposed framework,\nin turn proving that it is an improvement over traditional big data analysis\nmethods that use either Spark or Deep learning as individual elements. \n\n"}
{"id": "1711.09300", "contents": "Title: Learning Less-Overlapping Representations Abstract: In representation learning (RL), how to make the learned representations easy\nto interpret and less overfitted to training data are two important but\nchallenging issues. To address these problems, we study a new type of\nregulariza- tion approach that encourages the supports of weight vectors in RL\nmodels to have small overlap, by simultaneously promoting near-orthogonality\namong vectors and sparsity of each vector. We apply the proposed regularizer to\ntwo models: neural networks (NNs) and sparse coding (SC), and develop an\nefficient ADMM-based algorithm for regu- larized SC. Experiments on various\ndatasets demonstrate that weight vectors learned under our regularizer are more\ninterpretable and have better generalization performance. \n\n"}
{"id": "1711.10856", "contents": "Title: Semi-Supervised and Active Few-Shot Learning with Prototypical Networks Abstract: We consider the problem of semi-supervised few-shot classification where a\nclassifier needs to adapt to new tasks using a few labeled examples and\n(potentially many) unlabeled examples. We propose a clustering approach to the\nproblem. The features extracted with Prototypical Networks are clustered using\n$K$-means with the few labeled examples guiding the clustering process. We note\nthat in many real-world applications the adaptation performance can be\nsignificantly improved by requesting the few labels through user feedback. We\ndemonstrate good performance of the active adaptation strategy using image\ndata. \n\n"}
{"id": "1711.11561", "contents": "Title: Measuring the tendency of CNNs to Learn Surface Statistical Regularities Abstract: Deep CNNs are known to exhibit the following peculiarity: on the one hand\nthey generalize extremely well to a test set, while on the other hand they are\nextremely sensitive to so-called adversarial perturbations. The extreme\nsensitivity of high performance CNNs to adversarial examples casts serious\ndoubt that these networks are learning high level abstractions in the dataset.\nWe are concerned with the following question: How can a deep CNN that does not\nlearn any high level semantics of the dataset manage to generalize so well? The\ngoal of this article is to measure the tendency of CNNs to learn surface\nstatistical regularities of the dataset. To this end, we use Fourier filtering\nto construct datasets which share the exact same high level abstractions but\nexhibit qualitatively different surface statistical regularities. For the SVHN\nand CIFAR-10 datasets, we present two Fourier filtered variants: a low\nfrequency variant and a randomly filtered variant. Each of the Fourier\nfiltering schemes is tuned to preserve the recognizability of the objects. Our\nmain finding is that CNNs exhibit a tendency to latch onto the Fourier image\nstatistics of the training dataset, sometimes exhibiting up to a 28%\ngeneralization gap across the various test sets. Moreover, we observe that\nsignificantly increasing the depth of a network has a very marginal impact on\nclosing the aforementioned generalization gap. Thus we provide quantitative\nevidence supporting the hypothesis that deep CNNs tend to learn surface\nstatistical regularities in the dataset rather than higher-level abstract\nconcepts. \n\n"}
{"id": "1711.11581", "contents": "Title: Outlier-robust moment-estimation via sum-of-squares Abstract: We develop efficient algorithms for estimating low-degree moments of unknown\ndistributions in the presence of adversarial outliers. The guarantees of our\nalgorithms improve in many cases significantly over the best previous ones,\nobtained in recent works of Diakonikolas et al, Lai et al, and Charikar et al.\nWe also show that the guarantees of our algorithms match information-theoretic\nlower-bounds for the class of distributions we consider. These improved\nguarantees allow us to give improved algorithms for independent component\nanalysis and learning mixtures of Gaussians in the presence of outliers.\n  Our algorithms are based on a standard sum-of-squares relaxation of the\nfollowing conceptually-simple optimization problem: Among all distributions\nwhose moments are bounded in the same way as for the unknown distribution, find\nthe one that is closest in statistical distance to the empirical distribution\nof the adversarially-corrupted sample. \n\n"}
{"id": "1712.00443", "contents": "Title: Deep Neural Network Architectures for Modulation Classification Abstract: In this work, we investigate the value of employing deep learning for the\ntask of wireless signal modulation recognition. Recently in [1], a framework\nhas been introduced by generating a dataset using GNU radio that mimics the\nimperfections in a real wireless channel, and uses 10 different modulation\ntypes. Further, a convolutional neural network (CNN) architecture was developed\nand shown to deliver performance that exceeds that of expert-based approaches.\nHere, we follow the framework of [1] and find deep neural network architectures\nthat deliver higher accuracy than the state of the art. We tested the\narchitecture of [1] and found it to achieve an accuracy of approximately 75% of\ncorrectly recognizing the modulation type. We first tune the CNN architecture\nof [1] and find a design with four convolutional layers and two dense layers\nthat gives an accuracy of approximately 83.8% at high SNR. We then develop\narchitectures based on the recently introduced ideas of Residual Networks\n(ResNet [2]) and Densely Connected Networks (DenseNet [3]) to achieve high SNR\naccuracies of approximately 83.5% and 86.6%, respectively. Finally, we\nintroduce a Convolutional Long Short-term Deep Neural Network (CLDNN [4]) to\nachieve an accuracy of approximately 88.5% at high SNR. \n\n"}
{"id": "1712.00961", "contents": "Title: Learning Independent Causal Mechanisms Abstract: Statistical learning relies upon data sampled from a distribution, and we\nusually do not care what actually generated it in the first place. From the\npoint of view of causal modeling, the structure of each distribution is induced\nby physical mechanisms that give rise to dependences between observables.\nMechanisms, however, can be meaningful autonomous modules of generative models\nthat make sense beyond a particular entailed data distribution, lending\nthemselves to transfer between problems. We develop an algorithm to recover a\nset of independent (inverse) mechanisms from a set of transformed data points.\nThe approach is unsupervised and based on a set of experts that compete for\ndata generated by the mechanisms, driving specialization. We analyze the\nproposed method in a series of experiments on image data. Each expert learns to\nmap a subset of the transformed data back to a reference distribution. The\nlearned mechanisms generalize to novel domains. We discuss implications for\ntransfer learning and links to recent trends in generative modeling. \n\n"}
{"id": "1712.01423", "contents": "Title: Quantum Accelerators for High-Performance Computing Systems Abstract: We define some of the programming and system-level challenges facing the\napplication of quantum processing to high-performance computing. Alongside\nbarriers to physical integration, prominent differences in the execution of\nquantum and conventional programs challenges the intersection of these\ncomputational models. Following a brief overview of the state of the art, we\ndiscuss recent advances in programming and execution models for hybrid\nquantum-classical computing. We discuss a novel quantum-accelerator framework\nthat uses specialized kernels to offload select workloads while integrating\nwith existing computing infrastructure. We elaborate on the role of the host\noperating system to manage these unique accelerator resources, the prospects\nfor deploying quantum modules, and the requirements placed on the language\nhierarchy connecting these different system components. We draw on recent\nadvances in the modeling and simulation of quantum computing systems with the\ndevelopment of architectures for hybrid high-performance computing systems and\nthe realization of software stacks for controlling quantum devices. Finally, we\npresent simulation results that describe the expected system-level behavior of\nhigh-performance computing systems composed from compute nodes with quantum\nprocessing units. We describe performance for these hybrid systems in terms of\ntime-to-solution, accuracy, and energy consumption, and we use simple\napplication examples to estimate the performance advantage of quantum\nacceleration. \n\n"}
{"id": "1712.02630", "contents": "Title: Design of Efficient Reversible Logic Based Binary and BCD Adder Circuits Abstract: In this work, we present a class of new designs for reversible binary and BCD\nadder circuits. The proposed designs are primarily optimized for the number of\nancilla inputs and the number of garbage outputs and are designed for possible\nbest values for the quantum cost and delay. First, we propose two new designs\nfor the reversible ripple carry adder: (i) one with no input carry $c_0$ and no\nancilla input bits, and (ii) one with input carry $c_0$ and no ancilla input\nbits. The proposed reversible ripple carry adder designs with no ancilla input\nbits have less quantum cost and logic depth (delay) compared to their existing\ncounterparts in the literature. In these designs, the quantum cost and delay\nare reduced by deriving designs based on the reversible Peres gate and the TR\ngate. Next, four new designs for the reversible BCD adder are presented based\non the following two approaches: (i) the addition is performed in binary mode\nand correction is applied to convert to BCD when required through detection and\ncorrection, and (ii) the addition is performed in binary mode and the result is\nalways converted using a binary to BCD converter. The proposed reversible\nbinary and BCD adders can be applied in a wide variety of digital signal\nprocessing applications and constitute important design components of\nreversible computing. \n\n"}
{"id": "1712.03428", "contents": "Title: Cost-Sensitive Approach to Batch Size Adaptation for Gradient Descent Abstract: In this paper, we propose a novel approach to automatically determine the\nbatch size in stochastic gradient descent methods. The choice of the batch size\ninduces a trade-off between the accuracy of the gradient estimate and the cost\nin terms of samples of each update. We propose to determine the batch size by\noptimizing the ratio between a lower bound to a linear or quadratic Taylor\napproximation of the expected improvement and the number of samples used to\nestimate the gradient. The performance of the proposed approach is empirically\ncompared with related methods on popular classification tasks.\n  The work was presented at the NIPS workshop on Optimizing the Optimizers.\nBarcelona, Spain, 2016. \n\n"}
{"id": "1712.07557", "contents": "Title: Differentially Private Federated Learning: A Client Level Perspective Abstract: Federated learning is a recent advance in privacy protection. In this\ncontext, a trusted curator aggregates parameters optimized in decentralized\nfashion by multiple clients. The resulting model is then distributed back to\nall clients, ultimately converging to a joint representative model without\nexplicitly having to share the data. However, the protocol is vulnerable to\ndifferential attacks, which could originate from any party contributing during\nfederated optimization. In such an attack, a client's contribution during\ntraining and information about their data set is revealed through analyzing the\ndistributed model. We tackle this problem and propose an algorithm for client\nsided differential privacy preserving federated optimization. The aim is to\nhide clients' contributions during training, balancing the trade-off between\nprivacy loss and model performance. Empirical studies suggest that given a\nsufficiently large number of participating clients, our proposed procedure can\nmaintain client-level differential privacy at only a minor cost in model\nperformance. \n\n"}
{"id": "1712.07788", "contents": "Title: Deep Unsupervised Clustering Using Mixture of Autoencoders Abstract: Unsupervised clustering is one of the most fundamental challenges in machine\nlearning. A popular hypothesis is that data are generated from a union of\nlow-dimensional nonlinear manifolds; thus an approach to clustering is\nidentifying and separating these manifolds. In this paper, we present a novel\napproach to solve this problem by using a mixture of autoencoders. Our model\nconsists of two parts: 1) a collection of autoencoders where each autoencoder\nlearns the underlying manifold of a group of similar objects, and 2) a mixture\nassignment neural network, which takes the concatenated latent vectors from the\nautoencoders as input and infers the distribution over clusters. By jointly\noptimizing the two parts, we simultaneously assign data to clusters and learn\nthe underlying manifolds of each cluster. \n\n"}
{"id": "1712.08626", "contents": "Title: Obtaining Accurate Probabilistic Causal Inference by Post-Processing\n  Calibration Abstract: Discovery of an accurate causal Bayesian network structure from observational\ndata can be useful in many areas of science. Often the discoveries are made\nunder uncertainty, which can be expressed as probabilities. To guide the use of\nsuch discoveries, including directing further investigation, it is important\nthat those probabilities be well-calibrated. In this paper, we introduce a\nnovel framework to derive calibrated probabilities of causal relationships from\nobservational data. The framework consists of three components: (1) an\napproximate method for generating initial probability estimates of the edge\ntypes for each pair of variables, (2) the availability of a relatively small\nnumber of the causal relationships in the network for which the truth status is\nknown, which we call a calibration training set, and (3) a calibration method\nfor using the approximate probability estimates and the calibration training\nset to generate calibrated probabilities for the many remaining pairs of\nvariables. We also introduce a new calibration method based on a shallow neural\nnetwork. Our experiments on simulated data support that the proposed approach\nimproves the calibration of causal edge predictions. The results also support\nthat the approach often improves the precision and recall of predictions. \n\n"}
{"id": "1712.08642", "contents": "Title: Least-Squares Temporal Difference Learning for the Linear Quadratic\n  Regulator Abstract: Reinforcement learning (RL) has been successfully used to solve many\ncontinuous control tasks. Despite its impressive results however, fundamental\nquestions regarding the sample complexity of RL on continuous problems remain\nopen. We study the performance of RL in this setting by considering the\nbehavior of the Least-Squares Temporal Difference (LSTD) estimator on the\nclassic Linear Quadratic Regulator (LQR) problem from optimal control. We give\nthe first finite-time analysis of the number of samples needed to estimate the\nvalue function for a fixed static state-feedback policy to within\n$\\varepsilon$-relative error. In the process of deriving our result, we give a\ngeneral characterization for when the minimum eigenvalue of the empirical\ncovariance matrix formed along the sample path of a fast-mixing stochastic\nprocess concentrates above zero, extending a result by Koltchinskii and\nMendelson in the independent covariates setting. Finally, we provide\nexperimental evidence indicating that our analysis correctly captures the\nqualitative behavior of LSTD on several LQR instances. \n\n"}
{"id": "1712.08645", "contents": "Title: Dropout Feature Ranking for Deep Learning Models Abstract: Deep neural networks (DNNs) achieve state-of-the-art results in a variety of\ndomains. Unfortunately, DNNs are notorious for their non-interpretability, and\nthus limit their applicability in hypothesis-driven domains such as biology and\nhealthcare. Moreover, in the resource-constraint setting, it is critical to\ndesign tests relying on fewer more informative features leading to high\naccuracy performance within reasonable budget. We aim to close this gap by\nproposing a new general feature ranking method for deep learning. We show that\nour simple yet effective method performs on par or compares favorably to eight\nstrawman, classical and deep-learning feature ranking methods in two\nsimulations and five very different datasets on tasks ranging from\nclassification to regression, in both static and time series scenarios. We also\nillustrate the use of our method on a drug response dataset and show that it\nidentifies genes relevant to the drug-response. \n\n"}
{"id": "1712.09005", "contents": "Title: Efficient Algorithms for t-distributed Stochastic Neighborhood Embedding Abstract: t-distributed Stochastic Neighborhood Embedding (t-SNE) is a method for\ndimensionality reduction and visualization that has become widely popular in\nrecent years. Efficient implementations of t-SNE are available, but they scale\npoorly to datasets with hundreds of thousands to millions of high dimensional\ndata-points. We present Fast Fourier Transform-accelerated Interpolation-based\nt-SNE (FIt-SNE), which dramatically accelerates the computation of t-SNE. The\nmost time-consuming step of t-SNE is a convolution that we accelerate by\ninterpolating onto an equispaced grid and subsequently using the fast Fourier\ntransform to perform the convolution. We also optimize the computation of input\nsimilarities in high dimensions using multi-threaded approximate nearest\nneighbors. We further present a modification to t-SNE called \"late\nexaggeration,\" which allows for easier identification of clusters in t-SNE\nembeddings. Finally, for datasets that cannot be loaded into the memory, we\npresent out-of-core randomized principal component analysis (oocPCA), so that\nthe top principal components of a dataset can be computed without ever fully\nloading the matrix, hence allowing for t-SNE of large datasets to be computed\non resource-limited machines. \n\n"}
{"id": "1712.09707", "contents": "Title: Deep learning for universal linear embeddings of nonlinear dynamics Abstract: Identifying coordinate transformations that make strongly nonlinear dynamics\napproximately linear is a central challenge in modern dynamical systems. These\ntransformations have the potential to enable prediction, estimation, and\ncontrol of nonlinear systems using standard linear theory. The Koopman operator\nhas emerged as a leading data-driven embedding, as eigenfunctions of this\noperator provide intrinsic coordinates that globally linearize the dynamics.\nHowever, identifying and representing these eigenfunctions has proven to be\nmathematically and computationally challenging. This work leverages the power\nof deep learning to discover representations of Koopman eigenfunctions from\ntrajectory data of dynamical systems. Our network is parsimonious and\ninterpretable by construction, embedding the dynamics on a low-dimensional\nmanifold that is of the intrinsic rank of the dynamics and parameterized by the\nKoopman eigenfunctions. In particular, we identify nonlinear coordinates on\nwhich the dynamics are globally linear using a modified auto-encoder. We also\ngeneralize Koopman representations to include a ubiquitous class of systems\nthat exhibit continuous spectra, ranging from the simple pendulum to nonlinear\noptics and broadband turbulence. Our framework parametrizes the continuous\nfrequency using an auxiliary network, enabling a compact and efficient\nembedding at the intrinsic rank, while connecting our models to half a century\nof asymptotics. In this way, we benefit from the power and generality of deep\nlearning, while retaining the physical interpretability of Koopman embeddings. \n\n"}
{"id": "1712.10248", "contents": "Title: Deep Learning Interior Tomography for Region-of-Interest Reconstruction Abstract: Interior tomography for the region-of-interest (ROI) imaging has advantages\nof using a small detector and reducing X-ray radiation dose. However, standard\nanalytic reconstruction suffers from severe cupping artifacts due to existence\nof null space in the truncated Radon transform. Existing penalized\nreconstruction methods may address this problem but they require extensive\ncomputations due to the iterative reconstruction. Inspired by the recent deep\nlearning approaches to low-dose and sparse view CT, here we propose a deep\nlearning architecture that removes null space signals from the FBP\nreconstruction. Experimental results have shown that the proposed method\nprovides near-perfect reconstruction with about 7-10 dB improvement in PSNR\nover existing methods in spite of significantly reduced run-time complexity. \n\n"}
{"id": "1801.00209", "contents": "Title: Deep Reinforcement Learning for List-wise Recommendations Abstract: Recommender systems play a crucial role in mitigating the problem of\ninformation overload by suggesting users' personalized items or services. The\nvast majority of traditional recommender systems consider the recommendation\nprocedure as a static process and make recommendations following a fixed\nstrategy. In this paper, we propose a novel recommender system with the\ncapability of continuously improving its strategies during the interactions\nwith users. We model the sequential interactions between users and a\nrecommender system as a Markov Decision Process (MDP) and leverage\nReinforcement Learning (RL) to automatically learn the optimal strategies via\nrecommending trial-and-error items and receiving reinforcements of these items\nfrom users' feedbacks. In particular, we introduce an online user-agent\ninteracting environment simulator, which can pre-train and evaluate model\nparameters offline before applying the model online. Moreover, we validate the\nimportance of list-wise recommendations during the interactions between users\nand agent, and develop a novel approach to incorporate them into the proposed\nframework LIRD for list-wide recommendations. The experimental results based on\na real-world e-commerce dataset demonstrate the effectiveness of the proposed\nframework. \n\n"}
{"id": "1801.01081", "contents": "Title: High Performance Quantum Modular Multipliers Abstract: We present a novel set of reversible modular multipliers applicable to\nquantum computing, derived from three classical techniques: 1) traditional\ninteger division, 2) Montgomery residue arithmetic, and 3) Barrett reduction.\nEach multiplier computes an exact result for all binary input values, while\nmaintaining the asymptotic resource complexity of a single (non-modular)\ninteger multiplier. We additionally conduct an empirical resource analysis of\nour designs in order to determine the total gate count and circuit depth of\neach fully constructed circuit, with inputs as large as 2048 bits. Our\ncomparative analysis considers both circuit implementations which allow for\narbitrary (controlled) rotation gates, as well as those restricted to a typical\nfault-tolerant gate set. \n\n"}
{"id": "1801.02961", "contents": "Title: Representation Learning with Autoencoders for Electronic Health Records:\n  A Comparative Study Abstract: Increasing volume of Electronic Health Records (EHR) in recent years provides\ngreat opportunities for data scientists to collaborate on different aspects of\nhealthcare research by applying advanced analytics to these EHR clinical data.\nA key requirement however is obtaining meaningful insights from high\ndimensional, sparse and complex clinical data. Data science approaches\ntypically address this challenge by performing feature learning in order to\nbuild more reliable and informative feature representations from clinical data\nfollowed by supervised learning. In this paper, we propose a predictive\nmodeling approach based on deep learning based feature representations and word\nembedding techniques. Our method uses different deep architectures (stacked\nsparse autoencoders, deep belief network, adversarial autoencoders and\nvariational autoencoders) for feature representation in higher-level\nabstraction to obtain effective and robust features from EHRs, and then build\nprediction models on top of them. Our approach is particularly useful when the\nunlabeled data is abundant whereas labeled data is scarce. We investigate the\nperformance of representation learning through a supervised learning approach.\nOur focus is to present a comparative study to evaluate the performance of\ndifferent deep architectures through supervised learning and provide insights\nin the choice of deep feature representation techniques. Our experiments\ndemonstrate that for small data sets, stacked sparse autoencoder demonstrates a\nsuperior generality performance in prediction due to sparsity regularization\nwhereas variational autoencoders outperform the competing approaches for large\ndata sets due to its capability of learning the representation distribution. \n\n"}
{"id": "1801.03911", "contents": "Title: Stochastic Learning of Nonstationary Kernels for Natural Language\n  Modeling Abstract: Natural language processing often involves computations with semantic or\nsyntactic graphs to facilitate sophisticated reasoning based on structural\nrelationships. While convolution kernels provide a powerful tool for comparing\ngraph structure based on node (word) level relationships, they are difficult to\ncustomize and can be computationally expensive. We propose a generalization of\nconvolution kernels, with a nonstationary model, for better expressibility of\nnatural languages in supervised settings. For a scalable learning of the\nparameters introduced with our model, we propose a novel algorithm that\nleverages stochastic sampling on k-nearest neighbor graphs, along with\napproximations based on locality-sensitive hashing. We demonstrate the\nadvantages of our approach on a challenging real-world (structured inference)\nproblem of automatically extracting biological models from the text of\nscientific papers. \n\n"}
{"id": "1801.04055", "contents": "Title: A3T: Adversarially Augmented Adversarial Training Abstract: Recent research showed that deep neural networks are highly sensitive to\nso-called adversarial perturbations, which are tiny perturbations of the input\ndata purposely designed to fool a machine learning classifier. Most\nclassification models, including deep learning models, are highly vulnerable to\nadversarial attacks. In this work, we investigate a procedure to improve\nadversarial robustness of deep neural networks through enforcing representation\ninvariance. The idea is to train the classifier jointly with a discriminator\nattached to one of its hidden layer and trained to filter the adversarial\nnoise. We perform preliminary experiments to test the viability of the approach\nand to compare it to other standard adversarial training methods. \n\n"}
{"id": "1801.04211", "contents": "Title: Towards Arbitrary Noise Augmentation - Deep Learning for Sampling from\n  Arbitrary Probability Distributions Abstract: Accurate noise modelling is important for training of deep learning\nreconstruction algorithms. While noise models are well known for traditional\nimaging techniques, the noise distribution of a novel sensor may be difficult\nto determine a priori. Therefore, we propose learning arbitrary noise\ndistributions. To do so, this paper proposes a fully connected neural network\nmodel to map samples from a uniform distribution to samples of any explicitly\nknown probability density function. During the training, the Jensen-Shannon\ndivergence between the distribution of the model's output and the target\ndistribution is minimized. We experimentally demonstrate that our model\nconverges towards the desired state. It provides an alternative to existing\nsampling methods such as inversion sampling, rejection sampling, Gaussian\nmixture models and Markov-Chain-Monte-Carlo. Our model has high sampling\nefficiency and is easily applied to any probability distribution, without the\nneed of further analytical or numerical calculations. \n\n"}
{"id": "1801.04339", "contents": "Title: Estimating the Number of Connected Components in a Graph via Subgraph\n  Sampling Abstract: Learning properties of large graphs from samples has been an important\nproblem in statistical network analysis since the early work of Goodman\n\\cite{Goodman1949} and Frank \\cite{Frank1978}. We revisit a problem formulated\nby Frank \\cite{Frank1978} of estimating the number of connected components in a\nlarge graph based on the subgraph sampling model, in which we randomly sample a\nsubset of the vertices and observe the induced subgraph. The key question is\nwhether accurate estimation is achievable in the \\emph{sublinear} regime where\nonly a vanishing fraction of the vertices are sampled. We show that it is\nimpossible if the parent graph is allowed to contain high-degree vertices or\nlong induced cycles. For the class of chordal graphs, where induced cycles of\nlength four or above are forbidden, we characterize the optimal sample\ncomplexity within constant factors and construct linear-time estimators that\nprovably achieve these bounds. This significantly expands the scope of previous\nresults which have focused on unbiased estimators and special classes of graphs\nsuch as forests or cliques.\n  Both the construction and the analysis of the proposed methodology rely on\ncombinatorial properties of chordal graphs and identities of induced subgraph\ncounts. They, in turn, also play a key role in proving minimax lower bounds\nbased on construction of random instances of graphs with matching structures of\nsmall subgraphs. \n\n"}
{"id": "1801.04849", "contents": "Title: A Method of Finding a Lower Energy Solution to a QUBO/Ising Objective\n  Function Abstract: A new method to find a lower energy solution to a QUBO/Ising objective\nfunction will be presented in this paper. It is applied to samples returned\nfrom the D-Wave for various example cases. This method, multi-qubit correction\n(MQC), creates a sample with an equal-to or less-than energy than any of the\nD-wave samples used to create it. The method will be detailed and the results\nof 3 uses cases will be given to demonstrate its merit. \n\n"}
{"id": "1801.06889", "contents": "Title: Visual Analytics in Deep Learning: An Interrogative Survey for the Next\n  Frontiers Abstract: Deep learning has recently seen rapid development and received significant\nattention due to its state-of-the-art performance on previously-thought hard\nproblems. However, because of the internal complexity and nonlinear structure\nof deep neural networks, the underlying decision making processes for why these\nmodels are achieving such performance are challenging and sometimes mystifying\nto interpret. As deep learning spreads across domains, it is of paramount\nimportance that we equip users of deep learning with tools for understanding\nwhen a model works correctly, when it fails, and ultimately how to improve its\nperformance. Standardized toolkits for building neural networks have helped\ndemocratize deep learning; visual analytics systems have now been developed to\nsupport model explanation, interpretation, debugging, and improvement. We\npresent a survey of the role of visual analytics in deep learning research,\nwhich highlights its short yet impactful history and thoroughly summarizes the\nstate-of-the-art using a human-centered interrogative framework, focusing on\nthe Five W's and How (Why, Who, What, How, When, and Where). We conclude by\nhighlighting research directions and open research problems. This survey helps\nresearchers and practitioners in both visual analytics and deep learning to\nquickly learn key aspects of this young and rapidly growing body of research,\nwhose impact spans a diverse range of domains. \n\n"}
{"id": "1801.07194", "contents": "Title: Optimizing Prediction Intervals by Tuning Random Forest via\n  Meta-Validation Abstract: Recent studies have shown that tuning prediction models increases prediction\naccuracy and that Random Forest can be used to construct prediction intervals.\nHowever, to our best knowledge, no study has investigated the need to, and the\nmanner in which one can, tune Random Forest for optimizing prediction intervals\n{ this paper aims to fill this gap. We explore a tuning approach that combines\nan effectively exhaustive search with a validation technique on a single Random\nForest parameter. This paper investigates which, out of eight validation\ntechniques, are beneficial for tuning, i.e., which automatically choose a\nRandom Forest configuration constructing prediction intervals that are reliable\nand with a smaller width than the default configuration. Additionally, we\npresent and validate three meta-validation techniques to determine which are\nbeneficial, i.e., those which automatically chose a beneficial validation\ntechnique. This study uses data from our industrial partner (Keymind Inc.) and\nthe Tukutuku Research Project, related to post-release defect prediction and\nWeb application effort estimation, respectively. Results from our study\nindicate that: i) the default configuration is frequently unreliable, ii) most\nof the validation techniques, including previously successfully adopted ones\nsuch as 50/50 holdout and bootstrap, are counterproductive in most of the\ncases, and iii) the 75/25 holdout meta-validation technique is always\nbeneficial; i.e., it avoids the likely counterproductive effects of validation\ntechniques. \n\n"}
{"id": "1801.07827", "contents": "Title: Semi-Supervised Convolutional Neural Networks for Human Activity\n  Recognition Abstract: Labeled data used for training activity recognition classifiers are usually\nlimited in terms of size and diversity. Thus, the learned model may not\ngeneralize well when used in real-world use cases. Semi-supervised learning\naugments labeled examples with unlabeled examples, often resulting in improved\nperformance. However, the semi-supervised methods studied in the activity\nrecognition literatures assume that feature engineering is already done. In\nthis paper, we lift this assumption and present two semi-supervised methods\nbased on convolutional neural networks (CNNs) to learn discriminative hidden\nfeatures. Our semi-supervised CNNs learn from both labeled and unlabeled data\nwhile also performing feature learning on raw sensor data. In experiments on\nthree real world datasets, we show that our CNNs outperform supervised methods\nand traditional semi-supervised learning methods by up to 18% in mean F1-score\n(Fm). \n\n"}
{"id": "1802.00069", "contents": "Title: Leveraging Adiabatic Quantum Computation for Election Forecasting Abstract: Accurate, reliable sampling from fully-connected graphs with arbitrary\ncorrelations is a difficult problem. Such sampling requires knowledge of the\nprobabilities of observing every possible state of a graph. As graph size\ngrows, the number of model states becomes intractably large and efficient\ncomputation requires full sampling be replaced with heuristics and algorithms\nthat are only approximations of full sampling. This work investigates the\npotential impact of adiabatic quantum computation for sampling purposes,\nbuilding on recent successes training Boltzmann machines using a quantum\ndevice. We investigate the use case of quantum computation to train Boltzmann\nmachines for predicting the 2016 Presidential election. \n\n"}
{"id": "1802.00150", "contents": "Title: Alternating Multi-bit Quantization for Recurrent Neural Networks Abstract: Recurrent neural networks have achieved excellent performance in many\napplications. However, on portable devices with limited resources, the models\nare often too large to deploy. For applications on the server with large scale\nconcurrent requests, the latency during inference can also be very critical for\ncostly computing resources. In this work, we address these problems by\nquantizing the network, both weights and activations, into multiple binary\ncodes {-1,+1}. We formulate the quantization as an optimization problem. Under\nthe key observation that once the quantization coefficients are fixed the\nbinary codes can be derived efficiently by binary search tree, alternating\nminimization is then applied. We test the quantization for two well-known RNNs,\ni.e., long short term memory (LSTM) and gated recurrent unit (GRU), on the\nlanguage models. Compared with the full-precision counter part, by 2-bit\nquantization we can achieve ~16x memory saving and ~6x real inference\nacceleration on CPUs, with only a reasonable loss in the accuracy. By 3-bit\nquantization, we can achieve almost no loss in the accuracy or even surpass the\noriginal model, with ~10.5x memory saving and ~3x real inference acceleration.\nBoth results beat the exiting quantization works with large margins. We extend\nour alternating quantization to image classification tasks. In both RNNs and\nfeedforward neural networks, the method also achieves excellent performance. \n\n"}
{"id": "1802.00822", "contents": "Title: VIBNN: Hardware Acceleration of Bayesian Neural Networks Abstract: Bayesian Neural Networks (BNNs) have been proposed to address the problem of\nmodel uncertainty in training and inference. By introducing weights associated\nwith conditioned probability distributions, BNNs are capable of resolving the\noverfitting issue commonly seen in conventional neural networks and allow for\nsmall-data training, through the variational inference process. Frequent usage\nof Gaussian random variables in this process requires a properly optimized\nGaussian Random Number Generator (GRNG). The high hardware cost of conventional\nGRNG makes the hardware implementation of BNNs challenging.\n  In this paper, we propose VIBNN, an FPGA-based hardware accelerator design\nfor variational inference on BNNs. We explore the design space for massive\namount of Gaussian variable sampling tasks in BNNs. Specifically, we introduce\ntwo high performance Gaussian (pseudo) random number generators: the RAM-based\nLinear Feedback Gaussian Random Number Generator (RLF-GRNG), which is inspired\nby the properties of binomial distribution and linear feedback logics; and the\nBayesian Neural Network-oriented Wallace Gaussian Random Number Generator. To\nachieve high scalability and efficient memory access, we propose a deep\npipelined accelerator architecture with fast execution and good hardware\nutilization. Experimental results demonstrate that the proposed VIBNN\nimplementations on an FPGA can achieve throughput of 321,543.4 Images/s and\nenergy efficiency upto 52,694.8 Images/J while maintaining similar accuracy as\nits software counterpart. \n\n"}
{"id": "1802.01212", "contents": "Title: Non-Gaussian information from weak lensing data via deep learning Abstract: Weak lensing maps contain information beyond two-point statistics on small\nscales. Much recent work has tried to extract this information through a range\nof different observables or via nonlinear transformations of the lensing field.\nHere we train and apply a 2D convolutional neural network to simulated\nnoiseless lensing maps covering 96 different cosmological models over a range\nof {$\\Omega_m,\\sigma_8$}. Using the area of the confidence contour in the\n{$\\Omega_m,\\sigma_8$} plane as a figure-of-merit, derived from simulated\nconvergence maps smoothed on a scale of 1.0 arcmin, we show that the neural\nnetwork yields $\\approx 5 \\times$ tighter constraints than the power spectrum,\nand $\\approx 4 \\times$ tighter than the lensing peaks. Such gains illustrate\nthe extent to which weak lensing data encode cosmological information not\naccessible to the power spectrum or even other, non-Gaussian statistics such as\nlensing peaks. \n\n"}
{"id": "1802.01504", "contents": "Title: Linear Convergence of the Primal-Dual Gradient Method for Convex-Concave\n  Saddle Point Problems without Strong Convexity Abstract: We consider the convex-concave saddle point problem $\\min_{x}\\max_{y}\nf(x)+y^\\top A x-g(y)$ where $f$ is smooth and convex and $g$ is smooth and\nstrongly convex. We prove that if the coupling matrix $A$ has full column rank,\nthe vanilla primal-dual gradient method can achieve linear convergence even if\n$f$ is not strongly convex. Our result generalizes previous work which either\nrequires $f$ and $g$ to be quadratic functions or requires proximal mappings\nfor both $f$ and $g$. We adopt a novel analysis technique that in each\niteration uses a \"ghost\" update as a reference, and show that the iterates in\nthe primal-dual gradient method converge to this \"ghost\" sequence. Using the\nsame technique we further give an analysis for the primal-dual stochastic\nvariance reduced gradient (SVRG) method for convex-concave saddle point\nproblems with a finite-sum structure. \n\n"}
{"id": "1802.02290", "contents": "Title: Spectral Image Visualization Using Generative Adversarial Networks Abstract: Spectral images captured by satellites and radio-telescopes are analyzed to\nobtain information about geological compositions distributions, distant asters\nas well as undersea terrain. Spectral images usually contain tens to hundreds\nof continuous narrow spectral bands and are widely used in various fields. But\nthe vast majority of those image signals are beyond the visible range, which\ncalls for special visualization technique. The visualizations of spectral\nimages shall convey as much information as possible from the original signal\nand facilitate image interpretation. However, most of the existing visualizatio\nmethods display spectral images in false colors, which contradict with human's\nexperience and expectation. In this paper, we present a novel visualization\ngenerative adversarial network (GAN) to display spectral images in natural\ncolors. To achieve our goal, we propose a loss function which consists of an\nadversarial loss and a structure loss. The adversarial loss pushes our solution\nto the natural image distribution using a discriminator network that is trained\nto differentiate between false-color images and natural-color images. We also\nuse a cycle loss as the structure constraint to guarantee structure\nconsistency. Experimental results show that our method is able to generate\nstructure-preserved and natural-looking visualizations. \n\n"}
{"id": "1802.02498", "contents": "Title: Spectral Learning of Binomial HMMs for DNA Methylation Data Abstract: We consider learning parameters of Binomial Hidden Markov Models, which may\nbe used to model DNA methylation data. The standard algorithm for the problem\nis EM, which is computationally expensive for sequences of the scale of the\nmammalian genome. Recently developed spectral algorithms can learn parameters\nof latent variable models via tensor decomposition, and are highly efficient\nfor large data. However, these methods have only been applied to categorial\nHMMs, and the main challenge is how to extend them to Binomial HMMs while still\nretaining computational efficiency. We address this challenge by introducing a\nnew feature-map based approach that exploits specific properties of Binomial\nHMMs. We provide theoretical performance guarantees for our algorithm and\nevaluate it on real DNA methylation data. \n\n"}
{"id": "1802.02643", "contents": "Title: Gradient conjugate priors and multi-layer neural networks Abstract: The paper deals with learning probability distributions of observed data by\nartificial neural networks. We suggest a so-called gradient conjugate prior\n(GCP) update appropriate for neural networks, which is a modification of the\nclassical Bayesian update for conjugate priors. We establish a connection\nbetween the gradient conjugate prior update and the maximization of the\nlog-likelihood of the predictive distribution. Unlike for the Bayesian neural\nnetworks, we use deterministic weights of neural networks, but rather assume\nthat the ground truth distribution is normal with unknown mean and variance and\nlearn by the neural networks the parameters of a prior (normal-gamma\ndistribution) for these unknown mean and variance. The update of the parameters\nis done, using the gradient that, at each step, directs towards minimizing the\nKullback--Leibler divergence from the prior to the posterior distribution (both\nbeing normal-gamma). We obtain a corresponding dynamical system for the prior's\nparameters and analyze its properties. In particular, we study the limiting\nbehavior of all the prior's parameters and show how it differs from the case of\nthe classical full Bayesian update. The results are validated on synthetic and\nreal world data sets. \n\n"}
{"id": "1802.03583", "contents": "Title: Distributed One-class Learning Abstract: We propose a cloud-based filter trained to block third parties from uploading\nprivacy-sensitive images of others to online social media. The proposed filter\nuses Distributed One-Class Learning, which decomposes the cloud-based filter\ninto multiple one-class classifiers. Each one-class classifier captures the\nproperties of a class of privacy-sensitive images with an autoencoder. The\nmulti-class filter is then reconstructed by combining the parameters of the\none-class autoencoders. The training takes place on edge devices (e.g.\nsmartphones) and therefore users do not need to upload their private and/or\nsensitive images to the cloud. A major advantage of the proposed filter over\nexisting distributed learning approaches is that users cannot access, even\nindirectly, the parameters of other users. Moreover, the filter can cope with\nthe imbalanced and complex distribution of the image content and the\nindependent probability of addition of new users. We evaluate the performance\nof the proposed distributed filter using the exemplar task of blocking a user\nfrom sharing privacy-sensitive images of other users. In particular, we\nvalidate the behavior of the proposed multi-class filter with\nnon-privacy-sensitive images, the accuracy when the number of classes\nincreases, and the robustness to attacks when an adversary user has access to\nprivacy-sensitive images of other users. \n\n"}
{"id": "1802.03689", "contents": "Title: Dual Control Memory Augmented Neural Networks for Treatment\n  Recommendations Abstract: Machine-assisted treatment recommendations hold a promise to reduce physician\ntime and decision errors. We formulate the task as a sequence-to-sequence\nprediction model that takes the entire time-ordered medical history as input,\nand predicts a sequence of future clinical procedures and medications. It is\nbuilt on the premise that an effective treatment plan may have long-term\ndependencies from previous medical history. We approach the problem by using a\nmemory-augmented neural network, in particular, by leveraging the recent\ndifferentiable neural computer that consists of a neural controller and an\nexternal memory module. But differing from the original model, we use dual\ncontrollers, one for encoding the history followed by another for decoding the\ntreatment sequences. In the encoding phase, the memory is updated as new input\nis read; at the end of this phase, the memory holds not only the medical\nhistory but also the information about the current illness. During the decoding\nphase, the memory is write-protected. The decoding controller generates a\ntreatment sequence, one treatment option at a time. The resulting dual\ncontroller write-protected memory-augmented neural network is demonstrated on\nthe MIMIC-III dataset on two tasks: procedure prediction and medication\nprescription. The results show improved performance over both traditional\nbag-of-words and sequence-to-sequence methods. \n\n"}
{"id": "1802.03832", "contents": "Title: Quadrature-based features for kernel approximation Abstract: We consider the problem of improving kernel approximation via randomized\nfeature maps. These maps arise as Monte Carlo approximation to integral\nrepresentations of kernel functions and scale up kernel methods for larger\ndatasets. Based on an efficient numerical integration technique, we propose a\nunifying approach that reinterprets the previous random features methods and\nextends to better estimates of the kernel approximation. We derive the\nconvergence behaviour and conduct an extensive empirical study that supports\nour hypothesis. \n\n"}
{"id": "1802.04020", "contents": "Title: Efficient Bias-Span-Constrained Exploration-Exploitation in\n  Reinforcement Learning Abstract: We introduce SCAL, an algorithm designed to perform efficient\nexploration-exploitation in any unknown weakly-communicating Markov decision\nprocess (MDP) for which an upper bound $c$ on the span of the optimal bias\nfunction is known. For an MDP with $S$ states, $A$ actions and $\\Gamma \\leq S$\npossible next states, we prove a regret bound of $\\widetilde{O}(c\\sqrt{\\Gamma\nSAT})$, which significantly improves over existing algorithms (e.g., UCRL and\nPSRL), whose regret scales linearly with the MDP diameter $D$. In fact, the\noptimal bias span is finite and often much smaller than $D$ (e.g., $D=\\infty$\nin non-communicating MDPs). A similar result was originally derived by Bartlett\nand Tewari (2009) for REGAL.C, for which no tractable algorithm is available.\nIn this paper, we relax the optimization problem at the core of REGAL.C, we\ncarefully analyze its properties, and we provide the first computationally\nefficient algorithm to solve it. Finally, we report numerical simulations\nsupporting our theoretical findings and showing how SCAL significantly\noutperforms UCRL in MDPs with large diameter and small span. \n\n"}
{"id": "1802.04376", "contents": "Title: Few-Shot Learning with Metric-Agnostic Conditional Embeddings Abstract: Learning high quality class representations from few examples is a key\nproblem in metric-learning approaches to few-shot learning. To accomplish this,\nwe introduce a novel architecture where class representations are conditioned\nfor each few-shot trial based on a target image. We also deviate from\ntraditional metric-learning approaches by training a network to perform\ncomparisons between classes rather than relying on a static metric comparison.\nThis allows the network to decide what aspects of each class are important for\nthe comparison at hand. We find that this flexible architecture works well in\npractice, achieving state-of-the-art performance on the Caltech-UCSD birds\nfine-grained classification task. \n\n"}
{"id": "1802.04431", "contents": "Title: Detecting Spacecraft Anomalies Using LSTMs and Nonparametric Dynamic\n  Thresholding Abstract: As spacecraft send back increasing amounts of telemetry data, improved\nanomaly detection systems are needed to lessen the monitoring burden placed on\noperations engineers and reduce operational risk. Current spacecraft monitoring\nsystems only target a subset of anomaly types and often require costly expert\nknowledge to develop and maintain due to challenges involving scale and\ncomplexity. We demonstrate the effectiveness of Long Short-Term Memory (LSTMs)\nnetworks, a type of Recurrent Neural Network (RNN), in overcoming these issues\nusing expert-labeled telemetry anomaly data from the Soil Moisture Active\nPassive (SMAP) satellite and the Mars Science Laboratory (MSL) rover,\nCuriosity. We also propose a complementary unsupervised and nonparametric\nanomaly thresholding approach developed during a pilot implementation of an\nanomaly detection system for SMAP, and offer false positive mitigation\nstrategies along with other key improvements and lessons learned during\ndevelopment. \n\n"}
{"id": "1802.05394", "contents": "Title: Cost-Effective Training of Deep CNNs with Active Model Adaptation Abstract: Deep convolutional neural networks have achieved great success in various\napplications. However, training an effective DNN model for a specific task is\nrather challenging because it requires a prior knowledge or experience to\ndesign the network architecture, repeated trial-and-error process to tune the\nparameters, and a large set of labeled data to train the model. In this paper,\nwe propose to overcome these challenges by actively adapting a pre-trained\nmodel to a new task with less labeled examples. Specifically, the pre-trained\nmodel is iteratively fine tuned based on the most useful examples. The examples\nare actively selected based on a novel criterion, which jointly estimates the\npotential contribution of an instance on optimizing the feature representation\nas well as improving the classification model for the target task. On one hand,\nthe pre-trained model brings plentiful information from its original task,\navoiding redesign of the network architecture or training from scratch; and on\nthe other hand, the labeling cost can be significantly reduced by active label\nquerying. Experiments on multiple datasets and different pre-trained models\ndemonstrate that the proposed approach can achieve cost-effective training of\nDNNs. \n\n"}
{"id": "1802.05668", "contents": "Title: Model compression via distillation and quantization Abstract: Deep neural networks (DNNs) continue to make significant advances, solving\ntasks from image classification to translation or reinforcement learning. One\naspect of the field receiving considerable attention is efficiently executing\ndeep models in resource-constrained environments, such as mobile or embedded\ndevices. This paper focuses on this problem, and proposes two new compression\nmethods, which jointly leverage weight quantization and distillation of larger\nteacher networks into smaller student networks. The first method we propose is\ncalled quantized distillation and leverages distillation during the training\nprocess, by incorporating distillation loss, expressed with respect to the\nteacher, into the training of a student network whose weights are quantized to\na limited set of levels. The second method, differentiable quantization,\noptimizes the location of quantization points through stochastic gradient\ndescent, to better fit the behavior of the teacher model. We validate both\nmethods through experiments on convolutional and recurrent architectures. We\nshow that quantized shallow students can reach similar accuracy levels to\nfull-precision teacher models, while providing order of magnitude compression,\nand inference speedup that is linear in the depth reduction. In sum, our\nresults enable DNNs for resource-constrained environments to leverage\narchitecture and accuracy advances developed on more powerful devices. \n\n"}
{"id": "1802.06309", "contents": "Title: Learning Adversarially Fair and Transferable Representations Abstract: In this paper, we advocate for representation learning as the key to\nmitigating unfair prediction outcomes downstream. Motivated by a scenario where\nlearned representations are used by third parties with unknown objectives, we\npropose and explore adversarial representation learning as a natural method of\nensuring those parties act fairly. We connect group fairness (demographic\nparity, equalized odds, and equal opportunity) to different adversarial\nobjectives. Through worst-case theoretical guarantees and experimental\nvalidation, we show that the choice of this objective is crucial to fair\nprediction. Furthermore, we present the first in-depth experimental\ndemonstration of fair transfer learning and demonstrate empirically that our\nlearned representations admit fair predictions on new tasks while maintaining\nutility, an essential goal of fair representation learning. \n\n"}
{"id": "1802.06394", "contents": "Title: Training Big Random Forests with Little Resources Abstract: Without access to large compute clusters, building random forests on large\ndatasets is still a challenging problem. This is, in particular, the case if\nfully-grown trees are desired. We propose a simple yet effective framework that\nallows to efficiently construct ensembles of huge trees for hundreds of\nmillions or even billions of training instances using a cheap desktop computer\nwith commodity hardware. The basic idea is to consider a multi-level\nconstruction scheme, which builds top trees for small random subsets of the\navailable data and which subsequently distributes all training instances to the\ntop trees' leaves for further processing. While being conceptually simple, the\noverall efficiency crucially depends on the particular implementation of the\ndifferent phases. The practical merits of our approach are demonstrated using\ndense datasets with hundreds of millions of training instances. \n\n"}
{"id": "1802.06480", "contents": "Title: Accelerated Primal-Dual Policy Optimization for Safe Reinforcement\n  Learning Abstract: Constrained Markov Decision Process (CMDP) is a natural framework for\nreinforcement learning tasks with safety constraints, where agents learn a\npolicy that maximizes the long-term reward while satisfying the constraints on\nthe long-term cost. A canonical approach for solving CMDPs is the primal-dual\nmethod which updates parameters in primal and dual spaces in turn. Existing\nmethods for CMDPs only use on-policy data for dual updates, which results in\nsample inefficiency and slow convergence. In this paper, we propose a policy\nsearch method for CMDPs called Accelerated Primal-Dual Optimization (APDO),\nwhich incorporates an off-policy trained dual variable in the dual update\nprocedure while updating the policy in primal space with on-policy likelihood\nratio gradient. Experimental results on a simulated robot locomotion task show\nthat APDO achieves better sample efficiency and faster convergence than\nstate-of-the-art approaches for CMDPs. \n\n"}
{"id": "1802.07107", "contents": "Title: Learning of Optimal Forecast Aggregation in Partial Evidence\n  Environments Abstract: We consider the forecast aggregation problem in repeated settings, where the\nforecasts are done on a binary event. At each period multiple experts provide\nforecasts about an event. The goal of the aggregator is to aggregate those\nforecasts into a subjective accurate forecast. We assume that experts are\nBayesian; namely they share a common prior, each expert is exposed to some\nevidence, and each expert applies Bayes rule to deduce his forecast. The\naggregator is ignorant with respect to the information structure (i.e.,\ndistribution over evidence) according to which experts make their prediction.\nThe aggregator observes the experts' forecasts only. At the end of each period\nthe actual state is realized. We focus on the question whether the aggregator\ncan learn to aggregate optimally the forecasts of the experts, where the\noptimal aggregation is the Bayesian aggregation that takes into account all the\ninformation (evidence) in the system.\n  We consider the class of partial evidence information structures, where each\nexpert is exposed to a different subset of conditionally independent signals.\nOur main results are positive; We show that optimal aggregation can be learned\nin polynomial time in a quite wide range of instances of the partial evidence\nenvironments. We provide a tight characterization of the instances where\nlearning is possible and impossible. \n\n"}
{"id": "1802.07124", "contents": "Title: Out-distribution training confers robustness to deep neural networks Abstract: The easiness at which adversarial instances can be generated in deep neural\nnetworks raises some fundamental questions on their functioning and concerns on\ntheir use in critical systems. In this paper, we draw a connection between\nover-generalization and adversaries: a possible cause of adversaries lies in\nmodels designed to make decisions all over the input space, leading to\ninappropriate high-confidence decisions in parts of the input space not\nrepresented in the training set. We empirically show an augmented neural\nnetwork, which is not trained on any types of adversaries, can increase the\nrobustness by detecting black-box one-step adversaries, i.e. assimilated to\nout-distribution samples, and making generation of white-box one-step\nadversaries harder. \n\n"}
{"id": "1802.07207", "contents": "Title: AutoPrognosis: Automated Clinical Prognostic Modeling via Bayesian\n  Optimization with Structured Kernel Learning Abstract: Clinical prognostic models derived from largescale healthcare data can inform\ncritical diagnostic and therapeutic decisions. To enable off-theshelf usage of\nmachine learning (ML) in prognostic research, we developed AUTOPROGNOSIS: a\nsystem for automating the design of predictive modeling pipelines tailored for\nclinical prognosis. AUTOPROGNOSIS optimizes ensembles of pipeline\nconfigurations efficiently using a novel batched Bayesian optimization (BO)\nalgorithm that learns a low-dimensional decomposition of the pipelines\nhigh-dimensional hyperparameter space in concurrence with the BO procedure.\nThis is achieved by modeling the pipelines performances as a black-box function\nwith a Gaussian process prior, and modeling the similarities between the\npipelines baseline algorithms via a sparse additive kernel with a Dirichlet\nprior. Meta-learning is used to warmstart BO with external data from similar\npatient cohorts by calibrating the priors using an algorithm that mimics the\nempirical Bayes method. The system automatically explains its predictions by\npresenting the clinicians with logical association rules that link patients\nfeatures to predicted risk strata. We demonstrate the utility of AUTOPROGNOSIS\nusing 10 major patient cohorts representing various aspects of cardiovascular\npatient care. \n\n"}
{"id": "1802.07687", "contents": "Title: Stochastic Video Generation with a Learned Prior Abstract: Generating video frames that accurately predict future world states is\nchallenging. Existing approaches either fail to capture the full distribution\nof outcomes, or yield blurry generations, or both. In this paper we introduce\nan unsupervised video generation model that learns a prior model of uncertainty\nin a given environment. Video frames are generated by drawing samples from this\nprior and combining them with a deterministic estimate of the future frame. The\napproach is simple and easily trained end-to-end on a variety of datasets.\nSample generations are both varied and sharp, even many frames into the future,\nand compare favorably to those from existing approaches. \n\n"}
{"id": "1802.08241", "contents": "Title: Hessian-based Analysis of Large Batch Training and Robustness to\n  Adversaries Abstract: Large batch size training of Neural Networks has been shown to incur accuracy\nloss when trained with the current methods. The exact underlying reasons for\nthis are still not completely understood. Here, we study large batch size\ntraining through the lens of the Hessian operator and robust optimization. In\nparticular, we perform a Hessian based study to analyze exactly how the\nlandscape of the loss function changes when training with large batch size. We\ncompute the true Hessian spectrum, without approximation, by back-propagating\nthe second derivative. Extensive experiments on multiple networks show that\nsaddle-points are not the cause for generalization gap of large batch size\ntraining, and the results consistently show that large batch converges to\npoints with noticeably higher Hessian spectrum. Furthermore, we show that\nrobust training allows one to favor flat areas, as points with large Hessian\nspectrum show poor robustness to adversarial perturbation. We further study\nthis relationship, and provide empirical and theoretical proof that the inner\nloop for robust training is a saddle-free optimization problem \\textit{almost\neverywhere}. We present detailed experiments with five different network\narchitectures, including a residual network, tested on MNIST, CIFAR-10, and\nCIFAR-100 datasets. We have open sourced our method which can be accessed at\n[1]. \n\n"}
{"id": "1802.08635", "contents": "Title: Loss-aware Weight Quantization of Deep Networks Abstract: The huge size of deep networks hinders their use in small computing devices.\nIn this paper, we consider compressing the network by weight quantization. We\nextend a recently proposed loss-aware weight binarization scheme to\nternarization, with possibly different scaling parameters for the positive and\nnegative weights, and m-bit (where m > 2) quantization. Experiments on\nfeedforward and recurrent neural networks show that the proposed scheme\noutperforms state-of-the-art weight quantization algorithms, and is as accurate\n(or even more accurate) than the full-precision network. \n\n"}
{"id": "1802.09064", "contents": "Title: Model Agnostic Time Series Analysis via Matrix Estimation Abstract: We propose an algorithm to impute and forecast a time series by transforming\nthe observed time series into a matrix, utilizing matrix estimation to recover\nmissing values and de-noise observed entries, and performing linear regression\nto make predictions. At the core of our analysis is a representation result,\nwhich states that for a large model class, the transformed time series matrix\nis (approximately) low-rank. In effect, this generalizes the widely used\nSingular Spectrum Analysis (SSA) in time series literature, and allows us to\nestablish a rigorous link between time series analysis and matrix estimation.\nThe key to establishing this link is constructing a Page matrix with\nnon-overlapping entries rather than a Hankel matrix as is commonly done in the\nliterature (e.g., SSA). This particular matrix structure allows us to provide\nfinite sample analysis for imputation and prediction, and prove the asymptotic\nconsistency of our method. Another salient feature of our algorithm is that it\nis model agnostic with respect to both the underlying time dynamics and the\nnoise distribution in the observations. The noise agnostic property of our\napproach allows us to recover the latent states when only given access to noisy\nand partial observations a la a Hidden Markov Model; e.g., recovering the\ntime-varying parameter of a Poisson process without knowing that the underlying\nprocess is Poisson. Furthermore, since our forecasting algorithm requires\nregression with noisy features, our approach suggests a matrix estimation based\nmethod - coupled with a novel, non-standard matrix estimation error metric - to\nsolve the error-in-variable regression problem, which could be of interest in\nits own right. Through synthetic and real-world datasets, we demonstrate that\nour algorithm outperforms standard software packages (including R libraries) in\nthe presence of missing data as well as high levels of noise. \n\n"}
{"id": "1802.09514", "contents": "Title: Best Arm Identification for Contaminated Bandits Abstract: This paper studies active learning in the context of robust statistics.\nSpecifically, we propose a variant of the Best Arm Identification problem for\n\\emph{contaminated bandits}, where each arm pull has probability $\\varepsilon$\nof generating a sample from an arbitrary contamination distribution instead of\nthe true underlying distribution. The goal is to identify the best (or\napproximately best) true distribution with high probability, with a secondary\ngoal of providing guarantees on the quality of this distribution. The primary\nchallenge of the contaminated bandit setting is that the true distributions are\nonly partially identifiable, even with infinite samples. To address this, we\ndevelop tight, non-asymptotic sample complexity bounds for high-probability\nestimation of the first two robust moments (median and median absolute\ndeviation) from contaminated samples. These concentration inequalities are the\nmain technical contributions of the paper and may be of independent interest.\nUsing these results, we adapt several classical Best Arm Identification\nalgorithms to the contaminated bandit setting and derive sample complexity\nupper bounds for our problem. Finally, we provide matching\ninformation-theoretic lower bounds on the sample complexity (up to a small\nlogarithmic factor). \n\n"}
{"id": "1802.09902", "contents": "Title: Attention-Based Guided Structured Sparsity of Deep Neural Networks Abstract: Network pruning is aimed at imposing sparsity in a neural network\narchitecture by increasing the portion of zero-valued weights for reducing its\nsize regarding energy-efficiency consideration and increasing evaluation speed.\nIn most of the conducted research efforts, the sparsity is enforced for network\npruning without any attention to the internal network characteristics such as\nunbalanced outputs of the neurons or more specifically the distribution of the\nweights and outputs of the neurons. That may cause severe accuracy drop due to\nuncontrolled sparsity. In this work, we propose an attention mechanism that\nsimultaneously controls the sparsity intensity and supervised network pruning\nby keeping important information bottlenecks of the network to be active. On\nCIFAR-10, the proposed method outperforms the best baseline method by 6% and\nreduced the accuracy drop by 2.6x at the same level of sparsity. \n\n"}
{"id": "1802.09914", "contents": "Title: High-Dimensional Vector Semantics Abstract: In this paper we explore the \"vector semantics\" problem from the perspective\nof \"almost orthogonal\" property of high-dimensional random vectors. We show\nthat this intriguing property can be used to \"memorize\" random vectors by\nsimply adding them, and we provide an efficient probabilistic solution to the\nset membership problem. Also, we discuss several applications to word context\nvector embeddings, document sentences similarity, and spam filtering. \n\n"}
{"id": "1803.00652", "contents": "Title: Q#: Enabling scalable quantum computing and development with a\n  high-level domain-specific language Abstract: Quantum computing exploits quantum phenomena such as superposition and\nentanglement to realize a form of parallelism that is not available to\ntraditional computing. It offers the potential of significant computational\nspeed-ups in quantum chemistry, materials science, cryptography, and machine\nlearning. The dominant approach to programming quantum computers is to provide\nan existing high-level language with libraries that allow for the expression of\nquantum programs. This approach can permit computations that are meaningless in\na quantum context; prohibits succinct expression of interaction between\nclassical and quantum logic; and does not provide important constructs that are\nrequired for quantum programming. We present Q#, a quantum-focused\ndomain-specific language explicitly designed to correctly, clearly and\ncompletely express quantum algorithms. Q# provides a type system, a tightly\nconstrained environment to safely interleave classical and quantum\ncomputations; specialized syntax, symbolic code manipulation to automatically\ngenerate correct transformations of quantum operations, and powerful functional\nconstructs which aid composition. \n\n"}
{"id": "1803.00744", "contents": "Title: Clinically Meaningful Comparisons Over Time: An Approach to Measuring\n  Patient Similarity based on Subsequence Alignment Abstract: Longitudinal patient data has the potential to improve clinical risk\nstratification models for disease. However, chronic diseases that progress\nslowly over time are often heterogeneous in their clinical presentation.\nPatients may progress through disease stages at varying rates. This leads to\npathophysiological misalignment over time, making it difficult to consistently\ncompare patients in a clinically meaningful way. Furthermore, patients present\nclinically for the first time at different stages of disease. This eliminates\nthe possibility of simply aligning patients based on their initial\npresentation. Finally, patient data may be sampled at different rates due to\ndifferences in schedules or missed visits. To address these challenges, we\npropose a robust measure of patient similarity based on subsequence alignment.\nCompared to global alignment techniques that do not account for\npathophysiological misalignment, focusing on the most relevant subsequences\nallows for an accurate measure of similarity between patients. We demonstrate\nthe utility of our approach in settings where longitudinal data, while useful,\nare limited and lack a clear temporal alignment for comparison. Applied to the\ntask of stratifying patients for risk of progression to probable Alzheimer's\nDisease, our approach outperforms models that use only snapshot data (AUROC of\n0.839 vs. 0.812) and models that use global alignment techniques (AUROC of\n0.822). Our results support the hypothesis that patients' trajectories are\nuseful for quantifying inter-patient similarities and that using subsequence\nmatching and can help account for heterogeneity and misalignment in\nlongitudinal data. \n\n"}
{"id": "1803.01013", "contents": "Title: An Overview of Robust Subspace Recovery Abstract: This paper will serve as an introduction to the body of work on robust\nsubspace recovery. Robust subspace recovery involves finding an underlying\nlow-dimensional subspace in a dataset that is possibly corrupted with outliers.\nWhile this problem is easy to state, it has been difficult to develop optimal\nalgorithms due to its underlying nonconvexity. This work emphasizes advantages\nand disadvantages of proposed approaches and unsolved problems in the area. \n\n"}
{"id": "1803.01022", "contents": "Title: Programming Quantum Computers Using Design Automation Abstract: Recent developments in quantum hardware indicate that systems featuring more\nthan 50 physical qubits are within reach. At this scale, classical simulation\nwill no longer be feasible and there is a possibility that such quantum devices\nmay outperform even classical supercomputers at certain tasks. With the rapid\ngrowth of qubit numbers and coherence times comes the increasingly difficult\nchallenge of quantum program compilation. This entails the translation of a\nhigh-level description of a quantum algorithm to hardware-specific low-level\noperations which can be carried out by the quantum device. Some parts of the\ncalculation may still be performed manually due to the lack of efficient\nmethods. This, in turn, may lead to a design gap, which will prevent the\nprogramming of a quantum computer. In this paper, we discuss the challenges in\nfully-automatic quantum compilation. We motivate directions for future research\nto tackle these challenges. Yet, with the algorithms and approaches that exist\ntoday, we demonstrate how to automatically perform the quantum programming flow\nfrom algorithm to a physical quantum computer for a simple algorithmic\nbenchmark, namely the hidden shift problem. We present and use two tool flows\nwhich invoke RevKit. One which is based on ProjectQ and which targets the IBM\nQuantum Experience or a local simulator, and one which is based on Microsoft's\nquantum programming language Q$\\#$. \n\n"}
{"id": "1803.01088", "contents": "Title: Practical Contextual Bandits with Regression Oracles Abstract: A major challenge in contextual bandits is to design general-purpose\nalgorithms that are both practically useful and theoretically well-founded. We\npresent a new technique that has the empirical and computational advantages of\nrealizability-based approaches combined with the flexibility of agnostic\nmethods. Our algorithms leverage the availability of a regression oracle for\nthe value-function class, a more realistic and reasonable oracle than the\nclassification oracles over policies typically assumed by agnostic methods. Our\napproach generalizes both UCB and LinUCB to far more expressive possible model\nclasses and achieves low regret under certain distributional assumptions. In an\nextensive empirical evaluation, compared to both realizability-based and\nagnostic baselines, we find that our approach typically gives comparable or\nsuperior results. \n\n"}
{"id": "1803.02999", "contents": "Title: On First-Order Meta-Learning Algorithms Abstract: This paper considers meta-learning problems, where there is a distribution of\ntasks, and we would like to obtain an agent that performs well (i.e., learns\nquickly) when presented with a previously unseen task sampled from this\ndistribution. We analyze a family of algorithms for learning a parameter\ninitialization that can be fine-tuned quickly on a new task, using only\nfirst-order derivatives for the meta-learning updates. This family includes and\ngeneralizes first-order MAML, an approximation to MAML obtained by ignoring\nsecond-order derivatives. It also includes Reptile, a new algorithm that we\nintroduce here, which works by repeatedly sampling a task, training on it, and\nmoving the initialization towards the trained weights on that task. We expand\non the results from Finn et al. showing that first-order meta-learning\nalgorithms perform well on some well-established benchmarks for few-shot\nclassification, and we provide theoretical analysis aimed at understanding why\nthese algorithms work. \n\n"}
{"id": "1803.03376", "contents": "Title: Learning Approximate Inference Networks for Structured Prediction Abstract: Structured prediction energy networks (SPENs; Belanger & McCallum 2016) use\nneural network architectures to define energy functions that can capture\narbitrary dependencies among parts of structured outputs. Prior work used\ngradient descent for inference, relaxing the structured output to a set of\ncontinuous variables and then optimizing the energy with respect to them. We\nreplace this use of gradient descent with a neural network trained to\napproximate structured argmax inference. This \"inference network\" outputs\ncontinuous values that we treat as the output structure. We develop\nlarge-margin training criteria for joint training of the structured energy\nfunction and inference network. On multi-label classification we report\nspeed-ups of 10-60x compared to (Belanger et al, 2017) while also improving\naccuracy. For sequence labeling with simple structured energies, our approach\nperforms comparably to exact inference while being much faster at test time. We\nthen demonstrate improved accuracy by augmenting the energy with a \"label\nlanguage model\" that scores entire output label sequences, showing it can\nimprove handling of long-distance dependencies in part-of-speech tagging.\nFinally, we show how inference networks can replace dynamic programming for\ntest-time inference in conditional random fields, suggestive for their general\nuse for fast inference in structured settings. \n\n"}
{"id": "1803.03383", "contents": "Title: High-Accuracy Low-Precision Training Abstract: Low-precision computation is often used to lower the time and energy cost of\nmachine learning, and recently hardware accelerators have been developed to\nsupport it. Still, it has been used primarily for inference - not training.\nPrevious low-precision training algorithms suffered from a fundamental\ntradeoff: as the number of bits of precision is lowered, quantization noise is\nadded to the model, which limits statistical accuracy. To address this issue,\nwe describe a simple low-precision stochastic gradient descent variant called\nHALP. HALP converges at the same theoretical rate as full-precision algorithms\ndespite the noise introduced by using low precision throughout execution. The\nkey idea is to use SVRG to reduce gradient variance, and to combine this with a\nnovel technique called bit centering to reduce quantization error. We show that\non the CPU, HALP can run up to $4 \\times$ faster than full-precision SVRG and\ncan match its convergence trajectory. We implemented HALP in TensorQuant, and\nshow that it exceeds the validation performance of plain low-precision SGD on\ntwo deep learning tasks. \n\n"}
{"id": "1803.03467", "contents": "Title: RippleNet: Propagating User Preferences on the Knowledge Graph for\n  Recommender Systems Abstract: To address the sparsity and cold start problem of collaborative filtering,\nresearchers usually make use of side information, such as social networks or\nitem attributes, to improve recommendation performance. This paper considers\nthe knowledge graph as the source of side information. To address the\nlimitations of existing embedding-based and path-based methods for\nknowledge-graph-aware recommendation, we propose Ripple Network, an end-to-end\nframework that naturally incorporates the knowledge graph into recommender\nsystems. Similar to actual ripples propagating on the surface of water, Ripple\nNetwork stimulates the propagation of user preferences over the set of\nknowledge entities by automatically and iteratively extending a user's\npotential interests along links in the knowledge graph. The multiple \"ripples\"\nactivated by a user's historically clicked items are thus superposed to form\nthe preference distribution of the user with respect to a candidate item, which\ncould be used for predicting the final clicking probability. Through extensive\nexperiments on real-world datasets, we demonstrate that Ripple Network achieves\nsubstantial gains in a variety of scenarios, including movie, book and news\nrecommendation, over several state-of-the-art baselines. \n\n"}
{"id": "1803.04051", "contents": "Title: Representation Learning over Dynamic Graphs Abstract: How can we effectively encode evolving information over dynamic graphs into\nlow-dimensional representations? In this paper, we propose DyRep, an inductive\ndeep representation learning framework that learns a set of functions to\nefficiently produce low-dimensional node embeddings that evolves over time. The\nlearned embeddings drive the dynamics of two key processes namely,\ncommunication and association between nodes in dynamic graphs. These processes\nexhibit complex nonlinear dynamics that evolve at different time scales and\nsubsequently contribute to the update of node embeddings. We employ a\ntime-scale dependent multivariate point process model to capture these\ndynamics. We devise an efficient unsupervised learning procedure and\ndemonstrate that our approach significantly outperforms representative\nbaselines on two real-world datasets for the problem of dynamic link prediction\nand event time prediction. \n\n"}
{"id": "1803.04084", "contents": "Title: Link prediction for egocentrically sampled networks Abstract: Link prediction in networks is typically accomplished by estimating or\nranking the probabilities of edges for all pairs of nodes. In practice,\nespecially for social networks, the data are often collected by egocentric\nsampling, which means selecting a subset of nodes and recording all of their\nedges. This sampling mechanism requires different prediction tools than the\ntypical assumption of links missing at random. We propose a new computationally\nefficient link prediction algorithm for egocentrically sampled networks, which\nestimates the underlying probability matrix by estimating its row space. For\nnetworks created by sampling rows, our method outperforms many popular link\nprediction and graphon estimation techniques. \n\n"}
{"id": "1803.04189", "contents": "Title: Noise2Noise: Learning Image Restoration without Clean Data Abstract: We apply basic statistical reasoning to signal reconstruction by machine\nlearning -- learning to map corrupted observations to clean signals -- with a\nsimple and powerful conclusion: it is possible to learn to restore images by\nonly looking at corrupted examples, at performance at and sometimes exceeding\ntraining using clean data, without explicit image priors or likelihood models\nof the corruption. In practice, we show that a single model learns photographic\nnoise removal, denoising synthetic Monte Carlo images, and reconstruction of\nundersampled MRI scans -- all corrupted by different processes -- based on\nnoisy data only. \n\n"}
{"id": "1803.04300", "contents": "Title: Neural Conditional Gradients Abstract: The move from hand-designed to learned optimizers in machine learning has\nbeen quite successful for gradient-based and -free optimizers. When facing a\nconstrained problem, however, maintaining feasibility typically requires a\nprojection step, which might be computationally expensive and not\ndifferentiable. We show how the design of projection-free convex optimization\nalgorithms can be cast as a learning problem based on Frank-Wolfe Networks:\nrecurrent networks implementing the Frank-Wolfe algorithm aka. conditional\ngradients. This allows them to learn to exploit structure when, e.g.,\noptimizing over rank-1 matrices. Our LSTM-learned optimizers outperform\nhand-designed as well learned but unconstrained ones. We demonstrate this for\ntraining support vector machines and softmax classifiers. \n\n"}
{"id": "1803.04497", "contents": "Title: Automated software vulnerability detection with machine learning Abstract: Thousands of security vulnerabilities are discovered in production software\neach year, either reported publicly to the Common Vulnerabilities and Exposures\ndatabase or discovered internally in proprietary code. Vulnerabilities often\nmanifest themselves in subtle ways that are not obvious to code reviewers or\nthe developers themselves. With the wealth of open source code available for\nanalysis, there is an opportunity to learn the patterns of bugs that can lead\nto security vulnerabilities directly from data. In this paper, we present a\ndata-driven approach to vulnerability detection using machine learning,\nspecifically applied to C and C++ programs. We first compile a large dataset of\nhundreds of thousands of open-source functions labeled with the outputs of a\nstatic analyzer. We then compare methods applied directly to source code with\nmethods applied to artifacts extracted from the build process, finding that\nsource-based models perform better. We also compare the application of deep\nneural network models with more traditional models such as random forests and\nfind the best performance comes from combining features learned by deep models\nwith tree-based models. Ultimately, our highest performing model achieves an\narea under the precision-recall curve of 0.49 and an area under the ROC curve\nof 0.87. \n\n"}
{"id": "1803.04933", "contents": "Title: Approximate Quantum Fourier Transform with $O(n \\log(n))$ T gates Abstract: The ability to implement the Quantum Fourier Transform (QFT) efficiently on a\nquantum computer facilitates the advantages offered by a variety of fundamental\nquantum algorithms, such as those for integer factoring, computing discrete\nlogarithm over Abelian groups, solving systems of linear equations, and phase\nestimation, to name a few. The standard fault-tolerant implementation of an\n$n$-qubit unitary QFT approximates the desired transformation by removing\nsmall-angle controlled rotations and synthesizing the remaining ones into\nClifford+T gates, incurring the T-count complexity of $O(n \\log^2(n))$. In this\npaper, we show how to obtain approximate QFT with the T-count of $O(n\n\\log(n))$. Our approach relies on quantum circuits with measurements and\nfeedforward, and on reusing a special quantum state that induces the phase\ngradient transformation. We report asymptotic analysis as well as concrete\ncircuits, demonstrating significant advantages in both theory and practice. \n\n"}
{"id": "1803.05849", "contents": "Title: XNORBIN: A 95 TOp/s/W Hardware Accelerator for Binary Convolutional\n  Neural Networks Abstract: Deploying state-of-the-art CNNs requires power-hungry processors and off-chip\nmemory. This precludes the implementation of CNNs in low-power embedded\nsystems. Recent research shows CNNs sustain extreme quantization, binarizing\ntheir weights and intermediate feature maps, thereby saving 8-32\\x memory and\ncollapsing energy-intensive sum-of-products into XNOR-and-popcount operations.\n  We present XNORBIN, an accelerator for binary CNNs with computation tightly\ncoupled to memory for aggressive data reuse. Implemented in UMC 65nm technology\nXNORBIN achieves an energy efficiency of 95 TOp/s/W and an area efficiency of\n2.0 TOp/s/MGE at 0.8 V. \n\n"}
{"id": "1803.06272", "contents": "Title: Graph Partition Neural Networks for Semi-Supervised Classification Abstract: We present graph partition neural networks (GPNN), an extension of graph\nneural networks (GNNs) able to handle extremely large graphs. GPNNs alternate\nbetween locally propagating information between nodes in small subgraphs and\nglobally propagating information between the subgraphs. To efficiently\npartition graphs, we experiment with several partitioning algorithms and also\npropose a novel variant for fast processing of large scale graphs. We\nextensively test our model on a variety of semi-supervised node classification\ntasks. Experimental results indicate that GPNNs are either superior or\ncomparable to state-of-the-art methods on a wide variety of datasets for\ngraph-based semi-supervised classification. We also show that GPNNs can achieve\nsimilar performance as standard GNNs with fewer propagation steps. \n\n"}
{"id": "1803.06567", "contents": "Title: A Dual Approach to Scalable Verification of Deep Networks Abstract: This paper addresses the problem of formally verifying desirable properties\nof neural networks, i.e., obtaining provable guarantees that neural networks\nsatisfy specifications relating their inputs and outputs (robustness to bounded\nnorm adversarial perturbations, for example). Most previous work on this topic\nwas limited in its applicability by the size of the network, network\narchitecture and the complexity of properties to be verified. In contrast, our\nframework applies to a general class of activation functions and specifications\non neural network inputs and outputs. We formulate verification as an\noptimization problem (seeking to find the largest violation of the\nspecification) and solve a Lagrangian relaxation of the optimization problem to\nobtain an upper bound on the worst case violation of the specification being\nverified. Our approach is anytime i.e. it can be stopped at any time and a\nvalid bound on the maximum violation can be obtained. We develop specialized\nverification algorithms with provable tightness guarantees under special\nassumptions and demonstrate the practical significance of our general\nverification approach on a variety of verification tasks. \n\n"}
{"id": "1803.06585", "contents": "Title: Learning Long Term Dependencies via Fourier Recurrent Units Abstract: It is a known fact that training recurrent neural networks for tasks that\nhave long term dependencies is challenging. One of the main reasons is the\nvanishing or exploding gradient problem, which prevents gradient information\nfrom propagating to early layers. In this paper we propose a simple recurrent\narchitecture, the Fourier Recurrent Unit (FRU), that stabilizes the gradients\nthat arise in its training while giving us stronger expressive power.\nSpecifically, FRU summarizes the hidden states $h^{(t)}$ along the temporal\ndimension with Fourier basis functions. This allows gradients to easily reach\nany layer due to FRU's residual learning structure and the global support of\ntrigonometric functions. We show that FRU has gradient lower and upper bounds\nindependent of temporal dimension. We also show the strong expressivity of\nsparse Fourier basis, from which FRU obtains its strong expressive power. Our\nexperimental study also demonstrates that with fewer parameters the proposed\narchitecture outperforms other recurrent architectures on many tasks. \n\n"}
{"id": "1803.06898", "contents": "Title: A Mixture of Views Network with Applications to the Classification of\n  Breast Microcalcifications Abstract: In this paper we examine data fusion methods for multi-view data\nclassification. We present a decision concept which explicitly takes into\naccount the input multi-view structure, where for each case there is a\ndifferent subset of relevant views. This data fusion concept, which we dub\nMixture of Views, is implemented by a special purpose neural network\narchitecture. It is demonstrated on the task of classifying breast\nmicrocalcifications as benign or malignant based on CC and MLO mammography\nviews. The single view decisions are combined by a data-driven decision,\naccording to the relevance of each view in a given case, into a global\ndecision. The method is evaluated on a large multi-view dataset extracted from\nthe standardized digital database for screening mammography (DDSM). The\nexperimental results show that our method outperforms previously suggested\nfusion methods. \n\n"}
{"id": "1803.08375", "contents": "Title: Deep Learning using Rectified Linear Units (ReLU) Abstract: We introduce the use of rectified linear units (ReLU) as the classification\nfunction in a deep neural network (DNN). Conventionally, ReLU is used as an\nactivation function in DNNs, with Softmax function as their classification\nfunction. However, there have been several studies on using a classification\nfunction other than Softmax, and this study is an addition to those. We\naccomplish this by taking the activation of the penultimate layer $h_{n - 1}$\nin a neural network, then multiply it by weight parameters $\\theta$ to get the\nraw scores $o_{i}$. Afterwards, we threshold the raw scores $o_{i}$ by $0$,\ni.e. $f(o) = \\max(0, o_{i})$, where $f(o)$ is the ReLU function. We provide\nclass predictions $\\hat{y}$ through argmax function, i.e. argmax $f(x)$. \n\n"}
{"id": "1803.08841", "contents": "Title: The Convergence of Stochastic Gradient Descent in Asynchronous Shared\n  Memory Abstract: Stochastic Gradient Descent (SGD) is a fundamental algorithm in machine\nlearning, representing the optimization backbone for training several classic\nmodels, from regression to neural networks. Given the recent practical focus on\ndistributed machine learning, significant work has been dedicated to the\nconvergence properties of this algorithm under the inconsistent and noisy\nupdates arising from execution in a distributed environment. However,\nsurprisingly, the convergence properties of this classic algorithm in the\nstandard shared-memory model are still not well-understood.\n  In this work, we address this gap, and provide new convergence bounds for\nlock-free concurrent stochastic gradient descent, executing in the classic\nasynchronous shared memory model, against a strong adaptive adversary. Our\nresults give improved upper and lower bounds on the \"price of asynchrony\" when\nexecuting the fundamental SGD algorithm in a concurrent setting. They show that\nthis classic optimization tool can converge faster and with a wider range of\nparameters than previously known under asynchronous iterations. At the same\ntime, we exhibit a fundamental trade-off between the maximum delay in the\nsystem and the rate at which SGD can converge, which governs the set of\nparameters under which this algorithm can still work efficiently. \n\n"}
{"id": "1803.09702", "contents": "Title: HAMLET: Interpretable Human And Machine co-LEarning Technique Abstract: Efficient label acquisition processes are key to obtaining robust\nclassifiers. However, data labeling is often challenging and subject to high\nlevels of label noise. This can arise even when classification targets are well\ndefined, if instances to be labeled are more difficult than the prototypes used\nto define the class, leading to disagreements among the expert community. Here,\nwe enable efficient training of deep neural networks. From low-confidence\nlabels, we iteratively improve their quality by simultaneous learning of\nmachines and experts. We call it Human And Machine co-LEarning Technique\n(HAMLET). Throughout the process, experts become more consistent, while the\nalgorithm provides them with explainable feedback for confirmation. HAMLET uses\na neural embedding function and a memory module filled with diverse reference\nembeddings from different classes. Its output includes classification labels\nand highly relevant reference embeddings as explanation. We took the study of\nbrain monitoring at intensive care unit (ICU) as an application of HAMLET on\ncontinuous electroencephalography (cEEG) data. Although cEEG monitoring yields\nlarge volumes of data, labeling costs and difficulty make it hard to build a\nclassifier. Additionally, while experts agree on the labels of clear-cut\nexamples of cEEG patterns, labeling many real-world cEEG data can be extremely\nchallenging. Thus, a large minority of sequences might be mislabeled. HAMLET\nhas shown significant performance gain against deep learning and other\nbaselines, increasing accuracy from 7.03% to 68.75% on challenging inputs.\nBesides improved performance, clinical experts confirmed the interpretability\nof those reference embeddings in helping explaining the classification results\nby HAMLET. \n\n"}
{"id": "1803.10274", "contents": "Title: A Study of Clustering Techniques and Hierarchical Matrix Formats for\n  Kernel Ridge Regression Abstract: We present memory-efficient and scalable algorithms for kernel methods used\nin machine learning. Using hierarchical matrix approximations for the kernel\nmatrix the memory requirements, the number of floating point operations, and\nthe execution time are drastically reduced compared to standard dense linear\nalgebra routines. We consider both the general $\\mathcal{H}$ matrix\nhierarchical format as well as Hierarchically Semi-Separable (HSS) matrices.\nFurthermore, we investigate the impact of several preprocessing and clustering\ntechniques on the hierarchical matrix compression. Effective clustering of the\ninput leads to a ten-fold increase in efficiency of the compression. The\nalgorithms are implemented using the STRUMPACK solver library. These results\nconfirm that --- with correct tuning of the hyperparameters --- classification\nusing kernel ridge regression with the compressed matrix does not lose\nprediction accuracy compared to the exact --- not compressed --- kernel matrix\nand that our approach can be extended to $\\mathcal{O}(1M)$ datasets, for which\ncomputation with the full kernel matrix becomes prohibitively expensive. We\npresent numerical experiments in a distributed memory environment up to 1,024\nprocessors of the NERSC's Cori supercomputer using well-known datasets to the\nmachine learning community that range from dimension 8 up to 784. \n\n"}
{"id": "1803.10342", "contents": "Title: Classification of crystallization outcomes using deep convolutional\n  neural networks Abstract: The Machine Recognition of Crystallization Outcomes (MARCO) initiative has\nassembled roughly half a million annotated images of macromolecular\ncrystallization experiments from various sources and setups. Here,\nstate-of-the-art machine learning algorithms are trained and tested on\ndifferent parts of this data set. We find that more than 94% of the test images\ncan be correctly labeled, irrespective of their experimental origin. Because\ncrystal recognition is key to high-density screening and the systematic\nanalysis of crystallization experiments, this approach opens the door to both\nindustrial and fundamental research applications. \n\n"}
{"id": "1804.00097", "contents": "Title: Adversarial Attacks and Defences Competition Abstract: To accelerate research on adversarial examples and robustness of machine\nlearning classifiers, Google Brain organized a NIPS 2017 competition that\nencouraged researchers to develop new methods to generate adversarial examples\nas well as to develop new ways to defend against them. In this chapter, we\ndescribe the structure and organization of the competition and the solutions\ndeveloped by several of the top-placing teams. \n\n"}
{"id": "1804.00140", "contents": "Title: Generative Adversarial Networks (GANs): What it can generate and What it\n  cannot? Abstract: In recent years, Generative Adversarial Networks (GANs) have received\nsignificant attention from the research community. With a straightforward\nimplementation and outstanding results, GANs have been used for numerous\napplications. Despite the success, GANs lack a proper theoretical explanation.\nThese models suffer from issues like mode collapse, non-convergence, and\ninstability during training. To address these issues, researchers have proposed\ntheoretically rigorous frameworks inspired by varied fields of Game theory,\nStatistical theory, Dynamical systems, etc.\n  In this paper, we propose to give an appropriate structure to study these\ncontributions systematically. We essentially categorize the papers based on the\nissues they raise and the kind of novelty they introduce to address them.\nBesides, we provide insight into how each of the discussed articles solves the\nconcerned problems. We compare and contrast different results and put forth a\nsummary of theoretical contributions about GANs with focus on image/visual\napplications. We expect this summary paper to give a bird's eye view to a\nperson wishing to understand the theoretical progress in GANs so far. \n\n"}
{"id": "1804.00921", "contents": "Title: DeSIGN: Design Inspiration from Generative Networks Abstract: Can an algorithm create original and compelling fashion designs to serve as\nan inspirational assistant? To help answer this question, we design and\ninvestigate different image generation models associated with different loss\nfunctions to boost creativity in fashion generation. The dimensions of our\nexplorations include: (i) different Generative Adversarial Networks\narchitectures that start from noise vectors to generate fashion items, (ii)\nnovel loss functions that encourage novelty, inspired from Sharma-Mittal\ndivergence, a generalized mutual information measure for the widely used\nrelative entropies such as Kullback-Leibler, and (iii) a generation process\nfollowing the key elements of fashion design (disentangling shape and texture\ncomponents). A key challenge of this study is the evaluation of generated\ndesigns and the retrieval of best ones, hence we put together an evaluation\nprotocol associating automatic metrics and human experimental studies that we\nhope will help ease future research. We show that our proposed creativity\ncriterion yield better overall appreciation than the one employed in Creative\nAdversarial Networks. In the end, about 61% of our images are thought to be\ncreated by human designers rather than by a computer while also being\nconsidered original per our human subject experiments, and our proposed loss\nscores the highest compared to existing losses in both novelty and likability. \n\n"}
{"id": "1804.01119", "contents": "Title: Feature selection in weakly coherent matrices Abstract: A problem of paramount importance in both pure (Restricted Invertibility\nproblem) and applied mathematics (Feature extraction) is the one of selecting a\nsubmatrix of a given matrix, such that this submatrix has its smallest singular\nvalue above a specified level. Such problems can be addressed using\nperturbation analysis. In this paper, we propose a perturbation bound for the\nsmallest singular value of a given matrix after appending a column, under the\nassumption that its initial coherence is not large, and we use this bound to\nderive a fast algorithm for feature extraction. \n\n"}
{"id": "1804.02476", "contents": "Title: Associative Compression Networks for Representation Learning Abstract: This paper introduces Associative Compression Networks (ACNs), a new\nframework for variational autoencoding with neural networks. The system differs\nfrom existing variational autoencoders (VAEs) in that the prior distribution\nused to model each code is conditioned on a similar code from the dataset. In\ncompression terms this equates to sequentially transmitting the dataset using\nan ordering determined by proximity in latent space. Since the prior need only\naccount for local, rather than global variations in the latent space, the\ncoding cost is greatly reduced, leading to rich, informative codes. Crucially,\nthe codes remain informative when powerful, autoregressive decoders are used,\nwhich we argue is fundamentally difficult with normal VAEs. Experimental\nresults on MNIST, CIFAR-10, ImageNet and CelebA show that ACNs discover\nhigh-level latent features such as object class, writing style, pose and facial\nexpression, which can be used to cluster and classify the data, as well as to\ngenerate diverse and convincing samples. We conclude that ACNs are a promising\nnew direction for representation learning: one that steps away from IID\nmodelling, and towards learning a structured description of the dataset as a\nwhole. \n\n"}
{"id": "1804.03346", "contents": "Title: Learning Latent Events from Network Message Logs Abstract: We consider the problem of separating error messages generated in large\ndistributed data center networks into error events. In such networks, each\nerror event leads to a stream of messages generated by hardware and software\ncomponents affected by the event. These messages are stored in a giant message\nlog. We consider the unsupervised learning problem of identifying the\nsignatures of events that generated these messages; here, the signature of an\nerror event refers to the mixture of messages generated by the event. One of\nthe main contributions of the paper is a novel mapping of our problem which\ntransforms it into a problem of topic discovery in documents. Events in our\nproblem correspond to topics and messages in our problem correspond to words in\nthe topic discovery problem. However, there is no direct analog of documents.\nTherefore, we use a non-parametric change-point detection algorithm, which has\nlinear computational complexity in the number of messages, to divide the\nmessage log into smaller subsets called episodes, which serve as the\nequivalents of documents. After this mapping has been done, we use a well-known\nalgorithm for topic discovery, called LDA, to solve our problem. We\ntheoretically analyze the change-point detection algorithm, and show that it is\nconsistent and has low sample complexity. We also demonstrate the scalability\nof our algorithm on a real data set consisting of $97$ million messages\ncollected over a period of $15$ days, from a distributed data center network\nwhich supports the operations of a large wireless service provider. \n\n"}
{"id": "1804.03629", "contents": "Title: Probabilistic Prediction of Vehicle Semantic Intention and Motion Abstract: Accurately predicting the possible behaviors of traffic participants is an\nessential capability for future autonomous vehicles. The majority of current\nresearches fix the number of driving intentions by considering only a specific\nscenario. However, distinct driving environments usually contain various\npossible driving maneuvers. Therefore, a intention prediction method that can\nadapt to different traffic scenarios is needed. To further improve the overall\nvehicle prediction performance, motion information is usually incorporated with\nclassified intentions. As suggested in some literature, the methods that\ndirectly predict possible goal locations can achieve better performance for\nlong-term motion prediction than other approaches due to their automatic\nincorporation of environment constraints. Moreover, by obtaining the temporal\ninformation of the predicted destinations, the optimal trajectories for\npredicted vehicles as well as the desirable path for ego autonomous vehicle\ncould be easily generated. In this paper, we propose a Semantic-based Intention\nand Motion Prediction (SIMP) method, which can be adapted to any driving\nscenarios by using semantic-defined vehicle behaviors. It utilizes a\nprobabilistic framework based on deep neural network to estimate the\nintentions, final locations, and the corresponding time information for\nsurrounding vehicles. An exemplar real-world scenario was used to implement and\nexamine the proposed method. \n\n"}
{"id": "1804.04212", "contents": "Title: Word2Vec applied to Recommendation: Hyperparameters Matter Abstract: Skip-gram with negative sampling, a popular variant of Word2vec originally\ndesigned and tuned to create word embeddings for Natural Language Processing,\nhas been used to create item embeddings with successful applications in\nrecommendation. While these fields do not share the same type of data, neither\nevaluate on the same tasks, recommendation applications tend to use the same\nalready tuned hyperparameters values, even if optimal hyperparameters values\nare often known to be data and task dependent. We thus investigate the marginal\nimportance of each hyperparameter in a recommendation setting through large\nhyperparameter grid searches on various datasets. Results reveal that\noptimizing neglected hyperparameters, namely negative sampling distribution,\nnumber of epochs, subsampling parameter and window-size, significantly improves\nperformance on a recommendation task, and can increase it by an order of\nmagnitude. Importantly, we find that optimal hyperparameters configurations for\nNatural Language Processing tasks and Recommendation tasks are noticeably\ndifferent. \n\n"}
{"id": "1804.04577", "contents": "Title: Feature-Based Aggregation and Deep Reinforcement Learning: A Survey and\n  Some New Implementations Abstract: In this paper we discuss policy iteration methods for approximate solution of\na finite-state discounted Markov decision problem, with a focus on\nfeature-based aggregation methods and their connection with deep reinforcement\nlearning schemes. We introduce features of the states of the original problem,\nand we formulate a smaller \"aggregate\" Markov decision problem, whose states\nrelate to the features. We discuss properties and possible implementations of\nthis type of aggregation, including a new approach to approximate policy\niteration. In this approach the policy improvement operation combines\nfeature-based aggregation with feature construction using deep neural networks\nor other calculations. We argue that the cost function of a policy may be\napproximated much more accurately by the nonlinear function of the features\nprovided by aggregation, than by the linear function of the features provided\nby neural network-based reinforcement learning, thereby potentially leading to\nmore effective policy improvement. \n\n"}
{"id": "1804.04758", "contents": "Title: MOVI: A Model-Free Approach to Dynamic Fleet Management Abstract: Modern vehicle fleets, e.g., for ridesharing platforms and taxi companies,\ncan reduce passengers' waiting times by proactively dispatching vehicles to\nlocations where pickup requests are anticipated in the future. Yet it is\nunclear how to best do this: optimal dispatching requires optimizing over\nseveral sources of uncertainty, including vehicles' travel times to their\ndispatched locations, as well as coordinating between vehicles so that they do\nnot attempt to pick up the same passenger. While prior works have developed\nmodels for this uncertainty and used them to optimize dispatch policies, in\nthis work we introduce a model-free approach. Specifically, we propose MOVI, a\nDeep Q-network (DQN)-based framework that directly learns the optimal vehicle\ndispatch policy. Since DQNs scale poorly with a large number of possible\ndispatches, we streamline our DQN training and suppose that each individual\nvehicle independently learns its own optimal policy, ensuring scalability at\nthe cost of less coordination between vehicles. We then formulate a centralized\nreceding-horizon control (RHC) policy to compare with our DQN policies. To\ncompare these policies, we design and build MOVI as a large-scale realistic\nsimulator based on 15 million taxi trip records that simulates policy-agnostic\nresponses to dispatch decisions. We show that the DQN dispatch policy reduces\nthe number of unserviced requests by 76% compared to without dispatch and 20%\ncompared to the RHC approach, emphasizing the benefits of a model-free approach\nand suggesting that there is limited value to coordinating vehicle actions.\nThis finding may help to explain the success of ridesharing platforms, for\nwhich drivers make individual decisions. \n\n"}
{"id": "1804.04849", "contents": "Title: The unreasonable effectiveness of the forget gate Abstract: Given the success of the gated recurrent unit, a natural question is whether\nall the gates of the long short-term memory (LSTM) network are necessary.\nPrevious research has shown that the forget gate is one of the most important\ngates in the LSTM. Here we show that a forget-gate-only version of the LSTM\nwith chrono-initialized biases, not only provides computational savings but\noutperforms the standard LSTM on multiple benchmark datasets and competes with\nsome of the best contemporary models. Our proposed network, the JANET, achieves\naccuracies of 99% and 92.5% on the MNIST and pMNIST datasets, outperforming the\nstandard LSTM which yields accuracies of 98.5% and 91%. \n\n"}
{"id": "1804.06218", "contents": "Title: Hierarchical correlation reconstruction with missing data, for example\n  for biology-inspired neuron Abstract: Machine learning often needs to model density from a multidimensional data\nsample, including correlations between coordinates. Additionally, we often have\nmissing data case: that data points can miss values for some of coordinates.\nThis article adapts rapid parametric density estimation approach for this\npurpose: modelling density as a linear combination of orthonormal functions,\nfor which $L^2$ optimization says that (independently) estimated coefficient\nfor a given function is just average over the sample of value of this function.\nHierarchical correlation reconstruction first models probability density for\neach separate coordinate using all its appearances in data sample, then adds\ncorrections from independently modelled pairwise correlations using all samples\nhaving both coordinates, and so on independently adding correlations for\ngrowing numbers of variables using often decreasing evidence in data sample. A\nbasic application of such modelled multidimensional density can be imputation\nof missing coordinates: by inserting known coordinates to the density, and\ntaking expected values for the missing coordinates, or even their entire joint\nprobability distribution. Presented method can be compared with cascade\ncorrelations approach, offering several advantages in flexibility and accuracy.\nIt can be also used as artificial neuron: maximizing prediction capabilities\nfor only local behavior - modelling and predicting local connections. \n\n"}
{"id": "1804.06378", "contents": "Title: Graph-based Selective Outlier Ensembles Abstract: An ensemble technique is characterized by the mechanism that generates the\ncomponents and by the mechanism that combines them. A common way to achieve the\nconsensus is to enable each component to equally participate in the aggregation\nprocess. A problem with this approach is that poor components are likely to\nnegatively affect the quality of the consensus result. To address this issue,\nalternatives have been explored in the literature to build selective classifier\nand cluster ensembles, where only a subset of the components contributes to the\ncomputation of the consensus. Of the family of ensemble methods, outlier\nensembles are the least studied. Only recently, the selection problem for\noutlier ensembles has been discussed. In this work we define a new graph-based\nclass of ranking selection methods. A method in this class is characterized by\ntwo main steps: (1) Mapping the rankings onto a graph structure; and (2) Mining\nthe resulting graph to identify a subset of rankings. We define a specific\ninstance of the graph-based ranking selection class. Specifically, we map the\nproblem of selecting ensemble components onto a mining problem in a graph. An\nextensive evaluation was conducted on a variety of heterogeneous data and\nmethods. Our empirical results show that our approach outperforms\nstate-of-the-art selective outlier ensemble techniques. \n\n"}
{"id": "1804.07045", "contents": "Title: Semantic Adversarial Deep Learning Abstract: Fueled by massive amounts of data, models produced by machine-learning (ML)\nalgorithms, especially deep neural networks, are being used in diverse domains\nwhere trustworthiness is a concern, including automotive systems, finance,\nhealth care, natural language processing, and malware detection. Of particular\nconcern is the use of ML algorithms in cyber-physical systems (CPS), such as\nself-driving cars and aviation, where an adversary can cause serious\nconsequences. However, existing approaches to generating adversarial examples\nand devising robust ML algorithms mostly ignore the semantics and context of\nthe overall system containing the ML component. For example, in an autonomous\nvehicle using deep learning for perception, not every adversarial example for\nthe neural network might lead to a harmful consequence. Moreover, one may want\nto prioritize the search for adversarial examples towards those that\nsignificantly modify the desired semantics of the overall system. Along the\nsame lines, existing algorithms for constructing robust ML algorithms ignore\nthe specification of the overall system. In this paper, we argue that the\nsemantics and specification of the overall system has a crucial role to play in\nthis line of research. We present preliminary research results that support\nthis claim. \n\n"}
{"id": "1804.07209", "contents": "Title: NAIS-Net: Stable Deep Networks from Non-Autonomous Differential\n  Equations Abstract: This paper introduces Non-Autonomous Input-Output Stable Network(NAIS-Net), a\nvery deep architecture where each stacked processing block is derived from a\ntime-invariant non-autonomous dynamical system. Non-autonomy is implemented by\nskip connections from the block input to each of the unrolled processing stages\nand allows stability to be enforced so that blocks can be unrolled adaptively\nto a pattern-dependent processing depth. NAIS-Net induces non-trivial,\nLipschitz input-output maps, even for an infinite unroll length. We prove that\nthe network is globally asymptotically stable so that for every initial\ncondition there is exactly one input-dependent equilibrium assuming $tanh$\nunits, and incrementally stable for ReL units. An efficient implementation that\nenforces the stability under derived conditions for both fully-connected and\nconvolutional layers is also presented. Experimental results show how NAIS-Net\nexhibits stability in practice, yielding a significant reduction in\ngeneralization gap compared to ResNets. \n\n"}
{"id": "1804.07837", "contents": "Title: Online Improper Learning with an Approximation Oracle Abstract: We revisit the question of reducing online learning to approximate\noptimization of the offline problem. In this setting, we give two algorithms\nwith near-optimal performance in the full information setting: they guarantee\noptimal regret and require only poly-logarithmically many calls to the\napproximation oracle per iteration. Furthermore, these algorithms apply to the\nmore general improper learning problems. In the bandit setting, our algorithm\nalso significantly improves the best previously known oracle complexity while\nmaintaining the same regret. \n\n"}
{"id": "1804.08369", "contents": "Title: Gaussian Material Synthesis Abstract: We present a learning-based system for rapid mass-scale material synthesis\nthat is useful for novice and expert users alike. The user preferences are\nlearned via Gaussian Process Regression and can be easily sampled for new\nrecommendations. Typically, each recommendation takes 40-60 seconds to render\nwith global illumination, which makes this process impracticable for real-world\nworkflows. Our neural network eliminates this bottleneck by providing\nhigh-quality image predictions in real time, after which it is possible to pick\nthe desired materials from a gallery and assign them to a scene in an intuitive\nmanner. Workflow timings against Disney's \"principled\" shader reveal that our\nsystem scales well with the number of sought materials, thus empowering even\nnovice users to generate hundreds of high-quality material models without any\nexpertise in material modeling. Similarly, expert users experience a\nsignificant decrease in the total modeling time when populating a scene with\nmaterials. Furthermore, our proposed solution also offers controllable\nrecommendations and a novel latent space variant generation step to enable the\nreal-time fine-tuning of materials without requiring any domain expertise. \n\n"}
{"id": "1804.08615", "contents": "Title: QSAR Classification Modeling for Bioactivity of Molecular Structure via\n  SPL-Logsum Abstract: Quantitative structure-activity relationship (QSAR) modelling is effective\n'bridge' to search the reliable relationship related bioactivity to molecular\nstructure. A QSAR classification model contains a lager number of redundant,\nnoisy and irrelevant descriptors. To address this problem, various of methods\nhave been proposed for descriptor selection. Generally, they can be grouped\ninto three categories: filters, wrappers, and embedded methods. Regularization\nmethod is an important embedded technology, which can be used for continuous\nshrinkage and automatic descriptors selection. In recent years, the interest of\nresearchers in the application of regularization techniques is increasing in\ndescriptors selection , such as, logistic regression(LR) with $L_1$ penalty. In\nthis paper, we proposed a novel descriptor selection method based on self-paced\nlearning(SPL) with Logsum penalized LR for predicting the bioactivity of\nmolecular structure. SPL inspired by the learning process of humans and animals\nthat gradually learns from easy samples(smaller losses) to hard samples(bigger\nlosses) samples into training and Logsum regularization has capacity to select\nfew meaningful and significant molecular descriptors, respectively.\nExperimental results on simulation and three public QSAR datasets show that our\nproposed SPL-Logsum method outperforms other commonly used sparse methods in\nterms of classification performance and model interpretation. \n\n"}
{"id": "1804.09400", "contents": "Title: 3D Consistent & Robust Segmentation of Cardiac Images by Deep Learning\n  with Spatial Propagation Abstract: We propose a method based on deep learning to perform cardiac segmentation on\nshort axis MRI image stacks iteratively from the top slice (around the base) to\nthe bottom slice (around the apex). At each iteration, a novel variant of U-net\nis applied to propagate the segmentation of a slice to the adjacent slice below\nit. In other words, the prediction of a segmentation of a slice is dependent\nupon the already existing segmentation of an adjacent slice. 3D-consistency is\nhence explicitly enforced. The method is trained on a large database of 3078\ncases from UK Biobank. It is then tested on 756 different cases from UK Biobank\nand three other state-of-the-art cohorts (ACDC with 100 cases, Sunnybrook with\n30 cases, RVSC with 16 cases). Results comparable or even better than the\nstate-of-the-art in terms of distance measures are achieved. They also\nemphasize the assets of our method, namely enhanced spatial consistency\n(currently neither considered nor achieved by the state-of-the-art), and the\ngeneralization ability to unseen cases even from other databases. \n\n"}
{"id": "1805.00521", "contents": "Title: Direct Runge-Kutta Discretization Achieves Acceleration Abstract: We study gradient-based optimization methods obtained by directly\ndiscretizing a second-order ordinary differential equation (ODE) related to the\ncontinuous limit of Nesterov's accelerated gradient method. When the function\nis smooth enough, we show that acceleration can be achieved by a stable\ndiscretization of this ODE using standard Runge-Kutta integrators.\nSpecifically, we prove that under Lipschitz-gradient, convexity and\norder-$(s+2)$ differentiability assumptions, the sequence of iterates generated\nby discretizing the proposed second-order ODE converges to the optimal solution\nat a rate of $\\mathcal{O}({N^{-2\\frac{s}{s+1}}})$, where $s$ is the order of\nthe Runge-Kutta numerical integrator. Furthermore, we introduce a new local\nflatness condition on the objective, under which rates even faster than\n$\\mathcal{O}(N^{-2})$ can be achieved with low-order integrators and only\ngradient information. Notably, this flatness condition is satisfied by several\nstandard loss functions used in machine learning. We provide numerical\nexperiments that verify the theoretical rates predicted by our results. \n\n"}
{"id": "1805.00784", "contents": "Title: Markov Chain Neural Networks Abstract: In this work we present a modified neural network model which is capable to\nsimulate Markov Chains. We show how to express and train such a network, how to\nensure given statistical properties reflected in the training data and we\ndemonstrate several applications where the network produces non-deterministic\noutcomes. One example is a random walker model, e.g. useful for simulation of\nBrownian motions or a natural Tic-Tac-Toe network which ensures\nnon-deterministic game behavior. \n\n"}
{"id": "1805.01252", "contents": "Title: Improving a Neural Semantic Parser by Counterfactual Learning from Human\n  Bandit Feedback Abstract: Counterfactual learning from human bandit feedback describes a scenario where\nuser feedback on the quality of outputs of a historic system is logged and used\nto improve a target system. We show how to apply this learning framework to\nneural semantic parsing. From a machine learning perspective, the key challenge\nlies in a proper reweighting of the estimator so as to avoid known degeneracies\nin counterfactual learning, while still being applicable to stochastic gradient\noptimization. To conduct experiments with human users, we devise an easy-to-use\ninterface to collect human feedback on semantic parses. Our work is the first\nto show that semantic parsers can be improved significantly by counterfactual\nlearning from logged human feedback data. \n\n"}
{"id": "1805.01516", "contents": "Title: How deep should be the depth of convolutional neural networks: a\n  backyard dog case study Abstract: The work concerns the problem of reducing a pre-trained deep neuronal network\nto a smaller network, with just few layers, whilst retaining the network's\nfunctionality on a given task\n  The proposed approach is motivated by the observation that the aim to deliver\nthe highest accuracy possible in the broadest range of operational conditions,\nwhich many deep neural networks models strive to achieve, may not necessarily\nbe always needed, desired, or even achievable due to the lack of data or\ntechnical constraints. In relation to the face recognition problem, we\nformulated an example of such a usecase, the `backyard dog' problem. The\n`backyard dog', implemented by a lean network, should correctly identify\nmembers from a limited group of individuals, a `family', and should distinguish\nbetween them. At the same time, the network must produce an alarm to an image\nof an individual who is not in a member of the family. To produce such a\nnetwork, we propose a shallowing algorithm. The algorithm takes an existing\ndeep learning model on its input and outputs a shallowed version of it. The\nalgorithm is non-iterative and is based on the Advanced Supervised Principal\nComponent Analysis. Performance of the algorithm is assessed in exhaustive\nnumerical experiments. In the above usecase, the `backyard dog' problem, the\nmethod is capable of drastically reducing the depth of deep learning neural\nnetworks, albeit at the cost of mild performance deterioration.\n  We developed a simple non-iterative method for shallowing down pre-trained\ndeep networks. The method is generic in the sense that it applies to a broad\nclass of feed-forward networks, and is based on the Advanced Supervise\nPrincipal Component Analysis. The method enables generation of families of\nsmaller-size shallower specialized networks tuned for specific operational\nconditions and tasks from a single larger and more universal legacy network. \n\n"}
{"id": "1805.01626", "contents": "Title: Estimating Learnability in the Sublinear Data Regime Abstract: We consider the problem of estimating how well a model class is capable of\nfitting a distribution of labeled data. We show that it is often possible to\naccurately estimate this \"learnability\" even when given an amount of data that\nis too small to reliably learn any accurate model. Our first result applies to\nthe setting where the data is drawn from a $d$-dimensional distribution with\nisotropic covariance (or known covariance), and the label of each datapoint is\nan arbitrary noisy function of the datapoint. In this setting, we show that\nwith $O(\\sqrt{d})$ samples, one can accurately estimate the fraction of the\nvariance of the label that can be explained via the best linear function of the\ndata. In contrast to this sublinear sample size, finding an approximation of\nthe best-fit linear function requires on the order of $d$ samples. Our\nsublinear sample results and approach also extend to the non-isotropic setting,\nwhere the data distribution has an (unknown) arbitrary covariance matrix: we\nshow that, if the label $y$ of point $x$ is a linear function with independent\nnoise, $y = \\langle x , \\beta \\rangle + noise$ with $\\|\\beta \\|$ bounded, the\nvariance of the noise can be estimated to error $\\epsilon$ with\n$O(d^{1-1/\\log{1/\\epsilon}})$ if the covariance matrix has bounded condition\nnumber, or $O(d^{1-\\sqrt{\\epsilon}})$ if there are no bounds on the condition\nnumber. We also establish that these sample complexities are optimal, to\nconstant factors. Finally, we extend these techniques to the setting of binary\nclassification, where we obtain analogous sample complexities for the problem\nof estimating the prediction error of the best linear classifier, in a natural\nmodel of binary labeled data. We demonstrate the practical viability of our\napproaches on several real and synthetic datasets. \n\n"}
{"id": "1805.02136", "contents": "Title: Private Sequential Learning Abstract: We formulate a private learning model to study an intrinsic tradeoff between\nprivacy and query complexity in sequential learning. Our model involves a\nlearner who aims to determine a scalar value, $v^*$, by sequentially querying\nan external database and receiving binary responses. In the meantime, an\nadversary observes the learner's queries, though not the responses, and tries\nto infer from them the value of $v^*$. The objective of the learner is to\nobtain an accurate estimate of $v^*$ using only a small number of queries,\nwhile simultaneously protecting her privacy by making $v^*$ provably difficult\nto learn for the adversary. Our main results provide tight upper and lower\nbounds on the learner's query complexity as a function of desired levels of\nprivacy and estimation accuracy. We also construct explicit query strategies\nwhose complexity is optimal up to an additive constant. \n\n"}
{"id": "1805.02279", "contents": "Title: S4ND: Single-Shot Single-Scale Lung Nodule Detection Abstract: The state of the art lung nodule detection studies rely on computationally\nexpensive multi-stage frameworks to detect nodules from CT scans. To address\nthis computational challenge and provide better performance, in this paper we\npropose S4ND, a new deep learning based method for lung nodule detection. Our\napproach uses a single feed forward pass of a single network for detection and\nprovides better performance when compared to the current literature. The whole\ndetection pipeline is designed as a single $3D$ Convolutional Neural Network\n(CNN) with dense connections, trained in an end-to-end manner. S4ND does not\nrequire any further post-processing or user guidance to refine detection\nresults. Experimentally, we compared our network with the current\nstate-of-the-art object detection network (SSD) in computer vision as well as\nthe state-of-the-art published method for lung nodule detection (3D DCNN). We\nused publically available $888$ CT scans from LUNA challenge dataset and showed\nthat the proposed method outperforms the current literature both in terms of\nefficiency and accuracy by achieving an average FROC-score of $0.897$. We also\nprovide an in-depth analysis of our proposed network to shed light on the\nunclear paradigms of tiny object detection. \n\n"}
{"id": "1805.02306", "contents": "Title: Semi-orthogonal Non-negative Matrix Factorization with an Application in\n  Text Mining Abstract: Emergency Department (ED) crowding is a worldwide issue that affects the\nefficiency of hospital management and the quality of patient care. This occurs\nwhen the request for an admit ward-bed to receive a patient is delayed until an\nadmission decision is made by a doctor. To reduce the overcrowding and waiting\ntime of ED, we build a classifier to predict the disposition of patients using\nmanually-typed nurse notes collected during triage, thereby allowing hospital\nstaff to begin necessary preparation beforehand. However, these triage notes\ninvolve high dimensional, noisy, and also sparse text data which makes model\nfitting and interpretation difficult. To address this issue, we propose the\nsemi-orthogonal non-negative matrix factorization (SONMF) for both continuous\nand binary design matrices to first bi-cluster the patients and words into a\nreduced number of topics. The subjects can then be interpreted as a\nnon-subtractive linear combination of orthogonal basis topic vectors. These\ngenerated topic vectors provide the hospital with a direct understanding of the\ncause of admission. We show that by using a transformation of basis, the\nclassification accuracy can be further increased compared to the conventional\nbag-of-words model and alternative matrix factorization approaches. Through\nsimulated data experiments, we also demonstrate that the proposed method\noutperforms other non-negative matrix factorization (NMF) methods in terms of\nfactorization accuracy, rate of convergence, and degree of orthogonality. \n\n"}
{"id": "1805.02830", "contents": "Title: Several Tunable GMM Kernels Abstract: While tree methods have been popular in practice, researchers and\npractitioners are also looking for simple algorithms which can reach similar\naccuracy of trees. In 2010, (Ping Li UAI'10) developed the method of\n\"abc-robust-logitboost\" and compared it with other supervised learning methods\non datasets used by the deep learning literature. In this study, we propose a\nseries of \"tunable GMM kernels\" which are simple and perform largely comparably\nto tree methods on the same datasets. Note that \"abc-robust-logitboost\"\nsubstantially improved the original \"GDBT\" in that (a) it developed a\ntree-split formula based on second-order information of the derivatives of the\nloss function; (b) it developed a new set of derivatives for multi-class\nclassification formulation.\n  In the prior study in 2017, the \"generalized min-max\" (GMM) kernel was shown\nto have good performance compared to the \"radial-basis function\" (RBF) kernel.\nHowever, as demonstrated in this paper, the original GMM kernel is often not as\ncompetitive as tree methods on the datasets used in the deep learning\nliterature. Since the original GMM kernel has no parameters, we propose tunable\nGMM kernels by adding tuning parameters in various ways. Three basic (i.e.,\nwith only one parameter) GMM kernels are the \"$e$GMM kernel\", \"$p$GMM kernel\",\nand \"$\\gamma$GMM kernel\", respectively. Extensive experiments show that they\nare able to produce good results for a large number of classification tasks.\nFurthermore, the basic kernels can be combined to boost the performance. \n\n"}
{"id": "1805.03644", "contents": "Title: Improving GAN Training via Binarized Representation Entropy (BRE)\n  Regularization Abstract: We propose a novel regularizer to improve the training of Generative\nAdversarial Networks (GANs). The motivation is that when the discriminator D\nspreads out its model capacity in the right way, the learning signals given to\nthe generator G are more informative and diverse. These in turn help G to\nexplore better and discover the real data manifold while avoiding large\nunstable jumps due to the erroneous extrapolation made by D. Our regularizer\nguides the rectifier discriminator D to better allocate its model capacity, by\nencouraging the binary activation patterns on selected internal layers of D to\nhave a high joint entropy. Experimental results on both synthetic data and real\ndatasets demonstrate improvements in stability and convergence speed of the GAN\ntraining, as well as higher sample quality. The approach also leads to higher\nclassification accuracies in semi-supervised learning. \n\n"}
{"id": "1805.04246", "contents": "Title: Convex Programming Based Spectral Clustering Abstract: Clustering is a fundamental task in data analysis, and spectral clustering\nhas been recognized as a promising approach to it. Given a graph describing the\nrelationship between data, spectral clustering explores the underlying cluster\nstructure in two stages. The first stage embeds the nodes of the graph in real\nspace, and the second stage groups the embedded nodes into several clusters.\nThe use of the $k$-means method in the grouping stage is currently standard\npractice. We present a spectral clustering algorithm that uses convex\nprogramming in the grouping stage and study how well it works. This algorithm\nis designed based on the following observation. If a graph is well-clustered,\nthen the nodes with the largest degree in each cluster can be found by\ncomputing an enclosing ellipsoid of the nodes embedded in real space, and the\nclusters can be identified by using those nodes. We show that, for\nwell-clustered graphs, the algorithm can find clusters of nodes with minimal\nconductance. We also give an experimental assessment of the algorithm's\nperformance. \n\n"}
{"id": "1805.04645", "contents": "Title: Low cost quantum circuits for classically intractable instances of the\n  Hamiltonian dynamics simulation problem Abstract: We develop circuit implementations for digital-level quantum Hamiltonian\ndynamics simulation algorithms suitable for implementation on a reconfigurable\nquantum computer, such as trapped ions. Our focus is on the co-design of a\nproblem, its solution, and quantum hardware capable of executing the solution\nat the minimal cost expressed in terms of the quantum computing resources used\nwhile demonstrating the solution of an instance of a scientifically interesting\nproblem that is intractable classically. The choice for Hamiltonian dynamics\nsimulation is due to the combination of its usefulness in the study of\nequilibrium in closed quantum mechanical systems, a low cost in the\nimplementation by quantum algorithms, and the difficulty of classical\nsimulation. By targeting a specific type of quantum computer and tailoring the\nproblem instance and solution to suit physical constraints imposed by the\nhardware, we are able to reduce the resource counts by a factor of $10$ in a\nphysical-level implementation and a factor of $30$ to $60$ in a fault-tolerant\nimplementation over state of the art. \n\n"}
{"id": "1805.04756", "contents": "Title: Improving Predictive Uncertainty Estimation using Dropout -- Hamiltonian\n  Monte Carlo Abstract: Estimating predictive uncertainty is crucial for many computer vision tasks,\nfrom image classification to autonomous driving systems. Hamiltonian Monte\nCarlo (HMC) is an sampling method for performing Bayesian inference. On the\nother hand, Dropout regularization has been proposed as an approximate model\naveraging technique that tends to improve generalization in large scale models\nsuch as deep neural networks. Although, HMC provides convergence guarantees for\nmost standard Bayesian models, it does not handle discrete parameters arising\nfrom Dropout regularization. In this paper, we present a robust methodology for\nimproving predictive uncertainty in classification problems, based on Dropout\nand Hamiltonian Monte Carlo. Even though Dropout induces a non-smooth energy\nfunction with no such convergence guarantees, the resulting discretization of\nthe Hamiltonian proves empirical success. The proposed method allows to\neffectively estimate the predictive accuracy and to provide better\ngeneralization for difficult test examples. \n\n"}
{"id": "1805.04784", "contents": "Title: Nonlinear Metric Learning through Geodesic Interpolation within Lie\n  Groups Abstract: In this paper, we propose a nonlinear distance metric learning scheme based\non the fusion of component linear metrics. Instead of merging displacements at\neach data point, our model calculates the velocities induced by the component\ntransformations, via a geodesic interpolation on a Lie transfor- mation group.\nSuch velocities are later summed up to produce a global transformation that is\nguaranteed to be diffeomorphic. Consequently, pair-wise distances computed this\nway conform to a smooth and spatially varying metric, which can greatly benefit\nk-NN classification. Experiments on synthetic and real datasets demonstrate the\neffectiveness of our model. \n\n"}
{"id": "1805.05010", "contents": "Title: Detecting Adversarial Samples for Deep Neural Networks through Mutation\n  Testing Abstract: Recently, it has been shown that deep neural networks (DNN) are subject to\nattacks through adversarial samples. Adversarial samples are often crafted\nthrough adversarial perturbation, i.e., manipulating the original sample with\nminor modifications so that the DNN model labels the sample incorrectly. Given\nthat it is almost impossible to train perfect DNN, adversarial samples are\nshown to be easy to generate. As DNN are increasingly used in safety-critical\nsystems like autonomous cars, it is crucial to develop techniques for defending\nsuch attacks. Existing defense mechanisms which aim to make adversarial\nperturbation challenging have been shown to be ineffective. In this work, we\npropose an alternative approach. We first observe that adversarial samples are\nmuch more sensitive to perturbations than normal samples. That is, if we impose\nrandom perturbations on a normal and an adversarial sample respectively, there\nis a significant difference between the ratio of label change due to the\nperturbations. Observing this, we design a statistical adversary detection\nalgorithm called nMutant (inspired by mutation testing from software\nengineering community). Our experiments show that nMutant effectively detects\nmost of the adversarial samples generated by recently proposed attacking\nmethods. Furthermore, we provide an error bound with certain statistical\nsignificance along with the detection. \n\n"}
{"id": "1805.05217", "contents": "Title: Experimental investigation of performance differences between Coherent\n  Ising Machines and a quantum annealer Abstract: Physical annealing systems provide heuristic approaches to solving NP-hard\nIsing optimization problems. Here, we study the performance of two types of\nannealing machines--a commercially available quantum annealer built by D-Wave\nSystems, and measurement-feedback coherent Ising machines (CIMs) based on\noptical parametric oscillator networks--on two classes of problems, the\nSherrington-Kirkpatrick (SK) model and MAX-CUT. The D-Wave quantum annealer\noutperforms the CIMs on MAX-CUT on regular graphs of degree 3. On denser\nproblems, however, we observe an exponential penalty for the quantum annealer\n($\\exp(-\\alpha_\\textrm{DW} N^2)$) relative to CIMs ($\\exp(-\\alpha_\\textrm{CIM}\nN)$) for fixed anneal times, on both the SK model and on 50%-edge-density\nMAX-CUT, where the coefficients $\\alpha_\\textrm{CIM}$ and $\\alpha_\\textrm{DW}$\nare problem-class-dependent. On instances with over $50$ vertices, a\nseveral-orders-of-magnitude time-to-solution difference exists between CIMs and\nthe D-Wave annealer. An optimal-annealing-time analysis is also consistent with\na significant projected performance difference. The difference in performance\nbetween the sparsely connected D-Wave machine and the measurement-feedback\nfacilitated all-to-all connectivity of the CIMs provides strong experimental\nsupport for efforts to increase the connectivity of quantum annealers. \n\n"}
{"id": "1805.07430", "contents": "Title: Efficient Online Portfolio with Logarithmic Regret Abstract: We study the decades-old problem of online portfolio management and propose\nthe first algorithm with logarithmic regret that is not based on Cover's\nUniversal Portfolio algorithm and admits much faster implementation.\nSpecifically Universal Portfolio enjoys optimal regret $\\mathcal{O}(N\\ln T)$\nfor $N$ financial instruments over $T$ rounds, but requires log-concave\nsampling and has a large polynomial running time. Our algorithm, on the other\nhand, ensures a slightly larger but still logarithmic regret of\n$\\mathcal{O}(N^2(\\ln T)^4)$, and is based on the well-studied Online Mirror\nDescent framework with a novel regularizer that can be implemented via standard\noptimization methods in time $\\mathcal{O}(TN^{2.5})$ per round. The regret of\nall other existing works is either polynomial in $T$ or has a potentially\nunbounded factor such as the inverse of the smallest price relative. \n\n"}
{"id": "1805.07624", "contents": "Title: Nonparametric Bayesian Deep Networks with Local Competition Abstract: The aim of this work is to enable inference of deep networks that retain high\naccuracy for the least possible model complexity, with the latter deduced from\nthe data during inference. To this end, we revisit deep networks that comprise\ncompeting linear units, as opposed to nonlinear units that do not entail any\nform of (local) competition. In this context, our main technical innovation\nconsists in an inferential setup that leverages solid arguments from Bayesian\nnonparametrics. We infer both the needed set of connections or locally\ncompeting sets of units, as well as the required floating-point precision for\nstoring the network parameters. Specifically, we introduce auxiliary discrete\nlatent variables representing which initial network components are actually\nneeded for modeling the data at hand, and perform Bayesian inference over them\nby imposing appropriate stick-breaking priors. As we experimentally show using\nbenchmark datasets, our approach yields networks with less computational\nfootprint than the state-of-the-art, and with no compromises in predictive\naccuracy. \n\n"}
{"id": "1805.07909", "contents": "Title: Quickshift++: Provably Good Initializations for Sample-Based Mean Shift Abstract: We provide initial seedings to the Quick Shift clustering algorithm, which\napproximate the locally high-density regions of the data. Such seedings act as\nmore stable and expressive cluster-cores than the singleton modes found by\nQuick Shift. We establish statistical consistency guarantees for this\nmodification. We then show strong clustering performance on real datasets as\nwell as promising applications to image segmentation. \n\n"}
{"id": "1805.07943", "contents": "Title: Relating Leverage Scores and Density using Regularized Christoffel\n  Functions Abstract: Statistical leverage scores emerged as a fundamental tool for matrix\nsketching and column sampling with applications to low rank approximation,\nregression, random feature learning and quadrature. Yet, the very nature of\nthis quantity is barely understood. Borrowing ideas from the orthogonal\npolynomial literature, we introduce the regularized Christoffel function\nassociated to a positive definite kernel. This uncovers a variational\nformulation for leverage scores for kernel methods and allows to elucidate\ntheir relationships with the chosen kernel as well as population density. Our\nmain result quantitatively describes a decreasing relation between leverage\nscore and population density for a broad class of kernels on Euclidean spaces.\nNumerical simulations support our findings. \n\n"}
{"id": "1805.08061", "contents": "Title: NEWMA: a new method for scalable model-free online change-point\n  detection Abstract: We consider the problem of detecting abrupt changes in the distribution of a\nmulti-dimensional time series, with limited computing power and memory. In this\npaper, we propose a new, simple method for model-free online change-point\ndetection that relies only on fast and light recursive statistics, inspired by\nthe classical Exponential Weighted Moving Average algorithm (EWMA). The\nproposed idea is to compute two EWMA statistics on the stream of data with\ndifferent forgetting factors, and to compare them. By doing so, we show that we\nimplicitly compare recent samples with older ones, without the need to\nexplicitly store them. Additionally, we leverage Random Features (RFs) to\nefficiently use the Maximum Mean Discrepancy as a distance between\ndistributions, furthermore exploiting recent optical hardware to compute\nhigh-dimensional RFs in near constant time. We show that our method is\nsignificantly faster than usual non-parametric methods for a given accuracy. \n\n"}
{"id": "1805.08328", "contents": "Title: Verifiable Reinforcement Learning via Policy Extraction Abstract: While deep reinforcement learning has successfully solved many challenging\ncontrol tasks, its real-world applicability has been limited by the inability\nto ensure the safety of learned policies. We propose an approach to verifiable\nreinforcement learning by training decision tree policies, which can represent\ncomplex policies (since they are nonparametric), yet can be efficiently\nverified using existing techniques (since they are highly structured). The\nchallenge is that decision tree policies are difficult to train. We propose\nVIPER, an algorithm that combines ideas from model compression and imitation\nlearning to learn decision tree policies guided by a DNN policy (called the\noracle) and its Q-function, and show that it substantially outperforms two\nbaselines. We use VIPER to (i) learn a provably robust decision tree policy for\na variant of Atari Pong with a symbolic state space, (ii) learn a decision tree\npolicy for a toy game based on Pong that provably never loses, and (iii) learn\na provably stable decision tree policy for cart-pole. In each case, the\ndecision tree policy achieves performance equal to that of the original DNN\npolicy. \n\n"}
{"id": "1805.08355", "contents": "Title: Opening the black box of deep learning Abstract: The great success of deep learning shows that its technology contains\nprofound truth, and understanding its internal mechanism not only has important\nimplications for the development of its technology and effective application in\nvarious fields, but also provides meaningful insights into the understanding of\nhuman brain mechanism. At present, most of the theoretical research on deep\nlearning is based on mathematics. This dissertation proposes that the neural\nnetwork of deep learning is a physical system, examines deep learning from\nthree different perspectives: microscopic, macroscopic, and physical world\nviews, answers multiple theoretical puzzles in deep learning by using physics\nprinciples. For example, from the perspective of quantum mechanics and\nstatistical physics, this dissertation presents the calculation methods for\nconvolution calculation, pooling, normalization, and Restricted Boltzmann\nMachine, as well as the selection of cost functions, explains why deep learning\nmust be deep, what characteristics are learned in deep learning, why\nConvolutional Neural Networks do not have to be trained layer by layer, and the\nlimitations of deep learning, etc., and proposes the theoretical direction and\nbasis for the further development of deep learning now and in the future. The\nbrilliance of physics flashes in deep learning, we try to establish the deep\nlearning technology based on the scientific theory of physics. \n\n"}
{"id": "1805.08905", "contents": "Title: AffinityNet: semi-supervised few-shot learning for disease type\n  prediction Abstract: While deep learning has achieved great success in computer vision and many\nother fields, currently it does not work very well on patient genomic data with\nthe \"big p, small N\" problem (i.e., a relatively small number of samples with\nhigh-dimensional features). In order to make deep learning work with a small\namount of training data, we have to design new models that facilitate few-shot\nlearning. Here we present the Affinity Network Model (AffinityNet), a data\nefficient deep learning model that can learn from a limited number of training\nexamples and generalize well. The backbone of the AffinityNet model consists of\nstacked k-Nearest-Neighbor (kNN) attention pooling layers. The kNN attention\npooling layer is a generalization of the Graph Attention Model (GAM), and can\nbe applied to not only graphs but also any set of objects regardless of whether\na graph is given or not. As a new deep learning module, kNN attention pooling\nlayers can be plugged into any neural network model just like convolutional\nlayers. As a simple special case of kNN attention pooling layer, feature\nattention layer can directly select important features that are useful for\nclassification tasks. Experiments on both synthetic data and cancer genomic\ndata from TCGA projects show that our AffinityNet model has better\ngeneralization power than conventional neural network models with little\ntraining data. The code is freely available at\nhttps://github.com/BeautyOfWeb/AffinityNet . \n\n"}
{"id": "1805.08952", "contents": "Title: Dictionary Learning by Dynamical Neural Networks Abstract: A dynamical neural network consists of a set of interconnected neurons that\ninteract over time continuously. It can exhibit computational properties in the\nsense that the dynamical system's evolution and/or limit points in the\nassociated state space can correspond to numerical solutions to certain\nmathematical optimization or learning problems. Such a computational system is\nparticularly attractive in that it can be mapped to a massively parallel\ncomputer architecture for power and throughput efficiency, especially if each\nneuron can rely solely on local information (i.e., local memory). Deriving\ngradients from the dynamical network's various states while conforming to this\nlast constraint, however, is challenging. We show that by combining ideas of\ntop-down feedback and contrastive learning, a dynamical network for solving the\nl1-minimizing dictionary learning problem can be constructed, and the true\ngradients for learning are provably computable by individual neurons. Using\nspiking neurons to construct our dynamical network, we present a learning\nprocess, its rigorous mathematical analysis, and numerical results on several\ndictionary learning problems. \n\n"}
{"id": "1805.08974", "contents": "Title: Do Better ImageNet Models Transfer Better? Abstract: Transfer learning is a cornerstone of computer vision, yet little work has\nbeen done to evaluate the relationship between architecture and transfer. An\nimplicit hypothesis in modern computer vision research is that models that\nperform better on ImageNet necessarily perform better on other vision tasks.\nHowever, this hypothesis has never been systematically tested. Here, we compare\nthe performance of 16 classification networks on 12 image classification\ndatasets. We find that, when networks are used as fixed feature extractors or\nfine-tuned, there is a strong correlation between ImageNet accuracy and\ntransfer accuracy ($r = 0.99$ and $0.96$, respectively). In the former setting,\nwe find that this relationship is very sensitive to the way in which networks\nare trained on ImageNet; many common forms of regularization slightly improve\nImageNet accuracy but yield penultimate layer features that are much worse for\ntransfer learning. Additionally, we find that, on two small fine-grained image\nclassification datasets, pretraining on ImageNet provides minimal benefits,\nindicating the learned features from ImageNet do not transfer well to\nfine-grained tasks. Together, our results show that ImageNet architectures\ngeneralize well across datasets, but ImageNet features are less general than\npreviously suggested. \n\n"}
{"id": "1805.09076", "contents": "Title: Constrained Graph Variational Autoencoders for Molecule Design Abstract: Graphs are ubiquitous data structures for representing interactions between\nentities. With an emphasis on the use of graphs to represent chemical\nmolecules, we explore the task of learning to generate graphs that conform to a\ndistribution observed in training data. We propose a variational autoencoder\nmodel in which both encoder and decoder are graph-structured. Our decoder\nassumes a sequential ordering of graph extension steps and we discuss and\nanalyze design choices that mitigate the potential downsides of this\nlinearization. Experiments compare our approach with a wide range of baselines\non the molecule generation task and show that our method is more successful at\nmatching the statistics of the original dataset on semantically important\nmetrics. Furthermore, we show that by using appropriate shaping of the latent\nspace, our model allows us to design molecules that are (locally) optimal in\ndesired properties. \n\n"}
{"id": "1805.09112", "contents": "Title: Hyperbolic Neural Networks Abstract: Hyperbolic spaces have recently gained momentum in the context of machine\nlearning due to their high capacity and tree-likeliness properties. However,\nthe representational power of hyperbolic geometry is not yet on par with\nEuclidean geometry, mostly because of the absence of corresponding hyperbolic\nneural network layers. This makes it hard to use hyperbolic embeddings in\ndownstream tasks. Here, we bridge this gap in a principled manner by combining\nthe formalism of M\\\"obius gyrovector spaces with the Riemannian geometry of the\nPoincar\\'e model of hyperbolic spaces. As a result, we derive hyperbolic\nversions of important deep learning tools: multinomial logistic regression,\nfeed-forward and recurrent neural networks such as gated recurrent units. This\nallows to embed sequential data and perform classification in the hyperbolic\nspace. Empirically, we show that, even if hyperbolic optimization tools are\nlimited, hyperbolic sentence embeddings either outperform or are on par with\ntheir Euclidean variants on textual entailment and noisy-prefix recognition\ntasks. \n\n"}
{"id": "1805.09370", "contents": "Title: Towards Robust Training of Neural Networks by Regularizing Adversarial\n  Gradients Abstract: In recent years, neural networks have demonstrated outstanding effectiveness\nin a large amount of applications.However, recent works have shown that neural\nnetworks are susceptible to adversarial examples, indicating possible flaws\nintrinsic to the network structures. To address this problem and improve the\nrobustness of neural networks, we investigate the fundamental mechanisms behind\nadversarial examples and propose a novel robust training method via regulating\nadversarial gradients. The regulation effectively squeezes the adversarial\ngradients of neural networks and significantly increases the difficulty of\nadversarial example generation.Without any adversarial example involved, the\nrobust training method could generate naturally robust networks, which are\nnear-immune to various types of adversarial examples. Experiments show the\nnaturally robust networks can achieve optimal accuracy against Fast Gradient\nSign Method (FGSM) and C\\&W attacks on MNIST, Cifar10, and Google Speech\nCommand dataset. Moreover, our proposed method also provides neural networks\nwith consistent robustness against transferable attacks. \n\n"}
{"id": "1805.09484", "contents": "Title: Multi-Level Deep Cascade Trees for Conversion Rate Prediction in\n  Recommendation System Abstract: Developing effective and efficient recommendation methods is very challenging\nfor modern e-commerce platforms. Generally speaking, two essential modules\nnamed \"Click-Through Rate Prediction\" (\\textit{CTR}) and \"Conversion Rate\nPrediction\" (\\textit{CVR}) are included, where \\textit{CVR} module is a crucial\nfactor that affects the final purchasing volume directly. However, it is indeed\nvery challenging due to its sparseness nature. In this paper, we tackle this\nproblem by proposing multi-Level Deep Cascade Trees (\\textit{ldcTree}), which\nis a novel decision tree ensemble approach. It leverages deep cascade\nstructures by stacking Gradient Boosting Decision Trees (\\textit{GBDT}) to\neffectively learn feature representation. In addition, we propose to utilize\nthe cross-entropy in each tree of the preceding \\textit{GBDT} as the input\nfeature representation for next level \\textit{GBDT}, which has a clear\nexplanation, i.e., a traversal from root to leaf nodes in the next level\n\\textit{GBDT} corresponds to the combination of certain traversals in the\npreceding \\textit{GBDT}. The deep cascade structure and the combination rule\nenable the proposed \\textit{ldcTree} to have a stronger distributed feature\nrepresentation ability. Moreover, inspired by ensemble learning, we propose an\nEnsemble \\textit{ldcTree} (\\textit{E-ldcTree}) to encourage the model's\ndiversity and enhance the representation ability further. Finally, we propose\nan improved Feature learning method based on \\textit{EldcTree}\n(\\textit{F-EldcTree}) for taking adequate use of weak and strong correlation\nfeatures identified by pre-trained \\textit{GBDT} models. Experimental results\non off-line data set and online deployment demonstrate the effectiveness of the\nproposed methods. \n\n"}
{"id": "1805.09804", "contents": "Title: Implicit Autoencoders Abstract: In this paper, we describe the \"implicit autoencoder\" (IAE), a generative\nautoencoder in which both the generative path and the recognition path are\nparametrized by implicit distributions. We use two generative adversarial\nnetworks to define the reconstruction and the regularization cost functions of\nthe implicit autoencoder, and derive the learning rules based on\nmaximum-likelihood learning. Using implicit distributions allows us to learn\nmore expressive posterior and conditional likelihood distributions for the\nautoencoder. Learning an expressive conditional likelihood distribution enables\nthe latent code to only capture the abstract and high-level information of the\ndata, while the remaining low-level information is captured by the implicit\nconditional likelihood distribution. We show the applications of implicit\nautoencoders in disentangling content and style information, clustering,\nsemi-supervised classification, learning expressive variational distributions,\nand multimodal image-to-image translation from unpaired data. \n\n"}
{"id": "1805.09898", "contents": "Title: Performing Co-Membership Attacks Against Deep Generative Models Abstract: In this paper we propose a new membership attack method called co-membership\nattacks against deep generative models including Variational Autoencoders\n(VAEs) and Generative Adversarial Networks (GANs). Specifically, membership\nattack aims to check whether a given instance x was used in the training data\nor not. A co-membership attack checks whether the given bundle of n instances\nwere in the training, with the prior knowledge that the bundle was either\nentirely used in the training or none at all. Successful membership attacks can\ncompromise the privacy of training data when the generative model is published.\nOur main idea is to cast membership inference of target data x as the\noptimization of another neural network (called the attacker network) to search\nfor the latent encoding to reproduce x. The final reconstruction error is used\ndirectly to conclude whether x was in the training data or not. We conduct\nextensive experiments on a variety of datasets and generative models showing\nthat: our attacker network outperforms prior membership attacks; co-membership\nattacks can be substantially more powerful than single attacks; and VAEs are\nmore susceptible to membership attacks compared to GANs. \n\n"}
{"id": "1805.10111", "contents": "Title: Double Quantization for Communication-Efficient Distributed Optimization Abstract: Modern distributed training of machine learning models suffers from high\ncommunication overhead for synchronizing stochastic gradients and model\nparameters. In this paper, to reduce the communication complexity, we propose\n\\emph{double quantization}, a general scheme for quantizing both model\nparameters and gradients. Three communication-efficient algorithms are proposed\nunder this general scheme. Specifically, (i) we propose a low-precision\nalgorithm AsyLPG with asynchronous parallelism, (ii) we explore integrating\ngradient sparsification with double quantization and develop Sparse-AsyLPG,\n(iii) we show that double quantization can also be accelerated by momentum\ntechnique and design accelerated AsyLPG. We establish rigorous performance\nguarantees for the algorithms, and conduct experiments on a multi-server\ntest-bed to demonstrate that our algorithms can effectively save transmitted\nbits without performance degradation. \n\n"}
{"id": "1805.10377", "contents": "Title: Ergodic Inference: Accelerate Convergence by Optimisation Abstract: Statistical inference methods are fundamentally important in machine\nlearning. Most state-of-the-art inference algorithms are variants of Markov\nchain Monte Carlo (MCMC) or variational inference (VI). However, both methods\nstruggle with limitations in practice: MCMC methods can be computationally\ndemanding; VI methods may have large bias. In this work, we aim to improve upon\nMCMC and VI by a novel hybrid method based on the idea of reducing simulation\nbias of finite-length MCMC chains using gradient-based optimisation. The\nproposed method can generate low-biased samples by increasing the length of\nMCMC simulation and optimising the MCMC hyper-parameters, which offers\nattractive balance between approximation bias and computational efficiency. We\nshow that our method produces promising results on popular benchmarks when\ncompared to recent hybrid methods of MCMC and VI. \n\n"}
{"id": "1805.10477", "contents": "Title: Nonlinear Inductive Matrix Completion based on One-layer Neural Networks Abstract: The goal of a recommendation system is to predict the interest of a user in a\ngiven item by exploiting the existing set of ratings as well as certain\nuser/item features. A standard approach to modeling this problem is Inductive\nMatrix Completion where the predicted rating is modeled as an inner product of\nthe user and the item features projected onto a latent space. In order to learn\nthe parameters effectively from a small number of observed ratings, the latent\nspace is constrained to be low-dimensional which implies that the parameter\nmatrix is constrained to be low-rank. However, such bilinear modeling of the\nratings can be limiting in practice and non-linear prediction functions can\nlead to significant improvements. A natural approach to introducing\nnon-linearity in the prediction function is to apply a non-linear activation\nfunction on top of the projected user/item features. Imposition of\nnon-linearities further complicates an already challenging problem that has two\nsources of non-convexity: a) low-rank structure of the parameter matrix, and b)\nnon-linear activation function. We show that one can still solve the non-linear\nInductive Matrix Completion problem using gradient descent type methods as long\nas the solution is initialized well. That is, close to the optima, the\noptimization function is strongly convex and hence admits standard optimization\ntechniques, at least for certain activation functions, such as Sigmoid and\ntanh. We also highlight the importance of the activation function and show how\nReLU can behave significantly differently than say a sigmoid function. Finally,\nwe apply our proposed technique to recommendation systems and semi-supervised\nclustering, and show that our method can lead to much better performance than\nstandard linear Inductive Matrix Completion methods. \n\n"}
{"id": "1805.10616", "contents": "Title: Dynamic Network Model from Partial Observations Abstract: Can evolving networks be inferred and modeled without directly observing\ntheir nodes and edges? In many applications, the edges of a dynamic network\nmight not be observed, but one can observe the dynamics of stochastic cascading\nprocesses (e.g., information diffusion, virus propagation) occurring over the\nunobserved network. While there have been efforts to infer networks based on\nsuch data, providing a generative probabilistic model that is able to identify\nthe underlying time-varying network remains an open question. Here we consider\nthe problem of inferring generative dynamic network models based on network\ncascade diffusion data. We propose a novel framework for providing a\nnon-parametric dynamic network model--based on a mixture of coupled\nhierarchical Dirichlet processes-- based on data capturing cascade node\ninfection times. Our approach allows us to infer the evolving community\nstructure in networks and to obtain an explicit predictive distribution over\nthe edges of the underlying network--including those that were not involved in\ntransmission of any cascade, or are likely to appear in the future. We show the\neffectiveness of our approach using extensive experiments on synthetic as well\nas real-world networks. \n\n"}
{"id": "1805.10887", "contents": "Title: Block-optimized Variable Bit Rate Neural Image Compression Abstract: In this work, we propose an end-to-end block-based auto-encoder system for\nimage compression. We introduce novel contributions to neural-network based\nimage compression, mainly in achieving binarization simulation, variable bit\nrates with multiple networks, entropy-friendly representations, inference-stage\ncode optimization and performance-improving normalization layers in the\nauto-encoder. We evaluate and show the incremental performance increase of each\nof our contributions. \n\n"}
{"id": "1805.10982", "contents": "Title: Dynamically Sacrificing Accuracy for Reduced Computation: Cascaded\n  Inference Based on Softmax Confidence Abstract: We study the tradeoff between computational effort and classification\naccuracy in a cascade of deep neural networks. During inference, the user sets\nthe acceptable accuracy degradation which then automatically determines\nconfidence thresholds for the intermediate classifiers. As soon as the\nconfidence threshold is met, inference terminates immediately without having to\ncompute the output of the complete network. Confidence levels are derived\ndirectly from the softmax outputs of intermediate classifiers, as we do not\ntrain special decision functions. We show that using a softmax output as a\nconfidence measure in a cascade of deep neural networks leads to a reduction of\n15%-50% in the number of MAC operations while degrading the classification\naccuracy by roughly 1%. Our method can be easily incorporated into pre-trained\nnon-cascaded architectures, as we exemplify on ResNet. Our main contribution is\na method that dynamically adjusts the tradeoff between accuracy and computation\nwithout retraining the model. \n\n"}
{"id": "1805.11614", "contents": "Title: Deep Learning under Privileged Information Using Heteroscedastic Dropout Abstract: Unlike machines, humans learn through rapid, abstract model-building. The\nrole of a teacher is not simply to hammer home right or wrong answers, but\nrather to provide intuitive comments, comparisons, and explanations to a pupil.\nThis is what the Learning Under Privileged Information (LUPI) paradigm\nendeavors to model by utilizing extra knowledge only available during training.\nWe propose a new LUPI algorithm specifically designed for Convolutional Neural\nNetworks (CNNs) and Recurrent Neural Networks (RNNs). We propose to use a\nheteroscedastic dropout (i.e. dropout with a varying variance) and make the\nvariance of the dropout a function of privileged information. Intuitively, this\ncorresponds to using the privileged information to control the uncertainty of\nthe model output. We perform experiments using CNNs and RNNs for the tasks of\nimage classification and machine translation. Our method significantly\nincreases the sample efficiency during learning, resulting in higher accuracy\nwith a large margin when the number of training examples is limited. We also\ntheoretically justify the gains in sample efficiency by providing a\ngeneralization error bound decreasing with $O(\\frac{1}{n})$, where $n$ is the\nnumber of training examples, in an oracle case. \n\n"}
{"id": "1805.11640", "contents": "Title: K-Beam Minimax: Efficient Optimization for Deep Adversarial Learning Abstract: Minimax optimization plays a key role in adversarial training of machine\nlearning algorithms, such as learning generative models, domain adaptation,\nprivacy preservation, and robust learning. In this paper, we demonstrate the\nfailure of alternating gradient descent in minimax optimization problems due to\nthe discontinuity of solutions of the inner maximization. To address this, we\npropose a new epsilon-subgradient descent algorithm that addresses this problem\nby simultaneously tracking K candidate solutions. Practically, the algorithm\ncan find solutions that previous saddle-point algorithms cannot find, with only\na sublinear increase of complexity in K. We analyze the conditions under which\nthe algorithm converges to the true solution in detail. A significant\nimprovement in stability and convergence speed of the algorithm is observed in\nsimple representative problems, GAN training, and domain-adaptation problems. \n\n"}
{"id": "1805.12017", "contents": "Title: Robustifying Models Against Adversarial Attacks by Langevin Dynamics Abstract: Adversarial attacks on deep learning models have compromised their\nperformance considerably. As remedies, a lot of defense methods were proposed,\nwhich however, have been circumvented by newer attacking strategies. In the\nmidst of this ensuing arms race, the problem of robustness against adversarial\nattacks still remains unsolved. This paper proposes a novel, simple yet\neffective defense strategy where adversarial samples are relaxed onto the\nunderlying manifold of the (unknown) target class distribution. Specifically,\nour algorithm drives off-manifold adversarial samples towards high density\nregions of the data generating distribution of the target class by the\nMetroplis-adjusted Langevin algorithm (MALA) with perceptual boundary taken\ninto account. Although the motivation is similar to projection methods, e.g.,\nDefense-GAN, our algorithm, called MALA for DEfense (MALADE), is equipped with\nsignificant dispersion - projection is distributed broadly, and therefore any\nwhitebox attack cannot accurately align the input so that the MALADE moves it\nto a targeted untrained spot where the model predicts a wrong label. In our\nexperiments, MALADE exhibited state-of-the-art performance against various\nelaborate attacking strategies. \n\n"}
{"id": "1805.12076", "contents": "Title: Towards Understanding the Role of Over-Parametrization in Generalization\n  of Neural Networks Abstract: Despite existing work on ensuring generalization of neural networks in terms\nof scale sensitive complexity measures, such as norms, margin and sharpness,\nthese complexity measures do not offer an explanation of why neural networks\ngeneralize better with over-parametrization. In this work we suggest a novel\ncomplexity measure based on unit-wise capacities resulting in a tighter\ngeneralization bound for two layer ReLU networks. Our capacity bound correlates\nwith the behavior of test error with increasing network sizes, and could\npotentially explain the improvement in generalization with\nover-parametrization. We further present a matching lower bound for the\nRademacher complexity that improves over previous capacity lower bounds for\nneural networks. \n\n"}
{"id": "1805.12243", "contents": "Title: Novel Video Prediction for Large-scale Scene using Optical Flow Abstract: Making predictions of future frames is a critical challenge in autonomous\ndriving research. Most of the existing methods for video prediction attempt to\ngenerate future frames in simple and fixed scenes. In this paper, we propose a\nnovel and effective optical flow conditioned method for the task of video\nprediction with an application to complex urban scenes. In contrast with\nprevious work, the prediction model only requires video sequences and optical\nflow sequences for training and testing. Our method uses the rich\nspatial-temporal features in video sequences. The method takes advantage of the\nmotion information extracting from optical flow maps between neighbor images as\nwell as previous images. Empirical evaluations on the KITTI dataset and the\nCityscapes dataset demonstrate the effectiveness of our method. \n\n"}
{"id": "1805.12445", "contents": "Title: Optimizing Quantum Circuits for Arithmetic Abstract: Many quantum algorithms make use of oracles which evaluate classical\nfunctions on a superposition of inputs. In order to facilitate implementation,\ntesting, and resource estimation of such algorithms, we present quantum\ncircuits for evaluating functions that are often encountered in the quantum\nalgorithm literature. This includes Gaussians, hyperbolic tangent, sine/cosine,\ninverse square root, arcsine, and exponentials. We use insights from classical\nhigh-performance computing in order to optimize our circuits and implement a\nquantum software stack module which allows to automatically generate circuits\nfor evaluating piecewise smooth functions in the computational basis. Our\ncircuits enable more detailed cost analyses of various quantum algorithms,\nallowing to identify concrete applications of future quantum computing devices.\nFurthermore, our resource estimates may guide future research aiming to reduce\nthe costs or even the need for arithmetic in the computational basis\naltogether. \n\n"}
{"id": "1805.12573", "contents": "Title: Learning a Prior over Intent via Meta-Inverse Reinforcement Learning Abstract: A significant challenge for the practical application of reinforcement\nlearning in the real world is the need to specify an oracle reward function\nthat correctly defines a task. Inverse reinforcement learning (IRL) seeks to\navoid this challenge by instead inferring a reward function from expert\nbehavior. While appealing, it can be impractically expensive to collect\ndatasets of demonstrations that cover the variation common in the real world\n(e.g. opening any type of door). Thus in practice, IRL must commonly be\nperformed with only a limited set of demonstrations where it can be exceedingly\ndifficult to unambiguously recover a reward function. In this work, we exploit\nthe insight that demonstrations from other tasks can be used to constrain the\nset of possible reward functions by learning a \"prior\" that is specifically\noptimized for the ability to infer expressive reward functions from limited\nnumbers of demonstrations. We demonstrate that our method can efficiently\nrecover rewards from images for novel tasks and provide intuition as to how our\napproach is analogous to learning a prior. \n\n"}
{"id": "1806.00007", "contents": "Title: Multi-Layered Gradient Boosting Decision Trees Abstract: Multi-layered representation is believed to be the key ingredient of deep\nneural networks especially in cognitive tasks like computer vision. While\nnon-differentiable models such as gradient boosting decision trees (GBDTs) are\nthe dominant methods for modeling discrete or tabular data, they are hard to\nincorporate with such representation learning ability. In this work, we propose\nthe multi-layered GBDT forest (mGBDTs), with an explicit emphasis on exploring\nthe ability to learn hierarchical representations by stacking several layers of\nregression GBDTs as its building block. The model can be jointly trained by a\nvariant of target propagation across layers, without the need to derive\nback-propagation nor differentiability. Experiments and visualizations\nconfirmed the effectiveness of the model in terms of performance and\nrepresentation learning ability. \n\n"}
{"id": "1806.00176", "contents": "Title: Reparameterization Gradient for Non-differentiable Models Abstract: We present a new algorithm for stochastic variational inference that targets\nat models with non-differentiable densities. One of the key challenges in\nstochastic variational inference is to come up with a low-variance estimator of\nthe gradient of a variational objective. We tackle the challenge by\ngeneralizing the reparameterization trick, one of the most effective techniques\nfor addressing the variance issue for differentiable models, so that the trick\nworks for non-differentiable models as well. Our algorithm splits the space of\nlatent variables into regions where the density of the variables is\ndifferentiable, and their boundaries where the density may fail to be\ndifferentiable. For each differentiable region, the algorithm applies the\nstandard reparameterization trick and estimates the gradient restricted to the\nregion. For each potentially non-differentiable boundary, it uses a form of\nmanifold sampling and computes the direction for variational parameters that,\nif followed, would increase the boundary's contribution to the variational\nobjective. The sum of all the estimates becomes the gradient estimate of our\nalgorithm. Our estimator enjoys the reduced variance of the reparameterization\ngradient while remaining unbiased even for non-differentiable models. The\nexperiments with our preliminary implementation confirm the benefit of reduced\nvariance and unbiasedness. \n\n"}
{"id": "1806.01010", "contents": "Title: Meta-Learner with Linear Nulling Abstract: We propose a meta-learning algorithm utilizing a linear transformer that\ncarries out null-space projection of neural network outputs. The main idea is\nto construct an alternative classification space such that the error signals\nduring few-shot learning are quickly zero-forced on that space so that reliable\nclassification on low data is possible. The final decision on a query is\nobtained utilizing a null-space-projected distance measure between the network\noutput and reference vectors, both of which have been trained in the initial\nlearning phase. Among the known methods with a given model size, our\nmeta-learner achieves the best or near-best image classification accuracies\nwith Omniglot and miniImageNet datasets. \n\n"}
{"id": "1806.01445", "contents": "Title: Embedding Logical Queries on Knowledge Graphs Abstract: Learning low-dimensional embeddings of knowledge graphs is a powerful\napproach used to predict unobserved or missing edges between entities. However,\nan open challenge in this area is developing techniques that can go beyond\nsimple edge prediction and handle more complex logical queries, which might\ninvolve multiple unobserved edges, entities, and variables. For instance, given\nan incomplete biological knowledge graph, we might want to predict \"em what\ndrugs are likely to target proteins involved with both diseases X and Y?\" -- a\nquery that requires reasoning about all possible proteins that {\\em might}\ninteract with diseases X and Y. Here we introduce a framework to efficiently\nmake predictions about conjunctive logical queries -- a flexible but tractable\nsubset of first-order logic -- on incomplete knowledge graphs. In our approach,\nwe embed graph nodes in a low-dimensional space and represent logical operators\nas learned geometric operations (e.g., translation, rotation) in this embedding\nspace. By performing logical operations within a low-dimensional embedding\nspace, our approach achieves a time complexity that is linear in the number of\nquery variables, compared to the exponential complexity required by a naive\nenumeration-based approach. We demonstrate the utility of this framework in two\napplication studies on real-world datasets with millions of relations:\npredicting logical relationships in a network of drug-gene-disease interactions\nand in a graph-based representation of social interactions derived from a\npopular web forum. \n\n"}
{"id": "1806.01540", "contents": "Title: Combining Multiple Algorithms in Classifier Ensembles using Generalized\n  Mixture Functions Abstract: Classifier ensembles are pattern recognition structures composed of a set of\nclassification algorithms (members), organized in a parallel way, and a\ncombination method with the aim of increasing the classification accuracy of a\nclassification system. In this study, we investigate the application of a\ngeneralized mixture (GM) functions as a new approach for providing an efficient\ncombination procedure for these systems through the use of dynamic weights in\nthe combination process. Therefore, we present three GM functions to be applied\nas a combination method. The main advantage of these functions is that they can\ndefine dynamic weights at the member outputs, making the combination process\nmore efficient. In order to evaluate the feasibility of the proposed approach,\nan empirical analysis is conducted, applying classifier ensembles to 25\ndifferent classification data sets. In this analysis, we compare the use of the\nproposed approaches to ensembles using traditional combination methods as well\nas the state-of-the-art ensemble methods. Our findings indicated gains in terms\nof performance when comparing the proposed approaches to the traditional ones\nas well as comparable results with the state-of-the-art methods. \n\n"}
{"id": "1806.01670", "contents": "Title: On Latent Distributions Without Finite Mean in Generative Models Abstract: We investigate the properties of multidimensional probability distributions\nin the context of latent space prior distributions of implicit generative\nmodels. Our work revolves around the phenomena arising while decoding linear\ninterpolations between two random latent vectors -- regions of latent space in\nclose proximity to the origin of the space are sampled causing distribution\nmismatch. We show that due to the Central Limit Theorem, this region is almost\nnever sampled during the training process. As a result, linear interpolations\nmay generate unrealistic data and their usage as a tool to check quality of the\ntrained model is questionable. We propose to use multidimensional Cauchy\ndistribution as the latent prior. Cauchy distribution does not satisfy the\nassumptions of the CLT and has a number of properties that allow it to work\nwell in conjunction with linear interpolations. We also provide two general\nmethods of creating non-linear interpolations that are easily applicable to a\nlarge family of common latent distributions. Finally we empirically analyze the\nquality of data generated from low-probability-mass regions for the DCGAN model\non the CelebA dataset. \n\n"}
{"id": "1806.01678", "contents": "Title: A Projection Method for Metric-Constrained Optimization Abstract: We outline a new approach for solving optimization problems which enforce\ntriangle inequalities on output variables. We refer to this as\nmetric-constrained optimization, and give several examples where problems of\nthis form arise in machine learning applications and theoretical approximation\nalgorithms for graph clustering. Although these problem are interesting from a\ntheoretical perspective, they are challenging to solve in practice due to the\nhigh memory requirement of black-box solvers. In order to address this\nchallenge we first prove that the metric-constrained linear program relaxation\nof correlation clustering is equivalent to a special case of the metric\nnearness problem. We then developed a general solver for metric-constrained\nlinear and quadratic programs by generalizing and improving a simple projection\nalgorithm originally developed for metric nearness. We give several novel\napproximation guarantees for using our framework to find lower bounds for\noptimal solutions to several challenging graph clustering problems. We also\ndemonstrate the power of our framework by solving optimizing problems involving\nup to 10^{8} variables and 10^{11} constraints. \n\n"}
{"id": "1806.01743", "contents": "Title: A Machine Learning Framework for Stock Selection Abstract: This paper demonstrates how to apply machine learning algorithms to\ndistinguish good stocks from the bad stocks. To this end, we construct 244\ntechnical and fundamental features to characterize each stock, and label stocks\naccording to their ranking with respect to the return-to-volatility ratio.\nAlgorithms ranging from traditional statistical learning methods to recently\npopular deep learning method, e.g. Logistic Regression (LR), Random Forest\n(RF), Deep Neural Network (DNN), and the Stacking, are trained to solve the\nclassification task. Genetic Algorithm (GA) is also used to implement feature\nselection. The effectiveness of the stock selection strategy is validated in\nChinese stock market in both statistical and practical aspects, showing that:\n1) Stacking outperforms other models reaching an AUC score of 0.972; 2) Genetic\nAlgorithm picks a subset of 114 features and the prediction performances of all\nmodels remain almost unchanged after the selection procedure, which suggests\nsome features are indeed redundant; 3) LR and DNN are radical models; RF is\nrisk-neutral model; Stacking is somewhere between DNN and RF. 4) The portfolios\nconstructed by our models outperform market average in back tests. \n\n"}
{"id": "1806.01818", "contents": "Title: LSTM Benchmarks for Deep Learning Frameworks Abstract: This study provides benchmarks for different implementations of LSTM units\nbetween the deep learning frameworks PyTorch, TensorFlow, Lasagne and Keras.\nThe comparison includes cuDNN LSTMs, fused LSTM variants and less optimized,\nbut more flexible LSTM implementations. The benchmarks reflect two typical\nscenarios for automatic speech recognition, notably continuous speech\nrecognition and isolated digit recognition. These scenarios cover input\nsequences of fixed and variable length as well as the loss functions CTC and\ncross entropy. Additionally, a comparison between four different PyTorch\nversions is included. The code is available online\nhttps://github.com/stefbraun/rnn_benchmarks. \n\n"}
{"id": "1806.01838", "contents": "Title: Quantum singular value transformation and beyond: exponential\n  improvements for quantum matrix arithmetics Abstract: Quantum computing is powerful because unitary operators describing the\ntime-evolution of a quantum system have exponential size in terms of the number\nof qubits present in the system. We develop a new \"Singular value\ntransformation\" algorithm capable of harnessing this exponential advantage,\nthat can apply polynomial transformations to the singular values of a block of\na unitary, generalizing the optimal Hamiltonian simulation results of Low and\nChuang. The proposed quantum circuits have a very simple structure, often give\nrise to optimal algorithms and have appealing constant factors, while usually\nonly use a constant number of ancilla qubits. We show that singular value\ntransformation leads to novel algorithms. We give an efficient solution to a\ncertain \"non-commutative\" measurement problem and propose a new method for\nsingular value estimation. We also show how to exponentially improve the\ncomplexity of implementing fractional queries to unitaries with a gapped\nspectrum. Finally, as a quantum machine learning application we show how to\nefficiently implement principal component regression. \"Singular value\ntransformation\" is conceptually simple and efficient, and leads to a unified\nframework of quantum algorithms incorporating a variety of quantum speed-ups.\nWe illustrate this by showing how it generalizes a number of prominent quantum\nalgorithms, including: optimal Hamiltonian simulation, implementing the\nMoore-Penrose pseudoinverse with exponential precision, fixed-point amplitude\namplification, robust oblivious amplitude amplification, fast QMA\namplification, fast quantum OR lemma, certain quantum walk results and several\nquantum machine learning algorithms. In order to exploit the strengths of the\npresented method it is useful to know its limitations too, therefore we also\nprove a lower bound on the efficiency of singular value transformation, which\noften gives optimal bounds. \n\n"}
{"id": "1806.01845", "contents": "Title: Deep Neural Networks with Multi-Branch Architectures Are Less Non-Convex Abstract: Several recently proposed architectures of neural networks such as ResNeXt,\nInception, Xception, SqueezeNet and Wide ResNet are based on the designing idea\nof having multiple branches and have demonstrated improved performance in many\napplications. We show that one cause for such success is due to the fact that\nthe multi-branch architecture is less non-convex in terms of duality gap. The\nduality gap measures the degree of intrinsic non-convexity of an optimization\nproblem: smaller gap in relative value implies lower degree of intrinsic\nnon-convexity. The challenge is to quantitatively measure the duality gap of\nhighly non-convex problems such as deep neural networks. In this work, we\nprovide strong guarantees of this quantity for two classes of network\narchitectures. For the neural networks with arbitrary activation functions,\nmulti-branch architecture and a variant of hinge loss, we show that the duality\ngap of both population and empirical risks shrinks to zero as the number of\nbranches increases. This result sheds light on better understanding the power\nof over-parametrization where increasing the network width tends to make the\nloss surface less non-convex. For the neural networks with linear activation\nfunction and $\\ell_2$ loss, we show that the duality gap of empirical risk is\nzero. Our two results work for arbitrary depths and adversarial data, while the\nanalytical techniques might be of independent interest to non-convex\noptimization more broadly. Experiments on both synthetic and real-world\ndatasets validate our results. \n\n"}
{"id": "1806.01861", "contents": "Title: Advantages of a modular high-level quantum programming framework Abstract: We review some of the features of the ProjectQ software framework and\nquantify their impact on the resulting circuits. The concise high-level\nlanguage facilitates implementing even complex algorithms in a very\ntime-efficient manner while, at the same time, providing the compiler with\nadditional information for optimization through code annotation - so-called\nmeta-instructions. We investigate the impact of these annotations for the\nexample of Shor's algorithm in terms of logical gate counts. Furthermore, we\nanalyze the effect of different intermediate gate sets for optimization and how\nthe dimensions of the resulting circuit depend on a smart choice thereof.\nFinally, we demonstrate the benefits of a modular compilation framework by\nimplementing mapping procedures for one- and two-dimensional nearest neighbor\narchitectures which we then compare in terms of overhead for different problem\nsizes. \n\n"}
{"id": "1806.02199", "contents": "Title: SOM-VAE: Interpretable Discrete Representation Learning on Time Series Abstract: High-dimensional time series are common in many domains. Since human\ncognition is not optimized to work well in high-dimensional spaces, these areas\ncould benefit from interpretable low-dimensional representations. However, most\nrepresentation learning algorithms for time series data are difficult to\ninterpret. This is due to non-intuitive mappings from data features to salient\nproperties of the representation and non-smoothness over time. To address this\nproblem, we propose a new representation learning framework building on ideas\nfrom interpretable discrete dimensionality reduction and deep generative\nmodeling. This framework allows us to learn discrete representations of time\nseries, which give rise to smooth and interpretable embeddings with superior\nclustering performance. We introduce a new way to overcome the\nnon-differentiability in discrete representation learning and present a\ngradient-based version of the traditional self-organizing map algorithm that is\nmore performant than the original. Furthermore, to allow for a probabilistic\ninterpretation of our method, we integrate a Markov model in the representation\nspace. This model uncovers the temporal transition structure, improves\nclustering performance even further and provides additional explanatory\ninsights as well as a natural representation of uncertainty. We evaluate our\nmodel in terms of clustering performance and interpretability on static\n(Fashion-)MNIST data, a time series of linearly interpolated (Fashion-)MNIST\nimages, a chaotic Lorenz attractor system with two macro states, as well as on\na challenging real world medical time series application on the eICU data set.\nOur learned representations compare favorably with competitor methods and\nfacilitate downstream tasks on the real world data. \n\n"}
{"id": "1806.03342", "contents": "Title: Discovering Signals from Web Sources to Predict Cyber Attacks Abstract: Cyber attacks are growing in frequency and severity. Over the past year alone\nwe have witnessed massive data breaches that stole personal information of\nmillions of people and wide-scale ransomware attacks that paralyzed critical\ninfrastructure of several countries. Combating the rising cyber threat calls\nfor a multi-pronged strategy, which includes predicting when these attacks will\noccur. The intuition driving our approach is this: during the planning and\npreparation stages, hackers leave digital traces of their activities on both\nthe surface web and dark web in the form of discussions on platforms like\nhacker forums, social media, blogs and the like. These data provide predictive\nsignals that allow anticipating cyber attacks. In this paper, we describe\nmachine learning techniques based on deep neural networks and autoregressive\ntime series models that leverage external signals from publicly available Web\nsources to forecast cyber attacks. Performance of our framework across ground\ntruth data over real-world forecasting tasks shows that our methods yield a\nsignificant lift or increase of F1 for the top signals on predicted cyber\nattacks. Our results suggest that, when deployed, our system will be able to\nprovide an effective line of defense against various types of targeted cyber\nattacks. \n\n"}
{"id": "1806.03903", "contents": "Title: Multi-task learning of daily work and study round-trips from survey data Abstract: In this study, we present a machine learning approach to infer the worker and\nstudent mobility flows on daily basis from static censuses. The rapid\nurbanization has made the estimation of the human mobility flows a critical\ntask for transportation and urban planners. The primary objective of this paper\nis to complete individuals' census data with working and studying trips,\nallowing its merging with other mobility data to better estimate the complete\norigin-destination matrices. Worker and student mobility flows are among the\nmost weekly regular displacements and consequently generate road congestion\nproblems. Estimating their round-trips eases the decision-making processes for\nlocal authorities. Worker and student censuses often contain home location,\nwork places and educational institutions. We thus propose a neural network\nmodel that learns the temporal distribution of displacements from other\nmobility sources and tries to predict them on new censuses data. The inclusion\nof multi-task learning in our neural network results in a significant error\nrate control in comparison to single task learning. \n\n"}
{"id": "1806.03925", "contents": "Title: Gear Training: A new way to implement high-performance model-parallel\n  training Abstract: The training of Deep Neural Networks usually needs tremendous computing\nresources. Therefore many deep models are trained in large cluster instead of\nsingle machine or GPU. Though major researchs at present try to run whole model\non all machines by using asynchronous asynchronous stochastic gradient descent\n(ASGD), we present a new approach to train deep model parallely -- split the\nmodel and then seperately train different parts of it in different speed. \n\n"}
{"id": "1806.04016", "contents": "Title: Baselines and a datasheet for the Cerema AWP dataset Abstract: This paper presents the recently published Cerema AWP (Adverse Weather\nPedestrian) dataset for various machine learning tasks and its exports in\nmachine learning friendly format. We explain why this dataset can be\ninteresting (mainly because it is a greatly controlled and fully annotated\nimage dataset) and present baseline results for various tasks. Moreover, we\ndecided to follow the very recent suggestions of datasheets for dataset, trying\nto standardize all the available information of the dataset, with a\ntransparency objective. \n\n"}
{"id": "1806.04245", "contents": "Title: Learning to Speed Up Structured Output Prediction Abstract: Predicting structured outputs can be computationally onerous due to the\ncombinatorially large output spaces. In this paper, we focus on reducing the\nprediction time of a trained black-box structured classifier without losing\naccuracy. To do so, we train a speedup classifier that learns to mimic a\nblack-box classifier under the learning-to-search approach. As the structured\nclassifier predicts more examples, the speedup classifier will operate as a\nlearned heuristic to guide search to favorable regions of the output space. We\npresent a mistake bound for the speedup classifier and identify inference\nsituations where it can independently make correct judgments without input\nfeatures. We evaluate our method on the task of entity and relation extraction\nand show that the speedup classifier outperforms even greedy search in terms of\nspeed without loss of accuracy. \n\n"}
{"id": "1806.04310", "contents": "Title: MISSION: Ultra Large-Scale Feature Selection using Count-Sketches Abstract: Feature selection is an important challenge in machine learning. It plays a\ncrucial role in the explainability of machine-driven decisions that are rapidly\npermeating throughout modern society. Unfortunately, the explosion in the size\nand dimensionality of real-world datasets poses a severe challenge to standard\nfeature selection algorithms. Today, it is not uncommon for datasets to have\nbillions of dimensions. At such scale, even storing the feature vector is\nimpossible, causing most existing feature selection methods to fail.\nWorkarounds like feature hashing, a standard approach to large-scale machine\nlearning, helps with the computational feasibility, but at the cost of losing\nthe interpretability of features. In this paper, we present MISSION, a novel\nframework for ultra large-scale feature selection that performs stochastic\ngradient descent while maintaining an efficient representation of the features\nin memory using a Count-Sketch data structure. MISSION retains the simplicity\nof feature hashing without sacrificing the interpretability of the features\nwhile using only O(log^2(p)) working memory. We demonstrate that MISSION\naccurately and efficiently performs feature selection on real-world,\nlarge-scale datasets with billions of dimensions. \n\n"}
{"id": "1806.04326", "contents": "Title: Differentiable Compositional Kernel Learning for Gaussian Processes Abstract: The generalization properties of Gaussian processes depend heavily on the\nchoice of kernel, and this choice remains a dark art. We present the Neural\nKernel Network (NKN), a flexible family of kernels represented by a neural\nnetwork. The NKN architecture is based on the composition rules for kernels, so\nthat each unit of the network corresponds to a valid kernel. It can compactly\napproximate compositional kernel structures such as those used by the Automatic\nStatistician (Lloyd et al., 2014), but because the architecture is\ndifferentiable, it is end-to-end trainable with gradient-based optimization. We\nshow that the NKN is universal for the class of stationary kernels. Empirically\nwe demonstrate pattern discovery and extrapolation abilities of NKN on several\ntasks that depend crucially on identifying the underlying structure, including\ntime series and texture extrapolation, as well as Bayesian optimization. \n\n"}
{"id": "1806.04655", "contents": "Title: FigureNet: A Deep Learning model for Question-Answering on Scientific\n  Plots Abstract: Deep Learning has managed to push boundaries in a wide variety of tasks. One\narea of interest is to tackle problems in reasoning and understanding, with an\naim to emulate human intelligence. In this work, we describe a deep learning\nmodel that addresses the reasoning task of question-answering on categorical\nplots. We introduce a novel architecture FigureNet, that learns to identify\nvarious plot elements, quantify the represented values and determine a relative\nordering of these statistical values. We test our model on the FigureQA dataset\nwhich provides images and accompanying questions for scientific plots like bar\ngraphs and pie charts, augmented with rich annotations. Our approach\noutperforms the state-of-the-art Relation Networks baseline by approximately\n$7\\%$ on this dataset, with a training time that is over an order of magnitude\nlesser. \n\n"}
{"id": "1806.05134", "contents": "Title: Marginal Policy Gradients: A Unified Family of Estimators for Bounded\n  Action Spaces with Applications Abstract: Many complex domains, such as robotics control and real-time strategy (RTS)\ngames, require an agent to learn a continuous control. In the former, an agent\nlearns a policy over $\\mathbb{R}^d$ and in the latter, over a discrete set of\nactions each of which is parametrized by a continuous parameter. Such problems\nare naturally solved using policy based reinforcement learning (RL) methods,\nbut unfortunately these often suffer from high variance leading to instability\nand slow convergence. Unnecessary variance is introduced whenever policies over\nbounded action spaces are modeled using distributions with unbounded support by\napplying a transformation $T$ to the sampled action before execution in the\nenvironment. Recently, the variance reduced clipped action policy gradient\n(CAPG) was introduced for actions in bounded intervals, but to date no variance\nreduced methods exist when the action is a direction, something often seen in\nRTS games. To this end we introduce the angular policy gradient (APG), a\nstochastic policy gradient method for directional control. With the marginal\npolicy gradients family of estimators we present a unified analysis of the\nvariance reduction properties of APG and CAPG; our results provide a stronger\nguarantee than existing analyses for CAPG. Experimental results on a popular\nRTS game and a navigation task show that the APG estimator offers a substantial\nimprovement over the standard policy gradient. \n\n"}
{"id": "1806.05779", "contents": "Title: Deep Learning Approximation: Zero-Shot Neural Network Speedup Abstract: Neural networks offer high-accuracy solutions to a range of problems, but are\ncostly to run in production systems because of computational and memory\nrequirements during a forward pass. Given a trained network, we propose a\ntechique called Deep Learning Approximation to build a faster network in a tiny\nfraction of the time required for training by only manipulating the network\nstructure and coefficients without requiring re-training or access to the\ntraining data. Speedup is achieved by by applying a sequential series of\nindependent optimizations that reduce the floating-point operations (FLOPs)\nrequired to perform a forward pass. First, lossless optimizations are applied,\nfollowed by lossy approximations using singular value decomposition (SVD) and\nlow-rank matrix decomposition. The optimal approximation is chosen by weighing\nthe relative accuracy loss and FLOP reduction according to a single parameter\nspecified by the user. On PASCAL VOC 2007 with the YOLO network, we show an\nend-to-end 2x speedup in a network forward pass with a 5% drop in mAP that can\nbe re-gained by finetuning. \n\n"}
{"id": "1806.06142", "contents": "Title: Possibility results for graph clustering: A novel consistency axiom Abstract: Kleinberg introduced three natural clustering properties, or axioms, and\nshowed they cannot be simultaneously satisfied by any clustering algorithm. We\npresent a new clustering property, Monotonic Consistency, which avoids the\nwell-known problematic behaviour of Kleinberg's Consistency axiom, and the\nimpossibility result. Namely, we describe a clustering algorithm, Morse\nClustering, inspired by Morse Theory in Differential Topology, which satisfies\nKleinberg's original axioms with Consistency replaced by Monotonic Consistency.\nMorse clustering uncovers the underlying flow structure on a set or graph and\nreturns a partition into trees representing basins of attraction of critical\nvertices. We also generalise Kleinberg's axiomatic approach to sparse graphs,\nshowing an impossibility result for Consistency, and a possibility result for\nMonotonic Consistency and Morse clustering. \n\n"}
{"id": "1806.06266", "contents": "Title: On Strategyproof Conference Peer Review Abstract: We consider peer review in a conference setting where there is typically an\noverlap between the set of reviewers and the set of authors. This overlap can\nincentivize strategic reviews to influence the final ranking of one's own\npapers. In this work, we address this problem through the lens of social\nchoice, and present a theoretical framework for strategyproof and efficient\npeer review. We first present and analyze an algorithm for reviewer-assignment\nand aggregation that guarantees strategyproofness and a natural efficiency\nproperty called unanimity, when the authorship graph satisfies a simple\nproperty. Our algorithm is based on the so-called partitioning method, and can\nbe thought as a generalization of this method to conference peer review\nsettings. We then empirically show that the requisite property on the\nauthorship graph is indeed satisfied in the submission data from the ICLR\nconference, and further demonstrate a simple trick to make the partitioning\nmethod more practically appealing for conference peer review. Finally, we\ncomplement our positive results with negative theoretical results where we\nprove that under various ways of strengthening the requirements, it is\nimpossible for any algorithm to be strategyproof and efficient. \n\n"}
{"id": "1806.06384", "contents": "Title: Multi-variable LSTM neural network for autoregressive exogenous model Abstract: In this paper, we propose multi-variable LSTM capable of accurate forecasting\nand variable importance interpretation for time series with exogenous\nvariables. Current attention mechanism in recurrent neural networks mostly\nfocuses on the temporal aspect of data and falls short of characterizing\nvariable importance. To this end, the multi-variable LSTM equipped with\ntensorized hidden states is developed to learn hidden states for individual\nvariables, which give rise to our mixture temporal and variable attention.\nBased on such attention mechanism, we infer and quantify variable importance.\nExtensive experiments using real datasets with Granger-causality test and the\nsynthetic dataset with ground truth demonstrate the prediction performance and\ninterpretability of multi-variable LSTM in comparison to a variety of\nbaselines. It exhibits the prospect of multi-variable LSTM as an end-to-end\nframework for both forecasting and knowledge discovery. \n\n"}
{"id": "1806.06392", "contents": "Title: Task-Relevant Object Discovery and Categorization for Playing\n  First-person Shooter Games Abstract: We consider the problem of learning to play first-person shooter (FPS) video\ngames using raw screen images as observations and keyboard inputs as actions.\nThe high-dimensionality of the observations in this type of applications leads\nto prohibitive needs of training data for model-free methods, such as the deep\nQ-network (DQN), and its recurrent variant DRQN. Thus, recent works focused on\nlearning low-dimensional representations that may reduce the need for data.\nThis paper presents a new and efficient method for learning such\nrepresentations. Salient segments of consecutive frames are detected from their\noptical flow, and clustered based on their feature descriptors. The clusters\ntypically correspond to different discovered categories of objects. Segments\ndetected in new frames are then classified based on their nearest clusters.\nBecause only a few categories are relevant to a given task, the importance of a\ncategory is defined as the correlation between its occurrence and the agent's\nperformance. The result is encoded as a vector indicating objects that are in\nthe frame and their locations, and used as a side input to DRQN. Experiments on\nthe game Doom provide a good evidence for the benefit of this approach. \n\n"}
{"id": "1806.06977", "contents": "Title: Using Mode Connectivity for Loss Landscape Analysis Abstract: Mode connectivity is a recently introduced frame- work that empirically\nestablishes the connected- ness of minima by finding a high accuracy curve\nbetween two independently trained models. To investigate the limits of this\nsetup, we examine the efficacy of this technique in extreme cases where the\ninput models are trained or initialized differently. We find that the procedure\nis resilient to such changes. Given this finding, we propose using the\nframework for analyzing loss surfaces and training trajectories more generally,\nand in this direction, study SGD with cosine annealing and restarts (SGDR). We\nreport that while SGDR moves over barriers in its trajectory, propositions\nclaiming that it converges to and escapes from multiple local minima are not\nsubstantiated by our empirical results. \n\n"}
{"id": "1806.07104", "contents": "Title: Online Linear Quadratic Control Abstract: We study the problem of controlling linear time-invariant systems with known\nnoisy dynamics and adversarially chosen quadratic losses. We present the first\nefficient online learning algorithms in this setting that guarantee\n$O(\\sqrt{T})$ regret under mild assumptions, where $T$ is the time horizon. Our\nalgorithms rely on a novel SDP relaxation for the steady-state distribution of\nthe system. Crucially, and in contrast to previously proposed relaxations, the\nfeasible solutions of our SDP all correspond to \"strongly stable\" policies that\nmix exponentially fast to a steady state. \n\n"}
{"id": "1806.07385", "contents": "Title: Detecting and interpreting myocardial infarction using fully\n  convolutional neural networks Abstract: Objective: We aim to provide an algorithm for the detection of myocardial\ninfarction that operates directly on ECG data without any preprocessing and to\ninvestigate its decision criteria. Approach: We train an ensemble of fully\nconvolutional neural networks on the PTB ECG dataset and apply state-of-the-art\nattribution methods. Main results: Our classifier reaches 93.3% sensitivity and\n89.7% specificity evaluated using 10-fold cross-validation with sampling based\non patients. The presented method outperforms state-of-the-art approaches and\nreaches the performance level of human cardiologists for detection of\nmyocardial infarction. We are able to discriminate channel-specific regions\nthat contribute most significantly to the neural network's decision.\nInterestingly, the network's decision is influenced by signs also recognized by\nhuman cardiologists as indicative of myocardial infarction. Significance: Our\nresults demonstrate the high prospects of algorithmic ECG analysis for future\nclinical applications considering both its quantitative performance as well as\nthe possibility of assessing decision criteria on a per-example basis, which\nenhances the comprehensibility of the approach. \n\n"}
{"id": "1806.07492", "contents": "Title: On the Learning of Deep Local Features for Robust Face Spoofing\n  Detection Abstract: Biometrics emerged as a robust solution for security systems. However, given\nthe dissemination of biometric applications, criminals are developing\ntechniques to circumvent them by simulating physical or behavioral traits of\nlegal users (spoofing attacks). Despite face being a promising characteristic\ndue to its universality, acceptability and presence of cameras almost\neverywhere, face recognition systems are extremely vulnerable to such frauds\nsince they can be easily fooled with common printed facial photographs.\nState-of-the-art approaches, based on Convolutional Neural Networks (CNNs),\npresent good results in face spoofing detection. However, these methods do not\nconsider the importance of learning deep local features from each facial\nregion, even though it is known from face recognition that each facial region\npresents different visual aspects, which can also be exploited for face\nspoofing detection. In this work we propose a novel CNN architecture trained in\ntwo steps for such task. Initially, each part of the neural network learns\nfeatures from a given facial region. Afterwards, the whole model is fine-tuned\non the whole facial images. Results show that such pre-training step allows the\nCNN to learn different local spoofing cues, improving the performance and the\nconvergence speed of the final model, outperforming the state-of-the-art\napproaches. \n\n"}
{"id": "1806.07569", "contents": "Title: A Distributed Second-Order Algorithm You Can Trust Abstract: Due to the rapid growth of data and computational resources, distributed\noptimization has become an active research area in recent years. While\nfirst-order methods seem to dominate the field, second-order methods are\nnevertheless attractive as they potentially require fewer communication rounds\nto converge. However, there are significant drawbacks that impede their wide\nadoption, such as the computation and the communication of a large Hessian\nmatrix. In this paper we present a new algorithm for distributed training of\ngeneralized linear models that only requires the computation of diagonal blocks\nof the Hessian matrix on the individual workers. To deal with this approximate\ninformation we propose an adaptive approach that - akin to trust-region methods\n- dynamically adapts the auxiliary model to compensate for modeling errors. We\nprovide theoretical rates of convergence for a wide class of problems including\nL1-regularized objectives. We also demonstrate that our approach achieves\nstate-of-the-art results on multiple large benchmark datasets. \n\n"}
{"id": "1806.07863", "contents": "Title: Learning ReLU Networks via Alternating Minimization Abstract: We propose and analyze a new family of algorithms for training neural\nnetworks with ReLU activations. Our algorithms are based on the technique of\nalternating minimization: estimating the activation patterns of each ReLU for\nall given samples, interleaved with weight updates via a least-squares step.\nThe main focus of our paper are 1-hidden layer networks with $k$ hidden neurons\nand ReLU activation. We show that under standard distributional assumptions on\nthe $d-$dimensional input data, our algorithm provably recovers the true\n`ground truth' parameters in a linearly convergent fashion. This holds as long\nas the weights are sufficiently well initialized; furthermore, our method\nrequires only $n=\\widetilde{O}(dk^2)$ samples. We also analyze the special case\nof 1-hidden layer networks with skipped connections, commonly used in\nResNet-type architectures, and propose a novel initialization strategy for the\nsame. For ReLU based ResNet type networks, we provide the first linear\nconvergence guarantee with an end-to-end algorithm. We also extend this\nframework to deeper networks and empirically demonstrate its convergence to a\nglobal minimum. \n\n"}
{"id": "1806.08240", "contents": "Title: InfoCatVAE: Representation Learning with Categorical Variational\n  Autoencoders Abstract: This paper describes InfoCatVAE, an extension of the variational autoencoder\nthat enables unsupervised disentangled representation learning. InfoCatVAE uses\nmultimodal distributions for the prior and the inference network and then\nmaximizes the evidence lower bound objective (ELBO). We connect the new ELBO\nderived for our model with a natural soft clustering objective which explains\nthe robustness of our approach. We then adapt the InfoGANs method to our\nsetting in order to maximize the mutual information between the categorical\ncode and the generated inputs and obtain an improved model. \n\n"}
{"id": "1806.08295", "contents": "Title: How Many Random Seeds? Statistical Power Analysis in Deep Reinforcement\n  Learning Experiments Abstract: Consistently checking the statistical significance of experimental results is\none of the mandatory methodological steps to address the so-called\n\"reproducibility crisis\" in deep reinforcement learning. In this tutorial\npaper, we explain how the number of random seeds relates to the probabilities\nof statistical errors. For both the t-test and the bootstrap confidence\ninterval test, we recall theoretical guidelines to determine the number of\nrandom seeds one should use to provide a statistically significant comparison\nof the performance of two algorithms. Finally, we discuss the influence of\ndeviations from the assumptions usually made by statistical tests. We show that\nthey can lead to inaccurate evaluations of statistical errors and provide\nguidelines to counter these negative effects. We make our code available to\nperform the tests. \n\n"}
{"id": "1806.08422", "contents": "Title: Emulating the coherent Ising machine with a mean-field algorithm Abstract: The coherent Ising machine is an optical processor that uses coherent laser\npulses, but does not employ coherent quantum dynamics in a computational role.\nCore to its operation is the iterated simulation of all-to-all spin coupling\nvia mean-field calculation in a classical FPGA coprocessor. Although it has\nbeen described as \"operating at the quantum limit\" and a \"quantum artificial\nbrain\", interaction with the FPGA prevents the coherent Ising machine from\nexploiting quantum effects in its computations. Thus the question naturally\narises: Can the optical portion of the coherent Ising machine be replaced with\nclassical mean-field arithmetic? Here we answer this in the affirmative by\nshowing that a straightforward noisy version of mean-field annealing closely\nmatches CIM performance scaling, while running roughly 20 times faster in\nabsolute terms. \n\n"}
{"id": "1806.08462", "contents": "Title: Stochastic Wasserstein Autoencoder for Probabilistic Sentence Generation Abstract: The variational autoencoder (VAE) imposes a probabilistic distribution\n(typically Gaussian) on the latent space and penalizes the Kullback--Leibler\n(KL) divergence between the posterior and prior. In NLP, VAEs are extremely\ndifficult to train due to the problem of KL collapsing to zero. One has to\nimplement various heuristics such as KL weight annealing and word dropout in a\ncarefully engineered manner to successfully train a VAE for text. In this\npaper, we propose to use the Wasserstein autoencoder (WAE) for probabilistic\nsentence generation, where the encoder could be either stochastic or\ndeterministic. We show theoretically and empirically that, in the original WAE,\nthe stochastically encoded Gaussian distribution tends to become a Dirac-delta\nfunction, and we propose a variant of WAE that encourages the stochasticity of\nthe encoder. Experimental results show that the latent space learned by WAE\nexhibits properties of continuity and smoothness as in VAEs, while\nsimultaneously achieving much higher BLEU scores for sentence reconstruction. \n\n"}
{"id": "1806.08748", "contents": "Title: Persistent Hidden States and Nonlinear Transformation for Long\n  Short-Term Memory Abstract: Recurrent neural networks (RNNs) have been drawing much attention with great\nsuccess in many applications like speech recognition and neural machine\ntranslation. Long short-term memory (LSTM) is one of the most popular RNN units\nin deep learning applications. LSTM transforms the input and the previous\nhidden states to the next states with the affine transformation, multiplication\noperations and a nonlinear activation function, which makes a good data\nrepresentation for a given task. The affine transformation includes rotation\nand reflection, which change the semantic or syntactic information of\ndimensions in the hidden states. However, considering that a model interprets\nthe output sequence of LSTM over the whole input sequence, the dimensions of\nthe states need to keep the same type of semantic or syntactic information\nregardless of the location in the sequence. In this paper, we propose a simple\nvariant of the LSTM unit, persistent recurrent unit (PRU), where each dimension\nof hidden states keeps persistent information across time, so that the space\nkeeps the same meaning over the whole sequence. In addition, to improve the\nnonlinear transformation power, we add a feedforward layer in the PRU\nstructure. In the experiment, we evaluate our proposed methods with three\ndifferent tasks, and the results confirm that our methods have better\nperformance than the conventional LSTM. \n\n"}
{"id": "1806.08941", "contents": "Title: A Recursive PLS (Partial Least Squares) based Approach for Enterprise\n  Threat Management Abstract: Most of the existing solutions to enterprise threat management are preventive\napproaches prescribing means to prevent policy violations with varying degrees\nof success. In this paper we consider the complementary scenario where a number\nof security violations have already occurred, or security threats, or\nvulnerabilities have been reported and a security administrator needs to\ngenerate optimal response to these security events. We present a principled\napproach to study and model the human expertise in responding to the emergent\nthreats owing to these security events. A recursive Partial Least Squares based\nadaptive learning model is defined using a factorial analysis of the security\nevents together with a method for estimating the effect of global context\ndependent semantic information used by the security administrators. Presented\nmodel is theoretically optimal and operationally recursive in nature to deal\nwith the set of security events being generated continuously. We discuss the\nunderlying challenges and ways in which the model could be operationalized in\ncentralized versus decentralized, and real-time versus batch processing modes. \n\n"}
{"id": "1806.09186", "contents": "Title: Detection based Defense against Adversarial Examples from the\n  Steganalysis Point of View Abstract: Deep Neural Networks (DNNs) have recently led to significant improvements in\nmany fields. However, DNNs are vulnerable to adversarial examples which are\nsamples with imperceptible perturbations while dramatically misleading the\nDNNs. Moreover, adversarial examples can be used to perform an attack on\nvarious kinds of DNN based systems, even if the adversary has no access to the\nunderlying model. Many defense methods have been proposed, such as obfuscating\ngradients of the networks or detecting adversarial examples. However it is\nproved out that these defense methods are not effective or cannot resist\nsecondary adversarial attacks. In this paper, we point out that steganalysis\ncan be applied to adversarial examples detection, and propose a method to\nenhance steganalysis features by estimating the probability of modifications\ncaused by adversarial attacks. Experimental results show that the proposed\nmethod can accurately detect adversarial examples. Moreover, secondary\nadversarial attacks cannot be directly performed to our method because our\nmethod is not based on a neural network but based on high-dimensional\nartificial features and FLD (Fisher Linear Discriminant) ensemble. \n\n"}
{"id": "1806.09679", "contents": "Title: On the Resilience of RTL NN Accelerators: Fault Characterization and\n  Mitigation Abstract: Machine Learning (ML) is making a strong resurgence in tune with the massive\ngeneration of unstructured data which in turn requires massive computational\nresources. Due to the inherently compute- and power-intensive structure of\nNeural Networks (NNs), hardware accelerators emerge as a promising solution.\nHowever, with technology node scaling below 10nm, hardware accelerators become\nmore susceptible to faults, which in turn can impact the NN accuracy. In this\npaper, we study the resilience aspects of Register-Transfer Level (RTL) model\nof NN accelerators, in particular, fault characterization and mitigation. By\nfollowing a High-Level Synthesis (HLS) approach, first, we characterize the\nvulnerability of various components of RTL NN. We observed that the severity of\nfaults depends on both i) application-level specifications, i.e., NN data\n(inputs, weights, or intermediate), NN layers, and NN activation functions, and\nii) architectural-level specifications, i.e., data representation model and the\nparallelism degree of the underlying accelerator. Second, motivated by\ncharacterization results, we present a low-overhead fault mitigation technique\nthat can efficiently correct bit flips, by 47.3% better than state-of-the-art\nmethods. \n\n"}
{"id": "1806.09730", "contents": "Title: Analysis of Invariance and Robustness via Invertibility of ReLU-Networks Abstract: Studying the invertibility of deep neural networks (DNNs) provides a\nprincipled approach to better understand the behavior of these powerful models.\nDespite being a promising diagnostic tool, a consistent theory on their\ninvertibility is still lacking. We derive a theoretically motivated approach to\nexplore the preimages of ReLU-layers and mechanisms affecting the stability of\nthe inverse. Using the developed theory, we numerically show how this approach\nuncovers characteristic properties of the network. \n\n"}
{"id": "1806.09856", "contents": "Title: Dropout-based Active Learning for Regression Abstract: Active learning is relevant and challenging for high-dimensional regression\nmodels when the annotation of the samples is expensive. Yet most of the\nexisting sampling methods cannot be applied to large-scale problems, consuming\ntoo much time for data processing. In this paper, we propose a fast active\nlearning algorithm for regression, tailored for neural network models. It is\nbased on uncertainty estimation from stochastic dropout output of the network.\nExperiments on both synthetic and real-world datasets show comparable or better\nperformance (depending on the accuracy metric) as compared to the baselines.\nThis approach can be generalized to other deep learning architectures. It can\nbe used to systematically improve a machine-learning model as it offers a\ncomputationally efficient way of sampling additional data. \n\n"}
{"id": "1806.09888", "contents": "Title: Towards an understanding of CNNs: analysing the recovery of activation\n  pathways via Deep Convolutional Sparse Coding Abstract: Deep Convolutional Sparse Coding (D-CSC) is a framework reminiscent of deep\nconvolutional neural networks (DCNNs), but by omitting the learning of the\ndictionaries one can more transparently analyse the role of the activation\nfunction and its ability to recover activation paths through the layers.\nPapyan, Romano, and Elad conducted an analysis of such an architecture,\ndemonstrated the relationship with DCNNs and proved conditions under which the\nD-CSC is guaranteed to recover specific activation paths. A technical\ninnovation of their work highlights that one can view the efficacy of the ReLU\nnonlinear activation function of a DCNN through a new variant of the tensor's\nsparsity, referred to as stripe-sparsity. Using this they proved that\nrepresentations with an activation density proportional to the ambient\ndimension of the data are recoverable. We extend their uniform guarantees to a\nmodified model and prove that with high probability the true activation is\ntypically possible to recover for a greater density of activations per layer.\nOur extension follows from incorporating the prior work on one step\nthresholding by Schnass and Vandergheynst. \n\n"}
{"id": "1806.09981", "contents": "Title: Dynamic Spectrum Matching with One-shot Learning Abstract: Convolutional neural networks (CNN) have been shown to provide a good\nsolution for classification problems that utilize data obtained from\nvibrational spectroscopy. Moreover, CNNs are capable of identification from\nnoisy spectra without the need for additional preprocessing. However, their\napplication in practical spectroscopy is limited due to two shortcomings. The\neffectiveness of the classification using CNNs drops rapidly when only a small\nnumber of spectra per substance are available for training (which is a typical\nsituation in real applications). Additionally, to accommodate new, previously\nunseen substance classes, the network must be retrained which is\ncomputationally intensive. Here we address these issues by reformulating a\nmulti-class classification problem with a large number of classes, but a small\nnumber of samples per class, to a binary classification problem with sufficient\ndata available for representation learning. Namely, we define the learning task\nas identifying pairs of inputs as belonging to the same or different classes.\nWe achieve this using a Siamese convolutional neural network. A novel sampling\nstrategy is proposed to address the imbalance problem in training the Siamese\nNetwork. The trained network can effectively classify samples of unseen\nsubstance classes using just a single reference sample (termed as one-shot\nlearning in the machine learning community). Our results demonstrate better\naccuracy than other practical systems to date, while allowing effortless\nupdates of the system's database with novel substance classes. \n\n"}
{"id": "1806.10064", "contents": "Title: Adaptive Blending Units: Trainable Activation Functions for Deep Neural\n  Networks Abstract: The most widely used activation functions in current deep feed-forward neural\nnetworks are rectified linear units (ReLU), and many alternatives have been\nsuccessfully applied, as well. However, none of the alternatives have managed\nto consistently outperform the rest and there is no unified theory connecting\nproperties of the task and network with properties of activation functions for\nmost efficient training. A possible solution is to have the network learn its\npreferred activation functions. In this work, we introduce Adaptive Blending\nUnits (ABUs), a trainable linear combination of a set of activation functions.\nSince ABUs learn the shape, as well as the overall scaling of the activation\nfunction, we also analyze the effects of adaptive scaling in common activation\nfunctions. We experimentally demonstrate advantages of both adaptive scaling\nand ABUs over common activation functions across a set of systematically varied\nnetwork specifications. We further show that adaptive scaling works by\nmitigating covariate shifts during training, and that the observed advantages\nin performance of ABUs likewise rely largely on the activation function's\nability to adapt over the course of training. \n\n"}
{"id": "1806.10547", "contents": "Title: Online optimal task offloading with one-bit feedback Abstract: Task offloading is an emerging technology in fog-enabled networks. It allows\nusers to transmit tasks to neighbor fog nodes so as to utilize the computing\nresources of the networks. In this paper, we investigate a stochastic task\noffloading model and propose a multi-armed bandit framework to formulate this\nmodel. We consider the fact that different helper nodes prefer different kinds\nof tasks. Further, we assume each helper node just feeds back one-bit\ninformation to the task node to indicate the level of happiness. The key\nchallenge of this problem lies in the exploration-exploitation tradeoff. We\nthus implement a UCB-type algorithm to maximize the long-term happiness metric.\nNumerical simulations are given in the end of the paper to corroborate our\nstrategy. \n\n"}
{"id": "1806.10840", "contents": "Title: Training Discriminative Models to Evaluate Generative Ones Abstract: Generative models are known to be difficult to assess. Recent works,\nespecially on generative adversarial networks (GANs), produce good visual\nsamples of varied categories of images. However, the validation of their\nquality is still difficult to define and there is no existing agreement on the\nbest evaluation process. This paper aims at making a step toward an objective\nevaluation process for generative models. It presents a new method to assess a\ntrained generative model by evaluating the test accuracy of a classifier\ntrained with generated data. The test set is composed of real images.\nTherefore, The classifier accuracy is used as a proxy to evaluate if the\ngenerative model fit the true data distribution. By comparing results with\ndifferent generated datasets we are able to classify and compare generative\nmodels. The motivation of this approach is also to evaluate if generative\nmodels can help discriminative neural networks to learn, i.e., measure if\ntraining on generated data is able to make a model successful at testing on\nreal settings. Our experiments compare different generators from the\nVariational Auto-Encoders (VAE) and Generative Adversarial Network (GAN)\nframeworks on MNIST and fashion MNIST datasets. Our results show that none of\nthe generative models is able to replace completely true data to train a\ndiscriminative model. But they also show that the initial GAN and WGAN are the\nbest choices to generate on MNIST database (Modified National Institute of\nStandards and Technology database) and fashion MNIST database. \n\n"}
{"id": "1806.10897", "contents": "Title: Deep learning in business analytics and operations research: Models,\n  applications and managerial implications Abstract: Business analytics refers to methods and practices that create value through\ndata for individuals, firms, and organizations. This field is currently\nexperiencing a radical shift due to the advent of deep learning: deep neural\nnetworks promise improvements in prediction performance as compared to models\nfrom traditional machine learning. However, our research into the existing body\nof literature reveals a scarcity of research works utilizing deep learning in\nour discipline. Accordingly, the objectives of this overview article are as\nfollows: (1) we review research on deep learning for business analytics from an\noperational point of view. (2) We motivate why researchers and practitioners\nfrom business analytics should utilize deep neural networks and review\npotential use cases, necessary requirements, and benefits. (3) We investigate\nthe added value to operations research in different case studies with real data\nfrom entrepreneurial undertakings. All such cases demonstrate improvements in\noperational performance over traditional machine learning and thus direct value\ngains. (4) We provide guidelines and implications for researchers, managers and\npractitioners in operations research who want to advance their capabilities for\nbusiness analytics with regard to deep learning. (5) Our computational\nexperiments find that default, out-of-the-box architectures are often\nsuboptimal and thus highlight the value of customized architectures by\nproposing a novel deep-embedded network. \n\n"}
{"id": "1807.00123", "contents": "Title: Machine Learning for Integrating Data in Biology and Medicine:\n  Principles, Practice, and Opportunities Abstract: New technologies have enabled the investigation of biology and human health\nat an unprecedented scale and in multiple dimensions. These dimensions include\na myriad of properties describing genome, epigenome, transcriptome, microbiome,\nphenotype, and lifestyle. No single data type, however, can capture the\ncomplexity of all the factors relevant to understanding a phenomenon such as a\ndisease. Integrative methods that combine data from multiple technologies have\nthus emerged as critical statistical and computational approaches. The key\nchallenge in developing such approaches is the identification of effective\nmodels to provide a comprehensive and relevant systems view. An ideal method\ncan answer a biological or medical question, identifying important features and\npredicting outcomes, by harnessing heterogeneous data across several dimensions\nof biological variation. In this Review, we describe the principles of data\nintegration and discuss current methods and available implementations. We\nprovide examples of successful data integration in biology and medicine.\nFinally, we discuss current challenges in biomedical integrative methods and\nour perspective on the future development of the field. \n\n"}
{"id": "1807.00172", "contents": "Title: Algorithms for solving optimization problems arising from deep neural\n  net models: smooth problems Abstract: Machine Learning models incorporating multiple layered learning networks have\nbeen seen to provide effective models for various classification problems. The\nresulting optimization problem to solve for the optimal vector minimizing the\nempirical risk is, however, highly nonlinear. This presents a challenge to\napplication and development of appropriate optimization algorithms for solving\nthe problem. In this paper, we summarize the primary challenges involved and\npresent the case for a Newton-based method incorporating directions of negative\ncurvature, including promising numerical results on data arising from security\nanomally deetection. \n\n"}
{"id": "1807.00173", "contents": "Title: Algorithms for solving optimization problems arising from deep neural\n  net models: nonsmooth problems Abstract: Machine Learning models incorporating multiple layered learning networks have\nbeen seen to provide effective models for various classification problems. The\nresulting optimization problem to solve for the optimal vector minimizing the\nempirical risk is, however, highly nonconvex. This alone presents a challenge\nto application and development of appropriate optimization algorithms for\nsolving the problem. However, in addition, there are a number of interesting\nproblems for which the objective function is non- smooth and nonseparable. In\nthis paper, we summarize the primary challenges involved, the state of the art,\nand present some numerical results on an interesting and representative class\nof problems. \n\n"}
{"id": "1807.00448", "contents": "Title: Speeding up the Metabolism in E-commerce by Reinforcement Mechanism\n  Design Abstract: In a large E-commerce platform, all the participants compete for impressions\nunder the allocation mechanism of the platform. Existing methods mainly focus\non the short-term return based on the current observations instead of the\nlong-term return. In this paper, we formally establish the lifecycle model for\nproducts, by defining the introduction, growth, maturity and decline stages and\ntheir transitions throughout the whole life period. Based on such model, we\nfurther propose a reinforcement learning based mechanism design framework for\nimpression allocation, which incorporates the first principal component based\npermutation and the novel experiences generation method, to maximize short-term\nas well as long-term return of the platform. With the power of trial-and-error,\nit is possible to optimize impression allocation strategies globally which is\ncontribute to the healthy development of participants and the platform itself.\nWe evaluate our algorithm on a simulated environment built based on one of the\nlargest E-commerce platforms, and a significant improvement has been achieved\nin comparison with the baseline solutions. \n\n"}
{"id": "1807.00516", "contents": "Title: Balanced Distribution Adaptation for Transfer Learning Abstract: Transfer learning has achieved promising results by leveraging knowledge from\nthe source domain to annotate the target domain which has few or none labels.\nExisting methods often seek to minimize the distribution divergence between\ndomains, such as the marginal distribution, the conditional distribution or\nboth. However, these two distances are often treated equally in existing\nalgorithms, which will result in poor performance in real applications.\nMoreover, existing methods usually assume that the dataset is balanced, which\nalso limits their performances on imbalanced tasks that are quite common in\nreal problems. To tackle the distribution adaptation problem, in this paper, we\npropose a novel transfer learning approach, named as Balanced Distribution\n\\underline{A}daptation~(BDA), which can adaptively leverage the importance of\nthe marginal and conditional distribution discrepancies, and several existing\nmethods can be treated as special cases of BDA. Based on BDA, we also propose a\nnovel Weighted Balanced Distribution Adaptation~(W-BDA) algorithm to tackle the\nclass imbalance issue in transfer learning. W-BDA not only considers the\ndistribution adaptation between domains but also adaptively changes the weight\nof each class. To evaluate the proposed methods, we conduct extensive\nexperiments on several transfer learning tasks, which demonstrate the\neffectiveness of our proposed algorithms over several state-of-the-art methods. \n\n"}
{"id": "1807.01134", "contents": "Title: Welfare and Distributional Impacts of Fair Classification Abstract: Current methodologies in machine learning analyze the effects of various\nstatistical parity notions of fairness primarily in light of their impacts on\npredictive accuracy and vendor utility loss. In this paper, we propose a new\nframework for interpreting the effects of fairness criteria by converting the\nconstrained loss minimization problem into a social welfare maximization\nproblem. This translation moves a classifier and its output into utility space\nwhere individuals, groups, and society at-large experience different welfare\nchanges due to classification assignments. Under this characterization,\npredictions and fairness constraints are seen as shaping societal welfare and\ndistribution and revealing individuals' implied welfare weights in\nsociety--weights that may then be interpreted through a fairness lens. The\nsocial welfare formulation of the fairness problem brings to the fore concerns\nof distributive justice that have always had a central albeit more implicit\nrole in standard algorithmic fairness approaches. \n\n"}
{"id": "1807.01613", "contents": "Title: Conditional Neural Processes Abstract: Deep neural networks excel at function approximation, yet they are typically\ntrained from scratch for each new function. On the other hand, Bayesian\nmethods, such as Gaussian Processes (GPs), exploit prior knowledge to quickly\ninfer the shape of a new function at test time. Yet GPs are computationally\nexpensive, and it can be hard to design appropriate priors. In this paper we\npropose a family of neural models, Conditional Neural Processes (CNPs), that\ncombine the benefits of both. CNPs are inspired by the flexibility of\nstochastic processes such as GPs, but are structured as neural networks and\ntrained via gradient descent. CNPs make accurate predictions after observing\nonly a handful of training data points, yet scale to complex functions and\nlarge datasets. We demonstrate the performance and versatility of the approach\non a range of canonical machine learning tasks, including regression,\nclassification and image completion. \n\n"}
{"id": "1807.01703", "contents": "Title: An efficient quantum circuits optimizing scheme compared with QISKit Abstract: Recently, the development of quantum chips has made great progress-- the\nnumber of qubits is increasing and the fidelity is getting higher. However,\nqubits of these chips are not always fully connected, which sets additional\nbarriers for implementing quantum algorithms and programming quantum programs.\nIn this paper, we introduce a general circuit optimizing scheme, which can\nefficiently adjust and optimize quantum circuits according to arbitrary given\nqubits' layout by adding additional quantum gates, exchanging qubits and\nmerging single-qubit gates. Compared with the optimizing algorithm of IBM's\nQISKit, the quantum gates consumed by our scheme is 74.7%, and the execution\ntime is only 12.9% on average. \n\n"}
{"id": "1807.02023", "contents": "Title: Quantum circuits for floating-point arithmetic Abstract: Quantum algorithms to solve practical problems in quantum chemistry,\nmaterials science, and matrix inversion often involve a significant amount of\narithmetic operations which act on a superposition of inputs. These have to be\ncompiled to a set of fault-tolerant low-level operations and throughout this\ntranslation process, the compiler aims to come close to the Pareto-optimal\nfront between the number of required qubits and the depth of the resulting\ncircuit. In this paper, we provide quantum circuits for floating-point addition\nand multiplication which we find using two vastly different approaches. The\nfirst approach is to automatically generate circuits from classical Verilog\nimplementations using synthesis tools and the second is to generate and\noptimize these circuits by hand. We compare our two approaches and provide\nevidence that floating-point arithmetic is a viable candidate for use in\nquantum computing, at least for typical scientific applications, where addition\noperations usually do not dominate the computation. All our circuits were\nconstructed and tested using the software tools LIQ$Ui|\\rangle{}$ and RevKit. \n\n"}
{"id": "1807.02078", "contents": "Title: Goal-oriented Trajectories for Efficient Exploration Abstract: Exploration is a difficult challenge in reinforcement learning and even\nrecent state-of-the art curiosity-based methods rely on the simple\nepsilon-greedy strategy to generate novelty. We argue that pure random walks do\nnot succeed to properly expand the exploration area in most environments and\npropose to replace single random action choices by random goals selection\nfollowed by several steps in their direction. This approach is compatible with\nany curiosity-based exploration and off-policy reinforcement learning agents\nand generates longer and safer trajectories than individual random actions. To\nillustrate this, we present a task-independent agent that learns to reach\ncoordinates in screen frames and demonstrate its ability to explore with the\ngame Super Mario Bros. improving significantly the score of a baseline DQN\nagent. \n\n"}
{"id": "1807.02324", "contents": "Title: Sum-Product Networks for Sequence Labeling Abstract: We consider higher-order linear-chain conditional random fields (HO-LC-CRFs)\nfor sequence modelling, and use sum-product networks (SPNs) for representing\nhigher-order input- and output-dependent factors. SPNs are a recently\nintroduced class of deep models for which exact and efficient inference can be\nperformed. By combining HO-LC-CRFs with SPNs, expressive models over both the\noutput labels and the hidden variables are instantiated while still enabling\nefficient exact inference. Furthermore, the use of higher-order factors allows\nus to capture relations of multiple input segments and multiple output labels\nas often present in real-world data. These relations can not be modelled by the\ncommonly used first-order models and higher-order models with local factors\nincluding only a single output label. We demonstrate the effectiveness of our\nproposed models for sequence labeling. In extensive experiments, we outperform\nother state-of-the-art methods in optical character recognition and achieve\ncompetitive results in phone classification. \n\n"}
{"id": "1807.02391", "contents": "Title: Extracting Actionable Knowledge from Domestic Violence Discourses on\n  Social Media Abstract: Domestic Violence (DV) is considered as big social issue and there exists a\nstrong relationship between DV and health impacts of the public. Existing\nresearch studies have focused on social media to track and analyse real world\nevents like emerging trends, natural disasters, user sentiment analysis,\npolitical opinions, and health care. However there is less attention given on\nsocial welfare issues like DV and its impact on public health. Recently, the\nvictims of DV turned to social media platforms to express their feelings in the\nform of posts and seek the social and emotional support, for sympathetic\nencouragement, to show compassion and empathy among public. But, it is\ndifficult to mine the actionable knowledge from large conversational datasets\nfrom social media due to the characteristics of high dimensions, short, noisy,\nhuge volume, high velocity, and so on. Hence, this paper will propose a novel\nframework to model and discover the various themes related to DV from the\npublic domain. The proposed framework would possibly provide unprecedentedly\nvaluable information to the public health researchers, national family health\norganizations, government and public with data enrichment and consolidation to\nimprove the social welfare of the community. Thus provides actionable knowledge\nby monitoring and analysing continuous and rich user generated content. \n\n"}
{"id": "1807.02500", "contents": "Title: Overview and Comparison of Gate Level Quantum Software Platforms Abstract: Quantum computers are available to use over the cloud, but the recent\nexplosion of quantum software platforms can be overwhelming for those deciding\non which to use. In this paper, we provide a current picture of the rapidly\nevolving quantum computing landscape by comparing four software\nplatforms---Forest (pyQuil), Qiskit, ProjectQ, and the Quantum Developer Kit\n(Q\\#)---that enable researchers to use real and simulated quantum devices. Our\nanalysis covers requirements and installation, language syntax through example\nprograms, library support, and quantum simulator capabilities for each\nplatform. For platforms that have quantum computer support, we compare\nhardware, quantum assembly languages, and quantum compilers. We conclude by\ncovering features of each and briefly mentioning other quantum computing\nsoftware packages. \n\n"}
{"id": "1807.02999", "contents": "Title: Decreasing the size of the Restricted Boltzmann machine Abstract: We propose a method to decrease the number of hidden units of the restricted\nBoltzmann machine while avoiding decrease of the performance measured by the\nKullback-Leibler divergence. Then, we demonstrate our algorithm by using\nnumerical simulations. \n\n"}
{"id": "1807.03610", "contents": "Title: Window Opening Model using Deep Learning Methods Abstract: Occupant behavior (OB) and in particular window openings need to be\nconsidered in building performance simulation (BPS), in order to realistically\nmodel the indoor climate and energy consumption for heating ventilation and air\nconditioning (HVAC). However, the proposed OB window opening models are often\nbiased towards the over-represented class where windows remained closed. In\naddition, they require tuning for each occupant which can not be efficiently\nscaled to the increased number of occupants. This paper presents a window\nopening model for commercial buildings using deep learning methods. The model\nis trained using data from occupants from an office building in Germany. In\ntotal the model is evaluated using almost 20 mio. data points from 3\nindependent buildings, located in Aachen, Frankfurt and Philadelphia.\nEventually, the results of 3100 core hours of model development are summarized,\nwhich makes this study the largest of its kind in window states modeling.\nAdditionally, the practical potential of the proposed model was tested by\nincorporating it in the Modelica-based thermal building simulation. The\nresulting evaluation accuracy and F1 scores on the office buildings ranged\nbetween 86-89 % and 0.53-0.65 respectively. The performance dropped around 15 %\npoints in case of sparse input data, while the F1 score remained high. \n\n"}
{"id": "1807.03870", "contents": "Title: Learning Implicit Generative Models by Teaching Explicit Ones Abstract: Implicit generative models are difficult to train as no explicit density\nfunctions are defined. Generative adversarial nets (GANs) present a minimax\nframework to train such models, which however can suffer from mode collapse due\nto the nature of the JS-divergence. This paper presents a learning by teaching\n(LBT) approach to learning implicit models, which intrinsically avoids the mode\ncollapse problem by optimizing a KL-divergence rather than the JS-divergence in\nGANs. In LBT, an auxiliary density estimator is introduced to fit the implicit\nmodel's distribution while the implicit model teaches the density estimator to\nmatch the data distribution. LBT is formulated as a bilevel optimization\nproblem, whose optimal generator matches the true data distribution. LBT can be\nnaturally integrated with GANs to derive a hybrid LBT-GAN that enjoys\ncomplimentary benefits. Finally, we present a stochastic gradient ascent\nalgorithm with unrolling to solve the challenging learning problems.\nExperimental results demonstrate the effectiveness of our method. \n\n"}
{"id": "1807.03915", "contents": "Title: Seq2Seq2Sentiment: Multimodal Sequence to Sequence Models for Sentiment\n  Analysis Abstract: Multimodal machine learning is a core research area spanning the language,\nvisual and acoustic modalities. The central challenge in multimodal learning\ninvolves learning representations that can process and relate information from\nmultiple modalities. In this paper, we propose two methods for unsupervised\nlearning of joint multimodal representations using sequence to sequence\n(Seq2Seq) methods: a \\textit{Seq2Seq Modality Translation Model} and a\n\\textit{Hierarchical Seq2Seq Modality Translation Model}. We also explore\nmultiple different variations on the multimodal inputs and outputs of these\nseq2seq models. Our experiments on multimodal sentiment analysis using the\nCMU-MOSI dataset indicate that our methods learn informative multimodal\nrepresentations that outperform the baselines and achieve improved performance\non multimodal sentiment analysis, specifically in the Bimodal case where our\nmodel is able to improve F1 Score by twelve points. We also discuss future\ndirections for multimodal Seq2Seq methods. \n\n"}
{"id": "1807.04106", "contents": "Title: VFunc: a Deep Generative Model for Functions Abstract: We introduce a deep generative model for functions. Our model provides a\njoint distribution p(f, z) over functions f and latent variables z which lets\nus efficiently sample from the marginal p(f) and maximize a variational lower\nbound on the entropy H(f). We can thus maximize objectives of the form\nE_{f~p(f)}[R(f)] + c*H(f), where R(f) denotes, e.g., a data log-likelihood term\nor an expected reward. Such objectives encompass Bayesian deep learning in\nfunction space, rather than parameter space, and Bayesian deep RL with\nrepresentations of uncertainty that offer benefits over bootstrapping and\nparameter noise. In this short paper we describe our model, situate it in the\ncontext of prior work, and present proof-of-concept experiments for regression\nand RL. \n\n"}
{"id": "1807.04109", "contents": "Title: Modeling and Soft-fault Diagnosis of Underwater Thrusters with Recurrent\n  Neural Networks Abstract: Noncritical soft-faults and model deviations are a challenge for Fault\nDetection and Diagnosis (FDD) of resident Autonomous Underwater Vehicles\n(AUVs). Such systems may have a faster performance degradation due to the\npermanent exposure to the marine environment, and constant monitoring of\ncomponent conditions is required to ensure their reliability. This works\npresents an evaluation of Recurrent Neural Networks (RNNs) for a data-driven\nfault detection and diagnosis scheme for underwater thrusters with empirical\ndata. The nominal behavior of the thruster was modeled using the measured\ncontrol input, voltage, rotational speed and current signals. We evaluated the\nperformance of fault classification using all the measured signals compared to\nusing the computed residuals from the nominal model as features. \n\n"}
{"id": "1807.04119", "contents": "Title: Exploiting statistical dependencies of time series with hierarchical\n  correlation reconstruction Abstract: While we are usually focused on forecasting future values of time series, it\nis often valuable to additionally predict their entire probability\ndistributions, e.g. to evaluate risk, Monte Carlo simulations. On example of\ntime series of $\\approx$ 30000 Dow Jones Industrial Averages, there will be\npresented application of hierarchical correlation reconstruction for this\npurpose: MSE estimating polynomial as joint density for (current value,\ncontext), where context is for example a few previous values. Then substituting\nthe currently observed context and normalizing density to 1, we get predicted\nprobability distribution for the current value. In contrast to standard machine\nlearning approaches like neural networks, optimal polynomial coefficients here\nhave inexpensive direct formula, have controllable accuracy, are unique and\nindependently calculated, each has a specific cumulant-like interpretation, and\nsuch approximation can asymptotically approach complete description of any real\njoint distribution - providing universal tool to quantitatively describe and\nexploit statistical dependencies in time series, systematically enhancing\nARMA/ARCH-like approaches, also based on different distributions than Gaussian\nwhich turns out improper for daily log returns. There is also discussed\napplication for non-stationary time series like calculating linear time trend,\nor adapting coefficients to local statistical behavior. \n\n"}
{"id": "1807.04241", "contents": "Title: DeepMove: Learning Place Representations through Large Scale Movement\n  Data Abstract: Understanding and reasoning about places and their relationships are critical\nfor many applications. Places are traditionally curated by a small group of\npeople as place gazetteers and are represented by an ID with spatial extent,\ncategory, and other descriptions. However, a place context is described to a\nlarge extent by movements made from/to other places. Places are linked and\nrelated to each other by these movements. This important context is missing\nfrom the traditional representation.\n  We present DeepMove, a novel approach for learning latent representations of\nplaces. DeepMove advances the current deep learning based place representations\nby directly model movements between places. We demonstrate DeepMove's latent\nrepresentations on place categorization and clustering tasks on large place and\nmovement datasets with respect to important parameters. Our results show that\nDeepMove outperforms state-of-the-art baselines. DeepMove's representations can\nprovide up to 15% higher than competing methods in matching rate of place\ncategory and result in up to 39% higher silhouette coefficient value for place\nclusters.\n  DeepMove is spatial and temporal context aware. It is scalable. It\noutperforms competing models using much smaller training dataset (a month or\n1/12 of data). These qualities make it suitable for a broad class of real-world\napplications. \n\n"}
{"id": "1807.04439", "contents": "Title: Will it Blend? Composing Value Functions in Reinforcement Learning Abstract: An important property for lifelong-learning agents is the ability to combine\nexisting skills to solve unseen tasks. In general, however, it is unclear how\nto compose skills in a principled way. We provide a \"recipe\" for optimal value\nfunction composition in entropy-regularised reinforcement learning (RL) and\nthen extend this to the standard RL setting. Composition is demonstrated in a\nvideo game environment, where an agent with an existing library of policies is\nable to solve new tasks without the need for further learning. \n\n"}
{"id": "1807.05292", "contents": "Title: Neural Networks Regularization Through Representation Learning Abstract: Neural network models and deep models are one of the leading and state of the\nart models in machine learning. Most successful deep neural models are the ones\nwith many layers which highly increases their number of parameters. Training\nsuch models requires a large number of training samples which is not always\navailable. One of the fundamental issues in neural networks is overfitting\nwhich is the issue tackled in this thesis. Such problem often occurs when the\ntraining of large models is performed using few training samples. Many\napproaches have been proposed to prevent the network from overfitting and\nimprove its generalization performance such as data augmentation, early\nstopping, parameters sharing, unsupervised learning, dropout, batch\nnormalization, etc.\n  In this thesis, we tackle the neural network overfitting issue from a\nrepresentation learning perspective by considering the situation where few\ntraining samples are available which is the case of many real world\napplications. We propose three contributions. The first one presented in\nchapter 2 is dedicated to dealing with structured output problems to perform\nmultivariate regression when the output variable y contains structural\ndependencies between its components. The second contribution described in\nchapter 3 deals with the classification task where we propose to exploit prior\nknowledge about the internal representation of the hidden layers in neural\nnetworks. Our last contribution presented in chapter 4 showed the interest of\ntransfer learning in applications where only few samples are available. In this\ncontribution, we provide an automatic system based on such learning scheme with\nan application to medical domain. In this application, the task consists in\nlocalizing the third lumbar vertebra in a 3D CT scan. This work has been done\nin collaboration with the clinic Rouen Henri Becquerel Center who provided us\nwith data. \n\n"}
{"id": "1807.05836", "contents": "Title: Forecasting market states Abstract: We propose a novel methodology to define, analyze and forecast market states.\nIn our approach market states are identified by a reference sparse precision\nmatrix and a vector of expectation values. In our procedure, each multivariate\nobservation is associated with a given market state accordingly to a\nminimization of a penalized Mahalanobis distance. The procedure is made\ncomputationally very efficient and can be used with a large number of assets.\nWe demonstrate that this procedure is successful at clustering different states\nof the markets in an unsupervised manner. In particular, we describe an\nexperiment with one hundred log-returns and two states in which the methodology\nautomatically associates states prevalently to pre- and post- crisis periods\nwith one state gathering periods with average positive returns and the other\nstate periods with average negative returns, therefore discovering\nspontaneously the common classification of `bull' and `bear' markets. In\nanother experiment, with again one hundred log-returns and two states, we\ndemonstrate that this procedure can be efficiently used to forecast off-sample\nfuture market states with significant prediction accuracy. This methodology\nopens the way to a range of applications in risk management and trading\nstrategies in the context where the correlation structure plays a central role. \n\n"}
{"id": "1807.06630", "contents": "Title: Expressive power of outer product manifolds on feed-forward neural\n  networks Abstract: Hierarchical neural networks are exponentially more efficient than their\ncorresponding \"shallow\" counterpart with the same expressive power, but involve\nhuge number of parameters and require tedious amounts of training. Our main\nidea is to mathematically understand and describe the hierarchical structure of\nfeedforward neural networks by reparametrization invariant Riemannian metrics.\nBy computing or approximating the tangent subspace, we better utilize the\noriginal network via sparse representations that enables switching to shallow\nnetworks after a very early training stage. Our experiments show that the\nproposed approximation of the metric improves and sometimes even surpasses the\nachievable performance of the original network significantly even after a few\nepochs of training the original feedforward network. \n\n"}
{"id": "1807.07132", "contents": "Title: Newton-ADMM: A Distributed GPU-Accelerated Optimizer for Multiclass\n  Classification Problems Abstract: First-order optimization methods, such as stochastic gradient descent (SGD)\nand its variants, are widely used in machine learning applications due to their\nsimplicity and low per-iteration costs. However, they often require larger\nnumbers of iterations, with associated communication costs in distributed\nenvironments. In contrast, Newton-type methods, while having higher\nper-iteration costs, typically require a significantly smaller number of\niterations, which directly translates to reduced communication costs. In this\npaper, we present a novel distributed optimizer for classification problems,\nwhich integrates a GPU-accelerated Newton-type solver with the global consensus\nformulation of Alternating Direction of Method Multipliers (ADMM). By\nleveraging the communication efficiency of ADMM, GPU-accelerated inexact-Newton\nsolver, and an effective spectral penalty parameter selection strategy, we show\nthat our proposed method (i) yields better generalization performance on\nseveral classification problems; (ii) significantly outperforms\nstate-of-the-art methods in distributed time to solution; and (iii) offers\nbetter scaling on large distributed platforms. \n\n"}
{"id": "1807.07282", "contents": "Title: Anomaly Detection for Water Treatment System based on Neural Network\n  with Automatic Architecture Optimization Abstract: We continue to develop our neural network (NN) based forecasting approach to\nanomaly detection (AD) using the Secure Water Treatment (SWaT) industrial\ncontrol system (ICS) testbed dataset. We propose genetic algorithms (GA) to\nfind the best NN architecture for a given dataset, using the NAB metric to\nassess the quality of different architectures. The drawbacks of the F1-metric\nare analyzed. Several techniques are proposed to improve the quality of AD:\nexponentially weighted smoothing, mean p-powered error measure, individual\nerror weight for each variable, disjoint prediction windows. Based on the\ntechniques used, an approach to anomaly interpretation is introduced. \n\n"}
{"id": "1807.08706", "contents": "Title: Contrastive Explanations for Reinforcement Learning in terms of Expected\n  Consequences Abstract: Machine Learning models become increasingly proficient in complex tasks.\nHowever, even for experts in the field, it can be difficult to understand what\nthe model learned. This hampers trust and acceptance, and it obstructs the\npossibility to correct the model. There is therefore a need for transparency of\nmachine learning models. The development of transparent classification models\nhas received much attention, but there are few developments for achieving\ntransparent Reinforcement Learning (RL) models. In this study we propose a\nmethod that enables a RL agent to explain its behavior in terms of the expected\nconsequences of state transitions and outcomes. First, we define a translation\nof states and actions to a description that is easier to understand for human\nusers. Second, we developed a procedure that enables the agent to obtain the\nconsequences of a single action, as well as its entire policy. The method\ncalculates contrasts between the consequences of a policy derived from a user\nquery, and of the learned policy of the agent. Third, a format for generating\nexplanations was constructed. A pilot survey study was conducted to explore\npreferences of users for different explanation properties. Results indicate\nthat human users tend to favor explanations about policy rather than about\nsingle actions. \n\n"}
{"id": "1807.08820", "contents": "Title: RAIM: Recurrent Attentive and Intensive Model of Multimodal Patient\n  Monitoring Data Abstract: With the improvement of medical data capturing, vast amount of continuous\npatient monitoring data, e.g., electrocardiogram (ECG), real-time vital signs\nand medications, become available for clinical decision support at intensive\ncare units (ICUs). However, it becomes increasingly challenging to model such\ndata, due to high density of the monitoring data, heterogeneous data types and\nthe requirement for interpretable models. Integration of these high-density\nmonitoring data with the discrete clinical events (including diagnosis,\nmedications, labs) is challenging but potentially rewarding since richness and\ngranularity in such multimodal data increase the possibilities for accurate\ndetection of complex problems and predicting outcomes (e.g., length of stay and\nmortality). We propose Recurrent Attentive and Intensive Model (RAIM) for\njointly analyzing continuous monitoring data and discrete clinical events. RAIM\nintroduces an efficient attention mechanism for continuous monitoring data\n(e.g., ECG), which is guided by discrete clinical events (e.g, medication\nusage). We apply RAIM in predicting physiological decompensation and length of\nstay in those critically ill patients at ICU. With evaluations on MIMIC- III\nWaveform Database Matched Subset, we obtain an AUC-ROC score of 90.18% for\npredicting decompensation and an accuracy of 86.82% for forecasting length of\nstay with our final model, which outperforms our six baseline models. \n\n"}
{"id": "1807.09063", "contents": "Title: On complexity of post-processing in analyzing GATE-driven X-ray spectrum Abstract: Computed Tomography (CT) imaging is one of the most influential diagnostic\nmethods. In clinical reconstruction, an effective energy is used instead of\ntotal X-ray spectrum. This approximation causes an accuracy decline. To\nincrease the contrast, single source or dual source dual energy CT can be used\nto reach optimal values of tissue differentiation. However, these\ninfrastructures are still at the laboratory level, and their safeties for\npatients are still yet to mature. Therefore, computer modelling of DECT could\nbe used. We propose a novel post-processing approach for converting a total\nX-ray spectrum into irregular intervals of quantized energy. We simulate a\nphantom in GATE/GEANT4 and irradiate it based on CT configuration. Inverse\nRadon transform is applied to the acquired sinogram to construct the\nPixel-based Attenuation Matrix (PAM). To construct images represented by each\ninterval, water attenuation coefficient of the interval is extracted from NIST\nand used in the Hounsfield unit (HU) scale in conjunction with PAM. The CT\nimage is modified by using of an associated normalized photon flux and\ncalculated HU corresponding to the interval. We demonstrate the proposed method\nefficiency via complexity analysis, using absolute and relative complexities,\nentropy measures, Kolmogorov complexity, morphological richness, and\nquantitative segmentation criteria associated with standard fuzzy C-means. The\nirregularity of the modified CT images decreases over the simulated ones. \n\n"}
{"id": "1807.09142", "contents": "Title: Recurrent Neural Networks for Long and Short-Term Sequential\n  Recommendation Abstract: Recommender systems objectives can be broadly characterized as modeling user\npreferences over short-or long-term time horizon. A large body of previous\nresearch studied long-term recommendation through dimensionality reduction\ntechniques applied to the historical user-item interactions. A recently\nintroduced session-based recommendation setting highlighted the importance of\nmodeling short-term user preferences. In this task, Recurrent Neural Networks\n(RNN) have shown to be successful at capturing the nuances of user's\ninteractions within a short time window. In this paper, we evaluate RNN-based\nmodels on both short-term and long-term recommendation tasks. Our experimental\nresults suggest that RNNs are capable of predicting immediate as well as\ndistant user interactions. We also find the best performing configuration to be\na stacked RNN with layer normalization and tied item embeddings. \n\n"}
{"id": "1807.09586", "contents": "Title: Perturb and Combine to Identify Influential Spreaders in Real-World\n  Networks Abstract: Some of the most effective influential spreader detection algorithms are\nunstable to small perturbations of the network structure. Inspired by bagging\nin Machine Learning, we propose the first Perturb and Combine (P&C) procedure\nfor networks. It (1) creates many perturbed versions of a given graph, (2)\napplies a node scoring function separately to each graph, and (3) combines the\nresults. Experiments conducted on real-world networks of various sizes with the\nk-core, generalized k-core, and PageRank algorithms reveal that P&C brings\nsubstantial improvements. Moreover, this performance boost can be obtained at\nalmost no extra cost through parallelization. Finally, a bias-variance analysis\nsuggests that P&C works mainly by reducing bias, and that therefore, it should\nbe capable of improving the performance of all vertex scoring functions,\nincluding stable ones. \n\n"}
{"id": "1807.09901", "contents": "Title: Neural State Classification for Hybrid Systems Abstract: We introduce the State Classification Problem (SCP) for hybrid systems, and\npresent Neural State Classification (NSC) as an efficient solution technique.\nSCP generalizes the model checking problem as it entails classifying each state\n$s$ of a hybrid automaton as either positive or negative, depending on whether\nor not $s$ satisfies a given time-bounded reachability specification. This is\nan interesting problem in its own right, which NSC solves using\nmachine-learning techniques, Deep Neural Networks in particular. State\nclassifiers produced by NSC tend to be very efficient (run in constant time and\nspace), but may be subject to classification errors. To quantify and mitigate\nsuch errors, our approach comprises: i) techniques for certifying, with\nstatistical guarantees, that an NSC classifier meets given accuracy levels; ii)\ntuning techniques, including a novel technique based on adversarial sampling,\nthat can virtually eliminate false negatives (positive states classified as\nnegative), thereby making the classifier more conservative. We have applied NSC\nto six nonlinear hybrid system benchmarks, achieving an accuracy of 99.25% to\n99.98%, and a false-negative rate of 0.0033 to 0, which we further reduced to\n0.0015 to 0 after tuning the classifier. We believe that this level of accuracy\nis acceptable in many practical applications, and that these results\ndemonstrate the promise of the NSC approach. \n\n"}
{"id": "1807.10363", "contents": "Title: Message-passing neural networks for high-throughput polymer screening Abstract: Machine learning methods have shown promise in predicting molecular\nproperties, and given sufficient training data machine learning approaches can\nenable rapid high-throughput virtual screening of large libraries of compounds.\nGraph-based neural network architectures have emerged in recent years as the\nmost successful approach for predictions based on molecular structure, and have\nconsistently achieved the best performance on benchmark quantum chemical\ndatasets. However, these models have typically required optimized 3D structural\ninformation for the molecule to achieve the highest accuracy. These 3D\ngeometries are costly to compute for high levels of theory, limiting the\napplicability and practicality of machine learning methods in high-throughput\nscreening applications. In this study, we present a new database of candidate\nmolecules for organic photovoltaic applications, comprising approximately\n91,000 unique chemical structures.Compared to existing datasets, this dataset\ncontains substantially larger molecules (up to 200 atoms) as well as\nextrapolated properties for long polymer chains. We show that message-passing\nneural networks trained with and without 3D structural information for these\nmolecules achieve similar accuracy, comparable to state-of-the-art methods on\nexisting benchmark datasets. These results therefore emphasize that for larger\nmolecules with practical applications, near-optimal prediction results can be\nobtained without using optimized 3D geometry as an input. We further show that\nlearned molecular representations can be leveraged to reduce the training data\nrequired to transfer predictions to a new DFT functional. \n\n"}
{"id": "1807.10422", "contents": "Title: Understanding V2V Driving Scenarios through Traffic Primitives Abstract: Semantically understanding complex drivers' encountering behavior, wherein\ntwo or multiple vehicles are spatially close to each other, does potentially\nbenefit autonomous car's decision-making design. This paper presents a\nframework of analyzing various encountering behaviors through decomposing\ndriving encounter data into small building blocks, called driving primitives,\nusing nonparametric Bayesian learning (NPBL) approaches, which offers a\nflexible way to gain an insight into the complex driving encounters without any\nprerequisite knowledge. The effectiveness of our proposed primitive-based\nframework is validated based on 976 naturalistic driving encounters, from which\nmore than 4000 driving primitives are learned using NPBL - a sticky HDP-HMM,\ncombined a hidden Markov model (HMM) with a hierarchical Dirichlet process\n(HDP). After that, a dynamic time warping method integrated with k-means\nclustering is then developed to cluster all these extracted driving primitives\ninto groups. Experimental results find that there exist 20 kinds of driving\nprimitives capable of representing the basic components of driving encounters\nin our database. This primitive-based analysis methodology potentially reveals\nunderlying information of vehicle-vehicle encounters for self-driving\napplications. \n\n"}
{"id": "1807.10570", "contents": "Title: Embedded Implementation of a Deep Learning Smile Detector Abstract: In this paper we study the real time deployment of deep learning algorithms\nin low resource computational environments. As the use case, we compare the\naccuracy and speed of neural networks for smile detection using different\nneural network architectures and their system level implementation on NVidia\nJetson embedded platform. We also propose an asynchronous multithreading scheme\nfor parallelizing the pipeline. Within this framework, we experimentally\ncompare thirteen widely used network topologies. The experiments show that low\ncomplexity architectures can achieve almost equal performance as larger ones,\nwith a fraction of computation required. \n\n"}
{"id": "1807.10588", "contents": "Title: A Modality-Adaptive Method for Segmenting Brain Tumors and\n  Organs-at-Risk in Radiation Therapy Planning Abstract: In this paper we present a method for simultaneously segmenting brain tumors\nand an extensive set of organs-at-risk for radiation therapy planning of\nglioblastomas. The method combines a contrast-adaptive generative model for\nwhole-brain segmentation with a new spatial regularization model of tumor shape\nusing convolutional restricted Boltzmann machines. We demonstrate\nexperimentally that the method is able to adapt to image acquisitions that\ndiffer substantially from any available training data, ensuring its\napplicability across treatment sites; that its tumor segmentation accuracy is\ncomparable to that of the current state of the art; and that it captures most\norgans-at-risk sufficiently well for radiation therapy planning purposes. The\nproposed method may be a valuable step towards automating the delineation of\nbrain tumors and organs-at-risk in glioblastoma patients undergoing radiation\ntherapy. \n\n"}
{"id": "1807.10876", "contents": "Title: Transportation Modes Classification Using Feature Engineering Abstract: Predicting transportation modes from GPS (Global Positioning System) records\nis a hot topic in the trajectory mining domain. Each GPS record is called a\ntrajectory point and a trajectory is a sequence of these points. Trajectory\nmining has applications including but not limited to transportation mode\ndetection, tourism, traffic congestion, smart cities management, animal\nbehaviour analysis, environmental preservation, and traffic dynamics are some\nof the trajectory mining applications. Transportation modes prediction as one\nof the tasks in human mobility and vehicle mobility applications plays an\nimportant role in resource allocation, traffic management systems, tourism\nplanning and accident detection. In this work, the proposed framework in Etemad\net al. is extended to consider other aspects in the task of transportation\nmodes prediction. Wrapper search and information retrieval methods were\ninvestigated to find the best subset of trajectory features. Finding the best\nclassifier and the best feature subset, the framework is compared against two\nrelated papers that applied deep learning methods. The results show that our\nframework achieved better performance. Moreover, the ground truth noise removal\nimproved accuracy of transportation modes prediction task; however, the\nassumption of having access to test set labels in pre-processing task is\ninvalid. Furthermore, the cross validation approaches were investigated and the\nperformance results show that the random cross validation method provides\noptimistic results. \n\n"}
{"id": "1807.11346", "contents": "Title: Dropout-GAN: Learning from a Dynamic Ensemble of Discriminators Abstract: We propose to incorporate adversarial dropout in generative multi-adversarial\nnetworks, by omitting or dropping out, the feedback of each discriminator in\nthe framework with some probability at the end of each batch. Our approach\nforces the single generator not to constrain its output to satisfy a single\ndiscriminator, but, instead, to satisfy a dynamic ensemble of discriminators.\nWe show that this leads to a more generalized generator, promoting variety in\nthe generated samples and avoiding the common mode collapse problem commonly\nexperienced with generative adversarial networks (GANs). We further provide\nevidence that the proposed framework, named Dropout-GAN, promotes sample\ndiversity both within and across epochs, eliminating mode collapse and\nstabilizing training. \n\n"}
{"id": "1807.11573", "contents": "Title: State-of-the-art and gaps for deep learning on limited training data in\n  remote sensing Abstract: Deep learning usually requires big data, with respect to both volume and\nvariety. However, most remote sensing applications only have limited training\ndata, of which a small subset is labeled. Herein, we review three\nstate-of-the-art approaches in deep learning to combat this challenge. The\nfirst topic is transfer learning, in which some aspects of one domain, e.g.,\nfeatures, are transferred to another domain. The next is unsupervised learning,\ne.g., autoencoders, which operate on unlabeled data. The last is generative\nadversarial networks, which can generate realistic looking data that can fool\nthe likes of both a deep learning network and human. The aim of this article is\nto raise awareness of this dilemma, to direct the reader to existing work and\nto highlight current gaps that need solving. \n\n"}
{"id": "1807.11620", "contents": "Title: K-medoids Clustering of Data Sequences with Composite Distributions Abstract: This paper studies clustering of data sequences using the k-medoids\nalgorithm. All the data sequences are assumed to be generated from\n\\emph{unknown} continuous distributions, which form clusters with each cluster\ncontaining a composite set of closely located distributions (based on a certain\ndistance metric between distributions). The maximum intra-cluster distance is\nassumed to be smaller than the minimum inter-cluster distance, and both values\nare assumed to be known. The goal is to group the data sequences together if\ntheir underlying generative distributions (which are unknown) belong to one\ncluster. Distribution distance metrics based k-medoids algorithms are proposed\nfor known and unknown number of distribution clusters. Upper bounds on the\nerror probability and convergence results in the large sample regime are also\nprovided. It is shown that the error probability decays exponentially fast as\nthe number of samples in each data sequence goes to infinity. The error\nexponent has a simple form regardless of the distance metric applied when\ncertain conditions are satisfied. In particular, the error exponent is\ncharacterized when either the Kolmogrov-Smirnov distance or the maximum mean\ndiscrepancy are used as the distance metric. Simulation results are provided to\nvalidate the analysis. \n\n"}
{"id": "1807.11697", "contents": "Title: Multimodal Deep Domain Adaptation Abstract: Typically a classifier trained on a given dataset (source domain) does not\nperforms well if it is tested on data acquired in a different setting (target\ndomain). This is the problem that domain adaptation (DA) tries to overcome and,\nwhile it is a well explored topic in computer vision, it is largely ignored in\nrobotic vision where usually visual classification methods are trained and\ntested in the same domain. Robots should be able to deal with unknown\nenvironments, recognize objects and use them in the correct way, so it is\nimportant to explore the domain adaptation scenario also in this context. The\ngoal of the project is to define a benchmark and a protocol for multi-modal\ndomain adaptation that is valuable for the robot vision community. With this\npurpose some of the state-of-the-art DA methods are selected: Deep Adaptation\nNetwork (DAN), Domain Adversarial Training of Neural Network (DANN), Automatic\nDomain Alignment Layers (AutoDIAL) and Adversarial Discriminative Domain\nAdaptation (ADDA). Evaluations have been done using different data types: RGB\nonly, depth only and RGB-D over the following datasets, designed for the\nrobotic community: RGB-D Object Dataset (ROD), Web Object Dataset (WOD),\nAutonomous Robot Indoor Dataset (ARID), Big Berkeley Instance Recognition\nDataset (BigBIRD) and Active Vision Dataset. Although progresses have been made\non the formulation of effective adaptation algorithms and more realistic object\ndatasets are available, the results obtained show that, training a sufficiently\ngood object classifier, especially in the domain adaptation scenario, is still\nan unsolved problem. Also the best way to combine depth with RGB informations\nto improve the performance is a point that needs to be investigated more. \n\n"}
{"id": "1808.00098", "contents": "Title: Universal Approximation with Quadratic Deep Networks Abstract: Recently, deep learning has achieved huge successes in many important\napplications. In our previous studies, we proposed quadratic/second-order\nneurons and deep quadratic neural networks. In a quadratic neuron, the inner\nproduct of a vector of data and the corresponding weights in a conventional\nneuron is replaced with a quadratic function. The resultant quadratic neuron\nenjoys an enhanced expressive capability over the conventional neuron. However,\nhow quadratic neurons improve the expressing capability of a deep quadratic\nnetwork has not been studied up to now, preferably in relation to that of a\nconventional neural network. Regarding this, we ask four basic questions in\nthis paper: (1) for the one-hidden-layer network structure, is there any\nfunction that a quadratic network can approximate much more efficiently than a\nconventional network? (2) for the same multi-layer network structure, is there\nany function that can be expressed by a quadratic network but cannot be\nexpressed with conventional neurons in the same structure? (3) Does a quadratic\nnetwork give a new insight into universal approximation? (4) To approximate the\nsame class of functions with the same error bound, is a quantized quadratic\nnetwork able to enjoy a lower number of weights than a quantized conventional\nnetwork? Our main contributions are the four interconnected theorems shedding\nlight upon these four questions and demonstrating the merits of a quadratic\nnetwork in terms of expressive efficiency, unique capability, compact\narchitecture and computational capacity respectively. \n\n"}
{"id": "1808.00200", "contents": "Title: Anomaly Detection via Minimum Likelihood Generative Adversarial Networks Abstract: Anomaly detection aims to detect abnormal events by a model of normality. It\nplays an important role in many domains such as network intrusion detection,\ncriminal activity identity and so on. With the rapidly growing size of\naccessible training data and high computation capacities, deep learning based\nanomaly detection has become more and more popular. In this paper, a new\ndomain-based anomaly detection method based on generative adversarial networks\n(GAN) is proposed. Minimum likelihood regularization is proposed to make the\ngenerator produce more anomalies and prevent it from converging to normal data\ndistribution. Proper ensemble of anomaly scores is shown to improve the\nstability of discriminator effectively. The proposed method has achieved\nsignificant improvement than other anomaly detection methods on Cifar10 and UCI\ndatasets. \n\n"}
{"id": "1808.00209", "contents": "Title: Binarized Convolutional Neural Networks for Efficient Inference on GPUs Abstract: Convolutional neural networks have recently achieved significant\nbreakthroughs in various image classification tasks. However, they are\ncomputationally expensive,which can make their feasible mplementation on\nembedded and low-power devices difficult. In this paper convolutional neural\nnetwork binarization is implemented on GPU-based platforms for real-time\ninference on resource constrained devices. In binarized networks, all weights\nand intermediate computations between layers are quantized to +1 and -1,\nallowing multiplications and additions to be replaced with bit-wise operations\nbetween 32-bit words. This representation completely eliminates the need for\nfloating point multiplications and additions and decreases both the\ncomputational load and the memory footprint compared to a full-precision\nnetwork implemented in floating point, making it well-suited for\nresource-constrained environments. We compare the performance of our\nimplementation with an equivalent floating point implementation on one desktop\nand two embedded GPU platforms. Our implementation achieves a maximum speed up\nof 7. 4X with only 4.4% loss in accuracy compared to a reference\nimplementation. \n\n"}
{"id": "1808.00387", "contents": "Title: Just Interpolate: Kernel \"Ridgeless\" Regression Can Generalize Abstract: In the absence of explicit regularization, Kernel \"Ridgeless\" Regression with\nnonlinear kernels has the potential to fit the training data perfectly. It has\nbeen observed empirically, however, that such interpolated solutions can still\ngeneralize well on test data. We isolate a phenomenon of implicit\nregularization for minimum-norm interpolated solutions which is due to a\ncombination of high dimensionality of the input data, curvature of the kernel\nfunction, and favorable geometric properties of the data such as an eigenvalue\ndecay of the empirical covariance and kernel matrices. In addition to deriving\na data-dependent upper bound on the out-of-sample error, we present\nexperimental evidence suggesting that the phenomenon occurs in the MNIST\ndataset. \n\n"}
{"id": "1808.00408", "contents": "Title: Geometry of energy landscapes and the optimizability of deep neural\n  networks Abstract: Deep neural networks are workhorse models in machine learning with multiple\nlayers of non-linear functions composed in series. Their loss function is\nhighly non-convex, yet empirically even gradient descent minimisation is\nsufficient to arrive at accurate and predictive models. It is hitherto unknown\nwhy are deep neural networks easily optimizable. We analyze the energy\nlandscape of a spin glass model of deep neural networks using random matrix\ntheory and algebraic geometry. We analytically show that the multilayered\nstructure holds the key to optimizability: Fixing the number of parameters and\nincreasing network depth, the number of stationary points in the loss function\ndecreases, minima become more clustered in parameter space, and the tradeoff\nbetween the depth and width of minima becomes less severe. Our analytical\nresults are numerically verified through comparison with neural networks\ntrained on a set of classical benchmark datasets. Our model uncovers generic\ndesign principles of machine learning models. \n\n"}
{"id": "1808.00523", "contents": "Title: Mod-DeepESN: Modular Deep Echo State Network Abstract: Neuro-inspired recurrent neural network algorithms, such as echo state\nnetworks, are computationally lightweight and thereby map well onto untethered\ndevices. The baseline echo state network algorithms are shown to be efficient\nin solving small-scale spatio-temporal problems. However, they underperform for\ncomplex tasks that are characterized by multi-scale structures. In this\nresearch, an intrinsic plasticity-infused modular deep echo state network\narchitecture is proposed to solve complex and multiple timescale temporal\ntasks. It outperforms state-of-the-art for time series prediction tasks. \n\n"}
{"id": "1808.00529", "contents": "Title: Open Category Detection with PAC Guarantees Abstract: Open category detection is the problem of detecting \"alien\" test instances\nthat belong to categories or classes that were not present in the training\ndata. In many applications, reliably detecting such aliens is central to\nensuring the safety and accuracy of test set predictions. Unfortunately, there\nare no algorithms that provide theoretical guarantees on their ability to\ndetect aliens under general assumptions. Further, while there are algorithms\nfor open category detection, there are few empirical results that directly\nreport alien detection rates. Thus, there are significant theoretical and\nempirical gaps in our understanding of open category detection. In this paper,\nwe take a step toward addressing this gap by studying a simple, but\npractically-relevant variant of open category detection. In our setting, we are\nprovided with a \"clean\" training set that contains only the target categories\nof interest and an unlabeled \"contaminated\" training set that contains a\nfraction $\\alpha$ of alien examples. Under the assumption that we know an upper\nbound on $\\alpha$, we develop an algorithm with PAC-style guarantees on the\nalien detection rate, while aiming to minimize false alarms. Empirical results\non synthetic and standard benchmark datasets demonstrate the regimes in which\nthe algorithm can be effective and provide a baseline for further advancements. \n\n"}
{"id": "1808.00563", "contents": "Title: Data Augmentation for Robust Keyword Spotting under Playback\n  Interference Abstract: Accurate on-device keyword spotting (KWS) with low false accept and false\nreject rate is crucial to customer experience for far-field voice control of\nconversational agents. It is particularly challenging to maintain low false\nreject rate in real world conditions where there is (a) ambient noise from\nexternal sources such as TV, household appliances, or other speech that is not\ndirected at the device (b) imperfect cancellation of the audio playback from\nthe device, resulting in residual echo, after being processed by the Acoustic\nEcho Cancellation (AEC) system. In this paper, we propose a data augmentation\nstrategy to improve keyword spotting performance under these challenging\nconditions. The training set audio is artificially corrupted by mixing in music\nand TV/movie audio, at different signal to interference ratios. Our results\nshow that we get around 30-45% relative reduction in false reject rates, at a\nrange of false alarm rates, under audio playback from such devices. \n\n"}
{"id": "1808.00590", "contents": "Title: MLCapsule: Guarded Offline Deployment of Machine Learning as a Service Abstract: With the widespread use of machine learning (ML) techniques, ML as a service\nhas become increasingly popular. In this setting, an ML model resides on a\nserver and users can query it with their data via an API. However, if the\nuser's input is sensitive, sending it to the server is undesirable and\nsometimes even legally not possible. Equally, the service provider does not\nwant to share the model by sending it to the client for protecting its\nintellectual property and pay-per-query business model.\n  In this paper, we propose MLCapsule, a guarded offline deployment of machine\nlearning as a service. MLCapsule executes the model locally on the user's side\nand therefore the data never leaves the client. Meanwhile, MLCapsule offers the\nservice provider the same level of control and security of its model as the\ncommonly used server-side execution. In addition, MLCapsule is applicable to\noffline applications that require local execution. Beyond protecting against\ndirect model access, we couple the secure offline deployment with defenses\nagainst advanced attacks on machine learning models such as model stealing,\nreverse engineering, and membership inference. \n\n"}
{"id": "1808.00783", "contents": "Title: The Quest for the Golden Activation Function Abstract: Deep Neural Networks have been shown to be beneficial for a variety of tasks,\nin particular allowing for end-to-end learning and reducing the requirement for\nmanual design decisions. However, still many parameters have to be chosen in\nadvance, also raising the need to optimize them. One important, but often\nignored system parameter is the selection of a proper activation function.\nThus, in this paper we target to demonstrate the importance of activation\nfunctions in general and show that for different tasks different activation\nfunctions might be meaningful. To avoid the manual design or selection of\nactivation functions, we build on the idea of genetic algorithms to learn the\nbest activation function for a given task. In addition, we introduce two new\nactivation functions, ELiSH and HardELiSH, which can easily be incorporated in\nour framework. In this way, we demonstrate for three different image\nclassification benchmarks that different activation functions are learned, also\nshowing improved results compared to typically used baselines. \n\n"}
{"id": "1808.01357", "contents": "Title: A recurrent multi-scale approach to RBG-D Object Recognition Abstract: Technological development aims to produce generations of increasingly\nefficient robots able to perform complex tasks. This requires considerable\nefforts, from the scientific community, to find new algorithms that solve\ncomputer vision problems, such as object recognition. The diffusion of RGB-D\ncameras directed the study towards the research of new architectures able to\nexploit the RGB and Depth information. The project that is developed in this\nthesis concerns the realization of a new end-to-end architecture for the\nrecognition of RGB-D objects called RCFusion. Our method generates compact and\nhighly discriminative multi-modal features by combining complementary RGB and\ndepth information representing different levels of abstraction. We evaluate our\nmethod on standard object recognition datasets, RGB-D Object Dataset and\nJHUIT-50. The experiments performed show that our method outperforms the\nexisting approaches and establishes new state-of-the-art results for both\ndatasets. \n\n"}
{"id": "1808.01630", "contents": "Title: A Review of Learning with Deep Generative Models from Perspective of\n  Graphical Modeling Abstract: This document aims to provide a review on learning with deep generative\nmodels (DGMs), which is an highly-active area in machine learning and more\ngenerally, artificial intelligence. This review is not meant to be a tutorial,\nbut when necessary, we provide self-contained derivations for completeness.\nThis review has two features. First, though there are different perspectives to\nclassify DGMs, we choose to organize this review from the perspective of\ngraphical modeling, because the learning methods for directed DGMs and\nundirected DGMs are fundamentally different. Second, we differentiate model\ndefinitions from model learning algorithms, since different learning algorithms\ncan be applied to solve the learning problem on the same model, and an\nalgorithm can be applied to learn different models. We thus separate model\ndefinition and model learning, with more emphasis on reviewing, differentiating\nand connecting different learning algorithms. We also discuss promising future\nresearch directions. \n\n"}
{"id": "1808.01916", "contents": "Title: Residual Memory Networks: Feed-forward approach to learn long temporal\n  dependencies Abstract: Training deep recurrent neural network (RNN) architectures is complicated due\nto the increased network complexity. This disrupts the learning of higher order\nabstracts using deep RNN. In case of feed-forward networks training deep\nstructures is simple and faster while learning long-term temporal information\nis not possible. In this paper we propose a residual memory neural network\n(RMN) architecture to model short-time dependencies using deep feed-forward\nlayers having residual and time delayed connections. The residual connection\npaves way to construct deeper networks by enabling unhindered flow of gradients\nand the time delay units capture temporal information with shared weights. The\nnumber of layers in RMN signifies both the hierarchical processing depth and\ntemporal depth. The computational complexity in training RMN is significantly\nless when compared to deep recurrent networks. RMN is further extended as\nbi-directional RMN (BRMN) to capture both past and future information.\nExperimental analysis is done on AMI corpus to substantiate the capability of\nRMN in learning long-term information and hierarchical information. Recognition\nperformance of RMN trained with 300 hours of Switchboard corpus is compared\nwith various state-of-the-art LVCSR systems. The results indicate that RMN and\nBRMN gains 6 % and 3.8 % relative improvement over LSTM and BLSTM networks. \n\n"}
{"id": "1808.01975", "contents": "Title: A Survey on Surrogate Approaches to Non-negative Matrix Factorization Abstract: Motivated by applications in hyperspectral imaging we investigate methods for\napproximating a high-dimensional non-negative matrix $\\mathbf{\\mathit{Y}}$ by a\nproduct of two lower-dimensional, non-negative matrices $\\mathbf{\\mathit{K}}$\nand $\\mathbf{\\mathit{X}}.$ This so-called non-negative matrix factorization is\nbased on defining suitable Tikhonov functionals, which combine a discrepancy\nmeasure for $\\mathbf{\\mathit{Y}}\\approx\\mathbf{\\mathit{KX}}$ with penalty terms\nfor enforcing additional properties of $\\mathbf{\\mathit{K}}$ and\n$\\mathbf{\\mathit{X}}$. The minimization is based on alternating minimization\nwith respect to $\\mathbf{\\mathit{K}}$ or $\\mathbf{\\mathit{X}}$, where in each\niteration step one replaces the original Tikhonov functional by a locally\ndefined surrogate functional. The choice of surrogate functionals is crucial:\nIt should allow a comparatively simple minimization and simultaneously its\nfirst order optimality condition should lead to multiplicative update rules,\nwhich automatically preserve non-negativity of the iterates. We review the most\nstandard construction principles for surrogate functionals for Frobenius-norm\nand Kullback-Leibler discrepancy measures. We extend the known surrogate\nconstructions by a general framework, which allows to add a large variety of\npenalty terms. The paper finishes by deriving the corresponding alternating\nminimization schemes explicitely and by applying these methods to MALDI imaging\ndata. \n\n"}
{"id": "1808.02016", "contents": "Title: MCRM: Mother Compact Recurrent Memory Abstract: LSTMs and GRUs are the most common recurrent neural network architectures\nused to solve temporal sequence problems. The two architectures have differing\ndata flows dealing with a common component called the cell state (also referred\nto as the memory). We attempt to enhance the memory by presenting a\nmodification that we call the Mother Compact Recurrent Memory (MCRM). MCRMs are\na type of a nested LSTM-GRU architecture where the cell state is the GRU hidden\nstate. The concatenation of the forget gate and input gate interactions from\nthe LSTM are considered an input to the GRU cell. Because MCRMs has this type\nof nesting, MCRMs have a compact memory pattern consisting of neurons that acts\nexplicitly in both long-term and short-term fashions. For some specific tasks,\nempirical results show that MCRMs outperform previously used architectures. \n\n"}
{"id": "1808.03265", "contents": "Title: A Hybrid Recommender System for Patient-Doctor Matchmaking in Primary\n  Care Abstract: We partner with a leading European healthcare provider and design a mechanism\nto match patients with family doctors in primary care. We define the\nmatchmaking process for several distinct use cases given different levels of\navailable information about patients. Then, we adopt a hybrid recommender\nsystem to present each patient a list of family doctor recommendations. In\nparticular, we model patient trust of family doctors using a large-scale\ndataset of consultation histories, while accounting for the temporal dynamics\nof their relationships. Our proposed approach shows higher predictive accuracy\nthan both a heuristic baseline and a collaborative filtering approach, and the\nproposed trust measure further improves model performance. \n\n"}
{"id": "1808.04260", "contents": "Title: iNNvestigate neural networks! Abstract: In recent years, deep neural networks have revolutionized many application\ndomains of machine learning and are key components of many critical decision or\npredictive processes. Therefore, it is crucial that domain specialists can\nunderstand and analyze actions and pre- dictions, even of the most complex\nneural network architectures. Despite these arguments neural networks are often\ntreated as black boxes. In the attempt to alleviate this short- coming many\nanalysis methods were proposed, yet the lack of reference implementations often\nmakes a systematic comparison between the methods a major effort. The presented\nlibrary iNNvestigate addresses this by providing a common interface and\nout-of-the- box implementation for many analysis methods, including the\nreference implementation for PatternNet and PatternAttribution as well as for\nLRP-methods. To demonstrate the versatility of iNNvestigate, we provide an\nanalysis of image classifications for variety of state-of-the-art neural\nnetwork architectures. \n\n"}
{"id": "1808.04262", "contents": "Title: Connectivity-Driven Brain Parcellation via Consensus Clustering Abstract: We present two related methods for deriving connectivity-based brain atlases\nfrom individual connectomes. The proposed methods exploit a previously proposed\ndense connectivity representation, termed continuous connectivity, by first\nperforming graph-based hierarchical clustering of individual brains, and\nsubsequently aggregating the individual parcellations into a consensus\nparcellation. The search for consensus minimizes the sum of cluster membership\ndistances, effectively estimating a pseudo-Karcher mean of individual\nparcellations. We assess the quality of our parcellations using (1)\nKullback-Liebler and Jensen-Shannon divergence with respect to the dense\nconnectome representation, (2) inter-hemispheric symmetry, and (3) performance\nof the simplified connectome in a biological sex classification task. We find\nthat the parcellation based-atlas computed using a greedy search at a\nhierarchical depth 3 outperforms all other parcellation-based atlases as well\nas the standard Dessikan-Killiany anatomical atlas in all three assessments. \n\n"}
{"id": "1808.04433", "contents": "Title: Out of the Black Box: Properties of deep neural networks and their\n  applications Abstract: Deep neural networks are powerful machine learning approaches that have\nexhibited excellent results on many classification tasks. However, they are\nconsidered as black boxes and some of their properties remain to be formalized.\nIn the context of image recognition, it is still an arduous task to understand\nwhy an image is recognized or not. In this study, we formalize some properties\nshared by eight state-of-the-art deep neural networks in order to grasp the\nprinciples allowing a given deep neural network to classify an image. Our\nresults, tested on these eight networks, show that an image can be sub-divided\ninto several regions (patches) responding at different degrees of probability\n(local property). With the same patch, some locations in the image can answer\ntwo (or three) orders of magnitude higher than other locations (spatial\nproperty). Some locations are activators and others inhibitors\n(activation-inhibition property). The repetition of the same patch can increase\n(or decrease) the probability of recognition of an object (cumulative\nproperty). Furthermore, we propose a new approach called Deepception that\nexploits these properties to deceive a deep neural network. We obtain for the\nVGG-VDD-19 neural network a fooling ratio of 88\\%. Thanks to our\n\"Psychophysics\" approach, no prior knowledge on the networks architectures is\nrequired. \n\n"}
{"id": "1808.04572", "contents": "Title: Small Sample Learning in Big Data Era Abstract: As a promising area in artificial intelligence, a new learning paradigm,\ncalled Small Sample Learning (SSL), has been attracting prominent research\nattention in the recent years. In this paper, we aim to present a survey to\ncomprehensively introduce the current techniques proposed on this topic.\nSpecifically, current SSL techniques can be mainly divided into two categories.\nThe first category of SSL approaches can be called \"concept learning\", which\nemphasizes learning new concepts from only few related observations. The\npurpose is mainly to simulate human learning behaviors like recognition,\ngeneration, imagination, synthesis and analysis. The second category is called\n\"experience learning\", which usually co-exists with the large sample learning\nmanner of conventional machine learning. This category mainly focuses on\nlearning with insufficient samples, and can also be called small data learning\nin some literatures. More extensive surveys on both categories of SSL\ntechniques are introduced and some neuroscience evidences are provided to\nclarify the rationality of the entire SSL regime, and the relationship with\nhuman learning process. Some discussions on the main challenges and possible\nfuture research directions along this line are also presented. \n\n"}
{"id": "1808.04730", "contents": "Title: Analyzing Inverse Problems with Invertible Neural Networks Abstract: In many tasks, in particular in natural science, the goal is to determine\nhidden system parameters from a set of measurements. Often, the forward process\nfrom parameter- to measurement-space is a well-defined function, whereas the\ninverse problem is ambiguous: one measurement may map to multiple different\nsets of parameters. In this setting, the posterior parameter distribution,\nconditioned on an input measurement, has to be determined. We argue that a\nparticular class of neural networks is well suited for this task -- so-called\nInvertible Neural Networks (INNs). Although INNs are not new, they have, so\nfar, received little attention in literature. While classical neural networks\nattempt to solve the ambiguous inverse problem directly, INNs are able to learn\nit jointly with the well-defined forward process, using additional latent\noutput variables to capture the information otherwise lost. Given a specific\nmeasurement and sampled latent variables, the inverse pass of the INN provides\na full distribution over parameter space. We verify experimentally, on\nartificial data and real-world problems from astrophysics and medicine, that\nINNs are a powerful analysis tool to find multi-modalities in parameter space,\nto uncover parameter correlations, and to identify unrecoverable parameters. \n\n"}
{"id": "1808.04873", "contents": "Title: Generalization of Equilibrium Propagation to Vector Field Dynamics Abstract: The biological plausibility of the backpropagation algorithm has long been\ndoubted by neuroscientists. Two major reasons are that neurons would need to\nsend two different types of signal in the forward and backward phases, and that\npairs of neurons would need to communicate through symmetric bidirectional\nconnections. We present a simple two-phase learning procedure for fixed point\nrecurrent networks that addresses both these issues. In our model, neurons\nperform leaky integration and synaptic weights are updated through a local\nmechanism. Our learning method generalizes Equilibrium Propagation to vector\nfield dynamics, relaxing the requirement of an energy function. As a\nconsequence of this generalization, the algorithm does not compute the true\ngradient of the objective function, but rather approximates it at a precision\nwhich is proven to be directly related to the degree of symmetry of the\nfeedforward and feedback weights. We show experimentally that our algorithm\noptimizes the objective function. \n\n"}
{"id": "1808.05697", "contents": "Title: Deep Bayesian Active Learning for Natural Language Processing: Results\n  of a Large-Scale Empirical Study Abstract: Several recent papers investigate Active Learning (AL) for mitigating the\ndata dependence of deep learning for natural language processing. However, the\napplicability of AL to real-world problems remains an open question. While in\nsupervised learning, practitioners can try many different methods, evaluating\neach against a validation set before selecting a model, AL affords no such\nluxury. Over the course of one AL run, an agent annotates its dataset\nexhausting its labeling budget. Thus, given a new task, an active learner has\nno opportunity to compare models and acquisition functions. This paper provides\na large scale empirical study of deep active learning, addressing multiple\ntasks and, for each, multiple datasets, multiple models, and a full suite of\nacquisition functions. We find that across all settings, Bayesian active\nlearning by disagreement, using uncertainty estimates provided either by\nDropout or Bayes-by Backprop significantly improves over i.i.d. baselines and\nusually outperforms classic uncertainty sampling. \n\n"}
{"id": "1808.06107", "contents": "Title: Exact Passive-Aggressive Algorithms for Learning to Rank Using Interval\n  Labels Abstract: In this paper, we propose exact passive-aggressive (PA) online algorithms for\nlearning to rank. The proposed algorithms can be used even when we have\ninterval labels instead of actual labels for examples. The proposed algorithms\nsolve a convex optimization problem at every trial. We find exact solution to\nthose optimization problems to determine the updated parameters. We propose\nsupport class algorithm (SCA) which finds the active constraints using the KKT\nconditions of the optimization problems. These active constrains form support\nset which determines the set of thresholds that need to be updated. We derive\nupdate rules for PA, PA-I and PA-II. We show that the proposed algorithms\nmaintain the ordering of the thresholds after every trial. We provide the\nmistake bounds of the proposed algorithms in both ideal and general settings.\nWe also show experimentally that the proposed algorithms successfully learn\naccurate classifiers using interval labels as well as exact labels. Proposed\nalgorithms also do well compared to other approaches. \n\n"}
{"id": "1808.06537", "contents": "Title: Ricean K-factor Estimation based on Channel Quality Indicator in OFDM\n  Systems using Neural Network Abstract: Ricean channel model is widely used in wireless communications to\ncharacterize the channels with a line-of-sight path. The Ricean K factor,\ndefined as the ratio of direct path and scattered paths, provides a good\nindication of the link quality. Most existing works estimate K factor based on\neither maximum-likelihood criterion or higher-order moments, and the existing\nworks are targeted at K-factor estimation at receiver side. In this work, a\nnovel approach is proposed. Cast as a classification problem, the estimation of\nK factor by neural network provides high accuracy. Moreover, the proposed\nK-factor estimation is done at transmitter side for transmit processing, thus\nsaving the limited feedback bandwidth. \n\n"}
{"id": "1808.06581", "contents": "Title: The Deconfounded Recommender: A Causal Inference Approach to\n  Recommendation Abstract: The goal of recommendation is to show users items that they will like. Though\nusually framed as a prediction, the spirit of recommendation is to answer an\ninterventional question---for each user and movie, what would the rating be if\nwe \"forced\" the user to watch the movie? To this end, we develop a causal\napproach to recommendation, one where watching a movie is a \"treatment\" and a\nuser's rating is an \"outcome.\" The problem is there may be unobserved\nconfounders, variables that affect both which movies the users watch and how\nthey rate them; unobserved confounders impede causal predictions with\nobservational data. To solve this problem, we develop the deconfounded\nrecommender, a way to use classical recommendation models for causal\nrecommendation. Following Wang & Blei [23], the deconfounded recommender\ninvolves two probabilistic models. The first models which movies the users\nwatch; it provides a substitute for the unobserved confounders. The second one\nmodels how each user rates each movie; it employs the substitute to help\naccount for confounders. This two-stage approach removes bias due to\nconfounding. It improves recommendation and enjoys stable performance against\ninterventions on test sets. \n\n"}
{"id": "1808.06640", "contents": "Title: Adversarial Removal of Demographic Attributes from Text Data Abstract: Recent advances in Representation Learning and Adversarial Training seem to\nsucceed in removing unwanted features from the learned representation. We show\nthat demographic information of authors is encoded in -- and can be recovered\nfrom -- the intermediate representations learned by text-based neural\nclassifiers. The implication is that decisions of classifiers trained on\ntextual data are not agnostic to -- and likely condition on -- demographic\nattributes. When attempting to remove such demographic information using\nadversarial training, we find that while the adversarial component achieves\nchance-level development-set accuracy during training, a post-hoc classifier,\ntrained on the encoded sentences from the first part, still manages to reach\nsubstantially higher classification accuracies on the same data. This behavior\nis consistent across several tasks, demographic properties and datasets. We\nexplore several techniques to improve the effectiveness of the adversarial\ncomponent. Our main conclusion is a cautionary one: do not rely on the\nadversarial training to achieve invariant representation to sensitive features. \n\n"}
{"id": "1808.06865", "contents": "Title: Machine Learning for Spatiotemporal Sequence Forecasting: A Survey Abstract: Spatiotemporal systems are common in the real-world. Forecasting the\nmulti-step future of these spatiotemporal systems based on the past\nobservations, or, Spatiotemporal Sequence Forecasting (STSF), is a significant\nand challenging problem. Although lots of real-world problems can be viewed as\nSTSF and many research works have proposed machine learning based methods for\nthem, no existing work has summarized and compared these methods from a unified\nperspective. This survey aims to provide a systematic review of machine\nlearning for STSF. In this survey, we define the STSF problem and classify it\ninto three subcategories: Trajectory Forecasting of Moving Point Cloud\n(TF-MPC), STSF on Regular Grid (STSF-RG) and STSF on Irregular Grid (STSF-IG).\nWe then introduce the two major challenges of STSF: 1) how to learn a model for\nmulti-step forecasting and 2) how to adequately model the spatial and temporal\nstructures. After that, we review the existing works for solving these\nchallenges, including the general learning strategies for multi-step\nforecasting, the classical machine learning based methods for STSF, and the\ndeep learning based methods for STSF. We also compare these methods and point\nout some potential research directions. \n\n"}
{"id": "1808.06934", "contents": "Title: Backpropagation and Biological Plausibility Abstract: By and large, Backpropagation (BP) is regarded as one of the most important\nneural computation algorithms at the basis of the progress in machine learning,\nincluding the recent advances in deep learning. However, its computational\nstructure has been the source of many debates on its arguable biological\nplausibility. In this paper, it is shown that when framing supervised learning\nin the Lagrangian framework, while one can see a natural emergence of\nBackpropagation, biologically plausible local algorithms can also be devised\nthat are based on the search for saddle points in the learning adjoint space\ncomposed of weights, neural outputs, and Lagrangian multipliers. This might\nopen the doors to a truly novel class of learning algorithms where, because of\nthe introduction of the notion of support neurons, the optimization scheme also\nplays a fundamental role in the construction of the architecture. \n\n"}
{"id": "1808.07169", "contents": "Title: Statistical Neurodynamics of Deep Networks: Geometry of Signal Spaces Abstract: Statistical neurodynamics studies macroscopic behaviors of randomly connected\nneural networks. We consider a deep layered feedforward network where input\nsignals are processed layer by layer. The manifold of input signals is embedded\nin a higher dimensional manifold of the next layer as a curved submanifold,\nprovided the number of neurons is larger than that of inputs. We show\ngeometrical features of the embedded manifold, proving that the manifold\nenlarges or shrinks locally isotropically so that it is always embedded\nconformally. We study the curvature of the embedded manifold. The scalar\ncurvature converges to a constant or diverges to infinity slowly. The distance\nbetween two signals also changes, converging eventually to a stable fixed\nvalue, provided both the number of neurons in a layer and the number of layers\ntend to infinity. This causes a problem, since when we consider a curve in the\ninput space, it is mapped as a continuous curve of fractal nature, but our\ntheory contradictorily suggests that the curve eventually converges to a\ndiscrete set of equally spaced points. In reality, the numbers of neurons and\nlayers are finite and thus, it is expected that the finite size effect causes\nthe discrepancies between our theory and reality. We need to further study the\ndiscrepancies to understand their implications on information processing. \n\n"}
{"id": "1808.07288", "contents": "Title: Clustering and Labelling Auction Fraud Data Abstract: Although shill bidding is a common auction fraud, it is however very tough to\ndetect. Due to the unavailability and lack of training data, in this study, we\nbuild a high-quality labeled shill bidding dataset based on recently collected\nauctions from eBay. Labeling shill biding instances with multidimensional\nfeatures is a critical phase for the fraud classification task. For this\npurpose, we introduce a new approach to systematically label the fraud data\nwith the help of the hierarchical clustering CURE that returns remarkable\nresults as illustrated in the experiments. \n\n"}
{"id": "1808.07382", "contents": "Title: Convergence of Cubic Regularization for Nonconvex Optimization under KL\n  Property Abstract: Cubic-regularized Newton's method (CR) is a popular algorithm that guarantees\nto produce a second-order stationary solution for solving nonconvex\noptimization problems. However, existing understandings of the convergence rate\nof CR are conditioned on special types of geometrical properties of the\nobjective function. In this paper, we explore the asymptotic convergence rate\nof CR by exploiting the ubiquitous Kurdyka-Lojasiewicz (KL) property of\nnonconvex objective functions. In specific, we characterize the asymptotic\nconvergence rate of various types of optimality measures for CR including\nfunction value gap, variable distance gap, gradient norm and least eigenvalue\nof the Hessian matrix. Our results fully characterize the diverse convergence\nbehaviors of these optimality measures in the full parameter regime of the KL\nproperty. Moreover, we show that the obtained asymptotic convergence rates of\nCR are order-wise faster than those of first-order gradient descent algorithms\nunder the KL property. \n\n"}
{"id": "1808.07945", "contents": "Title: Maximal Jacobian-based Saliency Map Attack Abstract: The Jacobian-based Saliency Map Attack is a family of adversarial attack\nmethods for fooling classification models, such as deep neural networks for\nimage classification tasks. By saturating a few pixels in a given image to\ntheir maximum or minimum values, JSMA can cause the model to misclassify the\nresulting adversarial image as a specified erroneous target class. We propose\ntwo variants of JSMA, one which removes the requirement to specify a target\nclass, and another that additionally does not need to specify whether to only\nincrease or decrease pixel intensities. Our experiments highlight the\ncompetitive speeds and qualities of these variants when applied to datasets of\nhand-written digits and natural scenes. \n\n"}
{"id": "1808.08095", "contents": "Title: Multi-scenario deep learning for multi-speaker source separation Abstract: Research in deep learning for multi-speaker source separation has received a\nboost in the last years. However, most studies are restricted to mixtures of a\nspecific number of speakers, called a specific scenario. While some works\nincluded experiments for different scenarios, research towards combining data\nof different scenarios or creating a single model for multiple scenarios have\nbeen very rare. In this work it is shown that data of a specific scenario is\nrelevant for solving another scenario. Furthermore, it is concluded that a\nsingle model, trained on different scenarios is capable of matching performance\nof scenario specific models. \n\n"}
{"id": "1808.09270", "contents": "Title: Models for Predicting Community-Specific Interest in News Articles Abstract: In this work, we ask two questions: 1. Can we predict the type of community\ninterested in a news article using only features from the article content? and\n2. How well do these models generalize over time? To answer these questions, we\ncompute well-studied content-based features on over 60K news articles from 4\ncommunities on reddit.com. We train and test models over three different time\nperiods between 2015 and 2017 to demonstrate which features degrade in\nperformance the most due to concept drift. Our models can classify news\narticles into communities with high accuracy, ranging from 0.81 ROC AUC to 1.0\nROC AUC. However, while we can predict the community-specific popularity of\nnews articles with high accuracy, practitioners should approach these models\ncarefully. Predictions are both community-pair dependent and feature group\ndependent. Moreover, these feature groups generalize over time differently,\nwith some only degrading slightly over time, but others degrading greatly.\nTherefore, we recommend that community-interest predictions are done in a\nhierarchical structure, where multiple binary classifiers can be used to\nseparate community pairs, rather than a traditional multi-class model. Second,\nthese models should be retrained over time based on accuracy goals and the\navailability of training data. \n\n"}
{"id": "1808.09830", "contents": "Title: Searching Toward Pareto-Optimal Device-Aware Neural Architectures Abstract: Recent breakthroughs in Neural Architectural Search (NAS) have achieved\nstate-of-the-art performance in many tasks such as image classification and\nlanguage understanding. However, most existing works only optimize for model\naccuracy and largely ignore other important factors imposed by the underlying\nhardware and devices, such as latency and energy, when making inference. In\nthis paper, we first introduce the problem of NAS and provide a survey on\nrecent works. Then we deep dive into two recent advancements on extending NAS\ninto multiple-objective frameworks: MONAS and DPP-Net. Both MONAS and DPP-Net\nare capable of optimizing accuracy and other objectives imposed by devices,\nsearching for neural architectures that can be best deployed on a wide spectrum\nof devices: from embedded systems and mobile devices to workstations.\nExperimental results are poised to show that architectures found by MONAS and\nDPP-Net achieves Pareto optimality w.r.t the given objectives for various\ndevices. \n\n"}
{"id": "1808.09889", "contents": "Title: Zero-shot Transfer Learning for Semantic Parsing Abstract: While neural networks have shown impressive performance on large datasets,\napplying these models to tasks where little data is available remains a\nchallenging problem.\n  In this paper we propose to use feature transfer in a zero-shot experimental\nsetting on the task of semantic parsing.\n  We first introduce a new method for learning the shared space between\nmultiple domains based on the prediction of the domain label for each example.\n  Our experiments support the superiority of this method in a zero-shot\nexperimental setting in terms of accuracy metrics compared to state-of-the-art\ntechniques.\n  In the second part of this paper we study the impact of individual domains\nand examples on semantic parsing performance.\n  We use influence functions to this aim and investigate the sensitivity of\ndomain-label classification loss on each example.\n  Our findings reveal that cross-domain adversarial attacks identify useful\nexamples for training even from the domains the least similar to the target\ndomain. Augmenting our training data with these influential examples further\nboosts our accuracy at both the token and the sequence level. \n\n"}
{"id": "1808.10013", "contents": "Title: The implicit fairness criterion of unconstrained learning Abstract: We clarify what fairness guarantees we can and cannot expect to follow from\nunconstrained machine learning. Specifically, we characterize when\nunconstrained learning on its own implies group calibration, that is, the\noutcome variable is conditionally independent of group membership given the\nscore. We show that under reasonable conditions, the deviation from satisfying\ngroup calibration is upper bounded by the excess risk of the learned score\nrelative to the Bayes optimal score function. A lower bound confirms the\noptimality of our upper bound. Moreover, we prove that as the excess risk of\nthe learned score decreases, it strongly violates separation and independence,\ntwo other standard fairness criteria.\n  Our results show that group calibration is the fairness criterion that\nunconstrained learning implicitly favors. On the one hand, this means that\ncalibration is often satisfied on its own without the need for active\nintervention, albeit at the cost of violating other criteria that are at odds\nwith calibration. On the other hand, it suggests that we should be satisfied\nwith calibration as a fairness criterion only if we are at ease with the use of\nunconstrained machine learning in a given application. \n\n"}
{"id": "1808.10261", "contents": "Title: Centroid estimation based on symmetric KL divergence for Multinomial\n  text classification problem Abstract: We define a new method to estimate centroid for text classification based on\nthe symmetric KL-divergence between the distribution of words in training\ndocuments and their class centroids. Experiments on several standard data sets\nindicate that the new method achieves substantial improvements over the\ntraditional classifiers. \n\n"}
{"id": "1808.10867", "contents": "Title: Tensor Embedding: A Supervised Framework for Human Behavioral Data\n  Mining and Prediction Abstract: Today's densely instrumented world offers tremendous opportunities for\ncontinuous acquisition and analysis of multimodal sensor data providing\ntemporal characterization of an individual's behaviors. Is it possible to\nefficiently couple such rich sensor data with predictive modeling techniques to\nprovide contextual, and insightful assessments of individual performance and\nwellbeing? Prediction of different aspects of human behavior from these noisy,\nincomplete, and heterogeneous bio-behavioral temporal data is a challenging\nproblem, beyond unsupervised discovery of latent structures. We propose a\nSupervised Tensor Embedding (STE) algorithm for high dimension multimodal data\nwith join decomposition of input and target variable. Furthermore, we show that\nfeatures selection will help to reduce the contamination in the prediction and\nincrease the performance. The efficiently of the methods was tested via two\ndifferent real world datasets. \n\n"}
{"id": "1809.00862", "contents": "Title: Handwriting styles: benchmarks and evaluation metrics Abstract: Evaluating the style of handwriting generation is a challenging problem,\nsince it is not well defined. It is a key component in order to develop in\ndeveloping systems with more personalized experiences with humans. In this\npaper, we propose baseline benchmarks, in order to set anchors to estimate the\nrelative quality of different handwriting style methods. This will be done\nusing deep learning techniques, which have shown remarkable results in\ndifferent machine learning tasks, learning classification, regression, and most\nrelevant to our work, generating temporal sequences. We discuss the challenges\nassociated with evaluating our methods, which is related to evaluation of\ngenerative models in general. We then propose evaluation metrics, which we find\nrelevant to this problem, and we discuss how we evaluate the evaluation\nmetrics. In this study, we use IRON-OFF dataset. To the best of our knowledge,\nthere is no work done before in generating handwriting (either in terms of\nmethodology or the performance metrics), our in exploring styles using this\ndataset. \n\n"}
{"id": "1809.00946", "contents": "Title: Twin-GAN -- Unpaired Cross-Domain Image Translation with Weight-Sharing\n  GANs Abstract: We present a framework for translating unlabeled images from one domain into\nanalog images in another domain. We employ a progressively growing\nskip-connected encoder-generator structure and train it with a GAN loss for\nrealistic output, a cycle consistency loss for maintaining same-domain\ntranslation identity, and a semantic consistency loss that encourages the\nnetwork to keep the input semantic features in the output. We apply our\nframework on the task of translating face images, and show that it is capable\nof learning semantic mappings for face images with no supervised one-to-one\nimage mapping. \n\n"}
{"id": "1809.01302", "contents": "Title: Magic-State Functional Units: Mapping and Scheduling Multi-Level\n  Distillation Circuits for Fault-Tolerant Quantum Architectures Abstract: Quantum computers have recently made great strides and are on a long-term\npath towards useful fault-tolerant computation. A dominant overhead in\nfault-tolerant quantum computation is the production of high-fidelity encoded\nqubits, called magic states, which enable reliable error-corrected computation.\nWe present the first detailed designs of hardware functional units that\nimplement space-time optimized magic-state factories for surface code\nerror-corrected machines. Interactions among distant qubits require surface\ncode braids (physical pathways on chip) which must be routed. Magic-state\nfactories are circuits comprised of a complex set of braids that is more\ndifficult to route than quantum circuits considered in previous work [1]. This\npaper explores the impact of scheduling techniques, such as gate reordering and\nqubit renaming, and we propose two novel mapping techniques: braid repulsion\nand dipole moment braid rotation. We combine these techniques with graph\npartitioning and community detection algorithms, and further introduce a\nstitching algorithm for mapping subgraphs onto a physical machine. Our results\nshow a factor of 5.64 reduction in space-time volume compared to the best-known\nprevious designs for magic-state factories. \n\n"}
{"id": "1809.01465", "contents": "Title: Deep Bilevel Learning Abstract: We present a novel regularization approach to train neural networks that\nenjoys better generalization and test error than standard stochastic gradient\ndescent. Our approach is based on the principles of cross-validation, where a\nvalidation set is used to limit the model overfitting. We formulate such\nprinciples as a bilevel optimization problem. This formulation allows us to\ndefine the optimization of a cost on the validation set subject to another\noptimization on the training set. The overfitting is controlled by introducing\nweights on each mini-batch in the training set and by choosing their values so\nthat they minimize the error on the validation set. In practice, these weights\ndefine mini-batch learning rates in a gradient descent update equation that\nfavor gradients with better generalization capabilities. Because of its\nsimplicity, this approach can be integrated with other regularization methods\nand training schemes. We evaluate extensively our proposed algorithm on several\nneural network architectures and datasets, and find that it consistently\nimproves the generalization of the model, especially when labels are noisy. \n\n"}
{"id": "1809.01495", "contents": "Title: A Reinforcement Learning-driven Translation Model for Search-Oriented\n  Conversational Systems Abstract: Search-oriented conversational systems rely on information needs expressed in\nnatural language (NL). We focus here on the understanding of NL expressions for\nbuilding keyword-based queries. We propose a reinforcement-learning-driven\ntranslation model framework able to 1) learn the translation from NL\nexpressions to queries in a supervised way, and, 2) to overcome the lack of\nlarge-scale dataset by framing the translation model as a word selection\napproach and injecting relevance feedback in the learning process. Experiments\nare carried out on two TREC datasets and outline the effectiveness of our\napproach. \n\n"}
{"id": "1809.01498", "contents": "Title: Skip-gram word embeddings in hyperbolic space Abstract: Recent work has demonstrated that embeddings of tree-like graphs in\nhyperbolic space surpass their Euclidean counterparts in performance by a large\nmargin. Inspired by these results and scale-free structure in the word\nco-occurrence graph, we present an algorithm for learning word embeddings in\nhyperbolic space from free text. An objective function based on the hyperbolic\ndistance is derived and included in the skip-gram negative-sampling\narchitecture of word2vec. The hyperbolic word embeddings are then evaluated on\nword similarity and analogy benchmarks. The results demonstrate the potential\nof hyperbolic word embeddings, particularly in low dimensions, though without\nclear superiority over their Euclidean counterparts. We further discuss\nsubtleties in the formulation of the analogy task in curved spaces. \n\n"}
{"id": "1809.01635", "contents": "Title: A Differentially Private Wilcoxon Signed-Rank Test Abstract: Hypothesis tests are a crucial statistical tool for data mining and are the\nworkhorse of scientific research in many fields. Here we present a\ndifferentially private analogue of the classic Wilcoxon signed-rank hypothesis\ntest, which is used when comparing sets of paired (e.g., before-and-after) data\nvalues. We present not only a private estimate of the test statistic, but a\nmethod to accurately compute a p-value and assess statistical significance. We\nevaluate our test on both simulated and real data. Compared to the only\nexisting private test for this situation, that of Task and Clifton, we find\nthat our test requires less than half as much data to achieve the same\nstatistical power. \n\n"}
{"id": "1809.01852", "contents": "Title: GAMENet: Graph Augmented MEmory Networks for Recommending Medication\n  Combination Abstract: Recent progress in deep learning is revolutionizing the healthcare domain\nincluding providing solutions to medication recommendations, especially\nrecommending medication combination for patients with complex health\nconditions. Existing approaches either do not customize based on patient health\nhistory, or ignore existing knowledge on drug-drug interactions (DDI) that\nmight lead to adverse outcomes. To fill this gap, we propose the Graph\nAugmented Memory Networks (GAMENet), which integrates the drug-drug\ninteractions knowledge graph by a memory module implemented as a graph\nconvolutional networks, and models longitudinal patient records as the query.\nIt is trained end-to-end to provide safe and personalized recommendation of\nmedication combination. We demonstrate the effectiveness and safety of GAMENet\nby comparing with several state-of-the-art methods on real EHR data. GAMENet\noutperformed all baselines in all effectiveness measures, and also achieved\n3.60% DDI rate reduction from existing EHR data. \n\n"}
{"id": "1809.02066", "contents": "Title: Two Dimensional Stochastic Configuration Networks for Image Data\n  Analytics Abstract: Stochastic configuration networks (SCNs) as a class of randomized learner\nmodel have been successfully employed in data analytics due to its universal\napproximation capability and fast modelling property. The technical essence\nlies in stochastically configuring hidden nodes (or basis functions) based on a\nsupervisory mechanism rather than data-independent randomization as usually\nadopted for building randomized neural networks. Given image data modelling\ntasks, the use of one-dimensional SCNs potentially demolishes the spatial\ninformation of images, and may result in undesirable performance. This paper\nextends the original SCNs to two-dimensional version, termed 2DSCNs, for fast\nbuilding randomized learners with matrix-inputs. Some theoretical analyses on\nthe goodness of 2DSCNs against SCNs, including the complexity of the random\nparameter space, and the superiority of generalization, are presented.\nEmpirical results over one regression, four benchmark handwritten digits\nclassification, and two human face recognition datasets demonstrate that the\nproposed 2DSCNs perform favourably and show good potential for image data\nanalytics. \n\n"}
{"id": "1809.02121", "contents": "Title: Learn What Not to Learn: Action Elimination with Deep Reinforcement\n  Learning Abstract: Learning how to act when there are many available actions in each state is a\nchallenging task for Reinforcement Learning (RL) agents, especially when many\nof the actions are redundant or irrelevant. In such cases, it is sometimes\neasier to learn which actions not to take. In this work, we propose the\nAction-Elimination Deep Q-Network (AE-DQN) architecture that combines a Deep RL\nalgorithm with an Action Elimination Network (AEN) that eliminates sub-optimal\nactions. The AEN is trained to predict invalid actions, supervised by an\nexternal elimination signal provided by the environment. Simulations\ndemonstrate a considerable speedup and added robustness over vanilla DQN in\ntext-based games with over a thousand discrete actions. \n\n"}
{"id": "1809.02322", "contents": "Title: Beyond Gradient Descent for Regularized Segmentation Losses Abstract: The simplicity of gradient descent (GD) made it the default method for\ntraining ever-deeper and complex neural networks. Both loss functions and\narchitectures are often explicitly tuned to be amenable to this basic local\noptimization. In the context of weakly-supervised CNN segmentation, we\ndemonstrate a well-motivated loss function where an alternative optimizer (ADM)\nachieves the state-of-the-art while GD performs poorly. Interestingly, GD\nobtains its best result for a \"smoother\" tuning of the loss function. The\nresults are consistent across different network architectures. Our loss is\nmotivated by well-understood MRF/CRF regularization models in \"shallow\"\nsegmentation and their known global solvers. Our work suggests that network\ndesign/training should pay more attention to optimization methods. \n\n"}
{"id": "1809.02337", "contents": "Title: Information-Theoretic Active Learning for Content-Based Image Retrieval Abstract: We propose Information-Theoretic Active Learning (ITAL), a novel batch-mode\nactive learning method for binary classification, and apply it for acquiring\nmeaningful user feedback in the context of content-based image retrieval.\nInstead of combining different heuristics such as uncertainty, diversity, or\ndensity, our method is based on maximizing the mutual information between the\npredicted relevance of the images and the expected user feedback regarding the\nselected batch. We propose suitable approximations to this computationally\ndemanding problem and also integrate an explicit model of user behavior that\naccounts for possible incorrect labels and unnameable instances. Furthermore,\nour approach does not only take the structure of the data but also the expected\nmodel output change caused by the user feedback into account. In contrast to\nother methods, ITAL turns out to be highly flexible and provides\nstate-of-the-art performance across various datasets, such as MIRFLICKR and\nImageNet. \n\n"}
{"id": "1809.02497", "contents": "Title: Sparse Kernel PCA for Outlier Detection Abstract: In this paper, we propose a new method to perform Sparse Kernel Principal\nComponent Analysis (SKPCA) and also mathematically analyze the validity of\nSKPCA. We formulate SKPCA as a constrained optimization problem with elastic\nnet regularization (Hastie et al.) in kernel feature space and solve it. We\nconsider outlier detection (where KPCA is employed) as an application for\nSKPCA, using the RBF kernel. We test it on 5 real-world datasets and show that\nby using just 4% (or even less) of the principal components (PCs), where each\nPC has on average less than 12% non-zero elements in the worst case among all 5\ndatasets, we are able to nearly match and in 3 datasets even outperform KPCA.\nWe also compare the performance of our method with a recently proposed method\nfor SKPCA by Wang et al. and show that our method performs better in terms of\nboth accuracy and sparsity. We also provide a novel probabilistic proof to\njustify the existence of sparse solutions for KPCA using the RBF kernel. To the\nbest of our knowledge, this is the first attempt at theoretically analyzing the\nvalidity of SKPCA. \n\n"}
{"id": "1809.02728", "contents": "Title: Coupled IGMM-GANs for deep multimodal anomaly detection in human\n  mobility data Abstract: Detecting anomalous activity in human mobility data has a number of\napplications including road hazard sensing, telematic based insurance, and\nfraud detection in taxi services and ride sharing. In this paper we address two\nchallenges that arise in the study of anomalous human trajectories: 1) a lack\nof ground truth data on what defines an anomaly and 2) the dependence of\nexisting methods on significant pre-processing and feature engineering. While\ngenerative adversarial networks seem like a natural fit for addressing these\nchallenges, we find that existing GAN based anomaly detection algorithms\nperform poorly due to their inability to handle multimodal patterns. For this\npurpose we introduce an infinite Gaussian mixture model coupled with\n(bi-directional) generative adversarial networks, IGMM-GAN, that is able to\ngenerate synthetic, yet realistic, human mobility data and simultaneously\nfacilitates multimodal anomaly detection. Through estimation of a generative\nprobability density on the space of human trajectories, we are able to generate\nrealistic synthetic datasets that can be used to benchmark existing anomaly\ndetection methods. The estimated multimodal density also allows for a natural\ndefinition of outlier that we use for detecting anomalous trajectories. We\nillustrate our methodology and its improvement over existing GAN anomaly\ndetection on several human mobility datasets, along with MNIST. \n\n"}
{"id": "1809.02840", "contents": "Title: Neural Guided Constraint Logic Programming for Program Synthesis Abstract: Synthesizing programs using example input/outputs is a classic problem in\nartificial intelligence. We present a method for solving Programming By Example\n(PBE) problems by using a neural model to guide the search of a constraint\nlogic programming system called miniKanren. Crucially, the neural model uses\nminiKanren's internal representation as input; miniKanren represents a PBE\nproblem as recursive constraints imposed by the provided examples. We explore\nRecurrent Neural Network and Graph Neural Network models. We contribute a\nmodified miniKanren, drivable by an external agent, available at\nhttps://github.com/xuexue/neuralkanren. We show that our neural-guided approach\nusing constraints can synthesize programs faster in many cases, and\nimportantly, can generalize to larger problems. \n\n"}
{"id": "1809.02925", "contents": "Title: Discriminator-Actor-Critic: Addressing Sample Inefficiency and Reward\n  Bias in Adversarial Imitation Learning Abstract: We identify two issues with the family of algorithms based on the Adversarial\nImitation Learning framework. The first problem is implicit bias present in the\nreward functions used in these algorithms. While these biases might work well\nfor some environments, they can also lead to sub-optimal behavior in others.\nSecondly, even though these algorithms can learn from few expert\ndemonstrations, they require a prohibitively large number of interactions with\nthe environment in order to imitate the expert for many real-world\napplications. In order to address these issues, we propose a new algorithm\ncalled Discriminator-Actor-Critic that uses off-policy Reinforcement Learning\nto reduce policy-environment interaction sample complexity by an average factor\nof 10. Furthermore, since our reward function is designed to be unbiased, we\ncan apply our algorithm to many problems without making any task-specific\nadjustments. \n\n"}
{"id": "1809.03316", "contents": "Title: Hierarchical Video Understanding Abstract: We introduce a hierarchical architecture for video understanding that\nexploits the structure of real world actions by capturing targets at different\nlevels of granularity. We design the model such that it first learns simpler\ncoarse-grained tasks, and then moves on to learn more fine-grained targets. The\nmodel is trained with a joint loss on different granularity levels. We\ndemonstrate empirical results on the recent release of Something-Something\ndataset, which provides a hierarchy of targets, namely coarse-grained action\ngroups, fine-grained action categories, and captions. Experiments suggest that\nmodels that exploit targets at different levels of granularity achieve better\nperformance on all levels. \n\n"}
{"id": "1809.03322", "contents": "Title: Guiding the Creation of Deep Learning-based Object Detectors Abstract: Object detection is a computer vision field that has applications in several\ncontexts ranging from biomedicine and agriculture to security. In the last\nyears, several deep learning techniques have greatly improved object detection\nmodels. Among those techniques, we can highlight the YOLO approach, that allows\nthe construction of accurate models that can be employed in real-time\napplications. However, as most deep learning techniques, YOLO has a steep\nlearning curve and creating models using this approach might be challenging for\nnon-expert users. In this work, we tackle this problem by constructing a suite\nof Jupyter notebooks that democratizes the construction of object detection\nmodels using YOLO. The suitability of our approach has been proven with a\ndataset of stomata images where we have achieved a mAP of 90.91%. \n\n"}
{"id": "1809.03452", "contents": "Title: Qiskit Backend Specifications for OpenQASM and OpenPulse Experiments Abstract: As interest in quantum computing grows, there is a pressing need for\nstandardized API's so that algorithm designers, circuit designers, and\nphysicists can be provided a common reference frame for designing, executing,\nand optimizing experiments. There is also a need for a language specification\nthat goes beyond gates and allows users to specify the time dynamics of a\nquantum experiment and recover the time dynamics of the output. In this\ndocument we provide a specification for a common interface to backends\n(simulators and experiments) and a standarized data structure (Qobj --- quantum\nobject) for sending experiments to those backends via Qiskit. We also introduce\nOpenPulse, a language for specifying pulse level control (i.e. control of the\ncontinuous time dynamics) of a general quantum device independent of the\nspecific hardware implementation. \n\n"}
{"id": "1809.03548", "contents": "Title: VPE: Variational Policy Embedding for Transfer Reinforcement Learning Abstract: Reinforcement Learning methods are capable of solving complex problems, but\nresulting policies might perform poorly in environments that are even slightly\ndifferent. In robotics especially, training and deployment conditions often\nvary and data collection is expensive, making retraining undesirable.\nSimulation training allows for feasible training times, but on the other hand\nsuffers from a reality-gap when applied in real-world settings. This raises the\nneed of efficient adaptation of policies acting in new environments. We\nconsider this as a problem of transferring knowledge within a family of similar\nMarkov decision processes.\n  For this purpose we assume that Q-functions are generated by some\nlow-dimensional latent variable. Given such a Q-function, we can find a master\npolicy that can adapt given different values of this latent variable. Our\nmethod learns both the generative mapping and an approximate posterior of the\nlatent variables, enabling identification of policies for new tasks by\nsearching only in the latent space, rather than the space of all policies. The\nlow-dimensional space, and master policy found by our method enables policies\nto quickly adapt to new environments. We demonstrate the method on both a\npendulum swing-up task in simulation, and for simulation-to-real transfer on a\npushing task. \n\n"}
{"id": "1809.03779", "contents": "Title: Probabilistic approach to limited-data computed tomography\n  reconstruction Abstract: In this work, we consider the inverse problem of reconstructing the internal\nstructure of an object from limited x-ray projections. We use a Gaussian\nprocess prior to model the target function and estimate its (hyper)parameters\nfrom measured data. In contrast to other established methods, this comes with\nthe advantage of not requiring any manual parameter tuning, which usually\narises in classical regularization strategies. Our method uses a basis function\nexpansion technique for the Gaussian process which significantly reduces the\ncomputational complexity and avoids the need for numerical integration. The\napproach also allows for reformulation of come classical regularization methods\nas Laplacian and Tikhonov regularization as Gaussian process regression, and\nhence provides an efficient algorithm and principled means for their parameter\ntuning. Results from simulated and real data indicate that this approach is\nless sensitive to streak artifacts as compared to the commonly used method of\nfiltered backprojection. \n\n"}
{"id": "1809.04747", "contents": "Title: Geodesic Clustering in Deep Generative Models Abstract: Deep generative models are tremendously successful in learning\nlow-dimensional latent representations that well-describe the data. These\nrepresentations, however, tend to much distort relationships between points,\ni.e. pairwise distances tend to not reflect semantic similarities well. This\nrenders unsupervised tasks, such as clustering, difficult when working with the\nlatent representations. We demonstrate that taking the geometry of the\ngenerative model into account is sufficient to make simple clustering\nalgorithms work well over latent representations. Leaning on the recent finding\nthat deep generative models constitute stochastically immersed Riemannian\nmanifolds, we propose an efficient algorithm for computing geodesics (shortest\npaths) and computing distances in the latent space, while taking its distortion\ninto account. We further propose a new architecture for modeling uncertainty in\nvariational autoencoders, which is essential for understanding the geometry of\ndeep generative models. Experiments show that the geodesic distance is very\nlikely to reflect the internal structure of the data. \n\n"}
{"id": "1809.04790", "contents": "Title: Adversarial Examples: Opportunities and Challenges Abstract: Deep neural networks (DNNs) have shown huge superiority over humans in image\nrecognition, speech processing, autonomous vehicles and medical diagnosis.\nHowever, recent studies indicate that DNNs are vulnerable to adversarial\nexamples (AEs), which are designed by attackers to fool deep learning models.\nDifferent from real examples, AEs can mislead the model to predict incorrect\noutputs while hardly be distinguished by human eyes, therefore threaten\nsecurity-critical deep-learning applications. In recent years, the generation\nand defense of AEs have become a research hotspot in the field of artificial\nintelligence (AI) security. This article reviews the latest research progress\nof AEs. First, we introduce the concept, cause, characteristics and evaluation\nmetrics of AEs, then give a survey on the state-of-the-art AE generation\nmethods with the discussion of advantages and disadvantages. After that, we\nreview the existing defenses and discuss their limitations. Finally, future\nresearch opportunities and challenges on AEs are prospected. \n\n"}
{"id": "1809.04967", "contents": "Title: Gaussian process classification using posterior linearisation Abstract: This paper proposes a new algorithm for Gaussian process classification based\non posterior linearisation (PL). In PL, a Gaussian approximation to the\nposterior density is obtained iteratively using the best possible linearisation\nof the conditional mean of the labels and accounting for the linearisation\nerror. PL has some theoretical advantages over expectation propagation (EP):\nall calculated covariance matrices are positive definite and there is a local\nconvergence theorem. In experimental data, PL has better performance than EP\nwith the noisy threshold likelihood and the parallel implementation of the\nalgorithms. \n\n"}
{"id": "1809.05183", "contents": "Title: Explainable time series tweaking via irreversible and reversible\n  temporal transformations Abstract: Time series classification has received great attention over the past decade\nwith a wide range of methods focusing on predictive performance by exploiting\nvarious types of temporal features. Nonetheless, little emphasis has been\nplaced on interpretability and explainability. In this paper, we formulate the\nnovel problem of explainable time series tweaking, where, given a time series\nand an opaque classifier that provides a particular classification decision for\nthe time series, we want to find the minimum number of changes to be performed\nto the given time series so that the classifier changes its decision to another\nclass. We show that the problem is NP-hard, and focus on two instantiations of\nthe problem, which we refer to as reversible and irreversible time series\ntweaking. The classifier under investigation is the random shapelet forest\nclassifier. Moreover, we propose two algorithmic solutions for the two problems\nalong with simple optimizations, as well as a baseline solution using the\nnearest neighbor classifier. An extensive experimental evaluation on a variety\nof real datasets demonstrates the usefulness and effectiveness of our problem\nformulation and solutions. \n\n"}
{"id": "1809.05242", "contents": "Title: Neural Network Topologies for Sparse Training Abstract: The sizes of deep neural networks (DNNs) are rapidly outgrowing the capacity\nof hardware to store and train them. Research over the past few decades has\nexplored the prospect of sparsifying DNNs before, during, and after training by\npruning edges from the underlying topology. The resulting neural network is\nknown as a sparse neural network. More recent work has demonstrated the\nremarkable result that certain sparse DNNs can train to the same precision as\ndense DNNs at lower runtime and storage cost. An intriguing class of these\nsparse DNNs is the X-Nets, which are initialized and trained upon a sparse\ntopology with neither reference to a parent dense DNN nor subsequent pruning.\nWe present an algorithm that deterministically generates sparse DNN topologies\nthat, as a whole, are much more diverse than X-Net topologies, while preserving\nX-Nets' desired characteristics. \n\n"}
{"id": "1809.05259", "contents": "Title: Random Warping Series: A Random Features Method for Time-Series\n  Embedding Abstract: Time series data analytics has been a problem of substantial interests for\ndecades, and Dynamic Time Warping (DTW) has been the most widely adopted\ntechnique to measure dissimilarity between time series. A number of\nglobal-alignment kernels have since been proposed in the spirit of DTW to\nextend its use to kernel-based estimation method such as support vector\nmachine. However, those kernels suffer from diagonal dominance of the Gram\nmatrix and a quadratic complexity w.r.t. the sample size. In this work, we\nstudy a family of alignment-aware positive definite (p.d.) kernels, with its\nfeature embedding given by a distribution of \\emph{Random Warping Series\n(RWS)}. The proposed kernel does not suffer from the issue of diagonal\ndominance while naturally enjoys a \\emph{Random Features} (RF) approximation,\nwhich reduces the computational complexity of existing DTW-based techniques\nfrom quadratic to linear in terms of both the number and the length of\ntime-series. We also study the convergence of the RF approximation for the\ndomain of time series of unbounded length. Our extensive experiments on 16\nbenchmark datasets demonstrate that RWS outperforms or matches state-of-the-art\nclassification and clustering methods in both accuracy and computational time.\nOur code and data is available at {\n\\url{https://github.com/IBM/RandomWarpingSeries}}. \n\n"}
{"id": "1809.05476", "contents": "Title: Hardware-Aware Machine Learning: Modeling and Optimization Abstract: Recent breakthroughs in Deep Learning (DL) applications have made DL models a\nkey component in almost every modern computing system. The increased popularity\nof DL applications deployed on a wide-spectrum of platforms have resulted in a\nplethora of design challenges related to the constraints introduced by the\nhardware itself. What is the latency or energy cost for an inference made by a\nDeep Neural Network (DNN)? Is it possible to predict this latency or energy\nconsumption before a model is trained? If yes, how can machine learners take\nadvantage of these models to design the hardware-optimal DNN for deployment?\nFrom lengthening battery life of mobile devices to reducing the runtime\nrequirements of DL models executing in the cloud, the answers to these\nquestions have drawn significant attention.\n  One cannot optimize what isn't properly modeled. Therefore, it is important\nto understand the hardware efficiency of DL models during serving for making an\ninference, before even training the model. This key observation has motivated\nthe use of predictive models to capture the hardware performance or energy\nefficiency of DL applications. Furthermore, DL practitioners are challenged\nwith the task of designing the DNN model, i.e., of tuning the hyper-parameters\nof the DNN architecture, while optimizing for both accuracy of the DL model and\nits hardware efficiency. Therefore, state-of-the-art methodologies have\nproposed hardware-aware hyper-parameter optimization techniques. In this paper,\nwe provide a comprehensive assessment of state-of-the-art work and selected\nresults on the hardware-aware modeling and optimization for DL applications. We\nalso highlight several open questions that are poised to give rise to novel\nhardware-aware designs in the next few years, as DL applications continue to\nsignificantly impact associated hardware systems and platforms. \n\n"}
{"id": "1809.05550", "contents": "Title: Efficient Structured Surrogate Loss and Regularization in Structured\n  Prediction Abstract: In this dissertation, we focus on several important problems in structured\nprediction. In structured prediction, the label has a rich intrinsic\nsubstructure, and the loss varies with respect to the predicted label and the\ntrue label pair. Structured SVM is an extension of binary SVM to adapt to such\nstructured tasks.\n  In the first part of the dissertation, we study the surrogate losses and its\nefficient methods. To minimize the empirical risk, a surrogate loss which upper\nbounds the loss, is used as a proxy to minimize the actual loss. Since the\nobjective function is written in terms of the surrogate loss, the choice of the\nsurrogate loss is important, and the performance depends on it. Another issue\nregarding the surrogate loss is the efficiency of the argmax label inference\nfor the surrogate loss. Efficient inference is necessary for the optimization\nsince it is often the most time-consuming step. We present a new class of\nsurrogate losses named bi-criteria surrogate loss, which is a generalization of\nthe popular surrogate losses. We first investigate an efficient method for a\nslack rescaling formulation as a starting point utilizing decomposability of\nthe model. Then, we extend the algorithm to the bi-criteria surrogate loss,\nwhich is very efficient and also shows performance improvements.\n  In the second part of the dissertation, another important issue of\nregularization is studied. Specifically, we investigate a problem of\nregularization in hierarchical classification when a structural imbalance\nexists in the label structure. We present a method to normalize the structure,\nas well as a new norm, namely shared Frobenius norm. It is suitable for\nhierarchical classification that adapts to the data in addition to the label\nstructure. \n\n"}
{"id": "1809.05788", "contents": "Title: Mobility Mode Detection Using WiFi Signals Abstract: We utilize Wi-Fi communications from smartphones to predict their mobility\nmode, i.e. walking, biking and driving. Wi-Fi sensors were deployed at four\nstrategic locations in a closed loop on streets in downtown Toronto. Deep\nneural network (Multilayer Perceptron) along with three decision tree based\nclassifiers (Decision Tree, Bagged Decision Tree and Random Forest) are\ndeveloped. Results show that the best prediction accuracy is achieved by\nMultilayer Perceptron, with 86.52% correct predictions of mobility modes. \n\n"}
{"id": "1809.06146", "contents": "Title: Curriculum goal masking for continuous deep reinforcement learning Abstract: Deep reinforcement learning has recently gained a focus on problems where\npolicy or value functions are independent of goals. Evidence exists that the\nsampling of goals has a strong effect on the learning performance, but there is\na lack of general mechanisms that focus on optimizing the goal sampling\nprocess. In this work, we present a simple and general goal masking method that\nalso allows us to estimate a goal's difficulty level and thus realize a\ncurriculum learning approach for deep RL. Our results indicate that focusing on\ngoals with a medium difficulty level is appropriate for deep deterministic\npolicy gradient (DDPG) methods, while an \"aim for the stars and reach the\nmoon-strategy\", where hard goals are sampled much more often than simple goals,\nleads to the best learning performance in cases where DDPG is combined with for\nhindsight experience replay (HER). We demonstrate that the approach\nsignificantly outperforms standard goal sampling for different robotic object\nmanipulation problems. \n\n"}
{"id": "1809.06686", "contents": "Title: Domain Adaptation for Real-Time Student Performance Prediction Abstract: Increasingly fast development and update cycle of online course contents, and\ndiverse demographics of students in each online classroom, make student\nperformance prediction in real-time (before the course finishes) and/or on\ncurriculum without specific historical performance data available interesting\ntopics for both industrial research and practical needs. In this research, we\ntackle the problem of real-time student performance prediction with on-going\ncourses in a domain adaptation framework, which is a system trained on\nstudents' labeled outcome from one set of previous coursework but is meant to\nbe deployed on another. In particular, we first introduce recently-developed\nGritNet architecture which is the current state of the art for student\nperformance prediction problem, and develop a new \\emph{unsupervised} domain\nadaptation method to transfer a GritNet trained on a past course to a new\ncourse without any (students' outcome) label. Our results for real Udacity\nstudents' graduation predictions show that the GritNet not only\n\\emph{generalizes} well from one course to another across different Nanodegree\nprograms, but enhances real-time predictions explicitly in the first few weeks\nwhen accurate predictions are most challenging. \n\n"}
{"id": "1809.07023", "contents": "Title: Removing the Feature Correlation Effect of Multiplicative Noise Abstract: Multiplicative noise, including dropout, is widely used to regularize deep\nneural networks (DNNs), and is shown to be effective in a wide range of\narchitectures and tasks. From an information perspective, we consider injecting\nmultiplicative noise into a DNN as training the network to solve the task with\nnoisy information pathways, which leads to the observation that multiplicative\nnoise tends to increase the correlation between features, so as to increase the\nsignal-to-noise ratio of information pathways. However, high feature\ncorrelation is undesirable, as it increases redundancy in representations. In\nthis work, we propose non-correlating multiplicative noise (NCMN), which\nexploits batch normalization to remove the correlation effect in a simple yet\neffective way. We show that NCMN significantly improves the performance of\nstandard multiplicative noise on image classification tasks, providing a better\nalternative to dropout for batch-normalized networks. Additionally, we present\na unified view of NCMN and shake-shake regularization, which explains the\nperformance gain of the latter. \n\n"}
{"id": "1809.07402", "contents": "Title: Identifying Generalization Properties in Neural Networks Abstract: While it has not yet been proven, empirical evidence suggests that model\ngeneralization is related to local properties of the optima which can be\ndescribed via the Hessian. We connect model generalization with the local\nproperty of a solution under the PAC-Bayes paradigm. In particular, we prove\nthat model generalization ability is related to the Hessian, the higher-order\n\"smoothness\" terms characterized by the Lipschitz constant of the Hessian, and\nthe scales of the parameters. Guided by the proof, we propose a metric to score\nthe generalization capability of the model, as well as an algorithm that\noptimizes the perturbed model accordingly. \n\n"}
{"id": "1809.07703", "contents": "Title: Fighting Redundancy and Model Decay with Embeddings Abstract: Every day, hundreds of millions of new Tweets containing over 40 languages of\never-shifting vernacular flow through Twitter. Models that attempt to extract\ninsight from this firehose of information must face the torrential covariate\nshift that is endemic to the Twitter platform. While regularly-retrained\nalgorithms can maintain performance in the face of this shift, fixed model\nfeatures that fail to represent new trends and tokens can quickly become stale,\nresulting in performance degradation. To mitigate this problem we employ\nlearned features, or embedding models, that can efficiently represent the most\nrelevant aspects of a data distribution. Sharing these embedding models across\nteams can also reduce redundancy and multiplicatively increase cross-team\nmodeling productivity. In this paper, we detail the commoditized tools,\nalgorithms and pipelines that we have developed and are developing at Twitter\nto regularly generate high quality, up-to-date embeddings and share them\nbroadly across the company. \n\n"}
{"id": "1809.07945", "contents": "Title: SCC: Automatic Classification of Code Snippets Abstract: Determining the programming language of a source code file has been\nconsidered in the research community; it has been shown that Machine Learning\n(ML) and Natural Language Processing (NLP) algorithms can be effective in\nidentifying the programming language of source code files. However, determining\nthe programming language of a code snippet or a few lines of source code is\nstill a challenging task. Online forums such as Stack Overflow and code\nrepositories such as GitHub contain a large number of code snippets. In this\npaper, we describe Source Code Classification (SCC), a classifier that can\nidentify the programming language of code snippets written in 21 different\nprogramming languages. A Multinomial Naive Bayes (MNB) classifier is employed\nwhich is trained using Stack Overflow posts. It is shown to achieve an accuracy\nof 75% which is higher than that with Programming Languages Identification (PLI\na proprietary online classifier of snippets) whose accuracy is only 55.5%. The\naverage score for precision, recall and the F1 score with the proposed tool are\n0.76, 0.75 and 0.75, respectively. In addition, it can distinguish between code\nsnippets from a family of programming languages such as C, C++ and C#, and can\nalso identify the programming language version such as C# 3.0, C# 4.0 and C#\n5.0. \n\n"}
{"id": "1809.07998", "contents": "Title: Hierarchical System Mapping for Large-Scale Fault-Tolerant Quantum\n  Computing Abstract: Considering the large-scale quantum computer, it is important to know how\nmuch quantum computational resources is necessary precisely and quickly.\nUnfortunately the previous methods so far cannot support a large-scale quantum\ncomputing practically and therefore the analysis because they usually use a\nnon-structured code. To overcome this problem, we propose a fast mapping by\nusing the hierarchical assembly code which is much more compact than the\nnon-structured code. During the mapping process, the necessary modules and\ntheir interconnection can be dynamically mapped by using the communication bus\nat the cost of additional qubits. In our study, the proposed method works very\nfast such as 1 hour than 1500 days for Shor algorithm to factorize 512-bit\ninteger. Meanwhile, since the hierarchical assembly code has high degree of\nlocality, it has shorter SWAP chains and hence it does not increase the quantum\ncomputation time than expected. \n\n"}
{"id": "1809.08510", "contents": "Title: Towards Language Agnostic Universal Representations Abstract: When a bilingual student learns to solve word problems in math, we expect the\nstudent to be able to solve these problem in both languages the student is\nfluent in,even if the math lessons were only taught in one language. However,\ncurrent representations in machine learning are language dependent. In this\nwork, we present a method to decouple the language from the problem by learning\nlanguage agnostic representations and therefore allowing training a model in\none language and applying to a different one in a zero shot fashion. We learn\nthese representations by taking inspiration from linguistics and formalizing\nUniversal Grammar as an optimization process (Chomsky, 2014; Montague, 1970).\nWe demonstrate the capabilities of these representations by showing that the\nmodels trained on a single language using language agnostic representations\nachieve very similar accuracies in other languages. \n\n"}
{"id": "1809.08530", "contents": "Title: Provably Correct Automatic Subdifferentiation for Qualified Programs Abstract: The Cheap Gradient Principle (Griewank 2008) --- the computational cost of\ncomputing the gradient of a scalar-valued function is nearly the same (often\nwithin a factor of $5$) as that of simply computing the function itself --- is\nof central importance in optimization; it allows us to quickly obtain (high\ndimensional) gradients of scalar loss functions which are subsequently used in\nblack box gradient-based optimization procedures. The current state of affairs\nis markedly different with regards to computing subderivatives: widely used ML\nlibraries, including TensorFlow and PyTorch, do not correctly compute\n(generalized) subderivatives even on simple examples. This work considers the\nquestion: is there a Cheap Subgradient Principle? Our main result shows that,\nunder certain restrictions on our library of nonsmooth functions (standard in\nnonlinear programming), provably correct generalized subderivatives can be\ncomputed at a computational cost that is within a (dimension-free) factor of\n$6$ of the cost of computing the scalar function itself. \n\n"}
{"id": "1809.08705", "contents": "Title: On the Behavior of the Expectation-Maximization Algorithm for Mixture\n  Models Abstract: Finite mixture models are among the most popular statistical models used in\ndifferent data science disciplines. Despite their broad applicability,\ninference under these models typically leads to computationally challenging\nnon-convex problems. While the Expectation-Maximization (EM) algorithm is the\nmost popular approach for solving these non-convex problems, the behavior of\nthis algorithm is not well understood. In this work, we focus on the case of\nmixture of Laplacian (or Gaussian) distribution. We start by analyzing a simple\nequally weighted mixture of two single dimensional Laplacian distributions and\nshow that every local optimum of the population maximum likelihood estimation\nproblem is globally optimal. Then, we prove that the EM algorithm converges to\nthe ground truth parameters almost surely with random initialization. Our\nresult extends the existing results for Gaussian distribution to Laplacian\ndistribution. Then we numerically study the behavior of mixture models with\nmore than two components. Motivated by our extensive numerical experiments, we\npropose a novel stochastic method for estimating the mean of components of a\nmixture model. Our numerical experiments show that our algorithm outperforms\nthe Naive EM algorithm in almost all scenarios. \n\n"}
{"id": "1809.08899", "contents": "Title: Neural network approach to classifying alarming student responses to\n  online assessment Abstract: Automated scoring engines are increasingly being used to score the free-form\ntext responses that students give to questions. Such engines are not designed\nto appropriately deal with responses that a human reader would find alarming\nsuch as those that indicate an intention to self-harm or harm others, responses\nthat allude to drug abuse or sexual abuse or any response that would elicit\nconcern for the student writing the response. Our neural network models have\nbeen designed to help identify these anomalous responses from a large\ncollection of typical responses that students give. The responses identified by\nthe neural network can be assessed for urgency, severity, and validity more\nquickly by a team of reviewers than otherwise possible. Given the anomalous\nnature of these types of responses, our goal is to maximize the chance of\nflagging these responses for review given the constraint that only a fixed\npercentage of responses can viably be assessed by a team of reviewers. \n\n"}
{"id": "1809.09401", "contents": "Title: Hypergraph Neural Networks Abstract: In this paper, we present a hypergraph neural networks (HGNN) framework for\ndata representation learning, which can encode high-order data correlation in a\nhypergraph structure. Confronting the challenges of learning representation for\ncomplex data in real practice, we propose to incorporate such data structure in\na hypergraph, which is more flexible on data modeling, especially when dealing\nwith complex data. In this method, a hyperedge convolution operation is\ndesigned to handle the data correlation during representation learning. In this\nway, traditional hypergraph learning procedure can be conducted using hyperedge\nconvolution operations efficiently. HGNN is able to learn the hidden layer\nrepresentation considering the high-order data structure, which is a general\nframework considering the complex data correlations. We have conducted\nexperiments on citation network classification and visual object recognition\ntasks and compared HGNN with graph convolutional networks and other traditional\nmethods. Experimental results demonstrate that the proposed HGNN method\noutperforms recent state-of-the-art methods. We can also reveal from the\nresults that the proposed HGNN is superior when dealing with multi-modal data\ncompared with existing methods. \n\n"}
{"id": "1809.09574", "contents": "Title: Combined convolutional and recurrent neural networks for hierarchical\n  classification of images Abstract: Deep learning models based on CNNs are predominantly used in image\nclassification tasks. Such approaches, assuming independence of object\ncategories, normally use a CNN as a feature learner and apply a flat classifier\non top of it. Object classes in many settings have hierarchical relations, and\nclassifiers exploiting these relations should perform better. We propose\nhierarchical classification models combining a CNN to extract hierarchical\nrepresentations of images, and an RNN or sequence-to-sequence model to capture\na hierarchical tree of classes. In addition, we apply residual learning to the\nRNN part in oder to facilitate training our compound model and improve\ngeneralization of the model. Experimental results on a real world proprietary\ndataset of images show that our hierarchical networks perform better than\nstate-of-the-art CNNs. \n\n"}
{"id": "1809.09853", "contents": "Title: Stochastic Second-order Methods for Non-convex Optimization with Inexact\n  Hessian and Gradient Abstract: Trust region and cubic regularization methods have demonstrated good\nperformance in small scale non-convex optimization, showing the ability to\nescape from saddle points. Each iteration of these methods involves computation\nof gradient, Hessian and function value in order to obtain the search direction\nand adjust the radius or cubic regularization parameter. However, exactly\ncomputing those quantities are too expensive in large-scale problems such as\ntraining deep networks. In this paper, we study a family of stochastic trust\nregion and cubic regularization methods when gradient, Hessian and function\nvalues are computed inexactly, and show the iteration complexity to achieve\n$\\epsilon$-approximate second-order optimality is in the same order with\nprevious work for which gradient and function values are computed exactly. The\nmild conditions on inexactness can be achieved in finite-sum minimization using\nrandom sampling. We show the algorithm performs well on training convolutional\nneural networks compared with previous second-order methods. \n\n"}
{"id": "1809.10238", "contents": "Title: C4Synth: Cross-Caption Cycle-Consistent Text-to-Image Synthesis Abstract: Generating an image from its description is a challenging task worth solving\nbecause of its numerous practical applications ranging from image editing to\nvirtual reality. All existing methods use one single caption to generate a\nplausible image. A single caption by itself, can be limited, and may not be\nable to capture the variety of concepts and behavior that may be present in the\nimage. We propose two deep generative models that generate an image by making\nuse of multiple captions describing it. This is achieved by ensuring\n'Cross-Caption Cycle Consistency' between the multiple captions and the\ngenerated image(s). We report quantitative and qualitative results on the\nstandard Caltech-UCSD Birds (CUB) and Oxford-102 Flowers datasets to validate\nthe efficacy of the proposed approach. \n\n"}
{"id": "1809.10241", "contents": "Title: Classifying Mammographic Breast Density by Residual Learning Abstract: Mammographic breast density, a parameter used to describe the proportion of\nbreast tissue fibrosis, is widely adopted as an evaluation characteristic of\nthe likelihood of breast cancer incidence. In this study, we present a\nradiomics approach based on residual learning for the classification of\nmammographic breast densities. Our method possesses several encouraging\nproperties such as being almost fully automatic, possessing big model capacity\nand flexibility. It can obtain outstanding classification results without the\nnecessity of result compensation using mammographs taken from different views.\nThe proposed method was instantiated with the INbreast dataset and\nclassification accuracies of 92.6% and 96.8% were obtained for the four BI-RADS\n(Breast Imaging and Reporting Data System) category task and the two BI-RADS\ncategory task,respectively. The superior performances achieved compared to the\nexisting state-of-the-art methods along with its encouraging properties\nindicate that our method has a great potential to be applied as a\ncomputer-aided diagnosis tool. \n\n"}
{"id": "1809.10312", "contents": "Title: Vector Learning for Cross Domain Representations Abstract: Recently, generative adversarial networks have gained a lot of popularity for\nimage generation tasks. However, such models are associated with complex\nlearning mechanisms and demand very large relevant datasets. This work borrows\nconcepts from image and video captioning models to form an image generative\nframework. The model is trained in a similar fashion as recurrent captioning\nmodel and uses the learned weights for image generation. This is done in an\ninverse direction, where the input is a caption and the output is an image. The\nvector representation of the sentence and frames are extracted from an\nencoder-decoder model which is initially trained on similar sentence and image\npairs. Our model conditions image generation on a natural language caption. We\nleverage a sequence-to-sequence model to generate synthetic captions that have\nthe same meaning for having a robust image generation. One key advantage of our\nmethod is that the traditional image captioning datasets can be used for\nsynthetic sentence paraphrases. Results indicate that images generated through\nmultiple captions are better at capturing the semantic meaning of the family of\ncaptions. \n\n"}
{"id": "1809.10326", "contents": "Title: Boosting Trust Region Policy Optimization by Normalizing Flows Policy Abstract: We propose to improve trust region policy search with normalizing flows\npolicy. We illustrate that when the trust region is constructed by KL\ndivergence constraints, normalizing flows policy generates samples far from the\n'center' of the previous policy iterate, which potentially enables better\nexploration and helps avoid bad local optima. Through extensive comparisons, we\nshow that the normalizing flows policy significantly improves upon baseline\narchitectures especially on high-dimensional tasks with complex dynamics. \n\n"}
{"id": "1809.10678", "contents": "Title: Introducing Noise in Decentralized Training of Neural Networks Abstract: It has been shown that injecting noise into the neural network weights during\nthe training process leads to a better generalization of the resulting model.\nNoise injection in the distributed setup is a straightforward technique and it\nrepresents a promising approach to improve the locally trained models. We\ninvestigate the effects of noise injection into the neural networks during a\ndecentralized training process. We show both theoretically and empirically that\nnoise injection has no positive effect in expectation on linear models, though.\nHowever for non-linear neural networks we empirically show that noise injection\nsubstantially improves model quality helping to reach a generalization ability\nof a local model close to the serial baseline. \n\n"}
{"id": "1809.10680", "contents": "Title: Supervised Nonnegative Matrix Factorization to Predict ICU Mortality\n  Risk Abstract: ICU mortality risk prediction is a tough yet important task. On one hand, due\nto the complex temporal data collected, it is difficult to identify the\neffective features and interpret them easily; on the other hand, good\nprediction can help clinicians take timely actions to prevent the mortality.\nThese correspond to the interpretability and accuracy problems. Most existing\nmethods lack of the interpretability, but recently Subgraph Augmented\nNonnegative Matrix Factorization (SANMF) has been successfully applied to time\nseries data to provide a path to interpret the features well. Therefore, we\nadopted this approach as the backbone to analyze the patient data. One\nlimitation of the raw SANMF method is its poor prediction ability due to its\nunsupervised nature. To deal with this problem, we proposed a supervised SANMF\nalgorithm by integrating the logistic regression loss function into the NMF\nframework and solved it with an alternating optimization procedure. We used the\nsimulation data to verify the effectiveness of this method, and then we applied\nit to ICU mortality risk prediction and demonstrated its superiority over other\nconventional supervised NMF methods. \n\n"}
{"id": "1809.10941", "contents": "Title: Deep learning systems as complex networks Abstract: Thanks to the availability of large scale digital datasets and massive\namounts of computational power, deep learning algorithms can learn\nrepresentations of data by exploiting multiple levels of abstraction. These\nmachine learning methods have greatly improved the state-of-the-art in many\nchallenging cognitive tasks, such as visual object recognition, speech\nprocessing, natural language understanding and automatic translation. In\nparticular, one class of deep learning models, known as deep belief networks,\ncan discover intricate statistical structure in large data sets in a completely\nunsupervised fashion, by learning a generative model of the data using\nHebbian-like learning mechanisms. Although these self-organizing systems can be\nconveniently formalized within the framework of statistical mechanics, their\ninternal functioning remains opaque, because their emergent dynamics cannot be\nsolved analytically. In this article we propose to study deep belief networks\nusing techniques commonly employed in the study of complex networks, in order\nto gain some insights into the structural and functional properties of the\ncomputational graph resulting from the learning process. \n\n"}
{"id": "1809.11084", "contents": "Title: Reuse and Adaptation for Entity Resolution through Transfer Learning Abstract: Entity resolution (ER) is one of the fundamental problems in data\nintegration, where machine learning (ML) based classifiers often provide the\nstate-of-the-art results. Considerable human effort goes into feature\nengineering and training data creation. In this paper, we investigate a new\nproblem: Given a dataset D_T for ER with limited or no training data, is it\npossible to train a good ML classifier on D_T by reusing and adapting the\ntraining data of dataset D_S from same or related domain? Our major\ncontributions include (1) a distributed representation based approach to encode\neach tuple from diverse datasets into a standard feature space; (2)\nidentification of common scenarios where the reuse of training data can be\nbeneficial; and (3) five algorithms for handling each of the aforementioned\nscenarios. We have performed comprehensive experiments on 12 datasets from 5\ndifferent domains (publications, movies, songs, restaurants, and books). Our\nexperiments show that our algorithms provide significant benefits such as\nproviding superior performance for a fixed training data size. \n\n"}
{"id": "1809.11096", "contents": "Title: Large Scale GAN Training for High Fidelity Natural Image Synthesis Abstract: Despite recent progress in generative image modeling, successfully generating\nhigh-resolution, diverse samples from complex datasets such as ImageNet remains\nan elusive goal. To this end, we train Generative Adversarial Networks at the\nlargest scale yet attempted, and study the instabilities specific to such\nscale. We find that applying orthogonal regularization to the generator renders\nit amenable to a simple \"truncation trick,\" allowing fine control over the\ntrade-off between sample fidelity and variety by reducing the variance of the\nGenerator's input. Our modifications lead to models which set the new state of\nthe art in class-conditional image synthesis. When trained on ImageNet at\n128x128 resolution, our models (BigGANs) achieve an Inception Score (IS) of\n166.5 and Frechet Inception Distance (FID) of 7.4, improving over the previous\nbest IS of 52.52 and FID of 18.6. \n\n"}
{"id": "1809.11165", "contents": "Title: GPyTorch: Blackbox Matrix-Matrix Gaussian Process Inference with GPU\n  Acceleration Abstract: Despite advances in scalable models, the inference tools used for Gaussian\nprocesses (GPs) have yet to fully capitalize on developments in computing\nhardware. We present an efficient and general approach to GP inference based on\nBlackbox Matrix-Matrix multiplication (BBMM). BBMM inference uses a modified\nbatched version of the conjugate gradients algorithm to derive all terms for\ntraining and inference in a single call. BBMM reduces the asymptotic complexity\nof exact GP inference from $O(n^3)$ to $O(n^2)$. Adapting this algorithm to\nscalable approximations and complex GP models simply requires a routine for\nefficient matrix-matrix multiplication with the kernel and its derivative. In\naddition, BBMM uses a specialized preconditioner to substantially speed up\nconvergence. In experiments we show that BBMM effectively uses GPU hardware to\ndramatically accelerate both exact GP inference and scalable approximations.\nAdditionally, we provide GPyTorch, a software platform for scalable GP\ninference via BBMM, built on PyTorch. \n\n"}
{"id": "1810.00150", "contents": "Title: Directional Analysis of Stochastic Gradient Descent via von Mises-Fisher\n  Distributions in Deep learning Abstract: Although stochastic gradient descent (SGD) is a driving force behind the\nrecent success of deep learning, our understanding of its dynamics in a\nhigh-dimensional parameter space is limited. In recent years, some researchers\nhave used the stochasticity of minibatch gradients, or the signal-to-noise\nratio, to better characterize the learning dynamics of SGD. Inspired from these\nwork, we here analyze SGD from a geometrical perspective by inspecting the\nstochasticity of the norms and directions of minibatch gradients. We propose a\nmodel of the directional concentration for minibatch gradients through von\nMises-Fisher (VMF) distribution, and show that the directional uniformity of\nminibatch gradients increases over the course of SGD. We empirically verify our\nresult using deep convolutional networks and observe a higher correlation\nbetween the gradient stochasticity and the proposed directional uniformity than\nthat against the gradient norm stochasticity, suggesting that the directional\nstatistics of minibatch gradients is a major factor behind SGD. \n\n"}
{"id": "1810.00375", "contents": "Title: Assertion-Based Optimization of Quantum Programs Abstract: Quantum computers promise to perform certain computations exponentially\nfaster than any classical device. Precise control over their physical\nimplementation and proper shielding from unwanted interactions with the\nenvironment become more difficult as the space/time volume of the computation\ngrows. Code optimization is thus crucial in order to reduce resource\nrequirements to the greatest extent possible. Besides manual optimization,\nprevious work has adapted classical methods such as constant-folding and common\nsubexpression elimination to the quantum domain. However, such\nclassically-inspired methods fail to exploit certain optimization opportunities\nacross subroutine boundaries, limiting the effectiveness of software reuse. To\naddress this insufficiency, we introduce an optimization methodology which\nemploys annotations that describe how subsystems are entangled in order to\nexploit these optimization opportunities. We formalize our approach, prove its\ncorrectness, and present benchmarks: Without any prior manual optimization, our\nmethodology is able to reduce, e.g., the qubit requirements of a 64-bit\nfloating-point subroutine by $34\\times$. \n\n"}
{"id": "1810.00471", "contents": "Title: Identifying Bias in AI using Simulation Abstract: Machine learned models exhibit bias, often because the datasets used to train\nthem are biased. This presents a serious problem for the deployment of such\ntechnology, as the resulting models might perform poorly on populations that\nare minorities within the training set and ultimately present higher risks to\nthem. We propose to use high-fidelity computer simulations to interrogate and\ndiagnose biases within ML classifiers. We present a framework that leverages\nBayesian parameter search to efficiently characterize the high dimensional\nfeature space and more quickly identify weakness in performance. We apply our\napproach to an example domain, face detection, and show that it can be used to\nhelp identify demographic biases in commercial face application programming\ninterfaces (APIs). \n\n"}
{"id": "1810.00490", "contents": "Title: Learning Deep Representations from Clinical Data for Chronic Kidney\n  Disease Abstract: We study the behavior of a Time-Aware Long Short-Term Memory Autoencoder, a\nstate-of-the-art method, in the context of learning latent representations from\nirregularly sampled patient data. We identify a key issue in the way such\nrecurrent neural network models are being currently used and show that the\nsolution of the issue leads to significant improvements in the learnt\nrepresentations on both synthetic and real datasets. A detailed analysis of the\nimproved methodology for representing patients suffering from Chronic Kidney\nDisease (CKD) using clinical data is provided. Experimental results show that\nthe proposed T-LSTM model is able to capture the long-term trends in the data,\nwhile effectively handling the noise in the signal. Finally, we show that by\nusing the latent representations of the CKD patients obtained from the T-LSTM\nautoencoder, one can identify unusual patient profiles from the target\npopulation. \n\n"}
{"id": "1810.00520", "contents": "Title: FIRE-DES++: Enhanced Online Pruning of Base Classifiers for Dynamic\n  Ensemble Selection Abstract: Despite being very effective in several classification tasks, Dynamic\nEnsemble Selection (DES) techniques can select classifiers that classify all\nsamples in the region of competence as being from the same class. The Frienemy\nIndecision REgion DES (FIRE-DES) tackles this problem by pre-selecting\nclassifiers that correctly classify at least one pair of samples from different\nclasses in the region of competence of the test sample. However, FIRE-DES\napplies the pre-selection for the classification of a test sample if and only\nif its region of competence is composed of samples from different classes\n(indecision region), even though this criterion is not reliable for determining\nif a test sample is located close to the borders of classes (true indecision\nregion) when the region of competence is obtained using classical nearest\nneighbors approach. Because of that, FIRE-DES mistakes noisy regions for true\nindecision regions, leading to the pre-selection of incompetent classifiers,\nand mistakes true indecision regions for safe regions, leaving samples in such\nregions without any pre-selection. To tackle these issues, we propose the\nFIRE-DES++, an enhanced FIRE-DES that removes noise and reduces the overlap of\nclasses in the validation set; and defines the region of competence using an\nequal number of samples of each class, avoiding selecting a region of\ncompetence with samples of a single class. Experiments are conducted using\nFIRE-DES++ with 8 different dynamic selection techniques on 64 classification\ndatasets. Experimental results show that FIRE-DES++ increases the\nclassification performance of all DES techniques considered in this work,\noutperforming FIRE-DES with 7 out of the 8 DES techniques, and outperforming\nstate-of-the-art DES frameworks. \n\n"}
{"id": "1810.00656", "contents": "Title: Perfect Match: A Simple Method for Learning Representations For\n  Counterfactual Inference With Neural Networks Abstract: Learning representations for counterfactual inference from observational data\nis of high practical relevance for many domains, such as healthcare, public\npolicy and economics. Counterfactual inference enables one to answer \"What\nif...?\" questions, such as \"What would be the outcome if we gave this patient\ntreatment $t_1$?\". However, current methods for training neural networks for\ncounterfactual inference on observational data are either overly complex,\nlimited to settings with only two available treatments, or both. Here, we\npresent Perfect Match (PM), a method for training neural networks for\ncounterfactual inference that is easy to implement, compatible with any\narchitecture, does not add computational complexity or hyperparameters, and\nextends to any number of treatments. PM is based on the idea of augmenting\nsamples within a minibatch with their propensity-matched nearest neighbours.\nOur experiments demonstrate that PM outperforms a number of more complex\nstate-of-the-art methods in inferring counterfactual outcomes across several\nbenchmarks, particularly in settings with many treatments. \n\n"}
{"id": "1810.00679", "contents": "Title: Direct optimization of F-measure for retrieval-based personal question\n  answering Abstract: Recent advances in spoken language technologies and the introduction of many\ncustomer facing products, have given rise to a wide customer reliance on smart\npersonal assistants for many of their daily tasks. In this paper, we present a\nsystem to reduce users' cognitive load by extending personal assistants with\nlong-term personal memory where users can store and retrieve by voice,\narbitrary pieces of information. The problem is framed as a neural retrieval\nbased question answering system where answers are selected from previously\nstored user memories. We propose to directly optimize the end-to-end retrieval\nperformance, measured by the F1-score, using reinforcement learning, leading to\nbetter performance on our experimental test set(s). \n\n"}
{"id": "1810.00821", "contents": "Title: Variational Discriminator Bottleneck: Improving Imitation Learning,\n  Inverse RL, and GANs by Constraining Information Flow Abstract: Adversarial learning methods have been proposed for a wide range of\napplications, but the training of adversarial models can be notoriously\nunstable. Effectively balancing the performance of the generator and\ndiscriminator is critical, since a discriminator that achieves very high\naccuracy will produce relatively uninformative gradients. In this work, we\npropose a simple and general technique to constrain information flow in the\ndiscriminator by means of an information bottleneck. By enforcing a constraint\non the mutual information between the observations and the discriminator's\ninternal representation, we can effectively modulate the discriminator's\naccuracy and maintain useful and informative gradients. We demonstrate that our\nproposed variational discriminator bottleneck (VDB) leads to significant\nimprovements across three distinct application areas for adversarial learning\nalgorithms. Our primary evaluation studies the applicability of the VDB to\nimitation learning of dynamic continuous control skills, such as running. We\nshow that our method can learn such skills directly from \\emph{raw} video\ndemonstrations, substantially outperforming prior adversarial imitation\nlearning methods. The VDB can also be combined with adversarial inverse\nreinforcement learning to learn parsimonious reward functions that can be\ntransferred and re-optimized in new settings. Finally, we demonstrate that VDB\ncan train GANs more effectively for image generation, improving upon a number\nof prior stabilization methods. \n\n"}
{"id": "1810.01187", "contents": "Title: Thompson Sampling Algorithms for Cascading Bandits Abstract: Motivated by the pressing need for efficient optimization in online\nrecommender systems, we revisit the cascading bandit model proposed by Kveton\net al. (2015). While Thompson sampling (TS) algorithms have been shown to be\nempirically superior to Upper Confidence Bound (UCB) algorithms for cascading\nbandits, theoretical guarantees are only known for the latter. In this paper,\nwe first provide a problem-dependent upper bound on the regret of a TS\nalgorithm with Beta-Bernoulli updates; this upper bound is tighter than a\nrecent derivation under a more general setting by Huyuk and Tekin (2019). Next,\nwe design and analyze another TS algorithm with Gaussian updates, TS-Cascade.\nTS-Cascade achieves the state-of-the-art regret bound for cascading bandits.\nComplementarily, we consider a linear generalization of the cascading bandit\nmodel, which allows efficient learning in large cascading bandit problem\ninstances. We introduce and analyze a TS algorithm, which enjoys a regret bound\nthat depends on the dimension of the linear model but not the number of items.\nFinally, by using information-theoretic techniques and judiciously constructing\ncascading bandit instances, we derive a nearly matching regret lower bound for\nthe standard model. Our paper establishes the first theoretical guarantees on\nTS algorithms for stochastic combinatorial bandit problem model with partial\nfeedback. Numerical experiments demonstrate the superiority of the proposed TS\nalgorithms compared to existing UCB-based ones. \n\n"}
{"id": "1810.01875", "contents": "Title: Relaxed Quantization for Discretized Neural Networks Abstract: Neural network quantization has become an important research area due to its\ngreat impact on deployment of large models on resource constrained devices. In\norder to train networks that can be effectively discretized without loss of\nperformance, we introduce a differentiable quantization procedure.\nDifferentiability can be achieved by transforming continuous distributions over\nthe weights and activations of the network to categorical distributions over\nthe quantization grid. These are subsequently relaxed to continuous surrogates\nthat can allow for efficient gradient-based optimization. We further show that\nstochastic rounding can be seen as a special case of the proposed approach and\nthat under this formulation the quantization grid itself can also be optimized\nwith gradient descent. We experimentally validate the performance of our method\non MNIST, CIFAR 10 and Imagenet classification. \n\n"}
{"id": "1810.01963", "contents": "Title: Learning Scheduling Algorithms for Data Processing Clusters Abstract: Efficiently scheduling data processing jobs on distributed compute clusters\nrequires complex algorithms. Current systems, however, use simple generalized\nheuristics and ignore workload characteristics, since developing and tuning a\nscheduling policy for each workload is infeasible. In this paper, we show that\nmodern machine learning techniques can generate highly-efficient policies\nautomatically. Decima uses reinforcement learning (RL) and neural networks to\nlearn workload-specific scheduling algorithms without any human instruction\nbeyond a high-level objective such as minimizing average job completion time.\nOff-the-shelf RL techniques, however, cannot handle the complexity and scale of\nthe scheduling problem. To build Decima, we had to develop new representations\nfor jobs' dependency graphs, design scalable RL models, and invent RL training\nmethods for dealing with continuous stochastic job arrivals. Our prototype\nintegration with Spark on a 25-node cluster shows that Decima improves the\naverage job completion time over hand-tuned scheduling heuristics by at least\n21%, achieving up to 2x improvement during periods of high cluster load. \n\n"}
{"id": "1810.02019", "contents": "Title: Seq2Slate: Re-ranking and Slate Optimization with RNNs Abstract: Ranking is a central task in machine learning and information retrieval. In\nthis task, it is especially important to present the user with a slate of items\nthat is appealing as a whole. This in turn requires taking into account\ninteractions between items, since intuitively, placing an item on the slate\naffects the decision of which other items should be placed alongside it. In\nthis work, we propose a sequence-to-sequence model for ranking called\nseq2slate. At each step, the model predicts the next `best' item to place on\nthe slate given the items already selected. The sequential nature of the model\nallows complex dependencies between the items to be captured directly in a\nflexible and scalable way. We show how to learn the model end-to-end from weak\nsupervision in the form of easily obtained click-through data. We further\ndemonstrate the usefulness of our approach in experiments on standard ranking\nbenchmarks as well as in a real-world recommendation system. \n\n"}
{"id": "1810.02069", "contents": "Title: Finding Solutions to Generative Adversarial Privacy Abstract: We present heuristics for solving the maximin problem induced by the\ngenerative adversarial privacy setting for linear and convolutional neural\nnetwork (CNN) adversaries. In the linear adversary setting, we present a greedy\nalgorithm for approximating the optimal solution for the privatizer, which\nperforms better as the number of instances increases. We also provide an\nanalysis of the algorithm to show that it not only removes the features most\ncorrelated with the private label first, but also preserves the prediction\naccuracy of public labels that are sufficiently independent of the features\nthat are relevant to the private label. In the CNN adversary setting, we\npresent a method of hiding selected information from the adversary while\npreserving the others through alternately optimizing the goals of the\nprivatizer and the adversary using neural network backpropagation. We\nexperimentally show that our method succeeds on a fixed adversary. \n\n"}
{"id": "1810.02797", "contents": "Title: RCCNet: An Efficient Convolutional Neural Network for Histological\n  Routine Colon Cancer Nuclei Classification Abstract: Efficient and precise classification of histological cell nuclei is of utmost\nimportance due to its potential applications in the field of medical image\nanalysis. It would facilitate the medical practitioners to better understand\nand explore various factors for cancer treatment. The classification of\nhistological cell nuclei is a challenging task due to the cellular\nheterogeneity. This paper proposes an efficient Convolutional Neural Network\n(CNN) based architecture for classification of histological routine colon\ncancer nuclei named as RCCNet. The main objective of this network is to keep\nthe CNN model as simple as possible. The proposed RCCNet model consists of only\n1,512,868 learnable parameters which are significantly less compared to the\npopular CNN models such as AlexNet, CIFARVGG, GoogLeNet, and WRN. The\nexperiments are conducted over publicly available routine colon cancer\nhistological dataset \"CRCHistoPhenotypes\". The results of the proposed RCCNet\nmodel are compared with five state-of-the-art CNN models in terms of the\naccuracy, weighted average F1 score and training time. The proposed method has\nachieved a classification accuracy of 80.61% and 0.7887 weighted average F1\nscore. The proposed RCCNet is more efficient and generalized terms of the\ntraining time and data over-fitting, respectively. \n\n"}
{"id": "1810.02927", "contents": "Title: Scaling All-Goals Updates in Reinforcement Learning Using Convolutional\n  Neural Networks Abstract: Being able to reach any desired location in the environment can be a valuable\nasset for an agent. Learning a policy to navigate between all pairs of states\nindividually is often not feasible. An all-goals updating algorithm uses each\ntransition to learn Q-values towards all goals simultaneously and off-policy.\nHowever the expensive numerous updates in parallel limited the approach to\nsmall tabular cases so far. To tackle this problem we propose to use\nconvolutional network architectures to generate Q-values and updates for a\nlarge number of goals at once. We demonstrate the accuracy and generalization\nqualities of the proposed method on randomly generated mazes and Sokoban\npuzzles. In the case of on-screen goal coordinates the resulting mapping from\nframes to distance-maps directly informs the agent about which places are\nreachable and in how many steps. As an example of application we show that\nreplacing the random actions in epsilon-greedy exploration by several actions\ntowards feasible goals generates better exploratory trajectories on Montezuma's\nRevenge and Super Mario All-Stars games. \n\n"}
{"id": "1810.02966", "contents": "Title: Understanding Recurrent Neural Architectures by Analyzing and\n  Synthesizing Long Distance Dependencies in Benchmark Sequential Datasets Abstract: In order to build efficient deep recurrent neural architectures, it is\nessential to analyze the complexityof long distance dependencies (LDDs) of the\ndataset being modeled. In this paper, we presentdetailed analysis of the\ndependency decay curve exhibited by various datasets. The datasets sampledfrom\na similar process (e.g. natural language, sequential MNIST, Strictlyk-Piecewise\nlanguages,etc) display variations in the properties of the dependency decay\ncurve. Our analysis reveal thefactors resulting in these variations; such as\n(i) number of unique symbols in a dataset, (ii) size ofthe dataset, (iii)\nnumber of interacting symbols within a given LDD, and (iv) the distance\nbetweenthe interacting symbols. We test these factors by generating synthesized\ndatasets of the Strictlyk-Piecewise languages. Another advantage of these\nsynthesized datasets is that they enable targetedtesting of deep recurrent\nneural architectures in terms of their ability to model LDDs with\ndifferentcharacteristics. We also demonstrate that analysing dependency decay\ncurves can inform the selectionof optimal hyper-parameters for SOTA deep\nrecurrent neural architectures. This analysis can directlycontribute to the\ndevelopment of more accurate and efficient sequential models. \n\n"}
{"id": "1810.03372", "contents": "Title: Detecting Memorization in ReLU Networks Abstract: We propose a new notion of `non-linearity' of a network layer with respect to\nan input batch that is based on its proximity to a linear system, which is\nreflected in the non-negative rank of the activation matrix. We measure this\nnon-linearity by applying non-negative factorization to the activation matrix.\nConsidering batches of similar samples, we find that high non-linearity in deep\nlayers is indicative of memorization. Furthermore, by applying our approach\nlayer-by-layer, we find that the mechanism for memorization consists of\ndistinct phases. We perform experiments on fully-connected and convolutional\nneural networks trained on several image and audio datasets. Our results\ndemonstrate that as an indicator for memorization, our technique can be used to\nperform early stopping. \n\n"}
{"id": "1810.03442", "contents": "Title: Towards the Latent Transcriptome Abstract: In this work we propose a method to compute continuous embeddings for kmers\nfrom raw RNA-seq data, without the need for alignment to a reference genome.\nThe approach uses an RNN to transform kmers of the RNA-seq reads into a 2\ndimensional representation that is used to predict abundance of each kmer. We\nreport that our model captures information of both DNA sequence similarity as\nwell as DNA sequence abundance in the embedding latent space, that we call the\nLatent Transcriptome. We confirm the quality of these vectors by comparing them\nto known gene sub-structures and report that the latent space recovers exon\ninformation from raw RNA-Seq data from acute myeloid leukemia patients.\nFurthermore we show that this latent space allows the detection of genomic\nabnormalities such as translocations as well as patient-specific mutations,\nmaking this representation space both useful for visualization as well as\nanalysis. \n\n"}
{"id": "1810.03548", "contents": "Title: Meta-Learning: A Survey Abstract: Meta-learning, or learning to learn, is the science of systematically\nobserving how different machine learning approaches perform on a wide range of\nlearning tasks, and then learning from this experience, or meta-data, to learn\nnew tasks much faster than otherwise possible. Not only does this dramatically\nspeed up and improve the design of machine learning pipelines or neural\narchitectures, it also allows us to replace hand-engineered algorithms with\nnovel approaches learned in a data-driven way. In this chapter, we provide an\noverview of the state of the art in this fascinating and continuously evolving\nfield. \n\n"}
{"id": "1810.03764", "contents": "Title: Generalized Latent Variable Recovery for Generative Adversarial Networks Abstract: The Generator of a Generative Adversarial Network (GAN) is trained to\ntransform latent vectors drawn from a prior distribution into realistic looking\nphotos. These latent vectors have been shown to encode information about the\ncontent of their corresponding images. Projecting input images onto the latent\nspace of a GAN is non-trivial, but previous work has successfully performed\nthis task for latent spaces with a uniform prior. We extend these techniques to\nlatent spaces with a Gaussian prior, and demonstrate our technique's\neffectiveness. \n\n"}
{"id": "1810.03773", "contents": "Title: Average Margin Regularization for Classifiers Abstract: Adversarial robustness has become an important research topic given empirical\ndemonstrations on the lack of robustness of deep neural networks.\nUnfortunately, recent theoretical results suggest that adversarial training\ninduces a strict tradeoff between classification accuracy and adversarial\nrobustness. In this paper, we propose and then study a new regularization for\nany margin classifier or deep neural network. We motivate this regularization\nby a novel generalization bound that shows a tradeoff in classifier accuracy\nbetween maximizing its margin and average margin. We thus call our approach an\naverage margin (AM) regularization, and it consists of a linear term added to\nthe objective. We theoretically show that for certain distributions AM\nregularization can both improve classifier accuracy and robustness to\nadversarial attacks. We conclude by using both synthetic and real data to\nempirically show that AM regularization can strictly improve both accuracy and\nrobustness for support vector machine's (SVM's), relative to unregularized\nclassifiers and adversarially trained classifiers. \n\n"}
{"id": "1810.03958", "contents": "Title: Deterministic Variational Inference for Robust Bayesian Neural Networks Abstract: Bayesian neural networks (BNNs) hold great promise as a flexible and\nprincipled solution to deal with uncertainty when learning from finite data.\nAmong approaches to realize probabilistic inference in deep neural networks,\nvariational Bayes (VB) is theoretically grounded, generally applicable, and\ncomputationally efficient. With wide recognition of potential advantages, why\nis it that variational Bayes has seen very limited practical use for BNNs in\nreal applications? We argue that variational inference in neural networks is\nfragile: successful implementations require careful initialization and tuning\nof prior variances, as well as controlling the variance of Monte Carlo gradient\nestimates. We provide two innovations that aim to turn VB into a robust\ninference tool for Bayesian neural networks: first, we introduce a novel\ndeterministic method to approximate moments in neural networks, eliminating\ngradient variance; second, we introduce a hierarchical prior for parameters and\na novel Empirical Bayes procedure for automatically selecting prior variances.\nCombining these two innovations, the resulting method is highly efficient and\nrobust. On the application of heteroscedastic regression we demonstrate good\npredictive performance over alternative approaches. \n\n"}
{"id": "1810.04133", "contents": "Title: Learning One-hidden-layer Neural Networks under General Input\n  Distributions Abstract: Significant advances have been made recently on training neural networks,\nwhere the main challenge is in solving an optimization problem with abundant\ncritical points. However, existing approaches to address this issue crucially\nrely on a restrictive assumption: the training data is drawn from a Gaussian\ndistribution. In this paper, we provide a novel unified framework to design\nloss functions with desirable landscape properties for a wide range of general\ninput distributions. On these loss functions, remarkably, stochastic gradient\ndescent theoretically recovers the true parameters with global initializations\nand empirically outperforms the existing approaches. Our loss function design\nbridges the notion of score functions with the topic of neural network\noptimization. Central to our approach is the task of estimating the score\nfunction from samples, which is of basic and independent interest to\ntheoretical statistics. Traditional estimation methods (example: kernel based)\nfail right at the outset; we bring statistical methods of local likelihood to\ndesign a novel estimator of score functions, that provably adapts to the local\ngeometry of the unknown density. \n\n"}
{"id": "1810.04247", "contents": "Title: Feature Selection using Stochastic Gates Abstract: Feature selection problems have been extensively studied for linear\nestimation, for instance, Lasso, but less emphasis has been placed on feature\nselection for non-linear functions. In this study, we propose a method for\nfeature selection in high-dimensional non-linear function estimation problems.\nThe new procedure is based on minimizing the $\\ell_0$ norm of the vector of\nindicator variables that represent if a feature is selected or not. Our\napproach relies on the continuous relaxation of Bernoulli distributions, which\nallows our model to learn the parameters of the approximate Bernoulli\ndistributions via gradient descent. This general framework simultaneously\nminimizes a loss function while selecting relevant features. Furthermore, we\nprovide an information-theoretic justification of incorporating Bernoulli\ndistribution into our approach and demonstrate the potential of the approach on\nsynthetic and real-life applications. \n\n"}
{"id": "1810.04437", "contents": "Title: Persistence pays off: Paying Attention to What the LSTM Gating Mechanism\n  Persists Abstract: Language Models (LMs) are important components in several Natural Language\nProcessing systems. Recurrent Neural Network LMs composed of LSTM units,\nespecially those augmented with an external memory, have achieved\nstate-of-the-art results. However, these models still struggle to process long\nsequences which are more likely to contain long-distance dependencies because\nof information fading and a bias towards more recent information. In this paper\nwe demonstrate an effective mechanism for retrieving information in a memory\naugmented LSTM LM based on attending to information in memory in proportion to\nthe number of timesteps the LSTM gating mechanism persisted the information. \n\n"}
{"id": "1810.04491", "contents": "Title: Multi-class Classification Model Inspired by Quantum Detection Theory Abstract: Machine Learning has become very famous currently which assist in identifying\nthe patterns from the raw data. Technological advancement has led to\nsubstantial improvement in Machine Learning which, thus helping to improve\nprediction. Current Machine Learning models are based on Classical Theory,\nwhich can be replaced by Quantum Theory to improve the effectiveness of the\nmodel. In the previous work, we developed binary classifier inspired by Quantum\nDetection Theory. In this extended abstract, our main goal is to develop\nmulti-class classifier. We generally use the terminology multinomial\nclassification or multi-class classification when we have a classification\nproblem for classifying observations or instances into one of three or more\nclasses. \n\n"}
{"id": "1810.04586", "contents": "Title: The Laplacian in RL: Learning Representations with Efficient\n  Approximations Abstract: The smallest eigenvectors of the graph Laplacian are well-known to provide a\nsuccinct representation of the geometry of a weighted graph. In reinforcement\nlearning (RL), where the weighted graph may be interpreted as the state\ntransition process induced by a behavior policy acting on the environment,\napproximating the eigenvectors of the Laplacian provides a promising approach\nto state representation learning. However, existing methods for performing this\napproximation are ill-suited in general RL settings for two main reasons:\nFirst, they are computationally expensive, often requiring operations on large\nmatrices. Second, these methods lack adequate justification beyond simple,\ntabular, finite-state settings. In this paper, we present a fully general and\nscalable method for approximating the eigenvectors of the Laplacian in a\nmodel-free RL context. We systematically evaluate our approach and empirically\nshow that it generalizes beyond the tabular, finite-state setting. Even in\ntabular, finite-state settings, its ability to approximate the eigenvectors\noutperforms previous proposals. Finally, we show the potential benefits of\nusing a Laplacian representation learned using our method in goal-achieving RL\ntasks, providing evidence that our technique can be used to significantly\nimprove the performance of an RL agent. \n\n"}
{"id": "1810.04650", "contents": "Title: Multi-Task Learning as Multi-Objective Optimization Abstract: In multi-task learning, multiple tasks are solved jointly, sharing inductive\nbias between them. Multi-task learning is inherently a multi-objective problem\nbecause different tasks may conflict, necessitating a trade-off. A common\ncompromise is to optimize a proxy objective that minimizes a weighted linear\ncombination of per-task losses. However, this workaround is only valid when the\ntasks do not compete, which is rarely the case. In this paper, we explicitly\ncast multi-task learning as multi-objective optimization, with the overall\nobjective of finding a Pareto optimal solution. To this end, we use algorithms\ndeveloped in the gradient-based multi-objective optimization literature. These\nalgorithms are not directly applicable to large-scale learning problems since\nthey scale poorly with the dimensionality of the gradients and the number of\ntasks. We therefore propose an upper bound for the multi-objective loss and\nshow that it can be optimized efficiently. We further prove that optimizing\nthis upper bound yields a Pareto optimal solution under realistic assumptions.\nWe apply our method to a variety of multi-task deep learning problems including\ndigit classification, scene understanding (joint semantic segmentation,\ninstance segmentation, and depth estimation), and multi-label classification.\nOur method produces higher-performing models than recent multi-task learning\nformulations or per-task training. \n\n"}
{"id": "1810.04793", "contents": "Title: Patient2Vec: A Personalized Interpretable Deep Representation of the\n  Longitudinal Electronic Health Record Abstract: The wide implementation of electronic health record (EHR) systems facilitates\nthe collection of large-scale health data from real clinical settings. Despite\nthe significant increase in adoption of EHR systems, this data remains largely\nunexplored, but presents a rich data source for knowledge discovery from\npatient health histories in tasks such as understanding disease correlations\nand predicting health outcomes. However, the heterogeneity, sparsity, noise,\nand bias in this data present many complex challenges. This complexity makes it\ndifficult to translate potentially relevant information into machine learning\nalgorithms. In this paper, we propose a computational framework, Patient2Vec,\nto learn an interpretable deep representation of longitudinal EHR data which is\npersonalized for each patient. To evaluate this approach, we apply it to the\nprediction of future hospitalizations using real EHR data and compare its\npredictive performance with baseline methods. Patient2Vec produces a vector\nspace with meaningful structure and it achieves an AUC around 0.799\noutperforming baseline methods. In the end, the learned feature importance can\nbe visualized and interpreted at both the individual and population levels to\nbring clinical insights. \n\n"}
{"id": "1810.05466", "contents": "Title: Mode Normalization Abstract: Normalization methods are a central building block in the deep learning\ntoolbox. They accelerate and stabilize training, while decreasing the\ndependence on manually tuned learning rate schedules. When learning from\nmulti-modal distributions, the effectiveness of batch normalization (BN),\narguably the most prominent normalization method, is reduced. As a remedy, we\npropose a more flexible approach: by extending the normalization to more than a\nsingle mean and variance, we detect modes of data on-the-fly, jointly\nnormalizing samples that share common features. We demonstrate that our method\noutperforms BN and other widely used normalization techniques in several\nexperiments, including single and multi-task datasets. \n\n"}
{"id": "1810.05640", "contents": "Title: Inventory Balancing with Online Learning Abstract: We study a general problem of allocating limited resources to heterogeneous\ncustomers over time under model uncertainty. Each type of customer can be\nserviced using different actions, each of which stochastically consumes some\ncombination of resources, and returns different rewards for the resources\nconsumed. We consider a general model where the resource consumption\ndistribution associated with each (customer type, action)-combination is not\nknown, but is consistent and can be learned over time. In addition, the\nsequence of customer types to arrive over time is arbitrary and completely\nunknown.\n  We overcome both the challenges of model uncertainty and customer\nheterogeneity by judiciously synthesizing two algorithmic frameworks from the\nliterature: inventory balancing, which \"reserves\" a portion of each resource\nfor high-reward customer types which could later arrive, and online learning,\nwhich shows how to \"explore\" the resource consumption distributions of each\ncustomer type under different actions. We define an auxiliary problem, which\nallows for existing competitive ratio and regret bounds to be seamlessly\nintegrated. Furthermore, we show that the performance guarantee generated by\nour framework is tight, that is, we provide an information-theoretic lower\nbound which shows that both the loss from competitive ratio and the loss for\nregret are relevant in the combined problem.\n  Finally, we demonstrate the efficacy of our algorithms on a publicly\navailable hotel data set. Our framework is highly practical in that it requires\nno historical data (no fitted customer choice models, nor forecasting of\ncustomer arrival patterns) and can be used to initialize allocation strategies\nin fast-changing environments. \n\n"}
{"id": "1810.05644", "contents": "Title: Temporal Convolutional Memory Networks for Remaining Useful Life\n  Estimation of Industrial Machinery Abstract: Accurately estimating the remaining useful life (RUL) of industrial machinery\nis beneficial in many real-world applications. Estimation techniques have\nmainly utilized linear models or neural network based approaches with a focus\non short term time dependencies. This paper, introduces a system model that\nincorporates temporal convolutions with both long term and short term time\ndependencies. The proposed network learns salient features and complex temporal\nvariations in sensor values, and predicts the RUL. A data augmentation method\nis used for increased accuracy. The proposed method is compared with several\nstate-of-the-art algorithms on publicly available datasets. It demonstrates\npromising results, with superior results for datasets obtained from complex\nenvironments. \n\n"}
{"id": "1810.05741", "contents": "Title: Explaining Black Boxes on Sequential Data using Weighted Automata Abstract: Understanding how a learned black box works is of crucial interest for the\nfuture of Machine Learning. In this paper, we pioneer the question of the\nglobal interpretability of learned black box models that assign numerical\nvalues to symbolic sequential data. To tackle that task, we propose a spectral\nalgorithm for the extraction of weighted automata (WA) from such black boxes.\nThis algorithm does not require the access to a dataset or to the inner\nrepresentation of the black box: the inferred model can be obtained solely by\nquerying the black box, feeding it with inputs and analyzing its outputs.\nExperiments using Recurrent Neural Networks (RNN) trained on a wide collection\nof 48 synthetic datasets and 2 real datasets show that the obtained\napproximation is of great quality. \n\n"}
{"id": "1810.06509", "contents": "Title: Predictor-Corrector Policy Optimization Abstract: We present a predictor-corrector framework, called PicCoLO, that can\ntransform a first-order model-free reinforcement or imitation learning\nalgorithm into a new hybrid method that leverages predictive models to\naccelerate policy learning. The new \"PicCoLOed\" algorithm optimizes a policy by\nrecursively repeating two steps: In the Prediction Step, the learner uses a\nmodel to predict the unseen future gradient and then applies the predicted\nestimate to update the policy; in the Correction Step, the learner runs the\nupdated policy in the environment, receives the true gradient, and then\ncorrects the policy using the gradient error. Unlike previous algorithms,\nPicCoLO corrects for the mistakes of using imperfect predicted gradients and\nhence does not suffer from model bias. The development of PicCoLO is made\npossible by a novel reduction from predictable online learning to adversarial\nonline learning, which provides a systematic way to modify existing first-order\nalgorithms to achieve the optimal regret with respect to predictable\ninformation. We show, in both theory and simulation, that the convergence rate\nof several first-order model-free algorithms can be improved by PicCoLO. \n\n"}
{"id": "1810.06640", "contents": "Title: Adversarial Text Generation Without Reinforcement Learning Abstract: Generative Adversarial Networks (GANs) have experienced a recent surge in\npopularity, performing competitively in a variety of tasks, especially in\ncomputer vision. However, GAN training has shown limited success in natural\nlanguage processing. This is largely because sequences of text are discrete,\nand thus gradients cannot propagate from the discriminator to the generator.\nRecent solutions use reinforcement learning to propagate approximate gradients\nto the generator, but this is inefficient to train. We propose to utilize an\nautoencoder to learn a low-dimensional representation of sentences. A GAN is\nthen trained to generate its own vectors in this space, which decode to\nrealistic utterances. We report both random and interpolated samples from the\ngenerator. Visualization of sentence vectors indicate our model correctly\nlearns the latent space of the autoencoder. Both human ratings and BLEU scores\nshow that our model generates realistic text against competitive baselines. \n\n"}
{"id": "1810.06684", "contents": "Title: Column generation based math-heuristic for classification trees Abstract: This paper explores the use of Column Generation (CG) techniques in\nconstructing univariate binary decision trees for classification tasks. We\npropose a novel Integer Linear Programming (ILP) formulation, based on\nroot-to-leaf paths in decision trees. The model is solved via a Column\nGeneration based heuristic. To speed up the heuristic, we use a restricted\ninstance data by considering a subset of decision splits, sampled from the\nsolutions of the well-known CART algorithm. Extensive numerical experiments\nshow that our approach is competitive with the state-of-the-art ILP-based\nalgorithms. In particular, the proposed approach is capable of handling big\ndata sets with tens of thousands of data rows. Moreover, for large data sets,\nit finds solutions competitive to CART. \n\n"}
{"id": "1810.07076", "contents": "Title: Stochastic Negative Mining for Learning with Large Output Spaces Abstract: We consider the problem of retrieving the most relevant labels for a given\ninput when the size of the output space is very large. Retrieval methods are\nmodeled as set-valued classifiers which output a small set of classes for each\ninput, and a mistake is made if the label is not in the output set. Despite its\npractical importance, a statistically principled, yet practical solution to\nthis problem is largely missing. To this end, we first define a family of\nsurrogate losses and show that they are calibrated and convex under certain\nconditions on the loss parameters and data distribution, thereby establishing a\nstatistical and analytical basis for using these losses. Furthermore, we\nidentify a particularly intuitive class of loss functions in the aforementioned\nfamily and show that they are amenable to practical implementation in the large\noutput space setting (i.e. computation is possible without evaluating scores of\nall labels) by developing a technique called Stochastic Negative Mining. We\nalso provide generalization error bounds for the losses in the family. Finally,\nwe conduct experiments which demonstrate that Stochastic Negative Mining yields\nbenefits over commonly used negative sampling approaches. \n\n"}
{"id": "1810.07354", "contents": "Title: Fault Tolerance in Iterative-Convergent Machine Learning Abstract: Machine learning (ML) training algorithms often possess an inherent\nself-correcting behavior due to their iterative-convergent nature. Recent\nsystems exploit this property to achieve adaptability and efficiency in\nunreliable computing environments by relaxing the consistency of execution and\nallowing calculation errors to be self-corrected during training. However, the\nbehavior of such systems are only well understood for specific types of\ncalculation errors, such as those caused by staleness, reduced precision, or\nasynchronicity, and for specific types of training algorithms, such as\nstochastic gradient descent. In this paper, we develop a general framework to\nquantify the effects of calculation errors on iterative-convergent algorithms\nand use this framework to design new strategies for checkpoint-based fault\ntolerance. Our framework yields a worst-case upper bound on the iteration cost\nof arbitrary perturbations to model parameters during training. Our system,\nSCAR, employs strategies which reduce the iteration cost upper bound due to\nperturbations incurred when recovering from checkpoints. We show that SCAR can\nreduce the iteration cost of partial failures by 78% - 95% when compared with\ntraditional checkpoint-based fault tolerance across a variety of ML models and\ntraining algorithms. \n\n"}
{"id": "1810.07406", "contents": "Title: Adversarial Balancing for Causal Inference Abstract: Biases in observational data of treatments pose a major challenge to\nestimating expected treatment outcomes in different populations. An important\ntechnique that accounts for these biases is reweighting samples to minimize the\ndiscrepancy between treatment groups. We present a novel reweighting approach\nthat uses bi-level optimization to alternately train a discriminator to\nminimize classification error, and a balancing weights generator that uses\nexponentiated gradient descent to maximize this error. This approach borrows\nprinciples from generative adversarial networks (GANs) to exploit the power of\nclassifiers for measuring two-sample divergence. We provide theoretical results\nfor conditions in which the estimation error is bounded by two factors: (i) the\ndiscrepancy measure induced by the discriminator; and (ii) the weights\nvariability. Experimental results on several benchmarks comparing to previous\nstate-of-the-art reweighting methods demonstrate the effectiveness of this\napproach in estimating causal effects. \n\n"}
{"id": "1810.07481", "contents": "Title: Provable Robustness of ReLU networks via Maximization of Linear Regions Abstract: It has been shown that neural network classifiers are not robust. This raises\nconcerns about their usage in safety-critical systems. We propose in this paper\na regularization scheme for ReLU networks which provably improves the\nrobustness of the classifier by maximizing the linear regions of the classifier\nas well as the distance to the decision boundary. Our techniques allow even to\nfind the minimal adversarial perturbation for a fraction of test points for\nlarge networks. In the experiments we show that our approach improves upon\nadversarial training both in terms of lower and upper bounds on the robustness\nand is comparable or better than the state-of-the-art in terms of test error\nand robustness. \n\n"}
{"id": "1810.07770", "contents": "Title: Small ReLU networks are powerful memorizers: a tight analysis of\n  memorization capacity Abstract: We study finite sample expressivity, i.e., memorization power of ReLU\nnetworks. Recent results require $N$ hidden nodes to memorize/interpolate\narbitrary $N$ data points. In contrast, by exploiting depth, we show that\n3-layer ReLU networks with $\\Omega(\\sqrt{N})$ hidden nodes can perfectly\nmemorize most datasets with $N$ points. We also prove that width\n$\\Theta(\\sqrt{N})$ is necessary and sufficient for memorizing $N$ data points,\nproving tight bounds on memorization capacity. The sufficiency result can be\nextended to deeper networks; we show that an $L$-layer network with $W$\nparameters in the hidden layers can memorize $N$ data points if $W =\n\\Omega(N)$. Combined with a recent upper bound $O(WL\\log W)$ on VC dimension,\nour construction is nearly tight for any fixed $L$. Subsequently, we analyze\nmemorization capacity of residual networks under a general position assumption;\nwe prove results that substantially reduce the known requirement of $N$ hidden\nnodes. Finally, we study the dynamics of stochastic gradient descent (SGD), and\nshow that when initialized near a memorizing global minimum of the empirical\nrisk, SGD quickly finds a nearby point with much smaller empirical risk. \n\n"}
{"id": "1810.07845", "contents": "Title: On Statistical Learning of Simplices: Unmixing Problem Revisited Abstract: We study the sample complexity of learning a high-dimensional simplex from a\nset of points uniformly sampled from its interior. Learning of simplices is a\nlong studied problem in computer science and has applications in computational\nbiology and remote sensing, mostly under the name of `spectral unmixing'. We\ntheoretically show that a sufficient sample complexity for reliable learning of\na $K$-dimensional simplex up to a total-variation error of $\\epsilon$ is\n$O\\left(\\frac{K^2}{\\epsilon}\\log\\frac{K}{\\epsilon}\\right)$, which yields a\nsubstantial improvement over existing bounds. Based on our new theoretical\nframework, we also propose a heuristic approach for the inference of simplices.\nExperimental results on synthetic and real-world datasets demonstrate a\ncomparable performance for our method on noiseless samples, while we outperform\nthe state-of-the-art in noisy cases. \n\n"}
{"id": "1810.08305", "contents": "Title: Open Vocabulary Learning on Source Code with a Graph-Structured Cache Abstract: Machine learning models that take computer program source code as input\ntypically use Natural Language Processing (NLP) techniques. However, a major\nchallenge is that code is written using an open, rapidly changing vocabulary\ndue to, e.g., the coinage of new variable and method names. Reasoning over such\na vocabulary is not something for which most NLP methods are designed. We\nintroduce a Graph-Structured Cache to address this problem; this cache contains\na node for each new word the model encounters with edges connecting each word\nto its occurrences in the code. We find that combining this graph-structured\ncache strategy with recent Graph-Neural-Network-based models for supervised\nlearning on code improves the models' performance on a code completion task and\na variable naming task --- with over $100\\%$ relative improvement on the latter\n--- at the cost of a moderate increase in computation time. \n\n"}
{"id": "1810.08323", "contents": "Title: Learning Multi-Layer Transform Models Abstract: Learned data models based on sparsity are widely used in signal processing\nand imaging applications. A variety of methods for learning synthesis\ndictionaries, sparsifying transforms, etc., have been proposed in recent years,\noften imposing useful structures or properties on the models. In this work, we\nfocus on sparsifying transform learning, which enjoys a number of advantages.\nWe consider multi-layer or nested extensions of the transform model, and\npropose efficient learning algorithms. Numerical experiments with image data\nillustrate the behavior of the multi-layer transform learning algorithm and its\nusefulness for image denoising. Multi-layer models provide better denoising\nquality than single layer schemes. \n\n"}
{"id": "1810.08591", "contents": "Title: A Modern Take on the Bias-Variance Tradeoff in Neural Networks Abstract: The bias-variance tradeoff tells us that as model complexity increases, bias\nfalls and variances increases, leading to a U-shaped test error curve. However,\nrecent empirical results with over-parameterized neural networks are marked by\na striking absence of the classic U-shaped test error curve: test error keeps\ndecreasing in wider networks. This suggests that there might not be a\nbias-variance tradeoff in neural networks with respect to network width, unlike\nwas originally claimed by, e.g., Geman et al. (1992). Motivated by the shaky\nevidence used to support this claim in neural networks, we measure bias and\nvariance in the modern setting. We find that both bias and variance can\ndecrease as the number of parameters grows. To better understand this, we\nintroduce a new decomposition of the variance to disentangle the effects of\noptimization and data sampling. We also provide theoretical analysis in a\nsimplified setting that is consistent with our empirical findings. \n\n"}
{"id": "1810.08646", "contents": "Title: SLAYER: Spike Layer Error Reassignment in Time Abstract: Configuring deep Spiking Neural Networks (SNNs) is an exciting research\navenue for low power spike event based computation. However, the spike\ngeneration function is non-differentiable and therefore not directly compatible\nwith the standard error backpropagation algorithm. In this paper, we introduce\na new general backpropagation mechanism for learning synaptic weights and\naxonal delays which overcomes the problem of non-differentiability of the spike\nfunction and uses a temporal credit assignment policy for backpropagating error\nto preceding layers. We describe and release a GPU accelerated software\nimplementation of our method which allows training both fully connected and\nconvolutional neural network (CNN) architectures. Using our software, we\ncompare our method against existing SNN based learning approaches and standard\nANN to SNN conversion techniques and show that our method achieves state of the\nart performance for an SNN on the MNIST, NMNIST, DVS Gesture, and TIDIGITS\ndatasets. \n\n"}
{"id": "1810.08765", "contents": "Title: Attribute-aware Collaborative Filtering: Survey and Classification Abstract: Attribute-aware CF models aims at rating prediction given not only the\nhistorical rating from users to items, but also the information associated with\nusers (e.g. age), items (e.g. price), or even ratings (e.g. rating time). This\npaper surveys works in the past decade developing attribute-aware CF systems,\nand discovered that mathematically they can be classified into four different\ncategories. We provide the readers not only the high level mathematical\ninterpretation of the existing works in this area but also the mathematical\ninsight for each category of models. Finally we provide in-depth experiment\nresults comparing the effectiveness of the major works in each category. \n\n"}
{"id": "1810.08865", "contents": "Title: Some Coxeter Groups in Reversible and Quantum Compuation Abstract: In this article we show how the structure of Coxeter groups are present in\ngate sets of reversible and quantum computing. These groups have efficient word\nproblems which means that circuits built from these gates have potential to be\nshortened efficiently. This is especially useful in the case of quantum\ncomputing when one does not have the timescale to perform a long series of\ngates and so one must and a gate scheduling that minimizes circuit depth. As\nthe main example we consider the oracle for 3SAT. \n\n"}
{"id": "1810.09113", "contents": "Title: The Bregman chord divergence Abstract: Distances are fundamental primitives whose choice significantly impacts the\nperformances of algorithms in machine learning and signal processing. However\nselecting the most appropriate distance for a given task is an endeavor.\nInstead of testing one by one the entries of an ever-expanding dictionary of\n{\\em ad hoc} distances, one rather prefers to consider parametric classes of\ndistances that are exhaustively characterized by axioms derived from first\nprinciples. Bregman divergences are such a class. However fine-tuning a Bregman\ndivergence is delicate since it requires to smoothly adjust a functional\ngenerator. In this work, we propose an extension of Bregman divergences called\nthe Bregman chord divergences. This new class of distances does not require\ngradient calculations, uses two scalar parameters that can be easily tailored\nin applications, and generalizes asymptotically Bregman divergences. \n\n"}
{"id": "1810.09311", "contents": "Title: Revisiting Distributional Correspondence Indexing: A Python\n  Reimplementation and New Experiments Abstract: This paper introduces PyDCI, a new implementation of Distributional\nCorrespondence Indexing (DCI) written in Python. DCI is a transfer learning\nmethod for cross-domain and cross-lingual text classification for which we had\nprovided an implementation (here called JaDCI) built on top of JaTeCS, a Java\nframework for text classification. PyDCI is a stand-alone version of DCI that\nexploits scikit-learn and the SciPy stack. We here report on new experiments\nthat we have carried out in order to test PyDCI, and in which we use as\nbaselines new high-performing methods that have appeared after DCI was\noriginally proposed. These experiments show that, thanks to a few subtle ways\nin which we have improved DCI, PyDCI outperforms both JaDCI and the\nabove-mentioned high-performing methods, and delivers the best known results on\nthe two popular benchmarks on which we had tested DCI, i.e.,\nMultiDomainSentiment (a.k.a. MDS -- for cross-domain adaptation) and\nWebis-CLS-10 (for cross-lingual adaptation). PyDCI, together with the code\nallowing to replicate our experiments, is available at\nhttps://github.com/AlexMoreo/pydci . \n\n"}
{"id": "1810.09666", "contents": "Title: Online learning with feedback graphs and switching costs Abstract: We study online learning when partial feedback information is provided\nfollowing every action of the learning process, and the learner incurs\nswitching costs for changing his actions. In this setting, the feedback\ninformation system can be represented by a graph, and previous works studied\nthe expected regret of the learner in the case of a clique (Expert setup), or\ndisconnected single loops (Multi-Armed Bandits (MAB)). This work provides a\nlower bound on the expected regret in the Partial Information (PI) setting,\nnamely for general feedback graphs --excluding the clique. Additionally, it\nshows that all algorithms that are optimal without switching costs are\nnecessarily sub-optimal in the presence of switching costs, which motivates the\nneed to design new algorithms. We propose two new algorithms: Threshold Based\nEXP3 and EXP3. SC. For the two special cases of symmetric PI setting and MAB,\nthe expected regret of both of these algorithms is order optimal in the\nduration of the learning process. Additionally, Threshold Based EXP3 is order\noptimal in the switching cost, whereas EXP3. SC is not. Finally, empirical\nevaluations show that Threshold Based EXP3 outperforms the previously proposed\norder-optimal algorithms EXP3 SET in the presence of switching costs, and Batch\nEXP3 in the MAB setting with switching costs. \n\n"}
{"id": "1810.09712", "contents": "Title: Finding Appropriate Traffic Regulations via Graph Convolutional Networks Abstract: Appropriate traffic regulations, e.g. planned road closure, are important in\ncongested events. Crowd simulators have been used to find appropriate\nregulations by simulating multiple scenarios with different regulations.\nHowever, this approach requires multiple simulation runs, which are\ntime-consuming. In this paper, we propose a method to learn a function that\noutputs regulation effects given the current traffic situation as inputs. If\nthe function is learned using the training data of many simulation runs in\nadvance, we can obtain an appropriate regulation efficiently by bypassing\nsimulations for the current situation. We use the graph convolutional networks\nfor modeling the function, which enable us to find regulations even for unseen\nareas. With the proposed method, we construct a graph for each area, where a\nnode represents a road, and an edge represents the road connection. By running\ncrowd simulations with various regulations on various areas, we generate\ntraffic situations and regulation effects. The graph convolutional networks are\ntrained to output the regulation effects given the graph with the traffic\nsituation information as inputs. With experiments using real-world road\nnetworks and a crowd simulator, we demonstrate that the proposed method can\nfind a road to close that reduces the average time needed to reach the\ndestination. \n\n"}
{"id": "1810.09828", "contents": "Title: DCSVM: Fast Multi-class Classification using Support Vector Machines Abstract: We present DCSVM, an efficient algorithm for multi-class classification using\nSupport Vector Machines. DCSVM is a divide and conquer algorithm which relies\non data sparsity in high dimensional space and performs a smart partitioning of\nthe whole training data set into disjoint subsets that are easily separable. A\nsingle prediction performed between two partitions eliminates at once one or\nmore classes in one partition, leaving only a reduced number of candidate\nclasses for subsequent steps. The algorithm continues recursively, reducing the\nnumber of classes at each step, until a final binary decision is made between\nthe last two classes left in the competition. In the best case scenario, our\nalgorithm makes a final decision between $k$ classes in $O(\\log k)$ decision\nsteps and in the worst case scenario DCSVM makes a final decision in $k-1$\nsteps, which is not worse than the existent techniques. \n\n"}
{"id": "1810.09957", "contents": "Title: NSML: Meet the MLaaS platform with a real-world case study Abstract: The boom of deep learning induced many industries and academies to introduce\nmachine learning based approaches into their concern, competitively. However,\nexisting machine learning frameworks are limited to sufficiently fulfill the\ncollaboration and management for both data and models. We proposed NSML, a\nmachine learning as a service (MLaaS) platform, to meet these demands. NSML\nhelps machine learning work be easily launched on a NSML cluster and provides a\ncollaborative environment which can afford development at enterprise scale.\nFinally, NSML users can deploy their own commercial services with NSML cluster.\nIn addition, NSML furnishes convenient visualization tools which assist the\nusers in analyzing their work. To verify the usefulness and accessibility of\nNSML, we performed some experiments with common examples. Furthermore, we\nexamined the collaborative advantages of NSML through three competitions with\nreal-world use cases. \n\n"}
{"id": "1810.10098", "contents": "Title: Deblending galaxy superpositions with branched generative adversarial\n  networks Abstract: Near-future large galaxy surveys will encounter blended galaxy images at a\nfraction of up to 50% in the densest regions of the universe. Current\ndeblending techniques may segment the foreground galaxy while leaving missing\npixel intensities in the background galaxy flux. The problem is compounded by\nthe diffuse nature of galaxies in their outer regions, making segmentation\nsignificantly more difficult than in traditional object segmentation\napplications. We propose a novel branched generative adversarial network (GAN)\nto deblend overlapping galaxies, where the two branches produce images of the\ntwo deblended galaxies. We show that generative models are a powerful engine\nfor deblending given their innate ability to infill missing pixel values\noccluded by the superposition. We maintain high peak signal-to-noise ratio and\nstructural similarity scores with respect to ground truth images upon\ndeblending. Our model also predicts near-instantaneously, making it a natural\nchoice for the immense quantities of data soon to be created by large surveys\nsuch as LSST, Euclid and WFIRST. \n\n"}
{"id": "1810.10122", "contents": "Title: PoPPy: A Point Process Toolbox Based on PyTorch Abstract: PoPPy is a Point Process toolbox based on PyTorch, which achieves flexible\ndesigning and efficient learning of point process models. It can be used for\ninterpretable sequential data modeling and analysis, e.g., Granger causality\nanalysis of multi-variate point processes, point process-based simulation and\nprediction of event sequences. In practice, the key points of point\nprocess-based sequential data modeling include: 1) How to design intensity\nfunctions to describe the mechanism behind observed data? 2) How to learn the\nproposed intensity functions from observed data? The goal of PoPPy is providing\na user-friendly solution to the key points above and achieving large-scale\npoint process-based sequential data analysis, simulation and prediction. \n\n"}
{"id": "1810.10327", "contents": "Title: BshapeNet: Object Detection and Instance Segmentation with Bounding\n  Shape Masks Abstract: Recent object detectors use four-coordinate bounding box (bbox) regression to\npredict object locations. Providing additional information indicating the\nobject positions and coordinates will improve detection performance. Thus, we\npropose two types of masks: a bbox mask and a bounding shape (bshape) mask, to\nrepresent the object's bbox and boundary shape, respectively. For each of these\ntypes, we consider two variants: the Thick model and the Scored model, both of\nwhich have the same morphology but differ in ways to make their boundaries\nthicker. To evaluate the proposed masks, we design extended frameworks by\nadding a bshape mask (or a bbox mask) branch to a Faster R-CNN framework, and\ncall this BshapeNet (or BboxNet). Further, we propose BshapeNet+, a network\nthat combines a bshape mask branch with a Mask R-CNN to improve instance\nsegmentation as well as detection. Among our proposed models, BshapeNet+\ndemonstrates the best performance in both tasks and achieves highly competitive\nresults with state of the art (SOTA) models. Particularly, it improves the\ndetection results over Faster R-CNN+RoIAlign (37.3% and 28.9%) with a detection\nAP of 42.4% and 32.3% on MS COCO test-dev and Cityscapes val, respectively.\nFurthermore, for small objects, it achieves 24.9% AP on COCO test-dev, a\nsignificant improvement over previous SOTA models. For instance segmentation,\nit is substantially superior to Mask R-CNN on both test datasets. \n\n"}
{"id": "1810.10337", "contents": "Title: Projecting Trouble: Light Based Adversarial Attacks on Deep Learning\n  Classifiers Abstract: This work demonstrates a physical attack on a deep learning image\nclassification system using projected light onto a physical scene. Prior work\nis dominated by techniques for creating adversarial examples which directly\nmanipulate the digital input of the classifier. Such an attack is limited to\nscenarios where the adversary can directly update the inputs to the classifier.\nThis could happen by intercepting and modifying the inputs to an online API\nsuch as Clarifai or Cloud Vision. Such limitations have led to a vein of\nresearch around physical attacks where objects are constructed to be inherently\nadversarial or adversarial modifications are added to cause misclassification.\nOur work differs from other physical attacks in that we can cause\nmisclassification dynamically without altering physical objects in a permanent\nway.\n  We construct an experimental setup which includes a light projection source,\nan object for classification, and a camera to capture the scene. Experiments\nare conducted against 2D and 3D objects from CIFAR-10. Initial tests show\nprojected light patterns selected via differential evolution could degrade\nclassification from 98% to 22% and 89% to 43% probability for 2D and 3D targets\nrespectively. Subsequent experiments explore sensitivity to physical setup and\ncompare two additional baseline conditions for all 10 CIFAR classes. Some\nphysical targets are more susceptible to perturbation. Simple attacks show near\nequivalent success, and 6 of the 10 classes were disrupted by light. \n\n"}
{"id": "1810.10358", "contents": "Title: Implicit Modeling with Uncertainty Estimation for Intravoxel Incoherent\n  Motion Imaging Abstract: Intravoxel incoherent motion (IVIM) imaging allows contrast-agent free in\nvivo perfusion quantification with magnetic resonance imaging (MRI). However,\nits use is limited by typically low accuracy due to low signal-to-noise ratio\n(SNR) at large gradient encoding magnitudes as well as dephasing artefacts\ncaused by subject motion, which is particularly challenging in fetal MRI. To\nmitigate this problem, we propose an implicit IVIM signal acquisition model\nwith which we learn full posterior distribution of perfusion parameters using\nartificial neural networks. This posterior then encapsulates the uncertainty of\nthe inferred parameter estimates, which we validate herein via numerical\nexperiments with rejection-based Bayesian sampling. Compared to\nstate-of-the-art IVIM estimation method of segmented least-squares fitting, our\nproposed approach improves parameter estimation accuracy by 65% on synthetic\nanisotropic perfusion data. On paired rescans of in vivo fetal MRI, our method\nincreases repeatability of parameter estimation in placenta by 46%. \n\n"}
{"id": "1810.10667", "contents": "Title: Truncated Back-propagation for Bilevel Optimization Abstract: Bilevel optimization has been recently revisited for designing and analyzing\nalgorithms in hyperparameter tuning and meta learning tasks. However, due to\nits nested structure, evaluating exact gradients for high-dimensional problems\nis computationally challenging. One heuristic to circumvent this difficulty is\nto use the approximate gradient given by performing truncated back-propagation\nthrough the iterative optimization procedure that solves the lower-level\nproblem. Although promising empirical performance has been reported, its\ntheoretical properties are still unclear. In this paper, we analyze the\nproperties of this family of approximate gradients and establish sufficient\nconditions for convergence. We validate this on several hyperparameter tuning\nand meta learning tasks. We find that optimization with the approximate\ngradient computed using few-step back-propagation often performs comparably to\noptimization with the exact gradient, while requiring far less memory and half\nthe computation time. \n\n"}
{"id": "1810.10690", "contents": "Title: SpiderBoost and Momentum: Faster Stochastic Variance Reduction\n  Algorithms Abstract: SARAH and SPIDER are two recently developed stochastic variance-reduced\nalgorithms, and SPIDER has been shown to achieve a near-optimal first-order\noracle complexity in smooth nonconvex optimization. However, SPIDER uses an\naccuracy-dependent stepsize that slows down the convergence in practice, and\ncannot handle objective functions that involve nonsmooth regularizers. In this\npaper, we propose SpiderBoost as an improved scheme, which allows to use a much\nlarger constant-level stepsize while maintaining the same near-optimal oracle\ncomplexity, and can be extended with proximal mapping to handle composite\noptimization (which is nonsmooth and nonconvex) with provable convergence\nguarantee. In particular, we show that proximal SpiderBoost achieves an oracle\ncomplexity of $\\mathcal{O}(\\min\\{n^{1/2}\\epsilon^{-2},\\epsilon^{-3}\\})$ in\ncomposite nonconvex optimization, improving the state-of-the-art result by a\nfactor of $\\mathcal{O}(\\min\\{n^{1/6},\\epsilon^{-1/3}\\})$. We further develop a\nnovel momentum scheme to accelerate SpiderBoost for composite optimization,\nwhich achieves the near-optimal oracle complexity in theory and substantial\nimprovement in experiments. \n\n"}
{"id": "1810.11317", "contents": "Title: Superensemble Classifier for Improving Predictions in Imbalanced\n  Datasets Abstract: Learning from an imbalanced dataset is a tricky proposition. Because these\ndatasets are biased towards one class, most existing classifiers tend not to\nperform well on minority class examples. Conventional classifiers usually aim\nto optimize the overall accuracy without considering the relative distribution\nof each class. This article presents a superensemble classifier, to tackle and\nimprove predictions in imbalanced classification problems, that maps Hellinger\ndistance decision trees (HDDT) into radial basis function network (RBFN)\nframework. Regularity conditions for universal consistency and the idea of\nparameter optimization of the proposed model are provided. The proposed\ndistribution-free model can be applied for feature selection cum imbalanced\nclassification problems. We have also provided enough numerical evidence using\nvarious real-life data sets to assess the performance of the proposed model.\nIts effectiveness and competitiveness with respect to different\nstate-of-the-art models are shown. \n\n"}
{"id": "1810.11367", "contents": "Title: LAMVI-2: A Visual Tool for Comparing and Tuning Word Embedding Models Abstract: Tuning machine learning models, particularly deep learning architectures, is\na complex process. Automated hyperparameter tuning algorithms often depend on\nspecific optimization metrics. However, in many situations, a developer trades\none metric against another: accuracy versus overfitting, precision versus\nrecall, smaller models and accuracy, etc. With deep learning, not only are the\nmodel's representations opaque, the model's behavior when parameters \"knobs\"\nare changed may also be unpredictable. Thus, picking the \"best\" model often\nrequires time-consuming model comparison. In this work, we introduce LAMVI-2, a\nvisual analytics system to support a developer in comparing hyperparameter\nsettings and outcomes. By focusing on word-embedding models (\"deep learning for\ntext\") we integrate views to compare both high-level statistics as well as\ninternal model behaviors (e.g., comparing word 'distances'). We demonstrate how\ndevelopers can work with LAMVI-2 to more quickly and accurately narrow down an\nappropriate and effective application-specific model. \n\n"}
{"id": "1810.11586", "contents": "Title: Attended Temperature Scaling: A Practical Approach for Calibrating Deep\n  Neural Networks Abstract: Recently, Deep Neural Networks (DNNs) have been achieving impressive results\non wide range of tasks. However, they suffer from being well-calibrated. In\ndecision-making applications, such as autonomous driving or medical diagnosing,\nthe confidence of deep networks plays an important role to bring the trust and\nreliability to the system. To calibrate the deep networks' confidence, many\nprobabilistic and measure-based approaches are proposed. Temperature Scaling\n(TS) is a state-of-the-art among measure-based calibration methods which has\nlow time and memory complexity as well as effectiveness. In this paper, we\nstudy TS and show it does not work properly when the validation set that TS\nuses for calibration has small size or contains noisy-labeled samples. TS also\ncannot calibrate highly accurate networks as well as non-highly accurate ones.\nAccordingly, we propose Attended Temperature Scaling (ATS) which preserves the\nadvantages of TS while improves calibration in aforementioned challenging\nsituations. We provide theoretical justifications for ATS and assess its\neffectiveness on wide range of deep models and datasets. We also compare the\ncalibration results of TS and ATS on skin lesion detection application as a\npractical problem where well-calibrated system can play important role in\nmaking a decision. \n\n"}
{"id": "1810.11730", "contents": "Title: Low-shot Learning via Covariance-Preserving Adversarial Augmentation\n  Networks Abstract: Deep neural networks suffer from over-fitting and catastrophic forgetting\nwhen trained with small data. One natural remedy for this problem is data\naugmentation, which has been recently shown to be effective. However, previous\nworks either assume that intra-class variances can always be generalized to new\nclasses, or employ naive generation methods to hallucinate finite examples\nwithout modeling their latent distributions. In this work, we propose\nCovariance-Preserving Adversarial Augmentation Networks to overcome existing\nlimits of low-shot learning. Specifically, a novel Generative Adversarial\nNetwork is designed to model the latent distribution of each novel class given\nits related base counterparts. Since direct estimation of novel classes can be\ninductively biased, we explicitly preserve covariance information as the\n`variability' of base examples during the generation process. Empirical results\nshow that our model can generate realistic yet diverse examples, leading to\nsubstantial improvements on the ImageNet benchmark over the state of the art. \n\n"}
{"id": "1810.11740", "contents": "Title: A Convex Duality Framework for GANs Abstract: Generative adversarial network (GAN) is a minimax game between a generator\nmimicking the true model and a discriminator distinguishing the samples\nproduced by the generator from the real training samples. Given an\nunconstrained discriminator able to approximate any function, this game reduces\nto finding the generative model minimizing a divergence measure, e.g. the\nJensen-Shannon (JS) divergence, to the data distribution. However, in practice\nthe discriminator is constrained to be in a smaller class $\\mathcal{F}$ such as\nneural nets. Then, a natural question is how the divergence minimization\ninterpretation changes as we constrain $\\mathcal{F}$. In this work, we address\nthis question by developing a convex duality framework for analyzing GANs. For\na convex set $\\mathcal{F}$, this duality framework interprets the original GAN\nformulation as finding the generative model with minimum JS-divergence to the\ndistributions penalized to match the moments of the data distribution, with the\nmoments specified by the discriminators in $\\mathcal{F}$. We show that this\ninterpretation more generally holds for f-GAN and Wasserstein GAN. As a\nbyproduct, we apply the duality framework to a hybrid of f-divergence and\nWasserstein distance. Unlike the f-divergence, we prove that the proposed\nhybrid divergence changes continuously with the generative model, which\nsuggests regularizing the discriminator's Lipschitz constant in f-GAN and\nvanilla GAN. We numerically evaluate the power of the suggested regularization\nschemes for improving GAN's training performance. \n\n"}
{"id": "1810.11948", "contents": "Title: Parallel Entangling Operations on a Universal Ion Trap Quantum Computer Abstract: The circuit model of a quantum computer consists of sequences of gate\noperations between quantum bits (qubits), drawn from a universal family of\ndiscrete operations. The ability to execute parallel entangling quantum gates\noffers clear efficiency gains in numerous quantum circuits as well as for\nentire algorithms such as Shor's factoring algorithm and quantum simulations.\nIn cases such as full adders and multiple-control Toffoli gates, parallelism\ncan provide an exponential improvement in overall execution time. More\nimportantly, quantum gate parallelism is essential for the practical\nfault-tolerant error correction of qubits that suffer from idle errors. The\nimplementation of parallel quantum gates is complicated by potential crosstalk,\nespecially between qubits fully connected by a common-mode bus, such as in\nCoulomb-coupled trapped atomic ions or cavity-coupled superconducting\ntransmons. Here, we present the first experimental results for parallel 2-qubit\nentangling gates in an array of fully-connected trapped ion qubits. We\ndemonstrate an application of this capability by performing a 1-bit full\naddition operation on a quantum computer using a depth-4 quantum circuit. These\nresults exploit the power of highly connected qubit systems through classical\ncontrol techniques, and provide an advance toward speeding up quantum circuits\nand achieving fault tolerance with trapped ion quantum computers. \n\n"}
{"id": "1810.12091", "contents": "Title: Embedding Geographic Locations for Modelling the Natural Environment\n  using Flickr Tags and Structured Data Abstract: Meta-data from photo-sharing websites such as Flickr can be used to obtain\nrich bag-of-words descriptions of geographic locations, which have proven\nvaluable, among others, for modelling and predicting ecological features. One\nimportant insight from previous work is that the descriptions obtained from\nFlickr tend to be complementary to the structured information that is available\nfrom traditional scientific resources. To better integrate these two diverse\nsources of information, in this paper we consider a method for learning vector\nspace embeddings of geographic locations. We show experimentally that this\nmethod improves on existing approaches, especially in cases where structured\ninformation is available. \n\n"}
{"id": "1810.12165", "contents": "Title: Median activation functions for graph neural networks Abstract: Graph neural networks (GNNs) have been shown to replicate convolutional\nneural networks' (CNNs) superior performance in many problems involving graphs.\nBy replacing regular convolutions with linear shift-invariant graph filters\n(LSI-GFs), GNNs take into account the (irregular) structure of the graph and\nprovide meaningful representations of network data. However, LSI-GFs fail to\nencode local nonlinear graph signal behavior, and so do regular activation\nfunctions, which are nonlinear but pointwise. To address this issue, we propose\nmedian activation functions with support on graph neighborhoods instead of\nindividual nodes. A GNN architecture with a trainable multirresolution version\nof this activation function is then tested on synthetic and real-word datasets,\nwhere we show that median activation functions can improve GNN capacity with\nmarginal increase in complexity. \n\n"}
{"id": "1810.12482", "contents": "Title: Using Large Ensembles of Control Variates for Variational Inference Abstract: Variational inference is increasingly being addressed with stochastic\noptimization. In this setting, the gradient's variance plays a crucial role in\nthe optimization procedure, since high variance gradients lead to poor\nconvergence. A popular approach used to reduce gradient's variance involves the\nuse of control variates. Despite the good results obtained, control variates\ndeveloped for variational inference are typically looked at in isolation. In\nthis paper we clarify the large number of control variates that are available\nby giving a systematic view of how they are derived. We also present a Bayesian\nrisk minimization framework in which the quality of a procedure for combining\ncontrol variates is quantified by its effect on optimization convergence rates,\nwhich leads to a very simple combination rule. Results show that combining a\nlarge number of control variates this way significantly improves the\nconvergence of inference over using the typical gradient estimators or a\nreduced number of control variates. \n\n"}
{"id": "1810.12770", "contents": "Title: Explicit Feedbacks Meet with Implicit Feedbacks : A Combined Approach\n  for Recommendation System Abstract: Recommender systems recommend items more accurately by analyzing users'\npotential interest on different brands' items. In conjunction with users'\nrating similarity, the presence of users' implicit feedbacks like clicking\nitems, viewing items specifications, watching videos etc. have been proved to\nbe helpful for learning users' embedding, that helps better rating prediction\nof users. Most existing recommender systems focus on modeling of ratings and\nimplicit feedbacks ignoring users' explicit feedbacks. Explicit feedbacks can\nbe used to validate the reliability of the particular users and can be used to\nlearn about the users' characteristic. Users' characteristic mean what type of\nreviewers they are. In this paper, we explore three different models for\nrecommendation with more accuracy focusing on users' explicit feedbacks and\nimplicit feedbacks. First one is RHC-PMF that predicts users' rating more\naccurately based on user's three explicit feedbacks (rating, helpfulness score\nand centrality) and second one is RV-PMF, where user's implicit feedback (view\nrelationship) is considered. Last one is RHCV-PMF, where both type of feedbacks\nare considered. In this model users' explicit feedbacks' similarity indicate\nthe similarity of their reliability and characteristic and implicit feedback's\nsimilarity indicates their preference similarity. Extensive experiments on real\nworld dataset, i.e. Amazon.com online review dataset shows that our models\nperform better compare to base-line models in term of users' rating prediction.\nRHCV-PMF model also performs better rating prediction compare to baseline\nmodels for cold start users and cold start items. \n\n"}
{"id": "1810.12856", "contents": "Title: Discovering state-parameter mappings in subsurface models using\n  generative adversarial networks Abstract: A fundamental problem in geophysical modeling is related to the\nidentification and approximation of causal structures among physical processes.\nHowever, resolving the bidirectional mappings between physical parameters and\nmodel state variables (i.e., solving the forward and inverse problems) is\nchallenging, especially when parameter dimensionality is high. Deep learning\nhas opened a new door toward knowledge representation and complex pattern\nidentification. In particular, the recently introduced generative adversarial\nnetworks (GANs) hold strong promises in learning cross-domain mappings for\nimage translation. This study presents a state-parameter identification GAN\n(SPID-GAN) for simultaneously learning bidirectional mappings between a\nhigh-dimensional parameter space and the corresponding model state space.\nSPID-GAN is demonstrated using a series of representative problems from\nsubsurface flow modeling. Results show that SPID-GAN achieves satisfactory\nperformance in identifying the bidirectional state-parameter mappings,\nproviding a new deep-learning-based, knowledge representation paradigm for a\nwide array of complex geophysical problems. \n\n"}
{"id": "1810.13118", "contents": "Title: SplineNets: Continuous Neural Decision Graphs Abstract: We present SplineNets, a practical and novel approach for using conditioning\nin convolutional neural networks (CNNs). SplineNets are continuous\ngeneralizations of neural decision graphs, and they can dramatically reduce\nruntime complexity and computation costs of CNNs, while maintaining or even\nincreasing accuracy. Functions of SplineNets are both dynamic (i.e.,\nconditioned on the input) and hierarchical (i.e., conditioned on the\ncomputational path). SplineNets employ a unified loss function with a desired\nlevel of smoothness over both the network and decision parameters, while\nallowing for sparse activation of a subset of nodes for individual samples. In\nparticular, we embed infinitely many function weights (e.g. filters) on smooth,\nlow dimensional manifolds parameterized by compact B-splines, which are indexed\nby a position parameter. Instead of sampling from a categorical distribution to\npick a branch, samples choose a continuous position to pick a function weight.\nWe further show that by maximizing the mutual information between spline\npositions and class labels, the network can be optimally utilized and\nspecialized for classification tasks. Experiments show that our approach can\nsignificantly increase the accuracy of ResNets with negligible cost in speed,\nmatching the precision of a 110 level ResNet with a 32 level SplineNet. \n\n"}
{"id": "1810.13243", "contents": "Title: A Closer Look at Deep Learning Heuristics: Learning rate restarts,\n  Warmup and Distillation Abstract: The convergence rate and final performance of common deep learning models\nhave significantly benefited from heuristics such as learning rate schedules,\nknowledge distillation, skip connections, and normalization layers. In the\nabsence of theoretical underpinnings, controlled experiments aimed at\nexplaining these strategies can aid our understanding of deep learning\nlandscapes and the training dynamics. Existing approaches for empirical\nanalysis rely on tools of linear interpolation and visualizations with\ndimensionality reduction, each with their limitations. Instead, we revisit such\nanalysis of heuristics through the lens of recently proposed methods for loss\nsurface and representation analysis, viz., mode connectivity and canonical\ncorrelation analysis (CCA), and hypothesize reasons for the success of the\nheuristics. In particular, we explore knowledge distillation and learning rate\nheuristics of (cosine) restarts and warmup using mode connectivity and CCA. Our\nempirical analysis suggests that: (a) the reasons often quoted for the success\nof cosine annealing are not evidenced in practice; (b) that the effect of\nlearning rate warmup is to prevent the deeper layers from creating training\ninstability; and (c) that the latent knowledge shared by the teacher is\nprimarily disbursed to the deeper layers. \n\n"}
{"id": "1811.00075", "contents": "Title: The UEA multivariate time series classification archive, 2018 Abstract: In 2002, the UCR time series classification archive was first released with\nsixteen datasets. It gradually expanded, until 2015 when it increased in size\nfrom 45 datasets to 85 datasets. In October 2018 more datasets were added,\nbringing the total to 128. The new archive contains a wide range of problems,\nincluding variable length series, but it still only contains univariate time\nseries classification problems. One of the motivations for introducing the\narchive was to encourage researchers to perform a more rigorous evaluation of\nnewly proposed time series classification (TSC) algorithms. It has worked: most\nrecent research into TSC uses all 85 datasets to evaluate algorithmic advances.\nResearch into multivariate time series classification, where more than one\nseries are associated with each class label, is in a position where univariate\nTSC research was a decade ago. Algorithms are evaluated using very few datasets\nand claims of improvement are not based on statistical comparisons. We aim to\naddress this problem by forming the first iteration of the MTSC archive, to be\nhosted at the website www.timeseriesclassification.com. Like the univariate\narchive, this formulation was a collaborative effort between researchers at the\nUniversity of East Anglia (UEA) and the University of California, Riverside\n(UCR). The 2018 vintage consists of 30 datasets with a wide range of cases,\ndimensions and series lengths. For this first iteration of the archive we\nformat all data to be of equal length, include no series with missing data and\nprovide train/test splits. \n\n"}
{"id": "1811.00152", "contents": "Title: Mixture Density Generative Adversarial Networks Abstract: Generative Adversarial Networks have surprising ability for generating sharp\nand realistic images, though they are known to suffer from the so-called mode\ncollapse problem. In this paper, we propose a new GAN variant called Mixture\nDensity GAN that while being capable of generating high-quality images,\novercomes this problem by encouraging the Discriminator to form clusters in its\nembedding space, which in turn leads the Generator to exploit these and\ndiscover different modes in the data. This is achieved by positioning Gaussian\ndensity functions in the corners of a simplex, using the resulting Gaussian\nmixture as a likelihood function over discriminator embeddings, and formulating\nan objective function for GAN training that is based on these likelihoods. We\ndemonstrate empirically (1) the quality of the generated images in Mixture\nDensity GAN and their strong similarity to real images, as measured by the\nFr\\'echet Inception Distance (FID), which compares very favourably with\nstate-of-the-art methods, and (2) the ability to avoid mode collapse and\ndiscover all data modes. \n\n"}
{"id": "1811.00429", "contents": "Title: Temporal Regularization in Markov Decision Process Abstract: Several applications of Reinforcement Learning suffer from instability due to\nhigh variance. This is especially prevalent in high dimensional domains.\nRegularization is a commonly used technique in machine learning to reduce\nvariance, at the cost of introducing some bias. Most existing regularization\ntechniques focus on spatial (perceptual) regularization. Yet in reinforcement\nlearning, due to the nature of the Bellman equation, there is an opportunity to\nalso exploit temporal regularization based on smoothness in value estimates\nover trajectories. This paper explores a class of methods for temporal\nregularization. We formally characterize the bias induced by this technique\nusing Markov chain concepts. We illustrate the various characteristics of\ntemporal regularization via a sequence of simple discrete and continuous MDPs,\nand show that the technique provides improvement even in high-dimensional Atari\ngames. \n\n"}
{"id": "1811.00513", "contents": "Title: Auditing Data Provenance in Text-Generation Models Abstract: To help enforce data-protection regulations such as GDPR and detect\nunauthorized uses of personal data, we develop a new \\emph{model auditing}\ntechnique that helps users check if their data was used to train a machine\nlearning model. We focus on auditing deep-learning models that generate\nnatural-language text, including word prediction and dialog generation. These\nmodels are at the core of popular online services and are often trained on\npersonal data such as users' messages, searches, chats, and comments.\n  We design and evaluate a black-box auditing method that can detect, with very\nfew queries to a model, if a particular user's texts were used to train it\n(among thousands of other users). We empirically show that our method can\nsuccessfully audit well-generalized models that are not overfitted to the\ntraining data. We also analyze how text-generation models memorize word\nsequences and explain why this memorization makes them amenable to auditing. \n\n"}
{"id": "1811.00703", "contents": "Title: Learning Latent Fractional dynamics with Unknown Unknowns Abstract: Despite significant effort in understanding complex systems (CS), we lack a\ntheory for modeling, inference, analysis and efficient control of time-varying\ncomplex networks (TVCNs) in uncertain environments. From brain activity\ndynamics to microbiome, and even chromatin interactions within the genome\narchitecture, many such TVCNs exhibits a pronounced spatio-temporal fractality.\nMoreover, for many TVCNs only limited information (e.g., few variables) is\naccessible for modeling, which hampers the capabilities of analytical tools to\nuncover the true degrees of freedom and infer the CS model, the hidden states\nand their parameters. Another fundamental limitation is that of understanding\nand unveiling of unknown drivers of the dynamics that could sporadically excite\nthe network in ways that straightforward modeling does not work due to our\ninability to model non-stationary processes. Towards addressing these\nchallenges, in this paper, we consider the problem of learning the fractional\ndynamical complex networks under unknown unknowns (i.e., hidden drivers) and\npartial observability (i.e., only partial data is available). More precisely,\nwe consider a generalized modeling approach of TVCNs consisting of\ndiscrete-time fractional dynamical equations and propose an iterative framework\nto determine the network parameterization and predict the state of the system.\nWe showcase the performance of the proposed framework in the context of task\nclassification using real electroencephalogram data. \n\n"}
{"id": "1811.00836", "contents": "Title: Multi-Kernel Regression with Sparsity Constraint Abstract: In this paper, we provide a Banach-space formulation of supervised learning\nwith generalized total-variation (gTV) regularization. We identify the class of\nkernel functions that are admissible in this framework. Then, we propose a\nvariation of supervised learning in a continuous-domain hybrid search space\nwith gTV regularization. We show that the solution admits a multi-kernel\nexpansion with adaptive positions. In this representation, the number of active\nkernels is upper-bounded by the number of data points while the gTV\nregularization imposes an $\\ell_1$ penalty on the kernel coefficients. Finally,\nwe illustrate numerically the outcome of our theory. \n\n"}
{"id": "1811.00944", "contents": "Title: Spectral Methods from Tensor Networks Abstract: A tensor network is a diagram that specifies a way to \"multiply\" a collection\nof tensors together to produce another tensor (or matrix). Many existing\nalgorithms for tensor problems (such as tensor decomposition and tensor PCA),\nalthough they are not presented this way, can be viewed as spectral methods on\nmatrices built from simple tensor networks. In this work we leverage the full\npower of this abstraction to design new algorithms for certain continuous\ntensor decomposition problems.\n  An important and challenging family of tensor problems comes from orbit\nrecovery, a class of inference problems involving group actions (inspired by\napplications such as cryo-electron microscopy). Orbit recovery problems over\nfinite groups can often be solved via standard tensor methods. However, for\ninfinite groups, no general algorithms are known. We give a new spectral\nalgorithm based on tensor networks for one such problem: continuous\nmulti-reference alignment over the infinite group SO(2). Our algorithm extends\nto the more general heterogeneous case. \n\n"}
{"id": "1811.00972", "contents": "Title: Clustering and Learning from Imbalanced Data Abstract: A learning classifier must outperform a trivial solution, in case of\nimbalanced data, this condition usually does not hold true. To overcome this\nproblem, we propose a novel data level resampling method - Clustering Based\nOversampling for improved learning from class imbalanced datasets. The\nessential idea behind the proposed method is to use the distance between a\nminority class sample and its respective cluster centroid to infer the number\nof new sample points to be generated for that minority class sample. The\nproposed algorithm has very less dependence on the technique used for finding\ncluster centroids and does not effect the majority class learning in any way.\nIt also improves learning from imbalanced data by incorporating the\ndistribution structure of minority class samples in generation of new data\nsamples. The newly generated minority class data is handled in a way as to\nprevent outlier production and overfitting. Implementation analysis on\ndifferent datasets using deep neural networks as the learning classifier shows\nthe effectiveness of this method as compared to other synthetic data resampling\ntechniques across several evaluation metrics. \n\n"}
{"id": "1811.00986", "contents": "Title: Anomaly Detection for imbalanced datasets with Deep Generative Models Abstract: Many important data analysis applications present with severely imbalanced\ndatasets with respect to the target variable. A typical example is medical\nimage analysis, where positive samples are scarce, while performance is\ncommonly estimated against the correct detection of these positive examples. We\napproach this challenge by formulating the problem as anomaly detection with\ngenerative models. We train a generative model without supervision on the\n`negative' (common) datapoints and use this model to estimate the likelihood of\nunseen data. A successful model allows us to detect the `positive' case as low\nlikelihood datapoints.\n  In this position paper, we present the use of state-of-the-art deep\ngenerative models (GAN and VAE) for the estimation of a likelihood of the data.\nOur results show that on the one hand both GANs and VAEs are able to separate\nthe `positive' and `negative' samples in the MNIST case. On the other hand, for\nthe NLST case, neither GANs nor VAEs were able to capture the complexity of the\ndata and discriminate anomalies at the level that this task requires. These\nresults show that even though there are a number of successes presented in the\nliterature for using generative models in similar applications, there remain\nfurther challenges for broad successful implementation. \n\n"}
{"id": "1811.01158", "contents": "Title: Boosted Sparse and Low-Rank Tensor Regression Abstract: We propose a sparse and low-rank tensor regression model to relate a\nunivariate outcome to a feature tensor, in which each unit-rank tensor from the\nCP decomposition of the coefficient tensor is assumed to be sparse. This\nstructure is both parsimonious and highly interpretable, as it implies that the\noutcome is related to the features through a few distinct pathways, each of\nwhich may only involve subsets of feature dimensions. We take a\ndivide-and-conquer strategy to simplify the task into a set of sparse unit-rank\ntensor regression problems. To make the computation efficient and scalable, for\nthe unit-rank tensor regression, we propose a stagewise estimation procedure to\nefficiently trace out its entire solution path. We show that as the step size\ngoes to zero, the stagewise solution paths converge exactly to those of the\ncorresponding regularized regression. The superior performance of our approach\nis demonstrated on various real-world and synthetic examples. \n\n"}
{"id": "1811.01225", "contents": "Title: CAAD 2018: Powerful None-Access Black-Box Attack Based on Adversarial\n  Transformation Network Abstract: In this paper, we propose an improvement of Adversarial Transformation\nNetworks(ATN) to generate adversarial examples, which can fool white-box models\nand black-box models with a state of the art performance and won the 2rd place\nin the non-target task in CAAD 2018. \n\n"}
{"id": "1811.01466", "contents": "Title: Practical Batch Bayesian Optimization for Less Expensive Functions Abstract: Bayesian optimization (BO) and its batch extensions are successful for\noptimizing expensive black-box functions. However, these traditional BO\napproaches are not yet ideal for optimizing less expensive functions when the\ncomputational cost of BO can dominate the cost of evaluating the blackbox\nfunction. Examples of these less expensive functions are cheap machine learning\nmodels, inexpensive physical experiment through simulators, and acquisition\nfunction optimization in Bayesian optimization. In this paper, we consider a\nbatch BO setting for situations where function evaluations are less expensive.\nOur model is based on a new exploration strategy using geometric distance that\nprovides an alternative way for exploration, selecting a point far from the\nobserved locations. Using that intuition, we propose to use Sobol sequence to\nguide exploration that will get rid of running multiple global optimization\nsteps as used in previous works. Based on the proposed distance exploration, we\npresent an efficient batch BO approach. We demonstrate that our approach\noutperforms other baselines and global optimization methods when the function\nevaluations are less expensive. \n\n"}
{"id": "1811.02132", "contents": "Title: Student's t-Generative Adversarial Networks Abstract: Generative Adversarial Networks (GANs) have a great performance in image\ngeneration, but they need a large scale of data to train the entire framework,\nand often result in nonsensical results. We propose a new method referring to\nconditional GAN, which equipments the latent noise with mixture of Student's\nt-distribution with attention mechanism in addition to class information.\nStudent's t-distribution has long tails that can provide more diversity to the\nlatent noise. Meanwhile, the discriminator in our model implements two tasks\nsimultaneously, judging whether the images come from the true data\ndistribution, and identifying the class of each generated images. The\nparameters of the mixture model can be learned along with those of GANs.\nMoreover, we mathematically prove that any multivariate Student's\nt-distribution can be obtained by a linear transformation of a normal\nmultivariate Student's t-distribution. Experiments comparing the proposed\nmethod with typical GAN, DeliGAN and DCGAN indicate that, our method has a\ngreat performance on generating diverse and legible objects with limited data. \n\n"}
{"id": "1811.02564", "contents": "Title: On exponential convergence of SGD in non-convex over-parametrized\n  learning Abstract: Large over-parametrized models learned via stochastic gradient descent (SGD)\nmethods have become a key element in modern machine learning. Although SGD\nmethods are very effective in practice, most theoretical analyses of SGD\nsuggest slower convergence than what is empirically observed. In our recent\nwork [8] we analyzed how interpolation, common in modern over-parametrized\nlearning, results in exponential convergence of SGD with constant step size for\nconvex loss functions. In this note, we extend those results to a much broader\nnon-convex function class satisfying the Polyak-Lojasiewicz (PL) condition. A\nnumber of important non-convex problems in machine learning, including some\nclasses of neural networks, have been recently shown to satisfy the PL\ncondition. We argue that the PL condition provides a relevant and attractive\nsetting for many machine learning problems, particularly in the\nover-parametrized regime. \n\n"}
{"id": "1811.02659", "contents": "Title: Machine Learning Algorithms for Classification of Microcirculation\n  Images from Septic and Non-Septic Patients Abstract: Sepsis is a life-threatening disease and one of the major causes of death in\nhospitals. Imaging of microcirculatory dysfunction is a promising approach for\nautomated diagnosis of sepsis. We report a machine learning classifier capable\nof distinguishing non-septic and septic images from dark field microcirculation\nvideos of patients. The classifier achieves an accuracy of 89.45%. The area\nunder the receiver operating characteristics of the classifier was 0.92, the\nprecision was 0.92 and the recall was 0.84. Codes representing the learned\nfeature space of trained classifier were visualized using t-SNE embedding and\nwere separable and distinguished between images from critically ill and\nnon-septic patients. Using an unsupervised convolutional autoencoder,\nindependent of the clinical diagnosis, we also report clustering of learned\nfeatures from a compressed representation associated with healthy images and\nthose with microcirculatory dysfunction. The feature space used by our trained\nclassifier to distinguish between images from septic and non-septic patients\nhas potential diagnostic application. \n\n"}
{"id": "1811.02783", "contents": "Title: YASENN: Explaining Neural Networks via Partitioning Activation Sequences Abstract: We introduce a novel approach to feed-forward neural network interpretation\nbased on partitioning the space of sequences of neuron activations. In line\nwith this approach, we propose a model-specific interpretation method, called\nYASENN. Our method inherits many advantages of model-agnostic distillation,\nsuch as an ability to focus on the particular input region and to express an\nexplanation in terms of features different from those observed by a neural\nnetwork. Moreover, examination of distillation error makes the method\napplicable to the problems with low tolerance to interpretation mistakes.\nTechnically, YASENN distills the network with an ensemble of layer-wise\ngradient boosting decision trees and encodes the sequences of neuron\nactivations with leaf indices. The finite number of unique codes induces a\npartitioning of the input space. Each partition may be described in a variety\nof ways, including examination of an interpretable model (e.g. a logistic\nregression or a decision tree) trained to discriminate between objects of those\npartitions. Our experiments provide an intuition behind the method and\ndemonstrate revealed artifacts in neural network decision making. \n\n"}
{"id": "1811.03129", "contents": "Title: Global Optimality in Distributed Low-rank Matrix Factorization Abstract: We study the convergence of a variant of distributed gradient descent (DGD)\non a distributed low-rank matrix approximation problem wherein some\noptimization variables are used for consensus (as in classical DGD) and some\noptimization variables appear only locally at a single node in the network. We\nterm the resulting algorithm DGD+LOCAL. Using algorithmic connections to\ngradient descent and geometric connections to the well-behaved landscape of the\ncentralized low-rank matrix approximation problem, we identify sufficient\nconditions where DGD+LOCAL is guaranteed to converge with exact consensus to a\nglobal minimizer of the original centralized problem. For the distributed\nlow-rank matrix approximation problem, these guarantees are stronger---in terms\nof consensus and optimality---than what appear in the literature for classical\nDGD and more general problems. \n\n"}
{"id": "1811.03402", "contents": "Title: A Survey on Data Collection for Machine Learning: a Big Data -- AI\n  Integration Perspective Abstract: Data collection is a major bottleneck in machine learning and an active\nresearch topic in multiple communities. There are largely two reasons data\ncollection has recently become a critical issue. First, as machine learning is\nbecoming more widely-used, we are seeing new applications that do not\nnecessarily have enough labeled data. Second, unlike traditional machine\nlearning, deep learning techniques automatically generate features, which saves\nfeature engineering costs, but in return may require larger amounts of labeled\ndata. Interestingly, recent research in data collection comes not only from the\nmachine learning, natural language, and computer vision communities, but also\nfrom the data management community due to the importance of handling large\namounts of data. In this survey, we perform a comprehensive study of data\ncollection from a data management point of view. Data collection largely\nconsists of data acquisition, data labeling, and improvement of existing data\nor models. We provide a research landscape of these operations, provide\nguidelines on which technique to use when, and identify interesting research\nchallenges. The integration of machine learning and data management for data\ncollection is part of a larger trend of Big data and Artificial Intelligence\n(AI) integration and opens many opportunities for new research. \n\n"}
{"id": "1811.03508", "contents": "Title: A simple yet effective baseline for non-attributed graph classification Abstract: Graphs are complex objects that do not lend themselves easily to typical\nlearning tasks. Recently, a range of approaches based on graph kernels or graph\nneural networks have been developed for graph classification and for\nrepresentation learning on graphs in general. As the developed methodologies\nbecome more sophisticated, it is important to understand which components of\nthe increasingly complex methods are necessary or most effective.\n  As a first step, we develop a simple yet meaningful graph representation, and\nexplore its effectiveness in graph classification. We test our baseline\nrepresentation for the graph classification task on a range of graph datasets.\nInterestingly, this simple representation achieves similar performance as the\nstate-of-the-art graph kernels and graph neural networks for non-attributed\ngraph classification. Its performance on classifying attributed graphs is\nslightly weaker as it does not incorporate attributes. However, given its\nsimplicity and efficiency, we believe that it still serves as an effective\nbaseline for attributed graph classification. Our graph representation is\nefficient (linear-time) to compute. We also provide a simple connection with\nthe graph neural networks.\n  Note that these observations are only for the task of graph classification\nwhile existing methods are often designed for a broader scope including node\nembedding and link prediction. The results are also likely biased due to the\nlimited amount of benchmark datasets available. Nevertheless, the good\nperformance of our simple baseline calls for the development of new, more\ncomprehensive benchmark datasets so as to better evaluate and analyze different\ngraph learning methods. Furthermore, given the computational efficiency of our\ngraph summary, we believe that it is a good candidate as a baseline method for\nfuture graph classification (or even other graph learning) studies. \n\n"}
{"id": "1811.03531", "contents": "Title: A Geometric Perspective on the Transferability of Adversarial Directions Abstract: State-of-the-art machine learning models frequently misclassify inputs that\nhave been perturbed in an adversarial manner. Adversarial perturbations\ngenerated for a given input and a specific classifier often seem to be\neffective on other inputs and even different classifiers. In other words,\nadversarial perturbations seem to transfer between different inputs, models,\nand even different neural network architectures. In this work, we show that in\nthe context of linear classifiers and two-layer ReLU networks, there provably\nexist directions that give rise to adversarial perturbations for many\nclassifiers and data points simultaneously. We show that these \"transferable\nadversarial directions\" are guaranteed to exist for linear separators of a\ngiven set, and will exist with high probability for linear classifiers trained\non independent sets drawn from the same distribution. We extend our results to\nlarge classes of two-layer ReLU networks. We further show that adversarial\ndirections for ReLU networks transfer to linear classifiers while the reverse\nneed not hold, suggesting that adversarial perturbations for more complex\nmodels are more likely to transfer to other classifiers. We validate our\nfindings empirically, even for deeper ReLU networks. \n\n"}
{"id": "1811.03568", "contents": "Title: A Geometric Approach of Gradient Descent Algorithms in Linear Neural\n  Networks Abstract: In this paper, we propose a geometric framework to analyze the convergence\nproperties of gradient descent trajectories in the context of linear neural\nnetworks. We translate a well-known empirical observation of linear neural nets\ninto a conjecture that we call the \\emph{overfitting conjecture} which states\nthat, for almost all training data and initial conditions, the trajectory of\nthe corresponding gradient descent system converges to a global minimum. This\nwould imply that the solution achieved by vanilla gradient descent algorithms\nis equivalent to that of the least-squares estimation, for linear neural\nnetworks of an arbitrary number of hidden layers. Built upon a key invariance\nproperty induced by the network structure, we first establish convergence of\ngradient descent trajectories to critical points of the square loss function in\nthe case of linear networks of arbitrary depth. Our second result is the proof\nof the \\emph{overfitting conjecture} in the case of single-hidden-layer linear\nnetworks with an argument based on the notion of normal hyperbolicity and under\na generic property on the training data (i.e., holding for almost all training\ndata). \n\n"}
{"id": "1811.03717", "contents": "Title: Fast determinantal point processes via distortion-free intermediate\n  sampling Abstract: Given a fixed $n\\times d$ matrix $\\mathbf{X}$, where $n\\gg d$, we study the\ncomplexity of sampling from a distribution over all subsets of rows where the\nprobability of a subset is proportional to the squared volume of the\nparallelepiped spanned by the rows (a.k.a. a determinantal point process). In\nthis task, it is important to minimize the preprocessing cost of the procedure\n(performed once) as well as the sampling cost (performed repeatedly). To that\nend, we propose a new determinantal point process algorithm which has the\nfollowing two properties, both of which are novel: (1) a preprocessing step\nwhich runs in time $O(\\text{number-of-non-zeros}(\\mathbf{X})\\cdot\\log\nn)+\\text{poly}(d)$, and (2) a sampling step which runs in $\\text{poly}(d)$\ntime, independent of the number of rows $n$. We achieve this by introducing a\nnew regularized determinantal point process (R-DPP), which serves as an\nintermediate distribution in the sampling procedure by reducing the number of\nrows from $n$ to $\\text{poly}(d)$. Crucially, this intermediate distribution\ndoes not distort the probabilities of the target sample. Our key novelty in\ndefining the R-DPP is the use of a Poisson random variable for controlling the\nprobabilities of different subset sizes, leading to new determinantal formulas\nsuch as the normalization constant for this distribution. Our algorithm has\napplications in many diverse areas where determinantal point processes have\nbeen used, such as machine learning, stochastic optimization, data\nsummarization and low-rank matrix reconstruction. \n\n"}
{"id": "1811.03897", "contents": "Title: Deep Ensemble Bayesian Active Learning : Addressing the Mode Collapse\n  issue in Monte Carlo dropout via Ensembles Abstract: In image classification tasks, the ability of deep CNNs to deal with complex\nimage data has proven to be unrivalled. However, they require large amounts of\nlabeled training data to reach their full potential. In specialised domains\nsuch as healthcare, labeled data can be difficult and expensive to obtain.\nActive Learning aims to alleviate this problem, by reducing the amount of\nlabelled data needed for a specific task while delivering satisfactory\nperformance. We propose DEBAL, a new active learning strategy designed for deep\nneural networks. This method improves upon the current state-of-the-art deep\nBayesian active learning method, which suffers from the mode collapse problem.\nWe correct for this deficiency by making use of the expressive power and\nstatistical properties of model ensembles. Our proposed method manages to\ncapture superior data uncertainty, which translates into improved\nclassification performance. We demonstrate empirically that our ensemble method\nyields faster convergence of CNNs trained on the MNIST and CIFAR-10 datasets. \n\n"}
{"id": "1811.03963", "contents": "Title: Deep Compression of Sum-Product Networks on Tensor Networks Abstract: Sum-product networks (SPNs) represent an emerging class of neural networks\nwith clear probabilistic semantics and superior inference speed over graphical\nmodels. This work reveals a strikingly intimate connection between SPNs and\ntensor networks, thus leading to a highly efficient representation that we call\ntensor SPNs (tSPNs). For the first time, through mapping an SPN onto a tSPN and\nemploying novel optimization techniques, we demonstrate remarkable parameter\ncompression with negligible loss in accuracy. \n\n"}
{"id": "1811.04151", "contents": "Title: Design Rule Violation Hotspot Prediction Based on Neural Network\n  Ensembles Abstract: Design rule check is a critical step in the physical design of integrated\ncircuits to ensure manufacturability. However, it can be done only after a\ntime-consuming detailed routing procedure, which adds drastically to the time\nof design iterations. With advanced technology nodes, the outcomes of global\nrouting and detailed routing become less correlated, which adds to the\ndifficulty of predicting design rule violations from earlier stages. In this\npaper, a framework based on neural network ensembles is proposed to predict\ndesign rule violation hotspots using information from placement and global\nrouting. A soft voting structure and a PCA-based subset selection scheme are\ndeveloped on top of a baseline neural network from a recent work. Experimental\nresults show that the proposed architecture achieves significant improvement in\nmodel performance compared to the baseline case. For half of test cases, the\nperformance is even better than random forest, a commonly-used ensemble\nlearning model. \n\n"}
{"id": "1811.04351", "contents": "Title: Generalization Bounds for Vicinal Risk Minimization Principle Abstract: The vicinal risk minimization (VRM) principle, first proposed by\n\\citet{vapnik1999nature}, is an empirical risk minimization (ERM) variant that\nreplaces Dirac masses with vicinal functions. Although there is strong\nnumerical evidence showing that VRM outperforms ERM if appropriate vicinal\nfunctions are chosen, a comprehensive theoretical understanding of VRM is still\nlacking. In this paper, we study the generalization bounds for VRM. Our results\nsupport Vapnik's original arguments and additionally provide deeper insights\ninto VRM. First, we prove that the complexity of function classes convolving\nwith vicinal functions can be controlled by that of the original function\nclasses under the assumption that the function class is composed of\nLipschitz-continuous functions. Then, the resulting generalization bounds for\nVRM suggest that the generalization performance of VRM is also effected by the\nchoice of vicinity function and the quality of function classes. These findings\ncan be used to examine whether the choice of vicinal function is appropriate\nfor the VRM-based learning setting. Finally, we provide a theoretical\nexplanation for existing VRM models, e.g., uniform distribution-based models,\nGaussian distribution-based models, and mixup models. \n\n"}
{"id": "1811.04770", "contents": "Title: Packing Sparse Convolutional Neural Networks for Efficient Systolic\n  Array Implementations: Column Combining Under Joint Optimization Abstract: This paper describes a novel approach of packing sparse convolutional neural\nnetworks for their efficient systolic array implementations. By combining\nsubsets of columns in the original filter matrix associated with a\nconvolutional layer, we increase the utilization efficiency of the systolic\narray substantially (e.g., ~4x) due to the increased density of nonzeros in the\nresulting packed filter matrix. In combining columns, for each row, all filter\nweights but one with the largest magnitude are pruned. We retrain the remaining\nweights to preserve high accuracy. We demonstrate that in mitigating data\nprivacy concerns the retraining can be accomplished with only fractions of the\noriginal dataset (e.g., 10\\% for CIFAR-10). We study the effectiveness of this\njoint optimization for both high utilization and classification accuracy with\nASIC and FPGA designs based on efficient bit-serial implementations of\nmultiplier-accumulators. We present analysis and empirical evidence on the\nsuperior performance of our column combining approach against prior arts under\nmetrics such as energy efficiency (3x) and inference latency (12x). \n\n"}
{"id": "1811.04911", "contents": "Title: Boosting Model Performance through Differentially Private Model\n  Aggregation Abstract: A key factor in developing high performing machine learning models is the\navailability of sufficiently large datasets. This work is motivated by\napplications arising in Software as a Service (SaaS) companies where there\nexist numerous similar yet disjoint datasets from multiple client companies. To\novercome the challenges of insufficient data without explicitly aggregating the\nclients' datasets due to privacy concerns, one solution is to collect more data\nfor each individual client, another is to privately aggregate information from\nmodels trained on each client's data. In this work, two approaches for private\nmodel aggregation are proposed that enable the transfer of knowledge from\nexisting models trained on other companies' datasets to a new company with\nlimited labeled data while protecting each client company's underlying\nindividual sensitive information. The two proposed approaches are based on\nstate-of-the-art private learning algorithms: Differentially Private\nPermutation-based Stochastic Gradient Descent and Approximate Minima\nPerturbation. We empirically show that by leveraging differentially private\ntechniques, we can enable private model aggregation and augment data utility\nwhile providing provable mathematical guarantees on privacy. The proposed\nmethods thus provide significant business value for SaaS companies and their\nclients, specifically as a solution for the cold-start problem. \n\n"}
{"id": "1811.04968", "contents": "Title: PennyLane: Automatic differentiation of hybrid quantum-classical\n  computations Abstract: PennyLane is a Python 3 software framework for differentiable programming of\nquantum computers. The library provides a unified architecture for near-term\nquantum computing devices, supporting both qubit and continuous-variable\nparadigms. PennyLane's core feature is the ability to compute gradients of\nvariational quantum circuits in a way that is compatible with classical\ntechniques such as backpropagation. PennyLane thus extends the automatic\ndifferentiation algorithms common in optimization and machine learning to\ninclude quantum and hybrid computations. A plugin system makes the framework\ncompatible with any gate-based quantum simulator or hardware. We provide\nplugins for hardware providers including the Xanadu Cloud, Amazon Braket, and\nIBM Quantum, allowing PennyLane optimizations to be run on publicly accessible\nquantum devices. On the classical front, PennyLane interfaces with accelerated\nmachine learning libraries such as TensorFlow, PyTorch, JAX, and Autograd.\nPennyLane can be used for the optimization of variational quantum eigensolvers,\nquantum approximate optimization, quantum machine learning models, and many\nother applications. \n\n"}
{"id": "1811.05042", "contents": "Title: Exploiting Local Feature Patterns for Unsupervised Domain Adaptation Abstract: Unsupervised domain adaptation methods aim to alleviate performance\ndegradation caused by domain-shift by learning domain-invariant\nrepresentations. Existing deep domain adaptation methods focus on holistic\nfeature alignment by matching source and target holistic feature distributions,\nwithout considering local features and their multi-mode statistics. We show\nthat the learned local feature patterns are more generic and transferable and a\nfurther local feature distribution matching enables fine-grained feature\nalignment. In this paper, we present a method for learning domain-invariant\nlocal feature patterns and jointly aligning holistic and local feature\nstatistics. Comparisons to the state-of-the-art unsupervised domain adaptation\nmethods on two popular benchmark datasets demonstrate the superiority of our\napproach and its effectiveness on alleviating negative transfer. \n\n"}
{"id": "1811.05140", "contents": "Title: Amplitude-Aware Lossy Compression for Quantum Circuit Simulation Abstract: Classical simulation of quantum circuits is crucial for evaluating and\nvalidating the design of new quantum algorithms. However, the number of quantum\nstate amplitudes increases exponentially with the number of qubits, leading to\nthe exponential growth of the memory requirement for the simulations. In this\npaper, we present a new data reduction technique to reduce the memory\nrequirement of quantum circuit simulations. We apply our amplitude-aware lossy\ncompression technique to the quantum state amplitude vector to trade the\ncomputation time and fidelity for memory space. The experimental results show\nthat our simulator only needs 1/16 of the original memory requirement to\nsimulate Quantum Fourier Transform circuits with 99.95% fidelity. The reduction\namount of memory requirement suggests that we could increase 4 qubits in the\nquantum circuit simulation comparing to the simulation without our technique.\nAdditionally, for some specific circuits, like Grover's search, we could\nincrease the simulation size by 18 qubits. \n\n"}
{"id": "1811.05512", "contents": "Title: A domain agnostic measure for monitoring and evaluating GANs Abstract: Generative Adversarial Networks (GANs) have shown remarkable results in\nmodeling complex distributions, but their evaluation remains an unsettled\nissue. Evaluations are essential for: (i) relative assessment of different\nmodels and (ii) monitoring the progress of a single model throughout training.\nThe latter cannot be determined by simply inspecting the generator and\ndiscriminator loss curves as they behave non-intuitively. We leverage the\nnotion of duality gap from game theory to propose a measure that addresses both\n(i) and (ii) at a low computational cost. Extensive experiments show the\neffectiveness of this measure to rank different GAN models and capture the\ntypical GAN failure scenarios, including mode collapse and non-convergent\nbehaviours. This evaluation metric also provides meaningful monitoring on the\nprogression of the loss during training. It highly correlates with FID on\nnatural image datasets, and with domain specific scores for text, sound and\ncosmology data where FID is not directly suitable. In particular, our proposed\nmetric requires no labels or a pretrained classifier, making it domain\nagnostic. \n\n"}
{"id": "1811.05654", "contents": "Title: Sample complexity of partition identification using multi-armed bandits Abstract: Given a vector of probability distributions, or arms, each of which can be\nsampled independently, we consider the problem of identifying the partition to\nwhich this vector belongs from a finitely partitioned universe of such vector\nof distributions. We study this as a pure exploration problem in multi armed\nbandit settings and develop sample complexity bounds on the total mean number\nof samples required for identifying the correct partition with high\nprobability. This framework subsumes well studied problems such as finding the\nbest arm or the best few arms. We consider distributions belonging to the\nsingle parameter exponential family and primarily consider partitions where the\nvector of means of arms lie either in a given set or its complement. The sets\nconsidered correspond to distributions where there exists a mean above a\nspecified threshold, where the set is a half space and where either the set or\nits complement is a polytope, or more generally, a convex set. In these\nsettings, we characterize the lower bounds on mean number of samples for each\narm highlighting their dependence on the problem geometry. Further, inspired by\nthe lower bounds, we propose algorithms that can match these bounds\nasymptotically with decreasing probability of error. Applications of this\nframework may be diverse. We briefly discuss one associated with finance. \n\n"}
{"id": "1811.05695", "contents": "Title: Efficient and Scalable Multi-task Regression on Massive Number of Tasks Abstract: Many real-world large-scale regression problems can be formulated as\nMulti-task Learning (MTL) problems with a massive number of tasks, as in retail\nand transportation domains. However, existing MTL methods still fail to offer\nboth the generalization performance and the scalability for such problems.\nScaling up MTL methods to problems with a tremendous number of tasks is a big\nchallenge. Here, we propose a novel algorithm, named Convex Clustering\nMulti-Task regression Learning (CCMTL), which integrates with convex clustering\non the k-nearest neighbor graph of the prediction models. Further, CCMTL\nefficiently solves the underlying convex problem with a newly proposed\noptimization method. CCMTL is accurate, efficient to train, and empirically\nscales linearly in the number of tasks. On both synthetic and real-world\ndatasets, the proposed CCMTL outperforms seven state-of-the-art (SoA)\nmulti-task learning methods in terms of prediction accuracy as well as\ncomputational efficiency. On a real-world retail dataset with 23,812 tasks,\nCCMTL requires only around 30 seconds to train on a single thread, while the\nSoA methods need up to hours or even days. \n\n"}
{"id": "1811.05852", "contents": "Title: Predicting the time-evolution of multi-physics systems with\n  sequence-to-sequence models Abstract: In this work, sequence-to-sequence (seq2seq) models, originally developed for\nlanguage translation, are used to predict the temporal evolution of complex,\nmulti-physics computer simulations. The predictive performance of seq2seq\nmodels is compared to state transition models for datasets generated with\nmulti-physics codes with varying levels of complexity - from simple 1D\ndiffusion calculations to simulations of inertial confinement fusion\nimplosions. Seq2seq models demonstrate the ability to accurately emulate\ncomplex systems, enabling the rapid estimation of the evolution of quantities\nof interest in computationally expensive simulations. \n\n"}
{"id": "1811.05975", "contents": "Title: Machine Learning Analysis of Heterogeneity in the Effect of Student\n  Mindset Interventions Abstract: We study heterogeneity in the effect of a mindset intervention on\nstudent-level performance through an observational dataset from the National\nStudy of Learning Mindsets (NSLM). Our analysis uses machine learning (ML) to\naddress the following associated problems: assessing treatment group overlap\nand covariate balance, imputing conditional average treatment effects, and\ninterpreting imputed effects. By comparing several different model families we\nillustrate the flexibility of both off-the-shelf and purpose-built estimators.\nWe find that the mindset intervention has a positive average effect of 0.26,\n95%-CI [0.22, 0.30], and that heterogeneity in the range of [0.1, 0.4] is\nmoderated by school-level achievement level, poverty concentration, urbanicity,\nand student prior expectations. \n\n"}
{"id": "1811.06225", "contents": "Title: Reward-estimation variance elimination in sequential decision processes Abstract: Policy gradient methods are very attractive in reinforcement learning due to\ntheir model-free nature and convergence guarantees. These methods, however,\nsuffer from high variance in gradient estimation, resulting in poor sample\nefficiency. To mitigate this issue, a number of variance-reduction approaches\nhave been proposed. Unfortunately, in the challenging problems with delayed\nrewards, these approaches either bring a relatively modest improvement or do\nreduce variance at expense of introducing a bias and undermining convergence.\nThe unbiased methods of gradient estimation, in general, only partially reduce\nvariance, without eliminating it completely even in the limit of exact\nknowledge of the value functions and problem dynamics, as one might have\nwished. In this work we propose an unbiased method that does completely\neliminate variance under some, commonly encountered, conditions. Of practical\ninterest is the limit of deterministic dynamics and small policy stochasticity.\nIn the case of a quadratic value function, as in linear quadratic Gaussian\nmodels, the policy randomness need not be small. We use such a model to analyze\nperformance of the proposed variance-elimination approach and compare it with\nstandard variance-reduction methods. The core idea behind the approach is to\nuse control variates at all future times down the trajectory. We present both a\nmodel-based and model-free formulations. \n\n"}
{"id": "1811.06341", "contents": "Title: Spatio-temporal Stacked LSTM for Temperature Prediction in Weather\n  Forecasting Abstract: Long Short-Term Memory (LSTM) is a well-known method used widely on sequence\nlearning and time series prediction. In this paper we deployed stacked LSTM\nmodel in an application of weather forecasting. We propose a 2-layer\nspatio-temporal stacked LSTM model which consists of independent LSTM models\nper location in the first LSTM layer. Subsequently, the input of the second\nLSTM layer is formed based on the combination of the hidden states of the first\nlayer LSTM models. The experiments show that by utilizing the spatial\ninformation the prediction performance of the stacked LSTM model improves in\nmost of the cases. \n\n"}
{"id": "1811.06609", "contents": "Title: A Spectral View of Adversarially Robust Features Abstract: Given the apparent difficulty of learning models that are robust to\nadversarial perturbations, we propose tackling the simpler problem of\ndeveloping adversarially robust features. Specifically, given a dataset and\nmetric of interest, the goal is to return a function (or multiple functions)\nthat 1) is robust to adversarial perturbations, and 2) has significant\nvariation across the datapoints. We establish strong connections between\nadversarially robust features and a natural spectral property of the geometry\nof the dataset and metric of interest. This connection can be leveraged to\nprovide both robust features, and a lower bound on the robustness of any\nfunction that has significant variance across the dataset. Finally, we provide\nempirical evidence that the adversarially robust features given by this\nspectral approach can be fruitfully leveraged to learn a robust (and accurate)\nmodel. \n\n"}
{"id": "1811.06981", "contents": "Title: Learned Video Compression Abstract: We present a new algorithm for video coding, learned end-to-end for the\nlow-latency mode. In this setting, our approach outperforms all existing video\ncodecs across nearly the entire bitrate range. To our knowledge, this is the\nfirst ML-based method to do so.\n  We evaluate our approach on standard video compression test sets of varying\nresolutions, and benchmark against all mainstream commercial codecs, in the\nlow-latency mode. On standard-definition videos, relative to our algorithm,\nHEVC/H.265, AVC/H.264 and VP9 typically produce codes up to 60% larger. On\nhigh-definition 1080p videos, H.265 and VP9 typically produce codes up to 20%\nlarger, and H.264 up to 35% larger. Furthermore, our approach does not suffer\nfrom blocking artifacts and pixelation, and thus produces videos that are more\nvisually pleasing.\n  We propose two main contributions. The first is a novel architecture for\nvideo compression, which (1) generalizes motion estimation to perform any\nlearned compensation beyond simple translations, (2) rather than strictly\nrelying on previously transmitted reference frames, maintains a state of\narbitrary information learned by the model, and (3) enables jointly compressing\nall transmitted signals (such as optical flow and residual).\n  Secondly, we present a framework for ML-based spatial rate control: namely, a\nmechanism for assigning variable bitrates across space for each frame. This is\na critical component for video coding, which to our knowledge had not been\ndeveloped within a machine learning setting. \n\n"}
{"id": "1811.07023", "contents": "Title: An Infinite Parade of Giraffes: Expressive Augmentation and Complexity\n  Layers for Cartoon Drawing Abstract: In this paper, we explore creative image generation constrained by small\ndata. To partially automate the creation of cartoon sketches consistent with a\nspecific designer's style, where acquiring a very large original image set is\nimpossible or cost prohibitive, we exploit domain specific knowledge for a huge\nreduction in original image requirements, creating an effectively infinite\nnumber of cartoon giraffes from just nine original drawings. We introduce\n\"expressive augmentations\" for cartoon sketches, mathematical transformations\nthat create broad domain appropriate variation, far beyond the usual affine\ntransformations, and we show that chained GANs models trained on the temporal\nstages of drawing or \"complexity layers\" can effectively add character\nappropriate details and finish new drawings in the designer's style.\n  We discuss the application of these tools in design processes for textiles,\ngraphics, architectural elements and interior design. \n\n"}
{"id": "1811.07192", "contents": "Title: The Theory and Algorithm of Ergodic Inference Abstract: Approximate inference algorithm is one of the fundamental research fields in\nmachine learning. The two dominant theoretical inference frameworks in machine\nlearning are variational inference (VI) and Markov chain Monte Carlo (MCMC).\nHowever, because of the fundamental limitation in the theory, it is very\nchallenging to improve existing VI and MCMC methods on both the computational\nscalability and statistical efficiency. To overcome this obstacle, we propose a\nnew theoretical inference framework called ergodic Inference based on the\nfundamental property of ergodic transformations. The key contribution of this\nwork is to establish the theoretical foundation of ergodic inference for the\ndevelopment of practical algorithms in future work. \n\n"}
{"id": "1811.07350", "contents": "Title: Policy Optimization with Model-based Explorations Abstract: Model-free reinforcement learning methods such as the Proximal Policy\nOptimization algorithm (PPO) have successfully applied in complex\ndecision-making problems such as Atari games. However, these methods suffer\nfrom high variances and high sample complexity. On the other hand, model-based\nreinforcement learning methods that learn the transition dynamics are more\nsample efficient, but they often suffer from the bias of the transition\nestimation. How to make use of both model-based and model-free learning is a\ncentral problem in reinforcement learning. In this paper, we present a new\ntechnique to address the trade-off between exploration and exploitation, which\nregards the difference between model-free and model-based estimations as a\nmeasure of exploration value. We apply this new technique to the PPO algorithm\nand arrive at a new policy optimization method, named Policy Optimization with\nModel-based Explorations (POME). POME uses two components to predict the\nactions' target values: a model-free one estimated by Monte-Carlo sampling and\na model-based one which learns a transition model and predicts the value of the\nnext state. POME adds the error of these two target estimations as the\nadditional exploration value for each state-action pair, i.e, encourages the\nalgorithm to explore the states with larger target errors which are hard to\nestimate. We compare POME with PPO on Atari 2600 games, and it shows that POME\noutperforms PPO on 33 games out of 49 games. \n\n"}
{"id": "1811.07490", "contents": "Title: Memory In Memory: A Predictive Neural Network for Learning Higher-Order\n  Non-Stationarity from Spatiotemporal Dynamics Abstract: Natural spatiotemporal processes can be highly non-stationary in many ways,\ne.g. the low-level non-stationarity such as spatial correlations or temporal\ndependencies of local pixel values; and the high-level variations such as the\naccumulation, deformation or dissipation of radar echoes in precipitation\nforecasting. From Cramer's Decomposition, any non-stationary process can be\ndecomposed into deterministic, time-variant polynomials, plus a zero-mean\nstochastic term. By applying differencing operations appropriately, we may turn\ntime-variant polynomials into a constant, making the deterministic component\npredictable. However, most previous recurrent neural networks for\nspatiotemporal prediction do not use the differential signals effectively, and\ntheir relatively simple state transition functions prevent them from learning\ntoo complicated variations in spacetime. We propose the Memory In Memory (MIM)\nnetworks and corresponding recurrent blocks for this purpose. The MIM blocks\nexploit the differential signals between adjacent recurrent states to model the\nnon-stationary and approximately stationary properties in spatiotemporal\ndynamics with two cascaded, self-renewed memory modules. By stacking multiple\nMIM blocks, we could potentially handle higher-order non-stationarity. The MIM\nnetworks achieve the state-of-the-art results on four spatiotemporal prediction\ntasks across both synthetic and real-world datasets. We believe that the\ngeneral idea of this work can be potentially applied to other time-series\nforecasting tasks. \n\n"}
{"id": "1811.07755", "contents": "Title: Building Efficient Deep Neural Networks with Unitary Group Convolutions Abstract: We propose unitary group convolutions (UGConvs), a building block for CNNs\nwhich compose a group convolution with unitary transforms in feature space to\nlearn a richer set of representations than group convolution alone. UGConvs\ngeneralize two disparate ideas in CNN architecture, channel shuffling (i.e.\nShuffleNet) and block-circulant networks (i.e. CirCNN), and provide unifying\ninsights that lead to a deeper understanding of each technique. We\nexperimentally demonstrate that dense unitary transforms can outperform channel\nshuffling in DNN accuracy. On the other hand, different dense transforms\nexhibit comparable accuracy performance. Based on these observations we propose\nHadaNet, a UGConv network using Hadamard transforms. HadaNets achieve similar\naccuracy to circulant networks with lower computation complexity, and better\naccuracy than ShuffleNets with the same number of parameters and floating-point\nmultiplies. \n\n"}
{"id": "1811.07771", "contents": "Title: A Multi-Task Learning & Generation Framework: Valence-Arousal, Action\n  Units & Primary Expressions Abstract: Over the past few years many research efforts have been devoted to the field\nof affect analysis. Various approaches have been proposed for: i) discrete\nemotion recognition in terms of the primary facial expressions; ii) emotion\nanalysis in terms of facial Action Units (AUs), assuming a fixed expression\nintensity; iii) dimensional emotion analysis, in terms of valence and arousal\n(VA). These approaches can only be effective, if they are developed using\nlarge, appropriately annotated databases, showing behaviors of people\nin-the-wild, i.e., in uncontrolled environments. Aff-Wild has been the first,\nlarge-scale, in-the-wild database (including around 1,200,000 frames of 300\nvideos), annotated in terms of VA. In the vast majority of existing emotion\ndatabases, their annotation is limited to either primary expressions, or\nvalence-arousal, or action units. In this paper, we first annotate a part\n(around $234,000$ frames) of the Aff-Wild database in terms of $8$ AUs and\nanother part (around $288,000$ frames) in terms of the $7$ basic emotion\ncategories, so that parts of this database are annotated in terms of VA, as\nwell as AUs, or primary expressions. Then, we set up and tackle multi-task\nlearning for emotion recognition, as well as for facial image generation.\nMulti-task learning is performed using: i) a deep neural network with shared\nhidden layers, which learns emotional attributes by exploiting their\ninter-dependencies; ii) a discriminator of a generative adversarial network\n(GAN). On the other hand, image generation is implemented through the generator\nof the GAN. For these two tasks, we carefully design loss functions that fit\nthe examined set-up. Experiments are presented which illustrate the good\nperformance of the proposed approach when applied to the new annotated parts of\nthe Aff-Wild database. \n\n"}
{"id": "1811.08568", "contents": "Title: High-Level Strategy Selection under Partial Observability in StarCraft:\n  Brood War Abstract: We consider the problem of high-level strategy selection in the adversarial\nsetting of real-time strategy games from a reinforcement learning perspective,\nwhere taking an action corresponds to switching to the respective strategy.\nHere, a good strategy successfully counters the opponent's current and possible\nfuture strategies which can only be estimated using partial observations. We\ninvestigate whether we can utilize the full game state information during\ntraining time (in the form of an auxiliary prediction task) to increase\nperformance. Experiments carried out within a StarCraft: Brood War bot against\nstrong community bots show substantial win rate improvements over a\nfixed-strategy baseline and encouraging results when learning with the\nauxiliary task. \n\n"}
{"id": "1811.08812", "contents": "Title: Adversarial Classifier for Imbalanced Problems Abstract: Adversarial approach has been widely used for data generation in the last few\nyears. However, this approach has not been extensively utilized for classifier\ntraining. In this paper, we propose an adversarial framework for classifier\ntraining that can also handle imbalanced data. Indeed, a network is trained via\nan adversarial approach to give weights to samples of the majority class such\nthat the obtained classification problem becomes more challenging for the\ndiscriminator and thus boosts its classification capability. In addition to the\ngeneral imbalanced classification problems, the proposed method can also be\nused for problems such as graph representation learning in which it is desired\nto discriminate similar nodes from dissimilar nodes. Experimental results on\nimbalanced data classification and on the tasks like graph link prediction show\nthe superiority of the proposed method compared to the state-of-the-art\nmethods. \n\n"}
{"id": "1811.08840", "contents": "Title: Integrating Reinforcement Learning to Self Training for Pulmonary Nodule\n  Segmentation in Chest X-rays Abstract: Machine learning applications in medical imaging are frequently limited by\nthe lack of quality labeled data. In this paper, we explore the self training\nmethod, a form of semi-supervised learning, to address the labeling burden. By\nintegrating reinforcement learning, we were able to expand the application of\nself training to complex segmentation networks without any further human\nannotation. The proposed approach, reinforced self training (ReST), fine tunes\na semantic segmentation networks by introducing a policy network that learns to\ngenerate pseudolabels. We incorporate an expert demonstration network, based on\ninverse reinforcement learning, to enhance clinical validity and convergence of\nthe policy network. The model was tested on a pulmonary nodule segmentation\ntask in chest X-rays and achieved the performance of a standard U-Net while\nusing only 50% of the labeled data, by exploiting unlabeled data. When the same\nnumber of labeled data was used, a moderate to significant cross validation\naccuracy improvement was achieved depending on the absolute number of labels\nused. \n\n"}
{"id": "1811.08963", "contents": "Title: Multivariate Forecasting of Crude Oil Spot Prices using Neural Networks Abstract: Crude oil is a major component in most advanced economies of the world.\nAccurately predicting and understanding the behavior of crude oil prices is\nimportant for economists, analysts, forecasters, and traders, to name a few.\nThe price of crude oil has declined in the past decade and is seeing a phase of\nstability; but will this stability last? This work is an empirical study on how\nmultivariate analysis may be employed to predict crude oil spot prices using\nneural networks. The concept of using neural networks showed promising\npotential. A very simple neural network model was able to perform on par with\nARIMA models - the state-of-the-art model in time-series forecasting. Advanced\nneural network models using larger datasets may be used in the future to extend\nthis proof-of-concept to a full scale framework. \n\n"}
{"id": "1811.08985", "contents": "Title: On the Influence of Initial Qubit Placement During NISQ Circuit\n  Compilation Abstract: Noisy Intermediate-Scale Quantum (NISQ) machines are not fault-tolerant,\noperate few qubits (currently, less than hundred), but are capable of executing\ninteresting computations. Above the quantum supremacy threshold (approx. 60\nqubits), NISQ machines are expected to be more powerful than existing classical\ncomputers. One of the most stringent problems is that computations (expressed\nas quantum circuits) have to be adapted (compiled) to the NISQ hardware,\nbecause the hardware does not support arbitrary interactions between the\nqubits. This procedure introduces additional gates (e.g. SWAP gates) into the\ncircuits while leaving the implemented computations unchanged. Each additional\ngate increases the failure rate of the adapted (compiled) circuits, because the\nhardware and the circuits are not fault-tolerant. It is reasonable to expect\nthat the placement influences the number of additionally introduced gates.\nTherefore, a combinatorial problem arises: how are circuit qubits allocated\n(placed) initially to the hardware qubits? The novelty of this work relies on\nthe methodology used to investigate the influence of the initial placement. To\nthis end, we introduce a novel heuristic and cost model to estimate the number\nof gates necessary to adapt a circuit to a given NISQ architecture. We\nimplement the heuristic (source code available on github) and benchmark it\nusing a standard compiler (e.g. from IBM Qiskit) treated as a black box.\nPreliminary results indicate that cost reductions of up to 10\\% can be achieved\nfor practical circuit instances on realistic NISQ architectures only by placing\nqubits differently than default (trivial placement). \n\n"}
{"id": "1811.09083", "contents": "Title: Learning Goal Embeddings via Self-Play for Hierarchical Reinforcement\n  Learning Abstract: In hierarchical reinforcement learning a major challenge is determining\nappropriate low-level policies. We propose an unsupervised learning scheme,\nbased on asymmetric self-play from Sukhbaatar et al. (2018), that automatically\nlearns a good representation of sub-goals in the environment and a low-level\npolicy that can execute them. A high-level policy can then direct the lower one\nby generating a sequence of continuous sub-goal vectors. We evaluate our model\nusing Mazebase and Mujoco environments, including the challenging AntGather\ntask. Visualizations of the sub-goal embeddings reveal a logical decomposition\nof tasks within the environment. Quantitatively, our approach obtains\ncompelling performance gains over non-hierarchical approaches. \n\n"}
{"id": "1811.09385", "contents": "Title: On the Importance of Strong Baselines in Bayesian Deep Learning Abstract: Like all sub-fields of machine learning Bayesian Deep Learning is driven by\nempirical validation of its theoretical proposals. Given the many aspects of an\nexperiment it is always possible that minor or even major experimental flaws\ncan slip by both authors and reviewers. One of the most popular experiments\nused to evaluate approximate inference techniques is the regression experiment\non UCI datasets. However, in this experiment, models which have been trained to\nconvergence have often been compared with baselines trained only for a fixed\nnumber of iterations. We find that a well-established baseline, Monte Carlo\ndropout, when evaluated under the same experimental settings shows significant\nimprovements. In fact, the baseline outperforms or performs competitively with\nmethods that claimed to be superior to the very same baseline method when they\nwere introduced. Hence, by exposing this flaw in experimental procedure, we\nhighlight the importance of using identical experimental setups to evaluate,\ncompare, and benchmark methods in Bayesian Deep Learning. \n\n"}
{"id": "1811.09539", "contents": "Title: State of the Art in Fair ML: From Moral Philosophy and Legislation to\n  Fair Classifiers Abstract: Machine learning is becoming an ever present part in our lives as many\ndecisions, e.g. to lend a credit, are no longer made by humans but by machine\nlearning algorithms. However those decisions are often unfair and\ndiscriminating individuals belonging to protected groups based on race or\ngender. With the recent General Data Protection Regulation (GDPR) coming into\neffect, new awareness has been raised for such issues and with computer\nscientists having such a large impact on peoples lives it is necessary that\nactions are taken to discover and prevent discrimination. This work aims to\ngive an introduction into discrimination, legislative foundations to counter it\nand strategies to detect and prevent machine learning algorithms from showing\nsuch behavior. \n\n"}
{"id": "1811.09833", "contents": "Title: Implementing Entangled States on a Quantum Computer Abstract: The study of tensor network theory is an important field and promises a wide\nrange of experimental and quantum information theoretical applications. Matrix\nproduct state is the most well-known example of tensor network states, which\nprovides an effective and efficient representation of one-dimensional quantum\nsystems. Indeed, it lies at the heart of density matrix renormalization group\n(DMRG), a most common method for simulation of one-dimensional strongly\ncorrelated quantum systems. It has got attention from several areas varying\nfrom solid-state systems to quantum computing and quantum simulators. We have\nconsidered maximally entangled matrix product states (GHZ and W). Here, we\ndesigned the quantum circuits for implementing the matrix product states. In\nthis paper, we simulated the matrix product states in customized IBM (2-qubit,\n3-qubit and 4-qubit) quantum systems and determined the probability\ndistribution among the quantum states. \n\n"}
{"id": "1811.09834", "contents": "Title: Efficient Video Understanding via Layered Multi Frame-Rate Analysis Abstract: One of the greatest challenges in the design of a real-time perception system\nfor autonomous driving vehicles and drones is the conflicting requirement of\nsafety (high prediction accuracy) and efficiency. Traditional approaches use a\nsingle frame rate for the entire system. Motivated by the observation that the\nlack of robustness against environmental factors is the major weakness of\ncompact ConvNet architectures, we propose a dual frame-rate system that brings\nin the best of both worlds: A modulator stream that executes an expensive\nmodels robust to environmental factors at a low frame rate to extract slowly\nchanging features describing the environment, and a prediction stream that\nexecutes a light-weight model at real-time to extract transient signals that\ndescribes particularities of the current frame. The advantage of our design is\nvalidated by our extensive empirical study, showing that our solution leads to\nconsistent improvements using a variety of backbone architecture choice and\ninput resolutions. These findings suggest multiple frame-rate systems as a\npromising direction in designing efficient perception for autonomous agents. \n\n"}
{"id": "1811.10112", "contents": "Title: A Model-Based Reinforcement Learning Approach for a Rare Disease\n  Diagnostic Task Abstract: In this work, we present our various contributions to the objective of\nbuilding a decision support tool for the diagnosis of rare diseases. Our goal\nis to achieve a state of knowledge where the uncertainty about the patient's\ndisease is below a predetermined threshold. We aim to reach such states while\nminimizing the average number of medical tests to perform. In doing so, we take\ninto account the need, in many medical applications, to avoid, as much as\npossible, any misdiagnosis. To solve this optimization task, we investigate\nseveral reinforcement learning algorithm and make them operable in our\nhigh-dimensional and sparse-reward setting. We also present a way to combine\nexpert knowledge, expressed as conditional probabilities, with real clinical\ndata. This is crucial because the scarcity of data in the field of rare\ndiseases prevents any approach based solely on clinical data. Finally we show\nthat it is possible to integrate the ontological information about symptoms\nwhile remaining in our probabilistic reasoning. It enables our decision support\ntool to process information given at different level of precision by the user. \n\n"}
{"id": "1811.10119", "contents": "Title: Variational End-to-End Navigation and Localization Abstract: Deep learning has revolutionized the ability to learn \"end-to-end\" autonomous\nvehicle control directly from raw sensory data. While there have been recent\nextensions to handle forms of navigation instruction, these works are unable to\ncapture the full distribution of possible actions that could be taken and to\nreason about localization of the robot within the environment. In this paper,\nwe extend end-to-end driving networks with the ability to perform\npoint-to-point navigation as well as probabilistic localization using only\nnoisy GPS data. We define a novel variational network capable of learning from\nraw camera data of the environment as well as higher level roadmaps to predict\n(1) a full probability distribution over the possible control commands; and (2)\na deterministic control command capable of navigating on the route specified\nwithin the map. Additionally, we formulate how our model can be used to\nlocalize the robot according to correspondences between the map and the\nobserved visual road topology, inspired by the rough localization that human\ndrivers can perform. We test our algorithms on real-world driving data that the\nvehicle has never driven through before, and integrate our point-to-point\nnavigation algorithms onboard a full-scale autonomous vehicle for real-time\nperformance. Our localization algorithm is also evaluated over a new set of\nroads and intersections to demonstrates rough pose localization even in\nsituations without any GPS prior. \n\n"}
{"id": "1811.10481", "contents": "Title: ICPRAI 2018 SI: On dynamic ensemble selection and data preprocessing for\n  multi-class imbalance learning Abstract: Class-imbalance refers to classification problems in which many more\ninstances are available for certain classes than for others. Such imbalanced\ndatasets require special attention because traditional classifiers generally\nfavor the majority class which has a large number of instances. Ensemble of\nclassifiers have been reported to yield promising results. However, the\nmajority of ensemble methods applied to imbalanced learning are static ones.\nMoreover, they only deal with binary imbalanced problems. Hence, this paper\npresents an empirical analysis of dynamic selection techniques and data\npreprocessing methods for dealing with multi-class imbalanced problems. We\nconsidered five variations of preprocessing methods and fourteen dynamic\nselection schemes. Our experiments conducted on 26 multi-class imbalanced\nproblems show that the dynamic ensemble improves the AUC and the G-mean as\ncompared to the static ensemble. Moreover, data preprocessing plays an\nimportant role in such cases. \n\n"}
{"id": "1811.10746", "contents": "Title: MATCH-Net: Dynamic Prediction in Survival Analysis using Convolutional\n  Neural Networks Abstract: Accurate prediction of disease trajectories is critical for early\nidentification and timely treatment of patients at risk. Conventional methods\nin survival analysis are often constrained by strong parametric assumptions and\nlimited in their ability to learn from high-dimensional data, while existing\nneural network models are not readily-adapted to the longitudinal setting. This\npaper develops a novel convolutional approach that addresses these drawbacks.\nWe present MATCH-Net: a Missingness-Aware Temporal Convolutional Hitting-time\nNetwork, designed to capture temporal dependencies and heterogeneous\ninteractions in covariate trajectories and patterns of missingness. To the best\nof our knowledge, this is the first investigation of temporal convolutions in\nthe context of dynamic prediction for personalized risk prognosis. Using\nreal-world data from the Alzheimer's Disease Neuroimaging Initiative, we\ndemonstrate state-of-the-art performance without making any assumptions\nregarding underlying longitudinal or time-to-event processes attesting to the\nmodel's potential utility in clinical decision support. \n\n"}
{"id": "1811.10959", "contents": "Title: Dataset Distillation Abstract: Model distillation aims to distill the knowledge of a complex model into a\nsimpler one. In this paper, we consider an alternative formulation called\ndataset distillation: we keep the model fixed and instead attempt to distill\nthe knowledge from a large training dataset into a small one. The idea is to\nsynthesize a small number of data points that do not need to come from the\ncorrect data distribution, but will, when given to the learning algorithm as\ntraining data, approximate the model trained on the original data. For example,\nwe show that it is possible to compress 60,000 MNIST training images into just\n10 synthetic distilled images (one per class) and achieve close to original\nperformance with only a few gradient descent steps, given a fixed network\ninitialization. We evaluate our method in various initialization settings and\nwith different learning objectives. Experiments on multiple datasets show the\nadvantage of our approach compared to alternative methods. \n\n"}
{"id": "1811.11210", "contents": "Title: Calibrating Uncertainties in Object Localization Task Abstract: In many safety-critical applications such as autonomous driving and surgical\nrobots, it is desirable to obtain prediction uncertainties from object\ndetection modules to help support safe decision-making. Specifically, such\nmodules need to estimate the probability of each predicted object in a given\nregion and the confidence interval for its bounding box. While recent Bayesian\ndeep learning methods provide a principled way to estimate this uncertainty,\nthe estimates for the bounding boxes obtained using these methods are\nuncalibrated. In this paper, we address this problem for the single-object\nlocalization task by adapting an existing technique for calibrating regression\nmodels. We show, experimentally, that the resulting calibrated model obtains\nmore reliable uncertainty estimates. \n\n"}
{"id": "1811.11310", "contents": "Title: Using Attribution to Decode Dataset Bias in Neural Network Models for\n  Chemistry Abstract: Deep neural networks have achieved state of the art accuracy at classifying\nmolecules with respect to whether they bind to specific protein targets. A key\nbreakthrough would occur if these models could reveal the fragment\npharmacophores that are causally involved in binding. Extracting chemical\ndetails of binding from the networks could potentially lead to scientific\ndiscoveries about the mechanisms of drug actions. But doing so requires shining\nlight into the black box that is the trained neural network model, a task that\nhas proved difficult across many domains. Here we show how the binding\nmechanism learned by deep neural network models can be interrogated, using a\nrecently described attribution method. We first work with carefully constructed\nsynthetic datasets, in which the 'fragment logic' of binding is fully known. We\nfind that networks that achieve perfect accuracy on held out test datasets\nstill learn spurious correlations due to biases in the datasets, and we are\nable to exploit this non-robustness to construct adversarial examples that fool\nthe model. The dataset bias makes these models unreliable for accurately\nrevealing information about the mechanisms of protein-ligand binding. In light\nof our findings, we prescribe a test that checks for dataset bias given a\nhypothesis. If the test fails, it indicates that either the model must be\nsimplified or regularized and/or that the training dataset requires\naugmentation. \n\n"}
{"id": "1811.11441", "contents": "Title: Trajectory-based Learning for Ball-in-Maze Games Abstract: Deep Reinforcement Learning has shown tremendous success in solving several\ngames and tasks in robotics. However, unlike humans, it generally requires a\nlot of training instances. Trajectories imitating to solve the task at hand can\nhelp to increase sample-efficiency of deep RL methods. In this paper, we\npresent a simple approach to use such trajectories, applied to the challenging\nBall-in-Maze Games, recently introduced in the literature. We show that in\nspite of not using human-generated trajectories and just using the simulator as\na model to generate a limited number of trajectories, we can get a speed-up of\nabout 2-3x in the learning process. We also discuss some challenges we observed\nwhile using trajectory-based learning for very sparse reward functions. \n\n"}
{"id": "1811.11989", "contents": "Title: Sample Efficient Stochastic Variance-Reduced Cubic Regularization Method Abstract: We propose a sample efficient stochastic variance-reduced cubic\nregularization (Lite-SVRC) algorithm for finding the local minimum efficiently\nin nonconvex optimization. The proposed algorithm achieves a lower sample\ncomplexity of Hessian matrix computation than existing cubic regularization\nbased methods. At the heart of our analysis is the choice of a constant batch\nsize of Hessian matrix computation at each iteration and the stochastic\nvariance reduction techniques. In detail, for a nonconvex function with $n$\ncomponent functions, Lite-SVRC converges to the local minimum within\n$\\tilde{O}(n+n^{2/3}/\\epsilon^{3/2})$ Hessian sample complexity, which is\nfaster than all existing cubic regularization based methods. Numerical\nexperiments with different nonconvex optimization problems conducted on real\ndatasets validate our theoretical results. \n\n"}
{"id": "1811.12194", "contents": "Title: Automatic Diagnosis of Short-Duration 12-Lead ECG using a Deep\n  Convolutional Network Abstract: We present a model for predicting electrocardiogram (ECG) abnormalities in\nshort-duration 12-lead ECG signals which outperformed medical doctors on the\n4th year of their cardiology residency. Such exams can provide a full\nevaluation of heart activity and have not been studied in previous end-to-end\nmachine learning papers. Using the database of a large telehealth network, we\nbuilt a novel dataset with more than 2 million ECG tracings, orders of\nmagnitude larger than those used in previous studies. Moreover, our dataset is\nmore realistic, as it consist of 12-lead ECGs recorded during standard\nin-clinics exams. Using this data, we trained a residual neural network with 9\nconvolutional layers to map 7 to 10 second ECG signals to 6 classes of ECG\nabnormalities. Future work should extend these results to cover a large range\nof ECG abnormalities, which could improve the accessibility of this diagnostic\ntool and avoid wrong diagnosis from medical doctors. \n\n"}
{"id": "1811.12488", "contents": "Title: Leveraging Deep Stein's Unbiased Risk Estimator for Unsupervised X-ray\n  Denoising Abstract: Among the plethora of techniques devised to curb the prevalence of noise in\nmedical images, deep learning based approaches have shown the most promise.\nHowever, one critical limitation of these deep learning based denoisers is the\nrequirement of high-quality noiseless ground truth images that are difficult to\nobtain in many medical imaging applications such as X-rays. To circumvent this\nissue, we leverage recently proposed approach of [7] that incorporates Stein's\nUnbiased Risk Estimator (SURE) to train a deep convolutional neural network\nwithout requiring denoised ground truth X-ray data. Our experimental results\ndemonstrate the effectiveness of SURE based approach for denoising X-ray\nimages. \n\n"}
{"id": "1812.00068", "contents": "Title: GDPP: Learning Diverse Generations Using Determinantal Point Process Abstract: Generative models have proven to be an outstanding tool for representing\nhigh-dimensional probability distributions and generating realistic-looking\nimages. An essential characteristic of generative models is their ability to\nproduce multi-modal outputs. However, while training, they are often\nsusceptible to mode collapse, that is models are limited in mapping input noise\nto only a few modes of the true data distribution. In this work, we draw\ninspiration from Determinantal Point Process (DPP) to propose an unsupervised\npenalty loss that alleviates mode collapse while producing higher quality\nsamples. DPP is an elegant probabilistic measure used to model negative\ncorrelations within a subset and hence quantify its diversity. We use DPP\nkernel to model the diversity in real data as well as in synthetic data. Then,\nwe devise an objective term that encourages generators to synthesize data with\nsimilar diversity to real data. In contrast to previous state-of-the-art\ngenerative models that tend to use additional trainable parameters or complex\ntraining paradigms, our method does not change the original training scheme.\nEmbedded in an adversarial training and variational autoencoder, our Generative\nDPP approach shows a consistent resistance to mode-collapse on a wide variety\nof synthetic data and natural image datasets including MNIST, CIFAR10, and\nCelebA, while outperforming state-of-the-art methods for data-efficiency,\ngeneration quality, and convergence-time whereas being 5.8x faster than its\nclosest competitor. \n\n"}
{"id": "1812.00265", "contents": "Title: Discovering Molecular Functional Groups Using Graph Convolutional Neural\n  Networks Abstract: Functional groups (FGs) are molecular substructures that are served as a\nfoundation for analyzing and predicting chemical properties of molecules.\nAutomatic discovery of FGs will impact various fields of research, including\nmedicinal chemistry and material sciences, by reducing the amount of lab\nexperiments required for discovery or synthesis of new molecules. In this\npaper, we investigate methods based on graph convolutional neural networks\n(GCNNs) for localizing FGs that contribute to specific chemical properties of\ninterest. In our framework, molecules are modeled as undirected relational\ngraphs with atoms as nodes and bonds as edges. Using this relational graph\nstructure, we trained GCNNs in a supervised way on experimentally-validated\nmolecular training sets to predict specific chemical properties, e.g.,\ntoxicity. Upon learning a GCNN, we analyzed its activation patterns to\nautomatically identify FGs using four different explainability methods that we\nhave developed: gradient-based saliency maps, Class Activation Mapping (CAM),\ngradient-weighted CAM (Grad-CAM), and Excitation Back-Propagation. Although\nthese methods are originally derived for convolutional neural networks (CNNs),\nwe adapt them to develop the corresponding suitable versions for GCNNs. We\nevaluated the contrastive power of these methods with respect to the\nspecificity of the identified molecular substructures and their relevance for\nchemical functions. Grad-CAM had the highest contrastive power and generated\nqualitatively the best FGs. This work paves the way for automatic analysis and\ndesign of new molecules. \n\n"}
{"id": "1812.00342", "contents": "Title: Analysis on Gradient Propagation in Batch Normalized Residual Networks Abstract: We conduct mathematical analysis on the effect of batch normalization (BN) on\ngradient backpropogation in residual network training, which is believed to\nplay a critical role in addressing the gradient vanishing/explosion problem, in\nthis work. By analyzing the mean and variance behavior of the input and the\ngradient in the forward and backward passes through the BN and residual\nbranches, respectively, we show that they work together to confine the gradient\nvariance to a certain range across residual blocks in backpropagation. As a\nresult, the gradient vanishing/explosion problem is avoided. We also show the\nrelative importance of batch normalization w.r.t. the residual branches in\nresidual networks. \n\n"}
{"id": "1812.00415", "contents": "Title: Feature Selection Based on Unique Relevant Information for Health Data Abstract: Feature selection, which searches for the most representative features in\nobserved data, is critical for health data analysis. Unlike feature extraction,\nsuch as PCA and autoencoder based methods, feature selection preserves\ninterpretability, meaning that the selected features provide direct information\nabout certain health conditions (i.e., the label). Thus, feature selection\nallows domain experts, such as clinicians, to understand the predictions made\nby machine learning based systems, as well as improve their own diagnostic\nskills. Mutual information is often used as a basis for feature selection since\nit measures dependencies between features and labels. In this paper, we\nintroduce a novel mutual information based feature selection (MIBFS) method\ncalled SURI, which boosts features with high unique relevant information. We\ncompare SURI to existing MIBFS methods using 3 different classifiers on 6\npublicly available healthcare data sets. The results indicate that, in addition\nto preserving interpretability, SURI selects more relevant feature subsets\nwhich lead to higher classification performance. More importantly, we explore\nthe dynamics of mutual information on a public low-dimensional health data set\nvia exhaustive search. The results suggest the important role of unique\nrelevant information in feature selection and verify the principles behind\nSURI. \n\n"}
{"id": "1812.00417", "contents": "Title: Snorkel DryBell: A Case Study in Deploying Weak Supervision at\n  Industrial Scale Abstract: Labeling training data is one of the most costly bottlenecks in developing\nmachine learning-based applications. We present a first-of-its-kind study\nshowing how existing knowledge resources from across an organization can be\nused as weak supervision in order to bring development time and cost down by an\norder of magnitude, and introduce Snorkel DryBell, a new weak supervision\nmanagement system for this setting. Snorkel DryBell builds on the Snorkel\nframework, extending it in three critical aspects: flexible, template-based\ningestion of diverse organizational knowledge, cross-feature production\nserving, and scalable, sampling-free execution. On three classification tasks\nat Google, we find that Snorkel DryBell creates classifiers of comparable\nquality to ones trained with tens of thousands of hand-labeled examples,\nconverts non-servable organizational resources to servable models for an\naverage 52% performance improvement, and executes over millions of data points\nin tens of minutes. \n\n"}
{"id": "1812.00463", "contents": "Title: Personalizing Intervention Probabilities By Pooling Abstract: In many mobile health interventions, treatments should only be delivered in a\nparticular context, for example when a user is currently stressed, walking or\nsedentary. Even in an optimal context, concerns about user burden can restrict\nwhich treatments are sent. To diffuse the treatment delivery over times when a\nuser is in a desired context, it is critical to predict the future number of\ntimes the context will occur. The focus of this paper is on whether\npersonalization can improve predictions in these settings. Though the variance\nbetween individuals' behavioral patterns suggest that personalization should be\nuseful, the amount of individual-level data limits its capabilities. Thus, we\ninvestigate several methods which pool data across users to overcome these\ndeficiencies and find that pooling lowers the overall error rate relative to\nboth personalized and batch approaches. \n\n"}
{"id": "1812.00497", "contents": "Title: Using Multitask Learning to Improve 12-Lead Electrocardiogram\n  Classification Abstract: We develop a multi-task convolutional neural network (CNN) to classify\nmultiple diagnoses from 12-lead electrocardiograms (ECGs) using a dataset\ncomprised of over 40,000 ECGs, with labels derived from cardiologist clinical\ninterpretations. Since many clinically important classes can occur in low\nfrequencies, approaches are needed to improve performance on rare classes. We\ncompare the performance of several single-class classifiers on rare classes to\na multi-headed classifier across all available classes. We demonstrate that the\naddition of common classes can significantly improve CNN performance on rarer\nclasses when compared to a model trained on the rarer class in isolation. Using\nthis method, we develop a model with high performance as measured by F1 score\non multiple clinically relevant classes compared against the gold-standard\ncardiologist interpretation. \n\n"}
{"id": "1812.00554", "contents": "Title: Modeling Treatment Delays for Patients using Feature Label Pairs in a\n  Time Series Abstract: Pharmaceutical targeting is one of key inputs for making sales and marketing\nstrategy planning. Targeting list is built on predicting physician's sales\npotential of certain type of patient. In this paper, we present a\ntime-sensitive targeting framework leveraging time series model to predict\npatient's disease and treatment progression. We create time features by\nextracting service history within a certain period, and record whether the\nevent happens in a look-forward period. Such feature-label pairs are examined\nacross all time periods and all patients to train a model. It keeps the\ninherent order of services and evaluates features associated to the imminent\nfuture, which contribute to improved accuracy. \n\n"}
{"id": "1812.00797", "contents": "Title: Deep Signal Recovery with One-Bit Quantization Abstract: Machine learning, and more specifically deep learning, have shown remarkable\nperformance in sensing, communications, and inference. In this paper, we\nconsider the application of the deep unfolding technique in the problem of\nsignal reconstruction from its one-bit noisy measurements. Namely, we propose a\nmodel-based machine learning method and unfold the iterations of an inference\noptimization algorithm into the layers of a deep neural network for one-bit\nsignal recovery. The resulting network, which we refer to as DeepRec, can\nefficiently handle the recovery of high-dimensional signals from acquired\none-bit noisy measurements. The proposed method results in an improvement in\naccuracy and computational efficiency with respect to the original framework as\nshown through numerical analysis. \n\n"}
{"id": "1812.00883", "contents": "Title: Relation Networks for Optic Disc and Fovea Localization in Retinal\n  Images Abstract: Diabetic Retinopathy is the leading cause of blindness in the world. At least\n90\\% of new cases can be reduced with proper treatment and monitoring of the\neyes. However, scanning the entire population of patients is a difficult\nendeavor. Computer-aided diagnosis tools in retinal image analysis can make the\nprocess scalable and efficient. In this work, we focus on the problem of\nlocalizing the centers of the Optic disc and Fovea, a task crucial to the\nanalysis of retinal scans. Current systems recognize the Optic disc and Fovea\nindividually, without exploiting their relations during learning. We propose a\nnovel approach to localizing the centers of the Optic disc and Fovea by\nsimultaneously processing them and modeling their relative geometry and\nappearance. We show that our approach improves localization and recognition by\nincorporating object-object relations efficiently, and achieves highly\ncompetitive results. \n\n"}
{"id": "1812.01101", "contents": "Title: Automatic Seismic Salt Interpretation with Deep Convolutional Neural\n  Networks Abstract: One of the most crucial tasks in seismic reflection imaging is to identify\nthe salt bodies with high precision. Traditionally, this is accomplished by\nvisually picking the salt/sediment boundaries, which requires a great amount of\nmanual work and may introduce systematic bias. With recent progress of deep\nlearning algorithm and growing computational power, a great deal of efforts\nhave been made to replace human effort with machine power in salt body\ninterpretation. Currently, the method of Convolutional neural networks (CNN) is\nrevolutionizing the computer vision field and has been a hot topic in the image\nanalysis. In this paper, the benefits of CNN-based classification are\ndemonstrated by using a state-of-art network structure U-Net, along with the\nresidual learning framework ResNet, to delineate salt body with high precision.\nNetwork adjustments, including the Exponential Linear Units (ELU) activation\nfunction, the Lov\\'{a}sz-Softmax loss function, and stratified $K$-fold\ncross-validation, have been deployed to further improve the prediction\naccuracy. The preliminary result using SEG Advanced Modeling (SEAM) data shows\ngood agreement between the predicted salt body and manually interpreted salt\nbody, especially in areas with weak reflections. This indicates the great\npotential of applying CNN for salt-related interpretations. \n\n"}
{"id": "1812.01484", "contents": "Title: Privacy-Preserving Distributed Deep Learning for Clinical Data Abstract: Deep learning with medical data often requires larger samples sizes than are\navailable at single providers. While data sharing among institutions is\ndesirable to train more accurate and sophisticated models, it can lead to\nsevere privacy concerns due the sensitive nature of the data. This problem has\nmotivated a number of studies on distributed training of neural networks that\ndo not require direct sharing of the training data. However, simple distributed\ntraining does not offer provable privacy guarantees to satisfy technical safe\nstandards and may reveal information about the underlying patients. We present\na method to train neural networks for clinical data in a distributed fashion\nunder differential privacy. We demonstrate these methods on two datasets that\ninclude information from multiple independent sites, the eICU collaborative\nResearch Database and The Cancer Genome Atlas. \n\n"}
{"id": "1812.01662", "contents": "Title: Feed-Forward Neural Networks Need Inductive Bias to Learn Equality\n  Relations Abstract: Basic binary relations such as equality and inequality are fundamental to\nrelational data structures. Neural networks should learn such relations and\ngeneralise to new unseen data. We show in this study, however, that this\ngeneralisation fails with standard feed-forward networks on binary vectors.\nEven when trained with maximal training data, standard networks do not reliably\ndetect equality.We introduce differential rectifier (DR) units that we add to\nthe network in different configurations. The DR units create an inductive bias\nin the networks, so that they do learn to generalise, even from small numbers\nof examples and we have not found any negative effect of their inclusion in the\nnetwork. Given the fundamental nature of these relations, we hypothesize that\nfeed-forward neural network learning benefits from inductive bias in other\nrelations as well. Consequently, the further development of suitable inductive\nbiases will be beneficial to many tasks in relational learning with neural\nnetworks. \n\n"}
{"id": "1812.01699", "contents": "Title: Assigning a Grade: Accurate Measurement of Road Quality Using Satellite\n  Imagery Abstract: Roads are critically important infrastructure to societal and economic\ndevelopment, with huge investments made by governments every year. However,\nmethods for monitoring those investments tend to be time-consuming, laborious,\nand expensive, placing them out of reach for many developing regions. In this\nwork, we develop a model for monitoring the quality of road infrastructure\nusing satellite imagery. For this task, we harness two trends: the increasing\navailability of high-resolution, often-updated satellite imagery, and the\nenormous improvement in speed and accuracy of convolutional neural\nnetwork-based methods for performing computer vision tasks. We employ a unique\ndataset of road quality information on 7000km of roads in Kenya combined with\n50cm resolution satellite imagery. We create models for a binary classification\ntask as well as a comprehensive 5-category classification task, with accuracy\nscores of 88 and 73 percent respectively. We also provide evidence of the\nrobustness of our methods with challenging held-out scenarios, though we note\nsome improvement is still required for confident analysis of a never before\nseen road. We believe these results are well-positioned to have substantial\nimpact on a broad set of transport applications. \n\n"}
{"id": "1812.01719", "contents": "Title: Knowing what you know in brain segmentation using Bayesian deep neural\n  networks Abstract: In this paper, we describe a Bayesian deep neural network (DNN) for\npredicting FreeSurfer segmentations of structural MRI volumes, in minutes\nrather than hours. The network was trained and evaluated on a large dataset (n\n= 11,480), obtained by combining data from more than a hundred different sites,\nand also evaluated on another completely held-out dataset (n = 418). The\nnetwork was trained using a novel spike-and-slab dropout-based variational\ninference approach. We show that, on these datasets, the proposed Bayesian DNN\noutperforms previously proposed methods, in terms of the similarity between the\nsegmentation predictions and the FreeSurfer labels, and the usefulness of the\nestimate uncertainty of these predictions. In particular, we demonstrated that\nthe prediction uncertainty of this network at each voxel is a good indicator of\nwhether the network has made an error and that the uncertainty across the whole\nbrain can predict the manual quality control ratings of a scan. The proposed\nBayesian DNN method should be applicable to any new network architecture for\naddressing the segmentation problem. \n\n"}
{"id": "1812.01739", "contents": "Title: Benchmarking Keyword Spotting Efficiency on Neuromorphic Hardware Abstract: Using Intel's Loihi neuromorphic research chip and ABR's Nengo Deep Learning\ntoolkit, we analyze the inference speed, dynamic power consumption, and energy\ncost per inference of a two-layer neural network keyword spotter trained to\nrecognize a single phrase. We perform comparative analyses of this keyword\nspotter running on more conventional hardware devices including a CPU, a GPU,\nNvidia's Jetson TX1, and the Movidius Neural Compute Stick. Our results\nindicate that for this inference application, Loihi outperforms all of these\nalternatives on an energy cost per inference basis while maintaining equivalent\ninference accuracy. Furthermore, an analysis of tradeoffs between network size,\ninference speed, and energy cost indicates that Loihi's comparative advantage\nover other low-power computing devices improves for larger networks. \n\n"}
{"id": "1812.02240", "contents": "Title: Efficient and Robust Machine Learning for Real-World Systems Abstract: While machine learning is traditionally a resource intensive task, embedded\nsystems, autonomous navigation and the vision of the Internet-of-Things fuel\nthe interest in resource efficient approaches. These approaches require a\ncarefully chosen trade-off between performance and resource consumption in\nterms of computation and energy. On top of this, it is crucial to treat\nuncertainty in a consistent manner in all but the simplest applications of\nmachine learning systems. In particular, a desideratum for any real-world\nsystem is to be robust in the presence of outliers and corrupted data, as well\nas being `aware' of its limits, i.e.\\ the system should maintain and provide an\nuncertainty estimate over its own predictions. These complex demands are among\nthe major challenges in current machine learning research and key to ensure a\nsmooth transition of machine learning technology into every day's applications.\nIn this article, we provide an overview of the current state of the art of\nmachine learning techniques facilitating these real-world requirements. First\nwe provide a comprehensive review of resource-efficiency in deep neural\nnetworks with focus on techniques for model size reduction, compression and\nreduced precision. These techniques can be applied during training or as\npost-processing and are widely used to reduce both computational complexity and\nmemory footprint. As most (practical) neural networks are limited in their ways\nto treat uncertainty, we contrast them with probabilistic graphical models,\nwhich readily serve these desiderata by means of probabilistic inference. In\nthat way, we provide an extensive overview of the current state-of-the-art of\nrobust and efficient machine learning for real-world systems. \n\n"}
{"id": "1812.02335", "contents": "Title: Layer Flexible Adaptive Computational Time Abstract: Deep recurrent neural networks perform well on sequence data and are the\nmodel of choice. However, it is a daunting task to decide the structure of the\nnetworks, i.e. the number of layers, especially considering different\ncomputational needs of a sequence. We propose a layer flexible recurrent neural\nnetwork with adaptive computation time, and expand it to a sequence to sequence\nmodel. Different from the adaptive computation time model, our model has a\ndynamic number of transmission states which vary by step and sequence. We\nevaluate the model on a financial data set and Wikipedia language modeling.\nExperimental results show the performance improvement of 7\\% to 12\\% and\nindicate the model's ability to dynamically change the number of layers along\nwith the computational steps. \n\n"}
{"id": "1812.02765", "contents": "Title: Improving Reconstruction Autoencoder Out-of-distribution Detection with\n  Mahalanobis Distance Abstract: There is an increasingly apparent need for validating the classifications\nmade by deep learning systems in safety-critical applications like autonomous\nvehicle systems. A number of recent papers have proposed methods for detecting\nanomalous image data that appear different from known inlier data samples,\nincluding reconstruction-based autoencoders. Autoencoders optimize the\ncompression of input data to a latent space of a dimensionality smaller than\nthe original input and attempt to accurately reconstruct the input using that\ncompressed representation. Since the latent vector is optimized to capture the\nsalient features from the inlier class only, it is commonly assumed that images\nof objects from outside of the training class cannot effectively be compressed\nand reconstructed. Some thus consider reconstruction error as a kind of novelty\nmeasure. Here we suggest that reconstruction-based approaches fail to capture\nparticular anomalies that lie far from known inlier samples in latent space but\nnear the latent dimension manifold defined by the parameters of the model. We\npropose incorporating the Mahalanobis distance in latent space to better\ncapture these out-of-distribution samples and our results show that this method\noften improves performance over the baseline approach. \n\n"}
{"id": "1812.02848", "contents": "Title: Cyber Anomaly Detection Using Graph-node Role-dynamics Abstract: Intrusion detection systems (IDSs) generate valuable knowledge about network\nsecurity, but an abundance of false alarms and a lack of methods to capture the\ninterdependence among alerts hampers their utility for network defense. Here,\nwe explore a graph-based approach for fusing alerts generated by multiple IDSs\n(e.g., Snort, OSSEC, and Bro). Our approach generates a weighted graph of alert\nfields (not network topology) that makes explicit the connections between\nmultiple alerts, IDS systems, and other cyber artifacts. We use this\nmulti-modal graph to identify anomalous changes in the alert patterns of a\nnetwork. To detect the anomalies, we apply the role-dynamics approach, which\nhas successfully identified anomalies in social media, email, and IP\ncommunication graphs. In the cyber domain, each node (alert field) in the fused\nIDS alert graph is assigned a probability distribution across a small set of\nroles based on that node's features. A cyber attack should trigger IDS alerts\nand cause changes in the node features, but rather than track every feature for\nevery alert-field node individually, roles provide a succinct, integrated\nsummary of those feature changes. We measure changes in each node's\nprobabilistic role assignment over time, and identify anomalies as deviations\nfrom expected roles. We test our approach using simulations including three\nweeks of normal background traffic, as well as cyber attacks that occur near\nthe end of the simulations. This paper presents a novel approach to multi-modal\ndata fusion and a novel application of role dynamics within the cyber-security\ndomain. Our results show a drastic decrease in the false-positive rate when\nconsidering our anomaly indicator instead of the IDS alerts themselves, thereby\nreducing alarm fatigue and providing a promising avenue for threat intelligence\nin network defense. \n\n"}
{"id": "1812.03123", "contents": "Title: Deep Variational Transfer: Transfer Learning through Semi-supervised\n  Deep Generative Models Abstract: In real-world applications, it is often expensive and time-consuming to\nobtain labeled examples. In such cases, knowledge transfer from related\ndomains, where labels are abundant, could greatly reduce the need for extensive\nlabeling efforts. In this scenario, transfer learning comes in hand. In this\npaper, we propose Deep Variational Transfer (DVT), a variational autoencoder\nthat transfers knowledge across domains using a shared latent Gaussian mixture\nmodel. Thanks to the combination of a semi-supervised ELBO and parameters\nsharing across domains, we are able to simultaneously: (i) align all supervised\nexamples of the same class into the same latent Gaussian Mixture component,\nindependently from their domain; (ii) predict the class of unsupervised\nexamples from different domains and use them to better model the occurring\nshifts. We perform tests on MNIST and USPS digits datasets, showing DVT's\nability to perform transfer learning across heterogeneous datasets.\nAdditionally, we present DVT's top classification performances on the MNIST\nsemi-supervised learning challenge. We further validate DVT on a astronomical\ndatasets. DVT achieves states-of-the-art classification performances,\ntransferring knowledge across real stars surveys datasets, EROS, MACHO and\nHiTS, . In the worst performance, we double the achieved F1-score for rare\nclasses. These experiments show DVT's ability to tackle all major challenges\nposed by transfer learning: different covariate distributions, different and\nhighly imbalanced class distributions and different feature spaces. \n\n"}
{"id": "1812.03188", "contents": "Title: METCC: METric learning for Confounder Control Making distance matter in\n  high dimensional biological analysis Abstract: High-dimensional data acquired from biological experiments such as next\ngeneration sequencing are subject to a number of confounding effects. These\neffects include both technical effects, such as variation across batches from\ninstrument noise or sample processing, or institution-specific differences in\nsample acquisition and physical handling, as well as biological effects arising\nfrom true but irrelevant differences in the biology of each sample, such as age\nbiases in diseases. Prior work has used linear methods to adjust for such batch\neffects. Here, we apply contrastive metric learning by a non-linear triplet\nnetwork to optimize the ability to distinguish biologically distinct sample\nclasses in the presence of irrelevant technical and biological variation. Using\nwhole-genome cell-free DNA data from 817 patients, we demonstrate that our\napproach, METric learning for Confounder Control (METCC), is able to match or\nexceed the classification performance achieved using a best-in-class linear\nmethod (HCP) or no normalization. Critically, results from METCC appear less\nconfounded by irrelevant technical variables like institution and batch than\nthose from other methods even without access to high quality metadata\ninformation required by many existing techniques; offering hope for improved\ngeneralization. \n\n"}
{"id": "1812.03271", "contents": "Title: Generalized Batch Normalization: Towards Accelerating Deep Neural\n  Networks Abstract: Utilizing recently introduced concepts from statistics and quantitative risk\nmanagement, we present a general variant of Batch Normalization (BN) that\noffers accelerated convergence of Neural Network training compared to\nconventional BN. In general, we show that mean and standard deviation are not\nalways the most appropriate choice for the centering and scaling procedure\nwithin the BN transformation, particularly if ReLU follows the normalization\nstep. We present a Generalized Batch Normalization (GBN) transformation, which\ncan utilize a variety of alternative deviation measures for scaling and\nstatistics for centering, choices which naturally arise from the theory of\ngeneralized deviation measures and risk theory in general. When used in\nconjunction with the ReLU non-linearity, the underlying risk theory suggests\nnatural, arguably optimal choices for the deviation measure and statistic.\nUtilizing the suggested deviation measure and statistic, we show experimentally\nthat training is accelerated more so than with conventional BN, often with\nimproved error rate as well. Overall, we propose a more flexible BN\ntransformation supported by a complimentary theoretical framework that can\npotentially guide design choices. \n\n"}
{"id": "1812.03699", "contents": "Title: Taxi Demand-Supply Forecasting: Impact of Spatial Partitioning on the\n  Performance of Neural Networks Abstract: In this paper, we investigate the significance of choosing an appropriate\ntessellation strategy for a spatio-temporal taxi demand-supply modeling\nframework. Our study compares (i) the variable-sized polygon based Voronoi\ntessellation, and (ii) the fixed-sized grid based Geohash tessellation, using\ntaxi demand-supply GPS data for the cities of Bengaluru, India and New York,\nUSA. Long Short-Term Memory (LSTM) networks are used for modeling and\nincorporating information from spatial neighbors into the model. We find that\nthe LSTM model based on input features extracted from a variable-sized polygon\ntessellation yields superior performance over the LSTM model based on\nfixed-sized grid tessellation. Our study highlights the need to explore\nmultiple spatial partitioning techniques for improving the prediction\nperformance in neural network models. \n\n"}
{"id": "1812.03894", "contents": "Title: Physics-Based Learning for Robotic Environmental Sensing Abstract: We propose a physics-based method to learn environmental fields (EFs) using a\nmobile robot. Common purely data-driven methods require prohibitively many\nmeasurements to accurately learn such complex EFs. Alternatively, physics-based\nmodels provide global knowledge of EFs but require experimental validation,\ndepend on uncertain parameters, and are intractable for mobile robots. To\naddress these challenges, we propose a Bayesian framework to select the most\nlikely physics-based models of EFs in real-time, from a pool of numerical\nsolutions generated offline as a function of the uncertain parameters.\nSpecifically, we focus on turbulent flow fields and utilize Gaussian processes\n(GPs) to construct statistical models for them, using the pool of numerical\nsolutions to inform their prior mean. To incorporate flow measurements into\nthese GPs, we control a custom-built mobile robot through a sequence of\nwaypoints that maximize the information content of the measurements. We\nexperimentally demonstrate that our proposed framework constructs a posterior\ndistribution of the flow field that better approximates the real flow compared\nto the prior numerical solutions and purely data-driven methods. \n\n"}
{"id": "1812.04069", "contents": "Title: Individual Fairness in Hindsight Abstract: Since many critical decisions impacting human lives are increasingly being\nmade by algorithms, it is important to ensure that the treatment of individuals\nunder such algorithms is demonstrably fair under reasonable notions of\nfairness. One compelling notion proposed in the literature is that of\nindividual fairness (IF), which advocates that similar individuals should be\ntreated similarly (Dwork et al. 2012). Originally proposed for offline\ndecisions, this notion does not, however, account for temporal considerations\nrelevant for online decision-making. In this paper, we extend the notion of IF\nto account for the time at which a decision is made, in settings where there\nexists a notion of conduciveness of decisions as perceived by the affected\nindividuals. We introduce two definitions: (i) fairness-across-time (FT) and\n(ii) fairness-in-hindsight (FH). FT is the simplest temporal extension of IF\nwhere treatment of individuals is required to be individually fair relative to\nthe past as well as future, while in FH, we require a one-sided notion of\nindividual fairness that is defined relative to only the past decisions. We\nshow that these two definitions can have drastically different implications in\nthe setting where the principal needs to learn the utility model. Linear regret\nrelative to optimal individually fair decisions is inevitable under FT for\nnon-trivial examples. On the other hand, we design a new algorithm: Cautious\nFair Exploration (CaFE), which satisfies FH and achieves sub-linear regret\nguarantees for a broad range of settings. We characterize lower bounds showing\nthat these guarantees are order-optimal in the worst case. FH can thus be\nembedded as a primary safeguard against unfair discrimination in algorithmic\ndeployments, without hindering the ability to take good decisions in the\nlong-run. \n\n"}
{"id": "1812.04314", "contents": "Title: Adversarial Autoencoders with Constant-Curvature Latent Manifolds Abstract: Constant-curvature Riemannian manifolds (CCMs) have been shown to be ideal\nembedding spaces in many application domains, as their non-Euclidean geometry\ncan naturally account for some relevant properties of data, like hierarchy and\ncircularity. In this work, we introduce the CCM adversarial autoencoder\n(CCM-AAE), a probabilistic generative model trained to represent a data\ndistribution on a CCM. Our method works by matching the aggregated posterior of\nthe CCM-AAE with a probability distribution defined on a CCM, so that the\nencoder implicitly learns to represent data on the CCM to fool the\ndiscriminator network. The geometric constraint is also explicitly imposed by\njointly training the CCM-AAE to maximise the membership degree of the\nembeddings to the CCM. While a few works in recent literature make use of\neither hyperspherical or hyperbolic manifolds for different learning tasks,\nours is the first unified framework to seamlessly deal with CCMs of different\ncurvatures. We show the effectiveness of our model on three different datasets\ncharacterised by non-trivial geometry: semi-supervised classification on MNIST,\nlink prediction on two popular citation datasets, and graph-based molecule\ngeneration using the QM9 chemical database. Results show that our method\nimproves upon other autoencoders based on Euclidean and non-Euclidean\ngeometries on all tasks taken into account. \n\n"}
{"id": "1812.04690", "contents": "Title: Learning representations of molecules and materials with atomistic\n  neural networks Abstract: Deep Learning has been shown to learn efficient representations for\nstructured data such as image, text or audio. In this chapter, we present\nneural network architectures that are able to learn efficient representations\nof molecules and materials. In particular, the continuous-filter convolutional\nnetwork SchNet accurately predicts chemical properties across compositional and\nconfigurational space on a variety of datasets. Beyond that, we analyze the\nobtained representations to find evidence that their spatial and chemical\nproperties agree with chemical intuition. \n\n"}
{"id": "1812.04912", "contents": "Title: EasiCSDeep: A deep learning model for Cervical Spondylosis\n  Identification using surface electromyography signal Abstract: Cervical spondylosis (CS) is a common chronic disease that affects up to\ntwo-thirds of the population and poses a serious burden on individuals and\nsociety. The early identification has significant value in improving cure rate\nand reducing costs. However, the pathology is complex, and the mild symptoms\nincrease the difficulty of the diagnosis, especially in the early stage.\nBesides, the time-consuming and costliness of hospital medical service reduces\nthe attention to the CS identification. Thus, a convenient, low-cost\nintelligent CS identification method is imperious demanded. In this paper, we\npresent an intelligent method based on the deep learning to identify CS, using\nthe surface electromyography (sEMG) signal. Faced with the complex, high\ndimensionality and weak usability of the sEMG signal, we proposed and developed\na multi-channel EasiCSDeep algorithm based on the convolutional neural network,\nwhich consists of the feature extraction, spatial relationship representation\nand classification algorithm. To the best of our knowledge, this EasiCSDeep is\nthe first effort to employ the deep learning and the sEMG data to identify CS.\nCompared with previous state-of-the-art algorithm, our algorithm achieves a\nsignificant improvement. \n\n"}
{"id": "1812.05451", "contents": "Title: A Probabilistic Model of the Bitcoin Blockchain Abstract: The Bitcoin transaction graph is a public data structure organized as\ntransactions between addresses, each associated with a logical entity. In this\nwork, we introduce a complete probabilistic model of the Bitcoin Blockchain. We\nfirst formulate a set of conditional dependencies induced by the Bitcoin\nprotocol at the block level and derive a corresponding fully observed graphical\nmodel of a Bitcoin block. We then extend the model to include hidden entity\nattributes such as the functional category of the associated logical agent and\nderive asymptotic bounds on the privacy properties implied by this model. At\nthe network level, we show evidence of complex transaction-to-transaction\nbehavior and present a relevant discriminative model of the agent categories.\nPerformance of both the block-based graphical model and the network-level\ndiscriminative model is evaluated on a subset of the public Bitcoin Blockchain. \n\n"}
{"id": "1812.05555", "contents": "Title: Kalman-based Spectro-Temporal ECG Analysis using Deep Convolutional\n  Networks for Atrial Fibrillation Detection Abstract: In this article, we propose a novel ECG classification framework for atrial\nfibrillation (AF) detection using spectro-temporal representation (i.e., time\nvarying spectrum) and deep convolutional networks. In the first step we use a\nBayesian spectro-temporal representation based on the estimation of\ntime-varying coefficients of Fourier series using Kalman filter and smoother.\nNext, we derive an alternative model based on a stochastic oscillator\ndifferential equation to accelerate the estimation of the spectro-temporal\nrepresentation in lengthy signals. Finally, after comparative evaluations of\ndifferent convolutional architectures, we propose an efficient deep\nconvolutional neural network to classify the 2D spectro-temporal ECG data.\n  The ECG spectro-temporal data are classified into four different classes: AF,\nnon-AF normal rhythm (Normal), non-AF abnormal rhythm (Other), and noisy\nsegments (Noisy). The performance of the proposed methods is evaluated and\nscored with the PhysioNet/Computing in Cardiology (CinC) 2017 dataset. The\nexperimental results show that the proposed method achieves the overall F1\nscore of 80.2%, which is in line with the state-of-the-art algorithms. \n\n"}
{"id": "1812.05721", "contents": "Title: Stochastic Gradient Descent for Spectral Embedding with Implicit\n  Orthogonality Constraint Abstract: In this paper, we propose a scalable algorithm for spectral embedding. The\nlatter is a standard tool for graph clustering. However, its computational\nbottleneck is the eigendecomposition of the graph Laplacian matrix, which\nprevents its application to large-scale graphs. Our contribution consists of\nreformulating spectral embedding so that it can be solved via stochastic\noptimization. The idea is to replace the orthogonality constraint with an\northogonalization matrix injected directly into the criterion. As the gradient\ncan be computed through a Cholesky factorization, our reformulation allows us\nto develop an efficient algorithm based on mini-batch gradient descent.\nExperimental results, both on synthetic and real data, confirm the efficiency\nof the proposed method in term of execution speed with respect to similar\nexisting techniques. \n\n"}
{"id": "1812.06080", "contents": "Title: Reconciling meta-learning and continual learning with online mixtures of\n  tasks Abstract: Learning-to-learn or meta-learning leverages data-driven inductive bias to\nincrease the efficiency of learning on a novel task. This approach encounters\ndifficulty when transfer is not advantageous, for instance, when tasks are\nconsiderably dissimilar or change over time. We use the connection between\ngradient-based meta-learning and hierarchical Bayes to propose a Dirichlet\nprocess mixture of hierarchical Bayesian models over the parameters of an\narbitrary parametric model such as a neural network. In contrast to\nconsolidating inductive biases into a single set of hyperparameters, our\napproach of task-dependent hyperparameter selection better handles latent\ndistribution shift, as demonstrated on a set of evolving, image-based, few-shot\nlearning benchmarks. \n\n"}
{"id": "1812.06391", "contents": "Title: Fast MVAE: Joint separation and classification of mixed sources based on\n  multichannel variational autoencoder with auxiliary classifier Abstract: This paper proposes an alternative algorithm for multichannel variational\nautoencoder (MVAE), a recently proposed multichannel source separation\napproach. While MVAE is notable in its impressive source separation\nperformance, the convergence-guaranteed optimization algorithm and that it\nallows us to estimate source-class labels simultaneously with source\nseparation, there are still two major drawbacks, i.e., the high computational\ncomplexity and unsatisfactory source classification accuracy. To overcome these\ndrawbacks, the proposed method employs an auxiliary classifier VAE, an\ninformation-theoretic extension of the conditional VAE, for learning the\ngenerative model of the source spectrograms. Furthermore, with the trained\nauxiliary classifier, we introduce a novel algorithm for the optimization that\nis able to not only reduce the computational time but also improve the source\nclassification performance. We call the proposed method \"fast MVAE (fMVAE)\".\nExperimental evaluations revealed that fMVAE achieved comparative source\nseparation performance to MVAE and about 80% source classification accuracy\nrate while it reduced about 93% computational time. \n\n"}
{"id": "1812.06488", "contents": "Title: Feedback alignment in deep convolutional networks Abstract: Ongoing studies have identified similarities between neural representations\nin biological networks and in deep artificial neural networks. This has led to\nrenewed interest in developing analogies between the backpropagation learning\nalgorithm used to train artificial networks and the synaptic plasticity rules\noperative in the brain. These efforts are challenged by biologically\nimplausible features of backpropagation, one of which is a reliance on\nsymmetric forward and backward synaptic weights. A number of methods have been\nproposed that do not rely on weight symmetry but, thus far, these have failed\nto scale to deep convolutional networks and complex data. We identify principal\nobstacles to the scalability of such algorithms and introduce several\ntechniques to mitigate them. We demonstrate that a modification of the feedback\nalignment method that enforces a weaker form of weight symmetry, one that\nrequires agreement of weight sign but not magnitude, can achieve performance\ncompetitive with backpropagation. Our results complement those of Bartunov et\nal. (2018) and Xiao et al. (2018b) and suggest that mechanisms that promote\nalignment of feedforward and feedback weights are critical for learning in deep\nnetworks. \n\n"}
{"id": "1812.06647", "contents": "Title: Interpretable Matrix Completion: A Discrete Optimization Approach Abstract: We consider the problem of matrix completion on an $n \\times m$ matrix. We\nintroduce the problem of Interpretable Matrix Completion that aims to provide\nmeaningful insights for the low-rank matrix using side information. We show\nthat the problem can be reformulated as a binary convex optimization problem.\nWe design OptComplete, based on a novel concept of stochastic cutting planes to\nenable efficient scaling of the algorithm up to matrices of sizes $n=10^6$ and\n$m=10^6$. We report experiments on both synthetic and real-world datasets that\nshow that OptComplete has favorable scaling behavior and accuracy when compared\nwith state-of-the-art methods for other types of matrix completion, while\nproviding insight on the factors that affect the matrix. \n\n"}
{"id": "1812.09902", "contents": "Title: Invariant and Equivariant Graph Networks Abstract: Invariant and equivariant networks have been successfully used for learning\nimages, sets, point clouds, and graphs. A basic challenge in developing such\nnetworks is finding the maximal collection of invariant and equivariant linear\nlayers. Although this question is answered for the first three examples (for\npopular transformations, at-least), a full characterization of invariant and\nequivariant linear layers for graphs is not known.\n  In this paper we provide a characterization of all permutation invariant and\nequivariant linear layers for (hyper-)graph data, and show that their\ndimension, in case of edge-value graph data, is 2 and 15, respectively. More\ngenerally, for graph data defined on k-tuples of nodes, the dimension is the\nk-th and 2k-th Bell numbers. Orthogonal bases for the layers are computed,\nincluding generalization to multi-graph data. The constant number of basis\nelements and their characteristics allow successfully applying the networks to\ndifferent size graphs. From the theoretical point of view, our results\ngeneralize and unify recent advancement in equivariant deep learning. In\nparticular, we show that our model is capable of approximating any message\npassing neural network\n  Applying these new linear layers in a simple deep neural network framework is\nshown to achieve comparable results to state-of-the-art and to have better\nexpressivity than previous invariant and equivariant bases. \n\n"}
{"id": "1812.10252", "contents": "Title: Optimizing Market Making using Multi-Agent Reinforcement Learning Abstract: In this paper, reinforcement learning is applied to the problem of optimizing\nmarket making. A multi-agent reinforcement learning framework is used to\noptimally place limit orders that lead to successful trades. The framework\nconsists of two agents. The macro-agent optimizes on making the decision to\nbuy, sell, or hold an asset. The micro-agent optimizes on placing limit orders\nwithin the limit order book. For the context of this paper, the proposed\nframework is applied and studied on the Bitcoin cryptocurrency market. The goal\nof this paper is to show that reinforcement learning is a viable strategy that\ncan be applied to complex problems (with complex environments) such as market\nmaking. \n\n"}
{"id": "1812.10650", "contents": "Title: Sampling Using Neural Networks for colorizing the grayscale images Abstract: The main idea of this paper is to explore the possibilities of generating\nsamples from the neural networks, mostly focusing on the colorization of the\ngrey-scale images. I will compare the existing methods for colorization and\nexplore the possibilities of using new generative modeling to the task of\ncolorization. The contributions of this paper are to compare the existing\nstructures with similar generating structures(Decoders) and to apply the novel\nstructures including Conditional VAE(CVAE), Conditional Wasserstein GAN with\nGradient Penalty(CWGAN-GP), CWGAN-GP with L1 reconstruction loss, Adversarial\nGenerative Encoders(AGE) and Introspective VAE(IVAE). I trained these models\nusing CIFAR-10 images. To measure the performance, I use Inception Score(IS)\nwhich measures how distinctive each image is and how diverse overall samples\nare as well as human eyes for CIFAR-10 images. It turns out that CVAE with L1\nreconstruction loss and IVAE achieve the highest score in IS. CWGAN-GP with L1\ntends to learn faster than CWGAN-GP, but IS does not increase from CWGAN-GP.\nCWGAN-GP tends to generate more diverse images than other models using\nreconstruction loss. Also, I figured out that the proper regularization plays a\nvital role in generative modeling. \n\n"}
{"id": "1812.10659", "contents": "Title: Low Latency Privacy Preserving Inference Abstract: When applying machine learning to sensitive data, one has to find a balance\nbetween accuracy, information security, and computational-complexity. Recent\nstudies combined Homomorphic Encryption with neural networks to make inferences\nwhile protecting against information leakage. However, these methods are\nlimited by the width and depth of neural networks that can be used (and hence\nthe accuracy) and exhibit high latency even for relatively simple networks. In\nthis study we provide two solutions that address these limitations. In the\nfirst solution, we present more than $10\\times$ improvement in latency and\nenable inference on wider networks compared to prior attempts with the same\nlevel of security. The improved performance is achieved by novel methods to\nrepresent the data during the computation. In the second solution, we apply the\nmethod of transfer learning to provide private inference services using deep\nnetworks with latency of $\\sim0.16$ seconds. We demonstrate the efficacy of our\nmethods on several computer vision tasks. \n\n"}
{"id": "1812.10730", "contents": "Title: Neuromemrisitive Architecture of HTM with On-Device Learning and\n  Neurogenesis Abstract: Hierarchical temporal memory (HTM) is a biomimetic sequence memory algorithm\nthat holds promise for invariant representations of spatial and spatiotemporal\ninputs. This paper presents a comprehensive neuromemristive crossbar\narchitecture for the spatial pooler (SP) and the sparse distributed\nrepresentation classifier, which are fundamental to the algorithm. There are\nseveral unique features in the proposed architecture that tightly link with the\nHTM algorithm. A memristor that is suitable for emulating the HTM synapses is\nidentified and a new Z-window function is proposed. The architecture exploits\nthe concept of synthetic synapses to enable potential synapses in the HTM. The\ncrossbar for the SP avoids dark spots caused by unutilized crossbar regions and\nsupports rapid on-chip training within 2 clock cycles. This research also\nleverages plasticity mechanisms such as neurogenesis and homeostatic intrinsic\nplasticity to strengthen the robustness and performance of the SP. The proposed\ndesign is benchmarked for image recognition tasks using MNIST and Yale faces\ndatasets, and is evaluated using different metrics including entropy,\nsparseness, and noise robustness. Detailed power analysis at different stages\nof the SP operations is performed to demonstrate the suitability for mobile\nplatforms. \n\n"}
{"id": "1812.10761", "contents": "Title: Improving Generalization of Deep Neural Networks by Leveraging Margin\n  Distribution Abstract: Recent research has used margin theory to analyze the generalization\nperformance for deep neural networks (DNNs). The existed results are almost\nbased on the spectrally-normalized minimum margin. However, optimizing the\nminimum margin ignores a mass of information about the entire margin\ndistribution, which is crucial to generalization performance. In this paper, we\nprove a generalization upper bound dominated by the statistics of the entire\nmargin distribution. Compared with the minimum margin bounds, our bound\nhighlights an important measure for controlling the complexity, which is the\nratio of the margin standard deviation to the expected margin. We utilize a\nconvex margin distribution loss function on the deep neural networks to\nvalidate our theoretical results by optimizing the margin ratio. Experiments\nand visualizations confirm the effectiveness of our approach and the\ncorrelation between generalization gap and margin ratio. \n\n"}
{"id": "1812.10793", "contents": "Title: Automated Adaptation Strategies for Stream Learning Abstract: Automation of machine learning model development is increasingly becoming an\nestablished research area. While automated model selection and automated data\npre-processing have been studied in depth, there is, however, a gap concerning\nautomated model adaptation strategies when multiple strategies are available.\nManually developing an adaptation strategy can be time consuming and costly. In\nthis paper we address this issue by proposing the use of flexible adaptive\nmechanism deployment for automated development of adaptation strategies.\nExperimental results after using the proposed strategies with five adaptive\nalgorithms on 36 datasets confirm their viability. These strategies achieve\nbetter or comparable performance to the custom adaptation strategies and the\nrepeated deployment of any single adaptive mechanism. \n\n"}
{"id": "1812.11178", "contents": "Title: Drug cell line interaction prediction Abstract: Understanding the phenotypic drug response on cancer cell lines plays a vital\nrule in anti-cancer drug discovery and re-purposing. The Genomics of Drug\nSensitivity in Cancer (GDSC) database provides open data for researchers in\nphenotypic screening to test their models and methods. Previously, most\nresearch in these areas starts from the fingerprints or features of drugs,\ninstead of their structures. In this paper, we introduce a model for phenotypic\nscreening, which is called twin Convolutional Neural Network for drugs in\nSMILES format (tCNNS). tCNNS is comprised of CNN input channels for drugs in\nSMILES format and cancer cell lines respectively. Our model achieves $0.84$ for\nthe coefficient of determinant($R^2$) and $0.92$ for Pearson\ncorrelation($R_p$), which are significantly better than previous\nworks\\cite{ammad2014integrative,haider2015copula,menden2013machine}. Besides\nthese statistical metrics, tCNNS also provides some insights into phenotypic\nscreening. \n\n"}
{"id": "1812.11440", "contents": "Title: Brain MRI super-resolution using 3D generative adversarial networks Abstract: In this work we propose an adversarial learning approach to generate high\nresolution MRI scans from low resolution images. The architecture, based on the\nSRGAN model, adopts 3D convolutions to exploit volumetric information. For the\ndiscriminator, the adversarial loss uses least squares in order to stabilize\nthe training. For the generator, the loss function is a combination of a least\nsquares adversarial loss and a content term based on mean square error and\nimage gradients in order to improve the quality of the generated images. We\nexplore different solutions for the upsampling phase. We present promising\nresults that improve classical interpolation, showing the potential of the\napproach for 3D medical imaging super-resolution. Source code available at\nhttps://github.com/imatge-upc/3D-GAN-superresolution \n\n"}
{"id": "1901.00039", "contents": "Title: Mask-aware networks for crowd counting Abstract: Crowd counting problem aims to count the number of objects within an image or\na frame in the videos and is usually solved by estimating the density map\ngenerated from the object location annotations. The values in the density map,\nby nature, take two possible states: zero indicating no object around, a\nnon-zero value indicating the existence of objects and the value denoting the\nlocal object density. In contrast to traditional methods which do not\ndifferentiate the density prediction of these two states, we propose to use a\ndedicated network branch to predict the object/non-object mask and then combine\nits prediction with the input image to produce the density map. Our rationale\nis that the mask prediction could be better modeled as a binary segmentation\nproblem and the difficulty of estimating the density could be reduced if the\nmask is known. A key to the proposed scheme is the strategy of incorporating\nthe mask prediction into the density map estimator. To this end, we study five\npossible solutions, and via analysis and experimental validation we identify\nthe most effective one. Through extensive experiments on five public datasets,\nwe demonstrate the superior performance of the proposed approach over the\nbaselines and show that our network could achieve the state-of-the-art\nperformance. \n\n"}
{"id": "1901.00069", "contents": "Title: Recurrent Neural Networks for Time Series Forecasting Abstract: Time series forecasting is difficult. It is difficult even for recurrent\nneural networks with their inherent ability to learn sequentiality. This\narticle presents a recurrent neural network based time series forecasting\nframework covering feature engineering, feature importances, point and interval\npredictions, and forecast evaluation. The description of the method is followed\nby an empirical study using both LSTM and GRU networks. \n\n"}
{"id": "1901.00439", "contents": "Title: Deep Representation Learning for Clustering of Health Tweets Abstract: Twitter has been a prominent social media platform for mining\npopulation-level health data and accurate clustering of health-related tweets\ninto topics is important for extracting relevant health insights. In this work,\nwe propose deep convolutional autoencoders for learning compact representations\nof health-related tweets, further to be employed in clustering. We compare our\nmethod to several conventional tweet representation methods including\nbag-of-words, term frequency-inverse document frequency, Latent Dirichlet\nAllocation and Non-negative Matrix Factorization with 3 different clustering\nalgorithms. Our results show that the clustering performance using proposed\nrepresentation learning scheme significantly outperforms that of conventional\nmethods for all experiments of different number of clusters. In addition, we\npropose a constraint on the learned representations during the neural network\ntraining in order to further enhance the clustering performance. All in all,\nthis study introduces utilization of deep neural network-based architectures,\ni.e., deep convolutional autoencoders, for learning informative representations\nof health-related tweets. \n\n"}
{"id": "1901.00751", "contents": "Title: Low-Cost Device Prototype for Automatic Medical Diagnosis Using Deep\n  Learning Methods Abstract: This paper introduces a novel low-cost device prototype for the automatic\ndiagnosis of diseases, utilizing inputted symptoms and personal background. The\nengineering goal is to solve the problem of limited healthcare access with a\nsingle device. Diagnosing diseases automatically is an immense challenge, owing\nto their variable properties and symptoms. On the other hand, Neural Networks\nhave developed into a powerful tool in the field of machine learning, one that\nis showing to be extremely promising at computing diagnosis even with\ninconsistent variables.\n  In this research, a cheap device was created to allow for straightforward\ndiagnosis and treatment of human diseases. By utilizing Deep Neural Networks\n(DNNs) and Convolutional Neural Networks (CNNs), outfitted on a Raspberry Pi\nZero processor ($5), the device is able to detect up to 1537 different diseases\nand conditions and utilize a CNN for on-device visual diagnostics. The user can\ninput the symptoms using the buttons on the device and can take pictures using\nthe same mechanism. The algorithm processes inputted symptoms, providing\ndiagnosis and possible treatment options for common conditions. The purpose of\nthis work was to be able to diagnose diseases through an affordable processor\nwith high accuracy, as it is currently achieving an accuracy of 90% for Top-5\nsymptom-based diagnoses, and 91% for visual skin diseases. The NNs achieve\nperformance far above any other tested system, and its efficiency and ease of\nuse will prove it to be a helpful tool for people around the world. This device\ncould potentially provide low-cost universal access to vital diagnostics and\ntreatment options. \n\n"}
{"id": "1901.00786", "contents": "Title: Towards Global Remote Discharge Estimation: Using the Few to Estimate\n  The Many Abstract: Learning hydrologic models for accurate riverine flood prediction at scale is\na challenge of great importance. One of the key difficulties is the need to\nrely on in-situ river discharge measurements, which can be quite scarce and\nunreliable, particularly in regions where floods cause the most damage every\nyear. Accordingly, in this work we tackle the problem of river discharge\nestimation at different river locations. A core characteristic of the data at\nhand (e.g. satellite measurements) is that we have few measurements for many\nlocations, all sharing the same physics that underlie the water discharge. We\ncapture this scenario in a simple but powerful common mechanism regression\n(CMR) model with a local component as well as a shared one which captures the\nglobal discharge mechanism. The resulting learning objective is non-convex, but\nwe show that we can find its global optimum by leveraging the power of joining\nlocal measurements across sites. In particular, using a spectral initialization\nwith provable near-optimal accuracy, we can find the optimum using standard\ndescent methods. We demonstrate the efficacy of our approach for the problem of\ndischarge estimation using simulations. \n\n"}
{"id": "1901.00997", "contents": "Title: Concentration bounds for CVaR estimation: The cases of light-tailed and\n  heavy-tailed distributions Abstract: Conditional Value-at-Risk (CVaR) is a widely used risk metric in applications\nsuch as finance. We derive concentration bounds for CVaR estimates, considering\nseparately the cases of light-tailed and heavy-tailed distributions. In the\nlight-tailed case, we use a classical CVaR estimator based on the empirical\ndistribution constructed from the samples. For heavy-tailed random variables,\nwe assume a mild `bounded moment' condition, and derive a concentration bound\nfor a truncation-based estimator. Notably, our concentration bounds enjoy an\nexponential decay in the sample size, for heavy-tailed as well as light-tailed\ndistributions. To demonstrate the applicability of our concentration results,\nwe consider a CVaR optimization problem in a multi-armed bandit setting.\nSpecifically, we address the best CVaR-arm identification problem under a fixed\nbudget. We modify the well-known successive rejects algorithm to incorporate a\nCVaR-based criterion. Using the CVaR concentration result, we derive an\nupper-bound on the probability of incorrect identification by the proposed\nalgorithm. \n\n"}
{"id": "1901.01250", "contents": "Title: Learning Graph Embedding with Adversarial Training Methods Abstract: Graph embedding aims to transfer a graph into vectors to facilitate\nsubsequent graph analytics tasks like link prediction and graph clustering.\nMost approaches on graph embedding focus on preserving the graph structure or\nminimizing the reconstruction errors for graph data. They have mostly\noverlooked the embedding distribution of the latent codes, which unfortunately\nmay lead to inferior representation in many cases. In this paper, we present a\nnovel adversarially regularized framework for graph embedding. By employing the\ngraph convolutional network as an encoder, our framework embeds the topological\ninformation and node content into a vector representation, from which a graph\ndecoder is further built to reconstruct the input graph. The adversarial\ntraining principle is applied to enforce our latent codes to match a prior\nGaussian or Uniform distribution. Based on this framework, we derive two\nvariants of adversarial models, the adversarially regularized graph autoencoder\n(ARGA) and its variational version, adversarially regularized variational graph\nautoencoder (ARVGA), to learn the graph embedding effectively. We also exploit\nother potential variations of ARGA and ARVGA to get a deeper understanding on\nour designs. Experimental results compared among twelve algorithms for link\nprediction and twenty algorithms for graph clustering validate our solutions. \n\n"}
{"id": "1901.01484", "contents": "Title: LanczosNet: Multi-Scale Deep Graph Convolutional Networks Abstract: We propose the Lanczos network (LanczosNet), which uses the Lanczos algorithm\nto construct low rank approximations of the graph Laplacian for graph\nconvolution. Relying on the tridiagonal decomposition of the Lanczos algorithm,\nwe not only efficiently exploit multi-scale information via fast approximated\ncomputation of matrix power but also design learnable spectral filters. Being\nfully differentiable, LanczosNet facilitates both graph kernel learning as well\nas learning node embeddings. We show the connection between our LanczosNet and\ngraph based manifold learning methods, especially the diffusion maps. We\nbenchmark our model against several recent deep graph networks on citation\nnetworks and QM8 quantum chemistry dataset. Experimental results show that our\nmodel achieves the state-of-the-art performance in most tasks. Code is released\nat: \\url{https://github.com/lrjconan/LanczosNetwork}. \n\n"}
{"id": "1901.01960", "contents": "Title: Learning-based Optimization of the Under-sampling Pattern in MRI Abstract: Acquisition of Magnetic Resonance Imaging (MRI) scans can be accelerated by\nunder-sampling in k-space (i.e., the Fourier domain). In this paper, we\nconsider the problem of optimizing the sub-sampling pattern in a data-driven\nfashion. Since the reconstruction model's performance depends on the\nsub-sampling pattern, we combine the two problems. For a given sparsity\nconstraint, our method optimizes the sub-sampling pattern and reconstruction\nmodel, using an end-to-end learning strategy. Our algorithm learns from\nfull-resolution data that are under-sampled retrospectively, yielding a\nsub-sampling pattern and reconstruction model that are customized to the type\nof images represented in the training data. The proposed method, which we call\nLOUPE (Learning-based Optimization of the Under-sampling PattErn), was\nimplemented by modifying a U-Net, a widely-used convolutional neural network\narchitecture, that we append with the forward model that encodes the\nunder-sampling process. Our experiments with T1-weighted structural brain MRI\nscans show that the optimized sub-sampling pattern can yield significantly more\naccurate reconstructions compared to standard random uniform, variable density\nor equispaced under-sampling schemes. The code is made available at:\nhttps://github.com/cagladbahadir/LOUPE . \n\n"}
{"id": "1901.02291", "contents": "Title: Spectral Clustering via Ensemble Deep Autoencoder Learning (SC-EDAE) Abstract: Recently, a number of works have studied clustering strategies that combine\nclassical clustering algorithms and deep learning methods. These approaches\nfollow either a sequential way, where a deep representation is learned using a\ndeep autoencoder before obtaining clusters with k-means, or a simultaneous way,\nwhere deep representation and clusters are learned jointly by optimizing a\nsingle objective function. Both strategies improve clustering performance,\nhowever the robustness of these approaches is impeded by several deep\nautoencoder setting issues, among which the weights initialization, the width\nand number of layers or the number of epochs. To alleviate the impact of such\nhyperparameters setting on the clustering performance, we propose a new model\nwhich combines the spectral clustering and deep autoencoder strengths in an\nensemble learning framework. Extensive experiments on various benchmark\ndatasets demonstrate the potential and robustness of our approach compared to\nstate-of-the-art deep clustering methods. \n\n"}
{"id": "1901.02406", "contents": "Title: Using ZDDs in the Mapping of Quantum Circuits Abstract: A critical step in quantum compilation is the transformation of a\ntechnology-independent quantum circuit into a technology-dependent form for a\ntargeted device. In addition to mapping quantum gates into the supported gate\nset, it is necessary to map pseudo qubits in the technology-independent circuit\ninto physical qubits of the technology-dependent circuit such that coupling\nconstraints among qubits acting in multiple-qubit gates are satisfied. It is\nusually not possible to find such a mapping without adding SWAP gates into the\ncircuit. To cope with the technical limitations of NISQ-era quantum devices, it\nis advantageous to find a mapping that requires as few additional gates as\npossible. The large search space of possible mappings makes this task a\ndifficult combinatorial optimization problem. In this work, we demonstrate how\nzero-suppressed decision diagrams (ZDDs) can be used for typical implementation\ntasks in quantum mapping algorithms. We show how to maximally partition a\nquantum circuit into blocks of adjacent gates, and if adjacent gates within a\ncircuit do not share common mapping permutations, we attempt to combine them\nusing parallelized SWAP operations represented in a ZDD. Boundaries for the\npartitions are formed where adjacent gates are unable to be combined. Within\neach partition block, ZDDs represent all possible mappings of pseudo qubits to\nphysical qubits. \n\n"}
{"id": "1901.02511", "contents": "Title: Multi-stream CNN based Video Semantic Segmentation for Automated Driving Abstract: Majority of semantic segmentation algorithms operate on a single frame even\nin the case of videos. In this work, the goal is to exploit temporal\ninformation within the algorithm model for leveraging motion cues and temporal\nconsistency. We propose two simple high-level architectures based on Recurrent\nFCN (RFCN) and Multi-Stream FCN (MSFCN) networks. In case of RFCN, a recurrent\nnetwork namely LSTM is inserted between the encoder and decoder. MSFCN combines\nthe encoders of different frames into a fused encoder via 1x1 channel-wise\nconvolution. We use a ResNet50 network as the baseline encoder and construct\nthree networks namely MSFCN of order 2 & 3 and RFCN of order 2. MSFCN-3\nproduces the best results with an accuracy improvement of 9% and 15% for\nHighway and New York-like city scenarios in the SYNTHIA-CVPR'16 dataset using\nmean IoU metric. MSFCN-3 also produced 11% and 6% for SegTrack V2 and DAVIS\ndatasets over the baseline FCN network. We also designed an efficient version\nof MSFCN-2 and RFCN-2 using weight sharing among the two encoders. The\nefficient MSFCN-2 provided an improvement of 11% and 5% for KITTI and SYNTHIA\nwith negligible increase in computational complexity compared to the baseline\nversion. \n\n"}
{"id": "1901.02514", "contents": "Title: Autoencoders and Generative Adversarial Networks for Imbalanced Sequence\n  Classification Abstract: Generative Adversarial Networks (GANs) have been used in many different\napplications to generate realistic synthetic data. We introduce a novel GAN\nwith Autoencoder (GAN-AE) architecture to generate synthetic samples for\nvariable length, multi-feature sequence datasets. In this model, we develop a\nGAN architecture with an additional autoencoder component, where recurrent\nneural networks (RNNs) are used for each component of the model in order to\ngenerate synthetic data to improve classification accuracy for a highly\nimbalanced medical device dataset. In addition to the medical device dataset,\nwe also evaluate the GAN-AE performance on two additional datasets and\ndemonstrate the application of GAN-AE to a sequence-to-sequence task where both\nsynthetic sequence inputs and sequence outputs must be generated. To evaluate\nthe quality of the synthetic data, we train encoder-decoder models both with\nand without the synthetic data and compare the classification model\nperformance. We show that a model trained with GAN-AE generated synthetic data\noutperforms models trained with synthetic data generated both with standard\noversampling techniques such as SMOTE and Autoencoders as well as with state of\nthe art GAN-based models. \n\n"}
{"id": "1901.02871", "contents": "Title: The Lingering of Gradients: Theory and Applications Abstract: Classically, the time complexity of a first-order method is estimated by its\nnumber of gradient computations. In this paper, we study a more refined\ncomplexity by taking into account the `lingering' of gradients: once a gradient\nis computed at $x_k$, the additional time to compute gradients at\n$x_{k+1},x_{k+2},\\dots$ may be reduced.\n  We show how this improves the running time of several first-order methods.\nFor instance, if the `additional time' scales linearly with respect to the\ntraveled distance, then the `convergence rate' of gradient descent can be\nimproved from $1/T$ to $\\exp(-T^{1/3})$. On the application side, we solve a\nhypothetical revenue management problem on the Yahoo! Front Page Today Module\nwith 4.6m users to $10^{-6}$ error using only 6 passes of the dataset; and\nsolve a real-life support vector machine problem to an accuracy that is two\norders of magnitude better comparing to the state-of-the-art algorithm. \n\n"}
{"id": "1901.02878", "contents": "Title: A Constructive Approach for One-Shot Training of Neural Networks Using\n  Hypercube-Based Topological Coverings Abstract: In this paper we presented a novel constructive approach for training deep\nneural networks using geometric approaches. We show that a topological covering\ncan be used to define a class of distributed linear matrix inequalities, which\nin turn directly specify the shape and depth of a neural network architecture.\nThe key insight is a fundamental relationship between linear matrix\ninequalities and their ability to bound the shape of data, and the rectified\nlinear unit (ReLU) activation function employed in modern neural networks. We\nshow that unit cover geometry and cover porosity are two design variables in\ncover-constructive learning that play a critical role in defining the\ncomplexity of the model and generalizability of the resulting neural network\nclassifier. In the context of cover-constructive learning, these findings\nunderscore the age old trade-off between model complexity and overfitting (as\nquantified by the number of elements in the data cover) and generalizability on\ntest data. Finally, we benchmark on algorithm on the Iris, MNIST, and Wine\ndataset and show that the constructive algorithm is able to train a deep neural\nnetwork classifier in one shot, achieving equal or superior levels of training\nand test classification accuracy with reduced training time. \n\n"}
{"id": "1901.02920", "contents": "Title: TraceCaps: A Capsule-based Neural Network for Semantic Segmentation Abstract: In this paper, we propose a capsule-based neural network model to solve the\nsemantic segmentation problem. By taking advantage of the extractable\npart-whole dependencies available in capsule layers, we derive the\nprobabilities of the class labels for individual capsules through a recursive,\nlayer-by-layer procedure. We model this procedure as a traceback pipeline and\ntake it as a central piece to build an end-to-end segmentation network. Under\nthe proposed framework, image-level class labels and object boundaries are\njointly sought in an explicit manner, which poses a significant advantage over\nthe state-of-the-art fully convolutional network (FCN) solutions. With the\ncapability to extracted part-whole information, our traceback pipeline can\npotentially be utilized as the building blocks to design interpretable neural\nnetworks. Experiments conducted on modified MNIST and neuroimages demonstrate\nthat our model considerably enhance the segmentation performance compared to\nthe leading FCN variants. \n\n"}
{"id": "1901.03124", "contents": "Title: Active Learning for One-Class Classification Using Two One-Class\n  Classifiers Abstract: This paper introduces a novel, generic active learning method for one-class\nclassification. Active learning methods play an important role to reduce the\nefforts of manual labeling in the field of machine learning. Although many\nactive learning approaches have been proposed during the last years, most of\nthem are restricted on binary or multi-class problems. One-class classifiers\nuse samples from only one class, the so-called target class, during training\nand hence require special active learning strategies. The few strategies\nproposed for one-class classification either suffer from their limitation on\nspecific one-class classifiers or their performance depends on particular\nassumptions about datasets like imbalance. Our proposed method bases on using\ntwo one-class classifiers, one for the desired target class and one for the\nso-called outlier class. It allows to invent new query strategies, to use\nbinary query strategies and to define simple stopping criteria. Based on the\nnew method, two query strategies are proposed. The provided experiments compare\nthe proposed approach with known strategies on various datasets and show\nimproved results in almost all situations. \n\n"}
{"id": "1901.03407", "contents": "Title: Deep Learning for Anomaly Detection: A Survey Abstract: Anomaly detection is an important problem that has been well-studied within\ndiverse research areas and application domains. The aim of this survey is\ntwo-fold, firstly we present a structured and comprehensive overview of\nresearch methods in deep learning-based anomaly detection. Furthermore, we\nreview the adoption of these methods for anomaly across various application\ndomains and assess their effectiveness. We have grouped state-of-the-art\nresearch techniques into different categories based on the underlying\nassumptions and approach adopted. Within each category we outline the basic\nanomaly detection technique, along with its variants and present key\nassumptions, to differentiate between normal and anomalous behavior. For each\ncategory, we present we also present the advantages and limitations and discuss\nthe computational complexity of the techniques in real application domains.\nFinally, we outline open issues in research and challenges faced while adopting\nthese techniques. \n\n"}
{"id": "1901.03906", "contents": "Title: ChronoMID - Cross-Modal Neural Networks for 3-D Temporal Medical Imaging\n  Data Abstract: ChronoMID builds on the success of cross-modal convolutional neural networks\n(X-CNNs), making the novel application of the technique to medical imaging\ndata. Specifically, this paper presents and compares alternative approaches -\ntimestamps and difference images - to incorporate temporal information for the\nclassification of bone disease in mice, applied to micro-CT scans of mouse\ntibiae. Whilst much previous work on diseases and disease classification has\nbeen based on mathematical models incorporating domain expertise and the\nexplicit encoding of assumptions, the approaches given here utilise the growing\navailability of computing resources to analyse large datasets and uncover\nsubtle patterns in both space and time. After training on a balanced set of\nover 75000 images, all models incorporating temporal features outperformed a\nstate-of-the-art CNN baseline on an unseen, balanced validation set comprising\nover 20000 images. The top-performing model achieved 99.54% accuracy, compared\nto 73.02% for the CNN baseline. \n\n"}
{"id": "1901.03912", "contents": "Title: Real-time Joint Object Detection and Semantic Segmentation Network for\n  Automated Driving Abstract: Convolutional Neural Networks (CNN) are successfully used for various visual\nperception tasks including bounding box object detection, semantic\nsegmentation, optical flow, depth estimation and visual SLAM. Generally these\ntasks are independently explored and modeled. In this paper, we present a joint\nmulti-task network design for learning object detection and semantic\nsegmentation simultaneously. The main motivation is to achieve real-time\nperformance on a low power embedded SOC by sharing of encoder for both the\ntasks. We construct an efficient architecture using a small ResNet10 like\nencoder which is shared for both decoders. Object detection uses YOLO v2 like\ndecoder and semantic segmentation uses FCN8 like decoder. We evaluate the\nproposed network in two public datasets (KITTI, Cityscapes) and in our private\nfisheye camera dataset, and demonstrate that joint network provides the same\naccuracy as that of separate networks. We further optimize the network to\nachieve 30 fps for 1280x384 resolution image. \n\n"}
{"id": "1901.05808", "contents": "Title: AuxNet: Auxiliary tasks enhanced Semantic Segmentation for Automated\n  Driving Abstract: Decision making in automated driving is highly specific to the environment\nand thus semantic segmentation plays a key role in recognizing the objects in\nthe environment around the car. Pixel level classification once considered a\nchallenging task which is now becoming mature to be productized in a car.\nHowever, semantic annotation is time consuming and quite expensive. Synthetic\ndatasets with domain adaptation techniques have been used to alleviate the lack\nof large annotated datasets. In this work, we explore an alternate approach of\nleveraging the annotations of other tasks to improve semantic segmentation.\nRecently, multi-task learning became a popular paradigm in automated driving\nwhich demonstrates joint learning of multiple tasks improves overall\nperformance of each tasks. Motivated by this, we use auxiliary tasks like depth\nestimation to improve the performance of semantic segmentation task. We propose\nadaptive task loss weighting techniques to address scale issues in multi-task\nloss functions which become more crucial in auxiliary tasks. We experimented on\nautomotive datasets including SYNTHIA and KITTI and obtained 3% and 5%\nimprovement in accuracy respectively. \n\n"}
{"id": "1901.05850", "contents": "Title: Fast Deep Learning for Automatic Modulation Classification Abstract: In this work, we investigate the feasibility and effectiveness of employing\ndeep learning algorithms for automatic recognition of the modulation type of\nreceived wireless communication signals from subsampled data. Recent work\nconsidered a GNU radio-based data set that mimics the imperfections in a real\nwireless channel and uses 10 different modulation types. A Convolutional Neural\nNetwork (CNN) architecture was then developed and shown to achieve performance\nthat exceeds that of expert-based approaches. Here, we continue this line of\nwork and investigate deep neural network architectures that deliver high\nclassification accuracy. We identify three architectures - namely, a\nConvolutional Long Short-term Deep Neural Network (CLDNN), a Long Short-Term\nMemory neural network (LSTM), and a deep Residual Network (ResNet) - that lead\nto typical classification accuracy values around 90% at high SNR. We then study\nalgorithms to reduce the training time by minimizing the size of the training\ndata set, while incurring a minimal loss in classification accuracy. To this\nend, we demonstrate the performance of Principal Component Analysis in\nsignificantly reducing the training time, while maintaining good performance at\nlow SNR. We also investigate subsampling techniques that further reduce the\ntraining time, and pave the way for online classification at high SNR. Finally,\nwe identify representative SNR values for training each of the candidate\narchitectures, and consequently, realize drastic reductions of the training\ntime, with negligible loss in classification accuracy. \n\n"}
{"id": "1901.06413", "contents": "Title: Differentially Private High Dimensional Sparse Covariance Matrix\n  Estimation Abstract: In this paper, we study the problem of estimating the covariance matrix under\ndifferential privacy, where the underlying covariance matrix is assumed to be\nsparse and of high dimensions. We propose a new method, called DP-Thresholding,\nto achieve a non-trivial $\\ell_2$-norm based error bound, which is\nsignificantly better than the existing ones from adding noise directly to the\nempirical covariance matrix. We also extend the $\\ell_2$-norm based error bound\nto a general $\\ell_w$-norm based one for any $1\\leq w\\leq \\infty$, and show\nthat they share the same upper bound asymptotically. Our approach can be easily\nextended to local differential privacy. Experiments on the synthetic datasets\nshow consistent results with our theoretical claims. \n\n"}
{"id": "1901.06576", "contents": "Title: Towards Physically Safe Reinforcement Learning under Supervision Abstract: This paper addresses the question of how a previously available control\npolicy $\\pi_s$ can be used as a supervisor to more quickly and safely train a\nnew learned control policy $\\pi_L$ for a robot. A weighted average of the\nsupervisor and learned policies is used during trials, with a heavier weight\ninitially on the supervisor, in order to allow safe and useful physical trials\nwhile the learned policy is still ineffective. During the process, the weight\nis adjusted to favor the learned policy. As weights are adjusted, the learned\nnetwork must compensate so as to give safe and reasonable outputs under the\ndifferent weights. A pioneer network is introduced that pre-learns a policy\nthat performs similarly to the current learned policy under the planned next\nstep for new weights; this pioneer network then replaces the currently learned\nnetwork in the next set of trials. Experiments in OpenAI Gym demonstrate the\neffectiveness of the proposed method. \n\n"}
{"id": "1901.06580", "contents": "Title: Design of Real-time Semantic Segmentation Decoder for Automated Driving Abstract: Semantic segmentation remains a computationally intensive algorithm for\nembedded deployment even with the rapid growth of computation power. Thus\nefficient network design is a critical aspect especially for applications like\nautomated driving which requires real-time performance. Recently, there has\nbeen a lot of research on designing efficient encoders that are mostly task\nagnostic. Unlike image classification and bounding box object detection tasks,\ndecoders are computationally expensive as well for semantic segmentation task.\nIn this work, we focus on efficient design of the segmentation decoder and\nassume that an efficient encoder is already designed to provide shared features\nfor a multi-task learning system. We design a novel efficient non-bottleneck\nlayer and a family of decoders which fit into a small run-time budget using\nVGG10 as efficient encoder. We demonstrate in our dataset that experimentation\nwith various design choices led to an improvement of 10\\% from a baseline\nperformance. \n\n"}
{"id": "1901.07061", "contents": "Title: Prior Information Guided Regularized Deep Learning for Cell Nucleus\n  Detection Abstract: Cell nuclei detection is a challenging research topic because of limitations\nin cellular image quality and diversity of nuclear morphology, i.e. varying\nnuclei shapes, sizes, and overlaps between multiple cell nuclei. This has been\na topic of enduring interest with promising recent success shown by deep\nlearning methods. These methods train Convolutional Neural Networks (CNNs) with\na training set of input images and known, labeled nuclei locations. Many such\nmethods are supplemented by spatial or morphological processing. Using a set of\ncanonical cell nuclei shapes, prepared with the help of a domain expert, we\ndevelop a new approach that we call Shape Priors with Convolutional Neural\nNetworks (SP-CNN). We further extend the network to introduce a shape prior\n(SP) layer and then allowing it to become trainable (i.e. optimizable). We call\nthis network tunable SP-CNN (TSP-CNN). In summary, we present new network\nstructures that can incorporate 'expected behavior' of nucleus shapes via two\ncomponents: learnable layers that perform the nucleus detection and a fixed\nprocessing part that guides the learning with prior information. Analytically,\nwe formulate two new regularization terms that are targeted at: 1) learning the\nshapes, 2) reducing false positives while simultaneously encouraging detection\ninside the cell nucleus boundary. Experimental results on two challenging\ndatasets reveal that the proposed SP-CNN and TSP-CNN can outperform\nstate-of-the-art alternatives. \n\n"}
{"id": "1901.07329", "contents": "Title: The autofeat Python Library for Automated Feature Engineering and\n  Selection Abstract: This paper describes the autofeat Python library, which provides scikit-learn\nstyle linear regression and classification models with automated feature\nengineering and selection capabilities. Complex non-linear machine learning\nmodels, such as neural networks, are in practice often difficult to train and\neven harder to explain to non-statisticians, who require transparent analysis\nresults as a basis for important business decisions. While linear models are\nefficient and intuitive, they generally provide lower prediction accuracies.\nOur library provides a multi-step feature engineering and selection process,\nwhere first a large pool of non-linear features is generated, from which then a\nsmall and robust set of meaningful features is selected, which improve the\nprediction accuracy of a linear model while retaining its interpretability. \n\n"}
{"id": "1901.07657", "contents": "Title: Solving large Maximum Clique problems on a quantum annealer Abstract: Commercial quantum annealers from D-Wave Systems can find high quality\nsolutions of quadratic unconstrained binary optimization problems that can be\nembedded onto its hardware. However, even though such devices currently offer\nup to 2048 qubits, due to limitations on the connectivity of those qubits, the\nsize of problems that can typically be solved is rather small (around 65\nvariables). This limitation poses a problem for using D-Wave machines to solve\napplication-relevant problems, which can have thousands of variables. For the\nimportant Maximum Clique problem, this article investigates methods for\ndecomposing larger problem instances into smaller ones, which can subsequently\nbe solved on D-Wave. During the decomposition, we aim to prune as many\ngenerated subproblems that do not contribute to the solution as possible, in\norder to reduce the computational complexity. The reduction methods presented\nin this article include upper and lower bound heuristics in conjunction with\ngraph decomposition, vertex and edge extraction, and persistency analysis. \n\n"}
{"id": "1901.07860", "contents": "Title: Trust Region Value Optimization using Kalman Filtering Abstract: Policy evaluation is a key process in reinforcement learning. It assesses a\ngiven policy using estimation of the corresponding value function. When using a\nparameterized function to approximate the value, it is common to optimize the\nset of parameters by minimizing the sum of squared Bellman Temporal Differences\nerrors. However, this approach ignores certain distributional properties of\nboth the errors and value parameters. Taking these distributions into account\nin the optimization process can provide useful information on the amount of\nconfidence in value estimation. In this work we propose to optimize the value\nby minimizing a regularized objective function which forms a trust region over\nits parameters. We present a novel optimization method, the Kalman Optimization\nfor Value Approximation (KOVA), based on the Extended Kalman Filter. KOVA\nminimizes the regularized objective function by adopting a Bayesian perspective\nover both the value parameters and noisy observed returns. This distributional\nproperty provides information on parameter uncertainty in addition to value\nestimates. We provide theoretical results of our approach and analyze the\nperformance of our proposed optimizer on domains with large state and action\nspaces. \n\n"}
{"id": "1901.07868", "contents": "Title: Constant Time Graph Neural Networks Abstract: The recent advancements in graph neural networks (GNNs) have led to\nstate-of-the-art performances in various applications, including\nchemo-informatics, question-answering systems, and recommender systems.\nHowever, scaling up these methods to huge graphs, such as social networks and\nWeb graphs, remains a challenge. In particular, the existing methods for\naccelerating GNNs either are not theoretically guaranteed in terms of the\napproximation error or incur at least a linear time computation cost. In this\nstudy, we reveal the query complexity of the uniform node sampling scheme for\nMessage Passing Neural Networks, including GraphSAGE, graph attention networks\n(GATs), and graph convolutional networks (GCNs). Surprisingly, our analysis\nreveals that the complexity of the node sampling method is completely\nindependent of the number of the nodes, edges, and neighbors of the input and\ndepends only on the error tolerance and confidence probability while providing\na theoretical guarantee for the approximation error. To the best of our\nknowledge, this is the first paper to provide a theoretical guarantee of\napproximation for GNNs within constant time. Through experiments with synthetic\nand real-world datasets, we investigated the speed and precision of the node\nsampling scheme and validated our theoretical results. \n\n"}
{"id": "1901.07924", "contents": "Title: Online Learning with Diverse User Preferences Abstract: In this paper, we investigate the impact of diverse user preference on\nlearning under the stochastic multi-armed bandit (MAB) framework. We aim to\nshow that when the user preferences are sufficiently diverse and each arm can\nbe optimal for certain users, the O(log T) regret incurred by exploring the\nsub-optimal arms under the standard stochastic MAB setting can be reduced to a\nconstant. Our intuition is that to achieve sub-linear regret, the number of\ntimes an optimal arm being pulled should scale linearly in time; when all arms\nare optimal for certain users and pulled frequently, the estimated arm\nstatistics can quickly converge to their true values, thus reducing the need of\nexploration dramatically. We cast the problem into a stochastic linear bandits\nmodel, where both the users preferences and the state of arms are modeled as\n{independent and identical distributed (i.i.d)} d-dimensional random vectors.\nAfter receiving the user preference vector at the beginning of each time slot,\nthe learner pulls an arm and receives a reward as the linear product of the\npreference vector and the arm state vector. We also assume that the state of\nthe pulled arm is revealed to the learner once its pulled. We propose a\nWeighted Upper Confidence Bound (W-UCB) algorithm and show that it can achieve\na constant regret when the user preferences are sufficiently diverse. The\nperformance of W-UCB under general setups is also completely characterized and\nvalidated with synthetic data. \n\n"}
{"id": "1901.08013", "contents": "Title: DarwinML: A Graph-based Evolutionary Algorithm for Automated Machine\n  Learning Abstract: As an emerging field, Automated Machine Learning (AutoML) aims to reduce or\neliminate manual operations that require expertise in machine learning. In this\npaper, a graph-based architecture is employed to represent flexible\ncombinations of ML models, which provides a large searching space compared to\ntree-based and stacking-based architectures. Based on this, an evolutionary\nalgorithm is proposed to search for the best architecture, where the mutation\nand heredity operators are the key for architecture evolution. With Bayesian\nhyper-parameter optimization, the proposed approach can automate the workflow\nof machine learning. On the PMLB dataset, the proposed approach shows the\nstate-of-the-art performance compared with TPOT, Autostacker, and auto-sklearn.\nSome of the optimized models are with complex structures which are difficult to\nobtain in manual design. \n\n"}
{"id": "1901.08177", "contents": "Title: Generating and Aligning from Data Geometries with Generative Adversarial\n  Networks Abstract: Unsupervised domain mapping has attracted substantial attention in recent\nyears due to the success of models based on the cycle-consistency assumption.\nThese models map between two domains by fooling a probabilistic discriminator,\nthereby matching the probability distributions of the real and generated data.\nInstead of this probabilistic approach, we cast the problem in terms of\naligning the geometry of the manifolds of the two domains. We introduce the\nManifold Geometry Matching Generative Adversarial Network (MGM GAN), which adds\ntwo novel mechanisms to facilitate GANs sampling from the geometry of the\nmanifold rather than the density and then aligning two manifold geometries: (1)\nan importance sampling technique that reweights points based on their density\non the manifold, making the discriminator only able to discern geometry and (2)\na penalty adapted from traditional manifold alignment literature that\nexplicitly enforces the geometry to be preserved. The MGM GAN leverages the\nmanifolds arising from a pre-trained autoencoder to bridge the gap between\nformal manifold alignment literature and existing GAN work, and demonstrate the\nadvantages of modeling the manifold geometry over its density. \n\n"}
{"id": "1901.08244", "contents": "Title: Measurements of Three-Level Hierarchical Structure in the Outliers in\n  the Spectrum of Deepnet Hessians Abstract: We consider deep classifying neural networks. We expose a structure in the\nderivative of the logits with respect to the parameters of the model, which is\nused to explain the existence of outliers in the spectrum of the Hessian.\nPrevious works decomposed the Hessian into two components, attributing the\noutliers to one of them, the so-called Covariance of gradients. We show this\nterm is not a Covariance but a second moment matrix, i.e., it is influenced by\nmeans of gradients. These means possess an additive two-way structure that is\nthe source of the outliers in the spectrum. This structure can be used to\napproximate the principal subspace of the Hessian using certain \"averaging\"\noperations, avoiding the need for high-dimensional eigenanalysis. We\ncorroborate this claim across different datasets, architectures and sample\nsizes. \n\n"}
{"id": "1901.08652", "contents": "Title: Learning agile and dynamic motor skills for legged robots Abstract: Legged robots pose one of the greatest challenges in robotics. Dynamic and\nagile maneuvers of animals cannot be imitated by existing methods that are\ncrafted by humans. A compelling alternative is reinforcement learning, which\nrequires minimal craftsmanship and promotes the natural evolution of a control\npolicy. However, so far, reinforcement learning research for legged robots is\nmainly limited to simulation, and only few and comparably simple examples have\nbeen deployed on real systems. The primary reason is that training with real\nrobots, particularly with dynamically balancing systems, is complicated and\nexpensive. In the present work, we introduce a method for training a neural\nnetwork policy in simulation and transferring it to a state-of-the-art legged\nsystem, thereby leveraging fast, automated, and cost-effective data generation\nschemes. The approach is applied to the ANYmal robot, a sophisticated\nmedium-dog-sized quadrupedal system. Using policies trained in simulation, the\nquadrupedal machine achieves locomotion skills that go beyond what had been\nachieved with prior methods: ANYmal is capable of precisely and\nenergy-efficiently following high-level body velocity commands, running faster\nthan before, and recovering from falling even in complex configurations. \n\n"}
{"id": "1901.08665", "contents": "Title: Fairness risk measures Abstract: Ensuring that classifiers are non-discriminatory or fair with respect to a\nsensitive feature (e.g., race or gender) is a topical problem. Progress in this\ntask requires fixing a definition of fairness, and there have been several\nproposals in this regard over the past few years. Several of these, however,\nassume either binary sensitive features (thus precluding categorical or\nreal-valued sensitive groups), or result in non-convex objectives (thus\nadversely affecting the optimisation landscape). In this paper, we propose a\nnew definition of fairness that generalises some existing proposals, while\nallowing for generic sensitive features and resulting in a convex objective.\nThe key idea is to enforce that the expected losses (or risks) across each\nsubgroup induced by the sensitive feature are commensurate. We show how this\nrelates to the rich literature on risk measures from mathematical finance. As a\nspecial case, this leads to a new convex fairness-aware objective based on\nminimising the conditional value at risk (CVaR). \n\n"}
{"id": "1901.08817", "contents": "Title: State-Regularized Recurrent Neural Networks Abstract: Recurrent neural networks are a widely used class of neural architectures.\nThey have, however, two shortcomings. First, it is difficult to understand what\nexactly they learn. Second, they tend to work poorly on sequences requiring\nlong-term memorization, despite having this capacity in principle. We aim to\naddress both shortcomings with a class of recurrent networks that use a\nstochastic state transition mechanism between cell applications. This\nmechanism, which we term state-regularization, makes RNNs transition between a\nfinite set of learnable states. We evaluate state-regularized RNNs on (1)\nregular languages for the purpose of automata extraction; (2) nonregular\nlanguages such as balanced parentheses, palindromes, and the copy task where\nexternal memory is required; and (3) real-word sequence learning tasks for\nsentiment analysis, visual object recognition, and language modeling. We show\nthat state-regularization (a) simplifies the extraction of finite state\nautomata modeling an RNN's state transition dynamics; (b) forces RNNs to\noperate more like automata with external memory and less like finite state\nmachines; (c) makes RNNs have better interpretability and explainability. \n\n"}
{"id": "1901.08991", "contents": "Title: Diffusion Variational Autoencoders Abstract: A standard Variational Autoencoder, with a Euclidean latent space, is\nstructurally incapable of capturing topological properties of certain datasets.\nTo remove topological obstructions, we introduce Diffusion Variational\nAutoencoders with arbitrary manifolds as a latent space. A Diffusion\nVariational Autoencoder uses transition kernels of Brownian motion on the\nmanifold. In particular, it uses properties of the Brownian motion to implement\nthe reparametrization trick and fast approximations to the KL divergence. We\nshow that the Diffusion Variational Autoencoder is capable of capturing\ntopological properties of synthetic datasets. Additionally, we train MNIST on\nspheres, tori, projective spaces, SO(3), and a torus embedded in R3. Although a\nnatural dataset like MNIST does not have latent variables with a clear-cut\ntopological structure, training it on a manifold can still highlight\ntopological and geometrical properties. \n\n"}
{"id": "1901.09097", "contents": "Title: Driver Distraction Identification with an Ensemble of Convolutional\n  Neural Networks Abstract: The World Health Organization (WHO) reported 1.25 million deaths yearly due\nto road traffic accidents worldwide and the number has been continuously\nincreasing over the last few years. Nearly fifth of these accidents are caused\nby distracted drivers. Existing work of distracted driver detection is\nconcerned with a small set of distractions (mostly, cell phone usage).\nUnreliable ad-hoc methods are often used.In this paper, we present the first\npublicly available dataset for driver distraction identification with more\ndistraction postures than existing alternatives. In addition, we propose a\nreliable deep learning-based solution that achieves a 90% accuracy. The system\nconsists of a genetically-weighted ensemble of convolutional neural networks,\nwe show that a weighted ensemble of classifiers using a genetic algorithm\nyields in a better classification confidence. We also study the effect of\ndifferent visual elements in distraction detection by means of face and hand\nlocalizations, and skin segmentation. Finally, we present a thinned version of\nour ensemble that could achieve 84.64% classification accuracy and operate in a\nreal-time environment. \n\n"}
{"id": "1901.09206", "contents": "Title: Kernel-Guided Training of Implicit Generative Models with Stability\n  Guarantees Abstract: Modern implicit generative models such as generative adversarial networks\n(GANs) are generally known to suffer from issues such as instability,\nuninterpretability, and difficulty in assessing their performance. If we see\nthese implicit models as dynamical systems, some of these issues are caused by\nbeing unable to control their behavior in a meaningful way during the course of\ntraining. In this work, we propose a theoretically grounded method to guide the\ntraining trajectories of GANs by augmenting the GAN loss function with a\nkernel-based regularization term that controls local and global discrepancies\nbetween the model and true distributions. This control signal allows us to\ninject prior knowledge into the model. We provide theoretical guarantees on the\nstability of the resulting dynamical system and demonstrate different aspects\nof it via a wide range of experiments. \n\n"}
{"id": "1901.09335", "contents": "Title: Augment your batch: better training with larger batches Abstract: Large-batch SGD is important for scaling training of deep neural networks.\nHowever, without fine-tuning hyperparameter schedules, the generalization of\nthe model may be hampered. We propose to use batch augmentation: replicating\ninstances of samples within the same batch with different data augmentations.\nBatch augmentation acts as a regularizer and an accelerator, increasing both\ngeneralization and performance scaling. We analyze the effect of batch\naugmentation on gradient variance and show that it empirically improves\nconvergence for a wide variety of deep neural networks and datasets. Our\nresults show that batch augmentation reduces the number of necessary SGD\nupdates to achieve the same accuracy as the state-of-the-art. Overall, this\nsimple yet effective method enables faster training and better generalization\nby allowing more computational resources to be used concurrently. \n\n"}
{"id": "1901.09344", "contents": "Title: Stochastic Approximation of Smooth and Strongly Convex Functions: Beyond\n  the $O(1/T)$ Convergence Rate Abstract: Stochastic approximation (SA) is a classical approach for stochastic convex\noptimization. Previous studies have demonstrated that the convergence rate of\nSA can be improved by introducing either smoothness or strong convexity\ncondition. In this paper, we make use of smoothness and strong convexity\nsimultaneously to boost the convergence rate. Let $\\lambda$ be the modulus of\nstrong convexity, $\\kappa$ be the condition number, $F_*$ be the minimal risk,\nand $\\alpha>1$ be some small constant. First, we demonstrate that, in\nexpectation, an $O(1/[\\lambda T^\\alpha] + \\kappa F_*/T)$ risk bound is\nattainable when $T = \\Omega(\\kappa^\\alpha)$. Thus, when $F_*$ is small, the\nconvergence rate could be faster than $O(1/[\\lambda T])$ and approaches\n$O(1/[\\lambda T^\\alpha])$ in the ideal case. Second, to further benefit from\nsmall risk, we show that, in expectation, an $O(1/2^{T/\\kappa}+F_*)$ risk bound\nis achievable. Thus, the excess risk reduces exponentially until reaching\n$O(F_*)$, and if $F_*=0$, we obtain a global linear convergence. Finally, we\nemphasize that our proof is constructive and each risk bound is equipped with\nan efficient stochastic algorithm attaining that bound. \n\n"}
{"id": "1901.09462", "contents": "Title: A deep learning-based method for prostate segmentation in T2-weighted\n  magnetic resonance imaging Abstract: We propose a novel automatic method for accurate segmentation of the prostate\nin T2-weighted magnetic resonance imaging (MRI). Our method is based on\nconvolutional neural networks (CNNs). Because of the large variability in the\nshape, size, and appearance of the prostate and the scarcity of annotated\ntraining data, we suggest training two separate CNNs. A global CNN will\ndetermine a prostate bounding box, which is then resampled and sent to a local\nCNN for accurate delineation of the prostate boundary. This way, the local CNN\ncan effectively learn to segment the fine details that distinguish the prostate\nfrom the surrounding tissue using the small amount of available training data.\nTo fully exploit the training data, we synthesize additional data by deforming\nthe training images and segmentations using a learned shape model. We apply the\nproposed method on the PROMISE12 challenge dataset and achieve state of the art\nresults. Our proposed method generates accurate, smooth, and artifact-free\nsegmentations. On the test images, we achieve an average Dice score of 90.6\nwith a small standard deviation of 2.2, which is superior to all previous\nmethods. Our two-step segmentation approach and data augmentation strategy may\nbe highly effective in segmentation of other organs from small amounts of\nannotated medical images. \n\n"}
{"id": "1901.09532", "contents": "Title: Target Tracking for Contextual Bandits: Application to Demand Side\n  Management Abstract: We propose a contextual-bandit approach for demand side management by\noffering price incentives. More precisely, a target mean consumption is set at\neach round and the mean consumption is modeled as a complex function of the\ndistribution of prices sent and of some contextual variables such as the\ntemperature, weather, and so on. The performance of our strategies is measured\nin quadratic losses through a regret criterion. We offer $T^{2/3}$ upper bounds\non this regret (up to poly-logarithmic terms)---and even faster rates under\nstronger assumptions---for strategies inspired by standard strategies for\ncontextual bandits (like LinUCB, see Li et al., 2010). Simulations on a real\ndata set gathered by UK Power Networks, in which price incentives were offered,\nshow that our strategies are effective and may indeed manage demand response by\nsuitably picking the price levels. \n\n"}
{"id": "1901.09656", "contents": "Title: EXIT Analysis for Community Detection Abstract: This paper employs the extrinsic information transfer (EXIT) method, a\ntechnique imported from the analysis of the iterative decoding of error control\ncodes, to study the performance of belief propagation in community detection in\nthe presence of side information. We consider both the detection of a single\n(hidden) community, as well as the problem of identifying two symmetric\ncommunities. For single community detection, this paper demonstrates the\nsuitability of EXIT to predict the asymptotic phase transition for weak\nrecovery. More importantly, EXIT analysis is leveraged to produce useful\ninsights such as the performance of belief propagation near the threshold. For\ntwo symmetric communities, the asymptotic residual error for belief propagation\nis calculated under finite-alphabet side information, generalizing a previous\nresult with noisy labels. EXIT analysis is used to illuminate the effect of\nside information on community detection, its relative importance depending on\nthe correlation of the graphical information with node labels, as well as the\neffect of side information on residual errors. \n\n"}
{"id": "1901.09699", "contents": "Title: Dynamic Measurement Scheduling for Event Forecasting using Deep RL Abstract: Imagine a patient in critical condition. What and when should be measured to\nforecast detrimental events, especially under the budget constraints? We answer\nthis question by deep reinforcement learning (RL) that jointly minimizes the\nmeasurement cost and maximizes predictive gain, by scheduling\nstrategically-timed measurements. We learn our policy to be dynamically\ndependent on the patient's health history. To scale our framework to\nexponentially large action space, we distribute our reward in a sequential\nsetting that makes the learning easier. In our simulation, our policy\noutperforms heuristic-based scheduling with higher predictive gain and lower\ncost. In a real-world ICU mortality prediction task (MIMIC3), our policies\nreduce the total number of measurements by $31\\%$ or improve predictive gain by\na factor of $3$ as compared to physicians, under the off-policy policy\nevaluation. \n\n"}
{"id": "1901.09827", "contents": "Title: Depth creates no more spurious local minima Abstract: We show that for any convex differentiable loss, a deep linear network has no\nspurious local minima as long as it is true for the two layer case. This\nreduction greatly simplifies the study on the existence of spurious local\nminima in deep linear networks. When applied to the quadratic loss, our result\nimmediately implies the powerful result in [Kawaguchi 2016]. Further, with the\nwork in [Zhou and Liang 2018], we can remove all the assumptions in [Kawaguchi\n2016]. This property holds for more general \"multi-tower\" linear networks too.\nOur proof builds on [Laurent and von Brecht 2018] and develops a new\nperturbation argument to show that any spurious local minimum must have full\nrank, a structural property which can be useful more generally. \n\n"}
{"id": "1901.09888", "contents": "Title: Federated Collaborative Filtering for Privacy-Preserving Personalized\n  Recommendation System Abstract: The increasing interest in user privacy is leading to new privacy preserving\nmachine learning paradigms. In the Federated Learning paradigm, a master\nmachine learning model is distributed to user clients, the clients use their\nlocally stored data and model for both inference and calculating model updates.\nThe model updates are sent back and aggregated on the server to update the\nmaster model then redistributed to the clients. In this paradigm, the user data\nnever leaves the client, greatly enhancing the user' privacy, in contrast to\nthe traditional paradigm of collecting, storing and processing user data on a\nbackend server beyond the user's control. In this paper we introduce, as far as\nwe are aware, the first federated implementation of a Collaborative Filter. The\nfederated updates to the model are based on a stochastic gradient approach. As\na classical case study in machine learning, we explore a personalized\nrecommendation system based on users' implicit feedback and demonstrate the\nmethod's applicability to both the MovieLens and an in-house dataset. Empirical\nvalidation confirms a collaborative filter can be federated without a loss of\naccuracy compared to a standard implementation, hence enhancing the user's\nprivacy in a widely used recommender application while maintaining recommender\nperformance. \n\n"}
{"id": "1901.09919", "contents": "Title: Inferring Heterogeneous Causal Effects in Presence of Spatial\n  Confounding Abstract: We address the problem of inferring the causal effect of an exposure on an\noutcome across space, using observational data. The data is possibly subject to\nunmeasured confounding variables which, in a standard approach, must be\nadjusted for by estimating a nuisance function. Here we develop a method that\neliminates the nuisance function, while mitigating the resulting\nerrors-in-variables. The result is a robust and accurate inference method for\nspatially varying heterogeneous causal effects. The properties of the method\nare demonstrated on synthetic as well as real data from Germany and the US. \n\n"}
{"id": "1901.10417", "contents": "Title: Sliced generative models Abstract: In this paper we discuss a class of AutoEncoder based generative models based\non one dimensional sliced approach. The idea is based on the reduction of the\ndiscrimination between samples to one-dimensional case. Our experiments show\nthat methods can be divided into two groups. First consists of methods which\nare a modification of standard normality tests, while the second is based on\nclassical distances between samples. It turns out that both groups are correct\ngenerative models, but the second one gives a slightly faster decrease rate of\nFr\\'{e}chet Inception Distance (FID). \n\n"}
{"id": "1901.10435", "contents": "Title: A Deep Learning Framework for Assessing Physical Rehabilitation\n  Exercises Abstract: Computer-aided assessment of physical rehabilitation entails evaluation of\npatient performance in completing prescribed rehabilitation exercises, based on\nprocessing movement data captured with a sensory system. Despite the essential\nrole of rehabilitation assessment toward improved patient outcomes and reduced\nhealthcare costs, existing approaches lack versatility, robustness, and\npractical relevance. In this paper, we propose a deep learning-based framework\nfor automated assessment of the quality of physical rehabilitation exercises.\nThe main components of the framework are metrics for quantifying movement\nperformance, scoring functions for mapping the performance metrics into\nnumerical scores of movement quality, and deep neural network models for\ngenerating quality scores of input movements via supervised learning. The\nproposed performance metric is defined based on the log-likelihood of a\nGaussian mixture model, and encodes low-dimensional data representation\nobtained with a deep autoencoder network. The proposed deep spatio-temporal\nneural network arranges data into temporal pyramids, and exploits the spatial\ncharacteristics of human movements by using sub-networks to process joint\ndisplacements of individual body parts. The presented framework is validated\nusing a dataset of ten rehabilitation exercises. The significance of this work\nis that it is the first that implements deep neural networks for assessment of\nrehabilitation performance. \n\n"}
{"id": "1901.11084", "contents": "Title: A Comparative Analysis of Expected and Distributional Reinforcement\n  Learning Abstract: Since their introduction a year ago, distributional approaches to\nreinforcement learning (distributional RL) have produced strong results\nrelative to the standard approach which models expected values (expected RL).\nHowever, aside from convergence guarantees, there have been few theoretical\nresults investigating the reasons behind the improvements distributional RL\nprovides. In this paper we begin the investigation into this fundamental\nquestion by analyzing the differences in the tabular, linear approximation, and\nnon-linear approximation settings. We prove that in many realizations of the\ntabular and linear approximation settings, distributional RL behaves exactly\nthe same as expected RL. In cases where the two methods behave differently,\ndistributional RL can in fact hurt performance when it does not induce\nidentical behaviour. We then continue with an empirical analysis comparing\ndistributional and expected RL methods in control settings with non-linear\napproximators to tease apart where the improvements from distributional RL\nmethods are coming from. \n\n"}
{"id": "1901.11141", "contents": "Title: On the Consistency of Top-k Surrogate Losses Abstract: The top-$k$ error is often employed to evaluate performance for challenging\nclassification tasks in computer vision as it is designed to compensate for\nambiguity in ground truth labels. This practical success motivates our\ntheoretical analysis of consistent top-$k$ classification. Surprisingly, it is\nnot rigorously understood when taking the $k$-argmax of a vector is guaranteed\nto return the $k$-argmax of another vector, though doing so is crucial to\ndescribe Bayes optimality; we do both tasks. Then, we define top-$k$\ncalibration and show it is necessary and sufficient for consistency. Based on\nthe top-$k$ calibration analysis, we propose a class of top-$k$ calibrated\nBregman divergence surrogates. Our analysis continues by showing previously\nproposed hinge-like top-$k$ surrogate losses are not top-$k$ calibrated and\nsuggests no convex hinge loss is top-$k$ calibrated. On the other hand, we\npropose a new hinge loss which is consistent. We explore further, showing our\nhinge loss remains consistent under a restriction to linear functions, while\ncross entropy does not. Finally, we exhibit a differentiable, convex loss\nfunction which is top-$k$ calibrated for specific $k$. \n\n"}
{"id": "1901.11275", "contents": "Title: A Theory of Regularized Markov Decision Processes Abstract: Many recent successful (deep) reinforcement learning algorithms make use of\nregularization, generally based on entropy or Kullback-Leibler divergence. We\npropose a general theory of regularized Markov Decision Processes that\ngeneralizes these approaches in two directions: we consider a larger class of\nregularizers, and we consider the general modified policy iteration approach,\nencompassing both policy iteration and value iteration. The core building\nblocks of this theory are a notion of regularized Bellman operator and the\nLegendre-Fenchel transform, a classical tool of convex optimization. This\napproach allows for error propagation analyses of general algorithmic schemes\nof which (possibly variants of) classical algorithms such as Trust Region\nPolicy Optimization, Soft Q-learning, Stochastic Actor Critic or Dynamic Policy\nProgramming are special cases. This also draws connections to proximal convex\noptimization, especially to Mirror Descent. \n\n"}
{"id": "1901.11352", "contents": "Title: Deep Learning for Inverse Problems: Bounds and Regularizers Abstract: Inverse problems arise in a number of domains such as medical imaging, remote\nsensing, and many more, relying on the use of advanced signal and image\nprocessing approaches -- such as sparsity-driven techniques -- to determine\ntheir solution. This paper instead studies the use of deep learning approaches\nto approximate the solution of inverse problems. In particular, the paper\nprovides a new generalization bound, depending on key quantity associated with\na deep neural network -- its Jacobian matrix -- that also leads to a number of\ncomputationally efficient regularization strategies applicable to inverse\nproblems. The paper also tests the proposed regularization strategies in a\nnumber of inverse problems including image super-resolution ones. Our numerical\nresults conducted on various datasets show that both fully connected and\nconvolutional neural networks regularized using the regularization or proxy\nregularization strategies originating from our theory exhibit much better\nperformance than deep networks regularized with standard approaches such as\nweight-decay. \n\n"}
{"id": "1901.11379", "contents": "Title: TUNet: Incorporating segmentation maps to improve classification Abstract: Determining the localization of specific protein in human cells is important\nfor understanding cellular functions and biological processes of underlying\ndiseases. Among imaging techniques, high-throughput fluorescence microscopy\nimaging is an efficient biotechnology to stain the protein of interest in a\ncell. In this work, we present a novel classification model Twin U-Net (TUNet)\nfor processing and classifying the belonging of protein in the Atlas images.\nSeveral notable Deep Learning models including GoogleNet and Resnet have been\nemployed for comparison. Results have shown that our system obtaining\ncompetitive performance. \n\n"}
{"id": "1901.11409", "contents": "Title: Semantic Redundancies in Image-Classification Datasets: The 10% You\n  Don't Need Abstract: Large datasets have been crucial to the success of deep learning models in\nthe recent years, which keep performing better as they are trained with more\nlabelled data. While there have been sustained efforts to make these models\nmore data-efficient, the potential benefit of understanding the data itself, is\nlargely untapped. Specifically, focusing on object recognition tasks, we wonder\nif for common benchmark datasets we can do better than random subsets of the\ndata and find a subset that can generalize on par with the full dataset when\ntrained on. To our knowledge, this is the first result that can find notable\nredundancies in CIFAR-10 and ImageNet datasets (at least 10%). Interestingly,\nwe observe semantic correlations between required and redundant images. We hope\nthat our findings can motivate further research into identifying additional\nredundancies and exploiting them for more efficient training or\ndata-collection. \n\n"}
{"id": "1901.11518", "contents": "Title: Stochastic Recursive Variance-Reduced Cubic Regularization Methods Abstract: Stochastic Variance-Reduced Cubic regularization (SVRC) algorithms have\nreceived increasing attention due to its improved gradient/Hessian complexities\n(i.e., number of queries to stochastic gradient/Hessian oracles) to find local\nminima for nonconvex finite-sum optimization. However, it is unclear whether\nexisting SVRC algorithms can be further improved. Moreover, the semi-stochastic\nHessian estimator adopted in existing SVRC algorithms prevents the use of\nHessian-vector product-based fast cubic subproblem solvers, which makes SVRC\nalgorithms computationally intractable for high-dimensional problems. In this\npaper, we first present a Stochastic Recursive Variance-Reduced Cubic\nregularization method (SRVRC) using a recursively updated semi-stochastic\ngradient and Hessian estimators. It enjoys improved gradient and Hessian\ncomplexities to find an $(\\epsilon, \\sqrt{\\epsilon})$-approximate local\nminimum, and outperforms the state-of-the-art SVRC algorithms. Built upon\nSRVRC, we further propose a Hessian-free SRVRC algorithm, namely\nSRVRC$_{\\text{free}}$, which only requires stochastic gradient and\nHessian-vector product computations, and achieves $\\tilde O(dn\\epsilon^{-2}\n\\land d\\epsilon^{-3})$ runtime complexity, where $n$ is the number of component\nfunctions in the finite-sum structure, $d$ is the problem dimension, and\n$\\epsilon$ is the optimization precision. This outperforms the best-known\nruntime complexity $\\tilde O(d\\epsilon^{-3.5})$ achieved by stochastic cubic\nregularization algorithm proposed in Tripuraneni et al. 2018. \n\n"}
{"id": "quant-ph/0010082", "contents": "Title: Beyond Stabilizer Codes I: Nice Error Bases Abstract: Nice error bases have been introduced by Knill as a generalization of the\nPauli basis. These bases are shown to be projective representations of finite\ngroups. We classify all nice error bases of small degree, and all nice error\nbases with abelian index groups. We show that in general an index group of a\nnice error basis is necessarily solvable. \n\n"}
{"id": "quant-ph/0109088", "contents": "Title: Simulating Hamiltonians in Quantum Networks: Efficient Schemes and\n  Complexity Bounds Abstract: We address the problem of simulating pair-interaction Hamiltonians in n node\nquantum networks where the subsystems have arbitrary, possibly different,\ndimensions. We show that any pair-interaction can be used to simulate any other\nby applying sequences of appropriate local control sequences. Efficient schemes\nfor decoupling and time reversal can be constructed from orthogonal arrays.\nConditions on time optimal simulation are formulated in terms of spectral\nmajorization of matrices characterizing the coupling parameters. Moreover, we\nconsider a specific system of n harmonic oscillators with bilinear interaction.\nIn this case, decoupling can efficiently be achieved using the combinatorial\nconcept of difference schemes. For this type of interactions we present optimal\nschemes for inversion. \n\n"}
{"id": "quant-ph/0211014", "contents": "Title: Efficient Quantum Circuits for Non-Qubit Quantum Error-Correcting Codes Abstract: We present two methods for the construction of quantum circuits for quantum\nerror-correcting codes (QECC). The underlying quantum systems are tensor\nproducts of subsystems (qudits) of equal dimension which is a prime power. For\na QECC encoding k qudits into n qudits, the resulting quantum circuit has\nO(n(n-k)) gates. The running time of the classical algorithm to compute the\nquantum circuit is O(n(n-k)^2). \n\n"}
{"id": "quant-ph/0303091", "contents": "Title: Comment on \"Probabilistic Quantum Memories\" Abstract: This is a comment on two wrong Phys. Rev. Letters papers by C.A.\nTrugenberger. Trugenberger claimed that quantum registers could be used as\nexponentially large \"associative\" memories. We show that his scheme is no\nbetter than one where the quantum register is replaced with a classical one of\nequal size.\n  We also point out that the Holevo bound and more recent bounds on \"quantum\nrandom access codes\" pretty much rule out powerful memories (for classical\ninformation) based on quantum states. \n\n"}
{"id": "quant-ph/0309121", "contents": "Title: Quantum Software Reusability Abstract: The design of efficient quantum circuits is an important issue in quantum\ncomputing. It is in general a formidable task to find a highly optimized\nquantum circuit for a given unitary matrix. We propose a quantum circuit design\nmethod that has the following unique feature: It allows to construct efficient\nquantum circuits in a systematic way by reusing and combining a set of highly\noptimized quantum circuits. Specifically, the method realizes a quantum circuit\nfor a given unitary matrix by implementing a linear combination of representing\nmatrices of a group, which have known fast quantum circuits. We motivate and\nillustrate this method by deriving extremely efficient quantum circuits for the\ndiscrete Hartley transform and for the fractional Fourier transforms. The sound\nmathematical basis of this design method allows to give meaningful and natural\ninterpretations of the resulting circuits. We demonstrate this aspect by giving\na natural interpretation of known teleportation circuits. \n\n"}
{"id": "quant-ph/0312164", "contents": "Title: On optimal quantum codes Abstract: We present families of quantum error-correcting codes which are optimal in\nthe sense that the minimum distance is maximal. These maximum distance\nseparable (MDS) codes are defined over q-dimensional quantum systems, where q\nis an arbitrary prime power. It is shown that codes with parameters\n[[n,n-2d+2,d]]_q exist for all 3 <= n <= q and 1 <= d <= n/2+1. We also present\nquantum MDS codes with parameters [[q^2,q^2-2d+2,d]]_q for 1 <= d <= q which\nadditionally give rise to shortened codes [[q^2-s,q^2-2d+2-s,d]]_q for some s. \n\n"}
{"id": "quant-ph/0408078", "contents": "Title: Efficient Decoupling Schemes Based on Hamilton Cycles Abstract: Decoupling the interactions in a spin network governed by a pair-interaction\nHamiltonian is a well-studied problem. Combinatorial schemes for decoupling and\nfor manipulating the couplings of Hamiltonians have been developed which use\nselective pulses. In this paper we consider an additional requirement on these\npulse sequences: as few {\\em different} control operations as possible should\nbe used. This requirement is motivated by the fact that optimizing each\nindividual selective pulse will be expensive, i. e., it is desirable to use as\nfew different selective pulses as possible. For an arbitrary $d$-dimensional\nsystem we show that the ability to implement only two control operations is\nsufficient to turn off the time evolution. In case of a bipartite system with\nlocal control we show that four different control operations are sufficient.\nTurning to networks consisting of several $d$-dimensional nodes which are\ngoverned by a pair-interaction Hamiltonian, we show that decoupling can be\nachieved if one is able to control a number of different control operations\nwhich is logarithmic in the number of nodes. \n\n"}
{"id": "quant-ph/0409135", "contents": "Title: Equivalence of Decoupling Schemes and Orthogonal Arrays Abstract: We consider the problem of switching off unwanted interactions in a given\nmulti-partite Hamiltonian. This is known to be an important primitive in\nquantum information processing and several schemes have been presented in the\nliterature to achieve this task. A method to construct decoupling schemes for\nquantum systems of pairwise interacting qubits was introduced by M.\nStollsteimer and G. Mahler and is based on orthogonal arrays. Another approach\nbased on triples of Hadamard matrices that are closed under pointwise\nmultiplication was proposed by D. Leung. In this paper, we show that both\nmethods lead to the same class of decoupling schemes. Moreover, we establish a\ncharacterization of orthogonal arrays by showing that they are equivalent to\ndecoupling schemes which allow a refinement into equidistant time-slots.\nFurthermore, we show that decoupling schemes for networks of higher-dimensional\nquantum systems with t-local Hamiltonians can be constructed from classical\nerror-correcting codes. \n\n"}
{"id": "quant-ph/0502031", "contents": "Title: Mutually Unbiased Bases are Complex Projective 2-Designs Abstract: Mutually unbiased bases (MUBs) are a primitive used in quantum information\nprocessing to capture the principle of complementarity. While constructions of\nmaximal sets of d+1 such bases are known for systems of prime power dimension\nd, it is unknown whether this bound can be achieved for any non-prime power\ndimension. In this paper we demonstrate that maximal sets of MUBs come with a\nrich combinatorial structure by showing that they actually are the same objects\nas the complex projective 2-designs with angle set {0,1/d}. We also give a new\nand simple proof that symmetric informationally complete POVMs are complex\nprojective 2-designs with angle set {1/(d+1)}. \n\n"}
{"id": "quant-ph/0502101", "contents": "Title: Thresholds for Linear Optics Quantum Computing with Photon Loss at the\n  Detectors Abstract: We calculate the error threshold for the linear optics quantum computing\nproposal by Knill, Laflamme and Milburn [Nature 409, pp. 46--52 (2001)] under\nan error model where photon detectors have efficiency <100% but all other\ncomponents -- such as single photon sources, beam splitters and phase shifters\n-- are perfect and introduce no errors. We make use of the fact that the error\nmodel induced by the lossy hardware is that of an erasure channel, i.e., the\nerror locations are always known. Using a method based on a Markov chain\ndescription of the error correction procedure, our calculations show that, with\nthe 7 qubit CSS quantum code, the gate error threshold for fault tolerant\nquantum computation is bounded below by a value between 1.78% and 11.5%\ndepending on the construction of the entangling gates. \n\n"}
{"id": "quant-ph/0503239", "contents": "Title: On Approximately Symmetric Informationally Complete Positive\n  Operator-Valued Measures and Related Systems of Quantum States Abstract: We address the problem of constructing positive operator-valued measures\n(POVMs) in finite dimension $n$ consisting of $n^2$ operators of rank one which\nhave an inner product close to uniform. This is motivated by the related\nquestion of constructing symmetric informationally complete POVMs (SIC-POVMs)\nfor which the inner products are perfectly uniform. However, SIC-POVMs are\nnotoriously hard to construct and despite some success of constructing them\nnumerically, there is no analytic construction known. We present two\nconstructions of approximate versions of SIC-POVMs, where a small deviation\nfrom uniformity of the inner products is allowed. The first construction is\nbased on selecting vectors from a maximal collection of mutually unbiased bases\nand works whenever the dimension of the system is a prime power. The second\nconstruction is based on perturbing the matrix elements of a subset of mutually\nunbiased bases.\n  Moreover, we construct vector systems in $\\C^n$ which are almost orthogonal\nand which might turn out to be useful for quantum computation. Our\nconstructions are based on results of analytic number theory. \n\n"}
{"id": "quant-ph/0511148", "contents": "Title: Limitations of Quantum Coset States for Graph Isomorphism Abstract: It has been known for some time that graph isomorphism reduces to the hidden\nsubgroup problem (HSP). What is more, most exponential speedups in quantum\ncomputation are obtained by solving instances of the HSP. A common feature of\nthe resulting algorithms is the use of quantum coset states, which encode the\nhidden subgroup. An open question has been how hard it is to use these states\nto solve graph isomorphism. It was recently shown by Moore, Russell, and\nSchulman that only an exponentially small amount of information is available\nfrom one, or a pair of coset states. A potential source of power to exploit are\nentangled quantum measurements that act jointly on many states at once. We show\nthat entangled quantum measurements on at least \\Omega(n log n) coset states\nare necessary to get useful information for the case of graph isomorphism,\nmatching an information theoretic upper bound. This may be viewed as a negative\nresult because highly entangled measurements seem hard to implement in general.\nOur main theorem is very general and also rules out using joint measurements on\nfew coset states for some other groups, such as GL(n, F_{p^m}) and G^n where G\nis finite and satisfies a suitable property. \n\n"}
{"id": "quant-ph/9712040", "contents": "Title: Computing Local Invariants of Qubit Systems Abstract: We investigate means to describe the non-local properties of quantum systems\nand to test if two quantum systems are locally equivalent. For this we consider\nquantum systems that consist of several subsystems, especially multiple qubits.\nWe compute invariant polynomials, i. e., polynomial functions of the entries of\nthe density operator which are invariant under local unitary operations.\n  As an example, we consider a system of two qubits. We compute the Molien\nseries for the corresponding representation which gives information about the\nnumber of linearly independent invariants. Furthermore, we present a set of\npolynomials which generate all invariants (at least) up to degree 23. Finally,\nthe use of invariants to check whether two density operators are locally\nequivalent is demonstrated. \n\n"}
{"id": "quant-ph/9807064", "contents": "Title: Fast Quantum Fourier Transforms for a Class of Non-abelian Groups Abstract: An algorithm is presented allowing the construction of fast Fourier\ntransforms for any solvable group on a classical computer. The special\nstructure of the recursion formula being the core of this algorithm makes it a\ngood starting point to obtain systematically fast Fourier transforms for\nsolvable groups on a quantum computer. The inherent structure of the Hilbert\nspace imposed by the qubit architecture suggests to consider groups of order\n2^n first (where n is the number of qubits). As an example, fast quantum\nFourier transforms for all 4 classes of non-abelian 2-groups with cyclic normal\nsubgroup of index 2 are explicitly constructed in terms of quantum circuits.\nThe (quantum) complexity of the Fourier transform for these groups of size 2^n\nis O(n^2) in all cases. \n\n"}
{"id": "quant-ph/9812070", "contents": "Title: Polynomial-Time Solution to the Hidden Subgroup Problem for a Class of\n  non-abelian Groups Abstract: We present a family of non-abelian groups for which the hidden subgroup\nproblem can be solved efficiently on a quantum computer. \n\n"}
