{"id": "0705.3397", "contents": "Title: Beyond the PI Controllers in First-Order Time-Delay Systems Abstract: In this paper the following three control systems for first-order time-delay\nplants are studied and compared: the feedback proportional-integral controller\n(PI), the Smith Predictor (SP) and a proposed variable structure consisting of\ntwo blocks. This structure acts as an open-loop proportional controller, after\na setpoint change, and as a closed-loop integrating controller, when the error\nenters in a preset band. A chart, provided with the borderlines of the\nstability zone and with the curves of two design parameters, is implemented for\neach controller. The first parameter is the overshoot of the controlled\nvariable, evaluated during a step change of the setpoint and made equal to a\npreset value. The second parameter, only for the PI and SP controllers, is the\nintegral of the squared error (ISE), which must have the minimum allowable\nvalue. The ISE is also assumed as comparison index and the proposed controller\nappears as the best. \n\n"}
{"id": "0707.0335", "contents": "Title: Label-setting methods for Multimode Stochastic Shortest Path problems on\n  graphs Abstract: Stochastic shortest path (SSP) problems arise in a variety of discrete\nstochastic control contexts. An optimal solutions to such a problem is\ntypically computed using the value function, which can be found by solving the\ncorresponding dynamic programming equations. In the deterministic case, these\nequations can be often solved by the highly efficient label-setting methods\n(such as Dijkstra's and Dial's algorithms). In this paper we define and study a\nclass of Multimode Stochastic Shortest Path problems and develop sufficient\nconditions for the applicability of label-setting methods. We illustrate our\napproach on a number of discrete stochastic control examples. We also discuss\nthe relationship of SSPs with discretizations of static Hamilton-Jacobi\nequations and provide an alternative derivation for several fast\n(non-iterative) numerical methods for these PDEs. \n\n"}
{"id": "0709.3186", "contents": "Title: A Semismooth Newton Method for Tikhonov Functionals with Sparsity\n  Constraints Abstract: Minimization problems in $\\ell^2$ for Tikhonov functionals with sparsity\nconstraints are considered. Sparsity of the solution is ensured by a weighted\n$\\ell^1$ penalty term. The necessary and sufficient condition for optimality is\nshown to be slantly differentiable (Newton differentiable), hence a semismooth\nNewton method is applicable. Local superlinear convergence of this method is\nproved. Numerical examples are provided which show that our method compares\nfavorably with existing approaches. \n\n"}
{"id": "0802.3250", "contents": "Title: Valuation of Mortality Risk via the Instantaneous Sharpe Ratio:\n  Applications to Life Annuities Abstract: We develop a theory for valuing non-diversifiable mortality risk in an\nincomplete market. We do this by assuming that the company issuing a\nmortality-contingent claim requires compensation for this risk in the form of a\npre-specified instantaneous Sharpe ratio. We apply our method to value life\nannuities. One result of our paper is that the value of the life annuity is\n{\\it identical} to the upper good deal bound of Cochrane and Sa\\'{a}-Requejo\n(2000) and of Bj\\\"{o}rk and Slinko (2006) applied to our setting. A second\nresult of our paper is that the value per contract solves a {\\it linear}\npartial differential equation as the number of contracts approaches infinity.\nOne can represent the limiting value as an expectation with respect to an\nequivalent martingale measure (as in Blanchet-Scalliet, El Karoui, and\nMartellini (2005)), and from this representation, one can interpret the\ninstantaneous Sharpe ratio as an annuity market's price of mortality risk. \n\n"}
{"id": "0805.0910", "contents": "Title: Lyapunov control of a quantum particle in a decaying potential Abstract: A Lyapunov-based approach for the trajectory generation of an $N$-dimensional\nSchr{\\\"o}dinger equation in whole $\\RR^N$ is proposed. For the case of a\nquantum particle in an $N$-dimensional decaying potential the convergence is\nprecisely analyzed. The free system admitting a mixed spectrum, the dispersion\nthrough the absolutely continuous part is the main obstacle to ensure such a\nstabilization result. Whenever, the system is completely initialized in the\ndiscrete part of the spectrum, a Lyapunov strategy encoding both the distance\nwith respect to the target state and the penalization of the passage through\nthe continuous part of the spectrum, ensures the approximate stabilization. \n\n"}
{"id": "0807.1416", "contents": "Title: A Generalized Mixed Zero-sum Stochastic Differential Game and Double\n  Barrier Reflected BSDEs with Quadratic Growth Coefficient Abstract: This article is dedicated to the study of mixed zero-sum two-player\nstochastic differential games in the situation when the player's cost\nfunctionals are modeled by doubly controlled reflected backward stochastic\nequations with two barriers whose coefficients have quadratic growth in Z. This\nis a generalization of the risk-sensitive payoffs. We show that the lower and\nthe upper value function associated with this stochastic differential game with\nreflection are deterministic and they are also the unique viscosity solutions\nfor two Isaacs equations with obstacles. \n\n"}
{"id": "0902.2367", "contents": "Title: Dequantizing Compressed Sensing: When Oversampling and Non-Gaussian\n  Constraints Combine Abstract: In this paper we study the problem of recovering sparse or compressible\nsignals from uniformly quantized measurements. We present a new class of convex\noptimization programs, or decoders, coined Basis Pursuit DeQuantizer of moment\n$p$ (BPDQ$_p$), that model the quantization distortion more faithfully than the\ncommonly used Basis Pursuit DeNoise (BPDN) program. Our decoders proceed by\nminimizing the sparsity of the signal to be reconstructed subject to a\ndata-fidelity constraint expressed in the $\\ell_p$-norm of the residual error\nfor $2\\leq p\\leq \\infty$.\n  We show theoretically that, (i) the reconstruction error of these new\ndecoders is bounded if the sensing matrix satisfies an extended Restricted\nIsometry Property involving the $\\ell_p$ norm, and (ii), for Gaussian random\nmatrices and uniformly quantized measurements, BPDQ$_p$ performance exceeds\nthat of BPDN by dividing the reconstruction error due to quantization by\n$\\sqrt{p+1}$. This last effect happens with high probability when the number of\nmeasurements exceeds a value growing with $p$, i.e. in an oversampled situation\ncompared to what is commonly required by BPDN = BPDQ$_2$. To demonstrate the\ntheoretical power of BPDQ$_p$, we report numerical simulations on signal and\nimage reconstruction problems. \n\n"}
{"id": "0903.5332", "contents": "Title: On Gossez type (D) maximal monotone operators Abstract: Gossez type (D) operators are defined in non-reflexive Banach spaces and\nshare with the subdifferential a topological related property, characterized by\nbounded nets. In this work we present new properties and characterizations of\nthese operators. The class (NI) was defined after Gossez defined the class (D)\nand seemed to generalize the class (D). One of our main results is the proof\nthat these classes, type (D) and (NI), are identical. \n\n"}
{"id": "0905.1643", "contents": "Title: Fixed Point and Bregman Iterative Methods for Matrix Rank Minimization Abstract: The linearly constrained matrix rank minimization problem is widely\napplicable in many fields such as control, signal processing and system\nidentification. The tightest convex relaxation of this problem is the linearly\nconstrained nuclear norm minimization. Although the latter can be cast as a\nsemidefinite programming problem, such an approach is computationally expensive\nto solve when the matrices are large. In this paper, we propose fixed point and\nBregman iterative algorithms for solving the nuclear norm minimization problem\nand prove convergence of the first of these algorithms. By using a homotopy\napproach together with an approximate singular value decomposition procedure,\nwe get a very fast, robust and powerful algorithm, which we call FPCA (Fixed\nPoint Continuation with Approximate SVD), that can solve very large matrix rank\nminimization problems. Our numerical results on randomly generated and real\nmatrix completion problems demonstrate that this algorithm is much faster and\nprovides much better recoverability than semidefinite programming solvers such\nas SDPT3. For example, our algorithm can recover 1000 x 1000 matrices of rank\n50 with a relative error of 1e-5 in about 3 minutes by sampling only 20 percent\nof the elements. We know of no other method that achieves as good\nrecoverability. Numerical experiments on online recommendation, DNA microarray\ndata set and image inpainting problems demonstrate the effectiveness of our\nalgorithms. \n\n"}
{"id": "0906.3421", "contents": "Title: Q-system Cluster Algebras, Paths and Total Positivity Abstract: We review the solution of the $A_r$ Q-systems in terms of the partition\nfunction of paths on a weighted graph, and show that it is possible to modify\nthe graphs and transfer matrices so as to provide an explicit connection to the\ntheory of planar networks introduced in the context of totally positive\nmatrices by Fomin and Zelevinsky. \n\n"}
{"id": "0906.3499", "contents": "Title: Convergence of fixed-point continuation algorithms for matrix rank\n  minimization Abstract: The matrix rank minimization problem has applications in many fields such as\nsystem identification, optimal control, low-dimensional embedding, etc. As this\nproblem is NP-hard in general, its convex relaxation, the nuclear norm\nminimization problem, is often solved instead. Recently, Ma, Goldfarb and Chen\nproposed a fixed-point continuation algorithm for solving the nuclear norm\nminimization problem. By incorporating an approximate singular value\ndecomposition technique in this algorithm, the solution to the matrix rank\nminimization problem is usually obtained. In this paper, we study the\nconvergence/recoverability properties of the fixed-point continuation algorithm\nand its variants for matrix rank minimization. Heuristics for determining the\nrank of the matrix when its true rank is not known are also proposed. Some of\nthese algorithms are closely related to greedy algorithms in compressed\nsensing. Numerical results for these algorithms for solving affinely\nconstrained matrix rank minimization problems are reported. \n\n"}
{"id": "0909.0588", "contents": "Title: Receding horizon decoding of convolutional codes Abstract: Decoding of convolutional codes poses a significant challenge for coding\ntheory. Classical methods, based on e.g. Viterbi decoding, suffer from being\ncomputationally expensive and are restricted therefore to codes of small\ncomplexity. Based on analogies with model predictive optimal control, we\npropose a new iterative method for convolutional decoding that is cheaper to\nimplement than established algorithms, while still offering significant error\ncorrection capabilities. The algorithm is particularly well-suited for decoding\nspecial types of convolutional codes, such as e.g. cyclic convolutional codes. \n\n"}
{"id": "0912.1396", "contents": "Title: Time consistency and moving horizons for risk measures Abstract: We consider portfolio selection when decisions based on a dynamic risk\nmeasure are affected by the use of a moving horizon, and the possible\ninconsistencies that this creates. By giving a formal treatment of time\nconsistency which is independent of Bellman's equations, we show that there is\na new sense in which these decisions can be seen as consistent. \n\n"}
{"id": "0912.1845", "contents": "Title: Multiplicative Noise Removal Using Variable Splitting and Constrained\n  Optimization Abstract: Multiplicative noise (also known as speckle noise) models are central to the\nstudy of coherent imaging systems, such as synthetic aperture radar and sonar,\nand ultrasound and laser imaging. These models introduce two additional layers\nof difficulties with respect to the standard Gaussian additive noise scenario:\n(1) the noise is multiplied by (rather than added to) the original image; (2)\nthe noise is not Gaussian, with Rayleigh and Gamma being commonly used\ndensities. These two features of multiplicative noise models preclude the\ndirect application of most state-of-the-art algorithms, which are designed for\nsolving unconstrained optimization problems where the objective has two terms:\na quadratic data term (log-likelihood), reflecting the additive and Gaussian\nnature of the noise, plus a convex (possibly nonsmooth) regularizer (e.g., a\ntotal variation or wavelet-based regularizer/prior). In this paper, we address\nthese difficulties by: (1) converting the multiplicative model into an additive\none by taking logarithms, as proposed by some other authors; (2) using variable\nsplitting to obtain an equivalent constrained problem; and (3) dealing with\nthis optimization problem using the augmented Lagrangian framework. A set of\nexperiments shows that the proposed method, which we name MIDAL (multiplicative\nimage denoising by augmented Lagrangian), yields state-of-the-art results both\nin terms of speed and denoising performance. \n\n"}
{"id": "0912.3522", "contents": "Title: Proximal Splitting Methods in Signal Processing Abstract: The proximity operator of a convex function is a natural extension of the\nnotion of a projection operator onto a convex set. This tool, which plays a\ncentral role in the analysis and the numerical solution of convex optimization\nproblems, has recently been introduced in the arena of signal processing, where\nit has become increasingly important. In this paper, we review the basic\nproperties of proximity operators which are relevant to signal processing and\npresent optimization methods based on these operators. These proximal splitting\nmethods are shown to capture and extend several well-known algorithms in a\nunifying framework. Applications of proximal methods in signal recovery and\nsynthesis are discussed. \n\n"}
{"id": "1001.3015", "contents": "Title: Stochastic receding horizon control with output feedback and bounded\n  control inputs Abstract: We provide a solution to the problem of receding horizon control for\nstochastic discrete-time systems with bounded control inputs and imperfect\nstate measurements. For a suitable choice of control policies, we show that the\nfinite-horizon optimization problem to be solved on-line is convex and\nsuccessively feasible. Due to the inherent nonlinearity of the feedback loop, a\nslight extension of the Kalman filter is exploited to estimate the state\noptimally in mean-square sense. We show that the receding horizon\nimplementation of the resulting control policies renders the state of the\noverall system mean-square bounded under mild assumptions. Finally, we discuss\nhow some of the quantities required by the finite-horizon optimization problem\ncan be computed off-line, reducing the on-line computation, and present some\nnumerical examples. \n\n"}
{"id": "1003.3667", "contents": "Title: On synthesis of linear quantum stochastic systems by pure cascading Abstract: Recently, it has been demonstrated that an arbitrary linear quantum\nstochastic system can be realized as a cascade connection of simpler one degree\nof freedom quantum harmonic oscillators together with a direct interaction\nHamiltonian which is bilinear in the canonical operators of the oscillators.\nHowever, from an experimental point of view, realizations by pure cascading,\nwithout a direct interaction Hamiltonian, would be much simpler to implement\nand this raises the natural question of what class of linear quantum stochastic\nsystems are realizable by cascading alone. This paper gives a precise\ncharacterization of this class of linear quantum stochastic systems and then it\nis proved that, in the weaker sense of transfer function realizability, all\npassive linear quantum stochastic systems belong to this class. A constructive\nexample is given to show the transfer function realization of a two degrees of\nfreedom passive linear quantum stochastic system by pure cascading. \n\n"}
{"id": "1003.5941", "contents": "Title: A lower bound for distributed averaging algorithms Abstract: We derive lower bounds on the convergence speed of a widely used class of\ndistributed averaging algorithms. In particular, we prove that any distributed\naveraging algorithm whose state consists of a single real number and whose\n(possibly nonlinear) update function satisfies a natural smoothness condition\nhas a worst case running time of at least on the order of $n^2$ on a network of\n$n$ nodes. Our results suggest that increased memory or expansion of the state\nspace is crucial for improving the running times of distributed averaging\nalgorithms. \n\n"}
{"id": "1006.4392", "contents": "Title: Dynamics of Dengue epidemics using optimal control Abstract: We present an application of optimal control theory to Dengue epidemics. This\nepidemiologic disease is an important theme in tropical countries due to the\ngrowing number of infected individuals. The dynamic model is described by a set\nof nonlinear ordinary differential equations, that depend on the dynamic of the\nDengue mosquito, the number of infected individuals, and the people's\nmotivation to combat the mosquito. The cost functional depends not only on the\ncosts of medical treatment of the infected people but also on the costs related\nto educational and sanitary campaigns. Two approaches to solve the problem are\nconsidered: one using optimal control theory, another one by discretizing first\nthe problem and then solving it with nonlinear programming. The results\nobtained with OC-ODE and IPOPT solvers are given and discussed. We observe that\nwith current computational tools it is easy to obtain, in an efficient way,\nbetter solutions to Dengue problems, leading to a decrease of infected\nmosquitoes and individuals in less time and with lower costs. \n\n"}
{"id": "1006.5385", "contents": "Title: Matrix Completion by the Principle of Parsimony Abstract: Dempster's covariance selection method is extended first to general\nnonsingular matrices and then to full rank rectangular matrices. Dempster\nobserved that his completion solved a maximum entropy problem. We show that our\ngeneralized completions are also solutions of a suitable entropy-like\nvariational problem. \n\n"}
{"id": "1007.0584", "contents": "Title: Euler-Lagrange equations for composition functionals in calculus of\n  variations on time scales Abstract: In this paper we consider the problem of the calculus of variations for a\nfunctional which is the composition of a certain scalar function $H$ with the\ndelta integral of a vector valued field $f$, i.e., of the form\n$H(\\int_{a}^{b}f(t,x^{\\sigma}(t),x^{\\Delta}(t))\\Delta t)$. Euler-Lagrange\nequations, natural boundary conditions for such problems as well as a necessary\noptimality condition for isoperimetric problems, on a general time scale, are\ngiven. A number of corollaries are obtained, and several examples illustrating\nthe new results are discussed in detail. \n\n"}
{"id": "1007.0594", "contents": "Title: Necessary Optimality Conditions for Fractional Difference Problems of\n  the Calculus of Variations Abstract: We introduce a discrete-time fractional calculus of variations. First and\nsecond order necessary optimality conditions are established. Examples\nillustrating the use of the new Euler-Lagrange and Legendre type conditions are\ngiven. They show that the solutions of the fractional problems coincide with\nthe solutions of the corresponding non-fractional variational problems when the\norder of the discrete derivatives is an integer value. \n\n"}
{"id": "1009.2065", "contents": "Title: Templates for Convex Cone Problems with Applications to Sparse Signal\n  Recovery Abstract: This paper develops a general framework for solving a variety of convex cone\nproblems that frequently arise in signal processing, machine learning,\nstatistics, and other fields. The approach works as follows: first, determine a\nconic formulation of the problem; second, determine its dual; third, apply\nsmoothing; and fourth, solve using an optimal first-order method. A merit of\nthis approach is its flexibility: for example, all compressed sensing problems\ncan be solved via this approach. These include models with objective\nfunctionals such as the total-variation norm, ||Wx||_1 where W is arbitrary, or\na combination thereof. In addition, the paper also introduces a number of\ntechnical contributions such as a novel continuation scheme, a novel approach\nfor controlling the step size, and some new results showing that the smooth and\nunsmoothed problems are sometimes formally equivalent. Combined with our\nframework, these lead to novel, stable and computationally efficient\nalgorithms. For instance, our general implementation is competitive with\nstate-of-the-art methods for solving intensively studied problems such as the\nLASSO. Further, numerical experiments show that one can solve the Dantzig\nselector problem, for which no efficient large-scale solvers exist, in a few\nhundred iterations. Finally, the paper is accompanied with a software release.\nThis software is not a single, monolithic solver; rather, it is a suite of\nprograms and routines designed to serve as building blocks for constructing\ncomplete algorithms. \n\n"}
{"id": "1011.1547", "contents": "Title: Being Rational or Aggressive? A Revisit to Dunbar's Number in Online\n  Social Networks Abstract: Recent years have witnessed the explosion of online social networks (OSNs).\nThey provide powerful IT-innovations for online social activities such as\norganizing contacts, publishing contents, and sharing interests between friends\nwho may never meet before. As more and more people become the active users of\nonline social networks, one may ponder questions such as: (1) Do OSNs indeed\nimprove our sociability? (2) To what extent can we expand our offline social\nspectrum in OSNs? (3) Can we identify some interesting user behaviors in OSNs?\nOur work in this paper just aims to answer these interesting questions. To this\nend, we pay a revisit to the well-known Dunbar's number in online social\nnetworks. Our main research contributions are as follows. First, to our best\nknowledge, our work is the first one that systematically validates the\nexistence of the online Dunbar's number in the range of [200,300]. To reach\nthis, we combine using local-structure analysis and user-interaction analysis\nfor extensive real-world OSNs. Second, we divide OSNs users into two\ncategories: rational and aggressive, and find that rational users intend to\ndevelop close and reciprocated relationships, whereas aggressive users have no\nconsistent behaviors. Third, we build a simple model to capture the constraints\nof time and cognition that affect the evolution of online social networks.\nFinally, we show the potential use of our findings in viral marketing and\nprivacy management in online social networks. \n\n"}
{"id": "1011.4088", "contents": "Title: An Introduction to Conditional Random Fields Abstract: Often we wish to predict a large number of variables that depend on each\nother as well as on other observed variables. Structured prediction methods are\nessentially a combination of classification and graphical modeling, combining\nthe ability of graphical models to compactly model multivariate data with the\nability of classification methods to perform prediction using large sets of\ninput features. This tutorial describes conditional random fields, a popular\nprobabilistic method for structured prediction. CRFs have seen wide application\nin natural language processing, computer vision, and bioinformatics. We\ndescribe methods for inference and parameter estimation for CRFs, including\npractical issues for implementing large scale CRFs. We do not assume previous\nknowledge of graphical modeling, so this tutorial is intended to be useful to\npractitioners in a wide variety of fields. \n\n"}
{"id": "1011.5239", "contents": "Title: Preferential attachment in growing spatial networks Abstract: We obtain the degree distribution for a class of growing network models on\nflat and curved spaces. These models evolve by preferential attachment weighted\nby a function of the distance between nodes. The degree distribution of these\nmodels is similar to the one of the fitness model of Bianconi and Barabasi,\nwith a fitness distribution dependent on the metric and the density of nodes.\nWe show that curvature singularities in these spaces can give rise to\nasymptotic Bose-Einstein condensation, but transient condensation can be\nobserved also in smooth hyperbolic spaces with strong curvature. We provide\nnumerical results for spaces of constant curvature (sphere, flat and hyperbolic\nspace) and we discuss the conditions for the breakdown of this approach and the\ncritical points of the transition to distance-dominated attachment. Finally we\ndiscuss the distribution of link lengths. \n\n"}
{"id": "1011.6326", "contents": "Title: New Null Space Results and Recovery Thresholds for Matrix Rank\n  Minimization Abstract: Nuclear norm minimization (NNM) has recently gained significant attention for\nits use in rank minimization problems. Similar to compressed sensing, using\nnull space characterizations, recovery thresholds for NNM have been studied in\n\\cite{arxiv,Recht_Xu_Hassibi}. However simulations show that the thresholds are\nfar from optimal, especially in the low rank region. In this paper we apply the\nrecent analysis of Stojnic for compressed sensing \\cite{mihailo} to the null\nspace conditions of NNM. The resulting thresholds are significantly better and\nin particular our weak threshold appears to match with simulation results.\nFurther our curves suggest for any rank growing linearly with matrix size $n$\nwe need only three times of oversampling (the model complexity) for weak\nrecovery. Similar to \\cite{arxiv} we analyze the conditions for weak, sectional\nand strong thresholds. Additionally a separate analysis is given for special\ncase of positive semidefinite matrices. We conclude by discussing simulation\nresults and future research directions. \n\n"}
{"id": "1012.0975", "contents": "Title: Split Bregman Method for Sparse Inverse Covariance Estimation with\n  Matrix Iteration Acceleration Abstract: We consider the problem of estimating the inverse covariance matrix by\nmaximizing the likelihood function with a penalty added to encourage the\nsparsity of the resulting matrix. We propose a new approach based on the split\nBregman method to solve the regularized maximum likelihood estimation problem.\nWe show that our method is significantly faster than the widely used graphical\nlasso method, which is based on blockwise coordinate descent, on both\nartificial and real-world data. More importantly, different from the graphical\nlasso, the split Bregman based method is much more general, and can be applied\nto a class of regularization terms other than the $\\ell_1$ norm \n\n"}
{"id": "1012.1269", "contents": "Title: Identification of overlapping communities and their hierarchy by locally\n  calculating community-changing resolution levels Abstract: We propose a new local, deterministic and parameter-free algorithm that\ndetects fuzzy and crisp overlapping communities in a weighted network and\nsimultaneously reveals their hierarchy. Using a local fitness function, the\nalgorithm greedily expands natural communities of seeds until the whole graph\nis covered. The hierarchy of communities is obtained analytically by\ncalculating resolution levels at which communities grow rather than numerically\nby testing different resolution levels. This analytic procedure is not only\nmore exact than its numerical alternatives such as LFM and GCE but also much\nfaster. Critical resolution levels can be identified by searching for intervals\nin which large changes of the resolution do not lead to growth of communities.\nWe tested our algorithm on benchmark graphs and on a network of 492 papers in\ninformation science. Combined with a specific post-processing, the algorithm\ngives much more precise results on LFR benchmarks with high overlap compared to\nother algorithms and performs very similar to GCE. \n\n"}
{"id": "1012.1589", "contents": "Title: On the Support of Minimizers of Causal Variational Principles Abstract: A class of causal variational principles on a compact manifold is introduced\nand analyzed both numerically and analytically. It is proved under general\nassumptions that the support of a minimizing measure is either completely\ntimelike, or it is singular in the sense that its interior is empty. In the\nexamples of the circle, the sphere and certain flag manifolds, the general\nresults are supplemented by a more detailed and explicit analysis of the\nminimizers. On the sphere, we get a connection to packing problems and the\nTammes distribution. Moreover, the minimal action is estimated from above and\nbelow. \n\n"}
{"id": "1101.1057", "contents": "Title: Sparsity regret bounds for individual sequences in online linear\n  regression Abstract: We consider the problem of online linear regression on arbitrary\ndeterministic sequences when the ambient dimension d can be much larger than\nthe number of time rounds T. We introduce the notion of sparsity regret bound,\nwhich is a deterministic online counterpart of recent risk bounds derived in\nthe stochastic setting under a sparsity scenario. We prove such regret bounds\nfor an online-learning algorithm called SeqSEW and based on exponential\nweighting and data-driven truncation. In a second part we apply a\nparameter-free version of this algorithm to the stochastic setting (regression\nmodel with random design). This yields risk bounds of the same flavor as in\nDalalyan and Tsybakov (2011) but which solve two questions left open therein.\nIn particular our risk bounds are adaptive (up to a logarithmic factor) to the\nunknown variance of the noise if the latter is Gaussian. We also address the\nregression model with fixed design. \n\n"}
{"id": "1101.2478", "contents": "Title: Delay and Power-Optimal Control in Multi-Class Queueing Systems Abstract: We consider optimizing average queueing delay and average power consumption\nin a nonpreemptive multi-class M/G/1 queue with dynamic power control that\naffects instantaneous service rates. Four problems are studied: (1) satisfying\nper-class average delay constraints; (2) minimizing a separable convex function\nof average delays subject to per-class delay constraints; (3) minimizing\naverage power consumption subject to per-class delay constraints; (4)\nminimizing a separable convex function of average delays subject to an average\npower constraint. Combining an achievable region approach in queueing systems\nand the Lyapunov optimization theory suitable for optimizing dynamic systems\nwith time average constraints, we propose a unified framework to solve the\nabove problems. The solutions are variants of dynamic $c\\mu$ rules, and\nimplement weighted priority policies in every busy period, where weights are\ndetermined by past queueing delays in all job classes. Our solutions require\nlimited statistical knowledge of arrivals and service times, and no statistical\nknowledge is needed in the first problem. Overall, we provide a new set of\ntools for stochastic optimization and control over multi-class queueing systems\nwith time average constraints. \n\n"}
{"id": "1102.2620", "contents": "Title: Predicting economic market crises using measures of collective panic Abstract: Predicting panic is of critical importance in many areas of human and animal\nbehavior, notably in the context of economics. The recent financial crisis is a\ncase in point. Panic may be due to a specific external threat, or\nself-generated nervousness. Here we show that the recent economic crisis and\nearlier large single-day panics were preceded by extended periods of high\nlevels of market mimicry --- direct evidence of uncertainty and nervousness,\nand of the comparatively weak influence of external news. High levels of\nmimicry can be a quite general indicator of the potential for self-organized\ncrises. \n\n"}
{"id": "1104.0183", "contents": "Title: Exact and Efficient Algorithm to Discover Extreme Stochastic Events in\n  Wind Generation over Transmission Power Grids Abstract: In this manuscript we continue the thread of [M. Chertkov, F. Pan, M.\nStepanov, Predicting Failures in Power Grids: The Case of Static Overloads,\nIEEE Smart Grid 2011] and suggest a new algorithm discovering most probable\nextreme stochastic events in static power grids associated with intermittent\ngeneration of wind turbines. The algorithm becomes EXACT and EFFICIENT\n(polynomial) in the case of the proportional (or other low parametric) control\nof standard generation, and log-concave probability distribution of the\nrenewable generation, assumed known from the wind forecast. We illustrate the\nalgorithm's ability to discover problematic extreme events on the example of\nthe IEEE RTS-96 model of transmission with additions of 10%, 20% and 30% of\nrenewable generation. We observe that the probability of failure may grow but\nit may also decrease with increase in renewable penetration, if the latter is\nsufficiently diversified and distributed. \n\n"}
{"id": "1104.2970", "contents": "Title: On the Triality Theory in Global Optimization Abstract: Triality theory is proved for a general unconstrained global optimization\nproblem. The method adopted is simple but mathematically rigorous. Results show\nthat if the primal problem and its canonical dual have the same dimension, the\ntriality theory holds strongly in the tri-duality form as it was originally\nproposed. Otherwise, both the canonical min-max duality and the double-max\nduality still hold strongly, but the double-min duality holds weakly in a\nsuper-symmetrical form as it was expected. Additionally, a complementary weak\nsaddle min-max duality theorem is discovered. Therefore, an open problem on\nthis statement left in 2003 is solved completely. This theory can be used to\nidentify not only the global minimum, but also the largest local minimum,\nmaximum, and saddle points. Application is illustrated. Some fundamental\nconcepts in optimization and remaining challenging problems in canonical\nduality theory are discussed. \n\n"}
{"id": "1105.0728", "contents": "Title: Structured Sparsity via Alternating Direction Methods Abstract: We consider a class of sparse learning problems in high dimensional feature\nspace regularized by a structured sparsity-inducing norm which incorporates\nprior knowledge of the group structure of the features. Such problems often\npose a considerable challenge to optimization algorithms due to the\nnon-smoothness and non-separability of the regularization term. In this paper,\nwe focus on two commonly adopted sparsity-inducing regularization terms, the\noverlapping Group Lasso penalty $l_1/l_2$-norm and the $l_1/l_\\infty$-norm. We\npropose a unified framework based on the augmented Lagrangian method, under\nwhich problems with both types of regularization and their variants can be\nefficiently solved. As the core building-block of this framework, we develop\nnew algorithms using an alternating partial-linearization/splitting technique,\nand we prove that the accelerated versions of these algorithms require\n$O(\\frac{1}{\\sqrt{\\epsilon}})$ iterations to obtain an $\\epsilon$-optimal\nsolution. To demonstrate the efficiency and relevance of our algorithms, we\ntest them on a collection of data sets and apply them to two real-world\nproblems to compare the relative merits of the two norms. \n\n"}
{"id": "1105.2126", "contents": "Title: Fast First-Order Methods for Stable Principal Component Pursuit Abstract: The stable principal component pursuit (SPCP) problem is a non-smooth convex\noptimization problem, the solution of which has been shown both in theory and\nin practice to enable one to recover the low rank and sparse components of a\nmatrix whose elements have been corrupted by Gaussian noise. In this paper, we\nshow how several fast first-order methods can be applied to this problem very\nefficiently. Specifically, we show that the subproblems that arise when\napplying optimal gradient methods of Nesterov, alternating linearization\nmethods and alternating direction augmented Lagrangian methods to the SPCP\nproblem either have closed-form solutions or have solutions that can be\nobtained with very modest effort. All but one of the methods analyzed require\nat least one of the non-smooth terms in the objective function to be smoothed\nand obtain an eps-optimal solution to the SPCP problem in O(1/eps) iterations.\nThe method that works directly with the fully non-smooth objective function, is\nproved to be convergent under mild conditions on the sequence of parameters it\nuses. Our preliminary computational tests show that the latter method, although\nits complexity is not known, is fastest and substantially outperforms existing\nmethods for the SPCP problem. To best of our knowledge, an algorithm for the\nSPCP problem that has O(1/eps) iteration complexity and has a per iteration\ncomplexity equal to that of a singular value decomposition is given for the\nfirst time. \n\n"}
{"id": "1105.4049", "contents": "Title: A coordinate-free condition number for convex programming Abstract: We introduce and analyze a natural geometric version of Renegar's condition\nnumber R for the homogeneous convex feasibility problem associated with a\nregular cone C subseteq R^n. Let Gr_{n,m} denote the Grassmann manifold of\nm-dimensional linear subspaces of R^n and consider the projection distance\nd_p(W_1,W_2) := ||Pi_{W_1} - Pi_{W_2}|| (spectral norm) between W_1 and W_2 in\nGr_{n,m}, where Pi_{W_i} denotes the orthogonal projection onto W_i. We call\nC_G(W) := max {d_p(W,W')^{-1} | W' \\in Sigma_m} the Grassmann condition number\nof W in Gr_{n,m}, where the set of ill-posed instances Sigma_m subset Gr_{n,m}\nis defined as the set of linear subspaces touching C. We show that if W =\nim(A^T) for a matrix A in R^{m\\times n}, then C_G(W) \\le R(A) \\le C_G(W)\nkappa(A), where kappa(A) =||A|| ||A^\\dagger|| denotes the matrix condition\nnumber. This extends work by Belloni and Freund in Math. Program. 119:95-107\n(2009). Furthermore, we show that C_G(W) can as well be characterized in terms\nof the Riemannian distance metric on Gr_{n,m}. This differential geometric\ncharacterization of C_G(W) is the starting point of the sequel\n[arXiv:1112.2603] to this paper, where the first probabilistic analysis of\nRenegar's condition number for an arbitrary regular cone C is achieved. \n\n"}
{"id": "1106.5413", "contents": "Title: Accelerated Linearized Bregman Method Abstract: In this paper, we propose and analyze an accelerated linearized Bregman (ALB)\nmethod for solving the basis pursuit and related sparse optimization problems.\nThis accelerated algorithm is based on the fact that the linearized Bregman\n(LB) algorithm is equivalent to a gradient descent method applied to a certain\ndual formulation. We show that the LB method requires $O(1/\\epsilon)$\niterations to obtain an $\\epsilon$-optimal solution and the ALB algorithm\nreduces this iteration complexity to $O(1/\\sqrt{\\epsilon})$ while requiring\nalmost the same computational effort on each iteration. Numerical results on\ncompressed sensing and matrix completion problems are presented that\ndemonstrate that the ALB method can be significantly faster than the LB method. \n\n"}
{"id": "1106.6024", "contents": "Title: The Rate of Convergence of AdaBoost Abstract: The AdaBoost algorithm was designed to combine many \"weak\" hypotheses that\nperform slightly better than random guessing into a \"strong\" hypothesis that\nhas very low error. We study the rate at which AdaBoost iteratively converges\nto the minimum of the \"exponential loss.\" Unlike previous work, our proofs do\nnot require a weak-learning assumption, nor do they require that minimizers of\nthe exponential loss are finite. Our first result shows that at iteration $t$,\nthe exponential loss of AdaBoost's computed parameter vector will be at most\n$\\epsilon$ more than that of any parameter vector of $\\ell_1$-norm bounded by\n$B$ in a number of rounds that is at most a polynomial in $B$ and $1/\\epsilon$.\nWe also provide lower bounds showing that a polynomial dependence on these\nparameters is necessary. Our second result is that within $C/\\epsilon$\niterations, AdaBoost achieves a value of the exponential loss that is at most\n$\\epsilon$ more than the best possible value, where $C$ depends on the dataset.\nWe show that this dependence of the rate on $\\epsilon$ is optimal up to\nconstant factors, i.e., at least $\\Omega(1/\\epsilon)$ rounds are necessary to\nachieve within $\\epsilon$ of the optimal exponential loss. \n\n"}
{"id": "1107.1653", "contents": "Title: Multigrid methods for two-player zero-sum stochastic games Abstract: We present a fast numerical algorithm for large scale zero-sum stochastic\ngames with perfect information, which combines policy iteration and algebraic\nmultigrid methods. This algorithm can be applied either to a true finite state\nspace zero-sum two player game or to the discretization of an Isaacs equation.\nWe present numerical tests on discretizations of Isaacs equations or\nvariational inequalities. We also present a full multi-level policy iteration,\nsimilar to FMG, which allows to improve substantially the computation time for\nsolving some variational inequalities. \n\n"}
{"id": "1109.6179", "contents": "Title: On maximal S-free sets and the Helly number for the family of S-convex\n  sets Abstract: We study two combinatorial parameters, which we denote by f(S) and h(S),\nassociated to an arbitrary set S \\subseteq R^d, where d \\in N. In the\nnondegenerate situation, f(S) is the largest possible number of facets of a\nd-dimensional polyhedron L such that the interior of L is disjoint with S and L\nis inclusion-maximal with respect to this property. The parameter h(S) is the\nHelly number of the family of all sets that can be given as the intersection of\nS with a convex subset of R^d. We obtain the inequality f(S) \\le h(S) for an\narbitrary S and the equality f(S)=h(S) for every discrete S. Furthermore,\nmotivated by research in integer and mixed-integer optimization, we show that\n2^d is the sharp upper bound on f(S) in the case S = (Z^d \\times R^n) \\cap C,\nwhere n \\ge 0 and C \\subseteq R^{d+n} is convex. The presented material\ngeneralizes and unifies results of various authors, including the result h(Z^d)\n= 2^d of Doignon, the related result f(Z^d)=2^d of Lov\\'asz and the inequality\nf(Z^d \\cap C) \\le 2^d, which has recently been proved for every convex set C\n\\subseteq R^d by Dey & Mor\\'an. \n\n"}
{"id": "1112.1217", "contents": "Title: Entropy Search for Information-Efficient Global Optimization Abstract: Contemporary global optimization algorithms are based on local measures of\nutility, rather than a probability measure over location and value of the\noptimum. They thus attempt to collect low function values, not to learn about\nthe optimum. The reason for the absence of probabilistic global optimizers is\nthat the corresponding inference problem is intractable in several ways. This\npaper develops desiderata for probabilistic optimization algorithms, then\npresents a concrete algorithm which addresses each of the computational\nintractabilities with a sequence of approximations and explicitly adresses the\ndecision problem of maximizing information gain from each evaluation. \n\n"}
{"id": "1112.4002", "contents": "Title: Conjoining Speeds up Information Diffusion in Overlaying Social-Physical\n  Networks Abstract: We study the diffusion of information in an overlaying social-physical\nnetwork. Specifically, we consider the following set-up: There is a physical\ninformation network where information spreads amongst people through\nconventional communication media (e.g., face-to-face communication, phone\ncalls), and conjoint to this physical network, there are online social networks\nwhere information spreads via web sites such as Facebook, Twitter, FriendFeed,\nYouTube, etc. We quantify the size and the critical threshold of information\nepidemics in this conjoint social-physical network by assuming that information\ndiffuses according to the SIR epidemic model. One interesting finding is that\neven if there is no percolation in the individual networks, percolation (i.e.,\ninformation epidemics) can take place in the conjoint social-physical network.\nWe also show, both analytically and experimentally, that the fraction of\nindividuals who receive an item of information (started from an arbitrary node)\nis significantly larger in the conjoint social-physical network case, as\ncompared to the case where the networks are disjoint. These findings reveal\nthat conjoining the physical network with online social networks can have a\ndramatic impact on the speed and scale of information diffusion. \n\n"}
{"id": "1201.1712", "contents": "Title: Morphological methods for design of modular systems (a survey) Abstract: The article addresses morphological approaches to design of modular systems.\nThe following methods are briefly described: (i) basic version of morphological\nanalysis (MA), (ii) modification of MA as method of closeness to ideal\npoint(s), (iii reducing of MA to linear programming, (iv) multiple choice\nproblem, (v) quadratic assignment problem, (vi) Pareto-based MA (i.e.,\nrevelation of Pareto-efficient solutions), (vii) Hierarchical Morphological\nMulticriteria Design (HMMD) approach, and (viii) Hierarchical Morphological\nMulticriteria Design (HMMD) approach based on fuzzy estimates. The\nabove-mentioned methods are illustrated by schemes, models, and illustrative\nexamples. An additional realistic example (design of GSM network) is presented\nto illustrate main considered methods. \n\n"}
{"id": "1201.3218", "contents": "Title: Convex Optimization methods for computing the Lyapunov Exponent of\n  matrices Abstract: We introduce a new approach to evaluate the largest Lyapunov exponent of a\nfamily of nonnegative matrices. The method is based on using special positive\nhomogeneous functionals on $R^{d}_+,$ which gives iterative lower and upper\nbounds for the Lyapunov exponent. They improve previously known bounds and\nconverge to the real value. The rate of convergence is estimated and the\nefficiency of the algorithm is demonstrated on several problems from\napplications (in functional analysis, combinatorics, and lan- guage theory) and\non numerical examples with randomly generated matrices. The method computes the\nLyapunov exponent with a prescribed accuracy in relatively high dimensions (up\nto 60). We generalize this approach to all matrices, not necessar- ily\nnonnegative, derive a new universal upper bound for the Lyapunov exponent, and\nshow that such a lower bound, in general, does not exist. \n\n"}
{"id": "1201.3584", "contents": "Title: Ecological analysis of world trade Abstract: Ecological systems have a high level of complexity combined with stability\nand rich biodiversity. Recently, the analysis of their properties and evolution\nhas been pushed forward on a basis of concept of mutualistic networks that\nprovides a detailed understanding of their features being linked to a high\nnestedness of these networks. It was shown that the nestedness architecture of\nmutualistic networks of plants and their pollinators minimizes competition and\nincreases biodiversity. Here, using the United Nations COMTRADE database for\nyears 1962 - 2009, we show that a similar ecological analysis gives a valuable\ndescription of the world trade. In fact the countries and trade products are\nanalogous to plants and pollinators, and the whole trade network is\ncharacterized by a low nestedness temperature which is typical for the\necological networks. This approach provides new mutualistic features of the\nworld trade highlighting new significance of countries and trade products for\nthe world trade. \n\n"}
{"id": "1202.1119", "contents": "Title: Cramer Rao-Type Bounds for Sparse Bayesian Learning Abstract: In this paper, we derive Hybrid, Bayesian and Marginalized Cram\\'{e}r-Rao\nlower bounds (HCRB, BCRB and MCRB) for the single and multiple measurement\nvector Sparse Bayesian Learning (SBL) problem of estimating compressible\nvectors and their prior distribution parameters. We assume the unknown vector\nto be drawn from a compressible Student-t prior distribution. We derive CRBs\nthat encompass the deterministic or random nature of the unknown parameters of\nthe prior distribution and the regression noise variance. We extend the MCRB to\nthe case where the compressible vector is distributed according to a general\ncompressible prior distribution, of which the generalized Pareto distribution\nis a special case. We use the derived bounds to uncover the relationship\nbetween the compressibility and Mean Square Error (MSE) in the estimates.\nFurther, we illustrate the tightness and utility of the bounds through\nsimulations, by comparing them with the MSE performance of two popular\nSBL-based estimators. It is found that the MCRB is generally the tightest among\nthe bounds derived and that the MSE performance of the Expectation-Maximization\n(EM) algorithm coincides with the MCRB for the compressible vector. Through\nsimulations, we demonstrate the dependence of the MSE performance of SBL based\nestimators on the compressibility of the vector for several values of the\nnumber of observations and at different signal powers. \n\n"}
{"id": "1202.6144", "contents": "Title: Attack Detection and Identification in Cyber-Physical Systems -- Part I:\n  Models and Fundamental Limitations Abstract: Cyber-physical systems integrate computation, communication, and physical\ncapabilities to interact with the physical world and humans. Besides failures\nof components, cyber-physical systems are prone to malignant attacks, and\nspecific analysis tools as well as monitoring mechanisms need to be developed\nto enforce system security and reliability. This paper proposes a unified\nframework to analyze the resilience of cyber-physical systems against attacks\ncast by an omniscient adversary. We model cyber-physical systems as linear\ndescriptor systems, and attacks as exogenous unknown inputs. Despite its\nsimplicity, our model captures various real-world cyber-physical systems, and\nit includes and generalizes many prototypical attacks, including stealth,\n(dynamic) false-data injection and replay attacks. First, we characterize\nfundamental limitations of static, dynamic, and active monitors for attack\ndetection and identification. Second, we provide constructive algebraic\nconditions to cast undetectable and unidentifiable attacks. Third, by using the\nsystem interconnection structure, we describe graph-theoretic conditions for\nthe existence of undetectable and unidentifiable attacks. Finally, we validate\nour findings through some illustrative examples with different cyber-physical\nsystems, such as a municipal water supply network and two electrical power\ngrids. \n\n"}
{"id": "1203.1101", "contents": "Title: Structure theory for maximally monotone operators with points of\n  continuity Abstract: In this paper, we consider the structure of maximally monotone operators in\nBanach space whose domains have nonempty interior and we present new and\nexplicit structure formulas for such operators. Along the way, we provide new\nproofs of the norm-to-weak$^{*}$ closedness and of property (Q) for these\noperators (as recently proven by Voisei). Various applications and limiting\nexamples are given. \n\n"}
{"id": "1203.1422", "contents": "Title: A class of fractional optimal control problems and fractional\n  Pontryagin's systems. Existence of a fractional Noether's theorem Abstract: In this paper, we study a class of fractional optimal control problems. A\nnecessary condition for the existence of an optimal control is provided in the\nliterature. It is commonly given as the existence of a solution of a fractional\nPontryagin's system and the proof is based on the introduction of a Lagrange\nmultiplier. Assuming an additional condition on these problems, we suggest a\nnew presentation of this result with a proof using only classical mathematical\ntools adapted to the fractional case: calculus of variations, Gronwall's Lemma,\nCauchy-Lipschitz Theorem and stability under perturbations of differential\nequations. In this paper, we furthermore provide a way in order to transit from\na classical optimal control problem to its fractional version via the\nStanislavsky's formalism. We also solve a strict fractional example allowing to\ntest numerical schemes. Finally, we state a fractional Noether's theorem giving\nthe existence of an explicit constant of motion for fractional Pontryagin's\nsystems admitting a symmetry. \n\n"}
{"id": "1203.1676", "contents": "Title: Not all simplicial polytopes are weakly vertex-decomposable Abstract: In 1980 Provan and Billera defined the notion of weak $k$-decomposability for\npure simplicial complexes. They showed the diameter of a weakly\n$k$-decomposable simplicial complex $\\Delta$ is bounded above by a polynomial\nfunction of the number of $k$-faces in $\\Delta$ and its dimension. For weakly\n0-decomposable complexes, this bound is linear in the number of vertices and\nthe dimension. In this paper we exhibit the first examples of non-weakly\n0-decomposable simplicial polytopes. \n\n"}
{"id": "1203.3002", "contents": "Title: A Proximal-Gradient Homotopy Method for the Sparse Least-Squares Problem Abstract: We consider solving the $\\ell_1$-regularized least-squares ($\\ell_1$-LS)\nproblem in the context of sparse recovery, for applications such as compressed\nsensing. The standard proximal gradient method, also known as iterative\nsoft-thresholding when applied to this problem, has low computational cost per\niteration but a rather slow convergence rate. Nevertheless, when the solution\nis sparse, it often exhibits fast linear convergence in the final stage. We\nexploit the local linear convergence using a homotopy continuation strategy,\ni.e., we solve the $\\ell_1$-LS problem for a sequence of decreasing values of\nthe regularization parameter, and use an approximate solution at the end of\neach stage to warm start the next stage. Although similar strategies have been\nstudied in the literature, there have been no theoretical analysis of their\nglobal iteration complexity. This paper shows that under suitable assumptions\nfor sparse recovery, the proposed homotopy strategy ensures that all iterates\nalong the homotopy solution path are sparse. Therefore the objective function\nis effectively strongly convex along the solution path, and geometric\nconvergence at each stage can be established. As a result, the overall\niteration complexity of our method is $O(\\log(1/\\epsilon))$ for finding an\n$\\epsilon$-optimal solution, which can be interpreted as global geometric rate\nof convergence. We also present empirical results to support our theoretical\nanalysis. \n\n"}
{"id": "1204.0301", "contents": "Title: Tree Codes Improve Convergence Rate of Consensus Over Erasure Channels Abstract: We study the problem of achieving average consensus between a group of agents\nover a network with erasure links. In the context of consensus problems, the\nunreliability of communication links between nodes has been traditionally\nmodeled by allowing the underlying graph to vary with time. In other words,\ndepending on the realization of the link erasures, the underlying graph at each\ntime instant is assumed to be a subgraph of the original graph. Implicit in\nthis model is the assumption that the erasures are symmetric: if at time t the\npacket from node i to node j is dropped, the same is true for the packet\ntransmitted from node j to node i. However, in practical wireless communication\nsystems this assumption is unreasonable and, due to the lack of symmetry,\nstandard averaging protocols cannot guarantee that the network will reach\nconsensus to the true average. In this paper we explore the use of channel\ncoding to improve the performance of consensus algorithms. For symmetric\nerasures, we show that, for certain ranges of the system parameters, repetition\ncodes can speed up the convergence rate. For asymmetric erasures we show that\ntree codes (which have recently been designed for erasure channels) can be used\nto simulate the performance of the original \"unerased\" graph. Thus, unlike\nconventional consensus methods, we can guarantee convergence to the average in\nthe asymmetric case. The price is a slowdown in the convergence rate, relative\nto the unerased network, which is still often faster than the convergence rate\nof conventional consensus algorithms over noisy links. \n\n"}
{"id": "1204.0590", "contents": "Title: Linear System Identification via Atomic Norm Regularization Abstract: This paper proposes a new algorithm for linear system identification from\nnoisy measurements. The proposed algorithm balances a data fidelity term with a\nnorm induced by the set of single pole filters. We pose a convex optimization\nproblem that approximately solves the atomic norm minimization problem and\nidentifies the unknown system from noisy linear measurements. This problem can\nbe solved efficiently with standard, freely available software. We provide\nrigorous statistical guarantees that explicitly bound the estimation error (in\nthe H_2-norm) in terms of the stability radius, the Hankel singular values of\nthe true system and the number of measurements. These results in turn yield\ncomplexity bounds and asymptotic consistency. We provide numerical experiments\ndemonstrating the efficacy of our method for estimating linear systems from a\nvariety of linear measurements. \n\n"}
{"id": "1204.0991", "contents": "Title: Distributed Robust Power System State Estimation Abstract: Deregulation of energy markets, penetration of renewables, advanced metering\ncapabilities, and the urge for situational awareness, all call for system-wide\npower system state estimation (PSSE). Implementing a centralized estimator\nthough is practically infeasible due to the complexity scale of an\ninterconnection, the communication bottleneck in real-time monitoring, regional\ndisclosure policies, and reliability issues. In this context, distributed PSSE\nmethods are treated here under a unified and systematic framework. A novel\nalgorithm is developed based on the alternating direction method of\nmultipliers. It leverages existing PSSE solvers, respects privacy policies,\nexhibits low communication load, and its convergence to the centralized\nestimates is guaranteed even in the absence of local observability. Beyond the\nconventional least-squares based PSSE, the decentralized framework accommodates\na robust state estimator. By exploiting interesting links to the compressive\nsampling advances, the latter jointly estimates the state and identifies\ncorrupted measurements. The novel algorithms are numerically evaluated using\nthe IEEE 14-, 118-bus, and a 4,200-bus benchmarks. Simulations demonstrate that\nthe attainable accuracy can be reached within a few inter-area exchanges, while\nlargest residual tests are outperformed. \n\n"}
{"id": "1204.1664", "contents": "Title: Optimally-Weighted Herding is Bayesian Quadrature Abstract: Herding and kernel herding are deterministic methods of choosing samples\nwhich summarise a probability distribution. A related task is choosing samples\nfor estimating integrals using Bayesian quadrature. We show that the criterion\nminimised when selecting samples in kernel herding is equivalent to the\nposterior variance in Bayesian quadrature. We then show that sequential\nBayesian quadrature can be viewed as a weighted version of kernel herding which\nachieves performance superior to any other weighted herding method. We\ndemonstrate empirically a rate of convergence faster than O(1/N). Our results\nalso imply an upper bound on the empirical error of the Bayesian quadrature\nestimate. \n\n"}
{"id": "1204.4227", "contents": "Title: Estimating Unknown Sparsity in Compressed Sensing Abstract: In the theory of compressed sensing (CS), the sparsity ||x||_0 of the unknown\nsignal x\\in\\R^p is commonly assumed to be a known parameter. However, it is\ntypically unknown in practice. Due to the fact that many aspects of CS depend\non knowing ||x||_0, it is important to estimate this parameter in a data-driven\nway. A second practical concern is that ||x||_0 is a highly unstable function\nof x. In particular, for real signals with entries not exactly equal to 0, the\nvalue ||x||_0=p is not a useful description of the effective number of\ncoordinates. In this paper, we propose to estimate a stable measure of sparsity\ns(x):=||x||_1^2/||x||_2^2, which is a sharp lower bound on ||x||_0. Our\nestimation procedure uses only a small number of linear measurements, does not\nrely on any sparsity assumptions, and requires very little computation. A\nconfidence interval for s(x) is provided, and its width is shown to have no\ndependence on the signal dimension p. Moreover, this result extends naturally\nto the matrix recovery setting, where a soft version of matrix rank can be\nestimated with analogous guarantees. Finally, we show that the use of\nrandomized measurements is essential to estimating s(x). This is accomplished\nby proving that the minimax risk for estimating s(x) with deterministic\nmeasurements is large when n<<p. \n\n"}
{"id": "1204.4719", "contents": "Title: Optimal Surface Marker Locations for Tumor Motion Estimation in Lung\n  Cancer Radiotherapy Abstract: Using fiducial markers on patient's body surface to predict the tumor\nlocation is a widely used approach in lung cancer radiotherapy. The purpose of\nthis work is to propose an algorithm that automatically identifies a sparse set\nof locations on the patient's surface with the optimal prediction power for the\ntumor motion. The sparse selection of markers on the external surface and the\nassumed linear relationship between the marker motion and the internal tumor\nmotion are represented by a prediction matrix. Such a matrix is determined by\nsolving an optimization problem, where the objective function contains a\nsparsity term that penalizes the number of markers chosen on the patient's\nsurface. The performance of our algorithm has been tested on realistic clinical\ndata of four lung cancer patients. Thoracic 4DCT scans with 10 phases are used\nfor the study. On a reference phase, a grid of points are casted on the\npatient's surface (except for patient's back) and propagated to other phases\nvia deformable image registration of the corresponding CT images. Tumor\nlocations at each phase are also manually delineated. We use 9 out of 10 phases\nof the 4DCT images to identify a small group of surface markers that are most\ncorrelated with the motion of the tumor, and find the prediction matrix at the\nsame time. The 10th phase is then used to test the accuracy of the prediction.\nIt is found that on average 6 to 7 surface markers are necessary to predict\ntumor locations with a 3D error of about 1mm. In addition, the selected marker\nlocations lie closely in those areas where surface point motion has a high\ncorrelation with the tumor motion. Our method can automatically select sparse\nlocations on patient's external surface and estimate a correlation matrix based\non 4DCT, so that the selected surface locations can be used to place fiducial\nmarkers to optimally predict internal tumor motions. \n\n"}
{"id": "1204.5636", "contents": "Title: Social Networks with Competing Products Abstract: We introduce a new threshold model of social networks, in which the nodes\ninfluenced by their neighbours can adopt one out of several alternatives. We\ncharacterize social networks for which adoption of a product by the whole\nnetwork is possible (respectively necessary) and the ones for which a unique\noutcome is guaranteed. These characterizations directly yield polynomial time\nalgorithms that allow us to determine whether a given social network satisfies\none of the above properties.\n  We also study algorithmic questions for networks without unique outcomes. We\nshow that the problem of determining whether a final network exists in which\nall nodes adopted some product is NP-complete. In turn, the problems of\ndetermining whether a given node adopts some (respectively, a given) product in\nsome (respectively, all) network(s) are either co-NP complete or can be solved\nin polynomial time.\n  Further, we show that the problem of computing the minimum possible spread of\na product is NP-hard to approximate with an approximation ratio better than\n$\\Omega(n)$, in contrast to the maximum spread, which is efficiently\ncomputable. Finally, we clarify that some of the above problems can be solved\nin polynomial time when there are only two products. \n\n"}
{"id": "1204.6376", "contents": "Title: The Landscape of Complex Networks Abstract: Topological landscape is introduced for networks with functions defined on\nthe nodes. By extending the notion of gradient flows to the network setting,\ncritical nodes of different indices are defined. This leads to a concise and\nhierarchical representation of the network. Persistent homology from\ncomputational topology is used to design efficient algorithms for performing\nsuch analysis. Applications to some examples in social and biological networks\nare demonstrated, which show that critical nodes carry important information\nabout structures and dynamics of such networks. \n\n"}
{"id": "1205.2081", "contents": "Title: The Computational Complexity of the Restricted Isometry Property, the\n  Nullspace Property, and Related Concepts in Compressed Sensing Abstract: This paper deals with the computational complexity of conditions which\nguarantee that the NP-hard problem of finding the sparsest solution to an\nunderdetermined linear system can be solved by efficient algorithms. In the\nliterature, several such conditions have been introduced. The most well-known\nones are the mutual coherence, the restricted isometry property (RIP), and the\nnullspace property (NSP). While evaluating the mutual coherence of a given\nmatrix is easy, it has been suspected for some time that evaluating RIP and NSP\nis computationally intractable in general. We confirm these conjectures by\nshowing that for a given matrix A and positive integer k, computing the best\nconstants for which the RIP or NSP hold is, in general, NP-hard. These results\nare based on the fact that determining the spark of a matrix is NP-hard, which\nis also established in this paper. Furthermore, we also give several complexity\nstatements about problems related to the above concepts. \n\n"}
{"id": "1206.1156", "contents": "Title: A quasi-Newton proximal splitting method Abstract: A new result in convex analysis on the calculation of proximity operators in\ncertain scaled norms is derived. We describe efficient implementations of the\nproximity calculation for a useful class of functions; the implementations\nexploit the piece-wise linear nature of the dual problem. The second part of\nthe paper applies the previous result to acceleration of convex minimization\nproblems, and leads to an elegant quasi-Newton method. The optimization method\ncompares favorably against state-of-the-art alternatives. The algorithm has\nextensive applications including signal processing, sparse recovery and machine\nlearning and classification. \n\n"}
{"id": "1206.6325", "contents": "Title: Stochastic target games with controlled loss Abstract: We study a stochastic game where one player tries to find a strategy such\nthat the state process reaches a target of controlled-loss-type, no matter\nwhich action is chosen by the other player. We provide, in a general setup, a\nrelaxed geometric dynamic programming principle for this problem and derive,\nfor the case of a controlled SDE, the corresponding dynamic programming\nequation in the sense of viscosity solutions. As an example, we consider a\nproblem of partial hedging under Knightian uncertainty. \n\n"}
{"id": "1207.0188", "contents": "Title: Model-based clustering of large networks Abstract: We describe a network clustering framework, based on finite mixture models,\nthat can be applied to discrete-valued networks with hundreds of thousands of\nnodes and billions of edge variables. Relative to other recent model-based\nclustering work for networks, we introduce a more flexible modeling framework,\nimprove the variational-approximation estimation algorithm, discuss and\nimplement standard error estimation via a parametric bootstrap approach, and\napply these methods to much larger data sets than those seen elsewhere in the\nliterature. The more flexible framework is achieved through introducing novel\nparameterizations of the model, giving varying degrees of parsimony, using\nexponential family models whose structure may be exploited in various\ntheoretical and algorithmic ways. The algorithms are based on variational\ngeneralized EM algorithms, where the E-steps are augmented by a\nminorization-maximization (MM) idea. The bootstrapped standard error estimates\nare based on an efficient Monte Carlo network simulation idea. Last, we\ndemonstrate the usefulness of the model-based clustering framework by applying\nit to a discrete-valued network with more than 131,000 nodes and 17 billion\nedge variables. \n\n"}
{"id": "1207.1119", "contents": "Title: On unified view of nullspace-type conditions for recoveries associated\n  with general sparsity structures Abstract: We discuss a general notion of \"sparsity structure\" and associated recoveries\nof a sparse signal from its linear image of reduced dimension possibly\ncorrupted with noise. Our approach allows for unified treatment of (a) the\n\"usual sparsity\" and \"usual $\\ell_1$ recovery,\" (b) block-sparsity with\npossibly overlapping blocks and associated block-$\\ell_1$ recovery, and (c)\nlow-rank-oriented recovery by nuclear norm minimization. The proposed recovery\nroutines are natural extensions of the usual $\\ell_1$ minimization used in\nCompressed Sensing. Specifically we present nullspace-type sufficient\nconditions for the recovery to be precise on sparse signals in the noiseless\ncase. Then we derive error bounds for imperfect (nearly sparse signal, presence\nof observation noise, etc.) recovery under these conditions. In all of these\ncases, we present efficiently verifiable sufficient conditions for the validity\nof the associated nullspace properties. \n\n"}
{"id": "1207.1691", "contents": "Title: An exact duality theory for semidefinite programming based on sums of\n  squares Abstract: Farkas' lemma is a fundamental result from linear programming providing\nlinear certificates for infeasibility of systems of linear inequalities. In\nsemidefinite programming, such linear certificates only exist for strongly\ninfeasible linear matrix inequalities. We provide nonlinear algebraic\ncertificates for all infeasible linear matrix inequalities in the spirit of\nreal algebraic geometry: A linear matrix inequality is infeasible if and only\nif -1 lies in the quadratic module associated to it. We also present a new\nexact duality theory for semidefinite programming, motivated by the real\nradical and sums of squares certificates from real algebraic geometry. \n\n"}
{"id": "1207.3254", "contents": "Title: A variable smoothing algorithm for solving convex optimization problems Abstract: In this article we propose a method for solving unconstrained optimization\nproblems with convex and Lipschitz continuous objective functions. By making\nuse of the Moreau envelopes of the functions occurring in the objective, we\nsmooth the latter to a convex and differentiable function with Lipschitz\ncontinuous gradient by using both variable and constant smoothing parameters.\nThe resulting problem is solved via an accelerated first-order method and this\nallows us to recover approximately the optimal solutions to the initial\noptimization problem with a rate of convergence of order $\\O(\\tfrac{\\ln k}{k})$\nfor variable smoothing and of order $\\O(\\tfrac{1}{k})$ for constant smoothing.\nSome numerical experiments employing the variable smoothing method in image\nprocessing and in supervised learning classification are also presented. \n\n"}
{"id": "1207.4567", "contents": "Title: Efficient Core Maintenance in Large Dynamic Graphs Abstract: The $k$-core decomposition in a graph is a fundamental problem for social\nnetwork analysis. The problem of $k$-core decomposition is to calculate the\ncore number for every node in a graph. Previous studies mainly focus on\n$k$-core decomposition in a static graph. There exists a linear time algorithm\nfor $k$-core decomposition in a static graph. However, in many real-world\napplications such as online social networks and the Internet, the graph\ntypically evolves over time. Under such applications, a key issue is to\nmaintain the core number of nodes given the graph changes over time. A simple\nimplementation is to perform the linear time algorithm to recompute the core\nnumber for every node after the graph is updated. Such simple implementation is\nexpensive when the graph is very large. In this paper, we propose a new\nefficient algorithm to maintain the core number for every node in a dynamic\ngraph. Our main result is that only certain nodes need to update their core\nnumber given the graph is changed by inserting/deleting an edge. We devise an\nefficient algorithm to identify and recompute the core number of such nodes.\nThe complexity of our algorithm is independent of the graph size. In addition,\nto further accelerate the algorithm, we develop two pruning strategies by\nexploiting the lower and upper bounds of the core number. Finally, we conduct\nextensive experiments over both real-world and synthetic datasets, and the\nresults demonstrate the efficiency of the proposed algorithm. \n\n"}
{"id": "1207.5067", "contents": "Title: Simplified formulas for the mean and variance of linear stochastic\n  differential equations Abstract: Explicit formulas for the mean and variance of linear stochastic differential\nequations are derived in terms of an exponential matrix. This result improved a\nprevious one by means of which the mean and variance are expressed in terms of\na linear combination of higher dimensional exponential matrices. The important\nrole of the new formulas for the system identification as well as numerical\nalgorithms for their practical implementation are pointed out. \n\n"}
{"id": "1207.6023", "contents": "Title: Approximate linear minimum variance filters for continuous-discrete\n  state space models: convergence and practical algorithms Abstract: In this paper, approximate Linear Minimum Variance (LMV) filters for\ncontinuous-discrete state space models are introduced. The filters are obtained\nby means of a recursive approximation to the predictions for the first two\nmoments of the state equation. It is shown that the approximate filters\nconverge to the exact LMV filter when the error between the predictions and\ntheir approximations decreases. As particular instance, the order-$\\beta$ Local\nLinearization filters are presented and expounded in detail. Practical\nalgorithms are also provided and their performance in simulation is illustrated\nwith various examples. The proposed filters are intended for the recurrent\npractical situation where a nonlinear stochastic system should be identified\nfrom a reduced number of partial and noisy observations distant in time. \n\n"}
{"id": "1207.6430", "contents": "Title: Optimal Data Collection For Informative Rankings Expose Well-Connected\n  Graphs Abstract: Given a graph where vertices represent alternatives and arcs represent\npairwise comparison data, the statistical ranking problem is to find a\npotential function, defined on the vertices, such that the gradient of the\npotential function agrees with the pairwise comparisons. Our goal in this paper\nis to develop a method for collecting data for which the least squares\nestimator for the ranking problem has maximal Fisher information. Our approach,\nbased on experimental design, is to view data collection as a bi-level\noptimization problem where the inner problem is the ranking problem and the\nouter problem is to identify data which maximizes the informativeness of the\nranking. Under certain assumptions, the data collection problem decouples,\nreducing to a problem of finding multigraphs with large algebraic connectivity.\nThis reduction of the data collection problem to graph-theoretic questions is\none of the primary contributions of this work. As an application, we study the\nYahoo! Movie user rating dataset and demonstrate that the addition of a small\nnumber of well-chosen pairwise comparisons can significantly increase the\nFisher informativeness of the ranking. As another application, we study the\n2011-12 NCAA football schedule and propose schedules with the same number of\ngames which are significantly more informative. Using spectral clustering\nmethods to identify highly-connected communities within the division, we argue\nthat the NCAA could improve its notoriously poor rankings by simply scheduling\nmore out-of-conference games. \n\n"}
{"id": "1208.3291", "contents": "Title: When to look at a noisy Markov chain in sequential decision making if\n  measurements are costly? Abstract: A decision maker records measurements of a finite-state Markov chain\ncorrupted by noise. The goal is to decide when the Markov chain hits a specific\ntarget state. The decision maker can choose from a finite set of sampling\nintervals to pick the next time to look at the Markov chain. The aim is to\noptimize an objective comprising of false alarm, delay cost and cumulative\nmeasurement sampling cost. Taking more frequent measurements yields accurate\nestimates but incurs a higher measurement cost. Making an erroneous decision\ntoo soon incurs a false alarm penalty. Waiting too long to declare the target\nstate incurs a delay penalty. What is the optimal sequential strategy for the\ndecision maker? The paper shows that under reasonable conditions, the optimal\nstrategy has the following intuitive structure: when the Bayesian estimate\n(posterior distribution) of the Markov chain is away from the target state,\nlook less frequently; while if the posterior is close to the target state, look\nmore frequently. Bounds are derived for the optimal strategy. Also the\nachievable optimal cost of the sequential detector as a function of transition\ndynamics and observation distribution is analyzed. The sensitivity of the\noptimal achievable cost to parameter variations is bounded in terms of the\nKullback divergence. To prove the results in this paper, novel stochastic\ndominance results on the Bayesian filtering recursion are derived. The\nformulation in this paper generalizes quickest time change detection to\nconsider optimal sampling and also yields useful results in sensor scheduling\n(active sensing). \n\n"}
{"id": "1208.5529", "contents": "Title: Noether's Theorem in the Stochastic Calculus of Variations Abstract: We begin by presenting the classical deterministic problems of the calculus\nof variations, with emphasis on the necessary optimality conditions of\nEuler-Lagrange and the Noether theorem. As examples of application, we obtain\nthe conservation laws of momentum and energy from mechanics, valid along the\nEuler-Lagrange extremals. We then introduce the stochastic calculus of\nvariations, proving a recent stochastic Noether-type theorem obtained by\nCresson. We end by pointing out an interesting open problem. \n\n"}
{"id": "1209.0377", "contents": "Title: A Perturbation Inequality for the Schatten-$p$ Quasi-Norm and Its\n  Applications to Low-Rank Matrix Recovery Abstract: In this paper, we establish the following perturbation result concerning the\nsingular values of a matrix: Let $A,B \\in \\mathbb{R}^{m\\times n}$ be given\nmatrices, and let $f:\\mathbb{R}_+\\rightarrow\\mathbb{R}_+$ be a concave function\nsatisfying $f(0)=0$. Then, we have $$ \\sum_{i=1}^{\\min\\{m,n\\}} \\big|\nf(\\sigma_i(A)) - f(\\sigma_i(B)) \\big| \\le \\sum_{i=1}^{\\min\\{m,n\\}}\nf(\\sigma_i(A-B)), $$ where $\\sigma_i(\\cdot)$ denotes the $i$--th largest\nsingular value of a matrix. This answers an open question that is of interest\nto both the compressive sensing and linear algebra communities. In particular,\nby taking $f(\\cdot)=(\\cdot)^p$ for any $p \\in (0,1]$, we obtain a perturbation\ninequality for the so--called Schatten $p$--quasi--norm, which allows us to\nconfirm the validity of a number of previously conjectured conditions for the\nrecovery of low--rank matrices via the popular Schatten $p$--quasi--norm\nheuristic. We believe that our result will find further applications,\nespecially in the study of low--rank matrix recovery. \n\n"}
{"id": "1209.1557", "contents": "Title: Learning Model-Based Sparsity via Projected Gradient Descent Abstract: Several convex formulation methods have been proposed previously for\nstatistical estimation with structured sparsity as the prior. These methods\noften require a carefully tuned regularization parameter, often a cumbersome or\nheuristic exercise. Furthermore, the estimate that these methods produce might\nnot belong to the desired sparsity model, albeit accurately approximating the\ntrue parameter. Therefore, greedy-type algorithms could often be more desirable\nin estimating structured-sparse parameters. So far, these greedy methods have\nmostly focused on linear statistical models. In this paper we study the\nprojected gradient descent with non-convex structured-sparse parameter model as\nthe constraint set. Should the cost function have a Stable Model-Restricted\nHessian the algorithm produces an approximation for the desired minimizer. As\nan example we elaborate on application of the main results to estimation in\nGeneralized Linear Model. \n\n"}
{"id": "1209.4199", "contents": "Title: Discrete State Transition Algorithm for Unconstrained Integer\n  Optimization Problems Abstract: A recently new intelligent optimization algorithm called discrete state\ntransition algorithm is considered in this study, for solving unconstrained\ninteger optimization problems. Firstly, some key elements for discrete state\ntransition algorithm are summarized to guide its well development. Several\nintelligent operators are designed for local exploitation and global\nexploration. Then, a dynamic adjustment strategy ``risk and restoration in\nprobability\" is proposed to capture global solutions with high probability.\nFinally, numerical experiments are carried out to test the performance of the\nproposed algorithm compared with other heuristics, and they show that the\nsimilar intelligent operators can be applied to ranging from traveling salesman\nproblem, boolean integer programming, to discrete value selection problem,\nwhich indicates the adaptability and flexibility of the proposed intelligent\nelements. \n\n"}
{"id": "1209.5350", "contents": "Title: Learning Topic Models and Latent Bayesian Networks Under Expansion\n  Constraints Abstract: Unsupervised estimation of latent variable models is a fundamental problem\ncentral to numerous applications of machine learning and statistics. This work\npresents a principled approach for estimating broad classes of such models,\nincluding probabilistic topic models and latent linear Bayesian networks, using\nonly second-order observed moments. The sufficient conditions for\nidentifiability of these models are primarily based on weak expansion\nconstraints on the topic-word matrix, for topic models, and on the directed\nacyclic graph, for Bayesian networks. Because no assumptions are made on the\ndistribution among the latent variables, the approach can handle arbitrary\ncorrelations among the topics or latent factors. In addition, a tractable\nlearning method via $\\ell_1$ optimization is proposed and studied in numerical\nexperiments. \n\n"}
{"id": "1210.1594", "contents": "Title: Accuracy and Stability of The Continuous-Time 3DVAR Filter for The\n  Navier-Stokes Equation Abstract: The 3DVAR filter is prototypical of methods used to combine observed data\nwith a dynamical system, online, in order to improve estimation of the state of\nthe system. Such methods are used for high dimensional data assimilation\nproblems, such as those arising in weather forecasting. To gain understanding\nof filters in applications such as these, it is hence of interest to study\ntheir behaviour when applied to infinite dimensional dynamical systems. This\nmotivates study of the problem of accuracy and stability of 3DVAR filters for\nthe Navier-Stokes equation.\n  We work in the limit of high frequency observations and derive continuous\ntime filters. This leads to a stochastic partial differential equation (SPDE)\nfor state estimation, in the form of a damped-driven Navier-Stokes equation,\nwith mean-reversion to the signal, and spatially-correlated time-white noise.\nBoth forward and pullback accuracy and stability results are proved for this\nSPDE, showing in particular that when enough low Fourier modes are observed,\nand when the model uncertainty is larger than the data uncertainty in these\nmodes (variance inflation), then the filter can lock on to a small\nneighbourhood of the true signal, recovering from order one initial error, if\nthe error in the observations modes is small. Numerical examples are given to\nillustrate the theory. \n\n"}
{"id": "1210.2078", "contents": "Title: Path-Dependent Optimal Stochastic Control and Viscosity Solution of\n  Associated Bellman Equations Abstract: In this paper we study the optimal stochastic control problem for a\npath-dependent stochastic system under a recursive path-dependent cost\nfunctional, whose associated Bellman equation from dynamic programming\nprinciple is a path-dependent fully nonlinear partial differential equation of\nsecond order. A novel notion of viscosity solutions is introduced. Using\nDupire's functional It\\^o calculus, we characterize the value functional of the\noptimal stochastic control problem as the unique viscosity solution to the\nassociated path-dependent Bellman equation. \n\n"}
{"id": "1210.2986", "contents": "Title: A variable metric extension of the forward--backward--forward algorithm\n  for monotone operators Abstract: We propose a variable metric extension of the forward--backward-forward\nalgorithm for finding a zero of the sum of a maximally monotone operator and a\nLipschitzian monotone operator in Hilbert spaces. In turn, this framework\nprovides a variable metric splitting algorithm for solving monotone inclusions\ninvolving sums of composite operators. Several splitting algorithms recently\nproposed in the literature are recovered as special cases. \n\n"}
{"id": "1210.7221", "contents": "Title: The value of Markov Chain Games with incomplete information on both\n  sides Abstract: We consider zero-sum repeated games with incomplete information on both\nsides, where the states privately observed by each player follow independent\nMarkov chains. It generalizes the model, introduced by Aumann and Maschler in\nthe sixties and solved by Mertens and Zamir in the seventies, where the private\nstates of the players were fixed. It also includes the model introduced in\nRenault \\cite{R2006}, of Markov chain repeated games with lack of information\non one side, where only one player privately observes the sequence of states.\nWe prove here that the limit value exists, and we obtain a characterization via\nthe Mertens-Zamir system, where the \"non revealing value function\" plugged in\nthe system is now defined as the limit value of an auxiliary \"non revealing\"\ndynamic game. This non revealing game is defined by restricting the players not\nto reveal any information on the {\\it limit behavior} of their own Markov\nchain, as in Renault 2006. There are two key technical difficulties in the\nproof: 1) proving regularity, in the sense of equicontinuity, of the $T$-stage\nnon revealing value functions, and 2) constructing strategies by blocks in\norder to link the values of the non revealing games with the original values. \n\n"}
{"id": "1211.0169", "contents": "Title: Multi-Stratum Networks: toward a unified model of on-line identities Abstract: One of the reasons behind the success of Social Network Analysis is its\nsimple and general graph model made of nodes (representing individuals) and\nties. However, when we focus on our daily on-line experience we must confront a\nmore complex scenario: people inhabitate several on-line spaces interacting to\nseveral communities active on various technological infrastructures like\nTwitter, Facebook, YouTube or FourSquare and with distinct social objectives.\nThis constitutes a complex network of interconnected networks where users'\nidentities are spread and where information propagates navigating through\ndifferent communities and social platforms. In this article we introduce a\nmodel for this layered scenario that we call multi-stratum network. Through a\ntheoretical discussion and the analysis of real-world data we show how not only\nfocusing on a single network may provide a very partial understanding of the\nrole of its users, but also that considering all the networks separately may\nnot reveal the information contained in the whole multi-stratum model. \n\n"}
{"id": "1211.1740", "contents": "Title: An optimal control problem of forward-backward stochastic Volterra\n  integral equations with state constraints Abstract: This paper is devoted to the stochastic optimal control problems for systems\ngoverned by forward-backward stochastic Volterra integral equations (FBSVIEs,\nfor short) with state constraints. Using Ekeland's variational principle, we\nobtain one kind of variational inequality. Then, by dual method, we derive a\nstochastic maximum principle which gives the necessary conditions for the\noptimal controls. \n\n"}
{"id": "1211.2717", "contents": "Title: Proximal Stochastic Dual Coordinate Ascent Abstract: We introduce a proximal version of dual coordinate ascent method. We\ndemonstrate how the derived algorithmic framework can be used for numerous\nregularized loss minimization problems, including $\\ell_1$ regularization and\nstructured output SVM. The convergence rates we obtain match, and sometimes\nimprove, state-of-the-art results. \n\n"}
{"id": "1211.5484", "contents": "Title: Ranking the Importance of Nodes of Complex Networks by the Equivalence\n  Classes Approach Abstract: Identifying the importance of nodes of complex networks is of interest to the\nresearch of Social Networks, Biological Networks etc.. Current researchers have\nproposed several measures or algorithms, such as betweenness, PageRank and HITS\netc., to identify the node importance. However, these measures are based on\ndifferent aspects of properties of nodes, and often conflict with the others. A\nreasonable, fair standard is needed for evaluating and comparing these\nalgorithms. This paper develops a framework as the standard for ranking the\nimportance of nodes. Four intuitive rules are suggested to measure the node\nimportance, and the equivalence classes approach is employed to resolve the\nconflicts and aggregate the results of the rules. To quantitatively compare the\nalgorithms, the performance indicators are also proposed based on a similarity\nmeasure. Three widely used real-world networks are used as the test-beds. The\nexperimental results illustrate the feasibility of this framework and show that\nboth algorithms, PageRank and HITS, perform well with bias when dealing with\nthe tested networks. Furthermore, this paper uses the proposed approach to\nanalyze the structure of the Internet, and draws out the kernel of the Internet\nwith dense links. \n\n"}
{"id": "1211.6636", "contents": "Title: Edge Balance Ratio: Power Law from Vertices to Edges in Directed Complex\n  Network Abstract: Power law distribution is common in real-world networks including online\nsocial networks. Many studies on complex networks focus on the characteristics\nof vertices, which are always proved to follow the power law. However, few\nresearches have been done on edges in directed networks. In this paper, edge\nbalance ratio is firstly proposed to measure the balance property of edges in\ndirected networks. Based on edge balance ratio, balance profile and positivity\nare put forward to describe the balance level of the whole network. Then the\ndistribution of edge balance ratio is theoretically analyzed. In a directed\nnetwork whose vertex in-degree follows the power law with scaling exponent\n$\\gamma$, it is proved that the edge balance ratio follows a piecewise power\nlaw, with the scaling exponent of each section linearly dependents on $\\gamma$.\nThe theoretical analysis is verified by numerical simulations. Moreover, the\ntheoretical analysis is confirmed by statistics of real-world online social\nnetworks, including Twitter network with 35 million users and Sina Weibo\nnetwork with 110 million users. \n\n"}
{"id": "1211.6807", "contents": "Title: Scalable Spectral Algorithms for Community Detection in Directed\n  Networks Abstract: Community detection has been one of the central problems in network studies\nand directed network is particularly challenging due to asymmetry among its\nlinks. In this paper, we found that incorporating the direction of links\nreveals new perspectives on communities regarding to two different roles,\nsource and terminal, that a node plays in each community. Intriguingly, such\ncommunities appear to be connected with unique spectral property of the graph\nLaplacian of the adjacency matrix and we exploit this connection by using\nregularized SVD methods. We propose harvesting algorithms, coupled with\nregularized SVDs, that are linearly scalable for efficient identification of\ncommunities in huge directed networks. The proposed algorithm shows great\nperformance and scalability on benchmark networks in simulations and\nsuccessfully recovers communities in real network applications. \n\n"}
{"id": "1211.7343", "contents": "Title: Persistence and periodicity in a dynamic proximity network Abstract: The topology of social networks can be understood as being inherently\ndynamic, with edges having a distinct position in time. Most characterizations\nof dynamic networks discretize time by converting temporal information into a\nsequence of network \"snapshots\" for further analysis. Here we study a highly\nresolved data set of a dynamic proximity network of 66 individuals. We show\nthat the topology of this network evolves over a very broad distribution of\ntime scales, that its behavior is characterized by strong periodicities driven\nby external calendar cycles, and that the conversion of inherently\ncontinuous-time data into a sequence of snapshots can produce highly biased\nestimates of network structure. We suggest that dynamic social networks exhibit\na natural time scale \\Delta_{nat}, and that the best conversion of such dynamic\ndata to a discrete sequence of networks is done at this natural rate. \n\n"}
{"id": "1212.0510", "contents": "Title: Phase retrieval by power iterations Abstract: I show that the power iteration method applied to the phase retrieval problem\nconverges under special conditions. One is given the relative phases between\nsmall non-overlapping groups of pixels of a recorded intensity pattern, but no\ninformation on the phase between the groups of pixels. Numerical tests show\nthat the inverse block iteration recovers the solution in 1 iteration. \n\n"}
{"id": "1212.0873", "contents": "Title: Parallel Coordinate Descent Methods for Big Data Optimization Abstract: In this work we show that randomized (block) coordinate descent methods can\nbe accelerated by parallelization when applied to the problem of minimizing the\nsum of a partially separable smooth convex function and a simple separable\nconvex function. The theoretical speedup, as compared to the serial method, and\nreferring to the number of iterations needed to approximately solve the problem\nwith high probability, is a simple expression depending on the number of\nparallel processors and a natural and easily computable measure of separability\nof the smooth component of the objective function. In the worst case, when no\ndegree of separability is present, there may be no speedup; in the best case,\nwhen the problem is separable, the speedup is equal to the number of\nprocessors. Our analysis also works in the mode when the number of blocks being\nupdated at each iteration is random, which allows for modeling situations with\nbusy or unreliable processors. We show that our algorithm is able to solve a\nLASSO problem involving a matrix with 20 billion nonzeros in 2 hours on a large\nmemory node with 24 cores. \n\n"}
{"id": "1212.0895", "contents": "Title: The max-plus algebra approach in modelling of queueing networks Abstract: A class of queueing networks which consist of single-server fork-join nodes\nwith infinite buffers is examined to derive a representation of the network\ndynamics in terms of max-plus algebra. For the networks, we present a common\ndynamic state equation which relates the departure epochs of customers from the\nnetwork nodes in an explicit vector form determined by a state transition\nmatrix. We show how the matrix may be calculated from the service time of\ncustomers in the general case, and give examples of matrices inherent in\nparticular networks. \n\n"}
{"id": "1212.1633", "contents": "Title: Inferring Attitude in Online Social Networks Based On Quadratic\n  Correlation Abstract: The structure of an online social network in most cases cannot be described\njust by links between its members. We study online social networks, in which\nmembers may have certain attitude, positive or negative toward each other, and\nso the network consists of a mixture of both positive and negative\nrelationships. Our goal is to predict the sign of a given relationship based on\nthe evidences provided in the current snapshot of the network. More precisely,\nusing machine learning techniques we develop a model that after being trained\non a particular network predicts the sign of an unknown or hidden link. The\nmodel uses relationships and influences from peers as evidences for the guess,\nhowever, the set of peers used is not predefined but rather learned during the\ntraining process. We use quadratic correlation between peer members to train\nthe predictor. The model is tested on popular online datasets such as Epinions,\nSlashdot, and Wikipedia. In many cases it shows almost perfect prediction\naccuracy. Moreover, our model can also be efficiently updated as the\nunderlaying social network evolves. \n\n"}
{"id": "1212.1839", "contents": "Title: On Structured Realizability and Stabilizability of Linear Systems Abstract: We study the notion of structured realizability for linear systems defined\nover graphs. A stabilizable and detectable realization is structured if the\nstate-space matrices inherit the sparsity pattern of the adjacency matrix of\nthe associated graph. In this paper, we demonstrate that not every structured\ntransfer matrix has a structured realization and we reveal the practical\nmeaning of this fact. We also uncover a close connection between the structured\nrealizability of a plant and whether the plant can be stabilized by a\nstructured controller. In particular, we show that a structured stabilizing\ncontroller can only exist when the plant admits a structured realization.\nFinally, we give a parameterization of all structured stabilizing controllers\nand show that they always have structured realizations. \n\n"}
{"id": "1212.1862", "contents": "Title: On the response of quantum linear systems to single photon input fields Abstract: The purpose of this paper is to extend linear systems and signals theory to\ninclude single photon quantum signals. We provide detailed results describing\nhow quantum linear systems respond to multichannel single photon quantum\nsignals. In particular, we characterize the class of states (which we call {\\em\nphoton-Gaussian} states) that result when multichannel photons are input to a\nquantum linear system. We show that this class of quantum states is preserved\nby quantum linear systems. Multichannel photon-Gaussian states are defined via\nthe action of certain creation and annihilation operators on Gaussian states.\nOur results show how the output states are determined from the input states\nthrough a pair of transfer function relations. We also provide equations from\nwhich output signal intensities can be computed. Examples from quantum optics\nare provided to illustrate the results. \n\n"}
{"id": "1212.2170", "contents": "Title: Stochastic Perron's method for Hamilton-Jacobi-Bellman equations Abstract: We show that the value function of a stochastic control problem is the unique\nsolution of the associated Hamilton-Jacobi-Bellman (HJB) equation, completely\navoiding the proof of the so-called dynamic programming principle (DPP). Using\nStochastic Perron's method we construct a super-solution lying below the value\nfunction and a sub-solution dominating it. A comparison argument easily closes\nthe proof. The program has the precise meaning of verification for\nviscosity-solutions, obtaining the DPP as a conclusion. It also immediately\nfollows that the weak and strong formulations of the stochastic control problem\nhave the same value. Using this method we also capture the possible\nface-lifting phenomenon in a straightforward manner. \n\n"}
{"id": "1212.3349", "contents": "Title: Nonconvex notions of regularity and convergence of fundamental\n  algorithms for feasibility problems Abstract: We consider projection algorithms for solving (nonconvex) feasibility\nproblems in Euclidean spaces. Of special interest are the Method of Alternating\nProjections (MAP) and the Douglas-Rachford or Averaged Alternating Reflection\nAlgorithm (AAR). In the case of convex feasibility, firm nonexpansiveness of\nprojection mappings is a global property that yields global convergence of MAP\nand for consistent problems AAR. Based on (\\epsilon, \\delta)-regularity of sets\ndeveloped by Bauschke, Luke, Phan and Wang in 2012, a relaxed local version of\nfirm nonexpansiveness with respect to the intersection is introduced for\nconsistent feasibility problems. Together with a coercivity condition that\nrelates to the regularity of the intersection, this yields local linear\nconvergence of MAP for a wide class of nonconvex problems, \n\n"}
{"id": "1301.0790", "contents": "Title: Duality for Sudoku Abstract: We consider a mathematical model for the classical Sudoku puzzle, which we\ncall the primal problem and introduce a corresponding dual problem. Both\nproblems are constraint satisfaction models and a duality relation between them\nis proved. Based on these models, we introduce a primal and a dual optimization\nproblem and show weak and strong duality properties. \n\n"}
{"id": "1301.4506", "contents": "Title: Projection Methods: Swiss Army Knives for Solving Feasibility and Best\n  Approximation Problems with Halfspaces Abstract: We model a problem motivated by road design as a feasibility problem.\nProjections onto the constraint sets are obtained, and projection methods for\nsolving the feasibility problem are studied. We present results of numerical\nexperiments which demonstrate the efficacy of projection methods even for\nchallenging nonconvex problems. \n\n"}
{"id": "1301.4666", "contents": "Title: A Linearly Convergent Conditional Gradient Algorithm with Applications\n  to Online and Stochastic Optimization Abstract: Linear optimization is many times algorithmically simpler than non-linear\nconvex optimization. Linear optimization over matroid polytopes, matching\npolytopes and path polytopes are example of problems for which we have simple\nand efficient combinatorial algorithms, but whose non-linear convex counterpart\nis harder and admits significantly less efficient algorithms. This motivates\nthe computational model of convex optimization, including the offline, online\nand stochastic settings, using a linear optimization oracle. In this\ncomputational model we give several new results that improve over the previous\nstate-of-the-art. Our main result is a novel conditional gradient algorithm for\nsmooth and strongly convex optimization over polyhedral sets that performs only\na single linear optimization step over the domain on each iteration and enjoys\na linear convergence rate. This gives an exponential improvement in convergence\nrate over previous results.\n  Based on this new conditional gradient algorithm we give the first algorithms\nfor online convex optimization over polyhedral sets that perform only a single\nlinear optimization step over the domain while having optimal regret\nguarantees, answering an open question of Kalai and Vempala, and Hazan and\nKale. Our online algorithms also imply conditional gradient algorithms for\nnon-smooth and stochastic convex optimization with the same convergence rates\nas projected (sub)gradient methods. \n\n"}
{"id": "1302.0938", "contents": "Title: Stochastic differential games for fully coupled FBSDEs with jumps Abstract: This paper is concerned with stochastic differential games (SDGs) defined\nthrough fully coupled forward-backward stochastic differential equations\n(FBSDEs) which are governed by Brownian motion and Poisson random measure. For\nSDGs, the upper and the lower value functions are defined by the controlled\nfully coupled FBSDEs with jumps. Using a new transformation introduced in [6],\nwe prove that the upper and the lower value functions are deterministic. Then,\nafter establishing the dynamic programming principle for the upper and the\nlower value functions of this SDGs, we prove that the upper and the lower value\nfunctions are the viscosity solutions to the associated upper and the lower\nHamilton-Jacobi-Bellman-Isaacs (HJBI) equations, respectively. Furthermore, for\na special case (when $\\sigma,\\ h$ do not depend on $y,\\ z,\\ k$), under the\nIsaacs' condition, we get the existence of the value of the game. \n\n"}
{"id": "1302.1157", "contents": "Title: Excess-Risk of Distributed Stochastic Learners Abstract: This work studies the learning ability of consensus and diffusion distributed\nlearners from continuous streams of data arising from different but related\nstatistical distributions. Four distinctive features for diffusion learners are\nrevealed in relation to other decentralized schemes even under left-stochastic\ncombination policies. First, closed-form expressions for the evolution of their\nexcess-risk are derived for strongly-convex risk functions under a diminishing\nstep-size rule. Second, using these results, it is shown that the diffusion\nstrategy improves the asymptotic convergence rate of the excess-risk relative\nto non-cooperative schemes. Third, it is shown that when the in-network\ncooperation rules are designed optimally, the performance of the diffusion\nimplementation can outperform that of naive centralized processing. Finally,\nthe arguments further show that diffusion outperforms consensus strategies\nasymptotically, and that the asymptotic excess-risk expression is invariant to\nthe particular network topology. The framework adopted in this work studies\nconvergence in the stronger mean-square-error sense, rather than in\ndistribution, and develops tools that enable a close examination of the\ndifferences between distributed strategies in terms of asymptotic behavior, as\nwell as in terms of convergence rates. \n\n"}
{"id": "1302.7280", "contents": "Title: Bayesian Consensus Clustering Abstract: The task of clustering a set of objects based on multiple sources of data\narises in several modern applications. We propose an integrative statistical\nmodel that permits a separate clustering of the objects for each data source.\nThese separate clusterings adhere loosely to an overall consensus clustering,\nand hence they are not independent. We describe a computationally scalable\nBayesian framework for simultaneous estimation of both the consensus clustering\nand the source-specific clusterings. We demonstrate that this flexible approach\nis more robust than joint clustering of all data sources, and is more powerful\nthan clustering each data source separately. This work is motivated by the\nintegrated analysis of heterogeneous biomedical data, and we present an\napplication to subtype identification of breast cancer tumor samples using\npublicly available data from The Cancer Genome Atlas. Software is available at\nhttp://people.duke.edu/~el113/software.html. \n\n"}
{"id": "1303.0045", "contents": "Title: The Mesh of Civilizations and International Email Flows Abstract: In The Clash of Civilizations, Samuel Huntington argued that the primary axis\nof global conflict was no longer ideological or economic but cultural and\nreligious, and that this division would characterize the \"battle lines of the\nfuture.\" In contrast to the \"top down\" approach in previous research focused on\nthe relations among nation states, we focused on the flows of interpersonal\ncommunication as a bottom-up view of international alignments. To that end, we\nmapped the locations of the world's countries in global email networks to see\nif we could detect cultural fault lines. Using IP-geolocation on a worldwide\nanonymized dataset obtained from a large Internet company, we constructed a\nglobal email network. In computing email flows we employ a novel rescaling\nprocedure to account for differences due to uneven adoption of a particular\nInternet service across the world. Our analysis shows that email flows are\nconsistent with Huntington's thesis. In addition to location in Huntington's\n\"civilizations,\" our results also attest to the importance of both cultural and\neconomic factors in the patterning of inter-country communication ties. \n\n"}
{"id": "1303.0484", "contents": "Title: Onomastics 2.0 - The Power of Social Co-Occurrences Abstract: Onomastics is \"the science or study of the origin and forms of proper names\nof persons or places.\" [\"Onomastics\". Merriam-Webster.com, 2013.\nhttp://www.merriam-webster.com (11 February 2013)]. Especially personal names\nplay an important role in daily life, as all over the world future parents are\nfacing the task of finding a suitable given name for their child. This choice\nis influenced by different factors, such as the social context, language,\ncultural background and, in particular, personal taste.\n  With the rise of the Social Web and its applications, users more and more\ninteract digitally and participate in the creation of heterogeneous,\ndistributed, collaborative data collections. These sources of data also reflect\ncurrent and new naming trends as well as new emerging interrelations among\nnames.\n  The present work shows, how basic approaches from the field of social network\nanalysis and information retrieval can be applied for discovering relations\namong names, thus extending Onomastics by data mining techniques. The\nconsidered approach starts with building co-occurrence graphs relative to data\nfrom the Social Web, respectively for given names and city names. As a main\nresult, correlations between semantically grounded similarities among names\n(e.g., geographical distance for city names) and structural graph based\nsimilarities are observed.\n  The discovered relations among given names are the foundation of \"nameling\"\n[http://nameling.net], a search engine and academic research platform for given\nnames which attracted more than 30,000 users within four months,\nunderpinningthe relevance of the proposed methodology. \n\n"}
{"id": "1303.1598", "contents": "Title: Conditions for Existence of Dual Certificates in Rank-One Semidefinite\n  Problems Abstract: Several signal recovery tasks can be relaxed into semidefinite programs with\nrank-one minimizers. A common technique for proving these programs succeed is\nto construct a dual certificate. Unfortunately, dual certificates may not exist\nunder some formulations of semidefinite programs. In order to put problems into\na form where dual certificate arguments are possible, it is important to\ndevelop conditions under which the certificates exist. In this paper, we\nprovide an example where dual certificates do not exist. We then present a\ncompleteness condition under which they are guaranteed to exist. For programs\nthat do not satisfy the completeness condition, we present a completion process\nwhich produces an equivalent program that does satisfy the condition. The\nimportant message of this paper is that dual certificates may not exist for\nsemidefinite programs that involve orthogonal measurements with respect to\npositive-semidefinite matrices. Such measurements can interact with the\npositive-semidefinite constraint in a way that implies additional linear\nmeasurements. If these additional measurements are not included in the problem\nformulation, then dual certificates may fail to exist. As an illustration, we\npresent a semidefinite relaxation for the task of finding the sparsest element\nin a subspace. One formulation of this program does not admit dual\ncertificates. The completion process produces an equivalent formulation which\ndoes admit dual certificates. \n\n"}
{"id": "1303.2875", "contents": "Title: On the convergence rate improvement of a primal-dual splitting algorithm\n  for solving monotone inclusion problems Abstract: We present two modified versions of the primal-dual splitting algorithm\nrelying on forward-backward splitting proposed in \\cite{vu} for solving\nmonotone inclusion problems. Under strong monotonicity assumptions for some of\nthe operators involved we obtain for the sequences of iterates that approach\nthe solution orders of convergence of O(1/n) and O(\\omega^n), for $\\omega \\in\n(0,1)$, respectively. The investigated primal-dual algorithms are fully\ndecomposable, in the sense that the operators are processed individually at\neach iteration. We also discuss the modified algorithms in the context of\nconvex optimization problems and present numerical experiments in image\nprocessing and support vector machines classification. \n\n"}
{"id": "1303.3547", "contents": "Title: Spatial-Spectral Sensing using the Shrink & Match Algorithm in\n  Asynchronous MIMO OFDM Signals Abstract: Spectrum sensing (SS) in cognitive radio (CR) systems is of paramount\nimportance to approach the capacity limits for the Secondary Users (SU), while\nensuring the undisturbed transmission of Primary Users (PU). In this paper, we\nformulate a cognitive radio (CR)systems spectrum sensing (SS) problem in which\nSecondary Users (SU), with multiple receive antennae, sense a channel shared\namong multiple asynchronous Primary Users (PU) transmitting Multiple Input\nMultiple Output (MIMO) Orthogonal Frequency Division Multiplexing (OFDM)\nsignals. The method we propose to estimate the opportunities available to the\nSUs combines advances in array processing and compressed channel sensing, and\nleverages on both the so called \"shrinkage method\" as well as on an\nover-complete basis expansion of the PUs interference covariance matrix to\ndetect the occupied and idle angles of arrivals and subcarriers. The covariance\n\"shrinkage\" step and the sparse modeling step that follows, allow to resolve\nambiguities that arise when the observations are scarce, reducing the sensing\ncost for the SU, thereby increasing its spectrum exploitation capabilities\ncompared to competing sensing methods. Simulations corroborate that exploiting\nthe sparse representation of the covariance matrix in CR sensing resolves the\nspatial and frequency spectrum of the sources. \n\n"}
{"id": "1303.4476", "contents": "Title: A distributed adaptive steplength stochastic approximation method for\n  monotone stochastic Nash Games Abstract: We consider a distributed stochastic approximation (SA) scheme for computing\nan equilibrium of a stochastic Nash game. Standard SA schemes employ\ndiminishing steplength sequences that are square summable but not summable.\nSuch requirements provide a little or no guidance for how to leverage\nLipschitzian and monotonicity properties of the problem and naive choices\ngenerally do not preform uniformly well on a breadth of problems. While a\ncentralized adaptive stepsize SA scheme is proposed in [1] for the optimization\nframework, such a scheme provides no freedom for the agents in choosing their\nown stepsizes. Thus, a direct application of centralized stepsize schemes is\nimpractical in solving Nash games. Furthermore, extensions to game-theoretic\nregimes where players may independently choose steplength sequences are limited\nto recent work by Koshal et al. [2]. Motivated by these shortcomings, we\npresent a distributed algorithm in which each player updates his steplength\nbased on the previous steplength and some problem parameters. The steplength\nrules are derived from minimizing an upper bound of the errors associated with\nplayers' decisions. It is shown that these rules generate sequences that\nconverge almost surely to an equilibrium of the stochastic Nash game.\nImportantly, variants of this rule are suggested where players independently\nselect steplength sequences while abiding by an overall coordination\nrequirement. Preliminary numerical results are seen to be promising. \n\n"}
{"id": "1303.4645", "contents": "Title: Gradient methods for convex minimization: better rates under weaker\n  conditions Abstract: The convergence behavior of gradient methods for minimizing convex\ndifferentiable functions is one of the core questions in convex optimization.\nThis paper shows that their well-known complexities can be achieved under\nconditions weaker than the commonly accepted ones. We relax the common gradient\nLipschitz-continuity condition and strong convexity condition to ones that hold\nonly over certain line segments. Specifically, we establish complexities\n$O(\\frac{R}{\\epsilon})$ and $O(\\sqrt{\\frac{R}{\\epsilon}})$ for the ordinary and\naccelerate gradient methods, respectively, assuming that $\\nabla f$ is\nLipschitz continuous with constant $R$ over the line segment joining $x$ and\n$x-\\frac{1}{R}\\nabla f$ for each $x\\in\\dom f$. Then we improve them to\n$O(\\frac{R}{\\nu}\\log(\\frac{1}{\\epsilon}))$ and\n$O(\\sqrt{\\frac{R}{\\nu}}\\log(\\frac{1}{\\epsilon}))$ for function $f$ that also\nsatisfies the secant inequality $\\ < \\nabla f(x), x- x^*\\ > \\ge \\nu\\|x-x^*\\|^2$\nfor each $x\\in \\dom f$ and its projection $x^*$ to the minimizer set of $f$.\nThe secant condition is also shown to be necessary for the geometric decay of\nsolution error. Not only are the relaxed conditions met by more functions, the\nrestrictions give smaller $R$ and larger $\\nu$ than they are without the\nrestrictions and thus lead to better complexity bounds. We apply these results\nto sparse optimization and demonstrate a faster algorithm. \n\n"}
{"id": "1303.5588", "contents": "Title: Robust and Trend Following Student's t Kalman Smoothers Abstract: We present a Kalman smoothing framework based on modeling errors using the\nheavy tailed Student's t distribution, along with algorithms, convergence\ntheory, open-source general implementation, and several important applications.\nThe computational effort per iteration grows linearly with the length of the\ntime series, and all smoothers allow nonlinear process and measurement models.\n  Robust smoothers form an important subclass of smoothers within this\nframework. These smoothers work in situations where measurements are highly\ncontaminated by noise or include data unexplained by the forward model. Highly\nrobust smoothers are developed by modeling measurement errors using the\nStudent's t distribution, and outperform the recently proposed L1-Laplace\nsmoother in extreme situations with data containing 20% or more outliers.\n  A second special application we consider in detail allows tracking sudden\nchanges in the state. It is developed by modeling process noise using the\nStudent's t distribution, and the resulting smoother can track sudden changes\nin the state.\n  These features can be used separately or in tandem, and we present a general\nsmoother algorithm and open source implementation, together with convergence\nanalysis that covers a wide range of smoothers. A key ingredient of our\napproach is a technique to deal with the non-convexity of the Student's t loss\nfunction. Numerical results for linear and nonlinear models illustrate the\nperformance of the new smoothers for robust and tracking applications, as well\nas for mixed problems that have both types of features. \n\n"}
{"id": "1303.7160", "contents": "Title: Stochastic control with rough paths Abstract: We study a class of controlled rough differential equations. It is shown that\nthe value function satisfies a HJB type equation; we also establish a form of\nthe Pontryagin maximum principle. Deterministic problems of this type arise in\nthe duality theory for controlled diffusion processes and typically involve\nanticipating stochastic analysis. We propose a formulation based on rough paths\nand then obtain a generalization of Roger's duality formula [L. C. G. Rogers,\n2007] from discrete to continuous time. We also make the link to old work of\n[Davis--Burstein, 1987]. \n\n"}
{"id": "1304.1408", "contents": "Title: Restoration of Images Corrupted by Impulse Noise and Mixed Gaussian\n  Impulse Noise using Blind Inpainting Abstract: This article studies the problem of image restoration of observed images\ncorrupted by impulse noise and mixed Gaussian impulse noise. Since the pixels\ndamaged by impulse noise contain no information about the true image, how to\nfind this set correctly is a very important problem. We propose two methods\nbased on blind inpainting and $\\ell_0$ minimization that can simultaneously\nfind the damaged pixels and restore the image. By iteratively restoring the\nimage and updating the set of damaged pixels, these methods have better\nperformance than other methods, as shown in the experiments. In addition, we\nprovide convergence analysis for these methods, these algorithms will converge\nto coordinatewise minimum points. In addition, they will converge to local\nminimum points (or with probability one) with some modifications in the\nalgorithms. \n\n"}
{"id": "1304.3946", "contents": "Title: Strategyproof and Consistent Rules for Bipartite Flow Problems Abstract: We continue the study of Bochet et al. and Moulin and Sethuraman on fair\nallocation in bipartite networks. In these models, there is a moneyless market,\nin which a non-storable, homogeneous commodity is reallocated between agents\nwith single-peaked preferences. Agents are either suppliers or demanders. While\nthe egalitarian rule of Bochet et al. satisfies pareto optimality, no envy and\nstrategyproof, it is not consistent. On the other hand, the work of Moulin and\nSethuraman is related to consistent allocations and rules that are extensions\nof the uniform rule. We bridge the two streams of work by introducing the edge\nfair mechanism which is both consistent and groupstrategyproof. On the way, we\nexplore the \"price of consistency\" i.e. how the notion of consistency is\nfundamentally incompatible with certain notions of fairness like Lorenz\nDominance and No-Envy. The current work also introduces the idea of strong\ninvariance as desideratum for groupstrategyproofness and generalizes the proof\nof Chandramouli and Sethuraman to a more broader class of mechanisms. Finally,\nwe conclude with the study of the edge fair mechanism in a transshipment model\nwhere the strategic agents are on the links connecting different supply/demand\nlocations. \n\n"}
{"id": "1304.4731", "contents": "Title: On Synchronization of Interdependent Networks Abstract: It is well-known that the synchronization of diffusively-coupled systems on\nnetworks strongly depends on the network topology. In particular, the so-called\nalgebraic connectivity $\\mu_{N-1}$, or the smallest non-zero eigenvalue of the\ndiscrete Laplacian operator plays a crucial role on synchronization, graph\npartitioning, and network robustness. In our study, synchronization is placed\nin the general context of networks-of-networks, where single network models are\nreplaced by a more realistic hierarchy of interdependent networks. The present\nwork shows, analytically and numerically, how the algebraic connectivity\nexperiences sharp transitions after the addition of sufficient links among\ninterdependent networks. \n\n"}
{"id": "1304.5038", "contents": "Title: One condition for solution uniqueness and robustness of both\n  l1-synthesis and l1-analysis minimizations Abstract: The $\\ell_1$-synthesis model and the $\\ell_1$-analysis model recover\nstructured signals from their undersampled measurements. The solution of former\nis a sparse sum of dictionary atoms, and that of the latter makes sparse\ncorrelations with dictionary atoms. This paper addresses the question: when can\nwe trust these models to recover specific signals? We answer the question with\na condition that is both necessary and sufficient to guarantee the recovery to\nbe unique and exact and, in presence of measurement noise, to be robust. The\ncondition is one--for--all in the sense that it applies to both of the\n$\\ell_1$-synthesis and $\\ell_1$-analysis models, to both of their constrained\nand unconstrained formulations, and to both the exact recovery and robust\nrecovery cases. Furthermore, a convex infinity--norm program is introduced for\nnumerically verifying the condition. A comprehensive comparison with related\nexisting conditions are included. \n\n"}
{"id": "1304.5530", "contents": "Title: Inexact Coordinate Descent: Complexity and Preconditioning Abstract: In this paper we consider the problem of minimizing a convex function using a\nrandomized block coordinate descent method. One of the key steps at each\niteration of the algorithm is determining the update to a block of variables.\nExisting algorithms assume that in order to compute the update, a particular\nsubproblem is solved exactly. In his work we relax this requirement, and allow\nfor the subproblem to be solved inexactly, leading to an inexact block\ncoordinate descent method. Our approach incorporates the best known results for\nexact updates as a special case. Moreover, these theoretical guarantees are\ncomplemented by practical considerations: the use of iterative techniques to\ndetermine the update as well as the use of preconditioning for further\nacceleration. \n\n"}
{"id": "1304.7694", "contents": "Title: Convex risk minimization via proximal splitting methods Abstract: In this paper we investigate the applicability of a recently introduced\nprimal-dual splitting method in the context of solving portfolio optimization\nproblems which assume the minimization of risk measures associated to different\nconvex utility functions. We show that, due to the splitting characteristic of\nthe used primal-dual method, the main effort in implementing it constitutes in\nthe calculation of the proximal points of the utility functions, which assume\nexplicit expressions in a number of cases. When quantifying risk via the\nmeanwhile classical Conditional Value-at-Risk, an alternative approach relying\non the use of its dual representation is presented as well. The theoretical\nresults are finally illustrated via some numerical experiments on real and\nsynthetic data sets. \n\n"}
{"id": "1304.8083", "contents": "Title: Adaptive Video Streaming for Wireless Networks with Multiple Users and\n  Helpers Abstract: We consider the optimal design of a scheduling policy for adaptive video\nstreaming in a wireless network formed by several users and helpers. A feature\nof such networks is that any user is typically in the range of multiple\nhelpers. Hence, in order to cope with user-helper association, load balancing\nand inter-cell interference, an efficient streaming policy should allow the\nusers to dynamically select the helper node to download from, and determine\nadaptively the video quality level of the download. In order to obtain a\ntractable formulation, we follow a \"divide and conquer\" approach: i) Assuming\nthat each video packet (chunk) is delivered within its playback delay (\"smooth\nstreaming regime\"), the problem is formulated as a network utility maximization\n(NUM), subject to queue stability, where the network utility function is a\nconcave and componentwise non-decreasing function of the users' video quality\nmeasure. ii) We solve the NUM problem by using a Lyapunov Drift Plus Penalty\napproach, obtaining a scheme that naturally decomposes into two sub-policies\nreferred to as \"congestion control\" (adaptive video quality and helper station\nselection) and \"transmission scheduling\" (dynamic allocation of the helper-user\nphysical layer transmission rates).Our solution is provably optimal with\nrespect to the proposed NUM problem, in a strong per-sample path sense. iii)\nFinally, we propose a method to adaptively estimate the maximum queuing delays,\nsuch that each user can calculate its pre-buffering and re-buffering time in\norder to cope with the fluctuations of the queuing delays. Through simulations,\nwe evaluate the performance of the proposed algorithm under realistic\nassumptions of a network with densely deployed helper nodes, and demonstrate\nthe per-sample path optimality of the proposed solution by considering a\nnon-stationary non-ergodic scenario with user mobility, VBR video coding. \n\n"}
{"id": "1305.1454", "contents": "Title: A constrained tropical optimization problem: complete solution and\n  application example Abstract: The paper focuses on a multidimensional optimization problem, which is\nformulated in terms of tropical mathematics and consists in minimizing a\nnonlinear objective function subject to linear inequality constraints. To solve\nthe problem, we follow an approach based on the introduction of an additional\nunknown variable to reduce the problem to solving linear inequalities, where\nthe variable plays the role of a parameter. A necessary and sufficient\ncondition for the inequalities to hold is used to evaluate the parameter,\nwhereas the general solution of the inequalities is taken as a solution of the\noriginal problem. Under fairly general assumptions, a complete direct solution\nto the problem is obtained in a compact vector form. The result is applied to\nsolve a problem in project scheduling when an optimal schedule is given by\nminimizing the flow time of activities in a project under various activity\nprecedence constraints. As an illustration, a numerical example of optimal\nscheduling is also presented. \n\n"}
{"id": "1305.1885", "contents": "Title: Distributed Optimization With Local Domains: Applications in MPC and\n  Network Flows Abstract: In this paper we consider a network with $P$ nodes, where each node has\nexclusive access to a local cost function. Our contribution is a\ncommunication-efficient distributed algorithm that finds a vector $x^\\star$\nminimizing the sum of all the functions. We make the additional assumption that\nthe functions have intersecting local domains, i.e., each function depends only\non some components of the variable. Consequently, each node is interested in\nknowing only some components of $x^\\star$, not the entire vector. This allows\nfor improvement in communication-efficiency. We apply our algorithm to model\npredictive control (MPC) and to network flow problems and show, through\nexperiments on large networks, that our proposed algorithm requires less\ncommunications to converge than prior algorithms. \n\n"}
{"id": "1305.4372", "contents": "Title: Risk Limiting Dispatch with Ramping Constraints Abstract: Reliable operation in power systems is becoming more difficult as the\npenetration of random renewable resources increases. In particular, operators\nface the risk of not scheduling enough traditional generators in the times when\nrenewable energies becomes lower than expected. In this paper we study the\noptimal trade-off between system and risk, and the cost of scheduling reserve\ngenerators. We explicitly model the ramping constraints on the generators. We\nmodel the problem as a multi-period stochastic control problem, and we show the\nstructure of the optimal dispatch. We then show how to efficiently compute the\ndispatch using two methods: i) solving a surrogate chance constrained program,\nii) a MPC-type look ahead controller. Using real world data, we show the chance\nconstrained dispatch outperforms the MPC controller and is also robust to\nchanges in the probability distribution of the renewables. \n\n"}
{"id": "1305.6008", "contents": "Title: Arbitrage and duality in nondominated discrete-time models Abstract: We consider a nondominated model of a discrete-time financial market where\nstocks are traded dynamically, and options are available for static hedging. In\na general measure-theoretic setting, we show that absence of arbitrage in a\nquasi-sure sense is equivalent to the existence of a suitable family of\nmartingale measures. In the arbitrage-free case, we show that optimal\nsuperhedging strategies exist for general contingent claims, and that the\nminimal superhedging price is given by the supremum over the martingale\nmeasures. Moreover, we obtain a nondominated version of the Optional\nDecomposition Theorem. \n\n"}
{"id": "1306.1327", "contents": "Title: Symmetric Quantum Calculus Abstract: We generalize the Hahn variational calculus by studying problems of the\ncalculus of variations with higher-order derivatives. The symmetric quantum\ncalculus is studied, namely the $\\alpha,\\beta$-symmetric, the $q$-symmetric,\nand the Hahn symmetric quantum calculus. We introduce the symmetric quantum\nvariational calculus and an Euler-Lagrange type equation for the $q$-symmetric\nand Hahn's symmetric quantum calculus is proved. We define a symmetric\nderivative on time scales and derive some of its properties. Finally, we\nintroduce and study the diamond integral, which is a refined version of the\ndiamond-$\\alpha$ integral on time scales. \n\n"}
{"id": "1306.3764", "contents": "Title: Bounding ground state energy of Hopfield models Abstract: In this paper we look at a class of random optimization problems that arise\nin the forms typically known as Hopfield models. We view two scenarios which we\nterm as the positive Hopfield form and the negative Hopfield form. For both of\nthese scenarios we define the binary optimization problems that essentially\nemulate what would typically be known as the ground state energy of these\nmodels. We then present a simple mechanism that can be used to create a set of\ntheoretical rigorous bounds for these energies. In addition to purely\ntheoretical bounds, we also present a couple of fast optimization algorithms\nthat can also be used to provide solid (albeit a bit weaker) algorithmic bounds\nfor the ground state energies. \n\n"}
{"id": "1306.3779", "contents": "Title: Bounds on restricted isometry constants of random matrices Abstract: In this paper we look at isometry properties of random matrices. During the\nlast decade these properties gained a lot attention in a field called\ncompressed sensing in first place due to their initial use in \\cite{CRT,CT}.\nNamely, in \\cite{CRT,CT} these quantities were used as a critical tool in\nproviding a rigorous analysis of $\\ell_1$ optimization's ability to solve an\nunder-determined system of linear equations with sparse solutions. In such a\nframework a particular type of isometry, called restricted isometry, plays a\nkey role. One then typically introduces a couple of quantities, called upper\nand lower restricted isometry constants to characterize the isometry properties\nof random matrices. Those constants are then usually viewed as mathematical\nobjects of interest and their a precise characterization is desirable. The\nfirst estimates of these quantities within compressed sensing were given in\n\\cite{CRT,CT}. As the need for precisely estimating them grew further a finer\nimprovements of these initial estimates were obtained in e.g.\n\\cite{BCTsharp09,BT10}. These are typically obtained through a combination of\nunion-bounding strategy and powerful tail estimates of extreme eigenvalues of\nWishart (Gaussian) matrices (see, e.g. \\cite{Edelman88}). In this paper we\nattempt to circumvent such an approach and provide an alternative way to obtain\nsimilar estimates. \n\n"}
{"id": "1306.4391", "contents": "Title: On the Fundamental Limits of Recovering Tree Sparse Vectors from Noisy\n  Linear Measurements Abstract: Recent breakthrough results in compressive sensing (CS) have established that\nmany high dimensional signals can be accurately recovered from a relatively\nsmall number of non-adaptive linear observations, provided that the signals\npossess a sparse representation in some basis. Subsequent efforts have shown\nthat the performance of CS can be improved by exploiting additional structure\nin the locations of the nonzero signal coefficients during inference, or by\nutilizing some form of data-dependent adaptive measurement focusing during the\nsensing process. To our knowledge, our own previous work was the first to\nestablish the potential benefits that can be achieved when fusing the notions\nof adaptive sensing and structured sparsity -- that work examined the task of\nsupport recovery from noisy linear measurements, and established that an\nadaptive sensing strategy specifically tailored to signals that are tree-sparse\ncan significantly outperform adaptive and non-adaptive sensing strategies that\nare agnostic to the underlying structure. In this work we establish fundamental\nperformance limits for the task of support recovery of tree-sparse signals from\nnoisy measurements, in settings where measurements may be obtained either\nnon-adaptively (using a randomized Gaussian measurement strategy motivated by\ninitial CS investigations) or by any adaptive sensing strategy. Our main\nresults here imply that the adaptive tree sensing procedure analyzed in our\nprevious work is nearly optimal, in the sense that no other sensing and\nestimation strategy can perform fundamentally better for identifying the\nsupport of tree-sparse signals. \n\n"}
{"id": "1306.6812", "contents": "Title: Epidemics in Multipartite Networks: Emergent Dynamics Abstract: Single virus epidemics over complete networks are widely explored in the\nliterature as the fraction of infected nodes is, under appropriate microscopic\nmodeling of the virus infection, a Markov process. With non-complete networks,\nthis macroscopic variable is no longer Markov. In this paper, we study virus\ndiffusion, in particular, multi-virus epidemics, over non-complete stochastic\nnetworks. We focus on multipartite networks. In companying work\nhttp://arxiv.org/abs/1306.6198, we show that the peer-to-peer local random\nrules of virus infection lead, in the limit of large multipartite networks, to\nthe emergence of structured dynamics at the macroscale. The exact fluid limit\nevolution of the fraction of nodes infected by each virus strain across islands\nobeys a set of nonlinear coupled differential equations, see\nhttp://arxiv.org/abs/1306.6198. In this paper, we develop methods to analyze\nthe qualitative behavior of these limiting dynamics, establishing conditions on\nthe virus micro characteristics and network structure under which a virus\npersists or a natural selection phenomenon is observed. \n\n"}
{"id": "1306.6909", "contents": "Title: Exact Support Recovery for Sparse Spikes Deconvolution Abstract: This paper studies sparse spikes deconvolution over the space of measures. We\nfocus our attention to the recovery properties of the support of the measure,\ni.e. the location of the Dirac masses. For non-degenerate sums of Diracs, we\nshow that, when the signal-to-noise ratio is large enough, total variation\nregularization (which is the natural extension of the L1 norm of vectors to the\nsetting of measures) recovers the exact same number of Diracs. We also show\nthat both the locations and the heights of these Diracs converge toward those\nof the input measure when the noise drops to zero. The exact speed of\nconvergence is governed by a specific dual certificate, which can be computed\nby solving a linear system. We draw connections between the support of the\nrecovered measure on a continuous domain and on a discretized grid. We show\nthat when the signal-to-noise level is large enough, the solution of the\ndiscretized problem is supported on pairs of Diracs which are neighbors of the\nDiracs of the input measure. This gives a precise description of the\nconvergence of the solution of the discretized problem toward the solution of\nthe continuous grid-free problem, as the grid size tends to zero. \n\n"}
{"id": "1307.0141", "contents": "Title: Convergence of the shooting algorithm for singular optimal control\n  problems Abstract: In this article we propose a shooting algorithm for optimal control problems\ngoverned by systems that are affine in one part of the control variable.\nFinitely many equality constraints on the initial and final state are\nconsidered. We recall a second order sufficient condition for weak optimality,\nand show that it guarantees the local quadratic convergence of the algorithm.\nWe show an example and solve it numerically. \n\n"}
{"id": "1307.2009", "contents": "Title: Alternating Projections and Douglas-Rachford for Sparse Affine\n  Feasibility Abstract: The problem of finding a vector with the fewest nonzero elements that\nsatisfies an underdetermined system of linear equations is an NP-complete\nproblem that is typically solved numerically via convex heuristics or\nnicely-behaved nonconvex relaxations. In this work we consider elementary\nmethods based on projections for solving a sparse feasibility problem without\nemploying convex heuristics. In a recent paper Bauschke, Luke, Phan and Wang\n(2014) showed that, locally, the fundamental method of alternating projections\nmust converge linearly to a solution to the sparse feasibility problem with an\naffine constraint. In this paper we apply different analytical tools that allow\nus to show global linear convergence of alternating projections under familiar\nconstraint qualifications. These analytical tools can also be applied to other\nalgorithms. This is demonstrated with the prominent Douglas-Rachford algorithm\nwhere we establish local linear convergence of this method applied to the\nsparse affine feasibility problem. \n\n"}
{"id": "1307.2302", "contents": "Title: The blessing of transitivity in sparse and stochastic networks Abstract: The interaction between transitivity and sparsity, two common features in\nempirical networks, implies that there are local regions of large sparse\nnetworks that are dense. We call this the blessing of transitivity and it has\nconsequences for both modeling and inference. Extant research suggests that\nstatistical inference for the Stochastic Blockmodel is more difficult when the\nedges are sparse. However, this conclusion is confounded by the fact that the\nasymptotic limit in all of the previous studies is not merely sparse, but also\nnon-transitive. To retain transitivity, the blocks cannot grow faster than the\nexpected degree. Thus, in sparse models, the blocks must remain asymptotically\nsmall. \\n Previous algorithmic research demonstrates that small \"local\"\nclusters are more amenable to computation, visualization, and interpretation\nwhen compared to \"global\" graph partitions. This paper provides the first\nstatistical results that demonstrate how these small transitive clusters are\nalso more amenable to statistical estimation. Theorem 2 shows that a \"local\"\nclustering algorithm can, with high probability, detect a transitive stochastic\nblock of a fixed size (e.g. 30 nodes) embedded in a large graph. The only\nconstraint on the ambient graph is that it is large and sparse--it could be\ngenerated at random or by an adversary--suggesting a theoretical explanation\nfor the robust empirical performance of local clustering algorithms. \n\n"}
{"id": "1307.2342", "contents": "Title: Model Selection with Low Complexity Priors Abstract: Regularization plays a pivotal role when facing the challenge of solving\nill-posed inverse problems, where the number of observations is smaller than\nthe ambient dimension of the object to be estimated. A line of recent work has\nstudied regularization models with various types of low-dimensional structures.\nIn such settings, the general approach is to solve a regularized optimization\nproblem, which combines a data fidelity term and some regularization penalty\nthat promotes the assumed low-dimensional/simple structure. This paper provides\na general framework to capture this low-dimensional structure through what we\ncoin partly smooth functions relative to a linear manifold. These are convex,\nnon-negative, closed and finite-valued functions that will promote objects\nliving on low-dimensional subspaces. This class of regularizers encompasses\nmany popular examples such as the L1 norm, L1-L2 norm (group sparsity), as well\nas several others including the Linfty norm. We also show that the set of\npartly smooth functions relative to a linear manifold is closed under addition\nand pre-composition by a linear operator, which allows to cover mixed\nregularization, and the so-called analysis-type priors (e.g. total variation,\nfused Lasso, finite-valued polyhedral gauges). Our main result presents a\nunified sharp analysis of exact and robust recovery of the low-dimensional\nsubspace model associated to the object to recover from partial measurements.\nThis analysis is illustrated on a number of special and previously studied\ncases, and on an analysis of the performance of Linfty regularization in a\ncompressed sensing scenario. \n\n"}
{"id": "1307.7172", "contents": "Title: Structure and Dynamics of Coauthorship, Citation, and Impact within CSCW Abstract: CSCW has stabilized as an interdisciplinary venue for computer, information,\ncognitive, and social scientists but has also undergone significant changes in\nits format in recent years. This paper uses methods from social network\nanalysis and bibliometrics to re-examine the structures of CSCW a decade after\nits last systematic analysis. Using data from the ACM Digital Library, we\nanalyze changes in structures of coauthorship and citation between 1986 and\n2013. Statistical models reveal significant but distinct patterns between\npapers and authors in how brokerage and closure in these networks affects\nimpact as measured by citations and downloads. Specifically, impact is unduly\ninfluenced by structural position, such that ideas introduced by those in the\ncore of the CSCW community (e.g., elite researchers) are advantaged over those\nintroduced by peripheral participants (e.g., newcomers). This finding is\nexamined in the context of recent changes to the CSCW conference that may have\nthe effect of upsetting the preference for contributions from the core. \n\n"}
{"id": "1308.5000", "contents": "Title: Smoothing and Decomposition for Analysis Sparse Recovery Abstract: We consider algorithms and recovery guarantees for the analysis sparse model\nin which the signal is sparse with respect to a highly coherent frame. We\nconsider the use of a monotone version of the fast iterative shrinkage-\nthresholding algorithm (MFISTA) to solve the analysis sparse recovery problem.\nSince the proximal operator in MFISTA does not have a closed-form solution for\nthe analysis model, it cannot be applied directly. Instead, we examine two\nalternatives based on smoothing and decomposition transformations that relax\nthe original sparse recovery problem, and then implement MFISTA on the relaxed\nformulation. We refer to these two methods as smoothing-based and\ndecomposition-based MFISTA. We analyze the convergence of both algorithms, and\nestablish that smoothing- based MFISTA converges more rapidly when applied to\ngeneral nonsmooth optimization problems. We then derive a performance bound on\nthe reconstruction error using these techniques. The bound proves that our\nmethods can recover a signal sparse in a redundant tight frame when the\nmeasurement matrix satisfies a properly adapted restricted isometry property.\nNumerical examples demonstrate the performance of our methods and show that\nsmoothing-based MFISTA converges faster than the decomposition-based\nalternative in real applications, such as MRI image reconstruction. \n\n"}
{"id": "1308.5045", "contents": "Title: Network Coding meets Decentralized Control: Network Linearization and\n  Capacity-Stabilizablilty Equivalence Abstract: We take a unified view of network coding and decentralized control. Precisely\nspeaking, we consider both as linear time-invariant systems by appropriately\nrestricting channels and coding schemes of network coding to be linear\ntime-invariant, and the plant and controllers of decentralized control to be\nlinear time-invariant as well. First, we apply linear system theory to network\ncoding. This gives a novel way of converting an arbitrary relay network to an\nequivalent acyclic single-hop relay network, which we call Network\nLinearization. Based on network linearization, we prove that the fundamental\ndesign limit, mincut, is achievable by a linear time-invariant network-coding\nscheme regardless of the network topology.\n  Then, we use the network-coding to view decentralized linear systems. We\nargue that linear time-invariant controllers in a decentralized linear system\n\"communicate\" via linear network coding to stabilize the plant. To justify this\nargument, we give an algorithm to \"externalize\" the implicit communication\nbetween the controllers that we believe must be occurring to stabilize the\nplant. Based on this, we show that the stabilizability condition for\ndecentralized linear systems comes from an underlying communication limit,\nwhich can be described by the algebraic mincut-maxflow theorem. With this\nre-interpretation in hand, we also consider stabilizability over LTI networks\nto emphasize the connection with network coding. In particular, in broadcast\nand unicast problems, unintended messages at the receivers will be modeled as\nsecrecy constraints. \n\n"}
{"id": "1308.6337", "contents": "Title: A dual algorithm for a class of augmented convex models Abstract: Convex optimization models find interesting applications, especially in\nsignal/image processing and compressive sensing. We study some augmented convex\nmodels, which are perturbed by strongly convex functions, and propose a dual\ngradient algorithm. The proposed algorithm includes the linearized Bregman\nalgorithm and the singular value thresholding algorithm as special cases. Based\non fundamental properties of proximal operators, we present a concise approach\nto establish the convergence of both primal and dual sequences, improving the\nresults in the existing literature. \n\n"}
{"id": "1308.6718", "contents": "Title: Reduced-Complexity Semidefinite Relaxations of Optimal Power Flow\n  Problems Abstract: We propose a new method for generating semidefinite relaxations of optimal\npower flow problems. The method is based on chordal conversion techniques: by\ndropping some equality constraints in the conversion, we obtain semidefinite\nrelaxations that are computationally cheaper, but potentially weaker, than the\nstandard semidefinite relaxation. Our numerical results show that the new\nrelaxations often produce the same results as the standard semidefinite\nrelaxation, but at a lower computational cost. \n\n"}
{"id": "1309.0209", "contents": "Title: Optimal stochastic control and optimal consumption and portfolio with\n  G-Brownian motion Abstract: By the calculus of Peng's G-sublinear expectation and G-Brownian motion on a\nsublinear expectation space $(\\Omega, {\\cal H}, \\hat{\\mathbb{E}})$, we first\nset up an optimality principle of stochastic control problem. Then we\ninvestigate an optimal consumption and portfolio decision with a volatility\nambiguity by the derived verification theorem. Next the two-fund separation\ntheorem is explicitly obtained. And an illustrative example is provided. \n\n"}
{"id": "1309.1392", "contents": "Title: Bayesian Structural Inference for Hidden Processes Abstract: We introduce a Bayesian approach to discovering patterns in structurally\ncomplex processes. The proposed method of Bayesian Structural Inference (BSI)\nrelies on a set of candidate unifilar HMM (uHMM) topologies for inference of\nprocess structure from a data series. We employ a recently developed exact\nenumeration of topological epsilon-machines. (A sequel then removes the\ntopological restriction.) This subset of the uHMM topologies has the added\nbenefit that inferred models are guaranteed to be epsilon-machines,\nirrespective of estimated transition probabilities. Properties of\nepsilon-machines and uHMMs allow for the derivation of analytic expressions for\nestimating transition probabilities, inferring start states, and comparing the\nposterior probability of candidate model topologies, despite process internal\nstructure being only indirectly present in data. We demonstrate BSI's\neffectiveness in estimating a process's randomness, as reflected by the Shannon\nentropy rate, and its structure, as quantified by the statistical complexity.\nWe also compare using the posterior distribution over candidate models and the\nsingle, maximum a posteriori model for point estimation and show that the\nformer more accurately reflects uncertainty in estimated values. We apply BSI\nto in-class examples of finite- and infinite-order Markov processes, as well to\nan out-of-class, infinite-state hidden process. \n\n"}
{"id": "1309.2074", "contents": "Title: Learning Transformations for Clustering and Classification Abstract: A low-rank transformation learning framework for subspace clustering and\nclassification is here proposed. Many high-dimensional data, such as face\nimages and motion sequences, approximately lie in a union of low-dimensional\nsubspaces. The corresponding subspace clustering problem has been extensively\nstudied in the literature to partition such high-dimensional data into clusters\ncorresponding to their underlying low-dimensional subspaces. However,\nlow-dimensional intrinsic structures are often violated for real-world\nobservations, as they can be corrupted by errors or deviate from ideal models.\nWe propose to address this by learning a linear transformation on subspaces\nusing matrix rank, via its convex surrogate nuclear norm, as the optimization\ncriteria. The learned linear transformation restores a low-rank structure for\ndata from the same subspace, and, at the same time, forces a a maximally\nseparated structure for data from different subspaces. In this way, we reduce\nvariations within subspaces, and increase separation between subspaces for a\nmore robust subspace clustering. This proposed learned robust subspace\nclustering framework significantly enhances the performance of existing\nsubspace clustering methods. Basic theoretical results here presented help to\nfurther support the underlying framework. To exploit the low-rank structures of\nthe transformed subspaces, we further introduce a fast subspace clustering\ntechnique, which efficiently combines robust PCA with sparse modeling. When\nclass labels are present at the training stage, we show this low-rank\ntransformation framework also significantly enhances classification\nperformance. Extensive experiments using public datasets are presented, showing\nthat the proposed approach significantly outperforms state-of-the-art methods\nfor subspace clustering and classification. \n\n"}
{"id": "1309.2900", "contents": "Title: Mining for Spatially-Near Communities in Geo-Located Social Networks Abstract: Current approaches to community detection in social networks often ignore the\nspatial location of the nodes. In this paper, we look to extract spatially-near\ncommunities in a social network. We introduce a new metric to measure the\nquality of a community partition in a geolocated social networks called\n\"spatially-near modularity\" a value that increases based on aspects of the\nnetwork structure but decreases based on the distance between nodes in the\ncommunities. We then look to find an optimal partition with respect to this\nmeasure - which should be an \"ideal\" community with respect to both social ties\nand geographic location. Though an NP-hard problem, we introduce two heuristic\nalgorithms that attempt to maximize this measure and outperform non-geographic\ncommunity finding by an order of magnitude. Applications to counter-terrorism\nare also discussed. \n\n"}
{"id": "1309.3959", "contents": "Title: Bounded Confidence Opinion Dynamics in a Social Network of Bayesian\n  Decision Makers Abstract: Bounded confidence opinion dynamics model the propagation of information in\nsocial networks. However in the existing literature, opinions are only viewed\nas abstract quantities without semantics rather than as part of a\ndecision-making system. In this work, opinion dynamics are examined when agents\nare Bayesian decision makers that perform hypothesis testing or signal\ndetection, and the dynamics are applied to prior probabilities of hypotheses.\nBounded confidence is defined on prior probabilities through Bayes risk error\ndivergence, the appropriate measure between priors in hypothesis testing. This\ndefinition contrasts with the measure used between opinions in standard models:\nabsolute error. It is shown that the rapid convergence of prior probabilities\nto a small number of limiting values is similar to that seen in the standard\nKrause-Hegselmann model. The most interesting finding in this work is that the\nnumber of these limiting values and the time to convergence changes with the\nsignal-to-noise ratio in the detection task. The number of final values or\nclusters is maximal at intermediate signal-to-noise ratios, suggesting that the\nmost contentious issues lead to the largest number of factions. It is at these\nsame intermediate signal-to-noise ratios at which the degradation in detection\nperformance of the aggregate vote of the decision makers is greatest in\ncomparison to the Bayes optimal detection performance. \n\n"}
{"id": "1309.4306", "contents": "Title: Sparsity Based Poisson Denoising with Dictionary Learning Abstract: The problem of Poisson denoising appears in various imaging applications,\nsuch as low-light photography, medical imaging and microscopy. In cases of high\nSNR, several transformations exist so as to convert the Poisson noise into an\nadditive i.i.d. Gaussian noise, for which many effective algorithms are\navailable. However, in a low SNR regime, these transformations are\nsignificantly less accurate, and a strategy that relies directly on the true\nnoise statistics is required. A recent work by Salmon et al. took this route,\nproposing a patch-based exponential image representation model based on GMM\n(Gaussian mixture model), leading to state-of-the-art results. In this paper,\nwe propose to harness sparse-representation modeling to the image patches,\nadopting the same exponential idea. Our scheme uses a greedy pursuit with\nboot-strapping based stopping condition and dictionary learning within the\ndenoising process. The reconstruction performance of the proposed scheme is\ncompetitive with leading methods in high SNR, and achieving state-of-the-art\nresults in cases of low SNR. \n\n"}
{"id": "1309.4796", "contents": "Title: Bayesian Degree-Corrected Stochastic Blockmodels for Community Detection Abstract: Community detection in networks has drawn much attention in diverse fields,\nespecially social sciences. Given its significance, there has been a large body\nof literature with approaches from many fields. Here we present a statistical\nframework that is representative, extensible, and that yields an estimator with\ngood properties. Our proposed approach considers a stochastic blockmodel based\non a logistic regression formulation with node correction terms. We follow a\nBayesian approach that explicitly captures the community behavior via prior\nspecification. We further adopt a data augmentation strategy with latent\nPolya-Gamma variables to obtain posterior samples. We conduct inference based\non a principled, canonically mapped centroid estimator that formally addresses\nlabel non-identifiability and captures representative community assignments. We\ndemonstrate the proposed model and estimation on real-world as well as\nsimulated benchmark networks and show that the proposed model and estimator are\nmore flexible, representative, and yield smaller error rates when compared to\nthe MAP estimator from classical degree-corrected stochastic blockmodels. \n\n"}
{"id": "1309.6270", "contents": "Title: Optimal Resource Allocation for Network Protection Against Spreading\n  Processes Abstract: We study the problem of containing spreading processes in arbitrary directed\nnetworks by distributing protection resources throughout the nodes of the\nnetwork. We consider two types of protection resources are available: (i)\nPreventive resources able to defend nodes against the spreading (such as\nvaccines in a viral infection process), and (ii) corrective resources able to\nneutralize the spreading after it has reached a node (such as antidotes). We\nassume that both preventive and corrective resources have an associated cost\nand study the problem of finding the cost-optimal distribution of resources\nthroughout the nodes of the network. We analyze these questions in the context\nof viral spreading processes in directed networks. We study the following two\nproblems: (i) Given a fixed budget, find the optimal allocation of preventive\nand corrective resources in the network to achieve the highest level of\ncontainment, and (ii) when a budget is not specified, find the minimum budget\nrequired to control the spreading process. We show that both resource\nallocation problems can be solved in polynomial time using Geometric\nProgramming (GP) for arbitrary directed graphs of nonidentical nodes and a wide\nclass of cost functions. Furthermore, our approach allows to optimize\nsimultaneously over both preventive and corrective resources, even in the case\nof cost functions being node-dependent. We illustrate our approach by designing\noptimal protection strategies to contain an epidemic outbreak that propagates\nthrough an air transportation network. \n\n"}
{"id": "1309.7497", "contents": "Title: Meshless discretization of LQ-type stochastic control problems Abstract: We propose a novel Galerkin discretization scheme for stochastic optimal\ncontrol problems on an indefinite time horizon. The control problems are\nlinear-quadratic in the controls, but possibly nonlinear in the state\nvariables, and the discretization is based on the fact that problems of this\nkind can be transformed into linear boundary value problems by a logarithmic\ntransformation. We show that the discretized linear problem is dual to a Markov\ndecision problem, the precise form of which depends on the chosen Galerkin\nbasis. We prove a strong error bound in $L^{2}$ for the general scheme and\ndiscuss two special cases: a variant of the known Markov chain approximation\nobtained from a basis of characteristic functions of a box discretization, and\na sparse approximation that uses the basis of committor functions of metastable\nsets of the dynamics; the latter is particularly suited for high-dimensional\nsystems, e.g., control problems in molecular dynamics. We illustrate the method\nwith several numerical examples, one being the optimal control of Alanine\ndipeptide to its helical conformation. \n\n"}
{"id": "1310.3980", "contents": "Title: Decay towards the overall-healthy state in SIS epidemics on networks Abstract: The decay rate of SIS epidemics on the complete graph $K_{N}$ is computed\nanalytically, based on a new, algebraic method to compute the second largest\neigenvalue of a stochastic three-diagonal matrix up to arbitrary precision. The\nlatter problem has been addressed around 1950, mainly via the theory of\northogonal polynomials and probability theory. The accurate determination of\nthe second largest eigenvalue, also called the \\emph{decay parameter}, has been\nan outstanding problem appearing in general birth-death processes and random\nwalks. Application of our general framework to SIS epidemics shows that the\nmaximum average lifetime of an SIS epidemics in any network with $N$ nodes is\nnot larger (but tight for $K_{N}$) than \\[ E\\left[ T\\right]\n\\sim\\frac{1}{\\delta}\\frac{\\frac{\\tau}{\\tau_{c}}\\sqrt{2\\pi}% }{\\left(\n\\frac{\\tau}{\\tau_{c}}-1\\right) ^{2}}\\frac{\\exp\\left( N\\left\\{\n\\log\\frac{\\tau}{\\tau_{c}}+\\frac{\\tau_{c}}{\\tau}-1\\right\\} \\right) }{\\sqrt\n{N}}=O\\left( e^{N\\ln\\frac{\\tau}{\\tau_{c}}}\\right) \\] for large $N$ and for an\neffective infection rate $\\tau=\\frac{\\beta}{\\delta}$ above the epidemic\nthreshold $\\tau_{c}$. Our order estimate of $E\\left[ T\\right] $ sharpens the\norder estimate $E\\left[ T\\right] =O\\left( e^{bN^{a}}\\right) $ of Draief and\nMassouli\\'{e} \\cite{Draief_Massoulie}. Combining the lower bound results of\nMountford \\emph{et al.} \\cite{Mountford2013} and our upper bound, we conclude\nthat for almost all graphs, the average time to absorption for $\\tau>\\tau_{c}$\nis $E\\left[ T\\right] =O\\left( e^{c_{G}N}\\right) $, where $c_{G}>0$ depends on\nthe topological structure of the graph $G$ and $\\tau$. \n\n"}
{"id": "1310.4169", "contents": "Title: Naming Game on Networks: Let Everyone be Both Speaker and Hearer Abstract: To investigate how consensus is reached on a large self-organized\npeer-to-peer network, we extended the naming game model commonly used in\nlanguage and communication to Naming Game in Groups (NGG). Differing from other\nexisting naming game models, in NGG, everyone in the population (network) can\nbe both speaker and hearer simultaneously, which resembles in a closer manner\nto real-life scenarios. Moreover, NGG allows the transmission (communication)\nof multiple words (opinions) for multiple intra-group consensuses. The\ncommunications among indirectly-connected nodes are also enabled in NGG. We\nsimulated and analyzed the consensus process in some typical network\ntopologies, including random-graph networks, small-world networks and\nscale-free networks, to better understand how global convergence (consensus)\ncould be reached on one common word. The results are interpreted on group\nnegotiation of a peer-to-peer network, which shows that global consensus in the\npopulation can be reached more rapidly when more opinions are permitted within\neach group or when the negotiating groups in the population are larger in size.\nThe novel features and properties introduced by our model have demonstrated its\napplicability in better investigating general consensus problems on\npeer-to-peer networks. \n\n"}
{"id": "1310.6098", "contents": "Title: Optimal Shape Design by Partial Spectral Data Abstract: In this paper, we are concerned with a shape design problem, in which our\ntarget is to design, up to rigid transformations and scaling, the shape of an\nobject given either its polarization tensor at multiple contrasts or the\npartial eigenvalues of its Neumann-Poincar\\'e operator, which are known as the\nFredholm eigenvalues. We begin by proposing to recover the eigenvalues of the\nNeumann-Poincar\\'e operator from the polarization tensor by means of the\nholomorphic functional calculus. Then we develop a regularized Gauss-Newton\noptimization method for the shape reconstruction process. We present numerical\nresults to demonstrate the effectiveness of the proposed methods and to\nillustrate important properties of the Fredholm eigenvalues and their\nassociated eigenfunctions. Our results are expected to have important\napplications in the design of plasmon resonances in nanoparticles as well as in\nthe multifrequency or pulsed imaging of small anomalies. \n\n"}
{"id": "1310.6928", "contents": "Title: Non-asymptotic performance analysis of importance sampling schemes for\n  small noise diffusions Abstract: In this note we develop a prelimit analysis of performance measures for\nimportance sampling schemes related to small noise diffusion processes. In\nimportance sampling the performance of any change of measure is characterized\nby its second moment. For a given change of measure, we characterize the second\nmoment of the corresponding estimator as the solution to a PDE, which we\nanalyze via a full asymptotic expansion with respect to the size of the noise\nand obtain a precise statement on its accuracy. The main correction term to the\ndecay rate of the second moment solves a transport equation that can be solved\nexplicitly. The asymptotic expansion that we obtain identifies the source of\npossible poor performance of nevertheless asymptotically optimal importance\nsampling schemes and allows for more accurate comparison among competing\nimportance sampling schemes. \n\n"}
{"id": "1310.8499", "contents": "Title: Deep AutoRegressive Networks Abstract: We introduce a deep, generative autoencoder capable of learning hierarchies\nof distributed representations from data. Successive deep stochastic hidden\nlayers are equipped with autoregressive connections, which enable the model to\nbe sampled from quickly and exactly via ancestral sampling. We derive an\nefficient approximate parameter estimation method based on the minimum\ndescription length (MDL) principle, which can be seen as maximising a\nvariational lower bound on the log-likelihood, with a feedforward neural\nnetwork implementing approximate inference. We demonstrate state-of-the-art\ngenerative performance on a number of classic data sets: several UCI data sets,\nMNIST and Atari 2600 games. \n\n"}
{"id": "1311.2296", "contents": "Title: Newton based Stochastic Optimization using q-Gaussian Smoothed\n  Functional Algorithms Abstract: We present the first q-Gaussian smoothed functional (SF) estimator of the\nHessian and the first Newton-based stochastic optimization algorithm that\nestimates both the Hessian and the gradient of the objective function using\nq-Gaussian perturbations. Our algorithm requires only two system simulations\n(regardless of the parameter dimension) and estimates both the gradient and the\nHessian at each update epoch using these. We also present a proof of\nconvergence of the proposed algorithm. In a related recent work (Ghoshdastidar\net al., 2013), we presented gradient SF algorithms based on the q-Gaussian\nperturbations. Our work extends prior work on smoothed functional algorithms by\ngeneralizing the class of perturbation distributions as most distributions\nreported in the literature for which SF algorithms are known to work and turn\nout to be special cases of the q-Gaussian distribution. Besides studying the\nconvergence properties of our algorithm analytically, we also show the results\nof several numerical simulations on a model of a queuing network, that\nillustrate the significance of the proposed method. In particular, we observe\nthat our algorithm performs better in most cases, over a wide range of\nq-values, in comparison to Newton SF algorithms with the Gaussian (Bhatnagar,\n2007) and Cauchy perturbations, as well as the gradient q-Gaussian SF\nalgorithms (Ghoshdastidar et al., 2013). \n\n"}
{"id": "1311.7198", "contents": "Title: ADMM Algorithm for Graphical Lasso with an $\\ell_{\\infty}$ Element-wise\n  Norm Constraint Abstract: We consider the problem of Graphical lasso with an additional $\\ell_{\\infty}$\nelement-wise norm constraint on the precision matrix. This problem has\napplications in high-dimensional covariance decomposition such as in\n\\citep{Janzamin-12}. We propose an ADMM algorithm to solve this problem. We\nalso use a continuation strategy on the penalty parameter to have a fast\nimplemenation of the algorithm. \n\n"}
{"id": "1311.7320", "contents": "Title: Bayesian Inference for Gaussian Process Classifiers with Annealing and\n  Pseudo-Marginal MCMC Abstract: Kernel methods have revolutionized the fields of pattern recognition and\nmachine learning. Their success, however, critically depends on the choice of\nkernel parameters. Using Gaussian process (GP) classification as a working\nexample, this paper focuses on Bayesian inference of covariance (kernel)\nparameters using Markov chain Monte Carlo (MCMC) methods. The motivation is\nthat, compared to standard optimization of kernel parameters, they have been\nsystematically demonstrated to be superior in quantifying uncertainty in\npredictions. Recently, the Pseudo-Marginal MCMC approach has been proposed as a\npractical inference tool for GP models. In particular, it amounts in replacing\nthe analytically intractable marginal likelihood by an unbiased estimate\nobtainable by approximate methods and importance sampling. After discussing the\npotential drawbacks in employing importance sampling, this paper proposes the\napplication of annealed importance sampling. The results empirically\ndemonstrate that compared to importance sampling, annealed importance sampling\ncan reduce the variance of the estimate of the marginal likelihood\nexponentially in the number of data at a computational cost that scales only\npolynomially. The results on real data demonstrate that employing annealed\nimportance sampling in the Pseudo-Marginal MCMC approach represents a step\nforward in the development of fully automated exact inference engines for GP\nmodels. \n\n"}
{"id": "1312.0860", "contents": "Title: Community Specific Temporal Topic Discovery from Social Media Abstract: Studying temporal dynamics of topics in social media is very useful to\nunderstand online user behaviors. Most of the existing work on this subject\nusually monitors the global trends, ignoring variation among communities. Since\nusers from different communities tend to have varying tastes and interests,\ncapturing community-level temporal change can improve the understanding and\nmanagement of social content. Additionally, it can further facilitate the\napplications such as community discovery, temporal prediction and online\nmarketing. However, this kind of extraction becomes challenging due to the\nintricate interactions between community and topic, and intractable\ncomputational complexity.\n  In this paper, we take a unified solution towards the community-level topic\ndynamic extraction. A probabilistic model, CosTot (Community Specific\nTopics-over-Time) is proposed to uncover the hidden topics and communities, as\nwell as capture community-specific temporal dynamics. Specifically, CosTot\nconsiders text, time, and network information simultaneously, and well\ndiscovers the interactions between community and topic over time. We then\ndiscuss the approximate inference implementation to enable scalable computation\nof model parameters, especially for large social data. Based on this, the\napplication layer support for multi-scale temporal analysis and community\nexploration is also investigated.\n  We conduct extensive experimental studies on a large real microblog dataset,\nand demonstrate the superiority of proposed model on tasks of time stamp\nprediction, link prediction and topic perplexity. \n\n"}
{"id": "1312.1332", "contents": "Title: Optimization of Radiation Therapy Fractionation Schedules in the\n  Presence of Tumor Repopulation Abstract: We analyze the effect of tumor repopulation on optimal dose delivery in\nradiation therapy. We are primarily motivated by accelerated tumor repopulation\ntowards the end of radiation treatment, which is believed to play a role in\ntreatment failure for some tumor sites. A dynamic programming framework is\ndeveloped to determine an optimal fractionation scheme based on a model of cell\nkill due to radiation and tumor growth in between treatment days. We find that\nfaster tumor growth suggests shorter overall treatment duration. In addition,\nthe presence of accelerated repopulation suggests larger dose fractions later\nin the treatment to compensate for the increased tumor proliferation. We prove\nthat the optimal dose fractions are increasing over time. Numerical simulations\nindicate potential for improvement in treatment effectiveness. \n\n"}
{"id": "1312.1666", "contents": "Title: Semi-Stochastic Gradient Descent Methods Abstract: In this paper we study the problem of minimizing the average of a large\nnumber ($n$) of smooth convex loss functions. We propose a new method, S2GD\n(Semi-Stochastic Gradient Descent), which runs for one or several epochs in\neach of which a single full gradient and a random number of stochastic\ngradients is computed, following a geometric law. The total work needed for the\nmethod to output an $\\varepsilon$-accurate solution in expectation, measured in\nthe number of passes over data, or equivalently, in units equivalent to the\ncomputation of a single gradient of the loss, is\n$O((\\kappa/n)\\log(1/\\varepsilon))$, where $\\kappa$ is the condition number.\nThis is achieved by running the method for $O(\\log(1/\\varepsilon))$ epochs,\nwith a single gradient evaluation and $O(\\kappa)$ stochastic gradient\nevaluations in each. The SVRG method of Johnson and Zhang arises as a special\ncase. If our method is limited to a single epoch only, it needs to evaluate at\nmost $O((\\kappa/\\varepsilon)\\log(1/\\varepsilon))$ stochastic gradients. In\ncontrast, SVRG requires $O(\\kappa/\\varepsilon^2)$ stochastic gradients. To\nillustrate our theoretical results, S2GD only needs the workload equivalent to\nabout 2.1 full gradient evaluations to find an $10^{-6}$-accurate solution for\na problem with $n=10^9$ and $\\kappa=10^3$. \n\n"}
{"id": "1312.2184", "contents": "Title: Inverse coefficient problem for Grushin-type parabolic operators Abstract: The approach to Lipschitz stability for uniformly parabolic equations\nintroduced by Imanuvilov and Yamamoto in 1998 based on Carleman estimates,\nseems hard to apply to the case of Grushin-type operators studied in this\npaper. Indeed, such estimates are still missing for parabolic operators\ndegenerating in the interior of the space domain. Nevertheless, we are able to\nprove Lipschitz stability results for inverse coefficient problems for such\noperators, with locally distributed measurements in arbitrary space dimension.\nFor this purpose, we follow a strategy that combines Fourier decomposition and\nCarleman inequalities for certain heat equations with nonsmooth coefficients\n(solved by the Fourier modes). \n\n"}
{"id": "1312.5302", "contents": "Title: Parallel coordinate descent methods for composite minimization:\n  convergence analysis and error bounds Abstract: In this paper we propose a distributed version of a randomized\nblock-coordinate descent method for minimizing the sum of a partially separable\nsmooth convex function and a fully separable non-smooth convex function. Under\nthe assumption of block Lipschitz continuity of the gradient of the smooth\nfunction, this method is shown to have a sublinear convergence rate. Linear\nconvergence rate of the method is obtained for the newly introduced class of\ngeneralized error bound functions. We prove that the new class of generalized\nerror bound functions encompasses both global/local error bound functions and\nsmooth strongly convex functions. We also show that the theoretical estimates\non the convergence rate depend on the number of blocks chosen randomly and a\nnatural measure of separability of the objective function. \n\n"}
{"id": "1312.5604", "contents": "Title: Learning Transformations for Classification Forests Abstract: This work introduces a transformation-based learner model for classification\nforests. The weak learner at each split node plays a crucial role in a\nclassification tree. We propose to optimize the splitting objective by learning\na linear transformation on subspaces using nuclear norm as the optimization\ncriteria. The learned linear transformation restores a low-rank structure for\ndata from the same class, and, at the same time, maximizes the separation\nbetween different classes, thereby improving the performance of the split\nfunction. Theoretical and experimental results support the proposed framework. \n\n"}
{"id": "1312.5634", "contents": "Title: Fixed points of the EM algorithm and nonnegative rank boundaries Abstract: Mixtures of $r$ independent distributions for two discrete random variables\ncan be represented by matrices of nonnegative rank $r$. Likelihood inference\nfor the model of such joint distributions leads to problems in real algebraic\ngeometry that are addressed here for the first time. We characterize the set of\nfixed points of the Expectation-Maximization algorithm, and we study the\nboundary of the space of matrices with nonnegative rank at most $3$. Both of\nthese sets correspond to algebraic varieties with many irreducible components. \n\n"}
{"id": "1312.6211", "contents": "Title: An Empirical Investigation of Catastrophic Forgetting in Gradient-Based\n  Neural Networks Abstract: Catastrophic forgetting is a problem faced by many machine learning models\nand algorithms. When trained on one task, then trained on a second task, many\nmachine learning models \"forget\" how to perform the first task. This is widely\nbelieved to be a serious problem for neural networks. Here, we investigate the\nextent to which the catastrophic forgetting problem occurs for modern neural\nnetworks, comparing both established and recent gradient-based training\nalgorithms and activation functions. We also examine the effect of the\nrelationship between the first task and the second task on catastrophic\nforgetting. We find that it is always best to train using the dropout\nalgorithm--the dropout algorithm is consistently best at adapting to the new\ntask, remembering the old task, and has the best tradeoff curve between these\ntwo extremes. We find that different tasks and relationships between tasks\nresult in very different rankings of activation function performance. This\nsuggests the choice of activation function should always be cross-validated. \n\n"}
{"id": "1312.6722", "contents": "Title: On the limiting behavior of parameter-dependent network centrality\n  measures Abstract: We consider a broad class of walk-based, parameterized node centrality\nmeasures for network analysis. These measures are expressed in terms of\nfunctions of the adjacency matrix and generalize various well-known centrality\nindices, including Katz and subgraph centrality. We show that the parameter can\nbe \"tuned\" to interpolate between degree and eigenvector centrality, which\nappear as limiting cases. Our analysis helps explain certain correlations often\nobserved between the rankings obtained using different centrality measures, and\nprovides some guidance for the tuning of parameters. We also highlight the\nroles played by the spectral gap of the adjacency matrix and by the number of\ntriangles in the network. Our analysis covers both undirected and directed\nnetworks, including weighted ones. A brief discussion of PageRank is also\ngiven. \n\n"}
{"id": "1312.7559", "contents": "Title: A model selection approach for clustering a multinomial sequence with\n  non-negative factorization Abstract: We consider a problem of clustering a sequence of multinomial observations by\nway of a model selection criterion. We propose a form of a penalty term for the\nmodel selection procedure. Our approach subsumes both the conventional AIC and\nBIC criteria but also extends the conventional criteria in a way that it can be\napplicable also to a sequence of sparse multinomial observations, where even\nwithin a same cluster, the number of multinomial trials may be different for\ndifferent observations. In addition, as a preliminary estimation step to\nmaximum likelihood estimation, and more generally, to maximum $L_{q}$\nestimation, we propose to use reduced rank projection in combination with\nnon-negative factorization. We motivate our approach by showing that our model\nselection criterion and preliminary estimation step yield consistent estimates\nunder simplifying assumptions. We also illustrate our approach through\nnumerical experiments using real and simulated data. \n\n"}
{"id": "1401.0655", "contents": "Title: Critical Nodes In Directed Networks Abstract: Critical nodes or \"middlemen\" have an essential place in both social and\neconomic networks when considering the flow of information and trade. This\npaper extends the concept of critical nodes to directed networks. We identify\nstrong and weak middlemen. Node contestability is introduced as a form of\ncompetition in networks; a duality between uncontested intermediaries and\nmiddlemen is established. The brokerage power of middlemen is formally\nexpressed and a general algorithm is constructed to measure the brokerage power\nof each node from the networks adjacency matrix. Augmentations of the brokerage\npower measure are discussed to encapsulate relevant centrality measures. We use\nthese concepts to identify and measure middlemen in two empirical\nsocio-economic networks, the elite marriage network of Renaissance Florence and\nKrackhardt's advice network. \n\n"}
{"id": "1401.2257", "contents": "Title: Continuous selections of multivalued mappings Abstract: This survey covers in our opinion the most important results in the theory of\ncontinuous selections of multivalued mappings (approximately) from 2002 through\n2012. It extends and continues our previous such survey which appeared in\nRecent Progress in General Topology, II, which was published in 2002. In\ncomparison, our present survey considers more restricted and specific areas of\nmathematics. Note that we do not consider the theory of selectors (i.e.\ncontinuous choices of elements from subsets of topological spaces) since this\ntopics is covered by another survey in this volume. \n\n"}
{"id": "1401.3056", "contents": "Title: Power of individuals -- Controlling centrality of temporal networks Abstract: Temporal networks are such networks where nodes and interactions may appear\nand disappear at various time scales. With the evidence of ubiquity of temporal\nnetworks in our economy, nature and society, it's urgent and significant to\nfocus on structural controllability of temporal networks, which nowadays is\nstill an untouched topic. We develop graphic tools to study the structural\ncontrollability of temporal networks, identifying the intrinsic mechanism of\nthe ability of individuals in controlling a dynamic and large-scale temporal\nnetwork. Classifying temporal trees of a temporal network into different types,\nwe give (both upper and lower) analytical bounds of the controlling centrality,\nwhich are verified by numerical simulations of both artificial and empirical\ntemporal networks. We find that the scale-free distribution of node's\ncontrolling centrality is virtually independent of the time scale and types of\ndatasets, meaning the inherent heterogeneity and robustness of the controlling\ncentrality of temporal networks. \n\n"}
{"id": "1401.4786", "contents": "Title: Common Information based Markov Perfect Equilibria for Linear-Gaussian\n  Games with Asymmetric Information Abstract: We consider a class of two-player dynamic stochastic nonzero-sum games where\nthe state transition and observation equations are linear, and the primitive\nrandom variables are Gaussian. Each controller acquires possibly different\ndynamic information about the state process and the other controller's past\nactions and observations. This leads to a dynamic game of asymmetric\ninformation among the controllers. Building on our earlier work on finite games\nwith asymmetric information, we devise an algorithm to compute a Nash\nequilibrium by using the common information among the controllers. We call such\nequilibria common information based Markov perfect equilibria of the game,\nwhich can be viewed as a refinement of Nash equilibrium in games with\nasymmetric information. If the players' cost functions are quadratic, then we\nshow that under certain conditions a unique common information based Markov\nperfect equilibrium exists. Furthermore, this equilibrium can be computed by\nsolving a sequence of linear equations. We also show through an example that\nthere could be other Nash equilibria in a game of asymmetric information, not\ncorresponding to common information based Markov perfect equilibria. \n\n"}
{"id": "1401.6509", "contents": "Title: Linear Convergence of the Douglas-Rachford Method for Two Closed Sets Abstract: In this paper, we investigate the Douglas-Rachford method for two closed\n(possibly nonconvex) sets in Euclidean spaces. We show that under certain\nregularity conditions, the Douglas-Rachford method converges locally with\nR-linear rate. In convex settings, we prove that the linear convergence is\nglobal. Our study recovers recent results on the same topic. \n\n"}
{"id": "1401.6578", "contents": "Title: Simple Error Bounds for Regularized Noisy Linear Inverse Problems Abstract: Consider estimating a structured signal $\\mathbf{x}_0$ from linear,\nunderdetermined and noisy measurements\n$\\mathbf{y}=\\mathbf{A}\\mathbf{x}_0+\\mathbf{z}$, via solving a variant of the\nlasso algorithm: $\\hat{\\mathbf{x}}=\\arg\\min_\\mathbf{x}\\{\n\\|\\mathbf{y}-\\mathbf{A}\\mathbf{x}\\|_2+\\lambda f(\\mathbf{x})\\}$. Here, $f$ is a\nconvex function aiming to promote the structure of $\\mathbf{x}_0$, say\n$\\ell_1$-norm to promote sparsity or nuclear norm to promote low-rankness. We\nassume that the entries of $\\mathbf{A}$ are independent and normally\ndistributed and make no assumptions on the noise vector $\\mathbf{z}$, other\nthan it being independent of $\\mathbf{A}$. Under this generic setup, we derive\na general, non-asymptotic and rather tight upper bound on the $\\ell_2$-norm of\nthe estimation error $\\|\\hat{\\mathbf{x}}-\\mathbf{x}_0\\|_2$. Our bound is\ngeometric in nature and obeys a simple formula; the roles of $\\lambda$, $f$ and\n$\\mathbf{x}_0$ are all captured by a single summary parameter\n$\\delta(\\lambda\\partial((f(\\mathbf{x}_0)))$, termed the Gaussian squared\ndistance to the scaled subdifferential. We connect our result to the literature\nand verify its validity through simulations. \n\n"}
{"id": "1401.7020", "contents": "Title: A Stochastic Quasi-Newton Method for Large-Scale Optimization Abstract: The question of how to incorporate curvature information in stochastic\napproximation methods is challenging. The direct application of classical\nquasi- Newton updating techniques for deterministic optimization leads to noisy\ncurvature estimates that have harmful effects on the robustness of the\niteration. In this paper, we propose a stochastic quasi-Newton method that is\nefficient, robust and scalable. It employs the classical BFGS update formula in\nits limited memory form, and is based on the observation that it is beneficial\nto collect curvature information pointwise, and at regular intervals, through\n(sub-sampled) Hessian-vector products. This technique differs from the\nclassical approach that would compute differences of gradients, and where\ncontrolling the quality of the curvature estimates can be difficult. We present\nnumerical results on problems arising in machine learning that suggest that the\nproposed method shows much promise. \n\n"}
{"id": "1401.7569", "contents": "Title: Transversality and alternating projections for nonconvex sets Abstract: We consider the method of alternating projections for finding a point in the\nintersection of two closed sets, possibly nonconvex. Assuming only the standard\ntransversality condition (or a weaker version thereof), we prove local linear\nconvergence. When the two sets are semi-algebraic and bounded, but not\nnecessarily transversal, we nonetheless prove subsequence convergence. \n\n"}
{"id": "1402.1700", "contents": "Title: On the Prediction Performance of the Lasso Abstract: Although the Lasso has been extensively studied, the relationship between its\nprediction performance and the correlations of the covariates is not fully\nunderstood. In this paper, we give new insights into this relationship in the\ncontext of multiple linear regression. We show, in particular, that the\nincorporation of a simple correlation measure into the tuning parameter can\nlead to a nearly optimal prediction performance of the Lasso even for highly\ncorrelated covariates. However, we also reveal that for moderately correlated\ncovariates, the prediction performance of the Lasso can be mediocre\nirrespective of the choice of the tuning parameter. We finally show that our\nresults also lead to near-optimal rates for the least-squares estimator with\ntotal variation penalty. \n\n"}
{"id": "1402.2066", "contents": "Title: Distributed Robustness Analysis of Interconnected Uncertain Systems\n  Using Chordal Decomposition Abstract: Large-scale interconnected uncertain systems commonly have large state and\nuncertainty dimensions. Aside from the heavy computational cost of solving\ncentralized robust stability analysis techniques, privacy requirements in the\nnetwork can also introduce further issues. In this paper, we utilize IQC\nanalysis for analyzing large-scale interconnected uncertain systems and we\nevade these issues by describing a decomposition scheme that is based on the\ninterconnection structure of the system. This scheme is based on the so-called\nchordal decomposition and does not add any conservativeness to the analysis\napproach. The decomposed problem can be solved using distributed computational\nalgorithms without the need for a centralized computational unit. We further\ndiscuss the merits of the proposed analysis approach using a numerical\nexperiment. \n\n"}
{"id": "1402.4304", "contents": "Title: Automatic Construction and Natural-Language Description of Nonparametric\n  Regression Models Abstract: This paper presents the beginnings of an automatic statistician, focusing on\nregression problems. Our system explores an open-ended space of statistical\nmodels to discover a good explanation of a data set, and then produces a\ndetailed report with figures and natural-language text. Our approach treats\nunknown regression functions nonparametrically using Gaussian processes, which\nhas two important consequences. First, Gaussian processes can model functions\nin terms of high-level properties (e.g. smoothness, trends, periodicity,\nchangepoints). Taken together with the compositional structure of our language\nof models this allows us to automatically describe functions in simple terms.\nSecond, the use of flexible nonparametric models and a rich language for\ncomposing them in an open-ended manner also results in state-of-the-art\nextrapolation performance evaluated over 13 real time series data sets from\nvarious domains. \n\n"}
{"id": "1402.5709", "contents": "Title: Optimal Control of a Free Boundary Problem with Surface Tension Effects:\n  A Priori Error Analysis Abstract: We present a finite element method along with its analysis for the optimal\ncontrol of a model free boundary problem with surface tension effects,\nformulated and studied in \\cite{HAntil_RHNochetto_PSodre_2014a}. The state\nsystem couples the Laplace equation in the bulk with the Young-Laplace equation\non the free boundary to account for surface tension. We first prove that the\nstate and adjoint system have the requisite regularity for the error analysis\n(strong solutions). We discretize the state, adjoint and control variables via\npiecewise linear finite elements and show optimal $O(h)$ error estimates for\nall variables, including the control. This entails using the second order\nsufficient optimality conditions of \\cite{HAntil_RHNochetto_PSodre_2014a}, and\nthe first order necessary optimality conditions for both the continuous and\ndiscrete systems. We conclude with two numerical examples which examine the\nvarious error estimates. \n\n"}
{"id": "1403.0515", "contents": "Title: A Primal Dual Active Set with Continuation Algorithm for the\n  \\ell^0-Regularized Optimization Problem Abstract: We develop a primal dual active set with continuation algorithm for solving\nthe \\ell^0-regularized least-squares problem that frequently arises in\ncompressed sensing. The algorithm couples the the primal dual active set method\nwith a continuation strategy on the regularization parameter. At each inner\niteration, it first identifies the active set from both primal and dual\nvariables, and then updates the primal variable by solving a (typically small)\nleast-squares problem defined on the active set, from which the dual variable\ncan be updated explicitly. Under certain conditions on the sensing matrix,\ni.e., mutual incoherence property or restricted isometry property, and the\nnoise level, the finite step global convergence of the algorithm is\nestablished. Extensive numerical examples are presented to illustrate the\nefficiency and accuracy of the algorithm and the convergence analysis. \n\n"}
{"id": "1403.0603", "contents": "Title: Efficient Distributed Online Prediction and Stochastic Optimization with\n  Approximate Distributed Averaging Abstract: We study distributed methods for online prediction and stochastic\noptimization. Our approach is iterative: in each round nodes first perform\nlocal computations and then communicate in order to aggregate information and\nsynchronize their decision variables. Synchronization is accomplished through\nthe use of a distributed averaging protocol. When an exact distributed\naveraging protocol is used, it is known that the optimal regret bound of\n$\\mathcal{O}(\\sqrt{m})$ can be achieved using the distributed mini-batch\nalgorithm of Dekel et al. (2012), where $m$ is the total number of samples\nprocessed across the network. We focus on methods using approximate distributed\naveraging protocols and show that the optimal regret bound can also be achieved\nin this setting. In particular, we propose a gossip-based optimization method\nwhich achieves the optimal regret bound. The amount of communication required\ndepends on the network topology through the second largest eigenvalue of the\ntransition matrix of a random walk on the network. In the setting of stochastic\noptimization, the proposed gossip-based approach achieves nearly-linear\nscaling: the optimization error is guaranteed to be no more than $\\epsilon$\nafter $\\mathcal{O}(\\frac{1}{n \\epsilon^2})$ rounds, each of which involves\n$\\mathcal{O}(\\log n)$ gossip iterations, when nodes communicate over a\nwell-connected graph. This scaling law is also observed in numerical\nexperiments on a cluster. \n\n"}
{"id": "1403.1166", "contents": "Title: Mathematical optimization for packing problems Abstract: During the last few years several new results on packing problems were\nobtained using a blend of tools from semidefinite optimization, polynomial\noptimization, and harmonic analysis. We survey some of these results and the\ntechniques involved, concentrating on geometric packing problems such as the\nsphere-packing problem or the problem of packing regular tetrahedra in R^3. \n\n"}
{"id": "1403.1738", "contents": "Title: A Fast Active Set Block Coordinate Descent Algorithm for\n  $\\ell_1$-regularized least squares Abstract: The problem of finding sparse solutions to underdetermined systems of linear\nequations arises in several applications (e.g. signal and image processing,\ncompressive sensing, statistical inference). A standard tool for dealing with\nsparse recovery is the $\\ell_1$-regularized least-squares approach that has\nbeen recently attracting the attention of many researchers. In this paper, we\ndescribe an active set estimate (i.e. an estimate of the indices of the zero\nvariables in the optimal solution) for the considered problem that tries to\nquickly identify as many active variables as possible at a given point, while\nguaranteeing that some approximate optimality conditions are satisfied. A\nrelevant feature of the estimate is that it gives a significant reduction of\nthe objective function when setting to zero all those variables estimated\nactive. This enables to easily embed it into a given globally converging\nalgorithmic framework. In particular, we include our estimate into a block\ncoordinate descent algorithm for $\\ell_1$-regularized least squares, analyze\nthe convergence properties of this new active set method, and prove that its\nbasic version converges with linear rate. Finally, we report some numerical\nresults showing the effectiveness of the approach. \n\n"}
{"id": "1403.4879", "contents": "Title: A Compressive Sensing Based Approach to Sparse Wideband Array Design Abstract: Sparse wideband sensor array design for sensor location optimisation is\nhighly nonlinear and it is traditionally solved by genetic algorithms,\nsimulated annealing or other similar optimization methods. However, this is an\nextremely time-consuming process and more efficient solutions are needed. In\nthis work, this problem is studied from the viewpoint of compressive sensing\nand a formulation based on a modified $l_1$ norm is derived. As there are\nmultiple coefficients associated with each sensor, the key is to make sure that\nthese coefficients are simultaneously minimized in order to discard the\ncorresponding sensor locations. Design examples are provided to verify the\neffectiveness of the proposed methods. \n\n"}
{"id": "1403.5784", "contents": "Title: Subgradient algorithms for solving variable inequalities Abstract: In this paper we consider the variable inequality problem, that is, to find a\nsolution of the inclusion given by the sum of a function and a point-to-cone\napplication. This problem can be seen as a generalization of the classical\nsystem inequality problem taking a variable order structure. Exploiting this\nspecial structure, we propose two variants of the subgradient algorithm for\nsolving a system of variable inequalities. The convergence analysis is given\nunder convex-like conditions which, when the point-to-cone application is\nconstant, contains the old subgradient schemes. \n\n"}
{"id": "1403.6351", "contents": "Title: Submodularity of Energy Related Controllability Metrics Abstract: The quantification of controllability and observability has recently received\nnew interest in the context of large, complex networks of dynamical systems. A\nfundamental but computationally difficult problem is the placement or selection\nof actuators and sensors that optimize real-valued controllability and\nobservability metrics of the network. We show that several classes of energy\nrelated metrics associated with the controllability Gramian in linear dynamical\nsystems have a strong structural property, called submodularity. This property\nallows for an approximation guarantee by using a simple greedy heuristic for\ntheir maximization. The results are illustrated for randomly generated systems\nand for placement of power electronic actuators in a model of the European\npower grid. \n\n"}
{"id": "1403.7588", "contents": "Title: Scalable Robust Matrix Recovery: Frank-Wolfe Meets Proximal Methods Abstract: Recovering matrices from compressive and grossly corrupted observations is a\nfundamental problem in robust statistics, with rich applications in computer\nvision and machine learning. In theory, under certain conditions, this problem\ncan be solved in polynomial time via a natural convex relaxation, known as\nCompressive Principal Component Pursuit (CPCP). However, all existing provable\nalgorithms for CPCP suffer from superlinear per-iteration cost, which severely\nlimits their applicability to large scale problems. In this paper, we propose\nprovable, scalable and efficient methods to solve CPCP with (essentially)\nlinear per-iteration cost. Our method combines classical ideas from Frank-Wolfe\nand proximal methods. In each iteration, we mainly exploit Frank-Wolfe to\nupdate the low-rank component with rank-one SVD and exploit the proximal step\nfor the sparse term. Convergence results and implementation details are also\ndiscussed. We demonstrate the scalability of the proposed approach with\npromising numerical experiments on visual data. \n\n"}
{"id": "1403.8034", "contents": "Title: Keep Your Friends Close and Your Facebook Friends Closer: A Multiplex\n  Network Approach to the Analysis of Offline and Online Social Ties Abstract: Social media allow for an unprecedented amount of interaction between people\nonline. A fundamental aspect of human social behavior, however, is the tendency\nof people to associate themselves with like-minded individuals, forming\nhomogeneous social circles both online and offline. In this work, we apply a\nnew model that allows us to distinguish between social ties of varying\nstrength, and to observe evidence of homophily with regards to politics, music,\nhealth, residential sector & year in college, within the online and offline\nsocial network of 74 college students. We present a multiplex network approach\nto social tie strength, here applied to mobile communication data - calls, text\nmessages, and co-location, allowing us to dimensionally identify relationships\nby considering the number of communication channels utilized between students.\nWe find that strong social ties are characterized by maximal use of\ncommunication channels, while weak ties by minimal use. We are able to identify\n75% of close friendships, 90% of weaker ties, and 90% of Facebook friendships\nas compared to reported ground truth. We then show that stronger ties exhibit\ngreater profile similarity than weaker ones. Apart from high homogeneity in\nsocial circles with respect to political and health aspects, we observe strong\nhomophily driven by music, residential sector and year in college. Despite\nFacebook friendship being highly dependent on residence and year, exposure to\nless homogeneous content can be found in the online rather than the offline\nsocial circles of students, most notably in political and music aspects. \n\n"}
{"id": "1404.2427", "contents": "Title: Projection onto simplicial cones by a semi-smooth Newton method Abstract: By using Moreau's decomposition theorem for projecting onto cones, the\nproblem of projecting onto a simplicial cone is reduced to finding the unique\nsolution of a nonsmooth system of equations. It is shown that a semi-smooth\nNewton method applied to the system of equations associated to the problem of\nprojecting onto a simplicial cone is always well defined, and the generated\nsequence is bounded for any starting point and under a somewhat restrictive\nassumption it is finite. Besides, under a mild assumption on the simplicial\ncone, the generated sequence converges linearly to the solution of the\nassociated system of equations. \n\n"}
{"id": "1404.4582", "contents": "Title: An inertial alternating direction method of multipliers Abstract: In the context of convex optimization problems in Hilbert spaces, we induce\ninertial effects into the classical ADMM numerical scheme and obtain in this\nway so-called inertial ADMM algorithms, the convergence properties of which we\ninvestigate into detail. To this aim we make use of the inertial version of the\nDouglas-Rachford splitting method for monotone inclusion problems recently\nintroduced in [12], in the context of concomitantly solving a convex\nminimization problem and its Fenchel dual. The convergence of both sequences of\nthe generated iterates and of the objective function values is addressed. We\nalso show how the obtained results can be extended to the treating of convex\nminimization problems having as objective a finite sum of convex functions. \n\n"}
{"id": "1404.5071", "contents": "Title: Sparsity-Exploiting Moment-Based Relaxations of the Optimal Power Flow\n  Problem Abstract: Convex relaxations of non-convex optimal power flow (OPF) problems have\nrecently attracted significant interest. While existing relaxations globally\nsolve many OPF problems, there are practical problems for which existing\nrelaxations fail to yield physically meaningful solutions. This paper applies\nmoment relaxations to solve many of these OPF problems. The moment relaxations\nare developed from the Lasserre hierarchy for solving generalized moment\nproblems. Increasing the relaxation order in this hierarchy results in\n\"tighter\" relaxations at the computational cost of larger semidefinite\nprograms. Low-order moment relaxations are capable of globally solving many\nsmall OPF problems for which existing relaxations fail. By exploiting sparsity\nand only applying the higher-order relaxation to specific buses, global\nsolutions to larger problems are computationally tractable through the use of\nan iterative algorithm informed by a heuristic for choosing where to apply the\nhigher-order constraints. With standard semidefinite programming solvers, the\nalgorithm globally solves many test systems with up to 300 buses for which the\nexisting semidefinite relaxation fails to yield globally optimal solutions. \n\n"}
{"id": "1404.5222", "contents": "Title: Self-Averaging Property of Minimal Investment Risk of Mean-Variance\n  Model Abstract: In portfolio optimization problems, the minimum expected investment risk is\nnot always smaller than the expected minimal investment risk. That is, using a\nwell-known approach from operations research, it is possible to derive a\nstrategy that minimizes the expected investment risk, but this strategy does\nnot always result in the best rate of return on assets. Prior to making\ninvestment decisions, it is important to an investor to know the potential\nminimal investment risk (or the expected minimal investment risk) and to\ndetermine the strategy that will maximize the return on assets. We use the\nself-averaging property to analyze the potential minimal investment risk and\nthe concentrated investment level for the strategy that gives the best rate of\nreturn. We compare the results from our method with the results obtained by the\noperations research approach and with those obtained by a numerical simulation\nusing the optimal portfolio. The results of our method and the numerical\nsimulation are in agreement, but they differ from that of the operations\nresearch approach. \n\n"}
{"id": "1404.5903", "contents": "Title: Most Correlated Arms Identification Abstract: We study the problem of finding the most mutually correlated arms among many\narms. We show that adaptive arms sampling strategies can have significant\nadvantages over the non-adaptive uniform sampling strategy. Our proposed\nalgorithms rely on a novel correlation estimator. The use of this accurate\nestimator allows us to get improved results for a wide range of problem\ninstances. \n\n"}
{"id": "1404.6026", "contents": "Title: Proximal linearized iteratively reweighted least squares for a class of\n  nonconvex and nonsmooth problems Abstract: For solving a wide class of nonconvex and nonsmooth problems, we propose a\nproximal linearized iteratively reweighted least squares (PL-IRLS) algorithm.\nWe first approximate the original problem by smoothing methods, and second\nwrite the approximated problem into an auxiliary problem by introducing new\nvariables. PL-IRLS is then built on solving the auxiliary problem by utilizing\nthe proximal linearization technique and the iteratively reweighted least\nsquares (IRLS) method, and has remarkable computation advantages. We show that\nPL-IRLS can be extended to solve more general nonconvex and nonsmooth problems\nvia adjusting generalized parameters, and also to solve nonconvex and nonsmooth\nproblems with two or more blocks of variables. Theoretically, with the help of\nthe Kurdyka- Lojasiewicz property, we prove that each bounded sequence\ngenerated by PL-IRLS globally converges to a critical point of the approximated\nproblem. To the best of our knowledge, this is the first global convergence\nresult of applying IRLS idea to solve nonconvex and nonsmooth problems. At\nlast, we apply PL-IRLS to solve three representative nonconvex and nonsmooth\nproblems in sparse signal recovery and low-rank matrix recovery and obtain new\nglobally convergent algorithms. \n\n"}
{"id": "1404.6289", "contents": "Title: Solution Path Clustering with Adaptive Concave Penalty Abstract: Fast accumulation of large amounts of complex data has created a need for\nmore sophisticated statistical methodologies to discover interesting patterns\nand better extract information from these data. The large scale of the data\noften results in challenging high-dimensional estimation problems where only a\nminority of the data shows specific grouping patterns. To address these\nemerging challenges, we develop a new clustering methodology that introduces\nthe idea of a regularization path into unsupervised learning. A regularization\npath for a clustering problem is created by varying the degree of sparsity\nconstraint that is imposed on the differences between objects via the minimax\nconcave penalty with adaptive tuning parameters. Instead of providing a single\nsolution represented by a cluster assignment for each object, the method\nproduces a short sequence of solutions that determines not only the cluster\nassignment but also a corresponding number of clusters for each solution. The\noptimization of the penalized loss function is carried out through an MM\nalgorithm with block coordinate descent. The advantages of this clustering\nalgorithm compared to other existing methods are as follows: it does not\nrequire the input of the number of clusters; it is capable of simultaneously\nseparating irrelevant or noisy observations that show no grouping pattern,\nwhich can greatly improve data interpretation; it is a general methodology that\ncan be applied to many clustering problems. We test this method on various\nsimulated datasets and on gene expression data, where it shows better or\ncompetitive performance compared against several clustering methods. \n\n"}
{"id": "1404.7530", "contents": "Title: Design and analysis of experiments in networks: Reducing bias from\n  interference Abstract: Estimating the effects of interventions in networks is complicated when the\nunits are interacting, such that the outcomes for one unit may depend on the\ntreatment assignment and behavior of many or all other units (i.e., there is\ninterference). When most or all units are in a single connected component, it\nis impossible to directly experimentally compare outcomes under two or more\nglobal treatment assignments since the network can only be observed under a\nsingle assignment. Familiar formalism, experimental designs, and analysis\nmethods assume the absence of these interactions, and result in biased\nestimators of causal effects of interest. While some assumptions can lead to\nunbiased estimators, these assumptions are generally unrealistic, and we focus\nthis work on realistic assumptions. Thus, in this work, we evaluate methods for\ndesigning and analyzing randomized experiments that aim to reduce this bias and\nthereby reduce overall error. In design, we consider the ability to perform\nrandom assignment to treatments that is correlated in the network, such as\nthrough graph cluster randomization. In analysis, we consider incorporating\ninformation about the treatment assignment of network neighbors. We prove\nsufficient conditions for bias reduction through both design and analysis in\nthe presence of potentially global interference. Through simulations of the\nentire process of experimentation in networks, we measure the performance of\nthese methods under varied network structure and varied social behaviors,\nfinding substantial bias and error reductions. These improvements are largest\nfor networks with more clustering and data generating processes with both\nstronger direct effects of the treatment and stronger interactions between\nunits. \n\n"}
{"id": "1405.0766", "contents": "Title: Convex Relaxation of Optimal Power Flow, Part I: Formulations and\n  Equivalence Abstract: This tutorial summarizes recent advances in the convex relaxation of the\noptimal power flow (OPF) problem, focusing on structural properties rather than\nalgorithms. Part I presents two power flow models, formulates OPF and their\nrelaxations in each model, and proves equivalence relations among them. Part II\npresents sufficient conditions under which the convex relaxations are exact. \n\n"}
{"id": "1405.1004", "contents": "Title: Model Consistency of Partly Smooth Regularizers Abstract: This paper studies least-square regression penalized with partly smooth\nconvex regularizers. This class of functions is very large and versatile\nallowing to promote solutions conforming to some notion of low-complexity.\nIndeed, they force solutions of variational problems to belong to a\nlow-dimensional manifold (the so-called model) which is stable under small\nperturbations of the function. This property is crucial to make the underlying\nlow-complexity model robust to small noise. We show that a generalized\n\"irrepresentable condition\" implies stable model selection under small noise\nperturbations in the observations and the design matrix, when the\nregularization parameter is tuned proportionally to the noise level. This\ncondition is shown to be almost a necessary condition. We then show that this\ncondition implies model consistency of the regularized estimator. That is, with\na probability tending to one as the number of measurements increases, the\nregularized estimator belongs to the correct low-dimensional model manifold.\nThis work unifies and generalizes several previous ones, where model\nconsistency is known to hold for sparse, group sparse, total variation and\nlow-rank regularizations. \n\n"}
{"id": "1405.2566", "contents": "Title: Learning modular structures from network data and node variables Abstract: A standard technique for understanding underlying dependency structures among\na set of variables posits a shared conditional probability distribution for the\nvariables measured on individuals within a group. This approach is often\nreferred to as module networks, where individuals are represented by nodes in a\nnetwork, groups are termed modules, and the focus is on estimating the network\nstructure among modules. However, estimation solely from node-specific\nvariables can lead to spurious dependencies, and unverifiable structural\nassumptions are often used for regularization. Here, we propose an extended\nmodel that leverages direct observations about the network in addition to\nnode-specific variables. By integrating complementary data types, we avoid the\nneed for structural assumptions. We illustrate theoretical and practical\nsignificance of the model and develop a reversible-jump MCMC learning procedure\nfor learning modules and model parameters. We demonstrate the method accuracy\nin predicting modular structures from synthetic data and capability to learn\ninfluence structures in twitter data and regulatory modules in the\nMycobacterium tuberculosis gene regulatory network. \n\n"}
{"id": "1405.2690", "contents": "Title: Policy Gradients for CVaR-Constrained MDPs Abstract: We study a risk-constrained version of the stochastic shortest path (SSP)\nproblem, where the risk measure considered is Conditional Value-at-Risk (CVaR).\nWe propose two algorithms that obtain a locally risk-optimal policy by\nemploying four tools: stochastic approximation, mini batches, policy gradients\nand importance sampling. Both the algorithms incorporate a CVaR estimation\nprocedure, along the lines of Bardou et al. [2009], which in turn is based on\nRockafellar-Uryasev's representation for CVaR and utilize the likelihood ratio\nprinciple for estimating the gradient of the sum of one cost function\n(objective of the SSP) and the gradient of the CVaR of the sum of another cost\nfunction (in the constraint of SSP). The algorithms differ in the manner in\nwhich they approximate the CVaR estimates/necessary gradients - the first\nalgorithm uses stochastic approximation, while the second employ mini-batches\nin the spirit of Monte Carlo methods. We establish asymptotic convergence of\nboth the algorithms. Further, since estimating CVaR is related to rare-event\nsimulation, we incorporate an importance sampling based variance reduction\nscheme into our proposed algorithms. \n\n"}
{"id": "1405.3080", "contents": "Title: Accelerating Minibatch Stochastic Gradient Descent using Stratified\n  Sampling Abstract: Stochastic Gradient Descent (SGD) is a popular optimization method which has\nbeen applied to many important machine learning tasks such as Support Vector\nMachines and Deep Neural Networks. In order to parallelize SGD, minibatch\ntraining is often employed. The standard approach is to uniformly sample a\nminibatch at each step, which often leads to high variance. In this paper we\npropose a stratified sampling strategy, which divides the whole dataset into\nclusters with low within-cluster variance; we then take examples from these\nclusters using a stratified sampling technique. It is shown that the\nconvergence rate can be significantly improved by the algorithm. Encouraging\nexperimental results confirm the effectiveness of the proposed method. \n\n"}
{"id": "1405.4161", "contents": "Title: Long and winding central paths Abstract: We disprove a continuous analogue of the Hirsch conjecture proposed by Deza,\nTerlaky and Zinchenko, by constructing a family of linear programs with $3r+4$\ninequalities in dimension $2r+2$ where the central path has a total curvature\nin $\\Omega(2^r)$. Our method is to tropicalize the central path in linear\nprogramming. The tropical central path is the piecewise-linear limit of the\ncentral paths of parameterized families of classical linear programs viewed\nthrough logarithmic glasses. The lower bound for the classical curvature is\nobtained by developing a combinatorial concept of a tropical angle. \n\n"}
{"id": "1405.4658", "contents": "Title: Ergodicity conditions for zero-sum games Abstract: A basic question for zero-sum repeated games consists in determining whether\nthe mean payoff per time unit is independent of the initial state. In the\nspecial case of \"zero-player\" games, i.e., of Markov chains equipped with\nadditive functionals, the answer is provided by the mean ergodic theorem. We\ngeneralize this result to repeated games. We show that the mean payoff is\nindependent of the initial state for all state-dependent perturbations of the\nrewards if and only if an ergodicity condition is verified. The latter is\ncharacterized by the uniqueness modulo constants of nonlinear harmonic\nfunctions (fixed points of the recession function associated to the Shapley\noperator), or, in the special case of stochastic games with finite action\nspaces and perfect information, by a reachability condition involving conjugate\nsubsets of states in directed hypergraphs. We show that the ergodicity\ncondition for games only depends on the support of the transition probability,\nand that it can be checked in polynomial time when the number of states is\nfixed. \n\n"}
{"id": "1405.4807", "contents": "Title: Scalable Semidefinite Relaxation for Maximum A Posterior Estimation Abstract: Maximum a posteriori (MAP) inference over discrete Markov random fields is a\nfundamental task spanning a wide spectrum of real-world applications, which is\nknown to be NP-hard for general graphs. In this paper, we propose a novel\nsemidefinite relaxation formulation (referred to as SDR) to estimate the MAP\nassignment. Algorithmically, we develop an accelerated variant of the\nalternating direction method of multipliers (referred to as SDPAD-LR) that can\neffectively exploit the special structure of the new relaxation. Encouragingly,\nthe proposed procedure allows solving SDR for large-scale problems, e.g.,\nproblems on a grid graph comprising hundreds of thousands of variables with\nmultiple states per node. Compared with prior SDP solvers, SDPAD-LR is capable\nof attaining comparable accuracy while exhibiting remarkably improved\nscalability, in contrast to the commonly held belief that semidefinite\nrelaxation can only been applied on small-scale MRF problems. We have evaluated\nthe performance of SDR on various benchmark datasets including OPENGM2 and PIC\nin terms of both the quality of the solutions and computation time.\nExperimental results demonstrate that for a broad class of problems, SDPAD-LR\noutperforms state-of-the-art algorithms in producing better MAP assignment in\nan efficient manner. \n\n"}
{"id": "1405.4980", "contents": "Title: Convex Optimization: Algorithms and Complexity Abstract: This monograph presents the main complexity theorems in convex optimization\nand their corresponding algorithms. Starting from the fundamental theory of\nblack-box optimization, the material progresses towards recent advances in\nstructural optimization and stochastic optimization. Our presentation of\nblack-box optimization, strongly influenced by Nesterov's seminal book and\nNemirovski's lecture notes, includes the analysis of cutting plane methods, as\nwell as (accelerated) gradient descent schemes. We also pay special attention\nto non-Euclidean settings (relevant algorithms include Frank-Wolfe, mirror\ndescent, and dual averaging) and discuss their relevance in machine learning.\nWe provide a gentle introduction to structural optimization with FISTA (to\noptimize a sum of a smooth and a simple non-smooth term), saddle-point mirror\nprox (Nemirovski's alternative to Nesterov's smoothing), and a concise\ndescription of interior point methods. In stochastic optimization we discuss\nstochastic gradient descent, mini-batches, random coordinate descent, and\nsublinear algorithms. We also briefly touch upon convex relaxation of\ncombinatorial problems and the use of randomness to round solutions, as well as\nrandom walks based methods. \n\n"}
{"id": "1405.5850", "contents": "Title: Joint Image Reconstruction and Segmentation Using the Potts Model Abstract: We propose a new algorithmic approach to the non-smooth and non-convex Potts\nproblem (also called piecewise-constant Mumford-Shah problem) for inverse\nimaging problems. We derive a suitable splitting into specific subproblems that\ncan all be solved efficiently. Our method does not require a priori knowledge\non the gray levels nor on the number of segments of the reconstruction.\nFurther, it avoids anisotropic artifacts such as geometric staircasing. We\ndemonstrate the suitability of our method for joint image reconstruction and\nsegmentation. We focus on Radon data, where we in particular consider limited\ndata situations. For instance, our method is able to recover all segments of\nthe Shepp-Logan phantom from $7$ angular views only. We illustrate the\npractical applicability on a real PET dataset. As further applications, we\nconsider spherical Radon data as well as blurred data. \n\n"}
{"id": "1406.0724", "contents": "Title: An inertial Tseng's type proximal algorithm for nonsmooth and nonconvex\n  optimization problems Abstract: We investigate the convergence of a forward-backward-forward proximal-type\nalgorithm with inertial and memory effects when minimizing the sum of a\nnonsmooth function with a smooth one in the absence of convexity. The\nconvergence is obtained provided an appropriate regularization of the objective\nsatisfies the Kurdyka-\\L{}ojasiewicz inequality, which is for instance\nfulfilled for semi-algebraic functions. \n\n"}
{"id": "1406.0764", "contents": "Title: Constructing Dynamic Treatment Regimes in Infinite-Horizon Settings Abstract: The application of existing methods for constructing optimal dynamic\ntreatment regimes is limited to cases where investigators are interested in\noptimizing a utility function over a fixed period of time (finite horizon). In\nthis manuscript, we develop an inferential procedure based on temporal\ndifference residuals for optimal dynamic treatment regimes in infinite-horizon\nsettings, where there is no a priori fixed end of follow-up point. The proposed\nmethod can be used to determine the optimal regime in chronic diseases where\npatients are monitored and treated throughout their life. We derive large\nsample results necessary for conducting inference. We also simulate a cohort of\npatients with diabetes to mimic the third wave of the National Health and\nNutrition Examination Survey, and we examine the performance of the proposed\nmethod in controlling the level of hemoglobin A1c. Supplementary materials for\nthis article are available online. \n\n"}
{"id": "1406.1102", "contents": "Title: Linear Convergence of Variance-Reduced Stochastic Gradient without\n  Strong Convexity Abstract: Stochastic gradient algorithms estimate the gradient based on only one or a\nfew samples and enjoy low computational cost per iteration. They have been\nwidely used in large-scale optimization problems. However, stochastic gradient\nalgorithms are usually slow to converge and achieve sub-linear convergence\nrates, due to the inherent variance in the gradient computation. To accelerate\nthe convergence, some variance-reduced stochastic gradient algorithms, e.g.,\nproximal stochastic variance-reduced gradient (Prox-SVRG) algorithm, have\nrecently been proposed to solve strongly convex problems. Under the strongly\nconvex condition, these variance-reduced stochastic gradient algorithms achieve\na linear convergence rate. However, many machine learning problems are convex\nbut not strongly convex. In this paper, we introduce Prox-SVRG and its\nprojected variant called Variance-Reduced Projected Stochastic Gradient (VRPSG)\nto solve a class of non-strongly convex optimization problems widely used in\nmachine learning. As the main technical contribution of this paper, we show\nthat both VRPSG and Prox-SVRG achieve a linear convergence rate without strong\nconvexity. A key ingredient in our proof is a Semi-Strongly Convex (SSC)\ninequality which is the first to be rigorously proved for a class of\nnon-strongly convex problems in both constrained and regularized settings.\nMoreover, the SSC inequality is independent of algorithms and may be applied to\nanalyze other stochastic gradient algorithms besides VRPSG and Prox-SVRG, which\nmay be of independent interest. To the best of our knowledge, this is the first\nwork that establishes the linear convergence rate for the variance-reduced\nstochastic gradient algorithms on solving both constrained and regularized\nproblems without strong convexity. \n\n"}
{"id": "1406.2010", "contents": "Title: On low complexity Acceleration Techniques for Randomized Optimization:\n  Supplementary Online Material Abstract: Recently it was shown by Nesterov (2011) that techniques form convex\noptimization can be used to successfully accelerate simple derivative-free\nrandomized optimization methods. The appeal of those schemes lies in their low\ncomplexity, which is only $\\Theta(n)$ per iteration---compared to $\\Theta(n^2)$\nfor algorithms storing second-order information or covariance matrices. From a\nhigh-level point of view, those accelerated schemes employ correlations between\nsuccessive iterates---a concept looking similar to the evolution path used in\nCovariance Matrix Adaptation Evolution Strategies (CMA-ES).\n  In this contribution, we (i) implement and empirically test a simple\naccelerated random search scheme (SARP). Our study is the first to provide\nnumerical evidence that SARP can effectively be implemented with adaptive step\nsize control and does not require access to gradient or advanced line search\noracles. We (ii) try to empirically verify the supposed analogy between the\nevolution path and SARP. We propose an algorithm CMA-EP that uses only the\nevolution path to bias the search. This algorithm can be generalized to a\nfamily of low memory schemes, with complexity $\\Theta(mn)$ per iteration,\nfollowing a recent approach by Loshchilov (2014). The study shows that the\nperformance of CMA-EP heavily depends on the spectra of the objective function\nand thus it cannot accelerate as consistently as SARP. \n\n"}
{"id": "1406.3190", "contents": "Title: Online Optimization for Large-Scale Max-Norm Regularization Abstract: Max-norm regularizer has been extensively studied in the last decade as it\npromotes an effective low-rank estimation for the underlying data. However,\nsuch max-norm regularized problems are typically formulated and solved in a\nbatch manner, which prevents it from processing big data due to possible memory\nbudget. In this paper, hence, we propose an online algorithm that is scalable\nto large-scale setting. Particularly, we consider the matrix decomposition\nproblem as an example, although a simple variant of the algorithm and analysis\ncan be adapted to other important problems such as matrix completion. The\ncrucial technique in our implementation is to reformulating the max-norm to an\nequivalent matrix factorization form, where the factors consist of a (possibly\novercomplete) basis component and a coefficients one. In this way, we may\nmaintain the basis component in the memory and optimize over it and the\ncoefficients for each sample alternatively. Since the memory footprint of the\nbasis component is independent of the sample size, our algorithm is appealing\nwhen manipulating a large collection of samples. We prove that the sequence of\nthe solutions (i.e., the basis component) produced by our algorithm converges\nto a stationary point of the expected loss function asymptotically. Numerical\nstudy demonstrates encouraging results for the efficacy and robustness of our\nalgorithm compared to the widely used nuclear norm solvers. \n\n"}
{"id": "1406.3332", "contents": "Title: Convolutional Kernel Networks Abstract: An important goal in visual recognition is to devise image representations\nthat are invariant to particular transformations. In this paper, we address\nthis goal with a new type of convolutional neural network (CNN) whose\ninvariance is encoded by a reproducing kernel. Unlike traditional approaches\nwhere neural networks are learned either to represent data or for solving a\nclassification task, our network learns to approximate the kernel feature map\non training data. Such an approach enjoys several benefits over classical ones.\nFirst, by teaching CNNs to be invariant, we obtain simple network architectures\nthat achieve a similar accuracy to more complex ones, while being easy to train\nand robust to overfitting. Second, we bridge a gap between the neural network\nliterature and kernels, which are natural tools to model invariance. We\nevaluate our methodology on visual recognition tasks where CNNs have proven to\nperform well, e.g., digit recognition with the MNIST dataset, and the more\nchallenging CIFAR-10 and STL-10 datasets, where our accuracy is competitive\nwith the state of the art. \n\n"}
{"id": "1406.3339", "contents": "Title: Algorithms for CVaR Optimization in MDPs Abstract: In many sequential decision-making problems we may want to manage risk by\nminimizing some measure of variability in costs in addition to minimizing a\nstandard criterion. Conditional value-at-risk (CVaR) is a relatively new risk\nmeasure that addresses some of the shortcomings of the well-known\nvariance-related risk measures, and because of its computational efficiencies\nhas gained popularity in finance and operations research. In this paper, we\nconsider the mean-CVaR optimization problem in MDPs. We first derive a formula\nfor computing the gradient of this risk-sensitive objective function. We then\ndevise policy gradient and actor-critic algorithms that each uses a specific\nmethod to estimate this gradient and updates the policy parameters in the\ndescent direction. We establish the convergence of our algorithms to locally\nrisk-sensitive optimal policies. Finally, we demonstrate the usefulness of our\nalgorithms in an optimal stopping problem. \n\n"}
{"id": "1406.6529", "contents": "Title: Strong Regularities in Growth and Decline of Popularity of Social Media\n  Services Abstract: We analyze general trends and pattern in time series that characterize the\ndynamics of collective attention to social media services and Web-based\nbusinesses. Our study is based on search frequency data available from Google\nTrends and considers 175 different services. For each service, we collect data\nfrom 45 different countries as well as global averages. This way, we obtain\nmore than 8,000 time series which we analyze using diffusion models from the\neconomic sciences. We find that these models accurately characterize the\nempirical data and our analysis reveals that collective attention to social\nmedia grows and subsides in a highly regular and predictable manner.\nRegularities persist across regions, cultures, and topics and thus hint at\ngeneral mechanisms that govern the adoption of Web-based services. We discuss\nseveral cases in detail to highlight interesting findings. Our methods are of\neconomic interest as they may inform investment decisions and can help\nassessing at what stage of the general life-cycle a Web service is at. \n\n"}
{"id": "1406.6897", "contents": "Title: Edge Label Inference in Generalized Stochastic Block Models: from\n  Spectral Theory to Impossibility Results Abstract: The classical setting of community detection consists of networks exhibiting\na clustered structure. To more accurately model real systems we consider a\nclass of networks (i) whose edges may carry labels and (ii) which may lack a\nclustered structure. Specifically we assume that nodes possess latent\nattributes drawn from a general compact space and edges between two nodes are\nrandomly generated and labeled according to some unknown distribution as a\nfunction of their latent attributes. Our goal is then to infer the edge label\ndistributions from a partially observed network. We propose a computationally\nefficient spectral algorithm and show it allows for asymptotically correct\ninference when the average node degree could be as low as logarithmic in the\ntotal number of nodes. Conversely, if the average node degree is below a\nspecific constant threshold, we show that no algorithm can achieve better\ninference than guessing without using the observations. As a byproduct of our\nanalysis, we show that our model provides a general procedure to construct\nrandom graph models with a spectrum asymptotic to a pre-specified eigenvalue\ndistribution such as a power-law distribution. \n\n"}
{"id": "1406.7246", "contents": "Title: Modeling rationality to control self-organization of crowds: An\n  environmental approach Abstract: In this paper we propose a classification of crowd models in built\nenvironments based on the assumed pedestrian ability to foresee the movements\nof other walkers. At the same time, we introduce a new family of macroscopic\nmodels, which make it possible to tune the degree of predictiveness (i.e.,\nrationality) of the individuals. By means of these models we describe both the\nnatural behavior of pedestrians, i.e., their expected behavior according to\ntheir real limited predictive ability, and a target behavior, i.e., a\nparticularly efficient behavior one would like them to assume (for, e.g.,\nlogistic or safety reasons). Then we tackle a challenging shape optimization\nproblem, which consists in controlling the environment in such a way that the\nnatural behavior is as close as possible to the target one, thereby inducing\npedestrians to behave more rationally than what they would naturally do. We\npresent numerical tests which elucidate the role of rational/predictive\nabilities and show some promising results about the shape optimization problem. \n\n"}
{"id": "1407.0202", "contents": "Title: SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly\n  Convex Composite Objectives Abstract: In this work we introduce a new optimisation method called SAGA in the spirit\nof SAG, SDCA, MISO and SVRG, a set of recently proposed incremental gradient\nalgorithms with fast linear convergence rates. SAGA improves on the theory\nbehind SAG and SVRG, with better theoretical convergence rates, and has support\nfor composite objectives where a proximal operator is used on the regulariser.\nUnlike SDCA, SAGA supports non-strongly convex problems directly, and is\nadaptive to any inherent strong convexity of the problem. We give experimental\nresults showing the effectiveness of our method. \n\n"}
{"id": "1407.0671", "contents": "Title: Optimal rates of convergence of matrices with applications Abstract: We present a systematic study on the linear convergence rates of the powers\nof (real or complex) matrices. We derive a characterization when the optimal\nconvergence rate is attained. This characterization is given in terms of\nsemi-simpleness of all eigenvalues having the second-largest modulus after 1.\nWe also provide applications of our general results to analyze the optimal\nconvergence rates for several relaxed alternating projection methods and the\ngeneralized Douglas-Rachford splitting methods for finding the projection on\nthe intersection of two subspaces. Numerical experiments confirm our\nconvergence analysis. \n\n"}
{"id": "1407.1255", "contents": "Title: Dynamic message-passing equations for models with unidirectional\n  dynamics Abstract: Understanding and quantifying the dynamics of disordered out-of-equilibrium\nmodels is an important problem in many branches of science. Using the dynamic\ncavity method on time trajectories, we construct a general procedure for\nderiving the dynamic message-passing equations for a large class of models with\nunidirectional dynamics, which includes the zero-temperature random field Ising\nmodel, the susceptible-infected-recovered model, and rumor spreading models. We\nshow that unidirectionality of the dynamics is the key ingredient that makes\nthe problem solvable. These equations are applicable to single instances of the\ncorresponding problems with arbitrary initial conditions, and are\nasymptotically exact for problems defined on locally tree-like graphs. When\napplied to real-world networks, they generically provide a good analytic\napproximation of the real dynamics. \n\n"}
{"id": "1407.1543", "contents": "Title: Dictionary Learning and Tensor Decomposition via the Sum-of-Squares\n  Method Abstract: We give a new approach to the dictionary learning (also known as \"sparse\ncoding\") problem of recovering an unknown $n\\times m$ matrix $A$ (for $m \\geq\nn$) from examples of the form \\[ y = Ax + e, \\] where $x$ is a random vector in\n$\\mathbb R^m$ with at most $\\tau m$ nonzero coordinates, and $e$ is a random\nnoise vector in $\\mathbb R^n$ with bounded magnitude. For the case $m=O(n)$,\nour algorithm recovers every column of $A$ within arbitrarily good constant\naccuracy in time $m^{O(\\log m/\\log(\\tau^{-1}))}$, in particular achieving\npolynomial time if $\\tau = m^{-\\delta}$ for any $\\delta>0$, and time $m^{O(\\log\nm)}$ if $\\tau$ is (a sufficiently small) constant. Prior algorithms with\ncomparable assumptions on the distribution required the vector $x$ to be much\nsparser---at most $\\sqrt{n}$ nonzero coordinates---and there were intrinsic\nbarriers preventing these algorithms from applying for denser $x$.\n  We achieve this by designing an algorithm for noisy tensor decomposition that\ncan recover, under quite general conditions, an approximate rank-one\ndecomposition of a tensor $T$, given access to a tensor $T'$ that is\n$\\tau$-close to $T$ in the spectral norm (when considered as a matrix). To our\nknowledge, this is the first algorithm for tensor decomposition that works in\nthe constant spectral-norm noise regime, where there is no guarantee that the\nlocal optima of $T$ and $T'$ have similar structures.\n  Our algorithm is based on a novel approach to using and analyzing the Sum of\nSquares semidefinite programming hierarchy (Parrilo 2000, Lasserre 2001), and\nit can be viewed as an indication of the utility of this very general and\npowerful tool for unsupervised learning problems. \n\n"}
{"id": "1407.1598", "contents": "Title: Low Complexity Regularization of Linear Inverse Problems Abstract: Inverse problems and regularization theory is a central theme in contemporary\nsignal processing, where the goal is to reconstruct an unknown signal from\npartial indirect, and possibly noisy, measurements of it. A now standard method\nfor recovering the unknown signal is to solve a convex optimization problem\nthat enforces some prior knowledge about its structure. This has proved\nefficient in many problems routinely encountered in imaging sciences,\nstatistics and machine learning. This chapter delivers a review of recent\nadvances in the field where the regularization prior promotes solutions\nconforming to some notion of simplicity/low-complexity. These priors encompass\nas popular examples sparsity and group sparsity (to capture the compressibility\nof natural signals and images), total variation and analysis sparsity (to\npromote piecewise regularity), and low-rank (as natural extension of sparsity\nto matrix-valued data). Our aim is to provide a unified treatment of all these\nregularizations under a single umbrella, namely the theory of partial\nsmoothness. This framework is very general and accommodates all low-complexity\nregularizers just mentioned, as well as many others. Partial smoothness turns\nout to be the canonical way to encode low-dimensional models that can be linear\nspaces or more general smooth manifolds. This review is intended to serve as a\none stop shop toward the understanding of the theoretical properties of the\nso-regularized solutions. It covers a large spectrum including: (i) recovery\nguarantees and stability to noise, both in terms of $\\ell^2$-stability and\nmodel (manifold) identification; (ii) sensitivity analysis to perturbations of\nthe parameters involved (in particular the observations), with applications to\nunbiased risk estimation ; (iii) convergence properties of the forward-backward\nproximal splitting scheme, that is particularly well suited to solve the\ncorresponding large-scale regularized optimization problem. \n\n"}
{"id": "1407.4095", "contents": "Title: Positive semidefinite rank Abstract: Let M be a p-by-q matrix with nonnegative entries. The positive semidefinite\nrank (psd rank) of M is the smallest integer k for which there exist positive\nsemidefinite matrices $A_i, B_j$ of size $k \\times k$ such that $M_{ij} =\n\\text{trace}(A_i B_j)$. The psd rank has many appealing geometric\ninterpretations, including semidefinite representations of polyhedra and\ninformation-theoretic applications. In this paper we develop and survey the\nmain mathematical properties of psd rank, including its geometry, relationships\nwith other rank notions, and computational and algorithmic aspects. \n\n"}
{"id": "1407.4211", "contents": "Title: A marginal sampler for $\\sigma$-Stable Poisson-Kingman mixture models Abstract: We investigate the class of $\\sigma$-stable Poisson-Kingman random\nprobability measures (RPMs) in the context of Bayesian nonparametric mixture\nmodeling. This is a large class of discrete RPMs which encompasses most of the\nthe popular discrete RPMs used in Bayesian nonparametrics, such as the\nDirichlet process, Pitman-Yor process, the normalized inverse Gaussian process\nand the normalized generalized Gamma process. We show how certain sampling\nproperties and marginal characterizations of $\\sigma$-stable Poisson-Kingman\nRPMs can be usefully exploited for devising a Markov chain Monte Carlo (MCMC)\nalgorithm for making inference in Bayesian nonparametric mixture modeling.\nSpecifically, we introduce a novel and efficient MCMC sampling scheme in an\naugmented space that has a fixed number of auxiliary variables per iteration.\nWe apply our sampling scheme for a density estimation and clustering tasks with\nunidimensional and multidimensional datasets, and we compare it against\ncompeting sampling schemes. \n\n"}
{"id": "1407.6490", "contents": "Title: Multi-hop Diffusion LMS for Energy-constrained Distributed Estimation Abstract: We propose a multi-hop diffusion strategy for a sensor network to perform\ndistributed least mean-squares (LMS) estimation under local and network-wide\nenergy constraints. At each iteration of the strategy, each node can combine\nintermediate parameter estimates from nodes other than its physical neighbors\nvia a multi-hop relay path. We propose a rule to select combination weights for\nthe multi-hop neighbors, which can balance between the transient and the\nsteady-state network mean-square deviations (MSDs). We study two classes of\nnetworks: simple networks with a unique transmission path from one node to\nanother, and arbitrary networks utilizing diffusion consultations over at most\ntwo hops. We propose a method to optimize each node's information neighborhood\nsubject to local energy budgets and a network-wide energy budget for each\ndiffusion iteration. This optimization requires the network topology, and the\nnoise and data variance profiles of each node, and is performed offline before\nthe diffusion process. In addition, we develop a fully distributed and adaptive\nalgorithm that approximately optimizes the information neighborhood of each\nnode with only local energy budget constraints in the case where diffusion\nconsultations are performed over at most a predefined number of hops. Numerical\nresults suggest that our proposed multi-hop diffusion strategy achieves the\nsame steady-state MSD as the existing one-hop adapt-then-combine diffusion\nalgorithm but with a lower energy budget. \n\n"}
{"id": "1407.7205", "contents": "Title: A Smoothing SQP Framework for a Class of Composite $L_q$ Minimization\n  over Polyhedron Abstract: The composite $L_q~(0<q<1)$ minimization problem over a general polyhedron\nhas received various applications in machine learning, wireless communications,\nimage restoration, signal reconstruction, etc. This paper aims to provide a\ntheoretical study on this problem. Firstly, we show that for any fixed $0<q<1$,\nfinding the global minimizer of the problem, even its unconstrained\ncounterpart, is strongly NP-hard. Secondly, we derive Karush-Kuhn-Tucker (KKT)\noptimality conditions for local minimizers of the problem. Thirdly, we propose\na smoothing sequential quadratic programming framework for solving this\nproblem. The framework requires a (approximate) solution of a convex quadratic\nprogram at each iteration. Finally, we analyze the worst-case iteration\ncomplexity of the framework for returning an $\\epsilon$-KKT point; i.e., a\nfeasible point that satisfies a perturbed version of the derived KKT optimality\nconditions. To the best of our knowledge, the proposed framework is the first\none with a worst-case iteration complexity guarantee for solving composite\n$L_q$ minimization over a general polyhedron. \n\n"}
{"id": "1408.0578", "contents": "Title: A Cyclic Coordinate Descent Algorithm for lq Regularization Abstract: In recent studies on sparse modeling, $l_q$ ($0<q<1$) regularization has\nreceived considerable attention due to its superiorities on sparsity-inducing\nand bias reduction over the $l_1$ regularization.In this paper, we propose a\ncyclic coordinate descent (CCD) algorithm for $l_q$ regularization. Our main\nresult states that the CCD algorithm converges globally to a stationary point\nas long as the stepsize is less than a positive constant. Furthermore, we\ndemonstrate that the CCD algorithm converges to a local minimizer under certain\nadditional conditions. Our numerical experiments demonstrate the efficiency of\nthe CCD algorithm. \n\n"}
{"id": "1408.1073", "contents": "Title: In-Network Linear Regression with Arbitrarily Split Data Matrices Abstract: In this paper, we address the problem of how a network of agents can\ncollaboratively fit a linear model when each agent only ever has an arbitrary\nsummand of the regression data. This problem generalizes previously studied\ndata-matrix-splitting scenarios, allowing for some agents to have more\nmeasurements of some features than of others and even have measurements that\nother agents have. We present a variable-centric framework for distributed\noptimization in a network, and use this framework to develop a proximal\nalgorithm, based on the Douglas-Rachford method, that solves the problem. \n\n"}
{"id": "1408.4213", "contents": "Title: Reflection methods for inverse problems with application to protein\n  conformation determination Abstract: The Douglas-Rachford reflection method is a general purpose algorithm useful\nfor solving the feasibility problem of finding a point in the intersection of\nfinitely many sets. In this chapter we demonstrate that applied to a specific\nproblem, the method can benefit from heuristics specific to said problem which\nexploit its special structure. In particular, we focus on the problem of\nprotein conformation determination formulated within the framework of matrix\ncompletion, as was considered in a recent paper of the present authors. \n\n"}
{"id": "1408.4685", "contents": "Title: Partial facial reduction: simplified, equivalent SDPs via approximations\n  of the PSD cone Abstract: We develop a practical semidefinite programming (SDP) facial reduction\nprocedure that utilizes computationally efficient approximations of the\npositive semidefinite cone. The proposed method simplifies SDPs with no\nstrictly feasible solution (a frequent output of parsers) by solving a sequence\nof easier optimization problems and could be a useful pre-processing technique\nfor SDP solvers. We demonstrate effectiveness of the method on SDPs arising in\npractice, and describe our publicly-available software implementation. We also\nshow how to find maximum rank matrices in our PSD cone approximations (which\nhelps us find maximal simplifications), and we give a post-processing procedure\nfor dual solution recovery that generally applies to facial-reduction-based\npre-processing techniques. Finally, we show how approximations can be chosen to\npreserve problem sparsity. \n\n"}
{"id": "1408.4901", "contents": "Title: A Study of Proxies for Shapley Allocations of Transport Costs Abstract: We propose and evaluate a number of solutions to the problem of calculating\nthe cost to serve each location in a single-vehicle transport setting. Such\ncost to serve analysis has application both strategically and operationally in\ntransportation. The problem is formally given by the traveling salesperson game\n(TSG), a cooperative total utility game in which agents correspond to locations\nin a traveling salesperson problem (TSP). The cost to serve a location is an\nallocated portion of the cost of an optimal tour. The Shapley value is one of\nthe most important normative division schemes in cooperative games, giving a\nprincipled and fair allocation both for the TSG and more generally. We consider\na number of direct and sampling-based procedures for calculating the Shapley\nvalue, and present the first proof that approximating the Shapley value of the\nTSG within a constant factor is NP-hard. Treating the Shapley value as an ideal\nbaseline allocation, we then develop six proxies for that value which are\nrelatively easy to compute. We perform an experimental evaluation using\nSynthetic Euclidean games as well as games derived from real-world tours\ncalculated for fast-moving consumer goods scenarios. Our experiments show that\nseveral computationally tractable allocation techniques correspond to good\nproxies for the Shapley value. \n\n"}
{"id": "1408.5352", "contents": "Title: Nonconvex Statistical Optimization: Minimax-Optimal Sparse PCA in\n  Polynomial Time Abstract: Sparse principal component analysis (PCA) involves nonconvex optimization for\nwhich the global solution is hard to obtain. To address this issue, one popular\napproach is convex relaxation. However, such an approach may produce suboptimal\nestimators due to the relaxation effect. To optimally estimate sparse principal\nsubspaces, we propose a two-stage computational framework named \"tighten after\nrelax\": Within the 'relax' stage, we approximately solve a convex relaxation of\nsparse PCA with early stopping to obtain a desired initial estimator; For the\n'tighten' stage, we propose a novel algorithm called sparse orthogonal\niteration pursuit (SOAP), which iteratively refines the initial estimator by\ndirectly solving the underlying nonconvex problem. A key concept of this\ntwo-stage framework is the basin of attraction. It represents a local region\nwithin which the `tighten' stage has desired computational and statistical\nguarantees. We prove that, the initial estimator obtained from the 'relax'\nstage falls into such a region, and hence SOAP geometrically converges to a\nprincipal subspace estimator which is minimax-optimal within a certain model\nclass. Unlike most existing sparse PCA estimators, our approach applies to the\nnon-spiked covariance models, and adapts to non-Gaussianity as well as\ndependent data settings. Moreover, through analyzing the computational\ncomplexity of the two stages, we illustrate an interesting phenomenon that\nlarger sample size can reduce the total iteration complexity. Our framework\nmotivates a general paradigm for solving many complex statistical problems\nwhich involve nonconvex optimization with provable guarantees. \n\n"}
{"id": "1409.0205", "contents": "Title: Integrated structure investigation in complex networks by label\n  propagation Abstract: The investigation of network structure has important significance to\nunderstand the functions of various complex networks. The communities with\nhierarchical and overlapping structures and the special nodes like hubs and\noutliers are all common structure features to the networks. Network structure\ninvestigation has attracted considerable research effort recently. However,\nexisting studies have only partially explored the structure features. In this\npaper, a label propagation based integrated network structure investigation\nalgorithm (LINSIA) is proposed. The main novelty here is that LINSIA can\nuncover hierarchical and overlapping communities, as well as hubs and outliers.\nMoreover, LINSIA can provide insight into the label propagation mechanism and\npropose a parameter-free solution that requires no prior knowledge. In\naddition, LINSIA can give out a soft-partitioning result and depict the degree\nof overlapping nodes belonging to each relevant community. The proposed\nalgorithm is validated on various synthetic and real-world networks.\nExperimental results demonstrate that the algorithm outperforms several\nstate-of-the-art methods. \n\n"}
{"id": "1409.3207", "contents": "Title: Phase Transitions in Spectral Community Detection Abstract: Consider a network consisting of two subnetworks (communities) connected by\nsome external edges. Given the network topology, the community detection\nproblem can be cast as a graph partitioning problem that aims to identify the\nexternal edges as the graph cut that separates these two subnetworks. In this\npaper, we consider a general model where two arbitrarily connected subnetworks\nare connected by random external edges. Using random matrix theory and\nconcentration inequalities, we show that when one performs community detection\nvia spectral clustering there exists an abrupt phase transition as a function\nof the random external edge connection probability. Specifically, the community\ndetection performance transitions from almost perfect detectability to low\ndetectability near some critical value of the random external edge connection\nprobability. We derive upper and lower bounds on the critical value and show\nthat the bounds are equal to each other when two subnetwork sizes are\nidentical. Using simulated and experimental data we show how these bounds can\nbe empirically estimated to validate the detection reliability of any\ndiscovered communities. \n\n"}
{"id": "1410.0641", "contents": "Title: An inertial forward-backward algorithm for the minimization of the sum\n  of two nonconvex functions Abstract: We propose a forward-backward proximal-type algorithm with inertial/memory\neffects for minimizing the sum of a nonsmooth function with a smooth one in the\nnonconvex setting. The sequence of iterates generated by the algorithm\nconverges to a critical point of the objective function provided an appropriate\nregularization of the objective satisfies the Kurdyka-\\L{}ojasiewicz\ninequality, which is for instance fulfilled for semi-algebraic functions. We\nillustrate the theoretical results by considering two numerical experiments:\nthe first one concerns the ability of recovering the local optimal solutions of\nnonconvex optimization problems, while the second one refers to the restoration\nof a noisy blurred image. \n\n"}
{"id": "1410.1221", "contents": "Title: Scalable and efficient algorithms for the propagation of uncertainty\n  from data through inference to prediction for large-scale problems, with\n  application to flow of the Antarctic ice sheet Abstract: The majority of research on efficient and scalable algorithms in\ncomputational science and engineering has focused on the forward problem: given\nparameter inputs, solve the governing equations to determine output quantities\nof interest. In contrast, here we consider the broader question: given a\n(large-scale) model containing uncertain parameters, (possibly) noisy\nobservational data, and a prediction quantity of interest, how do we construct\nefficient and scalable algorithms to (1) infer the model parameters from the\ndata (the deterministic inverse problem), (2) quantify the uncertainty in the\ninferred parameters (the Bayesian inference problem), and (3) propagate the\nresulting uncertain parameters through the model to issue predictions with\nquantified uncertainties (the forward uncertainty propagation problem)? We\npresent efficient and scalable algorithms for this end-to-end,\ndata-to-prediction process under the Gaussian approximation and in the context\nof modeling the flow of the Antarctic ice sheet and its effect on sea level.\nThe ice is modeled as a viscous, incompressible, creeping, shear-thinning\nfluid. The observational data come from InSAR satellite measurements of surface\nice flow velocity, and the uncertain parameter field to be inferred is the\nbasal sliding parameter. The prediction quantity of interest is the present-day\nice mass flux from the Antarctic continent to the ocean. We show that the work\nrequired for executing this data-to-prediction process is independent of the\nstate dimension, parameter dimension, data dimension, and number of processor\ncores. The key to achieving this dimension independence is to exploit the fact\nthat the observational data typically provide only sparse information on model\nparameters. This property can be exploited to construct a low rank\napproximation of the linearized parameter-to-observable map. \n\n"}
{"id": "1410.4962", "contents": "Title: Robust Fundamental Theorem for Continuous Processes Abstract: We study a continuous-time financial market with continuous price processes\nunder model uncertainty, modeled via a family $\\mathcal{P}$ of possible\nphysical measures. A robust notion ${\\rm NA}_{1}(\\mathcal{P})$ of no-arbitrage\nof the first kind is introduced; it postulates that a nonnegative, nonvanishing\nclaim cannot be superhedged for free by using simple trading strategies. Our\nfirst main result is a version of the fundamental theorem of asset pricing:\n${\\rm NA}_{1}(\\mathcal{P})$ holds if and only if every $P\\in\\mathcal{P}$ admits\na martingale measure which is equivalent up to a certain lifetime. The second\nmain result provides the existence of optimal superhedging strategies for\ngeneral contingent claims and a representation of the superhedging price in\nterms of martingale measures. \n\n"}
{"id": "1410.8584", "contents": "Title: Light on the Infinite Group Relaxation Abstract: This is a survey on the infinite group problem, an infinite-dimensional\nrelaxation of integer linear optimization problems introduced by Ralph Gomory\nand Ellis Johnson in their groundbreaking papers titled \"Some continuous\nfunctions related to corner polyhedra I, II\" [Math. Programming 3 (1972),\n23-85, 359-389]. The survey presents the infinite group problem in the modern\ncontext of cut generating functions. It focuses on the recent developments,\nsuch as algorithms for testing extremality and breakthroughs for the k-row\nproblem for general k >= 1 that extend previous work on the single-row and\ntwo-row problems. The survey also includes some previously unpublished results;\namong other things, it unveils piecewise linear extreme functions with more\nthan four different slopes. An interactive companion program, implemented in\nthe open-source computer algebra package Sage, provides an updated compendium\nof known extreme functions. \n\n"}
{"id": "1411.1124", "contents": "Title: Nearly Linear-Time Packing and Covering LP Solvers Abstract: Packing and covering linear programs (PC-LPs) form an important class of\nlinear programs (LPs) across computer science, operations research, and\noptimization. In 1993, Luby and Nisan constructed an iterative algorithm for\napproximately solving PC-LPs in nearly linear time, where the time complexity\nscales nearly linearly in $N$, the number of nonzero entries of the matrix, and\npolynomially in $\\varepsilon$, the (multiplicative) approximation error.\nUnfortunately, all existing nearly linear-time algorithms for solving PC-LPs\nrequire time at least proportional to $\\varepsilon^{-2}$.\n  In this paper, we break this longstanding barrier by designing a packing\nsolver that runs in time $\\tilde{O}(N \\varepsilon^{-1})$ and covering LP solver\nthat runs in time $\\tilde{O}(N \\varepsilon^{-1.5})$. Our packing solver can be\nextended to run in time $\\tilde{O}(N \\varepsilon^{-1})$ for a class of\nwell-behaved covering programs. In a follow-up work, Wang et al. showed that\nall covering LPs can be converted into well-behaved ones by a reduction that\nblows up the problem size only logarithmically.\n  At high level, these two algorithms can be described as linear couplings of\nseveral first-order descent steps. This is an application of our linear\ncoupling technique to problems that are not amenable to blackbox applications\nknown iterative algorithms in convex optimization. \n\n"}
{"id": "1411.1719", "contents": "Title: Second order analysis of control-affine problems with scalar state\n  constraint Abstract: In this article we establish new second order necessary and sufficient\noptimality conditions for a class of control-affine problems with a scalar\ncontrol and a scalar state constraint. These optimality conditions extend to\nthe constrained state framework the Goh transform, which is the classical tool\nfor obtaining an extension of the Legendre condition. \n\n"}
{"id": "1411.2242", "contents": "Title: Core-Periphery in Networks: An Axiomatic Approach Abstract: Recent evidence shows that in many societies worldwide the relative sizes of\nthe economic and social elites are continuously shrinking. Is this a natural\nsocial phenomenon? What are the forces that shape this process? We try to\naddress these questions by studying a Core-Periphery social structure composed\nof a social elite, namely, a relatively small but well-connected and highly\ninfluential group of powerful individuals, and the rest of society, the\nperiphery. Herein, we present a novel axiom-based model for the forces\ngoverning the mutual influences between the elite and the periphery. Assuming a\nsimple set of axioms, capturing the elite's dominance, robustness, compactness\nand density, we are able to draw strong conclusions about the elite-periphery\nstructure. In particular, we show that a balance of powers between elite and\nperiphery and an elite size that is sub-linear in the network size are\nuniversal properties of elites in social networks that satisfy our axioms. We\nnote that the latter is in controversy to the common belief that the elite size\nconverges to a linear fraction of society (most recently claimed to be 1%). We\naccompany these findings with a large scale empirical study on about 100\nreal-world networks, which supports our results. \n\n"}
{"id": "1411.6314", "contents": "Title: On the High-dimensional Power of Linear-time Kernel Two-Sample Testing\n  under Mean-difference Alternatives Abstract: Nonparametric two sample testing deals with the question of consistently\ndeciding if two distributions are different, given samples from both, without\nmaking any parametric assumptions about the form of the distributions. The\ncurrent literature is split into two kinds of tests - those which are\nconsistent without any assumptions about how the distributions may differ\n(\\textit{general} alternatives), and those which are designed to specifically\ntest easier alternatives, like a difference in means (\\textit{mean-shift}\nalternatives).\n  The main contribution of this paper is to explicitly characterize the power\nof a popular nonparametric two sample test, designed for general alternatives,\nunder a mean-shift alternative in the high-dimensional setting. Specifically,\nwe explicitly derive the power of the linear-time Maximum Mean Discrepancy\nstatistic using the Gaussian kernel, where the dimension and sample size can\nboth tend to infinity at any rate, and the two distributions differ in their\nmeans. As a corollary, we find that if the signal-to-noise ratio is held\nconstant, then the test's power goes to one if the number of samples increases\nfaster than the dimension increases. This is the first explicit power\nderivation for a general nonparametric test in the high-dimensional setting,\nand also the first analysis of how tests designed for general alternatives\nperform when faced with easier ones. \n\n"}
{"id": "1411.6317", "contents": "Title: Lower bounds on the size of semidefinite programming relaxations Abstract: We introduce a method for proving lower bounds on the efficacy of\nsemidefinite programming (SDP) relaxations for combinatorial problems. In\nparticular, we show that the cut, TSP, and stable set polytopes on $n$-vertex\ngraphs are not the linear image of the feasible region of any SDP (i.e., any\nspectrahedron) of dimension less than $2^{n^c}$, for some constant $c > 0$.\nThis result yields the first super-polynomial lower bounds on the semidefinite\nextension complexity of any explicit family of polytopes.\n  Our results follow from a general technique for proving lower bounds on the\npositive semidefinite rank of a matrix. To this end, we establish a close\nconnection between arbitrary SDPs and those arising from the sum-of-squares SDP\nhierarchy. For approximating maximum constraint satisfaction problems, we prove\nthat SDPs of polynomial-size are equivalent in power to those arising from\ndegree-$O(1)$ sum-of-squares relaxations. This result implies, for instance,\nthat no family of polynomial-size SDP relaxations can achieve better than a\n7/8-approximation for MAX-3-SAT. \n\n"}
{"id": "1411.7632", "contents": "Title: Semidefinite Programming Approach to Gaussian Sequential Rate-Distortion\n  Trade-offs Abstract: Sequential rate-distortion (SRD) theory provides a framework for studying the\nfundamental trade-off between data-rate and data-quality in real-time\ncommunication systems. In this paper, we consider the SRD problem for\nmulti-dimensional time-varying Gauss-Markov processes under mean-square\ndistortion criteria. We first revisit the sensor-estimator separation\nprinciple, which asserts that considered SRD problem is equivalent to a joint\nsensor and estimator design problem in which data-rate of the sensor output is\nminimized while the estimator's performance satisfies the distortion criteria.\nWe then show that the optimal joint design can be performed by semidefinite\nprogramming. A semidefinite representation of the corresponding SRD function is\nobtained. Implications of the obtained result in the context of zero-delay\nsource coding theory and applications to networked control theory are also\ndiscussed. \n\n"}
{"id": "1412.1269", "contents": "Title: The evolutionary game of pressure (or interference), resistance and\n  collaboration Abstract: In this paper we extend the framework of evolutionary inspection game put\nforward recently by the author and coworkers to a large class of conflict\ninteractions dealing with the pressure executed by the major player (or\nprincipal) on the large group of small players that can resist this pressure or\ncollaborate with the major player. We prove rigorous results on the convergence\nof various Markov decision models of interacting small agents (including\nevolutionary growth), namely pairwise, in groups and by coalition formation, to\na deterministic evolution on the distributions of the state spaces of small\nplayers paying main attention to situations with an infinite state-space of\nsmall players. We supply rather precise rates of convergence. The theoretical\nresults of the paper are applied to the analysis of the processes of\ninspection, corruption, cyber-security, counter-terrorism, banks and firms\nmerging, strategically enhanced preferential attachment and many other. \n\n"}
{"id": "1412.4973", "contents": "Title: Overlapping Communities in Social Networks Abstract: Complex networks can be typically broken down into groups or modules.\nDiscovering this \"community structure\" is an important step in studying the\nlarge-scale structure of networks. Many algorithms have been proposed for\ncommunity detection and benchmarks have been created to evaluate their\nperformance. Typically algorithms for community detection either partition the\ngraph (non-overlapping communities) or find node covers (overlapping\ncommunities).\n  In this paper, we propose a particularly simple semi-supervised learning\nalgorithm for finding out communities. In essence, given the community\ninformation of a small number of \"seed nodes\", the method uses random walks\nfrom the seed nodes to uncover the community information of the whole network.\nThe algorithm runs in time $O(k \\cdot m \\cdot \\log n)$, where $m$ is the number\nof edges; $n$ the number of links; and $k$ the number of communities in the\nnetwork. In sparse networks with $m = O(n)$ and a constant number of\ncommunities, this running time is almost linear in the size of the network.\nAnother important feature of our algorithm is that it can be used for either\nnon-overlapping or overlapping communities.\n  We test our algorithm using the LFR benchmark created by Lancichinetti,\nFortunato, and Radicchi specifically for the purpose of evaluating such\nalgorithms. Our algorithm can compete with the best of algorithms for both\nnon-overlapping and overlapping communities as found in the comprehensive study\nof Lancichinetti and Fortunato. \n\n"}
{"id": "1412.6833", "contents": "Title: How little data is enough? Phase-diagram analysis of\n  sparsity-regularized X-ray CT Abstract: We introduce phase-diagram analysis, a standard tool in compressed sensing,\nto the X-ray CT community as a systematic method for determining how few\nprojections suffice for accurate sparsity-regularized reconstruction. In\ncompressed sensing a phase diagram is a convenient way to study and express\ncertain theoretical relations between sparsity and sufficient sampling. We\nadapt phase-diagram analysis for empirical use in X-ray CT for which the same\ntheoretical results do not hold. We demonstrate in three case studies the\npotential of phase-diagram analysis for providing quantitative answers to\nquestions of undersampling: First we demonstrate that there are cases where\nX-ray CT empirically performs comparable with an optimal compressed sensing\nstrategy, namely taking measurements with Gaussian sensing matrices. Second, we\nshow that, in contrast to what might have been anticipated, taking randomized\nCT measurements does not lead to improved performance compared to standard\nstructured sampling patterns. Finally, we show preliminary results of how well\nphase-diagram analysis can predict the sufficient number of projections for\naccurately reconstructing a large-scale image of a given sparsity by means of\ntotal-variation regularization. \n\n"}
{"id": "1501.00676", "contents": "Title: A variational formula for risk-sensitive reward Abstract: We derive a variational formula for the optimal growth rate of reward in the\ninfinite horizon risk-sensitive control problem for discrete time Markov\ndecision processes with compact metric state and action spaces, extending a\nformula of Donsker and Varadhan for the Perron-Frobenius eigenvalue of a\npositive operator. This leads to a concave maximization formulation of the\nproblem of determining this optimal growth rate. \n\n"}
{"id": "1501.00961", "contents": "Title: Ergodic optimization of prevalent super-continuous functions Abstract: Given a dynamical system, we say that a performance function has property P\nif its time averages along orbits are maximized at a periodic orbit. It is\nconjectured by several authors that for sufficiently hyperbolic dynamical\nsystems, property P should be typical among sufficiently regular performance\nfunctions. In this paper we address this problem using a probabilistic notion\nof typicality that is suitable to infinite dimension: the concept of prevalence\nas introduced by Hunt, Sauer, and Yorke. For the one-sided shift on two\nsymbols, we prove that property P is prevalent in spaces of functions with a\nstrong modulus of regularity. Our proof uses Haar wavelets to approximate the\nergodic optimization problem by a finite-dimensional one, which can be\nconveniently restated as a maximum cycle mean problem on a de Bruijin graph. \n\n"}
{"id": "1501.06661", "contents": "Title: Deterministic compressed sensing matrices: Construction via Euler\n  Squares and applications Abstract: In Compressed Sensing the matrices that satisfy the Restricted Isometry\nProperty (RIP) play an important role. But to date, very few results for\ndesigning such matrices are available. For applications such as multiplier-less\ndata compression, binary sensing matrices are of interest. The present work\nconstructs deterministic and binary sensing matrices using Euler Squares. In\nparticular, given a positive integer $m$ different from $p, p^2$ for a prime\n$p$, we show that it is possible to construct a binary sensing matrix of size\n$m \\times c (m\\mu)^2$, where $\\mu$ is the coherence parameter of the matrix and\n$c \\in [1,2)$. The matrices that we construct have smaller density (that is,\npercentage of nonzero entries in the matrix is small) with no function\nevaluation in their construction, which support algorithms with low\ncomputational complexity. Through experimental work, we show that our binary\nsensing matrices can be used for such applications as content based image\nretrieval. Our simulation results demonstrate that the Euler Square based CS\nmatrices give better performance than their Gaussian counterparts. \n\n"}
{"id": "1502.02268", "contents": "Title: SDNA: Stochastic Dual Newton Ascent for Empirical Risk Minimization Abstract: We propose a new algorithm for minimizing regularized empirical loss:\nStochastic Dual Newton Ascent (SDNA). Our method is dual in nature: in each\niteration we update a random subset of the dual variables. However, unlike\nexisting methods such as stochastic dual coordinate ascent, SDNA is capable of\nutilizing all curvature information contained in the examples, which leads to\nstriking improvements in both theory and practice - sometimes by orders of\nmagnitude. In the special case when an L2-regularizer is used in the primal,\nthe dual problem is a concave quadratic maximization problem plus a separable\nterm. In this regime, SDNA in each step solves a proximal subproblem involving\na random principal submatrix of the Hessian of the quadratic function; whence\nthe name of the method. If, in addition, the loss functions are quadratic, our\nmethod can be interpreted as a novel variant of the recently introduced\nIterative Hessian Sketch. \n\n"}
{"id": "1502.02362", "contents": "Title: Counterfactual Risk Minimization: Learning from Logged Bandit Feedback Abstract: We develop a learning principle and an efficient algorithm for batch learning\nfrom logged bandit feedback. This learning setting is ubiquitous in online\nsystems (e.g., ad placement, web search, recommendation), where an algorithm\nmakes a prediction (e.g., ad ranking) for a given input (e.g., query) and\nobserves bandit feedback (e.g., user clicks on presented ads). We first address\nthe counterfactual nature of the learning problem through propensity scoring.\nNext, we prove generalization error bounds that account for the variance of the\npropensity-weighted empirical risk estimator. These constructive bounds give\nrise to the Counterfactual Risk Minimization (CRM) principle. We show how CRM\ncan be used to derive a new learning method -- called Policy Optimizer for\nExponential Models (POEM) -- for learning stochastic linear rules for\nstructured output prediction. We present a decomposition of the POEM objective\nthat enables efficient stochastic gradient optimization. POEM is evaluated on\nseveral multi-label classification problems showing substantially improved\nrobustness and generalization performance compared to the state-of-the-art. \n\n"}
{"id": "1502.03163", "contents": "Title: Gaussian Process Models for HRTF based Sound-Source Localization and\n  Active-Learning Abstract: From a machine learning perspective, the human ability localize sounds can be\nmodeled as a non-parametric and non-linear regression problem between binaural\nspectral features of sound received at the ears (input) and their sound-source\ndirections (output). The input features can be summarized in terms of the\nindividual's head-related transfer functions (HRTFs) which measure the spectral\nresponse between the listener's eardrum and an external point in $3$D. Based on\nthese viewpoints, two related problems are considered: how can one achieve an\noptimal sampling of measurements for training sound-source localization (SSL)\nmodels, and how can SSL models be used to infer the subject's HRTFs in\nlistening tests. First, we develop a class of binaural SSL models based on\nGaussian process regression and solve a \\emph{forward selection} problem that\nfinds a subset of input-output samples that best generalize to all SSL\ndirections. Second, we use an \\emph{active-learning} approach that updates an\nonline SSL model for inferring the subject's SSL errors via headphones and a\ngraphical user interface. Experiments show that only a small fraction of HRTFs\nare required for $5^{\\circ}$ localization accuracy and that the learned HRTFs\nare localized closer to their intended directions than non-individualized\nHRTFs. \n\n"}
{"id": "1502.03365", "contents": "Title: Reconstruction in the Labeled Stochastic Block Model Abstract: The labeled stochastic block model is a random graph model representing\nnetworks with community structure and interactions of multiple types. In its\nsimplest form, it consists of two communities of approximately equal size, and\nthe edges are drawn and labeled at random with probability depending on whether\ntheir two endpoints belong to the same community or not.\n  It has been conjectured in \\cite{Heimlicher12} that correlated reconstruction\n(i.e.\\ identification of a partition correlated with the true partition into\nthe underlying communities) would be feasible if and only if a model parameter\nexceeds a threshold. We prove one half of this conjecture, i.e., reconstruction\nis impossible when below the threshold. In the positive direction, we introduce\na weighted graph to exploit the label information. With a suitable choice of\nweight function, we show that when above the threshold by a specific constant,\nreconstruction is achieved by (1) minimum bisection, (2) a semidefinite\nrelaxation of minimum bisection, and (3) a spectral method combined with\nremoval of edges incident to vertices of high degree. Furthermore, we show that\nhypothesis testing between the labeled stochastic block model and the labeled\nErd\\H{o}s-R\\'enyi random graph model exhibits a phase transition at the\nconjectured reconstruction threshold. \n\n"}
{"id": "1502.04726", "contents": "Title: ICR: Iterative Convex Refinement for Sparse Signal Recovery Using Spike\n  and Slab Priors Abstract: In this letter, we address sparse signal recovery using spike and slab\npriors. In particular, we focus on a Bayesian framework where sparsity is\nenforced on reconstruction coefficients via probabilistic priors. The\noptimization resulting from spike and slab prior maximization is known to be a\nhard non-convex problem, and existing solutions involve simplifying assumptions\nand/or relaxations. We propose an approach called Iterative Convex Refinement\n(ICR) that aims to solve the aforementioned optimization problem directly\nallowing for greater generality in the sparse structure. Essentially, ICR\nsolves a sequence of convex optimization problems such that sequence of\nsolutions converges to a sub-optimal solution of the original hard optimization\nproblem. We propose two versions of our algorithm: a.) an unconstrained\nversion, and b.) with a non-negativity constraint on sparse coefficients, which\nmay be required in some real-world problems. Experimental validation is\nperformed on both synthetic data and for a real-world image recovery problem,\nwhich illustrates merits of ICR over state of the art alternatives. \n\n"}
{"id": "1502.05156", "contents": "Title: Assessing the effectiveness of real-world network simplification Abstract: Many real-world networks are large, complex and thus hard to understand,\nanalyze or visualize. The data about networks is not always complete, their\nstructure may be hidden or they change quickly over time. Therefore,\nunderstanding how incomplete system differs from complete one is crucial. In\nthis paper, we study the changes in networks under simplification (i.e.,\nreduction in size). We simplify 30 real-world networks with six simplification\nmethods and analyze the similarity between original and simplified networks\nbased on preservation of several properties, for example degree distribution,\nclustering coefficient, betweenness centrality, density and degree mixing. We\npropose an approach for assessing the effectiveness of simplification process\nto define the most appropriate size of simplified networks and to determine the\nmethod, which preserves the most properties of original networks. The results\nreveal the type and size of original networks do not influence the changes of\nnetworks under simplification process, while the size of simplified networks\ndoes. Moreover, we investigate the performance of simplification methods when\nthe size of simplified networks is 10% of the original networks. The findings\nshow that sampling methods outperform merging ones, particularly random node\nselection based on degree and breadth-first sampling perform the best. \n\n"}
{"id": "1502.05480", "contents": "Title: Aggregated Demand Response Modelling for Future Grid Scenarios Abstract: With the increased penetration of intermittent renewable energy sources\n(RESs) in future grids (FGs), balancing between supply and demand will become\nmore dependent on demand response (DR) and energy storage. Thus, FG feasibility\nstudies will need to consider DR for modelling nett future demand. Against this\nbackdrop, this paper proposes a demand model which integrates the aggregated\neffect of DR in a simplified representation of the effect of market/dispatch\nprocesses aiming at minimising the overall cost of supplying electrical energy.\nThe conventional demand model in the optimisation formulation is augmented by\nincluding the aggregated effect of numerous users equipped with rooftop\nphotovoltaic (PV)-storage systems. The proposed model is suited for system\nstudies at higher voltage levels in which users are assumed to be price\nanticipators. As a case study, the effect of the demand model is studied on the\nload profile, balancing and loadability of the Australian National Electricity\nMarket in 2020 with the increased penetration of RESs. The results are compared\nwith the demand model in which users are assumed to be price takers. \n\n"}
{"id": "1502.05629", "contents": "Title: Strong Nash equilibria and mixed strategies Abstract: In this paper we consider strong Nash equilibria, in mixed strategies, for\nfinite games. Any strong Nash equilibrium outcome is Pareto efficient for each\ncoalition. First, we analyze the two--player setting. Our main result, in its\nsimplest form, states that if a game has a strong Nash equilibrium with full\nsupport (that is, both players randomize among all pure strategies), then the\ngame is strictly competitive. In order to get our result we use the\nindifference principle fulfilled by any Nash equilibrium, and the classical KKT\nconditions (in the vector setting), that are necessary conditions for Pareto\nefficiency. Our characterization enables us to design a\nstrong-Nash-equilibrium-finding algorithm with complexity in\nSmoothed-$\\mathcal{P}$. So, this problem---that Conitzer and Sandholm\n[Conitzer, V., Sandholm, T., 2008. New complexity results about Nash\nequilibria. Games Econ. Behav. 63, 621--641] proved to be computationally hard\nin the worst case---is generically easy. Hence, although the worst case\ncomplexity of finding a strong Nash equilibrium is harder than that of finding\na Nash equilibrium, once small perturbations are applied, finding a strong Nash\nis easier than finding a Nash equilibrium. Next we switch to the setting with\nmore than two players. We demonstrate that a strong Nash equilibrium can exist\nin which an outcome that is strictly Pareto dominated by a Nash equilibrium\noccurs with positive probability. Finally, we prove that games that have a\nstrong Nash equilibrium where at least one player puts positive probability on\nat least two pure strategies are extremely rare: they are of zero measure. \n\n"}
{"id": "1502.05744", "contents": "Title: Scale-Free Algorithms for Online Linear Optimization Abstract: We design algorithms for online linear optimization that have optimal regret\nand at the same time do not need to know any upper or lower bounds on the norm\nof the loss vectors. We achieve adaptiveness to norms of loss vectors by scale\ninvariance, i.e., our algorithms make exactly the same decisions if the\nsequence of loss vectors is multiplied by any positive constant. Our algorithms\nwork for any decision set, bounded or unbounded. For unbounded decisions sets,\nthese are the first truly adaptive algorithms for online linear optimization. \n\n"}
{"id": "1502.07162", "contents": "Title: Measuring Online Social Bubbles Abstract: Social media have quickly become a prevalent channel to access information,\nspread ideas, and influence opinions. However, it has been suggested that\nsocial and algorithmic filtering may cause exposure to less diverse points of\nview, and even foster polarization and misinformation. Here we explore and\nvalidate this hypothesis quantitatively for the first time, at the collective\nand individual levels, by mining three massive datasets of web traffic, search\nlogs, and Twitter posts. Our analysis shows that collectively, people access\ninformation from a significantly narrower spectrum of sources through social\nmedia and email, compared to search. The significance of this finding for\nindividual exposure is revealed by investigating the relationship between the\ndiversity of information sources experienced by users at the collective and\nindividual level. There is a strong correlation between collective and\nindividual diversity, supporting the notion that when we use social media we\nfind ourselves inside \"social bubbles\". Our results could lead to a deeper\nunderstanding of how technology biases our exposure to new information. \n\n"}
{"id": "1502.08009", "contents": "Title: Second-order Quantile Methods for Experts and Combinatorial Games Abstract: We aim to design strategies for sequential decision making that adjust to the\ndifficulty of the learning problem. We study this question both in the setting\nof prediction with expert advice, and for more general combinatorial decision\ntasks. We are not satisfied with just guaranteeing minimax regret rates, but we\nwant our algorithms to perform significantly better on easy data. Two popular\nways to formalize such adaptivity are second-order regret bounds and quantile\nbounds. The underlying notions of 'easy data', which may be paraphrased as \"the\nlearning problem has small variance\" and \"multiple decisions are useful\", are\nsynergetic. But even though there are sophisticated algorithms that exploit one\nof the two, no existing algorithm is able to adapt to both.\n  In this paper we outline a new method for obtaining such adaptive algorithms,\nbased on a potential function that aggregates a range of learning rates (which\nare essential tuning parameters). By choosing the right prior we construct\nefficient algorithms and show that they reap both benefits by proving the first\nbounds that are both second-order and incorporate quantiles. \n\n"}
{"id": "1503.01457", "contents": "Title: Time Averaged Consensus in a Direct Coupled Distributed Coherent Quantum\n  Observer Abstract: This paper considers the problem of constructing a distributed direct\ncoupling quantum observer for a closed linear quantum system. The proposed\ndistributed observer consists of a network of quantum harmonic oscillators and\nit is shown that the distributed observer converges to a consensus in a time\naveraged sense in which each component of the observer estimates the specified\noutput of the quantum plant. An example and simulations are included to\nillustrate the properties of the distributed observer. \n\n"}
{"id": "1503.02718", "contents": "Title: Attitude Estimation and Control Using Linear-Like Complementary Filters:\n  Theory and Experiment Abstract: This paper proposes new algorithms for attitude estimation and control based\non fused inertial vector measurements using linear complementary filters\nprinciple. First, n-order direct and passive complementary filters combined\nwith TRIAD algorithm are proposed to give attitude estimation solutions. These\nsolutions which are efficient with respect to noise include the gyro bias\nestimation. Thereafter, the same principle of data fusion is used to address\nthe problem of attitude tracking based on inertial vector measurements. Thus,\ninstead of using noisy raw measurements in the control law a new solution of\ncontrol that includes a linear-like complementary filter to deal with the noise\nis proposed. The stability analysis of the tracking error dynamics based on\nLaSalle's invariance theorem proved that almost all trajectories converge\nasymptotically to the desired equilibrium. Experimental results, obtained with\nDIY Quad equipped with the APM2.6 auto-pilot, show the effectiveness and the\nperformance of the proposed solutions. \n\n"}
{"id": "1503.04360", "contents": "Title: Quadratic Multi-Dimensional Signaling Games and Affine Equilibria Abstract: This paper studies the decentralized quadratic cheap talk and signaling game\nproblems when an encoder and a decoder, viewed as two decision makers, have\nmisaligned objective functions. The main contributions of this study are the\nextension of Crawford and Sobel's cheap talk formulation to multi-dimensional\nsources and to noisy channel setups. We consider both (simultaneous) Nash\nequilibria and (sequential) Stackelberg equilibria. We show that for arbitrary\nscalar sources, in the presence of misalignment, the quantized nature of all\nequilibrium policies holds for Nash equilibria in the sense that all Nash\nequilibria are equivalent to those achieved by quantized encoder policies. On\nthe other hand, all Stackelberg equilibria policies are fully informative. For\nmulti-dimensional setups, unlike the scalar case, Nash equilibrium policies may\nbe of non-quantized nature, and even linear. In the noisy setup, a Gaussian\nsource is to be transmitted over an additive Gaussian channel. The goals of the\nencoder and the decoder are misaligned by a bias term and encoder's cost also\nincludes a penalty term on signal power. Conditions for the existence of affine\nNash equilibria as well as general informative equilibria are presented. For\nthe noisy setup, the only Stackelberg equilibrium is the linear equilibrium\nwhen the variables are scalar. Our findings provide further conditions on when\naffine policies may be optimal in decentralized multi-criteria control problems\nand lead to conditions for the presence of active information transmission in\nstrategic environments. \n\n"}
{"id": "1503.06014", "contents": "Title: Optimal estimation with missing observations via balanced time-symmetric\n  stochastic models Abstract: We consider data fusion for the purpose of smoothing and interpolation based\non observation records with missing data. Stochastic processes are generated by\nlinear stochastic models. The paper begins by drawing a connection between time\nreversal in stochastic systems and all-pass extensions. A particular\nnormalization (choice of basis) between the two time-directions allows the two\nto share the same orthonormalized state process and simplifies the mathematics\nof data fusion. In this framework we derive symmetric and balanced\nMayne-Fraser-like formulas that apply simultaneously to smoothing and\ninterpolation. \n\n"}
{"id": "1503.06133", "contents": "Title: An Optimal Control Approach for the Data Harvesting Problem Abstract: We propose a new method for trajectory planning to solve the data harvesting\nproblem. In a two-dimensional mission space, $N$ mobile agents are tasked with\nthe collection of data generated at $M$ stationary sources and delivery to a\nbase aiming at minimizing expected delays. An optimal control formulation of\nthis problem provides some initial insights regarding its solution, but it is\ncomputationally intractable, especially in the case where the data generating\nprocesses are stochastic. We propose an agent trajectory parameterization in\nterms of general function families which can be subsequently optimized on line\nthrough the use of Infinitesimal Perturbation Analysis (IPA). Explicit results\nare provided for the case of elliptical and Fourier series trajectories and\nsome properties of the solution are identified, including robustness with\nrespect to the data generation processes and scalability in the size of an\nevent set characterizing the underlying hybrid dynamic system. \n\n"}
{"id": "1503.06592", "contents": "Title: Regularity of isoperimetric sets in $\\mathbb R^2$ with density Abstract: We consider the isoperimetric problem in $\\mathbb R^n$ with density for the\nplanar case $n=2$. We show that, if the density is ${\\rm C}^{0,\\alpha}$, then\nthe boundary of any isoperimetric is of class ${\\rm C}^{1,\\frac\n\\alpha{3-2\\alpha}}$. This improves the previously known regularity. \n\n"}
{"id": "1503.07561", "contents": "Title: Primal robustness and semidefinite cones Abstract: This paper reformulates and streamlines the core tools of robust stability\nand performance for LTI systems using now-standard methods in convex\noptimization. In particular, robustness analysis can be formulated directly as\na primal convex (semidefinite program or SDP) optimization problem using sets\nof gramians whose closure is a semidefinite cone. This allows various\nconstraints such as structured uncertainty to be included directly, and\nworst-case disturbances and perturbations constructed directly from the primal\nvariables. Well known results such as the KYP lemma and various scaled small\ngain tests can also be obtained directly through standard SDP duality. To\nreaders familiar with robustness and SDPs, the framework should appear obvious,\nif only in retrospect. But this is also part of its appeal and should enhance\npedagogy, and we hope suggest new research. There is a key lemma proving\nclosure of a grammian that is also obvious but our current proof appears\nunnecessarily cumbersome, and a final aim of this paper is to enlist the help\nof experts in robust control and convex optimization in finding simpler\nalternatives. \n\n"}
{"id": "1503.07760", "contents": "Title: Necessary stochastic maximum principle for dissipative systems on\n  infinite time horizon Abstract: We develop a necessary stochastic maximum principle for a finite-dimensional\nstochastic control problem in infinite horizon under a polynomial growth and\njoint monotonicity assumption on the coefficients. The second assumption\ngeneralizes the usual one in the sense that it is formulated as a joint\ncondition for the drift and the diffusion term. The main difficulties concern\nthe construction of the first and second order adjoint processes by solving\nbackward equations on an unbounded time interval. The first adjoint process is\ncharacterized as a solution to a backward SDE, which is well-posed thanks to a\nduality argument. The second one can be defined via another duality relation\nwritten in terms of the Hamiltonian of the system and linearized state\nequation. Some known models verifying the joint monotonicity assumption are\ndiscussed as well. \n\n"}
{"id": "1503.08316", "contents": "Title: A Variance Reduced Stochastic Newton Method Abstract: Quasi-Newton methods are widely used in practise for convex loss minimization\nproblems. These methods exhibit good empirical performance on a wide variety of\ntasks and enjoy super-linear convergence to the optimal solution. For\nlarge-scale learning problems, stochastic Quasi-Newton methods have been\nrecently proposed. However, these typically only achieve sub-linear convergence\nrates and have not been shown to consistently perform well in practice since\nnoisy Hessian approximations can exacerbate the effect of high-variance\nstochastic gradient estimates. In this work we propose Vite, a novel stochastic\nQuasi-Newton algorithm that uses an existing first-order technique to reduce\nthis variance. Without exploiting the specific form of the approximate Hessian,\nwe show that Vite reaches the optimum at a geometric rate with a constant\nstep-size when dealing with smooth strongly convex functions. Empirically, we\ndemonstrate improvements over existing stochastic Quasi-Newton and variance\nreduced stochastic gradient methods. \n\n"}
{"id": "1503.08601", "contents": "Title: Finding a low-rank basis in a matrix subspace Abstract: For a given matrix subspace, how can we find a basis that consists of\nlow-rank matrices? This is a generalization of the sparse vector problem. It\nturns out that when the subspace is spanned by rank-1 matrices, the matrices\ncan be obtained by the tensor CP decomposition. For the higher rank case, the\nsituation is not as straightforward. In this work we present an algorithm based\non a greedy process applicable to higher rank problems. Our algorithm first\nestimates the minimum rank by applying soft singular value thresholding to a\nnuclear norm relaxation, and then computes a matrix with that rank using the\nmethod of alternating projections. We provide local convergence results, and\ncompare our algorithm with several alternative approaches. Applications include\ndata compression beyond the classical truncated SVD, computing accurate\neigenvectors of a near-multiple eigenvalue, image separation and graph\nLaplacian eigenproblems. \n\n"}
{"id": "1503.08855", "contents": "Title: Decentralized learning for wireless communications and networking Abstract: This chapter deals with decentralized learning algorithms for in-network\nprocessing of graph-valued data. A generic learning problem is formulated and\nrecast into a separable form, which is iteratively minimized using the\nalternating-direction method of multipliers (ADMM) so as to gain the desired\ndegree of parallelization. Without exchanging elements from the distributed\ntraining sets and keeping inter-node communications at affordable levels, the\nlocal (per-node) learners consent to the desired quantity inferred globally,\nmeaning the one obtained if the entire training data set were centrally\navailable. Impact of the decentralized learning framework to contemporary\nwireless communications and networking tasks is illustrated through case\nstudies including target tracking using wireless sensor networks, unveiling\nInternet traffic anomalies, power system state estimation, as well as spectrum\ncartography for wireless cognitive radio networks. \n\n"}
{"id": "1504.00063", "contents": "Title: A fractional space-time optimal control problem: analysis and\n  discretization\\ Abstract: We study a linear-quadratic optimal control problem involving a parabolic\nequation with fractional diffusion and Caputo fractional time derivative of\norders $s \\in (0,1)$ and $\\gamma \\in (0,1]$, respectively. The spatial\nfractional diffusion is realized as the Dirichlet-to-Neumann map for a\nnonuniformly elliptic operator. Thus, we consider an equivalent formulation\nwith a quasi-stationary elliptic problem with a dynamic boundary condition as\nstate equation. The rapid decay of the solution to this problem suggests a\ntruncation that is suitable for numerical approximation. We consider a\nfully-discrete scheme: piecewise constant functions for the control and, for\nthe state, first-degree tensor product finite elements in space and a finite\ndifference discretization in time. We show convergence of this scheme and, for\n$s \\in (0,1)$ and $\\gamma = 1$, we derive a priori error estimates. \n\n"}
{"id": "1504.00874", "contents": "Title: Steering state statistics with output feedback Abstract: Consider a linear stochastic system whose initial state is a random vector\nwith a specified Gaussian distribution. Such a distribution may represent a\ncollection of particles abiding by the specified system dynamics. In recent\npublications, we have shown that, provided the system is controllable, it is\nalways possible to steer the state covariance to any specified terminal\nGaussian distribution using state feedback. The purpose of the present work is\nto show that, in the case where only partial state observation is available, a\nnecessary and sufficient condition for being able to steer the system to a\nspecified terminal Gaussian distribution for the state vector is that the\nterminal state covariance be greater (in the positive-definite sense) than the\nerror covariance of a corresponding Kalman filter. \n\n"}
{"id": "1504.01169", "contents": "Title: Efficient Dictionary Learning via Very Sparse Random Projections Abstract: Performing signal processing tasks on compressive measurements of data has\nreceived great attention in recent years. In this paper, we extend previous\nwork on compressive dictionary learning by showing that more general random\nprojections may be used, including sparse ones. More precisely, we examine\ncompressive K-means clustering as a special case of compressive dictionary\nlearning and give theoretical guarantees for its performance for a very general\nclass of random projections. We then propose a memory and computation efficient\ndictionary learning algorithm, specifically designed for analyzing large\nvolumes of high-dimensional data, which learns the dictionary from very sparse\nrandom projections. Experimental results demonstrate that our approach allows\nfor reduction of computational complexity and memory/data access, with\ncontrollable loss in accuracy. \n\n"}
{"id": "1504.01438", "contents": "Title: Convergence Time of Quantized Metropolis Consensus Over Time-Varying\n  Networks Abstract: We consider the quantized consensus problem on undirected time-varying\nconnected graphs with n nodes, and devise a protocol with fast convergence time\nto the set of consensus points. Specifically, we show that when the edges of\neach network in a sequence of connected time-varying networks are activated\nbased on Poisson processes with Metropolis rates, the expected convergence time\nto the set of consensus points is at most O(n^2 log^2 n), where each node\nperforms a constant number of updates per unit time. \n\n"}
{"id": "1504.05854", "contents": "Title: On-the-fly Approximation of Multivariate Total Variation Minimization Abstract: In the context of change-point detection, addressed by Total Variation\nminimization strategies, an efficient on-the-fly algorithm has been designed\nleading to exact solutions for univariate data. In this contribution, an\nextension of such an on-the-fly strategy to multivariate data is investigated.\nThe proposed algorithm relies on the local validation of the Karush-Kuhn-Tucker\nconditions on the dual problem. Showing that the non-local nature of the\nmultivariate setting precludes to obtain an exact on-the-fly solution, we\ndevise an on-the-fly algorithm delivering an approximate solution, whose\nquality is controlled by a practitioner-tunable parameter, acting as a\ntrade-off between quality and computational cost. Performance assessment shows\nthat high quality solutions are obtained on-the-fly while benefiting of\ncomputational costs several orders of magnitude lower than standard iterative\nprocedures. The proposed algorithm thus provides practitioners with an\nefficient multivariate change-point detection on-the-fly procedure. \n\n"}
{"id": "1504.06068", "contents": "Title: Analysis on Non-negative Factorizations and Applications Abstract: In this work we perform some mathematical analysis on non-negative matrix\nfactorizations (NMF) and apply NMF to some imaging and inverse problems. We\nwill propose a sparse low-rank approximation of big positive data and images in\nterms of tensor products of positive vectors, and investigate its effectiveness\nin terms of the number of tensor products to be used in the approximation. A\nnew concept of multi-level analysis (MLA) framework is also suggested to\nextract major components in the matrix representing structures of different\nresolutions, but still preserving the positivity of the basis and sparsity of\nthe approximation. We will also propose a semi-smooth Newton method based on\nprimal-dual active sets for the non-negative factorization. Numerical results\nare given to demonstrate the effectiveness of the proposed method to capture\nfeatures in images and structures of inverse problems under no a-priori\nassumption on the data structure, as well as to provide a sparse low-rank\nrepresentation of the data. \n\n"}
{"id": "1504.07101", "contents": "Title: Homophily and Triadic Closure in Evolving Social Networks Abstract: We present a new network model accounting for multidimensional assortativity.\nEach node is characterized by a number of features and the probability of a\nlink between two nodes depends on common features. We do not fix a priori the\ntotal number of possible features. The bipartite network of the nodes and the\nfeatures evolves according to a stochastic dynamics that depends on three\nparameters that respectively regulate the preferential attachment in the\ntransmission of the features to the nodes, the number of new features per node,\nand the power-law behavior of the total number of observed features. Our model\nalso takes into account a mechanism of triadic closure. We provide theoretical\nresults and statistical estimators for the parameters of the model. We validate\nour approach by means of simulations and an empirical analysis of a network of\nscientific collaborations. \n\n"}
{"id": "1505.00044", "contents": "Title: Incorporating Contact Network Structure in Cluster Randomized Trials Abstract: Whenever possible, the efficacy of a new treatment, such as a drug or\nbehavioral intervention, is investigated by randomly assigning some individuals\nto a treatment condition and others to a control condition, and comparing the\noutcomes between the two groups. Often, when the treatment aims to slow an\ninfectious disease, groups or clusters of individuals are assigned en masse to\neach treatment arm. The structure of interactions within and between clusters\ncan reduce the power of the trial, i.e. the probability of correctly detecting\na real treatment effect. We investigate the relationships among power,\nwithin-cluster structure, between-cluster mixing, and infectivity by simulating\nan infectious process on a collection of clusters. We demonstrate that current\npower calculations may be conservative for low levels of between-cluster\nmixing, but failing to account for moderate or high amounts can result in\nseverely underpowered studies. Power also depends on within-cluster network\nstructure for certain kinds of infectious spreading. Infections that spread\nopportunistically through very highly connected individuals have unpredictable\ninfectious breakouts, which makes it harder to distinguish between random\nvariation and real treatment effects. Our approach can be used before\nconducting a trial to assess power using network information if it is\navailable, and we demonstrate how empirical data can inform the extent of\nbetween-cluster mixing. \n\n"}
{"id": "1505.02434", "contents": "Title: Spike and Slab Gaussian Process Latent Variable Models Abstract: The Gaussian process latent variable model (GP-LVM) is a popular approach to\nnon-linear probabilistic dimensionality reduction. One design choice for the\nmodel is the number of latent variables. We present a spike and slab prior for\nthe GP-LVM and propose an efficient variational inference procedure that gives\na lower bound of the log marginal likelihood. The new model provides a more\nprincipled approach for selecting latent dimensions than the standard way of\nthresholding the length-scale parameters. The effectiveness of our approach is\ndemonstrated through experiments on real and simulated data. Further, we extend\nmulti-view Gaussian processes that rely on sharing latent dimensions (known as\nmanifold relevance determination) with spike and slab priors. This allows a\nmore principled approach for selecting a subset of the latent space for each\nview of data. The extended model outperforms the previous state-of-the-art when\napplied to a cross-modal multimedia retrieval task. \n\n"}
{"id": "1505.02475", "contents": "Title: Foundational principles for large scale inference: Illustrations through\n  correlation mining Abstract: When can reliable inference be drawn in the \"Big Data\" context? This paper\npresents a framework for answering this fundamental question in the context of\ncorrelation mining, with implications for general large scale inference. In\nlarge scale data applications like genomics, connectomics, and eco-informatics\nthe dataset is often variable-rich but sample-starved: a regime where the\nnumber $n$ of acquired samples (statistical replicates) is far fewer than the\nnumber $p$ of observed variables (genes, neurons, voxels, or chemical\nconstituents). Much of recent work has focused on understanding the\ncomputational complexity of proposed methods for \"Big Data.\" Sample complexity\nhowever has received relatively less attention, especially in the setting when\nthe sample size $n$ is fixed, and the dimension $p$ grows without bound. To\naddress this gap, we develop a unified statistical framework that explicitly\nquantifies the sample complexity of various inferential tasks. Sampling regimes\ncan be divided into several categories: 1) the classical asymptotic regime\nwhere the variable dimension is fixed and the sample size goes to infinity; 2)\nthe mixed asymptotic regime where both variable dimension and sample size go to\ninfinity at comparable rates; 3) the purely high dimensional asymptotic regime\nwhere the variable dimension goes to infinity and the sample size is fixed.\nEach regime has its niche but only the latter regime applies to exa-scale data\ndimension. We illustrate this high dimensional framework for the problem of\ncorrelation mining, where it is the matrix of pairwise and partial correlations\namong the variables that are of interest. We demonstrate various regimes of\ncorrelation mining based on the unifying perspective of high dimensional\nlearning rates and sample complexity for different structured covariance models\nand different inference tasks. \n\n"}
{"id": "1505.02796", "contents": "Title: On the order of the operators in the Douglas-Rachford algorithm Abstract: The Douglas-Rachford algorithm is a popular method for finding zeros of sums\nof monotone operators. By its definition, the Douglas-Rachford operator is not\nsymmetric with respect to the order of the two operators. In this paper we\nprovide a systematic study of the two possible Douglas-Rachford operators. We\nshow that the reflectors of the underlying operators act as bijections between\nthe fixed points sets of the two Douglas-Rachford operators. Some elegant\nformulae arise under additional assumptions. Various examples illustrate our\nresults. \n\n"}
{"id": "1505.04343", "contents": "Title: Provably Correct Algorithms for Matrix Column Subset Selection with\n  Selectively Sampled Data Abstract: We consider the problem of matrix column subset selection, which selects a\nsubset of columns from an input matrix such that the input can be well\napproximated by the span of the selected columns. Column subset selection has\nbeen applied to numerous real-world data applications such as population\ngenetics summarization, electronic circuits testing and recommendation systems.\nIn many applications the complete data matrix is unavailable and one needs to\nselect representative columns by inspecting only a small portion of the input\nmatrix. In this paper we propose the first provably correct column subset\nselection algorithms for partially observed data matrices. Our proposed\nalgorithms exhibit different merits and limitations in terms of statistical\naccuracy, computational efficiency, sample complexity and sampling schemes,\nwhich provides a nice exploration of the tradeoff between these desired\nproperties for column subset selection. The proposed methods employ the idea of\nfeedback driven sampling and are inspired by several sampling schemes\npreviously introduced for low-rank matrix approximation tasks (Drineas et al.,\n2008; Frieze et al., 2004; Deshpande and Vempala, 2006; Krishnamurthy and\nSingh, 2014). Our analysis shows that, under the assumption that the input data\nmatrix has incoherent rows but possibly coherent columns, all algorithms\nprovably converge to the best low-rank approximation of the original data as\nnumber of selected columns increases. Furthermore, two of the proposed\nalgorithms enjoy a relative error bound, which is preferred for column subset\nselection and matrix approximation purposes. We also demonstrate through both\ntheoretical and empirical analysis the power of feedback driven sampling\ncompared to uniform random sampling on input matrices with highly correlated\ncolumns. \n\n"}
{"id": "1505.06408", "contents": "Title: The Douglas-Rachford algorithm in the affine-convex case Abstract: The Douglas-Rachford algorithm is a simple yet effective method for solving\nconvex feasibility problems. However, if the underlying constraints are\ninconsistent, then the convergence theory is incomplete. We provide convergence\nresults when one constraint is an affine subspace. As a consequence, we extend\na result by Spingarn from halfspaces to general closed convex sets admitting\nleast-squares solutions. \n\n"}
{"id": "1505.07533", "contents": "Title: Optimal Stopping with Random Maturity under Nonlinear Expectations Abstract: We analyze an optimal stopping problem with random maturity under a nonlinear\nexpectation with respect to a weakly compact set of mutually singular\nprobabilities $\\mathcal{P}$. The maturity is specified as the hitting time to\nlevel $0$ of some continuous index process at which the payoff process is even\nallowed to have a positive jump. When $\\mathcal{P}$ is a collection of\nsemimartingale measures, the optimal stopping problem can be viewed as a {\\it\ndiscretionary} stopping problem for a player who can influence both drift and\nvolatility of the dynamic of underlying stochastic flow. \n\n"}
{"id": "1506.00036", "contents": "Title: Predicting Regional Economic Indices using Big Data of Individual Bank\n  Card Transactions Abstract: For centuries quality of life was a subject of studies across different\ndisciplines. However, only with the emergence of a digital era, it became\npossible to investigate this topic on a larger scale. Over time it became clear\nthat quality of life not only depends on one, but on three relatively different\nparameters: social, economic and well-being measures. In this study we focus\nonly on the first two, since the last one is often very subjective and\nconsequently hard to measure. Using a complete set of bank card transactions\nrecorded by Banco Bilbao Vizcaya Argentaria (BBVA) during 2011 in Spain, we\nfirst create a feature space by defining various meaningful characteristics of\na particular area performance through activity of its businesses, residents and\nvisitors. We then evaluate those quantities by considering available official\nstatistics for Spanish provinces (e.g., housing prices, unemployment rate, life\nexpectancy) and investigate whether they can be predicted based on our feature\nspace. For the purpose of prediction, our study proposes a supervised machine\nlearning approach. Our finding is that there is a clear correlation between\nindividual spending behavior and official socioeconomic indexes denoting\nquality of life. Moreover, we believe that this modus operandi is useful to\nunderstand, predict and analyze the impact of human activity on the wellness of\nour society on scales for which there is no consistent official statistics\navailable (e.g., cities and towns, districts or smaller neighborhoods). \n\n"}
{"id": "1506.00078", "contents": "Title: Sufficient Lie Algebraic Conditions for Sampled-Data Feedback\n  Stabilization Abstract: For nonlinear affine in the control systems, a Lie algebraic sufficient\ncondition for sampled-data feedback semi-global stabilization is established.\nWe use this result, in order to derive sufficient conditions for sampled-data\nfeedback stabilization for a couple of three-dimensional systems. \n\n"}
{"id": "1506.00738", "contents": "Title: A new fundamental solution for a class of differential Riccati equations Abstract: A class of differential Riccati equations (DREs) is considered whereby the\nevolution of any solution can be identified with the propagation of a value\nfunction of a corresponding optimal control problem arising in L2-gain\nanalysis. By exploiting the semigroup properties inherited from the attendant\ndynamic programming principle, a max-plus primal space fundamental solution\nsemigroup of max-plus linear max-plus integral operators is developed that\nencapsulates all such value function propagations. Using this semigroup, a new\none-parameter fundamental solution semigroup of matrices is developed for the\naforementioned class of DREs. It is demonstrated that this new semigroup can be\nused to compute particular solutions of these DREs, and to characterize finite\nescape times (should they exist) in a relatively simple way compared with that\nprovided by the standard symplectic fundamental solution semigroup. \n\n"}
{"id": "1506.02428", "contents": "Title: Robust Regression via Hard Thresholding Abstract: We study the problem of Robust Least Squares Regression (RLSR) where several\nresponse variables can be adversarially corrupted. More specifically, for a\ndata matrix X \\in R^{p x n} and an underlying model w*, the response vector is\ngenerated as y = X'w* + b where b \\in R^n is the corruption vector supported\nover at most C.n coordinates. Existing exact recovery results for RLSR focus\nsolely on L1-penalty based convex formulations and impose relatively strict\nmodel assumptions such as requiring the corruptions b to be selected\nindependently of X.\n  In this work, we study a simple hard-thresholding algorithm called TORRENT\nwhich, under mild conditions on X, can recover w* exactly even if b corrupts\nthe response variables in an adversarial manner, i.e. both the support and\nentries of b are selected adversarially after observing X and w*. Our results\nhold under deterministic assumptions which are satisfied if X is sampled from\nany sub-Gaussian distribution. Finally unlike existing results that apply only\nto a fixed w*, generated independently of X, our results are universal and hold\nfor any w* \\in R^p.\n  Next, we propose gradient descent-based extensions of TORRENT that can scale\nefficiently to large scale problems, such as high dimensional sparse recovery\nand prove similar recovery guarantees for these extensions. Empirically we find\nTORRENT, and more so its extensions, offering significantly faster recovery\nthan the state-of-the-art L1 solvers. For instance, even on moderate-sized\ndatasets (with p = 50K) with around 40% corrupted responses, a variant of our\nproposed method called TORRENT-HYB is more than 20x faster than the best L1\nsolver. \n\n"}
{"id": "1506.02557", "contents": "Title: Variational Dropout and the Local Reparameterization Trick Abstract: We investigate a local reparameterizaton technique for greatly reducing the\nvariance of stochastic gradients for variational Bayesian inference (SGVB) of a\nposterior over model parameters, while retaining parallelizability. This local\nreparameterization translates uncertainty about global parameters into local\nnoise that is independent across datapoints in the minibatch. Such\nparameterizations can be trivially parallelized and have variance that is\ninversely proportional to the minibatch size, generally leading to much faster\nconvergence. Additionally, we explore a connection with dropout: Gaussian\ndropout objectives correspond to SGVB with local reparameterization, a\nscale-invariant prior and proportionally fixed posterior variance. Our method\nallows inference of more flexibly parameterized posteriors; specifically, we\npropose variational dropout, a generalization of Gaussian dropout where the\ndropout rates are learned, often leading to better models. The method is\ndemonstrated through several experiments. \n\n"}
{"id": "1506.02911", "contents": "Title: Error bounds on the probabilistically optimal problem solving strategy Abstract: We consider a simple optimal probabilistic problem solving strategy that\nsearches through potential solution candidates in a specific order. We are\ninterested in what impact has interchanging the order of two solution\ncandidates with respect to this optimal strategy on the problem solving\neffectivity (i.e., the solution candidates examined as well as time spent\nbefore solving the problem). Such interchange can happen in the applications\nwith only partial information available. We derive bounds on these errors in\ngeneral as well as in three special systems in which we impose some\nrestrictions on the solution candidates. \n\n"}
{"id": "1506.03144", "contents": "Title: Superresolution without Separation Abstract: This paper provides a theoretical analysis of diffraction-limited\nsuperresolution, demonstrating that arbitrarily close point sources can be\nresolved in ideal situations. Precisely, we assume that the incoming signal is\na linear combination of M shifted copies of a known waveform with unknown\nshifts and amplitudes, and one only observes a finite collection of evaluations\nof this signal. We characterize properties of the base waveform such that the\nexact translations and amplitudes can be recovered from 2M + 1 observations.\nThis recovery is achieved by solving a a weighted version of basis pursuit over\na continuous dictionary. Our methods combine classical polynomial interpolation\ntechniques with contemporary tools from compressed sensing. \n\n"}
{"id": "1506.03382", "contents": "Title: Optimal Rates of Convergence for Noisy Sparse Phase Retrieval via\n  Thresholded Wirtinger Flow Abstract: This paper considers the noisy sparse phase retrieval problem: recovering a\nsparse signal $x \\in \\mathbb{R}^p$ from noisy quadratic measurements $y_j =\n(a_j' x )^2 + \\epsilon_j$, $j=1, \\ldots, m$, with independent sub-exponential\nnoise $\\epsilon_j$. The goals are to understand the effect of the sparsity of\n$x$ on the estimation precision and to construct a computationally feasible\nestimator to achieve the optimal rates. Inspired by the Wirtinger Flow [12]\nproposed for noiseless and non-sparse phase retrieval, a novel thresholded\ngradient descent algorithm is proposed and it is shown to adaptively achieve\nthe minimax optimal rates of convergence over a wide range of sparsity levels\nwhen the $a_j$'s are independent standard Gaussian random vectors, provided\nthat the sample size is sufficiently large compared to the sparsity of $x$. \n\n"}
{"id": "1506.03591", "contents": "Title: Optimal Control of a Semidiscrete Cahn-Hilliard-Navier-Stokes System\n  with Non-Matched Fluid Densities Abstract: This paper is concerned with the distributed optimal control of a\ntime-discrete Cahn--Hilliard/Navier--Stokes system with variable densities. It\nfocuses on the double-obstacle potential which yields an optimal control\nproblem for a family of coupled systems in each time instance of a variational\ninequality of fourth order and the Navier--Stokes equation. By proposing a\nsuitable time-discretization, energy estimates are proved and the existence of\nsolutions to the primal system and of optimal controls is established for the\noriginal problem as well as for a family of regularized problems. The latter\ncorrespond to Moreau--Yosida type approximations of the double-obstacle\npotential. The consistency of these approximations is shown and first order\noptimality conditions for the regularized problems are derived. Through a limit\nprocess, a stationarity system for the original problem is established which is\nrelated to a function space version of C-stationarity. \n\n"}
{"id": "1506.04737", "contents": "Title: A transverse Hamiltonian variational technique for open quantum\n  stochastic systems and its application to coherent quantum control Abstract: This paper is concerned with variational methods for nonlinear open quantum\nsystems with Markovian dynamics governed by Hudson-Parthasarathy quantum\nstochastic differential equations. The latter are driven by quantum Wiener\nprocesses of the external boson fields and are specified by the system\nHamiltonian and system-field coupling operators. We consider the system\nresponse to perturbations of these energy operators and introduce a transverse\nHamiltonian which encodes the propagation of the perturbations through the\nunitary system-field evolution. This provides a tool for the infinitesimal\nperturbation analysis and development of optimality conditions for coherent\nquantum control problems. We apply the transverse Hamiltonian variational\ntechnique to a mean square optimal coherent quantum filtering problem for a\nmeasurement-free cascade connection of quantum systems. \n\n"}
{"id": "1506.05404", "contents": "Title: Best of both worlds? Simultaneous evaluation of researchers and their\n  works Abstract: This paper explores a dual score system that simultaneously evaluates the\nrelative importance of researchers and their works. It is a modification of the\nCITEX algorithm recently described in Pal and Ruj (2015). Using available\npublication data for $m$ author keywords (as a proxy for researchers) and $n$\npapers it is possible to construct a $m \\times n$ author-paper feature matrix.\nThis is further combined with citation data to construct a HITS-like algorithm\nthat iteratively satisfies two criteria: first, \\emph{a good author is cited by\ngood authors}, and second, \\emph{a good paper is cited by good authors}.\nFollowing Pal and Ruj, the resulting algorithm produces an author eigenscore\nand a paper eigenscore. The algorithm is tested on 213,530 citable publications\nlisted under Thomson ISI's \"\\emph{Information Science \\& Library Science}\" JCR\ncategory from 1980--2012. \n\n"}
{"id": "1506.05600", "contents": "Title: Causality on Cross-Sectional Data: Stable Specification Search in\n  Constrained Structural Equation Modeling Abstract: Causal modeling has long been an attractive topic for many researchers and in\nrecent decades there has seen a surge in theoretical development and discovery\nalgorithms. Generally discovery algorithms can be divided into two approaches:\nconstraint-based and score-based. The constraint-based approach is able to\ndetect common causes of the observed variables but the use of independence\ntests makes it less reliable. The score-based approach produces a result that\nis easier to interpret as it also measures the reliability of the inferred\ncausal relationships, but it is unable to detect common confounders of the\nobserved variables. A drawback of both score-based and constrained-based\napproaches is the inherent instability in structure estimation. With finite\nsamples small changes in the data can lead to completely different optimal\nstructures. The present work introduces a new hypothesis-free score-based\ncausal discovery algorithm, called stable specification search, that is robust\nfor finite samples based on recent advances in stability selection using\nsubsampling and selection algorithms. Structure search is performed over\nStructural Equation Models. Our approach uses exploratory search but allows\nincorporation of prior background knowledge. We validated our approach on one\nsimulated data set, which we compare to the known ground truth, and two\nreal-world data sets for Chronic Fatigue Syndrome and Attention Deficit\nHyperactivity Disorder, which we compare to earlier medical studies. The\nresults on the simulated data set show significant improvement over alternative\napproaches and the results on the real-word data sets show consistency with the\nhypothesis driven models constructed by medical experts. \n\n"}
{"id": "1507.03454", "contents": "Title: Stability for the Brunn-Minkowski and Riesz rearrangement inequalities,\n  with applications to Gaussian concentration and finite range non-local\n  isoperimetry Abstract: We provide a simple, general argument to obtain improvements of\nconcentration-type inequalities starting from improvements of their\ncorresponding isoperimetric-type inequalities. We apply this argument to obtain\nrobust improvements of the Brunn-Minkowski inequality (for Minkowski sums\nbetween generic sets and convex sets) and of the Gaussian concentration\ninequality. The former inequality is then used to obtain a robust improvement\nof the Riesz rearrangement inequality under certain natural conditions. These\nconditions are compatible with the applications to a finite-range nonlocal\nisoperimetric problem arising in statistical mechanics. \n\n"}
{"id": "1507.05016", "contents": "Title: Incremental Variational Inference for Latent Dirichlet Allocation Abstract: We introduce incremental variational inference and apply it to latent\nDirichlet allocation (LDA). Incremental variational inference is inspired by\nincremental EM and provides an alternative to stochastic variational inference.\nIncremental LDA can process massive document collections, does not require to\nset a learning rate, converges faster to a local optimum of the variational\nbound and enjoys the attractive property of monotonically increasing it. We\nstudy the performance of incremental LDA on large benchmark data sets. We\nfurther introduce a stochastic approximation of incremental variational\ninference which extends to the asynchronous distributed setting. The resulting\ndistributed algorithm achieves comparable performance as single host\nincremental variational inference, but with a significant speed-up. \n\n"}
{"id": "1507.05854", "contents": "Title: Global Convergence of Non-Convex Gradient Descent for Computing Matrix\n  Squareroot Abstract: While there has been a significant amount of work studying gradient descent\ntechniques for non-convex optimization problems over the last few years, all\nexisting results establish either local convergence with good rates or global\nconvergence with highly suboptimal rates, for many problems of interest. In\nthis paper, we take the first step in getting the best of both worlds --\nestablishing global convergence and obtaining a good rate of convergence for\nthe problem of computing squareroot of a positive definite (PD) matrix, which\nis a widely studied problem in numerical linear algebra with applications in\nmachine learning and statistics among others. Given a PD matrix $M$ and a PD\nstarting point $U_0$, we show that gradient descent with appropriately chosen\nstep-size finds an $\\epsilon$-accurate squareroot of $M$ in $O(\\alpha \\log\n(\\|M-U_0^2\\|_F /\\epsilon))$ iterations, where $\\alpha =\n(\\max\\{\\|U_0\\|_2^2,\\|M\\|_2\\} / \\min \\{\\sigma_{\\min}^2(U_0),\\sigma_{\\min}(M) \\}\n)^{3/2}$. Our result is the first to establish global convergence for this\nproblem and that it is robust to errors in each iteration. A key contribution\nof our work is the general proof technique which we believe should further\nexcite research in understanding deterministic and stochastic variants of\nsimple non-convex gradient descent algorithms with good global convergence\nrates for other problems in machine learning and numerical linear algebra. \n\n"}
{"id": "1507.06092", "contents": "Title: On the Inefficiency of the Merit Order in Forward Electricity Markets\n  with Uncertain Supply Abstract: This paper provides insight on the economic inefficiency of the classical\nmerit-order dispatch in electricity markets with uncertain supply. For this, we\nconsider a power system whose operation is driven by a two-stage electricity\nmarket, with a forward and a real-time market. We analyze two different\nclearing mechanisms: a conventional one, whereby the forward and the balancing\nmarkets are independently cleared following a merit order, and a stochastic\none, whereby both market stages are co-optimized with a view to minimizing the\nexpected aggregate system operating cost. We first derive analytical formulae\nto determine the dispatch rule prompted by the co-optimized two-stage market\nfor a stylized power system with flexible, inflexible and stochastic power\ngeneration and infinite transmission capacity. This exercise sheds light on the\nconditions for the stochastic market-clearing mechanism to break the merit\norder. We then introduce and characterize two enhanced variants of the\nconventional two-stage market that result in either price-consistent or\ncost-efficient merit-order dispatch solutions, respectively. The first of these\nvariants corresponds to a conventional two-stage market that allows for virtual\nbidding, while the second requires that the stochastic power production be\ncentrally dispatched. Finally, we discuss the practical implications of our\nanalytical results and illustrate our conclusions through examples. \n\n"}
{"id": "1507.06380", "contents": "Title: Information Spread Over an Internet-mediated Social Network: Phases,\n  Speed, Width, and Effects of Promotion Abstract: In this study, we looked at the effect of promotion in the speed and width of\nspread of information on the Internet by tracking the diffusion of news\narticles over a social network. Speed of spread means the number of readers\nthat the news has reached in a given time, while width of spread means how far\nthe story has travelled from the news originator within the social network.\nAfter analyzing six stories in a 30-hour time span, we found out that the\nlifetime of a story's popularity among the members of the social network has\nthree phases: Expansion, Front-page, and Saturation. Expansion phase starts\nwhen a story is published and the article spreads from a source node to nodes\nwithin a connected component of the social network. Front-page phase happens\nwhen a news aggregator promotes the story in its front page resulting to the\nstory's faster rate of spread among the connected nodes while at the same time\nspreading the article to nodes outside the original connected component of the\nsocial network. Saturation phase is when the story ages and its rate of spread\nwithin the social network slows down, suggesting popularity saturation among\nthe nodes. Within these three phases, we observed minimal changes on the width\nof information spread as suggested by relatively low increase of the width of\nthe spread's diameter within the social network. We see that this paper\nprovides the various stakeholders a first-hand empirical data for modeling,\ndesigning, and improving the current web-based services, specifically the IT\neducators for designing and improving academic curricula, and for improving the\ncurrent web-enabled deployment of knowledge and online evaluation of skills. \n\n"}
{"id": "1507.06738", "contents": "Title: Linear Contextual Bandits with Knapsacks Abstract: We consider the linear contextual bandit problem with resource consumption,\nin addition to reward generation. In each round, the outcome of pulling an arm\nis a reward as well as a vector of resource consumptions. The expected values\nof these outcomes depend linearly on the context of that arm. The\nbudget/capacity constraints require that the total consumption doesn't exceed\nthe budget for each resource. The objective is once again to maximize the total\nreward. This problem turns out to be a common generalization of classic linear\ncontextual bandits (linContextual), bandits with knapsacks (BwK), and the\nonline stochastic packing problem (OSPP). We present algorithms with\nnear-optimal regret bounds for this problem. Our bounds compare favorably to\nresults on the unstructured version of the problem where the relation between\nthe contexts and the outcomes could be arbitrary, but the algorithm only\ncompetes against a fixed set of policies accessible through an optimization\noracle. We combine techniques from the work on linContextual, BwK, and OSPP in\na nontrivial manner while also tackling new difficulties that are not present\nin any of these special cases. \n\n"}
{"id": "1507.07212", "contents": "Title: A Laplacian-Based Approach for Finding Near Globally Optimal Solutions\n  to OPF Problems Abstract: A semidefinite programming (SDP) relaxation globally solves many optimal\npower flow (OPF) problems. For other OPF problems where the SDP relaxation only\nprovides a lower bound on the objective value rather than the globally optimal\ndecision variables, recent literature has proposed a penalization approach to\nfind feasible points that are often nearly globally optimal. A disadvantage of\nthis penalization approach is the need to specify penalty parameters. This\npaper presents an alternative approach that algorithmically determines a\npenalization appropriate for many OPF problems. The proposed approach\nconstrains the generation cost to be close to the lower bound from the SDP\nrelaxation. The objective function is specified using iteratively determined\nweights for a Laplacian matrix. This approach yields feasible points to the OPF\nproblem that are guaranteed to have objective values near the global optimum\ndue to the constraint on generation cost. The proposed approach is demonstrated\non both small OPF problems and a variety of large test cases representing\nportions of European power systems. \n\n"}
{"id": "1508.01551", "contents": "Title: A Knowledge Gradient Policy for Sequencing Experiments to Identify the\n  Structure of RNA Molecules Using a Sparse Additive Belief Model Abstract: We present a sparse knowledge gradient (SpKG) algorithm for adaptively\nselecting the targeted regions within a large RNA molecule to identify which\nregions are most amenable to interactions with other molecules. Experimentally,\nsuch regions can be inferred from fluorescence measurements obtained by binding\na complementary probe with fluorescence markers to the targeted regions. We use\na biophysical model which shows that the fluorescence ratio under the log scale\nhas a sparse linear relationship with the coefficients describing the\naccessibility of each nucleotide, since not all sites are accessible (due to\nthe folding of the molecule). The SpKG algorithm uniquely combines the Bayesian\nranking and selection problem with the frequentist $\\ell_1$ regularized\nregression approach Lasso. We use this algorithm to identify the sparsity\npattern of the linear model as well as sequentially decide the best regions to\ntest before experimental budget is exhausted. Besides, we also develop two\nother new algorithms: batch SpKG algorithm, which generates more suggestions\nsequentially to run parallel experiments; and batch SpKG with a procedure which\nwe call length mutagenesis. It dynamically adds in new alternatives, in the\nform of types of probes, are created by inserting, deleting or mutating\nnucleotides within existing probes. In simulation, we demonstrate these\nalgorithms on the Group I intron (a mid-size RNA molecule), showing that they\nefficiently learn the correct sparsity pattern, identify the most accessible\nregion, and outperform several other policies. \n\n"}
{"id": "1508.02087", "contents": "Title: A Linearly-Convergent Stochastic L-BFGS Algorithm Abstract: We propose a new stochastic L-BFGS algorithm and prove a linear convergence\nrate for strongly convex and smooth functions. Our algorithm draws heavily from\na recent stochastic variant of L-BFGS proposed in Byrd et al. (2014) as well as\na recent approach to variance reduction for stochastic gradient descent from\nJohnson and Zhang (2013). We demonstrate experimentally that our algorithm\nperforms well on large-scale convex and non-convex optimization problems,\nexhibiting linear convergence and rapidly solving the optimization problems to\nhigh levels of precision. Furthermore, we show that our algorithm performs well\nfor a wide-range of step sizes, often differing by several orders of magnitude. \n\n"}
{"id": "1508.02810", "contents": "Title: Convergence rates of sub-sampled Newton methods Abstract: We consider the problem of minimizing a sum of $n$ functions over a convex\nparameter set $\\mathcal{C} \\subset \\mathbb{R}^p$ where $n\\gg p\\gg 1$. In this\nregime, algorithms which utilize sub-sampling techniques are known to be\neffective. In this paper, we use sub-sampling techniques together with low-rank\napproximation to design a new randomized batch algorithm which possesses\ncomparable convergence rate to Newton's method, yet has much smaller\nper-iteration cost. The proposed algorithm is robust in terms of starting point\nand step size, and enjoys a composite convergence rate, namely, quadratic\nconvergence at start and linear convergence when the iterate is close to the\nminimizer. We develop its theoretical analysis which also allows us to select\nnear-optimal algorithm parameters. Our theoretical results can be used to\nobtain convergence rates of previously proposed sub-sampling based algorithms\nas well. We demonstrate how our results apply to well-known machine learning\nproblems. Lastly, we evaluate the performance of our algorithm on several\ndatasets under various scenarios. \n\n"}
{"id": "1508.02933", "contents": "Title: No Regret Bound for Extreme Bandits Abstract: Algorithms for hyperparameter optimization abound, all of which work well\nunder different and often unverifiable assumptions. Motivated by the general\nchallenge of sequentially choosing which algorithm to use, we study the more\nspecific task of choosing among distributions to use for random hyperparameter\noptimization. This work is naturally framed in the extreme bandit setting,\nwhich deals with sequentially choosing which distribution from a collection to\nsample in order to minimize (maximize) the single best cost (reward). Whereas\nthe distributions in the standard bandit setting are primarily characterized by\ntheir means, a number of subtleties arise when we care about the minimal cost\nas opposed to the average cost. For example, there may not be a well-defined\n\"best\" distribution as there is in the standard bandit setting. The best\ndistribution depends on the rewards that have been obtained and on the\nremaining time horizon. Whereas in the standard bandit setting, it is sensible\nto compare policies with an oracle which plays the single best arm, in the\nextreme bandit setting, there are multiple sensible oracle models. We define a\nsensible notion of \"extreme regret\" in the extreme bandit setting, which\nparallels the concept of regret in the standard bandit setting. We then prove\nthat no policy can asymptotically achieve no extreme regret. \n\n"}
{"id": "1508.05787", "contents": "Title: A discrete-pulse optimal control algorithm with an application to spin\n  systems Abstract: This article is aimed at extending the framework of optimal control\ntechniques to the situation where the control field values are restricted to a\nfinite set. We propose a generalization of the standard GRAPE algorithm suited\nto this constraint. We test the validity and the efficiency of this approach\nfor the inversion of an inhomogeneous ensemble of spin systems with different\noffset frequencies. It is shown that a remarkable efficiency can be achieved\neven for a very limited number of discrete values. Some applications in Nuclear\nMagnetic Resonance are discussed. \n\n"}
{"id": "1508.06451", "contents": "Title: Crossings as a side effect of dependency lengths Abstract: The syntactic structure of sentences exhibits a striking regularity:\ndependencies tend to not cross when drawn above the sentence. We investigate\ntwo competing explanations. The traditional hypothesis is that this trend\narises from an independent principle of syntax that reduces crossings\npractically to zero. An alternative to this view is the hypothesis that\ncrossings are a side effect of dependency lengths, i.e. sentences with shorter\ndependency lengths should tend to have fewer crossings. We are able to reject\nthe traditional view in the majority of languages considered. The alternative\nhypothesis can lead to a more parsimonious theory of language. \n\n"}
{"id": "1508.06901", "contents": "Title: Compressive Sensing via Low-Rank Gaussian Mixture Models Abstract: We develop a new compressive sensing (CS) inversion algorithm by utilizing\nthe Gaussian mixture model (GMM). While the compressive sensing is performed\nglobally on the entire image as implemented in our lensless camera, a low-rank\nGMM is imposed on the local image patches. This low-rank GMM is derived via\neigenvalue thresholding of the GMM trained on the projection of the measurement\ndata, thus learned {\\em in situ}. The GMM and the projection of the measurement\ndata are updated iteratively during the reconstruction. Our GMM algorithm\ndegrades to the piecewise linear estimator (PLE) if each patch is represented\nby a single Gaussian model. Inspired by this, a low-rank PLE algorithm is also\ndeveloped for CS inversion, constituting an additional contribution of this\npaper. Extensive results on both simulation data and real data captured by the\nlensless camera demonstrate the efficacy of the proposed algorithm.\nFurthermore, we compare the CS reconstruction results using our algorithm with\nthe JPEG compression. Simulation results demonstrate that when limited\nbandwidth is available (a small number of measurements), our algorithm can\nachieve comparable results as JPEG. \n\n"}
{"id": "1509.00189", "contents": "Title: Echo chambers in the age of misinformation Abstract: The wide availability of user-provided content in online social media\nfacilitates the aggregation of people around common interests, worldviews, and\nnarratives. Despite the enthusiastic rhetoric on the part of some that this\nprocess generates \"collective intelligence\", the WWW also allows the rapid\ndissemination of unsubstantiated conspiracy theories that often elicite rapid,\nlarge, but naive social responses such as the recent case of Jade Helm 15 --\nwhere a simple military exercise turned out to be perceived as the beginning of\nthe civil war in the US. We study how Facebook users consume information\nrelated to two different kinds of narrative: scientific and conspiracy news. We\nfind that although consumers of scientific and conspiracy stories present\nsimilar consumption patterns with respect to content, the sizes of the\nspreading cascades differ. Homogeneity appears to be the primary driver for the\ndiffusion of contents, but each echo chamber has its own cascade dynamics. To\nmimic these dynamics, we introduce a data-driven percolation model on signed\nnetworks. \n\n"}
{"id": "1509.01048", "contents": "Title: Multiscale functions, Scale dynamics and Applications to partial\n  differential equations Abstract: Modeling phenomena from experimental data, always begin with a \\emph{choice\nof hypothesis} on the observed dynamics such as \\emph{determinism},\n\\emph{randomness}, \\emph{derivability} etc. Depending on these choices,\ndifferent behaviors can be observed. The natural question associated to the\nmodeling problem is the following : \\emph{\"With a finite set of data concerning\na phenomenon, can we recover its underlying nature ?} From this problem, we\nintroduce in this paper the definition of \\emph{multi-scale functions},\n\\emph{scale calculus} and \\emph{scale dynamics} based on the \\emph{time-scale\ncalculus} (see \\cite{bohn}). These definitions will be illustrated on the\n\\emph{multi-scale Okamoto's functions}. The introduced formalism explains why\nthere exists different continuous models associated to an equation with\ndifferent \\emph{scale regimes} whereas the equation is \\emph{scale invariant}.\nA typical example of such an equation, is the \\emph{Euler-Lagrange equation}\nand particularly the \\emph{Newton's equation} which will be discussed. Notably,\nwe obtain a \\emph{non-linear diffusion equation} via the \\emph{scale Newton's\nequation} and also the \\emph{non-linear Schr\\\"odinger equation} via the\n\\emph{scale Newton's equation}. Under special assumptions, we recover the\nclassical \\emph{diffusion} equation and the \\emph{Schr\\\"odinger equation}. \n\n"}
{"id": "1509.01621", "contents": "Title: Symmetrizing quantum dynamics beyond gossip-type algorithms Abstract: Recently, consensus-type problems have been formulated in the quantum domain.\nObtaining average quantum consensus consists in the dynamical symmetrization of\na multipartite quantum system while preserving the expectation of a given\nglobal observable. In this paper, two improved ways of obtaining consensus via\ndissipative engineering are introduced, which employ on quasi local preparation\nof mixtures of symmetric pure states, and show better performance in terms of\npurity dynamics with respect to existing algorithms. In addition, the first\nmethod can be used in combination with simple control resources in order to\nengineer pure Dicke states, while the second method guarantees a stronger type\nof consensus, namely single-measurement consensus. This implies that outcomes\nof local measurements on different subsystems are perfectly correlated when\nconsensus is achieved. Both dynamics can be randomized and are suitable for\nfeedback implementation. \n\n"}
{"id": "1509.04632", "contents": "Title: The Shape of Data and Probability Measures Abstract: We introduce the notion of multiscale covariance tensor fields (CTF)\nassociated with Euclidean random variables as a gateway to the shape of their\ndistributions. Multiscale CTFs quantify variation of the data about every point\nin the data landscape at all spatial scales, unlike the usual covariance tensor\nthat only quantifies global variation about the mean. Empirical forms of\nlocalized covariance previously have been used in data analysis and\nvisualization, but we develop a framework for the systematic treatment of\ntheoretical questions and computational models based on localized covariance.\nWe prove strong stability theorems with respect to the Wasserstein distance\nbetween probability measures, obtain consistency results, as well as estimates\nfor the rate of convergence of empirical CTFs. These results ensure that CTFs\nare robust to sampling, noise and outliers. We provide numerous illustrations\nof how CTFs let us extract shape from data and also apply CTFs to manifold\nclustering, the problem of categorizing data points according to their noisy\nmembership in a collection of possibly intersecting, smooth submanifolds of\nEuclidean space. We prove that the proposed manifold clustering method is\nstable and carry out several experiments to validate the method. \n\n"}
{"id": "1509.05009", "contents": "Title: On the Expressive Power of Deep Learning: A Tensor Analysis Abstract: It has long been conjectured that hypotheses spaces suitable for data that is\ncompositional in nature, such as text or images, may be more efficiently\nrepresented with deep hierarchical networks than with shallow ones. Despite the\nvast empirical evidence supporting this belief, theoretical justifications to\ndate are limited. In particular, they do not account for the locality, sharing\nand pooling constructs of convolutional networks, the most successful deep\nlearning architecture to date. In this work we derive a deep network\narchitecture based on arithmetic circuits that inherently employs locality,\nsharing and pooling. An equivalence between the networks and hierarchical\ntensor factorizations is established. We show that a shallow network\ncorresponds to CP (rank-1) decomposition, whereas a deep network corresponds to\nHierarchical Tucker decomposition. Using tools from measure theory and matrix\nalgebra, we prove that besides a negligible set, all functions that can be\nimplemented by a deep network of polynomial size, require exponential size in\norder to be realized (or even approximated) by a shallow network. Since\nlog-space computation transforms our networks into SimNets, the result applies\ndirectly to a deep learning architecture demonstrating promising empirical\nperformance. The construction and theory developed in this paper shed new light\non various practices and ideas employed by the deep learning community. \n\n"}
{"id": "1509.05113", "contents": "Title: Revealed Preference at Scale: Learning Personalized Preferences from\n  Assortment Choices Abstract: We consider the problem of learning the preferences of a heterogeneous\npopulation by observing choices from an assortment of products, ads, or other\nofferings. Our observation model takes a form common in assortment planning\napplications: each arriving customer is offered an assortment consisting of a\nsubset of all possible offerings; we observe only the assortment and the\ncustomer's single choice.\n  In this paper we propose a mixture choice model with a natural underlying\nlow-dimensional structure, and show how to estimate its parameters. In our\nmodel, the preferences of each customer or segment follow a separate parametric\nchoice model, but the underlying structure of these parameters over all the\nmodels has low dimension. We show that a nuclear-norm regularized maximum\nlikelihood estimator can learn the preferences of all customers using a number\nof observations much smaller than the number of item-customer combinations.\nThis result shows the potential for structural assumptions to speed up learning\nand improve revenues in assortment planning and customization. We provide a\nspecialized factored gradient descent algorithm and study the success of the\napproach empirically. \n\n"}
{"id": "1509.05497", "contents": "Title: Quadratic Gaussian Privacy Games Abstract: A game-theoretic model for analysing the effects of privacy on strategic\ncommunication between agents is devised. In the model, a sender wishes to\nprovide an accurate measurement of the state to a receiver while also\nprotecting its private information (which is correlated with the state) private\nfrom a malicious agent that may eavesdrop on its communications with the\nreceiver. A family of nontrivial equilibria, in which the communicated messages\ncarry information, is constructed and its properties are studied. \n\n"}
{"id": "1510.00793", "contents": "Title: Skew-selfadjoint Dirac systems: stability of the procedure of explicit\n  solving the inverse problem Abstract: Procedures to recover explicitly discrete and continuous skew-selfadjoint\nDirac systems on semi-axis from rational Weyl matrix functions are considered.\nTheir stability is shown. Some new facts on asymptotics of pseudo-exponential\npotentials (i.e., of explicit solutions of inverse problems) are proved as\nwell. GBDT version of Backlund-Darboux transformation, methods from system\ntheory and results on algebraic Riccati equations are used for this purpose. \n\n"}
{"id": "1510.01091", "contents": "Title: Evolving Twitter: an experimental analysis of graph properties of the\n  social graph Abstract: Twitter is one of the most prominent Online Social Networks. It covers a\nsignificant part of the online worldwide population~20% and has impressive\ngrowth rates. The social graph of Twitter has been the subject of numerous\nstudies since it can reveal the intrinsic properties of large and complex\nonline communities. Despite the plethora of these studies, there is a limited\ncover on the properties of the social graph while they evolve over time.\nMoreover, due to the extreme size of this social network (millions of nodes,\nbillions of edges), there is a small subset of possible graph properties that\ncan be efficiently measured in a reasonable timescale. In this paper we propose\na sampling framework that allows the estimation of graph properties on large\nsocial networks. We apply this framework to a subset of Twitter's social\nnetwork that has 13.2 million users, 8.3 billion edges and covers the complete\nTwitter timeline (from April 2006 to January 2015). We derive estimation on the\ntime evolution of 24 graph properties many of which have never been measured on\nlarge social networks. We further discuss how these estimations shed more light\non the inner structure and growth dynamics of Twitter's social network. \n\n"}
{"id": "1510.01171", "contents": "Title: On the Online Frank-Wolfe Algorithms for Convex and Non-convex\n  Optimizations Abstract: In this paper, the online variants of the classical Frank-Wolfe algorithm are\nconsidered. We consider minimizing the regret with a stochastic cost. The\nonline algorithms only require simple iterative updates and a non-adaptive step\nsize rule, in contrast to the hybrid schemes commonly considered in the\nliterature. Several new results are derived for convex and non-convex losses.\nWith a strongly convex stochastic cost and when the optimal solution lies in\nthe interior of the constraint set or the constraint set is a polytope, the\nregret bound and anytime optimality are shown to be ${\\cal O}( \\log^3 T / T )$\nand ${\\cal O}( \\log^2 T / T)$, respectively, where $T$ is the number of rounds\nplayed. These results are based on an improved analysis on the stochastic\nFrank-Wolfe algorithms. Moreover, the online algorithms are shown to converge\neven when the loss is non-convex, i.e., the algorithms find a stationary point\nto the time-varying/stochastic loss at a rate of ${\\cal O}(\\sqrt{1/T})$.\nNumerical experiments on realistic data sets are presented to support our\ntheoretical claims. \n\n"}
{"id": "1510.02064", "contents": "Title: New Formulation and Strong MISOCP Relaxations for AC Optimal\n  Transmission Switching Problem Abstract: As the modern transmission control and relay technologies evolve,\ntransmission line switching has become an important option in power system\noperators' toolkits to reduce operational cost and improve system reliability.\nMost recent research has relied on the DC approximation of the power flow model\nin the optimal transmission switching problem. However, it is known that DC\napproximation may lead to inaccurate flow solutions and also overlook stability\nissues. In this paper, we focus on the optimal transmission switching problem\nwith the full AC power flow model, abbreviated as AC OTS. We propose a new\nexact formulation for AC OTS and its mixed-integer second-order conic\nprogramming (MISOCP) relaxation. We improve this relaxation via several types\nof strong valid inequalities inspired by the recent development for the closely\nrelated AC Optimal Power Flow (AC OPF) problem. We also propose a practical\nalgorithm to obtain high quality feasible solutions for the AC OTS problem.\nExtensive computational experiments show that the proposed formulation and\nalgorithms efficiently solve IEEE standard and congested instances and lead to\nsignificant cost benefits with provably tight bounds. \n\n"}
{"id": "1510.02197", "contents": "Title: A characterization of linearizable instances of the quadratic minimum\n  spanning tree problem Abstract: We investigate special cases of the quadratic minimum spanning tree problem\n(QMSTP) on a graph $G=(V,E)$ that can be solved as a linear minimum spanning\ntree problem. Characterization of such problems on graphs with special\nproperties are given. This include complete graphs, complete bipartite graphs,\ncactuses among others. Our characterization can be verified in $O(|E|^2)$ time.\nIn the case of complete graphs and when the cost matrix is given in factored\nform, we show that our characterization can be verified in $O(|E|)$ time.\nRelated open problems are also indicated. \n\n"}
{"id": "1510.03048", "contents": "Title: Minimum-Time Cavity Optomechanical Cooling Abstract: Optomechanical cooling is a prerequisite for many exotic applications\npromised by modern quantum technology and it is crucial to achieve it in short\ntimes, in order to minimize the undesirable effects of the environment. We\nformulate cavity optomechanical cooling as a minimum-time optimal control\nproblem on anti-de Sitter space of appropriate dimension and use the Legendre\npseudospectral optimization method to find the minimum time and the\ncorresponding optimal control, for various values of the maximum coupling rate\nbetween the cavity field and the mechanical resonator. The present framework\ncan also be applied to create optomechanical entanglement in minimum time and\nto improve the efficiency of an optomechanical quantum heat engine. \n\n"}
{"id": "1510.04214", "contents": "Title: LQG Control with Minimum Directed Information: Semidefinite Programming\n  Approach Abstract: We consider a discrete-time Linear-Quadratic-Gaussian (LQG) control problem\nin which Massey's directed information from the observed output of the plant to\nthe control input is minimized while required control performance is\nattainable. This problem arises in several different contexts, including joint\nencoder and controller design for data-rate minimization in networked control\nsystems. We show that the optimal control law is a Linear-Gaussian randomized\npolicy. We also identify the state space realization of the optimal policy,\nwhich can be synthesized by an efficient algorithm based on semidefinite\nprogramming. Our structural result indicates that the filter-controller\nseparation principle from the LQG control theory, and the sensor-filter\nseparation principle from the zero-delay rate-distortion theory for\nGauss-Markov sources hold simultaneously in the considered problem. A\nconnection to the data-rate theorem for mean-square stability by Nair and Evans\nis also established. \n\n"}
{"id": "1510.04568", "contents": "Title: Constraint interface preconditioning for topology optimization problems Abstract: The discretization of constrained nonlinear optimization problems arising in\nthe field of topology optimization yields algebraic systems which are\nchallenging to solve in practice, due to pathological ill-conditioning, strong\nnonlinearity and size. In this work we propose a methodology which brings\ntogether existing fast algorithms, namely, interior-point for the optimization\nproblem and a novel substructuring domain decomposition method for the ensuing\nlarge-scale linear systems. The main contribution is the choice of interface\npreconditioner which allows for the acceleration of the domain decomposition\nmethod, leading to performance independent of problem size. \n\n"}
{"id": "1510.06096", "contents": "Title: When Are Nonconvex Problems Not Scary? Abstract: In this note, we focus on smooth nonconvex optimization problems that obey:\n(1) all local minimizers are also global; and (2) around any saddle point or\nlocal maximizer, the objective has a negative directional curvature. Concrete\napplications such as dictionary learning, generalized phase retrieval, and\northogonal tensor decomposition are known to induce such structures. We\ndescribe a second-order trust-region algorithm that provably converges to a\nglobal minimizer efficiently, without special initializations. Finally we\nhighlight alternatives, and open problems in this direction. \n\n"}
{"id": "1510.07356", "contents": "Title: Decentralized Quadratically Approximated Alternating Direction Method of\n  Multipliers Abstract: This paper considers an optimization problem that components of the objective\nfunction are available at different nodes of a network and nodes are allowed to\nonly exchange information with their neighbors. The decentralized alternating\nmethod of multipliers (DADMM) is a well-established iterative method for\nsolving this category of problems; however, implementation of DADMM requires\nsolving an optimization subproblem at each iteration for each node. This\nprocedure is often computationally costly for the nodes. We introduce a\ndecentralized quadratic approximation of ADMM (DQM) that reduces computational\ncomplexity of DADMM by minimizing a quadratic approximation of the objective\nfunction. Notwithstanding that DQM successively minimizes approximations of the\ncost, it converges to the optimal arguments at a linear rate which is identical\nto the convergence rate of DADMM. Further, we show that as time passes the\ncoefficient of linear convergence for DQM approaches the one for DADMM.\nNumerical results demonstrate the effectiveness of DQM. \n\n"}
{"id": "1510.07471", "contents": "Title: A Parallel algorithm for $\\mathcal{X}$-Armed bandits Abstract: The target of $\\mathcal{X}$-armed bandit problem is to find the global\nmaximum of an unknown stochastic function $f$, given a finite budget of $n$\nevaluations. Recently, $\\mathcal{X}$-armed bandits have been widely used in\nmany situations. Many of these applications need to deal with large-scale data\nsets. To deal with these large-scale data sets, we study a distributed setting\nof $\\mathcal{X}$-armed bandits, where $m$ players collaborate to find the\nmaximum of the unknown function. We develop a novel anytime distributed\n$\\mathcal{X}$-armed bandit algorithm. Compared with prior work on\n$\\mathcal{X}$-armed bandits, our algorithm uses a quite different searching\nstrategy so as to fit distributed learning scenarios. Our theoretical analysis\nshows that our distributed algorithm is $m$ times faster than the classical\nsingle-player algorithm. Moreover, the number of communication rounds of our\nalgorithm is only logarithmic in $mn$. The numerical results show that our\nmethod can make effective use of every players to minimize the loss. Thus, our\ndistributed approach is attractive and useful. \n\n"}
{"id": "1510.08740", "contents": "Title: The rate of convergence of Nesterov's accelerated forward-backward\n  method is actually faster than $1/k^{2}$ Abstract: The {\\it forward-backward algorithm} is a powerful tool for solving\noptimization problems with a {\\it additively separable} and {\\it smooth} + {\\it\nnonsmooth} structure. In the convex setting, a simple but ingenious\nacceleration scheme developed by Nesterov has been proved useful to improve the\ntheoretical rate of convergence for the function values from the standard\n$\\mathcal O(k^{-1})$ down to $\\mathcal O(k^{-2})$. In this short paper, we\nprove that the rate of convergence of a slight variant of Nesterov's\naccelerated forward-backward method, which produces {\\it convergent} sequences,\nis actually $o(k^{-2})$, rather than $\\mathcal O(k^{-2})$. Our arguments rely\non the connection between this algorithm and a second-order differential\ninclusion with vanishing damping. \n\n"}
{"id": "1511.01277", "contents": "Title: Algorithm Portfolios for Noisy Optimization Abstract: Noisy optimization is the optimization of objective functions corrupted by\nnoise. A portfolio of solvers is a set of solvers equipped with an algorithm\nselection tool for distributing the computational power among them. Portfolios\nare widely and successfully used in combinatorial optimization. In this work,\nwe study portfolios of noisy optimization solvers. We obtain mathematically\nproved performance (in the sense that the portfolio performs nearly as well as\nthe best of its solvers) by an ad hoc portfolio algorithm dedicated to noisy\noptimization. A somehow surprising result is that it is better to compare\nsolvers with some lag, i.e., propose the current recommendation of best solver\nbased on their performance earlier in the run. An additional finding is a\nprincipled method for distributing the computational power among solvers in the\nportfolio. \n\n"}
{"id": "1511.01935", "contents": "Title: A Multiresolution Ensemble Kalman Filter using Wavelet Decomposition Abstract: We present a method of using classical wavelet based multiresolution analysis\nto separate scales in model and observations during data assimilation with the\nensemble Kalman filter. In many applications, the underlying physics of a\nphenomena involve the interaction of features at multiple scales. Blending of\nobservational and model error across scales can result in large forecast\ninaccuracies since large errors at one scale are interpreted as inexact data at\nall scales. Our method uses a transformation of the observation operator in\norder to separate the information from different scales of the observations.\nThis naturally induces a transformation of the observation covariance and we\nput forward several algorithms to efficiently compute the transformed\ncovariance. Another advantage of our multiresolution ensemble Kalman filter is\nthat scales can be weighted independently to adjust each scale's effect on the\nforecast. To demonstrate feasibility we present applications to a one\ndimensional Kuramoto-Sivashinsky (K-S) model with scale dependent observation\nnoise and an application involving the forecasting of solar photospheric flux.\nThe latter example demonstrates the multiresolution ensemble Kalman filter's\nability to account for scale dependent model error. Modeling of photospheric\nmagnetic flux transport is accomplished by the Air Force Data Assimilative\nPhotospheric Transport (ADAPT) model. \n\n"}
{"id": "1511.01965", "contents": "Title: Sequential Detection of Market shocks using Risk-averse Agent Based\n  Models Abstract: This paper considers a statistical signal processing problem involving agent\nbased models of financial markets which at a micro-level are driven by socially\naware and risk- averse trading agents. These agents trade (buy or sell) stocks\nby exploiting information about the decisions of previous agents (social\nlearning) via an order book in addition to a private (noisy) signal they\nreceive on the value of the stock. We are interested in the following: (1)\nModelling the dynamics of these risk averse agents, (2) Sequential detection of\na market shock based on the behaviour of these agents. Structural results which\ncharacterize social learning under a risk measure, CVaR (Conditional\nValue-at-risk), are presented and formulation of the Bayesian change point\ndetection problem is provided. The structural results exhibit two interesting\nprop- erties: (i) Risk averse agents herd more often than risk neutral agents\n(ii) The stopping set in the sequential detection problem is non-convex. The\nframework is validated on data from the Yahoo! Tech Buzz game dataset. \n\n"}
{"id": "1511.02204", "contents": "Title: An Extended Frank-Wolfe Method with \"In-Face\" Directions, and its\n  Application to Low-Rank Matrix Completion Abstract: Motivated principally by the low-rank matrix completion problem, we present\nan extension of the Frank-Wolfe method that is designed to induce near-optimal\nsolutions on low-dimensional faces of the feasible region. This is accomplished\nby a new approach to generating ``in-face\" directions at each iteration, as\nwell as through new choice rules for selecting between in-face and ``regular\"\nFrank-Wolfe steps. Our framework for generating in-face directions generalizes\nthe notion of away-steps introduced by Wolfe. In particular, the in-face\ndirections always keep the next iterate within the minimal face containing the\ncurrent iterate. We present computational guarantees for the new method that\ntrade off efficiency in computing near-optimal solutions with upper bounds on\nthe dimension of minimal faces of iterates. We apply the new method to the\nmatrix completion problem, where low-dimensional faces correspond to low-rank\nmatrices. We present computational results that demonstrate the effectiveness\nof our methodological approach at producing nearly-optimal solutions of very\nlow rank. On both artificial and real datasets, we demonstrate significant\nspeed-ups in computing very low-rank nearly-optimal solutions as compared to\neither the Frank-Wolfe method or its traditional away-step variant. \n\n"}
{"id": "1511.02963", "contents": "Title: Analysis and Design of Actuation-Sensing-Communication Interconnection\n  Structures towards Secured/Resilient Closed-loop Systems Abstract: This paper considers the analysis and design of resilient/robust\ndecentralized control systems. Specifically, we aim to assess how the pairing\nof sensors and actuators lead to architectures that are resilient to\nattacks/hacks for industrial control systems and other complex cyber-physical\nsystems. We consider inherent structural properties such as internal fixed\nmodes of a dynamical system depending on actuation, sensing, and\ninterconnection/communication structure for linear discrete time-invariant\ndynamical systems. We introduce the notion of resilient fixed-modes free system\nthat ensures the non-existence of fixed modes when the\nactuation-sensing-communication structure is compromised due to attacks by a\nmalicious agent on actuators, sensors, or communication components and natural\nfailures. Also, we provide a graph-theoretical characterization for the\nresilient structurally fixed modes that enables to capture the non-existence of\nresilient fixed modes for almost all possible systems' realizations.\nAdditionally, we address the minimum actuation-sensing-communication co-design\nensuring the non-existence of resiliently structurally fixed modes, which we\nshow to be NP-hard. Notwithstanding, we identify conditions that are often\nsatisfied in engineering settings and under which the co-design problem is\nsolvable in polynomial-time complexity. Furthermore, we leverage the structural\ninsights and properties to provide a convex optimization method to design the\ngain for a parametrized system and satisfying the sparsity of a given\ninformation pattern. Thus, exploring the interplay between structural and\nnon-structural systems to ensure their resilience. Finally, the efficacy of the\nproposed approach is demonstrated on a power grid example. \n\n"}
{"id": "1511.03222", "contents": "Title: Convergence Properties of Adaptive Systems and the Definition of\n  Exponential Stability Abstract: The convergence properties of adaptive systems in terms of excitation\nconditions on the regressor vector are well known. With persistent excitation\nof the regressor vector in model reference adaptive control the state error and\nthe adaptation error are globally exponentially stable, or equivalently,\nexponentially stable in the large. When the excitation condition however is\nimposed on the reference input or the reference model state it is often\nincorrectly concluded that the persistent excitation in those signals also\nimplies exponential stability in the large. The definition of persistent\nexcitation is revisited so as to address some possible confusion in the\nadaptive control literature. It is then shown that persistent excitation of the\nreference model only implies local persistent excitation (weak persistent\nexcitation). Weak persistent excitation of the regressor is still sufficient\nfor uniform asymptotic stability in the large, but not exponential stability in\nthe large. We show that there exists an infinite region in the state-space of\nadaptive systems where the state rate is bounded. This infinite region with\nfinite rate of convergence is shown to exist not only in classic open-loop\nreference model adaptive systems, but also in a new class of closed-loop\nreference model adaptive systems. \n\n"}
{"id": "1511.04508", "contents": "Title: Distillation as a Defense to Adversarial Perturbations against Deep\n  Neural Networks Abstract: Deep learning algorithms have been shown to perform extremely well on many\nclassical machine learning problems. However, recent studies have shown that\ndeep learning, like other machine learning techniques, is vulnerable to\nadversarial samples: inputs crafted to force a deep neural network (DNN) to\nprovide adversary-selected outputs. Such attacks can seriously undermine the\nsecurity of the system supported by the DNN, sometimes with devastating\nconsequences. For example, autonomous vehicles can be crashed, illicit or\nillegal content can bypass content filters, or biometric authentication systems\ncan be manipulated to allow improper access. In this work, we introduce a\ndefensive mechanism called defensive distillation to reduce the effectiveness\nof adversarial samples on DNNs. We analytically investigate the\ngeneralizability and robustness properties granted by the use of defensive\ndistillation when training DNNs. We also empirically study the effectiveness of\nour defense mechanisms on two DNNs placed in adversarial settings. The study\nshows that defensive distillation can reduce effectiveness of sample creation\nfrom 95% to less than 0.5% on a studied DNN. Such dramatic gains can be\nexplained by the fact that distillation leads gradients used in adversarial\nsample creation to be reduced by a factor of 10^30. We also find that\ndistillation increases the average minimum number of features that need to be\nmodified to create adversarial samples by about 800% on one of the DNNs we\ntested. \n\n"}
{"id": "1511.05328", "contents": "Title: Predictor-based networked control under uncertain transmission delays Abstract: We consider state-feedback predictor-based control of networked control\nsystems with large time-varying communication delays. We show that even a small\ncontroller-to-actuators delay uncertainty may lead to a non-small residual\nerror in a networked control system and reveal how to analyze such systems.\nThen we design an event-triggered predictor-based controller with sampled\nmeasurements and demonstrate that, depending on the delay uncertainty, one\nshould choose various predictor models to reduce the error due to triggering.\nFor the systems with a network only from a controller to actuators, we take\nadvantage of the continuously available measurements by using a continuous-time\npredictor and employing a recently proposed switching approach to\nevent-triggered control. By an example of an inverted pendulum on a cart we\ndemonstrate that the proposed approach is extremely efficient when the\nuncertain time-varying network-induced delays are too large for the system to\nbe stabilizable without a predictor. \n\n"}
{"id": "1511.05467", "contents": "Title: Predictive Entropy Search for Multi-objective Bayesian Optimization Abstract: We present PESMO, a Bayesian method for identifying the Pareto set of\nmulti-objective optimization problems, when the functions are expensive to\nevaluate. The central idea of PESMO is to choose evaluation points so as to\nmaximally reduce the entropy of the posterior distribution over the Pareto set.\nCritically, the PESMO multi-objective acquisition function can be decomposed as\na sum of objective-specific acquisition functions, which enables the algorithm\nto be used in \\emph{decoupled} scenarios in which the objectives can be\nevaluated separately and perhaps with different costs. This decoupling\ncapability also makes it possible to identify difficult objectives that require\nmore evaluations. PESMO also offers gains in efficiency, as its cost scales\nlinearly with the number of objectives, in comparison to the exponential cost\nof other methods. We compare PESMO with other related methods for\nmulti-objective Bayesian optimization on synthetic and real-world problems. The\nresults show that PESMO produces better recommendations with a smaller number\nof evaluations of the objectives, and that a decoupled evaluation can lead to\nimprovements in performance, particularly when the number of objectives is\nlarge. \n\n"}
{"id": "1511.06464", "contents": "Title: Unitary Evolution Recurrent Neural Networks Abstract: Recurrent neural networks (RNNs) are notoriously difficult to train. When the\neigenvalues of the hidden to hidden weight matrix deviate from absolute value\n1, optimization becomes difficult due to the well studied issue of vanishing\nand exploding gradients, especially when trying to learn long-term\ndependencies. To circumvent this problem, we propose a new architecture that\nlearns a unitary weight matrix, with eigenvalues of absolute value exactly 1.\nThe challenge we address is that of parametrizing unitary matrices in a way\nthat does not require expensive computations (such as eigendecomposition) after\neach weight update. We construct an expressive unitary weight matrix by\ncomposing several structured matrices that act as building blocks with\nparameters to be learned. Optimization with this parameterization becomes\nfeasible only when considering hidden states in the complex domain. We\ndemonstrate the potential of this architecture by achieving state of the art\nresults in several hard tasks involving very long-term dependencies. \n\n"}
{"id": "1511.08905", "contents": "Title: Stochastic Proximal Gradient Consensus Over Random Networks Abstract: We consider solving a convex, possibly stochastic optimization problem over a\nrandomly time-varying multi-agent network. Each agent has access to some local\nobjective function, and it only has unbiased estimates of the gradients of the\nsmooth component. We develop a dynamic stochastic proximal-gradient consensus\n(DySPGC) algorithm, with the following key features: i) it works for both the\nstatic and certain randomly time-varying networks, ii) it allows the agents to\nutilize either the exact or stochastic gradient information, iii) it is\nconvergent with provable rate. In particular, we show that the proposed\nalgorithm converges to a global optimal solution, with a rate of\n$\\mathcal{O}(1/r)$ [resp. $\\mathcal{O}(1/\\sqrt{r})$] when the exact (resp.\nstochastic) gradient is available, where r is the iteration counter.\n  Interestingly, the developed algorithm bridges a number of (seemingly\nunrelated) distributed optimization algorithms, such as the EXTRA (Shi et al.\n2014), the PG-EXTRA (Shi et al. 2015), the IC/IDC-ADMM (Chang et al. 2014), and\nthe DLM (Ling et al. 2015) and the classical distributed subgradient method.\nIdentifying such relationship allows for significant generalization of these\nmethods. We also discuss one such generalization which accelerates the DySPGC\n(hence accelerating EXTRA, PG-EXTRA, IC-ADMM). \n\n"}
{"id": "1511.08971", "contents": "Title: Evolving Models for Meso-Scale Structures Abstract: Real world complex networks are scale free and possess meso-scale properties\nlike core-periphery and community structure. We study evolution of the core\nover time in real world networks. This paper proposes evolving models for both\nunweighted and weighted scale free networks having local and global\ncore-periphery as well as community structure. Network evolves using\ntopological growth, self growth, and weight distribution function. To validate\nthe correctness of proposed models, we use K-shell and S-shell decomposition\nmethods. Simulation results show that the generated unweighted networks follow\npower law degree distribution with droop head and heavy tail. Similarly,\ngenerated weighted networks follow degree, strength, and edge-weight power law\ndistributions. We further study other properties of complex networks, such as\nclustering coefficient, nearest neighbor degree, and strength degree\ncorrelation. \n\n"}
{"id": "1512.02337", "contents": "Title: Fast spectral algorithms from sum-of-squares proofs: tensor\n  decomposition and planted sparse vectors Abstract: We consider two problems that arise in machine learning applications: the\nproblem of recovering a planted sparse vector in a random linear subspace and\nthe problem of decomposing a random low-rank overcomplete 3-tensor. For both\nproblems, the best known guarantees are based on the sum-of-squares method. We\ndevelop new algorithms inspired by analyses of the sum-of-squares method. Our\nalgorithms achieve the same or similar guarantees as sum-of-squares for these\nproblems but the running time is significantly faster.\n  For the planted sparse vector problem, we give an algorithm with running time\nnearly linear in the input size that approximately recovers a planted sparse\nvector with up to constant relative sparsity in a random subspace of $\\mathbb\nR^n$ of dimension up to $\\tilde \\Omega(\\sqrt n)$. These recovery guarantees\nmatch the best known ones of Barak, Kelner, and Steurer (STOC 2014) up to\nlogarithmic factors.\n  For tensor decomposition, we give an algorithm with running time close to\nlinear in the input size (with exponent $\\approx 1.086$) that approximately\nrecovers a component of a random 3-tensor over $\\mathbb R^n$ of rank up to\n$\\tilde \\Omega(n^{4/3})$. The best previous algorithm for this problem due to\nGe and Ma (RANDOM 2015) works up to rank $\\tilde \\Omega(n^{3/2})$ but requires\nquasipolynomial time. \n\n"}
{"id": "1512.02549", "contents": "Title: Facial Reduction and Partial Polyhedrality Abstract: We present FRA-Poly, a facial reduction algorithm (FRA) for conic linear\nprograms that is sensitive to the presence of polyhedral faces in the cone. The\nmain goals of FRA and FRA-Poly are the same, i.e., finding the minimal face\ncontaining the feasible region and detecting infeasibility, but FRA-Poly treats\npolyhedral constraints separately. This idea enables us to reduce the number of\niterations drastically when there are many linear inequality constraints. The\nworst case number of iterations for FRA-poly is written in the terms of a\n\"distance to polyhedrality\" quantity and provides better bounds than FRA under\nmild conditions. In particular, in the case of the doubly nonnegative cone,\nFRA-Poly gives a worst case bound of $n$ whereas the classical FRA is\n$\\mathcal{O}(n^2)$. Of possible independent interest, we prove a variant of\nGordan-Stiemke's Theorem and a proper separation theorem that takes into\naccount partial polyhedrality. We provide a discussion on the optimal facial\nreduction strategy and an instance that forces FRAs to perform many steps. We\nalso present a few applications. In particular, we will use FRA-poly to improve\nthe bounds recently obtained by Liu and Pataki on the dimension of certain\naffine subspaces which appear in weakly infeasible problems. \n\n"}
{"id": "1512.03838", "contents": "Title: Dynamical Decentralized Voltage Control of Multi-Terminal HVDC Grids Abstract: High-voltage direct current (HVDC) is a commonly used technology for\nlong-distance electric power transmission, mainly due to its low resistive\nlosses. When connecting multiple HVDC lines into a multi-terminal HVDC (MTDC)\nsystem, several challenges arise. To ensure safe and efficient operation of\nMTDC systems, the voltage of all terminals need to be steered to within an\noperational range. In this paper we study the commonly used decentralized\nvoltage droop controller, and show that it in general does not steer the\nvoltages to within the operational range. We propose a decentralized PI\ncontroller with deadband, and show that it always steers the voltages to within\nthe operational range regardless of the loads. Additionally we show that the\nproposed controller inherits the property of proportional power sharing from\nthe droop controller, provided that both the loads and the line resistances are\nsufficiently low. The results are validated through simulation in MATLAB. \n\n"}
{"id": "1512.04177", "contents": "Title: Conflict and Computation on Wikipedia: a Finite-State Machine Analysis\n  of Editor Interactions Abstract: What is the boundary between a vigorous argument and a breakdown of\nrelations? What drives a group of individuals across it? Taking Wikipedia as a\ntest case, we use a hidden Markov model to approximate the computational\nstructure and social grammar of more than a decade of cooperation and conflict\namong its editors. Across a wide range of pages, we discover a bursty war/peace\nstructure where the systems can become trapped, sometimes for months, in a\ncomputational subspace associated with significantly higher levels of\nconflict-tracking \"revert\" actions. Distinct patterns of behavior characterize\nthe lower-conflict subspace, including tit-for-tat reversion. While a fraction\nof the transitions between these subspaces are associated with top-down actions\ntaken by administrators, the effects are weak. Surprisingly, we find no\nstatistical signal that transitions are associated with the appearance of\nparticularly anti-social users, and only weak association with significant news\nevents outside the system. These findings are consistent with transitions being\ndriven by decentralized processes with no clear locus of control. Models of\nbelief revision in the presence of a common resource for information-sharing\npredict the existence of two distinct phases: a disordered high-conflict phase,\nand a frozen phase with spontaneously-broken symmetry. The bistability we\nobserve empirically may be a consequence of editor turn-over, which drives the\nsystem to a critical point between them. \n\n"}
{"id": "1512.05878", "contents": "Title: Non-representable hyperbolic matroids Abstract: The generalized Lax conjecture asserts that each hyperbolicity cone is a\nlinear slice of the cone of positive semidefinite matrices. Hyperbolic\npolynomials give rise to a class of (hyperbolic) matroids which properly\ncontains the class of matroids representable over the complex numbers. This\nconnection was used by the second author to construct counterexamples to\nalgebraic (stronger) versions of the generalized Lax conjecture by considering\na non-representable hyperbolic matroid. The V\\'amos matroid and a\ngeneralization of it are, prior to this work, the only known instances of\nnon-representable hyperbolic matroids.\n  We prove that the Non-Pappus and Non-Desargues matroids are non-representable\nhyperbolic matroids by exploiting a connection between Euclidean Jordan\nalgebras and projective geometries. We further identify a large class of\nhyperbolic matroids which contains the V\\'amos matroid and the generalized\nV\\'amos matroids recently studied by Burton, Vinzant and Youm. This proves a\nconjecture of Burton et al. We also prove that many of the matroids considered\nhere are non-representable. The proof of hyperbolicity for the matroids in the\nclass depends on proving nonnegativity of certain symmetric polynomials. In\nparticular we generalize and strengthen several inequalities in the literature,\nsuch as the Laguerre-Tur\\'an inequality and Jensen's inequality. Finally we\nexplore consequences to algebraic versions of the generalized Lax conjecture. \n\n"}
{"id": "1512.06098", "contents": "Title: Expectation propagation for continuous time stochastic processes Abstract: We consider the inverse problem of reconstructing the posterior measure over\nthe trajec- tories of a diffusion process from discrete time observations and\ncontinuous time constraints. We cast the problem in a Bayesian framework and\nderive approximations to the posterior distributions of single time marginals\nusing variational approximate inference. We then show how the approximation can\nbe extended to a wide class of discrete-state Markov jump pro- cesses by making\nuse of the chemical Langevin equation. Our empirical results show that the\nproposed method is computationally efficient and provides good approximations\nfor these classes of inverse problems. \n\n"}
{"id": "1512.06890", "contents": "Title: Stochastic Dual Ascent for Solving Linear Systems Abstract: We develop a new randomized iterative algorithm---stochastic dual ascent\n(SDA)---for finding the projection of a given vector onto the solution space of\na linear system. The method is dual in nature: with the dual being a\nnon-strongly concave quadratic maximization problem without constraints. In\neach iteration of SDA, a dual variable is updated by a carefully chosen point\nin a subspace spanned by the columns of a random matrix drawn independently\nfrom a fixed distribution. The distribution plays the role of a parameter of\nthe method. Our complexity results hold for a wide family of distributions of\nrandom matrices, which opens the possibility to fine-tune the stochasticity of\nthe method to particular applications. We prove that primal iterates associated\nwith the dual process converge to the projection exponentially fast in\nexpectation, and give a formula and an insightful lower bound for the\nconvergence rate. We also prove that the same rate applies to dual function\nvalues, primal function values and the duality gap. Unlike traditional\niterative methods, SDA converges under no additional assumptions on the system\n(e.g., rank, diagonal dominance) beyond consistency. In fact, our lower bound\nimproves as the rank of the system matrix drops. Many existing randomized\nmethods for linear systems arise as special cases of SDA, including randomized\nKaczmarz, randomized Newton, randomized coordinate descent, Gaussian descent,\nand their variants. In special cases where our method specializes to a known\nalgorithm, we either recover the best known rates, or improve upon them.\nFinally, we show that the framework can be applied to the distributed average\nconsensus problem to obtain an array of new algorithms. The randomized gossip\nalgorithm arises as a special case. \n\n"}
{"id": "1512.08787", "contents": "Title: Matrix Completion Under Monotonic Single Index Models Abstract: Most recent results in matrix completion assume that the matrix under\nconsideration is low-rank or that the columns are in a union of low-rank\nsubspaces. In real-world settings, however, the linear structure underlying\nthese models is distorted by a (typically unknown) nonlinear transformation.\nThis paper addresses the challenge of matrix completion in the face of such\nnonlinearities. Given a few observations of a matrix that are obtained by\napplying a Lipschitz, monotonic function to a low rank matrix, our task is to\nestimate the remaining unobserved entries. We propose a novel matrix completion\nmethod that alternates between low-rank matrix estimation and monotonic\nfunction estimation to estimate the missing matrix elements. Mean squared error\nbounds provide insight into how well the matrix can be estimated based on the\nsize, rank of the matrix and properties of the nonlinear transformation.\nEmpirical results on synthetic and real-world datasets demonstrate the\ncompetitiveness of the proposed approach. \n\n"}
{"id": "1601.01142", "contents": "Title: Streaming Gibbs Sampling for LDA Model Abstract: Streaming variational Bayes (SVB) is successful in learning LDA models in an\nonline manner. However previous attempts toward developing online Monte-Carlo\nmethods for LDA have little success, often by having much worse perplexity than\ntheir batch counterparts. We present a streaming Gibbs sampling (SGS) method,\nan online extension of the collapsed Gibbs sampling (CGS). Our empirical study\nshows that SGS can reach similar perplexity as CGS, much better than SVB. Our\ndistributed version of SGS, DSGS, is much more scalable than SVB mainly because\nthe updates' communication complexity is small. \n\n"}
{"id": "1601.01345", "contents": "Title: An Oracle Inequality for Quasi-Bayesian Non-Negative Matrix\n  Factorization Abstract: The aim of this paper is to provide some theoretical understanding of\nquasi-Bayesian aggregation methods non-negative matrix factorization. We derive\nan oracle inequality for an aggregated estimator. This result holds for a very\ngeneral class of prior distributions and shows how the prior affects the rate\nof convergence. \n\n"}
{"id": "1601.04737", "contents": "Title: Sub-Sampled Newton Methods I: Globally Convergent Algorithms Abstract: Large scale optimization problems are ubiquitous in machine learning and data\nanalysis and there is a plethora of algorithms for solving such problems. Many\nof these algorithms employ sub-sampling, as a way to either speed up the\ncomputations and/or to implicitly implement a form of statistical\nregularization. In this paper, we consider second-order iterative optimization\nalgorithms and we provide bounds on the convergence of the variants of Newton's\nmethod that incorporate uniform sub-sampling as a means to estimate the\ngradient and/or Hessian. Our bounds are non-asymptotic and quantitative. Our\nalgorithms are global and are guaranteed to converge from any initial iterate.\n  Using random matrix concentration inequalities, one can sub-sample the\nHessian to preserve the curvature information. Our first algorithm incorporates\nHessian sub-sampling while using the full gradient. We also give additional\nconvergence results for when the sub-sampled Hessian is regularized by\nmodifying its spectrum or ridge-type regularization. Next, in addition to\nHessian sub-sampling, we also consider sub-sampling the gradient as a way to\nfurther reduce the computational complexity per iteration. We use approximate\nmatrix multiplication results from randomized numerical linear algebra to\nobtain the proper sampling strategy. In all these algorithms, computing the\nupdate boils down to solving a large scale linear system, which can be\ncomputationally expensive. As a remedy, for all of our algorithms, we also give\nglobal convergence results for the case of inexact updates where such linear\nsystem is solved only approximately.\n  This paper has a more advanced companion paper, [42], in which we demonstrate\nthat, by doing a finer-grained analysis, we can get problem-independent bounds\nfor local convergence of these algorithms and explore trade-offs to improve\nupon the basic results of the present paper. \n\n"}
{"id": "1601.05111", "contents": "Title: Direct and Inverse Variational Problems on Time Scales: A Survey Abstract: We deal with direct and inverse problems of the calculus of variations on\narbitrary time scales. Firstly, using the Euler-Lagrange equation and the\nstrengthened Legendre condition, we give a general form for a variational\nfunctional to attain a local minimum at a given point of the vector space.\nFurthermore, we provide a necessary condition for a dynamic\nintegro-differential equation to be an Euler-Lagrange equation (Helmholtz's\nproblem of the calculus of variations on time scales). New and interesting\nresults for the discrete and quantum settings are obtained as particular cases.\nFinally, we consider very general problems of the calculus of variations given\nby the composition of a certain scalar function with delta and nabla integrals\nof a vector valued field. \n\n"}
{"id": "1601.06114", "contents": "Title: Nonconvex phase synchronization Abstract: We estimate $n$ phases (angles) from noisy pairwise relative phase\nmeasurements. The task is modeled as a nonconvex least-squares optimization\nproblem. It was recently shown that this problem can be solved in polynomial\ntime via convex relaxation, under some conditions on the noise. In this paper,\nunder similar but more restrictive conditions, we show that a modified version\nof the power method converges to the global optimum. This is simpler and\n(empirically) faster than convex approaches. Empirically, they both succeed in\nthe same regime. Further analysis shows that, in the same noise regime as\npreviously studied, second-order necessary optimality conditions for this\nquadratically constrained quadratic program are also sufficient, despite\nnonconvexity. \n\n"}
{"id": "1601.06200", "contents": "Title: GMRES-Accelerated ADMM for Quadratic Objectives Abstract: We consider the sequence acceleration problem for the alternating direction\nmethod-of-multipliers (ADMM) applied to a class of equality-constrained\nproblems with strongly convex quadratic objectives, which frequently arise as\nthe Newton subproblem of interior-point methods. Within this context, the ADMM\nupdate equations are linear, the iterates are confined within a Krylov\nsubspace, and the General Minimum RESidual (GMRES) algorithm is optimal in its\nability to accelerate convergence. The basic ADMM method solves a\n$\\kappa$-conditioned problem in $O(\\sqrt{\\kappa})$ iterations. We give\ntheoretical justification and numerical evidence that the GMRES-accelerated\nvariant consistently solves the same problem in $O(\\kappa^{1/4})$ iterations\nfor an order-of-magnitude reduction in iterations, despite a worst-case bound\nof $O(\\sqrt{\\kappa})$ iterations. The method is shown to be competitive against\nstandard preconditioned Krylov subspace methods for saddle-point problems. The\nmethod is embedded within SeDuMi, a popular open-source solver for conic\noptimization written in MATLAB, and used to solve many large-scale semidefinite\nprograms with error that decreases like $O(1/k^{2})$, instead of $O(1/k)$,\nwhere $k$ is the iteration index. \n\n"}
{"id": "1601.06527", "contents": "Title: Online Community Detection by Using Nearest Hubs Abstract: Community and cluster detection is a popular field of social network\nanalysis. Most algorithms focus on static graphs or series of snapshots.\n  In this paper we present an algorithm, which detects communities in dynamic\ngraphs. The method is based on shortest paths to high-connected nodes, so\ncalled hubs. Due to local message passing we can update the clustering results\nwith low computational power.\n  The presented algorithm is compared with other for some static social\nnetworks. The reached modularity is not as high as the Louvain method, but even\nhigher then spectral clustering. For large-scale real-world datasets with given\nground truth, we could reconstruct most of the given community structure. The\nadvantage of the algorithm is the good performance in dynamic scenarios. \n\n"}
{"id": "1601.06630", "contents": "Title: Bayesian Estimation of Bipartite Matchings for Record Linkage Abstract: The bipartite record linkage task consists of merging two disparate datafiles\ncontaining information on two overlapping sets of entities. This is non-trivial\nin the absence of unique identifiers and it is important for a wide variety of\napplications given that it needs to be solved whenever we have to combine\ninformation from different sources. Most statistical techniques currently used\nfor record linkage are derived from a seminal paper by Fellegi and Sunter\n(1969). These techniques usually assume independence in the matching statuses\nof record pairs to derive estimation procedures and optimal point estimators.\nWe argue that this independence assumption is unreasonable and instead target a\nbipartite matching between the two datafiles as our parameter of interest.\nBayesian implementations allow us to quantify uncertainty on the matching\ndecisions and derive a variety of point estimators using different loss\nfunctions. We propose partial Bayes estimates that allow uncertain parts of the\nbipartite matching to be left unresolved. We evaluate our approach to record\nlinkage using a variety of challenging scenarios and show that it outperforms\nthe traditional methodology. We illustrate the advantages of our methods\nmerging two datafiles on casualties from the civil war of El Salvador. \n\n"}
{"id": "1601.07108", "contents": "Title: Centrality Measures for Networks with Community Structure Abstract: Understanding the network structure, and finding out the influential nodes is\na challenging issue in the large networks. Identifying the most influential\nnodes in the network can be useful in many applications like immunization of\nnodes in case of epidemic spreading, during intentional attacks on complex\nnetworks. A lot of research is done to devise centrality measures which could\nefficiently identify the most influential nodes in the network. There are two\nmajor approaches to the problem: On one hand, deterministic strategies that\nexploit knowledge about the overall network topology in order to find the\ninfluential nodes, while on the other end, random strategies are completely\nagnostic about the network structure. Centrality measures that can deal with a\nlimited knowledge of the network structure are required. Indeed, in practice,\ninformation about the global structure of the overall network is rarely\navailable or hard to acquire. Even if available, the structure of the network\nmight be too large that it is too much computationally expensive to calculate\nglobal centrality measures. To that end, a centrality measure is proposed that\nrequires information only at the community level to identify the influential\nnodes in the network. Indeed, most of the real-world networks exhibit a\ncommunity structure that can be exploited efficiently to discover the\ninfluential nodes. We performed a comparative evaluation of prominent global\ndeterministic strategies together with stochastic strategies with an available\nand the proposed deterministic community-based strategy. Effectiveness of the\nproposed method is evaluated by performing experiments on synthetic and\nreal-world networks with community structure in the case of immunization of\nnodes for epidemic control. \n\n"}
{"id": "1601.07448", "contents": "Title: A Bayesian Approach for Parameter Estimation with Uncertainty for\n  Dynamic Power Systems Abstract: We address the problem of estimating the uncertainty in the solution of power\ngrid inverse problems within the framework of Bayesian inference. We\ninvestigate two approaches, an adjoint-based method and a stochastic spectral\nmethod. These methods are used to estimate the maximum a posteriori point of\nthe parameters and their variance, which quantifies their uncertainty. Within\nthis framework we estimate several parameters of the dynamic power system, such\nas generator inertias, which are not quantifiable in steady-state models. We\nillustrate the performance of these approaches on a 9-bus power grid example\nand analyze the dependence on measurement frequency, estimation horizon,\nperturbation size, and measurement noise. We assess the computational\nefficiency, and discuss the expected performance when these methods are applied\nto large systems. \n\n"}
{"id": "1602.00382", "contents": "Title: Distributed Constrained Recursive Nonlinear Least-Squares Estimation:\n  Algorithms and Asymptotics Abstract: This paper focuses on the problem of recursive nonlinear least squares\nparameter estimation in multi-agent networks, in which the individual agents\nobserve sequentially over time an independent and identically distributed\n(i.i.d.) time-series consisting of a nonlinear function of the true but unknown\nparameter corrupted by noise. A distributed recursive estimator of the\n\\emph{consensus} + \\emph{innovations} type, namely $\\mathcal{CIWNLS}$, is\nproposed, in which the agents update their parameter estimates at each\nobservation sampling epoch in a collaborative way by simultaneously processing\nthe latest locally sensed information~(\\emph{innovations}) and the parameter\nestimates from other agents~(\\emph{consensus}) in the local neighborhood\nconforming to a pre-specified inter-agent communication topology. Under rather\nweak conditions on the connectivity of the inter-agent communication and a\n\\emph{global observability} criterion, it is shown that at every network agent,\nthe proposed algorithm leads to consistent parameter estimates. Furthermore,\nunder standard smoothness assumptions on the local observation functions, the\ndistributed estimator is shown to yield order-optimal convergence rates, i.e.,\nas far as the order of pathwise convergence is concerned, the local parameter\nestimates at each agent are as good as the optimal centralized nonlinear least\nsquares estimator which would require access to all the observations across all\nthe agents at all times. In order to benchmark the performance of the proposed\ndistributed $\\mathcal{CIWNLS}$ estimator with that of the centralized nonlinear\nleast squares estimator, the asymptotic normality of the estimate sequence is\nestablished and the asymptotic covariance of the distributed estimator is\nevaluated. Finally, simulation results are presented which illustrate and\nverify the analytical findings. \n\n"}
{"id": "1602.00798", "contents": "Title: A Unified Framework for Information Consumption Based on Markov Chains Abstract: This paper establishes a Markov chain model as a unified framework for\nunderstanding information consumption processes in complex networks, with clear\nimplications to the Internet and big-data technologies. In particular, the\nproposed model is the first one to address the formation mechanism of the\n\"trichotomy\" in observed probability density functions from empirical data of\nvarious social and technical networks. Both simulation and experimental results\ndemonstrate a good match of the proposed model with real datasets, showing its\nsuperiority over the classical power-law models. \n\n"}
{"id": "1602.01716", "contents": "Title: Decentralized Prediction-Correction Methods for Networked Time-Varying\n  Convex Optimization Abstract: We develop algorithms that find and track the optimal solution trajectory of\ntime-varying convex optimization problems which consist of local and\nnetwork-related objectives. The algorithms are derived from the\nprediction-correction methodology, which corresponds to a strategy where the\ntime-varying problem is sampled at discrete time instances and then a sequence\nis generated via alternatively executing predictions on how the optimizers at\nthe next time sample are changing and corrections on how they actually have\nchanged. Prediction is based on how the optimality conditions evolve in time,\nwhile correction is based on a gradient or Newton method, leading to\nDecentralized Prediction-Correction Gradient (DPC-G) and Decentralized\nPrediction-Correction Newton (DPC-N). We extend these methods to cases where\nthe knowledge on how the optimization programs are changing in time is only\napproximate and propose Decentralized Approximate Prediction-Correction\nGradient (DAPC-G) and Decentralized Approximate Prediction-Correction Newton\n(DAPC-N). Convergence properties of all the proposed methods are studied and\nempirical performance is shown on an application of a resource allocation\nproblem in a wireless network. We observe that the proposed methods outperform\nexisting running algorithms by orders of magnitude. The numerical results\nshowcase a trade-off between convergence accuracy, sampling period, and network\ncommunications. \n\n"}
{"id": "1602.01768", "contents": "Title: Randomized Quasi-Newton Updates are Linearly Convergent Matrix Inversion\n  Algorithms Abstract: We develop and analyze a broad family of stochastic/randomized algorithms for\ninverting a matrix. We also develop specialized variants maintaining symmetry\nor positive definiteness of the iterates. All methods in the family converge\nglobally and linearly (i.e., the error decays exponentially), with explicit\nrates. In special cases, we obtain stochastic block variants of several\nquasi-Newton updates, including bad Broyden (BB), good Broyden (GB),\nPowell-symmetric-Broyden (PSB), Davidon-Fletcher-Powell (DFP) and\nBroyden-Fletcher-Goldfarb-Shanno (BFGS). Ours are the first stochastic versions\nof these updates shown to converge to an inverse of a fixed matrix. Through a\ndual viewpoint we uncover a fundamental link between quasi-Newton updates and\napproximate inverse preconditioning. Further, we develop an adaptive variant of\nrandomized block BFGS, where we modify the distribution underlying the\nstochasticity of the method throughout the iterative process to achieve faster\nconvergence. By inverting several matrices from varied applications, we\ndemonstrate that AdaRBFGS is highly competitive when compared to the well\nestablished Newton-Schulz and minimal residual methods. In particular, on\nlarge-scale problems our method outperforms the standard methods by orders of\nmagnitude. Development of efficient methods for estimating the inverse of very\nlarge matrices is a much needed tool for preconditioning and variable metric\noptimization methods in the advent of the big data era. \n\n"}
{"id": "1602.02923", "contents": "Title: Globally Optimal Energy-Efficient Power Control and Receiver Design in\n  Wireless Networks Abstract: The characterization of the global maximum of energy efficiency (EE) problems\nin wireless networks is a challenging problem due to the non-convex nature of\ninvestigated problems in interference channels. The aim of this work is to\ndevelop a new and general framework to achieve globally optimal solutions.\nFirst, the hidden monotonic structure of the most common EE maximization\nproblems is exploited jointly with fractional programming theory to obtain\nglobally optimal solutions with exponential complexity in the number of network\nlinks. To overcome this issue, we also propose a framework to compute\nsuboptimal power control strategies characterized by affordable complexity.\nThis is achieved by merging fractional programming and sequential optimization.\nThe proposed monotonic framework is used to shed light on the ultimate\nperformance of wireless networks in terms of EE and also to benchmark the\nperformance of the lower-complexity framework based on sequential programming.\nNumerical evidence is provided to show that the sequential fractional\nprogramming framework achieves global optimality in several practical\ncommunication scenarios. \n\n"}
{"id": "1602.05240", "contents": "Title: Robust Influence Maximization Abstract: Uncertainty about models and data is ubiquitous in the computational social\nsciences, and it creates a need for robust social network algorithms, which can\nsimultaneously provide guarantees across a spectrum of models and parameter\nsettings. We begin an investigation into this broad domain by studying robust\nalgorithms for the Influence Maximization problem, in which the goal is to\nidentify a set of k nodes in a social network whose joint influence on the\nnetwork is maximized.\n  We define a Robust Influence Maximization framework wherein an algorithm is\npresented with a set of influence functions, typically derived from different\ninfluence models or different parameter settings for the same model. The\ndifferent parameter settings could be derived from observed cascades on\ndifferent topics, under different conditions, or at different times. The\nalgorithm's goal is to identify a set of k nodes who are simultaneously\ninfluential for all influence functions, compared to the (function-specific)\noptimum solutions.\n  We show strong approximation hardness results for this problem unless the\nalgorithm gets to select at least a logarithmic factor more seeds than the\noptimum solution. However, when enough extra seeds may be selected, we show\nthat techniques of Krause et al. can be used to approximate the optimum robust\ninfluence to within a factor of 1 - 1/e. We evaluate this bicriteria\napproximation algorithm against natural heuristics on several real-world data\nsets. Our experiments indicate that the worst-case hardness does not\nnecessarily translate into bad performance on real-world data sets; all\nalgorithms perform fairly well. \n\n"}
{"id": "1602.06664", "contents": "Title: A Geometric Analysis of Phase Retrieval Abstract: Can we recover a complex signal from its Fourier magnitudes? More generally,\ngiven a set of $m$ measurements, $y_k = |\\mathbf a_k^* \\mathbf x|$ for $k = 1,\n\\dots, m$, is it possible to recover $\\mathbf x \\in \\mathbb{C}^n$ (i.e.,\nlength-$n$ complex vector)? This **generalized phase retrieval** (GPR) problem\nis a fundamental task in various disciplines, and has been the subject of much\nrecent investigation. Natural nonconvex heuristics often work remarkably well\nfor GPR in practice, but lack clear theoretical explanations. In this paper, we\ntake a step towards bridging this gap. We prove that when the measurement\nvectors $\\mathbf a_k$'s are generic (i.i.d. complex Gaussian) and the number of\nmeasurements is large enough ($m \\ge C n \\log^3 n$), with high probability, a\nnatural least-squares formulation for GPR has the following benign geometric\nstructure: (1) there are no spurious local minimizers, and all global\nminimizers are equal to the target signal $\\mathbf x$, up to a global phase;\nand (2) the objective function has a negative curvature around each saddle\npoint. This structure allows a number of iterative optimization methods to\nefficiently find a global minimizer, without special initialization. To\ncorroborate the claim, we describe and analyze a second-order trust-region\nalgorithm. \n\n"}
{"id": "1602.07137", "contents": "Title: Efficiency analysis of double perturbed pairwise comparison matrices Abstract: Efficiency is a core concept of multi-objective optimization problems and\nmulti-attribute decision making. In the case of pairwise comparison matrices a\nweight vector is called efficient if the approximations of the elements of the\npairwise comparison matrix made by the ratios of the weights cannot be improved\nin any position without making it worse in some other position. A pairwise\ncomparison matrix is called double perturbed if it can be made consistent by\naltering two elements and their reciprocals. The most frequently used weighting\nmethod, the eigenvector method is analyzed in the paper, and it is shown that\nit produces an efficient weight vector for double perturbed pairwise comparison\nmatrices. \n\n"}
{"id": "1602.07819", "contents": "Title: SOCP Reformulation for the Generalized Trust Region Subproblem via a\n  Canonical Form of Two Symmetric Matrices Abstract: We investigate in this paper the generalized trust region subproblem (GTRS)\nof minimizing a general quadratic objective function subject to a general\nquadratic inequality constraint. By applying a simultaneous block\ndiagonalization approach, we obtain a congruent canonical form for the\nsymmetric matrices in both the objective and constraint functions. By\nexploiting the block separability of the canonical form, we show that all GTRSs\nwith an optimal value bounded from below are second order cone programming\n(SOCP) representable. Our result generalizes the recent work of Ben-Tal and\nHertog (Math. Program. 143(1-2):1-29, 2014), which establishes the SOCP\nrepresentability of the GTRS under the assumption of the simultaneous\ndiagonalizability of the two matrices in the objective and constraint\nfunctions. Compared with the state-of-the-art approach to reformulate the GTRS\nas a semi-definite programming problem, our SOCP reformulation delivers a much\nfaster solution algorithm. We further extend our method to two variants of the\nGTRS in which the inequality constraint is replaced by either an equality\nconstraint or an interval constraint. Our methods also enable us to obtain\nsimplified versions of the classical S-lemma, the S-lemma with equality, and\nthe S-lemma with interval bounds. \n\n"}
{"id": "1602.08114", "contents": "Title: Bayesian Inference of Diffusion Networks with Unknown Infection Times Abstract: The analysis of diffusion processes in real-world propagation scenarios often\ninvolves estimating variables that are not directly observed. These hidden\nvariables include parental relationships, the strengths of connections between\nnodes, and the moments of time that infection occurs. In this paper, we propose\na framework in which all three sets of parameters are assumed to be hidden and\nwe develop a Bayesian approach to infer them. After justifying the model\nassumptions, we evaluate the performance efficiency of our proposed approach\nthrough numerical simulations on synthetic datasets and real-world diffusion\nprocesses. \n\n"}
{"id": "1602.08509", "contents": "Title: Estimating Distribution Grid Topologies: A Graphical Learning based\n  Approach Abstract: Distribution grids represent the final tier in electric networks consisting\nof medium and low voltage lines that connect the distribution substations to\nthe end-users. Traditionally, distribution networks have been operated in a\nradial topology that may be changed from time to time. Due to absence of a\nsignificant number of real-time line monitoring devices in the distribution\ngrid, estimation of the topology is a problem critical for its observability\nand control. This paper develops a novel graphical learning based approach to\nestimate the radial operational grid structure using voltage measurements\ncollected from the grid loads. The learning algorithm is based on conditional\nindependence tests for continuous variables over chordal graphs and has wide\napplicability. It is proven that the scheme can be used for several power flow\nlaws (DC or AC approximations) and more importantly is independent of the\nspecific probability distribution controlling individual bus power usage. The\ncomplexity of the algorithm is discussed and its performance is demonstrated by\nsimulations on distribution test cases. \n\n"}
{"id": "1602.08877", "contents": "Title: Design of PAR-Constrained Sequences for MIMO Channel Estimation via\n  Majorization-Minimization Abstract: PAR-constrained sequences are widely used in communication systems and radars\ndue to various practical needs; specifically, sequences are required to be\nunimodular or of low peak-to-average power ratio (PAR). For unimodular sequence\ndesign, plenty of efforts have been devoted to obtaining good correlation\nproperties. Regarding channel estimation, however, sequences of such properties\ndo not necessarily help produce optimal estimates. Tailored unimodular\nsequences for the specific criterion concerned are desirable especially when\nthe prior knowledge of the channel is taken into account as well. In this\npaper, we formulate the problem of optimal unimodular sequence design for\nminimum mean square error estimation of the channel impulse response and\nconditional mutual information maximization, respectively. Efficient algorithms\nbased on the majorization-minimization framework are proposed for both problems\nwith guaranteed convergence. As the unimodular constraint is a special case of\nthe low PAR constraint, optimal sequences of low PAR are also considered.\nNumerical examples are provided to show the performance of the proposed\ntraining sequences, with the efficiency of the derived algorithms demonstrated. \n\n"}
{"id": "1603.00211", "contents": "Title: On the Estimation Performance and Convergence Rate of the Generalized\n  Power Method for Phase Synchronization Abstract: An estimation problem of fundamental interest is that of phase\nsynchronization, in which the goal is to recover a collection of phases using\nnoisy measurements of relative phases. It is known that in the Gaussian noise\nsetting, the maximum likelihood estimator (MLE) has an expected squared\n$\\ell_2$-estimation error that is on the same order as the Cram\\'er-Rao lower\nbound. Moreover, even though the MLE is an optimal solution to a non-convex\nquadratic optimization problem, it can be found with high probability using\nsemidefinite programming (SDP), provided that the noise power is not too large.\nIn this paper, we study the estimation and convergence performance of a\nrecently-proposed low-complexity alternative to the SDP-based approach, namely,\nthe generalized power method (GPM). Our contribution is twofold. First, we\nbound the rate at which the estimation error decreases in each iteration of the\nGPM and use this bound to show that all iterates---not just the MLE---achieve\nan estimation error that is on the same order as the Cram\\'er-Rao bound. Our\nresult holds under the least restrictive assumption on the noise power and\ngives the best provable bound on the estimation error known to date. It also\nimplies that one can terminate the GPM at any iteration and still obtain an\nestimator that has a theoretical guarantee on its estimation error. Second, we\nshow that under the same assumption on the noise power as that for the\nSDP-based method, the GPM will converge to the MLE at a linear rate with high\nprobability. This answers a question raised in [3] and shows that the GPM is\ncompetitive in terms of both theoretical guarantees and numerical efficiency\nwith the SDP-based method. At the heart of our convergence rate analysis is a\nnew error bound for the non-convex quadratic optimization formulation of the\nphase synchronization problem, which could be of independent interest. \n\n"}
{"id": "1603.01681", "contents": "Title: A single-phase, proximal path-following framework Abstract: We propose a new proximal, path-following framework for a class of\nconstrained convex problems. We consider settings where the nonlinear---and\npossibly non-smooth---objective part is endowed with a proximity operator, and\nthe constraint set is equipped with a self-concordant barrier. Our approach\nrelies on the following two main ideas. First, we re-parameterize the\noptimality condition as an auxiliary problem, such that a good initial point is\navailable; by doing so, a family of alternative paths towards the optimum is\ngenerated. Second, we combine the proximal operator with path-following ideas\nto design a single-phase, proximal, path-following algorithm. Our method has\nseveral advantages. First, it allows handling non-smooth objectives via\nproximal operators; this avoids lifting the problem dimension in order to\naccommodate non-smooth components in optimization. Second, it consists of only\na \\emph{single phase}: While the overall convergence rate of classical\npath-following schemes for self-concordant objectives does not suffer from the\ninitialization phase, proximal path-following schemes undergo slow convergence,\nin order to obtain a good starting point \\cite{TranDinh2013e}. In this work, we\nshow how to overcome this limitation in the proximal setting and prove that our\nscheme has the same $\\mathcal{O}(\\sqrt{\\nu}\\log(1/\\varepsilon))$ worst-case\niteration-complexity with standard approaches \\cite{Nesterov2004,Nesterov1994}\nwithout requiring an initial phase, where $\\nu$ is the barrier parameter and\n$\\varepsilon$ is a desired accuracy. Finally, our framework allows errors in\nthe calculation of proximal-Newton directions, without sacrificing the\nworst-case iteration complexity. We demonstrate the merits of our algorithm via\nthree numerical examples, where proximal operators play a key role. \n\n"}
{"id": "1603.04222", "contents": "Title: Multiple seed structure and disconnected networks in respondent-driven\n  sampling Abstract: Respondent-driven sampling (RDS) is a link-tracing sampling method that is\nespecially suitable for sampling hidden populations. RDS combines an efficient\nsnowball-type sampling scheme with inferential procedures that yield unbiased\npopulation estimates under some assumptions about the sampling procedure and\npopulation structure. Several seed individuals are typically used to initiate\nRDS recruitment. However, standard RDS estimation theory assume that all\nsampled individuals originate from only one seed. We present an estimator,\nbased on a random walk with teleportation, which accounts for the multiple seed\nstructure of RDS. The new estimator can also be used on populations with\ndisconnected social networks. We numerically evaluate our estimator by\nsimulations on artificial and real networks. Our estimator outperforms previous\nestimators, especially when the proportion of seeds in the sample is large. We\nrecommend our new estimator to be used in RDS studies, in particular when the\nnumber of seeds is large or the social network of the population is\ndisconnected. \n\n"}
{"id": "1603.04547", "contents": "Title: Stochastic quasi-Newton methods for non-strongly convex problems:\n  convergence and rate analysis Abstract: Motivated by applications in optimization and machine learning, we consider\nstochastic quasi-Newton (SQN) methods for solving stochastic optimization\nproblems. In the literature, the convergence analysis of these algorithms\nrelies on strong convexity of the objective function. To our knowledge, no\ntheoretical analysis is provided for the rate statements in the absence of this\nassumption. Motivated by this gap, we allow the objective function to be merely\nconvex and we develop a cyclic regularized SQN method where the gradient\nmapping and the Hessian approximation matrix are both regularized at each\niteration and are updated in a cyclic manner. We show that, under suitable\nassumptions on the stepsize and regularization parameters, the objective\nfunction value converges to the optimal objective function of the original\nproblem in both almost sure and the expected senses. For each case, a class of\nfeasible sequences that guarantees the convergence is provided. Moreover, the\nrate of convergence in terms of the objective function value is derived. Our\nempirical analysis on a binary classification problem shows that the proposed\nscheme performs well compared to both classic regularization SQN schemes and\nstochastic approximation method. \n\n"}
{"id": "1603.04913", "contents": "Title: Bilateral Boundary Control of One-Dimensional First- and Second-Order\n  PDEs using Infinite-Dimensional Backstepping Abstract: This paper develops an extension of infinite-dimensional backstepping method\nfor parabolic and hyperbolic systems in one spatial dimension with two\nactuators. Typically, PDE backstepping is applied in 1-D domains with an\nactuator at one end. Here, we consider the use of two actuators, one at each\nend of the domain, which we refer to as bilateral control (as opposed to\nunilateral control). Bilateral control laws are derived for linear\nreaction-diffusion, wave and 2X2 hyperbolic 1-D systems (with same speed of\ntransport in both directions). The extension is nontrivial but straightforward\nif the backstepping transformation is adequately posed. The resulting bilateral\ncontrollers are compared with their unilateral counterparts in the\nreaction-diffusion case for constant coefficients, by making use of explicit\nsolutions, showing a reduction in control effort as a tradeoff for the presence\nof two actuators when the system coefficients are large. These results open the\ndoor for more sophisticated designs such as bilateral sensor/actuator output\nfeedback and fault-tolerant designs. \n\n"}
{"id": "1603.08252", "contents": "Title: Predictive Modeling of Opinion and Connectivity Dynamics in Social\n  Networks Abstract: Recent years saw an increased interest in modeling and understanding the\nmechanisms of opinion and innovation spread through human networks. Using\nanalysis of real-world social data, researchers are able to gain a better\nunderstanding of the dynamics of social networks and subsequently model the\nchanges in such networks over time. We developed a social network model that\nboth utilizes an agent-based approach with a dynamic update of opinions and\nconnections between agents and reflects opinion propagation and structural\nchanges over time as observed in real-world data. We validate the model using\ndata from the Social Evolution dataset of the MIT Human Dynamics Lab describing\nchanges in friendships and health self-perception in a targeted student\npopulation over a nine-month period. We demonstrate the effectiveness of the\napproach by predicting changes in both opinion spread and connectivity of the\nnetwork. We also use the model to evaluate how the network parameters, such as\nthe level of `openness' and willingness to incorporate opinions of neighboring\nagents, affect the outcome. The model not only provides insight into the\ndynamics of ever changing social networks, but also presents a tool with which\none can investigate opinion propagation strategies for networks of various\nstructures and opinion distributions. \n\n"}
{"id": "1603.08989", "contents": "Title: An a posteriori error analysis for an optimal control problem involving\n  the fractional Laplacian Abstract: In a previous work, we introduced a discretization scheme for a constrained\noptimal control problem involving the fractional Laplacian. For such a control\nproblem, we derived optimal a priori error estimates that demand the convexity\nof the domain and some compatibility conditions on the data. To relax such\nrestrictions, in this paper, we introduce and analyze an efficient and, under\ncertain assumptions, reliable a posteriori error estimator. We realize the\nfractional Laplacian as the Dirichlet-to-Neumann map for a nonuniformly\nelliptic problem posed on a semi--infinite cylinder in one more spatial\ndimension. This extra dimension further motivates the design of an posteriori\nerror indicator. The latter is defined as the sum of three contributions, which\ncome from the discretization of the state and adjoint equations and the control\nvariable. The indicator for the state and adjoint equations relies on an\nanisotropic error estimator in Muckenhoupt weighted Sobolev spaces. The\nanalysis is valid in any dimension. On the basis of the devised a posteriori\nerror estimator, we design a simple adaptive strategy that exhibits optimal\nexperimental rates of convergence. \n\n"}
{"id": "1604.00543", "contents": "Title: Decomposing Linearly Constrained Nonconvex Problems by a Proximal Primal\n  Dual Approach: Algorithms, Convergence, and Applications Abstract: In this paper, we propose a new decomposition approach named the proximal\nprimal dual algorithm (Prox-PDA) for smooth nonconvex linearly constrained\noptimization problems. The proposed approach is primal-dual based, where the\nprimal step minimizes certain approximation of the augmented Lagrangian of the\nproblem, and the dual step performs an approximate dual ascent. The\napproximation used in the primal step is able to decompose the variable blocks,\nmaking it possible to obtain simple subproblems by leveraging the problem\nstructures. Theoretically, we show that whenever the penalty parameter in the\naugmented Lagrangian is larger than a given threshold, the Prox-PDA converges\nto the set of stationary solutions, globally and in a sublinear manner (i.e.,\ncertain measure of stationarity decreases in the rate of $\\mathcal{O}(1/r)$,\nwhere $r$ is the iteration counter). Interestingly, when applying a variant of\nthe Prox-PDA to the problem of distributed nonconvex optimization (over a\nconnected undirected graph), the resulting algorithm coincides with the popular\nEXTRA algorithm [Shi et al 2014], which is only known to work in convex cases.\nOur analysis implies that EXTRA and its variants converge globally sublinearly\nto stationary solutions of certain nonconvex distributed optimization problem.\nThere are many possible extensions of the Prox-PDA, and we present one\nparticular extension to certain nonconvex distributed matrix factorization\nproblem. \n\n"}
{"id": "1604.01371", "contents": "Title: Attitude Estimation with Feedback Particle Filter Abstract: This paper presents theory, application, and comparisons of the feedback\nparticle filter (FPF) algorithm for the problem of attitude estimation. The\npaper builds upon our recent work on the exact FPF solution of the\ncontinuous-time nonlinear filtering problem on compact Lie groups. In this\npaper, the details of the FPF algorithm are presented for the problem of\nattitude estimation - a nonlinear filtering problem on SO(3). The quaternions\nare employed for computational purposes. The algorithm requires a numerical\nsolution of the filter gain function, and two methods are applied for this\npurpose. Comparisons are also provided between the FPF and some popular\nalgorithms for attitude estimation on SO(3), including the invariant EKF, the\nmultiplicative EKF, and the unscented Kalman filter. Simulation results are\npresented that help illustrate the comparisons. \n\n"}
{"id": "1604.02610", "contents": "Title: Network Topology Identification from Spectral Templates Abstract: Network topology inference is a cornerstone problem in statistical analyses\nof complex systems. In this context, the fresh look advocated here permeates\nbenefits from convex optimization and graph signal processing, to identify the\nso-termed graph shift operator (encoding the network topology) given only the\neigenvectors of the shift. These spectral templates can be obtained, for\nexample, from principal component analysis of a set of graph signals defined on\nthe particular network. The novel idea is to find a graph shift that while\nbeing consistent with the provided spectral information, it endows the network\nstructure with certain desired properties such as sparsity. The focus is on\ndeveloping efficient recovery algorithms along with identifiability conditions\nfor two particular shifts, the adjacency matrix and the normalized graph\nLaplacian. Application domains include network topology identification from\nsteady-state signals generated by a diffusion process, and design of a graph\nfilter that facilitates the distributed implementation of a prescribed linear\nnetwork operator. Numerical tests showcase the effectiveness of the proposed\nalgorithms in recovering synthetic and structural brain networks. \n\n"}
{"id": "1604.06260", "contents": "Title: The Dynamics of Initiative in Communication Networks Abstract: Human social interaction is often intermittent. Two acquainted persons can\nhave extended periods without social interaction punctuated by periods of\nrepeated interaction. In this case, the repeated interaction can be\ncharacterized by a seed initiative by either of the persons and a number of\nfollow-up interactions. The tendency to initiate social interaction plays an\nimportant role in the formation of social networks and is in general not\nsymmetric between persons. In this paper, we study the dynamics of initiative\nby analysing and modeling a detailed call and text message network sampled from\na group of 700 individuals. We show that in an average relationship between two\nindividuals, one part is almost twice as likely to initiate communication\ncompared to the other part. The asymmetry has social consequences and\nultimately might lead to the discontinuation of a relationship. We explain the\nobserved asymmetry by a positive feedback mechanism where individuals already\ntaking initiative are more likely to take initiative in the future. In general,\npeople with many initiatives receive attention from a broader spectrum of\nfriends than people with few initiatives. Lastly, we compare the likelihood of\ntaking initiative with the basic personality traits of the five factor model. \n\n"}
{"id": "1604.07070", "contents": "Title: Stochastic Variance-Reduced ADMM Abstract: The alternating direction method of multipliers (ADMM) is a powerful\noptimization solver in machine learning. Recently, stochastic ADMM has been\nintegrated with variance reduction methods for stochastic gradient, leading to\nSAG-ADMM and SDCA-ADMM that have fast convergence rates and low iteration\ncomplexities. However, their space requirements can still be high. In this\npaper, we propose an integration of ADMM with the method of stochastic variance\nreduced gradient (SVRG). Unlike another recent integration attempt called\nSCAS-ADMM, the proposed algorithm retains the fast convergence benefits of\nSAG-ADMM and SDCA-ADMM, but is more advantageous in that its storage\nrequirement is very low, even independent of the sample size $n$. We also\nextend the proposed method for nonconvex problems, and obtain a convergence\nrate of $O(1/T)$. Experimental results demonstrate that it is as fast as\nSAG-ADMM and SDCA-ADMM, much faster than SCAS-ADMM, and can be used on much\nbigger data sets. \n\n"}
{"id": "1604.07451", "contents": "Title: Learning Local Dependence In Ordered Data Abstract: In many applications, data come with a natural ordering. This ordering can\noften induce local dependence among nearby variables. However, in complex data,\nthe width of this dependence may vary, making simple assumptions such as a\nconstant neighborhood size unrealistic. We propose a framework for learning\nthis local dependence based on estimating the inverse of the Cholesky factor of\nthe covariance matrix. Penalized maximum likelihood estimation of this matrix\nyields a simple regression interpretation for local dependence in which\nvariables are predicted by their neighbors. Our proposed method involves\nsolving a convex, penalized Gaussian likelihood problem with a hierarchical\ngroup lasso penalty. The problem decomposes into independent subproblems which\ncan be solved efficiently in parallel using first-order methods. Our method\nyields a sparse, symmetric, positive definite estimator of the precision\nmatrix, encoding a Gaussian graphical model. We derive theoretical results not\nfound in existing methods attaining this structure. In particular, our\nconditions for signed support recovery and estimation consistency rates in\nmultiple norms are as mild as those in a regression problem. Empirical results\nshow our method performing favorably compared to existing methods. We apply our\nmethod to genomic data to flexibly model linkage disequilibrium. Our method is\nalso applied to improve the performance of discriminant analysis in sound\nrecording classification. \n\n"}
{"id": "1604.08201", "contents": "Title: Interpretable Deep Neural Networks for Single-Trial EEG Classification Abstract: Background: In cognitive neuroscience the potential of Deep Neural Networks\n(DNNs) for solving complex classification tasks is yet to be fully exploited.\nThe most limiting factor is that DNNs as notorious 'black boxes' do not provide\ninsight into neurophysiological phenomena underlying a decision. Layer-wise\nRelevance Propagation (LRP) has been introduced as a novel method to explain\nindividual network decisions. New Method: We propose the application of DNNs\nwith LRP for the first time for EEG data analysis. Through LRP the single-trial\nDNN decisions are transformed into heatmaps indicating each data point's\nrelevance for the outcome of the decision. Results: DNN achieves classification\naccuracies comparable to those of CSP-LDA. In subjects with low performance\nsubject-to-subject transfer of trained DNNs can improve the results. The\nsingle-trial LRP heatmaps reveal neurophysiologically plausible patterns,\nresembling CSP-derived scalp maps. Critically, while CSP patterns represent\nclass-wise aggregated information, LRP heatmaps pinpoint neural patterns to\nsingle time points in single trials. Comparison with Existing Method(s): We\ncompare the classification performance of DNNs to that of linear CSP-LDA on two\ndata sets related to motor-imaginery BCI. Conclusion: We have demonstrated that\nDNN is a powerful non-linear tool for EEG analysis. With LRP a new quality of\nhigh-resolution assessment of neural activity can be reached. LRP is a\npotential remedy for the lack of interpretability of DNNs that has limited\ntheir utility in neuroscientific applications. The extreme specificity of the\nLRP-derived heatmaps opens up new avenues for investigating neural activity\nunderlying complex perception or decision-related processes. \n\n"}
{"id": "1605.00076", "contents": "Title: Asynchronous Optimization Over Heterogeneous Networks via Consensus ADMM Abstract: This paper considers the distributed optimization of a sum of locally\nobservable, non-convex functions. The optimization is performed over a\nmulti-agent networked system, and each local function depends only on a subset\nof the variables. An asynchronous and distributed alternating directions method\nof multipliers (ADMM) method that allows the nodes to defer or skip the\ncomputation and transmission of updates is proposed in the paper. The proposed\nalgorithm utilizes different approximations in the update step, resulting in\nproximal and majorized ADMM variants. Both variants are shown to converge to a\nlocal minimum, under certain regularity conditions. The proposed asynchronous\nalgorithms are also applied to the problem of cooperative localization in\nwireless ad hoc networks, where it is shown to outperform the other\nstate-of-the-art localization algorithms. \n\n"}
{"id": "1605.00201", "contents": "Title: Further properties of the forward-backward envelope with applications to\n  difference-of-convex programming Abstract: In this paper, we further study the forward-backward envelope first\nintroduced in [28] and [30] for problems whose objective is the sum of a proper\nclosed convex function and a twice continuously differentiable possibly\nnonconvex function with Lipschitz continuous gradient. We derive sufficient\nconditions on the original problem for the corresponding forward-backward\nenvelope to be a level-bounded and Kurdyka-{\\L}ojasiewicz function with an\nexponent of $\\frac12$; these results are important for the efficient\nminimization of the forward-backward envelope by classical optimization\nalgorithms. In addition, we demonstrate how to minimize some\ndifference-of-convex regularized least squares problems by minimizing a\nsuitably constructed forward-backward envelope. Our preliminary numerical\nresults on randomly generated instances of large-scale $\\ell_{1-2}$ regularized\nleast squares problems [37] illustrate that an implementation of this approach\nwith a limited-memory BFGS scheme usually outperforms standard first-order\nmethods such as the nonmonotone proximal gradient method in [35]. \n\n"}
{"id": "1605.00606", "contents": "Title: A Limited-Feedback Approximation Scheme for Optimal Switching Problems\n  with Execution Delays Abstract: We consider a type of optimal switching problems with non-uniform execution\ndelays and ramping. Such problems frequently occur in the operation of\neconomical and engineering systems. We first provide a solution to the problem\nby applying a probabilistic method. The main contribution is, however, a scheme\nfor approximating the optimal control by limiting the information in the\nstate-feedback. In a numerical example the approximation routine gives a\nconsiderable computational performance enhancement, when compared to a\nconventional algorithm. \n\n"}
{"id": "1605.01639", "contents": "Title: Flux-dependent graphs for metabolic networks Abstract: Cells adapt their metabolic fluxes in response to changes in the environment.\nWe present a framework for the systematic construction of flux-based graphs\nderived from organism-wide metabolic networks. Our graphs encode the\ndirectionality of metabolic fluxes via edges that represent the flow of\nmetabolites from source to target reactions. The methodology can be applied in\nthe absence of a specific biological context by modelling fluxes\nprobabilistically, or can be tailored to different environmental conditions by\nincorporating flux distributions computed through constraint-based approaches\nsuch as Flux Balance Analysis. We illustrate our approach on the central carbon\nmetabolism of Escherichia coli and on a metabolic model of human hepatocytes.\nThe flux-dependent graphs under various environmental conditions and genetic\nperturbations exhibit systemic changes in their topological and community\nstructure, which capture the re-routing of metabolic fluxes and the varying\nimportance of specific reactions and pathways. By integrating constraint-based\nmodels and tools from network science, our framework allows the study of\ncontext-specific metabolic responses at a system level beyond standard pathway\ndescriptions. \n\n"}
{"id": "1605.02408", "contents": "Title: Structured Nonconvex and Nonsmooth Optimization: Algorithms and\n  Iteration Complexity Analysis Abstract: Nonconvex and nonsmooth optimization problems are frequently encountered in\nmuch of statistics, business, science and engineering, but they are not yet\nwidely recognized as a technology in the sense of scalability. A reason for\nthis relatively low degree of popularity is the lack of a well developed system\nof theory and algorithms to support the applications, as is the case for its\nconvex counterpart. This paper aims to take one step in the direction of\ndisciplined nonconvex and nonsmooth optimization. In particular, we consider in\nthis paper some constrained nonconvex optimization models in block decision\nvariables, with or without coupled affine constraints. In the case of without\ncoupled constraints, we show a sublinear rate of convergence to an\n$\\epsilon$-stationary solution in the form of variational inequality for a\ngeneralized conditional gradient method, where the convergence rate is shown to\nbe dependent on the H\\\"olderian continuity of the gradient of the smooth part\nof the objective. For the model with coupled affine constraints, we introduce\ncorresponding $\\epsilon$-stationarity conditions, and apply two proximal-type\nvariants of the ADMM to solve such a model, assuming the proximal ADMM updates\ncan be implemented for all the block variables except for the last block, for\nwhich either a gradient step or a majorization-minimization step is\nimplemented. We show an iteration complexity bound of $O(1/\\epsilon^2)$ to\nreach an $\\epsilon$-stationary solution for both algorithms. Moreover, we show\nthat the same iteration complexity of a proximal BCD method follows\nimmediately. Numerical results are provided to illustrate the efficacy of the\nproposed algorithms for tensor robust PCA. \n\n"}
{"id": "1605.05725", "contents": "Title: Quantitative convergence analysis of iterated expansive, set-valued\n  mappings Abstract: We develop a framework for quantitative convergence analysis of Picard\niterations of expansive set-valued fixed point mappings. There are two key\ncomponents of the analysis. The first is a natural generalization of\nsingle-valued averaged mappings to expansive, set-valued mappings that\ncharacterizes a type of strong calmness of the fixed point mapping. The second\ncomponent to this analysis is an extension of the well-established notion of\nmetric subregularity -- or inverse calmness -- of the mapping at fixed points.\nConvergence of expansive fixed point iterations is proved using these two\nproperties, and quantitative estimates are a natural byproduct of the\nframework. To demonstrate the application of the theory, we prove for the first\ntime a number of results showing local linear convergence of nonconvex cyclic\nprojections for inconsistent (and consistent) feasibility problems, local\nlinear convergence of the forward-backward algorithm for structured\noptimization without convexity, strong or otherwise, and local linear\nconvergence of the Douglas--Rachford algorithm for structured nonconvex\nminimization. This theory includes earlier approaches for known results, convex\nand nonconvex, as special cases. \n\n"}
{"id": "1605.05870", "contents": "Title: Interests Diffusion on a Semantic Multiplex Abstract: Exploiting the information about members of a Social Network (SN) represents\none of the most attractive and dwelling subjects for both academic and applied\nscientists. The community of Complexity Science and especially those\nresearchers working on multiplex social systems are devoting increasing efforts\nto outline general laws, models, and theories, to the purpose of predicting\nemergent phenomena in SN's (e.g. success of a product). On the other side the\nsemantic web community aims at engineering a new generation of advanced\nservices tailored to specific people needs. This implies defining constructs,\nmodels and methods for handling the semantic layer of SNs. We combined models\nand techniques from both the former fields to provide a hybrid approach to\nunderstand a basic (yet complex) phenomenon: the propagation of individual\ninterests along the social networks. Since information may move along different\nsocial networks, one should take into account a multiplex structure. Therefore\nwe introduced the notion of \"Semantic Multiplex\". In this paper we analyse two\ndifferent semantic social networks represented by authors publishing in the\nComputer Science and those in the American Physical Society Journals. The\ncomparison allows to outline common and specific features \n\n"}
{"id": "1605.05969", "contents": "Title: Randomized Primal-Dual Proximal Block Coordinate Updates Abstract: In this paper we propose a randomized primal-dual proximal block coordinate\nupdating framework for a general multi-block convex optimization model with\ncoupled objective function and linear constraints. Assuming mere convexity, we\nestablish its $O(1/t)$ convergence rate in terms of the objective value and\nfeasibility measure. The framework includes several existing algorithms as\nspecial cases such as a primal-dual method for bilinear saddle-point problems\n(PD-S), the proximal Jacobian ADMM (Prox-JADMM) and a randomized variant of the\nADMM method for multi-block convex optimization. Our analysis recovers and/or\nstrengthens the convergence properties of several existing algorithms. For\nexample, for PD-S our result leads to the same order of convergence rate\nwithout the previously assumed boundedness condition on the constraint sets,\nand for Prox-JADMM the new result provides convergence rate in terms of the\nobjective value and the feasibility violation. It is well known that the\noriginal ADMM may fail to converge when the number of blocks exceeds two. Our\nresult shows that if an appropriate randomization procedure is invoked to\nselect the updating blocks, then a sublinear rate of convergence in expectation\ncan be guaranteed for multi-block ADMM, without assuming any strong convexity.\nThe new approach is also extended to solve problems where only a stochastic\napproximation of the (sub-)gradient of the objective is available, and we\nestablish an $O(1/\\sqrt{t})$ convergence rate of the extended approach for\nsolving stochastic programming. \n\n"}
{"id": "1605.06444", "contents": "Title: Unreasonable Effectiveness of Learning Neural Networks: From Accessible\n  States and Robust Ensembles to Basic Algorithmic Schemes Abstract: In artificial neural networks, learning from data is a computationally\ndemanding task in which a large number of connection weights are iteratively\ntuned through stochastic-gradient-based heuristic processes over a\ncost-function. It is not well understood how learning occurs in these systems,\nin particular how they avoid getting trapped in configurations with poor\ncomputational performance. Here we study the difficult case of networks with\ndiscrete weights, where the optimization landscape is very rough even for\nsimple architectures, and provide theoretical and numerical evidence of the\nexistence of rare - but extremely dense and accessible - regions of\nconfigurations in the network weight space. We define a novel measure, which we\ncall the \"robust ensemble\" (RE), which suppresses trapping by isolated\nconfigurations and amplifies the role of these dense regions. We analytically\ncompute the RE in some exactly solvable models, and also provide a general\nalgorithmic scheme which is straightforward to implement: define a\ncost-function given by a sum of a finite number of replicas of the original\ncost-function, with a constraint centering the replicas around a driving\nassignment. To illustrate this, we derive several powerful new algorithms,\nranging from Markov Chains to message passing to gradient descent processes,\nwhere the algorithms target the robust dense states, resulting in substantial\nimprovements in performance. The weak dependence on the number of precision\nbits of the weights leads us to conjecture that very similar reasoning applies\nto more conventional neural networks. Analogous algorithmic schemes can also be\napplied to other optimization problems. \n\n"}
{"id": "1605.06892", "contents": "Title: Accelerated Randomized Mirror Descent Algorithms For Composite\n  Non-strongly Convex Optimization Abstract: We consider the problem of minimizing the sum of an average function of a\nlarge number of smooth convex components and a general, possibly\nnon-differentiable, convex function. Although many methods have been proposed\nto solve this problem with the assumption that the sum is strongly convex, few\nmethods support the non-strongly convex case. Adding a small quadratic\nregularization is a common devise used to tackle non-strongly convex problems;\nhowever, it may cause loss of sparsity of solutions or weaken the performance\nof the algorithms. Avoiding this devise, we propose an accelerated randomized\nmirror descent method for solving this problem without the strongly convex\nassumption. Our method extends the deterministic accelerated proximal gradient\nmethods of Paul Tseng and can be applied even when proximal points are computed\ninexactly. We also propose a scheme for solving the problem when the component\nfunctions are non-smooth. \n\n"}
{"id": "1605.07051", "contents": "Title: Convergence Analysis for Rectangular Matrix Completion Using\n  Burer-Monteiro Factorization and Gradient Descent Abstract: We address the rectangular matrix completion problem by lifting the unknown\nmatrix to a positive semidefinite matrix in higher dimension, and optimizing a\nnonconvex objective over the semidefinite factor using a simple gradient\ndescent scheme. With $O( \\mu r^2 \\kappa^2 n \\max(\\mu, \\log n))$ random\nobservations of a $n_1 \\times n_2$ $\\mu$-incoherent matrix of rank $r$ and\ncondition number $\\kappa$, where $n = \\max(n_1, n_2)$, the algorithm linearly\nconverges to the global optimum with high probability. \n\n"}
{"id": "1605.07066", "contents": "Title: A Unifying Framework for Gaussian Process Pseudo-Point Approximations\n  using Power Expectation Propagation Abstract: Gaussian processes (GPs) are flexible distributions over functions that\nenable high-level assumptions about unknown functions to be encoded in a\nparsimonious, flexible and general way. Although elegant, the application of\nGPs is limited by computational and analytical intractabilities that arise when\ndata are sufficiently numerous or when employing non-Gaussian models.\nConsequently, a wealth of GP approximation schemes have been developed over the\nlast 15 years to address these key limitations. Many of these schemes employ a\nsmall set of pseudo data points to summarise the actual data. In this paper, we\ndevelop a new pseudo-point approximation framework using Power Expectation\nPropagation (Power EP) that unifies a large number of these pseudo-point\napproximations. Unlike much of the previous venerable work in this area, the\nnew framework is built on standard methods for approximate inference\n(variational free-energy, EP and Power EP methods) rather than employing\napproximations to the probabilistic generative model itself. In this way, all\nof approximation is performed at `inference time' rather than at `modelling\ntime' resolving awkward philosophical and empirical questions that trouble\nprevious approaches. Crucially, we demonstrate that the new framework includes\nnew pseudo-point approximation methods that outperform current approaches on\nregression and classification tasks. \n\n"}
{"id": "1605.07174", "contents": "Title: Kernel-based Reconstruction of Graph Signals Abstract: A number of applications in engineering, social sciences, physics, and\nbiology involve inference over networks. In this context, graph signals are\nwidely encountered as descriptors of vertex attributes or features in\ngraph-structured data. Estimating such signals in all vertices given noisy\nobservations of their values on a subset of vertices has been extensively\nanalyzed in the literature of signal processing on graphs (SPoG). This paper\nadvocates kernel regression as a framework generalizing popular SPoG modeling\nand reconstruction and expanding their capabilities. Formulating signal\nreconstruction as a regression task on reproducing kernel Hilbert spaces of\ngraph signals permeates benefits from statistical learning, offers fresh\ninsights, and allows for estimators to leverage richer forms of prior\ninformation than existing alternatives. A number of SPoG notions such as\nbandlimitedness, graph filters, and the graph Fourier transform are naturally\naccommodated in the kernel framework. Additionally, this paper capitalizes on\nthe so-called representer theorem to devise simpler versions of existing\nThikhonov regularized estimators, and offers a novel probabilistic\ninterpretation of kernel methods on graphs based on graphical models. Motivated\nby the challenges of selecting the bandwidth parameter in SPoG estimators or\nthe kernel map in kernel-based methods, the present paper further proposes two\nmulti-kernel approaches with complementary strengths. Whereas the first enables\nestimation of the unknown bandwidth of bandlimited signals, the second allows\nfor efficient graph filter selection. Numerical tests with synthetic as well as\nreal data demonstrate the merits of the proposed methods relative to\nstate-of-the-art alternatives. \n\n"}
{"id": "1605.07272", "contents": "Title: Matrix Completion has No Spurious Local Minimum Abstract: Matrix completion is a basic machine learning problem that has wide\napplications, especially in collaborative filtering and recommender systems.\nSimple non-convex optimization algorithms are popular and effective in\npractice. Despite recent progress in proving various non-convex algorithms\nconverge from a good initial point, it remains unclear why random or arbitrary\ninitialization suffices in practice. We prove that the commonly used non-convex\nobjective function for \\textit{positive semidefinite} matrix completion has no\nspurious local minima --- all local minima must also be global. Therefore, many\npopular optimization algorithms such as (stochastic) gradient descent can\nprovably solve positive semidefinite matrix completion with \\textit{arbitrary}\ninitialization in polynomial time. The result can be generalized to the setting\nwhen the observed entries contain noise. We believe that our main proof\nstrategy can be useful for understanding geometric properties of other\nstatistical problems involving partial or noisy observations. \n\n"}
{"id": "1605.08101", "contents": "Title: Global rates of convergence for nonconvex optimization on manifolds Abstract: We consider the minimization of a cost function $f$ on a manifold $M$ using\nRiemannian gradient descent and Riemannian trust regions (RTR). We focus on\nsatisfying necessary optimality conditions within a tolerance $\\varepsilon$.\nSpecifically, we show that, under Lipschitz-type assumptions on the pullbacks\nof $f$ to the tangent spaces of $M$, both of these algorithms produce points\nwith Riemannian gradient smaller than $\\varepsilon$ in $O(1/\\varepsilon^2)$\niterations. Furthermore, RTR returns a point where also the Riemannian\nHessian's least eigenvalue is larger than $-\\varepsilon$ in\n$O(1/\\varepsilon^3)$ iterations. There are no assumptions on initialization.\nThe rates match their (sharp) unconstrained counterparts as a function of the\naccuracy $\\varepsilon$ (up to constants) and hence are sharp in that sense.\n  These are the first deterministic results for global rates of convergence to\napproximate first- and second-order Karush-Kuhn-Tucker points on manifolds.\nThey apply in particular for optimization constrained to compact submanifolds\nof $\\mathbb{R}^n$, under simpler assumptions. \n\n"}
{"id": "1605.08285", "contents": "Title: Solving Systems of Random Quadratic Equations via Truncated Amplitude\n  Flow Abstract: This paper presents a new algorithm, termed \\emph{truncated amplitude flow}\n(TAF), to recover an unknown vector $\\bm{x}$ from a system of quadratic\nequations of the form $y_i=|\\langle\\bm{a}_i,\\bm{x}\\rangle|^2$, where\n$\\bm{a}_i$'s are given random measurement vectors. This problem is known to be\n\\emph{NP-hard} in general. We prove that as soon as the number of equations is\non the order of the number of unknowns, TAF recovers the solution exactly (up\nto a global unimodular constant) with high probability and complexity growing\nlinearly with both the number of unknowns and the number of equations. Our TAF\napproach adopts the \\emph{amplitude-based} empirical loss function, and\nproceeds in two stages. In the first stage, we introduce an\n\\emph{orthogonality-promoting} initialization that can be obtained with a few\npower iterations. Stage two refines the initial estimate by successive updates\nof scalable \\emph{truncated generalized gradient iterations}, which are able to\nhandle the rather challenging nonconvex and nonsmooth amplitude-based objective\nfunction. In particular, when vectors $\\bm{x}$ and $\\bm{a}_i$'s are\nreal-valued, our gradient truncation rule provably eliminates erroneously\nestimated signs with high probability to markedly improve upon its untruncated\nversion. Numerical tests using synthetic data and real images demonstrate that\nour initialization returns more accurate and robust estimates relative to\nspectral initializations. Furthermore, even under the same initialization, the\nproposed amplitude-based refinement outperforms existing Wirtinger flow\nvariants, corroborating the superior performance of TAF over state-of-the-art\nalgorithms. \n\n"}
{"id": "1605.09010", "contents": "Title: Rate Control under Heavy Traffic with Strategic Servers Abstract: We consider a large queueing system that consists of many strategic servers\nthat are weakly interacting. Each server processes jobs from its unique\ncritically loaded buffer and controls the rate of arrivals and departures\nassociated with its queue to minimize its expected cost. The rates and the cost\nfunctions in addition to depending on the control action, can depend, in a\nsymmetric fashion, on the size of the individual queue and the empirical\nmeasure of the states of all queues in the system. In order to determine an\napproximate Nash equilibrium for this finite player game we construct a\nLasry-Lions type mean-field game (MFG) for certain reflected diffusions that\ngoverns the limiting behavior. Under conditions, we establish the convergence\nof the Nash-equilibrium value for the finite size queuing system to the value\nof the MFG. \n\n"}
{"id": "1605.09346", "contents": "Title: Minding the Gaps for Block Frank-Wolfe Optimization of Structured SVMs Abstract: In this paper, we propose several improvements on the block-coordinate\nFrank-Wolfe (BCFW) algorithm from Lacoste-Julien et al. (2013) recently used to\noptimize the structured support vector machine (SSVM) objective in the context\nof structured prediction, though it has wider applications. The key intuition\nbehind our improvements is that the estimates of block gaps maintained by BCFW\nreveal the block suboptimality that can be used as an adaptive criterion.\nFirst, we sample objects at each iteration of BCFW in an adaptive non-uniform\nway via gapbased sampling. Second, we incorporate pairwise and away-step\nvariants of Frank-Wolfe into the block-coordinate setting. Third, we cache\noracle calls with a cache-hit criterion based on the block gaps. Fourth, we\nprovide the first method to compute an approximate regularization path for\nSSVM. Finally, we provide an exhaustive empirical evaluation of all our methods\non four structured prediction datasets. \n\n"}
{"id": "1605.09346", "contents": "Title: Minding the Gaps for Block Frank-Wolfe Optimization of Structured SVMs Abstract: In this paper, we propose several improvements on the block-coordinate\nFrank-Wolfe (BCFW) algorithm from Lacoste-Julien et al. (2013) recently used to\noptimize the structured support vector machine (SSVM) objective in the context\nof structured prediction, though it has wider applications. The key intuition\nbehind our improvements is that the estimates of block gaps maintained by BCFW\nreveal the block suboptimality that can be used as an adaptive criterion.\nFirst, we sample objects at each iteration of BCFW in an adaptive non-uniform\nway via gapbased sampling. Second, we incorporate pairwise and away-step\nvariants of Frank-Wolfe into the block-coordinate setting. Third, we cache\noracle calls with a cache-hit criterion based on the block gaps. Fourth, we\nprovide the first method to compute an approximate regularization path for\nSSVM. Finally, we provide an exhaustive empirical evaluation of all our methods\non four structured prediction datasets. \n\n"}
{"id": "1606.00269", "contents": "Title: New Analysis of Linear Convergence of Gradient-type Methods via Unifying\n  Error Bound Conditions Abstract: This paper reveals that a common and central role, played in many error bound\n(EB) conditions and a variety of gradient-type methods, is a residual measure\noperator. On one hand, by linking this operator with other optimality measures,\nwe define a group of abstract EB conditions, and then analyze the interplay\nbetween them; on the other hand, by using this operator as an ascent direction,\nwe propose an abstract gradient-type method, and then derive EB conditions that\nare necessary and sufficient for its linear convergence. The former provides a\nunified framework that not only allows us to find new connections between many\nexisting EB conditions, but also paves a way to construct new EB conditions.\nThe latter allows us to claim the weakest conditions guaranteeing linear\nconvergence for a number of fundamental algorithms, including the gradient\nmethod, the proximal point algorithm, and the forward-backward splitting\nalgorithm. In addition, we show linear convergence for the proximal alternating\nlinearized minimization algorithm under a group of equivalent EB conditions,\nwhich are strictly weaker than the traditional strongly convex condition.\nMoreover, by defining a new EB condition, we show Q-linear convergence of\nNesterov's accelerated forward-backward algorithm without strong convexity.\nFinally, we verify EB conditions for a class of dual objective functions. \n\n"}
{"id": "1606.01583", "contents": "Title: Semi-Supervised Learning with Generative Adversarial Networks Abstract: We extend Generative Adversarial Networks (GANs) to the semi-supervised\ncontext by forcing the discriminator network to output class labels. We train a\ngenerative model G and a discriminator D on a dataset with inputs belonging to\none of N classes. At training time, D is made to predict which of N+1 classes\nthe input belongs to, where an extra class is added to correspond to the\noutputs of G. We show that this method can be used to create a more\ndata-efficient classifier and that it allows for generating higher quality\nsamples than a regular GAN. \n\n"}
{"id": "1606.03463", "contents": "Title: Opportunistic Scheduling over Renewal Systems: An Empirical Method Abstract: This paper considers an opportunistic scheduling problem over a renewal\nsystem. A controller observes a random event at the beginning of each renewal\nframe and then chooses an action in response to the event, which affects the\nduration of the frame, the amount of resources used, and a penalty metric. The\ngoal is to make frame-wise decisions so as to minimize the time average penalty\nsubject to time average resource constraints. This problem has applications to\ntask processing and communication in data networks, as well as to certain\nclasses of Markov decision problems. We formulate the problem as a dynamic\nfractional program and propose an adaptive algorithm which uses an empirical\naccumulation as a feedback parameter. A key feature of the proposed algorithm\nis that it does not require knowledge of the random event statistics and\npotentially allows (uncountably) infinite event sets. We prove the algorithm\nsatisfies all desired constraints and achieves $O(\\epsilon)$ near optimality\nwith probability 1. \n\n"}
{"id": "1606.03954", "contents": "Title: Cross-Gramian-Based Model Reduction: A Comparison Abstract: As an alternative to the popular balanced truncation method, the cross\nGramian matrix induces a class of balancing model reduction techniques. Besides\nthe classical computation of the cross Gramian by a Sylvester matrix equation,\nan empirical cross Gramian can be computed based on simulated trajectories.\nThis work assesses the cross Gramian and its empirical Gramian variant for\nstate-space reduction on a procedural benchmark based to the cross Gramian\nitself. \n\n"}
{"id": "1606.04666", "contents": "Title: The essential role of time in network-based recommendation Abstract: Random walks on bipartite networks have been used extensively to design\npersonalized recommendation methods. While aging has been identified as a key\ncomponent in the growth of information networks, most research has focused on\nthe networks' structural properties and neglected the often available time\ninformation. Time has been largely ignored both by the investigated\nrecommendation methods as well as by the methodology used to evaluate them. We\nshow that this time-unaware approach overestimates the methods' recommendation\nperformance. Motivated by microscopic rules of network growth, we propose a\ntime-aware modification of an existing recommendation method and show that by\ncombining the temporal and structural aspects, it outperforms the existing\nmethods. The performance improvements are particularly striking in systems with\nfast aging. \n\n"}
{"id": "1606.04970", "contents": "Title: The non-convex Burer-Monteiro approach works on smooth semidefinite\n  programs Abstract: Semidefinite programs (SDPs) can be solved in polynomial time by interior\npoint methods, but scalability can be an issue. To address this shortcoming,\nover a decade ago, Burer and Monteiro proposed to solve SDPs with few equality\nconstraints via rank-restricted, non-convex surrogates. Remarkably, for some\napplications, local optimization methods seem to converge to global optima of\nthese non-convex surrogates reliably. Although some theory supports this\nempirical success, a complete explanation of it remains an open question. In\nthis paper, we consider a class of SDPs which includes applications such as\nmax-cut, community detection in the stochastic block model, robust PCA, phase\nretrieval and synchronization of rotations. We show that the low-rank\nBurer--Monteiro formulation of SDPs in that class almost never has any spurious\nlocal optima. \n\n"}
{"id": "1606.05589", "contents": "Title: Human Attention in Visual Question Answering: Do Humans and Deep\n  Networks Look at the Same Regions? Abstract: We conduct large-scale studies on `human attention' in Visual Question\nAnswering (VQA) to understand where humans choose to look to answer questions\nabout images. We design and test multiple game-inspired novel\nattention-annotation interfaces that require the subject to sharpen regions of\na blurred image to answer a question. Thus, we introduce the VQA-HAT (Human\nATtention) dataset. We evaluate attention maps generated by state-of-the-art\nVQA models against human attention both qualitatively (via visualizations) and\nquantitatively (via rank-order correlation). Overall, our experiments show that\ncurrent attention models in VQA do not seem to be looking at the same regions\nas humans. \n\n"}
{"id": "1606.05693", "contents": "Title: Structured Stochastic Linear Bandits Abstract: The stochastic linear bandit problem proceeds in rounds where at each round\nthe algorithm selects a vector from a decision set after which it receives a\nnoisy linear loss parameterized by an unknown vector. The goal in such a\nproblem is to minimize the (pseudo) regret which is the difference between the\ntotal expected loss of the algorithm and the total expected loss of the best\nfixed vector in hindsight. In this paper, we consider settings where the\nunknown parameter has structure, e.g., sparse, group sparse, low-rank, which\ncan be captured by a norm, e.g., $L_1$, $L_{(1,2)}$, nuclear norm. We focus on\nconstructing confidence ellipsoids which contain the unknown parameter across\nall rounds with high-probability. We show the radius of such ellipsoids depend\non the Gaussian width of sets associated with the norm capturing the structure.\nSuch characterization leads to tighter confidence ellipsoids and, therefore,\nsharper regret bounds compared to bounds in the existing literature which are\nbased on the ambient dimensionality. \n\n"}
{"id": "1606.06450", "contents": "Title: Limited Random Walk Algorithm for Big Graph Data Clustering Abstract: Graph clustering is an important technique to understand the relationships\nbetween the vertices in a big graph. In this paper, we propose a novel\nrandom-walk-based graph clustering method. The proposed method restricts the\nreach of the walking agent using an inflation function and a normalization\nfunction. We analyze the behavior of the limited random walk procedure and\npropose a novel algorithm for both global and local graph clustering problems.\nPrevious random-walk-based algorithms depend on the chosen fitness function to\nfind the clusters around a seed vertex. The proposed algorithm tackles the\nproblem in an entirely different manner. We use the limited random walk\nprocedure to find attracting vertices in a graph and use them as features to\ncluster the vertices. According to the experimental results on the simulated\ngraph data and the real-world big graph data, the proposed method is superior\nto the state-of-the-art methods in solving graph clustering problems. Since the\nproposed method uses the embarrassingly parallel paradigm, it can be\nefficiently implemented and embedded in any parallel computing environment such\nas a MapReduce framework. Given enough computing resources, we are capable of\nclustering graphs with millions of vertices and hundreds millions of edges in a\nreasonable time. \n\n"}
{"id": "1606.06510", "contents": "Title: Opportunities for Price Manipulation by Aggregators in Electricity\n  Markets Abstract: Aggregators are playing an increasingly crucial role in the integration of\nrenewable generation in power systems. However, the intermittent nature of\nrenewable generation makes market interactions of aggregators difficult to\nmonitor and regulate, raising concerns about potential market manipulation by\naggregators. In this paper, we study this issue by quantifying the profit an\naggregator can obtain through strategic curtailment of generation in an\nelectricity market. We show that, while the problem of maximizing the benefit\nfrom curtailment is hard in general, efficient algorithms exist when the\ntopology of the network is radial (acyclic). Further, we highlight that\nsignificant increases in profit are possible via strategic curtailment in\npractical settings. \n\n"}
{"id": "1606.09375", "contents": "Title: Convolutional Neural Networks on Graphs with Fast Localized Spectral\n  Filtering Abstract: In this work, we are interested in generalizing convolutional neural networks\n(CNNs) from low-dimensional regular grids, where image, video and speech are\nrepresented, to high-dimensional irregular domains, such as social networks,\nbrain connectomes or words' embedding, represented by graphs. We present a\nformulation of CNNs in the context of spectral graph theory, which provides the\nnecessary mathematical background and efficient numerical schemes to design\nfast localized convolutional filters on graphs. Importantly, the proposed\ntechnique offers the same linear computational complexity and constant learning\ncomplexity as classical CNNs, while being universal to any graph structure.\nExperiments on MNIST and 20NEWS demonstrate the ability of this novel deep\nlearning system to learn local, stationary, and compositional features on\ngraphs. \n\n"}
{"id": "1607.00034", "contents": "Title: Ballpark Learning: Estimating Labels from Rough Group Comparisons Abstract: We are interested in estimating individual labels given only coarse,\naggregated signal over the data points. In our setting, we receive sets\n(\"bags\") of unlabeled instances with constraints on label proportions. We relax\nthe unrealistic assumption of known label proportions, made in previous work;\ninstead, we assume only to have upper and lower bounds, and constraints on bag\ndifferences. We motivate the problem, propose an intuitive formulation and\nalgorithm, and apply our methods to real-world scenarios. Across several\ndomains, we show how using only proportion constraints and no labeled examples,\nwe can achieve surprisingly high accuracy. In particular, we demonstrate how to\npredict income level using rough stereotypes and how to perform sentiment\nanalysis using very little information. We also apply our method to guide\nexploratory analysis, recovering geographical differences in twitter dialect. \n\n"}
{"id": "1607.00076", "contents": "Title: Multi-class classification: mirror descent approach Abstract: We consider the problem of multi-class classification and a stochastic opti-\nmization approach to it. We derive risk bounds for stochastic mirror descent\nalgorithm and provide examples of set geometries that make the use of the\nalgorithm efficient in terms of error in k. \n\n"}
{"id": "1607.00101", "contents": "Title: Randomized block proximal damped Newton method for composite\n  self-concordant minimization Abstract: In this paper we consider the composite self-concordant (CSC) minimization\nproblem, which minimizes the sum of a self-concordant function $f$ and a\n(possibly nonsmooth) proper closed convex function $g$. The CSC minimization is\nthe cornerstone of the path-following interior point methods for solving a\nbroad class of convex optimization problems. It has also found numerous\napplications in machine learning. The proximal damped Newton (PDN) methods have\nbeen well studied in the literature for solving this problem that enjoy a nice\niteration complexity. Given that at each iteration these methods typically\nrequire evaluating or accessing the Hessian of $f$ and also need to solve a\nproximal Newton subproblem, the cost per iteration can be prohibitively high\nwhen applied to large-scale problems. Inspired by the recent success of block\ncoordinate descent methods, we propose a randomized block proximal damped\nNewton (RBPDN) method for solving the CSC minimization. Compared to the PDN\nmethods, the computational cost per iteration of RBPDN is usually significantly\nlower. The computational experiment on a class of regularized logistic\nregression problems demonstrate that RBPDN is indeed promising in solving\nlarge-scale CSC minimization problems. The convergence of RBPDN is also\nanalyzed in the paper. In particular, we show that RBPDN is globally convergent\nwhen $g$ is Lipschitz continuous. It is also shown that RBPDN enjoys a local\nlinear convergence. Moreover, we show that for a class of $g$ including the\ncase where $g$ is Lipschitz differentiable, RBPDN enjoys a global linear\nconvergence. As a striking consequence, it shows that the classical damped\nNewton methods [22,40] and the PDN [31] for such $g$ are globally linearly\nconvergent, which was previously unknown in the literature. Moreover, this\nresult can be used to sharpen the existing iteration complexity of these\nmethods. \n\n"}
{"id": "1607.01575", "contents": "Title: On the steady-state behavior of a nonlinear power system model Abstract: In this article, we consider a dynamic model of a three-phase power system\nincluding nonlinear generator dynamics, transmission line dynamics, and static\nnonlinear loads. We define a synchronous steady-state behavior which\ncorresponds to the desired nominal operating point of a power system and obtain\nnecessary and sufficient conditions on the control inputs, load model, and\ntransmission network, under which the power system admits this steady-state\nbehavior. We arrive at a separation between the steady-state conditions of the\ntransmission network and generators, which allows us to recover the\nsteady-state of the entire power system solely from a prescribed operating\npoint of the transmission network. Moreover, we constructively obtain necessary\nand sufficient steady-state conditions based on network balance equations\ntypically encountered in power flow analysis. Our analysis results in several\nnecessary conditions that any power system control strategy needs to satisfy. \n\n"}
{"id": "1607.01861", "contents": "Title: Numerical Optimization Algorithm of Wavefront Phase Retrieval from\n  Multiple Measurements Abstract: Wavefront phase retrieval from a set of intensity measurements can be\nformulated as an optimization problem. Two nonconvex objective models (MLP and\nits variants LS) based on maximum likelihood estimation are investigated. We\ndevelop numerical optimization algorithms for real-valued function of complex\nvariables and apply them to solve the wavefront phase retrieval problem\nefficiently. Numerical simulation is given with application to three wavefront\nphase retrieval problems. LS model shows better numerical performances than MLP\nmodel. An explanation for this is that the distribution of the eigenvalues of\nHessian matrix of LS model is more clustered than MLP model. LBFGS shows more\nrobust performance and takes fewer calculations than other line search methods. \n\n"}
{"id": "1607.01991", "contents": "Title: Distributed optimal control of a nonstandard nonlocal phase field system\n  with double obstacle potential Abstract: This paper is concerned with a distributed optimal control problem for a\nnonlocal phase field model of Cahn-Hilliard type, which is a nonlocal version\nof a model for two-species phase segregation on an atomic lattice under the\npresence of diffusion. The local model has been investigated in a series of\npapers by P. Podio-Guidugli and the present authors; the nonlocal model studied\nhere consists of a highly nonlinear parabolic equation coupled to an ordinary\ndifferential inclusion of subdifferential type. The inclusion originates from a\nfree energy containing the indicator function of the interval in which the\norder parameter of the phase segregation attains its values. It also contains a\nnonlocal term modeling long-range interactions. Due to the strong nonlinear\ncouplings between the state variables (which even involve products with time\nderivatives), the analysis of the state system is difficult. In addition, the\npresence of the differential inclusion is the reason that standard arguments of\noptimal control theory cannot be applied to guarantee the existence of Lagrange\nmultipliers. In this paper, we employ recent results proved for smooth\nlogarithmic potentials and perform a so-called `deep quench' approximation to\nestablish existence and first-order necessary optimality conditions for the\nnonsmooth case of the double obstacle potential. \n\n"}
{"id": "1607.02130", "contents": "Title: Linear Quadratic Mean Field Type Control and Mean Field Games with\n  Common Noise, with Application to Production of an Exhaustible Resource Abstract: We study a general linear quadratic mean field type control problem and\nconnect it to mean field games of a similar type. The solution is given both in\nterms of a forward/backward system of stochastic differential equations and by\na pair of Riccati equations. In certain cases, the solution to the mean field\ntype control is also the equilibrium strategy for a class of mean field games.\nWe use this fact to study an economic model of production of exhaustible\nresources.\n  Keywords: mean field type control, mean field games, linear-quadratic,\noptimal control, riccati equations, exhaustible resource production \n\n"}
{"id": "1607.04153", "contents": "Title: On the Optimal Management of Public Debt: a Singular Stochastic Control\n  Problem Abstract: Consider the problem of a government that wants to reduce the debt-to-GDP\n(gross domestic product) ratio of a country. The government aims at choosing a\ndebt reduction policy which minimises the total expected cost of having debt,\nplus the total expected cost of interventions on the debt ratio. We model this\nproblem as a singular stochastic control problem over an infinite time-horizon.\nIn a general not necessarily Markovian framework, we first show by\nprobabilistic arguments that the optimal debt reduction policy can be expressed\nin terms of the optimal stopping rule of an auxiliary optimal stopping problem.\nWe then exploit such link to characterise the optimal control in a\ntwo-dimensional Markovian setting in which the state variables are the level of\nthe debt-to-GDP ratio and the current inflation rate of the country. The latter\nfollows uncontrolled Ornstein-Uhlenbeck dynamics and affects the growth rate of\nthe debt ratio. We show that it is optimal for the government to adopt a policy\nthat keeps the debt-to-GDP ratio under an inflation-dependent ceiling. This\ncurve is given in terms of the solution of a nonlinear integral equation\narising in the study of a fully two-dimensional optimal stopping problem. \n\n"}
{"id": "1607.07080", "contents": "Title: Robust ergodicity and tracking in antithetic integral control of\n  stochastic biochemical reaction networks Abstract: Controlling stochastic reactions networks is a challenging problem with\nimportant implications in various fields such as systems and synthetic biology.\nVarious regulation motifs have been discovered or posited over the recent\nyears, the most recent one being the so-called Antithetic Integral Control\n(AIC) motif in Briat et al. (Cell Systems, 2016). Several favorable properties\nfor the AIC motif have been demonstrated for classes of reaction networks that\nsatisfy certain irreducibility, ergodicity and output controllability\nconditions. Here we address the problem of verifying these conditions for large\nsets of reaction networks with fixed topology using two different approaches.\nThe first one is quantitative and relies on the notion of interval matrices\nwhile the second one is qualitative and is based on sign properties of\nmatrices. The obtained results lie in the same spirit as those obtained in\nBriat et al. (Cell Systems, 2016) where properties of reaction networks are\nindependently characterized in terms of control theoretic concepts, linear\nprogramming conditions and graph theoretic conditions. \n\n"}
{"id": "1608.00413", "contents": "Title: Inexact Alternating Minimization Algorithm for Distributed Optimization\n  with an Application to Distributed MPC Abstract: In this paper, we propose the inexact alternating minimization algorithm\n(inexact AMA), which allows inexact iterations in the algorithm, and its\naccelerated variant, called the inexact fast alternating minimization algorithm\n(inexact FAMA). We show that inexact AMA and inexact FAMA are equivalent to the\ninexact proximal-gradient method and its accelerated variant applied to the\ndual problem. Based on this equivalence, we derive complexity upper-bounds on\nthe number of iterations for the inexact algorithms. We apply inexact AMA and\ninexact FAMA to distributed optimization problems, with an emphasis on\ndistributed MPC applications, and show the convergence properties for this\nspecial case. By employing the complexity upper-bounds on the number of\niterations, we provide sufficient conditions on the inexact iterations for the\nconvergence of the algorithms. We further study the special case of quadratic\nlocal objectives in the distributed optimization problems, which is a standard\nform in distributed MPC. For this special case, we allow local computational\nerrors at each iteration. By exploiting a warm-starting strategy and the\nsufficient conditions on the errors for convergence, we propose an approach to\ncertify the number of iterations for solving local problems, which guarantees\nthat the local computational errors satisfy the sufficient conditions and the\ninexact distributed optimization algorithm converges to the optimal solution. \n\n"}
{"id": "1608.00635", "contents": "Title: Optimal Placement of Dynamic Var Sources by Using Empirical\n  Controllability Covariance Abstract: In this paper, the empirical controllability covariance (ECC), which is\ncalculated around the considered operating condition of a power system, is\napplied to quantify the degree of controllability of system voltages under\nspecific dynamic var source locations. An optimal dynamic var source placement\nmethod addressing fault-induced delayed voltage recovery (FIDVR) issues is\nfurther formulated as an optimization problem that maximizes the determinant of\nECC. The optimization problem is effectively solved by the NOMAD solver, which\nimplements the Mesh Adaptive Direct Search algorithm. The proposed method is\ntested on an NPCC 140-bus system and the results show that the proposed method\nwith fault specified ECC can solve the FIDVR issue caused by the most severe\ncontingency with fewer dynamic var sources than the Voltage Sensitivity Index\n(VSI) based method. The proposed method with fault unspecified ECC does not\ndepend on the settings of the contingency and can address more FIDVR issues\nthan VSI method when placing the same number of SVCs under different fault\ndurations. It is also shown that the proposed method can help mitigate voltage\ncollapse. \n\n"}
{"id": "1608.00663", "contents": "Title: Simulation Optimization of Risk Measures with Adaptive Risk Levels Abstract: Optimizing risk measures such as Value-at-Risk (VaR) and Conditional\nValue-at-Risk (CVaR) of a general loss distribution is usually difficult,\nbecause 1) the loss function might lack structural properties such as convexity\nor differentiability since it is often generated via black-box simulation of a\nstochastic system; 2) evaluation of risk measures often requires rare-event\nsimulation, which is computationally expensive. In this paper, we study the\nextension of the recently proposed gradient-based adaptive stochastic search\n(GASS) to the optimization of risk measures VaR and CVaR. Instead of optimizing\nVaR or CVaR at the target risk level directly, we incorporate an adaptive\nupdating scheme on the risk level, by initializing the algorithm at a small\nrisk level and adaptively increasing it until the target risk level is achieved\nwhile the algorithm converges at the same time. This enables us to adaptively\nreduce the number of samples required to estimate the risk measure at each\niteration, and thus improving the overall efficiency of the algorithm. \n\n"}
{"id": "1608.00704", "contents": "Title: Identifiable Phenotyping using Constrained Non-Negative Matrix\n  Factorization Abstract: This work proposes a new algorithm for automated and simultaneous phenotyping\nof multiple co-occurring medical conditions, also referred as comorbidities,\nusing clinical notes from the electronic health records (EHRs). A basic latent\nfactor estimation technique of non-negative matrix factorization (NMF) is\naugmented with domain specific constraints to obtain sparse latent factors that\nare anchored to a fixed set of chronic conditions. The proposed anchoring\nmechanism ensures a one-to-one identifiable and interpretable mapping between\nthe latent factors and the target comorbidities. Qualitative assessment of the\nempirical results by clinical experts suggests that the proposed model learns\nclinically interpretable phenotypes while being predictive of 30 day mortality.\nThe proposed method can be readily adapted to any non-negative EHR data across\nvarious healthcare institutions. \n\n"}
{"id": "1608.00708", "contents": "Title: Detection of money laundering groups using supervised learning in\n  networks Abstract: Money laundering is a major global problem, enabling criminal organisations\nto hide their ill-gotten gains and to finance further operations. Prevention of\nmoney laundering is seen as a high priority by many governments, however\ndetection of money laundering without prior knowledge of predicate crimes\nremains a significant challenge. Previous detection systems have tended to\nfocus on individuals, considering transaction histories and applying anomaly\ndetection to identify suspicious behaviour. However, money laundering involves\ngroups of collaborating individuals, and evidence of money laundering may only\nbe apparent when the collective behaviour of these groups is considered. In\nthis paper we describe a detection system that is capable of analysing group\nbehaviour, using a combination of network analysis and supervised learning.\nThis system is designed for real-world application and operates on networks\nconsisting of millions of interacting parties. Evaluation of the system using\nreal-world data indicates that suspicious activity is successfully detected.\nImportantly, the system exhibits a low rate of false positives, and is\ntherefore suitable for use in a live intelligence environment. \n\n"}
{"id": "1608.01771", "contents": "Title: Community Detection in Political Twitter Networks using Nonnegative\n  Matrix Factorization Methods Abstract: Community detection is a fundamental task in social network analysis. In this\npaper, first we develop an endorsement filtered user connectivity network by\nutilizing Heider's structural balance theory and certain Twitter triad\npatterns. Next, we develop three Nonnegative Matrix Factorization frameworks to\ninvestigate the contributions of different types of user connectivity and\ncontent information in community detection. We show that user content and\nendorsement filtered connectivity information are complementary to each other\nin clustering politically motivated users into pure political communities. Word\nusage is the strongest indicator of users' political orientation among all\ncontent categories. Incorporating user-word matrix and word similarity\nregularizer provides the missing link in connectivity only methods which suffer\nfrom detection of artificially large number of clusters for Twitter networks. \n\n"}
{"id": "1608.01976", "contents": "Title: Kernel Ridge Regression via Partitioning Abstract: In this paper, we investigate a divide and conquer approach to Kernel Ridge\nRegression (KRR). Given n samples, the division step involves separating the\npoints based on some underlying disjoint partition of the input space (possibly\nvia clustering), and then computing a KRR estimate for each partition. The\nconquering step is simple: for each partition, we only consider its own local\nestimate for prediction. We establish conditions under which we can give\ngeneralization bounds for this estimator, as well as achieve optimal minimax\nrates. We also show that the approximation error component of the\ngeneralization error is lesser than when a single KRR estimate is fit on the\ndata: thus providing both statistical and computational advantages over a\nsingle KRR estimate over the entire data (or an averaging over random\npartitions as in other recent work, [30]). Lastly, we provide experimental\nvalidation for our proposed estimator and our assumptions. \n\n"}
{"id": "1608.02240", "contents": "Title: The forward-backward algorithm and the normal problem Abstract: The forward-backward splitting technique is a popular method for solving\nmonotone inclusions that has applications in optimization. In this paper we\nexplore the behaviour of the algorithm when the inclusion problem has no\nsolution. We present a new formula to define the normal solutions using the\nforward-backward operator. We also provide a formula for the range of the\ndisplacement map of the forward-backward operator. Several examples illustrate\nour theory. \n\n"}
{"id": "1608.03008", "contents": "Title: Network Topology Inference from Spectral Templates Abstract: We address the problem of identifying a graph structure from the observation\nof signals defined on its nodes. Fundamentally, the unknown graph encodes\ndirect relationships between signal elements, which we aim to recover from\nobservable indirect relationships generated by a diffusion process on the\ngraph. The fresh look advocated here permeates benefits from convex\noptimization and stationarity of graph signals, in order to identify the graph\nshift operator (a matrix representation of the graph) given only its\neigenvectors. These spectral templates can be obtained, e.g., from the sample\ncovariance of independent graph signals diffused on the sought network. The\nnovel idea is to find a graph shift that, while being consistent with the\nprovided spectral information, endows the network with certain desired\nproperties such as sparsity. To that end we develop efficient inference\nalgorithms stemming from provably-tight convex relaxations of natural nonconvex\ncriteria, particularizing the results for two shifts: the adjacency matrix and\nthe normalized Laplacian. Algorithms and theoretical recovery conditions are\ndeveloped not only when the templates are perfectly known, but also when the\neigenvectors are noisy or when only a subset of them are given. Numerical tests\nshowcase the effectiveness of the proposed algorithms in recovering social,\nbrain, and amino-acid networks. \n\n"}
{"id": "1608.03928", "contents": "Title: Hybrid Jacobian and Gauss-Seidel proximal block coordinate update\n  methods for linearly constrained convex programming Abstract: Recent years have witnessed the rapid development of block coordinate update\n(BCU) methods, which are particularly suitable for problems involving\nlarge-sized data and/or variables. In optimization, BCU first appears as the\ncoordinate descent method that works well for smooth problems or those with\nseparable nonsmooth terms and/or separable constraints. As nonseparable\nconstraints exist, BCU can be applied under primal-dual settings.\n  In the literature, it has been shown that for weakly convex problems with\nnonseparable linear constraint, BCU with fully Gauss-Seidel updating rule may\nfail to converge and that with fully Jacobian rule can converge sublinearly.\nHowever, empirically the method with Jacobian update is usually slower than\nthat with Gauss-Seidel rule. To maintain their advantages, we propose a hybrid\nJacobian and Gauss-Seidel BCU method for solving linearly constrained\nmulti-block structured convex programming, where the objective may have a\nnonseparable quadratic term and separable nonsmooth terms. At each primal block\nvariable update, the method approximates the augmented Lagrangian function at\nan affine combination of the previous two iterates, and the affinely mixing\nmatrix with desired nice properties can be chosen through solving a\nsemidefinite programming. We show that the hybrid method enjoys the theoretical\nconvergence guarantee as Jacobian BCU. In addition, we numerically demonstrate\nthat the method can perform as well as Gauss-Seidel method and better than a\nrecently proposed randomized primal-dual BCU method. \n\n"}
{"id": "1608.04168", "contents": "Title: Low-Rank Matrix Completion using Nuclear Norm with Facial Reduction Abstract: Minimization of the nuclear norm is often used as a surrogate, convex\nrelaxation, for finding the minimum rank completion (recovery) of a partial\nmatrix. The minimum nuclear norm problem can be solved as a trace minimization\nsemidefinite programming problem, (SDP). The SDP and its dual are regular in\nthe sense that they both satisfy strict feasibility. Interior point algorithms\nare the current methods of choice for these problems. This means that it is\ndifficult to solve large scale problems and difficult to get high accuracy\nsolutions.\n  In this paper we take advantage of the structure at optimality for the\nminimum nuclear norm problem. We show that even though strict feasibility\nholds, the facial reduction framework can be successfully applied to obtain a\nproper face that contains the optimal set, and thus can dramatically reduce the\nsize of the final nuclear norm problem while guaranteeing a low-rank solution.\nWe include numerical tests for both exact and noisy cases. In all cases we\nassume that knowledge of a target rank is available. \n\n"}
{"id": "1608.06135", "contents": "Title: Resolution of ranking hierarchies in directed networks Abstract: Identifying hierarchies and rankings of nodes in directed graphs is\nfundamental in many applications such as social network analysis, biology,\neconomics, and finance. A recently proposed method identifies the hierarchy by\nfinding the ordered partition of nodes which minimises a score function, termed\nagony. This function penalises the links violating the hierarchy in a way\ndepending on the strength of the violation. To investigate the resolution of\nranking hierarchies we introduce an ensemble of random graphs, the Ranked\nStochastic Block Model. We find that agony may fail to identify hierarchies\nwhen the structure is not strong enough and the size of the classes is small\nwith respect to the whole network. We analytically characterise the resolution\nthreshold and we show that an iterated version of agony can partly overcome\nthis resolution limit. \n\n"}
{"id": "1608.07630", "contents": "Title: Global analysis of Expectation Maximization for mixtures of two\n  Gaussians Abstract: Expectation Maximization (EM) is among the most popular algorithms for\nestimating parameters of statistical models. However, EM, which is an iterative\nalgorithm based on the maximum likelihood principle, is generally only\nguaranteed to find stationary points of the likelihood objective, and these\npoints may be far from any maximizer. This article addresses this disconnect\nbetween the statistical principles behind EM and its algorithmic properties.\nSpecifically, it provides a global analysis of EM for specific models in which\nthe observations comprise an i.i.d. sample from a mixture of two Gaussians.\nThis is achieved by (i) studying the sequence of parameters from idealized\nexecution of EM in the infinite sample limit, and fully characterizing the\nlimit points of the sequence in terms of the initial parameters; and then (ii)\nbased on this convergence analysis, establishing statistical consistency (or\nlack thereof) for the actual sequence of parameters produced by EM. \n\n"}
{"id": "1608.07701", "contents": "Title: Proximal methods for stationary Mean Field Games with local couplings Abstract: We address the numerical approximation of Mean Field Games with local\ncouplings. For power-like Hamiltonians, we consider both unconstrained and\nconstrained stationary systems with density constraints in order to model hard\ncongestion effects. For finite difference discretizations of the Mean Field\nGame system, we follow a variational approach. We prove that the aforementioned\nschemes can be obtained as the optimality system of suitably defined\noptimization problems. In order to prove the existence of solutions of the\nscheme with a variational argument, the monotonicity of the coupling term is\nnot used, which allow us to recover general existence results. Next, assuming\nnext that the coupling term is monotone, the variational problem is cast as a\nconvex optimization problem for which we study and compare several proximal\ntype methods. These algorithms have several interesting features, such as\nglobal convergence and stability with respect to the viscosity parameter, which\ncan eventually be zero. We assess the performance of the methods via numerical\nexperiments. \n\n"}
{"id": "1608.08883", "contents": "Title: A first-order primal-dual algorithm with linesearch Abstract: The paper proposes a linesearch for a primal-dual method. Each iteration of\nthe linesearch requires to update only the dual (or primal) variable. For many\nproblems, in particular for regularized least squares, the linesearch does not\nrequire any additional matrix-vector multiplications. We prove convergence of\nthe proposed method under standard assumptions. We also show an ergodic\n$O(1/N)$ rate of convergence for our method. In case one or both of the\nprox-functions are strongly convex, we modify our basic method to get a better\nconvergence rate. Finally, we propose a linesearch for a saddle point problem\nwith an additional smooth term. Several numerical experiments confirm the\nefficiency of our proposed methods. \n\n"}
{"id": "1609.00951", "contents": "Title: A Unified Convergence Analysis of the Multiplicative Update Algorithm\n  for Regularized Nonnegative Matrix Factorization Abstract: The multiplicative update (MU) algorithm has been extensively used to\nestimate the basis and coefficient matrices in nonnegative matrix factorization\n(NMF) problems under a wide range of divergences and regularizers. However,\ntheoretical convergence guarantees have only been derived for a few special\ndivergences without regularization. In this work, we provide a conceptually\nsimple, self-contained, and unified proof for the convergence of the MU\nalgorithm applied on NMF with a wide range of divergences and regularizers. Our\nmain result shows the sequence of iterates (i.e., pairs of basis and\ncoefficient matrices) produced by the MU algorithm converges to the set of\nstationary points of the non-convex NMF optimization problem. Our proof\nstrategy has the potential to open up new avenues for analyzing similar\nproblems in machine learning and signal processing. \n\n"}
{"id": "1609.00978", "contents": "Title: Local Maxima in the Likelihood of Gaussian Mixture Models: Structural\n  Results and Algorithmic Consequences Abstract: We provide two fundamental results on the population (infinite-sample)\nlikelihood function of Gaussian mixture models with $M \\geq 3$ components. Our\nfirst main result shows that the population likelihood function has bad local\nmaxima even in the special case of equally-weighted mixtures of well-separated\nand spherical Gaussians. We prove that the log-likelihood value of these bad\nlocal maxima can be arbitrarily worse than that of any global optimum, thereby\nresolving an open question of Srebro (2007). Our second main result shows that\nthe EM algorithm (or a first-order variant of it) with random initialization\nwill converge to bad critical points with probability at least\n$1-e^{-\\Omega(M)}$. We further establish that a first-order variant of EM will\nnot converge to strict saddle points almost surely, indicating that the poor\nperformance of the first-order method can be attributed to the existence of bad\nlocal maxima rather than bad saddle points. Overall, our results highlight the\nnecessity of careful initialization when using the EM algorithm in practice,\neven when applied in highly favorable settings. \n\n"}
{"id": "1609.02247", "contents": "Title: Demixing Sines and Spikes: Robust Spectral Super-resolution in the\n  Presence of Outliers Abstract: We consider the problem of super-resolving the line spectrum of a\nmultisinusoidal signal from a finite number of samples, some of which may be\ncompletely corrupted. Measurements of this form can be modeled as an additive\nmixture of a sinusoidal and a sparse component. We propose to demix the two\ncomponents and super-resolve the spectrum of the multisinusoidal signal by\nsolving a convex program. Our main theoretical result is that-- up to\nlogarithmic factors-- this approach is guaranteed to be successful with high\nprobability for a number of spectral lines that is linear in the number of\nmeasurements, even if a constant fraction of the data are outliers. The result\nholds under the assumption that the phases of the sinusoidal and sparse\ncomponents are random and the line spectrum satisfies a minimum-separation\ncondition. We show that the method can be implemented via semidefinite\nprogramming and explain how to adapt it in the presence of dense perturbations,\nas well as exploring its connection to atomic-norm denoising. In addition, we\npropose a fast greedy demixing method which provides good empirical results\nwhen coupled with a local nonconvex-optimization step. \n\n"}
{"id": "1609.02654", "contents": "Title: Optimal Disease Outbreak Detection in a Community Using Network\n  Observability Abstract: Given a network, we would like to determine which subset of nodes should be\nmeasured by limited sensing facilities to maximize information about the entire\nnetwork. The optimal choice corresponds to the configuration that returns the\nhighest value of a measure of observability of the system. Here, the\ndeterminant of the inverse of the observability Gramian is used to evaluate the\ndegree of observability. Additionally, the effects of changes in the topology\nof the corresponding graph of a network on the observability of the network are\ninvestigated. The theory is illustrated on the problem of detection of an\nepidemic disease in a community. The purpose here is to find the smallest\nnumber of people who must be examined to predict the number of infected people\nin an arbitrary community. Results are demonstrated in simulation. \n\n"}
{"id": "1609.04903", "contents": "Title: Ensemble-Based Algorithms to Detect Disjoint and Overlapping Communities\n  in Networks Abstract: Given a set ${\\cal AL}$ of community detection algorithms and a graph $G$ as\ninputs, we propose two ensemble methods $\\mathtt{EnDisCO}$ and $\\mathtt{MeDOC}$\nthat (respectively) identify disjoint and overlapping communities in $G$.\n$\\mathtt{EnDisCO}$ transforms a graph into a latent feature space by leveraging\nmultiple base solutions and discovers disjoint community structure.\n$\\mathtt{MeDOC}$ groups similar base communities into a meta-community and\ndetects both disjoint and overlapping community structures. Experiments are\nconducted at different scales on both synthetically generated networks as well\nas on several real-world networks for which the underlying ground-truth\ncommunity structure is available. Our extensive experiments show that both\nalgorithms outperform state-of-the-art non-ensemble algorithms by a significant\nmargin. Moreover, we compare $\\mathtt{EnDisCO}$ and $\\mathtt{MeDOC}$ with a\nrecent ensemble method for disjoint community detection and show that our\napproaches achieve superior performance. To the best of our knowledge,\n$\\mathtt{MeDOC}$ is the first ensemble approach for overlapping community\ndetection. \n\n"}
{"id": "1609.05573", "contents": "Title: Optimality and Sub-optimality of PCA for Spiked Random Matrices and\n  Synchronization Abstract: A central problem of random matrix theory is to understand the eigenvalues of\nspiked random matrix models, in which a prominent eigenvector is planted into a\nrandom matrix. These distributions form natural statistical models for\nprincipal component analysis (PCA) problems throughout the sciences. Baik, Ben\nArous and P\\'ech\\'e showed that the spiked Wishart ensemble exhibits a sharp\nphase transition asymptotically: when the signal strength is above a critical\nthreshold, it is possible to detect the presence of a spike based on the top\neigenvalue, and below the threshold the top eigenvalue provides no information.\nSuch results form the basis of our understanding of when PCA can detect a\nlow-rank signal in the presence of noise.\n  However, not all the information about the spike is necessarily contained in\nthe spectrum. We study the fundamental limitations of statistical methods,\nincluding non-spectral ones. Our results include:\n  I) For the Gaussian Wigner ensemble, we show that PCA achieves the optimal\ndetection threshold for a variety of benign priors for the spike. We extend\nprevious work on the spherically symmetric and i.i.d. Rademacher priors through\nan elementary, unified analysis.\n  II) For any non-Gaussian Wigner ensemble, we show that PCA is always\nsuboptimal for detection. However, a variant of PCA achieves the optimal\nthreshold (for benign priors) by pre-transforming the matrix entries according\nto a carefully designed function. This approach has been stated before, and we\ngive a rigorous and general analysis.\n  III) For both the Gaussian Wishart ensemble and various synchronization\nproblems over groups, we show that inefficient procedures can work below the\nthreshold where PCA succeeds, whereas no known efficient algorithm achieves\nthis. This conjectural gap between what is statistically possible and what can\nbe done efficiently remains open. \n\n"}
{"id": "1609.06006", "contents": "Title: Synchronization, Consensus of Complex Networks and their relationships Abstract: In this paper, we focus on the topic Synchronization and consensus of Complex\nNetworks and their relationships. It is revealed that two topics are closely\nrelating to each other and all results given in \\cite{Li} can be obtained by\nthe results in \\cite{Lu2006}. \n\n"}
{"id": "1609.06331", "contents": "Title: Max-affine estimators for convex stochastic programming Abstract: In this paper, we consider two sequential decision making problems with a\nconvexity structure, namely an energy storage optimization task and a\nmulti-product assembly example. We formulate these problems in the stochastic\nprogramming framework and discuss an approximate dynamic programming technique\nfor their solutions. As the cost-to-go functions are convex in these cases, we\nuse max-affine estimates for their approximations. To train such a max-affine\nestimate, we provide a new convex regression algorithm, and evaluate it\nempirically for these planning scenarios. \n\n"}
{"id": "1609.06390", "contents": "Title: Learning HMMs with Nonparametric Emissions via Spectral Decompositions\n  of Continuous Matrices Abstract: Recently, there has been a surge of interest in using spectral methods for\nestimating latent variable models. However, it is usually assumed that the\ndistribution of the observations conditioned on the latent variables is either\ndiscrete or belongs to a parametric family. In this paper, we study the\nestimation of an $m$-state hidden Markov model (HMM) with only smoothness\nassumptions, such as H\\\"olderian conditions, on the emission densities. By\nleveraging some recent advances in continuous linear algebra and numerical\nanalysis, we develop a computationally efficient spectral algorithm for\nlearning nonparametric HMMs. Our technique is based on computing an SVD on\nnonparametric estimates of density functions by viewing them as\n\\emph{continuous matrices}. We derive sample complexity bounds via\nconcentration results for nonparametric density estimation and novel\nperturbation theory results for continuous matrices. We implement our method\nusing Chebyshev polynomial approximations. Our method is competitive with other\nbaselines on synthetic and real problems and is also very computationally\nefficient. \n\n"}
{"id": "1609.06804", "contents": "Title: Decoupled Asynchronous Proximal Stochastic Gradient Descent with\n  Variance Reduction Abstract: In the era of big data, optimizing large scale machine learning problems\nbecomes a challenging task and draws significant attention. Asynchronous\noptimization algorithms come out as a promising solution. Recently, decoupled\nasynchronous proximal stochastic gradient descent (DAP-SGD) is proposed to\nminimize a composite function. It is claimed to be able to off-loads the\ncomputation bottleneck from server to workers by allowing workers to evaluate\nthe proximal operators, therefore, server just need to do element-wise\noperations. However, it still suffers from slow convergence rate because of the\nvariance of stochastic gradient is nonzero. In this paper, we propose a faster\nmethod, decoupled asynchronous proximal stochastic variance reduced gradient\ndescent method (DAP-SVRG). We prove that our method has linear convergence for\nstrongly convex problem. Large-scale experiments are also conducted in this\npaper, and results demonstrate our theoretical analysis. \n\n"}
{"id": "1609.07199", "contents": "Title: Critical Multipliers in Variational Systems via Second-order Generalized\n  Differentiation Abstract: In this paper we introduce the notions of critical and noncritical\nmultipliers for subdifferential variational systems extending to a general\nframework the corresponding notions by Izmailov and Solodov developed for\nclassical Karush-Kuhn-Tucker (KKT) systems. It has been well recognized that\ncritical multipliers are largely responsible for slow convergence of major\nprimal-dual algorithms of optimization. The approach of this paper allows us to\ncover KKT systems arising in various classes of smooth and nonsmooth problems\nof constrained optimization including composite optimization, minimax problems,\netc. Concentrating on a polyhedral subdifferential case and employing recent\nresults of second-order subdifferential theory, we obtain complete\ncharacterizations of critical and noncritical multipliers via the problem data.\nIt is shown that noncriticality is equivalent to a certain error bound for a\nperturbed variational system and that critical multipliers can be ruled out by\nfull stability of local minimizers in problems of composite optimization. For\nthe latter class we establish the equivalence between noncriticality of\nmultipliers and robust isolated calmness of the associated solution map and\nthen derive explicit characterizations of these notions via appropriate\nsecond-order sufficient conditions. It is finally proved that the\nLipschitz-like/Aubin property of solution maps yields their robust isolated\ncalmness. \n\n"}
{"id": "1609.08397", "contents": "Title: Generalization Error Bounds for Optimization Algorithms via Stability Abstract: Many machine learning tasks can be formulated as Regularized Empirical Risk\nMinimization (R-ERM), and solved by optimization algorithms such as gradient\ndescent (GD), stochastic gradient descent (SGD), and stochastic variance\nreduction (SVRG). Conventional analysis on these optimization algorithms\nfocuses on their convergence rates during the training process, however, people\nin the machine learning community may care more about the generalization\nperformance of the learned model on unseen test data. In this paper, we\ninvestigate on this issue, by using stability as a tool. In particular, we\ndecompose the generalization error for R-ERM, and derive its upper bound for\nboth convex and non-convex cases. In convex cases, we prove that the\ngeneralization error can be bounded by the convergence rate of the optimization\nalgorithm and the stability of the R-ERM process, both in expectation (in the\norder of $\\mathcal{O}((1/n)+\\mathbb{E}\\rho(T))$, where $\\rho(T)$ is the\nconvergence error and $T$ is the number of iterations) and in high probability\n(in the order of\n$\\mathcal{O}\\left(\\frac{\\log{1/\\delta}}{\\sqrt{n}}+\\rho(T)\\right)$ with\nprobability $1-\\delta$). For non-convex cases, we can also obtain a similar\nexpected generalization error bound. Our theorems indicate that 1) along with\nthe training process, the generalization error will decrease for all the\noptimization algorithms under our investigation; 2) Comparatively speaking,\nSVRG has better generalization ability than GD and SGD. We have conducted\nexperiments on both convex and non-convex problems, and the experimental\nresults verify our theoretical findings. \n\n"}
{"id": "1609.09530", "contents": "Title: Fast L1-L2 minimization via a proximal operator Abstract: This paper aims to develop new and fast algorithms for recovering a sparse\nvector from a small number of measurements, which is a fundamental problem in\nthe field of compressive sensing (CS). Currently, CS favors incoherent systems,\nin which any two measurements are as little correlated as possible. In reality,\nhowever, many problems are coherent, and conventional methods such as $L_1$\nminimization do not work well. Recently, the difference of the $L_1$ and $L_2$\nnorms, denoted as $L_1$-$L_2$, is shown to have superior performance over the\nclassic $L_1$ method, but it is computationally expensive. We derive an\nanalytical solution for the proximal operator of the $L_1$-$L_2$ metric, and it\nmakes some fast $L_1$ solvers such as forward-backward splitting (FBS) and\nalternating direction method of multipliers (ADMM) applicable for $L_1$-$L_2$.\nWe describe in details how to incorporate the proximal operator into FBS and\nADMM and show that the resulting algorithms are convergent under mild\nconditions. Both algorithms are shown to be much more efficient than the\noriginal implementation of $L_1$-$L_2$ based on a difference-of-convex approach\nin the numerical experiments. \n\n"}
{"id": "1610.00246", "contents": "Title: HNP3: A Hierarchical Nonparametric Point Process for Modeling Content\n  Diffusion over Social Media Abstract: This paper introduces a novel framework for modeling temporal events with\ncomplex longitudinal dependency that are generated by dependent sources. This\nframework takes advantage of multidimensional point processes for modeling time\nof events. The intensity function of the proposed process is a mixture of\nintensities, and its complexity grows with the complexity of temporal patterns\nof data. Moreover, it utilizes a hierarchical dependent nonparametric approach\nto model marks of events. These capabilities allow the proposed model to adapt\nits temporal and topical complexity according to the complexity of data, which\nmakes it a suitable candidate for real world scenarios. An online inference\nalgorithm is also proposed that makes the framework applicable to a vast range\nof applications. The framework is applied to a real world application, modeling\nthe diffusion of contents over networks. Extensive experiments reveal the\neffectiveness of the proposed framework in comparison with state-of-the-art\nmethods. \n\n"}
{"id": "1610.02501", "contents": "Title: Revisiting Multiple Instance Neural Networks Abstract: Recently neural networks and multiple instance learning are both attractive\ntopics in Artificial Intelligence related research fields. Deep neural networks\nhave achieved great success in supervised learning problems, and multiple\ninstance learning as a typical weakly-supervised learning method is effective\nfor many applications in computer vision, biometrics, nature language\nprocessing, etc. In this paper, we revisit the problem of solving multiple\ninstance learning problems using neural networks. Neural networks are appealing\nfor solving multiple instance learning problem. The multiple instance neural\nnetworks perform multiple instance learning in an end-to-end way, which take a\nbag with various number of instances as input and directly output bag label.\nAll of the parameters in a multiple instance network are able to be optimized\nvia back-propagation. We propose a new multiple instance neural network to\nlearn bag representations, which is different from the existing multiple\ninstance neural networks that focus on estimating instance label. In addition,\nrecent tricks developed in deep learning have been studied in multiple instance\nnetworks, we find deep supervision is effective for boosting bag classification\naccuracy. In the experiments, the proposed multiple instance networks achieve\nstate-of-the-art or competitive performance on several MIL benchmarks.\nMoreover, it is extremely fast for both testing and training, e.g., it takes\nonly 0.0003 second to predict a bag and a few seconds to train on a MIL\ndatasets on a moderate CPU. \n\n"}
{"id": "1610.03303", "contents": "Title: The Phase Transition in 5 Point Energy Minimization Abstract: Let R_s(r)=sign(s)/r^s be the Riesz s-energy potential. (This is the usual\npower-law potential.) This monograph proves the existence of a computable\nnumber S=15.048... such that the triangular bi-pyramid is the unique minimizer\nwith respect to R_s, amongst all 5-point configurations on the sphere, if and\nonly if s lies in (-2,0) or (0,S). This establishes the existence of the\nlong-conjectured phase transition constant in 5-point energy minimization. \n\n"}
{"id": "1610.04574", "contents": "Title: Generalization Error of Invariant Classifiers Abstract: This paper studies the generalization error of invariant classifiers. In\nparticular, we consider the common scenario where the classification task is\ninvariant to certain transformations of the input, and that the classifier is\nconstructed (or learned) to be invariant to these transformations. Our approach\nrelies on factoring the input space into a product of a base space and a set of\ntransformations. We show that whereas the generalization error of a\nnon-invariant classifier is proportional to the complexity of the input space,\nthe generalization error of an invariant classifier is proportional to the\ncomplexity of the base space. We also derive a set of sufficient conditions on\nthe geometry of the base space and the set of transformations that ensure that\nthe complexity of the base space is much smaller than the complexity of the\ninput space. Our analysis applies to general classifiers such as convolutional\nneural networks. We demonstrate the implications of the developed theory for\nsuch classifiers with experiments on the MNIST and CIFAR-10 datasets. \n\n"}
{"id": "1610.06410", "contents": "Title: The convergence problem in mean field games with a local coupling Abstract: The paper studies the convergence, as $N$ tends to infinity, of a system of\n$N$ coupled Hamilton-Jacobi equations (the Nash system) when the coupling\nbetween the players becomes increasingly singular. The limit equation is a mean\nfield game system with local coupling. \n\n"}
{"id": "1610.06538", "contents": "Title: A general double-proximal gradient algorithm for d.c. programming Abstract: The possibilities of exploiting the special structure of d.c. programs, which\nconsist of optimizing the difference of convex functions, are currently more or\nless limited to variants of the DCA proposed by Pham Dinh Tao and Le Thi Hoai\nAn in 1997. These assume that either the convex or the concave part, or both,\nare evaluated by one of their subgradients.\n  In this paper we propose an algorithm which allows the evaluation of both the\nconcave and the convex part by their proximal points. Additionally, we allow a\nsmooth part, which is evaluated via its gradient. In the spirit of primal-dual\nsplitting algorithms, the concave part might be the composition of a concave\nfunction with a linear operator, which are, however, evaluated separately.\n  For this algorithm we show that every cluster point is a solution of the\noptimization problem. Furthermore, we show the connection to the Toland dual\nproblem and prove a descent property for the objective function values of a\nprimal-dual formulation of the problem. Convergence of the iterates is shown if\nthis objective function satisfies the Kurdyka--\\L ojasiewicz property. In the\nlast part, we apply the algorithm to an image processing model. \n\n"}
{"id": "1610.07556", "contents": "Title: On the set of points of smoothness for the value function for affine\n  optimal control problems Abstract: We study the regularity properties of the value function associated with an\naffine optimal control problem with quadratic cost plus a potential, for a\nfixed final time and initial point. Without assuming any condition on singular\nminimizers, we prove that the value function is continuous on an open and dense\nsubset of the interior of the attainable set. As a byproduct we obtain that it\nis actually smooth on a possibly smaller set, still open and dense. \n\n"}
{"id": "1611.00683", "contents": "Title: Improving variational methods via pairwise linear response identities Abstract: Inference methods are often formulated as variational approximations: these\napproximations allow easy evaluation of statistics by marginalization or linear\nresponse, but these estimates can be inconsistent. We show that by introducing\nconstraints on covariance, one can ensure consistency of linear response with\nthe variational parameters, and in so doing inference of marginal probability\ndistributions is improved. For the Bethe approximation and its generalizations,\nimprovements are achieved with simple choices of the constraints. The\napproximations are presented as variational frameworks; iterative procedures\nrelated to message passing are provided for finding the minima. \n\n"}
{"id": "1611.03537", "contents": "Title: Linear predictors for nonlinear dynamical systems: Koopman operator\n  meets model predictive control Abstract: This paper presents a class of linear predictors for nonlinear controlled\ndynamical systems. The basic idea is to lift the nonlinear dynamics into a\nhigher dimensional space where its evolution is approximately linear. In an\nuncontrolled setting, this procedure amounts to a numerical approximation of\nthe Koopman operator associated to the nonlinear dynamics. In this work, we\nextend the Koopman operator to controlled dynamical systems and compute a\nfinite-dimensional approximation of the operator in such a way that this\napproximation has the form a linear controlled dynamical system. In numerical\nexamples, the linear predictors obtained in this way exhibit a performance\nsuperior to existing linear predictors such as those based on local\nlinearization or the so-called Carleman linearization. Importantly, the\nprocedure to construct these linear predictors is completely data-driven and\nextremely simple -- it boils down to a nonlinear transformation of the data\n(the lifting) and a linear least squares problem in the lifted space that can\nbe readily solved for large data sets. These linear predictors can be readily\nused to design controllers for the nonlinear dynamical system using linear\ncontroller design methodologies. We focus in particular on model predictive\ncontrol (MPC) and show that MPC controllers designed in this way enjoy\ncomputational complexity of the underlying optimization problem comparable to\nthat of MPC for a linear dynamical system of the same size. Importantly, linear\ninequality constraints on the state and control inputs as well as nonlinear\nconstraints on the state can be imposed in a linear fashion in the proposed MPC\nscheme. Similarly, cost functions nonlinear in the state variable can be\nhandled in a linear fashion. Numerical examples (including a high-dimensional\nnonlinear PDE control) demonstrate the approach with the source code available\nonline. \n\n"}
{"id": "1611.03988", "contents": "Title: A Boltzmann approach to mean-field sparse feedback control Abstract: We study the synthesis of optimal control policies for large-scale\nmulti-agent systems. The optimal control design induces a parsimonious control\nintervention by means of l-1, sparsity-promoting control penalizations. We\nstudy instantaneous and infinite horizon sparse optimal feedback controllers.\nIn order to circumvent the dimensionality issues associated to the control of\nlarge-scale agent-based models, we follow a Boltzmann approach. We generate\n(sub)optimal controls signals for the kinetic limit of the multi-agent\ndynamics, by sampling of the optimal solution of the associated two-agent\ndynamics. Numerical experiments assess the performance of the proposed sparse\ndesign. \n\n"}
{"id": "1611.04529", "contents": "Title: Can information be spread as a virus? Viral Marketing as epidemiological\n  model Abstract: In epidemiology, an epidemic is defined as the spread of an infectious\ndisease to a large number of people in a given population within a short period\nof time. In the marketing context, a message is viral when it is broadly sent\nand received by the target market through person-to-person transmission. This\nspecific marketing communication strategy is commonly referred as viral\nmarketing. Due to this similarity between an epidemic and the viral marketing\nprocess and because the understanding of the critical factors to this\ncommunications strategy effectiveness remain largely unknown, the mathematical\nmodels in epidemiology are presented in this marketing specific field. In this\npaper, an epidemiological model SIR (Susceptible- Infected-Recovered) to study\nthe effects of a viral marketing strategy is presented. It is made a comparison\nbetween the disease parameters and the marketing application, and Matlab\nsimulations are performed. Finally, some conclusions are carried out and their\nmarketing implications are exposed: interactions across the parameters suggest\nsome recommendations to marketers, as the profitability of the investment or\nthe need to improve the targeting criteria of the communications campaigns. \n\n"}
{"id": "1611.04701", "contents": "Title: Errors-in-variables models with dependent measurements Abstract: Suppose that we observe $y \\in \\mathbb{R}^n$ and $X \\in \\mathbb{R}^{n \\times\nm}$ in the following errors-in-variables model: \\begin{eqnarray*} y & = & X_0\n\\beta^* +\\epsilon \\\\ X & = & X_0 + W, \\end{eqnarray*} where $X_0$ is an $n\n\\times m$ design matrix with independent subgaussian row vectors, $\\epsilon \\in\n\\mathbb{R}^n$ is a noise vector and $W$ is a mean zero $n \\times m$ random\nnoise matrix with independent subgaussian column vectors, independent of $X_0$\nand $\\epsilon$. This model is significantly different from those analyzed in\nthe literature in the sense that we allow the measurement error for each\ncovariate to be a dependent vector across its $n$ observations. Such error\nstructures appear in the science literature when modeling the trial-to-trial\nfluctuations in response strength shared across a set of neurons.\n  Under sparsity and restrictive eigenvalue type of conditions, we show that\none is able to recover a sparse vector $\\beta^* \\in \\mathbb{R}^m$ from the\nmodel given a single observation matrix $X$ and the response vector $y$. We\nestablish consistency in estimating $\\beta^*$ and obtain the rates of\nconvergence in the $\\ell_q$ norm, where $q = 1, 2$. We show error bounds which\napproach that of the regular Lasso and the Dantzig selector in case the errors\nin $W$ are tending to 0. We analyze the convergence rates of the gradient\ndescent methods for solving the nonconvex programs and show that the composite\ngradient descent algorithm is guaranteed to converge at a geometric rate to a\nneighborhood of the global minimizers: the size of the neighborhood is bounded\nby the statistical error in the $\\ell_2$ norm. Our analysis reveals interesting\nconnections between computational and statistical efficiency and the\nconcentration of measure phenomenon in random matrix theory. We provide\nsimulation evidence illuminating the theoretical predictions. \n\n"}
{"id": "1611.05433", "contents": "Title: ROS Regression: Integrating Regularization and Optimal Scaling\n  Regression Abstract: In this paper we combine two important extensions of ordinary least squares\nregression: regularization and optimal scaling. Optimal scaling (sometimes also\ncalled optimal scoring) has originally been developed for categorical data, and\nthe process finds quantifications for the categories that are optimal for the\nregression model in the sense that they maximize the multiple correlation.\nAlthough the optimal scaling method was developed initially for variables with\na limited number of categories, optimal transformations of continuous variables\nare a special case. We will consider a variety of transformation types;\ntypically we use step functions for categorical variables, and smooth (spline)\nfunctions for continuous variables. Both types of functions can be restricted\nto be monotonic, preserving the ordinal information in the data. In addition to\noptimal scaling, three regularization methods will be considered: Ridge\nregression, the Lasso, and the Elastic Net. The resulting method will be called\nROS Regression (Regularized Optimal Scaling Regression. We will show that the\nbasic OS algorithm provides straightforward and efficient estimation of the\nregularized regression coefficients, automatically gives the Group Lasso and\nBlockwise Sparse Regression, and extends them with monotonicity properties. We\nwill show that Optimal Scaling linearizes nonlinear relationships between\npredictors and outcome, and improves upon the condition of the predictor\ncorrelation matrix, increasing (on average) the conditional independence of the\npredictors. Alternative options for regularization of either regression\ncoefficients or category quantifications are mentioned. Extended examples are\nprovided.\n  Keywords: Categorical Data, Optimal Scaling, Conditional Independence, Step\nFunctions, Splines, Monotonic Transformations, Regularization, Lasso, Elastic\nNet, Group Lasso, Blockwise Sparse Regression. \n\n"}
{"id": "1611.05827", "contents": "Title: Towards a Mathematical Understanding of the Difficulty in Learning with\n  Feedforward Neural Networks Abstract: Training deep neural networks for solving machine learning problems is one\ngreat challenge in the field, mainly due to its associated optimisation problem\nbeing highly non-convex. Recent developments have suggested that many training\nalgorithms do not suffer from undesired local minima under certain scenario,\nand consequently led to great efforts in pursuing mathematical explanations for\nsuch observations. This work provides an alternative mathematical understanding\nof the challenge from a smooth optimisation perspective. By assuming exact\nlearning of finite samples, sufficient conditions are identified via a critical\npoint analysis to ensure any local minimum to be globally minimal as well.\nFurthermore, a state of the art algorithm, known as the Generalised\nGauss-Newton (GGN) algorithm, is rigorously revisited as an approximate\nNewton's algorithm, which shares the property of being locally quadratically\nconvergent to a global minimum under the condition of exact learning. \n\n"}
{"id": "1611.06534", "contents": "Title: Linear Thompson Sampling Revisited Abstract: We derive an alternative proof for the regret of Thompson sampling (\\ts) in\nthe stochastic linear bandit setting. While we obtain a regret bound of order\n$\\widetilde{O}(d^{3/2}\\sqrt{T})$ as in previous results, the proof sheds new\nlight on the functioning of the \\ts. We leverage on the structure of the\nproblem to show how the regret is related to the sensitivity (i.e., the\ngradient) of the objective function and how selecting optimal arms associated\nto \\textit{optimistic} parameters does control it. Thus we show that \\ts can be\nseen as a generic randomized algorithm where the sampling distribution is\ndesigned to have a fixed probability of being optimistic, at the cost of an\nadditional $\\sqrt{d}$ regret factor compared to a UCB-like approach.\nFurthermore, we show that our proof can be readily applied to regularized\nlinear optimization and generalized linear model problems. \n\n"}
{"id": "1611.08083", "contents": "Title: Survey of Expressivity in Deep Neural Networks Abstract: We survey results on neural network expressivity described in \"On the\nExpressive Power of Deep Neural Networks\". The paper motivates and develops\nthree natural measures of expressiveness, which all display an exponential\ndependence on the depth of the network. In fact, all of these measures are\nrelated to a fourth quantity, trajectory length. This quantity grows\nexponentially in the depth of the network, and is responsible for the depth\nsensitivity observed. These results translate to consequences for networks\nduring and after training. They suggest that parameters earlier in a network\nhave greater influence on its expressive power -- in particular, given a layer,\nits influence on expressivity is determined by the remaining depth of the\nnetwork after that layer. This is verified with experiments on MNIST and\nCIFAR-10. We also explore the effect of training on the input-output map, and\nfind that it trades off between the stability and expressivity. \n\n"}
{"id": "1611.08326", "contents": "Title: Detecting communities is hard, and counting them is even harder Abstract: We consider the algorithmic problem of community detection in networks. Given\nan undirected friendship graph $G=\\left(V,E\\right)$, a subset $S\\subseteq V$ is\nan $\\left(\\alpha,\\beta\\right)$-community if:\n  * Every member of the community is friends with an $\\alpha$-fraction of the\ncommunity;\n  * Every non-member is friends with at most a $\\beta$-fraction of the\ncommunity.\n  Arora et al [AGSS12] gave a quasi-polynomial time algorithm for enumerating\nall the $\\left(\\alpha,\\beta\\right)$-communities for any constants\n$\\alpha>\\beta$.\n  Here, we prove that, assuming the Exponential Time Hypothesis (ETH),\nquasi-polynomial time is in fact necessary - and even for a much weaker\napproximation desideratum. Namely, distinguishing between:\n  * $G$ contains an $\\left(1,o\\left(1\\right)\\right)$-community; and\n  * $G$ does not contain an\n$\\left(\\beta+o\\left(1\\right),\\beta\\right)$-community for any\n$\\beta\\in\\left[0,1\\right]$.\n  We also prove that counting the number of\n$\\left(1,o\\left(1\\right)\\right)$-communities requires quasi-polynomial time\nassuming the weaker #ETH. \n\n"}
{"id": "1611.08618", "contents": "Title: A Benchmark and Comparison of Active Learning for Logistic Regression Abstract: Logistic regression is by far the most widely used classifier in real-world\napplications. In this paper, we benchmark the state-of-the-art active learning\nmethods for logistic regression and discuss and illustrate their underlying\ncharacteristics. Experiments are carried out on three synthetic datasets and 44\nreal-world datasets, providing insight into the behaviors of these active\nlearning methods with respect to the area of the learning curve (which plots\nclassification accuracy as a function of the number of queried examples) and\ntheir computational costs. Surprisingly, one of the earliest and simplest\nsuggested active learning methods, i.e., uncertainty sampling, performs\nexceptionally well overall. Another remarkable finding is that random sampling,\nwhich is the rudimentary baseline to improve upon, is not overwhelmed by\nindividual active learning techniques in many cases. \n\n"}
{"id": "1611.08648", "contents": "Title: Patient-Driven Privacy Control through Generalized Distillation Abstract: The introduction of data analytics into medicine has changed the nature of\npatient treatment. In this, patients are asked to disclose personal information\nsuch as genetic markers, lifestyle habits, and clinical history. This data is\nthen used by statistical models to predict personalized treatments. However,\ndue to privacy concerns, patients often desire to withhold sensitive\ninformation. This self-censorship can impede proper diagnosis and treatment,\nwhich may lead to serious health complications and even death over time. In\nthis paper, we present privacy distillation, a mechanism which allows patients\nto control the type and amount of information they wish to disclose to the\nhealthcare providers for use in statistical models. Meanwhile, it retains the\naccuracy of models that have access to all patient data under a sufficient but\nnot full set of privacy-relevant information. We validate privacy distillation\nusing a corpus of patients prescribed to warfarin for a personalized dosage. We\nuse a deep neural network to implement privacy distillation for training and\nmaking dose predictions. We find that privacy distillation with sufficient\nprivacy-relevant information i) retains accuracy almost as good as having all\npatient data (only 3\\% worse), and ii) is effective at preventing errors that\nintroduce health-related risks (only 3.9\\% worse under- or over-prescriptions). \n\n"}
{"id": "1612.00316", "contents": "Title: Consensus Control for Linear Systems with Optimal Energy Cost Abstract: In this paper, we design an optimal energy cost controller for linear systems\nasymptotic consensus given the topology of the graph. The controller depends\nonly on relative information of the agents. Since finding the control gain for\nsuch controller is hard, we focus on finding an optimal controller among a\nclassical family of controllers which is based on Algebraic Riccati Equation\n(ARE) and guarantees asymptotic consensus. Through analysis, we find that the\nenergy cost is bounded by an interval and hence we minimize the upper bound. In\norder to do that, there are two classes of variables that need to be optimized:\nthe control gain and the edge weights of the graph and are hence designed from\ntwo perspectives. A suboptimal control gain is obtained by choosing $Q=0$ in\nthe ARE. Negative edge weights are allowed, and the problem is formulated as a\nSemi-definite Programming (SDP) problem. Having negative edge weights means\nthat \"competitions\" between the agents are allowed. The motivation behind this\nsetting is to have a better system performance. We provide a different proof\nfrom the angle of optimization and show that the lowest control energy cost is\nreached when the graph is complete and with equal edge weights. Furthermore,\ntwo sufficient conditions for the existence of negative optimal edge weights\nrealization are given. \n\n"}
{"id": "1612.00564", "contents": "Title: Metric and topological entropy bounds for optimal coding of stochastic\n  dynamical systems Abstract: We consider the problem of optimal zero-delay coding and estimation of a\nstochastic dynamical system over a noisy communication channel under three\nestimation criteria concerned with the low-distortion regime. The criteria\nconsidered are (i) a strong and (ii) a weak form of almost sure stability of\nthe estimation error as well as (ii) quadratic stability in expectation. For\nall three objectives, we derive lower bounds on the smallest channel capacity\n$C_0$ above which the objective can be achieved with an arbitrarily small\nerror. We first obtain bounds through a dynamical systems approach by\nconstructing an infinite-dimensional dynamical system and relating the capacity\nwith the topological and the metric entropy of this dynamical system. We also\nconsider information-theoretic and probability-theoretic approaches to address\nthe different criteria. Finally, we prove that a memoryless noisy channel in\ngeneral constitutes no obstruction to asymptotic almost sure state estimation\nwith arbitrarily small errors, when there is no noise in the system. The\nresults provide new solution methods for the criteria introduced (e.g.,\nstandard information-theoretic bounds cannot be applied for some of the\ncriteria) and establish further connections between dynamical systems,\nnetworked control, and information theory, and especially in the context of\nnonlinear stochastic systems. \n\n"}
{"id": "1612.01354", "contents": "Title: A semi-analytical approach for the positive semidefinite Procrustes\n  problem Abstract: The positive semidefinite Procrustes (PSDP) problem is the following: given\nrectangular matrices $X$ and $B$, find the symmetric positive semidefinite\nmatrix $A$ that minimizes the Frobenius norm of $AX-B$. No general procedure is\nknown that gives an exact solution. In this paper, we present a semi-analytical\napproach to solve the PSDP problem. First, we characterize completely the set\nof optimal solutions and identify the cases when the infimum is not attained.\nThis characterization requires the unique optimal solution of a smaller PSDP\nproblem where $B$ is square and $X$ is diagonal with positive diagonal\nelements. Second, we propose a very efficient strategy to solve the PSDP\nproblem, combining the semi-analytical approach, a new initialization strategy\nand the fast gradient method. We illustrate the effectiveness of the new\napproach, which is guaranteed to converge linearly, compared to\nstate-of-the-art methods. \n\n"}
{"id": "1612.03839", "contents": "Title: Tensor Decompositions via Two-Mode Higher-Order SVD (HOSVD) Abstract: Tensor decompositions have rich applications in statistics and machine\nlearning, and developing efficient, accurate algorithms for the problem has\nreceived much attention recently. Here, we present a new method built on\nKruskal's uniqueness theorem to decompose symmetric, nearly orthogonally\ndecomposable tensors. Unlike the classical higher-order singular value\ndecomposition which unfolds a tensor along a single mode, we consider\nunfoldings along two modes and use rank-1 constraints to characterize the\nunderlying components. This tensor decomposition method provably handles a\ngreater level of noise compared to previous methods and achieves a high\nestimation accuracy. Numerical results demonstrate that our algorithm is robust\nto various noise distributions and that it performs especially favorably as the\norder increases. \n\n"}
{"id": "1612.04082", "contents": "Title: What Lies Beneath the Surface: Topological-Shape Optimization With the\n  Kernel-Independent Fast Multipole Method Abstract: The paper presents a new method for shape and topology optimization based on\nan efficient and scalable boundary integral formulation for elasticity. To\noptimize topology, our approach uses iterative extraction of isosurfaces of a\ntopological derivative. The numerical solution of the elasticity boundary value\nproblem at every iteration is performed with the boundary element formulation\nand the kernel-independent fast multipole method. Providing excellent single\nnode performance, scalable parallelization and the best available asymptotic\ncomplexity, our method is among the fastest optimization tools available today.\nThe performance of our approach is studied on few illustrative examples,\nincluding the optimization of engineered constructions for the minimum\ncompliance and the optimization of the microstructure of a metamaterial for the\ndesired macroscopic tensor of elasticity. \n\n"}
{"id": "1612.04425", "contents": "Title: On the Convergence of Asynchronous Parallel Iteration with Unbounded\n  Delays Abstract: Recent years have witnessed the surge of asynchronous parallel\n(async-parallel) iterative algorithms due to problems involving very\nlarge-scale data and a large number of decision variables. Because of\nasynchrony, the iterates are computed with outdated information, and the age of\nthe outdated information, which we call delay, is the number of times it has\nbeen updated since its creation. Almost all recent works prove convergence\nunder the assumption of a finite maximum delay and set their stepsize\nparameters accordingly. However, the maximum delay is practically unknown.\n  This paper presents convergence analysis of an async-parallel method from a\nprobabilistic viewpoint, and it allows for large unbounded delays. An explicit\nformula of stepsize that guarantees convergence is given depending on delays'\nstatistics. With $p+1$ identical processors, we empirically measured that\ndelays closely follow the Poisson distribution with parameter $p$, matching our\ntheoretical model, and thus the stepsize can be set accordingly. Simulations on\nboth convex and nonconvex optimization problems demonstrate the validness of\nour analysis and also show that the existing maximum-delay induced stepsize is\ntoo conservative, often slowing down the convergence of the algorithm. \n\n"}
{"id": "1612.04453", "contents": "Title: Interactive Preference Learning of Utility Functions for Multi-Objective\n  Optimization Abstract: Real-world engineering systems are typically compared and contrasted using\nmultiple metrics. For practical machine learning systems, performance tuning is\noften more nuanced than minimizing a single expected loss objective, and it may\nbe more realistically discussed as a multi-objective optimization problem. We\npropose a novel generative model for scalar-valued utility functions to capture\nhuman preferences in a multi-objective optimization setting. We also outline an\ninteractive active learning system that sequentially refines the understanding\nof stakeholders ideal utility functions using binary preference queries. \n\n"}
{"id": "1612.04897", "contents": "Title: Learning binary or real-valued time-series via spike-timing dependent\n  plasticity Abstract: A dynamic Boltzmann machine (DyBM) has been proposed as a model of a spiking\nneural network, and its learning rule of maximizing the log-likelihood of given\ntime-series has been shown to exhibit key properties of spike-timing dependent\nplasticity (STDP), which had been postulated and experimentally confirmed in\nthe field of neuroscience as a learning rule that refines the Hebbian rule.\nHere, we relax some of the constraints in the DyBM in a way that it becomes\nmore suitable for computation and learning. We show that learning the DyBM can\nbe considered as logistic regression for binary-valued time-series. We also\nshow how the DyBM can learn real-valued data in the form of a Gaussian DyBM and\ndiscuss its relation to the vector autoregressive (VAR) model. The Gaussian\nDyBM extends the VAR by using additional explanatory variables, which\ncorrespond to the eligibility traces of the DyBM and capture long term\ndependency of the time-series. Numerical experiments show that the Gaussian\nDyBM significantly improves the predictive accuracy over VAR. \n\n"}
{"id": "1612.05554", "contents": "Title: Alternating Projections Methods for Discrete-time Stabilization of\n  Quantum States Abstract: We study sequences (both cyclic and randomized) of idempotent\ncompletely-positive trace-preserving quantum maps, and show how they\nasymptotically converge to the intersection of their fixed point sets via\nalternating projection methods. We characterize the robustness features of the\nprotocol against randomization and provide basic bounds on its convergence\nspeed. The general results are then specialized to stabilizing en- tangled\nstates in finite-dimensional multipartite quantum systems subject to a resource\nconstraint, a problem of key interest for quantum information applications. We\nconclude by suggesting further developments, including techniques to enlarge\nthe set of stabilizable states and ensure efficient, finite-time preparation. \n\n"}
{"id": "1612.06344", "contents": "Title: Random linear systems with sparse solutions -- finite dimensions Abstract: In our companion work \\cite{Stojnicl1RegPosasymldp} we revisited random\nunder-determined linear systems with sparse solutions. The main emphasis was on\nthe performance analysis of the $\\ell_1$ heuristic in the so-called asymptotic\nregime, i.e. in the regime where the systems' dimensions are large. Through an\nearlier sequence of work\n\\cite{DonohoPol,DonohoUnsigned,StojnicCSetam09,StojnicUpper10}, it is now well\nknown that in such a regime the $\\ell_1$ exhibits the so-called \\emph{phase\ntransition} (PT) phenomenon. \\cite{Stojnicl1RegPosasymldp} then went much\nfurther and established the so-called \\emph{large deviations principle} (LDP)\ntype of behavior that characterizes not only the breaking points of the\n$\\ell_1$'s success but also the behavior in the entire so-called\n\\emph{transition zone} around these points. Both of these concepts, the PTs and\nthe LDPs, are in fact defined so that one can use them to characterize the\nasymptotic behavior. In this paper we complement the results of\n\\cite{Stojnicl1RegPosasymldp} by providing an exact detailed analysis in the\nnon-asymptotic regime. Of course, not only are the non-asymptotic results\ncomplementing those from \\cite{Stojnicl1RegPosasymldp}, they actually are the\nones that ultimately fully characterize the $\\ell_1$'s behavior in the most\ngeneral sense. We introduce several novel high-dimensional geometry type of\nstrategies that enable us to eventually determine the $\\ell_1$'s behavior. \n\n"}
{"id": "1612.06516", "contents": "Title: Random linear under-determined systems with block-sparse solutions --\n  asymptotics, large deviations, and finite dimensions Abstract: In this paper we consider random linear under-determined systems with\nblock-sparse solutions. A standard subvariant of such systems, namely,\nprecisely the same type of systems without additional block structuring\nrequirement, gained a lot of popularity over the last decade. This is of course\nin first place due to the success in mathematical characterization of an\n$\\ell_1$ optimization technique typically used for solving such systems,\ninitially achieved in \\cite{CRT,DOnoho06CS} and later on perfected in\n\\cite{DonohoPol,DonohoUnsigned,StojnicCSetam09,StojnicUpper10}. The success\nthat we achieved in \\cite{StojnicCSetam09,StojnicUpper10} characterizing the\nstandard sparse solutions systems, we were then able to replicate in a sequence\nof papers\n\\cite{StojnicCSetamBlock09,StojnicUpperBlock10,StojnicICASSP09block,StojnicJSTSP09}\nwhere instead of the standard $\\ell_1$ optimization we utilized its an\n$\\ell_2/\\ell_1$ variant as a better fit for systems with block-sparse\nsolutions. All of these results finally settled the so-called threshold/phase\ntransitions phenomena (which naturally assume the asymptotic/large dimensional\nscenario). Here, in addition to a few novel asymptotic considerations, we also\ntry to raise the level a bit, step a bit away from the asymptotics, and\nconsider the finite dimensions scenarios as well. \n\n"}
{"id": "1612.06839", "contents": "Title: Box constrained $\\ell_1$ optimization in random linear systems -- finite\n  dimensions Abstract: Our companion work \\cite{Stojnicl1BnBxasymldp} considers random\nunder-determined linear systems with box-constrained sparse solutions and\nprovides an asymptotic analysis of a couple of modified $\\ell_1$ heuristics\nadjusted to handle such systems (we refer to these modifications of the\nstandard $\\ell_1$ as binary and box $\\ell_1$). Our earlier work\n\\cite{StojnicISIT2010binary} established that the binary $\\ell_1$ does exhibit\nthe so-called phase-transition phenomenon (basically the same phenomenon\nwell-known through earlier considerations to be a key feature of the standard\n$\\ell_1$, see, e.g.\n\\cite{DonohoPol,DonohoUnsigned,StojnicCSetam09,StojnicUpper10}). Moreover, in\n\\cite{StojnicISIT2010binary}, we determined the precise location of the\nco-called phase-transition (PT) curve. On the other hand, in\n\\cite{Stojnicl1BnBxasymldp} we provide a much deeper understanding of the PTs\nand do so through a large deviations principles (LDP) type of analysis. In this\npaper we complement the results of \\cite{Stojnicl1BnBxasymldp} by leaving the\nasymptotic regime naturally assumed in the PT and LDP considerations aside and\ninstead working in a finite dimensional setting. Along the same lines, we\nprovide for both, the binary and the box $\\ell_1$, precise finite dimensional\nanalyses and essentially determine their ultimate statistical performance\ncharacterizations. On top of that, we explain how the results created here can\nbe utilized in the asymptotic setting, considered in\n\\cite{Stojnicl1BnBxasymldp}, as well. Finally, for the completeness, we also\npresent a collection of results obtained through numerical simulations and\nobserve that they are in a massive agreement with our theoretical calculations. \n\n"}
{"id": "1612.07435", "contents": "Title: Partial $\\ell_1$ optimization in random linear systems -- phase\n  transitions and large deviations Abstract: $\\ell_1$ optimization is a well known heuristic often employed for solving\nvarious forms of sparse linear problems. In this paper we look at its a variant\nthat we refer to as the \\emph{partial} $\\ell_1$ and discuss its mathematical\nproperties when used for solving linear under-determined systems of equations.\nWe will focus on large random systems and discuss the phase transition (PT)\nphenomena and how they connect to the large deviation principles (LDP). Using a\nvariety of probabilistic and geometric techniques that we have developed in\nrecent years we will first present general guidelines that conceptually fully\ncharacterize both, the PTs and the LDPs. After that we will put an emphasis on\nproviding a collection of explicit analytical solutions to all of the\nunderlying mathematical problems. As a nice bonus to the developed concepts,\nthe forms of the analytical solutions will, in our view, turn out to be fairly\nelegant as well. \n\n"}
{"id": "1612.07436", "contents": "Title: Partial $\\ell_1$ optimization in random linear systems -- finite\n  dimensions Abstract: In this paper we provide a complementary set of results to those we present\nin our companion work \\cite{Stojnicl1HidParasymldp} regarding the behavior of\nthe so-called partial $\\ell_1$ (a variant of the standard $\\ell_1$ heuristic\noften employed for solving under-determined systems of linear equations). As is\nwell known through our earlier works\n\\cite{StojnicICASSP10knownsupp,StojnicTowBettCompSens13}, the partial $\\ell_1$\nalso exhibits the phase-transition (PT) phenomenon, discovered and well\nunderstood in the context of the standard $\\ell_1$ through Donoho's and our own\nworks \\cite{DonohoPol,DonohoUnsigned,StojnicCSetam09,StojnicUpper10}.\n\\cite{Stojnicl1HidParasymldp} goes much further though and, in addition to the\ndetermination of the partial $\\ell_1$'s phase-transition curves (PT curves)\n(which had already been done in\n\\cite{StojnicICASSP10knownsupp,StojnicTowBettCompSens13}), provides a\nsubstantially deeper understanding of the PT phenomena through a study of the\nunderlying large deviations principles (LDPs). As the PT and LDP phenomena are\nby their definitions related to large dimensional settings, both sets of our\nworks, \\cite{StojnicICASSP10knownsupp,StojnicTowBettCompSens13} and\n\\cite{Stojnicl1HidParasymldp}, consider what is typically called the asymptotic\nregime. In this paper we move things in a different direction and consider\nfinite dimensional scenarios. Basically, we provide explicit performance\ncharacterizations for any given collection of systems/parameters dimensions. We\ndo so for two different variants of the partial $\\ell_1$, one that we call\nexactly the partial $\\ell_1$ and another one, possibly a bit more practical,\nthat we call the hidden partial $\\ell_1$. \n\n"}
{"id": "1612.07659", "contents": "Title: Structured Sequence Modeling with Graph Convolutional Recurrent Networks Abstract: This paper introduces Graph Convolutional Recurrent Network (GCRN), a deep\nlearning model able to predict structured sequences of data. Precisely, GCRN is\na generalization of classical recurrent neural networks (RNN) to data\nstructured by an arbitrary graph. Such structured sequences can represent\nseries of frames in videos, spatio-temporal measurements on a network of\nsensors, or random walks on a vocabulary graph for natural language modeling.\nThe proposed model combines convolutional neural networks (CNN) on graphs to\nidentify spatial structures and RNN to find dynamic patterns. We study two\npossible architectures of GCRN, and apply the models to two practical problems:\npredicting moving MNIST data, and modeling natural language with the Penn\nTreebank dataset. Experiments show that exploiting simultaneously graph spatial\nand dynamic information about data can improve both precision and learning\nspeed. \n\n"}
{"id": "1612.09283", "contents": "Title: Generalized Intersection Kernel Abstract: Following the very recent line of work on the ``generalized min-max'' (GMM)\nkernel, this study proposes the ``generalized intersection'' (GInt) kernel and\nthe related ``normalized generalized min-max'' (NGMM) kernel. In computer\nvision, the (histogram) intersection kernel has been popular, and the GInt\nkernel generalizes it to data which can have both negative and positive\nentries. Through an extensive empirical classification study on 40 datasets\nfrom the UCI repository, we are able to show that this (tuning-free) GInt\nkernel performs fairly well.\n  The empirical results also demonstrate that the NGMM kernel typically\noutperforms the GInt kernel. Interestingly, the NGMM kernel has another\ninterpretation --- it is the ``asymmetrically transformed'' version of the GInt\nkernel, based on the idea of ``asymmetric hashing''. Just like the GMM kernel,\nthe NGMM kernel can be efficiently linearized through (e.g.,) generalized\nconsistent weighted sampling (GCWS), as empirically validated in our study.\nOwing to the discrete nature of hashed values, it also provides a scheme for\napproximate near neighbor search. \n\n"}
{"id": "1701.01207", "contents": "Title: Learning Semidefinite Regularizers Abstract: Regularization techniques are widely employed in optimization-based\napproaches for solving ill-posed inverse problems in data analysis and\nscientific computing. These methods are based on augmenting the objective with\na penalty function, which is specified based on prior domain-specific expertise\nto induce a desired structure in the solution. We consider the problem of\nlearning suitable regularization functions from data in settings in which\nprecise domain knowledge is not directly available. Previous work under the\ntitle of `dictionary learning' or `sparse coding' may be viewed as learning a\nregularization function that can be computed via linear programming. We\ndescribe generalizations of these methods to learn regularizers that can be\ncomputed and optimized via semidefinite programming. Our framework for learning\nsuch semidefinite regularizers is based on obtaining structured factorizations\nof data matrices, and our algorithmic approach for computing these\nfactorizations combines recent techniques for rank minimization problems along\nwith an operator analog of Sinkhorn scaling. Under suitable conditions on the\ninput data, our algorithm provides a locally linearly convergent method for\nidentifying the correct regularizer that promotes the type of structure\ncontained in the data. Our analysis is based on the stability properties of\nOperator Sinkhorn scaling and their relation to geometric aspects of\ndeterminantal varieties (in particular tangent spaces with respect to these\nvarieties). The regularizers obtained using our framework can be employed\neffectively in semidefinite programming relaxations for solving inverse\nproblems. \n\n"}
{"id": "1701.01945", "contents": "Title: A Framework for Wasserstein-1-Type Metrics Abstract: We propose a unifying framework for generalising the Wasserstein-1 metric to\na discrepancy measure between nonnegative measures of different mass. This\ngeneralization inherits the convexity and computational efficiency from the\nWasserstein-1 metric, and it includes several previous approaches from the\nliterature as special cases. For various specific instances of the generalized\nWasserstein-1 metric we furthermore demonstrate their usefulness in\napplications by numerical experiments. \n\n"}
{"id": "1701.03863", "contents": "Title: Breaking Locality Accelerates Block Gauss-Seidel Abstract: Recent work by Nesterov and Stich showed that momentum can be used to\naccelerate the rate of convergence for block Gauss-Seidel in the setting where\na fixed partitioning of the coordinates is chosen ahead of time. We show that\nthis setting is too restrictive, constructing instances where breaking locality\nby running non-accelerated Gauss-Seidel with randomly sampled coordinates\nsubstantially outperforms accelerated Gauss-Seidel with any fixed partitioning.\nMotivated by this finding, we analyze the accelerated block Gauss-Seidel\nalgorithm in the random coordinate sampling setting. Our analysis captures the\nbenefit of acceleration with a new data-dependent parameter which is well\nbehaved when the matrix sub-blocks are well-conditioned. Empirically, we show\nthat accelerated Gauss-Seidel with random coordinate sampling provides speedups\nfor large scale machine learning tasks when compared to non-accelerated\nGauss-Seidel and the classical conjugate-gradient algorithm. \n\n"}
{"id": "1701.05943", "contents": "Title: Structure of optimal strategies for remote estimation over\n  Gilbert-Elliott channel with feedback Abstract: We investigate remote estimation over a Gilbert-Elliot channel with feedback.\nWe assume that the channel state is observed by the receiver and fed back to\nthe transmitter with one unit delay. In addition, the transmitter gets ACK/NACK\nfeedback for successful/unsuccessful transmission. Using ideas from team\ntheory, we establish the structure of optimal transmission and estimation\nstrategies and identify a dynamic program to determine optimal strategies with\nthat structure. We then consider first-order autoregressive sources where the\nnoise process has unimodal and symmetric distribution. Using ideas from\nmajorization theory, we show that the optimal transmission strategy has a\nthreshold structure and the optimal estimation strategy is Kalman-like. \n\n"}
{"id": "1701.06064", "contents": "Title: On Recoverable and Two-Stage Robust Selection Problems with Budgeted\n  Uncertainty Abstract: In this paper the problem of selecting $p$ out of $n$ available items is\ndiscussed, such that their total cost is minimized. We assume that costs are\nnot known exactly, but stem from a set of possible outcomes.\n  Robust recoverable and two-stage models of this selection problem are\nanalyzed. In the two-stage problem, up to $p$ items is chosen in the first\nstage, and the solution is completed once the scenario becomes revealed in the\nsecond stage. In the recoverable problem, a set of $p$ items is selected in the\nfirst stage, and can be modified by exchanging up to $k$ items in the second\nstage, after a scenario reveals.\n  We assume that uncertain costs are modeled through bounded uncertainty sets,\ni.e., the interval uncertainty sets with an additional linear (budget)\nconstraint, in their discrete and continuous variants. Polynomial algorithms\nfor recoverable and two-stage selection problems with continuous bounded\nuncertainty, and compact mixed integer formulations in the case of discrete\nbounded uncertainty are constructed. \n\n"}
{"id": "1701.06538", "contents": "Title: Outrageously Large Neural Networks: The Sparsely-Gated\n  Mixture-of-Experts Layer Abstract: The capacity of a neural network to absorb information is limited by its\nnumber of parameters. Conditional computation, where parts of the network are\nactive on a per-example basis, has been proposed in theory as a way of\ndramatically increasing model capacity without a proportional increase in\ncomputation. In practice, however, there are significant algorithmic and\nperformance challenges. In this work, we address these challenges and finally\nrealize the promise of conditional computation, achieving greater than 1000x\nimprovements in model capacity with only minor losses in computational\nefficiency on modern GPU clusters. We introduce a Sparsely-Gated\nMixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward\nsub-networks. A trainable gating network determines a sparse combination of\nthese experts to use for each example. We apply the MoE to the tasks of\nlanguage modeling and machine translation, where model capacity is critical for\nabsorbing the vast quantities of knowledge available in the training corpora.\nWe present model architectures in which a MoE with up to 137 billion parameters\nis applied convolutionally between stacked LSTM layers. On large language\nmodeling and machine translation benchmarks, these models achieve significantly\nbetter results than state-of-the-art at lower computational cost. \n\n"}
{"id": "1701.06607", "contents": "Title: Stable Recovery Of Sparse Vectors From Random Sinusoidal Feature Maps Abstract: Random sinusoidal features are a popular approach for speeding up\nkernel-based inference in large datasets. Prior to the inference stage, the\napproach suggests performing dimensionality reduction by first multiplying each\ndata vector by a random Gaussian matrix, and then computing an element-wise\nsinusoid. Theoretical analysis shows that collecting a sufficient number of\nsuch features can be reliably used for subsequent inference in kernel\nclassification and regression.\n  In this work, we demonstrate that with a mild increase in the dimension of\nthe embedding, it is also possible to reconstruct the data vector from such\nrandom sinusoidal features, provided that the underlying data is sparse enough.\nIn particular, we propose a numerically stable algorithm for reconstructing the\ndata vector given the nonlinear features, and analyze its sample complexity.\nOur algorithm can be extended to other types of structured inverse problems,\nsuch as demixing a pair of sparse (but incoherent) vectors. We support the\nefficacy of our approach via numerical experiments. \n\n"}
{"id": "1701.07875", "contents": "Title: Wasserstein GAN Abstract: We introduce a new algorithm named WGAN, an alternative to traditional GAN\ntraining. In this new model, we show that we can improve the stability of\nlearning, get rid of problems like mode collapse, and provide meaningful\nlearning curves useful for debugging and hyperparameter searches. Furthermore,\nwe show that the corresponding optimization problem is sound, and provide\nextensive theoretical work highlighting the deep connections to other distances\nbetween distributions. \n\n"}
{"id": "1702.00518", "contents": "Title: Recovering True Classifier Performance in Positive-Unlabeled Learning Abstract: A common approach in positive-unlabeled learning is to train a classification\nmodel between labeled and unlabeled data. This strategy is in fact known to\ngive an optimal classifier under mild conditions; however, it results in biased\nempirical estimates of the classifier performance. In this work, we show that\nthe typically used performance measures such as the receiver operating\ncharacteristic curve, or the precision-recall curve obtained on such data can\nbe corrected with the knowledge of class priors; i.e., the proportions of the\npositive and negative examples in the unlabeled data. We extend the results to\na noisy setting where some of the examples labeled positive are in fact\nnegative and show that the correction also requires the knowledge of the\nproportion of noisy examples in the labeled positives. Using state-of-the-art\nalgorithms to estimate the positive class prior and the proportion of noise, we\nexperimentally evaluate two correction approaches and demonstrate their\nefficacy on real-life data. \n\n"}
{"id": "1702.01421", "contents": "Title: An extension of Chubanov's algorithm to symmetric cones Abstract: In this work we present an extension of Chubanov's algorithm to the case of\nhomogeneous feasibility problems over a symmetric cone K. As in Chubanov's\nmethod for linear feasibility problems, the algorithm consists of a basic\nprocedure and a step where the solutions are confined to the intersection of a\nhalf-space and K. Following an earlier work by Kitahara and Tsuchiya on second\norder cone feasibility problems, progress is measured through the volumes of\nthose intersections: when they become sufficiently small, we know it is time to\nstop. We never have to explicitly compute the volumes, it is only necessary to\nkeep track of the reductions between iterations. We show this is enough to\nobtain concrete upper bounds to the minimum eigenvalues of a scaled version of\nthe original feasibility problem. Another distinguishing feature of our\napproach is the usage of a spectral norm that takes into account the way that K\nis decomposed as simple cones. In several key cases, including semidefinite\nprogramming and second order cone programming, these norms make it possible to\nobtain better complexity bounds for the basic procedure when compared to a\nrecent approach by Pe\\~na and Soheili. Finally, in the appendix, we present a\ntranslation of the algorithm to the homogeneous feasibility problem in\nsemidefinite programming. \n\n"}
{"id": "1702.02241", "contents": "Title: Adapting Regularized Low Rank Models for Parallel Architectures Abstract: We introduce a reformulation of regularized low-rank recovery models to take\nadvantage of GPU, multiple CPU, and hybridized architectures. Low-rank recovery\noften involves nuclear-norm minimization through iterative thresholding of\nsingular values. These models are slow to fit and difficult to parallelize\nbecause of their dependence on computing a singular value decomposition at each\niteration. Regularized low-rank recovery models also incorporate non-smooth\nterms to separate structured components (e.g. sparse outliers) from the\nlow-rank component, making these problems more difficult.\n  Using Burer-Monteiro splitting and marginalization, we develop a smooth,\nnon-convex formulation of regularized low-rank recovery models that can be fit\nwith first-order solvers. We develop a computable certificate of convergence\nfor this non-convex program, and use it to establish bounds on the\nsuboptimality of any point. Using robust principal component analysis (RPCA) as\nan example, we include numerical experiments showing that this approach is an\norder-of-magnitude faster than existing RPCA solvers on the GPU. We also show\nthat this acceleration allows new applications for RPCA, including real-time\nbackground subtraction and MR image analysis. \n\n"}
{"id": "1702.03849", "contents": "Title: Non-convex learning via Stochastic Gradient Langevin Dynamics: a\n  nonasymptotic analysis Abstract: Stochastic Gradient Langevin Dynamics (SGLD) is a popular variant of\nStochastic Gradient Descent, where properly scaled isotropic Gaussian noise is\nadded to an unbiased estimate of the gradient at each iteration. This modest\nchange allows SGLD to escape local minima and suffices to guarantee asymptotic\nconvergence to global minimizers for sufficiently regular non-convex objectives\n(Gelfand and Mitter, 1991). The present work provides a nonasymptotic analysis\nin the context of non-convex learning problems, giving finite-time guarantees\nfor SGLD to find approximate minimizers of both empirical and population risks.\nAs in the asymptotic setting, our analysis relates the discrete-time SGLD\nMarkov chain to a continuous-time diffusion process. A new tool that drives the\nresults is the use of weighted transportation cost inequalities to quantify the\nrate of convergence of SGLD to a stationary distribution in the Euclidean\n$2$-Wasserstein distance. \n\n"}
{"id": "1702.04121", "contents": "Title: Practical Learning of Predictive State Representations Abstract: Over the past decade there has been considerable interest in spectral\nalgorithms for learning Predictive State Representations (PSRs). Spectral\nalgorithms have appealing theoretical guarantees; however, the resulting models\ndo not always perform well on inference tasks in practice. One reason for this\nbehavior is the mismatch between the intended task (accurate filtering or\nprediction) and the loss function being optimized by the algorithm (estimation\nerror in model parameters).\n  A natural idea is to improve performance by refining PSRs using an algorithm\nsuch as EM. Unfortunately it is not obvious how to apply apply an EM style\nalgorithm in the context of PSRs as the Log Likelihood is not well defined for\nall PSRs. We show that it is possible to overcome this problem using ideas from\nPredictive State Inference Machines.\n  We combine spectral algorithms for PSRs as a consistent and efficient\ninitialization with PSIM-style updates to refine the resulting model\nparameters. By combining these two ideas we develop Inference Gradients, a\nsimple, fast, and robust method for practical learning of PSRs. Inference\nGradients performs gradient descent in the PSR parameter space to optimize an\ninference-based loss function like PSIM. Because Inference Gradients uses a\nspectral initialization we get the same consistency benefits as PSRs. We show\nthat Inference Gradients outperforms both PSRs and PSIMs on real and synthetic\ndata sets. \n\n"}
{"id": "1702.04144", "contents": "Title: A constant step Forward-Backward algorithm involving random maximal\n  monotone operators Abstract: A stochastic Forward-Backward algorithm with a constant step is studied. At\neach time step, this algorithm involves an independent copy of a couple of\nrandom maximal monotone operators. Defining a mean operator as a selection\nintegral, the differential inclusion built from the sum of the two mean\noperators is considered. As a first result, it is shown that the interpolated\nprocess obtained from the iterates converges narrowly in the small step regime\nto the solution of this differential inclusion. In order to control the long\nterm behavior of the iterates, a stability result is needed in addition. To\nthis end, the sequence of the iterates is seen as a homogeneous Feller Markov\nchain whose transition kernel is parameterized by the algorithm step size. The\ncluster points of the Markov chains invariant measures in the small step regime\nare invariant for the semiflow induced by the differential inclusion.\nConclusions regarding the long run behavior of the iterates for small steps are\ndrawn. It is shown that when the sum of the mean operators is demipositive, the\nprobabilities that the iterates are away from the set of zeros of this sum are\nsmall in Ces\\`aro mean. The ergodic behavior of these iterates is studied as\nwell. Applications of the proposed algorithm are considered. In particular, a\ndetailed analysis of the random proximal gradient algorithm with constant step\nis performed. \n\n"}
{"id": "1702.05197", "contents": "Title: Throughput-Optimal Broadcast in Wireless Networks with\n  Point-to-Multipoint Transmissions Abstract: We consider the problem of efficient packet dissemination in wireless\nnetworks with point-to-multi-point wireless broadcast channels. We propose a\ndynamic policy, which achieves the broadcast capacity of the network. This\npolicy is obtained by first transforming the original multi-hop network into a\nprecedence-relaxed virtual single-hop network and then finding an optimal\nbroadcast policy for the relaxed network. The resulting policy is shown to be\nthroughput-optimal for the original wireless network using a sample-path\nargument. We also prove the NP-completeness of the finite-horizon broadcast\nproblem, which is in contrast with the polynomial time solvability of the\nproblem with point-to-point channels. Illustrative simulation results\ndemonstrate the efficacy of the proposed broadcast policy in achieving the full\nbroadcast capacity with low delay. \n\n"}
{"id": "1702.05538", "contents": "Title: Dataset Augmentation in Feature Space Abstract: Dataset augmentation, the practice of applying a wide array of\ndomain-specific transformations to synthetically expand a training set, is a\nstandard tool in supervised learning. While effective in tasks such as visual\nrecognition, the set of transformations must be carefully designed,\nimplemented, and tested for every new domain, limiting its re-use and\ngenerality. In this paper, we adopt a simpler, domain-agnostic approach to\ndataset augmentation. We start with existing data points and apply simple\ntransformations such as adding noise, interpolating, or extrapolating between\nthem. Our main insight is to perform the transformation not in input space, but\nin a learned feature space. A re-kindling of interest in unsupervised\nrepresentation learning makes this technique timely and more effective. It is a\nsimple proposal, but to-date one that has not been tested empirically. Working\nin the space of context vectors generated by sequence-to-sequence models, we\ndemonstrate a technique that is effective for both static and sequential data. \n\n"}
{"id": "1702.05575", "contents": "Title: A Hitting Time Analysis of Stochastic Gradient Langevin Dynamics Abstract: We study the Stochastic Gradient Langevin Dynamics (SGLD) algorithm for\nnon-convex optimization. The algorithm performs stochastic gradient descent,\nwhere in each step it injects appropriately scaled Gaussian noise to the\nupdate. We analyze the algorithm's hitting time to an arbitrary subset of the\nparameter space. Two results follow from our general theory: First, we prove\nthat for empirical risk minimization, if the empirical risk is point-wise close\nto the (smooth) population risk, then the algorithm achieves an approximate\nlocal minimum of the population risk in polynomial time, escaping suboptimal\nlocal minima that only exist in the empirical risk. Second, we show that SGLD\nimproves on one of the best known learnability results for learning linear\nclassifiers under the zero-one loss. \n\n"}
{"id": "1702.05777", "contents": "Title: Exponentially vanishing sub-optimal local minima in multilayer neural\n  networks Abstract: Background: Statistical mechanics results (Dauphin et al. (2014); Choromanska\net al. (2015)) suggest that local minima with high error are exponentially rare\nin high dimensions. However, to prove low error guarantees for Multilayer\nNeural Networks (MNNs), previous works so far required either a heavily\nmodified MNN model or training method, strong assumptions on the labels (e.g.,\n\"near\" linear separability), or an unrealistic hidden layer with\n$\\Omega\\left(N\\right)$ units.\n  Results: We examine a MNN with one hidden layer of piecewise linear units, a\nsingle output, and a quadratic loss. We prove that, with high probability in\nthe limit of $N\\rightarrow\\infty$ datapoints, the volume of differentiable\nregions of the empiric loss containing sub-optimal differentiable local minima\nis exponentially vanishing in comparison with the same volume of global minima,\ngiven standard normal input of dimension\n$d_{0}=\\tilde{\\Omega}\\left(\\sqrt{N}\\right)$, and a more realistic number of\n$d_{1}=\\tilde{\\Omega}\\left(N/d_{0}\\right)$ hidden units. We demonstrate our\nresults numerically: for example, $0\\%$ binary classification training error on\nCIFAR with only $N/d_{0}\\approx 16$ hidden neurons. \n\n"}
{"id": "1702.05870", "contents": "Title: Cosine Normalization: Using Cosine Similarity Instead of Dot Product in\n  Neural Networks Abstract: Traditionally, multi-layer neural networks use dot product between the output\nvector of previous layer and the incoming weight vector as the input to\nactivation function. The result of dot product is unbounded, thus increases the\nrisk of large variance. Large variance of neuron makes the model sensitive to\nthe change of input distribution, thus results in poor generalization, and\naggravates the internal covariate shift which slows down the training. To bound\ndot product and decrease the variance, we propose to use cosine similarity or\ncentered cosine similarity (Pearson Correlation Coefficient) instead of dot\nproduct in neural networks, which we call cosine normalization. We compare\ncosine normalization with batch, weight and layer normalization in\nfully-connected neural networks as well as convolutional networks on the data\nsets of MNIST, 20NEWS GROUP, CIFAR-10/100 and SVHN. Experiments show that\ncosine normalization achieves better performance than other normalization\ntechniques. \n\n"}
{"id": "1702.06154", "contents": "Title: Role model detection using low rank similarity matrix Abstract: Computing meaningful clusters of nodes is crucial to analyse large networks.\nIn this paper, we apply new clustering methods to improve the computational\ntime. We use the properties of the adjacency matrix to obtain better role\nextraction. We also define a new non-recursive similarity measure and compare\nits results with the ones obtained with Browet's similarity measure. We will\nshow the extraction of the different roles with a linear time complexity.\nFinally, we test our algorithm with real data structures and analyse the limit\nof our algorithm. \n\n"}
{"id": "1702.08166", "contents": "Title: Linear Convergence of the Proximal Incremental Aggregated Gradient\n  Method under Quadratic Growth Condition Abstract: Under the strongly convex assumption, several recent works studied the global\nlinear convergence rate of the proximal incremental aggregated gradient (PIAG)\nmethod for minimizing the sum of a large number of smooth component functions\nand a non-smooth convex function. In this paper, under \\textsl{the quadratic\ngrowth condition}--a strictly weaker condition than the strongly convex\nassumption, we derive a new global linear convergence rate result, which\nimplies that the PIAG method attains global linear convergence rates in both\nthe function value and iterate point errors. The main idea behind is to\nconstruct a certain Lyapunov function. \n\n"}
{"id": "1703.00102", "contents": "Title: SARAH: A Novel Method for Machine Learning Problems Using Stochastic\n  Recursive Gradient Abstract: In this paper, we propose a StochAstic Recursive grAdient algoritHm (SARAH),\nas well as its practical variant SARAH+, as a novel approach to the finite-sum\nminimization problems. Different from the vanilla SGD and other modern\nstochastic methods such as SVRG, S2GD, SAG and SAGA, SARAH admits a simple\nrecursive framework for updating stochastic gradient estimates; when comparing\nto SAG/SAGA, SARAH does not require a storage of past gradients. The linear\nconvergence rate of SARAH is proven under strong convexity assumption. We also\nprove a linear convergence rate (in the strongly convex case) for an inner loop\nof SARAH, the property that SVRG does not possess. Numerical experiments\ndemonstrate the efficiency of our algorithm. \n\n"}
{"id": "1703.00262", "contents": "Title: Variance-based stochastic extragradient methods with line search for\n  stochastic variational inequalities Abstract: A dynamic sampled stochastic approximated (DS-SA) extragradient method for\nstochastic variational inequalities (SVI) is proposed that is \\emph{robust}\nwith respect to an unknown Lipschitz constant $L$. To the best of our\nknowledge, it is the first provably convergent \\emph{robust} SA \\emph{method\nwith variance reduction}, either for SVIs or stochastic optimization, assuming\njust an unbiased stochastic oracle in a large sample regime. This widens the\napplicability and improves, up to constants, the desired efficient acceleration\nof previous variance reduction methods, all of which still assume knowledge of\n$L$ (and, hence, are not robust against its estimate). Precisely, compared to\nthe iteration and oracle complexities of $\\mathcal{O}(\\epsilon^{-2})$ of\nprevious robust methods with a small stepsize policy, our robust method obtains\nthe faster iteration complexity of $\\mathcal{O}(\\epsilon^{-1})$ with oracle\ncomplexity of $(\\ln L)\\mathcal{O}(d\\epsilon^{-2})$ (up to logs). This matches,\nup to constants, the sample complexity of the sample average approximation\nestimator which does not assume additional problem information (such as $L$).\nDifferently from previous robust methods for ill-conditioned problems, we allow\nan unbounded feasible set and an oracle with multiplicative noise (MN) whose\nvariance is not necessarily uniformly bounded. These properties are seen in our\ncomplexity estimates which depend only on $L$ and local second or forth moments\nat solutions. The robustness and variance reduction properties of our DS-SA\nline search scheme come at the expense of nonmartingale-like dependencies (NMD)\ndue to the needed inner statistical estimation of a lower bound for $L$. In\norder to handle a NMD and a MN, our proofs rely on a novel localization\nargument based on empirical process theory. We also propose another robust\nmethod for SVIs over the wider class of H\\\"older continuous operators. \n\n"}
{"id": "1703.00405", "contents": "Title: Stability and performance analysis of linear positive systems with\n  delays using input-output methods Abstract: It is known that input-output approaches based on scaled small-gain theorems\nwith constant $D$-scalings and integral linear constraints are non-conservative\nfor the analysis of some classes of linear positive systems interconnected with\nuncertain linear operators. This dramatically contrasts with the case of\ngeneral linear systems with delays where input-output approaches provide, in\ngeneral, sufficient conditions only. Using these results we provide simple\nalternative proofs for many of the existing results on the stability of linear\npositive systems with discrete/distributed/neutral time-invariant/-varying\ndelays and linear difference equations. In particular, we give a simple proof\nfor the characterization of diagonal Riccati stability for systems with\ndiscrete-delays and generalize this equation to other types of delay systems.\nThe fact that all those results can be reproved in a very simple way\ndemonstrates the importance and the efficiency of the input-output framework\nfor the analysis of linear positive systems. The approach is also used to\nderive performance results evaluated in terms of the $L_1$-, $L_2$- and\n$L_\\infty$-gains. It is also flexible enough to be used for design purposes. \n\n"}
{"id": "1703.00410", "contents": "Title: Detecting Adversarial Samples from Artifacts Abstract: Deep neural networks (DNNs) are powerful nonlinear architectures that are\nknown to be robust to random perturbations of the input. However, these models\nare vulnerable to adversarial perturbations--small input changes crafted\nexplicitly to fool the model. In this paper, we ask whether a DNN can\ndistinguish adversarial samples from their normal and noisy counterparts. We\ninvestigate model confidence on adversarial samples by looking at Bayesian\nuncertainty estimates, available in dropout neural networks, and by performing\ndensity estimation in the subspace of deep features learned by the model. The\nresult is a method for implicit adversarial detection that is oblivious to the\nattack algorithm. We evaluate this method on a variety of standard datasets\nincluding MNIST and CIFAR-10 and show that it generalizes well across different\narchitectures and attacks. Our findings report that 85-93% ROC-AUC can be\nachieved on a number of standard classification tasks with a negative class\nthat consists of both normal and noisy samples. \n\n"}
{"id": "1703.01594", "contents": "Title: Graph sampling with determinantal processes Abstract: We present a new random sampling strategy for k-bandlimited signals defined\non graphs, based on determinantal point processes (DPP). For small graphs, ie,\nin cases where the spectrum of the graph is accessible, we exhibit a DPP\nsampling scheme that enables perfect recovery of bandlimited signals. For large\ngraphs, ie, in cases where the graph's spectrum is not accessible, we\ninvestigate, both theoretically and empirically, a sub-optimal but much faster\nDPP based on loop-erased random walks on the graph. Preliminary experiments\nshow promising results especially in cases where the number of measurements\nshould stay as small as possible and for graphs that have a strong community\nstructure. Our sampling scheme is efficient and can be applied to graphs with\nup to $10^6$ nodes. \n\n"}
{"id": "1703.01942", "contents": "Title: On Time-Consistent Solution to Time-Inconsistent Linear-Quadratic\n  Optimal Control of Discrete-Time Stochastic Systems Abstract: In this paper, we investigate a class of time-inconsistent discrete-time\nstochastic linear-quadratic optimal control problems, whose time-consistent\nsolutions consist of an open-loop equilibrium control and a linear feedback\nequilibrium strategy. The open-loop equilibrium control is defined for a given\ninitial pair, while the linear feedback equilibrium strategy is defined for all\nthe initial pairs. Maximum-principle-type necessary and sufficient conditions\ncontaining stationary and convexity are derived for the existence of these two\ntime-consistent solutions, respectively. Furthermore, for the case where the\nsystem matrices are independent of the initial time, we show that the existence\nof the open-loop equilibrium control for a given initial pair is equivalent to\nthe solvability of a set of nonsymmetric generalized difference Riccati\nequations and a set of linear difference equations. Moreover, the existence of\nlinear feedback equilibrium strategy is equivalent to the solvability of\nanother set of symmetric generalized difference Riccati equations. \n\n"}
{"id": "1703.02100", "contents": "Title: Guarantees for Greedy Maximization of Non-submodular Functions with\n  Applications Abstract: We investigate the performance of the standard Greedy algorithm for\ncardinality constrained maximization of non-submodular nondecreasing set\nfunctions. While there are strong theoretical guarantees on the performance of\nGreedy for maximizing submodular functions, there are few guarantees for\nnon-submodular ones. However, Greedy enjoys strong empirical performance for\nmany important non-submodular functions, e.g., the Bayesian A-optimality\nobjective in experimental design. We prove theoretical guarantees supporting\nthe empirical performance. Our guarantees are characterized by a combination of\nthe (generalized) curvature $\\alpha$ and the submodularity ratio $\\gamma$. In\nparticular, we prove that Greedy enjoys a tight approximation guarantee of\n$\\frac{1}{\\alpha}(1- e^{-\\gamma\\alpha})$ for cardinality constrained\nmaximization. In addition, we bound the submodularity ratio and curvature for\nseveral important real-world objectives, including the Bayesian A-optimality\nobjective, the determinantal function of a square submatrix and certain linear\nprograms with combinatorial constraints. We experimentally validate our\ntheoretical findings for both synthetic and real-world applications. \n\n"}
{"id": "1703.03864", "contents": "Title: Evolution Strategies as a Scalable Alternative to Reinforcement Learning Abstract: We explore the use of Evolution Strategies (ES), a class of black box\noptimization algorithms, as an alternative to popular MDP-based RL techniques\nsuch as Q-learning and Policy Gradients. Experiments on MuJoCo and Atari show\nthat ES is a viable solution strategy that scales extremely well with the\nnumber of CPUs available: By using a novel communication strategy based on\ncommon random numbers, our ES implementation only needs to communicate scalars,\nmaking it possible to scale to over a thousand parallel workers. This allows us\nto solve 3D humanoid walking in 10 minutes and obtain competitive results on\nmost Atari games after one hour of training. In addition, we highlight several\nadvantages of ES as a black box optimization technique: it is invariant to\naction frequency and delayed rewards, tolerant of extremely long horizons, and\ndoes not need temporal discounting or value function approximation. \n\n"}
{"id": "1703.05038", "contents": "Title: Harmonic Mean Iteratively Reweighted Least Squares for Low-Rank Matrix\n  Recovery Abstract: We propose a new iteratively reweighted least squares (IRLS) algorithm for\nthe recovery of a matrix $X \\in \\mathbb{C}^{d_1\\times d_2}$ of rank $r\n\\ll\\min(d_1,d_2)$ from incomplete linear observations, solving a sequence of\nlow complexity linear problems. The easily implementable algorithm, which we\ncall harmonic mean iteratively reweighted least squares (HM-IRLS), optimizes a\nnon-convex Schatten-$p$ quasi-norm penalization to promote low-rankness and\ncarries three major strengths, in particular for the matrix completion setting.\nFirst, we observe a remarkable global convergence behavior of the algorithm's\niterates to the low-rank matrix for relevant, interesting cases, for which any\nother state-of-the-art optimization approach fails the recovery. Secondly,\nHM-IRLS exhibits an empirical recovery probability close to $1$ even for a\nnumber of measurements very close to the theoretical lower bound $r (d_1 +d_2\n-r)$, i.e., already for significantly fewer linear observations than any other\ntractable approach in the literature. Thirdly, HM-IRLS exhibits a locally\nsuperlinear rate of convergence (of order $2-p$) if the linear observations\nfulfill a suitable null space property. While for the first two properties we\nhave so far only strong empirical evidence, we prove the third property as our\nmain theoretical result. \n\n"}
{"id": "1703.06104", "contents": "Title: Nonconvex One-bit Single-label Multi-label Learning Abstract: We study an extreme scenario in multi-label learning where each training\ninstance is endowed with a single one-bit label out of multiple labels. We\nformulate this problem as a non-trivial special case of one-bit rank-one matrix\nsensing and develop an efficient non-convex algorithm based on alternating\npower iteration. The proposed algorithm is able to recover the underlying\nlow-rank matrix model with linear convergence. For a rank-$k$ model with $d_1$\nfeatures and $d_2$ classes, the proposed algorithm achieves $O(\\epsilon)$\nrecovery error after retrieving $O(k^{1.5}d_1 d_2/\\epsilon)$ one-bit labels\nwithin $O(kd)$ memory. Our bound is nearly optimal in the order of\n$O(1/\\epsilon)$. This significantly improves the state-of-the-art sampling\ncomplexity of one-bit multi-label learning. We perform experiments to verify\nour theory and evaluate the performance of the proposed algorithm. \n\n"}
{"id": "1703.06461", "contents": "Title: Regress-Later Monte Carlo for Optimal Inventory Control with\n  applications in energy Abstract: We develop a Monte-Carlo based numerical method for solving discrete-time\nstochastic optimal control problems with inventory. These are optimal control\nproblems in which the control affects only a deterministically evolving\ninventory process on a compact state space while the random underlying process\nmanifests itself through the objective functional. We propose a Regress Later\nmodification of the traditional Regression Monte Carlo which allows to decouple\ninventory levels in two successive time steps and to include in the basis\nfunctions of the regression the dependence on the inventory levels. We develop\na backward construction of trajectories for the inventory which enables us to\nuse policy iteration of Longstaff-Schwartz type avoiding nested simulations.\nOur algorithm improves on the grid discretisation procedure largely used in\nliterature and practice, and on the recently proposed control randomisation by\n[Kharroubi et al. (2014) Monte Carlo Methods and Applications, 20(2), pp.\n145-165]. We validate our approach on three numerical examples: a benchmark\nproblem of energy arbitrage used to compare different methods available in\nliterature; a multi-dimensional problem of control of two connected water\nreservoirs; and a high-dimensional problem of the management of a battery with\nthe purpose of assisting the operations of a wind turbine in providing\nelectricity to a group of buildings in a cost effective way. \n\n"}
{"id": "1703.06807", "contents": "Title: Guaranteed Sufficient Decrease for Variance Reduced Stochastic Gradient\n  Descent Abstract: In this paper, we propose a novel sufficient decrease technique for variance\nreduced stochastic gradient descent methods such as SAG, SVRG and SAGA. In\norder to make sufficient decrease for stochastic optimization, we design a new\nsufficient decrease criterion, which yields sufficient decrease versions of\nvariance reduction algorithms such as SVRG-SD and SAGA-SD as a byproduct. We\nintroduce a coefficient to scale current iterate and satisfy the sufficient\ndecrease property, which takes the decisions to shrink, expand or move in the\nopposite direction, and then give two specific update rules of the coefficient\nfor Lasso and ridge regression. Moreover, we analyze the convergence properties\nof our algorithms for strongly convex problems, which show that both of our\nalgorithms attain linear convergence rates. We also provide the convergence\nguarantees of our algorithms for non-strongly convex problems. Our experimental\nresults further verify that our algorithms achieve significantly better\nperformance than their counterparts. \n\n"}
{"id": "1703.07461", "contents": "Title: Local convergence of the Levenberg-Marquardt method under H\\\"{o}lder\n  metric subregularity Abstract: We describe and analyse Levenberg-Marquardt methods for solving systems of\nnonlinear equations. More specifically, we propose an adaptive formula for the\nLevenberg-Marquardt parameter and analyse the local convergence of the method\nunder H\\\"{o}lder metric subregularity of the function defining the equation and\nH\\\"older continuity of its gradient mapping. Further, we analyse the local\nconvergence of the method under the additional assumption that the\n\\L{}ojasiewicz gradient inequality holds. We finally report encouraging\nnumerical results confirming the theoretical findings for the problem of\ncomputing moiety conserved steady states in biochemical reaction networks. This\nproblem can be cast as finding a solution of a system of nonlinear equations,\nwhere the associated mapping satisfies the \\L{}ojasiewicz gradient inequality\nassumption. \n\n"}
{"id": "1703.09631", "contents": "Title: Algebraic Variety Models for High-Rank Matrix Completion Abstract: We consider a generalization of low-rank matrix completion to the case where\nthe data belongs to an algebraic variety, i.e. each data point is a solution to\na system of polynomial equations. In this case the original matrix is possibly\nhigh-rank, but it becomes low-rank after mapping each column to a higher\ndimensional space of monomial features. Many well-studied extensions of linear\nmodels, including affine subspaces and their union, can be described by a\nvariety model. In addition, varieties can be used to model a richer class of\nnonlinear quadratic and higher degree curves and surfaces. We study the\nsampling requirements for matrix completion under a variety model with a focus\non a union of affine subspaces. We also propose an efficient matrix completion\nalgorithm that minimizes a convex or non-convex surrogate of the rank of the\nmatrix of monomial features. Our algorithm uses the well-known \"kernel trick\"\nto avoid working directly with the high-dimensional monomial matrix. We show\nthe proposed algorithm is able to recover synthetically generated data up to\nthe predicted sampling complexity bounds. The proposed algorithm also\noutperforms standard low rank matrix completion and subspace clustering\ntechniques in experiments with real data. \n\n"}
{"id": "1703.10973", "contents": "Title: Modified Interior-Point Method for Large-and-Sparse Low-Rank\n  Semidefinite Programs Abstract: Semidefinite programs (SDPs) are powerful theoretical tools that have been\nstudied for over two decades, but their practical use remains limited due to\ncomputational difficulties in solving large-scale, realistic-sized problems. In\nthis paper, we describe a modified interior-point method for the efficient\nsolution of large-and-sparse low-rank SDPs, which finds applications in graph\ntheory, approximation theory, control theory, sum-of-squares, etc. Given that\nthe problem data is large-and-sparse, conjugate gradients (CG) can be used to\navoid forming, storing, and factoring the large and fully-dense interior-point\nHessian matrix, but the resulting convergence rate is usually slow due to\nill-conditioning. Our central insight is that, for a rank-$k$, size-$n$ SDP,\nthe Hessian matrix is ill-conditioned only due to a rank-$nk$ perturbation,\nwhich can be explicitly computed using a size-$n$ eigendecomposition. We\nconstruct a preconditioner to \"correct\" the low-rank perturbation, thereby\nallowing preconditioned CG to solve the Hessian equation in a few tens of\niterations. This modification is incorporated within SeDuMi, and used to reduce\nthe solution time and memory requirements of large-scale matrix-completion\nproblems by several orders of magnitude. \n\n"}
{"id": "1704.00116", "contents": "Title: Stochastic L-BFGS: Improved Convergence Rates and Practical Acceleration\n  Strategies Abstract: We revisit the stochastic limited-memory BFGS (L-BFGS) algorithm. By\nproposing a new framework for the convergence analysis, we prove improved\nconvergence rates and computational complexities of the stochastic L-BFGS\nalgorithms compared to previous works. In addition, we propose several\npractical acceleration strategies to speed up the empirical performance of such\nalgorithms. We also provide theoretical analyses for most of the strategies.\nExperiments on large-scale logistic and ridge regression problems demonstrate\nthat our proposed strategies yield significant improvements vis-\\`a-vis\ncompeting state-of-the-art algorithms. \n\n"}
{"id": "1704.00387", "contents": "Title: Identifying networks with common organizational principles Abstract: Many complex systems can be represented as networks, and the problem of\nnetwork comparison is becoming increasingly relevant. There are many techniques\nfor network comparison, from simply comparing network summary statistics to\nsophisticated but computationally costly alignment-based approaches. Yet it\nremains challenging to accurately cluster networks that are of a different size\nand density, but hypothesized to be structurally similar. In this paper, we\naddress this problem by introducing a new network comparison methodology that\nis aimed at identifying common organizational principles in networks. The\nmethodology is simple, intuitive and applicable in a wide variety of settings\nranging from the functional classification of proteins to tracking the\nevolution of a world trade network. \n\n"}
{"id": "1704.01858", "contents": "Title: An Online Hierarchical Algorithm for Extreme Clustering Abstract: Many modern clustering methods scale well to a large number of data items, N,\nbut not to a large number of clusters, K. This paper introduces PERCH, a new\nnon-greedy algorithm for online hierarchical clustering that scales to both\nmassive N and K--a problem setting we term extreme clustering. Our algorithm\nefficiently routes new data points to the leaves of an incrementally-built\ntree. Motivated by the desire for both accuracy and speed, our approach\nperforms tree rotations for the sake of enhancing subtree purity and\nencouraging balancedness. We prove that, under a natural separability\nassumption, our non-greedy algorithm will produce trees with perfect dendrogram\npurity regardless of online data arrival order. Our experiments demonstrate\nthat PERCH constructs more accurate trees than other tree-building clustering\nalgorithms and scales well with both N and K, achieving a higher quality\nclustering than the strongest flat clustering competitor in nearly half the\ntime. \n\n"}
{"id": "1704.01920", "contents": "Title: Encoder Based Lifelong Learning Abstract: This paper introduces a new lifelong learning solution where a single model\nis trained for a sequence of tasks. The main challenge that vision systems face\nin this context is catastrophic forgetting: as they tend to adapt to the most\nrecently seen task, they lose performance on the tasks that were learned\npreviously. Our method aims at preserving the knowledge of the previous tasks\nwhile learning a new one by using autoencoders. For each task, an\nunder-complete autoencoder is learned, capturing the features that are crucial\nfor its achievement. When a new task is presented to the system, we prevent the\nreconstructions of the features with these autoencoders from changing, which\nhas the effect of preserving the information on which the previous tasks are\nmainly relying. At the same time, the features are given space to adjust to the\nmost recent environment as only their projection into a low dimension\nsubmanifold is controlled. The proposed system is evaluated on image\nclassification tasks and shows a reduction of forgetting over the\nstate-of-the-art \n\n"}
{"id": "1704.02971", "contents": "Title: A Dual-Stage Attention-Based Recurrent Neural Network for Time Series\n  Prediction Abstract: The Nonlinear autoregressive exogenous (NARX) model, which predicts the\ncurrent value of a time series based upon its previous values as well as the\ncurrent and past values of multiple driving (exogenous) series, has been\nstudied for decades. Despite the fact that various NARX models have been\ndeveloped, few of them can capture the long-term temporal dependencies\nappropriately and select the relevant driving series to make predictions. In\nthis paper, we propose a dual-stage attention-based recurrent neural network\n(DA-RNN) to address these two issues. In the first stage, we introduce an input\nattention mechanism to adaptively extract relevant driving series (a.k.a.,\ninput features) at each time step by referring to the previous encoder hidden\nstate. In the second stage, we use a temporal attention mechanism to select\nrelevant encoder hidden states across all time steps. With this dual-stage\nattention scheme, our model can not only make predictions effectively, but can\nalso be easily interpreted. Thorough empirical studies based upon the SML 2010\ndataset and the NASDAQ 100 Stock dataset demonstrate that the DA-RNN can\noutperform state-of-the-art methods for time series prediction. \n\n"}
{"id": "1704.03261", "contents": "Title: Modeling of Information Diffusion on Social Networks with Applications\n  to WeChat Abstract: Traces of user activities recorded in online social networks such as the\ncreation, viewing and forwarding/sharing of information over time open new\npossibilities to quantitatively and systematically understand the information\ndiffusion process on social networks. From an online social network like\nWeChat, we could collect a large number of information cascade trees, each of\nwhich tells the spreading trajectory of a message/information such as which\nuser creates the information and which users view or forward the information\nshared by which neighbors. In this work, we propose two heterogeneous\nnon-linear models. Both models are validated by the WeChat data in reproducing\nand explaining key features of cascade trees. Specifically, we firstly apply\nthe Random Recursive Tree (RRT) to model the cascade tree topologies, capturing\nkey features, i.e. the average path length and degree variance of a cascade\ntree in relation to the number of nodes (size) of the tree. The RRT model with\na single parameter $\\theta$ describes the growth mechanism of a tree, where a\nnode in the existing tree has a probability $d_i^{\\theta}$ of being connected\nto a newly added node that depends on the degree $d_i$ of the existing node.\nThe identified parameter $\\theta$ quantifies the relative depth or broadness of\nthe cascade trees, indicating that information propagates via a star-like\nbroadcasting or viral-like hop by hop spreading. The RRT model explains the\nappearance of hubs, thus a possibly smaller average path length as the cascade\nsize increases, as observed in WeChat. We further propose the stochastic\nSusceptible View Forward Removed (SVFR) model to depict the dynamic user\nbehaviors including creating, viewing, forwarding and ignoring a message on a\ngiven social network. \n\n"}
{"id": "1704.03443", "contents": "Title: Solving the L1 regularized least square problem via a box-constrained\n  smooth minimization Abstract: In this paper, an equivalent smooth minimization for the L1 regularized least\nsquare problem is proposed. The proposed problem is a convex box-constrained\nsmooth minimization which allows applying fast optimization methods to find its\nsolution. Further, it is investigated that the property \"the dual of dual is\nprimal\" holds for the L1 regularized least square problem. A solver for the\nsmooth problem is proposed, and its affinity to the proximal gradient is shown.\nFinally, the experiments on L1 and total variation regularized problems are\nperformed, and the corresponding results are reported. \n\n"}
{"id": "1704.03816", "contents": "Title: Dynamic Signaling Games with Quadratic Criteria under Nash and\n  Stackelberg Equilibria Abstract: This paper considers dynamic (multi-stage) signaling games involving an\nencoder and a decoder who have subjective models on the cost functions. We\nconsider both Nash (simultaneous-move) and Stackelberg (leader-follower)\nequilibria of dynamic signaling games under quadratic criteria. For the\nmulti-stage scalar cheap talk, we show that the final stage equilibrium is\nalways quantized and under further conditions the equilibria for all time\nstages must be quantized. In contrast, the Stackelberg equilibria are always\nfully revealing. In the multi-stage signaling game where the transmission of a\nGauss-Markov source over a memoryless Gaussian channel is considered, affine\npolicies constitute an invariant subspace under best response maps for Nash\nequilibria; whereas the Stackelberg equilibria always admit linear policies for\nscalar sources but such policies may be non-linear for multi-dimensional\nsources. We obtain an explicit recursion for optimal linear encoding policies\nfor multi-dimensional sources, and derive conditions under which Stackelberg\nequilibria are informative. \n\n"}
{"id": "1704.04118", "contents": "Title: From Data to Decisions: Distributionally Robust Optimization is Optimal Abstract: We study stochastic programs where the decision-maker cannot observe the\ndistribution of the exogenous uncertainties but has access to a finite set of\nindependent samples from this distribution. In this setting, the goal is to\nfind a procedure that transforms the data to an estimate of the expected cost\nfunction under the unknown data-generating distribution, i.e., a predictor, and\nan optimizer of the estimated cost function that serves as a near-optimal\ncandidate decision, i.e., a prescriptor. As functions of the data, predictors\nand prescriptors constitute statistical estimators. We propose a\nmeta-optimization problem to find the least conservative predictors and\nprescriptors subject to constraints on their out-of-sample disappointment. The\nout-of-sample disappointment quantifies the probability that the actual\nexpected cost of the candidate decision under the unknown true distribution\nexceeds its predicted cost. Leveraging tools from large deviations theory, we\nprove that this meta-optimization problem admits a unique solution: The best\npredictor-prescriptor pair is obtained by solving a distributionally robust\noptimization problem over all distributions within a given relative entropy\ndistance from the empirical distribution of the data. \n\n"}
{"id": "1704.04997", "contents": "Title: Multimodal Prediction and Personalization of Photo Edits with Deep\n  Generative Models Abstract: Professional-grade software applications are powerful but\ncomplicated$-$expert users can achieve impressive results, but novices often\nstruggle to complete even basic tasks. Photo editing is a prime example: after\nloading a photo, the user is confronted with an array of cryptic sliders like\n\"clarity\", \"temp\", and \"highlights\". An automatically generated suggestion\ncould help, but there is no single \"correct\" edit for a given image$-$different\nexperts may make very different aesthetic decisions when faced with the same\nimage, and a single expert may make different choices depending on the intended\nuse of the image (or on a whim). We therefore want a system that can propose\nmultiple diverse, high-quality edits while also learning from and adapting to a\nuser's aesthetic preferences. In this work, we develop a statistical model that\nmeets these objectives. Our model builds on recent advances in neural network\ngenerative modeling and scalable inference, and uses hierarchical structure to\nlearn editing patterns across many diverse users. Empirically, we find that our\nmodel outperforms other approaches on this challenging multimodal prediction\ntask. \n\n"}
{"id": "1704.05434", "contents": "Title: Distributed Dynamic Event-Triggered Control for Multi-Agent Systems Abstract: We propose two distributed dynamic triggering laws to solve the consensus\nproblem for multi-agent systems with event-triggered control. Compared with\nexisting triggering laws, the proposed triggering laws involve internal dynamic\nvariables which play an essential role to guarantee that the triggering time\nsequence does not exhibit Zeno behavior. Some existing triggering laws are\nspecial cases of our dynamic triggering laws. Under the condition that the\nunderlying graph is undirected and connected, it is proven that the proposed\ndynamic triggering laws together with the event-triggered control make the\nstate of each agent converges exponentially to the average of the agents'\ninitial states. Numerical simulations illustrate the effectiveness of the\ntheoretical results and show that the dynamic triggering laws lead to reduction\nof actuation updates and inter-agent communications. \n\n"}
{"id": "1704.05690", "contents": "Title: A Fractional Gauss-Jacobi quadrature rule for approximating fractional\n  integrals and derivatives Abstract: We introduce an efficient algorithm for computing fractional integrals and\nderivatives and apply it for solving problems of the calculus of variations of\nfractional order. The proposed approximations are particularly useful for\nsolving fractional boundary value problems. As an application, we solve a\nspecial class of fractional Euler-Lagrange equations. The method is based on\nHale and Townsend algorithm for finding the roots and weights of the fractional\nGauss-Jacobi quadrature rule and the predictor-corrector method introduced by\nDiethelm for solving fractional differential equations. Illustrative examples\nshow that the given method is more accurate than the one introduced in [Comput.\nMath. Appl. 66 (2013), no. 5, 597--607], which uses the Golub-Welsch algorithm\nfor evaluating fractional directional integrals. \n\n"}
{"id": "1704.05786", "contents": "Title: Importance Sampled Stochastic Optimization for Variational Inference Abstract: Variational inference approximates the posterior distribution of a\nprobabilistic model with a parameterized density by maximizing a lower bound\nfor the model evidence. Modern solutions fit a flexible approximation with\nstochastic gradient descent, using Monte Carlo approximation for the gradients.\nThis enables variational inference for arbitrary differentiable probabilistic\nmodels, and consequently makes variational inference feasible for probabilistic\nprogramming languages. In this work we develop more efficient inference\nalgorithms for the task by considering importance sampling estimates for the\ngradients. We show how the gradient with respect to the approximation\nparameters can often be evaluated efficiently without needing to re-compute\ngradients of the model itself, and then proceed to derive practical algorithms\nthat use importance sampled estimates to speed up computation.We present\nimportance sampled stochastic gradient descent that outperforms standard\nstochastic gradient descent by a clear margin for a range of models, and\nprovide a justifiable variant of stochastic average gradients for variational\ninference. \n\n"}
{"id": "1704.06131", "contents": "Title: Learning to Acquire Information Abstract: We consider the problem of diagnosis where a set of simple observations are\nused to infer a potentially complex hidden hypothesis. Finding the optimal\nsubset of observations is intractable in general, thus we focus on the problem\nof active diagnosis, where the agent selects the next most-informative\nobservation based on the results of previous observations. We show that under\nthe assumption of uniform observation entropy, one can build an implication\nmodel which directly predicts the outcome of the potential next observation\nconditioned on the results of past observations, and selects the observation\nwith the maximum entropy. This approach enjoys reduced computation complexity\nby bypassing the complicated hypothesis space, and can be trained on\nobservation data alone, learning how to query without knowledge of the hidden\nhypothesis. \n\n"}
{"id": "1704.06872", "contents": "Title: Controlling the Kelvin Force: Basic Strategies and Applications to\n  Magnetic Drug Targeting Abstract: Motivated by problems arising in magnetic drug targeting, we propose to\ngenerate an almost constant Kelvin (magnetic) force in a target subdomain,\nmoving along a prescribed trajectory. This is carried out by solving a\nminimization problem with a tracking type cost functional. The magnetic sources\nare assumed to be dipoles and the control variables are the magnetic field\nintensity, the source location and the magnetic field direction. The resulting\nmagnetic field is shown to effectively steer the drug concentration, governed\nby a drift-diffusion PDE, from an initial to a desired location with limited\nspreading. \n\n"}
{"id": "1704.07228", "contents": "Title: Learning from Comparisons and Choices Abstract: When tracking user-specific online activities, each user's preference is\nrevealed in the form of choices and comparisons. For example, a user's purchase\nhistory is a record of her choices, i.e. which item was chosen among a subset\nof offerings. A user's preferences can be observed either explicitly as in\nmovie ratings or implicitly as in viewing times of news articles. Given such\nindividualized ordinal data in the form of comparisons and choices, we address\nthe problem of collaboratively learning representations of the users and the\nitems. The learned features can be used to predict a user's preference of an\nunseen item to be used in recommendation systems. This also allows one to\ncompute similarities among users and items to be used for categorization and\nsearch. Motivated by the empirical successes of the MultiNomial Logit (MNL)\nmodel in marketing and transportation, and also more recent successes in word\nembedding and crowdsourced image embedding, we pose this problem as learning\nthe MNL model parameters that best explain the data. We propose a convex\nrelaxation for learning the MNL model, and show that it is minimax optimal up\nto a logarithmic factor by comparing its performance to a fundamental lower\nbound. This characterizes the minimax sample complexity of the problem, and\nproves that the proposed estimator cannot be improved upon other than by a\nlogarithmic factor. Further, the analysis identifies how the accuracy depends\non the topology of sampling via the spectrum of the sampling graph. This\nprovides a guideline for designing surveys when one can choose which items are\nto be compared. This is accompanied by numerical simulations on synthetic and\nreal data sets, confirming our theoretical predictions. \n\n"}
{"id": "1704.07943", "contents": "Title: Reward Maximization Under Uncertainty: Leveraging Side-Observations on\n  Networks Abstract: We study the stochastic multi-armed bandit (MAB) problem in the presence of\nside-observations across actions that occur as a result of an underlying\nnetwork structure. In our model, a bipartite graph captures the relationship\nbetween actions and a common set of unknowns such that choosing an action\nreveals observations for the unknowns that it is connected to. This models a\ncommon scenario in online social networks where users respond to their friends'\nactivity, thus providing side information about each other's preferences. Our\ncontributions are as follows: 1) We derive an asymptotic lower bound (with\nrespect to time) as a function of the bi-partite network structure on the\nregret of any uniformly good policy that achieves the maximum long-term average\nreward. 2) We propose two policies - a randomized policy; and a policy based on\nthe well-known upper confidence bound (UCB) policies - both of which explore\neach action at a rate that is a function of its network position. We show,\nunder mild assumptions, that these policies achieve the asymptotic lower bound\non the regret up to a multiplicative factor, independent of the network\nstructure. Finally, we use numerical examples on a real-world social network\nand a routing example network to demonstrate the benefits obtained by our\npolicies over other existing policies. \n\n"}
{"id": "1704.08002", "contents": "Title: A second-order stochastic maximum principle for generalized mean-field\n  control problem Abstract: In this paper, we study the generalized mean-field stochastic control problem\nwhen the usual stochastic maximum principle (SMP) is not applicable due to the\nsingularity of the Hamiltonian function. In this case, we derive a second order\nSMP. We introduce the adjoint process by the generalized mean-field backward\nstochastic differential equation. The keys in the proofs are the expansion of\nthe cost functional in terms of a perturbation parameter, and the use of the\nrange theorem for vector-valued measures. \n\n"}
{"id": "1704.08197", "contents": "Title: Character Networks and Book Genre Classification Abstract: We compare the social character networks of biographical, legendary and\nfictional texts, in search for marks of genre differentiation. We examine the\ndegree distribution of character appearance and find a power law that does not\ndepend on the literary genre or historical content. We also analyze local and\nglobal complex networks measures, in particular, correlation plots between the\nrecently introduced Lobby (or Hirsh $H(1)$) index and Degree, Betweenness and\nCloseness centralities. Assortativity plots, which previous literature claims\nto separate fictional from real social networks, were also studied. We've found\nno relevant differences in the books for these network measures and we give a\nplausible explanation why the previous assortativity result is not correct. \n\n"}
{"id": "1705.02502", "contents": "Title: Linearized ADMM for Non-convex Non-smooth Optimization with Convergence\n  Analysis Abstract: Linearized alternating direction method of multipliers (ADMM) as an extension\nof ADMM has been widely used to solve linearly constrained problems in signal\nprocessing, machine leaning, communications, and many other fields. Despite its\nbroad applications in nonconvex optimization, for a great number of nonconvex\nand nonsmooth objective functions, its theoretical convergence guarantee is\nstill an open problem. In this paper, we propose a two-block linearized ADMM\nand a multi-block parallel linearized ADMM for problems with nonconvex and\nnonsmooth objectives. Mathematically, we present that the algorithms can\nconverge for a broader class of objective functions under less strict\nassumptions compared with previous works. Furthermore, our proposed algorithm\ncan update coupled variables in parallel and work for less restrictive\nnonconvex problems, where the traditional ADMM may have difficulties in solving\nsubproblems. \n\n"}
{"id": "1705.02802", "contents": "Title: Directed Information as Privacy Measure in Cloud-based Control Abstract: We consider cloud-based control scenarios in which clients with local control\ntasks outsource their computational or physical duties to a cloud service\nprovider. In order to address privacy concerns in such a control architecture,\nwe first investigate the issue of finding an appropriate privacy measure for\nclients who desire to keep local state information as private as possible\nduring the control operation. Specifically, we justify the use of Kramer's\nnotion of causally conditioned directed information as a measure of privacy\nloss based on an axiomatic argument. Then we propose a methodology to design an\noptimal \"privacy filter\" that minimizes privacy loss while a given level of\ncontrol performance is guaranteed. We show in particular that the optimal\nprivacy filter for cloud-based Linear-Quadratic-Gaussian (LQG) control can be\nsynthesized by a Linear-Matrix-Inequality (LMI) algorithm. The trade-off in the\ndesign is illustrated by a numerical example. \n\n"}
{"id": "1705.04535", "contents": "Title: Dynamic Models of Wasserstein-1-Type Unbalanced Transport Abstract: We consider a class of convex optimization problems modelling temporal mass\ntransport and mass change between two given mass distributions (the so-called\ndynamic formulation of unbalanced transport), where we focus on those models\nfor which transport costs are proportional to transport distance. For those\nmodels we derive an equivalent, computationally more efficient static\nformulation, we perform a detailed analysis of the model optimizers and the\nassociated optimal mass change and transport, and we examine which static\nmodels are generated by a corresponding equivalent dynamic one. Alongside we\ndiscuss thoroughly how the employed model formulations relate to other\nformulations found in the literature. \n\n"}
{"id": "1705.05615", "contents": "Title: Learning Edge Representations via Low-Rank Asymmetric Projections Abstract: We propose a new method for embedding graphs while preserving directed edge\ninformation. Learning such continuous-space vector representations (or\nembeddings) of nodes in a graph is an important first step for using network\ninformation (from social networks, user-item graphs, knowledge bases, etc.) in\nmany machine learning tasks.\n  Unlike previous work, we (1) explicitly model an edge as a function of node\nembeddings, and we (2) propose a novel objective, the \"graph likelihood\", which\ncontrasts information from sampled random walks with non-existent edges.\nIndividually, both of these contributions improve the learned representations,\nespecially when there are memory constraints on the total size of the\nembeddings. When combined, our contributions enable us to significantly improve\nthe state-of-the-art by learning more concise representations that better\npreserve the graph structure.\n  We evaluate our method on a variety of link-prediction task including social\nnetworks, collaboration networks, and protein interactions, showing that our\nproposed method learn representations with error reductions of up to 76% and\n55%, on directed and undirected graphs. In addition, we show that the\nrepresentations learned by our method are quite space efficient, producing\nembeddings which have higher structure-preserving accuracy but are 10 times\nsmaller. \n\n"}
{"id": "1705.05915", "contents": "Title: Lifted Polymatroid Inequalities for Mean-Risk Optimization with\n  Indicator Variables Abstract: We investigate a mixed 0-1 conic quadratic optimization problem with\nindicator variables arising in mean-risk optimization. The indicator variables\nare often used to model non-convexities such as fixed charges or cardinality\nconstraints. Observing that the problem reduces to a submodular function\nminimization for its binary restriction, we derive three classes of strong\nconvex valid inequalities by lifting the polymatroid inequalities on the binary\nvariables. Computational experiments demonstrate the effectiveness of the\ninequalities in strengthening the convex relaxations and, thereby, improving\nthe solution times for mean-risk problems with fixed charges and cardinality\nconstraints significantly. \n\n"}
{"id": "1705.05950", "contents": "Title: Kernel clustering: density biases and solutions Abstract: Kernel methods are popular in clustering due to their generality and\ndiscriminating power. However, we show that many kernel clustering criteria\nhave density biases theoretically explaining some practically significant\nartifacts empirically observed in the past. For example, we provide conditions\nand formally prove the density mode isolation bias in kernel K-means for a\ncommon class of kernels. We call it Breiman's bias due to its similarity to the\nhistogram mode isolation previously discovered by Breiman in decision tree\nlearning with Gini impurity. We also extend our analysis to other popular\nkernel clustering methods, e.g. average/normalized cut or dominant sets, where\ndensity biases can take different forms. For example, splitting isolated points\nby cut-based criteria is essentially the sparsest subset bias, which is the\nopposite of the density mode bias. Our findings suggest that a principled\nsolution for density biases in kernel clustering should directly address data\ninhomogeneity. We show that density equalization can be implicitly achieved\nusing either locally adaptive weights or locally adaptive kernels. Moreover,\ndensity equalization makes many popular kernel clustering objectives\nequivalent. Our synthetic and real data experiments illustrate density biases\nand proposed solutions. We anticipate that theoretical understanding of kernel\nclustering limitations and their principled solutions will be important for a\nbroad spectrum of data analysis applications across the disciplines. \n\n"}
{"id": "1705.08291", "contents": "Title: Sensitivity analysis of the utility maximization problem with respect to\n  model perturbations Abstract: We study the sensitivity of the expected utility maximization problem in a\ncontinuous semi-martingale market with respect to small changes in the market\nprice of risk. Assuming that the preferences of a rational economic agent are\nmodeled with a general utility function, we obtain a second-order expansion of\nthe value function, a first-order approximation of the terminal wealth, and\nconstruct trading strategies that match the indirect utility function up to the\nsecond order. If a risk-tolerance wealth process exists, using it as a\nnum\\'eraire and under an appropriate change of measure, we reduce the\napproximation problem to a Kunita-Watanabe decomposition. \n\n"}
{"id": "1705.08360", "contents": "Title: Efficient and principled score estimation with Nystr\\\"om kernel\n  exponential families Abstract: We propose a fast method with statistical guarantees for learning an\nexponential family density model where the natural parameter is in a\nreproducing kernel Hilbert space, and may be infinite-dimensional. The model is\nlearned by fitting the derivative of the log density, the score, thus avoiding\nthe need to compute a normalization constant. Our approach improves the\ncomputational efficiency of an earlier solution by using a low-rank,\nNystr\\\"om-like solution. The new solution retains the consistency and\nconvergence rates of the full-rank solution (exactly in Fisher distance, and\nnearly in other distances), with guarantees on the degree of cost and storage\nreduction. We evaluate the method in experiments on density estimation and in\nthe construction of an adaptive Hamiltonian Monte Carlo sampler. Compared to an\nexisting score learning approach using a denoising autoencoder, our estimator\nis empirically more data-efficient when estimating the score, runs faster, and\nhas fewer parameters (which can be tuned in a principled and interpretable\nway), in addition to providing statistical guarantees. \n\n"}
{"id": "1705.08494", "contents": "Title: Asynchronous Coordinate Descent under More Realistic Assumptions Abstract: Asynchronous-parallel algorithms have the potential to vastly speed up\nalgorithms by eliminating costly synchronization. However, our understanding to\nthese algorithms is limited because the current convergence of asynchronous\n(block) coordinate descent algorithms are based on somewhat unrealistic\nassumptions. In particular, the age of the shared optimization variables being\nused to update a block is assumed to be independent of the block being updated.\nAlso, it is assumed that the updates are applied to randomly chosen blocks. In\nthis paper, we argue that these assumptions either fail to hold or will imply\nless efficient implementations. We then prove the convergence of\nasynchronous-parallel block coordinate descent under more realistic\nassumptions, in particular, always without the independence assumption. The\nanalysis permits both the deterministic (essentially) cyclic and random rules\nfor block choices. Because a bound on the asynchronous delays may or may not be\navailable, we establish convergence for both bounded delays and unbounded\ndelays. The analysis also covers nonconvex, weakly convex, and strongly convex\nfunctions. We construct Lyapunov functions that directly model both objective\nprogress and delays, so delays are not treated errors or noise. A\ncontinuous-time ODE is provided to explain the construction at a high level. \n\n"}
{"id": "1705.08941", "contents": "Title: Dual Dynamic Programming with cut selection: convergence proof and\n  numerical experiments Abstract: We consider convex optimization problems formulated using dynamic programming\nequations. Such problems can be solved using the Dual Dynamic Programming\nalgorithm combined with the Level 1 cut selection strategy or the Territory\nalgorithm to select the most relevant Benders cuts. We propose a limited memory\nvariant of Level 1 and show the convergence of DDP combined with the Territory\nalgorithm, Level 1 or its variant for nonlinear optimization problems. In the\nspecial case of linear programs, we show convergence in a finite number of\niterations. Numerical simulations illustrate the interest of our variant and\nshow that it can be much quicker than a simplex algorithm on some large\ninstances of portfolio selection and inventory problems. \n\n"}
{"id": "1705.09199", "contents": "Title: Non-parametric estimation of Jensen-Shannon Divergence in Generative\n  Adversarial Network training Abstract: Generative Adversarial Networks (GANs) have become a widely popular framework\nfor generative modelling of high-dimensional datasets. However their training\nis well-known to be difficult. This work presents a rigorous statistical\nanalysis of GANs providing straight-forward explanations for common training\npathologies such as vanishing gradients. Furthermore, it proposes a new\ntraining objective, Kernel GANs, and demonstrates its practical effectiveness\non large-scale real-world data sets. A key element in the analysis is the\ndistinction between training with respect to the (unknown) data distribution,\nand its empirical counterpart. To overcome issues in GAN training, we pursue\nthe idea of smoothing the Jensen-Shannon Divergence (JSD) by incorporating\nnoise in the input distributions of the discriminator. As we show, this\neffectively leads to an empirical version of the JSD in which the true and the\ngenerator densities are replaced by kernel density estimates, which leads to\nKernel GANs. \n\n"}
{"id": "1705.09383", "contents": "Title: Uniqueness of optimal solutions for semi-discrete transport with p-norm\n  cost functions Abstract: Semi-discrete transport can be characterized in terms of real-valued shifts.\nOften, but not always, the solution to the shift-characterized problem\npartitions the continuous region. This paper gives examples of when\npartitioning fails, and offers a large class of semi-discrete transport\nproblems where the shift-characterized solution is always a partition. \n\n"}
{"id": "1705.10372", "contents": "Title: A New Voltage Stability-Constrained Optimal Power Flow Model: Sufficient\n  Condition, SOCP Representation, and Relaxation Abstract: A simple characterization of the solvability of power flow equations is of\ngreat importance in the monitoring, control, and protection of power systems.\nIn this paper, we introduce a sufficient condition for power flow Jacobian\nnonsingularity. We show that this condition is second-order conic representable\nwhen load powers are fixed. Through the incorporation of the sufficient\ncondition, we propose a voltage stability-constrained optimal power flow\n(VSC-OPF) formulation as a second-order cone program (SOCP). An approximate\nmodel is introduced to improve the scalability of the formulation to larger\nsystems. Extensive computation results on Matpower and NESTA instances confirm\nthe effectiveness and efficiency of the formulation. \n\n"}
{"id": "1705.10388", "contents": "Title: Model Selection in Bayesian Neural Networks via Horseshoe Priors Abstract: Bayesian Neural Networks (BNNs) have recently received increasing attention\nfor their ability to provide well-calibrated posterior uncertainties. However,\nmodel selection---even choosing the number of nodes---remains an open question.\nIn this work, we apply a horseshoe prior over node pre-activations of a\nBayesian neural network, which effectively turns off nodes that do not help\nexplain the data. We demonstrate that our prior prevents the BNN from\nunder-fitting even when the number of nodes required is grossly over-estimated.\nMoreover, this model selection over the number of nodes doesn't come at the\nexpense of predictive or computational performance; in fact, we learn smaller\nnetworks with comparable predictive performance to current approaches. \n\n"}
{"id": "1705.10407", "contents": "Title: Solving Almost all Systems of Random Quadratic Equations Abstract: This paper deals with finding an $n$-dimensional solution $x$ to a system of\nquadratic equations of the form $y_i=|\\langle{a}_i,x\\rangle|^2$ for $1\\le i \\le\nm$, which is also known as phase retrieval and is NP-hard in general. We put\nforth a novel procedure for minimizing the amplitude-based least-squares\nempirical loss, that starts with a weighted maximal correlation initialization\nobtainable with a few power or Lanczos iterations, followed by successive\nrefinements based upon a sequence of iteratively reweighted (generalized)\ngradient iterations. The two (both the initialization and gradient flow) stages\ndistinguish themselves from prior contributions by the inclusion of a fresh\n(re)weighting regularization technique. The overall algorithm is conceptually\nsimple, numerically scalable, and easy-to-implement. For certain random\nmeasurement models, the novel procedure is shown capable of finding the true\nsolution $x$ in time proportional to reading the data $\\{(a_i;y_i)\\}_{1\\le i\n\\le m}$. This holds with high probability and without extra assumption on the\nsignal $x$ to be recovered, provided that the number $m$ of equations is some\nconstant $c>0$ times the number $n$ of unknowns in the signal vector, namely,\n$m>cn$. Empirically, the upshots of this contribution are: i) (almost) $100\\%$\nperfect signal recovery in the high-dimensional (say e.g., $n\\ge 2,000$) regime\ngiven only an information-theoretic limit number of noiseless equations,\nnamely, $m=2n-1$ in the real-valued Gaussian case; and, ii) (nearly) optimal\nstatistical accuracy in the presence of additive noise of bounded support.\nFinally, substantial numerical tests using both synthetic data and real images\ncorroborate markedly improved signal recovery performance and computational\nefficiency of our novel procedure relative to state-of-the-art approaches. \n\n"}
{"id": "1705.10819", "contents": "Title: Surface Networks Abstract: We study data-driven representations for three-dimensional triangle meshes,\nwhich are one of the prevalent objects used to represent 3D geometry. Recent\nworks have developed models that exploit the intrinsic geometry of manifolds\nand graphs, namely the Graph Neural Networks (GNNs) and its spectral variants,\nwhich learn from the local metric tensor via the Laplacian operator. Despite\noffering excellent sample complexity and built-in invariances, intrinsic\ngeometry alone is invariant to isometric deformations, making it unsuitable for\nmany applications. To overcome this limitation, we propose several upgrades to\nGNNs to leverage extrinsic differential geometry properties of\nthree-dimensional surfaces, increasing its modeling power.\n  In particular, we propose to exploit the Dirac operator, whose spectrum\ndetects principal curvature directions --- this is in stark contrast with the\nclassical Laplace operator, which directly measures mean curvature. We coin the\nresulting models \\emph{Surface Networks (SN)}. We prove that these models\ndefine shape representations that are stable to deformation and to\ndiscretization, and we demonstrate the efficiency and versatility of SNs on two\nchallenging tasks: temporal prediction of mesh deformations under non-linear\ndynamics and generative models using a variational autoencoder framework with\nencoders/decoders given by SNs. \n\n"}
{"id": "1705.10915", "contents": "Title: Unsupervised Learning of Disentangled Representations from Video Abstract: We present a new model DrNET that learns disentangled image representations\nfrom video. Our approach leverages the temporal coherence of video and a novel\nadversarial loss to learn a representation that factorizes each frame into a\nstationary part and a temporally varying component. The disentangled\nrepresentation can be used for a range of tasks. For example, applying a\nstandard LSTM to the time-vary components enables prediction of future frames.\nWe evaluate our approach on a range of synthetic and real videos, demonstrating\nthe ability to coherently generate hundreds of steps into the future. \n\n"}
{"id": "1706.00098", "contents": "Title: Bayesian $l_0$-regularized Least Squares Abstract: Bayesian $l_0$-regularized least squares is a variable selection technique\nfor high dimensional predictors. The challenge is optimizing a non-convex\nobjective function via search over model space consisting of all possible\npredictor combinations. Spike-and-slab (a.k.a. Bernoulli-Gaussian) priors are\nthe gold standard for Bayesian variable selection, with a caveat of\ncomputational speed and scalability. Single Best Replacement (SBR) provides a\nfast scalable alternative. We provide a link between Bayesian regularization\nand proximal updating, which provides an equivalence between finding a\nposterior mode and a posterior mean with a different regularization prior. This\nallows us to use SBR to find the spike-and-slab estimator. To illustrate our\nmethodology, we provide simulation evidence and a real data example on the\nstatistical properties and computational efficiency of SBR versus direct\nposterior sampling using spike-and-slab priors. Finally, we conclude with\ndirections for future research. \n\n"}
{"id": "1706.00473", "contents": "Title: Deep Learning: A Bayesian Perspective Abstract: Deep learning is a form of machine learning for nonlinear high dimensional\npattern matching and prediction. By taking a Bayesian probabilistic\nperspective, we provide a number of insights into more efficient algorithms for\noptimisation and hyper-parameter tuning. Traditional high-dimensional data\nreduction techniques, such as principal component analysis (PCA), partial least\nsquares (PLS), reduced rank regression (RRR), projection pursuit regression\n(PPR) are all shown to be shallow learners. Their deep learning counterparts\nexploit multiple deep layers of data reduction which provide predictive\nperformance gains. Stochastic gradient descent (SGD) training optimisation and\nDropout (DO) regularization provide estimation and variable selection. Bayesian\nregularization is central to finding weights and connections in networks to\noptimize the predictive bias-variance trade-off. To illustrate our methodology,\nwe provide an analysis of international bookings on Airbnb. Finally, we\nconclude with directions for future research. \n\n"}
{"id": "1706.00550", "contents": "Title: On Unifying Deep Generative Models Abstract: Deep generative models have achieved impressive success in recent years.\nGenerative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), as\nemerging families for generative model learning, have largely been considered\nas two distinct paradigms and received extensive independent studies\nrespectively. This paper aims to establish formal connections between GANs and\nVAEs through a new formulation of them. We interpret sample generation in GANs\nas performing posterior inference, and show that GANs and VAEs involve\nminimizing KL divergences of respective posterior and inference distributions\nwith opposite directions, extending the two learning phases of classic\nwake-sleep algorithm, respectively. The unified view provides a powerful tool\nto analyze a diverse set of existing model variants, and enables to transfer\ntechniques across research lines in a principled way. For example, we apply the\nimportance weighting method in VAE literatures for improved GAN learning, and\nenhance VAEs with an adversarial mechanism that leverages generated samples.\nExperiments show generality and effectiveness of the transferred techniques. \n\n"}
{"id": "1706.00868", "contents": "Title: Active learning machine learns to create new quantum experiments Abstract: How useful can machine learning be in a quantum laboratory? Here we raise the\nquestion of the potential of intelligent machines in the context of scientific\nresearch. A major motivation for the present work is the unknown reachability\nof various entanglement classes in quantum experiments. We investigate this\nquestion by using the projective simulation model, a physics-oriented approach\nto artificial intelligence. In our approach, the projective simulation system\nis challenged to design complex photonic quantum experiments that produce\nhigh-dimensional entangled multiphoton states, which are of high interest in\nmodern quantum experiments. The artificial intelligence system learns to create\na variety of entangled states, and improves the efficiency of their\nrealization. In the process, the system autonomously (re)discovers experimental\ntechniques which are only now becoming standard in modern quantum optical\nexperiments - a trait which was not explicitly demanded from the system but\nemerged through the process of learning. Such features highlight the\npossibility that machines could have a significantly more creative role in\nfuture research. \n\n"}
{"id": "1706.01665", "contents": "Title: Stochastic Multi-objective Optimization on a Budget: Application to\n  multi-pass wire drawing with quantified uncertainties Abstract: Design optimization of engineering systems with multiple competing objectives\nis a painstakingly tedious process especially when the objective functions are\nexpensive-to-evaluate computer codes with parametric uncertainties. The\neffectiveness of the state-of-the-art techniques is greatly diminished because\nthey require a large number of objective evaluations, which makes them\nimpractical for problems of the above kind. Bayesian global optimization (BGO),\nhas managed to deal with these challenges in solving single-objective\noptimization problems and has recently been extended to multi-objective\noptimization (MOO). BGO models the objectives via probabilistic surrogates and\nuses the epistemic uncertainty to define an information acquisition function\n(IAF) that quantifies the merit of evaluating the objective at new designs.\nThis iterative data acquisition process continues until a stopping criterion is\nmet. The most commonly used IAF for MOO is the expected improvement over the\ndominated hypervolume (EIHV) which in its original form is unable to deal with\nparametric uncertainties or measurement noise. In this work, we provide a\nsystematic reformulation of EIHV to deal with stochastic MOO problems. The\nprimary contribution of this paper lies in being able to filter out the noise\nand reformulate the EIHV without having to observe or estimate the stochastic\nparameters. An addendum of the probabilistic nature of our methodology is that\nit enables us to characterize our confidence about the predicted Pareto front.\nWe verify and validate the proposed methodology by applying it to synthetic\ntest problems with known solutions. We demonstrate our approach on an\nindustrial problem of die pass design for a steel wire drawing process. \n\n"}
{"id": "1706.01807", "contents": "Title: GAN and VAE from an Optimal Transport Point of View Abstract: This short article revisits some of the ideas introduced in arXiv:1701.07875\nand arXiv:1705.07642 in a simple setup. This sheds some lights on the\nconnexions between Variational Autoencoders (VAE), Generative Adversarial\nNetworks (GAN) and Minimum Kantorovitch Estimators (MKE). \n\n"}
{"id": "1706.02542", "contents": "Title: A Deformable Interface for Human Touch Recognition using Stretchable\n  Carbon Nanotube Dielectric Elastomer Sensors and Deep Neural Networks Abstract: User interfaces provide an interactive window between physical and virtual\nenvironments. A new concept in the field of human-computer interaction is a\nsoft user interface; a compliant surface that facilitates touch interaction\nthrough deformation. Despite the potential of these interfaces, they currently\nlack a signal processing framework that can efficiently extract information\nfrom their deformation. Here we present OrbTouch, a device that uses\nstatistical learning algorithms, based on convolutional neural networks, to map\ndeformations from human touch to categorical labels (i.e., gestures) and touch\nlocation using stretchable capacitor signals as inputs. We demonstrate this\napproach by using the device to control the popular game Tetris. OrbTouch\nprovides a modular, robust framework to interpret deformation in soft media,\nlaying a foundation for new modes of human computer interaction through shape\nchanging solids. \n\n"}
{"id": "1706.02730", "contents": "Title: Random projections for trust region subproblems Abstract: The trust region method is an algorithm traditionally used in the field of\nderivative free optimization. The method works by iteratively constructing\nsurrogate models (often linear or quadratic functions) to approximate the true\nobjective function inside some neighborhood of a current iterate. The\nneighborhood is called \"trust region in the sense that the model is trusted to\nbe good enough inside the neighborhood. Updated points are found by solving the\ncorresponding trust region subproblems. In this paper, we describe an\napplication of random projections to solving trust region subproblems\napproximately. \n\n"}
{"id": "1706.03078", "contents": "Title: Group Invariance, Stability to Deformations, and Complexity of Deep\n  Convolutional Representations Abstract: The success of deep convolutional architectures is often attributed in part\nto their ability to learn multiscale and invariant representations of natural\nsignals. However, a precise study of these properties and how they affect\nlearning guarantees is still missing. In this paper, we consider deep\nconvolutional representations of signals; we study their invariance to\ntranslations and to more general groups of transformations, their stability to\nthe action of diffeomorphisms, and their ability to preserve signal\ninformation. This analysis is carried by introducing a multilayer kernel based\non convolutional kernel networks and by studying the geometry induced by the\nkernel mapping. We then characterize the corresponding reproducing kernel\nHilbert space (RKHS), showing that it contains a large class of convolutional\nneural networks with homogeneous activation functions. This analysis allows us\nto separate data representation from learning, and to provide a canonical\nmeasure of model complexity, the RKHS norm, which controls both stability and\ngeneralization of any learned model. In addition to models in the constructed\nRKHS, our stability analysis also applies to convolutional networks with\ngeneric activations such as rectified linear units, and we discuss its\nrelationship with recent generalization bounds based on spectral norms. \n\n"}
{"id": "1706.04632", "contents": "Title: Stochastic Gradient MCMC Methods for Hidden Markov Models Abstract: Stochastic gradient MCMC (SG-MCMC) algorithms have proven useful in scaling\nBayesian inference to large datasets under an assumption of i.i.d data. We\ninstead develop an SG-MCMC algorithm to learn the parameters of hidden Markov\nmodels (HMMs) for time-dependent data. There are two challenges to applying\nSG-MCMC in this setting: The latent discrete states, and needing to break\ndependencies when considering minibatches. We consider a marginal likelihood\nrepresentation of the HMM and propose an algorithm that harnesses the inherent\nmemory decay of the process. We demonstrate the effectiveness of our algorithm\non synthetic experiments and an ion channel recording data, with runtimes\nsignificantly outperforming batch MCMC. \n\n"}
{"id": "1706.05069", "contents": "Title: Generalization for Adaptively-chosen Estimators via Stable Median Abstract: Datasets are often reused to perform multiple statistical analyses in an\nadaptive way, in which each analysis may depend on the outcomes of previous\nanalyses on the same dataset. Standard statistical guarantees do not account\nfor these dependencies and little is known about how to provably avoid\noverfitting and false discovery in the adaptive setting. We consider a natural\nformalization of this problem in which the goal is to design an algorithm that,\ngiven a limited number of i.i.d.~samples from an unknown distribution, can\nanswer adaptively-chosen queries about that distribution.\n  We present an algorithm that estimates the expectations of $k$ arbitrary\nadaptively-chosen real-valued estimators using a number of samples that scales\nas $\\sqrt{k}$. The answers given by our algorithm are essentially as accurate\nas if fresh samples were used to evaluate each estimator. In contrast, prior\nwork yields error guarantees that scale with the worst-case sensitivity of each\nestimator. We also give a version of our algorithm that can be used to verify\nanswers to such queries where the sample complexity depends logarithmically on\nthe number of queries $k$ (as in the reusable holdout technique).\n  Our algorithm is based on a simple approximate median algorithm that\nsatisfies the strong stability guarantees of differential privacy. Our\ntechniques provide a new approach for analyzing the generalization guarantees\nof differentially private algorithms. \n\n"}
{"id": "1706.05120", "contents": "Title: On Structural Controllability of Symmetric (Brain) Networks Abstract: The question of controllability of natural and man-made network systems has\nrecently received considerable attention. In the context of the human brain,\nthe study of controllability may not only shed light into the organization and\nfunction of different neural circuits, but also inform the design and\nimplementation of minimally invasive yet effective intervention protocols to\ntreat neurological disorders. While the characterization of brain\ncontrollability is still in its infancy, some results have recently appeared\nand given rise to scientific debate. Among these, [1] has numerically shown\nthat a class of brain networks constructed from DSI/DTI imaging data are\ncontrollable from one brain region. That is, a single brain region is\ntheoretically capable of moving the whole brain network towards any desired\ntarget state. In this note we provide evidence supporting controllability of\nbrain networks from a single region as discussed in [1], thus contradicting the\nmain conclusion and methods developed in [2]. \n\n"}
{"id": "1706.05598", "contents": "Title: On the Optimization Landscape of Tensor Decompositions Abstract: Non-convex optimization with local search heuristics has been widely used in\nmachine learning, achieving many state-of-art results. It becomes increasingly\nimportant to understand why they can work for these NP-hard problems on typical\ndata. The landscape of many objective functions in learning has been\nconjectured to have the geometric property that \"all local optima are\n(approximately) global optima\", and thus they can be solved efficiently by\nlocal search algorithms. However, establishing such property can be very\ndifficult.\n  In this paper, we analyze the optimization landscape of the random\nover-complete tensor decomposition problem, which has many applications in\nunsupervised learning, especially in learning latent variable models. In\npractice, it can be efficiently solved by gradient ascent on a non-convex\nobjective. We show that for any small constant $\\epsilon > 0$, among the set of\npoints with function values $(1+\\epsilon)$-factor larger than the expectation\nof the function, all the local maxima are approximate global maxima.\nPreviously, the best-known result only characterizes the geometry in small\nneighborhoods around the true components. Our result implies that even with an\ninitialization that is barely better than the random guess, the gradient ascent\nalgorithm is guaranteed to solve this problem.\n  Our main technique uses Kac-Rice formula and random matrix theory. To our\nbest knowledge, this is the first time when Kac-Rice formula is successfully\napplied to counting the number of local minima of a highly-structured random\npolynomial with dependent coefficients. \n\n"}
{"id": "1706.07101", "contents": "Title: The energy landscape of a simple neural network Abstract: We explore the energy landscape of a simple neural network. In particular, we\nexpand upon previous work demonstrating that the empirical complexity of fitted\nneural networks is vastly less than a naive parameter count would suggest and\nthat this implicit regularization is actually beneficial for generalization\nfrom fitted models. \n\n"}
{"id": "1706.08150", "contents": "Title: Value Asymptotics in Dynamic Games on Large Horizons Abstract: This paper is concerned with two-person dynamic zero-sum games. Let games for\nsome family have common dynamics, running costs and capabilities of players,\nand let these games differ in densities only. We show that the Dynamic\nProgramming Principle directly leads to the General Tauberian Theorem---that\nthe existence of a uniform limit of the value functions for uniform\ndistribution or for exponential distribution implies that the value functions\nuniformly converge to the same limit for arbitrary distribution from large\nclass. No assumptions on strategies are necessary. Applications to differential\ngames and stochastic statement are considered. \n\n"}
{"id": "1706.08217", "contents": "Title: An Effective Way to Improve YouTube-8M Classification Accuracy in Google\n  Cloud Platform Abstract: Large-scale datasets have played a significant role in progress of neural\nnetwork and deep learning areas. YouTube-8M is such a benchmark dataset for\ngeneral multi-label video classification. It was created from over 7 million\nYouTube videos (450,000 hours of video) and includes video labels from a\nvocabulary of 4716 classes (3.4 labels/video on average). It also comes with\npre-extracted audio & visual features from every second of video (3.2 billion\nfeature vectors in total). Google cloud recently released the datasets and\norganized 'Google Cloud & YouTube-8M Video Understanding Challenge' on Kaggle.\nCompetitors are challenged to develop classification algorithms that assign\nvideo-level labels using the new and improved Youtube-8M V2 dataset. Inspired\nby the competition, we started exploration of audio understanding and\nclassification using deep learning algorithms and ensemble methods. We built\nseveral baseline predictions according to the benchmark paper and public github\ntensorflow code. Furthermore, we improved global prediction accuracy (GAP) from\nbase level 77% to 80.7% through approaches of ensemble. \n\n"}
{"id": "1706.09242", "contents": "Title: Total variation regularized non-negative matrix factorization for smooth\n  hyperspectral unmixing Abstract: Hyperspectral analysis has gained popularity over recent years as a way to\ninfer what materials are displayed on a picture whose pixels consist of a\nmixture of spectral signatures. Computing both signatures and mixture\ncoefficients is known as unsupervised unmixing, a set of techniques usually\nbased on non-negative matrix factorization. Unmixing is a difficult non-convex\nproblem, and algorithms may converge to one out of many local minima, which may\nbe far removed from the true global minimum. Computing this true minimum is\nNP-hard and seems therefore out of reach. Aiming for interesting local minima,\nwe investigate the addition of total variation regularization terms. Advantages\nof these regularizers are two-fold. Their computation is typically rather\nlight, and they are deemed to preserve sharp edges in pictures. This paper\ndescribes an algorithm for regularized hyperspectral unmixing based on the\nAlternating Direction Method of Multipliers. \n\n"}
{"id": "1707.00530", "contents": "Title: Finding the nearest positive-real system Abstract: The notion of positive realness for linear time-invariant (LTI) dynamical\nsystems, equivalent to passivity, is one of the oldest in system and control\ntheory. In this paper, we consider the problem of finding the nearest\npositive-real (PR) system to a non PR system: given an LTI control system\ndefined by $E \\dot{x}=Ax+Bu$ and $y=Cx+Du$, minimize the Frobenius norm of\n$(\\Delta_E,\\Delta_A,\\Delta_B,\\Delta_C,\\Delta_D)$ such that\n$(E+\\Delta_E,A+\\Delta_A,B+\\Delta_B,C+\\Delta_C,D+\\Delta_D)$ is a PR system. We\nfirst show that a system is extended strictly PR if and only if it can be\nwritten as a strict port-Hamiltonian system. This allows us to reformulate the\nnearest PR system problem into an optimization problem with a simple convex\nfeasible set. We then use a fast gradient method to obtain a nearby PR system\nto a given non PR system, and illustrate the behavior of our algorithm on\nseveral examples. This is, to the best of our knowledge, the first algorithm\nthat computes a nearby PR system to a given non PR system that (i) is not based\non the spectral properties of related Hamiltonian matrices or pencils, (ii)\nallows to perturb all matrices $(E,A,B,C,D)$ describing the system, and (iii)\ndoes not make any assumption on the original given system. \n\n"}
{"id": "1707.00768", "contents": "Title: Learning to Avoid Errors in GANs by Manipulating Input Spaces Abstract: Despite recent advances, large scale visual artifacts are still a common\noccurrence in images generated by GANs. Previous work has focused on improving\nthe generator's capability to accurately imitate the data distribution\n$p_{data}$. In this paper, we instead explore methods that enable GANs to\nactively avoid errors by manipulating the input space. The core idea is to\napply small changes to each noise vector in order to shift them away from areas\nin the input space that tend to result in errors. We derive three different\narchitectures from that idea. The main one of these consists of a simple\nresidual module that leads to significantly less visual artifacts, while only\nslightly decreasing diversity. The module is trivial to add to existing GANs\nand costs almost zero computation and memory. \n\n"}
{"id": "1707.00808", "contents": "Title: Deconvolution of Point Sources: A Sampling Theorem and Robustness\n  Guarantees Abstract: In this work we analyze a convex-programming method for estimating\nsuperpositions of point sources or spikes from nonuniform samples of their\nconvolution with a known kernel. We consider a one-dimensional model where the\nkernel is either a Gaussian function or a Ricker wavelet, inspired by\napplications in geophysics and imaging. Our analysis establishes that\nminimizing a continuous counterpart of the $\\ell_1$ norm achieves exact\nrecovery of the original spikes as long as (1) the signal support satisfies a\nminimum-separation condition and (2) there are at least two samples close to\nevery spike. In addition, we derive theoretical guarantees on the robustness of\nthe approach to both dense and sparse additive noise. \n\n"}
{"id": "1707.01173", "contents": "Title: B tensors and tensor complementarity problems Abstract: In this paper, one of our main purposes is to prove the boundedness of\nsolution set of tensor complementarity problem with B tensor such that the\nspecific bounds only depend on the structural properties of tensor. To achieve\nthis purpose, firstly, we present that each B tensor is strictly semi-positive\nand each B$_0$ tensor is semi-positive. Subsequencely, the strictly lower and\nupper bounds of different operator norms are given for two positively\nhomogeneous operators defined by B tensor. Finally, with the help of the upper\nbounds of different operator norms, we show the strcitly lower bound of\nsolution set of tensor complementarity problem with B tensor. Furthermore, the\nupper bounds of spectral radius and $E$-spectral radius of B (B$_0$) tensor are\nobtained, respectively, which achieves our another objective. In particular,\nsuch the upper bounds only depend on the principal diagonal entries of tensors. \n\n"}
{"id": "1707.01212", "contents": "Title: Efficient Data Representation by Selecting Prototypes with Importance\n  Weights Abstract: Prototypical examples that best summarizes and compactly represents an\nunderlying complex data distribution communicate meaningful insights to humans\nin domains where simple explanations are hard to extract. In this paper we\npresent algorithms with strong theoretical guarantees to mine these data sets\nand select prototypes a.k.a. representatives that optimally describes them. Our\nwork notably generalizes the recent work by Kim et al. (2016) where in addition\nto selecting prototypes, we also associate non-negative weights which are\nindicative of their importance. This extension provides a single coherent\nframework under which both prototypes and criticisms (i.e. outliers) can be\nfound. Furthermore, our framework works for any symmetric positive definite\nkernel thus addressing one of the key open questions laid out in Kim et al.\n(2016). By establishing that our objective function enjoys a key property of\nthat of weak submodularity, we present a fast ProtoDash algorithm and also\nderive approximation guarantees for the same. We demonstrate the efficacy of\nour method on diverse domains such as retail, digit recognition (MNIST) and on\npublicly available 40 health questionnaires obtained from the Center for\nDisease Control (CDC) website maintained by the US Dept. of Health. We validate\nthe results quantitatively as well as qualitatively based on expert feedback\nand recently published scientific studies on public health, thus showcasing the\npower of our technique in providing actionability (for retail), utility (for\nMNIST) and insight (on CDC datasets) which arguably are the hallmarks of an\neffective data mining method. \n\n"}
{"id": "1707.03269", "contents": "Title: Q-Learning Algorithm for VoLTE Closed-Loop Power Control in Indoor Small\n  Cells Abstract: We propose a reinforcement learning (RL) based closed loop power control\nalgorithm for the downlink of the voice over LTE (VoLTE) radio bearer for an\nindoor environment served by small cells. The main contributions of our paper\nare to 1) use RL to solve performance tuning problems in an indoor cellular\nnetwork for voice bearers and 2) show that our derived lower bound loss in\neffective signal to interference plus noise ratio due to neighboring cell\nfailure is sufficient for VoLTE power control purposes in practical cellular\nnetworks. In our simulation, the proposed RL-based power control algorithm\nsignificantly improves both voice retainability and mean opinion score compared\nto current industry standards. The improvement is due to maintaining an\neffective downlink signal to interference plus noise ratio against adverse\nnetwork operational issues and faults. \n\n"}
{"id": "1707.04191", "contents": "Title: Distributionally Ambiguous Optimization Techniques for Batch Bayesian\n  Optimization Abstract: We propose a novel, theoretically-grounded, acquisition function for Batch\nBayesian optimization informed by insights from distributionally ambiguous\noptimization. Our acquisition function is a lower bound on the well-known\nExpected Improvement function, which requires evaluation of a Gaussian\nExpectation over a multivariate piecewise affine function. Our bound is\ncomputed instead by evaluating the best-case expectation over all probability\ndistributions consistent with the same mean and variance as the original\nGaussian distribution. Unlike alternative approaches, including Expected\nImprovement, our proposed acquisition function avoids multi-dimensional\nintegrations entirely, and can be computed exactly - even on large batch sizes\n- as the solution of a tractable convex optimization problem. Our suggested\nacquisition function can also be optimized efficiently, since first and second\nderivative information can be calculated inexpensively as by-products of the\nacquisition function calculation itself. We derive various novel theorems that\nground our work theoretically and we demonstrate superior performance via\nsimple motivating examples, benchmark functions and real-world problems. \n\n"}
{"id": "1707.04477", "contents": "Title: Community Aliveness: Discovering Interaction Decay Patterns in Online\n  Social Communities Abstract: Online Social Communities (OSCs) provide a medium for connecting people,\nsharing news, eliciting information, and finding jobs, among others. The\ndynamics of the interaction among the members of OSCs is not always growth\ndynamics. Instead, a $\\textit{decay}$ or $\\textit{inactivity}$ dynamics often\nhappens, which makes an OSC obsolete. Understanding the behavior and the\ncharacteristics of the members of an inactive community help to sustain the\ngrowth dynamics of these communities and, possibly, prevents them from being\nout of service. In this work, we provide two prediction models for predicting\nthe interaction decay of community members, namely: a Simple Threshold Model\n(STM) and a supervised machine learning classification framework. We conducted\nevaluation experiments for our prediction models supported by a $\\textit{ground\ntruth}$ of decayed communities extracted from the StackExchange platform. The\nresults of the experiments revealed that it is possible, with satisfactory\nprediction performance in terms of the F1-score and the accuracy, to predict\nthe decay of the activity of the members of these communities using\nnetwork-based attributes and network-exogenous attributes of the members. The\nupper bound of the prediction performance of the methods we used is $0.91$ and\n$0.83$ for the F1-score and the accuracy, respectively. These results indicate\nthat network-based attributes are correlated with the activity of the members\nand that we can find decay patterns in terms of these attributes. The results\nalso showed that the structure of the decayed communities can be used to\nsupport the alive communities by discovering inactive members. \n\n"}
{"id": "1707.04659", "contents": "Title: Variational approach for learning Markov processes from time series data Abstract: Inference, prediction and control of complex dynamical systems from time\nseries is important in many areas, including financial markets, power grid\nmanagement, climate and weather modeling, or molecular dynamics. The analysis\nof such highly nonlinear dynamical systems is facilitated by the fact that we\ncan often find a (generally nonlinear) transformation of the system coordinates\nto features in which the dynamics can be excellently approximated by a linear\nMarkovian model. Moreover, the large number of system variables often change\ncollectively on large time- and length-scales, facilitating a low-dimensional\nanalysis in feature space. In this paper, we introduce a variational approach\nfor Markov processes (VAMP) that allows us to find optimal feature mappings and\noptimal Markovian models of the dynamics from given time series data. The key\ninsight is that the best linear model can be obtained from the top singular\ncomponents of the Koopman operator. This leads to the definition of a family of\nscore functions called VAMP-r which can be calculated from data, and can be\nemployed to optimize a Markovian model. In addition, based on the relationship\nbetween the variational scores and approximation errors of Koopman operators,\nwe propose a new VAMP-E score, which can be applied to cross-validation for\nhyper-parameter optimization and model selection in VAMP. VAMP is valid for\nboth reversible and nonreversible processes and for stationary and\nnon-stationary processes or realizations. \n\n"}
{"id": "1707.04926", "contents": "Title: Theoretical insights into the optimization landscape of\n  over-parameterized shallow neural networks Abstract: In this paper we study the problem of learning a shallow artificial neural\nnetwork that best fits a training data set. We study this problem in the\nover-parameterized regime where the number of observations are fewer than the\nnumber of parameters in the model. We show that with quadratic activations the\noptimization landscape of training such shallow neural networks has certain\nfavorable characteristics that allow globally optimal models to be found\nefficiently using a variety of local search heuristics. This result holds for\nan arbitrary training data of input/output pairs. For differentiable activation\nfunctions we also show that gradient descent, when suitably initialized,\nconverges at a linear rate to a globally optimal model. This result focuses on\na realizable model where the inputs are chosen i.i.d. from a Gaussian\ndistribution and the labels are generated according to planted weight\ncoefficients. \n\n"}
{"id": "1707.04981", "contents": "Title: The Optimal Equilibrium for Time-Inconsistent Stopping Problems -- the\n  Discrete-Time Case Abstract: We study an infinite-horizon discrete-time optimal stopping problem under\nnon-exponential discounting. A new method, which we call the iterative\napproach, is developed to find subgame perfect Nash equilibria. When the\ndiscount function induces decreasing impatience, we establish the existence of\nan equilibrium through fixed-point iterations. Moreover, we show that there\nexists a unique optimal equilibrium, which generates larger value than any\nother equilibrium does at all times. To the best of our knowledge, this is the\nfirst time a dominating subgame perfect Nash equilibrium is shown to exist in\nthe literature of time-inconsistency. \n\n"}
{"id": "1707.05377", "contents": "Title: Genetic Algorithm for Epidemic Mitigation by Removing Relationships Abstract: Min-SEIS-Cluster is an optimization problem which aims at minimizing the\ninfection spreading in networks. In this problem, nodes can be susceptible to\nan infection, exposed to an infection, or infectious. One of the main features\nof this problem is the fact that nodes have different dynamics when interacting\nwith other nodes from the same community. Thus, the problem is characterized by\ndistinct probabilities of infecting nodes from both the same and from different\ncommunities. This paper presents a new genetic algorithm that solves the\nMin-SEIS-Cluster problem. This genetic algorithm surpassed the current\nheuristic of this problem significantly, reducing the number of infected nodes\nduring the simulation of the epidemics. The results therefore suggest that our\nnew genetic algorithm is the state-of-the-art heuristic to solve this problem. \n\n"}
{"id": "1707.05747", "contents": "Title: Augmented Lagrangian Functions for Cone Constrained Optimization: the\n  Existence of Global Saddle Points and Exact Penalty Property Abstract: In the article we present a general theory of augmented Lagrangian functions\nfor cone constrained optimization problems that allows one to study almost all\nknown augmented Lagrangians for cone constrained programs within a unified\nframework. We develop a new general method for proving the existence of global\nsaddle points of augmented Lagrangian functions, called the localization\nprinciple. The localization principle unifies, generalizes and sharpens most of\nthe known results on existence of global saddle points, and, in essence,\nreduces the problem of the existence of saddle points to a local analysis of\noptimality conditions. With the use of the localization principle we obtain\nfirst necessary and sufficient conditions for the existence of a global saddle\npoint of an augmented Lagrangian for cone constrained minimax problems via both\nsecond and first order optimality conditions. In the second part of the paper,\nwe present a general approach to the construction of globally exact augmented\nLagrangian functions. The general approach developed in this paper allowed us\nnot only to sharpen most of the existing results on globally exact augmented\nLagrangians, but also to construct first globally exact augmented Lagrangian\nfunctions for equality constrained optimization problems, for nonlinear second\norder cone programs and for nonlinear semidefinite programs. These globally\nexact augmented Lagrangians can be utilized in order to design new\nsuperlinearly (or even quadratically) convergent optimization methods for cone\nconstrained optimization problems. \n\n"}
{"id": "1707.06626", "contents": "Title: Learning to Draw Samples with Amortized Stein Variational Gradient\n  Descent Abstract: We propose a simple algorithm to train stochastic neural networks to draw\nsamples from given target distributions for probabilistic inference. Our method\nis based on iteratively adjusting the neural network parameters so that the\noutput changes along a Stein variational gradient direction (Liu & Wang, 2016)\nthat maximally decreases the KL divergence with the target distribution. Our\nmethod works for any target distribution specified by their unnormalized\ndensity function, and can train any black-box architectures that are\ndifferentiable in terms of the parameters we want to adapt. We demonstrate our\nmethod with a number of applications, including variational autoencoder (VAE)\nwith expressive encoders to model complex latent space structures, and\nhyper-parameter learning of MCMC samplers that allows Bayesian inference to\nadaptively improve itself when seeing more data. \n\n"}
{"id": "1707.08167", "contents": "Title: On The Robustness of a Neural Network Abstract: With the development of neural networks based machine learning and their\nusage in mission critical applications, voices are rising against the\n\\textit{black box} aspect of neural networks as it becomes crucial to\nunderstand their limits and capabilities. With the rise of neuromorphic\nhardware, it is even more critical to understand how a neural network, as a\ndistributed system, tolerates the failures of its computing nodes, neurons, and\nits communication channels, synapses. Experimentally assessing the robustness\nof neural networks involves the quixotic venture of testing all the possible\nfailures, on all the possible inputs, which ultimately hits a combinatorial\nexplosion for the first, and the impossibility to gather all the possible\ninputs for the second.\n  In this paper, we prove an upper bound on the expected error of the output\nwhen a subset of neurons crashes. This bound involves dependencies on the\nnetwork parameters that can be seen as being too pessimistic in the average\ncase. It involves a polynomial dependency on the Lipschitz coefficient of the\nneurons activation function, and an exponential dependency on the depth of the\nlayer where a failure occurs. We back up our theoretical results with\nexperiments illustrating the extent to which our prediction matches the\ndependencies between the network parameters and robustness. Our results show\nthat the robustness of neural networks to the average crash can be estimated\nwithout the need to neither test the network on all failure configurations, nor\naccess the training set used to train the network, both of which are\npractically impossible requirements. \n\n"}
{"id": "1707.09241", "contents": "Title: Generator Reversal Abstract: We consider the problem of training generative models with deep neural\nnetworks as generators, i.e. to map latent codes to data points. Whereas the\ndominant paradigm combines simple priors over codes with complex deterministic\nmodels, we propose instead to use more flexible code distributions. These\ndistributions are estimated non-parametrically by reversing the generator map\nduring training. The benefits include: more powerful generative models, better\nmodeling of latent structure and explicit control of the degree of\ngeneralization. \n\n"}
{"id": "1708.01308", "contents": "Title: A Mean Field Competition Abstract: We introduce a mean field game with rank-based reward: competing agents\noptimize their effort to achieve a goal, are ranked according to their\ncompletion time, and paid a reward based on their relative rank. First, we\npropose a tractable Poissonian model in which we can describe the optimal\neffort for a given reward scheme. Second, we study the principal--agent problem\nof designing an optimal reward scheme. A surprising, explicit design is found\nto minimize the time until a given fraction of the population has reached the\ngoal. \n\n"}
{"id": "1708.01945", "contents": "Title: A Bootstrap Method for Error Estimation in Randomized Matrix\n  Multiplication Abstract: In recent years, randomized methods for numerical linear algebra have\nreceived growing interest as a general approach to large-scale problems.\nTypically, the essential ingredient of these methods is some form of randomized\ndimension reduction, which accelerates computations, but also creates random\napproximation error. In this way, the dimension reduction step encodes a\ntradeoff between cost and accuracy. However, the exact numerical relationship\nbetween cost and accuracy is typically unknown, and consequently, it may be\ndifficult for the user to precisely know (1) how accurate a given solution is,\nor (2) how much computation is needed to achieve a given level of accuracy. In\nthe current paper, we study randomized matrix multiplication (sketching) as a\nprototype setting for addressing these general problems. As a solution, we\ndevelop a bootstrap method for \\emph{directly estimating} the accuracy as a\nfunction of the reduced dimension (as opposed to deriving worst-case bounds on\nthe accuracy in terms of the reduced dimension). From a computational\nstandpoint, the proposed method does not substantially increase the cost of\nstandard sketching methods, and this is made possible by an \"extrapolation\"\ntechnique. In addition, we provide both theoretical and empirical results to\ndemonstrate the effectiveness of the proposed method. \n\n"}
{"id": "1708.02157", "contents": "Title: Exact solutions of infinite dimensional total-variation regularized\n  problems Abstract: We study the solutions of infinite dimensional linear inverse problems over\nBanach spaces. The regularizer is defined as the total variation of a linear\nmapping of the function to recover, while the data fitting term is a near\narbitrary convex function. The first contribution is about the solu-tion's\nstructure: we show that under suitable assumptions, there always exist an\nm-sparse solution, where m is the number of linear measurements of the signal.\nOur second contribution is about the computation of the solution. While most\nexisting works first discretize the problem, we show that exacts solutions of\nthe infinite dimensional problem can be obtained by solving two consecutive\nfinite dimensional convex programs. These results extend recent advances in the\nunderstanding of total-variation reg-ularized problems. \n\n"}
{"id": "1708.03020", "contents": "Title: Non-stationary Stochastic Optimization under $L_{p,q}$-Variation\n  Measures Abstract: We consider a non-stationary sequential stochastic optimization problem, in\nwhich the underlying cost functions change over time under a variation budget\nconstraint. We propose an $L_{p,q}$-variation functional to quantify the\nchange, which yields less variation for dynamic function sequences whose\nchanges are constrained to short time periods or small subsets of input domain.\nUnder the $L_{p,q}$-variation constraint, we derive both upper and matching\nlower regret bounds for smooth and strongly convex function sequences, which\ngeneralize previous results in Besbes et al. (2015). Furthermore, we provide an\nupper bound for general convex function sequences with noisy gradient feedback,\nwhich matches the optimal rate as $p\\to\\infty$. Our results reveal some\nsurprising phenomena under this general variation functional, such as the curse\nof dimensionality of the function domain. The key technical novelties in our\nanalysis include affinity lemmas that characterize the distance of the\nminimizers of two convex functions with bounded Lp difference, and a cubic\nspline based construction that attains matching lower bounds. \n\n"}
{"id": "1708.04887", "contents": "Title: Fixed effects testing in high-dimensional linear mixed models Abstract: Many scientific and engineering challenges -- ranging from pharmacokinetic\ndrug dosage allocation and personalized medicine to marketing mix (4Ps)\nrecommendations -- require an understanding of the unobserved heterogeneity in\norder to develop the best decision making-processes. In this paper, we develop\na hypothesis test and the corresponding p-value for testing for the\nsignificance of the homogeneous structure in linear mixed models. A robust\nmatching moment construction is used for creating a test that adapts to the\nsize of the model sparsity. When unobserved heterogeneity at a cluster level is\nconstant, we show that our test is both consistent and unbiased even when the\ndimension of the model is extremely high. Our theoretical results rely on a new\nfamily of adaptive sparse estimators of the fixed effects that do not require\nconsistent estimation of the random effects. Moreover, our inference results do\nnot require consistent model selection. We showcase that moment matching can be\nextended to nonlinear mixed effects models and to generalized linear mixed\neffects models. In numerical and real data experiments, we find that the\ndeveloped method is extremely accurate, that it adapts to the size of the\nunderlying model and is decidedly powerful in the presence of irrelevant\ncovariates. \n\n"}
{"id": "1708.08675", "contents": "Title: American options in an imperfect market with default Abstract: We study pricing and (super)hedging for American options in an imperfect\nmarket model with default, where the imperfections are taken into account via\nthe nonlinearity of the wealth dynamics. The payoff is given by an RCLL adapted\nprocess $(\\xi_t)$. We define the {\\em seller's superhedging price} of the\nAmerican option as the minimum of the initial capitals which allow the seller\nto build up a superhedging portfolio. We prove that this price coincides with\nthe value function of an optimal stopping problem with nonlinear expectations\ninduced by BSDEs with default jump, which corresponds to the solution of a\nreflected BSDE with lower barrier. Moreover, we show the existence of a\nsuperhedging portfolio strategy. We then consider the {\\em buyer's superhedging\nprice}, which is defined as the supremum of the initial wealths which allow the\nbuyer to select an exercise time $\\tau$ and a portfolio strategy $\\varphi$ so\nthat he/she is superhedged. Under the additional assumption of left upper\nsemicontinuity along stopping times of $(\\xi_t)$, we show the existence of a\nsuperhedge $(\\tau, \\varphi)$ for the buyer, as well as a characterization of\nthe buyer's superhedging price via the solution of a nonlinear reflected BSDE\nwith upper barrier. \n\n"}
{"id": "1708.08819", "contents": "Title: Coulomb GANs: Provably Optimal Nash Equilibria via Potential Fields Abstract: Generative adversarial networks (GANs) evolved into one of the most\nsuccessful unsupervised techniques for generating realistic images. Even though\nit has recently been shown that GAN training converges, GAN models often end up\nin local Nash equilibria that are associated with mode collapse or otherwise\nfail to model the target distribution. We introduce Coulomb GANs, which pose\nthe GAN learning problem as a potential field of charged particles, where\ngenerated samples are attracted to training set samples but repel each other.\nThe discriminator learns a potential field while the generator decreases the\nenergy by moving its samples along the vector (force) field determined by the\ngradient of the potential field. Through decreasing the energy, the GAN model\nlearns to generate samples according to the whole target distribution and does\nnot only cover some of its modes. We prove that Coulomb GANs possess only one\nNash equilibrium which is optimal in the sense that the model distribution\nequals the target distribution. We show the efficacy of Coulomb GANs on a\nvariety of image datasets. On LSUN and celebA, Coulomb GANs set a new state of\nthe art and produce a previously unseen variety of different samples. \n\n"}
{"id": "1708.09096", "contents": "Title: Transfer-Entropy-Regularized Markov Decision Processes Abstract: We consider the framework of transfer-entropy-regularized Markov Decision\nProcess (TERMDP) in which the weighted sum of the classical state-dependent\ncost and the transfer entropy from the state random process to the control\nrandom process is minimized. Although TERMDPs are generally formulated as\nnonconvex optimization problems, we derive an analytical necessary optimality\ncondition expressed as a finite set of nonlinear equations, based on which an\niterative forward-backward computational procedure similar to the\nArimoto-Blahut algorithm is proposed. It is shown that every limit point of the\nsequence generated by the proposed algorithm is a stationary point of the\nTERMDP. Applications of TERMDPs are discussed in the context of networked\ncontrol systems theory and non-equilibrium thermodynamics. The proposed\nalgorithm is applied to an information-constrained maze navigation problem,\nwhereby we study how the price of information qualitatively alters the optimal\ndecision polices. \n\n"}
{"id": "1708.09811", "contents": "Title: Efficient tracking of a growing number of experts Abstract: We consider a variation on the problem of prediction with expert advice,\nwhere new forecasters that were unknown until then may appear at each round. As\noften in prediction with expert advice, designing an algorithm that achieves\nnear-optimal regret guarantees is straightforward, using aggregation of\nexperts. However, when the comparison class is sufficiently rich, for instance\nwhen the best expert and the set of experts itself changes over time, such\nstrategies naively require to maintain a prohibitive number of weights\n(typically exponential with the time horizon). By contrast, designing\nstrategies that both achieve a near-optimal regret and maintain a reasonable\nnumber of weights is highly non-trivial. We consider three increasingly\nchallenging objectives (simple regret, shifting regret and sparse shifting\nregret) that extend existing notions defined for a fixed expert ensemble; in\neach case, we design strategies that achieve tight regret bounds, adaptive to\nthe parameters of the comparison class, while being computationally\ninexpensive. Moreover, our algorithms are anytime, agnostic to the number of\nincoming experts and completely parameter-free. Such remarkable results are\nmade possible thanks to two simple but highly effective recipes: first the\n\"abstention trick\" that comes from the specialist framework and enables to\nhandle the least challenging notions of regret, but is limited when addressing\nmore sophisticated objectives. Second, the \"muting trick\" that we introduce to\ngive more flexibility. We show how to combine these two tricks in order to\nhandle the most challenging class of comparison strategies. \n\n"}
{"id": "1709.00537", "contents": "Title: Communication-efficient Algorithm for Distributed Sparse Learning via\n  Two-way Truncation Abstract: We propose a communicationally and computationally efficient algorithm for\nhigh-dimensional distributed sparse learning. At each iteration, local machines\ncompute the gradient on local data and the master machine solves one shifted\n$l_1$ regularized minimization problem. The communication cost is reduced from\nconstant times of the dimension number for the state-of-the-art algorithm to\nconstant times of the sparsity number via Two-way Truncation procedure.\nTheoretically, we prove that the estimation error of the proposed algorithm\ndecreases exponentially and matches that of the centralized method under mild\nassumptions. Extensive experiments on both simulated data and real data verify\nthat the proposed algorithm is efficient and has performance comparable with\nthe centralized method on solving high-dimensional sparse learning problems. \n\n"}
{"id": "1709.00898", "contents": "Title: Communicating Zero-Sum Product Stochastic Games Abstract: We study two classes of zero-sum stochastic games with compact action sets\nand a finite product state space. These two classes assume a communication\nproperty on the state spaces of the players. For strongly communicating on one\nside games, we prove the existence of the uniform value. For weakly\ncommunicating on both sides games, we prove that the asymptotic value, and\ntherefore the uniform value, may fail to exist. \n\n"}
{"id": "1709.01006", "contents": "Title: Learning Implicit Generative Models Using Differentiable Graph Tests Abstract: Recently, there has been a growing interest in the problem of learning rich\nimplicit models - those from which we can sample, but can not evaluate their\ndensity. These models apply some parametric function, such as a deep network,\nto a base measure, and are learned end-to-end using stochastic optimization.\nOne strategy of devising a loss function is through the statistics of two\nsample tests - if we can fool a statistical test, the learned distribution\nshould be a good model of the true data. However, not all tests can easily fit\ninto this framework, as they might not be differentiable with respect to the\ndata points, and hence with respect to the parameters of the implicit model.\nMotivated by this problem, in this paper we show how two such classical tests,\nthe Friedman-Rafsky and k-nearest neighbour tests, can be effectively smoothed\nusing ideas from undirected graphical models - the matrix tree theorem and\ncardinality potentials. Moreover, as we show experimentally, smoothing can\nsignificantly increase the power of the test, which might of of independent\ninterest. Finally, we apply our method to learn implicit models. \n\n"}
{"id": "1709.01919", "contents": "Title: Estimation of a Low-rank Topic-Based Model for Information Cascades Abstract: We consider the problem of estimating the latent structure of a social\nnetwork based on the observed information diffusion events, or cascades, where\nthe observations for a given cascade consist of only the timestamps of\ninfection for infected nodes but not the source of the infection. Most of the\nexisting work on this problem has focused on estimating a diffusion matrix\nwithout any structural assumptions on it. In this paper, we propose a novel\nmodel based on the intuition that an information is more likely to propagate\namong two nodes if they are interested in similar topics which are also\nprominent in the information content. In particular, our model endows each node\nwith an influence vector (which measures how authoritative the node is on each\ntopic) and a receptivity vector (which measures how susceptible the node is for\neach topic). We show how this node-topic structure can be estimated from the\nobserved cascades, and prove the consistency of the estimator. Experiments on\nsynthetic and real data demonstrate the improved performance and better\ninterpretability of our model compared to existing state-of-the-art methods. \n\n"}
{"id": "1709.03374", "contents": "Title: When to arrive at a queue with earliness, tardiness and waiting costs Abstract: We consider a queueing facility where customers decide when to arrive. All\ncustomers have the same desired arrival time (w.l.o.g.\\ time zero). There is\none server, and the service times are independent and exponentially\ndistributed. The total number of customers that demand service is random, and\nfollows the Poisson distribution. Each customer wishes to minimize the sum of\nthree costs: earliness, tardiness and waiting. We assume that all three costs\nare linear with time and are defined as follows. Earliness is the time between\narrival and time zero, if there is any. Tardiness is simply the time of\nentering service, if it is after time zero. Waiting time is the time from\narrival until entering service. We focus on customers' rational behaviour,\nassuming that each customer wants to minimize his total cost, and in\nparticular, we seek a symmetric Nash equilibrium strategy. We show that such a\nstrategy is mixed, unless trivialities occur. We construct a set of equations\nthat its solution provides the symmetric Nash equilibrium. The solution is a\ncontinuous distribution on the real line. We also compare the socially optimal\nsolution (that is, the one that minimizes total cost across all customers) to\nthe overall cost resulting from the Nash equilibrium. \n\n"}
{"id": "1709.03384", "contents": "Title: Ghost Penalties in Nonconvex Constrained Optimization: Diminishing\n  Stepsizes and Iteration Complexity Abstract: We consider nonconvex constrained optimization problems and propose a new\napproach to the convergence analysis based on penalty functions. We make use of\nclassical penalty functions in an unconventional way, in that penalty functions\nonly enter in the theoretical analysis of convergence while the algorithm\nitself is penalty-free. Based on this idea, we are able to establish several\nnew results, including the first general analysis for diminishing stepsize\nmethods in nonconvex, constrained optimization, showing convergence to\ngeneralized stationary points, and a complexity study for SQP-type algorithms. \n\n"}
{"id": "1709.04072", "contents": "Title: A convergence framework for inexact nonconvex and nonsmooth algorithms\n  and its applications to several iterations Abstract: In this paper, we consider the convergence of an abstract inexact nonconvex\nand nonsmooth algorithm. We promise a pseudo sufficient descent condition and a\npseudo relative error condition, which are both related to an auxiliary\nsequence, for the algorithm; and a continuity condition is assumed to hold. In\nfact, a lot of classical inexact nonconvex and nonsmooth algorithms allow these\nthree conditions. Under a special kind of summable assumption on the auxiliary\nsequence, we prove the sequence generated by the general algorithm converges to\na critical point of the objective function if being assumed Kurdyka-\nLojasiewicz property. The core of the proofs lies in building a new Lyapunov\nfunction, whose successive difference provides a bound for the successive\ndifference of the points generated by the algorithm. And then, we apply our\nfindings to several classical nonconvex iterative algorithms and derive the\ncorresponding convergence results \n\n"}
{"id": "1709.04574", "contents": "Title: Towards personalized human AI interaction - adapting the behavior of AI\n  agents using neural signatures of subjective interest Abstract: Reinforcement Learning AI commonly uses reward/penalty signals that are\nobjective and explicit in an environment -- e.g. game score, completion time,\netc. -- in order to learn the optimal strategy for task performance. However,\nHuman-AI interaction for such AI agents should include additional reinforcement\nthat is implicit and subjective -- e.g. human preferences for certain AI\nbehavior -- in order to adapt the AI behavior to idiosyncratic human\npreferences. Such adaptations would mirror naturally occurring processes that\nincrease trust and comfort during social interactions. Here, we show how a\nhybrid brain-computer-interface (hBCI), which detects an individual's level of\ninterest in objects/events in a virtual environment, can be used to adapt the\nbehavior of a Deep Reinforcement Learning AI agent that is controlling a\nvirtual autonomous vehicle. Specifically, we show that the AI learns a driving\nstrategy that maintains a safe distance from a lead vehicle, and most novelly,\npreferentially slows the vehicle when the human passengers of the vehicle\nencounter objects of interest. This adaptation affords an additional 20\\%\nviewing time for subjectively interesting objects. This is the first\ndemonstration of how an hBCI can be used to provide implicit reinforcement to\nan AI agent in a way that incorporates user preferences into the control\nsystem. \n\n"}
{"id": "1709.06010", "contents": "Title: Learning Neural Networks with Two Nonlinear Layers in Polynomial Time Abstract: We give a polynomial-time algorithm for learning neural networks with one\nlayer of sigmoids feeding into any Lipschitz, monotone activation function\n(e.g., sigmoid or ReLU). We make no assumptions on the structure of the\nnetwork, and the algorithm succeeds with respect to {\\em any} distribution on\nthe unit ball in $n$ dimensions (hidden weight vectors also have unit norm).\nThis is the first assumption-free, provably efficient algorithm for learning\nneural networks with two nonlinear layers.\n  Our algorithm-- {\\em Alphatron}-- is a simple, iterative update rule that\ncombines isotonic regression with kernel methods. It outputs a hypothesis that\nyields efficient oracle access to interpretable features. It also suggests a\nnew approach to Boolean learning problems via real-valued conditional-mean\nfunctions, sidestepping traditional hardness results from computational\nlearning theory.\n  Along these lines, we subsume and improve many longstanding results for PAC\nlearning Boolean functions to the more general, real-valued setting of {\\em\nprobabilistic concepts}, a model that (unlike PAC learning) requires non-i.i.d.\nnoise-tolerance. \n\n"}
{"id": "1709.06390", "contents": "Title: Analogical-based Bayesian Optimization Abstract: Some real-world problems revolve to solve the optimization problem\n\\max_{x\\in\\mathcal{X}}f\\left(x\\right) where f\\left(.\\right) is a black-box\nfunction and X might be the set of non-vectorial objects (e.g., distributions)\nwhere we can only define a symmetric and non-negative similarity score on it.\nThis setting requires a novel view for the standard framework of Bayesian\nOptimization that generalizes the core insightful spirit of this framework.\nWith this spirit, in this paper, we propose Analogical-based Bayesian\nOptimization that can maximize black-box function over a domain where only a\nsimilarity score can be defined. Our pathway is as follows: we first base on\nthe geometric view of Gaussian Processes (GP) to define the concept of\ninfluence level that allows us to analytically represent predictive means and\nvariances of GP posteriors and base on that view to enable replacing kernel\nsimilarity by a more genetic similarity score. Furthermore, we also propose two\nstrategies to find a batch of query points that can efficiently handle high\ndimensional data. \n\n"}
{"id": "1709.06466", "contents": "Title: Evaluation of the Rate of Convergence in the PIA Abstract: Folklore says that Howard's Policy Improvement Algorithm converges\nextraordinarily fast, even for controlled diffusion settings.\n  In a previous paper, we proved that approximations of the solution of a\nparticular parabolic partial differential equation obtained via the policy\nimprovement algorithm show a quadratic local convergence.\n  In this paper, we show that we obtain the same rate of convergence of the\nalgorithm in a more general setup. This provides some explanation as to why the\nalgorithm converges fast.\n  We provide an example by solving a semilinear elliptic partial differential\nequation numerically by applying the algorithm and check how the approximations\nconverge to the analytic solution. \n\n"}
{"id": "1709.06990", "contents": "Title: Text Compression for Sentiment Analysis via Evolutionary Algorithms Abstract: Can textual data be compressed intelligently without losing accuracy in\nevaluating sentiment? In this study, we propose a novel evolutionary\ncompression algorithm, PARSEC (PARts-of-Speech for sEntiment Compression),\nwhich makes use of Parts-of-Speech tags to compress text in a way that\nsacrifices minimal classification accuracy when used in conjunction with\nsentiment analysis algorithms. An analysis of PARSEC with eight commercial and\nnon-commercial sentiment analysis algorithms on twelve English sentiment data\nsets reveals that accurate compression is possible with (0%, 1.3%, 3.3%) loss\nin sentiment classification accuracy for (20%, 50%, 75%) data compression with\nPARSEC using LingPipe, the most accurate of the sentiment algorithms. Other\nsentiment analysis algorithms are more severely affected by compression. We\nconclude that significant compression of text data is possible for sentiment\nanalysis depending on the accuracy demands of the specific application and the\nspecific sentiment analysis algorithm used. \n\n"}
{"id": "1709.06994", "contents": "Title: Structured Probabilistic Pruning for Convolutional Neural Network\n  Acceleration Abstract: In this paper, we propose a novel progressive parameter pruning method for\nConvolutional Neural Network acceleration, named Structured Probabilistic\nPruning (SPP), which effectively prunes weights of convolutional layers in a\nprobabilistic manner. Unlike existing deterministic pruning approaches, where\nunimportant weights are permanently eliminated, SPP introduces a pruning\nprobability for each weight, and pruning is guided by sampling from the pruning\nprobabilities. A mechanism is designed to increase and decrease pruning\nprobabilities based on importance criteria in the training process. Experiments\nshow that, with 4x speedup, SPP can accelerate AlexNet with only 0.3% loss of\ntop-5 accuracy and VGG-16 with 0.8% loss of top-5 accuracy in ImageNet\nclassification. Moreover, SPP can be directly applied to accelerate\nmulti-branch CNN networks, such as ResNet, without specific adaptations. Our 2x\nspeedup ResNet-50 only suffers 0.8% loss of top-5 accuracy on ImageNet. We\nfurther show the effectiveness of SPP on transfer learning tasks. \n\n"}
{"id": "1709.07253", "contents": "Title: Revisiting Resolution and Inter-Layer Coupling Factors in Modularity for\n  Multilayer Networks Abstract: Modularity for multilayer networks, also called multislice modularity, is\nparametric to a resolution factor and an inter-layer coupling factor. The\nformer is useful to express layer-specific relevance and the latter quantifies\nthe strength of node linkage across the layers of a network. However, such\nparameters can be set arbitrarily, thus discarding any structure information at\ngraph or community level. Other issues are related to the inability of properly\nmodeling order relations over the layers, which is required for dynamic\nnetworks.\n  In this paper we propose a new definition of modularity for multilayer\nnetworks that aims to overcome major issues of existing multislice modularity.\nWe revise the role and semantics of the layer-specific resolution and\ninter-layer coupling terms, and define parameter-free unsupervised approaches\nfor their computation, by using information from the within-layer and\ninter-layer structures of the communities. Moreover, our formulation of\nmultilayer modularity is general enough to account for an available ordering of\nthe layers and relating constraints on layer coupling. Experimental evaluation\nwas conducted using three state-of-the-art methods for multilayer community\ndetection and nine real-world multilayer networks. Results have shown the\nsignificance of our modularity, disclosing the effects of different\ncombinations of the resolution and inter-layer coupling functions. This work\ncan pave the way for the development of new optimization methods for\ndiscovering community structures in multilayer networks. \n\n"}
{"id": "1709.08841", "contents": "Title: Conic Optimization Theory: Convexification Techniques and Numerical\n  Algorithms Abstract: Optimization is at the core of control theory and appears in several areas of\nthis field, such as optimal control, distributed control, system\nidentification, robust control, state estimation, model predictive control and\ndynamic programming. The recent advances in various topics of modern\noptimization have also been revamping the area of machine learning. Motivated\nby the crucial role of optimization theory in the design, analysis, control and\noperation of real-world systems, this tutorial paper offers a detailed overview\nof some major advances in this area, namely conic optimization and its emerging\napplications. First, we discuss the importance of conic optimization in\ndifferent areas. Then, we explain seminal results on the design of hierarchies\nof convex relaxations for a wide range of nonconvex problems. Finally, we study\ndifferent numerical algorithms for large-scale conic optimization problems. \n\n"}
{"id": "1709.08915", "contents": "Title: Telling Cause from Effect using MDL-based Local and Global Regression Abstract: We consider the fundamental problem of inferring the causal direction between\ntwo univariate numeric random variables $X$ and $Y$ from observational data.\nThe two-variable case is especially difficult to solve since it is not possible\nto use standard conditional independence tests between the variables.\n  To tackle this problem, we follow an information theoretic approach based on\nKolmogorov complexity and use the Minimum Description Length (MDL) principle to\nprovide a practical solution. In particular, we propose a compression scheme to\nencode local and global functional relations using MDL-based regression. We\ninfer $X$ causes $Y$ in case it is shorter to describe $Y$ as a function of $X$\nthan the inverse direction. In addition, we introduce Slope, an efficient\nlinear-time algorithm that through thorough empirical evaluation on both\nsynthetic and real world data we show outperforms the state of the art by a\nwide margin. \n\n"}
{"id": "1709.10219", "contents": "Title: Information Geometry Connecting Wasserstein Distance and\n  Kullback-Leibler Divergence via the Entropy-Relaxed Transportation Problem Abstract: Two geometrical structures have been extensively studied for a manifold of\nprobability distributions. One is based on the Fisher information metric, which\nis invariant under reversible transformations of random variables, while the\nother is based on the Wasserstein distance of optimal transportation, which\nreflects the structure of the distance between random variables. Here, we\npropose a new information-geometrical theory that is a unified framework\nconnecting the Wasserstein distance and Kullback-Leibler (KL) divergence. We\nprimarily considered a discrete case consisting of $n$ elements and studied the\ngeometry of the probability simplex $S_{n-1}$, which is the set of all\nprobability distributions over $n$ elements. The Wasserstein distance was\nintroduced in $S_{n-1}$ by the optimal transportation of commodities from\ndistribution ${\\mathbf{p}}$ to distribution ${\\mathbf{q}}$, where\n${\\mathbf{p}}$, ${\\mathbf{q}} \\in S_{n-1}$. We relaxed the optimal\ntransportation by using entropy, which was introduced by Cuturi. The optimal\nsolution was called the entropy-relaxed stochastic transportation plan. The\nentropy-relaxed optimal cost $C({\\mathbf{p}}, {\\mathbf{q}})$ was\ncomputationally much less demanding than the original Wasserstein distance but\ndoes not define a distance because it is not minimized at\n${\\mathbf{p}}={\\mathbf{q}}$. To define a proper divergence while retaining the\ncomputational advantage, we first introduced a divergence function in the\nmanifold $S_{n-1} \\times S_{n-1}$ of optimal transportation plans. We fully\nexplored the information geometry of the manifold of the optimal transportation\nplans and subsequently constructed a new one-parameter family of divergences in\n$S_{n-1}$ that are related to both the Wasserstein distance and the\nKL-divergence. \n\n"}
{"id": "1710.01410", "contents": "Title: Learning Registered Point Processes from Idiosyncratic Observations Abstract: A parametric point process model is developed, with modeling based on the\nassumption that sequential observations often share latent phenomena, while\nalso possessing idiosyncratic effects. An alternating optimization method is\nproposed to learn a \"registered\" point process that accounts for shared\nstructure, as well as \"warping\" functions that characterize idiosyncratic\naspects of each observed sequence. Under reasonable constraints, in each\niteration we update the sample-specific warping functions by solving a set of\nconstrained nonlinear programming problems in parallel, and update the model by\nmaximum likelihood estimation. The justifiability, complexity and robustness of\nthe proposed method are investigated in detail, and the influence of sequence\nstitching on the learning results is examined empirically. Experiments on both\nsynthetic and real-world data demonstrate that the method yields explainable\npoint process models, achieving encouraging results compared to\nstate-of-the-art methods. \n\n"}
{"id": "1710.01931", "contents": "Title: Forecasting Player Behavioral Data and Simulating in-Game Events Abstract: Understanding player behavior is fundamental in game data science. Video\ngames evolve as players interact with the game, so being able to foresee player\nexperience would help to ensure a successful game development. In particular,\ngame developers need to evaluate beforehand the impact of in-game events.\nSimulation optimization of these events is crucial to increase player\nengagement and maximize monetization. We present an experimental analysis of\nseveral methods to forecast game-related variables, with two main aims: to\nobtain accurate predictions of in-app purchases and playtime in an operational\nproduction environment, and to perform simulations of in-game events in order\nto maximize sales and playtime. Our ultimate purpose is to take a step towards\nthe data-driven development of games. The results suggest that, even though the\nperformance of traditional approaches such as ARIMA is still better, the\noutcomes of state-of-the-art techniques like deep learning are promising. Deep\nlearning comes up as a well-suited general model that could be used to forecast\na variety of time series with different dynamic behaviors. \n\n"}
{"id": "1710.02048", "contents": "Title: Tightness of a new and enhanced semidefinite relaxation for MIMO\n  detection Abstract: In this paper, we consider a fundamental problem in modern digital\ncommunications known as multi-input multi-output (MIMO) detection, which can be\nformulated as a complex quadratic programming problem subject to unit-modulus\nand discrete argument constraints. Various semidefinite relaxation (SDR) based\nalgorithms have been proposed to solve the problem in the literature. In this\npaper, we first show that the conventional SDR is generally not tight for the\nproblem. Then, we propose a new and enhanced SDR and show its tightness under\nan easily checkable condition, which essentially requires the level of the\nnoise to be below a certain threshold. The above results have answered an open\nquestion posed by So in [35]. Numerical simulation results show that our\nproposed SDR significantly outperforms the conventional SDR in terms of the\nrelaxation gap. \n\n"}
{"id": "1710.02236", "contents": "Title: Primal-Dual Optimization Algorithms over Riemannian Manifolds: an\n  Iteration Complexity Analysis Abstract: In this paper we study nonconvex and nonsmooth multi-block optimization over\nRiemannian manifolds with coupled linear constraints. Such optimization\nproblems naturally arise from machine learning, statistical learning,\ncompressive sensing, image processing, and tensor PCA, among others. We develop\nan ADMM-like primal-dual approach based on decoupled solvable subroutines such\nas linearized proximal mappings. First, we introduce the optimality conditions\nfor the afore-mentioned optimization models. Then, the notion of\n$\\epsilon$-stationary solutions is introduced as a result. The main part of the\npaper is to show that the proposed algorithms enjoy an iteration complexity of\n$O(1/\\epsilon^2)$ to reach an $\\epsilon$-stationary solution. For prohibitively\nlarge-size tensor or machine learning models, we present a sampling-based\nstochastic algorithm with the same iteration complexity bound in expectation.\nIn case the subproblems are not analytically solvable, a feasible curvilinear\nline-search variant of the algorithm based on retraction operators is proposed.\nFinally, we show specifically how the algorithms can be implemented to solve a\nvariety of practical problems such as the NP-hard maximum bisection problem,\nthe $\\ell_q$ regularized sparse tensor principal component analysis and the\ncommunity detection problem. Our preliminary numerical results show great\npotentials of the proposed methods. \n\n"}
{"id": "1710.03504", "contents": "Title: Analysis of world terror networks from the reduced Google matrix of\n  Wikipedia Abstract: We apply the reduced Google matrix method to analyse interactions between 95\nterrorist groups and determine their relationships and influence on 64 world\ncountries. This is done on the basis of the Google matrix of the English\nWikipedia (2017) composed of 5416537 articles which accumulate a great part of\nglobal human knowledge. The reduced Google matrix takes into account the direct\nand hidden links between a selection of 159 nodes (articles) appearing due to\nall paths of a random surfer moving over the whole network. As a result we\nobtain the network structure of terrorist groups and their relations with\nselected countries. Using the sensitivity of PageRank to a weight variation of\nspecific links we determine the geopolitical sensitivity and influence of\nspecific terrorist groups on world countries. We argue that this approach can\nfind useful application for more extensive and detailed data bases analysis. \n\n"}
{"id": "1710.03914", "contents": "Title: Backward Approximate Dynamic Programming with Hidden Semi-Markov\n  Stochastic Models in Energy Storage Optimization Abstract: We consider an energy storage problem involving a wind farm with a forecasted\npower output, a stochastic load, an energy storage device, and a connection to\nthe larger power grid with stochastic prices. Electricity prices and wind power\nforecast errors are modeled using a novel hidden semi-Markov model that\naccurately replicates not just the distribution of the errors, but also\ncrossing times, capturing the amount of time each process stays above or below\nsome benchmark such as the forecast. This is an important property of\nstochastic processes involved in storage problems. We show that we achieve more\nrobust solutions using this model than when more common stochastic models are\nconsidered. The new model introduces some additional complexity to the problem\nas its information states are partially hidden, forming a partially observable\nMarkov decision process. We derive a near-optimal time-dependent policy using\nbackward approximate dynamic programming, which overcomes the computational\nhurdles of classical (exact) backward dynamic programming, with higher quality\nsolutions than the more familiar forward approximate dynamic programming\nmethods. \n\n"}
{"id": "1710.04329", "contents": "Title: Efficient Data-Driven Geologic Feature Detection from Pre-stack Seismic\n  Measurements using Randomized Machine-Learning Algorithm Abstract: Conventional seismic techniques for detecting the subsurface geologic\nfeatures are challenged by limited data coverage, computational inefficiency,\nand subjective human factors. We developed a novel data-driven geological\nfeature detection approach based on pre-stack seismic measurements. Our\ndetection method employs an efficient and accurate machine-learning detection\napproach to extract useful subsurface geologic features automatically.\nSpecifically, our method is based on kernel ridge regression model. The\nconventional kernel ridge regression can be computationally prohibited because\nof the large volume of seismic measurements. We employ a data reduction\ntechnique in combination with the conventional kernel ridge regression method\nto improve the computational efficiency and reduce memory usage. In particular,\nwe utilize a randomized numerical linear algebra technique, named Nystr\\\"om\nmethod, to effectively reduce the dimensionality of the feature space without\ncompromising the information content required for accurate detection. We\nprovide thorough computational cost analysis to show efficiency of our new\ngeological feature detection methods. We further validate the performance of\nour new subsurface geologic feature detection method using synthetic surface\nseismic data for 2D acoustic and elastic velocity models. Our numerical\nexamples demonstrate that our new detection method significantly improves the\ncomputational efficiency while maintaining comparable accuracy. Interestingly,\nwe show that our method yields a speed-up ratio on the order of $\\sim10^2$ to\n$\\sim 10^3$ in a multi-core computational environment. \n\n"}
{"id": "1710.05184", "contents": "Title: A Model Predictive Approach to Preventing Cascading Failures of Power\n  Systems Abstract: Power system blackouts are usually triggered by the initial contingency and\nthen deteriorate as the branch outage spreads quickly. Thus, it is crucial to\neliminate the propagation of cascading outages in its infancy. In this paper, a\nmodel predictive approach is proposed to protect power grids against cascading\nblackout by timely shedding load on buses. The cascading dynamics of power\ngrids is described by a cascading outage model of transmission lines coupled\nwith the DC power flow equation. In addition, a nonlinear convex optimization\nformulation is established to characterize the optimal load shedding for the\nmitigation of cascading failures. As a result, two protection schemes are\ndesigned on the basis of the optimization formulation. One scheme carries out\nthe remedial action once for all, while the other focuses on the consecutive\nprotection measures. Saddle point dynamics is employed to provide a numerical\nsolution to the proposed optimization problem, and its global convergence is\nguaranteed in theory. Finally, numerical simulations on IEEE 57 Bus Systems\nhave been implemented to validate the proposed approach in terms of preventing\nthe degradation of power grids. \n\n"}
{"id": "1710.05776", "contents": "Title: Nonsmooth Frank-Wolfe using Uniform Affine Approximations Abstract: Frank-Wolfe methods (FW) have gained significant interest in the machine\nlearning community due to its ability to efficiently solve large problems that\nadmit a sparse structure (e.g. sparse vectors and low-rank matrices). However\nthe performance of the existing FW method hinges on the quality of the linear\napproximation. This typically restricts FW to smooth functions for which the\napproximation quality, indicated by a global curvature measure, is reasonably\ngood.\n  In this paper, we propose a modified FW algorithm amenable to nonsmooth\nfunctions by optimizing for approximation quality over all affine\napproximations given a neighborhood of interest. We analyze theoretical\nproperties of the proposed algorithm and demonstrate that it overcomes many\nissues associated with existing methods in the context of nonsmooth low-rank\nmatrix estimation. \n\n"}
{"id": "1710.06612", "contents": "Title: Mirror Descent and Convex Optimization Problems With Non-Smooth\n  Inequality Constraints Abstract: We consider the problem of minimization of a convex function on a simple set\nwith convex non-smooth inequality constraint and describe first-order methods\nto solve such problems in different situations: smooth or non-smooth objective\nfunction; convex or strongly convex objective and constraint; deterministic or\nrandomized information about the objective and constraint. We hope that it is\nconvenient for a reader to have all the methods for different settings in one\nplace. Described methods are based on Mirror Descent algorithm and switching\nsubgradient scheme. One of our focus is to propose, for the listed different\nsettings, a Mirror Descent with adaptive stepsizes and adaptive stopping rule.\nThis means that neither stepsize nor stopping rule require to know the\nLipschitz constant of the objective or constraint. We also construct Mirror\nDescent for problems with objective function, which is not Lipschitz\ncontinuous, e.g. is a quadratic function. Besides that, we address the problem\nof recovering the solution of the dual problem. \n\n"}
{"id": "1710.07110", "contents": "Title: Meta-Learning via Feature-Label Memory Network Abstract: Deep learning typically requires training a very capable architecture using\nlarge datasets. However, many important learning problems demand an ability to\ndraw valid inferences from small size datasets, and such problems pose a\nparticular challenge for deep learning. In this regard, various researches on\n\"meta-learning\" are being actively conducted. Recent work has suggested a\nMemory Augmented Neural Network (MANN) for meta-learning. MANN is an\nimplementation of a Neural Turing Machine (NTM) with the ability to rapidly\nassimilate new data in its memory, and use this data to make accurate\npredictions. In models such as MANN, the input data samples and their\nappropriate labels from previous step are bound together in the same memory\nlocations. This often leads to memory interference when performing a task as\nthese models have to retrieve a feature of an input from a certain memory\nlocation and read only the label information bound to that location. In this\npaper, we tried to address this issue by presenting a more robust MANN. We\nrevisited the idea of meta-learning and proposed a new memory augmented neural\nnetwork by explicitly splitting the external memory into feature and label\nmemories. The feature memory is used to store the features of input data\nsamples and the label memory stores their labels. Hence, when predicting the\nlabel of a given input, our model uses its feature memory unit as a reference\nto extract the stored feature of the input, and based on that feature, it\nretrieves the label information of the input from the label memory unit. In\norder for the network to function in this framework, a new memory-writingmodule\nto encode label information into the label memory in accordance with the\nmeta-learning task structure is designed. Here, we demonstrate that our model\noutperforms MANN by a large margin in supervised one-shot classification tasks\nusing Omniglot and MNIST datasets. \n\n"}
{"id": "1710.07402", "contents": "Title: The Collatz-Wielandt quotient for pairs of nonnegative operators Abstract: In this paper we consider two versions of the Collatz-Wielandt quotient for a\npair of nonnegative operators A,B that map a given pointed generating cone in\nthe first space into a given pointed generating cone in the second space. If\nthe two spaces and two cones are identical, and B is the identity operator then\none version of this quotient is the spectral radius of A. In some applications,\nas commodity pricing, power control in wireless networks and quantum\ninformation theory, one needs to deal with the Collatz-Wielandt quotient for\ntwo nonnegative operators. In this paper we treat the two important cases: a\npair of rectangular nonnegative matrices and a pair completely positive\noperators. We give a characterization of minimal optimal solutions and\npolynomially computable bounds on the Collatz-Wielandt quotient. \n\n"}
{"id": "1710.07941", "contents": "Title: WristAuthen: A Dynamic Time Wrapping Approach for User Authentication by\n  Hand-Interaction through Wrist-Worn Devices Abstract: The growing trend of using wearable devices for context-aware computing and\npervasive sensing systems has raised its potentials for quick and reliable\nauthentication techniques. Since personal writing habitats differ from each\nother, it is possible to realize user authentication through writing. This is\nof great significance as sensible information is easily collected by these\ndevices. This paper presents a novel user authentication system through\nwrist-worn devices by analyzing the interaction behavior with users, which is\nboth accurate and efficient for future usage. The key feature of our approach\nlies in using much more effective Savitzky-Golay filter and Dynamic Time\nWrapping method to obtain fine-grained writing metrics for user authentication.\nThese new metrics are relatively unique from person to person and independent\nof the computing platform. Analyses are conducted on the wristband-interaction\ndata collected from 50 users with diversity in gender, age, and height.\nExtensive experimental results show that the proposed approach can identify\nusers in a timely and accurate manner, with a false-negative rate of 1.78\\%,\nfalse-positive rate of 6.7\\%, and Area Under ROC Curve of 0.983 . Additional\nexamination on robustness to various mimic attacks, tolerance to training data,\nand comparisons to further analyze the applicability. \n\n"}
{"id": "1710.10547", "contents": "Title: Interpretation of Neural Networks is Fragile Abstract: In order for machine learning to be deployed and trusted in many\napplications, it is crucial to be able to reliably explain why the machine\nlearning algorithm makes certain predictions. For example, if an algorithm\nclassifies a given pathology image to be a malignant tumor, then the doctor may\nneed to know which parts of the image led the algorithm to this classification.\nHow to interpret black-box predictors is thus an important and active area of\nresearch. A fundamental question is: how much can we trust the interpretation\nitself? In this paper, we show that interpretation of deep learning predictions\nis extremely fragile in the following sense: two perceptively indistinguishable\ninputs with the same predicted label can be assigned very different\ninterpretations. We systematically characterize the fragility of several\nwidely-used feature-importance interpretation methods (saliency maps, relevance\npropagation, and DeepLIFT) on ImageNet and CIFAR-10. Our experiments show that\neven small random perturbation can change the feature importance and new\nsystematic perturbations can lead to dramatically different interpretations\nwithout changing the label. We extend these results to show that\ninterpretations based on exemplars (e.g. influence functions) are similarly\nfragile. Our analysis of the geometry of the Hessian matrix gives insight on\nwhy fragility could be a fundamental challenge to the current interpretation\napproaches. \n\n"}
{"id": "1710.10784", "contents": "Title: How deep learning works --The geometry of deep learning Abstract: Why and how that deep learning works well on different tasks remains a\nmystery from a theoretical perspective. In this paper we draw a geometric\npicture of the deep learning system by finding its analogies with two existing\ngeometric structures, the geometry of quantum computations and the geometry of\nthe diffeomorphic template matching. In this framework, we give the geometric\nstructures of different deep learning systems including convolutional neural\nnetworks, residual networks, recursive neural networks, recurrent neural\nnetworks and the equilibrium prapagation framework. We can also analysis the\nrelationship between the geometrical structures and their performance of\ndifferent networks in an algorithmic level so that the geometric framework may\nguide the design of the structures and algorithms of deep learning systems. \n\n"}
{"id": "1710.10928", "contents": "Title: Optimization Landscape and Expressivity of Deep CNNs Abstract: We analyze the loss landscape and expressiveness of practical deep\nconvolutional neural networks (CNNs) with shared weights and max pooling\nlayers. We show that such CNNs produce linearly independent features at a\n\"wide\" layer which has more neurons than the number of training samples. This\ncondition holds e.g. for the VGG network. Furthermore, we provide for such wide\nCNNs necessary and sufficient conditions for global minima with zero training\nerror. For the case where the wide layer is followed by a fully connected layer\nwe show that almost every critical point of the empirical loss is a global\nminimum with zero training error. Our analysis suggests that both depth and\nwidth are very important in deep learning. While depth brings more\nrepresentational power and allows the network to learn high level features,\nwidth smoothes the optimization landscape of the loss function in the sense\nthat a sufficiently wide network has a well-behaved loss surface with almost no\nbad local minima. \n\n"}
{"id": "1711.00982", "contents": "Title: From which world is your graph? Abstract: Discovering statistical structure from links is a fundamental problem in the\nanalysis of social networks. Choosing a misspecified model, or equivalently, an\nincorrect inference algorithm will result in an invalid analysis or even\nfalsely uncover patterns that are in fact artifacts of the model. This work\nfocuses on unifying two of the most widely used link-formation models: the\nstochastic blockmodel (SBM) and the small world (or latent space) model (SWM).\nIntegrating techniques from kernel learning, spectral graph theory, and\nnonlinear dimensionality reduction, we develop the first statistically sound\npolynomial-time algorithm to discover latent patterns in sparse graphs for both\nmodels. When the network comes from an SBM, the algorithm outputs a block\nstructure. When it is from an SWM, the algorithm outputs estimates of each\nnode's latent position. \n\n"}
{"id": "1711.01012", "contents": "Title: Policy Optimization by Genetic Distillation Abstract: Genetic algorithms have been widely used in many practical optimization\nproblems. Inspired by natural selection, operators, including mutation,\ncrossover and selection, provide effective heuristics for search and black-box\noptimization. However, they have not been shown useful for deep reinforcement\nlearning, possibly due to the catastrophic consequence of parameter crossovers\nof neural networks. Here, we present Genetic Policy Optimization (GPO), a new\ngenetic algorithm for sample-efficient deep policy optimization. GPO uses\nimitation learning for policy crossover in the state space and applies policy\ngradient methods for mutation. Our experiments on MuJoCo tasks show that GPO as\na genetic algorithm is able to provide superior performance over the\nstate-of-the-art policy gradient methods and achieves comparable or higher\nsample efficiency. \n\n"}
{"id": "1711.01174", "contents": "Title: Nodal domains, spectral minimal partitions, and their relation to\n  Aharonov-Bohm operators Abstract: This survey is a short version of a chapter written by the first two authors\nin the book [A. Henrot, editor. Shape optimization and spectral theory. Berlin:\nDe Gruyter, 2017] (where more details and references are given) but we have\ndecided here to put more emphasis on the role of the Aharonov-Bohm operators\nwhich appear to be a useful tool coming from physics for understanding a\nproblem motivated either by spectral geometry or dynamics of population.\nSimilar questions appear also in Bose-Einstein theory. Finally some open\nproblems which might be of interest are mentioned. \n\n"}
{"id": "1711.01682", "contents": "Title: Estimation of Low-Rank Matrices via Approximate Message Passing Abstract: Consider the problem of estimating a low-rank matrix when its entries are\nperturbed by Gaussian noise. If the empirical distribution of the entries of\nthe spikes is known, optimal estimators that exploit this knowledge can\nsubstantially outperform simple spectral approaches. Recent work characterizes\nthe asymptotic accuracy of Bayes-optimal estimators in the high-dimensional\nlimit. In this paper we present a practical algorithm that can achieve\nBayes-optimal accuracy above the spectral threshold. A bold conjecture from\nstatistical physics posits that no polynomial-time algorithm achieves optimal\nerror below the same threshold (unless the best estimator is trivial). Our\napproach uses Approximate Message Passing (AMP) in conjunction with a spectral\ninitialization. AMP algorithms have proved successful in a variety of\nstatistical estimation tasks, and are amenable to exact asymptotic analysis via\nstate evolution. Unfortunately, state evolution is uninformative when the\nalgorithm is initialized near an unstable fixed point, as often happens in\nlow-rank matrix estimation. We develop a new analysis of AMP that allows for\nspectral initializations. Our main theorem is general and applies beyond matrix\nestimation. However, we use it to derive detailed predictions for the problem\nof estimating a rank-one matrix in noise. Special cases of this problem are\nclosely related---via universality arguments---to the network community\ndetection problem for two asymmetric communities. For general rank-one models,\nwe show that AMP can be used to construct confidence intervals and control\nfalse discovery rate. We provide illustrations of the general methodology by\nconsidering the cases of sparse low-rank matrices and of block-constant\nlow-rank matrices with symmetric blocks (we refer to the latter as to the\n`Gaussian Block Model'). \n\n"}
{"id": "1711.02033", "contents": "Title: Estimating Cosmological Parameters from the Dark Matter Distribution Abstract: A grand challenge of the 21st century cosmology is to accurately estimate the\ncosmological parameters of our Universe. A major approach to estimating the\ncosmological parameters is to use the large-scale matter distribution of the\nUniverse. Galaxy surveys provide the means to map out cosmic large-scale\nstructure in three dimensions. Information about galaxy locations is typically\nsummarized in a \"single\" function of scale, such as the galaxy correlation\nfunction or power-spectrum. We show that it is possible to estimate these\ncosmological parameters directly from the distribution of matter. This paper\npresents the application of deep 3D convolutional networks to volumetric\nrepresentation of dark-matter simulations as well as the results obtained using\na recently proposed distribution regression framework, showing that machine\nlearning techniques are comparable to, and can sometimes outperform,\nmaximum-likelihood point estimates using \"cosmological models\". This opens the\nway to estimating the parameters of our Universe with higher accuracy. \n\n"}
{"id": "1711.02613", "contents": "Title: Moonshine: Distilling with Cheap Convolutions Abstract: Many engineers wish to deploy modern neural networks in memory-limited\nsettings; but the development of flexible methods for reducing memory use is in\nits infancy, and there is little knowledge of the resulting cost-benefit. We\npropose structural model distillation for memory reduction using a strategy\nthat produces a student architecture that is a simple transformation of the\nteacher architecture: no redesign is needed, and the same hyperparameters can\nbe used. Using attention transfer, we provide Pareto curves/tables for\ndistillation of residual networks with four benchmark datasets, indicating the\nmemory versus accuracy payoff. We show that substantial memory savings are\npossible with very little loss of accuracy, and confirm that distillation\nprovides student network performance that is better than training that student\narchitecture directly on data. \n\n"}
{"id": "1711.02653", "contents": "Title: Neural system identification for large populations separating \"what\" and\n  \"where\" Abstract: Neuroscientists classify neurons into different types that perform similar\ncomputations at different locations in the visual field. Traditional methods\nfor neural system identification do not capitalize on this separation of 'what'\nand 'where'. Learning deep convolutional feature spaces that are shared among\nmany neurons provides an exciting path forward, but the architectural design\nneeds to account for data limitations: While new experimental techniques enable\nrecordings from thousands of neurons, experimental time is limited so that one\ncan sample only a small fraction of each neuron's response space. Here, we show\nthat a major bottleneck for fitting convolutional neural networks (CNNs) to\nneural data is the estimation of the individual receptive field locations, a\nproblem that has been scratched only at the surface thus far. We propose a CNN\narchitecture with a sparse readout layer factorizing the spatial (where) and\nfeature (what) dimensions. Our network scales well to thousands of neurons and\nshort recordings and can be trained end-to-end. We evaluate this architecture\non ground-truth data to explore the challenges and limitations of CNN-based\nsystem identification. Moreover, we show that our network model outperforms\ncurrent state-of-the art system identification models of mouse primary visual\ncortex. \n\n"}
{"id": "1711.03018", "contents": "Title: Stochastic Stability in Max-Product and Max-Plus Systems with Markovian\n  Jumps Abstract: We study Max-Product and Max-Plus Systems with Markovian Jumps and focus on\nstochastic stability problems. At first, a Lyapunov function is derived for the\nasymptotically stable deterministic Max-Product Systems. This Lyapunov function\nis then adjusted to derive sufficient conditions for the stochastic stability\nof Max-Product systems with Markovian Jumps. Many step Lyapunov functions are\nthen used to derive necessary and sufficient conditions for stochastic\nstability. The results for the Max-Product systems are then applied to Max-Plus\nsystems with Markovian Jumps, using an isomorphism and almost sure bounds for\nthe asymptotic behavior of the state are obtained. A numerical example\nillustrating the application of the stability results on a production system is\nalso given. \n\n"}
{"id": "1711.03634", "contents": "Title: Alternating minimization for dictionary learning: Local Convergence\n  Guarantees Abstract: We present theoretical guarantees for an alternating minimization algorithm\nfor the dictionary learning/sparse coding problem. The dictionary learning\nproblem is to factorize vector samples $y^{1},y^{2},\\ldots, y^{n}$ into an\nappropriate basis (dictionary) $A^*$ and sparse vectors $x^{1*},\\ldots,x^{n*}$.\nOur algorithm is a simple alternating minimization procedure that switches\nbetween $\\ell_1$ minimization and gradient descent in alternate steps.\nDictionary learning and specifically alternating minimization algorithms for\ndictionary learning are well studied both theoretically and empirically.\nHowever, in contrast to previous theoretical analyses for this problem, we\nreplace a condition on the operator norm (that is, the largest magnitude\nsingular value) of the true underlying dictionary $A^*$ with a condition on the\nmatrix infinity norm (that is, the largest magnitude term). Our guarantees are\nunder a reasonable generative model that allows for dictionaries with growing\noperator norms, and can handle an arbitrary level of overcompleteness, while\nhaving sparsity that is information theoretically optimal. We also establish\nupper bounds on the sample complexity of our algorithm. \n\n"}
{"id": "1711.05560", "contents": "Title: Variational Adaptive-Newton Method for Explorative Learning Abstract: We present the Variational Adaptive Newton (VAN) method which is a black-box\noptimization method especially suitable for explorative-learning tasks such as\nactive learning and reinforcement learning. Similar to Bayesian methods, VAN\nestimates a distribution that can be used for exploration, but requires\ncomputations that are similar to continuous optimization methods. Our\ntheoretical contribution reveals that VAN is a second-order method that unifies\nexisting methods in distinct fields of continuous optimization, variational\ninference, and evolution strategies. Our experimental results show that VAN\nperforms well on a wide-variety of learning tasks. This work presents a\ngeneral-purpose explorative-learning method that has the potential to improve\nlearning in areas such as active learning and reinforcement learning. \n\n"}
{"id": "1711.06114", "contents": "Title: Robust Unsupervised Domain Adaptation for Neural Networks via Moment\n  Alignment Abstract: A novel approach for unsupervised domain adaptation for neural networks is\nproposed. It relies on metric-based regularization of the learning process. The\nmetric-based regularization aims at domain-invariant latent feature\nrepresentations by means of maximizing the similarity between domain-specific\nactivation distributions. The proposed metric results from modifying an\nintegral probability metric such that it becomes less translation-sensitive on\na polynomial function space. The metric has an intuitive interpretation in the\ndual space as the sum of differences of higher order central moments of the\ncorresponding activation distributions. Under appropriate assumptions on the\ninput distributions, error minimization is proven for the continuous case. As\ndemonstrated by an analysis of standard benchmark experiments for sentiment\nanalysis, object recognition and digit recognition, the outlined approach is\nrobust regarding parameter changes and achieves higher classification\naccuracies than comparable approaches. The source code is available at\nhttps://github.com/wzell/mann. \n\n"}
{"id": "1711.06798", "contents": "Title: MorphNet: Fast & Simple Resource-Constrained Structure Learning of Deep\n  Networks Abstract: We present MorphNet, an approach to automate the design of neural network\nstructures. MorphNet iteratively shrinks and expands a network, shrinking via a\nresource-weighted sparsifying regularizer on activations and expanding via a\nuniform multiplicative factor on all layers. In contrast to previous\napproaches, our method is scalable to large networks, adaptable to specific\nresource constraints (e.g. the number of floating-point operations per\ninference), and capable of increasing the network's performance. When applied\nto standard network architectures on a wide variety of datasets, our approach\ndiscovers novel structures in each domain, obtaining higher performance while\nrespecting the resource constraint. \n\n"}
{"id": "1711.07050", "contents": "Title: A Classifying Variational Autoencoder with Application to Polyphonic\n  Music Generation Abstract: The variational autoencoder (VAE) is a popular probabilistic generative\nmodel. However, one shortcoming of VAEs is that the latent variables cannot be\ndiscrete, which makes it difficult to generate data from different modes of a\ndistribution. Here, we propose an extension of the VAE framework that\nincorporates a classifier to infer the discrete class of the modeled data. To\nmodel sequential data, we can combine our Classifying VAE with a recurrent\nneural network such as an LSTM. We apply this model to algorithmic music\ngeneration, where our model learns to generate musical sequences in different\nkeys. Most previous work in this area avoids modeling key by transposing data\ninto only one or two keys, as opposed to the 10+ different keys in the original\nmusic. We show that our Classifying VAE and Classifying VAE+LSTM models\noutperform the corresponding non-classifying models in generating musical\nsamples that stay in key. This benefit is especially apparent when trained on\nuntransposed music data in the original keys. \n\n"}
{"id": "1711.07414", "contents": "Title: The Promise and Peril of Human Evaluation for Model Interpretability Abstract: Transparency, user trust, and human comprehension are popular ethical\nmotivations for interpretable machine learning. In support of these goals,\nresearchers evaluate model explanation performance using humans and real world\napplications. This alone presents a challenge in many areas of artificial\nintelligence. In this position paper, we propose a distinction between\ndescriptive and persuasive explanations. We discuss reasoning suggesting that\nfunctional interpretability may be correlated with cognitive function and user\npreferences. If this is indeed the case, evaluation and optimization using\nfunctional metrics could perpetuate implicit cognitive bias in explanations\nthat threaten transparency. Finally, we propose two potential research\ndirections to disambiguate cognitive function and explanation models, retaining\ncontrol over the tradeoff between accuracy and interpretability. \n\n"}
{"id": "1711.07839", "contents": "Title: Application of generative autoencoder in de novo molecular design Abstract: A major challenge in computational chemistry is the generation of novel\nmolecular structures with desirable pharmacological and physiochemical\nproperties. In this work, we investigate the potential use of autoencoder, a\ndeep learning methodology, for de novo molecular design. Various generative\nautoencoders were used to map molecule structures into a continuous latent\nspace and vice versa and their performance as structure generator was assessed.\nOur results show that the latent space preserves chemical similarity principle\nand thus can be used for the generation of analogue structures. Furthermore,\nthe latent space created by autoencoders were searched systematically to\ngenerate novel compounds with predicted activity against dopamine receptor type\n2 and compounds similar to known active compounds not included in the training\nset were identified. \n\n"}
{"id": "1711.08851", "contents": "Title: Convex Relaxations for Nonlinear Stochastic Optimal Control Problems Abstract: This article presents a new method for computing guaranteed convex and\nconcave relaxations of nonlinear stochastic optimal control problems with\nfinal-time expected-value cost functions. This method is motivated by similar\nmethods for deterministic optimal control problems, which have been\nsuccessfully applied within spatial branch-and-bound (B&B) techniques to obtain\nguaranteed global optima. Relative to those methods, a key challenge here is\nthat the expected-value cost function cannot be expressed analytically in\nclosed form. Nonetheless, the presented relaxations provide rigorous lower and\nupper bounds on the optimal objective value with no sample-based approximation\nerror. In principle, this enables the use of spatial B&B global optimization\ntechniques, but we leave the details of such an algorithm for future work. \n\n"}
{"id": "1711.10175", "contents": "Title: PhasePack: A Phase Retrieval Library Abstract: Phase retrieval deals with the estimation of complex-valued signals solely\nfrom the magnitudes of linear measurements. While there has been a recent\nexplosion in the development of phase retrieval algorithms, the lack of a\ncommon interface has made it difficult to compare new methods against the\nstate-of-the-art. The purpose of PhasePack is to create a common software\ninterface for a wide range of phase retrieval algorithms and to provide a\ncommon testbed using both synthetic data and empirical imaging datasets.\nPhasePack is able to benchmark a large number of recent phase retrieval methods\nagainst one another to generate comparisons using a range of different\nperformance metrics. The software package handles single method testing as well\nas multiple method comparisons.\n  The algorithm implementations in PhasePack differ slightly from their\noriginal descriptions in the literature in order to achieve faster speed and\nimproved robustness. In particular, PhasePack uses adaptive stepsizes,\nline-search methods, and fast eigensolvers to speed up and automate\nconvergence. \n\n"}
{"id": "1711.10467", "contents": "Title: Implicit Regularization in Nonconvex Statistical Estimation: Gradient\n  Descent Converges Linearly for Phase Retrieval, Matrix Completion, and Blind\n  Deconvolution Abstract: Recent years have seen a flurry of activities in designing provably efficient\nnonconvex procedures for solving statistical estimation problems. Due to the\nhighly nonconvex nature of the empirical loss, state-of-the-art procedures\noften require proper regularization (e.g. trimming, regularized cost,\nprojection) in order to guarantee fast convergence. For vanilla procedures such\nas gradient descent, however, prior theory either recommends highly\nconservative learning rates to avoid overshooting, or completely lacks\nperformance guarantees.\n  This paper uncovers a striking phenomenon in nonconvex optimization: even in\nthe absence of explicit regularization, gradient descent enforces proper\nregularization implicitly under various statistical models. In fact, gradient\ndescent follows a trajectory staying within a basin that enjoys nice geometry,\nconsisting of points incoherent with the sampling mechanism. This \"implicit\nregularization\" feature allows gradient descent to proceed in a far more\naggressive fashion without overshooting, which in turn results in substantial\ncomputational savings. Focusing on three fundamental statistical estimation\nproblems, i.e. phase retrieval, low-rank matrix completion, and blind\ndeconvolution, we establish that gradient descent achieves near-optimal\nstatistical and computational guarantees without explicit regularization. In\nparticular, by marrying statistical modeling with generic optimization theory,\nwe develop a general recipe for analyzing the trajectories of iterative\nalgorithms via a leave-one-out perturbation argument. As a byproduct, for noisy\nmatrix completion, we demonstrate that gradient descent achieves near-optimal\nerror control --- measured entrywise and by the spectral norm --- which might\nbe of independent interest. \n\n"}
{"id": "1711.10938", "contents": "Title: Extreme Dimension Reduction for Handling Covariate Shift Abstract: In the covariate shift learning scenario, the training and test covariate\ndistributions differ, so that a predictor's average loss over the training and\ntest distributions also differ. In this work, we explore the potential of\nextreme dimension reduction, i.e. to very low dimensions, in improving the\nperformance of importance weighting methods for handling covariate shift, which\nfail in high dimensions due to potentially high train/test covariate divergence\nand the inability to accurately estimate the requisite density ratios. We first\nformulate and solve a problem optimizing over linear subspaces a combination of\ntheir predictive utility and train/test divergence within. Applying it to\nsimulated and real data, we show extreme dimension reduction helps sometimes\nbut not always, due to a bias introduced by dimension reduction. \n\n"}
{"id": "1712.00504", "contents": "Title: A Neural Stochastic Volatility Model Abstract: In this paper, we show that the recent integration of statistical models with\ndeep recurrent neural networks provides a new way of formulating volatility\n(the degree of variation of time series) models that have been widely used in\ntime series analysis and prediction in finance. The model comprises a pair of\ncomplementary stochastic recurrent neural networks: the generative network\nmodels the joint distribution of the stochastic volatility process; the\ninference network approximates the conditional distribution of the latent\nvariables given the observables. Our focus here is on the formulation of\ntemporal dynamics of volatility over time under a stochastic recurrent neural\nnetwork framework. Experiments on real-world stock price datasets demonstrate\nthat the proposed model generates a better volatility estimation and prediction\nthat outperforms mainstream methods, e.g., deterministic models such as GARCH\nand its variants, and stochastic models namely the MCMC-based model\n\\emph{stochvol} as well as the Gaussian process volatility model \\emph{GPVol},\non average negative log-likelihood. \n\n"}
{"id": "1712.01158", "contents": "Title: Statistical Inference for Incomplete Ranking Data: The Case of\n  Rank-Dependent Coarsening Abstract: We consider the problem of statistical inference for ranking data,\nspecifically rank aggregation, under the assumption that samples are incomplete\nin the sense of not comprising all choice alternatives. In contrast to most\nexisting methods, we explicitly model the process of turning a full ranking\ninto an incomplete one, which we call the coarsening process. To this end, we\npropose the concept of rank-dependent coarsening, which assumes that incomplete\nrankings are produced by projecting a full ranking to a random subset of ranks.\nFor a concrete instantiation of our model, in which full rankings are drawn\nfrom a Plackett-Luce distribution and observations take the form of pairwise\npreferences, we study the performance of various rank aggregation methods. In\naddition to predictive accuracy in the finite sample setting, we address the\ntheoretical question of consistency, by which we mean the ability to recover a\ntarget ranking when the sample size goes to infinity, despite a potential bias\nin the observations caused by the (unknown) coarsening. \n\n"}
{"id": "1712.01664", "contents": "Title: Learning a Generative Model for Validity in Complex Discrete Structures Abstract: Deep generative models have been successfully used to learn representations\nfor high-dimensional discrete spaces by representing discrete objects as\nsequences and employing powerful sequence-based deep models. Unfortunately,\nthese sequence-based models often produce invalid sequences: sequences which do\nnot represent any underlying discrete structure; invalid sequences hinder the\nutility of such models. As a step towards solving this problem, we propose to\nlearn a deep recurrent validator model, which can estimate whether a partial\nsequence can function as the beginning of a full, valid sequence. This\nvalidator provides insight as to how individual sequence elements influence the\nvalidity of the overall sequence, and can be used to constrain sequence based\nmodels to generate valid sequences -- and thus faithfully model discrete\nobjects. Our approach is inspired by reinforcement learning, where an oracle\nwhich can evaluate validity of complete sequences provides a sparse reward\nsignal. We demonstrate its effectiveness as a generative model of Python 3\nsource code for mathematical expressions, and in improving the ability of a\nvariational autoencoder trained on SMILES strings to decode valid molecular\nstructures. \n\n"}
{"id": "1712.01727", "contents": "Title: OL\\'E: Orthogonal Low-rank Embedding, A Plug and Play Geometric Loss for\n  Deep Learning Abstract: Deep neural networks trained using a softmax layer at the top and the\ncross-entropy loss are ubiquitous tools for image classification. Yet, this\ndoes not naturally enforce intra-class similarity nor inter-class margin of the\nlearned deep representations. To simultaneously achieve these two goals,\ndifferent solutions have been proposed in the literature, such as the pairwise\nor triplet losses. However, such solutions carry the extra task of selecting\npairs or triplets, and the extra computational burden of computing and learning\nfor many combinations of them. In this paper, we propose a plug-and-play loss\nterm for deep networks that explicitly reduces intra-class variance and\nenforces inter-class margin simultaneously, in a simple and elegant geometric\nmanner. For each class, the deep features are collapsed into a learned linear\nsubspace, or union of them, and inter-class subspaces are pushed to be as\northogonal as possible. Our proposed Orthogonal Low-rank Embedding (OL\\'E) does\nnot require carefully crafting pairs or triplets of samples for training, and\nworks standalone as a classification loss, being the first reported deep metric\nlearning framework of its kind. Because of the improved margin between features\nof different classes, the resulting deep networks generalize better, are more\ndiscriminative, and more robust. We demonstrate improved classification\nperformance in general object recognition, plugging the proposed loss term into\nexisting off-the-shelf architectures. In particular, we show the advantage of\nthe proposed loss in the small data/model scenario, and we significantly\nadvance the state-of-the-art on the Stanford STL-10 benchmark. \n\n"}
{"id": "1712.02083", "contents": "Title: A Local Analysis of Block Coordinate Descent for Gaussian Phase\n  Retrieval Abstract: While convergence of the Alternating Direction Method of Multipliers (ADMM)\non convex problems is well studied, convergence on nonconvex problems is only\npartially understood. In this paper, we consider the Gaussian phase retrieval\nproblem, formulated as a linear constrained optimization problem with a\nbiconvex objective. The particular structure allows for a novel application of\nthe ADMM. It can be shown that the dual variable is zero at the global\nminimizer. This motivates the analysis of a block coordinate descent algorithm,\nwhich is equivalent to the ADMM with the dual variable fixed to be zero. We\nshow that the block coordinate descent algorithm converges to the global\nminimizer at a linear rate, when starting from a deterministically achievable\ninitialization point. \n\n"}
{"id": "1712.02164", "contents": "Title: On the Singular Control of Exchange Rates Abstract: Consider the problem of a central bank that wants to manage the exchange rate\nbetween its domestic currency and a foreign one. The central bank can purchase\nand sell the foreign currency, and each intervention on the exchange market\nleads to a proportional cost whose instantaneous marginal value depends on the\ncurrent level of the exchange rate. The central bank aims at minimizing the\ntotal expected costs of interventions on the exchange market, plus a total\nexpected holding cost. We formulate this problem as an infinite time-horizon\nstochastic control problem with controls that have paths which are locally of\nbounded variation. The exchange rate evolves as a general linearly controlled\none-dimensional diffusion, and the two nondecreasing processes giving the\nminimal decomposition of a bounded-variation control model the cumulative\namount of foreign currency that has been purchased and sold by the central\nbank. We provide a complete solution to this problem by finding the explicit\nexpression of the value function and a complete characterization of the optimal\ncontrol. At each instant of time, the optimally controlled exchange rate is\nkept within a band whose size is endogenously determined as part of the\nsolution to the problem. We also study the expected exit time from the band,\nand the sensitivity of the width of the band with respect to the model's\nparameters in the case when the exchange rate evolves (in absence of any\nintervention) as an Ornstein-Uhlenbeck process, and the marginal costs of\ncontrols are constant. The techniques employed in the paper are those of the\ntheory of singular stochastic control and of one-dimensional diffusions. \n\n"}
{"id": "1712.03888", "contents": "Title: A semi-implicit scheme based on Arrow-Hurwicz method for saddle point\n  problems Abstract: We search saddle points for a large class of convex-concave Lagrangian. A\ngeneralized explicit iterative scheme based on Arrow-Hurwicz method converges\nto a saddle point of the problem. We also propose in this work, a convergent\nsemi-implicit scheme in order to accelerate the convergence of the iterative\nprocess. Numerical experiments are provided for a nontrivial numerical problem\nmodeling an optimal shape problem of thin torsion rods. This semi-implicit\nscheme is figured out in practice robustly efficient in comparison with the\nexplicit one. \n\n"}
{"id": "1712.04045", "contents": "Title: Choose your path wisely: gradient descent in a Bregman distance\n  framework Abstract: We propose an extension of a special form of gradient descent -- in the\nliterature known as linearised Bregman iteration -- to a larger class of\nnon-convex functions. We replace the classical (squared) two norm metric in the\ngradient descent setting with a generalised Bregman distance, based on a\nproper, convex and lower semi-continuous function. The algorithm's global\nconvergence is proven for functions that satisfy the Kurdyka-\\L ojasiewicz\nproperty. Examples illustrate that features of different scale are being\nintroduced throughout the iteration, transitioning from coarse to fine. This\ncoarse-to-fine approach with respect to scale allows to recover solutions of\nnon-convex optimisation problems that are superior to those obtained with\nconventional gradient descent, or even projected and proximal gradient descent.\nThe effectiveness of the linearised Bregman iteration in combination with early\nstopping is illustrated for the applications of parallel magnetic resonance\nimaging, blind deconvolution as well as image classification with neural\nnetworks. \n\n"}
{"id": "1712.04567", "contents": "Title: Practical Bayesian optimization in the presence of outliers Abstract: Inference in the presence of outliers is an important field of research as\noutliers are ubiquitous and may arise across a variety of problems and domains.\nBayesian optimization is method that heavily relies on probabilistic inference.\nThis allows outstanding sample efficiency because the probabilistic machinery\nprovides a memory of the whole optimization process. However, that virtue\nbecomes a disadvantage when the memory is populated with outliers, inducing\nbias in the estimation. In this paper, we present an empirical evaluation of\nBayesian optimization methods in the presence of outliers. The empirical\nevidence shows that Bayesian optimization with robust regression often produces\nsuboptimal results. We then propose a new algorithm which combines robust\nregression (a Gaussian process with Student-t likelihood) with outlier\ndiagnostics to classify data points as outliers or inliers. By using an\nscheduler for the classification of outliers, our method is more efficient and\nhas better convergence over the standard robust regression. Furthermore, we\nshow that even in controlled situations with no expected outliers, our method\nis able to produce better results. \n\n"}
{"id": "1712.04677", "contents": "Title: Convex programming in optimal control and information theory Abstract: The main theme of this thesis is the development of computational methods for\nclasses of infinite-dimensional optimization problems arising in optimal\ncontrol and information theory. The first part of the thesis is concerned with\nthe optimal control of discrete-time continuous space Markov decision processes\n(MDP). The second part is centred around two fundamental problems in\ninformation theory that can be expressed as optimization problems: the channel\ncapacity problem as well as the entropy maximization subject to moment\nconstraints. \n\n"}
{"id": "1712.06356", "contents": "Title: On convergence of infinite matrix products with alternating factors from\n  two sets of matrices Abstract: We consider the problem of convergence to zero of matrix products\n$A_{n}B_{n}\\cdots A_{1}B_{1}$ with factors from two sets of matrices,\n$A_{i}\\in\\mathscr{A}$ and $B_{i}\\in\\mathscr{B}$, due to a suitable choice of\nmatrices $\\{B_{i}\\}$. It is assumed that for any sequence of matrices\n$\\{A_{i}\\}$ there is a sequence of matrices $\\{B_{i}\\}$ such that the\ncorresponding matrix product $A_{n}B_{n}\\cdots A_{1}B_{1}$ converges to zero.\nWe show that in this case the convergence of the matrix products under\nconsideration is uniformly exponential, that is, $\\|A_{n}B_{n}\\cdots\nA_{1}B_{1}\\|\\le C\\lambda^{n}$, where the constants $C>0$ and $\\lambda\\in(0,1)$\ndo not depend on the sequence $\\{A_{i}\\}$ and the corresponding sequence\n$\\{B_{i}\\}$. \n\n"}
{"id": "1712.08923", "contents": "Title: The Support of Integer Optimal Solutions Abstract: The support of a vector is the number of nonzero-components. We show that\ngiven an integral $m\\times n$ matrix $A$, the integer linear optimization\nproblem $\\max\\left\\{\\boldsymbol{c}^T\\boldsymbol{x} : A\\boldsymbol{x} =\n\\boldsymbol{b}, \\, \\boldsymbol{x}\\ge\\boldsymbol{0},\n\\,\\boldsymbol{x}\\in\\mathbb{Z}^n\\right\\}$ has an optimal solution whose support\nis bounded by $2m \\, \\log (2 \\sqrt{m} \\| A \\|_\\infty)$, where $ \\| A \\|_\\infty$\nis the largest absolute value of an entry of $A$. Compared to previous bounds,\nthe one presented here is independent on the objective function. We furthermore\nprovide a nearly matching asymptotic lower bound on the support of optimal\nsolutions. \n\n"}
{"id": "1712.09983", "contents": "Title: Random Feature-based Online Multi-kernel Learning in Environments with\n  Unknown Dynamics Abstract: Kernel-based methods exhibit well-documented performance in various nonlinear\nlearning tasks. Most of them rely on a preselected kernel, whose prudent choice\npresumes task-specific prior information. Especially when the latter is not\navailable, multi-kernel learning has gained popularity thanks to its\nflexibility in choosing kernels from a prescribed kernel dictionary. Leveraging\nthe random feature approximation and its recent orthogonality-promoting\nvariant, the present contribution develops a scalable multi-kernel learning\nscheme (termed Raker) to obtain the sought nonlinear learning function `on the\nfly,' first for static environments. To further boost performance in dynamic\nenvironments, an adaptive multi-kernel learning scheme (termed AdaRaker) is\ndeveloped. AdaRaker accounts not only for data-driven learning of kernel\ncombination, but also for the unknown dynamics. Performance is analyzed in\nterms of both static and dynamic regrets. AdaRaker is uniquely capable of\ntracking nonlinear learning functions in environments with unknown dynamics,\nand with with analytic performance guarantees. Tests with synthetic and real\ndatasets are carried out to showcase the effectiveness of the novel algorithms. \n\n"}
{"id": "1801.01610", "contents": "Title: On Convergence to Essential Singularities Abstract: An iterative optimization method applied to a function $f$ on $\\mathbb{R}^n$\nwill produce a sequence of arguments $\\{\\mathbf{x}_k\\}_{k \\in \\mathbb{N}}$;\nthis sequence is often constrained such that $\\{f(\\mathbf{x}_k)\\}_{k \\in\n\\mathbb{N}}$ is monotonic. As part of the analysis of an iterative method, one\nmay ask under what conditions the sequence $\\{\\mathbf{x}_k\\}_{k \\in\n\\mathbb{N}}$ converges. In 2005, Absil et al.\\ employed the {\\L}ojasiewicz\ngradient inequality in a proof of convergence; this requires that the objective\nfunction exist at a cluster point of the sequence. Here we provide a\nconvergence result that does not require $f$ to be defined at the limit\n$\\lim_{k \\to \\infty} \\mathbf{x}_k$, should the limit exist. We show that a\nvariant of the {\\L}ojasiewicz gradient inequality holds on sets adjacent to\nsingularities of bounded multivariate rational functions. We extend the results\nof Absil et al.\\ to prove that if $\\{\\mathbf{x}_k\\}_{k \\in \\mathbb{N}} \\subset\n\\mathbb{R}^n$ has a cluster point $\\mathbf{x}_*$, if $f$ is a bounded\nmultivariate rational function on $\\mathbb{R}^n$, and if a technical condition\nholds, then $\\mathbf{x}_k \\to \\mathbf{x}_*$ even if $\\mathbf{x}_*$ is not in\nthe domain of $f$. We demonstrate how this may be employed to analyze divergent\nsequences by mapping them to projective space, and consider the implications\nthis has for the study of low-rank tensor approximations. \n\n"}
{"id": "1801.02161", "contents": "Title: Metastability in Stochastic Replicator Dynamics Abstract: We consider a novel model of stochastic replicator dynamics for potential\ngames that converts to a Langevin equation on a sphere after a change of\nvariables. This is distinct from the models studied earlier. In particular, it\nis ill-posed due to non-uniqueness of solutions, but is amenable to a natural\nselection principle that picks a unique solution. The model allows us to make\nspecific statements regarding metastable states such as small noise asymptotics\nfor mean exit times from their domain of attraction, and quasi-stationary\nmeasures. We illustrate the general results by specializing them to replicator\ndynamics on graphs and demonstrate that the numerical experiments support\ntheoretical predictions. \n\n"}
{"id": "1801.02796", "contents": "Title: Towards Trusted Social Networks with Blockchain Technology Abstract: Large-scale rumor spreading could pose severe social and economic damages.\nThe emergence of online social networks along with the new media can even make\nrumor spreading more severe. Effective control of rumor spreading is of\ntheoretical and practical significance. This paper takes the first step to\nunderstand how the blockchain technology can help limit the spread of rumors.\nSpecifically, we develop a new paradigm for social networks embedded with the\nblockchain technology, which employs decentralized contracts to motivate trust\nnetworks as well as secure information exchange contract. We design a\nblockchain-based sequential algorithm which utilizes virtual information\ncredits for each peer-to-peer information exchange. We validate the\neffectiveness of the blockchain-enabled social network on limiting the rumor\nspreading. Simulation results validate our algorithm design in avoiding rapid\nand intense rumor spreading, and motivate better mechanism design for trusted\nsocial networks. \n\n"}
{"id": "1801.03164", "contents": "Title: Paranom: A Parallel Anomaly Dataset Generator Abstract: In this paper, we present Paranom, a parallel anomaly dataset generator. We\ndiscuss its design and provide brief experimental results demonstrating its\nusefulness in improving the classification correctness of LSTM-AD, a\nstate-of-the-art anomaly detection model. \n\n"}
{"id": "1801.03592", "contents": "Title: Estimation of the Robin coefficient field in a Poisson problem with\n  uncertain conductivity field Abstract: We consider the reconstruction of a heterogeneous coefficient field in a\nRobin boundary condition on an inaccessible part of the boundary in a Poisson\nproblem with an uncertain (or unknown) inhomogeneous conductivity field in the\ninterior of the domain. To account for model errors that stem from the\nuncertainty in the conductivity coefficient, we treat the unknown conductivity\nas a nuisance parameter and carry out approximative premarginalization over it,\nand invert for the Robin coefficient field only. We approximate the related\nmodelling errors via the Bayesian approximation error (BAE) approach. The\nuncertainty analysis presented here relies on a local linearization of the\nparameter-to-observable map at the maximum a posteriori (MAP) estimates, which\nleads to a normal (Gaussian) approximation of the parameter posterior density.\nTo compute the MAP point we apply an inexact Newton conjugate gradient approach\nbased on the adjoint methodology. The construction of the covariance is made\ntractable by invoking a low-rank approximation of the data misfit component of\nthe Hessian. Two numerical experiments are considered: one where the prior\ncovariance on the conductivity is isotropic, and one where the prior covariance\non the conductivity is anisotropic. Results are compared to those based on\nstandard error models, with particular emphasis on the feasibility of the\nposterior uncertainty estimates. We show that the BAE approach is a feasible\none in the sense that the predicted posterior uncertainty is consistent with\nthe actual estimation errors, while neglecting the related modelling error\nyields infeasible estimates for the Robin coefficient. In addition, we\ndemonstrate that the BAE approach is approximately as computationally expensive\n(measured in the number of PDE solves) as the conventional error approach. \n\n"}
{"id": "1801.03765", "contents": "Title: Non-stationary Douglas-Rachford and alternating direction method of\n  multipliers: adaptive stepsizes and convergence Abstract: We revisit the classical Douglas-Rachford (DR) method for finding a zero of\nthe sum of two maximal monotone operators. Since the practical performance of\nthe DR method crucially depends on the stepsizes, we aim at developing an\nadaptive stepsize rule. To that end, we take a closer look at a linear case of\nthe problem and use our findings to develop a stepsize strategy that eliminates\nthe need for stepsize tuning. We analyze a general non-stationary DR scheme and\nprove its convergence for a convergent sequence of stepsizes with summable\nincrements. This, in turn, proves the convergence of the method with the new\nadaptive stepsize rule. We also derive the related non-stationary alternating\ndirection method of multipliers (ADMM) from such a non-stationary DR method. We\nillustrate the efficiency of the proposed methods on several numerical\nexamples. \n\n"}
{"id": "1801.04503", "contents": "Title: Multivariate LSTM-FCNs for Time Series Classification Abstract: Over the past decade, multivariate time series classification has received\ngreat attention. We propose transforming the existing univariate time series\nclassification models, the Long Short Term Memory Fully Convolutional Network\n(LSTM-FCN) and Attention LSTM-FCN (ALSTM-FCN), into a multivariate time series\nclassification model by augmenting the fully convolutional block with a\nsqueeze-and-excitation block to further improve accuracy. Our proposed models\noutperform most state-of-the-art models while requiring minimum preprocessing.\nThe proposed models work efficiently on various complex multivariate time\nseries classification tasks such as activity recognition or action recognition.\nFurthermore, the proposed models are highly efficient at test time and small\nenough to deploy on memory constrained systems. \n\n"}
{"id": "1801.06313", "contents": "Title: BinaryRelax: A Relaxation Approach For Training Deep Neural Networks\n  With Quantized Weights Abstract: We propose BinaryRelax, a simple two-phase algorithm, for training deep\nneural networks with quantized weights. The set constraint that characterizes\nthe quantization of weights is not imposed until the late stage of training,\nand a sequence of \\emph{pseudo} quantized weights is maintained. Specifically,\nwe relax the hard constraint into a continuous regularizer via Moreau envelope,\nwhich turns out to be the squared Euclidean distance to the set of quantized\nweights. The pseudo quantized weights are obtained by linearly interpolating\nbetween the float weights and their quantizations. A continuation strategy is\nadopted to push the weights towards the quantized state by gradually increasing\nthe regularization parameter. In the second phase, exact quantization scheme\nwith a small learning rate is invoked to guarantee fully quantized weights. We\ntest BinaryRelax on the benchmark CIFAR and ImageNet color image datasets to\ndemonstrate the superiority of the relaxed quantization approach and the\nimproved accuracy over the state-of-the-art training methods. Finally, we prove\nthe convergence of BinaryRelax under an approximate orthogonality condition. \n\n"}
{"id": "1801.06879", "contents": "Title: Bayesian Deep Convolutional Encoder-Decoder Networks for Surrogate\n  Modeling and Uncertainty Quantification Abstract: We are interested in the development of surrogate models for uncertainty\nquantification and propagation in problems governed by stochastic PDEs using a\ndeep convolutional encoder-decoder network in a similar fashion to approaches\nconsidered in deep learning for image-to-image regression tasks. Since normal\nneural networks are data intensive and cannot provide predictive uncertainty,\nwe propose a Bayesian approach to convolutional neural nets. A recently\nintroduced variational gradient descent algorithm based on Stein's method is\nscaled to deep convolutional networks to perform approximate Bayesian inference\non millions of uncertain network parameters. This approach achieves state of\nthe art performance in terms of predictive accuracy and uncertainty\nquantification in comparison to other approaches in Bayesian neural networks as\nwell as techniques that include Gaussian processes and ensemble methods even\nwhen the training data size is relatively small. To evaluate the performance of\nthis approach, we consider standard uncertainty quantification benchmark\nproblems including flow in heterogeneous media defined in terms of limited\ndata-driven permeability realizations. The performance of the surrogate model\ndeveloped is very good even though there is no underlying structure shared\nbetween the input (permeability) and output (flow/pressure) fields as is often\nthe case in the image-to-image regression models used in computer vision\nproblems. Studies are performed with an underlying stochastic input\ndimensionality up to $4,225$ where most other uncertainty quantification\nmethods fail. Uncertainty propagation tasks are considered and the predictive\noutput Bayesian statistics are compared to those obtained with Monte Carlo\nestimates. \n\n"}
{"id": "1801.07114", "contents": "Title: Global Deterministic Optimization with Artificial Neural Networks\n  Embedded Abstract: Artificial neural networks (ANNs) are used in various applications for\ndata-driven black-box modeling and subsequent optimization. Herein, we present\nan efficient method for deterministic global optimization of ANN embedded\noptimization problems. The proposed method is based on relaxations of\nalgorithms using McCormick relaxations in a reduced-space [\\textit{SIOPT}, 20\n(2009), pp. 573-601] including the convex and concave envelopes of the\nnonlinear activation function of ANNs. The optimization problem is solved using\nour in-house global deterministic solver MAiNGO. The performance of the\nproposed method is shown in four optimization examples: an illustrative\nfunction, a fermentation process, a compressor plant and a chemical process\noptimization. The results show that computational solution time is favorable\ncompared to the global general-purpose optimization solver BARON. \n\n"}
{"id": "1801.08133", "contents": "Title: Causal Inference in Disease Spread across a Heterogeneous Social System Abstract: Diffusion processes are governed by external triggers and internal dynamics\nin complex systems. Timely and cost-effective control of infectious disease\nspread critically relies on uncovering the underlying diffusion mechanisms,\nwhich is challenging due to invisible causality between events and their\ntime-evolving intensity. We infer causal relationships between infections and\nquantify the reflexivity of a meta-population, the level of feedback on event\noccurrences by its internal dynamics (likelihood of a regional outbreak\ntriggered by previous cases). These are enabled by our new proposed model, the\nLatent Influence Point Process (LIPP) which models disease spread by\nincorporating macro-level internal dynamics of meta-populations based on human\nmobility. We analyse 15-year dengue cases in Queensland, Australia. From our\ncausal inference, outbreaks are more likely driven by statewide global\ndiffusion over time, leading to complex behavior of disease spread. In terms of\nreflexivity, precursory growth and symmetric decline in populous regions is\nattributed to slow but persistent feedback on preceding outbreaks via\ninter-group dynamics, while abrupt growth but sharp decline in peripheral areas\nis led by rapid but inconstant feedback via intra-group dynamics. Our proposed\nmodel reveals probabilistic causal relationships between discrete events based\non intra- and inter-group dynamics and also covers direct and indirect\ndiffusion processes (contact-based and vector-borne disease transmissions). \n\n"}
{"id": "1801.08694", "contents": "Title: PDNet: Semantic Segmentation integrated with a Primal-Dual Network for\n  Document binarization Abstract: Binarization of digital documents is the task of classifying each pixel in an\nimage of the document as belonging to the background (parchment/paper) or\nforeground (text/ink). Historical documents are often subjected to\ndegradations, that make the task challenging. In the current work a deep neural\nnetwork architecture is proposed that combines a fully convolutional network\nwith an unrolled primal-dual network that can be trained end-to-end to achieve\nstate of the art binarization on four out of seven datasets. Document\nbinarization is formulated as an energy minimization problem. A fully\nconvolutional neural network is trained for semantic segmentation of pixels\nthat provides labeling cost associated with each pixel. This cost estimate is\nrefined along the edges to compensate for any over or under estimation of the\nforeground class using a primal-dual approach. We provide necessary overview on\nproximal operator that facilitates theoretical underpinning required to train a\nprimal-dual network using a gradient descent algorithm. Numerical instabilities\nencountered due to the recurrent nature of primal-dual approach are handled. We\nprovide experimental results on document binarization competition dataset along\nwith network changes and hyperparameter tuning required for stability and\nperformance of the network. The network when pre-trained on synthetic dataset\nperforms better as per the competition metrics. \n\n"}
{"id": "1801.08704", "contents": "Title: Event-triggered stabilization of disturbed linear systems over digital\n  channels Abstract: We present an event-triggered control strategy for stabilizing a scalar,\ncontinuous-time, time-invariant, linear system over a digital communication\nchannel having bounded delay, and in the presence of bounded system\ndisturbance. We propose an encoding-decoding scheme, and determine lower bounds\non the packet size and on the information transmission rate which are\nsufficient for stabilization. We show that for small values of the delay, the\ntiming information implicit in the triggering events is enough to stabilize the\nsystem with any positive rate. In contrast, when the delay increases beyond a\ncritical threshold, the timing information alone is not enough to stabilize the\nsystem and the transmission rate begins to increase. Finally, large values of\nthe delay require transmission rates higher than what prescribed by the classic\ndata-rate theorem. The results are numerically validated using a linearized\nmodel of an inverted pendulum. \n\n"}
{"id": "1801.09070", "contents": "Title: Fast cosmic web simulations with generative adversarial networks Abstract: Dark matter in the universe evolves through gravity to form a complex network\nof halos, filaments, sheets and voids, that is known as the cosmic web.\nComputational models of the underlying physical processes, such as classical\nN-body simulations, are extremely resource intensive, as they track the action\nof gravity in an expanding universe using billions of particles as tracers of\nthe cosmic matter distribution. Therefore, upcoming cosmology experiments will\nface a computational bottleneck that may limit the exploitation of their full\nscientific potential. To address this challenge, we demonstrate the application\nof a machine learning technique called Generative Adversarial Networks (GAN) to\nlearn models that can efficiently generate new, physically realistic\nrealizations of the cosmic web. Our training set is a small, representative\nsample of 2D image snapshots from N-body simulations of size 500 and 100 Mpc.\nWe show that the GAN-generated samples are qualitatively and quantitatively\nvery similar to the originals. For the larger boxes of size 500 Mpc, it is very\ndifficult to distinguish them visually. The agreement of the power spectrum\n$P_k$ is 1-2\\% for most of the range, between $k=0.06$ and $k=0.4$. An\nimportant advantage of generating cosmic web realizations with a GAN is the\nconsiderable gains in terms of computation time. Each new sample generated by a\nGAN takes a fraction of a second, compared to the many hours needed by\ntraditional N-body techniques. We anticipate that the use of generative models\nsuch as GANs will therefore play an important role in providing extremely fast\nand precise simulations of cosmic web in the era of large cosmological surveys,\nsuch as Euclid and Large Synoptic Survey Telescope (LSST). \n\n"}
{"id": "1801.10141", "contents": "Title: Random Access Communication for Wireless Control Systems with Energy\n  Harvesting Sensors Abstract: In this paper, we study wireless networked control systems in which the\nsensing devices are powered by energy harvesting. We consider a scenario with\nmultiple plants, where the sensors communicate their measurements to their\nrespective controllers over a shared wireless channel. Due to the shared nature\nof the medium, sensors transmitting simultaneously can lead to packet\ncollisions. In order to deal with this, we propose the use of random access\ncommunication policies and, to this end, we translate the control performance\nrequirements to successful packet reception probabilities. The optimal\nscheduling decision is to transmit with a certain probability, which is\nadaptive to plant, channel and battery conditions. Moreover, we provide a\nstochastic dual method to compute the optimal scheduling solution, which is\ndecoupled across sensors, with only some of the dual variables needed to be\nshared between nodes. Furthermore, we also consider asynchronicity in the\nvalues of the variables across sensor nodes and provide theoretical guarantees\non the stability of the control systems under the proposed random access\nmechanism. Finally, we provide extensive numerical results that corroborate our\nclaims. \n\n"}
{"id": "1802.00047", "contents": "Title: Matrix completion with deterministic pattern - a geometric perspective Abstract: We consider the matrix completion problem with a deterministic pattern of\nobserved entries. In this setting, we aim to answer the question: under what\ncondition there will be (at least locally) unique solution to the matrix\ncompletion problem, i.e., the underlying true matrix is identifiable. We answer\nthe question from a certain point of view and outline a geometric perspective.\nWe give an algebraically verifiable sufficient condition, which we call the\nwell-posedness condition, for the local uniqueness of MRMC solutions. We argue\nthat this condition is necessary for local stability of MRMC solutions, and we\nshow that the condition is generic using the characteristic rank. We also argue\nthat the low-rank approximation approaches are more stable than MRMC and\nfurther propose a sequential statistical testing procedure to determine the\n\"true\" rank from observed entries. Finally, we provide numerical examples aimed\nat verifying validity of the presented theory. \n\n"}
{"id": "1802.01079", "contents": "Title: General maximum principles for optimal control problems of stochastic\n  Volterra integral equations Abstract: Optimal control problems of forward stochastic Volterra integral equations\n(SVIEs) are formulated and studied. When control region is arbitrary subset of\nEuclidean space and control enters into the diffusion, necessary conditions of\nPontryagin's type for optimal controls are established via spike variation. Our\nconclusions naturally cover the analogue of stochastic differential equations\n(SDEs), and our developed methodology drops the reliance on It\\^o formula and\nsecond-order adjoint equations. Some new features, that are concealed in the\nSDEs framework, are revealed in our situation. For example, instead of using\nsecond-order adjoint equations, it is more appropriate to introduce\nsecond-order adjoint processes. Moreover, the conventional way of using one\nsecond-order adjoint equation is inadequate here. In other words, two adjoint\nprocesses, which just merge into the solution of second-order adjoint equation\nin SDEs situation, are actually required and proposed in our setting. \n\n"}
{"id": "1802.01914", "contents": "Title: Polynomial algorithm for $k$-partition minimization of monotone\n  submodular function Abstract: For a fixed $k$, this study considers $k$-partition minimization of\nsubmodular system $(V, f)$ with a finite set $V$ and symmetric submodular\nfunction $f: 2^{V} \\mapsto \\mathbb{R}$. Our algorithm uses the Queyranne's\n(1998) algorithm for 2-partition minimization which arises at each step of the\nrecursive decomposition of subsets of the original $k$-partition minimization.\nWe show that the computational complexity of this minimizer is $O(n^{3(k-1)})$. \n\n"}
{"id": "1802.02511", "contents": "Title: DeepHeart: Semi-Supervised Sequence Learning for Cardiovascular Risk\n  Prediction Abstract: We train and validate a semi-supervised, multi-task LSTM on 57,675\nperson-weeks of data from off-the-shelf wearable heart rate sensors, showing\nhigh accuracy at detecting multiple medical conditions, including diabetes\n(0.8451), high cholesterol (0.7441), high blood pressure (0.8086), and sleep\napnea (0.8298). We compare two semi-supervised train- ing methods,\nsemi-supervised sequence learning and heuristic pretraining, and show they\noutperform hand-engineered biomarkers from the medical literature. We believe\nour work suggests a new approach to patient risk stratification based on\ncardiovascular risk scores derived from popular wearables such as Fitbit, Apple\nWatch, or Android Wear. \n\n"}
{"id": "1802.02538", "contents": "Title: Yes, but Did It Work?: Evaluating Variational Inference Abstract: While it's always possible to compute a variational approximation to a\nposterior distribution, it can be difficult to discover problems with this\napproximation. We propose two diagnostic algorithms to alleviate this problem.\nThe Pareto-smoothed importance sampling (PSIS) diagnostic gives a goodness of\nfit measurement for joint distributions, while simultaneously improving the\nerror in the estimate. The variational simulation-based calibration (VSBC)\nassesses the average performance of point estimates. \n\n"}
{"id": "1802.03184", "contents": "Title: Self-Bounded Prediction Suffix Tree via Approximate String Matching Abstract: Prediction suffix trees (PST) provide an effective tool for sequence\nmodelling and prediction. Current prediction techniques for PSTs rely on exact\nmatching between the suffix of the current sequence and the previously observed\nsequence. We present a provably correct algorithm for learning a PST with\napproximate suffix matching by relaxing the exact matching condition. We then\npresent a self-bounded enhancement of our algorithm where the depth of suffix\ntree grows automatically in response to the model performance on a training\nsequence. Through experiments on synthetic datasets as well as three real-world\ndatasets, we show that the approximate matching PST results in better\npredictive performance than the other variants of PST. \n\n"}
{"id": "1802.04087", "contents": "Title: Deep learning based supervised semantic segmentation of Electron\n  Cryo-Subtomograms Abstract: Cellular Electron Cryo-Tomography (CECT) is a powerful imaging technique for\nthe 3D visualization of cellular structure and organization at submolecular\nresolution. It enables analyzing the native structures of macromolecular\ncomplexes and their spatial organization inside single cells. However, due to\nthe high degree of structural complexity and practical imaging limitations,\nsystematic macromolecular structural recovery inside CECT images remains\nchallenging. Particularly, the recovery of a macromolecule is likely to be\nbiased by its neighbor structures due to the high molecular crowding. To reduce\nthe bias, here we introduce a novel 3D convolutional neural network inspired by\nFully Convolutional Network and Encoder-Decoder Architecture for the supervised\nsegmentation of macromolecules of interest in subtomograms. The tests of our\nmodels on realistically simulated CECT data demonstrate that our new approach\nhas significantly improved segmentation performance compared to our baseline\napproach. Also, we demonstrate that the proposed model has generalization\nability to segment new structures that do not exist in training data. \n\n"}
{"id": "1802.04697", "contents": "Title: Learning to Search with MCTSnets Abstract: Planning problems are among the most important and well-studied problems in\nartificial intelligence. They are most typically solved by tree search\nalgorithms that simulate ahead into the future, evaluate future states, and\nback-up those evaluations to the root of a search tree. Among these algorithms,\nMonte-Carlo tree search (MCTS) is one of the most general, powerful and widely\nused. A typical implementation of MCTS uses cleverly designed rules, optimized\nto the particular characteristics of the domain. These rules control where the\nsimulation traverses, what to evaluate in the states that are reached, and how\nto back-up those evaluations. In this paper we instead learn where, what and\nhow to search. Our architecture, which we call an MCTSnet, incorporates\nsimulation-based search inside a neural network, by expanding, evaluating and\nbacking-up a vector embedding. The parameters of the network are trained\nend-to-end using gradient-based optimisation. When applied to small searches in\nthe well known planning problem Sokoban, the learned search algorithm\nsignificantly outperformed MCTS baselines. \n\n"}
{"id": "1802.04796", "contents": "Title: Stochastic Variance-Reduced Cubic Regularized Newton Method Abstract: We propose a stochastic variance-reduced cubic regularized Newton method for\nnon-convex optimization. At the core of our algorithm is a novel\nsemi-stochastic gradient along with a semi-stochastic Hessian, which are\nspecifically designed for cubic regularization method. We show that our\nalgorithm is guaranteed to converge to an\n$(\\epsilon,\\sqrt{\\epsilon})$-approximately local minimum within\n$\\tilde{O}(n^{4/5}/\\epsilon^{3/2})$ second-order oracle calls, which\noutperforms the state-of-the-art cubic regularization algorithms including\nsubsampled cubic regularization. Our work also sheds light on the application\nof variance reduction technique to high-order non-convex optimization methods.\nThorough experiments on various non-convex optimization problems support our\ntheory. \n\n"}
{"id": "1802.04943", "contents": "Title: $\\mathcal{CIRFE}$: A Distributed Random Fields Estimator Abstract: This paper presents a communication efficient distributed algorithm,\n$\\mathcal{CIRFE}$ of the \\emph{consensus}+\\emph{innovations} type, to estimate\na high-dimensional parameter in a multi-agent network, in which each agent is\ninterested in reconstructing only a few components of the parameter. This\nproblem arises for example when monitoring the high-dimensional distributed\nstate of a large-scale infrastructure with a network of limited capability\nsensors and where each sensor is tasked with estimating some local components\nof the state. At each observation sampling epoch, each agent updates its local\nestimate of the parameter components in its interest set by simultaneously\nprocessing the latest locally sensed information~(\\emph{innovations}) and the\nparameter estimates from agents~(\\emph{consensus}) in its communication\nneighborhood given by a time-varying possibly sparse graph. Under minimal\nconditions on the inter-agent communication network and the sensing models,\nalmost sure convergence of the estimate sequence at each agent to the\ncomponents of the true parameter in its interest set is established.\nFurthermore, the paper establishes the performance of $\\mathcal{CIRFE}$ in\nterms of asymptotic covariance of the estimate sequences and specifically\ncharacterizes the dependencies of the component wise asymptotic covariance in\nterms of the number of agents tasked with estimating it. Finally, simulation\nexperiments demonstrate the efficacy of $\\mathcal{CIRFE}$. \n\n"}
{"id": "1802.06403", "contents": "Title: RadialGAN: Leveraging multiple datasets to improve target-specific\n  predictive models using Generative Adversarial Networks Abstract: Training complex machine learning models for prediction often requires a\nlarge amount of data that is not always readily available. Leveraging these\nexternal datasets from related but different sources is therefore an important\ntask if good predictive models are to be built for deployment in settings where\ndata can be rare. In this paper we propose a novel approach to the problem in\nwhich we use multiple GAN architectures to learn to translate from one dataset\nto another, thereby allowing us to effectively enlarge the target dataset, and\ntherefore learn better predictive models than if we simply used the target\ndataset. We show the utility of such an approach, demonstrating that our method\nimproves the prediction performance on the target domain over using just the\ntarget dataset and also show that our framework outperforms several other\nbenchmarks on a collection of real-world medical datasets. \n\n"}
{"id": "1802.07072", "contents": "Title: Composite Optimization by Nonconvex Majorization-Minimization Abstract: The minimization of a nonconvex composite function can model a variety of\nimaging tasks. A popular class of algorithms for solving such problems are\nmajorization-minimization techniques which iteratively approximate the\ncomposite nonconvex function by a majorizing function that is easy to minimize.\nMost techniques, e.g. gradient descent, utilize convex majorizers in order to\nguarantee that the majorizer is easy to minimize. In our work we consider a\nnatural class of nonconvex majorizers for these functions, and show that these\nmajorizers are still sufficient for a globally convergent optimization scheme.\nNumerical results illustrate that by applying this scheme, one can often obtain\nsuperior local optima compared to previous majorization-minimization methods,\nwhen the nonconvex majorizers are solved to global optimality. Finally, we\nillustrate the behavior of our algorithm for depth super-resolution from raw\ntime-of-flight data. \n\n"}
{"id": "1802.07714", "contents": "Title: Detecting Learning vs Memorization in Deep Neural Networks using Shared\n  Structure Validation Sets Abstract: The roles played by learning and memorization represent an important topic in\ndeep learning research. Recent work on this subject has shown that the\noptimization behavior of DNNs trained on shuffled labels is qualitatively\ndifferent from DNNs trained with real labels. Here, we propose a novel\npermutation approach that can differentiate memorization from learning in deep\nneural networks (DNNs) trained as usual (i.e., using the real labels to guide\nthe learning, rather than shuffled labels). The evaluation of weather the DNN\nhas learned and/or memorized, happens in a separate step where we compare the\npredictive performance of a shallow classifier trained with the features\nlearned by the DNN, against multiple instances of the same classifier, trained\non the same input, but using shuffled labels as outputs. By evaluating these\nshallow classifiers in validation sets that share structure with the training\nset, we are able to tell apart learning from memorization. Application of our\npermutation approach to multi-layer perceptrons and convolutional neural\nnetworks trained on image data corroborated many findings from other groups.\nMost importantly, our illustrations also uncovered interesting dynamic patterns\nabout how DNNs memorize over increasing numbers of training epochs, and support\nthe surprising result that DNNs are still able to learn, rather than only\nmemorize, when trained with pure Gaussian noise as input. \n\n"}
{"id": "1802.07990", "contents": "Title: Joint Antenna Selection and Phase-Only Beamforming Using Mixed-Integer\n  Nonlinear Programming Abstract: In this paper, we consider the problem of joint antenna selection and analog\nbeamformer design in downlink single-group multicast networks. Our objective is\nto reduce the hardware costs by minimizing the number of required phase\nshifters at the transmitter while fulfilling given distortion limits at the\nreceivers. We formulate the problem as an L0 minimization problem and devise a\nnovel branch-and-cut based algorithm to solve the resulting mixed-integer\nnonlinear program to optimality. We also propose a suboptimal heuristic\nalgorithm to solve the above problem approximately with a low computational\ncomplexity. Computational results illustrate that the solutions produced by the\nproposed heuristic algorithm are optimal in most cases. The results also\nindicate that the performance of the optimal methods can be significantly\nimproved by initializing with the result of the suboptimal method. \n\n"}
{"id": "1802.08033", "contents": "Title: Approximating the nearest stable discrete-time system Abstract: In this paper, we consider the problem of stabilizing discrete-time linear\nsystems by computing a nearby stable matrix to an unstable one. To do so, we\nprovide a new characterization for the set of stable matrices. We show that a\nmatrix $A$ is stable if and only if it can be written as $A=S^{-1}UBS$, where\n$S$ is positive definite, $U$ is orthogonal, and $B$ is a positive semidefinite\ncontraction (that is, the singular values of $B$ are less or equal to 1). This\ncharacterization results in an equivalent non-convex optimization problem with\na feasible set on which it is easy to project. We propose a very efficient fast\nprojected gradient method to tackle the problem in variables $(S,U,B)$ and\ngenerate locally optimal solutions. We show the effectiveness of the proposed\nmethod compared to other approaches. \n\n"}
{"id": "1802.08526", "contents": "Title: The Weighted Kendall and High-order Kernels for Permutations Abstract: We propose new positive definite kernels for permutations. First we introduce\na weighted version of the Kendall kernel, which allows to weight unequally the\ncontributions of different item pairs in the permutations depending on their\nranks. Like the Kendall kernel, we show that the weighted version is invariant\nto relabeling of items and can be computed efficiently in $O(n \\ln(n))$\noperations, where $n$ is the number of items in the permutation. Second, we\npropose a supervised approach to learn the weights by jointly optimizing them\nwith the function estimated by a kernel machine. Third, while the Kendall\nkernel considers pairwise comparison between items, we extend it by considering\nhigher-order comparisons among tuples of items and show that the supervised\napproach of learning the weights can be systematically generalized to\nhigher-order permutation kernels. \n\n"}
{"id": "1802.08679", "contents": "Title: Learning Optimal Policies from Observational Data Abstract: Choosing optimal (or at least better) policies is an important problem in\ndomains from medicine to education to finance and many others. One approach to\nthis problem is through controlled experiments/trials - but controlled\nexperiments are expensive. Hence it is important to choose the best policies on\nthe basis of observational data. This presents two difficult challenges: (i)\nmissing counterfactuals, and (ii) selection bias. This paper presents\ntheoretical bounds on estimation errors of counterfactuals from observational\ndata by making connections to domain adaptation theory. It also presents a\nprincipled way of choosing optimal policies using domain adversarial neural\nnetworks. We illustrate the effectiveness of domain adversarial training\ntogether with various features of our algorithm on a semi-synthetic breast\ncancer dataset and a supervised UCI dataset (Statlog). \n\n"}
{"id": "1802.08941", "contents": "Title: Gradient Primal-Dual Algorithm Converges to Second-Order Stationary\n  Solutions for Nonconvex Distributed Optimization Abstract: In this work, we study two first-order primal-dual based algorithms, the\nGradient Primal-Dual Algorithm (GPDA) and the Gradient Alternating Direction\nMethod of Multipliers (GADMM), for solving a class of linearly constrained\nnon-convex optimization problems. We show that with random initialization of\nthe primal and dual variables, both algorithms are able to compute second-order\nstationary solutions (ss2) with probability one. This is the first result\nshowing that primal-dual algorithm is capable of finding ss2 when only using\nfirst-order information, it also extends the existing results for first-order,\nbut primal-only algorithms.\n  An important implication of our result is that it also gives rise to the\nfirst global convergence result to the ss2, for two classes of unconstrained\ndistributed non-convex learning problems over multi-agent networks. \n\n"}
{"id": "1802.09901", "contents": "Title: Learning to recognize touch gestures: recurrent vs. convolutional\n  features and dynamic sampling Abstract: We propose a fully automatic method for learning gestures on big touch\ndevices in a potentially multi-user context. The goal is to learn general\nmodels capable of adapting to different gestures, user styles and hardware\nvariations (e.g. device sizes, sampling frequencies and regularities).\n  Based on deep neural networks, our method features a novel dynamic sampling\nand temporal normalization component, transforming variable length gestures\ninto fixed length representations while preserving finger/surface contact\ntransitions, that is, the topology of the signal. This sequential\nrepresentation is then processed with a convolutional model capable, unlike\nrecurrent networks, of learning hierarchical representations with different\nlevels of abstraction.\n  To demonstrate the interest of the proposed method, we introduce a new touch\ngestures dataset with 6591 gestures performed by 27 people, which is, up to our\nknowledge, the first of its kind: a publicly available multi-touch gesture\ndataset for interaction.\n  We also tested our method on a standard dataset of symbolic touch gesture\nrecognition, the MMG dataset, outperforming the state of the art and reporting\nclose to perfect performance. \n\n"}
{"id": "1802.09932", "contents": "Title: VR-SGD: A Simple Stochastic Variance Reduction Method for Machine\n  Learning Abstract: In this paper, we propose a simple variant of the original SVRG, called\nvariance reduced stochastic gradient descent (VR-SGD). Unlike the choices of\nsnapshot and starting points in SVRG and its proximal variant, Prox-SVRG, the\ntwo vectors of VR-SGD are set to the average and last iterate of the previous\nepoch, respectively. The settings allow us to use much larger learning rates,\nand also make our convergence analysis more challenging. We also design two\ndifferent update rules for smooth and non-smooth objective functions,\nrespectively, which means that VR-SGD can tackle non-smooth and/or non-strongly\nconvex problems directly without any reduction techniques. Moreover, we analyze\nthe convergence properties of VR-SGD for strongly convex problems, which show\nthat VR-SGD attains linear convergence. Different from its counterparts that\nhave no convergence guarantees for non-strongly convex problems, we also\nprovide the convergence guarantees of VR-SGD for this case, and empirically\nverify that VR-SGD with varying learning rates achieves similar performance to\nits momentum accelerated variant that has the optimal convergence rate\n$\\mathcal{O}(1/T^2)$. Finally, we apply VR-SGD to solve various machine\nlearning problems, such as convex and non-convex empirical risk minimization,\nand leading eigenvalue computation. Experimental results show that VR-SGD\nconverges significantly faster than SVRG and Prox-SVRG, and usually outperforms\nstate-of-the-art accelerated methods, e.g., Katyusha. \n\n"}
{"id": "1802.10026", "contents": "Title: Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs Abstract: The loss functions of deep neural networks are complex and their geometric\nproperties are not well understood. We show that the optima of these complex\nloss functions are in fact connected by simple curves over which training and\ntest accuracy are nearly constant. We introduce a training procedure to\ndiscover these high-accuracy pathways between modes. Inspired by this new\ngeometric insight, we also propose a new ensembling method entitled Fast\nGeometric Ensembling (FGE). Using FGE we can train high-performing ensembles in\nthe time required to train a single model. We achieve improved performance\ncompared to the recent state-of-the-art Snapshot Ensembles, on CIFAR-10,\nCIFAR-100, and ImageNet. \n\n"}
{"id": "1802.10418", "contents": "Title: On the Sublinear Convergence of Randomly Perturbed Alternating Gradient\n  Descent to Second Order Stationary Solutions Abstract: The alternating gradient descent (AGD) is a simple but popular algorithm\nwhich has been applied to problems in optimization, machine learning, data\nming, and signal processing, etc. The algorithm updates two blocks of variables\nin an alternating manner, in which a gradient step is taken on one block, while\nkeeping the remaining block fixed. When the objective function is nonconvex, it\nis well-known the AGD converges to the first-order stationary solution with a\nglobal sublinear rate.\n  In this paper, we show that a variant of AGD-type algorithms will not be\ntrapped by \"bad\" stationary solutions such as saddle points and local maximum\npoints. In particular, we consider a smooth unconstrained optimization problem,\nand propose a perturbed AGD (PA-GD) which converges (with high probability) to\nthe set of second-order stationary solutions (SS2) with a global sublinear\nrate. To the best of our knowledge, this is the first alternating type\nalgorithm which takes $\\mathcal{O}(\\text{polylog}(d)/\\epsilon^{7/3})$\niterations to achieve SS2 with high probability [where polylog$(d)$ is\npolynomial of the logarithm of dimension $d$ of the problem]. \n\n"}
{"id": "1802.10576", "contents": "Title: Modeling Activity Tracker Data Using Deep Boltzmann Machines Abstract: Commercial activity trackers are set to become an essential tool in health\nresearch, due to increasing availability in the general population. The\ncorresponding vast amounts of mostly unlabeled data pose a challenge to\nstatistical modeling approaches. To investigate the feasibility of deep\nlearning approaches for unsupervised learning with such data, we examine weekly\nusage patterns of Fitbit activity trackers with deep Boltzmann machines (DBMs).\nThis method is particularly suitable for modeling complex joint distributions\nvia latent variables. We also chose this specific procedure because it is a\ngenerative approach, i.e., artificial samples can be generated to explore the\nlearned structure. We describe how the data can be preprocessed to be\ncompatible with binary DBMs. The results reveal two distinct usage patterns in\nwhich one group frequently uses trackers on Mondays and Tuesdays, whereas the\nother uses trackers during the entire week. This exemplary result shows that\nDBMs are feasible and can be useful for modeling activity tracker data. \n\n"}
{"id": "1803.00190", "contents": "Title: On the Finite Number of Directional Stationary Values of Piecewise\n  Programs Abstract: Extending a fundamental result for (indefinite) quadratic programs, this\npaper shows that certain non-convex piecewise programs have only a finite\nnumber of directional stationary values, and thus, possess only finitely many\nlocally minimum values. We present various special cases of our main results,\nin particular, an application to a least-squares piecewise affine regression\nproblem for which every directional stationary point is locally minimizing. \n\n"}
{"id": "1803.00218", "contents": "Title: Interval-based Prediction Uncertainty Bound Computation in Learning with\n  Missing Values Abstract: The problem of machine learning with missing values is common in many areas.\nA simple approach is to first construct a dataset without missing values simply\nby discarding instances with missing entries or by imputing a fixed value for\neach missing entry, and then train a prediction model with the new dataset. A\ndrawback of this naive approach is that the uncertainty in the missing entries\nis not properly incorporated in the prediction. In order to evaluate prediction\nuncertainty, the multiple imputation (MI) approach has been studied, but the\nperformance of MI is sensitive to the choice of the probabilistic model of the\ntrue values in the missing entries, and the computational cost of MI is high\nbecause multiple models must be trained. In this paper, we propose an\nalternative approach called the Interval-based Prediction Uncertainty Bounding\n(IPUB) method. The IPUB method represents the uncertainties due to missing\nentries as intervals, and efficiently computes the lower and upper bounds of\nthe prediction results when all possible training sets constructed by imputing\narbitrary values in the intervals are considered. The IPUB method can be\napplied to a wide class of convex learning algorithms including penalized\nleast-squares regression, support vector machine (SVM), and logistic\nregression. We demonstrate the advantages of the IPUB method by comparing it\nwith an existing method in numerical experiment with benchmark datasets. \n\n"}
{"id": "1803.00301", "contents": "Title: (Sub)Optimal feedback control of mean field multi-population dynamics Abstract: We study a multiscale approach for the control of agent-based, two-population\nmodels. The control variable acts over one population of leaders, which\ninfluence the population of followers via the coupling generated by their\ninteraction. We cast a quadratic optimal control problem for the large-scale\nmicroscale model, which is approximated via a Boltzmann approach. By sampling\nsolutions of the optimal control problem associated to binary two-population\ndynamics, we generate sub-optimal control laws for the kinetic limit of the\nmulti-population model. We present numerical experiments related to opinion\ndynamics assessing the performance of the proposed control design. \n\n"}
{"id": "1803.00502", "contents": "Title: Understand Functionality and Dimensionality of Vector Embeddings: the\n  Distributional Hypothesis, the Pairwise Inner Product Loss and Its\n  Bias-Variance Trade-off Abstract: Vector embedding is a foundational building block of many deep learning\nmodels, especially in natural language processing. In this paper, we present a\ntheoretical framework for understanding the effect of dimensionality on vector\nembeddings. We observe that the distributional hypothesis, a governing\nprinciple of statistical semantics, requires a natural unitary-invariance for\nvector embeddings. Motivated by the unitary-invariance observation, we propose\nthe Pairwise Inner Product (PIP) loss, a unitary-invariant metric on the\nsimilarity between two embeddings. We demonstrate that the PIP loss captures\nthe difference in functionality between embeddings, and that the PIP loss is\ntightly connect with two basic properties of vector embeddings, namely\nsimilarity and compositionality. By formulating the embedding training process\nas matrix factorization with noise, we reveal a fundamental bias-variance\ntrade-off between the signal spectrum and noise power in the dimensionality\nselection process. This bias-variance trade-off sheds light on many empirical\nobservations which have not been thoroughly explained, for example the\nexistence of an optimal dimensionality. Moreover, we discover two new results\nabout vector embeddings, namely their robustness against over-parametrization\nand their forward stability. The bias-variance trade-off of the PIP loss\nexplicitly answers the fundamental open problem of dimensionality selection for\nvector embeddings. \n\n"}
{"id": "1803.00590", "contents": "Title: Hierarchical Imitation and Reinforcement Learning Abstract: We study how to effectively leverage expert feedback to learn sequential\ndecision-making policies. We focus on problems with sparse rewards and long\ntime horizons, which typically pose significant challenges in reinforcement\nlearning. We propose an algorithmic framework, called hierarchical guidance,\nthat leverages the hierarchical structure of the underlying problem to\nintegrate different modes of expert interaction. Our framework can incorporate\ndifferent combinations of imitation learning (IL) and reinforcement learning\n(RL) at different levels, leading to dramatic reductions in both expert effort\nand cost of exploration. Using long-horizon benchmarks, including Montezuma's\nRevenge, we demonstrate that our approach can learn significantly faster than\nhierarchical RL, and be significantly more label-efficient than standard IL. We\nalso theoretically analyze labeling cost for certain instantiations of our\nframework. \n\n"}
{"id": "1803.00641", "contents": "Title: Re-examination of Bregman functions and new properties of their\n  divergences Abstract: The Bregman divergence (Bregman distance, Bregman measure of distance) is a\ncertain useful substitute for a distance, obtained from a well-chosen function\n(the \"Bregman function\"). Bregman functions and divergences have been\nextensively investigated during the last decades and have found applications in\noptimization, operations research, information theory, nonlinear analysis,\nmachine learning and more. This paper re-examines various aspects related to\nthe theory of Bregman functions and divergences. In particular, it presents\nmany sufficient conditions which allow the construction of Bregman functions in\na general setting and introduces new Bregman functions (such as a negative\niterated log entropy). Moreover, it sheds new light on several known Bregman\nfunctions such as quadratic entropies, the negative Havrda-Charv\\'at-Tsallis\nentropy, and the negative Boltzmann-Gibbs-Shannon entropy, and it shows that\nthe negative Burg entropy, which is not a Bregman function according to the\nclassical theory but nevertheless is known to have \"Bregmanian properties\",\ncan, by our re-examination of the theory, be considered as a Bregman function.\nOur analysis yields several by-products of independent interest such as the\nintroduction of the concept of relative uniform convexity (a certain\ngeneralization of uniform convexity), new properties of uniformly and strongly\nconvex functions, and results in Banach space theory. \n\n"}
{"id": "1803.01329", "contents": "Title: One Mirror Descent Algorithm for Convex Constrained Optimization\n  Problems with non-standard growth properties Abstract: The paper is devoted to a special Mirror Descent algorithm for problems of\nconvex minimization with functional constraints. The objective function may not\nsatisfy the Lipschitz condition, but it must necessarily have the\nLipshitz-continuous gradient. We assume, that the functional constraint can be\nnon-smooth, but satisfying the Lipschitz condition. In particular, such\nfunctionals appear in the well-known Truss Topology Design problem. Also we\nhave applied the technique of restarts in the mentioned version of Mirror\nDescent for strongly convex problems. Some estimations for a rate of\nconvergence are investigated for considered Mirror Descent algorithms. \n\n"}
{"id": "1803.02282", "contents": "Title: The Preeminence of Ethnic Diversity in Scientific Collaboration Abstract: Inspired by the social and economic benefits of diversity, we analyze over 9\nmillion papers and 6 million scientists to study the relationship between\nresearch impact and five classes of diversity: ethnicity, discipline, gender,\naffiliation, and academic age. Using randomized baseline models, we establish\nthe presence of homophily in ethnicity, gender and affiliation. We then study\nthe effect of diversity on scientific impact, as reflected in citations.\nRemarkably, of the classes considered, ethnic diversity had the strongest\ncorrelation with scientific impact. To further isolate the effects of ethnic\ndiversity, we used randomized baseline models and again found a clear link\nbetween diversity and impact. To further support these findings, we use\ncoarsened exact matching to compare the scientific impact of ethnically diverse\npapers and scientists with closely-matched control groups. Here, we find that\nethnic diversity resulted in an impact gain of 10.63% for papers, and 47.67%\nfor scientists. \n\n"}
{"id": "1803.02525", "contents": "Title: Fast Robust Methods for Singular State-Space Models Abstract: State-space models are used in a wide range of time series analysis\nformulations. Kalman filtering and smoothing are work-horse algorithms in these\nsettings. While classic algorithms assume Gaussian errors to simplify\nestimation, recent advances use a broader range of optimization formulations to\nallow outlier-robust estimation, as well as constraints to capture prior\ninformation.\n  Here we develop methods on state-space models where either innovations or\nerror covariances may be singular. These models frequently arise in navigation\n(e.g. for `colored noise' models or deterministic integrals) and are ubiquitous\nin auto-correlated time series models such as ARMA. We reformulate all\nstate-space models (singular as well as nonsinguar) as constrained convex\noptimization problems, and develop an efficient algorithm for this\nreformulation. The convergence rate is {\\it locally linear}, with constants\nthat do not depend on the conditioning of the problem.\n  Numerical comparisons show that the new approach outperforms competing\napproaches for {\\it nonsingular} models, including state of the art interior\npoint (IP) methods. IP methods converge at superlinear rates; we expect them to\ndominate. However, the steep rate of the proposed approach (independent of\nproblem conditioning) combined with cheap iterations wins against IP in a\nrun-time comparison. We therefore suggest that the proposed approach be the\n{\\it default choice} for estimating state space models outside of the Gaussian\ncontext, regardless of whether the error covariances are singular or not. \n\n"}
{"id": "1803.02682", "contents": "Title: A Suboptimality Approach to Distributed Linear Quadratic Optimal Control Abstract: This paper is concerned with the distributed linear quadratic optimal control\nproblem. In particular, we consider a suboptimal version of the distributed\noptimal control problem for undirected multi-agent networks. Given a\nmulti-agent system with identical agent dynamics and an associated global\nquadratic cost functional, our objective is to design suboptimal distributed\ncontrol laws that guarantee the controlled network to reach consensus and the\nassociated cost to be smaller than an a priori given upper bound. We first\nanalyze the suboptimality for a given linear system and then apply the results\nto linear multiagent systems. Two design methods are then provided to compute\nsuch suboptimal distributed controllers, involving the solution of a single\nRiccati inequality of dimension equal to the dimension of the agent dynamics,\nand the smallest nonzero and the largest eigenvalue of the graph Laplacian.\nFurthermore, we relax the requirement of exact knowledge of the smallest\nnonzero and largest eigenvalue of the graph Laplacian by using only lower and\nupper bounds on these eigenvalues. Finally, a simulation example is provided to\nillustrate our design method. \n\n"}
{"id": "1803.03379", "contents": "Title: A Phase Model Approach for Thermostatically Controlled Load Demand\n  Response Abstract: A significant portion of electricity consumed worldwide is used to power\nthermostatically controlled loads (TCLs) such as air conditioners,\nrefrigerators, and water heaters. Because the short-term timing of operation of\nsuch systems is inconsequential as long as their long-run average power\nconsumption is maintained, they are increasingly used in demand response (DR)\nprograms to balance supply and demand on the power grid. Here, we present an\n\\textit{ab initio} phase model for general TCLs, and use the concept to develop\na continuous oscillator model of a TCL and compute its phase response to\nchanges in temperature and applied power. This yields a simple control system\nmodel that can be used to evaluate control policies for modulating the power\nconsumption of aggregated loads with parameter heterogeneity and stochastic\ndrift. We demonstrate this concept by comparing simulations of ensembles of\nheterogeneous loads using the continuous state model and an established hybrid\nstate model. The developed phase model approach is a novel means of evaluating\nDR provision using TCLs, and is instrumental in estimating the capacity of\nancillary services or DR on different time scales. We further propose a novel\nphase response based open-loop control policy that effectively modulates the\naggregate power of a heterogeneous TCL population while maintaining load\ndiversity and minimizing power overshoots. This is demonstrated by low-error\ntracking of a regulation signal by filtering it into frequency bands and using\nTCL sub-ensembles with duty cycles in corresponding ranges. Control policies\nthat can maintain a uniform distribution of power consumption by aggregated\nheterogeneous loads will enable distribution system management (DSM) approaches\nthat maintain stability as well as power quality, and further allow more\nintegration of renewable energy sources. \n\n"}
{"id": "1803.04298", "contents": "Title: Error estimates for the approximation of multibang control problems Abstract: This work is concerned with optimal control problems where the objective\nfunctional consists of a tracking-type functional and an additional \"multibang\"\nregularization functional that promotes optimal control taking values from a\ngiven discrete set pointwise almost everywhere. Under a regularity condition on\nthe set where these discrete values are attained, error estimates for the\nMoreau--Yosida approximation (which allows its solution by a semismooth Newton\nmethod) and the discretization of the problem are derived. Numerical results\nsupport the theoretical findings. \n\n"}
{"id": "1803.04674", "contents": "Title: Hierarchical Reinforcement Learning: Approximating Optimal Discounted\n  TSP Using Local Policies Abstract: In this work, we provide theoretical guarantees for reward decomposition in\ndeterministic MDPs. Reward decomposition is a special case of Hierarchical\nReinforcement Learning, that allows one to learn many policies in parallel and\ncombine them into a composite solution. Our approach builds on mapping this\nproblem into a Reward Discounted Traveling Salesman Problem, and then deriving\napproximate solutions for it. In particular, we focus on approximate solutions\nthat are local, i.e., solutions that only observe information about the current\nstate. Local policies are easy to implement and do not require substantial\ncomputational resources as they do not perform planning. While local\ndeterministic policies, like Nearest Neighbor, are being used in practice for\nhierarchical reinforcement learning, we propose three stochastic policies that\nguarantee better performance than any deterministic policy. \n\n"}
{"id": "1803.04912", "contents": "Title: Data-Driven Distributionally Robust Optimal Power Flow for Distribution\n  Systems Abstract: Increasing penetration of distributed energy resources complicate operations\nof electric power distribution systems by amplifying volatility of nodal power\ninjections. On the other hand, these resources can provide additional control\nmeans to the distribution system operator (DSO). This paper takes the DSO\nperspective and leverages a data-driven distributionally robust decision-making\nframework to overcome the uncertainty of these injections and its impact on the\ndistribution system operations. We develop an AC OPF formulation for radial\ndistribution systems based on the LinDistFlow AC power flow approximation and\nexploit distributionally robust optimization to immunize the optimized\ndecisions against uncertainty in the probabilistic models of forecast errors\nobtained from the available observations. The model is reformulated to be\ncomputationally tractable and tested on multiple IEEE distribution test\nsystems. We also release the code supplement that implements the proposed model\nin Julia and can be used to reproduce our numerical results. \n\n"}
{"id": "1803.05256", "contents": "Title: Newton-type Alternating Minimization Algorithm for Convex Optimization Abstract: We propose NAMA (Newton-type Alternating Minimization Algorithm) for solving\nstructured nonsmooth convex optimization problems where the sum of two\nfunctions is to be minimized, one being strongly convex and the other composed\nwith a linear mapping. The proposed algorithm is a line-search method over a\ncontinuous, real-valued, exact penalty function for the corresponding dual\nproblem, which is computed by evaluating the augmented Lagrangian at the primal\npoints obtained by alternating minimizations. As a consequence, NAMA relies on\nexactly the same computations as the classical alternating minimization\nalgorithm (AMA), also known as the dual proximal gradient method. Under\nstandard assumptions the proposed algorithm possesses strong convergence\nproperties, while under mild additional assumptions the asymptotic convergence\nis superlinear, provided that the search directions are chosen according to\nquasi-Newton formulas. Due to its simplicity, the proposed method is well\nsuited for embedded applications and large-scale problems. Experiments show\nthat using limited-memory directions in NAMA greatly improves the convergence\nspeed over AMA and its accelerated variant. \n\n"}
{"id": "1803.05671", "contents": "Title: Spectral radii of asymptotic mappings and the convergence speed of the\n  standard fixed point algorithm Abstract: Important problems in wireless networks can often be solved by computing\nfixed points of standard or contractive interference mappings, and the\nconventional fixed point algorithm is widely used for this purpose. Knowing\nthat the mapping used in the algorithm is not only standard but also\ncontractive (or only contractive) is valuable information because we obtain a\nguarantee of geometric convergence rate, and the rate is related to a property\nof the mapping called modulus of contraction. To date, contractive mappings and\ntheir moduli of contraction have been identified with case-by-case approaches\nthat can be difficult to generalize. To address this limitation of existing\napproaches, we show in this study that the spectral radii of asymptotic\nmappings can be used to identify an important subclass of contractive mappings\nand also to estimate their moduli of contraction. In addition, if the fixed\npoint algorithm is applied to compute fixed points of positive concave\nmappings, we show that the spectral radii of asymptotic mappings provide us\nwith simple lower bounds for the estimation error of the iterates. An immediate\napplication of this result proves that a known algorithm for load estimation in\nwireless networks becomes slower with increasing traffic. \n\n"}
{"id": "1803.05830", "contents": "Title: A policy iteration algorithm for nonzero-sum stochastic impulse games Abstract: This work presents a novel policy iteration algorithm to tackle nonzero-sum\nstochastic impulse games arising naturally in many applications. Despite the\nobvious impact of solving such problems, there are no suitable numerical\nmethods available, to the best of our knowledge. Our method relies on the\nrecently introduced characterization of the value functions and Nash\nequilibrium via a system of quasi-variational inequalities. While our algorithm\nis heuristic and we do not provide a convergence analysis, numerical tests show\nthat it performs convincingly in a wide range of situations, including the only\nanalytically solvable example available in the literature at the time of\nwriting. \n\n"}
{"id": "1803.06521", "contents": "Title: Beyond the Low-Degree Algorithm: Mixtures of Subcubes and Their\n  Applications Abstract: We introduce the problem of learning mixtures of $k$ subcubes over\n$\\{0,1\\}^n$, which contains many classic learning theory problems as a special\ncase (and is itself a special case of others). We give a surprising $n^{O(\\log\nk)}$-time learning algorithm based on higher-order multilinear moments. It is\nnot possible to learn the parameters because the same distribution can be\nrepresented by quite different models. Instead, we develop a framework for\nreasoning about how multilinear moments can pinpoint essential features of the\nmixture, like the number of components.\n  We also give applications of our algorithm to learning decision trees with\nstochastic transitions (which also capture interesting scenarios where the\ntransitions are deterministic but there are latent variables). Using our\nalgorithm for learning mixtures of subcubes, we can approximate the Bayes\noptimal classifier within additive error $\\epsilon$ on $k$-leaf decision trees\nwith at most $s$ stochastic transitions on any root-to-leaf path in $n^{O(s +\n\\log k)}\\cdot\\text{poly}(1/\\epsilon)$ time. In this stochastic setting, the\nclassic Occam algorithms for learning decision trees with zero stochastic\ntransitions break down, while the low-degree algorithm of Linial et al.\ninherently has a quasipolynomial dependence on $1/\\epsilon$.\n  In contrast, as we will show, mixtures of $k$ subcubes are uniquely\ndetermined by their degree $2 \\log k$ moments and hence provide a useful\nabstraction for simultaneously achieving the polynomial dependence on\n$1/\\epsilon$ of the classic Occam algorithms for decision trees and the\nflexibility of the low-degree algorithm in being able to accommodate stochastic\ntransitions. Using our multilinear moment techniques, we also give the first\nimproved upper and lower bounds since the work of Feldman et al. for the\nrelated but harder problem of learning mixtures of binary product\ndistributions. \n\n"}
{"id": "1803.06804", "contents": "Title: Stochastic maximum principle, dynamic programming principle, and their\n  relationship for fully coupled forward-backward stochastic control systems Abstract: Within the framework of viscosity solution, we study the relationship between\nthe maximum principle (MP) in [9] and the dynamic programming principle (DPP)\nin [10] for a fully coupled forward-backward stochastic controlled system\n(FBSCS) with a nonconvex control domain. For a fully coupled FBSCS, both the\ncorresponding MP and the corresponding Hamilton-Jacobi-Bellman (HJB) equation\ncombine an algebra equation respectively. So this relationship becomes more\ncomplicated and almost no work involves this issue. With the help of a new\ndecoupling technique, we obtain the desirable estimates for the fully coupled\nforward-backward variational equations and establish the relationship.\nFurthermore, for the smooth case, we discover the connection between the\nderivatives of the solution to the algebra equation and some terms in the first\nand second-order adjoint equations. Finally, we study the local case under the\nmonotonicity conditions as in [14,27] and obtain the relationship between the\nMP in [27] and the DPP in [14]. \n\n"}
{"id": "1803.07033", "contents": "Title: Natural gradient via optimal transport Abstract: We study a natural Wasserstein gradient flow on manifolds of probability\ndistributions with discrete sample spaces. We derive the Riemannian structure\nfor the probability simplex from the dynamical formulation of the Wasserstein\ndistance on a weighted graph. We pull back the geometric structure to the\nparameter space of any given probability model, which allows us to define a\nnatural gradient flow there. In contrast to the natural Fisher-Rao gradient,\nthe natural Wasserstein gradient incorporates a ground metric on sample space.\nWe illustrate the analysis of elementary exponential family examples and\ndemonstrate an application of the Wasserstein natural gradient to maximum\nlikelihood estimation. \n\n"}
{"id": "1803.07726", "contents": "Title: Gradient Descent with Random Initialization: Fast Global Convergence for\n  Nonconvex Phase Retrieval Abstract: This paper considers the problem of solving systems of quadratic equations,\nnamely, recovering an object of interest\n$\\mathbf{x}^{\\natural}\\in\\mathbb{R}^{n}$ from $m$ quadratic equations/samples\n$y_{i}=(\\mathbf{a}_{i}^{\\top}\\mathbf{x}^{\\natural})^{2}$, $1\\leq i\\leq m$. This\nproblem, also dubbed as phase retrieval, spans multiple domains including\nphysical sciences and machine learning.\n  We investigate the efficiency of gradient descent (or Wirtinger flow)\ndesigned for the nonconvex least squares problem. We prove that under Gaussian\ndesigns, gradient descent --- when randomly initialized --- yields an\n$\\epsilon$-accurate solution in $O\\big(\\log n+\\log(1/\\epsilon)\\big)$ iterations\ngiven nearly minimal samples, thus achieving near-optimal computational and\nsample complexities at once. This provides the first global convergence\nguarantee concerning vanilla gradient descent for phase retrieval, without the\nneed of (i) carefully-designed initialization, (ii) sample splitting, or (iii)\nsophisticated saddle-point escaping schemes. All of these are achieved by\nexploiting the statistical models in analyzing optimization algorithms, via a\nleave-one-out approach that enables the decoupling of certain statistical\ndependency between the gradient descent iterates and the data. \n\n"}
{"id": "1803.08021", "contents": "Title: Error Estimation for Randomized Least-Squares Algorithms via the\n  Bootstrap Abstract: Over the course of the past decade, a variety of randomized algorithms have\nbeen proposed for computing approximate least-squares (LS) solutions in\nlarge-scale settings. A longstanding practical issue is that, for any given\ninput, the user rarely knows the actual error of an approximate solution\n(relative to the exact solution). Likewise, it is difficult for the user to\nknow precisely how much computation is needed to achieve the desired error\ntolerance. Consequently, the user often appeals to worst-case error bounds that\ntend to offer only qualitative guidance. As a more practical alternative, we\npropose a bootstrap method to compute a posteriori error estimates for\nrandomized LS algorithms. These estimates permit the user to numerically assess\nthe error of a given solution, and to predict how much work is needed to\nimprove a \"preliminary\" solution. In addition, we provide theoretical\nconsistency results for the method, which are the first such results in this\ncontext (to the best of our knowledge). From a practical standpoint, the method\nalso has considerable flexibility, insofar as it can be applied to several\npopular sketching algorithms, as well as a variety of error metrics. Moreover,\nthe extra step of error estimation does not add much cost to an underlying\nsketching algorithm. Finally, we demonstrate the effectiveness of the method\nwith empirical results. \n\n"}
{"id": "1803.08823", "contents": "Title: A high-bias, low-variance introduction to Machine Learning for\n  physicists Abstract: Machine Learning (ML) is one of the most exciting and dynamic areas of modern\nresearch and application. The purpose of this review is to provide an\nintroduction to the core concepts and tools of machine learning in a manner\neasily understood and intuitive to physicists. The review begins by covering\nfundamental concepts in ML and modern statistics such as the bias-variance\ntradeoff, overfitting, regularization, generalization, and gradient descent\nbefore moving on to more advanced topics in both supervised and unsupervised\nlearning. Topics covered in the review include ensemble models, deep learning\nand neural networks, clustering and data visualization, energy-based models\n(including MaxEnt models and Restricted Boltzmann Machines), and variational\nmethods. Throughout, we emphasize the many natural connections between ML and\nstatistical physics. A notable aspect of the review is the use of Python\nJupyter notebooks to introduce modern ML/statistical packages to readers using\nphysics-inspired datasets (the Ising Model and Monte-Carlo simulations of\nsupersymmetric decays of proton-proton collisions). We conclude with an\nextended outlook discussing possible uses of machine learning for furthering\nour understanding of the physical world as well as open problems in ML where\nphysicists may be able to contribute. (Notebooks are available at\nhttps://physics.bu.edu/~pankajm/MLnotebooks.html ) \n\n"}
{"id": "1803.08832", "contents": "Title: Golden Ratio Algorithms for Variational Inequalities Abstract: The paper presents a fully explicit algorithm for monotone variational\ninequalities. The method uses variable stepsizes that are computed using two\nprevious iterates as an approximation of the local Lipschitz constant without\nrunning a linesearch. Thus, each iteration of the method requires only one\nevaluation of a monotone operator $F$ and a proximal mapping $g$. The operator\n$F$ need not be Lipschitz-continuous, which also makes the algorithm\ninteresting in the area of composite minimization where one cannot use the\ndescent lemma. The method exhibits an ergodic $O(1/k)$ convergence rate and\n$R$-linear rate, if $F, g$ satisfy the error bound condition. We discuss\npossible applications of the method to fixed point problems. We discuss\npossible applications of the method to fixed point problems as well as its\ndifferent generalizations. \n\n"}
{"id": "1803.09266", "contents": "Title: New SOCP relaxation and branching rule for bipartite bilinear programs Abstract: A bipartite bilinear program (BBP) is a quadratically constrained quadratic\noptimization problem where the variables can be partitioned into two sets such\nthat fixing the variables in any one of the sets results in a linear program.\nWe propose a new second order cone representable (SOCP) relaxation for BBP,\nwhich we show is stronger than the standard SDP relaxation intersected with the\nboolean quadratic polytope. We then propose a new branching rule inspired by\nthe construction of the SOCP relaxation. We describe a new application of BBP\ncalled as the finite element model updating problem, which is a fundamental\nproblem in structural engineering. Our computational experiments on this\nproblem class show that the new branching rule together with an polyhedral\nouter approximation of the SOCP relaxation outperforms a state-of-the-art\ncommercial global solver in obtaining dual bounds. \n\n"}
{"id": "1803.09539", "contents": "Title: On Matching Pursuit and Coordinate Descent Abstract: Two popular examples of first-order optimization methods over linear spaces\nare coordinate descent and matching pursuit algorithms, with their randomized\nvariants. While the former targets the optimization by moving along\ncoordinates, the latter considers a generalized notion of directions.\nExploiting the connection between the two algorithms, we present a unified\nanalysis of both, providing affine invariant sublinear $\\mathcal{O}(1/t)$ rates\non smooth objectives and linear convergence on strongly convex objectives. As a\nbyproduct of our affine invariant analysis of matching pursuit, our rates for\nsteepest coordinate descent are the tightest known. Furthermore, we show the\nfirst accelerated convergence rate $\\mathcal{O}(1/t^2)$ for matching pursuit\nand steepest coordinate descent on convex objectives. \n\n"}
{"id": "1803.09788", "contents": "Title: Highly entangled tensors Abstract: A geometric measure for the entanglement of a unit length tensor $T \\in\n(\\mathbb{C}^n)^{\\otimes k}$ is given by $- 2 \\log_2 ||T||_\\sigma$, where\n$||.||_\\sigma$ denotes the spectral norm. A simple induction gives an upper\nbound of $(k-1) \\log_2(n)$ for the entanglement. We show the existence of\ntensors with entanglement larger than $k \\log_2(n) - \\log_2(k) - o(\\log_2(k))$.\nFriedland and Kemp have similar results in the case of symmetric tensors. Our\ntechniques give improvements in this case. \n\n"}
{"id": "1803.09941", "contents": "Title: Iteration complexity of first-order augmented Lagrangian methods for\n  convex conic programming Abstract: In this paper we consider a class of convex conic programming. In particular,\nwe first propose an inexact augmented Lagrangian (I-AL) method that resembles\nthe classical I-AL method for solving this problem, in which the augmented\nLagrangian subproblems are solved approximately by a variant of Nesterov's\noptimal first-order method. We show that the total number of first-order\niterations of the proposed I-AL method for finding an $\\epsilon$-KKT solution\nis at most $\\mathcal{O}(\\epsilon^{-7/4})$. We then propose an adaptively\nregularized I-AL method and show that it achieves a first-order iteration\ncomplexity $\\mathcal{O}(\\epsilon^{-1}\\log\\epsilon^{-1})$, which significantly\nimproves existing complexity bounds achieved by first-order I-AL methods for\nfinding an $\\epsilon$-KKT solution. Our complexity analysis of the I-AL methods\nis based on a sharp analysis of inexact proximal point algorithm (PPA) and the\nconnection between the I-AL methods and inexact PPA. It is vastly different\nfrom existing complexity analyses of the first-order I-AL methods in the\nliterature, which typically regard the I-AL methods as an inexact dual gradient\nmethod. \n\n"}
{"id": "1804.00130", "contents": "Title: Locally Convex Sparse Learning over Networks Abstract: We consider a distributed learning setup where a sparse signal is estimated\nover a network. Our main interest is to save communication resource for\ninformation exchange over the network and reduce processing time. Each node of\nthe network uses a convex optimization based algorithm that provides a locally\noptimum solution for that node. The nodes exchange their signal estimates over\nthe network in order to refine their local estimates. At a node, the\noptimization algorithm is based on an $\\ell_1$-norm minimization with\nappropriate modifications to promote sparsity as well as to include influence\nof estimates from neighboring nodes. Our expectation is that local estimates in\neach node improve fast and converge, resulting in a limited demand for\ncommunication of estimates between nodes and reducing the processing time. We\nprovide restricted-isometry-property (RIP)-based theoretical analysis on\nestimation quality. In the scenario of clean observation, it is shown that the\nlocal estimates converge to the exact sparse signal under certain technical\nconditions. Simulation results show that the proposed algorithms show\ncompetitive performance compared to a globally optimum distributed LASSO\nalgorithm in the sense of convergence speed and estimation error. \n\n"}
{"id": "1804.01465", "contents": "Title: Predicting interactions between individuals with structural and\n  dynamical information Abstract: Capturing both the structural and temporal aspects of interactions is crucial\nfor many real world datasets like contact between individuals. Using the link\nstream formalism to capture the dynamic of the systems, we tackle the issue of\nactivity prediction in link streams, that is to say predicting the number of\nlinks occurring during a given period of time and we present a protocol that\ntakes advantage of the temporal and structural information contained in the\nlink stream. Using a supervised learning method, we are able to model the\ndynamic of our system to improve the prediction. We investigate the behavior of\nour algorithm and crucial elements affecting the prediction. By introducing\ndifferent categories of pair of nodes, we are able to improve the quality as\nwell as increase the diversity of our prediction. \n\n"}
{"id": "1804.02008", "contents": "Title: Deterministic guarantees for Burer-Monteiro factorizations of smooth\n  semidefinite programs Abstract: We consider semidefinite programs (SDPs) with equality constraints. The\nvariable to be optimized is a positive semidefinite matrix $X$ of size $n$.\nFollowing the Burer--Monteiro approach, we optimize a factor $Y$ of size $n\n\\times p$ instead, such that $X = YY^T$. This ensures positive semidefiniteness\nat no cost and can reduce the dimension of the problem if $p$ is small, but\nresults in a non-convex optimization problem with a quadratic cost function and\nquadratic equality constraints in $Y$. In this paper, we show that if the set\nof constraints on $Y$ regularly defines a smooth manifold, then, despite\nnon-convexity, first- and second-order necessary optimality conditions are also\nsufficient, provided $p$ is large enough. For smaller values of $p$, we show a\nsimilar result holds for almost all (linear) cost functions. Under those\nconditions, a global optimum $Y$ maps to a global optimum $X = YY^T$ of the\nSDP. We deduce old and new consequences for SDP relaxations of the generalized\neigenvector problem, the trust-region subproblem and quadratic optimization\nover several spheres, as well as for the Max-Cut and Orthogonal-Cut SDPs which\nare common relaxations in stochastic block modeling and synchronization of\nrotations. \n\n"}
{"id": "1804.02197", "contents": "Title: Singular value decay of operator-valued differential Lyapunov and\n  Riccati equations Abstract: We consider operator-valued differential Lyapunov and Riccati equations,\nwhere the operators $B$ and $C$ may be relatively unbounded with respect to $A$\n(in the standard notation). In this setting, we prove that the singular values\nof the solutions decay fast under certain conditions. In fact, the decay is\nexponential in the negative square root if $A$ generates an analytic semigroup\nand the range of $C$ has finite dimension. This extends previous similar\nresults for algebraic equations to the differential case. When the initial\ncondition is zero, we also show that the singular values converge to zero as\ntime goes to zero, with a certain rate that depends on the degree of\nunboundedness of $C$. A fast decay of the singular values corresponds to a low\nnumerical rank, which is a critical feature in large-scale applications. The\nresults reported here provide a theoretical foundation for the observation\nthat, in practice, a low-rank factorization usually exists. \n\n"}
{"id": "1804.03728", "contents": "Title: Tensor Robust Principal Component Analysis with A New Tensor Nuclear\n  Norm Abstract: In this paper, we consider the Tensor Robust Principal Component Analysis\n(TRPCA) problem, which aims to exactly recover the low-rank and sparse\ncomponents from their sum. Our model is based on the recently proposed\ntensor-tensor product (or t-product). Induced by the t-product, we first\nrigorously deduce the tensor spectral norm, tensor nuclear norm, and tensor\naverage rank, and show that the tensor nuclear norm is the convex envelope of\nthe tensor average rank within the unit ball of the tensor spectral norm. These\ndefinitions, their relationships and properties are consistent with matrix\ncases. Equipped with the new tensor nuclear norm, we then solve the TRPCA\nproblem by solving a convex program and provide the theoretical guarantee for\nthe exact recovery. Our TRPCA model and recovery guarantee include matrix RPCA\nas a special case. Numerical experiments verify our results, and the\napplications to image recovery and background modeling problems demonstrate the\neffectiveness of our method. \n\n"}
{"id": "1804.03811", "contents": "Title: Estimating Time-Varying Graphical Models Abstract: In this paper, we study time-varying graphical models based on data measured\nover a temporal grid. Such models are motivated by the needs to describe and\nunderstand evolving interacting relationships among a set of random variables\nin many real applications, for instance the study of how stocks interact with\neach other and how such interactions change over time.\n  We propose a new model, LOcal Group Graphical Lasso Estimation (loggle),\nunder the assumption that the graph topology changes gradually over time.\nSpecifically, loggle uses a novel local group-lasso type penalty to efficiently\nincorporate information from neighboring time points and to impose structural\nsmoothness of the graphs. We implement an ADMM based algorithm to fit the\nloggle model. This algorithm utilizes blockwise fast computation and\npseudo-likelihood approximation to improve computational efficiency. An R\npackage loggle has also been developed.\n  We evaluate the performance of loggle by simulation experiments. We also\napply loggle to S&P 500 stock price data and demonstrate that loggle is able to\nreveal the interacting relationships among stocks and among industrial sectors\nin a time period that covers the recent global financial crisis. \n\n"}
{"id": "1804.03842", "contents": "Title: OLCPM: An Online Framework for Detecting Overlapping Communities in\n  Dynamic Social Networks Abstract: Community structure is one of the most prominent features of complex\nnetworks. Community structure detection is of great importance to provide\ninsights into the network structure and functionalities. Most proposals focus\non static networks. However, finding communities in a dynamic network is even\nmore challenging, especially when communities overlap with each other. In this\narticle , we present an online algorithm, called OLCPM, based on clique\npercolation and label propagation methods. OLCPM can detect overlapping\ncommunities and works on temporal networks with a fine granularity. By locally\nupdating the community structure, OLCPM delivers significant improvement in\nrunning time compared with previous clique percolation techniques. The\nexperimental results on both synthetic and real-world networks illustrate the\neffectiveness of the method. \n\n"}
{"id": "1804.04262", "contents": "Title: The Voice Conversion Challenge 2018: Promoting Development of Parallel\n  and Nonparallel Methods Abstract: We present the Voice Conversion Challenge 2018, designed as a follow up to\nthe 2016 edition with the aim of providing a common framework for evaluating\nand comparing different state-of-the-art voice conversion (VC) systems. The\nobjective of the challenge was to perform speaker conversion (i.e. transform\nthe vocal identity) of a source speaker to a target speaker while maintaining\nlinguistic information. As an update to the previous challenge, we considered\nboth parallel and non-parallel data to form the Hub and Spoke tasks,\nrespectively. A total of 23 teams from around the world submitted their\nsystems, 11 of them additionally participated in the optional Spoke task. A\nlarge-scale crowdsourced perceptual evaluation was then carried out to rate the\nsubmitted converted speech in terms of naturalness and similarity to the target\nspeaker identity. In this paper, we present a brief summary of the\nstate-of-the-art techniques for VC, followed by a detailed explanation of the\nchallenge tasks and the results that were obtained. \n\n"}
{"id": "1804.04449", "contents": "Title: Herding Positive, Complex Networks Abstract: The problem of controlling complex networks is of interest to disciplines\nranging from biology to swarm robotics. However, controllability can be too\nstrict a condition, failing to capture a range of desirable behaviors.\nHerdability, which describes the ability to drive a system to a specific set in\nthe state space, was recently introduced as an alternative network control\nnotion. This paper considers the application of herdability to the study of\ncomplex networks under the assumption that a positive system evolves on the\nnetwork. The herdability of a class of networked systems is investigated and\ntwo problems related to ensuring system herdability are explored. The first is\nthe input addition problem, which investigates which nodes in a network should\nreceive inputs to ensure that the system is herdable. The second is a related\nproblem of selecting the best single node from which to herd the network, in\nthe case that a single node is guaranteed to make the system is herdable. In\norder to select the best herding node, a novel control energy based herdability\ncentrality measure is introduced. \n\n"}
{"id": "1804.05368", "contents": "Title: A Variable Sample-size Stochastic Quasi-Newton Method for Smooth and\n  Nonsmooth Stochastic Convex Optimization Abstract: Classical theory for quasi-Newton schemes has focused on smooth deterministic\nunconstrained optimization while recent forays into stochastic convex\noptimization have largely resided in smooth, unconstrained, and strongly convex\nregimes. Naturally, there is a compelling need to address nonsmoothness, the\nlack of strong convexity, and the presence of constraints. Accordingly, this\npaper presents a quasi-Newton framework that can process merely convex and\npossibly nonsmooth (but smoothable) stochastic convex problems. We propose a\nframework that combines iterative smoothing and regularization with a\nvariance-reduced scheme reliant on using increasing sample-sizes of gradients.\nWe make the following contributions. (i) We develop a regularized and smoothed\nvariable sample-size BFGS update (rsL-BFGS) that generates a sequence of\nHessian approximations and can accommodate nonsmooth convex objectives by\nutilizing iterative regularization and smoothing. (ii) In strongly convex\nregimes with state-dependent noise, the proposed variable sample-size\nstochastic quasi-Newton scheme admits a non-asymptotic linear rate of\nconvergence while the oracle complexity of computing an $\\epsilon$-solution is\n$\\mathcal{O}(\\kappa^{m+1}/\\epsilon)$ where $\\kappa$ is the condition number and\n$m\\geq 1$. In nonsmooth (but smoothable) regimes, using Moreau smoothing\nretains the linear convergence rate. To contend with the possible\nunavailability of Lipschitzian and strong convexity parameters, we also provide\nsublinear rates; (iii) In merely convex but smooth settings, the regularized\nVS-SQN scheme rVS-SQN displays a rate of $\\mathcal{O}(1/k^{(1-\\varepsilon)})$.\nWhen the smoothness requirements are weakened, the rate for the regularized and\nsmoothed VS-SQN scheme worsens to $\\mathcal{O}(k^{-1/3})$. Such statements\nallow for a state-dependent noise assumption under a quadratic growth property. \n\n"}
{"id": "1804.06539", "contents": "Title: Successive Convexification: A Superlinearly Convergent Algorithm for\n  Non-convex Optimal Control Problems Abstract: This paper presents the SCvx algorithm, a successive convexification\nalgorithm designed to solve non-convex constrained optimal control problems\nwith global convergence and superlinear convergence-rate guarantees. The\nproposed algorithm can handle nonlinear dynamics and non-convex state and\ncontrol constraints. It solves the original problem to optimality by\nsuccessively linearizing non-convex dynamics and constraints about the solution\nof the previous iteration. The resulting convex subproblems are numerically\ntractable, and can be computed quickly and reliably using convex optimization\nsolvers, making the SCvx algorithm well suited for real-time applications.\nAnalysis is presented to show that the algorithm converges both globally and\nsuperlinearly, guaranteeing i) local optimality recovery: if the converged\nsolution is feasible with respect to the original problem, then it is also a\nlocal optimum; ii) strong convergence: if the Kurdyka-Lojasiewicz (KL)\ninequality holds at the converged solution, then the solution is unique. The\nsuperlinear rate of convergence is obtained by exploiting the structure of\noptimal control problems, showcasing that faster rate of convergence can be\nachieved by leveraging specific problem properties when compared to generic\nnonlinear programming methods. Numerical simulations are performed for a\nnon-convex quad-rotor motion planning problem, and corresponding results\nobtained using Sequential Quadratic Programming (SQP) and general purpose\nInterior Point Method (IPM) solvers are provided for comparison. The results\nshow that the convergence rate of the SCvx algorithm is indeed superlinear, and\nthat SCvx outperforms the other two methods by converging in less number of\niterations. \n\n"}
{"id": "1804.07223", "contents": "Title: Phase Transition of the 2-Choices Dynamics on Core-Periphery Networks Abstract: Consider the following process on a network: Each agent initially holds\neither opinion blue or red; then, in each round, each agent looks at two random\nneighbors and, if the two have the same opinion, the agent adopts it. This\nprocess is known as the 2-Choices dynamics and is arguably the most basic\nnon-trivial opinion dynamics modeling voting behavior on social networks.\nDespite its apparent simplicity, 2-Choices has been analytically characterized\nonly on restricted network classes---under assumptions on the initial\nconfiguration that establish it as a fast majority consensus protocol.\n  In this work, we aim at contributing to the understanding of the 2-Choices\ndynamics by considering its behavior on a class of networks with core-periphery\nstructure, a well-known topological assumption in social networks. In a\nnutshell, assume that a densely-connected subset of agents, the core, holds a\ndifferent opinion from the rest of the network, the periphery. Then, depending\non the strength of the cut between the core and the periphery, a\nphase-transition phenomenon occurs: Either the core's opinion rapidly spreads\namong the rest of the network, or a metastability phase takes place, in which\nboth opinions coexist in the network for superpolynomial time. The interest of\nour result is twofold. On the one hand, by looking at the 2-Choices dynamics as\na simplistic model of competition among opinions in social networks, our\ntheorem sheds light on the influence of the core on the rest of the network, as\na function of the core's connectivity toward the latter. On the other hand, we\nprovide one of the first analytical results which shows a heterogeneous\nbehavior of a simple dynamics as a function of structural parameters of the\nnetwork. Finally, we validate our theoretical predictions with extensive\nexperiments on real networks. \n\n"}
{"id": "1804.08369", "contents": "Title: Gaussian Material Synthesis Abstract: We present a learning-based system for rapid mass-scale material synthesis\nthat is useful for novice and expert users alike. The user preferences are\nlearned via Gaussian Process Regression and can be easily sampled for new\nrecommendations. Typically, each recommendation takes 40-60 seconds to render\nwith global illumination, which makes this process impracticable for real-world\nworkflows. Our neural network eliminates this bottleneck by providing\nhigh-quality image predictions in real time, after which it is possible to pick\nthe desired materials from a gallery and assign them to a scene in an intuitive\nmanner. Workflow timings against Disney's \"principled\" shader reveal that our\nsystem scales well with the number of sought materials, thus empowering even\nnovice users to generate hundreds of high-quality material models without any\nexpertise in material modeling. Similarly, expert users experience a\nsignificant decrease in the total modeling time when populating a scene with\nmaterials. Furthermore, our proposed solution also offers controllable\nrecommendations and a novel latent space variant generation step to enable the\nreal-time fine-tuning of materials without requiring any domain expertise. \n\n"}
{"id": "1804.10328", "contents": "Title: Scalable Bilinear $\\pi$ Learning Using State and Action Features Abstract: Approximate linear programming (ALP) represents one of the major algorithmic\nfamilies to solve large-scale Markov decision processes (MDP). In this work, we\nstudy a primal-dual formulation of the ALP, and develop a scalable, model-free\nalgorithm called bilinear $\\pi$ learning for reinforcement learning when a\nsampling oracle is provided. This algorithm enjoys a number of advantages.\nFirst, it adopts (bi)linear models to represent the high-dimensional value\nfunction and state-action distributions, using given state and action features.\nIts run-time complexity depends on the number of features, not the size of the\nunderlying MDPs. Second, it operates in a fully online fashion without having\nto store any sample, thus having minimal memory footprint. Third, we prove that\nit is sample-efficient, solving for the optimal policy to high precision with a\nsample complexity linear in the dimension of the parameter space. \n\n"}
{"id": "1804.10488", "contents": "Title: Offline Evaluation of Ranking Policies with Click Models Abstract: Many web systems rank and present a list of items to users, from recommender\nsystems to search and advertising. An important problem in practice is to\nevaluate new ranking policies offline and optimize them before they are\ndeployed. We address this problem by proposing evaluation algorithms for\nestimating the expected number of clicks on ranked lists from historical logged\ndata. The existing algorithms are not guaranteed to be statistically efficient\nin our problem because the number of recommended lists can grow exponentially\nwith their length. To overcome this challenge, we use models of user\ninteraction with the list of items, the so-called click models, to construct\nestimators that learn statistically efficiently. We analyze our estimators and\nprove that they are more efficient than the estimators that do not use the\nstructure of the click model, under the assumption that the click model holds.\nWe evaluate our estimators in a series of experiments on a real-world dataset\nand show that they consistently outperform prior estimators. \n\n"}
{"id": "1804.10587", "contents": "Title: An improvement of the convergence proof of the ADAM-Optimizer Abstract: A common way to train neural networks is the Backpropagation. This algorithm\nincludes a gradient descent method, which needs an adaptive step size. In the\narea of neural networks, the ADAM-Optimizer is one of the most popular adaptive\nstep size methods. It was invented in \\cite{Kingma.2015} by Kingma and Ba. The\n$5865$ citations in only three years shows additionally the importance of the\ngiven paper. We discovered that the given convergence proof of the optimizer\ncontains some mistakes, so that the proof will be wrong. In this paper we give\nan improvement to the convergence proof of the ADAM-Optimizer. \n\n"}
{"id": "1804.10850", "contents": "Title: Drug Similarity Integration Through Attentive Multi-view Graph\n  Auto-Encoders Abstract: Drug similarity has been studied to support downstream clinical tasks such as\ninferring novel properties of drugs (e.g. side effects, indications,\ninteractions) from known properties. The growing availability of new types of\ndrug features brings the opportunity of learning a more comprehensive and\naccurate drug similarity that represents the full spectrum of underlying drug\nrelations. However, it is challenging to integrate these heterogeneous, noisy,\nnonlinear-related information to learn accurate similarity measures especially\nwhen labels are scarce. Moreover, there is a trade-off between accuracy and\ninterpretability. In this paper, we propose to learn accurate and interpretable\nsimilarity measures from multiple types of drug features. In particular, we\nmodel the integration using multi-view graph auto-encoders, and add attentive\nmechanism to determine the weights for each view with respect to corresponding\ntasks and features for better interpretability. Our model has flexible design\nfor both semi-supervised and unsupervised settings. Experimental results\ndemonstrated significant predictive accuracy improvement. Case studies also\nshowed better model capacity (e.g. embed node features) and interpretability. \n\n"}
{"id": "1804.10932", "contents": "Title: Scenario Approach for Robust Blackbox Optimization in the Bandit Setting Abstract: This paper discusses a scenario approach to robust optimization of a blackbox\nfunction in a bandit setting. We assume that the blackbox function can be\nmodeled as a Gaussian Process (GP) for every realization of the uncertain\nparameter. We adopt a scenario approach in which we draw fixed independent\nsamples of the uncertain parameter. For a given policy, i.e., a sequence of\nquery points and uncertain parameters in the sampled set, we introduce a notion\nof regret defined with respect to additional draws of the uncertain parameter,\ntermed as scenario regret under re-draw. We present a scenario-based iterative\nalgorithm using the upper confidence bound (UCB) of the fixed independent\nscenarios to compute a policy for the blackbox optimization. For this\nalgorithm, we characterize a high probability upper bound on the regret under\nre-draw for any finite number of iterations of the algorithm. We further\ncharacterize parameter regimes in which the regret tends to zero asymptotically\nwith the number of iterations with high probability. Finally, we supplement our\nanalysis with numerical results. \n\n"}
{"id": "1805.00089", "contents": "Title: Concolic Testing for Deep Neural Networks Abstract: Concolic testing combines program execution and symbolic analysis to explore\nthe execution paths of a software program. This paper presents the first\nconcolic testing approach for Deep Neural Networks (DNNs). More specifically,\nwe formalise coverage criteria for DNNs that have been studied in the\nliterature, and then develop a coherent method for performing concolic testing\nto increase test coverage. Our experimental results show the effectiveness of\nthe concolic testing approach in both achieving high coverage and finding\nadversarial examples. \n\n"}
{"id": "1805.00310", "contents": "Title: On the Limitation of MagNet Defense against $L_1$-based Adversarial\n  Examples Abstract: In recent years, defending adversarial perturbations to natural examples in\norder to build robust machine learning models trained by deep neural networks\n(DNNs) has become an emerging research field in the conjunction of deep\nlearning and security. In particular, MagNet consisting of an adversary\ndetector and a data reformer is by far one of the strongest defenses in the\nblack-box oblivious attack setting, where the attacker aims to craft\ntransferable adversarial examples from an undefended DNN model to bypass an\nunknown defense module deployed on the same DNN model. Under this setting,\nMagNet can successfully defend a variety of attacks in DNNs, including the\nhigh-confidence adversarial examples generated by the Carlini and Wagner's\nattack based on the $L_2$ distortion metric. However, in this paper, under the\nsame attack setting we show that adversarial examples crafted based on the\n$L_1$ distortion metric can easily bypass MagNet and mislead the target DNN\nimage classifiers on MNIST and CIFAR-10. We also provide explanations on why\nthe considered approach can yield adversarial examples with superior attack\nperformance and conduct extensive experiments on variants of MagNet to verify\nits lack of robustness to $L_1$ distortion based attacks. Notably, our results\nsubstantially weaken the assumption of effective threat models on MagNet that\nrequire knowing the deployed defense technique when attacking DNNs (i.e., the\ngray-box attack setting). \n\n"}
{"id": "1805.01136", "contents": "Title: Nonparametric Pricing Analytics with Customer Covariates Abstract: Personalized pricing analytics is becoming an essential tool in retailing.\nUpon observing the personalized information of each arriving customer, the firm\nneeds to set a price accordingly based on the covariates such as income,\neducation background, past purchasing history to extract more revenue. For new\nentrants of the business, the lack of historical data may severely limit the\npower and profitability of personalized pricing. We propose a nonparametric\npricing policy to simultaneously learn the preference of customers based on the\ncovariates and maximize the expected revenue over a finite horizon. The policy\ndoes not depend on any prior assumptions on how the personalized information\naffects consumers' preferences (such as linear models). It is adaptively splits\nthe covariate space into smaller bins (hyper-rectangles) and clusters customers\nbased on their covariates and preferences, offering similar prices for\ncustomers who belong to the same cluster trading off granularity and accuracy.\nWe show that the algorithm achieves a regret of order $O(\\log(T)^2\nT^{(2+d)/(4+d)})$, where $T$ is the length of the horizon and $d$ is the\ndimension of the covariate. It improves the current regret in the literature\n\\citep{slivkins2014contextual}, under mild technical conditions in the pricing\ncontext (smoothness and local concavity). We also prove that no policy can\nachieve a regret less than $O(T^{(2+d)/(4+d)})$ for a particular instance and\nthus demonstrate the near optimality of the proposed policy. \n\n"}
{"id": "1805.01209", "contents": "Title: Found Graph Data and Planted Vertex Covers Abstract: A typical way in which network data is recorded is to measure all the\ninteractions among a specified set of core nodes; this produces a graph\ncontaining this core together with a potentially larger set of fringe nodes\nthat have links to the core. Interactions between pairs of nodes in the fringe,\nhowever, are not recorded by this process, and hence not present in the\nresulting graph data. For example, a phone service provider may only have\nrecords of calls in which at least one of the participants is a customer; this\ncan include calls between a customer and a non-customer, but not between pairs\nof non-customers.\n  Knowledge of which nodes belong to the core is an important piece of metadata\nthat is crucial for interpreting the network dataset. But in many cases, this\nmetadata is not available, either because it has been lost due to difficulties\nin data provenance, or because the network consists of found data obtained in\nsettings such as counter-surveillance. This leads to a natural algorithmic\nproblem, namely the recovery of the core set. Since the core set forms a vertex\ncover of the graph, we essentially have a planted vertex cover problem, but\nwith an arbitrary underlying graph. We develop a theoretical framework for\nanalyzing this planted vertex cover problem, based on results in the theory of\nfixed-parameter tractability, together with algorithms for recovering the core.\nOur algorithms are fast, simple to implement, and out-perform several methods\nbased on network core-periphery structure on various real-world datasets. \n\n"}
{"id": "1805.01554", "contents": "Title: A Deep Learning Model with Hierarchical LSTMs and Supervised Attention\n  for Anti-Phishing Abstract: Anti-phishing aims to detect phishing content/documents in a pool of textual\ndata. This is an important problem in cybersecurity that can help to guard\nusers from fraudulent information. Natural language processing (NLP) offers a\nnatural solution for this problem as it is capable of analyzing the textual\ncontent to perform intelligent recognition. In this work, we investigate\nstate-of-the-art techniques for text categorization in NLP to address the\nproblem of anti-phishing for emails (i.e, predicting if an email is phishing or\nnot). These techniques are based on deep learning models that have attracted\nmuch attention from the community recently. In particular, we present a\nframework with hierarchical long short-term memory networks (H-LSTMs) and\nattention mechanisms to model the emails simultaneously at the word and the\nsentence level. Our expectation is to produce an effective model for\nanti-phishing and demonstrate the effectiveness of deep learning for problems\nin cybersecurity. \n\n"}
{"id": "1805.04228", "contents": "Title: Trajectory tracking with an aggregation of domestic hot water heaters:\n  Combining model-based and model-free control in a commercial deployment Abstract: Scalable demand response of residential electric loads has been a timely\nresearch topic in recent years. The commercial coming of age or residential\ndemand response requires a scalable control architecture that is both efficient\nand practical to use. This work presents such a strategy for domestic hot water\nheaters and present a commercial proof-of-concept deployment. The strategy\ncombines state of the art in aggregate-and-dispatch with a novel dispatch\nstrategy leveraging recent developments in reinforcement learning and is tested\nin a hardware-in-the-loop simulation environment. The results are promising and\npresent how model-based and model-free control strategies can be merged to\nobtain a mature and commercially viable control strategy for residential demand\nresponse. \n\n"}
{"id": "1805.04272", "contents": "Title: An $O(N)$ Sorting Algorithm: Machine Learning Sort Abstract: We propose an $O(N\\cdot M)$ sorting algorithm by Machine Learning method,\nwhich shows a huge potential sorting big data. This sorting algorithm can be\napplied to parallel sorting and is suitable for GPU or TPU acceleration.\nFurthermore, we discuss the application of this algorithm to sparse hash table. \n\n"}
{"id": "1805.04577", "contents": "Title: Fast Rates of ERM and Stochastic Approximation: Adaptive to Error Bound\n  Conditions Abstract: Error bound conditions (EBC) are properties that characterize the growth of\nan objective function when a point is moved away from the optimal set. They\nhave recently received increasing attention in the field of optimization for\ndeveloping optimization algorithms with fast convergence. However, the studies\nof EBC in statistical learning are hitherto still limited. The main\ncontributions of this paper are two-fold. First, we develop fast and\nintermediate rates of empirical risk minimization (ERM) under EBC for risk\nminimization with Lipschitz continuous, and smooth convex random functions.\nSecond, we establish fast and intermediate rates of an efficient stochastic\napproximation (SA) algorithm for risk minimization with Lipschitz continuous\nrandom functions, which requires only one pass of $n$ samples and adapts to\nEBC. For both approaches, the convergence rates span a full spectrum between\n$\\widetilde O(1/\\sqrt{n})$ and $\\widetilde O(1/n)$ depending on the power\nconstant in EBC, and could be even faster than $O(1/n)$ in special cases for\nERM. Moreover, these convergence rates are automatically adaptive without using\nany knowledge of EBC. Overall, this work not only strengthens the understanding\nof ERM for statistical learning but also brings new fast stochastic algorithms\nfor solving a broad range of statistical learning problems. \n\n"}
{"id": "1805.06439", "contents": "Title: Prediction Rule Reshaping Abstract: Two methods are proposed for high-dimensional shape-constrained regression\nand classification. These methods reshape pre-trained prediction rules to\nsatisfy shape constraints like monotonicity and convexity. The first method can\nbe applied to any pre-trained prediction rule, while the second method deals\nspecifically with random forests. In both cases, efficient algorithms are\ndeveloped for computing the estimators, and experiments are performed to\ndemonstrate their performance on four datasets. We find that reshaping methods\nenforce shape constraints without compromising predictive accuracy. \n\n"}
{"id": "1805.06546", "contents": "Title: Joint Classification and Prediction CNN Framework for Automatic Sleep\n  Stage Classification Abstract: Correctly identifying sleep stages is important in diagnosing and treating\nsleep disorders. This work proposes a joint classification-and-prediction\nframework based on CNNs for automatic sleep staging, and, subsequently,\nintroduces a simple yet efficient CNN architecture to power the framework.\nGiven a single input epoch, the novel framework jointly determines its label\n(classification) and its neighboring epochs' labels (prediction) in the\ncontextual output. While the proposed framework is orthogonal to the widely\nadopted classification schemes, which take one or multiple epochs as contextual\ninputs and produce a single classification decision on the target epoch, we\ndemonstrate its advantages in several ways. First, it leverages the dependency\namong consecutive sleep epochs while surpassing the problems experienced with\nthe common classification schemes. Second, even with a single model, the\nframework has the capacity to produce multiple decisions, which are essential\nin obtaining a good performance as in ensemble-of-models methods, with very\nlittle induced computational overhead. Probabilistic aggregation techniques are\nthen proposed to leverage the availability of multiple decisions. We conducted\nexperiments on two public datasets: Sleep-EDF Expanded with 20 subjects, and\nMontreal Archive of Sleep Studies dataset with 200 subjects. The proposed\nframework yields an overall classification accuracy of 82.3% and 83.6%,\nrespectively. We also show that the proposed framework not only is superior to\nthe baselines based on the common classification schemes but also outperforms\nexisting deep-learning approaches. To our knowledge, this is the first work\ngoing beyond the standard single-output classification to consider multitask\nneural networks for automatic sleep staging. This framework provides avenues\nfor further studies of different neural-network architectures for automatic\nsleep staging. \n\n"}
{"id": "1805.07123", "contents": "Title: Tree Edit Distance Learning via Adaptive Symbol Embeddings:\n  Supplementary Materials and Results Abstract: Metric learning has the aim to improve classification accuracy by learning a\ndistance measure which brings data points from the same class closer together\nand pushes data points from different classes further apart. Recent research\nhas demonstrated that metric learning approaches can also be applied to trees,\nsuch as molecular structures, abstract syntax trees of computer programs, or\nsyntax trees of natural language, by learning the cost function of an edit\ndistance, i.e. the costs of replacing, deleting, or inserting nodes in a tree.\nHowever, learning such costs directly may yield an edit distance which violates\nmetric axioms, is challenging to interpret, and may not generalize well. In\nthis contribution, we propose a novel metric learning approach for trees which\nlearns an edit distance indirectly by embedding the tree nodes as vectors, such\nthat the Euclidean distance between those vectors supports class\ndiscrimination. We learn such embeddings by reducing the distance to\nprototypical trees from the same class and increasing the distance to\nprototypical trees from different classes. In our experiments, we show that our\nproposed metric learning approach improves upon the state-of-the-art in metric\nlearning for trees on six benchmark data sets, ranging from computer science\nover biomedical data to a natural-language processing data set containing over\n300,000 nodes. \n\n"}
{"id": "1805.07242", "contents": "Title: Siamese Capsule Networks Abstract: Capsule Networks have shown encouraging results on \\textit{defacto} benchmark\ncomputer vision datasets such as MNIST, CIFAR and smallNORB. Although, they are\nyet to be tested on tasks where (1) the entities detected inherently have more\ncomplex internal representations and (2) there are very few instances per class\nto learn from and (3) where point-wise classification is not suitable. Hence,\nthis paper carries out experiments on face verification in both controlled and\nuncontrolled settings that together address these points. In doing so we\nintroduce \\textit{Siamese Capsule Networks}, a new variant that can be used for\npairwise learning tasks. The model is trained using contrastive loss with\n$\\ell_2$-normalized capsule encoded pose features. We find that \\textit{Siamese\nCapsule Networks} perform well against strong baselines on both pairwise\nlearning datasets, yielding best results in the few-shot learning setting where\nimage pairs in the test set contain unseen subjects. \n\n"}
{"id": "1805.07418", "contents": "Title: Sequential Learning of Principal Curves: Summarizing Data Streams on the\n  Fly Abstract: When confronted with massive data streams, summarizing data with dimension\nreduction methods such as PCA raises theoretical and algorithmic pitfalls.\nPrincipal curves act as a nonlinear generalization of PCA and the present paper\nproposes a novel algorithm to automatically and sequentially learn principal\ncurves from data streams. We show that our procedure is supported by regret\nbounds with optimal sublinear remainder terms. A greedy local search\nimplementation (called \\texttt{slpc}, for Sequential Learning Principal Curves)\nthat incorporates both sleeping experts and multi-armed bandit ingredients is\npresented, along with its regret computation and performance on synthetic and\nreal-life data. \n\n"}
{"id": "1805.07458", "contents": "Title: PG-TS: Improved Thompson Sampling for Logistic Contextual Bandits Abstract: We address the problem of regret minimization in logistic contextual bandits,\nwhere a learner decides among sequential actions or arms given their respective\ncontexts to maximize binary rewards. Using a fast inference procedure with\nPolya-Gamma distributed augmentation variables, we propose an improved version\nof Thompson Sampling, a Bayesian formulation of contextual bandits with\nnear-optimal performance. Our approach, Polya-Gamma augmented Thompson Sampling\n(PG-TS), achieves state-of-the-art performance on simulated and real data.\nPG-TS explores the action space efficiently and exploits high-reward arms,\nquickly converging to solutions of low regret. Its explicit estimation of the\nposterior distribution of the context feature covariance leads to substantial\nempirical gains over approximate approaches. PG-TS is the first approach to\ndemonstrate the benefits of Polya-Gamma augmentation in bandits and to propose\nan efficient Gibbs sampler for approximating the analytically unsolvable\nintegral of logistic contextual bandits. \n\n"}
{"id": "1805.07484", "contents": "Title: A new method for quantifying network cyclic structure to improve\n  community detection Abstract: A distinguishing property of communities in networks is that cycles are more\nprevalent within communities than across communities. Thus, the detection of\nthese communities may be aided through the incorporation of measures of the\nlocal \"richness\" of the cyclic structure. In this paper, we introduce renewal\nnon-backtracking random walks (RNBRW) as a way of quantifying this structure.\nRNBRW gives a weight to each edge equal to the probability that a\nnon-backtracking random walk completes a cycle with that edge. Hence, edges\nwith larger weights may be thought of as more important to the formation of\ncycles. Of note, since separate random walks can be performed in parallel,\nRNBRW weights can be estimated very quickly, even for large graphs. We give\nsimulation results showing that pre-weighting edges through RNBRW may\nsubstantially improve the performance of common community detection algorithms.\nOur results suggest that RNBRW is especially efficient for the challenging case\nof detecting communities in sparse graphs. \n\n"}
{"id": "1805.07544", "contents": "Title: Conditional Network Embeddings Abstract: Network Embeddings (NEs) map the nodes of a given network into\n$d$-dimensional Euclidean space $\\mathbb{R}^d$. Ideally, this mapping is such\nthat `similar' nodes are mapped onto nearby points, such that the NE can be\nused for purposes such as link prediction (if `similar' means being `more\nlikely to be connected') or classification (if `similar' means `being more\nlikely to have the same label'). In recent years various methods for NE have\nbeen introduced, all following a similar strategy: defining a notion of\nsimilarity between nodes (typically some distance measure within the network),\na distance measure in the embedding space, and a loss function that penalizes\nlarge distances for similar nodes and small distances for dissimilar nodes.\n  A difficulty faced by existing methods is that certain networks are\nfundamentally hard to embed due to their structural properties: (approximate)\nmultipartiteness, certain degree distributions, assortativity, etc. To overcome\nthis, we introduce a conceptual innovation to the NE literature and propose to\ncreate \\emph{Conditional Network Embeddings} (CNEs); embeddings that maximally\nadd information with respect to given structural properties (e.g. node degrees,\nblock densities, etc.). We use a simple Bayesian approach to achieve this, and\npropose a block stochastic gradient descent algorithm for fitting it\nefficiently. We demonstrate that CNEs are superior for link prediction and\nmulti-label classification when compared to state-of-the-art methods, and this\nwithout adding significant mathematical or computational complexity. Finally,\nwe illustrate the potential of CNE for network visualization. \n\n"}
{"id": "1805.07588", "contents": "Title: Robust Optimization over Multiple Domains Abstract: In this work, we study the problem of learning a single model for multiple\ndomains. Unlike the conventional machine learning scenario where each domain\ncan have the corresponding model, multiple domains (i.e., applications/users)\nmay share the same machine learning model due to maintenance loads in cloud\ncomputing services. For example, a digit-recognition model should be applicable\nto hand-written digits, house numbers, car plates, etc. Therefore, an ideal\nmodel for cloud computing has to perform well at each applicable domain. To\naddress this new challenge from cloud computing, we develop a framework of\nrobust optimization over multiple domains. In lieu of minimizing the empirical\nrisk, we aim to learn a model optimized to the adversarial distribution over\nmultiple domains. Hence, we propose to learn the model and the adversarial\ndistribution simultaneously with the stochastic algorithm for efficiency.\nTheoretically, we analyze the convergence rate for convex and non-convex\nmodels. To our best knowledge, we first study the convergence rate of learning\na robust non-convex model with a practical algorithm. Furthermore, we\ndemonstrate that the robustness of the framework and the convergence rate can\nbe further enhanced by appropriate regularizers over the adversarial\ndistribution. The empirical study on real-world fine-grained visual\ncategorization and digits recognition tasks verifies the effectiveness and\nefficiency of the proposed framework. \n\n"}
{"id": "1805.08463", "contents": "Title: Variational Learning on Aggregate Outputs with Gaussian Processes Abstract: While a typical supervised learning framework assumes that the inputs and the\noutputs are measured at the same levels of granularity, many applications,\nincluding global mapping of disease, only have access to outputs at a much\ncoarser level than that of the inputs. Aggregation of outputs makes\ngeneralization to new inputs much more difficult. We consider an approach to\nthis problem based on variational learning with a model of output aggregation\nand Gaussian processes, where aggregation leads to intractability of the\nstandard evidence lower bounds. We propose new bounds and tractable\napproximations, leading to improved prediction accuracy and scalability to\nlarge datasets, while explicitly taking uncertainty into account. We develop a\nframework which extends to several types of likelihoods, including the Poisson\nmodel for aggregated count data. We apply our framework to a challenging and\nimportant problem, the fine-scale spatial modelling of malaria incidence, with\nover 1 million observations. \n\n"}
{"id": "1805.08671", "contents": "Title: Adding One Neuron Can Eliminate All Bad Local Minima Abstract: One of the main difficulties in analyzing neural networks is the\nnon-convexity of the loss function which may have many bad local minima.\n  In this paper, we study the landscape of neural networks for binary\nclassification tasks. Under mild assumptions, we prove that after adding one\nspecial neuron with a skip connection to the output, or one special neuron per\nlayer, every local minimum is a global minimum. \n\n"}
{"id": "1805.09122", "contents": "Title: Probabilistic Riemannian submanifold learning with wrapped Gaussian\n  process latent variable models Abstract: Latent variable models (LVMs) learn probabilistic models of data manifolds\nlying in an \\emph{ambient} Euclidean space. In a number of applications, a\npriori known spatial constraints can shrink the ambient space into a\nconsiderably smaller manifold. Additionally, in these applications the\nEuclidean geometry might induce a suboptimal similarity measure, which could be\nimproved by choosing a different metric. Euclidean models ignore such\ninformation and assign probability mass to data points that can never appear as\ndata, and vastly different likelihoods to points that are similar under the\ndesired metric. We propose the wrapped Gaussian process latent variable model\n(WGPLVM), that extends Gaussian process latent variable models to take values\nstrictly on a given ambient Riemannian manifold, making the model blind to\nimpossible data points. This allows non-linear, probabilistic inference of\nlow-dimensional Riemannian submanifolds from data. Our evaluation on diverse\ndatasets show that we improve performance on several tasks, including encoding,\nvisualization and uncertainty quantification. \n\n"}
{"id": "1805.09453", "contents": "Title: Solving Large-Scale Optimization Problems with a Convergence Rate\n  Independent of Grid Size Abstract: We present a primal-dual method to solve L1-type non-smooth optimization\nproblems independently of the grid size. We apply these results to two\nimportant problems : the Rudin-Osher-Fatemi image denoising model and the L1\nearth mover's distance from optimal transport. Crucially, we provide analysis\nthat determines the choice of optimal step sizes and we prove that our method\nconverges independently of the grid size. Our approach allows us to solve these\nproblems on grids as large as 4096 by 4096 in a few minutes without\nparallelization. \n\n"}
{"id": "1805.09480", "contents": "Title: Optimal Algorithms for Continuous Non-monotone Submodular and\n  DR-Submodular Maximization Abstract: In this paper we study the fundamental problems of maximizing a continuous\nnon-monotone submodular function over the hypercube, both with and without\ncoordinate-wise concavity. This family of optimization problems has several\napplications in machine learning, economics, and communication systems. Our\nmain result is the first $\\frac{1}{2}$-approximation algorithm for continuous\nsubmodular function maximization; this approximation factor of $\\frac{1}{2}$ is\nthe best possible for algorithms that only query the objective function at\npolynomially many points. For the special case of DR-submodular maximization,\ni.e. when the submodular functions is also coordinate wise concave along all\ncoordinates, we provide a different $\\frac{1}{2}$-approximation algorithm that\nruns in quasilinear time. Both of these results improve upon prior work [Bian\net al, 2017, Soma and Yoshida, 2017].\n  Our first algorithm uses novel ideas such as reducing the guaranteed\napproximation problem to analyzing a zero-sum game for each coordinate, and\nincorporates the geometry of this zero-sum game to fix the value at this\ncoordinate. Our second algorithm exploits coordinate-wise concavity to identify\na monotone equilibrium condition sufficient for getting the required\napproximation guarantee, and hunts for the equilibrium point using binary\nsearch. We further run experiments to verify the performance of our proposed\nalgorithms in related machine learning applications. \n\n"}
{"id": "1805.09793", "contents": "Title: New Insights into Bootstrapping for Bandits Abstract: We investigate the use of bootstrapping in the bandit setting. We first show\nthat the commonly used non-parametric bootstrapping (NPB) procedure can be\nprovably inefficient and establish a near-linear lower bound on the regret\nincurred by it under the bandit model with Bernoulli rewards. We show that NPB\nwith an appropriate amount of forced exploration can result in sub-linear\nalbeit sub-optimal regret. As an alternative to NPB, we propose a weighted\nbootstrapping (WB) procedure. For Bernoulli rewards, WB with multiplicative\nexponential weights is mathematically equivalent to Thompson sampling (TS) and\nresults in near-optimal regret bounds. Similarly, in the bandit setting with\nGaussian rewards, we show that WB with additive Gaussian weights achieves\nnear-optimal regret. Beyond these special cases, we show that WB leads to\nbetter empirical performance than TS for several reward distributions bounded\non $[0,1]$. For the contextual bandit setting, we give practical guidelines\nthat make bootstrapping simple and efficient to implement and result in good\nempirical performance on real-world datasets. \n\n"}
{"id": "1805.09980", "contents": "Title: Deep Graph Translation Abstract: Inspired by the tremendous success of deep generative models on generating\ncontinuous data like image and audio, in the most recent year, few deep graph\ngenerative models have been proposed to generate discrete data such as graphs.\nThey are typically unconditioned generative models which has no control on\nmodes of the graphs being generated. Differently, in this paper, we are\ninterested in a new problem named \\emph{Deep Graph Translation}: given an input\ngraph, we want to infer a target graph based on their underlying (both global\nand local) translation mapping. Graph translation could be highly desirable in\nmany applications such as disaster management and rare event forecasting, where\nthe rare and abnormal graph patterns (e.g., traffic congestions and terrorism\nevents) will be inferred prior to their occurrence even without historical data\non the abnormal patterns for this graph (e.g., a road network or human contact\nnetwork). To achieve this, we propose a novel Graph-Translation-Generative\nAdversarial Networks (GT-GAN) which will generate a graph translator from input\nto target graphs. GT-GAN consists of a graph translator where we propose new\ngraph convolution and deconvolution layers to learn the global and local\ntranslation mapping. A new conditional graph discriminator has also been\nproposed to classify target graphs by conditioning on input graphs. Extensive\nexperiments on multiple synthetic and real-world datasets demonstrate the\neffectiveness and scalability of the proposed GT-GAN. \n\n"}
{"id": "1805.10265", "contents": "Title: Training verified learners with learned verifiers Abstract: This paper proposes a new algorithmic framework, predictor-verifier training,\nto train neural networks that are verifiable, i.e., networks that provably\nsatisfy some desired input-output properties. The key idea is to simultaneously\ntrain two networks: a predictor network that performs the task at hand,e.g.,\npredicting labels given inputs, and a verifier network that computes a bound on\nhow well the predictor satisfies the properties being verified. Both networks\ncan be trained simultaneously to optimize a weighted combination of the\nstandard data-fitting loss and a term that bounds the maximum violation of the\nproperty. Experiments show that not only is the predictor-verifier architecture\nable to train networks to achieve state of the art verified robustness to\nadversarial examples with much shorter training times (outperforming previous\nalgorithms on small datasets like MNIST and SVHN), but it can also be scaled to\nproduce the first known (to the best of our knowledge) verifiably robust\nnetworks for CIFAR-10. \n\n"}
{"id": "1805.11048", "contents": "Title: Scalable Spectral Clustering Using Random Binning Features Abstract: Spectral clustering is one of the most effective clustering approaches that\ncapture hidden cluster structures in the data. However, it does not scale well\nto large-scale problems due to its quadratic complexity in constructing\nsimilarity graphs and computing subsequent eigendecomposition. Although a\nnumber of methods have been proposed to accelerate spectral clustering, most of\nthem compromise considerable information loss in the original data for reducing\ncomputational bottlenecks. In this paper, we present a novel scalable spectral\nclustering method using Random Binning features (RB) to simultaneously\naccelerate both similarity graph construction and the eigendecomposition.\nSpecifically, we implicitly approximate the graph similarity (kernel) matrix by\nthe inner product of a large sparse feature matrix generated by RB. Then we\nintroduce a state-of-the-art SVD solver to effectively compute eigenvectors of\nthis large matrix for spectral clustering. Using these two building blocks, we\nreduce the computational cost from quadratic to linear in the number of data\npoints while achieving similar accuracy. Our theoretical analysis shows that\nspectral clustering via RB converges faster to the exact spectral clustering\nthan the standard Random Feature approximation. Extensive experiments on 8\nbenchmarks show that the proposed method either outperforms or matches the\nstate-of-the-art methods in both accuracy and runtime. Moreover, our method\nexhibits linear scalability in both the number of data samples and the number\nof RB features. \n\n"}
{"id": "1805.11752", "contents": "Title: Multi-turn Dialogue Response Generation in an Adversarial Learning\n  Framework Abstract: We propose an adversarial learning approach for generating multi-turn\ndialogue responses. Our proposed framework, hredGAN, is based on conditional\ngenerative adversarial networks (GANs). The GAN's generator is a modified\nhierarchical recurrent encoder-decoder network (HRED) and the discriminator is\na word-level bidirectional RNN that shares context and word embeddings with the\ngenerator. During inference, noise samples conditioned on the dialogue history\nare used to perturb the generator's latent space to generate several possible\nresponses. The final response is the one ranked best by the discriminator. The\nhredGAN shows improved performance over existing methods: (1) it generalizes\nbetter than networks trained using only the log-likelihood criterion, and (2)\nit generates longer, more informative and more diverse responses with high\nutterance and topic relevance even with limited training data. This improvement\nis demonstrated on the Movie triples and Ubuntu dialogue datasets using both\nautomatic and human evaluations. \n\n"}
{"id": "1805.11769", "contents": "Title: Fast Incremental von Neumann Graph Entropy Computation: Theory,\n  Algorithm, and Applications Abstract: The von Neumann graph entropy (VNGE) facilitates measurement of information\ndivergence and distance between graphs in a graph sequence. It has been\nsuccessfully applied to various learning tasks driven by network-based data.\nWhile effective, VNGE is computationally demanding as it requires the full\neigenspectrum of the graph Laplacian matrix. In this paper, we propose a new\ncomputational framework, Fast Incremental von Neumann Graph EntRopy (FINGER),\nwhich approaches VNGE with a performance guarantee. FINGER reduces the cubic\ncomplexity of VNGE to linear complexity in the number of nodes and edges, and\nthus enables online computation based on incremental graph changes. We also\nshow asymptotic equivalence of FINGER to the exact VNGE, and derive its\napproximation error bounds. Based on FINGER, we propose efficient algorithms\nfor computing Jensen-Shannon distance between graphs. Our experimental results\non different random graph models demonstrate the computational efficiency and\nthe asymptotic equivalence of FINGER. In addition, we apply FINGER to two\nreal-world applications and one synthesized anomaly detection dataset, and\ncorroborate its superior performance over seven baseline graph similarity\nmethods. \n\n"}
{"id": "1805.11835", "contents": "Title: Optimal Control Via Neural Networks: A Convex Approach Abstract: Control of complex systems involves both system identification and controller\ndesign. Deep neural networks have proven to be successful in many\nidentification tasks, however, from model-based control perspective, these\nnetworks are difficult to work with because they are typically nonlinear and\nnonconvex. Therefore many systems are still identified and controlled based on\nsimple linear models despite their poor representation capability. In this\npaper we bridge the gap between model accuracy and control tractability faced\nby neural networks, by explicitly constructing networks that are convex with\nrespect to their inputs. We show that these input convex networks can be\ntrained to obtain accurate models of complex physical systems. In particular,\nwe design input convex recurrent neural networks to capture temporal behavior\nof dynamical systems. Then optimal controllers can be achieved via solving a\nconvex model predictive control problem. Experiment results demonstrate the\ngood potential of the proposed input convex neural network based approach in a\nvariety of control applications. In particular we show that in the MuJoCo\nlocomotion tasks, we could achieve over 10% higher performance using 5* less\ntime compared with state-of-the-art model-based reinforcement learning method;\nand in the building HVAC control example, our method achieved up to 20% energy\nreduction compared with classic linear models. \n\n"}
{"id": "1805.11916", "contents": "Title: On the Spectrum of Random Features Maps of High Dimensional Data Abstract: Random feature maps are ubiquitous in modern statistical machine learning,\nwhere they generalize random projections by means of powerful, yet often\ndifficult to analyze nonlinear operators. In this paper, we leverage the\n\"concentration\" phenomenon induced by random matrix theory to perform a\nspectral analysis on the Gram matrix of these random feature maps, here for\nGaussian mixture models of simultaneously large dimension and size. Our results\nare instrumental to a deeper understanding on the interplay of the nonlinearity\nand the statistics of the data, thereby allowing for a better tuning of random\nfeature-based techniques. \n\n"}
{"id": "1805.12002", "contents": "Title: Why Is My Classifier Discriminatory? Abstract: Recent attempts to achieve fairness in predictive models focus on the balance\nbetween fairness and accuracy. In sensitive applications such as healthcare or\ncriminal justice, this trade-off is often undesirable as any increase in\nprediction error could have devastating consequences. In this work, we argue\nthat the fairness of predictions should be evaluated in context of the data,\nand that unfairness induced by inadequate samples sizes or unmeasured\npredictive variables should be addressed through data collection, rather than\nby constraining the model. We decompose cost-based metrics of discrimination\ninto bias, variance, and noise, and propose actions aimed at estimating and\nreducing each term. Finally, we perform case-studies on prediction of income,\nmortality, and review ratings, confirming the value of this analysis. We find\nthat data collection is often a means to reduce discrimination without\nsacrificing accuracy. \n\n"}
{"id": "1805.12168", "contents": "Title: A Flexible Framework for Multi-Objective Bayesian Optimization using\n  Random Scalarizations Abstract: Many real world applications can be framed as multi-objective optimization\nproblems, where we wish to simultaneously optimize for multiple criteria.\nBayesian optimization techniques for the multi-objective setting are pertinent\nwhen the evaluation of the functions in question are expensive. Traditional\nmethods for multi-objective optimization, both Bayesian and otherwise, are\naimed at recovering the Pareto front of these objectives. However, in certain\ncases a practitioner might desire to identify Pareto optimal points only in a\nsubset of the Pareto front due to external considerations. In this work, we\npropose a strategy based on random scalarizations of the objectives that\naddresses this problem. Our approach is able to flexibly sample from desired\nregions of the Pareto front and, computationally, is considerably cheaper than\nmost approaches for MOO. We also study a notion of regret in the\nmulti-objective setting and show that our strategy achieves sublinear regret.\nWe experiment with both synthetic and real-life problems, and demonstrate\nsuperior performance of our proposed algorithm in terms of the flexibility and\nregret. \n\n"}
{"id": "1805.12549", "contents": "Title: Channel Gating Neural Networks Abstract: This paper introduces channel gating, a dynamic, fine-grained, and\nhardware-efficient pruning scheme to reduce the computation cost for\nconvolutional neural networks (CNNs). Channel gating identifies regions in the\nfeatures that contribute less to the classification result, and skips the\ncomputation on a subset of the input channels for these ineffective regions.\nUnlike static network pruning, channel gating optimizes CNN inference at\nrun-time by exploiting input-specific characteristics, which allows\nsubstantially reducing the compute cost with almost no accuracy loss. We\nexperimentally show that applying channel gating in state-of-the-art networks\nachieves 2.7-8.0$\\times$ reduction in floating-point operations (FLOPs) and\n2.0-4.4$\\times$ reduction in off-chip memory accesses with a minimal accuracy\nloss on CIFAR-10. Combining our method with knowledge distillation reduces the\ncompute cost of ResNet-18 by 2.6$\\times$ without accuracy drop on ImageNet. We\nfurther demonstrate that channel gating can be realized in hardware\nefficiently. Our approach exhibits sparsity patterns that are well-suited to\ndense systolic arrays with minimal additional hardware. We have designed an\naccelerator for channel gating networks, which can be implemented using either\nFPGAs or ASICs. Running a quantized ResNet-18 model for ImageNet, our\naccelerator achieves an encouraging speedup of 2.4$\\times$ on average, with a\ntheoretical FLOP reduction of 2.8$\\times$. \n\n"}
{"id": "1806.00125", "contents": "Title: Accelerating Incremental Gradient Optimization with Curvature\n  Information Abstract: This paper studies an acceleration technique for incremental aggregated\ngradient ({\\sf IAG}) method through the use of \\emph{curvature} information for\nsolving strongly convex finite sum optimization problems. These optimization\nproblems of interest arise in large-scale learning applications. Our technique\nutilizes a curvature-aided gradient tracking step to produce accurate gradient\nestimates incrementally using Hessian information. We propose and analyze two\nmethods utilizing the new technique, the curvature-aided IAG ({\\sf CIAG})\nmethod and the accelerated CIAG ({\\sf A-CIAG}) method, which are analogous to\ngradient method and Nesterov's accelerated gradient method, respectively.\nSetting $\\kappa$ to be the condition number of the objective function, we prove\nthe $R$ linear convergence rates of $1 - \\frac{4c_0 \\kappa}{(\\kappa+1)^2}$ for\nthe {\\sf CIAG} method, and $1 - \\sqrt{\\frac{c_1}{2\\kappa}}$ for the {\\sf\nA-CIAG} method, where $c_0,c_1 \\leq 1$ are constants inversely proportional to\nthe distance between the initial point and the optimal solution. When the\ninitial iterate is close to the optimal solution, the $R$ linear convergence\nrates match with the gradient and accelerated gradient method, albeit {\\sf\nCIAG} and {\\sf A-CIAG} operate in an incremental setting with strictly lower\ncomputation complexity. Numerical experiments confirm our findings. The source\ncodes used for this paper can be found on\n\\url{http://github.com/hoitowai/ciag/}. \n\n"}
{"id": "1806.00260", "contents": "Title: The Proximal Alternating Minimization Algorithm for two-block separable\n  convex optimization problems with linear constraints Abstract: The Alternating Minimization Algorithm (AMA) has been proposed by Tseng to\nsolve convex programming problems with two-block separable linear constraints\nand objectives, whereby (at least) one of the components of the latter is\nassumed to be strongly convex. The fact that one of the subproblems to be\nsolved within the iteration process of AMA does not usually correspond to the\ncalculation of a proximal operator through a closed formula, affects the\nimplementability of the algorithm. In this paper we allow in each block of the\nobjective a further smooth convex function and propose a proximal version of\nAMA, called Proximal AMA, which is achieved by equipping the algorithm with\nproximal terms induced by variable metrics. For suitable choices of the latter,\nthe solving of the two subproblems in the iterative scheme can be reduced to\nthe computation of proximal operators. We investigate the convergence of the\nproposed algorithm in a real Hilbert space setting and illustrate its numerical\nperformances on two applications in image processing and machine learning. \n\n"}
{"id": "1806.00420", "contents": "Title: Whitening and Coloring batch transform for GANs Abstract: Batch Normalization (BN) is a common technique used to speed-up and stabilize\ntraining. On the other hand, the learnable parameters of BN are commonly used\nin conditional Generative Adversarial Networks (cGANs) for representing\nclass-specific information using conditional Batch Normalization (cBN). In this\npaper we propose to generalize both BN and cBN using a Whitening and Coloring\nbased batch normalization. We show that our conditional Coloring can represent\ncategorical conditioning information which largely helps the cGAN qualitative\nresults. Moreover, we show that full-feature whitening is important in a\ngeneral GAN scenario in which the training process is known to be highly\nunstable. We test our approach on different datasets and using different GAN\nnetworks and training protocols, showing a consistent improvement in all the\ntested frameworks. Our CIFAR-10 conditioned results are higher than all\nprevious works on this dataset. \n\n"}
{"id": "1806.00451", "contents": "Title: Do CIFAR-10 Classifiers Generalize to CIFAR-10? Abstract: Machine learning is currently dominated by largely experimental work focused\non improvements in a few key tasks. However, the impressive accuracy numbers of\nthe best performing models are questionable because the same test sets have\nbeen used to select these models for multiple years now. To understand the\ndanger of overfitting, we measure the accuracy of CIFAR-10 classifiers by\ncreating a new test set of truly unseen images. Although we ensure that the new\ntest set is as close to the original data distribution as possible, we find a\nlarge drop in accuracy (4% to 10%) for a broad range of deep learning models.\nYet more recent models with higher original accuracy show a smaller drop and\nbetter overall performance, indicating that this drop is likely not due to\noverfitting based on adaptivity. Instead, we view our results as evidence that\ncurrent accuracy numbers are brittle and susceptible to even minute natural\nvariations in the data distribution. \n\n"}
{"id": "1806.00811", "contents": "Title: Causal Inference with Noisy and Missing Covariates via Matrix\n  Factorization Abstract: Valid causal inference in observational studies often requires controlling\nfor confounders. However, in practice measurements of confounders may be noisy,\nand can lead to biased estimates of causal effects. We show that we can reduce\nthe bias caused by measurement noise using a large number of noisy measurements\nof the underlying confounders. We propose the use of matrix factorization to\ninfer the confounders from noisy covariates, a flexible and principled\nframework that adapts to missing values, accommodates a wide variety of data\ntypes, and can augment many causal inference methods. We bound the error for\nthe induced average treatment effect estimator and show it is consistent in a\nlinear regression setting, using Exponential Family Matrix Completion\npreprocessing. We demonstrate the effectiveness of the proposed procedure in\nnumerical experiments with both synthetic data and real clinical data. \n\n"}
{"id": "1806.00900", "contents": "Title: Algorithmic Regularization in Learning Deep Homogeneous Models: Layers\n  are Automatically Balanced Abstract: We study the implicit regularization imposed by gradient descent for learning\nmulti-layer homogeneous functions including feed-forward fully connected and\nconvolutional deep neural networks with linear, ReLU or Leaky ReLU activation.\nWe rigorously prove that gradient flow (i.e. gradient descent with\ninfinitesimal step size) effectively enforces the differences between squared\nnorms across different layers to remain invariant without any explicit\nregularization. This result implies that if the weights are initially small,\ngradient flow automatically balances the magnitudes of all layers. Using a\ndiscretization argument, we analyze gradient descent with positive step size\nfor the non-convex low-rank asymmetric matrix factorization problem without any\nregularization. Inspired by our findings for gradient flow, we prove that\ngradient descent with step sizes $\\eta_t = O\\left(t^{-\\left(\n\\frac12+\\delta\\right)} \\right)$ ($0<\\delta\\le\\frac12$) automatically balances\ntwo low-rank factors and converges to a bounded global optimum. Furthermore,\nfor rank-$1$ asymmetric matrix factorization we give a finer analysis showing\ngradient descent with constant step size converges to the global minimum at a\nglobally linear rate. We believe that the idea of examining the invariance\nimposed by first order algorithms in learning homogeneous models could serve as\na fundamental building block for studying optimization for learning deep\nmodels. \n\n"}
{"id": "1806.01248", "contents": "Title: Dynamically Hierarchy Revolution: DirNet for Compressing Recurrent\n  Neural Network on Mobile Devices Abstract: Recurrent neural networks (RNNs) achieve cutting-edge performance on a\nvariety of problems. However, due to their high computational and memory\ndemands, deploying RNNs on resource constrained mobile devices is a challenging\ntask. To guarantee minimum accuracy loss with higher compression rate and\ndriven by the mobile resource requirement, we introduce a novel model\ncompression approach DirNet based on an optimized fast dictionary learning\nalgorithm, which 1) dynamically mines the dictionary atoms of the projection\ndictionary matrix within layer to adjust the compression rate 2) adaptively\nchanges the sparsity of sparse codes cross the hierarchical layers.\nExperimental results on language model and an ASR model trained with a 1000h\nspeech dataset demonstrate that our method significantly outperforms prior\napproaches. Evaluated on off-the-shelf mobile devices, we are able to reduce\nthe size of original model by eight times with real-time model inference and\nnegligible accuracy loss. \n\n"}
{"id": "1806.01551", "contents": "Title: Deep Mixed Effect Model using Gaussian Processes: A Personalized and\n  Reliable Prediction for Healthcare Abstract: We present a personalized and reliable prediction model for healthcare, which\ncan provide individually tailored medical services such as diagnosis, disease\ntreatment, and prevention. Our proposed framework targets at making\npersonalized and reliable predictions from time-series data, such as Electronic\nHealth Records (EHR), by modeling two complementary components: i) a shared\ncomponent that captures global trend across diverse patients and ii) a\npatient-specific component that models idiosyncratic variability for each\npatient. To this end, we propose a composite model of a deep neural network to\nlearn complex global trends from the large number of patients, and Gaussian\nProcesses (GP) to probabilistically model individual time-series given\nrelatively small number of visits per patient. We evaluate our model on diverse\nand heterogeneous tasks from EHR datasets and show practical advantages over\nstandard time-series deep models such as pure Recurrent Neural Network (RNN). \n\n"}
{"id": "1806.01827", "contents": "Title: Performance Metric Elicitation from Pairwise Classifier Comparisons Abstract: Given a binary prediction problem, which performance metric should the\nclassifier optimize? We address this question by formalizing the problem of\nMetric Elicitation. The goal of metric elicitation is to discover the\nperformance metric of a practitioner, which reflects her innate rewards (costs)\nfor correct (incorrect) classification. In particular, we focus on eliciting\nbinary classification performance metrics from pairwise feedback, where a\npractitioner is queried to provide relative preference between two classifiers.\nBy exploiting key geometric properties of the space of confusion matrices, we\nobtain provably query efficient algorithms for eliciting linear and\nlinear-fractional performance metrics. We further show that our method is\nrobust to feedback and finite sample noise. \n\n"}
{"id": "1806.01933", "contents": "Title: Explainable Neural Networks based on Additive Index Models Abstract: Machine Learning algorithms are increasingly being used in recent years due\nto their flexibility in model fitting and increased predictive performance.\nHowever, the complexity of the models makes them hard for the data analyst to\ninterpret the results and explain them without additional tools. This has led\nto much research in developing various approaches to understand the model\nbehavior. In this paper, we present the Explainable Neural Network (xNN), a\nstructured neural network designed especially to learn interpretable features.\nUnlike fully connected neural networks, the features engineered by the xNN can\nbe extracted from the network in a relatively straightforward manner and the\nresults displayed. With appropriate regularization, the xNN provides a\nparsimonious explanation of the relationship between the features and the\noutput. We illustrate this interpretable feature--engineering property on\nsimulated examples. \n\n"}
{"id": "1806.02389", "contents": "Title: Not All Attributes are Created Equal: $d_{\\mathcal{X}}$-Private\n  Mechanisms for Linear Queries Abstract: Differential privacy provides strong privacy guarantees simultaneously\nenabling useful insights from sensitive datasets. However, it provides the same\nlevel of protection for all elements (individuals and attributes) in the data.\nThere are practical scenarios where some data attributes need more/less\nprotection than others. In this paper, we consider $d_{\\mathcal{X}}$-privacy,\nan instantiation of the privacy notion introduced in\n\\cite{chatzikokolakis2013broadening}, which allows this flexibility by\nspecifying a separate privacy budget for each pair of elements in the data\ndomain. We describe a systematic procedure to tailor any existing\ndifferentially private mechanism that assumes a query set and a sensitivity\nvector as input into its $d_{\\mathcal{X}}$-private variant, specifically\nfocusing on linear queries. Our proposed meta procedure has broad applications\nas linear queries form the basis of a range of data analysis and machine\nlearning algorithms, and the ability to define a more flexible privacy budget\nacross the data domain results in improved privacy/utility tradeoff in these\napplications. We propose several $d_{\\mathcal{X}}$-private mechanisms, and\nprovide theoretical guarantees on the trade-off between utility and privacy. We\nalso experimentally demonstrate the effectiveness of our procedure, by\nevaluating our proposed $d_{\\mathcal{X}}$-private Laplace mechanism on both\nsynthetic and real datasets using a set of randomly generated linear queries. \n\n"}
{"id": "1806.02865", "contents": "Title: Kernel Machines With Missing Responses Abstract: Missing responses is a missing data format in which outcomes are not always\nobserved. In this work we develop kernel machines that can handle missing\nresponses. First, we propose a kernel machine family that uses mainly the\ncomplete cases. For the quadratic loss, we then propose a family of\ndoubly-robust kernel machines. The proposed kernel-machine estimators can be\napplied to both regression and classification problems. We prove oracle\ninequalities for the finite-sample differences between the kernel machine risk\nand Bayes risk. We use these oracle inequalities to prove consistency and to\ncalculate convergence rates. We demonstrate the performance of the two proposed\nkernel machine families using both a simulation study and a real-world data\nanalysis. \n\n"}
{"id": "1806.02958", "contents": "Title: Efficient Full-Matrix Adaptive Regularization Abstract: Adaptive regularization methods pre-multiply a descent direction by a\npreconditioning matrix. Due to the large number of parameters of machine\nlearning problems, full-matrix preconditioning methods are prohibitively\nexpensive. We show how to modify full-matrix adaptive regularization in order\nto make it practical and effective. We also provide a novel theoretical\nanalysis for adaptive regularization in non-convex optimization settings. The\ncore of our algorithm, termed GGT, consists of the efficient computation of the\ninverse square root of a low-rank matrix. Our preliminary experiments show\nimproved iteration-wise convergence rates across synthetic tasks and standard\ndeep learning benchmarks, and that the more carefully-preconditioned steps\nsometimes lead to a better solution. \n\n"}
{"id": "1806.03185", "contents": "Title: Wave-U-Net: A Multi-Scale Neural Network for End-to-End Audio Source\n  Separation Abstract: Models for audio source separation usually operate on the magnitude spectrum,\nwhich ignores phase information and makes separation performance dependant on\nhyper-parameters for the spectral front-end. Therefore, we investigate\nend-to-end source separation in the time-domain, which allows modelling phase\ninformation and avoids fixed spectral transformations. Due to high sampling\nrates for audio, employing a long temporal input context on the sample level is\ndifficult, but required for high quality separation results because of\nlong-range temporal correlations. In this context, we propose the Wave-U-Net,\nan adaptation of the U-Net to the one-dimensional time domain, which repeatedly\nresamples feature maps to compute and combine features at different time\nscales. We introduce further architectural improvements, including an output\nlayer that enforces source additivity, an upsampling technique and a\ncontext-aware prediction framework to reduce output artifacts. Experiments for\nsinging voice separation indicate that our architecture yields a performance\ncomparable to a state-of-the-art spectrogram-based U-Net architecture, given\nthe same data. Finally, we reveal a problem with outliers in the currently used\nSDR evaluation metrics and suggest reporting rank-based statistics to alleviate\nthis problem. \n\n"}
{"id": "1806.03386", "contents": "Title: A Graph Model with Indirect Co-location Links Abstract: Graph models are widely used to analyse diffusion processes embedded in\nsocial contacts and to develop applications. A range of graph models are\navailable to replicate the underlying social structures and dynamics\nrealistically. However, most of the current graph models can only consider\nconcurrent interactions among individuals in the co-located interaction\nnetworks. However, they do not account for indirect interactions that can\ntransmit spreading items to individuals who visit the same locations at\ndifferent times but within a certain time limit. The diffusion phenomena\noccurring through direct and indirect interactions is called same place\ndifferent time (SPDT) diffusion. This paper introduces a model to synthesize\nco-located interaction graphs capturing both direct interactions, where\nindividuals meet at a location, and indirect interactions, where individuals\nvisit the same location at different times within a set timeframe. We analyze\n60 million location updates made by 2 million users from a social networking\napplication to characterize the graph properties, including the space-time\ncorrelations and its time evolving characteristics, such as bursty or ongoing\nbehaviors. The generated synthetic graph reproduces diffusion dynamics of a\nrealistic contact graph, and reduces the prediction error by up to 82% when\ncompare to other contact graph models demonstrating its potential for\nforecasting epidemic spread. \n\n"}
{"id": "1806.03920", "contents": "Title: Convergence Rates for Projective Splitting Abstract: Projective splitting is a family of methods for solving inclusions involving\nsums of maximal monotone operators. First introduced by Eckstein and Svaiter in\n2008, these methods have enjoyed significant innovation in recent years,\nbecoming one of the most flexible operator splitting frameworks available.\nWhile weak convergence of the iterates to a solution has been established,\nthere have been few attempts to study convergence rates of projective\nsplitting. The purpose of this paper is to do so under various assumptions. To\nthis end, there are three main contributions. First, in the context of convex\noptimization, we establish an $O(1/k)$ ergodic function convergence rate.\nSecond, for strongly monotone inclusions, strong convergence is established as\nwell as an ergodic $O(1/\\sqrt{k})$ convergence rate for the distance of the\niterates to the solution. Finally, for inclusions featuring strong monotonicity\nand cocoercivity, linear convergence is established. \n\n"}
{"id": "1806.04472", "contents": "Title: Trading algorithms with learning in latent alpha models Abstract: Alpha signals for statistical arbitrage strategies are often driven by latent\nfactors. This paper analyses how to optimally trade with latent factors that\ncause prices to jump and diffuse. Moreover, we account for the effect of the\ntrader's actions on quoted prices and the prices they receive from trading.\nUnder fairly general assumptions, we demonstrate how the trader can learn the\nposterior distribution over the latent states, and explicitly solve the latent\noptimal trading problem. We provide a verification theorem, and a methodology\nfor calibrating the model by deriving a variation of the\nexpectation-maximization algorithm. To illustrate the efficacy of the optimal\nstrategy, we demonstrate its performance through simulations and compare it to\nstrategies which ignore learning in the latent factors. We also provide\ncalibration results for a particular model using Intel Corporation stock as an\nexample. \n\n"}
{"id": "1806.04838", "contents": "Title: Partial AUC Maximization via Nonlinear Scoring Functions Abstract: We propose a method for maximizing a partial area under a receiver operating\ncharacteristic (ROC) curve (pAUC) for binary classification tasks. In binary\nclassification tasks, accuracy is the most commonly used as a measure of\nclassifier performance. In some applications such as anomaly detection and\ndiagnostic testing, accuracy is not an appropriate measure since prior\nprobabilties are often greatly biased. Although in such cases the pAUC has been\nutilized as a performance measure, few methods have been proposed for directly\nmaximizing the pAUC. This optimization is achieved by using a scoring function.\nThe conventional approach utilizes a linear function as the scoring function.\nIn contrast we newly introduce nonlinear scoring functions for this purpose.\nSpecifically, we present two types of nonlinear scoring functions based on\ngenerative models and deep neural networks. We show experimentally that\nnonlinear scoring fucntions improve the conventional methods through the\napplication of a binary classification of real and bogus objects obtained with\nthe Hyper Suprime-Cam on the Subaru telescope. \n\n"}
{"id": "1806.04910", "contents": "Title: Bilevel Programming for Hyperparameter Optimization and Meta-Learning Abstract: We introduce a framework based on bilevel programming that unifies\ngradient-based hyperparameter optimization and meta-learning. We show that an\napproximate version of the bilevel problem can be solved by taking into\nexplicit account the optimization dynamics for the inner objective. Depending\non the specific setting, the outer variables take either the meaning of\nhyperparameters in a supervised learning problem or parameters of a\nmeta-learner. We provide sufficient conditions under which solutions of the\napproximate problem converge to those of the exact problem. We instantiate our\napproach for meta-learning in the case of deep learning where representation\nlayers are treated as hyperparameters shared across a set of training episodes.\nIn experiments, we confirm our theoretical findings, present encouraging\nresults for few-shot learning and contrast the bilevel approach against\nclassical approaches for learning-to-learn. \n\n"}
{"id": "1806.05322", "contents": "Title: Infinite-dimensional bilinear and stochastic balanced truncation with\n  error bounds Abstract: Along the ideas of Curtain and Glover, we extend the balanced truncation\nmethod for infinite-dimensional linear systems to bilinear and stochastic\nsystems. Specifically , we apply Hilbert space techniques used in many-body\nquantum mechanics to establish error bounds for the truncated system and prove\nconvergence results. The functional analytic setting allows us to obtain mixed\nHardy space error bounds for both finite-and infinite-dimensional systems, and\nit is then applied to the model reduction of stochastic evolution equations\ndriven by Wiener noise. \n\n"}
{"id": "1806.05393", "contents": "Title: Dynamical Isometry and a Mean Field Theory of CNNs: How to Train\n  10,000-Layer Vanilla Convolutional Neural Networks Abstract: In recent years, state-of-the-art methods in computer vision have utilized\nincreasingly deep convolutional neural network architectures (CNNs), with some\nof the most successful models employing hundreds or even thousands of layers. A\nvariety of pathologies such as vanishing/exploding gradients make training such\ndeep networks challenging. While residual connections and batch normalization\ndo enable training at these depths, it has remained unclear whether such\nspecialized architecture designs are truly necessary to train deep CNNs. In\nthis work, we demonstrate that it is possible to train vanilla CNNs with ten\nthousand layers or more simply by using an appropriate initialization scheme.\nWe derive this initialization scheme theoretically by developing a mean field\ntheory for signal propagation and by characterizing the conditions for\ndynamical isometry, the equilibration of singular values of the input-output\nJacobian matrix. These conditions require that the convolution operator be an\northogonal transformation in the sense that it is norm-preserving. We present\nan algorithm for generating such random initial orthogonal convolution kernels\nand demonstrate empirically that they enable efficient training of extremely\ndeep architectures. \n\n"}
{"id": "1806.05490", "contents": "Title: Inference in Deep Gaussian Processes using Stochastic Gradient\n  Hamiltonian Monte Carlo Abstract: Deep Gaussian Processes (DGPs) are hierarchical generalizations of Gaussian\nProcesses that combine well calibrated uncertainty estimates with the high\nflexibility of multilayer models. One of the biggest challenges with these\nmodels is that exact inference is intractable. The current state-of-the-art\ninference method, Variational Inference (VI), employs a Gaussian approximation\nto the posterior distribution. This can be a potentially poor unimodal\napproximation of the generally multimodal posterior. In this work, we provide\nevidence for the non-Gaussian nature of the posterior and we apply the\nStochastic Gradient Hamiltonian Monte Carlo method to generate samples. To\nefficiently optimize the hyperparameters, we introduce the Moving Window MCEM\nalgorithm. This results in significantly better predictions at a lower\ncomputational cost than its VI counterpart. Thus our method establishes a new\nstate-of-the-art for inference in DGPs. \n\n"}
{"id": "1806.05618", "contents": "Title: Stochastic Variance-Reduced Policy Gradient Abstract: In this paper, we propose a novel reinforcement- learning algorithm\nconsisting in a stochastic variance-reduced version of policy gradient for\nsolving Markov Decision Processes (MDPs). Stochastic variance-reduced gradient\n(SVRG) methods have proven to be very successful in supervised learning.\nHowever, their adaptation to policy gradient is not straightforward and needs\nto account for I) a non-concave objective func- tion; II) approximations in the\nfull gradient com- putation; and III) a non-stationary sampling pro- cess. The\nresult is SVRPG, a stochastic variance- reduced policy gradient algorithm that\nleverages on importance weights to preserve the unbiased- ness of the gradient\nestimate. Under standard as- sumptions on the MDP, we provide convergence\nguarantees for SVRPG with a convergence rate that is linear under increasing\nbatch sizes. Finally, we suggest practical variants of SVRPG, and we\nempirically evaluate them on continuous MDPs. \n\n"}
{"id": "1806.05896", "contents": "Title: Interior Point Methods and Preconditioning for PDE-Constrained\n  Optimization Problems Involving Sparsity Terms Abstract: PDE-constrained optimization problems with control or state constraints are\nchallenging from an analytical as well as numerical perspective. The\ncombination of these constraints with a sparsity-promoting $\\rm L^1$ term\nwithin the objective function requires sophisticated optimization methods. We\npropose the use of an Interior Point scheme applied to a smoothed reformulation\nof the discretized problem, and illustrate that such a scheme exhibits robust\nperformance with respect to parameter changes. To increase the potency of this\nmethod we introduce fast and efficient preconditioners which enable us to solve\nproblems from a number of PDE applications in low iteration numbers and CPU\ntimes, even when the parameters involved are altered dramatically. \n\n"}
{"id": "1806.06123", "contents": "Title: On the Relationship between Data Efficiency and Error for Uncertainty\n  Sampling Abstract: While active learning offers potential cost savings, the actual data\nefficiency---the reduction in amount of labeled data needed to obtain the same\nerror rate---observed in practice is mixed. This paper poses a basic question:\nwhen is active learning actually helpful? We provide an answer for logistic\nregression with the popular active learning algorithm, uncertainty sampling.\nEmpirically, on 21 datasets from OpenML, we find a strong inverse correlation\nbetween data efficiency and the error rate of the final classifier.\nTheoretically, we show that for a variant of uncertainty sampling, the\nasymptotic data efficiency is within a constant factor of the inverse error\nrate of the limiting classifier. \n\n"}
{"id": "1806.06415", "contents": "Title: Feature Learning and Classification in Neuroimaging: Predicting\n  Cognitive Impairment from Magnetic Resonance Imaging Abstract: Due to the rapid innovation of technology and the desire to find and employ\nbiomarkers for neurodegenerative disease, high-dimensional data classification\nproblems are routinely encountered in neuroimaging studies. To avoid\nover-fitting and to explore relationships between disease and potential\nbiomarkers, feature learning and selection plays an important role in\nclassifier construction and is an important area in machine learning. In this\narticle, we review several important feature learning and selection techniques\nincluding lasso-based methods, PCA, the two-sample t-test, and stacked\nauto-encoders. We compare these approaches using a numerical study involving\nthe prediction of Alzheimer's disease from Magnetic Resonance Imaging. \n\n"}
{"id": "1806.06913", "contents": "Title: Deep Learning based Estimation of Weaving Target Maneuvers Abstract: In target tracking, the estimation of an unknown weaving target frequency is\ncrucial for improving the miss distance. The estimation process is commonly\ncarried out in a Kalman framework. The objective of this paper is to examine\nthe potential of using neural networks in target tracking applications. To that\nend, we propose estimating the weaving frequency using deep neural networks,\ninstead of classical Kalman framework based estimation. Particularly, we focus\non the case where a set of possible constant target frequencies is known.\nSeveral neural network architectures, requiring low computational resources\nwere designed to estimate the unknown frequency out of the known set of\nfrequencies. The proposed approach performance is compared with the multiple\nmodel adaptive estimation algorithm. Simulation results show that in the\nexamined scenarios, deep neural network outperforms multiple model adaptive\nestimation in terms of accuracy and the amount of required measurements to\nconvergence. \n\n"}
{"id": "1806.07528", "contents": "Title: Uncertainty in Multitask Transfer Learning Abstract: Using variational Bayes neural networks, we develop an algorithm capable of\naccumulating knowledge into a prior from multiple different tasks. The result\nis a rich and meaningful prior capable of few-shot learning on new tasks. The\nposterior can go beyond the mean field approximation and yields good\nuncertainty on the performed experiments. Analysis on toy tasks shows that it\ncan learn from significantly different tasks while finding similarities among\nthem. Experiments of Mini-Imagenet yields the new state of the art with 74.5%\naccuracy on 5 shot learning. Finally, we provide experiments showing that other\nexisting methods can fail to perform well in different benchmarks. \n\n"}
{"id": "1806.07808", "contents": "Title: Learning One-hidden-layer ReLU Networks via Gradient Descent Abstract: We study the problem of learning one-hidden-layer neural networks with\nRectified Linear Unit (ReLU) activation function, where the inputs are sampled\nfrom standard Gaussian distribution and the outputs are generated from a noisy\nteacher network. We analyze the performance of gradient descent for training\nsuch kind of neural networks based on empirical risk minimization, and provide\nalgorithm-dependent guarantees. In particular, we prove that tensor\ninitialization followed by gradient descent can converge to the ground-truth\nparameters at a linear rate up to some statistical error. To the best of our\nknowledge, this is the first work characterizing the recovery guarantee for\npractical learning of one-hidden-layer ReLU networks with multiple neurons.\nNumerical experiments verify our theoretical findings. \n\n"}
{"id": "1806.08836", "contents": "Title: Smart Inverter Grid Probing for Learning Loads: Part II - Probing\n  Injection Design Abstract: This two-part work puts forth the idea of engaging power electronics to probe\nan electric grid to infer non-metered loads. Probing can be accomplished by\ncommanding inverters to perturb their power injections and record the induced\nvoltage response. Once a probing setup is deemed topologically observable by\nthe tests of Part I, Part II provides a methodology for designing probing\ninjections abiding by inverter and network constraints to improve load\nestimates. The task is challenging since system estimates depend on both\nprobing injections and unknown loads in an implicit nonlinear fashion. The\nmethodology first constructs a library of candidate probing vectors by sampling\nover the feasible set of inverter injections. Leveraging a linearized grid\nmodel and a robust approach, the candidate probing vectors violating voltage\nconstraints for any anticipated load value are subsequently rejected. Among the\nqualified candidates, the design finally identifies the probing vectors\nyielding the most diverse system states. The probing task under noisy phasor\nand non-phasor data is tackled using a semidefinite-program (SDP) relaxation.\nNumerical tests using synthetic and real-world data on a benchmark feeder\nvalidate the conditions of Part I; the SDP-based solver; the importance of\nprobing design; and the effects of probing duration and noise. \n\n"}
{"id": "1806.09235", "contents": "Title: Towards a Better Understanding and Regularization of GAN Training\n  Dynamics Abstract: Generative adversarial networks (GANs) are notoriously difficult to train and\nthe reasons underlying their (non-)convergence behaviors are still not\ncompletely understood. By first considering a simple yet representative GAN\nexample, we mathematically analyze its local convergence behavior in a\nnon-asymptotic way. Furthermore, the analysis is extended to general GANs under\ncertain assumptions. We find that in order to ensure a good convergence rate,\ntwo factors of the Jacobian in the GAN training dynamics should be\nsimultaneously avoided, which are (i) the Phase Factor, i.e., the Jacobian has\ncomplex eigenvalues with a large imaginary-to-real ratio, and (ii) the\nConditioning Factor, i.e., the Jacobian is ill-conditioned. Previous methods of\nregularizing the Jacobian can only alleviate one of these two factors, while\nmaking the other more severe. Thus we propose a new JAcobian REgularization\n(JARE) for GANs, which simultaneously addresses both factors by construction.\nFinally, we conduct experiments that confirm our theoretical analysis and\ndemonstrate the advantages of JARE over previous methods in stabilizing GANs. \n\n"}
{"id": "1806.09277", "contents": "Title: Towards Optimal Transport with Global Invariances Abstract: Many problems in machine learning involve calculating correspondences between\nsets of objects, such as point clouds or images. Discrete optimal transport\nprovides a natural and successful approach to such tasks whenever the two sets\nof objects can be represented in the same space, or at least distances between\nthem can be directly evaluated. Unfortunately neither requirement is likely to\nhold when object representations are learned from data. Indeed, automatically\nderived representations such as word embeddings are typically fixed only up to\nsome global transformations, for example, reflection or rotation. As a result,\npairwise distances across two such instances are ill-defined without specifying\ntheir relative transformation. In this work, we propose a general framework for\noptimal transport in the presence of latent global transformations. We cast the\nproblem as a joint optimization over transport couplings and transformations\nchosen from a flexible class of invariances, propose algorithms to solve it,\nand show promising results in various tasks, including a popular unsupervised\nword translation benchmark. \n\n"}
{"id": "1806.09810", "contents": "Title: On Representer Theorems and Convex Regularization Abstract: We establish a general principle which states that regularizing an inverse\nproblem with a convex function yields solutions which are convex combinations\nof a small number of atoms. These atoms are identified with the extreme points\nand elements of the extreme rays of the regularizer level sets. An extension to\na broader class of quasi-convex regularizers is also discussed. As a side\nresult, we characterize the minimizers of the total gradient variation, which\nwas still an unresolved problem. \n\n"}
{"id": "1806.09929", "contents": "Title: Chance Constraints Integrated MPC Navigation in Uncertainty amongst\n  Dynamic Obstacles: An overlap of Gaussians approach Abstract: In this paper, we formulate a novel trajectory optimization scheme that takes\ninto consideration the state uncertainty of the robot and obstacle into its\ncollision avoidance routine. The collision avoidance under uncertainty is\nmodeled here as an overlap between two distributions that represent the state\nof the robot and obstacle respectively. We adopt the minmax procedure to\ncharacterize the area of overlap between two Gaussian distributions, and\ncompare it with the method of Bhattacharyya distance. We provide closed form\nexpressions that can characterize the overlap as a function of control. Our\nproposed algorithm can avoid overlapping uncertainty distributions in two\npossible ways. Firstly when a prescribed overlapping area that needs to be\navoided is posed as a confidence contour lower bound, control commands are\naccordingly realized through a MPC framework such that these bounds are\nrespected. Secondly in tight spaces control commands are computed such that the\noverlapping distribution respects a prescribed range of overlap characterized\nby lower and upper bounds of the confidence contours. We test our proposal with\nextensive set of simulations carried out under various constrained\nenvironmental configurations. We show usefulness of proposal under tight spaces\nwhere finding control maneuvers with minimal risk behavior becomes an\ninevitable task. \n\n"}
{"id": "1806.10792", "contents": "Title: Hierarchical Reinforcement Learning with Abductive Planning Abstract: One of the key challenges in applying reinforcement learning to real-life\nproblems is that the amount of train-and-error required to learn a good policy\nincreases drastically as the task becomes complex. One potential solution to\nthis problem is to combine reinforcement learning with automated symbol\nplanning and utilize prior knowledge on the domain. However, existing methods\nhave limitations in their applicability and expressiveness. In this paper we\npropose a hierarchical reinforcement learning method based on abductive\nsymbolic planning. The planner can deal with user-defined evaluation functions\nand is not based on the Herbrand theorem. Therefore it can utilize prior\nknowledge of the rewards and can work in a domain where the state space is\nunknown. We demonstrate empirically that our architecture significantly\nimproves learning efficiency with respect to the amount of training examples on\nthe evaluation domain, in which the state space is unknown and there exist\nmultiple goals. \n\n"}
{"id": "1806.11311", "contents": "Title: Guaranteed Deterministic Bounds on the Total Variation Distance between\n  Univariate Mixtures Abstract: The total variation distance is a core statistical distance between\nprobability measures that satisfies the metric axioms, with value always\nfalling in $[0,1]$. This distance plays a fundamental role in machine learning\nand signal processing: It is a member of the broader class of $f$-divergences,\nand it is related to the probability of error in Bayesian hypothesis testing.\nSince the total variation distance does not admit closed-form expressions for\nstatistical mixtures (like Gaussian mixture models), one often has to rely in\npractice on costly numerical integrations or on fast Monte Carlo approximations\nthat however do not guarantee deterministic lower and upper bounds. In this\nwork, we consider two methods for bounding the total variation of univariate\nmixture models: The first method is based on the information monotonicity\nproperty of the total variation to design guaranteed nested deterministic lower\nbounds. The second method relies on computing the geometric lower and upper\nenvelopes of weighted mixture components to derive deterministic bounds based\non density ratio. We demonstrate the tightness of our bounds in a series of\nexperiments on Gaussian, Gamma and Rayleigh mixture models. \n\n"}
{"id": "1807.00042", "contents": "Title: Neural Networks Trained to Solve Differential Equations Learn General\n  Representations Abstract: We introduce a technique based on the singular vector canonical correlation\nanalysis (SVCCA) for measuring the generality of neural network layers across a\ncontinuously-parametrized set of tasks. We illustrate this method by studying\ngenerality in neural networks trained to solve parametrized boundary value\nproblems based on the Poisson partial differential equation. We find that the\nfirst hidden layer is general, and that deeper layers are successively more\nspecific. Next, we validate our method against an existing technique that\nmeasures layer generality using transfer learning experiments. We find\nexcellent agreement between the two methods, and note that our method is much\nfaster, particularly for continuously-parametrized problems. Finally, we\nvisualize the general representations of the first layers, and interpret them\nas generalized coordinates over the input domain. \n\n"}
{"id": "1807.01251", "contents": "Title: Training behavior of deep neural network in frequency domain Abstract: Why deep neural networks (DNNs) capable of overfitting often generalize well\nin practice is a mystery [#zhang2016understanding]. To find a potential\nmechanism, we focus on the study of implicit biases underlying the training\nprocess of DNNs. In this work, for both real and synthetic datasets, we\nempirically find that a DNN with common settings first quickly captures the\ndominant low-frequency components, and then relatively slowly captures the\nhigh-frequency ones. We call this phenomenon Frequency Principle (F-Principle).\nThe F-Principle can be observed over DNNs of various structures, activation\nfunctions, and training algorithms in our experiments. We also illustrate how\nthe F-Principle help understand the effect of early-stopping as well as the\ngeneralization of DNNs. This F-Principle potentially provides insights into a\ngeneral principle underlying DNN optimization and generalization. \n\n"}
{"id": "1807.01430", "contents": "Title: SGAD: Soft-Guided Adaptively-Dropped Neural Network Abstract: Deep neural networks (DNNs) have been proven to have many redundancies.\nHence, many efforts have been made to compress DNNs. However, the existing\nmodel compression methods treat all the input samples equally while ignoring\nthe fact that the difficulties of various input samples being correctly\nclassified are different. To address this problem, DNNs with adaptive dropping\nmechanism are well explored in this work. To inform the DNNs how difficult the\ninput samples can be classified, a guideline that contains the information of\ninput samples is introduced to improve the performance. Based on the developed\nguideline and adaptive dropping mechanism, an innovative soft-guided\nadaptively-dropped (SGAD) neural network is proposed in this paper. Compared\nwith the 32 layers residual neural networks, the presented SGAD can reduce the\nFLOPs by 77% with less than 1% drop in accuracy on CIFAR-10. \n\n"}
{"id": "1807.02089", "contents": "Title: Linear Bandits with Stochastic Delayed Feedback Abstract: Stochastic linear bandits are a natural and well-studied model for structured\nexploration/exploitation problems and are widely used in applications such as\nonline marketing and recommendation. One of the main challenges faced by\npractitioners hoping to apply existing algorithms is that usually the feedback\nis randomly delayed and delays are only partially observable. For example,\nwhile a purchase is usually observable some time after the display, the\ndecision of not buying is never explicitly sent to the system. In other words,\nthe learner only observes delayed positive events. We formalize this problem as\na novel stochastic delayed linear bandit and propose ${\\tt OTFLinUCB}$ and\n${\\tt OTFLinTS}$, two computationally efficient algorithms able to integrate\nnew information as it becomes available and to deal with the permanently\ncensored feedback. We prove optimal $\\tilde O(\\smash{d\\sqrt{T}})$ bounds on the\nregret of the first algorithm and study the dependency on delay-dependent\nparameters. Our model, assumptions and results are validated by experiments on\nsimulated and real data. \n\n"}
{"id": "1807.02374", "contents": "Title: A Structured Prediction Approach for Label Ranking Abstract: We propose to solve a label ranking problem as a structured output regression\ntask. We adopt a least square surrogate loss approach that solves a supervised\nlearning problem in two steps: the regression step in a well-chosen feature\nspace and the pre-image step. We use specific feature maps/embeddings for\nranking data, which convert any ranking/permutation into a vector\nrepresentation. These embeddings are all well-tailored for our approach, either\nby resulting in consistent estimators, or by solving trivially the pre-image\nproblem which is often the bottleneck in structured prediction. We also propose\ntheir natural extension to the case of partial rankings and prove their\nefficiency on real-world datasets. \n\n"}
{"id": "1807.03064", "contents": "Title: Temporal Difference Learning with Neural Networks - Study of the Leakage\n  Propagation Problem Abstract: Temporal-Difference learning (TD) [Sutton, 1988] with function approximation\ncan converge to solutions that are worse than those obtained by Monte-Carlo\nregression, even in the simple case of on-policy evaluation. To increase our\nunderstanding of the problem, we investigate the issue of approximation errors\nin areas of sharp discontinuities of the value function being further\npropagated by bootstrap updates. We show empirical evidence of this leakage\npropagation, and show analytically that it must occur, in a simple Markov\nchain, when function approximation errors are present. For reversible policies,\nthe result can be interpreted as the tension between two terms of the loss\nfunction that TD minimises, as recently described by [Ollivier, 2018]. We show\nthat the upper bounds from [Tsitsiklis and Van Roy, 1997] hold, but they do not\nimply that leakage propagation occurs and under what conditions. Finally, we\ntest whether the problem could be mitigated with a better state representation,\nand whether it can be learned in an unsupervised manner, without rewards or\nprivileged information. \n\n"}
{"id": "1807.03650", "contents": "Title: On a Class of Stochastic Multilayer Networks Abstract: In this paper, we introduce a new class of stochastic multilayer networks. A\nstochastic multilayer network is the aggregation of $M$ networks (one per\nlayer) where each is a subgraph of a foundational network $G$. Each layer\nnetwork is the result of probabilistically removing links and nodes from $G$.\nThe resulting network includes any link that appears in at least $K$ layers.\nThis model is an instance of a non-standard site-bond percolation model. Two\nsets of results are obtained: first, we derive the probability distribution\nthat the $M$-layer network is in a given configuration for some particular\ngraph structures (explicit results are provided for a line and an algorithm is\nprovided for a tree), where a configuration is the collective state of all\nlinks (each either active or inactive). Next, we show that for appropriate\nscalings of the node and link selection processes in a layer, links are\nasymptotically independent as the number of layers goes to infinity, and follow\nPoisson distributions. Numerical results are provided to highlight the impact\nof having several layers on some metrics of interest (including expected size\nof the cluster a node belongs to in the case of the line). This model finds\napplications in wireless communication networks with multichannel radios,\nmultiple social networks with overlapping memberships, transportation networks,\nand, more generally, in any scenario where a common set of nodes can be linked\nvia co-existing means of connectivity. \n\n"}
{"id": "1807.03765", "contents": "Title: Is Q-learning Provably Efficient? Abstract: Model-free reinforcement learning (RL) algorithms, such as Q-learning,\ndirectly parameterize and update value functions or policies without explicitly\nmodeling the environment. They are typically simpler, more flexible to use, and\nthus more prevalent in modern deep RL than model-based approaches. However,\nempirical work has suggested that model-free algorithms may require more\nsamples to learn [Deisenroth and Rasmussen 2011, Schulman et al. 2015]. The\ntheoretical question of \"whether model-free algorithms can be made sample\nefficient\" is one of the most fundamental questions in RL, and remains unsolved\neven in the basic scenario with finitely many states and actions.\n  We prove that, in an episodic MDP setting, Q-learning with UCB exploration\nachieves regret $\\tilde{O}(\\sqrt{H^3 SAT})$, where $S$ and $A$ are the numbers\nof states and actions, $H$ is the number of steps per episode, and $T$ is the\ntotal number of steps. This sample efficiency matches the optimal regret that\ncan be achieved by any model-based approach, up to a single $\\sqrt{H}$ factor.\nTo the best of our knowledge, this is the first analysis in the model-free\nsetting that establishes $\\sqrt{T}$ regret without requiring access to a\n\"simulator.\" \n\n"}
{"id": "1807.03929", "contents": "Title: Quantification under prior probability shift: the ratio estimator and\n  its extensions Abstract: The quantification problem consists of determining the prevalence of a given\nlabel in a target population. However, one often has access to the labels in a\nsample from the training population but not in the target population. A common\nassumption in this situation is that of prior probability shift, that is, once\nthe labels are known, the distribution of the features is the same in the\ntraining and target populations. In this paper, we derive a new lower bound for\nthe risk of the quantification problem under the prior shift assumption.\nComplementing this lower bound, we present a new approximately minimax class of\nestimators, ratio estimators, which generalize several previous proposals in\nthe literature. Using a weaker version of the prior shift assumption, which can\nbe tested, we show that ratio estimators can be used to build confidence\nintervals for the quantification problem. We also extend the ratio estimator so\nthat it can: (i) incorporate labels from the target population, when they are\navailable and (ii) estimate how the prevalence of positive labels varies\naccording to a function of certain covariates. \n\n"}
{"id": "1807.04199", "contents": "Title: Optimal control problems with oscillations, concentrations and\n  discontinuities Abstract: Optimal control problems with oscillations (chattering controls) and\nconcentrations (impulsive controls) can have integral performance criteria such\nthat concentration of the control signal occurs at a discontinuity of the state\nsignal. Techniques from functional analysis (anisotropic parametrized measures)\nare applied to give a precise meaning of the integral cost and to allow for the\nsound application of numerical methods. We show how this can be combined with\nthe Lasserre hierarchy of semidefinite programming relaxations. \n\n"}
{"id": "1807.04481", "contents": "Title: A note on approximating the nearest stable discrete-time descriptor\n  system with fixed rank Abstract: Consider a discrete-time linear time-invariant descriptor system\n$Ex(k+1)=Ax(k)$ for $k \\in \\mathbb Z_{+}$. In this paper, we tackle for the\nfirst time the problem of stabilizing such systems by computing a nearby\nregular index one stable system $\\hat E x(k+1)= \\hat A x(k)$ with\n$\\text{rank}(\\hat E)=r$. We reformulate this highly nonconvex problem into an\nequivalent optimization problem with a relatively simple feasible set onto\nwhich it is easy to project. This allows us to employ a block coordinate\ndescent method to obtain a nearby regular index one stable system. We\nillustrate the effectiveness of the algorithm on several examples. \n\n"}
{"id": "1807.04715", "contents": "Title: Orthogonal Matching Pursuit for Text Classification Abstract: In text classification, the problem of overfitting arises due to the high\ndimensionality, making regularization essential. Although classic regularizers\nprovide sparsity, they fail to return highly accurate models. On the contrary,\nstate-of-the-art group-lasso regularizers provide better results at the expense\nof low sparsity. In this paper, we apply a greedy variable selection algorithm,\ncalled Orthogonal Matching Pursuit, for the text classification task. We also\nextend standard group OMP by introducing overlapping Group OMP to handle\noverlapping groups of features. Empirical analysis verifies that both OMP and\noverlapping GOMP constitute powerful regularizers, able to produce effective\nand very sparse models. Code and data are available online:\nhttps://github.com/y3nk0/OMP-for-Text-Classification . \n\n"}
{"id": "1807.05185", "contents": "Title: Model Reconstruction from Model Explanations Abstract: We show through theory and experiment that gradient-based explanations of a\nmodel quickly reveal the model itself. Our results speak to a tension between\nthe desire to keep a proprietary model secret and the ability to offer model\nexplanations. On the theoretical side, we give an algorithm that provably\nlearns a two-layer ReLU network in a setting where the algorithm may query the\ngradient of the model with respect to chosen inputs. The number of queries is\nindependent of the dimension and nearly optimal in its dependence on the model\nsize. Of interest not only from a learning-theoretic perspective, this result\nhighlights the power of gradients rather than labels as a learning primitive.\nComplementing our theory, we give effective heuristics for reconstructing\nmodels from gradient explanations that are orders of magnitude more\nquery-efficient than reconstruction attacks relying on prediction interfaces. \n\n"}
{"id": "1807.05832", "contents": "Title: Manifold Adversarial Learning Abstract: Recently proposed adversarial training methods show the robustness to both\nadversarial and original examples and achieve state-of-the-art results in\nsupervised and semi-supervised learning. All the existing adversarial training\nmethods consider only how the worst perturbed examples (i.e., adversarial\nexamples) could affect the model output. Despite their success, we argue that\nsuch setting may be in lack of generalization, since the output space (or label\nspace) is apparently less informative.In this paper, we propose a novel method,\ncalled Manifold Adversarial Training (MAT). MAT manages to build an adversarial\nframework based on how the worst perturbation could affect the distributional\nmanifold rather than the output space. Particularly, a latent data space with\nthe Gaussian Mixture Model (GMM) will be first derived.On one hand, MAT tries\nto perturb the input samples in the way that would rough the distributional\nmanifold the worst. On the other hand, the deep learning model is trained\ntrying to promote in the latent space the manifold smoothness, measured by the\nvariation of Gaussian mixtures (given the local perturbation around the data\npoint). Importantly, since the latent space is more informative than the output\nspace, the proposed MAT can learn better a robust and compact data\nrepresentation, leading to further performance improvement. The proposed MAT is\nimportant in that it can be considered as a superset of one recently-proposed\ndiscriminative feature learning approach called center loss. We conducted a\nseries of experiments in both supervised and semi-supervised learning on three\nbenchmark data sets, showing that the proposed MAT can achieve remarkable\nperformance, much better than those of the state-of-the-art adversarial\napproaches. We also present a series of visualization which could generate\nfurther understanding or explanation on adversarial examples. \n\n"}
{"id": "1807.07132", "contents": "Title: Newton-ADMM: A Distributed GPU-Accelerated Optimizer for Multiclass\n  Classification Problems Abstract: First-order optimization methods, such as stochastic gradient descent (SGD)\nand its variants, are widely used in machine learning applications due to their\nsimplicity and low per-iteration costs. However, they often require larger\nnumbers of iterations, with associated communication costs in distributed\nenvironments. In contrast, Newton-type methods, while having higher\nper-iteration costs, typically require a significantly smaller number of\niterations, which directly translates to reduced communication costs. In this\npaper, we present a novel distributed optimizer for classification problems,\nwhich integrates a GPU-accelerated Newton-type solver with the global consensus\nformulation of Alternating Direction of Method Multipliers (ADMM). By\nleveraging the communication efficiency of ADMM, GPU-accelerated inexact-Newton\nsolver, and an effective spectral penalty parameter selection strategy, we show\nthat our proposed method (i) yields better generalization performance on\nseveral classification problems; (ii) significantly outperforms\nstate-of-the-art methods in distributed time to solution; and (iii) offers\nbetter scaling on large distributed platforms. \n\n"}
{"id": "1807.07761", "contents": "Title: Controllability of Social Networks and the Strategic Use of Random\n  Information Abstract: This work is aimed at studying realistic social control strategies for social\nnetworks based on the introduction of random information into the state of\nselected driver agents. Deliberately exposing selected agents to random\ninformation is a technique already experimented in recommender systems or\nsearch engines, and represents one of the few options for influencing the\nbehavior of a social context that could be accepted as ethical, could be fully\ndisclosed to members, and does not involve the use of force or of deception.\nOur research is based on a model of knowledge diffusion applied to a\ntime-varying adaptive network, and considers two well-known strategies for\ninfluencing social contexts. One is the selection of few influencers for\nmanipulating their actions in order to drive the whole network to a certain\nbehavior; the other, instead, drives the network behavior acting on the state\nof a large subset of ordinary, scarcely influencing users. The two approaches\nhave been studied in terms of network and diffusion effects. The network effect\nis analyzed through the changes induced on network average degree and\nclustering coefficient, while the diffusion effect is based on two ad-hoc\nmetrics defined to measure the degree of knowledge diffusion and skill level,\nas well as the polarization of agent interests. The results, obtained through\nsimulations on synthetic networks, show a rich dynamics and strong effects on\nthe communication structure and on the distribution of knowledge and skills,\nsupporting our hypothesis that the strategic use of random information could\nrepresent a realistic approach to social network controllability, and that with\nboth strategies, in principle, the control effect could be remarkable. \n\n"}
{"id": "1807.08518", "contents": "Title: Implementing Neural Turing Machines Abstract: Neural Turing Machines (NTMs) are an instance of Memory Augmented Neural\nNetworks, a new class of recurrent neural networks which decouple computation\nfrom memory by introducing an external memory unit. NTMs have demonstrated\nsuperior performance over Long Short-Term Memory Cells in several sequence\nlearning tasks. A number of open source implementations of NTMs exist but are\nunstable during training and/or fail to replicate the reported performance of\nNTMs. This paper presents the details of our successful implementation of a\nNTM. Our implementation learns to solve three sequential learning tasks from\nthe original NTM paper. We find that the choice of memory contents\ninitialization scheme is crucial in successfully implementing a NTM. Networks\nwith memory contents initialized to small constant values converge on average 2\ntimes faster than the next best memory contents initialization scheme. \n\n"}
{"id": "1807.09659", "contents": "Title: A Surprising Linear Relationship Predicts Test Performance in Deep\n  Networks Abstract: Given two networks with the same training loss on a dataset, when would they\nhave drastically different test losses and errors? Better understanding of this\nquestion of generalization may improve practical applications of deep networks.\nIn this paper we show that with cross-entropy loss it is surprisingly simple to\ninduce significantly different generalization performances for two networks\nthat have the same architecture, the same meta parameters and the same training\nerror: one can either pretrain the networks with different levels of\n\"corrupted\" data or simply initialize the networks with weights of different\nGaussian standard deviations. A corollary of recent theoretical results on\noverfitting shows that these effects are due to an intrinsic problem of\nmeasuring test performance with a cross-entropy/exponential-type loss, which\ncan be decomposed into two components both minimized by SGD -- one of which is\nnot related to expected classification performance. However, if we factor out\nthis component of the loss, a linear relationship emerges between training and\ntest losses. Under this transformation, classical generalization bounds are\nsurprisingly tight: the empirical/training loss is very close to the\nexpected/test loss. Furthermore, the empirical relation between classification\nerror and normalized cross-entropy loss seem to be approximately monotonic \n\n"}
{"id": "1807.10251", "contents": "Title: Aggregated Learning: A Deep Learning Framework Based on\n  Information-Bottleneck Vector Quantization Abstract: Based on the notion of information bottleneck (IB), we formulate a\nquantization problem called \"IB quantization\". We show that IB quantization is\nequivalent to learning based on the IB principle. Under this equivalence, the\nstandard neural network models can be viewed as scalar (single sample) IB\nquantizers. It is known, from conventional rate-distortion theory, that scalar\nquantizers are inferior to vector (multi-sample) quantizers. Such a deficiency\nthen inspires us to develop a novel learning framework, AgrLearn, that\ncorresponds to vector IB quantizers for learning with neural networks. Unlike\nstandard networks, AgrLearn simultaneously optimizes against multiple data\nsamples. We experimentally verify that AgrLearn can result in significant\nimprovements when applied to several current deep learning architectures for\nimage recognition and text classification. We also empirically show that\nAgrLearn can reduce up to 80% of the training samples needed for ResNet\ntraining. \n\n"}
{"id": "1807.10263", "contents": "Title: Are extreme dissipation events predictable in turbulent fluid flows? Abstract: We derive precursors of extreme dissipation events in a turbulent channel\nflow. Using a recently developed method that combines dynamics and statistics\nfor the underlying attractor, we extract a characteristic state that precedes\nlaminarization events that subsequently lead to extreme dissipation episodes.\nOur approach utilizes coarse statistical information for the turbulent\nattractor, in the form of second order statistics, to identify high-likelihood\nregions in the state space. We then search within this high probability\nmanifold for the state that leads to the most finite-time growth of the flow\nkinetic energy. This state has both high probability of occurrence and leads to\nextreme values of dissipation. We use the alignment between a given turbulent\nstate and this critical state as a precursor for extreme events and demonstrate\nits favorable properties for prediction of extreme dissipation events. Finally,\nwe analyze the physical relevance of the derived precursor and show its robust\ncharacter for different Reynolds numbers. Overall, we find that our choice of\nprecursor works well at the Reynolds number it is computed at and at higher\nReynolds number flows with similar extreme events. \n\n"}
{"id": "1808.00076", "contents": "Title: News Session-Based Recommendations using Deep Neural Networks Abstract: News recommender systems are aimed to personalize users experiences and help\nthem to discover relevant articles from a large and dynamic search space.\nTherefore, news domain is a challenging scenario for recommendations, due to\nits sparse user profiling, fast growing number of items, accelerated item's\nvalue decay, and users preferences dynamic shift. Some promising results have\nbeen recently achieved by the usage of Deep Learning techniques on Recommender\nSystems, specially for item's feature extraction and for session-based\nrecommendations with Recurrent Neural Networks. In this paper, it is proposed\nan instantiation of the CHAMELEON -- a Deep Learning Meta-Architecture for News\nRecommender Systems. This architecture is composed of two modules, the first\nresponsible to learn news articles representations, based on their text and\nmetadata, and the second module aimed to provide session-based recommendations\nusing Recurrent Neural Networks. The recommendation task addressed in this work\nis next-item prediction for users sessions: \"what is the next most likely\narticle a user might read in a session?\" Users sessions context is leveraged by\nthe architecture to provide additional information in such extreme cold-start\nscenario of news recommendation. Users' behavior and item features are both\nmerged in an hybrid recommendation approach. A temporal offline evaluation\nmethod is also proposed as a complementary contribution, for a more realistic\nevaluation of such task, considering dynamic factors that affect global\nreadership interests like popularity, recency, and seasonality. Experiments\nwith an extensive number of session-based recommendation methods were performed\nand the proposed instantiation of CHAMELEON meta-architecture obtained a\nsignificant relative improvement in top-n accuracy and ranking metrics (10% on\nHit Rate and 13% on MRR) over the best benchmark methods. \n\n"}
{"id": "1808.00380", "contents": "Title: A Differentially Private Kernel Two-Sample Test Abstract: Kernel two-sample testing is a useful statistical tool in determining whether\ndata samples arise from different distributions without imposing any parametric\nassumptions on those distributions. However, raw data samples can expose\nsensitive information about individuals who participate in scientific studies,\nwhich makes the current tests vulnerable to privacy breaches. Hence, we design\na new framework for kernel two-sample testing conforming to differential\nprivacy constraints, in order to guarantee the privacy of subjects in the data.\nUnlike existing differentially private parametric tests that simply add noise\nto data, kernel-based testing imposes a challenge due to a complex dependence\nof test statistics on the raw data, as these statistics correspond to\nestimators of distances between representations of probability measures in\nHilbert spaces. Our approach considers finite dimensional approximations to\nthose representations. As a result, a simple chi-squared test is obtained,\nwhere a test statistic depends on a mean and covariance of empirical\ndifferences between the samples, which we perturb for a privacy guarantee. We\ninvestigate the utility of our framework in two realistic settings and conclude\nthat our method requires only a relatively modest increase in sample size to\nachieve a similar level of power to the non-private tests in both settings. \n\n"}
{"id": "1808.00668", "contents": "Title: On the achievability of blind source separation for high-dimensional\n  nonlinear source mixtures Abstract: For many years, a combination of principal component analysis (PCA) and\nindependent component analysis (ICA) has been used for blind source separation\n(BSS). However, it remains unclear why these linear methods work well with\nreal-world data that involve nonlinear source mixtures. This work theoretically\nvalidates that a cascade of linear PCA and ICA can solve a nonlinear BSS\nproblem accurately -- when the sensory inputs are generated from hidden sources\nvia nonlinear mappings with sufficient dimensionality. Our proposed theorem,\ntermed the asymptotic linearization theorem, theoretically guarantees that\napplying linear PCA to the inputs can reliably extract a subspace spanned by\nthe linear projections from every hidden source as the major components -- and\nthus projecting the inputs onto their major eigenspace can effectively recover\na linear transformation of the hidden sources. Then, subsequent application of\nlinear ICA can separate all the true independent hidden sources accurately.\nZero-element-wise-error nonlinear BSS is asymptotically attained when the\nsource dimensionality is large and the input dimensionality is sufficiently\nlarger than the source dimensionality. Our proposed theorem is validated\nanalytically and numerically. Moreover, the same computation can be performed\nby using Hebbian-like plasticity rules, implying the biological plausibility of\nthis nonlinear BSS strategy. Our results highlight the utility of linear PCA\nand ICA for accurately and reliably recovering nonlinearly mixed sources -- and\nfurther suggest the importance of employing sensors with sufficient\ndimensionality to identify true hidden sources of real-world data. \n\n"}
{"id": "1808.00831", "contents": "Title: Efficient Bayesian Inference of Sigmoidal Gaussian Cox Processes Abstract: We present an approximate Bayesian inference approach for estimating the\nintensity of an inhomogeneous Poisson process, where the intensity function is\nmodelled using a Gaussian process (GP) prior via a sigmoid link function.\nAugmenting the model using a latent marked Poisson process and P\\'olya--Gamma\nrandom variables we obtain a representation of the likelihood which is\nconjugate to the GP prior. We estimate the posterior using a variational\nfree--form mean field optimisation together with the framework of sparse GPs.\nFurthermore, as alternative approximation we suggest a sparse Laplace's method\nfor the posterior, for which an efficient expectation--maximisation algorithm\nis derived to find the posterior's mode. Both algorithms compare well against\nexact inference obtained by a Markov Chain Monte Carlo sampler and standard\nvariational Gauss approach solving the same model, while being one order of\nmagnitude faster. Furthermore, the performance and speed of our method is\ncompetitive with that of another recently proposed Poisson process model based\non a quadratic link function, while not being limited to GPs with squared\nexponential kernels and rectangular domains. \n\n"}
{"id": "1808.02530", "contents": "Title: Randomized sketch descent methods for non-separable linearly constrained\n  optimization Abstract: In this paper we consider large-scale smooth optimization problems with\nmultiple linear coupled constraints. Due to the non-separability of the\nconstraints, arbitrary random sketching would not be guaranteed to work. Thus,\nwe first investigate necessary and sufficient conditions for the sketch\nsampling to have well-defined algorithms. Based on these sampling conditions we\ndeveloped new sketch descent methods for solving general smooth linearly\nconstrained problems, in particular, random sketch descent and accelerated\nrandom sketch descent methods. From our knowledge, this is the first\nconvergence analysis of random sketch descent algorithms for optimization\nproblems with multiple non-separable linear constraints. For the general case,\nwhen the objective function is smooth and non-convex, we prove for the\nnon-accelerated variant sublinear rate in expectation for an appropriate\noptimality measure. In the smooth convex case, we derive for both algorithms,\nnon-accelerated and accelerated random sketch descent, sublinear convergence\nrates in the expected values of the objective function. Additionally, if the\nobjective function satisfies a strong convexity type condition, both algorithms\nconverge linearly in expectation. In special cases, where complexity bounds are\nknown for some particular sketching algorithms, such as coordinate descent\nmethods for optimization problems with a single linear coupled constraint, our\ntheory recovers the best-known bounds. We also show that when random sketch is\nsketching the coordinate directions randomly produces better results than the\nfixed selection rule. Finally, we present some numerical examples to illustrate\nthe performances of our new algorithms. \n\n"}
{"id": "1808.02745", "contents": "Title: On the convergence of closed-loop Nash equilibria to the mean field game\n  limit Abstract: This paper continues the study of the mean field game (MFG) convergence\nproblem: In what sense do the Nash equilibria of $n$-player stochastic\ndifferential games converge to the mean field game as $n\\rightarrow\\infty$?\nPrevious work on this problem took two forms. First, when the $n$-player\nequilibria are open-loop, compactness arguments permit a characterization of\nall limit points of $n$-player equilibria as weak MFG equilibria, which contain\nadditional randomness compared to the standard (strong) equilibrium concept. On\nthe other hand, when the $n$-player equilibria are closed-loop, the convergence\nto the MFG equilibrium is known only when the MFG equilibrium is unique and the\nassociated \"master equation\" is solvable and sufficiently smooth. This paper\nadapts the compactness arguments to the closed-loop case, proving a convergence\ntheorem that holds even when the MFG equilibrium is non-unique. Every limit\npoint of $n$-player equilibria is shown to be the same kind of weak MFG\nequilibrium as in the open-loop case. Some partial results and examples are\ndiscussed for the converse question, regarding which of the weak MFG equilibria\ncan arise as the limit of $n$-player (approximate) equilibria. \n\n"}
{"id": "1808.02933", "contents": "Title: Sequential Monte Carlo Bandits Abstract: We extend Bayesian multi-armed bandit (MAB) algorithms beyond their original\nsetting by making use of sequential Monte Carlo (SMC) methods.\n  A MAB is a sequential decision making problem where the goal is to learn a\npolicy that maximizes long term payoff, where only the reward of the executed\naction is observed. In the stochastic MAB, the reward for each action is\ngenerated from an unknown distribution, often assumed to be stationary. To\ndecide which action to take next, a MAB agent must learn the characteristics of\nthe unknown reward distribution, e.g., compute its sufficient statistics.\nHowever, closed-form expressions for these statistics are analytically\nintractable except for simple, stationary cases.\n  We here utilize SMC for estimation of the statistics Bayesian MAB agents\ncompute, and devise flexible policies that can address a rich class of bandit\nproblems: i.e., MABs with nonlinear, stateless- and context-dependent reward\ndistributions that evolve over time. We showcase how non-stationary bandits,\nwhere time dynamics are modeled via linear dynamical systems, can be\nsuccessfully addressed by SMC-based Bayesian bandit agents. We empirically\ndemonstrate good regret performance of the proposed SMC-based bandit policies\nin several MAB scenarios that have remained elusive, i.e., in non-stationary\nbandits with nonlinear rewards. \n\n"}
{"id": "1808.03408", "contents": "Title: A Unified Analysis of AdaGrad with Weighted Aggregation and Momentum\n  Acceleration Abstract: Integrating adaptive learning rate and momentum techniques into SGD leads to\na large class of efficiently accelerated adaptive stochastic algorithms, such\nas AdaGrad, RMSProp, Adam, AccAdaGrad, \\textit{etc}. In spite of their\neffectiveness in practice, there is still a large gap in their theories of\nconvergences, especially in the difficult non-convex stochastic setting. To\nfill this gap, we propose \\emph{weighted AdaGrad with unified momentum}, dubbed\nAdaUSM, which has the main characteristics that (1) it incorporates a unified\nmomentum scheme which covers both the heavy ball momentum and the Nesterov\naccelerated gradient momentum; (2) it adopts a novel weighted adaptive learning\nrate that can unify the learning rates of AdaGrad, AccAdaGrad, Adam, and\nRMSProp. Moreover, when we take polynomially growing weights in AdaUSM, we\nobtain its $\\mathcal{O}(\\log(T)/\\sqrt{T})$ convergence rate in the non-convex\nstochastic setting. We also show that the adaptive learning rates of Adam and\nRMSProp correspond to taking exponentially growing weights in AdaUSM, thereby\nproviding a new perspective for understanding Adam and RMSProp. Lastly,\ncomparative experiments of AdaUSM against SGD with momentum, AdaGrad, AdaEMA,\nAdam, and AMSGrad on various deep learning models and datasets are also carried\nout. \n\n"}
{"id": "1808.03994", "contents": "Title: Time-Varying Semidefinite Programs Abstract: We study time-varying semidefinite programs (TV-SDPs), which are semidefinite\nprograms whose data (and solutions) are functions of time. Our focus is on the\nsetting where the data varies polynomially with time. We show that under a\nstrict feasibility assumption, restricting the solutions to also be polynomial\nfunctions of time does not change the optimal value of the TV-SDP. Moreover, by\nusing a Positivstellensatz on univariate polynomial matrices, we show that the\nbest polynomial solution of a given degree to a TV-SDP can be found by solving\na semidefinite program of tractable size. We also provide a sequence of dual\nproblems which can be cast as SDPs and that give upper bounds on the optimal\nvalue of a TV-SDP (in maximization form). We prove that under a boundedness\nassumption, this sequence of upper bounds converges to the optimal value of the\nTV-SDP. Under the same assumption, we also show that the optimal value of the\nTV-SDP is attained. We demonstrate the efficacy of our algorithms on a\nmaximum-flow problem with time-varying edge capacities, a wireless coverage\nproblem with time-varying coverage requirements, and on bi-objective\nsemidefinite optimization where the goal is to approximate the Pareto curve in\none shot. \n\n"}
{"id": "1808.04162", "contents": "Title: A Forward-Backward Splitting Method for Monotone Inclusions Without\n  Cocoercivity Abstract: In this work, we propose a simple modification of the forward-backward\nsplitting method for finding a zero in the sum of two monotone operators. Our\nmethod converges under the same assumptions as Tseng's forward-backward-forward\nmethod, namely, it does not require cocoercivity of the single-valued operator.\nMoreover, each iteration only requires one forward evaluation rather than two\nas is the case for Tseng's method. Variants of the method incorporating a\nlinesearch, relaxation and inertia, or a structured three operator inclusion\nare also discussed. \n\n"}
{"id": "1808.04318", "contents": "Title: A simple counterexample to the Monge ansatz in multi-marginal optimal\n  transport, convex geometry of the set of Kantorovich plans, and the\n  Frenkel-Kontorova model Abstract: It is known from clever mathematical examples \\cite{Ca10} that the Monge\nansatz may fail in continuous two-marginal optimal transport (alias optimal\ncoupling alias optimal assignment) problems. Here we show that this effect\nalready occurs for finite assignment problems with $N=3$ marginals, $\\ell=3$\n'sites', and symmetric pairwise costs, with the values for $N$ and $\\ell$ both\nbeing optimal. Our counterexample is a transparent consequence of the convex\ngeometry of the set of symmetric Kantorovich plans for $N=\\ell=3$, which -- as\nwe show -- possess 22 extreme points, only 7 of which are Monge. These extreme\npoints have a simple physical meaning as irreducible molecular packings, and\nthe example corresponds to finding the minimum energy packing for\nFrenkel-Kontorova interactions. Our finite example naturally gives rise, by\nsuperposition, to a continuous one, where failure of the Monge ansatz manifests\nitself as nonattainment and formation of 'microstructure'. \n\n"}
{"id": "1808.04475", "contents": "Title: Kernel Flows: from learning kernels from data into the abyss Abstract: Learning can be seen as approximating an unknown function by interpolating\nthe training data. Kriging offers a solution to this problem based on the prior\nspecification of a kernel. We explore a numerical approximation approach to\nkernel selection/construction based on the simple premise that a kernel must be\ngood if the number of interpolation points can be halved without significant\nloss in accuracy (measured using the intrinsic RKHS norm $\\|\\cdot\\|$ associated\nwith the kernel). We first test and motivate this idea on a simple problem of\nrecovering the Green's function of an elliptic PDE (with inhomogeneous\ncoefficients) from the sparse observation of one of its solutions. Next we\nconsider the problem of learning non-parametric families of deep kernels of the\nform $K_1(F_n(x),F_n(x'))$ with $F_{n+1}=(I_d+\\epsilon G_{n+1})\\circ F_n$ and\n$G_{n+1} \\in \\operatorname{Span}\\{K_1(F_n(x_i),\\cdot)\\}$. With the proposed\napproach constructing the kernel becomes equivalent to integrating a stochastic\ndata driven dynamical system, which allows for the training of very deep\n(bottomless) networks and the exploration of their properties. These networks\nlearn by constructing flow maps in the kernel and input spaces via incremental\ndata-dependent deformations/perturbations (appearing as the cooperative\ncounterpart of adversarial examples) and, at profound depths, they (1) can\nachieve accurate classification from only one data point per class (2) appear\nto learn archetypes of each class (3) expand distances between points that are\nin different classes and contract distances between points in the same class.\nFor kernels parameterized by the weights of Convolutional Neural Networks,\nminimizing approximation errors incurred by halving random subsets of\ninterpolation points, appears to outperform training (the same CNN\narchitecture) with relative entropy and dropout. \n\n"}
{"id": "1808.05274", "contents": "Title: Frank-Wolfe Style Algorithms for Large Scale Optimization Abstract: We introduce a few variants on Frank-Wolfe style algorithms suitable for\nlarge scale optimization. We show how to modify the standard Frank-Wolfe\nalgorithm using stochastic gradients, approximate subproblem solutions, and\nsketched decision variables in order to scale to enormous problems while\npreserving (up to constants) the optimal convergence rate\n$\\mathcal{O}(\\frac{1}{k})$. \n\n"}
{"id": "1808.05776", "contents": "Title: Optimum Experimental Design for Interface Identification Problems Abstract: The identification of the interface of an inclusion in a diffusion process is\nconsidered. This task is viewed as a parameter identification problem in which\nthe parameter space bears the structure of a shape manifold. A corresponding\noptimum experimental design (OED) problem is formulated in which the activation\npattern of an array of sensors in space and time serves as experimental\ncondition. The goal is to improve the estimation precision within a certain\nsubspace of the infinite dimensional tangent space of shape variations to the\nmanifold, and to find those shape variations of best and worst identifiability.\nNumerical results for the OED problem obtained by a simplicial decomposition\nalgorithm are presented. \n\n"}
{"id": "1808.05784", "contents": "Title: Multiview Boosting by Controlling the Diversity and the Accuracy of\n  View-specific Voters Abstract: In this paper we propose a boosting based multiview learning algorithm,\nreferred to as PB-MVBoost, which iteratively learns i) weights over\nview-specific voters capturing view-specific information; and ii) weights over\nviews by optimizing a PAC-Bayes multiview C-Bound that takes into account the\naccuracy of view-specific classifiers and the diversity between the views. We\nderive a generalization bound for this strategy following the PAC-Bayes theory\nwhich is a suitable tool to deal with models expressed as weighted combination\nover a set of voters. Different experiments on three publicly available\ndatasets show the efficiency of the proposed approach with respect to\nstate-of-art models. \n\n"}
{"id": "1808.05904", "contents": "Title: Correlated Multi-armed Bandits with a Latent Random Source Abstract: We consider a novel multi-armed bandit framework where the rewards obtained\nby pulling the arms are functions of a common latent random variable. The\ncorrelation between arms due to the common random source can be used to design\na generalized upper-confidence-bound (UCB) algorithm that identifies certain\narms as $non-competitive$, and avoids exploring them. As a result, we reduce a\n$K$-armed bandit problem to a $C+1$-armed problem, where $C+1$ includes the\nbest arm and $C$ $competitive$ arms. Our regret analysis shows that the\ncompetitive arms need to be pulled $\\mathcal{O}(\\log T)$ times, while the\nnon-competitive arms are pulled only $\\mathcal{O}(1)$ times. As a result, there\nare regimes where our algorithm achieves a $\\mathcal{O}(1)$ regret as opposed\nto the typical logarithmic regret scaling of multi-armed bandit algorithms. We\nalso evaluate lower bounds on the expected regret and prove that our\ncorrelated-UCB algorithm achieves $\\mathcal{O}(1)$ regret whenever possible. \n\n"}
{"id": "1808.06508", "contents": "Title: Life-Long Disentangled Representation Learning with Cross-Domain Latent\n  Homologies Abstract: Intelligent behaviour in the real-world requires the ability to acquire new\nknowledge from an ongoing sequence of experiences while preserving and reusing\npast knowledge. We propose a novel algorithm for unsupervised representation\nlearning from piece-wise stationary visual data: Variational Autoencoder with\nShared Embeddings (VASE). Based on the Minimum Description Length principle,\nVASE automatically detects shifts in the data distribution and allocates spare\nrepresentational capacity to new knowledge, while simultaneously protecting\npreviously learnt representations from catastrophic forgetting. Our approach\nencourages the learnt representations to be disentangled, which imparts a\nnumber of desirable properties: VASE can deal sensibly with ambiguous inputs,\nit can enhance its own representations through imagination-based exploration,\nand most importantly, it exhibits semantically meaningful sharing of latents\nbetween different datasets. Compared to baselines with entangled\nrepresentations, our approach is able to reason beyond surface-level statistics\nand perform semantically meaningful cross-domain inference. \n\n"}
{"id": "1808.06573", "contents": "Title: A Semi-Supervised and Inductive Embedding Model for Churn Prediction of\n  Large-Scale Mobile Games Abstract: Mobile gaming has emerged as a promising market with billion-dollar revenues.\nA variety of mobile game platforms and services have been developed around the\nworld. One critical challenge for these platforms and services is to understand\nuser churn behavior in mobile games. Accurate churn prediction will benefit\nmany stakeholders such as game developers, advertisers, and platform operators.\nIn this paper, we present the first large-scale churn prediction solution for\nmobile games. In view of the common limitations of the state-of-the-art methods\nbuilt upon traditional machine learning models, we devise a novel\nsemi-supervised and inductive embedding model that jointly learns the\nprediction function and the embedding function for user-app relationships. We\nmodel these two functions by deep neural networks with a unique edge embedding\ntechnique that is able to capture both contextual information and relationship\ndynamics. We also design a novel attributed random walk technique that takes\ninto consideration both topological adjacency and attribute similarities. To\nevaluate the performance of our solution, we collect real-world data from the\nSamsung Game Launcher platform that includes tens of thousands of games and\nhundreds of millions of user-app interactions. The experimental results with\nthis data demonstrate the superiority of our proposed model against existing\nstate-of-the-art methods. \n\n"}
{"id": "1808.06670", "contents": "Title: Learning deep representations by mutual information estimation and\n  maximization Abstract: In this work, we perform unsupervised learning of representations by\nmaximizing mutual information between an input and the output of a deep neural\nnetwork encoder. Importantly, we show that structure matters: incorporating\nknowledge about locality of the input to the objective can greatly influence a\nrepresentation's suitability for downstream tasks. We further control\ncharacteristics of the representation by matching to a prior distribution\nadversarially. Our method, which we call Deep InfoMax (DIM), outperforms a\nnumber of popular unsupervised learning methods and competes with\nfully-supervised learning on several classification tasks. DIM opens new\navenues for unsupervised learning of representations and is an important step\ntowards flexible formulations of representation-learning objectives for\nspecific end-goals. \n\n"}
{"id": "1808.06918", "contents": "Title: On a New Improvement-Based Acquisition Function for Bayesian\n  Optimization Abstract: Bayesian optimization (BO) is a popular algorithm for solving challenging\noptimization tasks. It is designed for problems where the objective function is\nexpensive to evaluate, perhaps not available in exact form, without gradient\ninformation and possibly returning noisy values. Different versions of the\nalgorithm vary in the choice of the acquisition function, which recommends the\npoint to query the objective at next. Initially, researchers focused on\nimprovement-based acquisitions, while recently the attention has shifted to\nmore computationally expensive information-theoretical measures. In this paper\nwe present two major contributions to the literature. First, we propose a new\nimprovement-based acquisition function that recommends query points where the\nimprovement is expected to be high with high confidence. The proposed algorithm\nis evaluated on a large set of benchmark functions from the global optimization\nliterature, where it turns out to perform at least as well as current\nstate-of-the-art acquisition functions, and often better. This suggests that it\nis a powerful default choice for BO. The novel policy is then compared to\nwidely used global optimization solvers in order to confirm that BO methods\nreduce the computational costs of the optimization by keeping the number of\nfunction evaluations small. The second main contribution represents an\napplication to precision medicine, where the interest lies in the estimation of\nparameters of a partial differential equations model of the human pulmonary\nblood circulation system. Once inferred, these parameters can help clinicians\nin diagnosing a patient with pulmonary hypertension without going through the\nstandard invasive procedure of right heart catheterization, which can lead to\nside effects and complications (e.g. severe pain, internal bleeding,\nthrombosis). \n\n"}
{"id": "1808.10807", "contents": "Title: Risk averse stochastic programming: time consistency and optimal\n  stopping Abstract: Bellman formulated a vague principle for optimization over time, which\ncharacterizes optimal policies by stating that a decision maker should not\nregret previous decisions retrospectively. This paper addresses time\nconsistency in stochastic optimization. The problem is stated in generality\nfirst. The paper discusses time consistent decision-making by addressing risk\nmeasures which are recursive, nested, dynamically or time consistent and\nintroduces stopping time risk measures. It turns out that the paradigm of time\nconsistency is in conflict with various desirable, classical properties of\ngeneral risk measures. \n\n"}
{"id": "1809.00512", "contents": "Title: A Bilevel Approach to Optimal Price-Setting of Time-and-Level-of-Use\n  Tariffs Abstract: Time-and-Level-of-Use (TLOU) is a recently proposed pricing policy for\nenergy, extending Time-of-Use with the addition of a capacity that users can\nbook for a given time frame, reducing their expected energy cost if they\nrespect this self-determined capacity limit. We introduce a variant of the TLOU\ndefined in the literature, aligned with the supplier interest to prevent\nunplanned over-consumption. The optimal price-setting problem of TLOU is\ndefined as a bilevel, bi-objective problem anticipating user choices in the\nsupplier decision. An efficient resolution scheme is developed, based on the\nspecific discrete structure of the lower-level user problem. Computational\nexperiments using consumption distributions estimated from historical data\nillustrate the effectiveness of the proposed framework. \n\n"}
{"id": "1809.01495", "contents": "Title: A Reinforcement Learning-driven Translation Model for Search-Oriented\n  Conversational Systems Abstract: Search-oriented conversational systems rely on information needs expressed in\nnatural language (NL). We focus here on the understanding of NL expressions for\nbuilding keyword-based queries. We propose a reinforcement-learning-driven\ntranslation model framework able to 1) learn the translation from NL\nexpressions to queries in a supervised way, and, 2) to overcome the lack of\nlarge-scale dataset by framing the translation model as a word selection\napproach and injecting relevance feedback in the learning process. Experiments\nare carried out on two TREC datasets and outline the effectiveness of our\napproach. \n\n"}
{"id": "1809.02121", "contents": "Title: Learn What Not to Learn: Action Elimination with Deep Reinforcement\n  Learning Abstract: Learning how to act when there are many available actions in each state is a\nchallenging task for Reinforcement Learning (RL) agents, especially when many\nof the actions are redundant or irrelevant. In such cases, it is sometimes\neasier to learn which actions not to take. In this work, we propose the\nAction-Elimination Deep Q-Network (AE-DQN) architecture that combines a Deep RL\nalgorithm with an Action Elimination Network (AEN) that eliminates sub-optimal\nactions. The AEN is trained to predict invalid actions, supervised by an\nexternal elimination signal provided by the environment. Simulations\ndemonstrate a considerable speedup and added robustness over vanilla DQN in\ntext-based games with over a thousand discrete actions. \n\n"}
{"id": "1809.02209", "contents": "Title: ProdSumNet: reducing model parameters in deep neural networks via\n  product-of-sums matrix decompositions Abstract: We consider a general framework for reducing the number of trainable model\nparameters in deep learning networks by decomposing linear operators as a\nproduct of sums of simpler linear operators. Recently proposed deep learning\narchitectures such as CNN, KFC, Dilated CNN, etc. are all subsumed in this\nframework and we illustrate other types of neural network architectures within\nthis framework. We show that good accuracy on MNIST and Fashion MNIST can be\nobtained using a relatively small number of trainable parameters. In addition,\nsince implementation of the convolutional layer is resource-heavy, we consider\nan approach in the transform domain that obviates the need for convolutional\nlayers. One of the advantages of this general framework over prior approaches\nis that the number of trainable parameters is not fixed and can be varied\narbitrarily. In particular, we illustrate the tradeoff of varying the number of\ntrainable variables and the corresponding error rate. As an example, by using\nthis decomposition on a reference CNN architecture for MNIST with over 3x10^6\ntrainable parameters, we are able to obtain an accuracy of 98.44% using only\n3554 trainable parameters. \n\n"}
{"id": "1809.02441", "contents": "Title: StackNet: Stacking Parameters for Continual learning Abstract: Training a neural network for a classification task typically assumes that\nthe data to train are given from the beginning. However, in the real world,\nadditional data accumulate gradually and the model requires additional training\nwithout accessing the old training data. This usually leads to the catastrophic\nforgetting problem which is inevitable for the traditional training methodology\nof neural networks. In this paper, we propose a continual learning method that\nis able to learn additional tasks while retaining the performance of previously\nlearned tasks by stacking parameters. Composed of two complementary components,\nthe index module and the StackNet, our method estimates the index of the\ncorresponding task for an input sample with the index module and utilizes a\nparticular portion of StackNet with this index. The StackNet guarantees no\ndegradation in the performance of the previously learned tasks and the index\nmodule shows high confidence in finding the origin of an input sample. Compared\nto the previous work of PackNet, our method is competitive and highly\nintuitive. \n\n"}
{"id": "1809.04197", "contents": "Title: Change-Point Detection on Hierarchical Circadian Models Abstract: This paper addresses the problem of change-point detection on sequences of\nhigh-dimensional and heterogeneous observations, which also possess a periodic\ntemporal structure. Due to the dimensionality problem, when the time between\nchange-points is on the order of the dimension of the model parameters, drifts\nin the underlying distribution can be misidentified as changes. To overcome\nthis limitation, we assume that the observations lie in a lower-dimensional\nmanifold that admits a latent variable representation. In particular, we\npropose a hierarchical model that is computationally feasible, widely\napplicable to heterogeneous data and robust to missing instances. Additionally,\nthe observations' periodic dependencies are captured by non-stationary periodic\ncovariance functions. The proposed technique is particularly fitted to (and\nmotivated by) the problem of detecting changes in human behavior using\nsmartphones and its application to relapse detection in psychiatric patients.\nFinally, we validate the technique on synthetic examples and we demonstrate its\nutility in the detection of behavioral changes using real data acquired by\nsmartphones. \n\n"}
{"id": "1809.04886", "contents": "Title: Error estimates for space-time discretization of parabolic time-optimal\n  control problems with bang-bang controls Abstract: In this paper a priori error estimates are derived for full discretization\n(in space and time) of time-optimal control problems. Various convergence\nresults for the optimal time and the control variable are proved under\ndifferent assumptions. Especially the case of bang-bang controls is\ninvestigated. Numerical examples are provided to illustrate the results. \n\n"}
{"id": "1809.05550", "contents": "Title: Efficient Structured Surrogate Loss and Regularization in Structured\n  Prediction Abstract: In this dissertation, we focus on several important problems in structured\nprediction. In structured prediction, the label has a rich intrinsic\nsubstructure, and the loss varies with respect to the predicted label and the\ntrue label pair. Structured SVM is an extension of binary SVM to adapt to such\nstructured tasks.\n  In the first part of the dissertation, we study the surrogate losses and its\nefficient methods. To minimize the empirical risk, a surrogate loss which upper\nbounds the loss, is used as a proxy to minimize the actual loss. Since the\nobjective function is written in terms of the surrogate loss, the choice of the\nsurrogate loss is important, and the performance depends on it. Another issue\nregarding the surrogate loss is the efficiency of the argmax label inference\nfor the surrogate loss. Efficient inference is necessary for the optimization\nsince it is often the most time-consuming step. We present a new class of\nsurrogate losses named bi-criteria surrogate loss, which is a generalization of\nthe popular surrogate losses. We first investigate an efficient method for a\nslack rescaling formulation as a starting point utilizing decomposability of\nthe model. Then, we extend the algorithm to the bi-criteria surrogate loss,\nwhich is very efficient and also shows performance improvements.\n  In the second part of the dissertation, another important issue of\nregularization is studied. Specifically, we investigate a problem of\nregularization in hierarchical classification when a structural imbalance\nexists in the label structure. We present a method to normalize the structure,\nas well as a new norm, namely shared Frobenius norm. It is suitable for\nhierarchical classification that adapts to the data in addition to the label\nstructure. \n\n"}
{"id": "1809.06460", "contents": "Title: Non-Uniform Stability, Detectability, and, Sliding Mode Observer Design\n  for Time Varying Systems with Unknown Inputs Abstract: This paper discusses stability and robustness properties of a recently\nproposed observer algorithm for linear time varying systems. The observer is\nbased on the approximation and subsequent modification of the non-negative\nLyapunov exponents which yields a (non-uniform) exponentially stable error\nsystem. Theoretical insights in the construction of the observer are given and\nthe error system is analyzed with respect to bounded unknown inputs. Therefor,\nnew conditions for bounded input bounded state stability for linear time\nvarying systems are presented. It is shown that for a specific class of linear\ntime varying systems, a cascaded observer based on higher order sliding mode\ndifferentiators can be designed to achieve finite time exact reconstruction of\nthe system states despite the unknown input. A numerical simulation example\nshows the applicability of the proposed approach. \n\n"}
{"id": "1809.07180", "contents": "Title: Projective Splitting with Forward Steps only Requires Continuity Abstract: A recent innovation in projective splitting algorithms for monotone operator\ninclusions has been the development of a procedure using two forward steps\ninstead of the customary proximal steps for operators that are Lipschitz\ncontinuous. This paper shows that the Lipschitz assumption is unnecessary when\nthe forward steps are performed in finite-dimensional spaces: a backtracking\nlinesearch yields a convergent algorithm for operators that are merely\ncontinuous with full domain. \n\n"}
{"id": "1809.07199", "contents": "Title: Primal-dual algorithms for multi-agent structured optimization over\n  message-passing architectures with bounded communication delays Abstract: We consider algorithms for solving structured convex optimization problems\nover a network of agents with communication delays. It is assumed that each\nagent performs its local updates by using possibly outdated information from\nits neighbors under the assumption that the delay with respect to each neighbor\nis bounded but otherwise arbitrary. The private objective of each agent is\nrepresented by the sum of two possibly nonsmooth functions, one of which is\ncomposed with a linear mapping. The global optimization problem is the\naggregate of the local cost functions and a common Lipschitz-differentiable\nterm. When the coupling between the agents is represented only through the\ncommon function the primal-dual algorithm proposed by V\\~u and Condat can be\nconveniently employed, while for more general structures a new algorithm is\nproposed. Moreover, a randomized variant is presented that allows the agents to\nwake up at random and independently from one another. The convergence of each\nof the proposed algorithms is established under different strong convexity\nassumptions. \n\n"}
{"id": "1809.07276", "contents": "Title: Music Mood Detection Based On Audio And Lyrics With Deep Neural Net Abstract: We consider the task of multimodal music mood prediction based on the audio\nsignal and the lyrics of a track. We reproduce the implementation of\ntraditional feature engineering based approaches and propose a new model based\non deep learning. We compare the performance of both approaches on a database\ncontaining 18,000 tracks with associated valence and arousal values and show\nthat our approach outperforms classical models on the arousal detection task,\nand that both approaches perform equally on the valence prediction task. We\nalso compare the a posteriori fusion with fusion of modalities optimized\nsimultaneously with each unimodal model, and observe a significant improvement\nof valence prediction. We release part of our database for comparison purposes. \n\n"}
{"id": "1809.07496", "contents": "Title: Optimal mass transport and kernel density estimation for state-dependent\n  networked dynamic systems Abstract: State-dependent networked dynamical systems are ones where the\ninterconnections between agents change as a function of the states of the\nagents. Such systems are highly nonlinear, and a cohesive strategy for their\ncontrol is lacking in the literature. In this paper, we present two techniques\npertaining to the density control of such systems. Agent states are initially\ndistributed according to some density, and a feedback law is designed to move\nthe agents to a target density profile. We use optimal mass transport to design\na feedforward control law propelling the agents towards this target density.\nKernel density estimation, with constraints imposed by the state-dependent\ndynamics, is then used to allow each agent to estimate the local density of the\nagents. \n\n"}
{"id": "1809.08327", "contents": "Title: Quantifying total uncertainty in physics-informed neural networks for\n  solving forward and inverse stochastic problems Abstract: Physics-informed neural networks (PINNs) have recently emerged as an\nalternative way of solving partial differential equations (PDEs) without the\nneed of building elaborate grids, instead, using a straightforward\nimplementation. In particular, in addition to the deep neural network (DNN) for\nthe solution, a second DNN is considered that represents the residual of the\nPDE. The residual is then combined with the mismatch in the given data of the\nsolution in order to formulate the loss function. This framework is effective\nbut is lacking uncertainty quantification of the solution due to the inherent\nrandomness in the data or due to the approximation limitations of the DNN\narchitecture. Here, we propose a new method with the objective of endowing the\nDNN with uncertainty quantification for both sources of uncertainty, i.e., the\nparametric uncertainty and the approximation uncertainty. We first account for\nthe parametric uncertainty when the parameter in the differential equation is\nrepresented as a stochastic process. Multiple DNNs are designed to learn the\nmodal functions of the arbitrary polynomial chaos (aPC) expansion of its\nsolution by using stochastic data from sparse sensors. We can then make\npredictions from new sensor measurements very efficiently with the trained\nDNNs. Moreover, we employ dropout to correct the over-fitting and also to\nquantify the uncertainty of DNNs in approximating the modal functions. We then\ndesign an active learning strategy based on the dropout uncertainty to place\nnew sensors in the domain to improve the predictions of DNNs. Several numerical\ntests are conducted for both the forward and the inverse problems to quantify\nthe effectiveness of PINNs combined with uncertainty quantification. This\nNN-aPC new paradigm of physics-informed deep learning with uncertainty\nquantification can be readily applied to other types of stochastic PDEs in\nmulti-dimensions. \n\n"}
{"id": "1809.08359", "contents": "Title: A convex program for bilinear inversion of sparse vectors Abstract: We consider the bilinear inverse problem of recovering two vectors,\n$\\boldsymbol{x}\\in\\mathbb{R}^L$ and $\\boldsymbol{w}\\in\\mathbb{R}^L$, from their\nentrywise product. We consider the case where $\\boldsymbol{x}$ and\n$\\boldsymbol{w}$ have known signs and are sparse with respect to known\ndictionaries of size $K$ and $N$, respectively. Here, $K$ and $N$ may be larger\nthan, smaller than, or equal to $L$. We introduce $\\ell_1$-BranchHull, which is\na convex program posed in the natural parameter space and does not require an\napproximate solution or initialization in order to be stated or solved. We\nstudy the case where $\\boldsymbol{x}$ and $\\boldsymbol{w}$ are $S_1$- and\n$S_2$-sparse with respect to a random dictionary and present a recovery\nguarantee that only depends on the number of measurements as\n$L\\geq\\Omega(S_1+S_2)\\log^{2}(K+N)$. Numerical experiments verify that the\nscaling constant in the theorem is not too large. One application of this\nproblem is the sweep distortion removal task in dielectric imaging, where one\nof the signals is a nonnegative reflectivity, and the other signal lives in a\nknown subspace, for example that given by dominant wavelet coefficients. We\nalso introduce a variants of $\\ell_1$-BranchHull for the purposes of tolerating\nnoise and outliers, and for the purpose of recovering piecewise constant\nsignals. We provide an ADMM implementation of these variants and show they can\nextract piecewise constant behavior from real images. \n\n"}
{"id": "1809.08694", "contents": "Title: Second-order Guarantees of Distributed Gradient Algorithms Abstract: We consider distributed smooth nonconvex unconstrained optimization over\nnetworks, modeled as a connected graph. We examine the behavior of distributed\ngradient-based algorithms near strict saddle points. Specifically, we establish\nthat (i) the renowned Distributed Gradient Descent (DGD) algorithm likely\nconverges to a neighborhood of a Second-order Stationary (SoS) solution; and\n(ii) the more recent class of distributed algorithms based on gradient\ntracking--implementable also over digraphs--likely converges to exact SoS\nsolutions, thus avoiding (strict) saddle-points. Furthermore, new convergence\nrate results to first-order critical points is established for the latter class\nof algorithms. \n\n"}
{"id": "1809.08706", "contents": "Title: Is Ordered Weighted $\\ell_1$ Regularized Regression Robust to\n  Adversarial Perturbation? A Case Study on OSCAR Abstract: Many state-of-the-art machine learning models such as deep neural networks\nhave recently shown to be vulnerable to adversarial perturbations, especially\nin classification tasks. Motivated by adversarial machine learning, in this\npaper we investigate the robustness of sparse regression models with strongly\ncorrelated covariates to adversarially designed measurement noises.\nSpecifically, we consider the family of ordered weighted $\\ell_1$ (OWL)\nregularized regression methods and study the case of OSCAR (octagonal shrinkage\nclustering algorithm for regression) in the adversarial setting. Under a\nnorm-bounded threat model, we formulate the process of finding a maximally\ndisruptive noise for OWL-regularized regression as an optimization problem and\nillustrate the steps towards finding such a noise in the case of OSCAR.\nExperimental results demonstrate that the regression performance of grouping\nstrongly correlated features can be severely degraded under our adversarial\nsetting, even when the noise budget is significantly smaller than the\nground-truth signals. \n\n"}
{"id": "1809.08720", "contents": "Title: Synchronization of Kuramoto Oscillators: Inverse Taylor Expansions Abstract: Synchronization in networks of coupled oscillators is a widely studied topic\nwith extensive scientific and engineering applications. In this paper, we study\nthe frequency synchronization problem for networks of Kuramoto oscillators with\narbitrary topology and heterogeneous edge weights. We propose a novel\nequivalent transcription for the equilibrium synchronization equation. Using\nthis transcription, we develop a power series expansion to compute the\nsynchronized solution of the Kuramoto model as well as a sufficient condition\nfor the strong convergence of this series expansion. Truncating the power\nseries provides (i) an efficient approximation scheme for computing the\nsynchronized solution, and (ii) a simple-to-check, statistically-correct\nhierarchy of increasingly accurate synchronization tests. This hierarchy of\ntests provides a theoretical foundation for and generalizes the best-known\napproximate synchronization test in the literature. Our numerical experiments\nillustrate the accuracy and the computational efficiency of the truncated\nseries approximation compared to existing iterative methods and existing\nsynchronization tests. \n\n"}
{"id": "1809.08991", "contents": "Title: Analysis and optimisation of a variational model for mixed Gaussian and\n  Salt & Pepper noise removal Abstract: We analyse a variational regularisation problem for mixed noise removal that\nwas recently proposed in [14]. The data discrepancy term of the model combines\n$L^1$ and $L^2$ terms in an infimal convolution fashion and it is appropriate\nfor the joint removal of Gaussian and Salt & Pepper noise. In this work we\nperform a finer analysis of the model which emphasises on the balancing effect\nof the two parameters appearing in the discrepancy term. Namely, we study the\nasymptotic behaviour of the model for large and small values of these\nparameters and we compare it to the corresponding variational models with $L^1$\nand $L^2$ data fidelity. Furthermore, we compute exact solutions for simple\ndata functions taking the total variation as regulariser. Using these\ntheoretical results, we then analytically study a bilevel optimisation strategy\nfor automatically selecting the parameters of the model by means of a training\nset. Finally, we report some numerical results on the selection of the optimal\nnoise model via such strategy which confirm the validity of our analysis and\nthe use of popular data models in the case of \"blind\" model selection. \n\n"}
{"id": "1809.09037", "contents": "Title: Optimal Distributed Control of a Cahn-Hilliard-Darcy System with Mass\n  Sources Abstract: In this paper, we study an optimal control problem for a two-dimensional\nCahn-Hilliard-Darcy system with mass sources that arises in the modeling of\ntumor growth. The aim is to monitor the tumor fraction in a finite time\ninterval in such a way that both the tumor fraction, measured in terms of a\ntracking type cost functional, is kept under control and minimal harm is\ninflicted to the patient by administering the control, which could either be a\ndrug or nutrition. We first prove that the optimal control problem admits a\nsolution. Then we show that the control-to-state operator is Fr\\'echet\ndifferentiable between suitable Banach spaces and derive the first-order\nnecessary optimality conditions in terms of the adjoint variables and the usual\nvariational inequality. \n\n"}
{"id": "1809.09043", "contents": "Title: An experimental approach for global polynomial optimization based on\n  Moments and Semidefinite Programming Abstract: In this article we provide an experimental algorithm that in many cases gives\nus an upper bound of the global infimum of a real polynomial on $\\R^{n}$. It is\nvery well known that to find the global infimum of a real polynomial on\n$\\R^{n}$, often reduces to solve a hierarchy of positive semidefinite programs,\ncalled moment relaxations. The algorithm that we present involves to solve a\nseries of positive semidefinite programs whose feasible set is included in the\nfeasible set of a moment relaxation. Our additional constraint try to provoke a\nflatness condition, like used by Curto and Fialkow, for the computed moments.\nAt the end we present numerical results of the application of the algorithm to\nnonnegative polynomials which are not sums of squares. We also provide\nnumerical results for the application of a version of the algorithm based on\nthe method proposed by Nie, Demmel and Sturmfels for the problem of minimizing\na polynomial over its gradient variety. \n\n"}
{"id": "1809.09613", "contents": "Title: Size Agnostic Change Point Detection Framework for Evolving Networks Abstract: Changes in the structure of observed social and complex networks' structure\ncan indicate a significant underlying change in an organization, or reflect the\nresponse of the network to an external event. Automatic detection of change\npoints in evolving networks is rudimentary to the research and the\nunderstanding of the effect of such events on networks. Here we present an\neasy-to-implement and fast framework for change point detection in temporal\nevolving networks. Unlike previous approaches, our method is size agnostic, and\ndoes not require either prior knowledge about the network's size and structure,\nnor does it require obtaining historical information or nodal identities over\ntime. We use both synthetic data derived from dynamic models and two real\ndatasets: Enron email exchange and Ask-Ubuntu forum. Our framework succeeds\nwith both precision and recall and outperforms previous solutions \n\n"}
{"id": "1810.00021", "contents": "Title: Feedback control of parametrized PDEs via model order reduction and\n  dynamic programming principle Abstract: In this paper we investigate infinite horizon optimal control problems for\nparametrized partial differential equations. We are interested in feedback\ncontrol via dynamic programming equations which is well-known to suffer from\nthe curse of dimensionality. Thus, we apply parametric model order reduction\ntechniques to construct low-dimensional subspaces with suitable information on\nthe control problem, where the dynamic programming equations can be\napproximated. To guarantee a low number of basis functions, we combine recent\nbasis generation methods and parameter partitioning techniques. Furthermore, we\npresent a novel technique to construct nonuniform grids in the reduced domain,\nwhich is based on statistical information. Finally, we discuss numerical\nexamples to illustrate the effectiveness of the proposed methods for PDEs in\ntwo space dimensions. \n\n"}
{"id": "1810.00150", "contents": "Title: Directional Analysis of Stochastic Gradient Descent via von Mises-Fisher\n  Distributions in Deep learning Abstract: Although stochastic gradient descent (SGD) is a driving force behind the\nrecent success of deep learning, our understanding of its dynamics in a\nhigh-dimensional parameter space is limited. In recent years, some researchers\nhave used the stochasticity of minibatch gradients, or the signal-to-noise\nratio, to better characterize the learning dynamics of SGD. Inspired from these\nwork, we here analyze SGD from a geometrical perspective by inspecting the\nstochasticity of the norms and directions of minibatch gradients. We propose a\nmodel of the directional concentration for minibatch gradients through von\nMises-Fisher (VMF) distribution, and show that the directional uniformity of\nminibatch gradients increases over the course of SGD. We empirically verify our\nresult using deep convolutional networks and observe a higher correlation\nbetween the gradient stochasticity and the proposed directional uniformity than\nthat against the gradient norm stochasticity, suggesting that the directional\nstatistics of minibatch gradients is a major factor behind SGD. \n\n"}
{"id": "1810.00803", "contents": "Title: Large Scale Clustering with Variational EM for Gaussian Mixture Models Abstract: This paper represents a preliminary (pre-reviewing) version of a sublinear\nvariational algorithm for isotropic Gaussian mixture models (GMMs). Further\ndevelopments of the algorithm for GMMs with diagonal covariance matrices\n(instead of isotropic clusters) and their corresponding benchmarking results\nhave been published by TPAMI (doi:10.1109/TPAMI.2021.3133763) in the paper \"A\nVariational EM Acceleration for Efficient Clustering at Very Large Scales\". We\nkindly refer the reader to the TPAMI paper instead of this much earlier arXiv\nversion (the TPAMI paper is also open access). Publicly available source code\naccompanies the paper (see\nhttps://github.com/variational-sublinear-clustering). Please note that the\nTPAMI paper does not contain the benchmark on the 80 Million Tiny Images\ndataset anymore because we followed the call of the dataset creators to\ndiscontinue the use of that dataset.\n  The aim of the project (which resulted in this arXiv version and the later\nTPAMI paper) is the exploration of the current efficiency and large-scale\nlimits in fitting a parametric model for clustering to data distributions. To\nreduce computational complexity, we used a clustering objective based on\ntruncated variational EM (which reduces complexity for many clusters) in\ncombination with coreset objectives (which reduce complexity for many data\npoints). We used efficient coreset construction and efficient seeding to\ntranslate the theoretical sublinear complexity gains into an efficient\nalgorithm. In applications to standard large-scale benchmarks for clustering,\nwe then observed substantial wall-clock speedups compared to already highly\nefficient clustering approaches. To demonstrate that the observed efficiency\nenables applications previously considered unfeasible, we clustered the entire\nand unscaled 80 Million Tiny Images dataset into up to 32,000 clusters. \n\n"}
{"id": "1810.00859", "contents": "Title: Dynamic Sparse Graph for Efficient Deep Learning Abstract: We propose to execute deep neural networks (DNNs) with dynamic and sparse\ngraph (DSG) structure for compressive memory and accelerative execution during\nboth training and inference. The great success of DNNs motivates the pursuing\nof lightweight models for the deployment onto embedded devices. However, most\nof the previous studies optimize for inference while neglect training or even\ncomplicate it. Training is far more intractable, since (i) the neurons dominate\nthe memory cost rather than the weights in inference; (ii) the dynamic\nactivation makes previous sparse acceleration via one-off optimization on fixed\nweight invalid; (iii) batch normalization (BN) is critical for maintaining\naccuracy while its activation reorganization damages the sparsity. To address\nthese issues, DSG activates only a small amount of neurons with high\nselectivity at each iteration via a dimension-reduction search (DRS) and\nobtains the BN compatibility via a double-mask selection (DMS). Experiments\nshow significant memory saving (1.7-4.5x) and operation reduction (2.3-4.4x)\nwith little accuracy loss on various benchmarks. \n\n"}
{"id": "1810.01400", "contents": "Title: Sketching for Latent Dirichlet-Categorical Models Abstract: Recent work has explored transforming data sets into smaller, approximate\nsummaries in order to scale Bayesian inference. We examine a related problem in\nwhich the parameters of a Bayesian model are very large and expensive to store\nin memory, and propose more compact representations of parameter values that\ncan be used during inference. We focus on a class of graphical models that we\nrefer to as latent Dirichlet-Categorical models, and show how a combination of\ntwo sketching algorithms known as count-min sketch and approximate counters\nprovide an efficient representation for them. We show that this sketch\ncombination -- which, despite having been used before in NLP applications, has\nnot been previously analyzed -- enjoys desirable properties. We prove that for\nthis class of models, when the sketches are used during Markov Chain Monte\nCarlo inference, the equilibrium of sketched MCMC converges to that of the\nexact chain as sketch parameters are tuned to reduce the error rate. \n\n"}
{"id": "1810.01405", "contents": "Title: GrAMME: Semi-Supervised Learning using Multi-layered Graph Attention\n  Models Abstract: Modern data analysis pipelines are becoming increasingly complex due to the\npresence of multi-view information sources. While graphs are effective in\nmodeling complex relationships, in many scenarios a single graph is rarely\nsufficient to succinctly represent all interactions, and hence multi-layered\ngraphs have become popular. Though this leads to richer representations,\nextending solutions from the single-graph case is not straightforward.\nConsequently, there is a strong need for novel solutions to solve classical\nproblems, such as node classification, in the multi-layered case. In this\npaper, we consider the problem of semi-supervised learning with multi-layered\ngraphs. Though deep network embeddings, e.g. DeepWalk, are widely adopted for\ncommunity discovery, we argue that feature learning with random node\nattributes, using graph neural networks, can be more effective. To this end, we\npropose to use attention models for effective feature learning, and develop two\nnovel architectures, GrAMME-SG and GrAMME-Fusion, that exploit the inter-layer\ndependencies for building multi-layered graph embeddings. Using empirical\nstudies on several benchmark datasets, we evaluate the proposed approaches and\ndemonstrate significant performance improvements in comparison to\nstate-of-the-art network embedding strategies. The results also show that using\nsimple random features is an effective choice, even in cases where explicit\nnode attributes are not available. \n\n"}
{"id": "1810.02321", "contents": "Title: Optimal Learning with Anisotropic Gaussian SVMs Abstract: This paper investigates the nonparametric regression problem using SVMs with\nanisotropic Gaussian RBF kernels. Under the assumption that the target\nfunctions are resided in certain anisotropic Besov spaces, we establish the\nalmost optimal learning rates, more precisely, optimal up to some logarithmic\nfactor, presented by the effective smoothness. By taking the effective\nsmoothness into consideration, our almost optimal learning rates are faster\nthan those obtained with the underlying RKHSs being certain anisotropic Sobolev\nspaces. Moreover, if the target function depends only on fewer dimensions,\nfaster learning rates can be further achieved. \n\n"}
{"id": "1810.03307", "contents": "Title: Local Explanation Methods for Deep Neural Networks Lack Sensitivity to\n  Parameter Values Abstract: Explaining the output of a complicated machine learning model like a deep\nneural network (DNN) is a central challenge in machine learning. Several\nproposed local explanation methods address this issue by identifying what\ndimensions of a single input are most responsible for a DNN's output. The goal\nof this work is to assess the sensitivity of local explanations to DNN\nparameter values. Somewhat surprisingly, we find that DNNs with\nrandomly-initialized weights produce explanations that are both visually and\nquantitatively similar to those produced by DNNs with learned weights. Our\nconjecture is that this phenomenon occurs because these explanations are\ndominated by the lower level features of a DNN, and that a DNN's architecture\nprovides a strong prior which significantly affects the representations learned\nat these lower layers. NOTE: This work is now subsumed by our recent\nmanuscript, Sanity Checks for Saliency Maps (to appear NIPS 2018), where we\nexpand on findings and address concerns raised in Sundararajan et. al. (2018). \n\n"}
{"id": "1810.03505", "contents": "Title: CINIC-10 is not ImageNet or CIFAR-10 Abstract: In this brief technical report we introduce the CINIC-10 dataset as a plug-in\nextended alternative for CIFAR-10. It was compiled by combining CIFAR-10 with\nimages selected and downsampled from the ImageNet database. We present the\napproach to compiling the dataset, illustrate the example images for different\nclasses, give pixel distributions for each part of the repository, and give\nsome standard benchmarks for well known models. Details for download, usage,\nand compilation can be found in the associated github repository. \n\n"}
{"id": "1810.03545", "contents": "Title: Stein Neural Sampler Abstract: We propose two novel samplers to generate high-quality samples from a given\n(un-normalized) probability density. Motivated by the success of generative\nadversarial networks, we construct our samplers using deep neural networks that\ntransform a reference distribution to the target distribution. Training schemes\nare developed to minimize two variations of the Stein discrepancy, which is\ndesigned to work with un-normalized densities. Once trained, our samplers are\nable to generate samples instantaneously. We show that the proposed methods are\ntheoretically sound and experience fewer convergence issues compared with\ntraditional sampling approaches according to our empirical studies. \n\n"}
{"id": "1810.03869", "contents": "Title: A sub-Finsler problem on the Cartan group Abstract: In this paper we study a sub-Finsler geometric problem on the free-nilpotent\ngroup of rank 2 and step 3. Such a group is also called Cartan group and has a\nnatural structure of Carnot group, which we metrize considering the\n$\\ell_\\infty$ norm on its first layer. We adopt the point of view of\ntime-optimal control theory. We characterize extremal curves via Pontryagin\nmaximum principle. We describe abnormal and singular arcs, and construct the\nbang-bang flow. \n\n"}
{"id": "1810.05471", "contents": "Title: Safe Grid Search with Optimal Complexity Abstract: Popular machine learning estimators involve regularization parameters that\ncan be challenging to tune, and standard strategies rely on grid search for\nthis task. In this paper, we revisit the techniques of approximating the\nregularization path up to predefined tolerance $\\epsilon$ in a unified\nframework and show that its complexity is $O(1/\\sqrt[d]{\\epsilon})$ for\nuniformly convex loss of order $d \\geq 2$ and $O(1/\\sqrt{\\epsilon})$ for\nGeneralized Self-Concordant functions. This framework encompasses least-squares\nbut also logistic regression, a case that as far as we know was not handled as\nprecisely in previous works. We leverage our technique to provide refined\nbounds on the validation error as well as a practical algorithm for\nhyperparameter tuning. The latter has global convergence guarantee when\ntargeting a prescribed accuracy on the validation set. Last but not least, our\napproach helps relieving the practitioner from the (often neglected) task of\nselecting a stopping criterion when optimizing over the training set: our\nmethod automatically calibrates this criterion based on the targeted accuracy\non the validation set. \n\n"}
{"id": "1810.05558", "contents": "Title: Variational Bayesian Monte Carlo Abstract: Many probabilistic models of interest in scientific computing and machine\nlearning have expensive, black-box likelihoods that prevent the application of\nstandard techniques for Bayesian inference, such as MCMC, which would require\naccess to the gradient or a large number of likelihood evaluations. We\nintroduce here a novel sample-efficient inference framework, Variational\nBayesian Monte Carlo (VBMC). VBMC combines variational inference with\nGaussian-process based, active-sampling Bayesian quadrature, using the latter\nto efficiently approximate the intractable integral in the variational\nobjective. Our method produces both a nonparametric approximation of the\nposterior distribution and an approximate lower bound of the model evidence,\nuseful for model selection. We demonstrate VBMC both on several synthetic\nlikelihoods and on a neuronal model with data from real neurons. Across all\ntested problems and dimensions (up to $D = 10$), VBMC performs consistently\nwell in reconstructing the posterior and the model evidence with a limited\nbudget of likelihood evaluations, unlike other methods that work only in very\nlow dimensions. Our framework shows great promise as a novel tool for posterior\nand model inference with expensive, black-box likelihoods. \n\n"}
{"id": "1810.05876", "contents": "Title: A space-time pseudospectral discretization method for solving diffusion\n  optimal control problems with two-sided fractional derivatives Abstract: We propose a direct numerical method for the solution of an optimal control\nproblem governed by a two-side space-fractional diffusion equation. The\npresented method contains two main steps. In the first step, the space variable\nis discretized by using the Jacobi-Gauss pseudospectral discretization and, in\nthis way, the original problem is transformed into a classical integer-order\noptimal control problem. The main challenge, which we faced in this step, is to\nderive the left and right fractional differentiation matrices. In this respect,\nnovel techniques for derivation of these matrices are presented. In the second\nstep, the Legendre-Gauss-Radau pseudospectral method is employed. With these\ntwo steps, the original problem is converted into a convex quadratic\noptimization problem, which can be solved efficiently by available methods. Our\napproach can be easily implemented and extended to cover fractional optimal\ncontrol problems with state constraints. Five test examples are provided to\ndemonstrate the efficiency and validity of the presented method. The results\nshow that our method reaches the solutions with good accuracy and a low CPU\ntime. \n\n"}
{"id": "1810.05947", "contents": "Title: Robust Model Predictive Control of Irrigation Systems with Active\n  Uncertainty Learning and Data Analytics Abstract: We develop a novel data-driven robust model predictive control (DDRMPC)\napproach for automatic control of irrigation systems. The fundamental idea is\nto integrate both mechanistic models, which describe dynamics in soil moisture\nvariations, and data-driven models, which characterize uncertainty in forecast\nerrors of evapotranspiration and precipitation, into a holistic systems control\nframework. To better capture the support of uncertainty distribution, we take a\nnew learning-based approach by constructing uncertainty sets from historical\ndata. For evapotranspiration forecast error, the support vector\nclustering-based uncertainty set is adopted, which can be conveniently built\nfrom historical data. As for precipitation forecast errors, we analyze the\ndependence of their distribution on forecast values, and further design a\ntailored uncertainty set based on the properties of this type of uncertainty.\nIn this way, the overall uncertainty distribution can be elaborately described,\nwhich finally contributes to rational and efficient control decisions. To\nassure the quality of data-driven uncertainty sets, a training-calibration\nscheme is used to provide theoretical performance guarantees. A generalized\naffine decision rule is adopted to obtain tractable approximations of optimal\ncontrol problems, thereby ensuring the practicability of DDRMPC. Case studies\nusing real data show that, DDRMPC can reliably maintain soil moisture above the\nsafety level and avoid crop devastation. The proposed DDRMPC approach leads to\na 40% reduction of total water consumption compared to the fine-tuned open-loop\ncontrol strategy. In comparison with the carefully tuned rule-based control and\ncertainty equivalent model predictive control, the proposed DDRMPC approach can\nsignificantly reduce the total water consumption and improve the control\nperformance. \n\n"}
{"id": "1810.06256", "contents": "Title: A Polynomial-Time Method for Testing Admissibility of Uncertain Power\n  Injections in Microgrids Abstract: We study the admissibility of power injections in single-phase microgrids,\nwhere the electrical state is represented by complex nodal voltages and\ncontrolled by nodal power injections. Assume that (i) there is an initial\nelectrical state that satisfies security constraints and the non-singularity of\nload-flow Jacobian, and (ii) power injections reside in some uncertainty set.\nWe say that the uncertainty set is admissible for the initial electrical state\nif any continuous trajectory of the electrical state is ensured to be secured\nand non-singular as long as power injections remain in the uncertainty set. We\nuse the recently proposed V-control and show two new results. First, if a\ncomplex nodal voltage set V is convex and every element in V is nonsingular,\nthen V is a domain of uniqueness. Second, we give sufficient conditions to\nguarantee that every element in some power injection set S has a load-flow\nsolution in V, based on impossibility of obtaining load-flow solutions at the\nboundary of V. By these results, we develop a framework for the\nadmissibility-test method; this framework is extensible to multi-phase grids.\nWithin the framework, we establish a polynomial-time method, using the\ninfeasibility check of convex optimizations. The method is evaluated\nnumerically. \n\n"}
{"id": "1810.08164", "contents": "Title: A Unified Approach to Translate Classical Bandit Algorithms to the\n  Structured Bandit Setting Abstract: We consider a finite-armed structured bandit problem in which mean rewards of\ndifferent arms are known functions of a common hidden parameter $\\theta^*$.\nSince we do not place any restrictions of these functions, the problem setting\nsubsumes several previously studied frameworks that assume linear or invertible\nreward functions. We propose a novel approach to gradually estimate the hidden\n$\\theta^*$ and use the estimate together with the mean reward functions to\nsubstantially reduce exploration of sub-optimal arms. This approach enables us\nto fundamentally generalize any classic bandit algorithm including UCB and\nThompson Sampling to the structured bandit setting. We prove via regret\nanalysis that our proposed UCB-C algorithm (structured bandit versions of UCB)\npulls only a subset of the sub-optimal arms $O(\\log T)$ times while the other\nsub-optimal arms (referred to as non-competitive arms) are pulled $O(1)$ times.\nAs a result, in cases where all sub-optimal arms are non-competitive, which can\nhappen in many practical scenarios, the proposed algorithms achieve bounded\nregret. We also conduct simulations on the Movielens recommendations dataset to\ndemonstrate the improvement of the proposed algorithms over existing structured\nbandit algorithms. \n\n"}
{"id": "1810.09063", "contents": "Title: Optimal electricity demand response contracting with responsiveness\n  incentives Abstract: Despite the success of demand response programs in retail electricity markets\nin reducing average consumption, the random responsiveness of consumers to\nprice event makes their efficiency questionable to achieve the flexibility\nneeded for electric systems with a large share of renewable energy. The\nvariance of consumers' responses depreciates the value of these mechanisms and\nmakes them weakly reliable. This paper aims at designing demand response\ncontracts which allow to act on both the average consumption and its variance.\nThe interaction between a risk--averse producer and a risk--averse consumer is\nmodelled through a Principal--Agent problem, thus accounting for the moral\nhazard underlying demand response contracts. We provide closed--form solution\nfor the optimal contract in the case of constant marginal costs of energy and\nvolatility for the producer and constant marginal value of energy for the\nconsumer. We show that the optimal contract has a rebate form where the initial\ncondition of the consumption serves as a baseline. Further, the consumer cannot\nmanipulate the baseline at his own advantage. The second--best price for energy\nand volatility are non--constant and non--increasing in time. The price for\nenergy is lower (resp. higher) than the marginal cost of energy during\npeak--load (resp. off--peak) periods. We illustrate the potential benefit\nissued from the implementation of an incentive mechanism on the responsiveness\nof the consumer by calibrating our model with publicly available data. We\npredict a significant increase of responsiveness under our optimal contract and\na significant increase of the producer satisfaction. \n\n"}
{"id": "1810.09132", "contents": "Title: Distributed Mixed Voltage Angle and Frequency Droop Control of Microgrid\n  Interconnections with Loss of Distribution-PMU Measurements Abstract: Recent advances in distribution-level phasor measurement unit (D-PMU)\ntechnology have enabled the use of voltage phase angle measurements for direct\nload sharing control in distribution-level microgrid interconnections with high\npenetration of renewable distributed energy resources (DERs). In particular,\nD-PMU enabled voltage angle droop control has the potential to enhance\nstability and transient performance in such microgrid interconnections.\nHowever, these angle droop control designs are vulnerable to D-PMU angle\nmeasurement losses that frequently occur due to the unavailability of a GPS\nsignal for synchronization. In the event of such measurement losses, angle\ndroop controlled microgrid interconnections may suffer from poor performance\nand potentially lose stability. In this paper, we propose a novel distributed\nmixed voltage angle and frequency droop control (D-MAFD) framework to improve\nthe reliability of angle droop controlled microgrid interconnections. In this\nframework, when the D-PMU phase angle measurement is lost at a microgrid,\nconventional frequency droop control is temporarily used for primary control in\nplace of angle droop control to guarantee stability. We model the microgrid\ninterconnection with this primary control architecture as a nonlinear switched\nsystem and design distributed secondary controllers to guarantee transient\nstability of the network. Further, we incorporate performance specifications\nsuch as robustness to generation-load mismatch and network topology changes in\nthe distributed control design. We demonstrate the performance of this control\nframework by simulation on a test 123-feeder distribution network. \n\n"}
{"id": "1810.09177", "contents": "Title: Compositional Coding Capsule Network with K-Means Routing for Text\n  Classification Abstract: Text classification is a challenging problem which aims to identify the\ncategory of texts. In the process of training, word embeddings occupy a large\npart of parameters. Under the limitation of limited computing resources, it\nindirectly limits the ability of subsequent network designs. In order to reduce\nthe number of parameters, the compositional coding mechanism has been proposed\nrecently. Based on this, this paper further explores compositional coding and\nproposes a compositional weighted coding method. And we apply capsule network\nto model the relationship between word embeddings, a new routing algorithm,\nwhich is based on k-means clustering theory, is proposed to fully mine the\nrelationship between word embeddings. Combined with our compositional weighted\ncoding method and the routing algorithm, we design a neural network for text\nclassification. Experiments conducted on eight challenging text classification\ndatasets show that the proposed method achieves competitive accuracy compared\nto the state-of-the-art approach with significantly fewer parameters. \n\n"}
{"id": "1810.09868", "contents": "Title: Automatic Full Compilation of Julia Programs and ML Models to Cloud TPUs Abstract: Google's Cloud TPUs are a promising new hardware architecture for machine\nlearning workloads. They have powered many of Google's milestone machine\nlearning achievements in recent years. Google has now made TPUs available for\ngeneral use on their cloud platform and as of very recently has opened them up\nfurther to allow use by non-TensorFlow frontends. We describe a method and\nimplementation for offloading suitable sections of Julia programs to TPUs via\nthis new API and the Google XLA compiler. Our method is able to completely fuse\nthe forward pass of a VGG19 model expressed as a Julia program into a single\nTPU executable to be offloaded to the device. Our method composes well with\nexisting compiler-based automatic differentiation techniques on Julia code, and\nwe are thus able to also automatically obtain the VGG19 backwards pass and\nsimilarly offload it to the TPU. Targeting TPUs using our compiler, we are able\nto evaluate the VGG19 forward pass on a batch of 100 images in 0.23s which\ncompares favorably to the 52.4s required for the original model on the CPU. Our\nimplementation is less than 1000 lines of Julia, with no TPU specific changes\nmade to the core Julia compiler or any other Julia packages. \n\n"}
{"id": "1810.09977", "contents": "Title: Learning First-to-Spike Policies for Neuromorphic Control Using Policy\n  Gradients Abstract: Artificial Neural Networks (ANNs) are currently being used as function\napproximators in many state-of-the-art Reinforcement Learning (RL) algorithms.\nSpiking Neural Networks (SNNs) have been shown to drastically reduce the energy\nconsumption of ANNs by encoding information in sparse temporal binary spike\nstreams, hence emulating the communication mechanism of biological neurons. Due\nto their low energy consumption, SNNs are considered to be important candidates\nas co-processors to be implemented in mobile devices. In this work, the use of\nSNNs as stochastic policies is explored under an energy-efficient\nfirst-to-spike action rule, whereby the action taken by the RL agent is\ndetermined by the occurrence of the first spike among the output neurons. A\npolicy gradient-based algorithm is derived considering a Generalized Linear\nModel (GLM) for spiking neurons. Experimental results demonstrate the\ncapability of online trained SNNs as stochastic policies to gracefully trade\nenergy consumption, as measured by the number of spikes, and control\nperformance. Significant gains are shown as compared to the standard approach\nof converting an offline trained ANN into an SNN. \n\n"}
{"id": "1810.10065", "contents": "Title: Statistical mechanics of low-rank tensor decomposition Abstract: Often, large, high dimensional datasets collected across multiple modalities\ncan be organized as a higher order tensor. Low-rank tensor decomposition then\narises as a powerful and widely used tool to discover simple low dimensional\nstructures underlying such data. However, we currently lack a theoretical\nunderstanding of the algorithmic behavior of low-rank tensor decompositions. We\nderive Bayesian approximate message passing (AMP) algorithms for recovering\narbitrarily shaped low-rank tensors buried within noise, and we employ dynamic\nmean field theory to precisely characterize their performance. Our theory\nreveals the existence of phase transitions between easy, hard and impossible\ninference regimes, and displays an excellent match with simulations. Moreover,\nit reveals several qualitative surprises compared to the behavior of symmetric,\ncubic tensor decomposition. Finally, we compare our AMP algorithm to the most\ncommonly used algorithm, alternating least squares (ALS), and demonstrate that\nAMP significantly outperforms ALS in the presence of noise. \n\n"}
{"id": "1810.10132", "contents": "Title: Smoothed Online Optimization for Regression and Control Abstract: We consider Online Convex Optimization (OCO) in the setting where the costs\nare $m$-strongly convex and the online learner pays a switching cost for\nchanging decisions between rounds. We show that the recently proposed Online\nBalanced Descent (OBD) algorithm is constant competitive in this setting, with\ncompetitive ratio $3 + O(1/m)$, irrespective of the ambient dimension.\nAdditionally, we show that when the sequence of cost functions is\n$\\epsilon$-smooth, OBD has near-optimal dynamic regret and maintains strong\nper-round accuracy. We demonstrate the generality of our approach by showing\nthat the OBD framework can be used to construct competitive algorithms for a\nvariety of online problems across learning and control, including online\nvariants of ridge regression, logistic regression, maximum likelihood\nestimation, and LQR control. \n\n"}
{"id": "1810.10180", "contents": "Title: Understanding and correcting pathologies in the training of learned\n  optimizers Abstract: Deep learning has shown that learned functions can dramatically outperform\nhand-designed functions on perceptual tasks. Analogously, this suggests that\nlearned optimizers may similarly outperform current hand-designed optimizers,\nespecially for specific problems. However, learned optimizers are notoriously\ndifficult to train and have yet to demonstrate wall-clock speedups over\nhand-designed optimizers, and thus are rarely used in practice. Typically,\nlearned optimizers are trained by truncated backpropagation through an unrolled\noptimization process resulting in gradients that are either strongly biased\n(for short truncations) or have exploding norm (for long truncations). In this\nwork we propose a training scheme which overcomes both of these difficulties,\nby dynamically weighting two unbiased gradient estimators for a variational\nloss on optimizer performance, allowing us to train neural networks to perform\noptimization of a specific task faster than tuned first-order methods. We\ndemonstrate these results on problems where our learned optimizer trains\nconvolutional networks faster in wall-clock time compared to tuned first-order\nmethods and with an improvement in test loss. \n\n"}
{"id": "1810.10321", "contents": "Title: Active Ranking with Subset-wise Preferences Abstract: We consider the problem of probably approximately correct (PAC) ranking $n$\nitems by adaptively eliciting subset-wise preference feedback. At each round,\nthe learner chooses a subset of $k$ items and observes stochastic feedback\nindicating preference information of the winner (most preferred) item of the\nchosen subset drawn according to a Plackett-Luce (PL) subset choice model\nunknown a priori. The objective is to identify an $\\epsilon$-optimal ranking of\nthe $n$ items with probability at least $1 - \\delta$. When the feedback in each\nsubset round is a single Plackett-Luce-sampled item, we show $(\\epsilon,\n\\delta)$-PAC algorithms with a sample complexity of\n$O\\left(\\frac{n}{\\epsilon^2} \\ln \\frac{n}{\\delta} \\right)$ rounds, which we\nestablish as being order-optimal by exhibiting a matching sample complexity\nlower bound of $\\Omega\\left(\\frac{n}{\\epsilon^2} \\ln \\frac{n}{\\delta}\n\\right)$---this shows that there is essentially no improvement possible from\nthe pairwise comparisons setting ($k = 2$). When, however, it is possible to\nelicit top-$m$ ($\\leq k$) ranking feedback according to the PL model from each\nadaptively chosen subset of size $k$, we show that an $(\\epsilon, \\delta)$-PAC\nranking sample complexity of $O\\left(\\frac{n}{m \\epsilon^2} \\ln\n\\frac{n}{\\delta} \\right)$ is achievable with explicit algorithms, which\nrepresents an $m$-wise reduction in sample complexity compared to the pairwise\ncase. This again turns out to be order-wise unimprovable across the class of\nsymmetric ranking algorithms. Our algorithms rely on a novel {pivot trick} to\nmaintain only $n$ itemwise score estimates, unlike $O(n^2)$ pairwise score\nestimates that has been used in prior work. We report results of numerical\nexperiments that corroborate our findings. \n\n"}
{"id": "1810.10535", "contents": "Title: Meta-modeling game for deriving theoretical-consistent,\n  micro-structural-based traction-separation laws via deep reinforcement\n  learning Abstract: This paper presents a new meta-modeling framework to employ deep\nreinforcement learning (DRL) to generate mechanical constitutive models for\ninterfaces. The constitutive models are conceptualized as information flow in\ndirected graphs. The process of writing constitutive models are simplified as a\nsequence of forming graph edges with the goal of maximizing the model score (a\nfunction of accuracy, robustness and forward prediction quality). Thus\nmeta-modeling can be formulated as a Markov decision process with well-defined\nstates, actions, rules, objective functions, and rewards. By using neural\nnetworks to estimate policies and state values, the computer agent is able to\nefficiently self-improve the constitutive model it generated through\nself-playing, in the same way AlphaGo Zero (the algorithm that outplayed the\nworld champion in the game of Go)improves its gameplay. Our numerical examples\nshow that this automated meta-modeling framework not only produces models which\noutperform existing cohesive models on benchmark traction-separation data but\nis also capable of detecting hidden mechanisms among micro-structural features\nand incorporating them in constitutive models to improve the forward prediction\naccuracy, which are difficult tasks to do manually. \n\n"}
{"id": "1810.10799", "contents": "Title: Relevance of backtracking paths in epidemic spreading on networks Abstract: The understanding of epidemics on networks has greatly benefited from the\nrecent application of message-passing approaches, which allow to derive exact\nresults for irreversible spreading (i.e. diseases with permanent acquired\nimmunity) in locally-tree like topologies. This success has suggested the\napplication of the same approach to reversible epidemics, for which an\nindividual can contract the epidemic and recover repeatedly. The underlying\nassumption is that backtracking paths (i.e. an individual is reinfected by a\nneighbor he/she previously infected) do not play a relevant role. In this paper\nwe show that this is not the case for reversible epidemics, since the neglect\nof backtracking paths leads to a formula for the epidemic threshold that is\nqualitatively incorrect in the large size limit. Moreover we define a modified\nreversible dynamics which explicitly forbids direct backtracking events and\nshow that this modification completely upsets the phenomenology. \n\n"}
{"id": "1810.11900", "contents": "Title: Cultural transmission modes of music sampling traditions remain stable\n  despite delocalization in the digital age Abstract: Music sampling is a common practice among hip-hop and electronic producers\nthat has played a critical role in the development of particular subgenres.\nArtists preferentially sample drum breaks, and previous studies have suggested\nthat these may be culturally transmitted. With the advent of digital sampling\ntechnologies and social media the modes of cultural transmission may have\nshifted, and music communities may have become decoupled from geography. The\naim of the current study was to determine whether drum breaks are culturally\ntransmitted through musical collaboration networks, and to identify the factors\ndriving the evolution of these networks. Using network-based diffusion analysis\nwe found strong evidence for the cultural transmission of drum breaks via\ncollaboration between artists, and identified several demographic variables\nthat bias transmission. Additionally, using network evolution methods we found\nevidence that the structure of the collaboration network is no longer biased by\ngeographic proximity after the year 2000, and that gender disparity has relaxed\nover the same period. Despite the delocalization of communities by the\ninternet, collaboration remains a key transmission mode of music sampling\ntraditions. The results of this study provide valuable insight into how\ndemographic biases shape cultural transmission in complex networks, and how the\nevolution of these networks has shifted in the digital age. \n\n"}
{"id": "1811.00007", "contents": "Title: Robustly Disentangled Causal Mechanisms: Validating Deep Representations\n  for Interventional Robustness Abstract: The ability to learn disentangled representations that split underlying\nsources of variation in high dimensional, unstructured data is important for\ndata efficient and robust use of neural networks. While various approaches\naiming towards this goal have been proposed in recent times, a commonly\naccepted definition and validation procedure is missing. We provide a causal\nperspective on representation learning which covers disentanglement and domain\nshift robustness as special cases. Our causal framework allows us to introduce\na new metric for the quantitative evaluation of deep latent variable models. We\nshow how this metric can be estimated from labeled observational data and\nfurther provide an efficient estimation algorithm that scales linearly in the\ndataset size. \n\n"}
{"id": "1811.00076", "contents": "Title: Large Tournament Games Abstract: We consider a stochastic tournament game in which each player is rewarded\nbased on her rank in terms of the completion time of her own task and is\nsubject to cost of effort. When players are homogeneous and the rewards are\npurely rank dependent, the equilibrium has a surprisingly explicit\ncharacterization, which allows us to conduct comparative statics and obtain\nexplicit solution to several optimal reward design problems. In the general\ncase when the players are heterogenous and payoffs are not purely rank\ndependent, we prove the existence, uniqueness and stability of the Nash\nequilibrium of the associated mean field game, and the existence of an\napproximate Nash equilibrium of the finite-player game. Our results have some\npotential economic implications; e.g., they lend support to government\nsubsidies for R and D, assuming the profits to be made are substantial. \n\n"}
{"id": "1811.00429", "contents": "Title: Temporal Regularization in Markov Decision Process Abstract: Several applications of Reinforcement Learning suffer from instability due to\nhigh variance. This is especially prevalent in high dimensional domains.\nRegularization is a commonly used technique in machine learning to reduce\nvariance, at the cost of introducing some bias. Most existing regularization\ntechniques focus on spatial (perceptual) regularization. Yet in reinforcement\nlearning, due to the nature of the Bellman equation, there is an opportunity to\nalso exploit temporal regularization based on smoothness in value estimates\nover trajectories. This paper explores a class of methods for temporal\nregularization. We formally characterize the bias induced by this technique\nusing Markov chain concepts. We illustrate the various characteristics of\ntemporal regularization via a sequence of simple discrete and continuous MDPs,\nand show that the technique provides improvement even in high-dimensional Atari\ngames. \n\n"}
{"id": "1811.01132", "contents": "Title: VIREL: A Variational Inference Framework for Reinforcement Learning Abstract: Applying probabilistic models to reinforcement learning (RL) enables the\napplication of powerful optimisation tools such as variational inference to RL.\nHowever, existing inference frameworks and their algorithms pose significant\nchallenges for learning optimal policies, e.g., the absence of mode capturing\nbehaviour in pseudo-likelihood methods and difficulties learning deterministic\npolicies in maximum entropy RL based approaches. We propose VIREL, a novel,\ntheoretically grounded probabilistic inference framework for RL that utilises a\nparametrised action-value function to summarise future dynamics of the\nunderlying MDP. This gives VIREL a mode-seeking form of KL divergence, the\nability to learn deterministic optimal polices naturally from inference and the\nability to optimise value functions and policies in separate, iterative steps.\nIn applying variational expectation-maximisation to VIREL we thus show that the\nactor-critic algorithm can be reduced to expectation-maximisation, with policy\nimprovement equivalent to an E-step and policy evaluation to an M-step. We then\nderive a family of actor-critic methods from VIREL, including a scheme for\nadaptive exploration. Finally, we demonstrate that actor-critic algorithms from\nthis family outperform state-of-the-art methods based on soft value functions\nin several domains. \n\n"}
{"id": "1811.01174", "contents": "Title: Nonparallel Emotional Speech Conversion Abstract: We propose a nonparallel data-driven emotional speech conversion method. It\nenables the transfer of emotion-related characteristics of a speech signal\nwhile preserving the speaker's identity and linguistic content. Most existing\napproaches require parallel data and time alignment, which is not available in\nmost real applications. We achieve nonparallel training based on an\nunsupervised style transfer technique, which learns a translation model between\ntwo distributions instead of a deterministic one-to-one mapping between paired\nexamples. The conversion model consists of an encoder and a decoder for each\nemotion domain. We assume that the speech signal can be decomposed into an\nemotion-invariant content code and an emotion-related style code in latent\nspace. Emotion conversion is performed by extracting and recombining the\ncontent code of the source speech and the style code of the target emotion. We\ntested our method on a nonparallel corpora with four emotions. Both subjective\nand objective evaluations show the effectiveness of our approach. \n\n"}
{"id": "1811.01302", "contents": "Title: Adversarial Gain Abstract: Adversarial examples can be defined as inputs to a model which induce a\nmistake - where the model output is different than that of an oracle, perhaps\nin surprising or malicious ways. Original models of adversarial attacks are\nprimarily studied in the context of classification and computer vision tasks.\nWhile several attacks have been proposed in natural language processing (NLP)\nsettings, they often vary in defining the parameters of an attack and what a\nsuccessful attack would look like. The goal of this work is to propose a\nunifying model of adversarial examples suitable for NLP tasks in both\ngenerative and classification settings. We define the notion of adversarial\ngain: based in control theory, it is a measure of the change in the output of a\nsystem relative to the perturbation of the input (caused by the so-called\nadversary) presented to the learner. This definition, as we show, can be used\nunder different feature spaces and distance conditions to determine attack or\ndefense effectiveness across different intuitive manifolds. This notion of\nadversarial gain not only provides a useful way for evaluating adversaries and\ndefenses, but can act as a building block for future work in robustness under\nadversaries due to its rooted nature in stability and manifold theory. \n\n"}
{"id": "1811.01452", "contents": "Title: Assembly in populations of social networks Abstract: In-depth studies of sociotechnical systems are largely limited to single\ninstances. Network surveys are expensive, and platforms vary in important ways,\nfrom interface design, to social norms, to historical contingencies. With\nsingle examples, we can not in general know how much of observed network\nstructure is explained by historical accidents, random noise, or meaningful\nsocial processes, nor can we claim that network structure predicts outcomes,\nsuch as organization success or ecosystem health. Here, I show how we can adopt\na comparative approach for settings where we have, or can cleverly construct,\nmultiple instances of a network to estimate the natural variability in social\nsystems. The comparative approach makes previously untested theories testable.\nDrawing on examples from the social networks literature, I discuss emerging\ndirections in the study of populations of sociotechnical systems using insights\nfrom organization theory and ecology. \n\n"}
{"id": "1811.01777", "contents": "Title: Non-ergodic Convergence Analysis of Heavy-Ball Algorithms Abstract: In this paper, we revisit the convergence of the Heavy-ball method, and\npresent improved convergence complexity results in the convex setting. We\nprovide the first non-ergodic O(1/k) rate result of the Heavy-ball algorithm\nwith constant step size for coercive objective functions. For objective\nfunctions satisfying a relaxed strongly convex condition, the linear\nconvergence is established under weaker assumptions on the step size and\ninertial parameter than made in the existing literature. We extend our results\nto multi-block version of the algorithm with both the cyclic and stochastic\nupdate rules. In addition, our results can also be extended to decentralized\noptimization, where the ergodic analysis is not applicable. \n\n"}
{"id": "1811.01940", "contents": "Title: Dynamic Programming Deconstructed: Transformations of the Bellman\n  Equation and Computational Efficiency Abstract: Some approaches to solving challenging dynamic programming problems, such as\nQ-learning, begin by transforming the Bellman equation into an alternative\nfunctional equation, in order to open up a new line of attack. Our paper\nstudies this idea systematically, with a focus on boosting computational\nefficiency. We provide a characterization of the set of valid transformations\nof the Bellman equation, where validity means that the transformed Bellman\nequation maintains the link to optimality held by the original Bellman\nequation. We then examine the solutions of the transformed Bellman equations\nand analyze correspondingly transformed versions of the algorithms used to\nsolve for optimal policies. These investigations yield new approaches to a\nvariety of discrete time dynamic programming problems, including those with\nfeatures such as recursive preferences or desire for robustness. Increased\ncomputational efficiency is demonstrated via time complexity arguments and\nnumerical experiments. \n\n"}
{"id": "1811.02525", "contents": "Title: Double Adaptive Stochastic Gradient Optimization Abstract: Adaptive moment methods have been remarkably successful in deep learning\noptimization, particularly in the presence of noisy and/or sparse gradients. We\nfurther the advantages of adaptive moment techniques by proposing a family of\ndouble adaptive stochastic gradient methods~\\textsc{DASGrad}. They leverage the\ncomplementary ideas of the adaptive moment algorithms widely used by deep\nlearning community, and recent advances in adaptive probabilistic algorithms.We\nanalyze the theoretical convergence improvements of our approach in a\nstochastic convex optimization setting, and provide empirical validation of our\nfindings with convex and non convex objectives. We observe that the benefits\nof~\\textsc{DASGrad} increase with the model complexity and variability of the\ngradients, and we explore the resulting utility in extensions of\ndistribution-matching multitask learning. \n\n"}
{"id": "1811.02693", "contents": "Title: Deep Reinforcement Learning via L-BFGS Optimization Abstract: Reinforcement Learning (RL) algorithms allow artificial agents to improve\ntheir action selections so as to increase rewarding experiences in their\nenvironments. Deep Reinforcement Learning algorithms require solving a\nnonconvex and nonlinear unconstrained optimization problem. Methods for solving\nthe optimization problems in deep RL are restricted to the class of first-order\nalgorithms, such as stochastic gradient descent (SGD). The major drawback of\nthe SGD methods is that they have the undesirable effect of not escaping saddle\npoints and their performance can be seriously obstructed by ill-conditioning.\nFurthermore, SGD methods require exhaustive trial and error to fine-tune many\nlearning parameters. Using second derivative information can result in improved\nconvergence properties, but computing the Hessian matrix for large-scale\nproblems is not practical. Quasi-Newton methods require only first-order\ngradient information, like SGD, but they can construct a low rank approximation\nof the Hessian matrix and result in superlinear convergence. The limited-memory\nBroyden-Fletcher-Goldfarb-Shanno (L-BFGS) approach is one of the most popular\nquasi-Newton methods that construct positive definite Hessian approximations.\nIn this paper, we introduce an efficient optimization method, based on the\nlimited memory BFGS quasi-Newton method using line search strategy -- as an\nalternative to SGD methods. Our method bridges the disparity between first\norder methods and second order methods by continuing to use gradient\ninformation to calculate a low-rank Hessian approximations. We provide formal\nconvergence analysis as well as empirical results on a subset of the classic\nATARI 2600 games. Our results show a robust convergence with preferred\ngeneralization characteristics, as well as fast training time and no need for\nthe experience replaying mechanism. \n\n"}
{"id": "1811.02783", "contents": "Title: YASENN: Explaining Neural Networks via Partitioning Activation Sequences Abstract: We introduce a novel approach to feed-forward neural network interpretation\nbased on partitioning the space of sequences of neuron activations. In line\nwith this approach, we propose a model-specific interpretation method, called\nYASENN. Our method inherits many advantages of model-agnostic distillation,\nsuch as an ability to focus on the particular input region and to express an\nexplanation in terms of features different from those observed by a neural\nnetwork. Moreover, examination of distillation error makes the method\napplicable to the problems with low tolerance to interpretation mistakes.\nTechnically, YASENN distills the network with an ensemble of layer-wise\ngradient boosting decision trees and encodes the sequences of neuron\nactivations with leaf indices. The finite number of unique codes induces a\npartitioning of the input space. Each partition may be described in a variety\nof ways, including examination of an interpretable model (e.g. a logistic\nregression or a decision tree) trained to discriminate between objects of those\npartitions. Our experiments provide an intuition behind the method and\ndemonstrate revealed artifacts in neural network decision making. \n\n"}
{"id": "1811.03706", "contents": "Title: Maximizing Diversity of Opinion in Social Networks Abstract: We study the problem of maximizing opinion diversity in a social network that\nincludes opinion leaders with binary opposing opinions. The members of the\nnetwork who are not leaders form their opinions using the French-DeGroot model\nof opinion dynamics. To quantify the diversity of such a system, we adapt two\ndiversity measures from ecology to our setting, the Simpson Diversity Index and\nthe Shannon Index. Using these two measures, we formalize the problem of how to\nplace a single leader with opinion 1, given a network with a leader with\nopinion 0, so as to maximize the opinion diversity. We give analytical\nsolutions to these problems for paths, cycles, and trees, and we highlight our\nresults through a numerical example. \n\n"}
{"id": "1811.03717", "contents": "Title: Fast determinantal point processes via distortion-free intermediate\n  sampling Abstract: Given a fixed $n\\times d$ matrix $\\mathbf{X}$, where $n\\gg d$, we study the\ncomplexity of sampling from a distribution over all subsets of rows where the\nprobability of a subset is proportional to the squared volume of the\nparallelepiped spanned by the rows (a.k.a. a determinantal point process). In\nthis task, it is important to minimize the preprocessing cost of the procedure\n(performed once) as well as the sampling cost (performed repeatedly). To that\nend, we propose a new determinantal point process algorithm which has the\nfollowing two properties, both of which are novel: (1) a preprocessing step\nwhich runs in time $O(\\text{number-of-non-zeros}(\\mathbf{X})\\cdot\\log\nn)+\\text{poly}(d)$, and (2) a sampling step which runs in $\\text{poly}(d)$\ntime, independent of the number of rows $n$. We achieve this by introducing a\nnew regularized determinantal point process (R-DPP), which serves as an\nintermediate distribution in the sampling procedure by reducing the number of\nrows from $n$ to $\\text{poly}(d)$. Crucially, this intermediate distribution\ndoes not distort the probabilities of the target sample. Our key novelty in\ndefining the R-DPP is the use of a Poisson random variable for controlling the\nprobabilities of different subset sizes, leading to new determinantal formulas\nsuch as the normalization constant for this distribution. Our algorithm has\napplications in many diverse areas where determinantal point processes have\nbeen used, such as machine learning, stochastic optimization, data\nsummarization and low-rank matrix reconstruction. \n\n"}
{"id": "1811.03968", "contents": "Title: Collaboratively Learning the Best Option on Graphs, Using Bounded Local\n  Memory Abstract: We consider multi-armed bandit problems in social groups wherein each\nindividual has bounded memory and shares the common goal of learning the best\narm/option. We say an individual learns the best option if eventually (as $t\\to\n\\infty$) it pulls only the arm with the highest expected reward. While this\ngoal is provably impossible for an isolated individual due to bounded memory,\nwe show that, in social groups, this goal can be achieved easily with the aid\nof social persuasion (i.e., communication) as long as the communication\nnetworks/graphs satisfy some mild conditions. To deal with the interplay\nbetween the randomness in the rewards and in the social interaction, we employ\nthe {\\em mean-field approximation} method. Considering the possibility that the\nindividuals in the networks may not be exchangeable when the communication\nnetworks are not cliques, we go beyond the classic mean-field techniques and\napply a refined version of mean-field approximation:\n  (1) Using coupling we show that, if the communication graph is connected and\nis either regular or has doubly-stochastic degree-weighted adjacency matrix,\nwith probability $\\to 1$ as the social group size $N \\to \\infty$, every\nindividual in the social group learns the best option.\n  (2) If the minimum degree of the graph diverges as $N \\to \\infty$, over an\narbitrary but given finite time horizon, the sample paths describing the\nopinion evolutions of the individuals are asymptotically independent. In\naddition, the proportions of the population with different opinions converge to\nthe unique solution of a system of ODEs. In the solution of the obtained ODEs,\nthe proportion of the population holding the correct opinion converges to $1$\nexponentially fast in time.\n  Notably, our results hold even if the communication graphs are highly sparse. \n\n"}
{"id": "1811.04713", "contents": "Title: Gauges, Loops, and Polynomials for Partition Functions of Graphical\n  Models Abstract: Graphical models represent multivariate and generally not normalized\nprobability distributions. Computing the normalization factor, called the\npartition function, is the main inference challenge relevant to multiple\nstatistical and optimization applications. The problem is of an exponential\ncomplexity with respect to the number of variables. In this manuscript, aimed\nat approximating the PF, we consider Multi-Graph Models where binary variables\nand multivariable factors are associated with edges and nodes, respectively, of\nan undirected multi-graph. We suggest a new methodology for analysis and\ncomputations that combines the Gauge Function technique with the technique from\nthe field of real stable polynomials. We show that the Gauge Function has a\nnatural polynomial representation in terms of gauges/variables associated with\nedges of the multi-graph. Moreover, it can be used to recover the Partition\nFunction through a sequence of transformations allowing appealing algebraic and\ngraphical interpretations. Algebraically, one step in the sequence consists in\napplication of a differential operator over gauges associated with an edge.\nGraphically, the sequence is interpreted as a repetitive elimination of edges\nresulting in a sequence of models on decreasing in size graphs with the same\nPartition Function. Even though complexity of computing factors in the sequence\nmodels grow exponentially with the number of eliminated edges, polynomials\nassociated with the new factors remain bi-stable if the original factors have\nthis property. Moreover, we show that Belief Propagation estimations in the\nsequence do not decrease, each low-bounding the Partition Function. \n\n"}
{"id": "1811.05321", "contents": "Title: Correction of AI systems by linear discriminants: Probabilistic\n  foundations Abstract: Artificial Intelligence (AI) systems sometimes make errors and will make\nerrors in the future, from time to time. These errors are usually unexpected,\nand can lead to dramatic consequences. Intensive development of AI and its\npractical applications makes the problem of errors more important. Total\nre-engineering of the systems can create new errors and is not always possible\ndue to the resources involved. The important challenge is to develop fast\nmethods to correct errors without damaging existing skills. We formulated the\ntechnical requirements to the 'ideal' correctors. Such correctors include\nbinary classifiers, which separate the situations with high risk of errors from\nthe situations where the AI systems work properly. Surprisingly, for\nessentially high-dimensional data such methods are possible: simple linear\nFisher discriminant can separate the situations with errors from correctly\nsolved tasks even for exponentially large samples. The paper presents the\nprobabilistic basis for fast non-destructive correction of AI systems. A series\nof new stochastic separation theorems is proven. These theorems provide new\ninstruments for fast non-iterative correction of errors of legacy AI systems.\nThe new approaches become efficient in high-dimensions, for correction of\nhigh-dimensional systems in high-dimensional world (i.e. for processing of\nessentially high-dimensional data by large systems). \n\n"}
{"id": "1811.07209", "contents": "Title: A Statistical Approach to Assessing Neural Network Robustness Abstract: We present a new approach to assessing the robustness of neural networks\nbased on estimating the proportion of inputs for which a property is violated.\nSpecifically, we estimate the probability of the event that the property is\nviolated under an input model. Our approach critically varies from the formal\nverification framework in that when the property can be violated, it provides\nan informative notion of how robust the network is, rather than just the\nconventional assertion that the network is not verifiable. Furthermore, it\nprovides an ability to scale to larger networks than formal verification\napproaches. Though the framework still provides a formal guarantee of\nsatisfiability whenever it successfully finds one or more violations, these\nadvantages do come at the cost of only providing a statistical estimate of\nunsatisfiability whenever no violation is found. Key to the practical success\nof our approach is an adaptation of multi-level splitting, a Monte Carlo\napproach for estimating the probability of rare events, to our statistical\nrobustness framework. We demonstrate that our approach is able to emulate\nformal verification procedures on benchmark problems, while scaling to larger\nnetworks and providing reliable additional information in the form of accurate\nestimates of the violation probability. \n\n"}
{"id": "1811.07429", "contents": "Title: Stochastic Deep Networks Abstract: Machine learning is increasingly targeting areas where input data cannot be\naccurately described by a single vector, but can be modeled instead using the\nmore flexible concept of random vectors, namely probability measures or more\nsimply point clouds of varying cardinality. Using deep architectures on\nmeasures poses, however, many challenging issues. Indeed, deep architectures\nare originally designed to handle fixedlength vectors, or, using recursive\nmechanisms, ordered sequences thereof. In sharp contrast, measures describe a\nvarying number of weighted observations with no particular order. We propose in\nthis work a deep framework designed to handle crucial aspects of measures,\nnamely permutation invariances, variations in weights and cardinality.\nArchitectures derived from this pipeline can (i) map measures to measures -\nusing the concept of push-forward operators; (ii) bridge the gap between\nmeasures and Euclidean spaces - through integration steps. This allows to\ndesign discriminative networks (to classify or reduce the dimensionality of\ninput measures), generative architectures (to synthesize measures) and\nrecurrent pipelines (to predict measure dynamics). We provide a theoretical\nanalysis of these building blocks, review our architectures' approximation\nabilities and robustness w.r.t. perturbation, and try them on various\ndiscriminative and generative tasks. \n\n"}
{"id": "1811.07455", "contents": "Title: On Geometric Alignment in Low Doubling Dimension Abstract: In real-world, many problems can be formulated as the alignment between two\ngeometric patterns. Previously, a great amount of research focus on the\nalignment of 2D or 3D patterns, especially in the field of computer vision.\nRecently, the alignment of geometric patterns in high dimension finds several\nnovel applications, and has attracted more and more attentions. However, the\nresearch is still rather limited in terms of algorithms. To the best of our\nknowledge, most existing approaches for high dimensional alignment are just\nsimple extensions of their counterparts for 2D and 3D cases, and often suffer\nfrom the issues such as high complexities. In this paper, we propose an\neffective framework to compress the high dimensional geometric patterns and\napproximately preserve the alignment quality. As a consequence, existing\nalignment approach can be applied to the compressed geometric patterns and thus\nthe time complexity is significantly reduced. Our idea is inspired by the\nobservation that high dimensional data often has a low intrinsic dimension. We\nadopt the widely used notion \"doubling dimension\" to measure the extents of our\ncompression and the resulting approximation. Finally, we test our method on\nboth random and real datasets, the experimental results reveal that running the\nalignment algorithm on compressed patterns can achieve similar qualities,\ncomparing with the results on the original patterns, but the running times\n(including the times cost for compression) are substantially lower. \n\n"}
{"id": "1811.08109", "contents": "Title: Faster First-Order Methods for Stochastic Non-Convex Optimization on\n  Riemannian Manifolds Abstract: SPIDER (Stochastic Path Integrated Differential EstimatoR) is an efficient\ngradient estimation technique developed for non-convex stochastic optimization.\nAlthough having been shown to attain nearly optimal computational complexity\nbounds, the SPIDER-type methods are limited to linear metric spaces. In this\npaper, we introduce the Riemannian SPIDER (R-SPIDER) method as a novel\nnonlinear-metric extension of SPIDER for efficient non-convex optimization on\nRiemannian manifolds. We prove that for finite-sum problems with $n$\ncomponents, R-SPIDER converges to an $\\epsilon$-accuracy stationary point\nwithin\n$\\mathcal{O}\\big(\\min\\big(n+\\frac{\\sqrt{n}}{\\epsilon^2},\\frac{1}{\\epsilon^3}\\big)\\big)$\nstochastic gradient evaluations, which is sharper in magnitude than the prior\nRiemannian first-order methods. For online optimization, R-SPIDER is shown to\nconverge with $\\mathcal{O}\\big(\\frac{1}{\\epsilon^3}\\big)$ complexity which is,\nto the best of our knowledge, the first non-asymptotic result for online\nRiemannian optimization. Especially, for gradient dominated functions, we\nfurther develop a variant of R-SPIDER and prove its linear convergence rate.\nNumerical results demonstrate the computational efficiency of the proposed\nmethods. \n\n"}
{"id": "1811.08511", "contents": "Title: Joint association and classification analysis of multi-view data Abstract: Multi-view data, that is matched sets of measurements on the same subjects,\nhave become increasingly common with advances in multi-omics technology. Often,\nit is of interest to find associations between the views that are related to\nthe intrinsic class memberships. Existing association methods cannot directly\nincorporate class information, while existing classification methods do not\ntake into account between-views associations. In this work, we propose a\nframework for Joint Association and Classification Analysis of multi-view data\n(JACA). Our goal is not to merely improve the misclassification rates, but to\nprovide a latent representation of high-dimensional data that is both relevant\nfor the subtype discrimination and coherent across the views. We motivate the\nmethodology by establishing a connection between canonical correlation analysis\nand discriminant analysis. We also establish the estimation consistency of JACA\nin high-dimensional settings. A distinct advantage of JACA is that it can be\napplied to the multi-view data with block-missing structure, that is to cases\nwhere a subset of views or class labels is missing for some subjects. The\napplication of JACA to quantify the associations between RNAseq and miRNA views\nwith respect to consensus molecular subtypes in colorectal cancer data from The\nCancer Genome Atlas project leads to improved misclassification rates and\nstronger found associations compared to existing methods. \n\n"}
{"id": "1811.08968", "contents": "Title: Spread Divergence Abstract: For distributions $\\mathbb{P}$ and $\\mathbb{Q}$ with different supports or\nundefined densities, the divergence $\\textrm{D}(\\mathbb{P}||\\mathbb{Q})$ may\nnot exist. We define a Spread Divergence\n$\\tilde{\\textrm{D}}(\\mathbb{P}||\\mathbb{Q})$ on modified $\\mathbb{P}$ and\n$\\mathbb{Q}$ and describe sufficient conditions for the existence of such a\ndivergence. We demonstrate how to maximize the discriminatory power of a given\ndivergence by parameterizing and learning the spread. We also give examples of\nusing a Spread Divergence to train implicit generative models, including linear\nmodels (Independent Components Analysis) and non-linear models (Deep Generative\nNetworks). \n\n"}
{"id": "1811.10146", "contents": "Title: Frequency Principle in Deep Learning with General Loss Functions and Its\n  Potential Application Abstract: Previous studies have shown that deep neural networks (DNNs) with common\nsettings often capture target functions from low to high frequency, which is\ncalled Frequency Principle (F-Principle). It has also been shown that\nF-Principle can provide an understanding to the often observed good\ngeneralization ability of DNNs. However, previous studies focused on the loss\nfunction of mean square error, while various loss functions are used in\npractice. In this work, we show that the F-Principle holds for a general loss\nfunction (e.g., mean square error, cross entropy, etc.). In addition, DNN's\nF-Principle may be applied to develop numerical schemes for solving various\nproblems which would benefit from a fast converging of low frequency. As an\nexample of the potential usage of F-Principle, we apply DNN in solving\ndifferential equations, in which conventional methods (e.g., Jacobi method) is\nusually slow in solving problems due to the convergence from high to low\nfrequency. \n\n"}
{"id": "1811.10337", "contents": "Title: Multiple Partitioning of Multiplex Signed Networks: Application to\n  European Parliament Votes Abstract: For more than a decade, graphs have been used to model the voting behavior\ntaking place in parliaments. However, the methods described in the literature\nsuffer from several limitations. The two main ones are that 1) they rely on\nsome temporal integration of the raw data, which causes some information loss,\nand/or 2) they identify groups of antagonistic voters, but not the context\nassociated to their occurrence. In this article, we propose a novel method\ntaking advantage of multiplex signed graphs to solve both these issues. It\nconsists in first partitioning separately each layer, before grouping these\npartitions by similarity. We show the interest of our approach by applying it\nto a European Parliament dataset. \n\n"}
{"id": "1811.11212", "contents": "Title: Self-Supervised GANs via Auxiliary Rotation Loss Abstract: Conditional GANs are at the forefront of natural image synthesis. The main\ndrawback of such models is the necessity for labeled data. In this work we\nexploit two popular unsupervised learning techniques, adversarial training and\nself-supervision, and take a step towards bridging the gap between conditional\nand unconditional GANs. In particular, we allow the networks to collaborate on\nthe task of representation learning, while being adversarial with respect to\nthe classic GAN game. The role of self-supervision is to encourage the\ndiscriminator to learn meaningful feature representations which are not\nforgotten during training. We test empirically both the quality of the learned\nimage representations, and the quality of the synthesized images. Under the\nsame conditions, the self-supervised GAN attains a similar performance to\nstate-of-the-art conditional counterparts. Finally, we show that this approach\nto fully unsupervised learning can be scaled to attain an FID of 23.4 on\nunconditional ImageNet generation. \n\n"}
{"id": "1811.11684", "contents": "Title: Shared Representational Geometry Across Neural Networks Abstract: Different neural networks trained on the same dataset often learn similar\ninput-output mappings with very different weights. Is there some correspondence\nbetween these neural network solutions? For linear networks, it has been shown\nthat different instances of the same network architecture encode the same\nrepresentational similarity matrix, and their neural activity patterns are\nconnected by orthogonal transformations. However, it is unclear if this holds\nfor non-linear networks. Using a shared response model, we show that different\nneural networks encode the same input examples as different orthogonal\ntransformations of an underlying shared representation. We test this claim\nusing both standard convolutional neural networks and residual networks on\nCIFAR10 and CIFAR100. \n\n"}
{"id": "1811.12183", "contents": "Title: Analyzing and provably improving fixed budget ranking and selection\n  algorithms Abstract: This paper studies the fixed budget formulation of the Ranking and Selection\n(R&S) problem with independent normal samples, where the goal is to investigate\ndifferent algorithms' convergence rate in terms of their resulting probability\nof false selection (PFS). First, we reveal that for the well-known Optimal\nComputing Budget Allocation (OCBA) algorithm and its two variants, a constant\ninitial sample size (independent of the total budget) only amounts to a\nsub-exponential (or even polynomial) convergence rate. After that, a\nmodification is proposed to achieve an exponential convergence rate, where the\nimprovement is shown by a finite-sample bound on the PFS as well as numerical\nresults. Finally, we focus on a more tractable two-design case and explicitly\ncharacterize the large deviations rate of PFS for some simplified algorithms.\nOur analysis not only develops insights into the algorithms' properties, but\nalso highlights several useful techniques for analyzing the convergence rate of\nfixed budget R\\&S algorithms. \n\n"}
{"id": "1811.12583", "contents": "Title: Rethinking clinical prediction: Why machine learning must consider year\n  of care and feature aggregation Abstract: Machine learning for healthcare often trains models on de-identified datasets\nwith randomly-shifted calendar dates, ignoring the fact that data were\ngenerated under hospital operation practices that change over time. These\nchanging practices induce definitive changes in observed data which confound\nevaluations which do not account for dates and limit the generalisability of\ndate-agnostic models. In this work, we establish the magnitude of this problem\non MIMIC, a public hospital dataset, and showcase a simple solution. We augment\nMIMIC with the year in which care was provided and show that a model trained\nusing standard feature representations will significantly degrade in quality\nover time. We find a deterioration of 0.3 AUC when evaluating mortality\nprediction on data from 10 years later. We find a similar deterioration of 0.15\nAUC for length-of-stay. In contrast, we demonstrate that clinically-oriented\naggregates of raw features significantly mitigate future deterioration. Our\nsuggested aggregated representations, when retrained yearly, have prediction\nquality comparable to year-agnostic models. \n\n"}
{"id": "1811.12739", "contents": "Title: Neural separation of observed and unobserved distributions Abstract: Separating mixed distributions is a long standing challenge for machine\nlearning and signal processing. Most current methods either rely on making\nstrong assumptions on the source distributions or rely on having training\nsamples of each source in the mixture. In this work, we introduce a new\nmethod---Neural Egg Separation---to tackle the scenario of extracting a signal\nfrom an unobserved distribution additively mixed with a signal from an observed\ndistribution. Our method iteratively learns to separate the known distribution\nfrom progressively finer estimates of the unknown distribution. In some\nsettings, Neural Egg Separation is initialization sensitive, we therefore\nintroduce Latent Mixture Masking which ensures a good initialization. Extensive\nexperiments on audio and image separation tasks show that our method\noutperforms current methods that use the same level of supervision, and often\nachieves similar performance to full supervision. \n\n"}
{"id": "1812.00146", "contents": "Title: Operator Splitting Performance Estimation: Tight contraction factors and\n  optimal parameter selection Abstract: We propose a methodology for studying the performance of common splitting\nmethods through semidefinite programming. We prove tightness of the methodology\nand demonstrate its value by presenting two applications of it. First, we use\nthe methodology as a tool for computer-assisted proofs to prove tight\nanalytical contraction factors for Douglas--Rachford splitting that are likely\ntoo complicated for a human to find bare-handed. Second, we use the methodology\nas an algorithmic tool to computationally select the optimal splitting method\nparameters by solving a series of semidefinite programs. \n\n"}
{"id": "1812.00332", "contents": "Title: ProxylessNAS: Direct Neural Architecture Search on Target Task and\n  Hardware Abstract: Neural architecture search (NAS) has a great impact by automatically\ndesigning effective neural network architectures. However, the prohibitive\ncomputational demand of conventional NAS algorithms (e.g. $10^4$ GPU hours)\nmakes it difficult to \\emph{directly} search the architectures on large-scale\ntasks (e.g. ImageNet). Differentiable NAS can reduce the cost of GPU hours via\na continuous representation of network architecture but suffers from the high\nGPU memory consumption issue (grow linearly w.r.t. candidate set size). As a\nresult, they need to utilize~\\emph{proxy} tasks, such as training on a smaller\ndataset, or learning with only a few blocks, or training just for a few epochs.\nThese architectures optimized on proxy tasks are not guaranteed to be optimal\non the target task. In this paper, we present \\emph{ProxylessNAS} that can\n\\emph{directly} learn the architectures for large-scale target tasks and target\nhardware platforms. We address the high memory consumption issue of\ndifferentiable NAS and reduce the computational cost (GPU hours and GPU memory)\nto the same level of regular training while still allowing a large candidate\nset. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of\ndirectness and specialization. On CIFAR-10, our model achieves 2.08\\% test\nerror with only 5.7M parameters, better than the previous state-of-the-art\narchitecture AmoebaNet-B, while using 6$\\times$ fewer parameters. On ImageNet,\nour model achieves 3.1\\% better top-1 accuracy than MobileNetV2, while being\n1.2$\\times$ faster with measured GPU latency. We also apply ProxylessNAS to\nspecialize neural architectures for hardware with direct hardware metrics (e.g.\nlatency) and provide insights for efficient CNN architecture design. \n\n"}
{"id": "1812.00463", "contents": "Title: Personalizing Intervention Probabilities By Pooling Abstract: In many mobile health interventions, treatments should only be delivered in a\nparticular context, for example when a user is currently stressed, walking or\nsedentary. Even in an optimal context, concerns about user burden can restrict\nwhich treatments are sent. To diffuse the treatment delivery over times when a\nuser is in a desired context, it is critical to predict the future number of\ntimes the context will occur. The focus of this paper is on whether\npersonalization can improve predictions in these settings. Though the variance\nbetween individuals' behavioral patterns suggest that personalization should be\nuseful, the amount of individual-level data limits its capabilities. Thus, we\ninvestigate several methods which pool data across users to overcome these\ndeficiencies and find that pooling lowers the overall error rate relative to\nboth personalized and batch approaches. \n\n"}
{"id": "1812.00557", "contents": "Title: Signal Reconstruction from Modulo Observations Abstract: We consider the problem of reconstructing a signal from under-determined\nmodulo observations (or measurements). This observation model is inspired by a\n(relatively) less well-known imaging mechanism called modulo imaging, which can\nbe used to extend the dynamic range of imaging systems; variations of this\nmodel have also been studied under the category of phase unwrapping. Signal\nreconstruction in the under-determined regime with modulo observations is a\nchallenging ill-posed problem, and existing reconstruction methods cannot be\nused directly. In this paper, we propose a novel approach to solving the\ninverse problem limited to two modulo periods, inspired by recent advances in\nalgorithms for phase retrieval under sparsity constraints. We show that given a\nsufficient number of measurements, our algorithm perfectly recovers the\nunderlying signal and provides improved performance over other existing\nalgorithms. We also provide experiments validating our approach on both\nsynthetic and real data to depict its superior performance. \n\n"}
{"id": "1812.00632", "contents": "Title: Linear-Quadratic McKean-Vlasov Stochastic Differential Games Abstract: We consider a multi-player stochastic differential game with linear\nMcKean-Vlasov dynamics and quadratic cost functional depending on the variance\nand mean of the state and control actions of the players in open-loop form.\nFinite and infinite horizon problems with possibly some random coefficients as\nwell as common noise are addressed. We propose a simple direct approach based\non weak martingale optimality principle together with a fixed point argument in\nthe space of controls for solving this game problem. The Nash equilibria are\ncharacterized in terms of systems of Riccati ordinary differential equations\nand linear mean-field backward stochastic differential equations: existence and\nuniqueness conditions are provided for such systems. Finally, we illustrate our\nresults on a toy example. \n\n"}
{"id": "1812.00979", "contents": "Title: Deep Reinforcement Learning for Intelligent Transportation Systems Abstract: Intelligent Transportation Systems (ITSs) are envisioned to play a critical\nrole in improving traffic flow and reducing congestion, which is a pervasive\nissue impacting urban areas around the globe. Rapidly advancing vehicular\ncommunication and edge cloud computation technologies provide key enablers for\nsmart traffic management. However, operating viable real-time actuation\nmechanisms on a practically relevant scale involves formidable challenges,\ne.g., policy iteration and conventional Reinforcement Learning (RL) techniques\nsuffer from poor scalability due to state space explosion. Motivated by these\nissues, we explore the potential for Deep Q-Networks (DQN) to optimize traffic\nlight control policies. As an initial benchmark, we establish that the DQN\nalgorithms yield the \"thresholding\" policy in a single-intersection. Next, we\nexamine the scalability properties of DQN algorithms and their performance in a\nlinear network topology with several intersections along a main artery. We\ndemonstrate that DQN algorithms produce intelligent behavior, such as the\nemergence of \"greenwave\" patterns, reflecting their ability to learn favorable\ntraffic light actuations. \n\n"}
{"id": "1812.01106", "contents": "Title: Improving Traffic Safety Through Video Analysis in Jakarta, Indonesia Abstract: This project presents the results of a partnership between the Data Science\nfor Social Good fellowship, Jakarta Smart City and Pulse Lab Jakarta to create\na video analysis pipeline for the purpose of improving traffic safety in\nJakarta. The pipeline transforms raw traffic video footage into databases that\nare ready to be used for traffic analysis. By analyzing these patterns, the\ncity of Jakarta will better understand how human behavior and built\ninfrastructure contribute to traffic challenges and safety risks. The results\nof this work should also be broadly applicable to smart city initiatives around\nthe globe as they improve urban planning and sustainability through data\nscience approaches. \n\n"}
{"id": "1812.01552", "contents": "Title: Exploration versus exploitation in reinforcement learning: a stochastic\n  control approach Abstract: We consider reinforcement learning (RL) in continuous time and study the\nproblem of achieving the best trade-off between exploration of a black box\nenvironment and exploitation of current knowledge. We propose an\nentropy-regularized reward function involving the differential entropy of the\ndistributions of actions, and motivate and devise an exploratory formulation\nfor the feature dynamics that captures repetitive learning under exploration.\nThe resulting optimization problem is a revitalization of the classical relaxed\nstochastic control. We carry out a complete analysis of the problem in the\nlinear--quadratic (LQ) setting and deduce that the optimal feedback control\ndistribution for balancing exploitation and exploration is Gaussian. This in\nturn interprets and justifies the widely adopted Gaussian exploration in RL,\nbeyond its simplicity for sampling. Moreover, the exploitation and exploration\nare captured, respectively and mutual-exclusively, by the mean and variance of\nthe Gaussian distribution. We also find that a more random environment contains\nmore learning opportunities in the sense that less exploration is needed. We\ncharacterize the cost of exploration, which, for the LQ case, is shown to be\nproportional to the entropy regularization weight and inversely proportional to\nthe discount rate. Finally, as the weight of exploration decays to zero, we\nprove the convergence of the solution of the entropy-regularized LQ problem to\nthe one of the classical LQ problem. \n\n"}
{"id": "1812.01662", "contents": "Title: Feed-Forward Neural Networks Need Inductive Bias to Learn Equality\n  Relations Abstract: Basic binary relations such as equality and inequality are fundamental to\nrelational data structures. Neural networks should learn such relations and\ngeneralise to new unseen data. We show in this study, however, that this\ngeneralisation fails with standard feed-forward networks on binary vectors.\nEven when trained with maximal training data, standard networks do not reliably\ndetect equality.We introduce differential rectifier (DR) units that we add to\nthe network in different configurations. The DR units create an inductive bias\nin the networks, so that they do learn to generalise, even from small numbers\nof examples and we have not found any negative effect of their inclusion in the\nnetwork. Given the fundamental nature of these relations, we hypothesize that\nfeed-forward neural network learning benefits from inductive bias in other\nrelations as well. Consequently, the further development of suitable inductive\nbiases will be beneficial to many tasks in relational learning with neural\nnetworks. \n\n"}
{"id": "1812.01664", "contents": "Title: A Stable Cardinality Distance for Topological Classification Abstract: This work incorporates topological features via persistence diagrams to\nclassify point cloud data arising from materials science. Persistence diagrams\nare multisets summarizing the connectedness and holes of given data. A new\ndistance on the space of persistence diagrams generates relevant input features\nfor a classification algorithm for materials science data. This distance\nmeasures the similarity of persistence diagrams using the cost of matching\npoints and a regularization term corresponding to cardinality differences\nbetween diagrams. Establishing stability properties of this distance provides\ntheoretical justification for the use of the distance in comparisons of such\ndiagrams. The classification scheme succeeds in determining the crystal\nstructure of materials on noisy and sparse data retrieved from synthetic atom\nprobe tomography experiments. \n\n"}
{"id": "1812.02289", "contents": "Title: Learning Dynamic Embeddings from Temporal Interactions Abstract: Modeling a sequence of interactions between users and items (e.g., products,\nposts, or courses) is crucial in domains such as e-commerce, social networking,\nand education to predict future interactions. Representation learning presents\nan attractive solution to model the dynamic evolution of user and item\nproperties, where each user/item can be embedded in a euclidean space and its\nevolution can be modeled by dynamic changes in embedding. However, existing\nembedding methods either generate static embeddings, treat users and items\nindependently, or are not scalable.\n  Here we present JODIE, a coupled recurrent model to jointly learn the dynamic\nembeddings of users and items from a sequence of user-item interactions. JODIE\nhas three components. First, the update component updates the user and item\nembedding from each interaction using their previous embeddings with the two\nmutually-recursive Recurrent Neural Networks. Second, a novel projection\ncomponent is trained to forecast the embedding of users at any future time.\nFinally, the prediction component directly predicts the embedding of the item\nin a future interaction. For models that learn from a sequence of interactions,\ntraditional training data batching cannot be done due to complex user-user\ndependencies. Therefore, we present a novel batching algorithm called t-Batch\nthat generates time-consistent batches of training data that can run in\nparallel, giving massive speed-up.\n  We conduct six experiments on two prediction tasks---future interaction\nprediction and state change prediction---using four real-world datasets. We\nshow that JODIE outperforms six state-of-the-art algorithms in these tasks by\nup to 22.4%. Moreover, we show that JODIE is highly scalable and up to 9.2x\nfaster than comparable models. As an additional experiment, we illustrate that\nJODIE can predict student drop-out from courses five interactions in advance. \n\n"}
{"id": "1812.02890", "contents": "Title: Three Tools for Practical Differential Privacy Abstract: Differentially private learning on real-world data poses challenges for\nstandard machine learning practice: privacy guarantees are difficult to\ninterpret, hyperparameter tuning on private data reduces the privacy budget,\nand ad-hoc privacy attacks are often required to test model privacy. We\nintroduce three tools to make differentially private machine learning more\npractical: (1) simple sanity checks which can be carried out in a centralized\nmanner before training, (2) an adaptive clipping bound to reduce the effective\nnumber of tuneable privacy parameters, and (3) we show that large-batch\ntraining improves model performance. \n\n"}
{"id": "1812.03528", "contents": "Title: On uniform exponential ergodicity of Markovian multiclass many-server\n  queues in the Halfin-Whitt regime Abstract: We study ergodic properties of Markovian multiclass many-server queues which\nare uniform over scheduling policies, as well as the size n of the system. The\nsystem is heavily loaded in the Halfin-Whitt regime, and the scheduling\npolicies are work-conserving and preemptive. We provide a unified approach via\na Lyapunov function method that establishes Foster-Lyapunov equations for both\nthe limiting diffusion and the prelimit diffusion-scaled queueing processes\nsimultaneously.\n  We first study the limiting controlled diffusion, and we show that if the\nspare capacity (safety staffing) parameter is positive, then the diffusion is\nexponentially ergodic uniformly over all stationary Markov controls, and the\ninvariant probability measures have uniform exponential tails. This result is\nsharp, since when there is no abandonment and the spare capacity parameter is\nnegative, then the controlled diffusion is transient under any Markov control.\nIn addition, we show that if all the abandonment rates are positive, the\ninvariant probability measures have sub-Gaussian tails, regardless whether the\nspare capacity parameter is positive or negative.\n  Using the above results, we proceed to establish the corresponding ergodic\nproperties for the diffusion-scaled queueing processes. In addition to\nproviding a simpler proof of the results in Gamarnik and Stolyar [Queueing Syst\n(2012) 71:25-51], we extend these results to the multiclass models with renewal\narrival processes, albeit under the assumption that the mean residual life\nfunctions are bounded. For the Markovian model with Poisson arrivals, we obtain\nstronger results and show that the convergence to the stationary distribution\nis at an exponential rate uniformly over all work-conserving stationary Markov\nscheduling policies. \n\n"}
{"id": "1812.04300", "contents": "Title: Deep neural networks algorithms for stochastic control problems on\n  finite horizon: convergence analysis Abstract: This paper develops algorithms for high-dimensional stochastic control\nproblems based on deep learning and dynamic programming. Unlike classical\napproximate dynamic programming approaches, we first approximate the optimal\npolicy by means of neural networks in the spirit of deep reinforcement\nlearning, and then the value function by Monte Carlo regression. This is\nachieved in the dynamic programming recursion by performance or hybrid\niteration, and regress now methods from numerical probabilities. We provide a\ntheoretical justification of these algorithms. Consistency and rate of\nconvergence for the control and value function estimates are analyzed and\nexpressed in terms of the universal approximation error of the neural networks,\nand of the statistical error when estimating network function, leaving aside\nthe optimization error. Numerical results on various applications are presented\nin a companion paper (arxiv.org/abs/1812.05916) and illustrate the performance\nof the proposed algorithms. \n\n"}
{"id": "1812.04355", "contents": "Title: Convex Regularization and Representer Theorems Abstract: We establish a result which states that regularizing an inverse problem with\nthe gauge of a convex set $C$ yields solutions which are linear combinations of\na few extreme points or elements of the extreme rays of $C$. These can be\nunderstood as the \\textit{atoms} of the regularizer. We then explicit that\ngeneral principle by using a few popular applications. In particular, we relate\nit to the common wisdom that total gradient variation minimization favors the\nreconstruction of piecewise constant images. \n\n"}
{"id": "1812.05243", "contents": "Title: A New Homotopy Proximal Variable-Metric Framework for Composite Convex\n  Minimization Abstract: This paper suggests two novel ideas to develop new proximal variable-metric\nmethods for solving a class of composite convex optimization problems. The\nfirst idea is a new parameterization of the optimality condition which allows\nus to develop a class of homotopy proximal variable-metric methods. We show\nthat under appropriate assumptions such as strong convexity-type and\nsmoothness, or self-concordance, our new schemes can achieve finite global\niteration-complexity bounds. Our second idea is a primal-dual-primal framework\nfor proximal-Newton methods which can lead to some useful computational\nfeatures for a subclass of nonsmooth composite convex optimization problems.\nStarting from the primal problem, we formulate its dual problem, and use our\nhomotopy proximal Newton method to solve this dual problem. Instead of solving\nthe subproblem directly in the dual space, we suggest to dualize this\nsubproblem to go back to the primal space. The resulting subproblem shares some\nsimilarity promoted by the regularizer of the original problem and leads to\nsome computational advantages. As a byproduct, we specialize the proposed\nalgorithm to solve covariance estimation problems. Surprisingly, our new\nalgorithm does not require any matrix inversion or Cholesky factorization, and\nfunction evaluation, while it works in the primal space with sparsity\nstructures that are promoted by the regularizer. Numerical examples on several\napplications are given to illustrate our theoretical development and to compare\nwith state-of-the-arts. \n\n"}
{"id": "1812.05555", "contents": "Title: Kalman-based Spectro-Temporal ECG Analysis using Deep Convolutional\n  Networks for Atrial Fibrillation Detection Abstract: In this article, we propose a novel ECG classification framework for atrial\nfibrillation (AF) detection using spectro-temporal representation (i.e., time\nvarying spectrum) and deep convolutional networks. In the first step we use a\nBayesian spectro-temporal representation based on the estimation of\ntime-varying coefficients of Fourier series using Kalman filter and smoother.\nNext, we derive an alternative model based on a stochastic oscillator\ndifferential equation to accelerate the estimation of the spectro-temporal\nrepresentation in lengthy signals. Finally, after comparative evaluations of\ndifferent convolutional architectures, we propose an efficient deep\nconvolutional neural network to classify the 2D spectro-temporal ECG data.\n  The ECG spectro-temporal data are classified into four different classes: AF,\nnon-AF normal rhythm (Normal), non-AF abnormal rhythm (Other), and noisy\nsegments (Noisy). The performance of the proposed methods is evaluated and\nscored with the PhysioNet/Computing in Cardiology (CinC) 2017 dataset. The\nexperimental results show that the proposed method achieves the overall F1\nscore of 80.2%, which is in line with the state-of-the-art algorithms. \n\n"}
{"id": "1812.05692", "contents": "Title: Bayesian Sparsification of Gated Recurrent Neural Networks Abstract: Bayesian methods have been successfully applied to sparsify weights of neural\nnetworks and to remove structure units from the networks, e. g. neurons. We\napply and further develop this approach for gated recurrent architectures.\nSpecifically, in addition to sparsification of individual weights and neurons,\nwe propose to sparsify preactivations of gates and information flow in LSTM. It\nmakes some gates and information flow components constant, speeds up forward\npass and improves compression. Moreover, the resulting structure of gate\nsparsity is interpretable and depends on the task. Code is available on github:\nhttps://github.com/tipt0p/SparseBayesianRNN \n\n"}
{"id": "1812.06790", "contents": "Title: Information Diffusion in Social Networks: Friendship Paradox based\n  Models and Statistical Inference Abstract: Dynamic models and statistical inference for the diffusion of information in\nsocial networks is an area which has witnessed remarkable progress in the last\ndecade due to the proliferation of social networks. Modeling and inference of\ndiffusion of information has applications in targeted advertising and\nmarketing, forecasting elections, predicting investor sentiment and identifying\nepidemic outbreaks. This chapter discusses three important aspects related to\ninformation diffusion in social networks: (i) How does observation bias named\nfriendship paradox (a graph theoretic consequence) and monophilic contagion\n(influence of friends of friends) affect information diffusion dynamics. (ii)\nHow can social networks adapt their structural connectivity depending on the\nstate of information diffusion. (iii) How one can estimate the state of the\nnetwork induced by information diffusion. The motivation for all three topics\nconsidered in this chapter stems from recent findings in network science and\nsocial sensing. Further, several directions for future research that arise from\nthese topics are also discussed. \n\n"}
{"id": "1812.07210", "contents": "Title: Expanding the Reach of Federated Learning by Reducing Client Resource\n  Requirements Abstract: Communication on heterogeneous edge networks is a fundamental bottleneck in\nFederated Learning (FL), restricting both model capacity and user\nparticipation. To address this issue, we introduce two novel strategies to\nreduce communication costs: (1) the use of lossy compression on the global\nmodel sent server-to-client; and (2) Federated Dropout, which allows users to\nefficiently train locally on smaller subsets of the global model and also\nprovides a reduction in both client-to-server communication and local\ncomputation. We empirically show that these strategies, combined with existing\ncompression approaches for client-to-server communication, collectively provide\nup to a $14\\times$ reduction in server-to-client communication, a $1.7\\times$\nreduction in local computation, and a $28\\times$ reduction in upload\ncommunication, all without degrading the quality of the final model. We thus\ncomprehensively reduce FL's impact on client device resources, allowing higher\ncapacity models to be trained, and a more diverse set of users to be reached. \n\n"}
{"id": "1812.07534", "contents": "Title: Value of Information in Feedback Control: Quantification Abstract: Although transmission of a data packet containing sensory information in a\nnetworked control system improves the quality of regulation, it has indeed a\nprice from the communication perspective. It is, therefore, rational that such\na data packet be transmitted only if it is valuable in the sense of a\ncost-benefit analysis. Yet, the fact is that little is known so far about this\nvaluation of information and its connection with traditional event-triggered\ncommunication. In the present article, we study this intrinsic property of\nnetworked control systems by formulating a rate-regulation tradeoff between the\npacket rate and the regulation cost with an event trigger and a controller as\ntwo distributed decision makers, and show that the valuation of information is\nconceivable and quantifiable grounded on this tradeoff. In particular, we\ncharacterize an equilibrium in the rate-regulation tradeoff, and quantify the\nvalue of information $\\text{VoI}_k$ there as the variation in a so-called value\nfunction with respect to a piece of sensory information that can be\ncommunicated to the controller at each time $k$. We prove that, for a\nmulti-dimensional Gauss-Markov process, $\\text{VoI}_k$ is a symmetric function\nof the discrepancy between the state estimates at the event trigger and the\ncontroller, and that a data packet containing sensory information at time $k$\nshould be transmitted to the controller only if $\\text{VoI}_k$ is nonnegative.\nMoreover, we discuss that $\\text{VoI}_k$ can be computed with arbitrary\naccuracy, and that it can be approximated by a closed-form quadratic function\nwith a performance guarantee. \n\n"}
{"id": "1812.08287", "contents": "Title: Multisource and Multitemporal Data Fusion in Remote Sensing Abstract: The sharp and recent increase in the availability of data captured by\ndifferent sensors combined with their considerably heterogeneous natures poses\na serious challenge for the effective and efficient processing of remotely\nsensed data. Such an increase in remote sensing and ancillary datasets,\nhowever, opens up the possibility of utilizing multimodal datasets in a joint\nmanner to further improve the performance of the processing approaches with\nrespect to the application at hand. Multisource data fusion has, therefore,\nreceived enormous attention from researchers worldwide for a wide variety of\napplications. Moreover, thanks to the revisit capability of several spaceborne\nsensors, the integration of the temporal information with the spatial and/or\nspectral/backscattering information of the remotely sensed data is possible and\nhelps to move from a representation of 2D/3D data to 4D data structures, where\nthe time variable adds new information as well as challenges for the\ninformation extraction algorithms. There are a huge number of research works\ndedicated to multisource and multitemporal data fusion, but the methods for the\nfusion of different modalities have expanded in different paths according to\neach research community. This paper brings together the advances of multisource\nand multitemporal data fusion approaches with respect to different research\ncommunities and provides a thorough and discipline-specific starting point for\nresearchers at different levels (i.e., students, researchers, and senior\nresearchers) willing to conduct novel investigations on this challenging topic\nby supplying sufficient detail and references. \n\n"}
{"id": "1812.08733", "contents": "Title: Heteroscedastic Gaussian processes for uncertainty modeling in\n  large-scale crowdsourced traffic data Abstract: Accurately modeling traffic speeds is a fundamental part of efficient\nintelligent transportation systems. Nowadays, with the widespread deployment of\nGPS-enabled devices, it has become possible to crowdsource the collection of\nspeed information to road users (e.g. through mobile applications or dedicated\nin-vehicle devices). Despite its rather wide spatial coverage, crowdsourced\nspeed data also brings very important challenges, such as the highly variable\nmeasurement noise in the data due to a variety of driving behaviors and sample\nsizes. When not properly accounted for, this noise can severely compromise any\napplication that relies on accurate traffic data. In this article, we propose\nthe use of heteroscedastic Gaussian processes (HGP) to model the time-varying\nuncertainty in large-scale crowdsourced traffic data. Furthermore, we develop a\nHGP conditioned on sample size and traffic regime (SRC-HGP), which makes use of\nsample size information (probe vehicles per minute) as well as previous\nobserved speeds, in order to more accurately model the uncertainty in observed\nspeeds. Using 6 months of crowdsourced traffic data from Copenhagen, we\nempirically show that the proposed heteroscedastic models produce significantly\nbetter predictive distributions when compared to current state-of-the-art\nmethods for both speed imputation and short-term forecasting tasks. \n\n"}
{"id": "1812.08985", "contents": "Title: Non-Adversarial Image Synthesis with Generative Latent Nearest Neighbors Abstract: Unconditional image generation has recently been dominated by generative\nadversarial networks (GANs). GAN methods train a generator which regresses\nimages from random noise vectors, as well as a discriminator that attempts to\ndifferentiate between the generated images and a training set of real images.\nGANs have shown amazing results at generating realistic looking images. Despite\ntheir success, GANs suffer from critical drawbacks including: unstable training\nand mode-dropping. The weaknesses in GANs have motivated research into\nalternatives including: variational auto-encoders (VAEs), latent embedding\nlearning methods (e.g. GLO) and nearest-neighbor based implicit maximum\nlikelihood estimation (IMLE). Unfortunately at the moment, GANs still\nsignificantly outperform the alternative methods for image generation. In this\nwork, we present a novel method - Generative Latent Nearest Neighbors (GLANN) -\nfor training generative models without adversarial training. GLANN combines the\nstrengths of IMLE and GLO in a way that overcomes the main drawbacks of each\nmethod. Consequently, GLANN generates images that are far better than GLO and\nIMLE. Our method does not suffer from mode collapse which plagues GAN training\nand is much more stable. Qualitative results show that GLANN outperforms a\nbaseline consisting of 800 GANs and VAEs on commonly used datasets. Our models\nare also shown to be effective for training truly non-adversarial unsupervised\nimage translation. \n\n"}
{"id": "1812.10160", "contents": "Title: The convex hull of a quadratic constraint over a polytope Abstract: A quadratically constrained quadratic program (QCQP) is an optimization\nproblem in which the objective function is a quadratic function and the\nfeasible region is defined by quadratic constraints. Solving non-convex QCQP to\nglobal optimality is a well-known NP-hard problem and a traditional approach is\nto use convex relaxations and branch-and-bound algorithms. This paper makes a\ncontribution in this direction by showing that the exact convex hull of a\ngeneral quadratic equation intersected with any bounded polyhedron is\nsecond-order cone representable. We present a simple constructive proof of this\nresult. \n\n"}
{"id": "1812.10386", "contents": "Title: ECG Segmentation by Neural Networks: Errors and Correction Abstract: In this study we examined the question of how error correction occurs in an\nensemble of deep convolutional networks, trained for an important applied\nproblem: segmentation of Electrocardiograms(ECG). We also explore the\npossibility of using the information about ensemble errors to evaluate a\nquality of data representation, built by the network. This possibility arises\nfrom the effect of distillation of outliers, which was demonstarted for the\nensemble, described in this paper. \n\n"}
{"id": "1812.10620", "contents": "Title: Evasive path planning under surveillance uncertainty Abstract: The classical setting of optimal control theory assumes full knowledge of the\nprocess dynamics and the costs associated with every control strategy. The\nproblem becomes much harder if the controller only knows a finite set of\npossible running cost functions, but has no way of checking which of these\nrunning costs is actually in place. In this paper we address this challenge for\na class of evasive path planning problems on a continuous domain, in which an\nEvader needs to reach a target while minimizing his exposure to an enemy\nObserver, who is in turn selecting from a finite set of known surveillance\nplans. Our key assumption is that both the evader and the observer need to\ncommit to their (possibly probabilistic) strategies in advance and cannot\nimmediately change their actions based on any newly discovered information\nabout the opponent's current position. We consider two types of evader\nbehavior: in the first one, a completely risk-averse evader seeks a trajectory\nminimizing his {\\em worst-case} cumulative observability, and in the second,\nthe evader is concerned with minimizing the {\\em average-case} cumulative\nobservability. The latter version is naturally interpreted as a semi-infinite\nstrategic game, and we provide an efficient method for approximating its Nash\nequilibrium. The proposed approach draws on methods from game theory, convex\noptimization, optimal control, and multiobjective dynamic programming. We\nillustrate our algorithm using numerical examples and discuss the computational\ncomplexity, including for the generalized version with multiple evaders. \n\n"}
{"id": "1812.11295", "contents": "Title: Monocular 3D Pose Recovery via Nonconvex Sparsity with Theoretical\n  Analysis Abstract: For recovering 3D object poses from 2D images, a prevalent method is to\npre-train an over-complete dictionary $\\mathcal D=\\{B_i\\}_i^D$ of 3D basis\nposes. During testing, the detected 2D pose $Y$ is matched to dictionary by $Y\n\\approx \\sum_i M_i B_i$ where $\\{M_i\\}_i^D=\\{c_i \\Pi R_i\\}$, by estimating the\nrotation $R_i$, projection $\\Pi$ and sparse combination coefficients $c \\in\n\\mathbb R_{+}^D$. In this paper, we propose non-convex regularization $H(c)$ to\nlearn coefficients $c$, including novel leaky capped $\\ell_1$-norm\nregularization (LCNR), \\begin{align*} H(c)=\\alpha \\sum_{i } \\min(|c_i|,\\tau)+\n\\beta \\sum_{i } \\max(| c_i|,\\tau), \\end{align*} where $0\\leq \\beta \\leq \\alpha$\nand $0<\\tau$ is a certain threshold, so the invalid components smaller than\n$\\tau$ are composed with larger regularization and other valid components with\nsmaller regularization. We propose a multi-stage optimizer with convex\nrelaxation and ADMM. We prove that the estimation error $\\mathcal L(l)$ decays\nw.r.t. the stages $l$, \\begin{align*} Pr\\left(\\mathcal L(l) < \\rho^{l-1}\n\\mathcal L(0) + \\delta \\right) \\geq 1- \\epsilon, \\end{align*} where $0< \\rho\n<1, 0<\\delta, 0<\\epsilon \\ll 1$. Experiments on large 3D human datasets like\nH36M are conducted to support our improvement upon previous approaches. To the\nbest of our knowledge, this is the first theoretical analysis in this line of\nresearch, to understand how the recovery error is affected by fundamental\nfactors, e.g. dictionary size, observation noises, optimization times. We\ncharacterize the trade-off between speed and accuracy towards real-time\ninference in applications. \n\n"}
{"id": "1812.11528", "contents": "Title: Parametric normal form classification for Eulerian and rotational\n  non-resonant double Hopf singularities Abstract: In this paper we provide novel results on the infinite level normal form and\norbital normal form classifications of nonlinear Eulerian and rotational vector\nfields with two pairs of non-resonant imaginary modes. We use the method of\nmultiple Lie brackets and its extension along with time rescaling for orbital\nnormal form classification. Furthermore, we apply two reduction techniques. The\nfirst is to use the radical Lie ideal of rotational vector fields and its\ncorresponding quotient Lie algebra. The second technique is to employ a Schur\ncomplement block matrix type in Gaussian elimination and analysis of block\nmatrices. The infinite level parametric normal form classification are also\npresented. The latter is also viewed as a normal form result for multiple-input\ncontrolled systems with non-resonant double Hopf singularity. We also discuss\nnonlinear symmetry transformations associated with the nonlinear symmetry group\nof the simplest normal forms. Symbolic normal form transformation generators\nare derived for computer algebra implementation. Further, the results are\nefficiently implemented and verified using Maple for all three types of normal\nform computations up to arbitrary degree, where they can also include both\nsmall bifurcation parameters and arbitrary symbolic constant coefficients. \n\n"}
{"id": "1901.00612", "contents": "Title: Adversarial Learning of a Sampler Based on an Unnormalized Distribution Abstract: We investigate adversarial learning in the case when only an unnormalized\nform of the density can be accessed, rather than samples. With insights so\ngarnered, adversarial learning is extended to the case for which one has access\nto an unnormalized form u(x) of the target density function, but no samples.\nFurther, new concepts in GAN regularization are developed, based on learning\nfrom samples or from u(x). The proposed method is compared to alternative\napproaches, with encouraging results demonstrated across a range of\napplications, including deep soft Q-learning. \n\n"}
{"id": "1901.02210", "contents": "Title: Real-Time Optimal Control for Irregular Asteroid Landings Using Deep\n  Neural Networks Abstract: Precise soft landings on asteroids are central to many deep space missions\nfor surface exploration and resource exploitation. To improve the autonomy and\nintelligence of landing control, a real-time optimal control approach is\nproposed using deep neural networks (DNN) for asteroid landing problems wherein\nthe developed DNN-based landing controller is capable of steering the lander to\na preselected landing site with high robustness to initial conditions. First,\nto significantly reduce the time consumption of gravity calculation, DNNs are\nused to approximate the irregular gravitational field of the asteroid based on\nthe samples from a polyhedral method. Then, an approximate indirect method is\npresented to solve the time-optimal landing problems with high computational\nefficiency by taking advantage of the designed gravity approximation method and\na homotopy technique. Furthermore, five DNNs are developed to learn the\nfunctional relationship between the state and optimal actions obtained by the\napproximate indirect method, and the resulting DNNs can generate the optimal\ncontrol instructions in real time because there is no longer need to solve the\noptimal landing problems onboard. Finally, a DNN-based landing controller\ncomposed of these five DNNs is devised to achieve the real-time optimal control\nfor asteroid landings. Simulation results of the time-optimal landing for Eros\nare given to substantiate the effectiveness of these techniques and illustrate\nthe real-time performance, control optimality, and robustness of the developed\nDNN-based optimal landing controller. \n\n"}
{"id": "1901.02347", "contents": "Title: Comparing Sample-wise Learnability Across Deep Neural Network Models Abstract: Estimating the relative importance of each sample in a training set has\nimportant practical and theoretical value, such as in importance sampling or\ncurriculum learning. This kind of focus on individual samples invokes the\nconcept of sample-wise learnability: How easy is it to correctly learn each\nsample (cf. PAC learnability)? In this paper, we approach the sample-wise\nlearnability problem within a deep learning context. We propose a measure of\nthe learnability of a sample with a given deep neural network (DNN) model. The\nbasic idea is to train the given model on the training set, and for each\nsample, aggregate the hits and misses over the entire training epochs. Our\nexperiments show that the sample-wise learnability measure collected this way\nis highly linearly correlated across different DNN models (ResNet-20, VGG-16,\nand MobileNet), suggesting that such a measure can provide deep general\ninsights on the data's properties. We expect our method to help develop better\ncurricula for training, and help us better understand the data itself. \n\n"}
{"id": "1901.03478", "contents": "Title: Deep Learning for Ranking Response Surfaces with Applications to Optimal\n  Stopping Problems Abstract: In this paper, we propose deep learning algorithms for ranking response\nsurfaces, with applications to optimal stopping problems in financial\nmathematics. The problem of ranking response surfaces is motivated by\nestimating optimal feedback policy maps in stochastic control problems, aiming\nto efficiently find the index associated to the minimal response across the\nentire continuous input space $\\mathcal{X} \\subseteq \\mathbb{R}^d$. By\nconsidering points in $\\mathcal{X}$ as pixels and indices of the minimal\nsurfaces as labels, we recast the problem as an image segmentation problem,\nwhich assigns a label to every pixel in an image such that pixels with the same\nlabel share certain characteristics. This provides an alternative method for\nefficiently solving the problem instead of using sequential design in our\nprevious work [R. Hu and M. Ludkovski, SIAM/ASA Journal on Uncertainty\nQuantification, 5 (2017), 212--239].\n  Deep learning algorithms are scalable, parallel and model-free, i.e., no\nparametric assumptions needed on the response surfaces. Considering ranking\nresponse surfaces as image segmentation allows one to use a broad class of deep\nneural networks, e.g., UNet, SegNet, DeconvNet, which have been widely applied\nand numerically proved to possess high accuracy in the field. We also\nsystematically study the dependence of deep learning algorithms on the input\ndata generated on uniform grids or by sequential design sampling, and observe\nthat the performance of deep learning is {\\it not} sensitive to the noise and\nlocations (close to/away from boundaries) of training data. We present a few\nexamples including synthetic ones and the Bermudan option pricing problem to\nshow the efficiency and accuracy of this method. \n\n"}
{"id": "1901.03676", "contents": "Title: On the Flow Problem in Water Distribution Networks: Uniqueness and\n  Solvers Abstract: Increasing concerns on the security and quality of water distribution systems\n(WDS), call for computational tools with performance guarantees. To this end,\nthis work revisits the physical laws governing water flow and provides a\nhierarchy of solvers of complementary value. Given the water injection or\npressure at each WDS node, finding the water flows within pipes and pumps along\nwith the pressures at all WDS nodes constitutes the water flow (WF) problem.\nThe latter entails solving a set of (non)-linear equations. We extend\nuniqueness claims on the solution to the WF equations in setups with multiple\nfixed-pressure nodes and detailed pump models. For networks without pumps, the\nWF solution is already known to be the minimizer of a convex function. The\nlatter approach is extended to networks with pumps but not in cycles, through a\nstitching algorithm. For networks with non-overlapping cycles, a provably exact\nconvex relaxation of the pressure drop equations yields a mixed-integer\nquadratically-constrained quadratic program (MI-QCQP) solver. A hybrid scheme\ncombining the MI-QCQP with the stitching algorithm can handle WDS with\noverlapping cycles, but without pumps on them. Each solver is guaranteed to\nconverge regardless of initialization, as numerically validated on a benchmark\nWDS. \n\n"}
{"id": "1901.03860", "contents": "Title: Prototypical Metric Transfer Learning for Continuous Speech Keyword\n  Spotting With Limited Training Data Abstract: Continuous Speech Keyword Spotting (CSKS) is the problem of spotting keywords\nin recorded conversations, when a small number of instances of keywords are\navailable in training data. Unlike the more common Keyword Spotting, where an\nalgorithm needs to detect lone keywords or short phrases like \"Alexa\",\n\"Cortana\", \"Hi Alexa!\", \"Whatsup Octavia?\" etc. in speech, CSKS needs to filter\nout embedded words from a continuous flow of speech, ie. spot \"Anna\" and\n\"github\" in \"I know a developer named Anna who can look into this github\nissue.\" Apart from the issue of limited training data availability, CSKS is an\nextremely imbalanced classification problem. We address the limitations of\nsimple keyword spotting baselines for both aforementioned challenges by using a\nnovel combination of loss functions (Prototypical networks' loss and metric\nloss) and transfer learning. Our method improves F1 score by over 10%. \n\n"}
{"id": "1901.04055", "contents": "Title: Gradient Boosted Feature Selection Abstract: A feature selection algorithm should ideally satisfy four conditions:\nreliably extract relevant features; be able to identify non-linear feature\ninteractions; scale linearly with the number of features and dimensions; allow\nthe incorporation of known sparsity structure. In this work we propose a novel\nfeature selection algorithm, Gradient Boosted Feature Selection (GBFS), which\nsatisfies all four of these requirements. The algorithm is flexible, scalable,\nand surprisingly straight-forward to implement as it is based on a modification\nof Gradient Boosted Trees. We evaluate GBFS on several real world data sets and\nshow that it matches or out-performs other state of the art feature selection\nalgorithms. Yet it scales to larger data set sizes and naturally allows for\ndomain-specific side information. \n\n"}
{"id": "1901.05168", "contents": "Title: How Will the Presence of Autonomous Vehicles Affect the Equilibrium\n  State of Traffic Networks? Abstract: It is known that connected and autonomous vehicles are capable of maintaining\nshorter headways and distances when they form platoons of vehicles. Thus, such\ntechnologies can result in increases in the capacities of traffic networks.\nConsequently, it is envisioned that their deployment will boost the network\nmobility. In this paper, we verify the validity of this impact under selfish\nrouting behavior of drivers in traffic networks with mixed autonomy, i.e.\ntraffic networks with both regular and autonomous vehicles. We consider a\nnonatomic routing game on a network with inelastic (fixed) demands for the set\nof network O/D pairs, and study how replacing a fraction of regular vehicles by\nautonomous vehicles will affect the mobility of the network. Using the well\nknown US bureau of public roads (BPR) traffic delay models, we show that the\nresulting Wardrop equilibrium is not necessarily unique even in its weak sense\nfor networks with mixed autonomy. We state the conditions under which the total\nnetwork delay is guaranteed not to increase as a result of autonomy increase.\nHowever, we show that when these conditions do not hold, counter intuitive\nbehaviors may occur: the total delay can grow by increasing the network\nautonomy. In particular, we prove that for networks with a single O/D pair, if\nthe road degrees of asymmetry are homogeneous, the total delay is 1) unique,\nand 2) a nonincreasing continuous function of network autonomy fraction. We\nshow that for heterogeneous degrees of asymmetry, the total delay is not\nunique, and it can further grow with autonomy increase. We demonstrate that\nsimilar behaviors may be observed in networks with multiple O/D pairs. We\nfurther bound such performance degradations due to the introduction of autonomy\nin homogeneous networks. \n\n"}
{"id": "1901.05583", "contents": "Title: A Multilevel Approach for Stochastic Nonlinear Optimal Control Abstract: We consider a class of finite time horizon nonlinear stochastic optimal\ncontrol problem, where the control acts additively on the dynamics and the\ncontrol cost is quadratic. This framework is flexible and has found\napplications in many domains. Although the optimal control admits a path\nintegral representation for this class of control problems, efficient\ncomputation of the associated path integrals remains a challenging Monte Carlo\ntask. The focus of this article is to propose a new Monte Carlo approach that\nsignificantly improves upon existing methodology. Our proposed methodology\nfirst tackles the issue of exponential growth in variance with the time horizon\nby casting optimal control estimation as a smoothing problem for a state space\nmodel associated with the control problem, and applying smoothing algorithms\nbased on particle Markov chain Monte Carlo. To further reduce computational\ncost, we then develop a multilevel Monte Carlo method which allows us to obtain\nan estimator of the optimal control with $\\mathcal{O}(\\epsilon^2)$ mean squared\nerror with a computational cost of\n$\\mathcal{O}(\\epsilon^{-2}\\log(\\epsilon)^2)$. In contrast, a computational cost\nof $\\mathcal{O}(\\epsilon^{-3})$ is required for existing methodology to achieve\nthe same mean squared error. Our approach is illustrated on two numerical\nexamples, which validate our theory. \n\n"}
{"id": "1901.07648", "contents": "Title: Finite-Sum Smooth Optimization with SARAH Abstract: The total complexity (measured as the total number of gradient computations)\nof a stochastic first-order optimization algorithm that finds a first-order\nstationary point of a finite-sum smooth nonconvex objective function\n$F(w)=\\frac{1}{n} \\sum_{i=1}^n f_i(w)$ has been proven to be at least\n$\\Omega(\\sqrt{n}/\\epsilon)$ for $n \\leq \\mathcal{O}(\\epsilon^{-2})$ where\n$\\epsilon$ denotes the attained accuracy $\\mathbb{E}[ \\|\\nabla\nF(\\tilde{w})\\|^2] \\leq \\epsilon$ for the outputted approximation $\\tilde{w}$\n(Fang et al., 2018). In this paper, we provide a convergence analysis for a\nslightly modified version of the SARAH algorithm (Nguyen et al., 2017a;b) and\nachieve total complexity that matches the lower-bound worst case complexity in\n(Fang et al., 2018) up to a constant factor when $n \\leq\n\\mathcal{O}(\\epsilon^{-2})$ for nonconvex problems. For convex optimization, we\npropose SARAH++ with sublinear convergence for general convex and linear\nconvergence for strongly convex problems; and we provide a practical version\nfor which numerical experiments on various datasets show an improved\nperformance. \n\n"}
{"id": "1901.08275", "contents": "Title: Multi-fidelity Bayesian Optimization with Max-value Entropy Search and\n  its parallelization Abstract: In a standard setting of Bayesian optimization (BO), the objective function\nevaluation is assumed to be highly expensive. Multi-fidelity Bayesian\noptimization (MFBO) accelerates BO by incorporating lower fidelity observations\navailable with a lower sampling cost. In this paper, we focus on the\ninformation-based approach, which is a popular and empirically successful\napproach in BO. For MFBO, however, existing information-based methods are\nplagued by difficulty in estimating the information gain. We propose an\napproach based on max-value entropy search (MES), which greatly facilitates\ncomputations by considering the entropy of the optimal function value instead\nof the optimal input point. We show that, in our multi-fidelity MES (MF-MES),\nmost of additional computations, compared with usual MES, is reduced to\nanalytical computations. Although an additional numerical integration is\nnecessary for the information across different fidelities, this is only in one\ndimensional space, which can be performed efficiently and accurately. Further,\nwe also propose parallelization of MF-MES. Since there exist a variety of\ndifferent sampling costs, queries typically occur asynchronously in MFBO. We\nshow that similar simple computations can be derived for asynchronous parallel\nMFBO. We demonstrate effectiveness of our approach by using benchmark datasets\nand a real-world application to materials science data. \n\n"}
{"id": "1901.08585", "contents": "Title: Graph heat mixture model learning Abstract: Graph inference methods have recently attracted a great interest from the\nscientific community, due to the large value they bring in data interpretation\nand analysis. However, most of the available state-of-the-art methods focus on\nscenarios where all available data can be explained through the same graph, or\ngroups corresponding to each graph are known a priori. In this paper, we argue\nthat this is not always realistic and we introduce a generative model for mixed\nsignals following a heat diffusion process on multiple graphs. We propose an\nexpectation-maximisation algorithm that can successfully separate signals into\ncorresponding groups, and infer multiple graphs that govern their behaviour. We\ndemonstrate the benefits of our method on both synthetic and real data. \n\n"}
{"id": "1901.08730", "contents": "Title: Better accuracy with quantified privacy: representations learned via\n  reconstructive adversarial network Abstract: The remarkable success of machine learning, especially deep learning, has\nproduced a variety of cloud-based services for mobile users. Such services\nrequire an end user to send data to the service provider, which presents a\nserious challenge to end-user privacy. To address this concern, prior works\neither add noise to the data or send features extracted from the raw data. They\nstruggle to balance between the utility and privacy because added noise reduces\nutility and raw data can be reconstructed from extracted features. This work\nrepresents a methodical departure from prior works: we balance between a\nmeasure of privacy and another of utility by leveraging adversarial learning to\nfind a sweeter tradeoff. We design an encoder that optimizes against the\nreconstruction error (a measure of privacy), adversarially by a Decoder, and\nthe inference accuracy (a measure of utility) by a Classifier. The result is\nRAN, a novel deep model with a new training algorithm that automatically\nextracts features for classification that are both private and useful. It turns\nout that adversarially forcing the extracted features to only conveys the\nintended information required by classification leads to an implicit\nregularization leading to better classification accuracy than the original\nmodel which completely ignores privacy. Thus, we achieve better privacy with\nbetter utility, a surprising possibility in machine learning! We conducted\nextensive experiments on five popular datasets over four training schemes, and\ndemonstrate the superiority of RAN compared with existing alternatives. \n\n"}
{"id": "1901.09021", "contents": "Title: Complexity of Linear Regions in Deep Networks Abstract: It is well-known that the expressivity of a neural network depends on its\narchitecture, with deeper networks expressing more complex functions. In the\ncase of networks that compute piecewise linear functions, such as those with\nReLU activation, the number of distinct linear regions is a natural measure of\nexpressivity. It is possible to construct networks with merely a single region,\nor for which the number of linear regions grows exponentially with depth; it is\nnot clear where within this range most networks fall in practice, either before\nor after training. In this paper, we provide a mathematical framework to count\nthe number of linear regions of a piecewise linear network and measure the\nvolume of the boundaries between these regions. In particular, we prove that\nfor networks at initialization, the average number of regions along any\none-dimensional subspace grows linearly in the total number of neurons, far\nbelow the exponential upper bound. We also find that the average distance to\nthe nearest region boundary at initialization scales like the inverse of the\nnumber of neurons. Our theory suggests that, even after training, the number of\nlinear regions is far below exponential, an intuition that matches our\nempirical observations. We conclude that the practical expressivity of neural\nnetworks is likely far below that of the theoretical maximum, and that this gap\ncan be quantified. \n\n"}
{"id": "1901.09109", "contents": "Title: DADAM: A Consensus-based Distributed Adaptive Gradient Method for Online\n  Optimization Abstract: Adaptive gradient-based optimization methods such as \\textsc{Adagrad},\n\\textsc{Rmsprop}, and \\textsc{Adam} are widely used in solving large-scale\nmachine learning problems including deep learning. A number of schemes have\nbeen proposed in the literature aiming at parallelizing them, based on\ncommunications of peripheral nodes with a central node, but incur high\ncommunications cost. To address this issue, we develop a novel consensus-based\ndistributed adaptive moment estimation method (\\textsc{Dadam}) for online\noptimization over a decentralized network that enables data parallelization, as\nwell as decentralized computation. The method is particularly useful, since it\ncan accommodate settings where access to local data is allowed. Further, as\nestablished theoretically in this work, it can outperform centralized adaptive\nalgorithms, for certain classes of loss functions used in applications. We\nanalyze the convergence properties of the proposed algorithm and provide a\ndynamic regret bound on the convergence rate of adaptive moment estimation\nmethods in both stochastic and deterministic settings. Empirical results\ndemonstrate that \\textsc{Dadam} works also well in practice and compares\nfavorably to competing online optimization methods. \n\n"}
{"id": "1901.09583", "contents": "Title: ML for Flood Forecasting at Scale Abstract: Effective riverine flood forecasting at scale is hindered by a multitude of\nfactors, most notably the need to rely on human calibration in current\nmethodology, the limited amount of data for a specific location, and the\ncomputational difficulty of building continent/global level models that are\nsufficiently accurate. Machine learning (ML) is primed to be useful in this\nscenario: learned models often surpass human experts in complex\nhigh-dimensional scenarios, and the framework of transfer or multitask learning\nis an appealing solution for leveraging local signals to achieve improved\nglobal performance. We propose to build on these strengths and develop ML\nsystems for timely and accurate riverine flood prediction. \n\n"}
{"id": "1901.09671", "contents": "Title: ErasureHead: Distributed Gradient Descent without Delays Using\n  Approximate Gradient Coding Abstract: We present ErasureHead, a new approach for distributed gradient descent (GD)\nthat mitigates system delays by employing approximate gradient coding. Gradient\ncoded distributed GD uses redundancy to exactly recover the gradient at each\niteration from a subset of compute nodes. ErasureHead instead uses approximate\ngradient codes to recover an inexact gradient at each iteration, but with\nhigher delay tolerance. Unlike prior work on gradient coding, we provide a\nperformance analysis that combines both delay and convergence guarantees. We\nestablish that down to a small noise floor, ErasureHead converges as quickly as\ndistributed GD and has faster overall runtime under a probabilistic delay\nmodel. We conduct extensive experiments on real world datasets and distributed\nclusters and demonstrate that our method can lead to significant speedups over\nboth standard and gradient coded GD. \n\n"}
{"id": "1901.10604", "contents": "Title: Improved Path-length Regret Bounds for Bandits Abstract: We study adaptive regret bounds in terms of the variation of the losses (the\nso-called path-length bounds) for both multi-armed bandit and more generally\nlinear bandit. We first show that the seemingly suboptimal path-length bound of\n(Wei and Luo, 2018) is in fact not improvable for adaptive adversary. Despite\nthis negative result, we then develop two new algorithms, one that strictly\nimproves over (Wei and Luo, 2018) with a smaller path-length measure, and the\nother which improves over (Wei and Luo, 2018) for oblivious adversary when the\npath-length is large. Our algorithms are based on the well-studied optimistic\nmirror descent framework, but importantly with several novel techniques,\nincluding new optimistic predictions, a slight bias towards recently selected\narms, and the use of a hybrid regularizer similar to that of (Bubeck et al.,\n2018).\n  Furthermore, we extend our results to linear bandit by showing a reduction to\nobtaining dynamic regret for a full-information problem, followed by a further\nreduction to convex body chasing. We propose a simple greedy chasing algorithm\nfor squared 2-norm, leading to new dynamic regret results and as a consequence\nthe first path-length regret for general linear bandit as well. \n\n"}
{"id": "1901.11173", "contents": "Title: Peer-to-peer Federated Learning on Graphs Abstract: We consider the problem of training a machine learning model over a network\nof nodes in a fully decentralized framework. The nodes take a Bayesian-like\napproach via the introduction of a belief over the model parameter space. We\npropose a distributed learning algorithm in which nodes update their belief by\naggregate information from their one-hop neighbors to learn a model that best\nfits the observations over the entire network. In addition, we also obtain\nsufficient conditions to ensure that the probability of error is small for\nevery node in the network. We discuss approximations required for applying this\nalgorithm to train Deep Neural Networks (DNNs). Experiments on training linear\nregression model and on training a DNN show that the proposed learning rule\nalgorithm provides a significant improvement in the accuracy compared to the\ncase where nodes learn without cooperation. \n\n"}
{"id": "math/0005281", "contents": "Title: Connections between Linear Systems and Convolutional Codes Abstract: The article reviews different definitions for a convolutional code which can\nbe found in the literature. The algebraic differences between the definitions\nare worked out in detail. It is shown that bi-infinite support systems are dual\nto finite-support systems under Pontryagin duality. In this duality the dual of\na controllable system is observable and vice versa. Uncontrollability can occur\nonly if there are bi-infinite support trajectories in the behavior, so finite\nand half-infinite-support systems must be controllable. Unobservability can\noccur only if there are finite support trajectories in the behavior, so\nbi-infinite and half-infinite-support systems must be observable. It is shown\nthat the different definitions for convolutional codes are equivalent if one\nrestricts attention to controllable and observable codes. \n\n"}
{"id": "math/0211450", "contents": "Title: Symmetry groups, semidefinite programs, and sums of squares Abstract: We investigate the representation of symmetric polynomials as a sum of\nsquares. Since this task is solved using semidefinite programming tools we\nexplore the geometric, algebraic, and computational implications of the\npresence of discrete symmetries in semidefinite programs. It is shown that\nsymmetry exploitation allows a significant reduction in both matrix size and\nnumber of decision variables. This result is applied to semidefinite programs\narising from the computation of sum of squares decompositions for multivariate\npolynomials. The results, reinterpreted from an invariant-theoretic viewpoint,\nprovide a novel representation of a class of nonnegative symmetric polynomials.\nThe main theorem states that an invariant sum of squares polynomial is a sum of\ninner products of pairs of matrices, whose entries are invariant polynomials.\nIn these pairs, one of the matrices is computed based on the real irreducible\nrepresentations of the group, and the other is a sum of squares matrix. The\nreduction techniques enable the numerical solution of large-scale instances,\notherwise computationally infeasible to solve. \n\n"}
{"id": "math/0307196", "contents": "Title: Convolutional Codes with Maximum Distance Profile Abstract: Maximum distance profile codes are characterized by the property that two\ntrajectories which start at the same state and proceed to a different state\nwill have the maximum possible distance from each other relative to any other\nconvolutional code of the same rate and degree.\n  In this paper we use methods from systems theory to characterize maximum\ndistance profile codes algebraically. Tha main result shows that maximum\ndistance profile codes form a generic set inside the variety which parametrizes\nthe set of convolutional codes of a fixed rate and a fixed degree. \n\n"}
{"id": "math/0309048", "contents": "Title: A Harmonic Analysis Solution to the Static Basket Arbitrage Problem Abstract: We consider the problem of computing upper and lower bounds on the price of a\nEuropean basket call option, given prices on other similar baskets. We focus\nhere on an interpretation of this program as a generalized moment problem.\nRecent results by Berg & Maserick (1984), Putinar & Vasilescu (1999) and\nLasserre (2001) on harmonic analysis on semigroups, the K-moment problem and\nits applications to optimization, allow us to derive tractable necessary and\nsufficient conditions for the absence of static arbitrage between basket\nstraddles, hence between basket calls and puts. \n\n"}
{"id": "math/0312083", "contents": "Title: On the Curvature of the Central Path of Linear Programming Theory Abstract: We prove a linear bound on the average total curvature of the central path of\nlinear programming theory in terms on the number of independent variables of\nthe primal problem, and independent on the number of constraints. \n\n"}
{"id": "math/0404184", "contents": "Title: A max-plus finite element method for solving finite horizon\n  deterministic optimal control problems Abstract: We introduce a max-plus analogue of the Petrov-Galerkin finite element\nmethod, to solve finite horizon deterministic optimal control problems. The\nmethod relies on a max-plus variational formulation, and exploits the\nproperties of projectors on max-plus semimodules. We obtain a nonlinear\ndiscretized semigroup, corresponding to a zero-sum two players game. We give an\nerror estimate of order $(\\Delta t)^{1/2}+\\Delta x(\\Delta t)^{-1}$, for a\nsubclass of problems in dimension 1. We compare our method with a max-plus\nbased discretization method previously introduced by Fleming and McEneaney. \n\n"}
{"id": "math/0404511", "contents": "Title: A New Approach to Adaptive Nonlinear Regulation Abstract: This paper shows how the theory of adaptive observers can be effectively used\nin the design internal models for nonlinear output regulation. The main result\nobtained in this way is a new method for the synthesis of adaptive internal\nmodels which substantially enhances the existing theory of adaptive output\nregulation, by allowing nonlinear internal models and more general classes of\ncontrolled plants. \n\n"}
{"id": "math/0507330", "contents": "Title: Solutions to Monge-Kantorovich equations as stationary points of a\n  dynamical system Abstract: Solutions to Monge-Kantorovich equations, expressing optimality condition in\nmass transportation problem with cost equal to distance, are stationary points\nof a critical-slope model for sand surface evolution. Using a dual variational\nformulation of sand model, we compute both the optimal transport density and\nKantorovich potential as a stationary limit of evolving sand flux and sand\nsurface, respectively. \n\n"}
{"id": "math/0509250", "contents": "Title: The max-plus finite element method for optimal control problems: further\n  approximation results Abstract: We develop the max-plus finite element method to solve finite horizon\ndeterministic optimal control problems. This method, that we introduced in a\nprevious work, relies on a max-plus variational formulation, and exploits the\nproperties of projectors on max-plus semimodules. We prove here a convergence\nresult, in arbitrary dimension, showing that for a subclass of problems, the\nerror estimate is of order $\\delta+\\Delta x(\\delta)^{-1}$, where $\\delta$ and\n$\\Delta x$ are the time and space steps respectively. We also show how the\nmax-plus analogues of the mass and stiffness matrices can be computed by convex\noptimization, even when the global problem is non convex. We illustrate the\nmethod by numerical examples in dimension 2. \n\n"}
{"id": "math/0510333", "contents": "Title: Optimal Bond Portfolios Abstract: We aim to construct a general framework for portfolio management in\ncontinuous time, encompassing both stocks and bonds. In these lecture notes we\ngive an overview of the state of the art of optimal bond portfolios and we\nre-visit main results and mathematical constructions introduced in our previous\npublications (Ann. Appl. Probab. \\textbf{15}, 1260--1305 (2005) and Fin. Stoch.\n{\\bf9}, 429--452 (2005)).\n  A solution of the optimal bond portfolio problem is given for general utility\nfunctions and volatility operator processes, provided that the market price of\nrisk process has certain Malliavin differentiability properties or is finite\ndimensional.\n  The text is essentially self-contained. \n\n"}
{"id": "math/0603619", "contents": "Title: The max-plus finite element method for solving deterministic optimal\n  control problems: basic properties and convergence analysis Abstract: We introduce a max-plus analogue of the Petrov-Galerkin finite element method\nto solve finite horizon deterministic optimal control problems. The method\nrelies on a max-plus variational formulation. We show that the error in the sup\nnorm can be bounded from the difference between the value function and its\nprojections on max-plus and min-plus semimodules, when the max-plus analogue of\nthe stiffness matrix is exactly known. In general, the stiffness matrix must be\napproximated: this requires approximating the operation of the Lax-Oleinik\nsemigroup on finite elements. We consider two approximations relying on the\nHamiltonian. We derive a convergence result, in arbitrary dimension, showing\nthat for a class of problems, the error estimate is of order $\\delta+\\Delta\nx(\\delta)^{-1}$ or $\\sqrt{\\delta}+\\Delta x(\\delta)^{-1}$, depending on the\nchoice of the approximation, where $\\delta$ and $\\Delta x$ are respectively the\ntime and space discretization steps. We compare our method with another\nmax-plus based discretization method previously introduced by Fleming and\nMcEneaney. We give numerical examples in dimension 1 and 2. \n\n"}
{"id": "math/9702218", "contents": "Title: Some Remarks on Real and Complex Output Feedback Abstract: We provide some new necessary and sufficient conditions which guarantee\narbitrary pole placement of a particular linear system over the complex\nnumbers. We exhibit a non-trivial real linear system which is not controllable\nby real static output feedback and discuss a conjecture from algebraic geometry\nconcerning the existence of real linear systems for which all static feedback\nlaws are real. \n\n"}
