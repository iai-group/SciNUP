{"id": "0901.3202", "contents": "Title: Model-Consistent Sparse Estimation through the Bootstrap Abstract: We consider the least-square linear regression problem with regularization by\nthe $\\ell^1$-norm, a problem usually referred to as the Lasso. In this paper,\nwe first present a detailed asymptotic analysis of model consistency of the\nLasso in low-dimensional settings. For various decays of the regularization\nparameter, we compute asymptotic equivalents of the probability of correct\nmodel selection. For a specific rate decay, we show that the Lasso selects all\nthe variables that should enter the model with probability tending to one\nexponentially fast, while it selects all other variables with strictly positive\nprobability. We show that this property implies that if we run the Lasso for\nseveral bootstrapped replications of a given sample, then intersecting the\nsupports of the Lasso bootstrap estimates leads to consistent model selection.\nThis novel variable selection procedure, referred to as the Bolasso, is\nextended to high-dimensional settings by a provably consistent two-step\nprocedure. \n\n"}
{"id": "0910.5932", "contents": "Title: Metric and Kernel Learning using a Linear Transformation Abstract: Metric and kernel learning are important in several machine learning\napplications. However, most existing metric learning algorithms are limited to\nlearning metrics over low-dimensional data, while existing kernel learning\nalgorithms are often limited to the transductive setting and do not generalize\nto new data points. In this paper, we study metric learning as a problem of\nlearning a linear transformation of the input data. We show that for\nhigh-dimensional data, a particular framework for learning a linear\ntransformation of the data based on the LogDet divergence can be efficiently\nkernelized to learn a metric (or equivalently, a kernel function) over an\narbitrarily high dimensional space. We further demonstrate that a wide class of\nconvex loss functions for learning linear transformations can similarly be\nkernelized, thereby considerably expanding the potential applications of metric\nlearning. We demonstrate our learning approach by applying it to large-scale\nreal world problems in computer vision and text mining. \n\n"}
{"id": "1010.0556", "contents": "Title: Regularizers for Structured Sparsity Abstract: We study the problem of learning a sparse linear regression vector under\nadditional conditions on the structure of its sparsity pattern. This problem is\nrelevant in machine learning, statistics and signal processing. It is well\nknown that a linear regression can benefit from knowledge that the underlying\nregression vector is sparse. The combinatorial problem of selecting the nonzero\ncomponents of this vector can be \"relaxed\" by regularizing the squared error\nwith a convex penalty function like the $\\ell_1$ norm. However, in many\napplications, additional conditions on the structure of the regression vector\nand its sparsity pattern are available. Incorporating this information into the\nlearning method may lead to a significant decrease of the estimation error. In\nthis paper, we present a family of convex penalty functions, which encode prior\nknowledge on the structure of the vector formed by the absolute values of the\nregression coefficients. This family subsumes the $\\ell_1$ norm and is flexible\nenough to include different models of sparsity patterns, which are of practical\nand theoretical importance. We establish the basic properties of these penalty\nfunctions and discuss some examples where they can be computed explicitly.\nMoreover, we present a convergent optimization algorithm for solving\nregularized least squares with these penalty functions. Numerical simulations\nhighlight the benefit of structured sparsity and the advantage offered by our\napproach over the Lasso method and other related methods. \n\n"}
{"id": "1012.4116", "contents": "Title: lp-Recovery of the Most Significant Subspace among Multiple Subspaces\n  with Outliers Abstract: We assume data sampled from a mixture of d-dimensional linear subspaces with\nspherically symmetric distributions within each subspace and an additional\noutlier component with spherically symmetric distribution within the ambient\nspace (for simplicity we may assume that all distributions are uniform on their\ncorresponding unit spheres). We also assume mixture weights for the different\ncomponents. We say that one of the underlying subspaces of the model is most\nsignificant if its mixture weight is higher than the sum of the mixture weights\nof all other subspaces. We study the recovery of the most significant subspace\nby minimizing the lp-averaged distances of data points from d-dimensional\nsubspaces, where p>0. Unlike other lp minimization problems, this minimization\nis non-convex for all p>0 and thus requires different methods for its analysis.\nWe show that if 0<p<=1, then for any fraction of outliers the most significant\nsubspace can be recovered by lp minimization with overwhelming probability\n(which depends on the generating distribution and its parameters). We show that\nwhen adding small noise around the underlying subspaces the most significant\nsubspace can be nearly recovered by lp minimization for any 0<p<=1 with an\nerror proportional to the noise level. On the other hand, if p>1 and there is\nmore than one underlying subspace, then with overwhelming probability the most\nsignificant subspace cannot be recovered or nearly recovered. This last result\ndoes not require spherically symmetric outliers. \n\n"}
{"id": "1104.0729", "contents": "Title: Online and Batch Learning Algorithms for Data with Missing Features Abstract: We introduce new online and batch algorithms that are robust to data with\nmissing features, a situation that arises in many practical applications. In\nthe online setup, we allow for the comparison hypothesis to change as a\nfunction of the subset of features that is observed on any given round,\nextending the standard setting where the comparison hypothesis is fixed\nthroughout. In the batch setup, we present a convex relation of a non-convex\nproblem to jointly estimate an imputation function, used to fill in the values\nof missing features, along with the classification hypothesis. We prove regret\nbounds in the online setting and Rademacher complexity bounds for the batch\ni.i.d. setting. The algorithms are tested on several UCI datasets, showing\nsuperior performance over baselines. \n\n"}
{"id": "1107.2021", "contents": "Title: Multi-Instance Learning with Any Hypothesis Class Abstract: In the supervised learning setting termed Multiple-Instance Learning (MIL),\nthe examples are bags of instances, and the bag label is a function of the\nlabels of its instances. Typically, this function is the Boolean OR. The\nlearner observes a sample of bags and the bag labels, but not the instance\nlabels that determine the bag labels. The learner is then required to emit a\nclassification rule for bags based on the sample. MIL has numerous\napplications, and many heuristic algorithms have been used successfully on this\nproblem, each adapted to specific settings or applications. In this work we\nprovide a unified theoretical analysis for MIL, which holds for any underlying\nhypothesis class, regardless of a specific application or problem domain. We\nshow that the sample complexity of MIL is only poly-logarithmically dependent\non the size of the bag, for any underlying hypothesis class. In addition, we\nintroduce a new PAC-learning algorithm for MIL, which uses a regular supervised\nlearning algorithm as an oracle. We prove that efficient PAC-learning for MIL\ncan be generated from any efficient non-MIL supervised learning algorithm that\nhandles one-sided error. The computational complexity of the resulting\nalgorithm is only polynomially dependent on the bag size. \n\n"}
{"id": "1110.6497", "contents": "Title: Bayesian Optimization for Adaptive MCMC Abstract: This paper proposes a new randomized strategy for adaptive MCMC using\nBayesian optimization. This approach applies to non-differentiable objective\nfunctions and trades off exploration and exploitation to reduce the number of\npotentially costly objective function evaluations. We demonstrate the strategy\nin the complex setting of sampling from constrained, discrete and densely\nconnected probabilistic graphical models where, for each variation of the\nproblem, one needs to adjust the parameters of the proposal mechanism\nautomatically to ensure efficient mixing of the Markov chains. \n\n"}
{"id": "1202.3079", "contents": "Title: Towards minimax policies for online linear optimization with bandit\n  feedback Abstract: We address the online linear optimization problem with bandit feedback. Our\ncontribution is twofold. First, we provide an algorithm (based on exponential\nweights) with a regret of order $\\sqrt{d n \\log N}$ for any finite action set\nwith $N$ actions, under the assumption that the instantaneous loss is bounded\nby 1. This shaves off an extraneous $\\sqrt{d}$ factor compared to previous\nworks, and gives a regret bound of order $d \\sqrt{n \\log n}$ for any compact\nset of actions. Without further assumptions on the action set, this last bound\nis minimax optimal up to a logarithmic factor. Interestingly, our result also\nshows that the minimax regret for bandit linear optimization with expert advice\nin $d$ dimension is the same as for the basic $d$-armed bandit with expert\nadvice. Our second contribution is to show how to use the Mirror Descent\nalgorithm to obtain computationally efficient strategies with minimax optimal\nregret bounds in specific examples. More precisely we study two canonical\naction sets: the hypercube and the Euclidean ball. In the former case, we\nobtain the first computationally efficient algorithm with a $d \\sqrt{n}$\nregret, thus improving by a factor $\\sqrt{d \\log n}$ over the best known result\nfor a computationally efficient algorithm. In the latter case, our approach\ngives the first algorithm with a $\\sqrt{d n \\log n}$ regret, again shaving off\nan extraneous $\\sqrt{d}$ compared to previous works. \n\n"}
{"id": "1203.5181", "contents": "Title: $k$-MLE: A fast algorithm for learning statistical mixture models Abstract: We describe $k$-MLE, a fast and efficient local search algorithm for learning\nfinite statistical mixtures of exponential families such as Gaussian mixture\nmodels. Mixture models are traditionally learned using the\nexpectation-maximization (EM) soft clustering technique that monotonically\nincreases the incomplete (expected complete) likelihood. Given prescribed\nmixture weights, the hard clustering $k$-MLE algorithm iteratively assigns data\nto the most likely weighted component and update the component models using\nMaximum Likelihood Estimators (MLEs). Using the duality between exponential\nfamilies and Bregman divergences, we prove that the local convergence of the\ncomplete likelihood of $k$-MLE follows directly from the convergence of a dual\nadditively weighted Bregman hard clustering. The inner loop of $k$-MLE can be\nimplemented using any $k$-means heuristic like the celebrated Lloyd's batched\nor Hartigan's greedy swap updates. We then show how to update the mixture\nweights by minimizing a cross-entropy criterion that implies to update weights\nby taking the relative proportion of cluster points, and reiterate the mixture\nparameter update and mixture weight update processes until convergence. Hard EM\nis interpreted as a special case of $k$-MLE when both the component update and\nthe weight update are performed successively in the inner loop. To initialize\n$k$-MLE, we propose $k$-MLE++, a careful initialization of $k$-MLE guaranteeing\nprobabilistically a global bound on the best possible complete likelihood. \n\n"}
{"id": "1203.6130", "contents": "Title: Spectral dimensionality reduction for HMMs Abstract: Hidden Markov Models (HMMs) can be accurately approximated using\nco-occurrence frequencies of pairs and triples of observations by using a fast\nspectral method in contrast to the usual slow methods like EM or Gibbs\nsampling. We provide a new spectral method which significantly reduces the\nnumber of model parameters that need to be estimated, and generates a sample\ncomplexity that does not depend on the size of the observation vocabulary. We\npresent an elementary proof giving bounds on the relative accuracy of\nprobability estimates from our model. (Correlaries show our bounds can be\nweakened to provide either L1 bounds or KL bounds which provide easier direct\ncomparisons to previous work.) Our theorem uses conditions that are checkable\nfrom the data, instead of putting conditions on the unobservable Markov\ntransition matrix. \n\n"}
{"id": "1205.5075", "contents": "Title: Efficient Sparse Group Feature Selection via Nonconvex Optimization Abstract: Sparse feature selection has been demonstrated to be effective in handling\nhigh-dimensional data. While promising, most of the existing works use convex\nmethods, which may be suboptimal in terms of the accuracy of feature selection\nand parameter estimation. In this paper, we expand a nonconvex paradigm to\nsparse group feature selection, which is motivated by applications that require\nidentifying the underlying group structure and performing feature selection\nsimultaneously. The main contributions of this article are twofold: (1)\nstatistically, we introduce a nonconvex sparse group feature selection model\nwhich can reconstruct the oracle estimator. Therefore, consistent feature\nselection and parameter estimation can be achieved; (2) computationally, we\npropose an efficient algorithm that is applicable to large-scale problems.\nNumerical results suggest that the proposed nonconvex method compares favorably\nagainst its competitors on synthetic data and real-world applications, thus\nachieving desired goal of delivering high performance. \n\n"}
{"id": "1206.6381", "contents": "Title: Shortest path distance in random k-nearest neighbor graphs Abstract: Consider a weighted or unweighted k-nearest neighbor graph that has been\nbuilt on n data points drawn randomly according to some density p on R^d. We\nstudy the convergence of the shortest path distance in such graphs as the\nsample size tends to infinity. We prove that for unweighted kNN graphs, this\ndistance converges to an unpleasant distance function on the underlying space\nwhose properties are detrimental to machine learning. We also study the\nbehavior of the shortest path distance in weighted kNN graphs. \n\n"}
{"id": "1207.3031", "contents": "Title: Distributed Strongly Convex Optimization Abstract: A lot of effort has been invested into characterizing the convergence rates\nof gradient based algorithms for non-linear convex optimization. Recently,\nmotivated by large datasets and problems in machine learning, the interest has\nshifted towards distributed optimization. In this work we present a distributed\nalgorithm for strongly convex constrained optimization. Each node in a network\nof n computers converges to the optimum of a strongly convex, L-Lipchitz\ncontinuous, separable objective at a rate O(log (sqrt(n) T) / T) where T is the\nnumber of iterations. This rate is achieved in the online setting where the\ndata is revealed one at a time to the nodes, and in the batch setting where\neach node has access to its full local dataset from the start. The same\nconvergence rate is achieved in expectation when the subgradients used at each\nnode are corrupted with additive zero-mean noise. \n\n"}
{"id": "1207.6005", "contents": "Title: The expected performance of stellar parametrization with Gaia\n  spectrophotometry Abstract: Gaia will obtain astrometry and spectrophotometry for essentially all sources\nin the sky down to a broad band magnitude limit of G=20, an expected yield of\n10^9 stars. Its main scientific objective is to reveal the formation and\nevolution of our Galaxy through chemo-dynamical analysis. In addition to\ninferring positions, parallaxes and proper motions from the astrometry, we must\nalso infer the astrophysical parameters of the stars from the\nspectrophotometry, the BP/RP spectrum. Here we investigate the performance of\nthree different algorithms (SVM, ILIUM, Aeneas) for estimating the effective\ntemperature, line-of-sight interstellar extinction, metallicity and surface\ngravity of A-M stars over a wide range of these parameters and over the full\nmagnitude range Gaia will observe (G=6-20mag). One of the algorithms, Aeneas,\ninfers the posterior probability density function over all parameters, and can\noptionally take into account the parallax and the Hertzsprung-Russell diagram\nto improve the estimates. For all algorithms the accuracy of estimation depends\non G and on the value of the parameters themselves, so a broad summary of\nperformance is only approximate. For stars at G=15 with less than two\nmagnitudes extinction, we expect to be able to estimate Teff to within 1%, logg\nto 0.1-0.2dex, and [Fe/H] (for FGKM stars) to 0.1-0.2dex, just using the BP/RP\nspectrum (mean absolute error statistics are quoted). Performance degrades at\nlarger extinctions, but not always by a large amount. Extinction can be\nestimated to an accuracy of 0.05-0.2mag for stars across the full parameter\nrange with a priori unknown extinction between 0 and 10mag. Performance\ndegrades at fainter magnitudes, but even at G=19 we can estimate logg to better\nthan 0.2dex for all spectral types, and [Fe/H] to within 0.35dex for FGKM\nstars, for extinctions below 1mag. \n\n"}
{"id": "1208.0432", "contents": "Title: Efficient Point-to-Subspace Query in $\\ell^1$ with Application to Robust\n  Object Instance Recognition Abstract: Motivated by vision tasks such as robust face and object recognition, we\nconsider the following general problem: given a collection of low-dimensional\nlinear subspaces in a high-dimensional ambient (image) space, and a query point\n(image), efficiently determine the nearest subspace to the query in $\\ell^1$\ndistance. In contrast to the naive exhaustive search which entails large-scale\nlinear programs, we show that the computational burden can be cut down\nsignificantly by a simple two-stage algorithm: (1) projecting the query and\ndata-base subspaces into lower-dimensional space by random Cauchy matrix, and\nsolving small-scale distance evaluations (linear programs) in the projection\nspace to locate candidate nearest; (2) with few candidates upon independent\nrepetition of (1), getting back to the high-dimensional space and performing\nexhaustive search. To preserve the identity of the nearest subspace with\nnontrivial probability, the projection dimension typically is low-order\npolynomial of the subspace dimension multiplied by logarithm of number of the\nsubspaces (Theorem 2.1). The reduced dimensionality and hence complexity\nrenders the proposed algorithm particularly relevant to vision application such\nas robust face and object instance recognition that we investigate empirically. \n\n"}
{"id": "1209.1688", "contents": "Title: Rank Centrality: Ranking from Pair-wise Comparisons Abstract: The question of aggregating pair-wise comparisons to obtain a global ranking\nover a collection of objects has been of interest for a very long time: be it\nranking of online gamers (e.g. MSR's TrueSkill system) and chess players,\naggregating social opinions, or deciding which product to sell based on\ntransactions. In most settings, in addition to obtaining a ranking, finding\n`scores' for each object (e.g. player's rating) is of interest for\nunderstanding the intensity of the preferences.\n  In this paper, we propose Rank Centrality, an iterative rank aggregation\nalgorithm for discovering scores for objects (or items) from pair-wise\ncomparisons. The algorithm has a natural random walk interpretation over the\ngraph of objects with an edge present between a pair of objects if they are\ncompared; the score, which we call Rank Centrality, of an object turns out to\nbe its stationary probability under this random walk. To study the efficacy of\nthe algorithm, we consider the popular Bradley-Terry-Luce (BTL) model\n(equivalent to the Multinomial Logit (MNL) for pair-wise comparisons) in which\neach object has an associated score which determines the probabilistic outcomes\nof pair-wise comparisons between objects. In terms of the pair-wise marginal\nprobabilities, which is the main subject of this paper, the MNL model and the\nBTL model are identical. We bound the finite sample error rates between the\nscores assumed by the BTL model and those estimated by our algorithm. In\nparticular, the number of samples required to learn the score well with high\nprobability depends on the structure of the comparison graph. When the\nLaplacian of the comparison graph has a strictly positive spectral gap, e.g.\neach item is compared to a subset of randomly chosen items, this leads to\ndependence on the number of samples that is nearly order-optimal. \n\n"}
{"id": "1211.1043", "contents": "Title: Soft (Gaussian CDE) regression models and loss functions Abstract: Regression, unlike classification, has lacked a comprehensive and effective\napproach to deal with cost-sensitive problems by the reuse (and not a\nre-training) of general regression models. In this paper, a wide variety of\ncost-sensitive problems in regression (such as bids, asymmetric losses and\nrejection rules) can be solved effectively by a lightweight but powerful\napproach, consisting of: (1) the conversion of any traditional one-parameter\ncrisp regression model into a two-parameter soft regression model, seen as a\nnormal conditional density estimator, by the use of newly-introduced enrichment\nmethods; and (2) the reframing of an enriched soft regression model to new\ncontexts by an instance-dependent optimisation of the expected loss derived\nfrom the conditional normal distribution. \n\n"}
{"id": "1211.5037", "contents": "Title: Bayesian nonparametric Plackett-Luce models for the analysis of\n  preferences for college degree programmes Abstract: In this paper we propose a Bayesian nonparametric model for clustering\npartial ranking data. We start by developing a Bayesian nonparametric extension\nof the popular Plackett-Luce choice model that can handle an infinite number of\nchoice items. Our framework is based on the theory of random atomic measures,\nwith the prior specified by a completely random measure. We characterise the\nposterior distribution given data, and derive a simple and effective Gibbs\nsampler for posterior simulation. We then develop a Dirichlet process mixture\nextension of our model and apply it to investigate the clustering of\npreferences for college degree programmes amongst Irish secondary school\ngraduates. The existence of clusters of applicants who have similar preferences\nfor degree programmes is established and we determine that subject matter and\ngeographical location of the third level institution characterise these\nclusters. \n\n"}
{"id": "1212.0171", "contents": "Title: Message-Passing Algorithms for Quadratic Minimization Abstract: Gaussian belief propagation (GaBP) is an iterative algorithm for computing\nthe mean of a multivariate Gaussian distribution, or equivalently, the minimum\nof a multivariate positive definite quadratic function. Sufficient conditions,\nsuch as walk-summability, that guarantee the convergence and correctness of\nGaBP are known, but GaBP may fail to converge to the correct solution given an\narbitrary positive definite quadratic function. As was observed in previous\nwork, the GaBP algorithm fails to converge if the computation trees produced by\nthe algorithm are not positive definite. In this work, we will show that the\nfailure modes of the GaBP algorithm can be understood via graph covers, and we\nprove that a parameterized generalization of the min-sum algorithm can be used\nto ensure that the computation trees remain positive definite whenever the\ninput matrix is positive definite. We demonstrate that the resulting algorithm\nis closely related to other iterative schemes for quadratic minimization such\nas the Gauss-Seidel and Jacobi algorithms. Finally, we observe, empirically,\nthat there always exists a choice of parameters such that the above\ngeneralization of the GaBP algorithm converges. \n\n"}
{"id": "1212.1496", "contents": "Title: Excess risk bounds for multitask learning with trace norm regularization Abstract: Trace norm regularization is a popular method of multitask learning. We give\nexcess risk bounds with explicit dependence on the number of tasks, the number\nof examples per task and properties of the data distribution. The bounds are\nindependent of the dimension of the input space, which may be infinite as in\nthe case of reproducing kernel Hilbert spaces. A byproduct of the proof are\nbounds on the expected norm of sums of random positive semidefinite matrices\nwith subexponential moments. \n\n"}
{"id": "1212.5156", "contents": "Title: Nonparametric ridge estimation Abstract: We study the problem of estimating the ridges of a density function. Ridge\nestimation is an extension of mode finding and is useful for understanding the\nstructure of a density. It can also be used to find hidden structure in point\ncloud data. We show that, under mild regularity conditions, the ridges of the\nkernel density estimator consistently estimate the ridges of the true density.\nWhen the data are noisy measurements of a manifold, we show that the ridges are\nclose and topologically similar to the hidden manifold. To find the estimated\nridges in practice, we adapt the modified mean-shift algorithm proposed by\nOzertem and Erdogmus [J. Mach. Learn. Res. 12 (2011) 1249-1286]. Some numerical\nexperiments verify that the algorithm is accurate. \n\n"}
{"id": "1301.1318", "contents": "Title: Efficient Eigen-updating for Spectral Graph Clustering Abstract: Partitioning a graph into groups of vertices such that those within each\ngroup are more densely connected than vertices assigned to different groups,\nknown as graph clustering, is often used to gain insight into the organisation\nof large scale networks and for visualisation purposes. Whereas a large number\nof dedicated techniques have been recently proposed for static graphs, the\ndesign of on-line graph clustering methods tailored for evolving networks is a\nchallenging problem, and much less documented in the literature. Motivated by\nthe broad variety of applications concerned, ranging from the study of\nbiological networks to the analysis of networks of scientific references\nthrough the exploration of communications networks such as the World Wide Web,\nit is the main purpose of this paper to introduce a novel, computationally\nefficient, approach to graph clustering in the evolutionary context. Namely,\nthe method promoted in this article can be viewed as an incremental eigenvalue\nsolution for the spectral clustering method described by Ng. et al. (2001). The\nincremental eigenvalue solution is a general technique for finding the\napproximate eigenvectors of a symmetric matrix given a change. As well as\noutlining the approach in detail, we present a theoretical bound on the quality\nof the approximate eigenvectors using perturbation theory. We then derive a\nnovel spectral clustering algorithm called Incremental Approximate Spectral\nClustering (IASC). The IASC algorithm is simple to implement and its efficacy\nis demonstrated on both synthetic and real datasets modelling the evolution of\na HIV epidemic, a citation network and the purchase history graph of an\ne-commerce website. \n\n"}
{"id": "1301.3461", "contents": "Title: Factorized Topic Models Abstract: In this paper we present a modification to a latent topic model, which makes\nthe model exploit supervision to produce a factorized representation of the\nobserved data. The structured parameterization separately encodes variance that\nis shared between classes from variance that is private to each class by the\nintroduction of a new prior over the topic space. The approach allows for a\nmore eff{}icient inference and provides an intuitive interpretation of the data\nin terms of an informative signal together with structured noise. The\nfactorized representation is shown to enhance inference performance for image,\ntext, and video classification. \n\n"}
{"id": "1302.0082", "contents": "Title: Distribution-Free Distribution Regression Abstract: `Distribution regression' refers to the situation where a response Y depends\non a covariate P where P is a probability distribution. The model is Y=f(P) +\nmu where f is an unknown regression function and mu is a random error.\nTypically, we do not observe P directly, but rather, we observe a sample from\nP. In this paper we develop theory and methods for distribution-free versions\nof distribution regression. This means that we do not make distributional\nassumptions about the error term mu and covariate P. We prove that when the\neffective dimension is small enough (as measured by the doubling dimension),\nthen the excess prediction risk converges to zero with a polynomial rate. \n\n"}
{"id": "1302.1611", "contents": "Title: Bounded regret in stochastic multi-armed bandits Abstract: We study the stochastic multi-armed bandit problem when one knows the value\n$\\mu^{(\\star)}$ of an optimal arm, as a well as a positive lower bound on the\nsmallest positive gap $\\Delta$. We propose a new randomized policy that attains\na regret {\\em uniformly bounded over time} in this setting. We also prove\nseveral lower bounds, which show in particular that bounded regret is not\npossible if one only knows $\\Delta$, and bounded regret of order $1/\\Delta$ is\nnot possible if one only knows $\\mu^{(\\star)}$ \n\n"}
{"id": "1302.2576", "contents": "Title: The trace norm constrained matrix-variate Gaussian process for multitask\n  bipartite ranking Abstract: We propose a novel hierarchical model for multitask bipartite ranking. The\nproposed approach combines a matrix-variate Gaussian process with a generative\nmodel for task-wise bipartite ranking. In addition, we employ a novel trace\nconstrained variational inference approach to impose low rank structure on the\nposterior matrix-variate Gaussian process. The resulting posterior covariance\nfunction is derived in closed form, and the posterior mean function is the\nsolution to a matrix-variate regression with a novel spectral elastic net\nregularizer. Further, we show that variational inference for the trace\nconstrained matrix-variate Gaussian process combined with maximum likelihood\nparameter estimation for the bipartite ranking model is jointly convex. Our\nmotivating application is the prioritization of candidate disease genes. The\ngoal of this task is to aid the identification of unobserved associations\nbetween human genes and diseases using a small set of observed associations as\nwell as kernels induced by gene-gene interaction networks and disease\nontologies. Our experimental results illustrate the performance of the proposed\nmodel on real world datasets. Moreover, we find that the resulting low rank\nsolution improves the computational scalability of training and testing as\ncompared to baseline models. \n\n"}
{"id": "1302.4387", "contents": "Title: Online Learning with Switching Costs and Other Adaptive Adversaries Abstract: We study the power of different types of adaptive (nonoblivious) adversaries\nin the setting of prediction with expert advice, under both full-information\nand bandit feedback. We measure the player's performance using a new notion of\nregret, also known as policy regret, which better captures the adversary's\nadaptiveness to the player's behavior. In a setting where losses are allowed to\ndrift, we characterize ---in a nearly complete manner--- the power of adaptive\nadversaries with bounded memories and switching costs. In particular, we show\nthat with switching costs, the attainable rate with bandit feedback is\n$\\widetilde{\\Theta}(T^{2/3})$. Interestingly, this rate is significantly worse\nthan the $\\Theta(\\sqrt{T})$ rate attainable with switching costs in the\nfull-information case. Via a novel reduction from experts to bandits, we also\nshow that a bounded memory adversary can force $\\widetilde{\\Theta}(T^{2/3})$\nregret even in the full information case, proving that switching costs are\neasier to control than bounded memory adversaries. Our lower bounds rely on a\nnew stochastic adversary strategy that generates loss processes with strong\ndependencies. \n\n"}
{"id": "1302.4886", "contents": "Title: Fast methods for denoising matrix completion formulations, with\n  applications to robust seismic data interpolation Abstract: Recent SVD-free matrix factorization formulations have enabled rank\nminimization for systems with millions of rows and columns, paving the way for\nmatrix completion in extremely large-scale applications, such as seismic data\ninterpolation.\n  In this paper, we consider matrix completion formulations designed to hit a\ntarget data-fitting error level provided by the user, and propose an algorithm\ncalled LR-BPDN that is able to exploit factorized formulations to solve the\ncorresponding optimization problem. Since practitioners typically have strong\nprior knowledge about target error level, this innovation makes it easy to\napply the algorithm in practice, leaving only the factor rank to be determined.\n  Within the established framework, we propose two extensions that are highly\nrelevant to solving practical challenges of data interpolation. First, we\npropose a weighted extension that allows known subspace information to improve\nthe results of matrix completion formulations. We show how this weighting can\nbe used in the context of frequency continuation, an essential aspect to\nseismic data interpolation. Second, we propose matrix completion formulations\nthat are robust to large measurement errors in the available data.\n  We illustrate the advantages of LR-BPDN on the collaborative filtering\nproblem using the MovieLens 1M, 10M, and Netflix 100M datasets. Then, we use\nthe new method, along with its robust and subspace re-weighted extensions, to\nobtain high-quality reconstructions for large scale seismic interpolation\nproblems with real data, even in the presence of data contamination. \n\n"}
{"id": "1303.7226", "contents": "Title: Detecting Overlapping Temporal Community Structure in Time-Evolving\n  Networks Abstract: We present a principled approach for detecting overlapping temporal community\nstructure in dynamic networks. Our method is based on the following framework:\nfind the overlapping temporal community structure that maximizes a quality\nfunction associated with each snapshot of the network subject to a temporal\nsmoothness constraint. A novel quality function and a smoothness constraint are\nproposed to handle overlaps, and a new convex relaxation is used to solve the\nresulting combinatorial optimization problem. We provide theoretical guarantees\nas well as experimental results that reveal community structure in real and\nsynthetic networks. Our main insight is that certain structures can be\nidentified only when temporal correlation is considered and when communities\nare allowed to overlap. In general, discovering such overlapping temporal\ncommunity structure can enhance our understanding of real-world complex\nnetworks by revealing the underlying stability behind their seemingly chaotic\nevolution. \n\n"}
{"id": "1304.5583", "contents": "Title: Distributed Low-rank Subspace Segmentation Abstract: Vision problems ranging from image clustering to motion segmentation to\nsemi-supervised learning can naturally be framed as subspace segmentation\nproblems, in which one aims to recover multiple low-dimensional subspaces from\nnoisy and corrupted input data. Low-Rank Representation (LRR), a convex\nformulation of the subspace segmentation problem, is provably and empirically\naccurate on small problems but does not scale to the massive sizes of modern\nvision datasets. Moreover, past work aimed at scaling up low-rank matrix\nfactorization is not applicable to LRR given its non-decomposable constraints.\nIn this work, we propose a novel divide-and-conquer algorithm for large-scale\nsubspace segmentation that can cope with LRR's non-decomposable constraints and\nmaintains LRR's strong recovery guarantees. This has immediate implications for\nthe scalability of subspace segmentation, which we demonstrate on a benchmark\nface recognition dataset and in simulations. We then introduce novel\napplications of LRR-based subspace segmentation to large-scale semi-supervised\nlearning for multimedia event detection, concept detection, and image tagging.\nIn each case, we obtain state-of-the-art results and order-of-magnitude speed\nups. \n\n"}
{"id": "1305.5306", "contents": "Title: A Supervised Neural Autoregressive Topic Model for Simultaneous Image\n  Classification and Annotation Abstract: Topic modeling based on latent Dirichlet allocation (LDA) has been a\nframework of choice to perform scene recognition and annotation. Recently, a\nnew type of topic model called the Document Neural Autoregressive Distribution\nEstimator (DocNADE) was proposed and demonstrated state-of-the-art performance\nfor document modeling. In this work, we show how to successfully apply and\nextend this model to the context of visual scene modeling. Specifically, we\npropose SupDocNADE, a supervised extension of DocNADE, that increases the\ndiscriminative power of the hidden topic features by incorporating label\ninformation into the training objective of the model. We also describe how to\nleverage information about the spatial position of the visual words and how to\nembed additional image annotations, so as to simultaneously perform image\nclassification and annotation. We test our model on the Scene15, LabelMe and\nUIUC-Sports datasets and show that it compares favorably to other topic models\nsuch as the supervised variant of LDA. \n\n"}
{"id": "1306.0543", "contents": "Title: Predicting Parameters in Deep Learning Abstract: We demonstrate that there is significant redundancy in the parameterization\nof several deep learning models. Given only a few weight values for each\nfeature it is possible to accurately predict the remaining values. Moreover, we\nshow that not only can the parameter values be predicted, but many of them need\nnot be learned at all. We train several different architectures by learning\nonly a small number of weights and predicting the rest. In the best case we are\nable to predict more than 95% of the weights of a network without any drop in\naccuracy. \n\n"}
{"id": "1306.3476", "contents": "Title: Hyperparameter Optimization and Boosting for Classifying Facial\n  Expressions: How good can a \"Null\" Model be? Abstract: One of the goals of the ICML workshop on representation and learning is to\nestablish benchmark scores for a new data set of labeled facial expressions.\nThis paper presents the performance of a \"Null\" model consisting of\nconvolutions with random weights, PCA, pooling, normalization, and a linear\nreadout. Our approach focused on hyperparameter optimization rather than novel\nmodel components. On the Facial Expression Recognition Challenge held by the\nKaggle website, our hyperparameter optimization approach achieved a score of\n60% accuracy on the test data. This paper also introduces a new ensemble\nconstruction variant that combines hyperparameter optimization with the\nconstruction of ensembles. This algorithm constructed an ensemble of four\nmodels that scored 65.5% accuracy. These scores rank 12th and 5th respectively\namong the 56 challenge participants. It is worth noting that our approach was\ndeveloped prior to the release of the data set, and applied without\nmodification; our strong competition performance suggests that the TPE\nhyperparameter optimization algorithm and domain expertise encoded in our Null\nmodel can generalize to new image classification data sets. \n\n"}
{"id": "1307.3617", "contents": "Title: MCMC Learning Abstract: The theory of learning under the uniform distribution is rich and deep, with\nconnections to cryptography, computational complexity, and the analysis of\nboolean functions to name a few areas. This theory however is very limited due\nto the fact that the uniform distribution and the corresponding Fourier basis\nare rarely encountered as a statistical model.\n  A family of distributions that vastly generalizes the uniform distribution on\nthe Boolean cube is that of distributions represented by Markov Random Fields\n(MRF). Markov Random Fields are one of the main tools for modeling high\ndimensional data in many areas of statistics and machine learning.\n  In this paper we initiate the investigation of extending central ideas,\nmethods and algorithms from the theory of learning under the uniform\ndistribution to the setup of learning concepts given examples from MRF\ndistributions. In particular, our results establish a novel connection between\nproperties of MCMC sampling of MRFs and learning under the MRF distribution. \n\n"}
{"id": "1307.4891", "contents": "Title: Robust Subspace Clustering via Thresholding Abstract: The problem of clustering noisy and incompletely observed high-dimensional\ndata points into a union of low-dimensional subspaces and a set of outliers is\nconsidered. The number of subspaces, their dimensions, and their orientations\nare assumed unknown. We propose a simple low-complexity subspace clustering\nalgorithm, which applies spectral clustering to an adjacency matrix obtained by\nthresholding the correlations between data points. In other words, the\nadjacency matrix is constructed from the nearest neighbors of each data point\nin spherical distance. A statistical performance analysis shows that the\nalgorithm exhibits robustness to additive noise and succeeds even when the\nsubspaces intersect. Specifically, our results reveal an explicit tradeoff\nbetween the affinity of the subspaces and the tolerable noise level. We\nfurthermore prove that the algorithm succeeds even when the data points are\nincompletely observed with the number of missing entries allowed to be (up to a\nlog-factor) linear in the ambient dimension. We also propose a simple scheme\nthat provably detects outliers, and we present numerical results on real and\nsynthetic data. \n\n"}
{"id": "1307.8136", "contents": "Title: DeBaCl: A Python Package for Interactive DEnsity-BAsed CLustering Abstract: The level set tree approach of Hartigan (1975) provides a probabilistically\nbased and highly interpretable encoding of the clustering behavior of a\ndataset. By representing the hierarchy of data modes as a dendrogram of the\nlevel sets of a density estimator, this approach offers many advantages for\nexploratory analysis and clustering, especially for complex and\nhigh-dimensional data. Several R packages exist for level set tree estimation,\nbut their practical usefulness is limited by computational inefficiency,\nabsence of interactive graphical capabilities and, from a theoretical\nperspective, reliance on asymptotic approximations. To make it easier for\npractitioners to capture the advantages of level set trees, we have written the\nPython package DeBaCl for DEnsity-BAsed CLustering. In this article we\nillustrate how DeBaCl's level set tree estimates can be used for difficult\nclustering tasks and interactive graphical data analysis. The package is\nintended to promote the practical use of level set trees through improvements\nin computational efficiency and a high degree of user customization. In\naddition, the flexible algorithms implemented in DeBaCl enjoy finite sample\naccuracy, as demonstrated in recent literature on density clustering. Finally,\nwe show the level set tree framework can be easily extended to deal with\nfunctional data. \n\n"}
{"id": "1308.2655", "contents": "Title: KL-based Control of the Learning Schedule for Surrogate Black-Box\n  Optimization Abstract: This paper investigates the control of an ML component within the Covariance\nMatrix Adaptation Evolution Strategy (CMA-ES) devoted to black-box\noptimization. The known CMA-ES weakness is its sample complexity, the number of\nevaluations of the objective function needed to approximate the global optimum.\nThis weakness is commonly addressed through surrogate optimization, learning an\nestimate of the objective function a.k.a. surrogate model, and replacing most\nevaluations of the true objective function with the (inexpensive) evaluation of\nthe surrogate model. This paper presents a principled control of the learning\nschedule (when to relearn the surrogate model), based on the Kullback-Leibler\ndivergence of the current search distribution and the training distribution of\nthe former surrogate model. The experimental validation of the proposed\napproach shows significant performance gains on a comprehensive set of\nill-conditioned benchmark problems, compared to the best state of the art\nincluding the quasi-Newton high-precision BFGS method. \n\n"}
{"id": "1308.3383", "contents": "Title: Axioms for graph clustering quality functions Abstract: We investigate properties that intuitively ought to be satisfied by graph\nclustering quality functions, that is, functions that assign a score to a\nclustering of a graph. Graph clustering, also known as network community\ndetection, is often performed by optimizing such a function. Two axioms\ntailored for graph clustering quality functions are introduced, and the four\naxioms introduced in previous work on distance based clustering are\nreformulated and generalized for the graph setting. We show that modularity, a\nstandard quality function for graph clustering, does not satisfy all of these\nsix properties. This motivates the derivation of a new family of quality\nfunctions, adaptive scale modularity, which does satisfy the proposed axioms.\nAdaptive scale modularity has two parameters, which give greater flexibility in\nthe kinds of clusterings that can be found. Standard graph clustering quality\nfunctions, such as normalized cut and unnormalized cut, are obtained as special\ncases of adaptive scale modularity.\n  In general, the results of our investigation indicate that the considered\naxiomatic framework covers existing `good' quality functions for graph\nclustering, and can be used to derive an interesting new family of quality\nfunctions. \n\n"}
{"id": "1308.3506", "contents": "Title: Computational Rationalization: The Inverse Equilibrium Problem Abstract: Modeling the purposeful behavior of imperfect agents from a small number of\nobservations is a challenging task. When restricted to the single-agent\ndecision-theoretic setting, inverse optimal control techniques assume that\nobserved behavior is an approximately optimal solution to an unknown decision\nproblem. These techniques learn a utility function that explains the example\nbehavior and can then be used to accurately predict or imitate future behavior\nin similar observed or unobserved situations.\n  In this work, we consider similar tasks in competitive and cooperative\nmulti-agent domains. Here, unlike single-agent settings, a player cannot\nmyopically maximize its reward; it must speculate on how the other agents may\nact to influence the game's outcome. Employing the game-theoretic notion of\nregret and the principle of maximum entropy, we introduce a technique for\npredicting and generalizing behavior. \n\n"}
{"id": "1308.5038", "contents": "Title: Group-Sparse Signal Denoising: Non-Convex Regularization, Convex\n  Optimization Abstract: Convex optimization with sparsity-promoting convex regularization is a\nstandard approach for estimating sparse signals in noise. In order to promote\nsparsity more strongly than convex regularization, it is also standard practice\nto employ non-convex optimization. In this paper, we take a third approach. We\nutilize a non-convex regularization term chosen such that the total cost\nfunction (consisting of data consistency and regularization terms) is convex.\nTherefore, sparsity is more strongly promoted than in the standard convex\nformulation, but without sacrificing the attractive aspects of convex\noptimization (unique minimum, robust algorithms, etc.). We use this idea to\nimprove the recently developed 'overlapping group shrinkage' (OGS) algorithm\nfor the denoising of group-sparse signals. The algorithm is applied to the\nproblem of speech enhancement with favorable results in terms of both SNR and\nperceptual quality. \n\n"}
{"id": "1309.1952", "contents": "Title: A Clustering Approach to Learn Sparsely-Used Overcomplete Dictionaries Abstract: We consider the problem of learning overcomplete dictionaries in the context\nof sparse coding, where each sample selects a sparse subset of dictionary\nelements. Our main result is a strategy to approximately recover the unknown\ndictionary using an efficient algorithm. Our algorithm is a clustering-style\nprocedure, where each cluster is used to estimate a dictionary element. The\nresulting solution can often be further cleaned up to obtain a high accuracy\nestimate, and we provide one simple scenario where $\\ell_1$-regularized\nregression can be used for such a second stage. \n\n"}
{"id": "1309.3533", "contents": "Title: Mixed Membership Models for Time Series Abstract: In this article we discuss some of the consequences of the mixed membership\nperspective on time series analysis. In its most abstract form, a mixed\nmembership model aims to associate an individual entity with some set of\nattributes based on a collection of observed data. Although much of the\nliterature on mixed membership models considers the setting in which\nexchangeable collections of data are associated with each member of a set of\nentities, it is equally natural to consider problems in which an entire time\nseries is viewed as an entity and the goal is to characterize the time series\nin terms of a set of underlying dynamic attributes or \"dynamic regimes\".\nIndeed, this perspective is already present in the classical hidden Markov\nmodel, where the dynamic regimes are referred to as \"states\", and the\ncollection of states realized in a sample path of the underlying process can be\nviewed as a mixed membership characterization of the observed time series. Our\ngoal here is to review some of the richer modeling possibilities for time\nseries that are provided by recent developments in the mixed membership\nframework. \n\n"}
{"id": "1310.0509", "contents": "Title: Summary Statistics for Partitionings and Feature Allocations Abstract: Infinite mixture models are commonly used for clustering. One can sample from\nthe posterior of mixture assignments by Monte Carlo methods or find its maximum\na posteriori solution by optimization. However, in some problems the posterior\nis diffuse and it is hard to interpret the sampled partitionings. In this\npaper, we introduce novel statistics based on block sizes for representing\nsample sets of partitionings and feature allocations. We develop an\nelement-based definition of entropy to quantify segmentation among their\nelements. Then we propose a simple algorithm called entropy agglomeration (EA)\nto summarize and visualize this information. Experiments on various infinite\nmixture posteriors as well as a feature allocation dataset demonstrate that the\nproposed statistics are useful in practice. \n\n"}
{"id": "1310.1533", "contents": "Title: CAM: Causal additive models, high-dimensional order search and penalized\n  regression Abstract: We develop estimation for potentially high-dimensional additive structural\nequation models. A key component of our approach is to decouple order search\namong the variables from feature or edge selection in a directed acyclic graph\nencoding the causal structure. We show that the former can be done with\nnonregularized (restricted) maximum likelihood estimation while the latter can\nbe efficiently addressed using sparse regression techniques. Thus, we\nsubstantially simplify the problem of structure search and estimation for an\nimportant class of causal models. We establish consistency of the (restricted)\nmaximum likelihood estimator for low- and high-dimensional scenarios, and we\nalso allow for misspecification of the error distribution. Furthermore, we\ndevelop an efficient computational algorithm which can deal with many\nvariables, and the new method's accuracy and performance is illustrated on\nsimulated and real data. \n\n"}
{"id": "1311.1644", "contents": "Title: The Maximum Entropy Relaxation Path Abstract: The relaxed maximum entropy problem is concerned with finding a probability\ndistribution on a finite set that minimizes the relative entropy to a given\nprior distribution, while satisfying relaxed max-norm constraints with respect\nto a third observed multinomial distribution. We study the entire relaxation\npath for this problem in detail. We show existence and a geometric description\nof the relaxation path. Specifically, we show that the maximum entropy\nrelaxation path admits a planar geometric description as an increasing,\npiecewise linear function in the inverse relaxation parameter. We derive fast\nalgorithms for tracking the path. In various realistic settings, our algorithms\nrequire $O(n\\log(n))$ operations for probability distributions on $n$ points,\nmaking it possible to handle large problems. Once the path has been recovered,\nwe show that given a validation set, the family of admissible models is reduced\nfrom an infinite family to a small, discrete set. We demonstrate the merits of\nour approach in experiments with synthetic data and discuss its potential for\nthe estimation of compact n-gram language models. \n\n"}
{"id": "1311.1780", "contents": "Title: Learned-Norm Pooling for Deep Feedforward and Recurrent Neural Networks Abstract: In this paper we propose and investigate a novel nonlinear unit, called $L_p$\nunit, for deep neural networks. The proposed $L_p$ unit receives signals from\nseveral projections of a subset of units in the layer below and computes a\nnormalized $L_p$ norm. We notice two interesting interpretations of the $L_p$\nunit. First, the proposed unit can be understood as a generalization of a\nnumber of conventional pooling operators such as average, root-mean-square and\nmax pooling widely used in, for instance, convolutional neural networks (CNN),\nHMAX models and neocognitrons. Furthermore, the $L_p$ unit is, to a certain\ndegree, similar to the recently proposed maxout unit (Goodfellow et al., 2013)\nwhich achieved the state-of-the-art object recognition results on a number of\nbenchmark datasets. Secondly, we provide a geometrical interpretation of the\nactivation function based on which we argue that the $L_p$ unit is more\nefficient at representing complex, nonlinear separating boundaries. Each $L_p$\nunit defines a superelliptic boundary, with its exact shape defined by the\norder $p$. We claim that this makes it possible to model arbitrarily shaped,\ncurved boundaries more efficiently by combining a few $L_p$ units of different\norders. This insight justifies the need for learning different orders for each\nunit in the model. We empirically evaluate the proposed $L_p$ units on a number\nof datasets and show that multilayer perceptrons (MLP) consisting of the $L_p$\nunits achieve the state-of-the-art results on a number of benchmark datasets.\nFurthermore, we evaluate the proposed $L_p$ unit on the recently proposed deep\nrecurrent neural networks (RNN). \n\n"}
{"id": "1311.2547", "contents": "Title: Learning Mixtures of Linear Classifiers Abstract: We consider a discriminative learning (regression) problem, whereby the\nregression function is a convex combination of k linear classifiers. Existing\napproaches are based on the EM algorithm, or similar techniques, without\nprovable guarantees. We develop a simple method based on spectral techniques\nand a `mirroring' trick, that discovers the subspace spanned by the\nclassifiers' parameter vectors. Under a probabilistic assumption on the feature\nvector distribution, we prove that this approach has nearly optimal statistical\nefficiency. \n\n"}
{"id": "1311.3494", "contents": "Title: Fundamental Limits of Online and Distributed Algorithms for Statistical\n  Learning and Estimation Abstract: Many machine learning approaches are characterized by information constraints\non how they interact with the training data. These include memory and\nsequential access constraints (e.g. fast first-order methods to solve\nstochastic optimization problems); communication constraints (e.g. distributed\nlearning); partial access to the underlying data (e.g. missing features and\nmulti-armed bandits) and more. However, currently we have little understanding\nhow such information constraints fundamentally affect our performance,\nindependent of the learning problem semantics. For example, are there learning\nproblems where any algorithm which has small memory footprint (or can use any\nbounded number of bits from each example, or has certain communication\nconstraints) will perform worse than what is possible without such constraints?\nIn this paper, we describe how a single set of results implies positive answers\nto the above, for several different settings. \n\n"}
{"id": "1311.5871", "contents": "Title: Finding sparse solutions of systems of polynomial equations via\n  group-sparsity optimization Abstract: The paper deals with the problem of finding sparse solutions to systems of\npolynomial equations possibly perturbed by noise. In particular, we show how\nthese solutions can be recovered from group-sparse solutions of a derived\nsystem of linear equations. Then, two approaches are considered to find these\ngroup-sparse solutions. The first one is based on a convex relaxation resulting\nin a second-order cone programming formulation which can benefit from efficient\nreweighting techniques for sparsity enhancement. For this approach, sufficient\nconditions for the exact recovery of the sparsest solution to the polynomial\nsystem are derived in the noiseless setting, while stable recovery results are\nobtained for the noisy case. Though lacking a similar analysis, the second\napproach provides a more computationally efficient algorithm based on a greedy\nstrategy adding the groups one-by-one. With respect to previous work, the\nproposed methods recover the sparsest solution in a very short computing time\nwhile remaining at least as accurate in terms of the probability of success.\nThis probability is empirically analyzed to emphasize the relationship between\nthe ability of the methods to solve the polynomial system and the sparsity of\nthe solution. \n\n"}
{"id": "1311.6182", "contents": "Title: Robust Low-rank Tensor Recovery: Models and Algorithms Abstract: Robust tensor recovery plays an instrumental role in robustifying tensor\ndecompositions for multilinear data analysis against outliers, gross\ncorruptions and missing values and has a diverse array of applications. In this\npaper, we study the problem of robust low-rank tensor recovery in a convex\noptimization framework, drawing upon recent advances in robust Principal\nComponent Analysis and tensor completion. We propose tailored optimization\nalgorithms with global convergence guarantees for solving both the constrained\nand the Lagrangian formulations of the problem. These algorithms are based on\nthe highly efficient alternating direction augmented Lagrangian and accelerated\nproximal gradient methods. We also propose a nonconvex model that can often\nimprove the recovery results from the convex models. We investigate the\nempirical recoverability properties of the convex and nonconvex formulations\nand compare the computational performance of the algorithms on simulated data.\nWe demonstrate through a number of real applications the practical\neffectiveness of this convex optimization framework for robust low-rank tensor\nrecovery. \n\n"}
{"id": "1312.0232", "contents": "Title: Stochastic continuum armed bandit problem of few linear parameters in\n  high dimensions Abstract: We consider a stochastic continuum armed bandit problem where the arms are\nindexed by the $\\ell_2$ ball $B_{d}(1+\\nu)$ of radius $1+\\nu$ in\n$\\mathbb{R}^d$. The reward functions $r :B_{d}(1+\\nu) \\rightarrow \\mathbb{R}$\nare considered to intrinsically depend on $k \\ll d$ unknown linear parameters\nso that $r(\\mathbf{x}) = g(\\mathbf{A} \\mathbf{x})$ where $\\mathbf{A}$ is a full\nrank $k \\times d$ matrix. Assuming the mean reward function to be smooth we\nmake use of results from low-rank matrix recovery literature and derive an\nefficient randomized algorithm which achieves a regret bound of $O(C(k,d)\nn^{\\frac{1+k}{2+k}} (\\log n)^{\\frac{1}{2+k}})$ with high probability. Here\n$C(k,d)$ is at most polynomial in $d$ and $k$ and $n$ is the number of rounds\nor the sampling budget which is assumed to be known beforehand. \n\n"}
{"id": "1312.3429", "contents": "Title: Unsupervised learning of depth and motion Abstract: We present a model for the joint estimation of disparity and motion. The\nmodel is based on learning about the interrelations between images from\nmultiple cameras, multiple frames in a video, or the combination of both. We\nshow that learning depth and motion cues, as well as their combinations, from\ndata is possible within a single type of architecture and a single type of\nlearning algorithm, by using biologically inspired \"complex cell\" like units,\nwhich encode correlations between the pixels across image pairs. Our\nexperimental results show that the learning of depth and motion makes it\npossible to achieve state-of-the-art performance in 3-D activity analysis, and\nto outperform existing hand-engineered 3-D motion features by a very large\nmargin. \n\n"}
{"id": "1312.5604", "contents": "Title: Learning Transformations for Classification Forests Abstract: This work introduces a transformation-based learner model for classification\nforests. The weak learner at each split node plays a crucial role in a\nclassification tree. We propose to optimize the splitting objective by learning\na linear transformation on subspaces using nuclear norm as the optimization\ncriteria. The learned linear transformation restores a low-rank structure for\ndata from the same class, and, at the same time, maximizes the separation\nbetween different classes, thereby improving the performance of the split\nfunction. Theoretical and experimental results support the proposed framework. \n\n"}
{"id": "1312.5847", "contents": "Title: Deep learning for neuroimaging: a validation study Abstract: Deep learning methods have recently made notable advances in the tasks of\nclassification and representation learning. These tasks are important for brain\nimaging and neuroscience discovery, making the methods attractive for porting\nto a neuroimager's toolbox. Success of these methods is, in part, explained by\nthe flexibility of deep learning models. However, this flexibility makes the\nprocess of porting to new areas a difficult parameter optimization problem. In\nthis work we demonstrate our results (and feasible parameter ranges) in\napplication of deep learning methods to structural and functional brain imaging\ndata. We also describe a novel constraint-based approach to visualizing high\ndimensional data. We use it to analyze the effect of parameter choices on data\ntransformations. Our results show that deep learning methods are able to learn\nphysiologically important representations and detect latent relations in\nneuroimaging data. \n\n"}
{"id": "1401.0579", "contents": "Title: More Algorithms for Provable Dictionary Learning Abstract: In dictionary learning, also known as sparse coding, the algorithm is given\nsamples of the form $y = Ax$ where $x\\in \\mathbb{R}^m$ is an unknown random\nsparse vector and $A$ is an unknown dictionary matrix in $\\mathbb{R}^{n\\times\nm}$ (usually $m > n$, which is the overcomplete case). The goal is to learn $A$\nand $x$. This problem has been studied in neuroscience, machine learning,\nvisions, and image processing. In practice it is solved by heuristic algorithms\nand provable algorithms seemed hard to find. Recently, provable algorithms were\nfound that work if the unknown feature vector $x$ is $\\sqrt{n}$-sparse or even\nsparser. Spielman et al. \\cite{DBLP:journals/jmlr/SpielmanWW12} did this for\ndictionaries where $m=n$; Arora et al. \\cite{AGM} gave an algorithm for\novercomplete ($m >n$) and incoherent matrices $A$; and Agarwal et al.\n\\cite{DBLP:journals/corr/AgarwalAN13} handled a similar case but with weaker\nguarantees.\n  This raised the problem of designing provable algorithms that allow sparsity\n$\\gg \\sqrt{n}$ in the hidden vector $x$. The current paper designs algorithms\nthat allow sparsity up to $n/poly(\\log n)$. It works for a class of matrices\nwhere features are individually recoverable, a new notion identified in this\npaper that may motivate further work.\n  The algorithm runs in quasipolynomial time because they use limited\nenumeration. \n\n"}
{"id": "1401.1137", "contents": "Title: Sparse graphs using exchangeable random measures Abstract: Statistical network modeling has focused on representing the graph as a\ndiscrete structure, namely the adjacency matrix, and considering the\nexchangeability of this array. In such cases, the Aldous-Hoover representation\ntheorem (Aldous, 1981;Hoover, 1979} applies and informs us that the graph is\nnecessarily either dense or empty. In this paper, we instead consider\nrepresenting the graph as a measure on $\\mathbb{R}_+^2$. For the associated\ndefinition of exchangeability in this continuous space, we rely on the\nKallenberg representation theorem (Kallenberg, 2005). We show that for certain\nchoices of such exchangeable random measures underlying our graph construction,\nour network process is sparse with power-law degree distribution. In\nparticular, we build on the framework of completely random measures (CRMs) and\nuse the theory associated with such processes to derive important network\nproperties, such as an urn representation for our analysis and network\nsimulation. Our theoretical results are explored empirically and compared to\ncommon network models. We then present a Hamiltonian Monte Carlo algorithm for\nefficient exploration of the posterior distribution and demonstrate that we are\nable to recover graphs ranging from dense to sparse--and perform associated\ntests--based on our flexible CRM-based formulation. We explore network\nproperties in a range of real datasets, including Facebook social circles, a\npolitical blogosphere, protein networks, citation networks, and world wide web\nnetworks, including networks with hundreds of thousands of nodes and millions\nof edges. \n\n"}
{"id": "1401.3409", "contents": "Title: Low-Rank Modeling and Its Applications in Image Analysis Abstract: Low-rank modeling generally refers to a class of methods that solve problems\nby representing variables of interest as low-rank matrices. It has achieved\ngreat success in various fields including computer vision, data mining, signal\nprocessing and bioinformatics. Recently, much progress has been made in\ntheories, algorithms and applications of low-rank modeling, such as exact\nlow-rank matrix recovery via convex programming and matrix completion applied\nto collaborative filtering. These advances have brought more and more\nattentions to this topic. In this paper, we review the recent advance of\nlow-rank modeling, the state-of-the-art algorithms, and related applications in\nimage analysis. We first give an overview to the concept of low-rank modeling\nand challenging problems in this area. Then, we summarize the models and\nalgorithms for low-rank matrix recovery and illustrate their advantages and\nlimitations with numerical experiments. Next, we introduce a few applications\nof low-rank modeling in the context of image analysis. Finally, we conclude\nthis paper with some discussions. \n\n"}
{"id": "1401.4489", "contents": "Title: An Analysis of Random Projections in Cancelable Biometrics Abstract: With increasing concerns about security, the need for highly secure physical\nbiometrics-based authentication systems utilizing \\emph{cancelable biometric}\ntechnologies is on the rise. Because the problem of cancelable template\ngeneration deals with the trade-off between template security and matching\nperformance, many state-of-the-art algorithms successful in generating high\nquality cancelable biometrics all have random projection as one of their early\nprocessing steps. This paper therefore presents a formal analysis of why random\nprojections is an essential step in cancelable biometrics. By formally defining\nthe notion of an \\textit{Independent Subspace Structure} for datasets, it can\nbe shown that random projection preserves the subspace structure of data\nvectors generated from a union of independent linear subspaces. The bound on\nthe minimum number of random vectors required for this to hold is also derived\nand is shown to depend logarithmically on the number of data samples, not only\nin independent subspaces but in disjoint subspace settings as well. The\ntheoretical analysis presented is supported in detail with empirical results on\nreal-world face recognition datasets. \n\n"}
{"id": "1402.0453", "contents": "Title: Fine-Grained Visual Categorization via Multi-stage Metric Learning Abstract: Fine-grained visual categorization (FGVC) is to categorize objects into\nsubordinate classes instead of basic classes. One major challenge in FGVC is\nthe co-occurrence of two issues: 1) many subordinate classes are highly\ncorrelated and are difficult to distinguish, and 2) there exists the large\nintra-class variation (e.g., due to object pose). This paper proposes to\nexplicitly address the above two issues via distance metric learning (DML). DML\naddresses the first issue by learning an embedding so that data points from the\nsame class will be pulled together while those from different classes should be\npushed apart from each other; and it addresses the second issue by allowing the\nflexibility that only a portion of the neighbors (not all data points) from the\nsame class need to be pulled together. However, feature representation of an\nimage is often high dimensional, and DML is known to have difficulty in dealing\nwith high dimensional feature vectors since it would require $\\mathcal{O}(d^2)$\nfor storage and $\\mathcal{O}(d^3)$ for optimization. To this end, we proposed a\nmulti-stage metric learning framework that divides the large-scale high\ndimensional learning problem to a series of simple subproblems, achieving\n$\\mathcal{O}(d)$ computational complexity. The empirical study with FVGC\nbenchmark datasets verifies that our method is both effective and efficient\ncompared to the state-of-the-art FGVC approaches. \n\n"}
{"id": "1402.1958", "contents": "Title: Better Optimism By Bayes: Adaptive Planning with Rich Models Abstract: The computational costs of inference and planning have confined Bayesian\nmodel-based reinforcement learning to one of two dismal fates: powerful\nBayes-adaptive planning but only for simplistic models, or powerful, Bayesian\nnon-parametric models but using simple, myopic planning strategies such as\nThompson sampling. We ask whether it is feasible and truly beneficial to\ncombine rich probabilistic models with a closer approximation to fully Bayesian\nplanning. First, we use a collection of counterexamples to show formal problems\nwith the over-optimism inherent in Thompson sampling. Then we leverage\nstate-of-the-art techniques in efficient Bayes-adaptive planning and\nnon-parametric Bayesian methods to perform qualitatively better than both\nexisting conventional algorithms and Thompson sampling on two contextual\nbandit-like problems. \n\n"}
{"id": "1402.3973", "contents": "Title: Dimensionality reduction with subgaussian matrices: a unified theory Abstract: We present a theory for Euclidean dimensionality reduction with subgaussian\nmatrices which unifies several restricted isometry property and\nJohnson-Lindenstrauss type results obtained earlier for specific data sets. In\nparticular, we recover and, in several cases, improve results for sets of\nsparse and structured sparse vectors, low-rank matrices and tensors, and smooth\nmanifolds. In addition, we establish a new Johnson-Lindenstrauss embedding for\ndata sets taking the form of an infinite union of subspaces of a Hilbert space. \n\n"}
{"id": "1402.5176", "contents": "Title: Pareto-depth for Multiple-query Image Retrieval Abstract: Most content-based image retrieval systems consider either one single query,\nor multiple queries that include the same object or represent the same semantic\ninformation. In this paper we consider the content-based image retrieval\nproblem for multiple query images corresponding to different image semantics.\nWe propose a novel multiple-query information retrieval algorithm that combines\nthe Pareto front method (PFM) with efficient manifold ranking (EMR). We show\nthat our proposed algorithm outperforms state of the art multiple-query\nretrieval algorithms on real-world image databases. We attribute this\nperformance improvement to concavity properties of the Pareto fronts, and prove\na theoretical result that characterizes the asymptotic concavity of the fronts. \n\n"}
{"id": "1402.5180", "contents": "Title: Guaranteed Non-Orthogonal Tensor Decomposition via Alternating Rank-$1$\n  Updates Abstract: In this paper, we provide local and global convergence guarantees for\nrecovering CP (Candecomp/Parafac) tensor decomposition. The main step of the\nproposed algorithm is a simple alternating rank-$1$ update which is the\nalternating version of the tensor power iteration adapted for asymmetric\ntensors. Local convergence guarantees are established for third order tensors\nof rank $k$ in $d$ dimensions, when $k=o \\bigl( d^{1.5} \\bigr)$ and the tensor\ncomponents are incoherent. Thus, we can recover overcomplete tensor\ndecomposition. We also strengthen the results to global convergence guarantees\nunder stricter rank condition $k \\le \\beta d$ (for arbitrary constant $\\beta >\n1$) through a simple initialization procedure where the algorithm is\ninitialized by top singular vectors of random tensor slices. Furthermore, the\napproximate local convergence guarantees for $p$-th order tensors are also\nprovided under rank condition $k=o \\bigl( d^{p/2} \\bigr)$. The guarantees also\ninclude tight perturbation analysis given noisy tensor. \n\n"}
{"id": "1403.2301", "contents": "Title: Phase Retrieval using Lipschitz Continuous Maps Abstract: In this note we prove that reconstruction from magnitudes of frame\ncoefficients (the so called \"phase retrieval problem\") can be performed using\nLipschitz continuous maps. Specifically we show that when the nonlinear\nanalysis map $\\alpha:{\\mathcal H}\\rightarrow\\mathbb{R}^m$ is injective, with\n$(\\alpha(x))_k=|<x,f_k>|^2$, where $\\{f_1,\\ldots,f_m\\}$ is a frame for the\nHilbert space ${\\mathcal H}$, then there exists a left inverse map\n$\\omega:\\mathbb{R}^m\\rightarrow {\\mathcal H}$ that is Lipschitz continuous.\nAdditionally we obtain the Lipschitz constant of this inverse map in terms of\nthe lower Lipschitz constant of $\\alpha$. Surprisingly the increase in\nLipschitz constant is independent of the space dimension or frame redundancy. \n\n"}
{"id": "1403.6530", "contents": "Title: Variance-Constrained Actor-Critic Algorithms for Discounted and Average\n  Reward MDPs Abstract: In many sequential decision-making problems we may want to manage risk by\nminimizing some measure of variability in rewards in addition to maximizing a\nstandard criterion. Variance related risk measures are among the most common\nrisk-sensitive criteria in finance and operations research. However, optimizing\nmany such criteria is known to be a hard problem. In this paper, we consider\nboth discounted and average reward Markov decision processes. For each\nformulation, we first define a measure of variability for a policy, which in\nturn gives us a set of risk-sensitive criteria to optimize. For each of these\ncriteria, we derive a formula for computing its gradient. We then devise\nactor-critic algorithms that operate on three timescales - a TD critic on the\nfastest timescale, a policy gradient (actor) on the intermediate timescale, and\na dual ascent for Lagrange multipliers on the slowest timescale. In the\ndiscounted setting, we point out the difficulty in estimating the gradient of\nthe variance of the return and incorporate simultaneous perturbation approaches\nto alleviate this. The average setting, on the other hand, allows for an actor\nupdate using compatible features to estimate the gradient of the variance. We\nestablish the convergence of our algorithms to locally risk-sensitive optimal\npolicies. Finally, we demonstrate the usefulness of our algorithms in a traffic\nsignal control application. \n\n"}
{"id": "1404.3840", "contents": "Title: Surpassing Human-Level Face Verification Performance on LFW with\n  GaussianFace Abstract: Face verification remains a challenging problem in very complex conditions\nwith large variations such as pose, illumination, expression, and occlusions.\nThis problem is exacerbated when we rely unrealistically on a single training\ndata source, which is often insufficient to cover the intrinsically complex\nface variations. This paper proposes a principled multi-task learning approach\nbased on Discriminative Gaussian Process Latent Variable Model, named\nGaussianFace, to enrich the diversity of training data. In comparison to\nexisting methods, our model exploits additional data from multiple\nsource-domains to improve the generalization performance of face verification\nin an unknown target-domain. Importantly, our model can adapt automatically to\ncomplex data distributions, and therefore can well capture complex face\nvariations inherent in multiple sources. Extensive experiments demonstrate the\neffectiveness of the proposed model in learning from diverse data sources and\ngeneralize to unseen domain. Specifically, the accuracy of our algorithm\nachieves an impressive accuracy rate of 98.52% on the well-known and\nchallenging Labeled Faces in the Wild (LFW) benchmark. For the first time, the\nhuman-level performance in face verification (97.53%) on LFW is surpassed. \n\n"}
{"id": "1405.2798", "contents": "Title: Two-Stage Metric Learning Abstract: In this paper, we present a novel two-stage metric learning algorithm. We\nfirst map each learning instance to a probability distribution by computing its\nsimilarities to a set of fixed anchor points. Then, we define the distance in\nthe input data space as the Fisher information distance on the associated\nstatistical manifold. This induces in the input data space a new family of\ndistance metric with unique properties. Unlike kernelized metric learning, we\ndo not require the similarity measure to be positive semi-definite. Moreover,\nit can also be interpreted as a local metric learning algorithm with well\ndefined distance approximation. We evaluate its performance on a number of\ndatasets. It outperforms significantly other metric learning methods and SVM. \n\n"}
{"id": "1405.3034", "contents": "Title: G-AMA: Sparse Gaussian graphical model estimation via alternating\n  minimization Abstract: Several methods have been recently proposed for estimating sparse Gaussian\ngraphical models using $\\ell_{1}$ regularization on the inverse covariance\nmatrix. Despite recent advances, contemporary applications require methods that\nare even faster in order to handle ill-conditioned high dimensional modern day\ndatasets. In this paper, we propose a new method, G-AMA, to solve the sparse\ninverse covariance estimation problem using Alternating Minimization Algorithm\n(AMA), that effectively works as a proximal gradient algorithm on the dual\nproblem. Our approach has several novel advantages over existing methods.\nFirst, we demonstrate that G-AMA is faster than the previous best algorithms by\nmany orders of magnitude and is thus an ideal approach for modern high\nthroughput applications. Second, global linear convergence of G-AMA is\ndemonstrated rigorously, underscoring its good theoretical properties. Third,\nthe dual algorithm operates on the covariance matrix, and thus easily\nfacilitates incorporating additional constraints on pairwise/marginal\nrelationships between feature pairs based on domain specific knowledge. Over\nand above estimating a sparse inverse covariance matrix, we also illustrate how\nto (1) incorporate constraints on the (bivariate) correlations and, (2)\nincorporate equality (equisparsity) or linear constraints between individual\ninverse covariance elements. Fourth, we also show that G-AMA is better adept at\nhandling extremely ill-conditioned problems, as is often the case with real\ndata. The methodology is demonstrated on both simulated and real datasets to\nillustrate its superior performance over recently proposed methods. \n\n"}
{"id": "1405.3726", "contents": "Title: Topic words analysis based on LDA model Abstract: Social network analysis (SNA), which is a research field describing and\nmodeling the social connection of a certain group of people, is popular among\nnetwork services. Our topic words analysis project is a SNA method to visualize\nthe topic words among emails from Obama.com to accounts registered in Columbus,\nOhio. Based on Latent Dirichlet Allocation (LDA) model, a popular topic model\nof SNA, our project characterizes the preference of senders for target group of\nreceptors. Gibbs sampling is used to estimate topic and word distribution. Our\ntraining and testing data are emails from the carbon-free server\nDatagreening.com. We use parallel computing tool BashReduce for word processing\nand generate related words under each latent topic to discovers typical\ninformation of political news sending specially to local Columbus receptors.\nRunning on two instances using paralleling tool BashReduce, our project\ncontributes almost 30% speedup processing the raw contents, comparing with\nprocessing contents on one instance locally. Also, the experimental result\nshows that the LDA model applied in our project provides precision rate 53.96%\nhigher than TF-IDF model finding target words, on the condition that\nappropriate size of topic words list is selected. \n\n"}
{"id": "1405.5239", "contents": "Title: Sequential Advantage Selection for Optimal Treatment Regimes Abstract: Variable selection for optimal treatment regime in a clinical trial or an\nobservational study is getting more attention. Most existing variable selection\ntechniques focused on selecting variables that are important for prediction,\ntherefore some variables that are poor in prediction but are critical for\ndecision-making may be ignored. A qualitative interaction of a variable with\ntreatment arises when treatment effect changes direction as the value of this\nvariable varies. The qualitative interaction indicates the importance of this\nvariable for decision-making. Gunter et al. (2011) proposed S-score which\ncharacterizes the magnitude of qualitative interaction of each variable with\ntreatment individually. In this article, we developed a sequential advantage\nselection method based on the modified S-score. Our method selects\nqualitatively interacted variables sequentially, and hence excludes marginally\nimportant but jointly unimportant variables {or vice versa}. The optimal\ntreatment regime based on variables selected via joint model is more\ncomprehensive and reliable. With the proposed stopping criteria, our method can\nhandle a large amount of covariates even if sample size is small. Simulation\nresults show our method performs well in practical settings. We further applied\nour method to data from a clinical trial for depression. \n\n"}
{"id": "1405.6159", "contents": "Title: A Bi-clustering Framework for Consensus Problems Abstract: We consider grouping as a general characterization for problems such as\nclustering, community detection in networks, and multiple parametric model\nestimation. We are interested in merging solutions from different grouping\nalgorithms, distilling all their good qualities into a consensus solution. In\nthis paper, we propose a bi-clustering framework and perspective for reaching\nconsensus in such grouping problems. In particular, this is the first time that\nthe task of finding/fitting multiple parametric models to a dataset is formally\nposed as a consensus problem. We highlight the equivalence of these tasks and\nestablish the connection with the computational Gestalt program, that seeks to\nprovide a psychologically-inspired detection theory for visual events. We also\npresent a simple but powerful bi-clustering algorithm, specially tuned to the\nnature of the problem we address, though general enough to handle many\ndifferent instances inscribed within our characterization. The presentation is\naccompanied with diverse and extensive experimental results in clustering,\ncommunity detection, and multiple parametric model estimation in image\nprocessing applications. \n\n"}
{"id": "1406.2206", "contents": "Title: Efficient Sparse Clustering of High-Dimensional Non-spherical Gaussian\n  Mixtures Abstract: We consider the problem of clustering data points in high dimensions, i.e.\nwhen the number of data points may be much smaller than the number of\ndimensions. Specifically, we consider a Gaussian mixture model (GMM) with\nnon-spherical Gaussian components, where the clusters are distinguished by only\na few relevant dimensions. The method we propose is a combination of a recent\napproach for learning parameters of a Gaussian mixture model and sparse linear\ndiscriminant analysis (LDA). In addition to cluster assignments, the method\nreturns an estimate of the set of features relevant for clustering. Our results\nindicate that the sample complexity of clustering depends on the sparsity of\nthe relevant feature set, while only scaling logarithmically with the ambient\ndimension. Additionally, we require much milder assumptions than existing work\non clustering in high dimensions. In particular, we do not require spherical\nclusters nor necessitate mean separation along relevant dimensions. \n\n"}
{"id": "1406.2969", "contents": "Title: Truncated Nuclear Norm Minimization for Image Restoration Based On\n  Iterative Support Detection Abstract: Recovering a large matrix from limited measurements is a challenging task\narising in many real applications, such as image inpainting, compressive\nsensing and medical imaging, and this kind of problems are mostly formulated as\nlow-rank matrix approximation problems. Due to the rank operator being\nnon-convex and discontinuous, most of the recent theoretical studies use the\nnuclear norm as a convex relaxation and the low-rank matrix recovery problem is\nsolved through minimization of the nuclear norm regularized problem. However, a\nmajor limitation of nuclear norm minimization is that all the singular values\nare simultaneously minimized and the rank may not be well approximated\n\\cite{hu2012fast}. Correspondingly, in this paper, we propose a new multi-stage\nalgorithm, which makes use of the concept of Truncated Nuclear Norm\nRegularization (TNNR) proposed in \\citep{hu2012fast} and Iterative Support\nDetection (ISD) proposed in \\citep{wang2010sparse} to overcome the above\nlimitation. Besides matrix completion problems considered in\n\\citep{hu2012fast}, the proposed method can be also extended to the general\nlow-rank matrix recovery problems. Extensive experiments well validate the\nsuperiority of our new algorithms over other state-of-the-art methods. \n\n"}
{"id": "1406.3332", "contents": "Title: Convolutional Kernel Networks Abstract: An important goal in visual recognition is to devise image representations\nthat are invariant to particular transformations. In this paper, we address\nthis goal with a new type of convolutional neural network (CNN) whose\ninvariance is encoded by a reproducing kernel. Unlike traditional approaches\nwhere neural networks are learned either to represent data or for solving a\nclassification task, our network learns to approximate the kernel feature map\non training data. Such an approach enjoys several benefits over classical ones.\nFirst, by teaching CNNs to be invariant, we obtain simple network architectures\nthat achieve a similar accuracy to more complex ones, while being easy to train\nand robust to overfitting. Second, we bridge a gap between the neural network\nliterature and kernels, which are natural tools to model invariance. We\nevaluate our methodology on visual recognition tasks where CNNs have proven to\nperform well, e.g., digit recognition with the MNIST dataset, and the more\nchallenging CIFAR-10 and STL-10 datasets, where our accuracy is competitive\nwith the state of the art. \n\n"}
{"id": "1406.4444", "contents": "Title: PRISM: Person Re-Identification via Structured Matching Abstract: Person re-identification (re-id), an emerging problem in visual surveillance,\ndeals with maintaining entities of individuals whilst they traverse various\nlocations surveilled by a camera network. From a visual perspective re-id is\nchallenging due to significant changes in visual appearance of individuals in\ncameras with different pose, illumination and calibration. Globally the\nchallenge arises from the need to maintain structurally consistent matches\namong all the individual entities across different camera views. We propose\nPRISM, a structured matching method to jointly account for these challenges. We\nview the global problem as a weighted graph matching problem and estimate edge\nweights by learning to predict them based on the co-occurrences of visual\npatterns in the training examples. These co-occurrence based scores in turn\naccount for appearance changes by inferring likely and unlikely visual\nco-occurrences appearing in training instances. We implement PRISM on single\nshot and multi-shot scenarios. PRISM uniformly outperforms state-of-the-art in\nterms of matching rate while being computationally efficient. \n\n"}
{"id": "1406.4566", "contents": "Title: Guaranteed Scalable Learning of Latent Tree Models Abstract: We present an integrated approach for structure and parameter estimation in\nlatent tree graphical models. Our overall approach follows a\n\"divide-and-conquer\" strategy that learns models over small groups of variables\nand iteratively merges onto a global solution. The structure learning involves\ncombinatorial operations such as minimum spanning tree construction and local\nrecursive grouping; the parameter learning is based on the method of moments\nand on tensor decompositions. Our method is guaranteed to correctly recover the\nunknown tree structure and the model parameters with low sample complexity for\nthe class of linear multivariate latent tree models which includes discrete and\nGaussian distributions, and Gaussian mixtures. Our bulk asynchronous parallel\nalgorithm is implemented in parallel and the parallel computation complexity\nincreases only logarithmically with the number of variables and linearly with\ndimensionality of each variable. \n\n"}
{"id": "1406.5311", "contents": "Title: Towards A Deeper Geometric, Analytic and Algorithmic Understanding of\n  Margins Abstract: Given a matrix $A$, a linear feasibility problem (of which linear\nclassification is a special case) aims to find a solution to a primal problem\n$w: A^Tw > \\textbf{0}$ or a certificate for the dual problem which is a\nprobability distribution $p: Ap = \\textbf{0}$. Inspired by the continued\nimportance of \"large-margin classifiers\" in machine learning, this paper\nstudies a condition measure of $A$ called its \\textit{margin} that determines\nthe difficulty of both the above problems. To aid geometrical intuition, we\nfirst establish new characterizations of the margin in terms of relevant balls,\ncones and hulls. Our second contribution is analytical, where we present\ngeneralizations of Gordan's theorem, and variants of Hoffman's theorems, both\nusing margins. We end by proving some new results on a classical iterative\nscheme, the Perceptron, whose convergence rates famously depends on the margin.\nOur results are relevant for a deeper understanding of margin-based learning\nand proving convergence rates of iterative schemes, apart from providing a\nunifying perspective on this vast topic. \n\n"}
{"id": "1406.5647", "contents": "Title: On semidefinite relaxations for the block model Abstract: The stochastic block model (SBM) is a popular tool for community detection in\nnetworks, but fitting it by maximum likelihood (MLE) involves a computationally\ninfeasible optimization problem. We propose a new semidefinite programming\n(SDP) solution to the problem of fitting the SBM, derived as a relaxation of\nthe MLE. We put ours and previously proposed SDPs in a unified framework, as\nrelaxations of the MLE over various sub-classes of the SBM, revealing a\nconnection to sparse PCA. Our main relaxation, which we call SDP-1, is tighter\nthan other recently proposed SDP relaxations, and thus previously established\ntheoretical guarantees carry over. However, we show that SDP-1 exactly recovers\ntrue communities over a wider class of SBMs than those covered by current\nresults. In particular, the assumption of strong assortativity of the SBM,\nimplicit in consistency conditions for previously proposed SDPs, can be relaxed\nto weak assortativity for our approach, thus significantly broadening the class\nof SBMs covered by the consistency results. We also show that strong\nassortativity is indeed a necessary condition for exact recovery for previously\nproposed SDP approaches and not an artifact of the proofs. Our analysis of SDPs\nis based on primal-dual witness constructions, which provides some insight into\nthe nature of the solutions of various SDPs. We show how to combine features\nfrom SDP-1 and already available SDPs to achieve the most flexibility in terms\nof both assortativity and block-size constraints, as our relaxation has the\ntendency to produce communities of similar sizes. This tendency makes it the\nideal tool for fitting network histograms, a method gaining popularity in the\ngraphon estimation literature, as we illustrate on an example of a social\nnetworks of dolphins. We also provide empirical evidence that SDPs outperform\nspectral methods for fitting SBMs with a large number of blocks. \n\n"}
{"id": "1407.0754", "contents": "Title: Structured Learning via Logistic Regression Abstract: A successful approach to structured learning is to write the learning\nobjective as a joint function of linear parameters and inference messages, and\niterate between updates to each. This paper observes that if the inference\nproblem is \"smoothed\" through the addition of entropy terms, for fixed\nmessages, the learning objective reduces to a traditional (non-structured)\nlogistic regression problem with respect to parameters. In these logistic\nregression problems, each training example has a bias term determined by the\ncurrent set of messages. Based on this insight, the structured energy function\ncan be extended from linear factors to any function class where an \"oracle\"\nexists to minimize a logistic loss. \n\n"}
{"id": "1408.3060", "contents": "Title: Fastfood: Approximate Kernel Expansions in Loglinear Time Abstract: Despite their successes, what makes kernel methods difficult to use in many\nlarge scale problems is the fact that storing and computing the decision\nfunction is typically expensive, especially at prediction time. In this paper,\nwe overcome this difficulty by proposing Fastfood, an approximation that\naccelerates such computation significantly. Key to Fastfood is the observation\nthat Hadamard matrices, when combined with diagonal Gaussian matrices, exhibit\nproperties similar to dense Gaussian random matrices. Yet unlike the latter,\nHadamard and diagonal matrices are inexpensive to multiply and store. These two\nmatrices can be used in lieu of Gaussian matrices in Random Kitchen Sinks\nproposed by Rahimi and Recht (2009) and thereby speeding up the computation for\na large range of kernel functions. Specifically, Fastfood requires O(n log d)\ntime and O(n) storage to compute n non-linear basis functions in d dimensions,\na significant improvement from O(nd) computation and storage, without\nsacrificing accuracy.\n  Our method applies to any translation invariant and any dot-product kernel,\nsuch as the popular RBF kernels and polynomial kernels. We prove that the\napproximation is unbiased and has low variance. Experiments show that we\nachieve similar accuracy to full kernel expansions and Random Kitchen Sinks\nwhile being 100x faster and using 1000x less memory. These improvements,\nespecially in terms of memory usage, make kernel methods more practical for\napplications that have large training sets and/or require real-time prediction. \n\n"}
{"id": "1408.3467", "contents": "Title: Evaluating Visual Properties via Robust HodgeRank Abstract: Nowadays, how to effectively evaluate visual properties has become a popular\ntopic for fine-grained visual comprehension. In this paper we study the problem\nof how to estimate such visual properties from a ranking perspective with the\nhelp of the annotators from online crowdsourcing platforms. The main challenges\nof our task are two-fold. On one hand, the annotations often contain\ncontaminated information, where a small fraction of label flips might ruin the\nglobal ranking of the whole dataset. On the other hand, considering the large\ndata capacity, the annotations are often far from being complete. What is\nworse, there might even exist imbalanced annotations where a small subset of\nsamples are frequently annotated. Facing such challenges, we propose a robust\nranking framework based on the principle of Hodge decomposition of imbalanced\nand incomplete ranking data. According to the HodgeRank theory, we find that\nthe major source of the contamination comes from the cyclic ranking component\nof the Hodge decomposition. This leads us to an outlier detection formulation\nas sparse approximations of the cyclic ranking projection. Taking a step\nfurther, it facilitates a novel outlier detection model as Huber's LASSO in\nrobust statistics. Moreover, simple yet scalable algorithms are developed based\non Linearized Bregman Iteration to achieve an even less biased estimator.\nStatistical consistency of outlier detection is established in both cases under\nnearly the same conditions. Our studies are supported by experiments with both\nsimulated examples and real-world data. The proposed framework provides us a\npromising tool for robust ranking with large scale crowdsourcing data arising\nfrom computer vision. \n\n"}
{"id": "1409.2620", "contents": "Title: Learning Machines Implemented on Non-Deterministic Hardware Abstract: This paper highlights new opportunities for designing large-scale machine\nlearning systems as a consequence of blurring traditional boundaries that have\nallowed algorithm designers and application-level practitioners to stay -- for\nthe most part -- oblivious to the details of the underlying hardware-level\nimplementations. The hardware/software co-design methodology advocated here\nhinges on the deployment of compute-intensive machine learning kernels onto\ncompute platforms that trade-off determinism in the computation for improvement\nin speed and/or energy efficiency. To achieve this, we revisit digital\nstochastic circuits for approximating matrix computations that are ubiquitous\nin machine learning algorithms. Theoretical and empirical evaluation is\nundertaken to assess the impact of the hardware-induced computational noise on\nalgorithm performance. As a proof-of-concept, a stochastic hardware simulator\nis employed for training deep neural networks for image recognition problems. \n\n"}
{"id": "1409.7074", "contents": "Title: Variational Pseudolikelihood for Regularized Ising Inference Abstract: I propose a variational approach to maximum pseudolikelihood inference of the\nIsing model. The variational algorithm is more computationally efficient, and\ndoes a better job predicting out-of-sample correlations than $L_2$ regularized\nmaximum pseudolikelihood inference as well as mean field and isolated spin pair\napproximations with pseudocount regularization. The key to the approach is a\nvariational energy that regularizes the inference problem by shrinking the\ncouplings towards zero, while still allowing some large couplings to explain\nstrong correlations. The utility of the variational pseudolikelihood approach\nis illustrated by training an Ising model to represent the letters A-J using\nsamples of letters from different computer fonts. \n\n"}
{"id": "1409.8327", "contents": "Title: Bayesian and regularization approaches to multivariable linear system\n  identification: the role of rank penalties Abstract: Recent developments in linear system identification have proposed the use of\nnon-parameteric methods, relying on regularization strategies, to handle the\nso-called bias/variance trade-off. This paper introduces an impulse response\nestimator which relies on an $\\ell_2$-type regularization including a\nrank-penalty derived using the log-det heuristic as a smooth approximation to\nthe rank function. This allows to account for different properties of the\nestimated impulse response (e.g. smoothness and stability) while also\npenalizing high-complexity models. This also allows to account and enforce\ncoupling between different input-output channels in MIMO systems. According to\nthe Bayesian paradigm, the parameters defining the relative weight of the two\nregularization terms as well as the structure of the rank penalty are estimated\noptimizing the marginal likelihood. Once these hyperameters have been\nestimated, the impulse response estimate is available in closed form.\nExperiments show that the proposed method is superior to the estimator relying\non the \"classic\" $\\ell_2$-regularization alone as well as those based in atomic\nand nuclear norm. \n\n"}
{"id": "1410.5392", "contents": "Title: Scalable Parallel Factorizations of SDD Matrices and Efficient Sampling\n  for Gaussian Graphical Models Abstract: Motivated by a sampling problem basic to computational statistical inference,\nwe develop a nearly optimal algorithm for a fundamental problem in spectral\ngraph theory and numerical analysis. Given an $n\\times n$ SDDM matrix ${\\bf\n\\mathbf{M}}$, and a constant $-1 \\leq p \\leq 1$, our algorithm gives efficient\naccess to a sparse $n\\times n$ linear operator $\\tilde{\\mathbf{C}}$ such that\n$${\\mathbf{M}}^{p} \\approx \\tilde{\\mathbf{C}} \\tilde{\\mathbf{C}}^\\top.$$ The\nsolution is based on factoring ${\\bf \\mathbf{M}}$ into a product of simple and\nsparse matrices using squaring and spectral sparsification. For ${\\mathbf{M}}$\nwith $m$ non-zero entries, our algorithm takes work nearly-linear in $m$, and\npolylogarithmic depth on a parallel machine with $m$ processors. This gives the\nfirst sampling algorithm that only requires nearly linear work and $n$ i.i.d.\nrandom univariate Gaussian samples to generate i.i.d. random samples for\n$n$-dimensional Gaussian random fields with SDDM precision matrices. For\nsampling this natural subclass of Gaussian random fields, it is optimal in the\nrandomness and nearly optimal in the work and parallel complexity. In addition,\nour sampling algorithm can be directly extended to Gaussian random fields with\nSDD precision matrices. \n\n"}
{"id": "1410.5884", "contents": "Title: Mean-Field Networks Abstract: The mean field algorithm is a widely used approximate inference algorithm for\ngraphical models whose exact inference is intractable. In each iteration of\nmean field, the approximate marginals for each variable are updated by getting\ninformation from the neighbors. This process can be equivalently converted into\na feedforward network, with each layer representing one iteration of mean field\nand with tied weights on all layers. This conversion enables a few natural\nextensions, e.g. untying the weights in the network. In this paper, we study\nthese mean field networks (MFNs), and use them as inference tools as well as\ndiscriminative models. Preliminary experiment results show that MFNs can learn\nto do inference very efficiently and perform significantly better than mean\nfield as discriminative models. \n\n"}
{"id": "1410.6264", "contents": "Title: Capturing spatial interdependence in image features: the counting grid,\n  an epitomic representation for bags of features Abstract: In recent scene recognition research images or large image regions are often\nrepresented as disorganized \"bags\" of features which can then be analyzed using\nmodels originally developed to capture co-variation of word counts in text.\nHowever, image feature counts are likely to be constrained in different ways\nthan word counts in text. For example, as a camera pans upwards from a building\nentrance over its first few floors and then further up into the sky Fig. 1,\nsome feature counts in the image drop while others rise -- only to drop again\ngiving way to features found more often at higher elevations. The space of all\npossible feature count combinations is constrained both by the properties of\nthe larger scene and the size and the location of the window into it. To\ncapture such variation, in this paper we propose the use of the counting grid\nmodel. This generative model is based on a grid of feature counts, considerably\nlarger than any of the modeled images, and considerably smaller than the real\nestate needed to tile the images next to each other tightly. Each modeled image\nis assumed to have a representative window in the grid in which the feature\ncounts mimic the feature distribution in the image. We provide a learning\nprocedure that jointly maps all images in the training set to the counting grid\nand estimates the appropriate local counts in it. Experimentally, we\ndemonstrate that the resulting representation captures the space of feature\ncount combinations more accurately than the traditional models, not only when\nthe input images come from a panning camera, but even when modeling images of\ndifferent scenes from the same category. \n\n"}
{"id": "1410.6382", "contents": "Title: Attribute Efficient Linear Regression with Data-Dependent Sampling Abstract: In this paper we analyze a budgeted learning setting, in which the learner\ncan only choose and observe a small subset of the attributes of each training\nexample. We develop efficient algorithms for ridge and lasso linear regression,\nwhich utilize the geometry of the data by a novel data-dependent sampling\nscheme. When the learner has prior knowledge on the second moments of the\nattributes, the optimal sampling probabilities can be calculated precisely, and\nresult in data-dependent improvements factors for the excess risk over the\nstate-of-the-art that may be as large as $O(\\sqrt{d})$, where $d$ is the\nproblem's dimension. Moreover, under reasonable assumptions our algorithms can\nuse less attributes than full-information algorithms, which is the main concern\nin budgeted learning settings. To the best of our knowledge, these are the\nfirst algorithms able to do so in our setting. Where no such prior knowledge is\navailable, we develop a simple estimation technique that given a sufficient\namount of training examples, achieves similar improvements. We complement our\ntheoretical analysis with experiments on several data sets which support our\nclaims. \n\n"}
{"id": "1411.0972", "contents": "Title: Convex Optimization for Big Data Abstract: This article reviews recent advances in convex optimization algorithms for\nBig Data, which aim to reduce the computational, storage, and communications\nbottlenecks. We provide an overview of this emerging field, describe\ncontemporary approximation techniques like first-order methods and\nrandomization for scalability, and survey the important role of parallel and\ndistributed computation. The new Big Data algorithms are based on surprisingly\nsimple principles and attain staggering accelerations even on classical\nproblems. \n\n"}
{"id": "1411.0997", "contents": "Title: Iterated geometric harmonics for data imputation and reconstruction of\n  missing data Abstract: The method of geometric harmonics is adapted to the situation of incomplete\ndata by means of the iterated geometric harmonics (IGH) scheme. The method is\ntested on natural and synthetic data sets with 50--500 data points and\ndimensionality of 400--10,000. Experiments suggest that the algorithm converges\nto a near optimal solution within 4--6 iterations, at runtimes of less than 30\nminutes on a medium-grade desktop computer. The imputation of missing data\nvalues is applied to collections of damaged images (suffering from data\nannihilation rates of up to 70\\%) which are reconstructed with a surprising\ndegree of accuracy. \n\n"}
{"id": "1411.1784", "contents": "Title: Conditional Generative Adversarial Nets Abstract: Generative Adversarial Nets [8] were recently introduced as a novel way to\ntrain generative models. In this work we introduce the conditional version of\ngenerative adversarial nets, which can be constructed by simply feeding the\ndata, y, we wish to condition on to both the generator and discriminator. We\nshow that this model can generate MNIST digits conditioned on class labels. We\nalso illustrate how this model could be used to learn a multi-modal model, and\nprovide preliminary examples of an application to image tagging in which we\ndemonstrate how this approach can generate descriptive tags which are not part\nof training labels. \n\n"}
{"id": "1411.4503", "contents": "Title: Outlier-Robust Convex Segmentation Abstract: We derive a convex optimization problem for the task of segmenting sequential\ndata, which explicitly treats presence of outliers. We describe two algorithms\nfor solving this problem, one exact and one a top-down novel approach, and we\nderive a consistency results for the case of two segments and no outliers.\nRobustness to outliers is evaluated on two real-world tasks related to speech\nsegmentation. Our algorithms outperform baseline segmentation algorithms. \n\n"}
{"id": "1411.5417", "contents": "Title: Private Empirical Risk Minimization Beyond the Worst Case: The Effect of\n  the Constraint Set Geometry Abstract: Empirical Risk Minimization (ERM) is a standard technique in machine\nlearning, where a model is selected by minimizing a loss function over\nconstraint set. When the training dataset consists of private information, it\nis natural to use a differentially private ERM algorithm, and this problem has\nbeen the subject of a long line of work started with Chaudhuri and Monteleoni\n2008. A private ERM algorithm outputs an approximate minimizer of the loss\nfunction and its error can be measured as the difference from the optimal value\nof the loss function. When the constraint set is arbitrary, the required error\nbounds are fairly well understood \\cite{BassilyST14}. In this work, we show\nthat the geometric properties of the constraint set can be used to derive\nsignificantly better results. Specifically, we show that a differentially\nprivate version of Mirror Descent leads to error bounds of the form\n$\\tilde{O}(G_{\\mathcal{C}}/n)$ for a lipschitz loss function, improving on the\n$\\tilde{O}(\\sqrt{p}/n)$ bounds in Bassily, Smith and Thakurta 2014. Here $p$ is\nthe dimensionality of the problem, $n$ is the number of data points in the\ntraining set, and $G_{\\mathcal{C}}$ denotes the Gaussian width of the\nconstraint set that we optimize over. We show similar improvements for strongly\nconvex functions, and for smooth functions. In addition, we show that when the\nloss function is Lipschitz with respect to the $\\ell_1$ norm and $\\mathcal{C}$\nis $\\ell_1$-bounded, a differentially private version of the Frank-Wolfe\nalgorithm gives error bounds of the form $\\tilde{O}(n^{-2/3})$. This captures\nthe important and common case of sparse linear regression (LASSO), when the\ndata $x_i$ satisfies $|x_i|_{\\infty} \\leq 1$ and we optimize over the $\\ell_1$\nball. We show new lower bounds for this setting, that together with known\nbounds, imply that all our upper bounds are tight. \n\n"}
{"id": "1411.7245", "contents": "Title: Heuristics for Exact Nonnegative Matrix Factorization Abstract: The exact nonnegative matrix factorization (exact NMF) problem is the\nfollowing: given an $m$-by-$n$ nonnegative matrix $X$ and a factorization rank\n$r$, find, if possible, an $m$-by-$r$ nonnegative matrix $W$ and an $r$-by-$n$\nnonnegative matrix $H$ such that $X = WH$. In this paper, we propose two\nheuristics for exact NMF, one inspired from simulated annealing and the other\nfrom the greedy randomized adaptive search procedure. We show that these two\nheuristics are able to compute exact nonnegative factorizations for several\nclasses of nonnegative matrices (namely, linear Euclidean distance matrices,\nslack matrices, unique-disjointness matrices, and randomly generated matrices)\nand as such demonstrate their superiority over standard multi-start strategies.\nWe also consider a hybridization between these two heuristics that allows us to\ncombine the advantages of both methods. Finally, we discuss the use of these\nheuristics to gain insight on the behavior of the nonnegative rank, i.e., the\nminimum factorization rank such that an exact NMF exists. In particular, we\ndisprove a conjecture on the nonnegative rank of a Kronecker product, propose a\nnew upper bound on the extension complexity of generic $n$-gons and conjecture\nthe exact value of (i) the extension complexity of regular $n$-gons and (ii)\nthe nonnegative rank of a submatrix of the slack matrix of the correlation\npolytope. \n\n"}
{"id": "1412.2432", "contents": "Title: MLitB: Machine Learning in the Browser Abstract: With few exceptions, the field of Machine Learning (ML) research has largely\nignored the browser as a computational engine. Beyond an educational resource\nfor ML, the browser has vast potential to not only improve the state-of-the-art\nin ML research, but also, inexpensively and on a massive scale, to bring\nsophisticated ML learning and prediction to the public at large. This paper\nintroduces MLitB, a prototype ML framework written entirely in JavaScript,\ncapable of performing large-scale distributed computing with heterogeneous\nclasses of devices. The development of MLitB has been driven by several\nunderlying objectives whose aim is to make ML learning and usage ubiquitous (by\nusing ubiquitous compute devices), cheap and effortlessly distributed, and\ncollaborative. This is achieved by allowing every internet capable device to\nrun training algorithms and predictive models with no software installation and\nby saving models in universally readable formats. Our prototype library is\ncapable of training deep neural networks with synchronized, distributed\nstochastic gradient descent. MLitB offers several important opportunities for\nnovel ML research, including: development of distributed learning algorithms,\nadvancement of web GPU algorithms, novel field and mobile applications, privacy\npreserving computing, and green grid-computing. MLitB is available as open\nsource software. \n\n"}
{"id": "1412.3474", "contents": "Title: Deep Domain Confusion: Maximizing for Domain Invariance Abstract: Recent reports suggest that a generic supervised deep CNN model trained on a\nlarge-scale dataset reduces, but does not remove, dataset bias on a standard\nbenchmark. Fine-tuning deep models in a new domain can require a significant\namount of data, which for many applications is simply not available. We propose\na new CNN architecture which introduces an adaptation layer and an additional\ndomain confusion loss, to learn a representation that is both semantically\nmeaningful and domain invariant. We additionally show that a domain confusion\nmetric can be used for model selection to determine the dimension of an\nadaptation layer and the best position for the layer in the CNN architecture.\nOur proposed adaptation method offers empirical performance which exceeds\npreviously published results on a standard benchmark visual domain adaptation\ntask. \n\n"}
{"id": "1412.4182", "contents": "Title: The Statistics of Streaming Sparse Regression Abstract: We present a sparse analogue to stochastic gradient descent that is\nguaranteed to perform well under similar conditions to the lasso. In the linear\nregression setup with irrepresentable noise features, our algorithm recovers\nthe support set of the optimal parameter vector with high probability, and\nachieves a statistically quasi-optimal rate of convergence of Op(k log(d)/T),\nwhere k is the sparsity of the solution, d is the number of features, and T is\nthe number of training examples. Meanwhile, our algorithm does not require any\nmore computational resources than stochastic gradient descent. In our\nexperiments, we find that our method substantially out-performs existing\nstreaming algorithms on both real and simulated data. \n\n"}
{"id": "1412.6544", "contents": "Title: Qualitatively characterizing neural network optimization problems Abstract: Training neural networks involves solving large-scale non-convex optimization\nproblems. This task has long been believed to be extremely difficult, with fear\nof local minima and other obstacles motivating a variety of schemes to improve\noptimization, such as unsupervised pretraining. However, modern neural networks\nare able to achieve negligible training error on complex tasks, using only\ndirect training with stochastic gradient descent. We introduce a simple\nanalysis technique to look for evidence that such networks are overcoming local\noptima. We find that, in fact, on a straight path from initialization to\nsolution, a variety of state of the art neural networks never encounter any\nsignificant obstacles. \n\n"}
{"id": "1412.6830", "contents": "Title: Learning Activation Functions to Improve Deep Neural Networks Abstract: Artificial neural networks typically have a fixed, non-linear activation\nfunction at each neuron. We have designed a novel form of piecewise linear\nactivation function that is learned independently for each neuron using\ngradient descent. With this adaptive activation function, we are able to\nimprove upon deep neural network architectures composed of static rectified\nlinear units, achieving state-of-the-art performance on CIFAR-10 (7.51%),\nCIFAR-100 (30.83%), and a benchmark from high-energy physics involving Higgs\nboson decay modes. \n\n"}
{"id": "1412.7210", "contents": "Title: Denoising autoencoder with modulated lateral connections learns\n  invariant representations of natural images Abstract: Suitable lateral connections between encoder and decoder are shown to allow\nhigher layers of a denoising autoencoder (dAE) to focus on invariant\nrepresentations. In regular autoencoders, detailed information needs to be\ncarried through the highest layers but lateral connections from encoder to\ndecoder relieve this pressure. It is shown that abstract invariant features can\nbe translated to detailed reconstructions when invariant features are allowed\nto modulate the strength of the lateral connection. Three dAE structures with\nmodulated and additive lateral connections, and without lateral connections\nwere compared in experiments using real-world images. The experiments verify\nthat adding modulated lateral connections to the model 1) improves the accuracy\nof the probability model for inputs, as measured by denoising performance; 2)\nresults in representations whose degree of invariance grows faster towards the\nhigher layers; and 3) supports the formation of diverse invariant poolings. \n\n"}
{"id": "1501.00287", "contents": "Title: Consistent Classification Algorithms for Multi-class Non-Decomposable\n  Performance Metrics Abstract: We study consistency of learning algorithms for a multi-class performance\nmetric that is a non-decomposable function of the confusion matrix of a\nclassifier and cannot be expressed as a sum of losses on individual data\npoints; examples of such performance metrics include the macro F-measure\npopular in information retrieval and the G-mean metric used in class-imbalanced\nproblems. While there has been much work in recent years in understanding the\nconsistency properties of learning algorithms for `binary' non-decomposable\nmetrics, little is known either about the form of the optimal classifier for a\ngeneral multi-class non-decomposable metric, or about how these learning\nalgorithms generalize to the multi-class case. In this paper, we provide a\nunified framework for analysing a multi-class non-decomposable performance\nmetric, where the problem of finding the optimal classifier for the performance\nmetric is viewed as an optimization problem over the space of all confusion\nmatrices achievable under the given distribution. Using this framework, we show\nthat (under a continuous distribution) the optimal classifier for a multi-class\nperformance metric can be obtained as the solution of a cost-sensitive\nclassification problem, thus generalizing several previous results on specific\nbinary non-decomposable metrics. We then design a consistent learning algorithm\nfor concave multi-class performance metrics that proceeds via a sequence of\ncost-sensitive classification problems, and can be seen as applying the\nconditional gradient (CG) optimization method over the space of feasible\nconfusion matrices. To our knowledge, this is the first efficient learning\nalgorithm (whose running time is polynomial in the number of classes) that is\nconsistent for a large family of multi-class non-decomposable metrics. Our\nconsistency proof uses a novel technique based on the convergence analysis of\nthe CG method. \n\n"}
{"id": "1501.00559", "contents": "Title: The Learnability of Unknown Quantum Measurements Abstract: Quantum machine learning has received significant attention in recent years,\nand promising progress has been made in the development of quantum algorithms\nto speed up traditional machine learning tasks. In this work, however, we focus\non investigating the information-theoretic upper bounds of sample complexity -\nhow many training samples are sufficient to predict the future behaviour of an\nunknown target function. This kind of problem is, arguably, one of the most\nfundamental problems in statistical learning theory and the bounds for\npractical settings can be completely characterised by a simple measure of\ncomplexity.\n  Our main result in the paper is that, for learning an unknown quantum\nmeasurement, the upper bound, given by the fat-shattering dimension, is\nlinearly proportional to the dimension of the underlying Hilbert space.\nLearning an unknown quantum state becomes a dual problem to ours, and as a\nbyproduct, we can recover Aaronson's famous result [Proc. R. Soc. A\n463:3089-3144 (2007)] solely using a classical machine learning technique. In\naddition, other famous complexity measures like covering numbers and Rademacher\ncomplexities are derived explicitly. We are able to connect measures of sample\ncomplexity with various areas in quantum information science, e.g. quantum\nstate/measurement tomography, quantum state discrimination and quantum random\naccess codes, which may be of independent interest. Lastly, with the assistance\nof general Bloch-sphere representation, we show that learning quantum\nmeasurements/states can be mathematically formulated as a neural network.\nConsequently, classical ML algorithms can be applied to efficiently accomplish\nthe two quantum learning tasks. \n\n"}
{"id": "1501.04467", "contents": "Title: Implementable confidence sets in high dimensional regression Abstract: We consider the setting of linear regression in high dimension. We focus on\nthe problem of constructing adaptive and honest confidence sets for the sparse\nparameter \\theta, i.e. we want to construct a confidence set for theta that\ncontains theta with high probability, and that is as small as possible. The l_2\ndiameter of a such confidence set should depend on the sparsity S of \\theta -\nthe larger S, the wider the confidence set. However, in practice, S is unknown.\nThis paper focuses on constructing a confidence set for \\theta which contains\n\\theta with high probability, whose diameter is adaptive to the unknown\nsparsity S, and which is implementable in practice. \n\n"}
{"id": "1501.06241", "contents": "Title: Sequential Sensing with Model Mismatch Abstract: We characterize the performance of sequential information guided sensing,\nInfo-Greedy Sensing, when there is a mismatch between the true signal model and\nthe assumed model, which may be a sample estimate. In particular, we consider a\nsetup where the signal is low-rank Gaussian and the measurements are taken in\nthe directions of eigenvectors of the covariance matrix in a decreasing order\nof eigenvalues. We establish a set of performance bounds when a mismatched\ncovariance matrix is used, in terms of the gap of signal posterior entropy, as\nwell as the additional amount of power required to achieve the same signal\nrecovery precision. Based on this, we further study how to choose an\ninitialization for Info-Greedy Sensing using the sample covariance matrix, or\nusing an efficient covariance sketching scheme. \n\n"}
{"id": "1502.08009", "contents": "Title: Second-order Quantile Methods for Experts and Combinatorial Games Abstract: We aim to design strategies for sequential decision making that adjust to the\ndifficulty of the learning problem. We study this question both in the setting\nof prediction with expert advice, and for more general combinatorial decision\ntasks. We are not satisfied with just guaranteeing minimax regret rates, but we\nwant our algorithms to perform significantly better on easy data. Two popular\nways to formalize such adaptivity are second-order regret bounds and quantile\nbounds. The underlying notions of 'easy data', which may be paraphrased as \"the\nlearning problem has small variance\" and \"multiple decisions are useful\", are\nsynergetic. But even though there are sophisticated algorithms that exploit one\nof the two, no existing algorithm is able to adapt to both.\n  In this paper we outline a new method for obtaining such adaptive algorithms,\nbased on a potential function that aggregates a range of learning rates (which\nare essential tuning parameters). By choosing the right prior we construct\nefficient algorithms and show that they reap both benefits by proving the first\nbounds that are both second-order and incorporate quantiles. \n\n"}
{"id": "1503.00623", "contents": "Title: Unregularized Online Learning Algorithms with General Loss Functions Abstract: In this paper, we consider unregularized online learning algorithms in a\nReproducing Kernel Hilbert Spaces (RKHS). Firstly, we derive explicit\nconvergence rates of the unregularized online learning algorithms for\nclassification associated with a general gamma-activating loss (see Definition\n1 in the paper). Our results extend and refine the results in Ying and Pontil\n(2008) for the least-square loss and the recent result in Bach and Moulines\n(2011) for the loss function with a Lipschitz-continuous gradient. Moreover, we\nestablish a very general condition on the step sizes which guarantees the\nconvergence of the last iterate of such algorithms. Secondly, we establish, for\nthe first time, the convergence of the unregularized pairwise learning\nalgorithm with a general loss function and derive explicit rates under the\nassumption of polynomially decaying step sizes. Concrete examples are used to\nillustrate our main results. The main techniques are tools from convex\nanalysis, refined inequalities of Gaussian averages, and an induction approach. \n\n"}
{"id": "1503.00778", "contents": "Title: Simple, Efficient, and Neural Algorithms for Sparse Coding Abstract: Sparse coding is a basic task in many fields including signal processing,\nneuroscience and machine learning where the goal is to learn a basis that\nenables a sparse representation of a given set of data, if one exists. Its\nstandard formulation is as a non-convex optimization problem which is solved in\npractice by heuristics based on alternating minimization. Re- cent work has\nresulted in several algorithms for sparse coding with provable guarantees, but\nsomewhat surprisingly these are outperformed by the simple alternating\nminimization heuristics. Here we give a general framework for understanding\nalternating minimization which we leverage to analyze existing heuristics and\nto design new ones also with provable guarantees. Some of these algorithms seem\nimplementable on simple neural architectures, which was the original motivation\nof Olshausen and Field (1997a) in introducing sparse coding. We also give the\nfirst efficient algorithm for sparse coding that works almost up to the\ninformation theoretic limit for sparse recovery on incoherent dictionaries. All\nprevious algorithms that approached or surpassed this limit run in time\nexponential in some natural parameter. Finally, our algorithms improve upon the\nsample complexity of existing approaches. We believe that our analysis\nframework will have applications in other settings where simple iterative\nalgorithms are used. \n\n"}
{"id": "1503.03613", "contents": "Title: On the Impossibility of Learning the Missing Mass Abstract: This paper shows that one cannot learn the probability of rare events without\nimposing further structural assumptions. The event of interest is that of\nobtaining an outcome outside the coverage of an i.i.d. sample from a discrete\ndistribution. The probability of this event is referred to as the \"missing\nmass\". The impossibility result can then be stated as: the missing mass is not\ndistribution-free PAC-learnable in relative error. The proof is\nsemi-constructive and relies on a coupling argument using a dithered geometric\ndistribution. This result formalizes the folklore that in order to predict rare\nevents, one necessarily needs distributions with \"heavy tails\". \n\n"}
{"id": "1503.08542", "contents": "Title: Nonparametric Relational Topic Models through Dependent Gamma Processes Abstract: Traditional Relational Topic Models provide a way to discover the hidden\ntopics from a document network. Many theoretical and practical tasks, such as\ndimensional reduction, document clustering, link prediction, benefit from this\nrevealed knowledge. However, existing relational topic models are based on an\nassumption that the number of hidden topics is known in advance, and this is\nimpractical in many real-world applications. Therefore, in order to relax this\nassumption, we propose a nonparametric relational topic model in this paper.\nInstead of using fixed-dimensional probability distributions in its generative\nmodel, we use stochastic processes. Specifically, a gamma process is assigned\nto each document, which represents the topic interest of this document.\nAlthough this method provides an elegant solution, it brings additional\nchallenges when mathematically modeling the inherent network structure of\ntypical document network, i.e., two spatially closer documents tend to have\nmore similar topics. Furthermore, we require that the topics are shared by all\nthe documents. In order to resolve these challenges, we use a subsampling\nstrategy to assign each document a different gamma process from the global\ngamma process, and the subsampling probabilities of documents are assigned with\na Markov Random Field constraint that inherits the document network structure.\nThrough the designed posterior inference algorithm, we can discover the hidden\ntopics and its number simultaneously. Experimental results on both synthetic\nand real-world network datasets demonstrate the capabilities of learning the\nhidden topics and, more importantly, the number of topics. \n\n"}
{"id": "1503.08855", "contents": "Title: Decentralized learning for wireless communications and networking Abstract: This chapter deals with decentralized learning algorithms for in-network\nprocessing of graph-valued data. A generic learning problem is formulated and\nrecast into a separable form, which is iteratively minimized using the\nalternating-direction method of multipliers (ADMM) so as to gain the desired\ndegree of parallelization. Without exchanging elements from the distributed\ntraining sets and keeping inter-node communications at affordable levels, the\nlocal (per-node) learners consent to the desired quantity inferred globally,\nmeaning the one obtained if the entire training data set were centrally\navailable. Impact of the decentralized learning framework to contemporary\nwireless communications and networking tasks is illustrated through case\nstudies including target tracking using wireless sensor networks, unveiling\nInternet traffic anomalies, power system state estimation, as well as spectrum\ncartography for wireless cognitive radio networks. \n\n"}
{"id": "1504.01515", "contents": "Title: Simultaneously sparse and low-rank abundance matrix estimation for\n  hyperspectral image unmixing Abstract: In a plethora of applications dealing with inverse problems, e.g. in image\nprocessing, social networks, compressive sensing, biological data processing\netc., the signal of interest is known to be structured in several ways at the\nsame time. This premise has recently guided the research to the innovative and\nmeaningful idea of imposing multiple constraints on the parameters involved in\nthe problem under study. For instance, when dealing with problems whose\nparameters form sparse and low-rank matrices, the adoption of suitably combined\nconstraints imposing sparsity and low-rankness, is expected to yield\nsubstantially enhanced estimation results. In this paper, we address the\nspectral unmixing problem in hyperspectral images. Specifically, two novel\nunmixing algorithms are introduced, in an attempt to exploit both spatial\ncorrelation and sparse representation of pixels lying in homogeneous regions of\nhyperspectral images. To this end, a novel convex mixed penalty term is first\ndefined consisting of the sum of the weighted $\\ell_1$ and the weighted nuclear\nnorm of the abundance matrix corresponding to a small area of the image\ndetermined by a sliding square window. This penalty term is then used to\nregularize a conventional quadratic cost function and impose simultaneously\nsparsity and row-rankness on the abundance matrix. The resulting regularized\ncost function is minimized by a) an incremental proximal sparse and low-rank\nunmixing algorithm and b) an algorithm based on the alternating minimization\nmethod of multipliers (ADMM). The effectiveness of the proposed algorithms is\nillustrated in experiments conducted both on simulated and real data. \n\n"}
{"id": "1504.01823", "contents": "Title: Structured Matrix Completion with Applications to Genomic Data\n  Integration Abstract: Matrix completion has attracted significant recent attention in many fields\nincluding statistics, applied mathematics and electrical engineering. Current\nliterature on matrix completion focuses primarily on independent sampling\nmodels under which the individual observed entries are sampled independently.\nMotivated by applications in genomic data integration, we propose a new\nframework of structured matrix completion (SMC) to treat structured missingness\nby design. Specifically, our proposed method aims at efficient matrix recovery\nwhen a subset of the rows and columns of an approximately low-rank matrix are\nobserved. We provide theoretical justification for the proposed SMC method and\nderive lower bound for the estimation errors, which together establish the\noptimal rate of recovery over certain classes of approximately low-rank\nmatrices. Simulation studies show that the method performs well in finite\nsample under a variety of configurations. The method is applied to integrate\nseveral ovarian cancer genomic studies with different extent of genomic\nmeasurements, which enables us to construct more accurate prediction rules for\novarian cancer survival. \n\n"}
{"id": "1504.05006", "contents": "Title: Partition MCMC for inference on acyclic digraphs Abstract: Acyclic digraphs are the underlying representation of Bayesian networks, a\nwidely used class of probabilistic graphical models. Learning the underlying\ngraph from data is a way of gaining insights about the structural properties of\na domain. Structure learning forms one of the inference challenges of\nstatistical graphical models.\n  MCMC methods, notably structure MCMC, to sample graphs from the posterior\ndistribution given the data are probably the only viable option for Bayesian\nmodel averaging. Score modularity and restrictions on the number of parents of\neach node allow the graphs to be grouped into larger collections, which can be\nscored as a whole to improve the chain's convergence. Current examples of\nalgorithms taking advantage of grouping are the biased order MCMC, which acts\non the alternative space of permuted triangular matrices, and non ergodic edge\nreversal moves.\n  Here we propose a novel algorithm, which employs the underlying combinatorial\nstructure of DAGs to define a new grouping. As a result convergence is improved\ncompared to structure MCMC, while still retaining the property of producing an\nunbiased sample. Finally the method can be combined with edge reversal moves to\nimprove the sampler further. \n\n"}
{"id": "1504.07575", "contents": "Title: Becoming the Expert - Interactive Multi-Class Machine Teaching Abstract: Compared to machines, humans are extremely good at classifying images into\ncategories, especially when they possess prior knowledge of the categories at\nhand. If this prior information is not available, supervision in the form of\nteaching images is required. To learn categories more quickly, people should\nsee important and representative images first, followed by less important\nimages later - or not at all. However, image-importance is individual-specific,\ni.e. a teaching image is important to a student if it changes their overall\nability to discriminate between classes. Further, students keep learning, so\nwhile image-importance depends on their current knowledge, it also varies with\ntime.\n  In this work we propose an Interactive Machine Teaching algorithm that\nenables a computer to teach challenging visual concepts to a human. Our\nadaptive algorithm chooses, online, which labeled images from a teaching set\nshould be shown to the student as they learn. We show that a teaching strategy\nthat probabilistically models the student's ability and progress, based on\ntheir correct and incorrect answers, produces better 'experts'. We present\nresults using real human participants across several varied and challenging\nreal-world datasets. \n\n"}
{"id": "1504.08215", "contents": "Title: Lateral Connections in Denoising Autoencoders Support Supervised\n  Learning Abstract: We show how a deep denoising autoencoder with lateral connections can be used\nas an auxiliary unsupervised learning task to support supervised learning. The\nproposed model is trained to minimize simultaneously the sum of supervised and\nunsupervised cost functions by back-propagation, avoiding the need for\nlayer-wise pretraining. It improves the state of the art significantly in the\npermutation-invariant MNIST classification task. \n\n"}
{"id": "1504.08291", "contents": "Title: Deep Neural Networks with Random Gaussian Weights: A Universal\n  Classification Strategy? Abstract: Three important properties of a classification machinery are: (i) the system\npreserves the core information of the input data; (ii) the training examples\nconvey information about unseen data; and (iii) the system is able to treat\ndifferently points from different classes. In this work we show that these\nfundamental properties are satisfied by the architecture of deep neural\nnetworks. We formally prove that these networks with random Gaussian weights\nperform a distance-preserving embedding of the data, with a special treatment\nfor in-class and out-of-class data. Similar points at the input of the network\nare likely to have a similar output. The theoretical analysis of deep networks\nhere presented exploits tools used in the compressed sensing and dictionary\nlearning literature, thereby making a formal connection between these important\ntopics. The derived results allow drawing conclusions on the metric learning\nproperties of the network and their relation to its structure, as well as\nproviding bounds on the required size of the training set such that the\ntraining examples would represent faithfully the unseen data. The results are\nvalidated with state-of-the-art trained networks. \n\n"}
{"id": "1505.00482", "contents": "Title: Risk Bounds For Mode Clustering Abstract: Density mode clustering is a nonparametric clustering method. The clusters\nare the basins of attraction of the modes of a density estimator. We study the\nrisk of mode-based clustering. We show that the clustering risk over the\ncluster cores --- the regions where the density is high --- is very small even\nin high dimensions. And under a low noise condition, the overall cluster risk\nis small even beyond the cores, in high dimensions. \n\n"}
{"id": "1505.01462", "contents": "Title: Estimation from Pairwise Comparisons: Sharp Minimax Bounds with Topology\n  Dependence Abstract: Data in the form of pairwise comparisons arises in many domains, including\npreference elicitation, sporting competitions, and peer grading among others.\nWe consider parametric ordinal models for such pairwise comparison data\ninvolving a latent vector $w^* \\in \\mathbb{R}^d$ that represents the\n\"qualities\" of the $d$ items being compared; this class of models includes the\ntwo most widely used parametric models--the Bradley-Terry-Luce (BTL) and the\nThurstone models. Working within a standard minimax framework, we provide tight\nupper and lower bounds on the optimal error in estimating the quality score\nvector $w^*$ under this class of models. The bounds depend on the topology of\nthe comparison graph induced by the subset of pairs being compared via its\nLaplacian spectrum. Thus, in settings where the subset of pairs may be chosen,\nour results provide principled guidelines for making this choice. Finally, we\ncompare these error rates to those under cardinal measurement models and show\nthat the error rates in the ordinal and cardinal settings have identical\nscalings apart from constant pre-factors. \n\n"}
{"id": "1505.02250", "contents": "Title: Newton Sketch: A Linear-time Optimization Algorithm with\n  Linear-Quadratic Convergence Abstract: We propose a randomized second-order method for optimization known as the\nNewton Sketch: it is based on performing an approximate Newton step using a\nrandomly projected or sub-sampled Hessian. For self-concordant functions, we\nprove that the algorithm has super-linear convergence with exponentially high\nprobability, with convergence and complexity guarantees that are independent of\ncondition numbers and related problem-dependent quantities. Given a suitable\ninitialization, similar guarantees also hold for strongly convex and smooth\nobjectives without self-concordance. When implemented using randomized\nprojections based on a sub-sampled Hadamard basis, the algorithm typically has\nsubstantially lower complexity than Newton's method. We also describe\nextensions of our methods to programs involving convex constraints that are\nequipped with self-concordant barriers. We discuss and illustrate applications\nto linear programs, quadratic programs with convex constraints, logistic\nregression and other generalized linear models, as well as semidefinite\nprograms. \n\n"}
{"id": "1505.02462", "contents": "Title: Soft-Deep Boltzmann Machines Abstract: We present a layered Boltzmann machine (BM) that can better exploit the\nadvantages of a distributed representation. It is widely believed that deep BMs\n(DBMs) have far greater representational power than its shallow counterpart,\nrestricted Boltzmann machines (RBMs). However, this expectation on the\nsupremacy of DBMs over RBMs has not ever been validated in a theoretical\nfashion. In this paper, we provide both theoretical and empirical evidences\nthat the representational power of DBMs can be actually rather limited in\ntaking advantages of distributed representations. We propose an approximate\nmeasure for the representational power of a BM regarding to the efficiency of a\ndistributed representation. With this measure, we show a surprising fact that\nDBMs can make inefficient use of distributed representations. Based on these\nobservations, we propose an alternative BM architecture, which we dub soft-deep\nBMs (sDBMs). We show that sDBMs can more efficiently exploit the distributed\nrepresentations in terms of the measure. Experiments demonstrate that sDBMs\noutperform several state-of-the-art models, including DBMs, in generative tasks\non binarized MNIST and Caltech-101 silhouettes. \n\n"}
{"id": "1505.04732", "contents": "Title: Layered Adaptive Importance Sampling Abstract: Monte Carlo methods represent the \"de facto\" standard for approximating\ncomplicated integrals involving multidimensional target distributions. In order\nto generate random realizations from the target distribution, Monte Carlo\ntechniques use simpler proposal probability densities to draw candidate\nsamples. The performance of any such method is strictly related to the\nspecification of the proposal distribution, such that unfortunate choices\neasily wreak havoc on the resulting estimators. In this work, we introduce a\nlayered (i.e., hierarchical) procedure to generate samples employed within a\nMonte Carlo scheme. This approach ensures that an appropriate equivalent\nproposal density is always obtained automatically (thus eliminating the risk of\na catastrophic performance), although at the expense of a moderate increase in\nthe complexity. Furthermore, we provide a general unified importance sampling\n(IS) framework, where multiple proposal densities are employed and several IS\nschemes are introduced by applying the so-called deterministic mixture\napproach. Finally, given these schemes, we also propose a novel class of\nadaptive importance samplers using a population of proposals, where the\nadaptation is driven by independent parallel or interacting Markov Chain Monte\nCarlo (MCMC) chains. The resulting algorithms efficiently combine the benefits\nof both IS and MCMC methods. \n\n"}
{"id": "1506.01782", "contents": "Title: High-dimensional Ordinary Least-squares Projection for Screening\n  Variables Abstract: Variable selection is a challenging issue in statistical applications when\nthe number of predictors $p$ far exceeds the number of observations $n$. In\nthis ultra-high dimensional setting, the sure independence screening (SIS)\nprocedure was introduced to significantly reduce the dimensionality by\npreserving the true model with overwhelming probability, before a refined\nsecond stage analysis. However, the aforementioned sure screening property\nstrongly relies on the assumption that the important variables in the model\nhave large marginal correlations with the response, which rarely holds in\nreality. To overcome this, we propose a novel and simple screening technique\ncalled the high-dimensional ordinary least-squares projection (HOLP). We show\nthat HOLP possesses the sure screening property and gives consistent variable\nselection without the strong correlation assumption, and has a low\ncomputational complexity. A ridge type HOLP procedure is also discussed.\nSimulation study shows that HOLP performs competitively compared to many other\nmarginal correlation based methods. An application to a mammalian eye disease\ndata illustrates the attractiveness of HOLP. \n\n"}
{"id": "1506.02078", "contents": "Title: Visualizing and Understanding Recurrent Networks Abstract: Recurrent Neural Networks (RNNs), and specifically a variant with Long\nShort-Term Memory (LSTM), are enjoying renewed interest as a result of\nsuccessful applications in a wide range of machine learning problems that\ninvolve sequential data. However, while LSTMs provide exceptional results in\npractice, the source of their performance and their limitations remain rather\npoorly understood. Using character-level language models as an interpretable\ntestbed, we aim to bridge this gap by providing an analysis of their\nrepresentations, predictions and error types. In particular, our experiments\nreveal the existence of interpretable cells that keep track of long-range\ndependencies such as line lengths, quotes and brackets. Moreover, our\ncomparative analysis with finite horizon n-gram models traces the source of the\nLSTM improvements to long-range structural dependencies. Finally, we provide\nanalysis of the remaining errors and suggests areas for further study. \n\n"}
{"id": "1506.02108", "contents": "Title: Deeply Learning the Messages in Message Passing Inference Abstract: Deep structured output learning shows great promise in tasks like semantic\nimage segmentation. We proffer a new, efficient deep structured model learning\nscheme, in which we show how deep Convolutional Neural Networks (CNNs) can be\nused to estimate the messages in message passing inference for structured\nprediction with Conditional Random Fields (CRFs). With such CNN message\nestimators, we obviate the need to learn or evaluate potential functions for\nmessage calculation. This confers significant efficiency for learning, since\notherwise when performing structured learning for a CRF with CNN potentials it\nis necessary to undertake expensive inference for every stochastic gradient\niteration. The network output dimension for message estimation is the same as\nthe number of classes, in contrast to the network output for general CNN\npotential functions in CRFs, which is exponential in the order of the\npotentials. Hence CNN message learning has fewer network parameters and is more\nscalable for cases that a large number of classes are involved. We apply our\nmethod to semantic image segmentation on the PASCAL VOC 2012 dataset. We\nachieve an intersection-over-union score of 73.4 on its test set, which is the\nbest reported result for methods using the VOC training images alone. This\nimpressive performance demonstrates the effectiveness and usefulness of our CNN\nmessage learning method. \n\n"}
{"id": "1506.02785", "contents": "Title: On the Error of Random Fourier Features Abstract: Kernel methods give powerful, flexible, and theoretically grounded approaches\nto solving many problems in machine learning. The standard approach, however,\nrequires pairwise evaluations of a kernel function, which can lead to\nscalability issues for very large datasets. Rahimi and Recht (2007) suggested a\npopular approach to handling this problem, known as random Fourier features.\nThe quality of this approximation, however, is not well understood. We improve\nthe uniform error bound of that paper, as well as giving novel understandings\nof the embedding's variance, approximation error, and use in some machine\nlearning methods. We also point out that surprisingly, of the two main variants\nof those features, the more widely used is strictly higher-variance for the\nGaussian kernel and has worse bounds. \n\n"}
{"id": "1506.06272", "contents": "Title: Aligning where to see and what to tell: image caption with region-based\n  attention and scene factorization Abstract: Recent progress on automatic generation of image captions has shown that it\nis possible to describe the most salient information conveyed by images with\naccurate and meaningful sentences. In this paper, we propose an image caption\nsystem that exploits the parallel structures between images and sentences. In\nour model, the process of generating the next word, given the previously\ngenerated ones, is aligned with the visual perception experience where the\nattention shifting among the visual regions imposes a thread of visual\nordering. This alignment characterizes the flow of \"abstract meaning\", encoding\nwhat is semantically shared by both the visual scene and the text description.\nOur system also makes another novel modeling contribution by introducing\nscene-specific contexts that capture higher-level semantic information encoded\nin an image. The contexts adapt language models for word generation to specific\nscene types. We benchmark our system and contrast to published results on\nseveral popular datasets. We show that using either region-based attention or\nscene-specific contexts improves systems without those components. Furthermore,\ncombining these two modeling ingredients attains the state-of-the-art\nperformance. \n\n"}
{"id": "1506.07540", "contents": "Title: Global Optimality in Tensor Factorization, Deep Learning, and Beyond Abstract: Techniques involving factorization are found in a wide range of applications\nand have enjoyed significant empirical success in many fields. However, common\nto a vast majority of these problems is the significant disadvantage that the\nassociated optimization problems are typically non-convex due to a multilinear\nform or other convexity destroying transformation. Here we build on ideas from\nconvex relaxations of matrix factorizations and present a very general\nframework which allows for the analysis of a wide range of non-convex\nfactorization problems - including matrix factorization, tensor factorization,\nand deep neural network training formulations. We derive sufficient conditions\nto guarantee that a local minimum of the non-convex optimization problem is a\nglobal minimum and show that if the size of the factorized variables is large\nenough then from any initialization it is possible to find a global minimizer\nusing a purely local descent algorithm. Our framework also provides a partial\ntheoretical justification for the increasingly common use of Rectified Linear\nUnits (ReLUs) in deep neural networks and offers guidance on deep network\narchitectures and regularization strategies to facilitate efficient\noptimization. \n\n"}
{"id": "1506.07868", "contents": "Title: The local convexity of solving systems of quadratic equations Abstract: This paper considers the recovery of a rank $r$ positive semidefinite matrix\n$X X^T\\in\\mathbb{R}^{n\\times n}$ from $m$ scalar measurements of the form $y_i\n:= a_i^T X X^T a_i$ (i.e., quadratic measurements of $X$). Such problems arise\nin a variety of applications, including covariance sketching of\nhigh-dimensional data streams, quadratic regression, quantum state tomography,\namong others. A natural approach to this problem is to minimize the loss\nfunction $f(U) = \\sum_i (y_i - a_i^TUU^Ta_i)^2$ which has an entire manifold of\nsolutions given by $\\{XO\\}_{O\\in\\mathcal{O}_r}$ where $\\mathcal{O}_r$ is the\northogonal group of $r\\times r$ orthogonal matrices; this is {\\it non-convex}\nin the $n\\times r$ matrix $U$, but methods like gradient descent are simple and\neasy to implement (as compared to semidefinite relaxation approaches).\n  In this paper we show that once we have $m \\geq C nr \\log^2(n)$ samples from\nisotropic gaussian $a_i$, with high probability {\\em (a)} this function admits\na dimension-independent region of {\\em local strong convexity} on lines\nperpendicular to the solution manifold, and {\\em (b)} with an additional\npolynomial factor of $r$ samples, a simple spectral initialization will land\nwithin the region of convexity with high probability. Together, this implies\nthat gradient descent with initialization (but no re-sampling) will converge\nlinearly to the correct $X$, up to an orthogonal transformation. We believe\nthat this general technique (local convexity reachable by spectral\ninitialization) should prove applicable to a broader class of nonconvex\noptimization problems. \n\n"}
{"id": "1507.00052", "contents": "Title: Gaussian Process for Noisy Inputs with Ordering Constraints Abstract: We study the Gaussian Process regression model in the context of training\ndata with noise in both input and output. The presence of two sources of noise\nmakes the task of learning accurate predictive models extremely challenging.\nHowever, in some instances additional constraints may be available that can\nreduce the uncertainty in the resulting predictive models. In particular, we\nconsider the case of monotonically ordered latent input, which occurs in many\napplication domains that deal with temporal data. We present a novel inference\nand learning approach based on non-parametric Gaussian variational\napproximation to learn the GP model while taking into account the new\nconstraints. The resulting strategy allows one to gain access to posterior\nestimates of both the input and the output and results in improved predictive\nperformance. We compare our proposed models to state-of-the-art Noisy Input\nGaussian Process (NIGP) and other competing approaches on synthetic and real\nsea-level rise data. Experimental results suggest that the proposed approach\nconsistently outperforms selected methods while, at the same time, reducing the\ncomputational costs of learning and inference. \n\n"}
{"id": "1507.00814", "contents": "Title: Incentivizing Exploration In Reinforcement Learning With Deep Predictive\n  Models Abstract: Achieving efficient and scalable exploration in complex domains poses a major\nchallenge in reinforcement learning. While Bayesian and PAC-MDP approaches to\nthe exploration problem offer strong formal guarantees, they are often\nimpractical in higher dimensions due to their reliance on enumerating the\nstate-action space. Hence, exploration in complex domains is often performed\nwith simple epsilon-greedy methods. In this paper, we consider the challenging\nAtari games domain, which requires processing raw pixel inputs and delayed\nrewards. We evaluate several more sophisticated exploration strategies,\nincluding Thompson sampling and Boltzman exploration, and propose a new\nexploration method based on assigning exploration bonuses from a concurrently\nlearned model of the system dynamics. By parameterizing our learned model with\na neural network, we are able to develop a scalable and efficient approach to\nexploration bonuses that can be applied to tasks with complex, high-dimensional\nstate spaces. In the Atari domain, our method provides the most consistent\nimprovement across a range of games that pose a major challenge for prior\nmethods. In addition to raw game-scores, we also develop an AUC-100 metric for\nthe Atari Learning domain to evaluate the impact of exploration on this\nbenchmark. \n\n"}
{"id": "1507.00825", "contents": "Title: Ridge Regression, Hubness, and Zero-Shot Learning Abstract: This paper discusses the effect of hubness in zero-shot learning, when ridge\nregression is used to find a mapping between the example space to the label\nspace. Contrary to the existing approach, which attempts to find a mapping from\nthe example space to the label space, we show that mapping labels into the\nexample space is desirable to suppress the emergence of hubs in the subsequent\nnearest neighbor search step. Assuming a simple data model, we prove that the\nproposed approach indeed reduces hubness. This was verified empirically on the\ntasks of bilingual lexicon extraction and image labeling: hubness was reduced\nwith both of these tasks and the accuracy was improved accordingly. \n\n"}
{"id": "1507.01661", "contents": "Title: Semiblind Hyperspectral Unmixing in the Presence of Spectral Library\n  Mismatches Abstract: The dictionary-aided sparse regression (SR) approach has recently emerged as\na promising alternative to hyperspectral unmixing (HU) in remote sensing. By\nusing an available spectral library as a dictionary, the SR approach identifies\nthe underlying materials in a given hyperspectral image by selecting a small\nsubset of spectral samples in the dictionary to represent the whole image. A\ndrawback with the current SR developments is that an actual spectral signature\nin the scene is often assumed to have zero mismatch with its corresponding\ndictionary sample, and such an assumption is considered too ideal in practice.\nIn this paper, we tackle the spectral signature mismatch problem by proposing a\ndictionary-adjusted nonconvex sparsity-encouraging regression (DANSER)\nframework. The main idea is to incorporate dictionary correcting variables in\nan SR formulation. A simple and low per-iteration complexity algorithm is\ntailor-designed for practical realization of DANSER. Using the same dictionary\ncorrecting idea, we also propose a robust subspace solution for dictionary\npruning. Extensive simulations and real-data experiments show that the proposed\nmethod is effective in mitigating the undesirable spectral signature mismatch\neffects. \n\n"}
{"id": "1507.04513", "contents": "Title: Scalable Gaussian Process Classification via Expectation Propagation Abstract: Variational methods have been recently considered for scaling the training\nprocess of Gaussian process classifiers to large datasets. As an alternative,\nwe describe here how to train these classifiers efficiently using expectation\npropagation. The proposed method allows for handling datasets with millions of\ndata instances. More precisely, it can be used for (i) training in a\ndistributed fashion where the data instances are sent to different nodes in\nwhich the required computations are carried out, and for (ii) maximizing an\nestimate of the marginal likelihood using a stochastic approximation of the\ngradient. Several experiments indicate that the method described is competitive\nwith the variational approach. \n\n"}
{"id": "1507.04544", "contents": "Title: Practical Bayesian model evaluation using leave-one-out cross-validation\n  and WAIC Abstract: Leave-one-out cross-validation (LOO) and the widely applicable information\ncriterion (WAIC) are methods for estimating pointwise out-of-sample prediction\naccuracy from a fitted Bayesian model using the log-likelihood evaluated at the\nposterior simulations of the parameter values. LOO and WAIC have various\nadvantages over simpler estimates of predictive error such as AIC and DIC but\nare less used in practice because they involve additional computational steps.\nHere we lay out fast and stable computations for LOO and WAIC that can be\nperformed using existing simulation draws. We introduce an efficient\ncomputation of LOO using Pareto-smoothed importance sampling (PSIS), a new\nprocedure for regularizing importance weights. Although WAIC is asymptotically\nequal to LOO, we demonstrate that PSIS-LOO is more robust in the finite case\nwith weak priors or influential observations. As a byproduct of our\ncalculations, we also obtain approximate standard errors for estimated\npredictive errors and for comparing of predictive errors between two models. We\nimplement the computations in an R package called 'loo' and demonstrate using\nmodels fit with the Bayesian inference package Stan. \n\n"}
{"id": "1507.06970", "contents": "Title: Perturbed Iterate Analysis for Asynchronous Stochastic Optimization Abstract: We introduce and analyze stochastic optimization methods where the input to\neach gradient update is perturbed by bounded noise. We show that this framework\nforms the basis of a unified approach to analyze asynchronous implementations\nof stochastic optimization algorithms.In this framework, asynchronous\nstochastic optimization algorithms can be thought of as serial methods\noperating on noisy inputs. Using our perturbed iterate framework, we provide\nnew analyses of the Hogwild! algorithm and asynchronous stochastic coordinate\ndescent, that are simpler than earlier analyses, remove many assumptions of\nprevious models, and in some cases yield improved upper bounds on the\nconvergence rates. We proceed to apply our framework to develop and analyze\nKroMagnon: a novel, parallel, sparse stochastic variance-reduced gradient\n(SVRG) algorithm. We demonstrate experimentally on a 16-core machine that the\nsparse and parallel version of SVRG is in some cases more than four orders of\nmagnitude faster than the standard SVRG algorithm. \n\n"}
{"id": "1507.06977", "contents": "Title: String and Membrane Gaussian Processes Abstract: In this paper we introduce a novel framework for making exact nonparametric\nBayesian inference on latent functions, that is particularly suitable for Big\nData tasks. Firstly, we introduce a class of stochastic processes we refer to\nas string Gaussian processes (string GPs), which are not to be mistaken for\nGaussian processes operating on text. We construct string GPs so that their\nfinite-dimensional marginals exhibit suitable local conditional independence\nstructures, which allow for scalable, distributed, and flexible nonparametric\nBayesian inference, without resorting to approximations, and while ensuring\nsome mild global regularity constraints. Furthermore, string GP priors\nnaturally cope with heterogeneous input data, and the gradient of the learned\nlatent function is readily available for explanatory analysis. Secondly, we\nprovide some theoretical results relating our approach to the standard GP\nparadigm. In particular, we prove that some string GPs are Gaussian processes,\nwhich provides a complementary global perspective on our framework. Finally, we\nderive a scalable and distributed MCMC scheme for supervised learning tasks\nunder string GP priors. The proposed MCMC scheme has computational time\ncomplexity $\\mathcal{O}(N)$ and memory requirement $\\mathcal{O}(dN)$, where $N$\nis the data size and $d$ the dimension of the input space. We illustrate the\nefficacy of the proposed approach on several synthetic and real-world datasets,\nincluding a dataset with $6$ millions input points and $8$ attributes. \n\n"}
{"id": "1508.00317", "contents": "Title: Time-series modeling with undecimated fully convolutional neural\n  networks Abstract: We present a new convolutional neural network-based time-series model.\nTypical convolutional neural network (CNN) architectures rely on the use of\nmax-pooling operators in between layers, which leads to reduced resolution at\nthe top layers. Instead, in this work we consider a fully convolutional network\n(FCN) architecture that uses causal filtering operations, and allows for the\nrate of the output signal to be the same as that of the input signal. We\nfurthermore propose an undecimated version of the FCN, which we refer to as the\nundecimated fully convolutional neural network (UFCNN), and is motivated by the\nundecimated wavelet transform. Our experimental results verify that using the\nundecimated version of the FCN is necessary in order to allow for effective\ntime-series modeling. The UFCNN has several advantages compared to other\ntime-series models such as the recurrent neural network (RNN) and long\nshort-term memory (LSTM), since it does not suffer from either the vanishing or\nexploding gradients problems, and is therefore easier to train. Convolution\noperations can also be implemented more efficiently compared to the recursion\nthat is involved in RNN-based models. We evaluate the performance of our model\nin a synthetic target tracking task using bearing only measurements generated\nfrom a state-space model, a probabilistic modeling of polyphonic music\nsequences problem, and a high frequency trading task using a time-series of\nask/bid quotes and their corresponding volumes. Our experimental results using\nsynthetic and real datasets verify the significant advantages of the UFCNN\ncompared to the RNN and LSTM baselines. \n\n"}
{"id": "1508.00842", "contents": "Title: Perceptron like Algorithms for Online Learning to Rank Abstract: Perceptron is a classic online algorithm for learning a classification\nfunction. In this paper, we provide a novel extension of the perceptron\nalgorithm to the learning to rank problem in information retrieval. We consider\npopular listwise performance measures such as Normalized Discounted Cumulative\nGain (NDCG) and Average Precision (AP). A modern perspective on perceptron for\nclassification is that it is simply an instance of online gradient descent\n(OGD), during mistake rounds, using the hinge loss function. Motivated by this\ninterpretation, we propose a novel family of listwise, large margin ranking\nsurrogates. Members of this family can be thought of as analogs of the hinge\nloss. Exploiting a certain self-bounding property of the proposed family, we\nprovide a guarantee on the cumulative NDCG (or AP) induced loss incurred by our\nperceptron-like algorithm. We show that, if there exists a perfect oracle\nranker which can correctly rank each instance in an online sequence of ranking\ndata, with some margin, the cumulative loss of perceptron algorithm on that\nsequence is bounded by a constant, irrespective of the length of the sequence.\nThis result is reminiscent of Novikoff's convergence theorem for the\nclassification perceptron. Moreover, we prove a lower bound on the cumulative\nloss achievable by any deterministic algorithm, under the assumption of\nexistence of perfect oracle ranker. The lower bound shows that our perceptron\nbound is not tight, and we propose another, \\emph{purely online}, algorithm\nwhich achieves the lower bound. We provide empirical results on simulated and\nlarge commercial datasets to corroborate our theoretical results. \n\n"}
{"id": "1508.01746", "contents": "Title: Using Deep Learning for Detecting Spoofing Attacks on Speech Signals Abstract: It is well known that speaker verification systems are subject to spoofing\nattacks. The Automatic Speaker Verification Spoofing and Countermeasures\nChallenge -- ASVSpoof2015 -- provides a standard spoofing database, containing\nattacks based on synthetic speech, along with a protocol for experiments. This\npaper describes CPqD's systems submitted to the ASVSpoof2015 Challenge, based\non deep neural networks, working both as a classifier and as a feature\nextraction module for a GMM and a SVM classifier. Results show the validity of\nthis approach, achieving less than 0.5\\% EER for known attacks. \n\n"}
{"id": "1508.02087", "contents": "Title: A Linearly-Convergent Stochastic L-BFGS Algorithm Abstract: We propose a new stochastic L-BFGS algorithm and prove a linear convergence\nrate for strongly convex and smooth functions. Our algorithm draws heavily from\na recent stochastic variant of L-BFGS proposed in Byrd et al. (2014) as well as\na recent approach to variance reduction for stochastic gradient descent from\nJohnson and Zhang (2013). We demonstrate experimentally that our algorithm\nperforms well on large-scale convex and non-convex optimization problems,\nexhibiting linear convergence and rapidly solving the optimization problems to\nhigh levels of precision. Furthermore, we show that our algorithm performs well\nfor a wide-range of step sizes, often differing by several orders of magnitude. \n\n"}
{"id": "1508.04210", "contents": "Title: Zero-Truncated Poisson Tensor Factorization for Massive Binary Tensors Abstract: We present a scalable Bayesian model for low-rank factorization of massive\ntensors with binary observations. The proposed model has the following key\nproperties: (1) in contrast to the models based on the logistic or probit\nlikelihood, using a zero-truncated Poisson likelihood for binary data allows\nour model to scale up in the number of \\emph{ones} in the tensor, which is\nespecially appealing for massive but sparse binary tensors; (2)\nside-information in form of binary pairwise relationships (e.g., an adjacency\nnetwork) between objects in any tensor mode can also be leveraged, which can be\nespecially useful in \"cold-start\" settings; and (3) the model admits simple\nBayesian inference via batch, as well as \\emph{online} MCMC; the latter allows\nscaling up even for \\emph{dense} binary data (i.e., when the number of ones in\nthe tensor/network is also massive). In addition, non-negative factor matrices\nin our model provide easy interpretability, and the tensor rank can be inferred\nfrom the data. We evaluate our model on several large-scale real-world binary\ntensors, achieving excellent computational scalability, and also demonstrate\nits usefulness in leveraging side-information provided in form of\nmode-network(s). \n\n"}
{"id": "1508.07744", "contents": "Title: Ethnicity sensitive author disambiguation using semi-supervised learning Abstract: Author name disambiguation in bibliographic databases is the problem of\ngrouping together scientific publications written by the same person,\naccounting for potential homonyms and/or synonyms. Among solutions to this\nproblem, digital libraries are increasingly offering tools for authors to\nmanually curate their publications and claim those that are theirs. Indirectly,\nthese tools allow for the inexpensive collection of large annotated training\ndata, which can be further leveraged to build a complementary automated\ndisambiguation system capable of inferring patterns for identifying\npublications written by the same person. Building on more than 1 million\npublicly released crowdsourced annotations, we propose an automated author\ndisambiguation solution exploiting this data (i) to learn an accurate\nclassifier for identifying coreferring authors and (ii) to guide the clustering\nof scientific publications by distinct authors in a semi-supervised way. To the\nbest of our knowledge, our analysis is the first to be carried out on data of\nthis size and coverage. With respect to the state of the art, we validate the\ngeneral pipeline used in most existing solutions, and improve by: (i) proposing\nphonetic-based blocking strategies, thereby increasing recall; and (ii) adding\nstrong ethnicity-sensitive features for learning a linkage function, thereby\ntailoring disambiguation to non-Western author names whenever necessary. \n\n"}
{"id": "1509.00151", "contents": "Title: Learning A Task-Specific Deep Architecture For Clustering Abstract: While sparse coding-based clustering methods have shown to be successful,\ntheir bottlenecks in both efficiency and scalability limit the practical usage.\nIn recent years, deep learning has been proved to be a highly effective,\nefficient and scalable feature learning tool. In this paper, we propose to\nemulate the sparse coding-based clustering pipeline in the context of deep\nlearning, leading to a carefully crafted deep model benefiting from both. A\nfeed-forward network structure, named TAGnet, is constructed based on a\ngraph-regularized sparse coding algorithm. It is then trained with\ntask-specific loss functions from end to end. We discover that connecting deep\nlearning to sparse coding benefits not only the model performance, but also its\ninitialization and interpretation. Moreover, by introducing auxiliary\nclustering tasks to the intermediate feature hierarchy, we formulate DTAGnet\nand obtain a further performance boost. Extensive experiments demonstrate that\nthe proposed model gains remarkable margins over several state-of-the-art\nmethods. \n\n"}
{"id": "1509.01240", "contents": "Title: Train faster, generalize better: Stability of stochastic gradient\n  descent Abstract: We show that parametric models trained by a stochastic gradient method (SGM)\nwith few iterations have vanishing generalization error. We prove our results\nby arguing that SGM is algorithmically stable in the sense of Bousquet and\nElisseeff. Our analysis only employs elementary tools from convex and\ncontinuous optimization. We derive stability bounds for both convex and\nnon-convex optimization under standard Lipschitz and smoothness assumptions.\n  Applying our results to the convex case, we provide new insights for why\nmultiple epochs of stochastic gradient methods generalize well in practice. In\nthe non-convex case, we give a new interpretation of common practices in neural\nnetworks, and formally show that popular techniques for training large deep\nmodels are indeed stability-promoting. Our findings conceptually underscore the\nimportance of reducing training time beyond its obvious benefit. \n\n"}
{"id": "1509.01520", "contents": "Title: An On-line Variational Bayesian Model for Multi-Person Tracking from\n  Cluttered Scenes Abstract: Object tracking is an ubiquitous problem that appears in many applications\nsuch as remote sensing, audio processing, computer vision, human-machine\ninterfaces, human-robot interaction, etc. Although thoroughly investigated in\ncomputer vision, tracking a time-varying number of persons remains a\nchallenging open problem. In this paper, we propose an on-line variational\nBayesian model for multi-person tracking from cluttered visual observations\nprovided by person detectors. The contributions of this paper are the\nfollowings. First, we propose a variational Bayesian framework for tracking an\nunknown and varying number of persons. Second, our model results in a\nvariational expectation-maximization (VEM) algorithm with closed-form\nexpressions for the posterior distributions of the latent variables and for the\nestimation of the model parameters. Third, the proposed model exploits\nobservations from multiple detectors, and it is therefore multimodal by nature.\nFinally, we propose to embed both object-birth and object-visibility processes\nin an effort to robustly handle person appearances and disappearances over\ntime. Evaluated on classical multiple person tracking datasets, our method\nshows competitive results with respect to state-of-the-art multiple-object\ntracking models, such as the probability hypothesis density (PHD) filter among\nothers. \n\n"}
{"id": "1509.03025", "contents": "Title: Fast low-rank estimation by projected gradient descent: General\n  statistical and algorithmic guarantees Abstract: Optimization problems with rank constraints arise in many applications,\nincluding matrix regression, structured PCA, matrix completion and matrix\ndecomposition problems. An attractive heuristic for solving such problems is to\nfactorize the low-rank matrix, and to run projected gradient descent on the\nnonconvex factorized optimization problem. The goal of this problem is to\nprovide a general theoretical framework for understanding when such methods\nwork well, and to characterize the nature of the resulting fixed point. We\nprovide a simple set of conditions under which projected gradient descent, when\ngiven a suitable initialization, converges geometrically to a statistically\nuseful solution. Our results are applicable even when the initial solution is\noutside any region of local convexity, and even when the problem is globally\nconcave. Working in a non-asymptotic framework, we show that our conditions are\nsatisfied for a wide range of concrete models, including matrix regression,\nstructured PCA, matrix completion with real and quantized observations, matrix\ndecomposition, and graph clustering problems. Simulation results show excellent\nagreement with the theoretical predictions. \n\n"}
{"id": "1509.03248", "contents": "Title: A deep matrix factorization method for learning attribute\n  representations Abstract: Semi-Non-negative Matrix Factorization is a technique that learns a\nlow-dimensional representation of a dataset that lends itself to a clustering\ninterpretation. It is possible that the mapping between this new representation\nand our original data matrix contains rather complex hierarchical information\nwith implicit lower-level hidden attributes, that classical one level\nclustering methodologies can not interpret. In this work we propose a novel\nmodel, Deep Semi-NMF, that is able to learn such hidden representations that\nallow themselves to an interpretation of clustering according to different,\nunknown attributes of a given dataset. We also present a semi-supervised\nversion of the algorithm, named Deep WSF, that allows the use of (partial)\nprior information for each of the known attributes of a dataset, that allows\nthe model to be used on datasets with mixed attribute knowledge. Finally, we\nshow that our models are able to learn low-dimensional representations that are\nbetter suited for clustering, but also classification, outperforming\nSemi-Non-negative Matrix Factorization, but also other state-of-the-art\nmethodologies variants. \n\n"}
{"id": "1509.04612", "contents": "Title: Adapting Resilient Propagation for Deep Learning Abstract: The Resilient Propagation (Rprop) algorithm has been very popular for\nbackpropagation training of multilayer feed-forward neural networks in various\napplications. The standard Rprop however encounters difficulties in the context\nof deep neural networks as typically happens with gradient-based learning\nalgorithms. In this paper, we propose a modification of the Rprop that combines\nstandard Rprop steps with a special drop out technique. We apply the method for\ntraining Deep Neural Networks as standalone components and in ensemble\nformulations. Results on the MNIST dataset show that the proposed modification\nalleviates standard Rprop's problems demonstrating improved learning speed and\naccuracy. \n\n"}
{"id": "1509.05789", "contents": "Title: BLC: Private Matrix Factorization Recommenders via Automatic Group\n  Learning Abstract: We propose a privacy-enhanced matrix factorization recommender that exploits\nthe fact that users can often be grouped together by interest. This allows a\nform of \"hiding in the crowd\" privacy. We introduce a novel matrix\nfactorization approach suited to making recommendations in a shared group (or\nnym) setting and the BLC algorithm for carrying out this matrix factorization\nin a privacy-enhanced manner. We demonstrate that the increased privacy does\nnot come at the cost of reduced recommendation accuracy. \n\n"}
{"id": "1509.06088", "contents": "Title: Significance Analysis of High-Dimensional, Low-Sample Size Partially\n  Labeled Data Abstract: Classification and clustering are both important topics in statistical\nlearning. A natural question herein is whether predefined classes are really\ndifferent from one another, or whether clusters are really there. Specifically,\nwe may be interested in knowing whether the two classes defined by some class\nlabels (when they are provided), or the two clusters tagged by a clustering\nalgorithm (where class labels are not provided), are from the same underlying\ndistribution. Although both are challenging questions for the high-dimensional,\nlow-sample size data, there has been some recent development for both. However,\nwhen it is costly to manually place labels on observations, it is often that\nonly a small portion of the class labels is available. In this article, we\npropose a significance analysis approach for such type of data, namely\npartially labeled data. Our method makes use of the whole data and tries to\ntest the class difference as if all the labels were observed. Compared to a\ntesting method that ignores the label information, our method provides a\ngreater power, meanwhile, maintaining the size, illustrated by a comprehensive\nsimulation study. Theoretical properties of the proposed method are studied\nwith emphasis on the high-dimensional, low-sample size setting. Our simulated\nexamples help to understand when and how the information extracted from the\nlabeled data can be effective. A real data example further illustrates the\nusefulness of the proposed method. \n\n"}
{"id": "1509.08144", "contents": "Title: Optimal Copula Transport for Clustering Multivariate Time Series Abstract: This paper presents a new methodology for clustering multivariate time series\nleveraging optimal transport between copulas. Copulas are used to encode both\n(i) intra-dependence of a multivariate time series, and (ii) inter-dependence\nbetween two time series. Then, optimal copula transport allows us to define two\ndistances between multivariate time series: (i) one for measuring\nintra-dependence dissimilarity, (ii) another one for measuring inter-dependence\ndissimilarity based on a new multivariate dependence coefficient which is\nrobust to noise, deterministic, and which can target specified dependencies. \n\n"}
{"id": "1511.00561", "contents": "Title: SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image\n  Segmentation Abstract: We present a novel and practical deep fully convolutional neural network\narchitecture for semantic pixel-wise segmentation termed SegNet. This core\ntrainable segmentation engine consists of an encoder network, a corresponding\ndecoder network followed by a pixel-wise classification layer. The architecture\nof the encoder network is topologically identical to the 13 convolutional\nlayers in the VGG16 network. The role of the decoder network is to map the low\nresolution encoder feature maps to full input resolution feature maps for\npixel-wise classification. The novelty of SegNet lies is in the manner in which\nthe decoder upsamples its lower resolution input feature map(s). Specifically,\nthe decoder uses pooling indices computed in the max-pooling step of the\ncorresponding encoder to perform non-linear upsampling. This eliminates the\nneed for learning to upsample. The upsampled maps are sparse and are then\nconvolved with trainable filters to produce dense feature maps. We compare our\nproposed architecture with the widely adopted FCN and also with the well known\nDeepLab-LargeFOV, DeconvNet architectures. This comparison reveals the memory\nversus accuracy trade-off involved in achieving good segmentation performance.\n  SegNet was primarily motivated by scene understanding applications. Hence, it\nis designed to be efficient both in terms of memory and computational time\nduring inference. It is also significantly smaller in the number of trainable\nparameters than other competing architectures. We also performed a controlled\nbenchmark of SegNet and other architectures on both road scenes and SUN RGB-D\nindoor scene segmentation tasks. We show that SegNet provides good performance\nwith competitive inference time and more efficient inference memory-wise as\ncompared to other architectures. We also provide a Caffe implementation of\nSegNet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet/. \n\n"}
{"id": "1511.00871", "contents": "Title: Properties of the Sample Mean in Graph Spaces and the\n  Majorize-Minimize-Mean Algorithm Abstract: One of the most fundamental concepts in statistics is the concept of sample\nmean. Properties of the sample mean that are well-defined in Euclidean spaces\nbecome unwieldy or even unclear in graph spaces. Open problems related to the\nsample mean of graphs include: non-existence, non-uniqueness, statistical\ninconsistency, lack of convergence results of mean algorithms, non-existence of\nmidpoints, and disparity to midpoints. We present conditions to resolve all six\nproblems and propose a Majorize-Minimize-Mean (MMM) Algorithm. Experiments on\ngraph datasets representing images and molecules show that the MMM-Algorithm\nbest approximates a sample mean of graphs compared to six other mean\nalgorithms. \n\n"}
{"id": "1511.03163", "contents": "Title: Semi-supervised Tuning from Temporal Coherence Abstract: Recent works demonstrated the usefulness of temporal coherence to regularize\nsupervised training or to learn invariant features with deep architectures. In\nparticular, enforcing smooth output changes while presenting temporally-closed\nframes from video sequences, proved to be an effective strategy. In this paper\nwe prove the efficacy of temporal coherence for semi-supervised incremental\ntuning. We show that a deep architecture, just mildly trained in a supervised\nmanner, can progressively improve its classification accuracy, if exposed to\nvideo sequences of unlabeled data. The extent to which, in some cases, a\nsemi-supervised tuning allows to improve classification accuracy (approaching\nthe supervised one) is somewhat surprising. A number of control experiments\npointed out the fundamental role of temporal coherence. \n\n"}
{"id": "1511.03962", "contents": "Title: Document Context Language Models Abstract: Text documents are structured on multiple levels of detail: individual words\nare related by syntax, but larger units of text are related by discourse\nstructure. Existing language models generally fail to account for discourse\nstructure, but it is crucial if we are to have language models that reward\ncoherence and generate coherent texts. We present and empirically evaluate a\nset of multi-level recurrent neural network language models, called\nDocument-Context Language Models (DCLM), which incorporate contextual\ninformation both within and beyond the sentence. In comparison with word-level\nrecurrent neural network language models, the DCLM models obtain slightly\nbetter predictive likelihoods, and considerably better assessments of document\ncoherence. \n\n"}
{"id": "1511.05101", "contents": "Title: How (not) to Train your Generative Model: Scheduled Sampling,\n  Likelihood, Adversary? Abstract: Modern applications and progress in deep learning research have created\nrenewed interest for generative models of text and of images. However, even\ntoday it is unclear what objective functions one should use to train and\nevaluate these models. In this paper we present two contributions.\n  Firstly, we present a critique of scheduled sampling, a state-of-the-art\ntraining method that contributed to the winning entry to the MSCOCO image\ncaptioning benchmark in 2015. Here we show that despite this impressive\nempirical performance, the objective function underlying scheduled sampling is\nimproper and leads to an inconsistent learning algorithm.\n  Secondly, we revisit the problems that scheduled sampling was meant to\naddress, and present an alternative interpretation. We argue that maximum\nlikelihood is an inappropriate training objective when the end-goal is to\ngenerate natural-looking samples. We go on to derive an ideal objective\nfunction to use in this situation instead. We introduce a generalisation of\nadversarial training, and show how such method can interpolate between maximum\nlikelihood training and our ideal training objective. To our knowledge this is\nthe first theoretical analysis that explains why adversarial training tends to\nproduce samples with higher perceived quality. \n\n"}
{"id": "1511.06434", "contents": "Title: Unsupervised Representation Learning with Deep Convolutional Generative\n  Adversarial Networks Abstract: In recent years, supervised learning with convolutional networks (CNNs) has\nseen huge adoption in computer vision applications. Comparatively, unsupervised\nlearning with CNNs has received less attention. In this work we hope to help\nbridge the gap between the success of CNNs for supervised learning and\nunsupervised learning. We introduce a class of CNNs called deep convolutional\ngenerative adversarial networks (DCGANs), that have certain architectural\nconstraints, and demonstrate that they are a strong candidate for unsupervised\nlearning. Training on various image datasets, we show convincing evidence that\nour deep convolutional adversarial pair learns a hierarchy of representations\nfrom object parts to scenes in both the generator and discriminator.\nAdditionally, we use the learned features for novel tasks - demonstrating their\napplicability as general image representations. \n\n"}
{"id": "1511.06683", "contents": "Title: Top-k Multiclass SVM Abstract: Class ambiguity is typical in image classification problems with a large\nnumber of classes. When classes are difficult to discriminate, it makes sense\nto allow k guesses and evaluate classifiers based on the top-k error instead of\nthe standard zero-one loss. We propose top-k multiclass SVM as a direct method\nto optimize for top-k performance. Our generalization of the well-known\nmulticlass SVM is based on a tight convex upper bound of the top-k error. We\npropose a fast optimization scheme based on an efficient projection onto the\ntop-k simplex, which is of its own interest. Experiments on five datasets show\nconsistent improvements in top-k accuracy compared to various baselines. \n\n"}
{"id": "1511.07528", "contents": "Title: The Limitations of Deep Learning in Adversarial Settings Abstract: Deep learning takes advantage of large datasets and computationally efficient\ntraining algorithms to outperform other approaches at various machine learning\ntasks. However, imperfections in the training phase of deep neural networks\nmake them vulnerable to adversarial samples: inputs crafted by adversaries with\nthe intent of causing deep neural networks to misclassify. In this work, we\nformalize the space of adversaries against deep neural networks (DNNs) and\nintroduce a novel class of algorithms to craft adversarial samples based on a\nprecise understanding of the mapping between inputs and outputs of DNNs. In an\napplication to computer vision, we show that our algorithms can reliably\nproduce samples correctly classified by human subjects but misclassified in\nspecific targets by a DNN with a 97% adversarial success rate while only\nmodifying on average 4.02% of the input features per sample. We then evaluate\nthe vulnerability of different sample classes to adversarial perturbations by\ndefining a hardness measure. Finally, we describe preliminary work outlining\ndefenses against adversarial samples by defining a predictive measure of\ndistance between a benign input and a target classification. \n\n"}
{"id": "1511.08551", "contents": "Title: Regularized EM Algorithms: A Unified Framework and Statistical\n  Guarantees Abstract: Latent variable models are a fundamental modeling tool in machine learning\napplications, but they present significant computational and analytical\nchallenges. The popular EM algorithm and its variants, is a much used\nalgorithmic tool; yet our rigorous understanding of its performance is highly\nincomplete. Recently, work in Balakrishnan et al. (2014) has demonstrated that\nfor an important class of problems, EM exhibits linear local convergence. In\nthe high-dimensional setting, however, the M-step may not be well defined. We\naddress precisely this setting through a unified treatment using\nregularization. While regularization for high-dimensional problems is by now\nwell understood, the iterative EM algorithm requires a careful balancing of\nmaking progress towards the solution while identifying the right structure\n(e.g., sparsity or low-rank). In particular, regularizing the M-step using the\nstate-of-the-art high-dimensional prescriptions (e.g., Wainwright (2014)) is\nnot guaranteed to provide this balance. Our algorithm and analysis are linked\nin a way that reveals the balance between optimization and statistical errors.\nWe specialize our general framework to sparse gaussian mixture models,\nhigh-dimensional mixed regression, and regression with missing variables,\nobtaining statistical guarantees for each of these examples. \n\n"}
{"id": "1512.02134", "contents": "Title: A Large Dataset to Train Convolutional Networks for Disparity, Optical\n  Flow, and Scene Flow Estimation Abstract: Recent work has shown that optical flow estimation can be formulated as a\nsupervised learning task and can be successfully solved with convolutional\nnetworks. Training of the so-called FlowNet was enabled by a large\nsynthetically generated dataset. The present paper extends the concept of\noptical flow estimation via convolutional networks to disparity and scene flow\nestimation. To this end, we propose three synthetic stereo video datasets with\nsufficient realism, variation, and size to successfully train large networks.\nOur datasets are the first large-scale datasets to enable training and\nevaluating scene flow methods. Besides the datasets, we present a convolutional\nnetwork for real-time disparity estimation that provides state-of-the-art\nresults. By combining a flow and disparity estimation network and training it\njointly, we demonstrate the first scene flow estimation with a convolutional\nnetwork. \n\n"}
{"id": "1512.02752", "contents": "Title: A Novel Regularized Principal Graph Learning Framework on Explicit Graph\n  Representation Abstract: Many scientific datasets are of high dimension, and the analysis usually\nrequires visual manipulation by retaining the most important structures of\ndata. Principal curve is a widely used approach for this purpose. However, many\nexisting methods work only for data with structures that are not\nself-intersected, which is quite restrictive for real applications. A few\nmethods can overcome the above problem, but they either require complicated\nhuman-made rules for a specific task with lack of convergence guarantee and\nadaption flexibility to different tasks, or cannot obtain explicit structures\nof data. To address these issues, we develop a new regularized principal graph\nlearning framework that captures the local information of the underlying graph\nstructure based on reversed graph embedding. As showcases, models that can\nlearn a spanning tree or a weighted undirected $\\ell_1$ graph are proposed, and\na new learning algorithm is developed that learns a set of principal points and\na graph structure from data, simultaneously. The new algorithm is simple with\nguaranteed convergence. We then extend the proposed framework to deal with\nlarge-scale data. Experimental results on various synthetic and six real world\ndatasets show that the proposed method compares favorably with baselines and\ncan uncover the underlying structure correctly. \n\n"}
{"id": "1512.02866", "contents": "Title: Multi-Player Bandits -- a Musical Chairs Approach Abstract: We consider a variant of the stochastic multi-armed bandit problem, where\nmultiple players simultaneously choose from the same set of arms and may\ncollide, receiving no reward. This setting has been motivated by problems\narising in cognitive radio networks, and is especially challenging under the\nrealistic assumption that communication between players is limited. We provide\na communication-free algorithm (Musical Chairs) which attains constant regret\nwith high probability, as well as a sublinear-regret, communication-free\nalgorithm (Dynamic Musical Chairs) for the more difficult setting of players\ndynamically entering and leaving throughout the game. Moreover, both algorithms\ndo not require prior knowledge of the number of players. To the best of our\nknowledge, these are the first communication-free algorithms with these types\nof formal guarantees. We also rigorously compare our algorithms to previous\nworks, and complement our theoretical findings with experiments. \n\n"}
{"id": "1512.03844", "contents": "Title: Efficient Deep Feature Learning and Extraction via StochasticNets Abstract: Deep neural networks are a powerful tool for feature learning and extraction\ngiven their ability to model high-level abstractions in highly complex data.\nOne area worth exploring in feature learning and extraction using deep neural\nnetworks is efficient neural connectivity formation for faster feature learning\nand extraction. Motivated by findings of stochastic synaptic connectivity\nformation in the brain as well as the brain's uncanny ability to efficiently\nrepresent information, we propose the efficient learning and extraction of\nfeatures via StochasticNets, where sparsely-connected deep neural networks can\nbe formed via stochastic connectivity between neurons. To evaluate the\nfeasibility of such a deep neural network architecture for feature learning and\nextraction, we train deep convolutional StochasticNets to learn abstract\nfeatures using the CIFAR-10 dataset, and extract the learned features from\nimages to perform classification on the SVHN and STL-10 datasets. Experimental\nresults show that features learned using deep convolutional StochasticNets,\nwith fewer neural connections than conventional deep convolutional neural\nnetworks, can allow for better or comparable classification accuracy than\nconventional deep neural networks: relative test error decrease of ~4.5% for\nclassification on the STL-10 dataset and ~1% for classification on the SVHN\ndataset. Furthermore, it was shown that the deep features extracted using deep\nconvolutional StochasticNets can provide comparable classification accuracy\neven when only 10% of the training data is used for feature learning. Finally,\nit was also shown that significant gains in feature extraction speed can be\nachieved in embedded applications using StochasticNets. As such, StochasticNets\nallow for faster feature learning and extraction performance while facilitate\nfor better or comparable accuracy performances. \n\n"}
{"id": "1512.03883", "contents": "Title: Sparse Generalized Principal Component Analysis for Large-scale\n  Applications beyond Gaussianity Abstract: Principal Component Analysis (PCA) is a dimension reduction technique. It\nproduces inconsistent estimators when the dimensionality is moderate to high,\nwhich is often the problem in modern large-scale applications where algorithm\nscalability and model interpretability are difficult to achieve, not to mention\nthe prevalence of missing values. While existing sparse PCA methods alleviate\ninconsistency, they are constrained to the Gaussian assumption of classical PCA\nand fail to address algorithm scalability issues. We generalize sparse PCA to\nthe broad exponential family distributions under high-dimensional setup, with\nbuilt-in treatment for missing values. Meanwhile we propose a family of\niterative sparse generalized PCA (SG-PCA) algorithms such that despite the\nnon-convexity and non-smoothness of the optimization task, the loss function\ndecreases in every iteration. In terms of ease and intuitive parameter tuning,\nour sparsity-inducing regularization is far superior to the popular Lasso.\nFurthermore, to promote overall scalability, accelerated gradient is integrated\nfor fast convergence, while a progressive screening technique gradually\nsqueezes out nuisance dimensions of a large-scale problem for feasible\noptimization. High-dimensional simulation and real data experiments demonstrate\nthe efficiency and efficacy of SG-PCA. \n\n"}
{"id": "1512.08204", "contents": "Title: New Perspectives on $k$-Support and Cluster Norms Abstract: We study a regularizer which is defined as a parameterized infimum of\nquadratics, and which we call the box-norm. We show that the k-support norm, a\nregularizer proposed by [Argyriou et al, 2012] for sparse vector prediction\nproblems, belongs to this family, and the box-norm can be generated as a\nperturbation of the former. We derive an improved algorithm to compute the\nproximity operator of the squared box-norm, and we provide a method to compute\nthe norm. We extend the norms to matrices, introducing the spectral k-support\nnorm and spectral box-norm. We note that the spectral box-norm is essentially\nequivalent to the cluster norm, a multitask learning regularizer introduced by\n[Jacob et al. 2009a], and which in turn can be interpreted as a perturbation of\nthe spectral k-support norm. Centering the norm is important for multitask\nlearning and we also provide a method to use centered versions of the norms as\nregularizers. Numerical experiments indicate that the spectral k-support and\nbox-norms and their centered variants provide state of the art performance in\nmatrix completion and multitask learning problems respectively. \n\n"}
{"id": "1512.09302", "contents": "Title: Linear Convergence of Proximal Gradient Algorithm with Extrapolation for\n  a Class of Nonconvex Nonsmooth Minimization Problems Abstract: In this paper, we study the proximal gradient algorithm with extrapolation\nfor minimizing the sum of a Lipschitz differentiable function and a proper\nclosed convex function. Under the error bound condition used in [19] for\nanalyzing the convergence of the proximal gradient algorithm, we show that\nthere exists a threshold such that if the extrapolation coefficients are chosen\nbelow this threshold, then the sequence generated converges $R$-linearly to a\nstationary point of the problem. Moreover, the corresponding sequence of\nobjective values is also $R$-linearly convergent. In addition, the threshold\nreduces to $1$ for convex problems and, as a consequence, we obtain the\n$R$-linear convergence of the sequence generated by FISTA with fixed restart.\nFinally, we present some numerical experiments to illustrate our results. \n\n"}
{"id": "1601.03945", "contents": "Title: Improved graph-based SFA: Information preservation complements the\n  slowness principle Abstract: Slow feature analysis (SFA) is an unsupervised-learning algorithm that\nextracts slowly varying features from a multi-dimensional time series. A\nsupervised extension to SFA for classification and regression is graph-based\nSFA (GSFA). GSFA is based on the preservation of similarities, which are\nspecified by a graph structure derived from the labels. It has been shown that\nhierarchical GSFA (HGSFA) allows learning from images and other\nhigh-dimensional data. The feature space spanned by HGSFA is complex due to the\ncomposition of the nonlinearities of the nodes in the network. However, we show\nthat the network discards useful information prematurely before it reaches\nhigher nodes, resulting in suboptimal global slowness and an under-exploited\nfeature space.\n  To counteract these problems, we propose an extension called hierarchical\ninformation-preserving GSFA (HiGSFA), where information preservation\ncomplements the slowness-maximization goal. We build a 10-layer HiGSFA network\nto estimate human age from facial photographs of the MORPH-II database,\nachieving a mean absolute error of 3.50 years, improving the state-of-the-art\nperformance. HiGSFA and HGSFA support multiple-labels and offer a rich feature\nspace, feed-forward training, and linear complexity in the number of samples\nand dimensions. Furthermore, HiGSFA outperforms HGSFA in terms of feature\nslowness, estimation accuracy and input reconstruction, giving rise to a\npromising hierarchical supervised-learning approach. \n\n"}
{"id": "1602.01818", "contents": "Title: Random Feature Maps via a Layered Random Projection (LaRP) Framework for\n  Object Classification Abstract: The approximation of nonlinear kernels via linear feature maps has recently\ngained interest due to their applications in reducing the training and testing\ntime of kernel-based learning algorithms. Current random projection methods\navoid the curse of dimensionality by embedding the nonlinear feature space into\na low dimensional Euclidean space to create nonlinear kernels. We introduce a\nLayered Random Projection (LaRP) framework, where we model the linear kernels\nand nonlinearity separately for increased training efficiency. The proposed\nLaRP framework was assessed using the MNIST hand-written digits database and\nthe COIL-100 object database, and showed notable improvement in object\nclassification performance relative to other state-of-the-art random projection\nmethods. \n\n"}
{"id": "1602.02196", "contents": "Title: BISTRO: An Efficient Relaxation-Based Method for Contextual Bandits Abstract: We present efficient algorithms for the problem of contextual bandits with\ni.i.d. covariates, an arbitrary sequence of rewards, and an arbitrary class of\npolicies. Our algorithm BISTRO requires d calls to the empirical risk\nminimization (ERM) oracle per round, where d is the number of actions. The\nmethod uses unlabeled data to make the problem computationally simple. When the\nERM problem itself is computationally hard, we extend the approach by employing\nmultiplicative approximation algorithms for the ERM. The integrality gap of the\nrelaxation only enters in the regret bound rather than the benchmark. Finally,\nwe show that the adversarial version of the contextual bandit problem is\nlearnable (and efficient) whenever the full-information supervised online\nlearning problem has a non-trivial regret guarantee (and efficient). \n\n"}
{"id": "1602.04283", "contents": "Title: Deep Learning on FPGAs: Past, Present, and Future Abstract: The rapid growth of data size and accessibility in recent years has\ninstigated a shift of philosophy in algorithm design for artificial\nintelligence. Instead of engineering algorithms by hand, the ability to learn\ncomposable systems automatically from massive amounts of data has led to\nground-breaking performance in important domains such as computer vision,\nspeech recognition, and natural language processing. The most popular class of\ntechniques used in these domains is called deep learning, and is seeing\nsignificant attention from industry. However, these models require incredible\namounts of data and compute power to train, and are limited by the need for\nbetter hardware acceleration to accommodate scaling beyond current data and\nmodel sizes. While the current solution has been to use clusters of graphics\nprocessing units (GPU) as general purpose processors (GPGPU), the use of field\nprogrammable gate arrays (FPGA) provide an interesting alternative. Current\ntrends in design tools for FPGAs have made them more compatible with the\nhigh-level software practices typically practiced in the deep learning\ncommunity, making FPGAs more accessible to those who build and deploy models.\nSince FPGA architectures are flexible, this could also allow researchers the\nability to explore model-level optimizations beyond what is possible on fixed\narchitectures such as GPUs. As well, FPGAs tend to provide high performance per\nwatt of power consumption, which is of particular importance for application\nscientists interested in large scale server-based deployment or\nresource-limited embedded applications. This review takes a look at deep\nlearning and FPGAs from a hardware acceleration perspective, identifying trends\nand innovations that make these technologies a natural fit, and motivates a\ndiscussion on how FPGAs may best serve the needs of the deep learning community\nmoving forward. \n\n"}
{"id": "1602.04951", "contents": "Title: Q($\\lambda$) with Off-Policy Corrections Abstract: We propose and analyze an alternate approach to off-policy multi-step\ntemporal difference learning, in which off-policy returns are corrected with\nthe current Q-function in terms of rewards, rather than with the target policy\nin terms of transition probabilities. We prove that such approximate\ncorrections are sufficient for off-policy convergence both in policy evaluation\nand control, provided certain conditions. These conditions relate the distance\nbetween the target and behavior policies, the eligibility trace parameter and\nthe discount factor, and formalize an underlying tradeoff in off-policy\nTD($\\lambda$). We illustrate this theoretical relationship empirically on a\ncontinuous-state control task. \n\n"}
{"id": "1603.00531", "contents": "Title: LOFS: Library of Online Streaming Feature Selection Abstract: As an emerging research direction, online streaming feature selection deals\nwith sequentially added dimensions in a feature space while the number of data\ninstances is fixed. Online streaming feature selection provides a new,\ncomplementary algorithmic methodology to enrich online feature selection,\nespecially targets to high dimensionality in big data analytics. This paper\nintroduces the first comprehensive open-source library for use in MATLAB that\nimplements the state-of-the-art algorithms of online streaming feature\nselection. The library is designed to facilitate the development of new\nalgorithms in this exciting research direction and make comparisons between the\nnew methods and existing ones available. \n\n"}
{"id": "1603.02644", "contents": "Title: Online but Accurate Inference for Latent Variable Models with Local\n  Gibbs Sampling Abstract: We study parameter inference in large-scale latent variable models. We first\npropose an unified treatment of online inference for latent variable models\nfrom a non-canonical exponential family, and draw explicit links between\nseveral previously proposed frequentist or Bayesian methods. We then propose a\nnovel inference method for the frequentist estimation of parameters, that\nadapts MCMC methods to online inference of latent variable models with the\nproper use of local Gibbs sampling. Then, for latent Dirich-let allocation,we\nprovide an extensive set of experiments and comparisons with existing work,\nwhere our new approach outperforms all previously proposed methods. In\nparticular, using Gibbs sampling for latent variable inference is superior to\nvariational inference in terms of test log-likelihoods. Moreover, Bayesian\ninference through variational methods perform poorly, sometimes leading to\nworse fits with latent variables of higher dimensionality. \n\n"}
{"id": "1603.03788", "contents": "Title: A Primer on the Signature Method in Machine Learning Abstract: We provide an introduction to the signature method, focusing on its\ntheoretical properties and machine learning applications. Our presentation is\ndivided into two parts. In the first part, we present the definition and\nfundamental properties of the signature of a path. The signature is a sequence\nof numbers associated with a path that captures many of its important analytic\nand geometric properties. As a sequence of numbers, the signature serves as a\ncompact description (dimension reduction) of a path. In presenting its\ntheoretical properties, we assume only familiarity with classical real analysis\nand integration, and supplement theory with straightforward examples. We also\nmention several advanced topics, including the role of the signature in rough\npath theory. In the second part, we present practical applications of the\nsignature to the area of machine learning. The signature method is a\nnon-parametric way of transforming data into a set of features that can be used\nin machine learning tasks. In this method, data are converted into\nmulti-dimensional paths, by means of embedding algorithms, of which the\nsignature is then computed. We describe this pipeline in detail, making a link\nwith the properties of the signature presented in the first part. We\nfurthermore review some of the developments of the signature method in machine\nlearning and, as an illustrative example, present a detailed application of the\nmethod to handwritten digit classification. \n\n"}
{"id": "1603.04119", "contents": "Title: Exploratory Gradient Boosting for Reinforcement Learning in Complex\n  Domains Abstract: High-dimensional observations and complex real-world dynamics present major\nchallenges in reinforcement learning for both function approximation and\nexploration. We address both of these challenges with two complementary\ntechniques: First, we develop a gradient-boosting style, non-parametric\nfunction approximator for learning on $Q$-function residuals. And second, we\npropose an exploration strategy inspired by the principles of state abstraction\nand information acquisition under uncertainty. We demonstrate the empirical\neffectiveness of these techniques, first, as a preliminary check, on two\nstandard tasks (Blackjack and $n$-Chain), and then on two much larger and more\nrealistic tasks with high-dimensional observation spaces. Specifically, we\nintroduce two benchmarks built within the game Minecraft where the observations\nare pixel arrays of the agent's visual field. A combination of our two\nalgorithmic techniques performs competitively on the standard\nreinforcement-learning tasks while consistently and substantially outperforming\nbaselines on the two tasks with high-dimensional observation spaces. The new\nfunction approximator, exploration strategy, and evaluation benchmarks are each\nof independent interest in the pursuit of reinforcement-learning methods that\nscale to real-world domains. \n\n"}
{"id": "1603.06159", "contents": "Title: Fast Incremental Method for Nonconvex Optimization Abstract: We analyze a fast incremental aggregated gradient method for optimizing\nnonconvex problems of the form $\\min_x \\sum_i f_i(x)$. Specifically, we analyze\nthe SAGA algorithm within an Incremental First-order Oracle framework, and show\nthat it converges to a stationary point provably faster than both gradient\ndescent and stochastic gradient descent. We also discuss a Polyak's special\nclass of nonconvex problems for which SAGA converges at a linear rate to the\nglobal optimum. Finally, we analyze the practically valuable regularized and\nminibatch variants of SAGA. To our knowledge, this paper presents the first\nanalysis of fast convergence for an incremental aggregated gradient method for\nnonconvex problems. \n\n"}
{"id": "1603.06881", "contents": "Title: Feeling the Bern: Adaptive Estimators for Bernoulli Probabilities of\n  Pairwise Comparisons Abstract: We study methods for aggregating pairwise comparison data in order to\nestimate outcome probabilities for future comparisons among a collection of n\nitems. Working within a flexible framework that imposes only a form of strong\nstochastic transitivity (SST), we introduce an adaptivity index defined by the\nindifference sets of the pairwise comparison probabilities. In addition to\nmeasuring the usual worst-case risk of an estimator, this adaptivity index also\ncaptures the extent to which the estimator adapts to instance-specific\ndifficulty relative to an oracle estimator. We prove three main results that\ninvolve this adaptivity index and different algorithms. First, we propose a\nthree-step estimator termed Count-Randomize-Least squares (CRL), and show that\nit has adaptivity index upper bounded as $\\sqrt{n}$ up to logarithmic factors.\nWe then show that that conditional on the hardness of planted clique, no\ncomputationally efficient estimator can achieve an adaptivity index smaller\nthan $\\sqrt{n}$. Second, we show that a regularized least squares estimator can\nachieve a poly-logarithmic adaptivity index, thereby demonstrating a\n$\\sqrt{n}$-gap between optimal and computationally achievable adaptivity.\nFinally, we prove that the standard least squares estimator, which is known to\nbe optimally adaptive in several closely related problems, fails to adapt in\nthe context of estimating pairwise probabilities. \n\n"}
{"id": "1603.07834", "contents": "Title: An end-to-end convolutional selective autoencoder approach to Soybean\n  Cyst Nematode eggs detection Abstract: This paper proposes a novel selective autoencoder approach within the\nframework of deep convolutional networks. The crux of the idea is to train a\ndeep convolutional autoencoder to suppress undesired parts of an image frame\nwhile allowing the desired parts resulting in efficient object detection. The\nefficacy of the framework is demonstrated on a critical plant science problem.\nIn the United States, approximately $1 billion is lost per annum due to a\nnematode infection on soybean plants. Currently, plant-pathologists rely on\nlabor-intensive and time-consuming identification of Soybean Cyst Nematode\n(SCN) eggs in soil samples via manual microscopy. The proposed framework\nattempts to significantly expedite the process by using a series of manually\nlabeled microscopic images for training followed by automated high-throughput\negg detection. The problem is particularly difficult due to the presence of a\nlarge population of non-egg particles (disturbances) in the image frames that\nare very similar to SCN eggs in shape, pose and illumination. Therefore, the\nselective autoencoder is trained to learn unique features related to the\ninvariant shapes and sizes of the SCN eggs without handcrafting. After that, a\ncomposite non-maximum suppression and differencing is applied at the\npost-processing stage. \n\n"}
{"id": "1603.08482", "contents": "Title: Estimating Mixture Models via Mixtures of Polynomials Abstract: Mixture modeling is a general technique for making any simple model more\nexpressive through weighted combination. This generality and simplicity in part\nexplains the success of the Expectation Maximization (EM) algorithm, in which\nupdates are easy to derive for a wide class of mixture models. However, the\nlikelihood of a mixture model is non-convex, so EM has no known global\nconvergence guarantees. Recently, method of moments approaches offer global\nguarantees for some mixture models, but they do not extend easily to the range\nof mixture models that exist. In this work, we present Polymom, an unifying\nframework based on method of moments in which estimation procedures are easily\nderivable, just as in EM. Polymom is applicable when the moments of a single\nmixture component are polynomials of the parameters. Our key observation is\nthat the moments of the mixture model are a mixture of these polynomials, which\nallows us to cast estimation as a Generalized Moment Problem. We solve its\nrelaxations using semidefinite optimization, and then extract parameters using\nideas from computer algebra. This framework allows us to draw insights and\napply tools from convex optimization, computer algebra and the theory of\nmoments to study problems in statistical estimation. \n\n"}
{"id": "1603.08988", "contents": "Title: Towards Practical Bayesian Parameter and State Estimation Abstract: Joint state and parameter estimation is a core problem for dynamic Bayesian\nnetworks. Although modern probabilistic inference toolkits make it relatively\neasy to specify large and practically relevant probabilistic models, the silver\nbullet---an efficient and general online inference algorithm for such\nproblems---remains elusive, forcing users to write special-purpose code for\neach application. We propose a novel blackbox algorithm -- a hybrid of particle\nfiltering for state variables and assumed density filtering for parameter\nvariables. It has following advantages: (a) it is efficient due to its online\nnature, and (b) it is applicable to both discrete and continuous parameter\nspaces . On a variety of toy and real models, our system is able to generate\nmore accurate results within a fixed computation budget. This preliminary\nevidence indicates that the proposed approach is likely to be of practical use. \n\n"}
{"id": "1604.00974", "contents": "Title: Writer-independent Feature Learning for Offline Signature Verification\n  using Deep Convolutional Neural Networks Abstract: Automatic Offline Handwritten Signature Verification has been researched over\nthe last few decades from several perspectives, using insights from graphology,\ncomputer vision, signal processing, among others. In spite of the advancements\non the field, building classifiers that can separate between genuine signatures\nand skilled forgeries (forgeries made targeting a particular signature) is\nstill hard. We propose approaching the problem from a feature learning\nperspective. Our hypothesis is that, in the absence of a good model of the data\ngeneration process, it is better to learn the features from data, instead of\nusing hand-crafted features that have no resemblance to the signature\ngeneration process. To this end, we use Deep Convolutional Neural Networks to\nlearn features in a writer-independent format, and use this model to obtain a\nfeature representation on another set of users, where we train writer-dependent\nclassifiers. We tested our method in two datasets: GPDS-960 and Brazilian\nPUC-PR. Our experimental results show that the features learned in a subset of\nthe users are discriminative for the other users, including across different\ndatasets, reaching close to the state-of-the-art in the GPDS dataset, and\nimproving the state-of-the-art in the Brazilian PUC-PR dataset. \n\n"}
{"id": "1604.02275", "contents": "Title: Online Open World Recognition Abstract: As we enter into the big data age and an avalanche of images have become\nreadily available, recognition systems face the need to move from close, lab\nsettings where the number of classes and training data are fixed, to dynamic\nscenarios where the number of categories to be recognized grows continuously\nover time, as well as new data providing useful information to update the\nsystem. Recent attempts, like the open world recognition framework, tried to\ninject dynamics into the system by detecting new unknown classes and adding\nthem incrementally, while at the same time continuously updating the models for\nthe known classes. incrementally adding new classes and detecting instances\nfrom unknown classes, while at the same time continuously updating the models\nfor the known classes. In this paper we argue that to properly capture the\nintrinsic dynamic of open world recognition, it is necessary to add to these\naspects (a) the incremental learning of the underlying metric, (b) the\nincremental estimate of confidence thresholds for the unknown classes, and (c)\nthe use of local learning to precisely describe the space of classes. We extend\nthree existing metric learning algorithms towards these goals by using online\nmetric learning. Experimentally we validate our approach on two large-scale\ndatasets in different learning scenarios. For all these scenarios our proposed\nmethods outperform their non-online counterparts. We conclude that local and\nonline learning is important to capture the full dynamics of open world\nrecognition. \n\n"}
{"id": "1604.03227", "contents": "Title: Recurrent Attentional Networks for Saliency Detection Abstract: Convolutional-deconvolution networks can be adopted to perform end-to-end\nsaliency detection. But, they do not work well with objects of multiple scales.\nTo overcome such a limitation, in this work, we propose a recurrent attentional\nconvolutional-deconvolution network (RACDNN). Using spatial transformer and\nrecurrent network units, RACDNN is able to iteratively attend to selected image\nsub-regions to perform saliency refinement progressively. Besides tackling the\nscale problem, RACDNN can also learn context-aware features from past\niterations to enhance saliency refinement in future iterations. Experiments on\nseveral challenging saliency detection datasets validate the effectiveness of\nRACDNN, and show that RACDNN outperforms state-of-the-art saliency detection\nmethods. \n\n"}
{"id": "1604.05417", "contents": "Title: Triplet Probabilistic Embedding for Face Verification and Clustering Abstract: Despite significant progress made over the past twenty five years,\nunconstrained face verification remains a challenging problem. This paper\nproposes an approach that couples a deep CNN-based approach with a\nlow-dimensional discriminative embedding learned using triplet probability\nconstraints to solve the unconstrained face verification problem. Aside from\nyielding performance improvements, this embedding provides significant\nadvantages in terms of memory and for post-processing operations like subject\nspecific clustering. Experiments on the challenging IJB-A dataset show that the\nproposed algorithm performs comparably or better than the state of the art\nmethods in verification and identification metrics, while requiring much less\ntraining data and training time. The superior performance of the proposed\nmethod on the CFP dataset shows that the representation learned by our deep CNN\nis robust to extreme pose variation. Furthermore, we demonstrate the robustness\nof the deep features to challenges including age, pose, blur and clutter by\nperforming simple clustering experiments on both IJB-A and LFW datasets. \n\n"}
{"id": "1605.00042", "contents": "Title: Improved Sparse Low-Rank Matrix Estimation Abstract: We address the problem of estimating a sparse low-rank matrix from its noisy\nobservation. We propose an objective function consisting of a data-fidelity\nterm and two parameterized non-convex penalty functions. Further, we show how\nto set the parameters of the non-convex penalty functions, in order to ensure\nthat the objective function is strictly convex. The proposed objective function\nbetter estimates sparse low-rank matrices than a convex method which utilizes\nthe sum of the nuclear norm and the $\\ell_1$ norm. We derive an algorithm (as\nan instance of ADMM) to solve the proposed problem, and guarantee its\nconvergence provided the scalar augmented Lagrangian parameter is set\nappropriately. We demonstrate the proposed method for denoising an audio signal\nand an adjacency matrix representing protein interactions in the `Escherichia\ncoli' bacteria. \n\n"}
{"id": "1605.00252", "contents": "Title: Fast Rates for General Unbounded Loss Functions: from ERM to Generalized\n  Bayes Abstract: We present new excess risk bounds for general unbounded loss functions\nincluding log loss and squared loss, where the distribution of the losses may\nbe heavy-tailed. The bounds hold for general estimators, but they are optimized\nwhen applied to $\\eta$-generalized Bayesian, MDL, and empirical risk\nminimization estimators. In the case of log loss, the bounds imply convergence\nrates for generalized Bayesian inference under misspecification in terms of a\ngeneralization of the Hellinger metric as long as the learning rate $\\eta$ is\nset correctly. For general loss functions, our bounds rely on two separate\nconditions: the $v$-GRIP (generalized reversed information projection)\nconditions, which control the lower tail of the excess loss; and the newly\nintroduced witness condition, which controls the upper tail. The parameter $v$\nin the $v$-GRIP conditions determines the achievable rate and is akin to the\nexponent in the Tsybakov margin condition and the Bernstein condition for\nbounded losses, which the $v$-GRIP conditions generalize; favorable $v$ in\ncombination with small model complexity leads to $\\tilde{O}(1/n)$ rates. The\nwitness condition allows us to connect the excess risk to an \"annealed\" version\nthereof, by which we generalize several previous results connecting Hellinger\nand R\\'enyi divergence to KL divergence. \n\n"}
{"id": "1605.01384", "contents": "Title: Multilevel Monte Carlo methods for the approximation of invariant\n  measures of stochastic differential equations Abstract: We develop a framework that allows the use of the multi-level Monte Carlo\n(MLMC) methodology (Giles2015) to calculate expectations with respect to the\ninvariant measure of an ergodic SDE. In that context, we study the\n(over-damped) Langevin equations with a strongly concave potential. We show\nthat, when appropriate contracting couplings for the numerical integrators are\navailable, one can obtain a uniform in time estimate of the MLMC variance in\ncontrast to the majority of the results in the MLMC literature. As a\nconsequence, a root mean square error of $\\mathcal{O}(\\varepsilon)$ is achieved\nwith $\\mathcal{O}(\\varepsilon^{-2})$ complexity on par with Markov Chain Monte\nCarlo (MCMC) methods, which however can be computationally intensive when\napplied to large data sets. Finally, we present a multi-level version of the\nrecently introduced Stochastic Gradient Langevin Dynamics (SGLD) method\n(Welling and Teh, 2011) built for large datasets applications. We show that\nthis is the first stochastic gradient MCMC method with complexity\n$\\mathcal{O}(\\varepsilon^{-2}|\\log {\\varepsilon}|^{3})$, in contrast to the\ncomplexity $\\mathcal{O}(\\varepsilon^{-3})$ of currently available methods.\nNumerical experiments confirm our theoretical findings. \n\n"}
{"id": "1605.01749", "contents": "Title: Rank Ordered Autoencoders Abstract: A new method for the unsupervised learning of sparse representations using\nautoencoders is proposed and implemented by ordering the output of the hidden\nunits by their activation value and progressively reconstructing the input in\nthis order. This can be done efficiently in parallel with the use of cumulative\nsums and sorting only slightly increasing the computational costs. Minimizing\nthe difference of this progressive reconstruction with respect to the input can\nbe seen as minimizing the number of active output units required for the\nreconstruction of the input. The model thus learns to reconstruct optimally\nusing the least number of active output units. This leads to high sparsity\nwithout the need for extra hyperparameters, the amount of sparsity is instead\nimplicitly learned by minimizing this progressive reconstruction error. Results\nof the trained model are given for patches of the CIFAR10 dataset, showing\nrapid convergence of features and extremely sparse output activations while\nmaintaining a minimal reconstruction error and showing extreme robustness to\noverfitting. Additionally the reconstruction as function of number of active\nunits is presented which shows the autoencoder learns a rank order code over\nthe input where the highest ranked units correspond to the highest decrease in\nreconstruction error. \n\n"}
{"id": "1605.02470", "contents": "Title: Randomized Kaczmarz for Rank Aggregation from Pairwise Comparisons Abstract: We revisit the problem of inferring the overall ranking among entities in the\nframework of Bradley-Terry-Luce (BTL) model, based on available empirical data\non pairwise preferences. By a simple transformation, we can cast the problem as\nthat of solving a noisy linear system, for which a ready algorithm is available\nin the form of the randomized Kaczmarz method. This scheme is provably\nconvergent, has excellent empirical performance, and is amenable to on-line,\ndistributed and asynchronous variants. Convergence, convergence rate, and error\nanalysis of the proposed algorithm are presented and several numerical\nexperiments are conducted whose results validate our theoretical findings. \n\n"}
{"id": "1605.06359", "contents": "Title: Learning to Discover Sparse Graphical Models Abstract: We consider structure discovery of undirected graphical models from\nobservational data. Inferring likely structures from few examples is a complex\ntask often requiring the formulation of priors and sophisticated inference\nprocedures. Popular methods rely on estimating a penalized maximum likelihood\nof the precision matrix. However, in these approaches structure recovery is an\nindirect consequence of the data-fit term, the penalty can be difficult to\nadapt for domain-specific knowledge, and the inference is computationally\ndemanding. By contrast, it may be easier to generate training samples of data\nthat arise from graphs with the desired structure properties. We propose here\nto leverage this latter source of information as training data to learn a\nfunction, parametrized by a neural network that maps empirical covariance\nmatrices to estimated graph structures. Learning this function brings two\nbenefits: it implicitly models the desired structure or sparsity properties to\nform suitable priors, and it can be tailored to the specific problem of edge\nstructure discovery, rather than maximizing data likelihood. Applying this\nframework, we find our learnable graph-discovery method trained on synthetic\ndata generalizes well: identifying relevant edges in both synthetic and real\ndata, completely unknown at training time. We find that on genetics, brain\nimaging, and simulation data we obtain performance generally superior to\nanalytical methods. \n\n"}
{"id": "1605.07139", "contents": "Title: Fairness in Learning: Classic and Contextual Bandits Abstract: We introduce the study of fairness in multi-armed bandit problems. Our\nfairness definition can be interpreted as demanding that given a pool of\napplicants (say, for college admission or mortgages), a worse applicant is\nnever favored over a better one, despite a learning algorithm's uncertainty\nover the true payoffs. We prove results of two types.\n  First, in the important special case of the classic stochastic bandits\nproblem (i.e., in which there are no contexts), we provide a provably fair\nalgorithm based on \"chained\" confidence intervals, and provide a cumulative\nregret bound with a cubic dependence on the number of arms. We further show\nthat any fair algorithm must have such a dependence. When combined with regret\nbounds for standard non-fair algorithms such as UCB, this proves a strong\nseparation between fair and unfair learning, which extends to the general\ncontextual case.\n  In the general contextual case, we prove a tight connection between fairness\nand the KWIK (Knows What It Knows) learning model: a KWIK algorithm for a class\nof functions can be transformed into a provably fair contextual bandit\nalgorithm, and conversely any fair contextual bandit algorithm can be\ntransformed into a KWIK learning algorithm. This tight connection allows us to\nprovide a provably fair algorithm for the linear contextual bandit problem with\na polynomial dependence on the dimension, and to show (for a different class of\nfunctions) a worst-case exponential gap in regret between fair and non-fair\nlearning algorithms \n\n"}
{"id": "1605.07371", "contents": "Title: Semiparametric energy-based probabilistic models Abstract: Probabilistic models can be defined by an energy function, where the\nprobability of each state is proportional to the exponential of the state's\nnegative energy. This paper considers a generalization of energy-based models\nin which the probability of a state is proportional to an arbitrary positive,\nstrictly decreasing, and twice differentiable function of the state's energy.\nThe precise shape of the nonlinear map from energies to unnormalized\nprobabilities has to be learned from data together with the parameters of the\nenergy function. As a case study we show that the above generalization of a\nfully visible Boltzmann machine yields an accurate model of neural activity of\nretinal ganglion cells. We attribute this success to the model's ability to\neasily capture distributions whose probabilities span a large dynamic range, a\npossible consequence of latent variables that globally couple the system.\nSimilar features have recently been observed in many datasets, suggesting that\nour new method has wide applicability. \n\n"}
{"id": "1605.07422", "contents": "Title: Computing Web-scale Topic Models using an Asynchronous Parameter Server Abstract: Topic models such as Latent Dirichlet Allocation (LDA) have been widely used\nin information retrieval for tasks ranging from smoothing and feedback methods\nto tools for exploratory search and discovery. However, classical methods for\ninferring topic models do not scale up to the massive size of today's publicly\navailable Web-scale data sets. The state-of-the-art approaches rely on custom\nstrategies, implementations and hardware to facilitate their asynchronous,\ncommunication-intensive workloads.\n  We present APS-LDA, which integrates state-of-the-art topic modeling with\ncluster computing frameworks such as Spark using a novel asynchronous parameter\nserver. Advantages of this integration include convenient usage of existing\ndata processing pipelines and eliminating the need for disk writes as data can\nbe kept in memory from start to finish. Our goal is not to outperform highly\ncustomized implementations, but to propose a general high-performance topic\nmodeling framework that can easily be used in today's data processing\npipelines. We compare APS-LDA to the existing Spark LDA implementations and\nshow that our system can, on a 480-core cluster, process up to 135 times more\ndata and 10 times more topics without sacrificing model quality. \n\n"}
{"id": "1605.07596", "contents": "Title: Local Minimax Complexity of Stochastic Convex Optimization Abstract: We extend the traditional worst-case, minimax analysis of stochastic convex\noptimization by introducing a localized form of minimax complexity for\nindividual functions. Our main result gives function-specific lower and upper\nbounds on the number of stochastic subgradient evaluations needed to optimize\neither the function or its \"hardest local alternative\" to a given numerical\nprecision. The bounds are expressed in terms of a localized and computational\nanalogue of the modulus of continuity that is central to statistical minimax\nanalysis. We show how the computational modulus of continuity can be explicitly\ncalculated in concrete cases, and relates to the curvature of the function at\nthe optimum. We also prove a superefficiency result that demonstrates it is a\nmeaningful benchmark, acting as a computational analogue of the Fisher\ninformation in statistical estimation. The nature and practical implications of\nthe results are demonstrated in simulations. \n\n"}
{"id": "1605.08233", "contents": "Title: Stochastic Variance Reduced Riemannian Eigensolver Abstract: We study the stochastic Riemannian gradient algorithm for matrix\neigen-decomposition. The state-of-the-art stochastic Riemannian algorithm\nrequires the learning rate to decay to zero and thus suffers from slow\nconvergence and sub-optimal solutions. In this paper, we address this issue by\ndeploying the variance reduction (VR) technique of stochastic gradient descent\n(SGD). The technique was originally developed to solve convex problems in the\nEuclidean space. We generalize it to Riemannian manifolds and realize it to\nsolve the non-convex eigen-decomposition problem. We are the first to propose\nand analyze the generalization of SVRG to Riemannian manifolds. Specifically,\nwe propose the general variance reduction form, SVRRG, in the framework of the\nstochastic Riemannian gradient optimization. It's then specialized to the\nproblem with eigensolvers and induces the SVRRG-EIGS algorithm. We provide a\nnovel and elegant theoretical analysis on this algorithm. The theory shows that\na fixed learning rate can be used in the Riemannian setting with an exponential\nglobal convergence rate guaranteed. The theoretical results make a significant\nimprovement over existing studies, with the effectiveness empirically verified. \n\n"}
{"id": "1605.08833", "contents": "Title: Muffled Semi-Supervised Learning Abstract: We explore a novel approach to semi-supervised learning. This approach is\ncontrary to the common approach in that the unlabeled examples serve to\n\"muffle,\" rather than enhance, the guidance provided by the labeled examples.\nWe provide several variants of the basic algorithm and show experimentally that\nthey can achieve significantly higher AUC than boosted trees, random forests\nand logistic regression when unlabeled examples are available. \n\n"}
{"id": "1605.09046", "contents": "Title: TripleSpin - a generic compact paradigm for fast machine learning\n  computations Abstract: We present a generic compact computational framework relying on structured\nrandom matrices that can be applied to speed up several machine learning\nalgorithms with almost no loss of accuracy. The applications include new fast\nLSH-based algorithms, efficient kernel computations via random feature maps,\nconvex optimization algorithms, quantization techniques and many more. Certain\nmodels of the presented paradigm are even more compressible since they apply\nonly bit matrices. This makes them suitable for deploying on mobile devices.\nAll our findings come with strong theoretical guarantees. In particular, as a\nbyproduct of the presented techniques and by using relatively new\nBerry-Esseen-type CLT for random vectors, we give the first theoretical\nguarantees for one of the most efficient existing LSH algorithms based on the\n$\\textbf{HD}_{3}\\textbf{HD}_{2}\\textbf{HD}_{1}$ structured matrix (\"Practical\nand Optimal LSH for Angular Distance\"). These guarantees as well as theoretical\nresults for other aforementioned applications follow from the same general\ntheoretical principle that we present in the paper. Our structured family\ncontains as special cases all previously considered structured schemes,\nincluding the recently introduced $P$-model. Experimental evaluation confirms\nthe accuracy and efficiency of TripleSpin matrices. \n\n"}
{"id": "1606.00915", "contents": "Title: DeepLab: Semantic Image Segmentation with Deep Convolutional Nets,\n  Atrous Convolution, and Fully Connected CRFs Abstract: In this work we address the task of semantic image segmentation with Deep\nLearning and make three main contributions that are experimentally shown to\nhave substantial practical merit. First, we highlight convolution with\nupsampled filters, or 'atrous convolution', as a powerful tool in dense\nprediction tasks. Atrous convolution allows us to explicitly control the\nresolution at which feature responses are computed within Deep Convolutional\nNeural Networks. It also allows us to effectively enlarge the field of view of\nfilters to incorporate larger context without increasing the number of\nparameters or the amount of computation. Second, we propose atrous spatial\npyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP\nprobes an incoming convolutional feature layer with filters at multiple\nsampling rates and effective fields-of-views, thus capturing objects as well as\nimage context at multiple scales. Third, we improve the localization of object\nboundaries by combining methods from DCNNs and probabilistic graphical models.\nThe commonly deployed combination of max-pooling and downsampling in DCNNs\nachieves invariance but has a toll on localization accuracy. We overcome this\nby combining the responses at the final DCNN layer with a fully connected\nConditional Random Field (CRF), which is shown both qualitatively and\nquantitatively to improve localization performance. Our proposed \"DeepLab\"\nsystem sets the new state-of-art at the PASCAL VOC-2012 semantic image\nsegmentation task, reaching 79.7% mIOU in the test set, and advances the\nresults on three other datasets: PASCAL-Context, PASCAL-Person-Part, and\nCityscapes. All of our code is made publicly available online. \n\n"}
{"id": "1606.00925", "contents": "Title: Convolutional Imputation of Matrix Networks Abstract: A matrix network is a family of matrices, with relatedness modeled by a\nweighted graph. We consider the task of completing a partially observed matrix\nnetwork. We assume a novel sampling scheme where a fraction of matrices might\nbe completely unobserved. How can we recover the entire matrix network from\nincomplete observations? This mathematical problem arises in many applications\nincluding medical imaging and social networks.\n  To recover the matrix network, we propose a structural assumption that the\nmatrices have a graph Fourier transform which is low-rank. We formulate a\nconvex optimization problem and prove an exact recovery guarantee for the\noptimization problem. Furthermore, we numerically characterize the exact\nrecovery regime for varying rank and sampling rate and discover a new phase\ntransition phenomenon. Then we give an iterative imputation algorithm to\nefficiently solve the optimization problem and complete large scale matrix\nnetworks. We demonstrate the algorithm with a variety of applications such as\nMRI and Facebook user network. \n\n"}
{"id": "1606.02147", "contents": "Title: ENet: A Deep Neural Network Architecture for Real-Time Semantic\n  Segmentation Abstract: The ability to perform pixel-wise semantic segmentation in real-time is of\nparamount importance in mobile applications. Recent deep neural networks aimed\nat this task have the disadvantage of requiring a large number of floating\npoint operations and have long run-times that hinder their usability. In this\npaper, we propose a novel deep neural network architecture named ENet\n(efficient neural network), created specifically for tasks requiring low\nlatency operation. ENet is up to 18$\\times$ faster, requires 75$\\times$ less\nFLOPs, has 79$\\times$ less parameters, and provides similar or better accuracy\nto existing models. We have tested it on CamVid, Cityscapes and SUN datasets\nand report on comparisons with existing state-of-the-art methods, and the\ntrade-offs between accuracy and processing time of a network. We present\nperformance measurements of the proposed architecture on embedded systems and\nsuggest possible software improvements that could make ENet even faster. \n\n"}
{"id": "1606.03141", "contents": "Title: Mutual Exclusivity Loss for Semi-Supervised Deep Learning Abstract: In this paper we consider the problem of semi-supervised learning with deep\nConvolutional Neural Networks (ConvNets). Semi-supervised learning is motivated\non the observation that unlabeled data is cheap and can be used to improve the\naccuracy of classifiers. In this paper we propose an unsupervised\nregularization term that explicitly forces the classifier's prediction for\nmultiple classes to be mutually-exclusive and effectively guides the decision\nboundary to lie on the low density space between the manifolds corresponding to\ndifferent classes of data. Our proposed approach is general and can be used\nwith any backpropagation-based learning method. We show through different\nexperiments that our method can improve the object recognition performance of\nConvNets using unlabeled data. \n\n"}
{"id": "1606.03498", "contents": "Title: Improved Techniques for Training GANs Abstract: We present a variety of new architectural features and training procedures\nthat we apply to the generative adversarial networks (GANs) framework. We focus\non two applications of GANs: semi-supervised learning, and the generation of\nimages that humans find visually realistic. Unlike most work on generative\nmodels, our primary goal is not to train a model that assigns high likelihood\nto test data, nor do we require the model to be able to learn well without\nusing any labels. Using our new techniques, we achieve state-of-the-art results\nin semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated\nimages are of high quality as confirmed by a visual Turing test: our model\ngenerates MNIST samples that humans cannot distinguish from real data, and\nCIFAR-10 samples that yield a human error rate of 21.3%. We also present\nImageNet samples with unprecedented resolution and show that our methods enable\nthe model to learn recognizable features of ImageNet classes. \n\n"}
{"id": "1606.03841", "contents": "Title: Efficient Learning with a Family of Nonconvex Regularizers by\n  Redistributing Nonconvexity Abstract: The use of convex regularizers allows for easy optimization, though they\noften produce biased estimation and inferior prediction performance. Recently,\nnonconvex regularizers have attracted a lot of attention and outperformed\nconvex ones. However, the resultant optimization problem is much harder. In\nthis paper, for a large class of nonconvex regularizers, we propose to move the\nnonconvexity from the regularizer to the loss. The nonconvex regularizer is\nthen transformed to a familiar convex regularizer, while the resultant loss\nfunction can still be guaranteed to be smooth. Learning with the convexified\nregularizer can be performed by existing efficient algorithms originally\ndesigned for convex regularizers (such as the proximal algorithm, Frank-Wolfe\nalgorithm, alternating direction method of multipliers and stochastic gradient\ndescent). Extensions are made when the convexified regularizer does not have\nclosed-form proximal step, and when the loss function is nonconvex, nonsmooth.\nExtensive experiments on a variety of machine learning application scenarios\nshow that optimizing the transformed problem is much faster than running the\nstate-of-the-art on the original problem. \n\n"}
{"id": "1606.05027", "contents": "Title: Learning Optimal Interventions Abstract: Our goal is to identify beneficial interventions from observational data. We\nconsider interventions that are narrowly focused (impacting few covariates) and\nmay be tailored to each individual or globally enacted over a population. For\napplications where harmful intervention is drastically worse than proposing no\nchange, we propose a conservative definition of the optimal intervention.\nAssuming the underlying relationship remains invariant under intervention, we\ndevelop efficient algorithms to identify the optimal intervention policy from\nlimited data and provide theoretical guarantees for our approach in a Gaussian\nProcess setting. Although our methods assume covariates can be precisely\nadjusted, they remain capable of improving outcomes in misspecified settings\nwhere interventions incur unintentional downstream effects. Empirically, our\napproach identifies good interventions in two practical applications: gene\nperturbation and writing improvement. \n\n"}
{"id": "1606.07153", "contents": "Title: Fast robustness quantification with variational Bayes Abstract: Bayesian hierarchical models are increasing popular in economics. When using\nhierarchical models, it is useful not only to calculate posterior expectations,\nbut also to measure the robustness of these expectations to reasonable\nalternative prior choices. We use variational Bayes and linear response methods\nto provide fast, accurate posterior means and robustness measures with an\napplication to measuring the effectiveness of microcredit in the developing\nworld. \n\n"}
{"id": "1606.07326", "contents": "Title: DropNeuron: Simplifying the Structure of Deep Neural Networks Abstract: Deep learning using multi-layer neural networks (NNs) architecture manifests\nsuperb power in modern machine learning systems. The trained Deep Neural\nNetworks (DNNs) are typically large. The question we would like to address is\nwhether it is possible to simplify the NN during training process to achieve a\nreasonable performance within an acceptable computational time. We presented a\nnovel approach of optimising a deep neural network through regularisation of\nnet- work architecture. We proposed regularisers which support a simple\nmechanism of dropping neurons during a network training process. The method\nsupports the construction of a simpler deep neural networks with compatible\nperformance with its simplified version. As a proof of concept, we evaluate the\nproposed method with examples including sparse linear regression, deep\nautoencoder and convolutional neural network. The valuations demonstrate\nexcellent performance.\n  The code for this work can be found in\nhttp://www.github.com/panweihit/DropNeuron \n\n"}
{"id": "1606.09282", "contents": "Title: Learning without Forgetting Abstract: When building a unified vision system or gradually adding new capabilities to\na system, the usual assumption is that training data for all tasks is always\navailable. However, as the number of tasks grows, storing and retraining on\nsuch data becomes infeasible. A new problem arises where we add new\ncapabilities to a Convolutional Neural Network (CNN), but the training data for\nits existing capabilities are unavailable. We propose our Learning without\nForgetting method, which uses only new task data to train the network while\npreserving the original capabilities. Our method performs favorably compared to\ncommonly used feature extraction and fine-tuning adaption techniques and\nperforms similarly to multitask learning that uses original task data we assume\nunavailable. A more surprising observation is that Learning without Forgetting\nmay be able to replace fine-tuning with similar old and new task datasets for\nimproved new task performance. \n\n"}
{"id": "1606.09375", "contents": "Title: Convolutional Neural Networks on Graphs with Fast Localized Spectral\n  Filtering Abstract: In this work, we are interested in generalizing convolutional neural networks\n(CNNs) from low-dimensional regular grids, where image, video and speech are\nrepresented, to high-dimensional irregular domains, such as social networks,\nbrain connectomes or words' embedding, represented by graphs. We present a\nformulation of CNNs in the context of spectral graph theory, which provides the\nnecessary mathematical background and efficient numerical schemes to design\nfast localized convolutional filters on graphs. Importantly, the proposed\ntechnique offers the same linear computational complexity and constant learning\ncomplexity as classical CNNs, while being universal to any graph structure.\nExperiments on MNIST and 20NEWS demonstrate the ability of this novel deep\nlearning system to learn local, stationary, and compositional features on\ngraphs. \n\n"}
{"id": "1607.01231", "contents": "Title: Stochastic Quasi-Newton Methods for Nonconvex Stochastic Optimization Abstract: In this paper we study stochastic quasi-Newton methods for nonconvex\nstochastic optimization, where we assume that noisy information about the\ngradients of the objective function is available via a stochastic first-order\noracle (SFO). We propose a general framework for such methods, for which we\nprove almost sure convergence to stationary points and analyze its worst-case\niteration complexity. When a randomly chosen iterate is returned as the output\nof such an algorithm, we prove that in the worst-case, the SFO-calls complexity\nis $O(\\epsilon^{-2})$ to ensure that the expectation of the squared norm of the\ngradient is smaller than the given accuracy tolerance $\\epsilon$. We also\npropose a specific algorithm, namely a stochastic damped L-BFGS (SdLBFGS)\nmethod, that falls under the proposed framework. {Moreover, we incorporate the\nSVRG variance reduction technique into the proposed SdLBFGS method, and analyze\nits SFO-calls complexity. Numerical results on a nonconvex binary\nclassification problem using SVM, and a multiclass classification problem using\nneural networks are reported. \n\n"}
{"id": "1607.05241", "contents": "Title: Imitation Learning with Recurrent Neural Networks Abstract: We present a novel view that unifies two frameworks that aim to solve\nsequential prediction problems: learning to search (L2S) and recurrent neural\nnetworks (RNN). We point out equivalences between elements of the two\nframeworks. By complementing what is missing from one framework comparing to\nthe other, we introduce a more advanced imitation learning framework that, on\none hand, augments L2S s notion of search space and, on the other hand,\nenhances RNNs training procedure to be more robust to compounding errors\narising from training on highly correlated examples. \n\n"}
{"id": "1607.05691", "contents": "Title: Information-theoretical label embeddings for large-scale image\n  classification Abstract: We present a method for training multi-label, massively multi-class image\nclassification models, that is faster and more accurate than supervision via a\nsigmoid cross-entropy loss (logistic regression). Our method consists in\nembedding high-dimensional sparse labels onto a lower-dimensional dense sphere\nof unit-normed vectors, and treating the classification problem as a cosine\nproximity regression problem on this sphere. We test our method on a dataset of\n300 million high-resolution images with 17,000 labels, where it yields\nconsiderably faster convergence, as well as a 7% higher mean average precision\ncompared to logistic regression. \n\n"}
{"id": "1608.00354", "contents": "Title: hdm: High-Dimensional Metrics Abstract: In this article the package High-dimensional Metrics (\\texttt{hdm}) is\nintroduced. It is a collection of statistical methods for estimation and\nquantification of uncertainty in high-dimensional approximately sparse models.\nIt focuses on providing confidence intervals and significance testing for\n(possibly many) low-dimensional subcomponents of the high-dimensional parameter\nvector. Efficient estimators and uniformly valid confidence intervals for\nregression coefficients on target variables (e.g., treatment or policy\nvariable) in a high-dimensional approximately sparse regression model, for\naverage treatment effect (ATE) and average treatment effect for the treated\n(ATET), as well for extensions of these parameters to the endogenous setting\nare provided. Theory grounded, data-driven methods for selecting the\npenalization parameter in Lasso regressions under heteroscedastic and\nnon-Gaussian errors are implemented. Moreover, joint/ simultaneous confidence\nintervals for regression coefficients of a high-dimensional sparse regression\nare implemented. Data sets which have been used in the literature and might be\nuseful for classroom demonstration and for testing new estimators are included. \n\n"}
{"id": "1608.01230", "contents": "Title: Learning a Driving Simulator Abstract: Comma.ai's approach to Artificial Intelligence for self-driving cars is based\non an agent that learns to clone driver behaviors and plans maneuvers by\nsimulating future events in the road. This paper illustrates one of our\nresearch approaches for driving simulation. One where we learn to simulate.\nHere we investigate variational autoencoders with classical and learned cost\nfunctions using generative adversarial networks for embedding road frames.\nAfterwards, we learn a transition model in the embedded space using action\nconditioned Recurrent Neural Networks. We show that our approach can keep\npredicting realistic looking video for several frames despite the transition\nmodel being optimized without a cost function in the pixel space. \n\n"}
{"id": "1608.02341", "contents": "Title: Towards Representation Learning with Tractable Probabilistic Models Abstract: Probabilistic models learned as density estimators can be exploited in\nrepresentation learning beside being toolboxes used to answer inference queries\nonly. However, how to extract useful representations highly depends on the\nparticular model involved. We argue that tractable inference, i.e. inference\nthat can be computed in polynomial time, can enable general schemes to extract\nfeatures from black box models. We plan to investigate how Tractable\nProbabilistic Models (TPMs) can be exploited to generate embeddings by random\nquery evaluations. We devise two experimental designs to assess and compare\ndifferent TPMs as feature extractors in an unsupervised representation learning\nframework. We show some experimental results on standard image datasets by\napplying such a method to Sum-Product Networks and Mixture of Trees as\ntractable models generating embeddings. \n\n"}
{"id": "1608.04236", "contents": "Title: Generative and Discriminative Voxel Modeling with Convolutional Neural\n  Networks Abstract: When working with three-dimensional data, choice of representation is key. We\nexplore voxel-based models, and present evidence for the viability of\nvoxellated representations in applications including shape modeling and object\nclassification. Our key contributions are methods for training voxel-based\nvariational autoencoders, a user interface for exploring the latent space\nlearned by the autoencoder, and a deep convolutional neural network\narchitecture for object classification. We address challenges unique to\nvoxel-based representations, and empirically evaluate our models on the\nModelNet benchmark, where we demonstrate a 51.5% relative improvement in the\nstate of the art for object classification. \n\n"}
{"id": "1608.04471", "contents": "Title: Stein Variational Gradient Descent: A General Purpose Bayesian Inference\n  Algorithm Abstract: We propose a general purpose variational inference algorithm that forms a\nnatural counterpart of gradient descent for optimization. Our method\niteratively transports a set of particles to match the target distribution, by\napplying a form of functional gradient descent that minimizes the KL\ndivergence. Empirical studies are performed on various real world models and\ndatasets, on which our method is competitive with existing state-of-the-art\nmethods. The derivation of our method is based on a new theoretical result that\nconnects the derivative of KL divergence under smooth transforms with Stein's\nidentity and a recently proposed kernelized Stein discrepancy, which is of\nindependent interest. \n\n"}
{"id": "1608.04478", "contents": "Title: A Geometrical Approach to Topic Model Estimation Abstract: In the probabilistic topic models, the quantity of interest---a low-rank\nmatrix consisting of topic vectors---is hidden in the text corpus matrix,\nmasked by noise, and the Singular Value Decomposition (SVD) is a potentially\nuseful tool for learning such a low-rank matrix. However, the connection\nbetween this low-rank matrix and the singular vectors of the text corpus matrix\nare usually complicated and hard to spell out, so how to use SVD for learning\ntopic models faces challenges. In this paper, we overcome the challenge by\nrevealing a surprising insight: there is a low-dimensional simplex structure\nwhich can be viewed as a bridge between the low-rank matrix of interest and the\nSVD of the text corpus matrix, and allows us to conveniently reconstruct the\nformer using the latter. Such an insight motivates a new SVD approach to\nlearning topic models, which we analyze with delicate random matrix theory and\nderive the rate of convergence. We support our methods and theory numerically,\nusing both simulated data and real data. \n\n"}
{"id": "1608.05275", "contents": "Title: A Tight Convex Upper Bound on the Likelihood of a Finite Mixture Abstract: The likelihood function of a finite mixture model is a non-convex function\nwith multiple local maxima and commonly used iterative algorithms such as EM\nwill converge to different solutions depending on initial conditions. In this\npaper we ask: is it possible to assess how far we are from the global maximum\nof the likelihood? Since the likelihood of a finite mixture model can grow\nunboundedly by centering a Gaussian on a single datapoint and shrinking the\ncovariance, we constrain the problem by assuming that the parameters of the\nindividual models are members of a large discrete set (e.g. estimating a\nmixture of two Gaussians where the means and variances of both Gaussians are\nmembers of a set of a million possible means and variances). For this setting\nwe show that a simple upper bound on the likelihood can be computed using\nconvex optimization and we analyze conditions under which the bound is\nguaranteed to be tight. This bound can then be used to assess the quality of\nsolutions found by EM (where the final result is projected on the discrete set)\nor any other mixture estimation algorithm. For any dataset our method allows us\nto find a finite mixture model together with a dataset-specific bound on how\nfar the likelihood of this mixture is from the global optimum of the likelihood \n\n"}
{"id": "1608.05889", "contents": "Title: Online Feature Selection with Group Structure Analysis Abstract: Online selection of dynamic features has attracted intensive interest in\nrecent years. However, existing online feature selection methods evaluate\nfeatures individually and ignore the underlying structure of feature stream.\nFor instance, in image analysis, features are generated in groups which\nrepresent color, texture and other visual information. Simply breaking the\ngroup structure in feature selection may degrade performance. Motivated by this\nfact, we formulate the problem as an online group feature selection. The\nproblem assumes that features are generated individually but there are group\nstructure in the feature stream. To the best of our knowledge, this is the\nfirst time that the correlation among feature stream has been considered in the\nonline feature selection process. To solve this problem, we develop a novel\nonline group feature selection method named OGFS. Our proposed approach\nconsists of two stages: online intra-group selection and online inter-group\nselection. In the intra-group selection, we design a criterion based on\nspectral analysis to select discriminative features in each group. In the\ninter-group selection, we utilize a linear regression model to select an\noptimal subset. This two-stage procedure continues until there are no more\nfeatures arriving or some predefined stopping conditions are met. %Our method\nhas been applied Finally, we apply our method to multiple tasks including image\nclassification %, face verification and face verification. Extensive empirical\nstudies performed on real-world and benchmark data sets demonstrate that our\nmethod outperforms other state-of-the-art online feature selection %method\nmethods. \n\n"}
{"id": "1608.05983", "contents": "Title: Inverting Variational Autoencoders for Improved Generative Accuracy Abstract: Recent advances in semi-supervised learning with deep generative models have\nshown promise in generalizing from small labeled datasets\n($\\mathbf{x},\\mathbf{y}$) to large unlabeled ones ($\\mathbf{x}$). In the case\nwhere the codomain has known structure, a large unfeatured dataset\n($\\mathbf{y}$) is potentially available. We develop a parameter-efficient, deep\nsemi-supervised generative model for the purpose of exploiting this untapped\ndata source. Empirical results show improved performance in disentangling\nlatent variable semantics as well as improved discriminative prediction on\nMartian spectroscopic and handwritten digit domains. \n\n"}
{"id": "1609.03499", "contents": "Title: WaveNet: A Generative Model for Raw Audio Abstract: This paper introduces WaveNet, a deep neural network for generating raw audio\nwaveforms. The model is fully probabilistic and autoregressive, with the\npredictive distribution for each audio sample conditioned on all previous ones;\nnonetheless we show that it can be efficiently trained on data with tens of\nthousands of samples per second of audio. When applied to text-to-speech, it\nyields state-of-the-art performance, with human listeners rating it as\nsignificantly more natural sounding than the best parametric and concatenative\nsystems for both English and Mandarin. A single WaveNet can capture the\ncharacteristics of many different speakers with equal fidelity, and can switch\nbetween them by conditioning on the speaker identity. When trained to model\nmusic, we find that it generates novel and often highly realistic musical\nfragments. We also show that it can be employed as a discriminative model,\nreturning promising results for phoneme recognition. \n\n"}
{"id": "1609.03677", "contents": "Title: Unsupervised Monocular Depth Estimation with Left-Right Consistency Abstract: Learning based methods have shown very promising results for the task of\ndepth estimation in single images. However, most existing approaches treat\ndepth prediction as a supervised regression problem and as a result, require\nvast quantities of corresponding ground truth depth data for training. Just\nrecording quality depth data in a range of environments is a challenging\nproblem. In this paper, we innovate beyond existing approaches, replacing the\nuse of explicit depth data during training with easier-to-obtain binocular\nstereo footage.\n  We propose a novel training objective that enables our convolutional neural\nnetwork to learn to perform single image depth estimation, despite the absence\nof ground truth depth data. Exploiting epipolar geometry constraints, we\ngenerate disparity images by training our network with an image reconstruction\nloss. We show that solving for image reconstruction alone results in poor\nquality depth images. To overcome this problem, we propose a novel training\nloss that enforces consistency between the disparities produced relative to\nboth the left and right images, leading to improved performance and robustness\ncompared to existing approaches. Our method produces state of the art results\nfor monocular depth estimation on the KITTI driving dataset, even outperforming\nsupervised methods that have been trained with ground truth depth. \n\n"}
{"id": "1609.05807", "contents": "Title: Inherent Trade-Offs in the Fair Determination of Risk Scores Abstract: Recent discussion in the public sphere about algorithmic classification has\ninvolved tension between competing notions of what it means for a probabilistic\nclassification to be fair to different groups. We formalize three fairness\nconditions that lie at the heart of these debates, and we prove that except in\nhighly constrained special cases, there is no method that can satisfy these\nthree conditions simultaneously. Moreover, even satisfying all three conditions\napproximately requires that the data lie in an approximate version of one of\nthe constrained special cases identified by our theorem. These results suggest\nsome of the ways in which key notions of fairness are incompatible with each\nother, and hence provide a framework for thinking about the trade-offs between\nthem. \n\n"}
{"id": "1609.06831", "contents": "Title: Hawkes Processes with Stochastic Excitations Abstract: We propose an extension to Hawkes processes by treating the levels of\nself-excitation as a stochastic differential equation. Our new point process\nallows better approximation in application domains where events and intensities\naccelerate each other with correlated levels of contagion. We generalize a\nrecent algorithm for simulating draws from Hawkes processes whose levels of\nexcitation are stochastic processes, and propose a hybrid Markov chain Monte\nCarlo approach for model fitting. Our sampling procedure scales linearly with\nthe number of required events and does not require stationarity of the point\nprocess. A modular inference procedure consisting of a combination between\nGibbs and Metropolis Hastings steps is put forward. We recover expectation\nmaximization as a special case. Our general approach is illustrated for\ncontagion following geometric Brownian motion and exponential Langevin\ndynamics. \n\n"}
{"id": "1609.08209", "contents": "Title: Automatic Construction of a Recurrent Neural Network based Classifier\n  for Vehicle Passage Detection Abstract: Recurrent Neural Networks (RNNs) are extensively used for time-series\nmodeling and prediction. We propose an approach for automatic construction of a\nbinary classifier based on Long Short-Term Memory RNNs (LSTM-RNNs) for\ndetection of a vehicle passage through a checkpoint. As an input to the\nclassifier we use multidimensional signals of various sensors that are\ninstalled on the checkpoint. Obtained results demonstrate that the previous\napproach to handcrafting a classifier, consisting of a set of deterministic\nrules, can be successfully replaced by an automatic RNN training on an\nappropriately labelled data. \n\n"}
{"id": "1609.08675", "contents": "Title: YouTube-8M: A Large-Scale Video Classification Benchmark Abstract: Many recent advancements in Computer Vision are attributed to large datasets.\nOpen-source software packages for Machine Learning and inexpensive commodity\nhardware have reduced the barrier of entry for exploring novel approaches at\nscale. It is possible to train models over millions of examples within a few\ndays. Although large-scale datasets exist for image understanding, such as\nImageNet, there are no comparable size video classification datasets.\n  In this paper, we introduce YouTube-8M, the largest multi-label video\nclassification dataset, composed of ~8 million videos (500K hours of video),\nannotated with a vocabulary of 4800 visual entities. To get the videos and\ntheir labels, we used a YouTube video annotation system, which labels videos\nwith their main topics. While the labels are machine-generated, they have\nhigh-precision and are derived from a variety of human-based signals including\nmetadata and query click signals. We filtered the video labels (Knowledge Graph\nentities) using both automated and manual curation strategies, including asking\nhuman raters if the labels are visually recognizable. Then, we decoded each\nvideo at one-frame-per-second, and used a Deep CNN pre-trained on ImageNet to\nextract the hidden representation immediately prior to the classification\nlayer. Finally, we compressed the frame features and make both the features and\nvideo-level labels available for download.\n  We trained various (modest) classification models on the dataset, evaluated\nthem using popular evaluation metrics, and report them as baselines. Despite\nthe size of the dataset, some of our models train to convergence in less than a\nday on a single machine using TensorFlow. We plan to release code for training\na TensorFlow model and for computing metrics. \n\n"}
{"id": "1610.01206", "contents": "Title: A Survey of Multi-View Representation Learning Abstract: Recently, multi-view representation learning has become a rapidly growing\ndirection in machine learning and data mining areas. This paper introduces two\ncategories for multi-view representation learning: multi-view representation\nalignment and multi-view representation fusion. Consequently, we first review\nthe representative methods and theories of multi-view representation learning\nbased on the perspective of alignment, such as correlation-based alignment.\nRepresentative examples are canonical correlation analysis (CCA) and its\nseveral extensions. Then from the perspective of representation fusion we\ninvestigate the advancement of multi-view representation learning that ranges\nfrom generative methods including multi-modal topic learning, multi-view sparse\ncoding, and multi-view latent space Markov networks, to neural network-based\nmethods including multi-modal autoencoders, multi-view convolutional neural\nnetworks, and multi-modal recurrent neural networks. Further, we also\ninvestigate several important applications of multi-view representation\nlearning. Overall, this survey aims to provide an insightful overview of\ntheoretical foundation and state-of-the-art developments in the field of\nmulti-view representation learning and to help researchers find the most\nappropriate tools for particular applications. \n\n"}
{"id": "1610.02357", "contents": "Title: Xception: Deep Learning with Depthwise Separable Convolutions Abstract: We present an interpretation of Inception modules in convolutional neural\nnetworks as being an intermediate step in-between regular convolution and the\ndepthwise separable convolution operation (a depthwise convolution followed by\na pointwise convolution). In this light, a depthwise separable convolution can\nbe understood as an Inception module with a maximally large number of towers.\nThis observation leads us to propose a novel deep convolutional neural network\narchitecture inspired by Inception, where Inception modules have been replaced\nwith depthwise separable convolutions. We show that this architecture, dubbed\nXception, slightly outperforms Inception V3 on the ImageNet dataset (which\nInception V3 was designed for), and significantly outperforms Inception V3 on a\nlarger image classification dataset comprising 350 million images and 17,000\nclasses. Since the Xception architecture has the same number of parameters as\nInception V3, the performance gains are not due to increased capacity but\nrather to a more efficient use of model parameters. \n\n"}
{"id": "1610.04798", "contents": "Title: Communication-efficient Distributed Sparse Linear Discriminant Analysis Abstract: We propose a communication-efficient distributed estimation method for sparse\nlinear discriminant analysis (LDA) in the high dimensional regime. Our method\ndistributes the data of size $N$ into $m$ machines, and estimates a local\nsparse LDA estimator on each machine using the data subset of size $N/m$. After\nthe distributed estimation, our method aggregates the debiased local estimators\nfrom $m$ machines, and sparsifies the aggregated estimator. We show that the\naggregated estimator attains the same statistical rate as the centralized\nestimation method, as long as the number of machines $m$ is chosen\nappropriately. Moreover, we prove that our method can attain the model\nselection consistency under a milder condition than the centralized method.\nExperiments on both synthetic and real datasets corroborate our theory. \n\n"}
{"id": "1610.04804", "contents": "Title: Dynamic Stacked Generalization for Node Classification on Networks Abstract: We propose a novel stacked generalization (stacking) method as a dynamic\nensemble technique using a pool of heterogeneous classifiers for node label\nclassification on networks. The proposed method assigns component models a set\nof functional coefficients, which can vary smoothly with certain topological\nfeatures of a node. Compared to the traditional stacking model, the proposed\nmethod can dynamically adjust the weights of individual models as we move\nacross the graph and provide a more versatile and significantly more accurate\nstacking model for label prediction on a network. We demonstrate the benefits\nof the proposed model using both a simulation study and real data analysis. \n\n"}
{"id": "1610.05083", "contents": "Title: Efficient Metric Learning for the Analysis of Motion Data Abstract: We investigate metric learning in the context of dynamic time warping (DTW),\nthe by far most popular dissimilarity measure used for the comparison and\nanalysis of motion capture data. While metric learning enables a\nproblem-adapted representation of data, the majority of methods has been\nproposed for vectorial data only. In this contribution, we extend the popular\nprinciple offered by the large margin nearest neighbors learner (LMNN) to DTW\nby treating the resulting component-wise dissimilarity values as features. We\ndemonstrate that this principle greatly enhances the classification accuracy in\nseveral benchmarks. Further, we show that recent auxiliary concepts such as\nmetric regularization can be transferred from the vectorial case to\ncomponent-wise DTW in a similar way. We illustrate that metric regularization\nconstitutes a crucial prerequisite for the interpretation of the resulting\nrelevance profiles. \n\n"}
{"id": "1610.06940", "contents": "Title: Safety Verification of Deep Neural Networks Abstract: Deep neural networks have achieved impressive experimental results in image\nclassification, but can surprisingly be unstable with respect to adversarial\nperturbations, that is, minimal changes to the input image that cause the\nnetwork to misclassify it. With potential applications including perception\nmodules and end-to-end controllers for self-driving cars, this raises concerns\nabout their safety. We develop a novel automated verification framework for\nfeed-forward multi-layer neural networks based on Satisfiability Modulo Theory\n(SMT). We focus on safety of image classification decisions with respect to\nimage manipulations, such as scratches or changes to camera angle or lighting\nconditions that would result in the same class being assigned by a human, and\ndefine safety for an individual decision in terms of invariance of the\nclassification within a small neighbourhood of the original image. We enable\nexhaustive search of the region by employing discretisation, and propagate the\nanalysis layer by layer. Our method works directly with the network code and,\nin contrast to existing methods, can guarantee that adversarial examples, if\nthey exist, are found for the given region and family of manipulations. If\nfound, adversarial examples can be shown to human testers and/or used to\nfine-tune the network. We implement the techniques using Z3 and evaluate them\non state-of-the-art networks, including regularised and deep learning networks.\nWe also compare against existing techniques to search for adversarial examples\nand estimate network robustness. \n\n"}
{"id": "1610.08445", "contents": "Title: New Liftable Classes for First-Order Probabilistic Inference Abstract: Statistical relational models provide compact encodings of probabilistic\ndependencies in relational domains, but result in highly intractable graphical\nmodels. The goal of lifted inference is to carry out probabilistic inference\nwithout needing to reason about each individual separately, by instead treating\nexchangeable, undistinguished objects as a whole. In this paper, we study the\ndomain recursion inference rule, which, despite its central role in early\ntheoretical results on domain-lifted inference, has later been believed\nredundant. We show that this rule is more powerful than expected, and in fact\nsignificantly extends the range of models for which lifted inference runs in\ntime polynomial in the number of individuals in the domain. This includes an\nopen problem called S4, the symmetric transitivity model, and a first-order\nlogic encoding of the birthday paradox. We further identify new classes S2FO2\nand S2RU of domain-liftable theories, which respectively subsume FO2 and\nrecursively unary theories, the largest classes of domain-liftable theories\nknown so far, and show that using domain recursion can achieve exponential\nspeedup even in theories that cannot fully be lifted with the existing set of\ninference rules. \n\n"}
{"id": "1610.09559", "contents": "Title: Fair Algorithms for Infinite and Contextual Bandits Abstract: We study fairness in linear bandit problems. Starting from the notion of\nmeritocratic fairness introduced in Joseph et al. [2016], we carry out a more\nrefined analysis of a more general problem, achieving better performance\nguarantees with fewer modelling assumptions on the number and structure of\navailable choices as well as the number selected. We also analyze the\npreviously-unstudied question of fairness in infinite linear bandit problems,\nobtaining instance-dependent regret upper bounds as well as lower bounds\ndemonstrating that this instance-dependence is necessary. The result is a\nframework for meritocratic fairness in an online linear setting that is\nsubstantially more powerful, general, and realistic than the current state of\nthe art. \n\n"}
{"id": "1610.09780", "contents": "Title: Flexible Models for Microclustering with Application to Entity\n  Resolution Abstract: Most generative models for clustering implicitly assume that the number of\ndata points in each cluster grows linearly with the total number of data\npoints. Finite mixture models, Dirichlet process mixture models, and\nPitman--Yor process mixture models make this assumption, as do all other\ninfinitely exchangeable clustering models. However, for some applications, this\nassumption is inappropriate. For example, when performing entity resolution,\nthe size of each cluster should be unrelated to the size of the data set, and\neach cluster should contain a negligible fraction of the total number of data\npoints. These applications require models that yield clusters whose sizes grow\nsublinearly with the size of the data set. We address this requirement by\ndefining the microclustering property and introducing a new class of models\nthat can exhibit this property. We compare models within this class to two\ncommonly used clustering models using four entity-resolution data sets. \n\n"}
{"id": "1611.00328", "contents": "Title: Variational Inference via $\\chi$-Upper Bound Minimization Abstract: Variational inference (VI) is widely used as an efficient alternative to\nMarkov chain Monte Carlo. It posits a family of approximating distributions $q$\nand finds the closest member to the exact posterior $p$. Closeness is usually\nmeasured via a divergence $D(q || p)$ from $q$ to $p$. While successful, this\napproach also has problems. Notably, it typically leads to underestimation of\nthe posterior variance. In this paper we propose CHIVI, a black-box variational\ninference algorithm that minimizes $D_{\\chi}(p || q)$, the $\\chi$-divergence\nfrom $p$ to $q$. CHIVI minimizes an upper bound of the model evidence, which we\nterm the $\\chi$ upper bound (CUBO). Minimizing the CUBO leads to improved\nposterior uncertainty, and it can also be used with the classical VI lower\nbound (ELBO) to provide a sandwich estimate of the model evidence. We study\nCHIVI on three models: probit regression, Gaussian process classification, and\na Cox process model of basketball plays. When compared to expectation\npropagation and classical VI, CHIVI produces better error rates and more\naccurate estimates of posterior variance. \n\n"}
{"id": "1611.00953", "contents": "Title: High-dimensional regression over disease subgroups Abstract: We consider high-dimensional regression over subgroups of observations. Our\nwork is motivated by biomedical problems, where disease subtypes, for example,\nmay differ with respect to underlying regression models, but sample sizes at\nthe subgroup-level may be limited. We focus on the case in which\nsubgroup-specific models may be expected to be similar but not necessarily\nidentical. Our approach is to treat subgroups as related problem instances and\njointly estimate subgroup-specific regression coefficients. This is done in a\npenalized framework, combining an $\\ell_1$ term with an additional term that\npenalizes differences between subgroup-specific coefficients. This gives\nsolutions that are globally sparse but that allow information-sharing between\nthe subgroups. We present algorithms for estimation and empirical results on\nsimulated data and using Alzheimer's disease, amyotrophic lateral sclerosis and\ncancer datasets. These examples demonstrate the gains our approach can offer in\nterms of prediction and the ability to estimate subgroup-specific sparsity\npatterns. \n\n"}
{"id": "1611.01236", "contents": "Title: Adversarial Machine Learning at Scale Abstract: Adversarial examples are malicious inputs designed to fool machine learning\nmodels. They often transfer from one model to another, allowing attackers to\nmount black box attacks without knowledge of the target model's parameters.\nAdversarial training is the process of explicitly training a model on\nadversarial examples, in order to make it more robust to attack or to reduce\nits test error on clean inputs. So far, adversarial training has primarily been\napplied to small problems. In this research, we apply adversarial training to\nImageNet. Our contributions include: (1) recommendations for how to succesfully\nscale adversarial training to large models and datasets, (2) the observation\nthat adversarial training confers robustness to single-step attack methods, (3)\nthe finding that multi-step attack methods are somewhat less transferable than\nsingle-step attack methods, so single-step attacks are the best for mounting\nblack-box attacks, and (4) resolution of a \"label leaking\" effect that causes\nadversarially trained models to perform better on adversarial examples than on\nclean examples, because the adversarial example construction process uses the\ntrue label and the model can learn to exploit regularities in the construction\nprocess. \n\n"}
{"id": "1611.01722", "contents": "Title: Learning to Draw Samples: With Application to Amortized MLE for\n  Generative Adversarial Learning Abstract: We propose a simple algorithm to train stochastic neural networks to draw\nsamples from given target distributions for probabilistic inference. Our method\nis based on iteratively adjusting the neural network parameters so that the\noutput changes along a Stein variational gradient that maximumly decreases the\nKL divergence with the target distribution. Our method works for any target\ndistribution specified by their unnormalized density function, and can train\nany black-box architectures that are differentiable in terms of the parameters\nwe want to adapt. As an application of our method, we propose an amortized MLE\nalgorithm for training deep energy model, where a neural sampler is adaptively\ntrained to approximate the likelihood function. Our method mimics an\nadversarial game between the deep energy model and the neural sampler, and\nobtains realistic-looking images competitive with the state-of-the-art results. \n\n"}
{"id": "1611.01957", "contents": "Title: Linear Convergence of SVRG in Statistical Estimation Abstract: SVRG and its variants are among the state of art optimization algorithms for\nlarge scale machine learning problems. It is well known that SVRG converges\nlinearly when the objective function is strongly convex. However this setup can\nbe restrictive, and does not include several important formulations such as\nLasso, group Lasso, logistic regression, and some non-convex models including\ncorrected Lasso and SCAD. In this paper, we prove that, for a class of\nstatistical M-estimators covering examples mentioned above, SVRG solves the\nformulation with {\\em a linear convergence rate} without strong convexity or\neven convexity. Our analysis makes use of {\\em restricted strong convexity},\nunder which we show that SVRG converges linearly to the fundamental statistical\nprecision of the model, i.e., the difference between true unknown parameter\n$\\theta^*$ and the optimal solution $\\hat{\\theta}$ of the model. \n\n"}
{"id": "1611.02268", "contents": "Title: Optimal Binary Autoencoding with Pairwise Correlations Abstract: We formulate learning of a binary autoencoder as a biconvex optimization\nproblem which learns from the pairwise correlations between encoded and decoded\nbits. Among all possible algorithms that use this information, ours finds the\nautoencoder that reconstructs its inputs with worst-case optimal loss. The\noptimal decoder is a single layer of artificial neurons, emerging entirely from\nthe minimax loss minimization, and with weights learned by convex optimization.\nAll this is reflected in competitive experimental results, demonstrating that\nbinary autoencoding can be done efficiently by conveying information in\npairwise correlations in an optimal fashion. \n\n"}
{"id": "1611.04528", "contents": "Title: Benchmarking Quantum Hardware for Training of Fully Visible Boltzmann\n  Machines Abstract: Quantum annealing (QA) is a hardware-based heuristic optimization and\nsampling method applicable to discrete undirected graphical models. While\nsimilar to simulated annealing, QA relies on quantum, rather than thermal,\neffects to explore complex search spaces. For many classes of problems, QA is\nknown to offer computational advantages over simulated annealing. Here we\nreport on the ability of recent QA hardware to accelerate training of fully\nvisible Boltzmann machines. We characterize the sampling distribution of QA\nhardware, and show that in many cases, the quantum distributions differ\nsignificantly from classical Boltzmann distributions. In spite of this\ndifference, training (which seeks to match data and model statistics) using\nstandard classical gradient updates is still effective. We investigate the use\nof QA for seeding Markov chains as an alternative to contrastive divergence\n(CD) and persistent contrastive divergence (PCD). Using $k=50$ Gibbs steps, we\nshow that for problems with high-energy barriers between modes, QA-based seeds\ncan improve upon chains with CD and PCD initializations. For these hard\nproblems, QA gradient estimates are more accurate, and allow for faster\nlearning. Furthermore, and interestingly, even the case of raw QA samples (that\nis, $k=0$) achieved similar improvements. We argue that this relates to the\nfact that we are training a quantum rather than classical Boltzmann\ndistribution in this case. The learned parameters give rise to hardware QA\ndistributions closely approximating classical Boltzmann distributions that are\nhard to train with CD/PCD. \n\n"}
{"id": "1611.04835", "contents": "Title: Multilinear Low-Rank Tensors on Graphs & Applications Abstract: We propose a new framework for the analysis of low-rank tensors which lies at\nthe intersection of spectral graph theory and signal processing. As a first\nstep, we present a new graph based low-rank decomposition which approximates\nthe classical low-rank SVD for matrices and multi-linear SVD for tensors. Then,\nbuilding on this novel decomposition we construct a general class of convex\noptimization problems for approximately solving low-rank tensor inverse\nproblems, such as tensor Robust PCA. The whole framework is named as\n'Multilinear Low-rank tensors on Graphs (MLRTG)'. Our theoretical analysis\nshows: 1) MLRTG stands on the notion of approximate stationarity of\nmulti-dimensional signals on graphs and 2) the approximation error depends on\nthe eigen gaps of the graphs. We demonstrate applications for a wide variety of\n4 artificial and 12 real tensor datasets, such as EEG, FMRI, BCI, surveillance\nvideos and hyperspectral images. Generalization of the tensor concepts to\nnon-euclidean domain, orders of magnitude speed-up, low-memory requirement and\nsignificantly enhanced performance at low SNR are the key aspects of our\nframework. \n\n"}
{"id": "1611.06440", "contents": "Title: Pruning Convolutional Neural Networks for Resource Efficient Inference Abstract: We propose a new formulation for pruning convolutional kernels in neural\nnetworks to enable efficient inference. We interleave greedy criteria-based\npruning with fine-tuning by backpropagation - a computationally efficient\nprocedure that maintains good generalization in the pruned network. We propose\na new criterion based on Taylor expansion that approximates the change in the\ncost function induced by pruning network parameters. We focus on transfer\nlearning, where large pretrained networks are adapted to specialized tasks. The\nproposed criterion demonstrates superior performance compared to other\ncriteria, e.g. the norm of kernel weights or feature map activation, for\npruning large CNNs after adaptation to fine-grained classification tasks\n(Birds-200 and Flowers-102) relaying only on the first order gradient\ninformation. We also show that pruning can lead to more than 10x theoretical\n(5x practical) reduction in adapted 3D-convolutional filters with a small drop\nin accuracy in a recurrent gesture classifier. Finally, we show results for the\nlarge-scale ImageNet dataset to emphasize the flexibility of our approach. \n\n"}
{"id": "1611.06475", "contents": "Title: Dealing with Range Anxiety in Mean Estimation via Statistical Queries Abstract: We give algorithms for estimating the expectation of a given real-valued\nfunction $\\phi:X\\to {\\bf R}$ on a sample drawn randomly from some unknown\ndistribution $D$ over domain $X$, namely ${\\bf E}_{{\\bf x}\\sim D}[\\phi({\\bf\nx})]$. Our algorithms work in two well-studied models of restricted access to\ndata samples. The first one is the statistical query (SQ) model in which an\nalgorithm has access to an SQ oracle for the input distribution $D$ over $X$\ninstead of i.i.d. samples from $D$. Given a query function $\\phi:X \\to [0,1]$,\nthe oracle returns an estimate of ${\\bf E}_{{\\bf x}\\sim D}[\\phi({\\bf x})]$\nwithin some tolerance $\\tau$. The second, is a model in which only a single bit\nis communicated from each sample. In both of these models the error obtained\nusing a naive implementation would scale polynomially with the range of the\nrandom variable $\\phi({\\bf x})$ (which might even be infinite). In contrast,\nwithout restrictions on access to data the expected error scales with the\nstandard deviation of $\\phi({\\bf x})$. Here we give a simple algorithm whose\nerror scales linearly in standard deviation of $\\phi({\\bf x})$ and\nlogarithmically with an upper bound on the second moment of $\\phi({\\bf x})$.\n  As corollaries, we obtain algorithms for high dimensional mean estimation and\nstochastic convex optimization in these models that work in more general\nsettings than previously known solutions. \n\n"}
{"id": "1611.07119", "contents": "Title: Max-Margin Deep Generative Models for (Semi-)Supervised Learning Abstract: Deep generative models (DGMs) are effective on learning multilayered\nrepresentations of complex data and performing inference of input data by\nexploring the generative ability. However, it is relatively insufficient to\nempower the discriminative ability of DGMs on making accurate predictions. This\npaper presents max-margin deep generative models (mmDGMs) and a\nclass-conditional variant (mmDCGMs), which explore the strongly discriminative\nprinciple of max-margin learning to improve the predictive performance of DGMs\nin both supervised and semi-supervised learning, while retaining the generative\ncapability. In semi-supervised learning, we use the predictions of a max-margin\nclassifier as the missing labels instead of performing full posterior inference\nfor efficiency; we also introduce additional max-margin and label-balance\nregularization terms of unlabeled data for effectiveness. We develop an\nefficient doubly stochastic subgradient algorithm for the piecewise linear\nobjectives in different settings. Empirical results on various datasets\ndemonstrate that: (1) max-margin learning can significantly improve the\nprediction performance of DGMs and meanwhile retain the generative ability; (2)\nin supervised learning, mmDGMs are competitive to the best fully discriminative\nnetworks when employing convolutional neural networks as the generative and\nrecognition models; and (3) in semi-supervised learning, mmDCGMs can perform\nefficient inference and achieve state-of-the-art classification results on\nseveral benchmarks. \n\n"}
{"id": "1611.07460", "contents": "Title: Poisson Random Fields for Dynamic Feature Models Abstract: We present the Wright-Fisher Indian buffet process (WF-IBP), a probabilistic\nmodel for time-dependent data assumed to have been generated by an unknown\nnumber of latent features. This model is suitable as a prior in Bayesian\nnonparametric feature allocation models in which the features underlying the\nobserved data exhibit a dependency structure over time. More specifically, we\nestablish a new framework for generating dependent Indian buffet processes,\nwhere the Poisson random field model from population genetics is used as a way\nof constructing dependent beta processes. Inference in the model is complex,\nand we describe a sophisticated Markov Chain Monte Carlo algorithm for exact\nposterior simulation. We apply our construction to develop a nonparametric\nfocused topic model for collections of time-stamped text documents and test it\non the full corpus of NIPS papers published from 1987 to 2015. \n\n"}
{"id": "1611.07725", "contents": "Title: iCaRL: Incremental Classifier and Representation Learning Abstract: A major open problem on the road to artificial intelligence is the\ndevelopment of incrementally learning systems that learn about more and more\nconcepts over time from a stream of data. In this work, we introduce a new\ntraining strategy, iCaRL, that allows learning in such a class-incremental way:\nonly the training data for a small number of classes has to be present at the\nsame time and new classes can be added progressively. iCaRL learns strong\nclassifiers and a data representation simultaneously. This distinguishes it\nfrom earlier works that were fundamentally limited to fixed data\nrepresentations and therefore incompatible with deep learning architectures. We\nshow by experiments on CIFAR-100 and ImageNet ILSVRC 2012 data that iCaRL can\nlearn many classes incrementally over a long period of time where other\nstrategies quickly fail. \n\n"}
{"id": "1611.09482", "contents": "Title: Fast Wavenet Generation Algorithm Abstract: This paper presents an efficient implementation of the Wavenet generation\nprocess called Fast Wavenet. Compared to a naive implementation that has\ncomplexity O(2^L) (L denotes the number of layers in the network), our proposed\napproach removes redundant convolution operations by caching previous\ncalculations, thereby reducing the complexity to O(L) time. Timing experiments\nshow significant advantages of our fast implementation over a naive one. While\nthis method is presented for Wavenet, the same scheme can be applied anytime\none wants to perform autoregressive generation or online prediction using a\nmodel with dilated convolution layers. The code for our method is publicly\navailable. \n\n"}
{"id": "1612.00383", "contents": "Title: Tuning the Scheduling of Distributed Stochastic Gradient Descent with\n  Bayesian Optimization Abstract: We present an optimizer which uses Bayesian optimization to tune the system\nparameters of distributed stochastic gradient descent (SGD). Given a specific\ncontext, our goal is to quickly find efficient configurations which\nappropriately balance the load between the available machines to minimize the\naverage SGD iteration time. Our experiments consider setups with over thirty\nparameters. Traditional Bayesian optimization, which uses a Gaussian process as\nits model, is not well suited to such high dimensional domains. To reduce\nconvergence time, we exploit the available structure. We design a probabilistic\nmodel which simulates the behavior of distributed SGD and use it within\nBayesian optimization. Our model can exploit many runtime measurements for\ninference per evaluation of the objective function. Our experiments show that\nour resulting optimizer converges to efficient configurations within ten\niterations, the optimized configurations outperform those found by generic\noptimizer in thirty iterations by up to 2X. \n\n"}
{"id": "1612.01988", "contents": "Title: Local Group Invariant Representations via Orbit Embeddings Abstract: Invariance to nuisance transformations is one of the desirable properties of\neffective representations. We consider transformations that form a \\emph{group}\nand propose an approach based on kernel methods to derive local group invariant\nrepresentations. Locality is achieved by defining a suitable probability\ndistribution over the group which in turn induces distributions in the input\nfeature space. We learn a decision function over these distributions by\nappealing to the powerful framework of kernel methods and generate local\ninvariant random feature maps via kernel approximations. We show uniform\nconvergence bounds for kernel approximation and provide excess risk bounds for\nlearning with these features. We evaluate our method on three real datasets,\nincluding Rotated MNIST and CIFAR-10, and observe that it outperforms competing\nkernel based approaches. The proposed method also outperforms deep CNN on\nRotated-MNIST and performs comparably to the recently proposed\ngroup-equivariant CNN. \n\n"}
{"id": "1612.02803", "contents": "Title: The Physical Systems Behind Optimization Algorithms Abstract: We use differential equations based approaches to provide some {\\it\n\\textbf{physics}} insights into analyzing the dynamics of popular optimization\nalgorithms in machine learning. In particular, we study gradient descent,\nproximal gradient descent, coordinate gradient descent, proximal coordinate\ngradient, and Newton's methods as well as their Nesterov's accelerated variants\nin a unified framework motivated by a natural connection of optimization\nalgorithms to physical systems. Our analysis is applicable to more general\nalgorithms and optimization problems {\\it \\textbf{beyond}} convexity and strong\nconvexity, e.g. Polyak-\\L ojasiewicz and error bound conditions (possibly\nnonconvex). \n\n"}
{"id": "1612.03226", "contents": "Title: Active Learning for Speech Recognition: the Power of Gradients Abstract: In training speech recognition systems, labeling audio clips can be\nexpensive, and not all data is equally valuable. Active learning aims to label\nonly the most informative samples to reduce cost. For speech recognition,\nconfidence scores and other likelihood-based active learning methods have been\nshown to be effective. Gradient-based active learning methods, however, are\nstill not well-understood. This work investigates the Expected Gradient Length\n(EGL) approach in active learning for end-to-end speech recognition. We justify\nEGL from a variance reduction perspective, and observe that EGL's measure of\ninformativeness picks novel samples uncorrelated with confidence scores.\nExperimentally, we show that EGL can reduce word errors by 11\\%, or\nalternatively, reduce the number of samples to label by 50\\%, when compared to\nrandom sampling. \n\n"}
{"id": "1612.03350", "contents": "Title: Non-negative Factorization of the Occurrence Tensor from Financial\n  Contracts Abstract: We propose an algorithm for the non-negative factorization of an occurrence\ntensor built from heterogeneous networks. We use l0 norm to model sparse errors\nover discrete values (occurrences), and use decomposed factors to model the\nembedded groups of nodes. An efficient splitting method is developed to\noptimize the nonconvex and nonsmooth objective. We study both synthetic\nproblems and a new dataset built from financial documents, resMBS. \n\n"}
{"id": "1612.04111", "contents": "Title: Parsimonious Online Learning with Kernels via Sparse Projections in\n  Function Space Abstract: Despite their attractiveness, popular perception is that techniques for\nnonparametric function approximation do not scale to streaming data due to an\nintractable growth in the amount of storage they require. To solve this problem\nin a memory-affordable way, we propose an online technique based on functional\nstochastic gradient descent in tandem with supervised sparsification based on\ngreedy function subspace projections. The method, called parsimonious online\nlearning with kernels (POLK), provides a controllable tradeoff? between its\nsolution accuracy and the amount of memory it requires. We derive conditions\nunder which the generated function sequence converges almost surely to the\noptimal function, and we establish that the memory requirement remains finite.\nWe evaluate POLK for kernel multi-class logistic regression and kernel\nhinge-loss classification on three canonical data sets: a synthetic Gaussian\nmixture model, the MNIST hand-written digits, and the Brodatz texture database.\nOn all three tasks, we observe a favorable tradeoff of objective function\nevaluation, classification performance, and complexity of the nonparametric\nregressor extracted the proposed method. \n\n"}
{"id": "1612.04440", "contents": "Title: Disentangling Space and Time in Video with Hierarchical Variational\n  Auto-encoders Abstract: There are many forms of feature information present in video data. Principle\namong them are object identity information which is largely static across\nmultiple video frames, and object pose and style information which continuously\ntransforms from frame to frame. Most existing models confound these two types\nof representation by mapping them to a shared feature space. In this paper we\npropose a probabilistic approach for learning separable representations of\nobject identity and pose information using unsupervised video data. Our\napproach leverages a deep generative model with a factored prior distribution\nthat encodes properties of temporal invariances in the hidden feature set.\nLearning is achieved via variational inference. We present results of learning\nidentity and pose information on a dataset of moving characters as well as a\ndataset of rotating 3D objects. Our experimental results demonstrate our\nmodel's success in factoring its representation, and demonstrate that the model\nachieves improved performance in transfer learning tasks. \n\n"}
{"id": "1612.05628", "contents": "Title: An Alternative Softmax Operator for Reinforcement Learning Abstract: A softmax operator applied to a set of values acts somewhat like the\nmaximization function and somewhat like an average. In sequential decision\nmaking, softmax is often used in settings where it is necessary to maximize\nutility but also to hedge against problems that arise from putting all of one's\nweight behind a single maximum utility decision. The Boltzmann softmax operator\nis the most commonly used softmax operator in this setting, but we show that\nthis operator is prone to misbehavior. In this work, we study a differentiable\nsoftmax operator that, among other properties, is a non-expansion ensuring a\nconvergent behavior in learning and planning. We introduce a variant of SARSA\nalgorithm that, by utilizing the new operator, computes a Boltzmann policy with\na state-dependent temperature parameter. We show that the algorithm is\nconvergent and that it performs favorably in practice. \n\n"}
{"id": "1612.07640", "contents": "Title: Deep Learning and Its Applications to Machine Health Monitoring: A\n  Survey Abstract: Since 2006, deep learning (DL) has become a rapidly growing research\ndirection, redefining state-of-the-art performances in a wide range of areas\nsuch as object recognition, image segmentation, speech recognition and machine\ntranslation. In modern manufacturing systems, data-driven machine health\nmonitoring is gaining in popularity due to the widespread deployment of\nlow-cost sensors and their connection to the Internet. Meanwhile, deep learning\nprovides useful tools for processing and analyzing these big machinery data.\nThe main purpose of this paper is to review and summarize the emerging research\nwork of deep learning on machine health monitoring. After the brief\nintroduction of deep learning techniques, the applications of deep learning in\nmachine health monitoring systems are reviewed mainly from the following\naspects: Auto-encoder (AE) and its variants, Restricted Boltzmann Machines and\nits variants including Deep Belief Network (DBN) and Deep Boltzmann Machines\n(DBM), Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN).\nFinally, some new trends of DL-based machine health monitoring methods are\ndiscussed. \n\n"}
{"id": "1701.00160", "contents": "Title: NIPS 2016 Tutorial: Generative Adversarial Networks Abstract: This report summarizes the tutorial presented by the author at NIPS 2016 on\ngenerative adversarial networks (GANs). The tutorial describes: (1) Why\ngenerative modeling is a topic worth studying, (2) how generative models work,\nand how GANs compare to other generative models, (3) the details of how GANs\nwork, (4) research frontiers in GANs, and (5) state-of-the-art image models\nthat combine GANs with other methods. Finally, the tutorial contains three\nexercises for readers to complete, and the solutions to these exercises. \n\n"}
{"id": "1701.00874", "contents": "Title: Neural Probabilistic Model for Non-projective MST Parsing Abstract: In this paper, we propose a probabilistic parsing model, which defines a\nproper conditional probability distribution over non-projective dependency\ntrees for a given sentence, using neural representations as inputs. The neural\nnetwork architecture is based on bi-directional LSTM-CNNs which benefits from\nboth word- and character-level representations automatically, by using\ncombination of bidirectional LSTM and CNN. On top of the neural network, we\nintroduce a probabilistic structured layer, defining a conditional log-linear\nmodel over non-projective trees. We evaluate our model on 17 different\ndatasets, across 14 different languages. By exploiting Kirchhoff's Matrix-Tree\nTheorem (Tutte, 1984), the partition functions and marginals can be computed\nefficiently, leading to a straight-forward end-to-end model training procedure\nvia back-propagation. Our parser achieves state-of-the-art parsing performance\non nine datasets. \n\n"}
{"id": "1701.08974", "contents": "Title: Towards Adversarial Retinal Image Synthesis Abstract: Synthesizing images of the eye fundus is a challenging task that has been\npreviously approached by formulating complex models of the anatomy of the eye.\nNew images can then be generated by sampling a suitable parameter space. In\nthis work, we propose a method that learns to synthesize eye fundus images\ndirectly from data. For that, we pair true eye fundus images with their\nrespective vessel trees, by means of a vessel segmentation technique. These\npairs are then used to learn a mapping from a binary vessel tree to a new\nretinal image. For this purpose, we use a recent image-to-image translation\ntechnique, based on the idea of adversarial learning. Experimental results show\nthat the original and the generated images are visually different in terms of\ntheir global appearance, in spite of sharing the same vessel tree.\nAdditionally, a quantitative quality analysis of the synthetic retinal images\nconfirms that the produced images retain a high proportion of the true image\nset quality. \n\n"}
{"id": "1702.00403", "contents": "Title: Generative Adversarial Networks recover features in astrophysical images\n  of galaxies beyond the deconvolution limit Abstract: Observations of astrophysical objects such as galaxies are limited by various\nsources of random and systematic noise from the sky background, the optical\nsystem of the telescope and the detector used to record the data. Conventional\ndeconvolution techniques are limited in their ability to recover features in\nimaging data by the Shannon-Nyquist sampling theorem. Here we train a\ngenerative adversarial network (GAN) on a sample of $4,550$ images of nearby\ngalaxies at $0.01<z<0.02$ from the Sloan Digital Sky Survey and conduct\n$10\\times$ cross validation to evaluate the results. We present a method using\na GAN trained on galaxy images that can recover features from artificially\ndegraded images with worse seeing and higher noise than the original with a\nperformance which far exceeds simple deconvolution. The ability to better\nrecover detailed features such as galaxy morphology from low-signal-to-noise\nand low angular resolution imaging data significantly increases our ability to\nstudy existing data sets of astrophysical objects as well as future\nobservations with observatories such as the Large Synoptic Sky Telescope (LSST)\nand the Hubble and James Webb space telescopes. \n\n"}
{"id": "1702.04267", "contents": "Title: On Detecting Adversarial Perturbations Abstract: Machine learning and deep learning in particular has advanced tremendously on\nperceptual tasks in recent years. However, it remains vulnerable against\nadversarial perturbations of the input that have been crafted specifically to\nfool the system while being quasi-imperceptible to a human. In this work, we\npropose to augment deep neural networks with a small \"detector\" subnetwork\nwhich is trained on the binary classification task of distinguishing genuine\ndata from data containing adversarial perturbations. Our method is orthogonal\nto prior work on addressing adversarial perturbations, which has mostly focused\non making the classification network itself more robust. We show empirically\nthat adversarial perturbations can be detected surprisingly well even though\nthey are quasi-imperceptible to humans. Moreover, while the detectors have been\ntrained to detect only a specific adversary, they generalize to similar and\nweaker adversaries. In addition, we propose an adversarial attack that fools\nboth the classifier and the detector and a novel training procedure for the\ndetector that counteracts this attack. \n\n"}
{"id": "1702.05538", "contents": "Title: Dataset Augmentation in Feature Space Abstract: Dataset augmentation, the practice of applying a wide array of\ndomain-specific transformations to synthetically expand a training set, is a\nstandard tool in supervised learning. While effective in tasks such as visual\nrecognition, the set of transformations must be carefully designed,\nimplemented, and tested for every new domain, limiting its re-use and\ngenerality. In this paper, we adopt a simpler, domain-agnostic approach to\ndataset augmentation. We start with existing data points and apply simple\ntransformations such as adding noise, interpolating, or extrapolating between\nthem. Our main insight is to perform the transformation not in input space, but\nin a learned feature space. A re-kindling of interest in unsupervised\nrepresentation learning makes this technique timely and more effective. It is a\nsimple proposal, but to-date one that has not been tested empirically. Working\nin the space of context vectors generated by sequence-to-sequence models, we\ndemonstrate a technique that is effective for both static and sequential data. \n\n"}
{"id": "1702.06818", "contents": "Title: Stochastic Approximation for Canonical Correlation Analysis Abstract: We propose novel first-order stochastic approximation algorithms for\ncanonical correlation analysis (CCA). Algorithms presented are instances of\ninexact matrix stochastic gradient (MSG) and inexact matrix exponentiated\ngradient (MEG), and achieve $\\epsilon$-suboptimality in the population\nobjective in $\\operatorname{poly}(\\frac{1}{\\epsilon})$ iterations. We also\nconsider practical variants of the proposed algorithms and compare them with\nother methods for CCA both theoretically and empirically. \n\n"}
{"id": "1702.07834", "contents": "Title: Efficient coordinate-wise leading eigenvector computation Abstract: We develop and analyze efficient \"coordinate-wise\" methods for finding the\nleading eigenvector, where each step involves only a vector-vector product. We\nestablish global convergence with overall runtime guarantees that are at least\nas good as Lanczos's method and dominate it for slowly decaying spectrum. Our\nmethods are based on combining a shift-and-invert approach with coordinate-wise\nalgorithms for linear regression. \n\n"}
{"id": "1702.07958", "contents": "Title: Efficient Online Bandit Multiclass Learning with $\\tilde{O}(\\sqrt{T})$\n  Regret Abstract: We present an efficient second-order algorithm with\n$\\tilde{O}(\\frac{1}{\\eta}\\sqrt{T})$ regret for the bandit online multiclass\nproblem. The regret bound holds simultaneously with respect to a family of loss\nfunctions parameterized by $\\eta$, for a range of $\\eta$ restricted by the norm\nof the competitor. The family of loss functions ranges from hinge loss\n($\\eta=0$) to squared hinge loss ($\\eta=1$). This provides a solution to the\nopen problem of (J. Abernethy and A. Rakhlin. An efficient bandit algorithm for\n$\\sqrt{T}$-regret in online multiclass prediction? In COLT, 2009). We test our\nalgorithm experimentally, showing that it also performs favorably against\nearlier algorithms. \n\n"}
{"id": "1702.08586", "contents": "Title: Can Boltzmann Machines Discover Cluster Updates ? Abstract: Boltzmann machines are physics informed generative models with wide\napplications in machine learning. They can learn the probability distribution\nfrom an input dataset and generate new samples accordingly. Applying them back\nto physics, the Boltzmann machines are ideal recommender systems to accelerate\nMonte Carlo simulation of physical systems due to their flexibility and\neffectiveness. More intriguingly, we show that the generative sampling of the\nBoltzmann Machines can even discover unknown cluster Monte Carlo algorithms.\nThe creative power comes from the latent representation of the Boltzmann\nmachines, which learn to mediate complex interactions and identify clusters of\nthe physical system. We demonstrate these findings with concrete examples of\nthe classical Ising model with and without four spin plaquette interactions.\nOur results endorse a fresh research paradigm where intelligent machines are\ndesigned to create or inspire human discovery of innovative algorithms. \n\n"}
{"id": "1702.08670", "contents": "Title: On architectural choices in deep learning: From network structure to\n  gradient convergence and parameter estimation Abstract: We study mechanisms to characterize how the asymptotic convergence of\nbackpropagation in deep architectures, in general, is related to the network\nstructure, and how it may be influenced by other design choices including\nactivation type, denoising and dropout rate. We seek to analyze whether network\narchitecture and input data statistics may guide the choices of learning\nparameters and vice versa. Given the broad applicability of deep architectures,\nthis issue is interesting both from theoretical and a practical standpoint.\nUsing properties of general nonconvex objectives (with first-order\ninformation), we first build the association between structural, distributional\nand learnability aspects of the network vis-\\`a-vis their interaction with\nparameter convergence rates. We identify a nice relationship between feature\ndenoising and dropout, and construct families of networks that achieve the same\nlevel of convergence. We then derive a workflow that provides systematic\nguidance regarding the choice of network sizes and learning parameters often\nmediated4 by input statistics. Our technical results are corroborated by an\nextensive set of evaluations, presented in this paper as well as independent\nempirical observations reported by other groups. We also perform experiments\nshowing the practical implications of our framework for choosing the best\nfully-connected design for a given problem. \n\n"}
{"id": "1702.08835", "contents": "Title: Deep Forest Abstract: Current deep learning models are mostly build upon neural networks, i.e.,\nmultiple layers of parameterized differentiable nonlinear modules that can be\ntrained by backpropagation. In this paper, we explore the possibility of\nbuilding deep models based on non-differentiable modules. We conjecture that\nthe mystery behind the success of deep neural networks owes much to three\ncharacteristics, i.e., layer-by-layer processing, in-model feature\ntransformation and sufficient model complexity. We propose the gcForest\napproach, which generates \\textit{deep forest} holding these characteristics.\nThis is a decision tree ensemble approach, with much less hyper-parameters than\ndeep neural networks, and its model complexity can be automatically determined\nin a data-dependent way. Experiments show that its performance is quite robust\nto hyper-parameter settings, such that in most cases, even across different\ndata from different domains, it is able to get excellent performance by using\nthe same default setting. This study opens the door of deep learning based on\nnon-differentiable modules, and exhibits the possibility of constructing deep\nmodels without using backpropagation. \n\n"}
{"id": "1703.00676", "contents": "Title: A Unifying View of Explicit and Implicit Feature Maps of Graph Kernels Abstract: Non-linear kernel methods can be approximated by fast linear ones using\nsuitable explicit feature maps allowing their application to large scale\nproblems. We investigate how convolution kernels for structured data are\ncomposed from base kernels and construct corresponding feature maps. On this\nbasis we propose exact and approximative feature maps for widely used graph\nkernels based on the kernel trick. We analyze for which kernels and graph\nproperties computation by explicit feature maps is feasible and actually more\nefficient. In particular, we derive approximative, explicit feature maps for\nstate-of-the-art kernels supporting real-valued attributes including the\nGraphHopper and graph invariant kernels. In extensive experiments we show that\nour approaches often achieve a classification accuracy close to the exact\nmethods based on the kernel trick, but require only a fraction of their running\ntime. Moreover, we propose and analyze algorithms for computing random walk,\nshortest-path and subgraph matching kernels by explicit and implicit feature\nmaps. Our theoretical results are confirmed experimentally by observing a phase\ntransition when comparing running time with respect to label diversity, walk\nlengths and subgraph size, respectively. \n\n"}
{"id": "1703.00862", "contents": "Title: Binarized Convolutional Landmark Localizers for Human Pose Estimation\n  and Face Alignment with Limited Resources Abstract: Our goal is to design architectures that retain the groundbreaking\nperformance of CNNs for landmark localization and at the same time are\nlightweight, compact and suitable for applications with limited computational\nresources. To this end, we make the following contributions: (a) we are the\nfirst to study the effect of neural network binarization on localization tasks,\nnamely human pose estimation and face alignment. We exhaustively evaluate\nvarious design choices, identify performance bottlenecks, and more importantly\npropose multiple orthogonal ways to boost performance. (b) Based on our\nanalysis, we propose a novel hierarchical, parallel and multi-scale residual\narchitecture that yields large performance improvement over the standard\nbottleneck block while having the same number of parameters, thus bridging the\ngap between the original network and its binarized counterpart. (c) We perform\na large number of ablation studies that shed light on the properties and the\nperformance of the proposed block. (d) We present results for experiments on\nthe most challenging datasets for human pose estimation and face alignment,\nreporting in many cases state-of-the-art performance. Code can be downloaded\nfrom https://www.adrianbulat.com/binary-cnn-landmarks \n\n"}
{"id": "1703.01014", "contents": "Title: Active Learning for Cost-Sensitive Classification Abstract: We design an active learning algorithm for cost-sensitive multiclass\nclassification: problems where different errors have different costs. Our\nalgorithm, COAL, makes predictions by regressing to each label's cost and\npredicting the smallest. On a new example, it uses a set of regressors that\nperform well on past data to estimate possible costs for each label. It queries\nonly the labels that could be the best, ignoring the sure losers. We prove COAL\ncan be efficiently implemented for any regression family that admits squared\nloss optimization; it also enjoys strong guarantees with respect to predictive\nperformance and labeling effort. We empirically compare COAL to passive\nlearning and several active learning baselines, showing significant\nimprovements in labeling effort and test cost on real-world datasets. \n\n"}
{"id": "1703.02391", "contents": "Title: Learning from Noisy Labels with Distillation Abstract: The ability of learning from noisy labels is very useful in many visual\nrecognition tasks, as a vast amount of data with noisy labels are relatively\neasy to obtain. Traditionally, the label noises have been treated as\nstatistical outliers, and approaches such as importance re-weighting and\nbootstrap have been proposed to alleviate the problem. According to our\nobservation, the real-world noisy labels exhibit multi-mode characteristics as\nthe true labels, rather than behaving like independent random outliers. In this\nwork, we propose a unified distillation framework to use side information,\nincluding a small clean dataset and label relations in knowledge graph, to\n\"hedge the risk\" of learning from noisy labels. Furthermore, unlike the\ntraditional approaches evaluated based on simulated label noises, we propose a\nsuite of new benchmark datasets, in Sports, Species and Artifacts domains, to\nevaluate the task of learning from noisy labels in the practical setting. The\nempirical study demonstrates the effectiveness of our proposed method in all\nthe domains. \n\n"}
{"id": "1703.02723", "contents": "Title: Scalable Greedy Feature Selection via Weak Submodularity Abstract: Greedy algorithms are widely used for problems in machine learning such as\nfeature selection and set function optimization. Unfortunately, for large\ndatasets, the running time of even greedy algorithms can be quite high. This is\nbecause for each greedy step we need to refit a model or calculate a function\nusing the previously selected choices and the new candidate.\n  Two algorithms that are faster approximations to the greedy forward selection\nwere introduced recently ([Mirzasoleiman et al. 2013, 2015]). They achieve\nbetter performance by exploiting distributed computation and stochastic\nevaluation respectively. Both algorithms have provable performance guarantees\nfor submodular functions.\n  In this paper we show that divergent from previously held opinion,\nsubmodularity is not required to obtain approximation guarantees for these two\nalgorithms. Specifically, we show that a generalized concept of weak\nsubmodularity suffices to give multiplicative approximation guarantees. Our\nresult extends the applicability of these algorithms to a larger class of\nfunctions. Furthermore, we show that a bounded submodularity ratio can be used\nto provide data dependent bounds that can sometimes be tighter also for\nsubmodular functions. We empirically validate our work by showing superior\nperformance of fast greedy approximations versus several established baselines\non artificial and real datasets. \n\n"}
{"id": "1703.04334", "contents": "Title: Probabilistic Matching: Causal Inference under Measurement Errors Abstract: The abundance of data produced daily from large variety of sources has\nboosted the need of novel approaches on causal inference analysis from\nobservational data. Observational data often contain noisy or missing entries.\nMoreover, causal inference studies may require unobserved high-level\ninformation which needs to be inferred from other observed attributes. In such\ncases, inaccuracies of the applied inference methods will result in noisy\noutputs. In this study, we propose a novel approach for causal inference when\none or more key variables are noisy. Our method utilizes the knowledge about\nthe uncertainty of the real values of key variables in order to reduce the bias\ninduced by noisy measurements. We evaluate our approach in comparison with\nexisting methods both on simulated and real scenarios and we demonstrate that\nour method reduces the bias and avoids false causal inference conclusions in\nmost cases. \n\n"}
{"id": "1703.04775", "contents": "Title: Discriminate-and-Rectify Encoders: Learning from Image Transformation\n  Sets Abstract: The complexity of a learning task is increased by transformations in the\ninput space that preserve class identity. Visual object recognition for example\nis affected by changes in viewpoint, scale, illumination or planar\ntransformations. While drastically altering the visual appearance, these\nchanges are orthogonal to recognition and should not be reflected in the\nrepresentation or feature encoding used for learning. We introduce a framework\nfor weakly supervised learning of image embeddings that are robust to\ntransformations and selective to the class distribution, using sets of\ntransforming examples (orbit sets), deep parametrizations and a novel\norbit-based loss. The proposed loss combines a discriminative, contrastive part\nfor orbits with a reconstruction error that learns to rectify orbit\ntransformations. The learned embeddings are evaluated in distance metric-based\ntasks, such as one-shot classification under geometric transformations, as well\nas face verification and retrieval under more realistic visual variability. Our\nresults suggest that orbit sets, suitably computed or observed, can be used for\nefficient, weakly-supervised learning of semantically relevant image\nembeddings. \n\n"}
{"id": "1703.04782", "contents": "Title: Online Learning Rate Adaptation with Hypergradient Descent Abstract: We introduce a general method for improving the convergence rate of\ngradient-based optimizers that is easy to implement and works well in practice.\nWe demonstrate the effectiveness of the method in a range of optimization\nproblems by applying it to stochastic gradient descent, stochastic gradient\ndescent with Nesterov momentum, and Adam, showing that it significantly reduces\nthe need for the manual tuning of the initial learning rate for these commonly\nused algorithms. Our method works by dynamically updating the learning rate\nduring optimization using the gradient with respect to the learning rate of the\nupdate rule itself. Computing this \"hypergradient\" needs little additional\ncomputation, requires only one extra copy of the original gradient to be stored\nin memory, and relies upon nothing more than what is provided by reverse-mode\nautomatic differentiation. \n\n"}
{"id": "1703.05298", "contents": "Title: Neural Networks for Beginners. A fast implementation in Matlab, Torch,\n  TensorFlow Abstract: This report provides an introduction to some Machine Learning tools within\nthe most common development environments. It mainly focuses on practical\nproblems, skipping any theoretical introduction. It is oriented to both\nstudents trying to approach Machine Learning and experts looking for new\nframeworks. \n\n"}
{"id": "1703.07047", "contents": "Title: High-Resolution Breast Cancer Screening with Multi-View Deep\n  Convolutional Neural Networks Abstract: Advances in deep learning for natural images have prompted a surge of\ninterest in applying similar techniques to medical images. The majority of the\ninitial attempts focused on replacing the input of a deep convolutional neural\nnetwork with a medical image, which does not take into consideration the\nfundamental differences between these two types of images. Specifically, fine\ndetails are necessary for detection in medical images, unlike in natural images\nwhere coarse structures matter most. This difference makes it inadequate to use\nthe existing network architectures developed for natural images, because they\nwork on heavily downscaled images to reduce the memory requirements. This hides\ndetails necessary to make accurate predictions. Additionally, a single exam in\nmedical imaging often comes with a set of views which must be fused in order to\nreach a correct conclusion. In our work, we propose to use a multi-view deep\nconvolutional neural network that handles a set of high-resolution medical\nimages. We evaluate it on large-scale mammography-based breast cancer screening\n(BI-RADS prediction) using 886,000 images. We focus on investigating the impact\nof the training set size and image size on the prediction accuracy. Our results\nhighlight that performance increases with the size of training set, and that\nthe best performance can only be achieved using the original resolution. In the\nreader study, performed on a random subset of the test set, we confirmed the\nefficacy of our model, which achieved performance comparable to a committee of\nradiologists when presented with the same data. \n\n"}
{"id": "1703.07131", "contents": "Title: Knowledge distillation using unlabeled mismatched images Abstract: Current approaches for Knowledge Distillation (KD) either directly use\ntraining data or sample from the training data distribution. In this paper, we\ndemonstrate effectiveness of 'mismatched' unlabeled stimulus to perform KD for\nimage classification networks. For illustration, we consider scenarios where\nthis is a complete absence of training data, or mismatched stimulus has to be\nused for augmenting a small amount of training data. We demonstrate that\nstimulus complexity is a key factor for distillation's good performance. Our\nexamples include use of various datasets for stimulating MNIST and CIFAR\nteachers. \n\n"}
{"id": "1703.08383", "contents": "Title: Smart Augmentation - Learning an Optimal Data Augmentation Strategy Abstract: A recurring problem faced when training neural networks is that there is\ntypically not enough data to maximize the generalization capability of deep\nneural networks(DNN). There are many techniques to address this, including data\naugmentation, dropout, and transfer learning. In this paper, we introduce an\nadditional method which we call Smart Augmentation and we show how to use it to\nincrease the accuracy and reduce overfitting on a target network. Smart\nAugmentation works by creating a network that learns how to generate augmented\ndata during the training process of a target network in a way that reduces that\nnetworks loss. This allows us to learn augmentations that minimize the error of\nthat network.\n  Smart Augmentation has shown the potential to increase accuracy by\ndemonstrably significant measures on all datasets tested. In addition, it has\nshown potential to achieve similar or improved performance levels with\nsignificantly smaller network sizes in a number of tested cases. \n\n"}
{"id": "1703.08403", "contents": "Title: Asymmetric Learning Vector Quantization for Efficient Nearest Neighbor\n  Classification in Dynamic Time Warping Spaces Abstract: The nearest neighbor method together with the dynamic time warping (DTW)\ndistance is one of the most popular approaches in time series classification.\nThis method suffers from high storage and computation requirements for large\ntraining sets. As a solution to both drawbacks, this article extends learning\nvector quantization (LVQ) from Euclidean spaces to DTW spaces. The proposed LVQ\nscheme uses asymmetric weighted averaging as update rule. Empirical results\nexhibited superior performance of asymmetric generalized LVQ (GLVQ) over other\nstate-of-the-art prototype generation methods for nearest neighbor\nclassification. \n\n"}
{"id": "1703.10146", "contents": "Title: Community Detection and Stochastic Block Models Abstract: The stochastic block model (SBM) is a random graph model with different group\nof vertices connecting differently. It is widely employed as a canonical model\nto study clustering and community detection, and provides a fertile ground to\nstudy the information-theoretic and computational tradeoffs that arise in\ncombinatorial statistics and more generally data science.\n  This monograph surveys the recent developments that establish the fundamental\nlimits for community detection in the SBM, both with respect to\ninformation-theoretic and computational tradeoffs, and for various recovery\nrequirements such as exact, partial and weak recovery. The main results\ndiscussed are the phase transitions for exact recovery at the\nChernoff-Hellinger threshold, the phase transition for weak recovery at the\nKesten-Stigum threshold, the optimal SNR-mutual information tradeoff for\npartial recovery, and the gap between information-theoretic and computational\nthresholds.\n  The monograph gives a principled derivation of the main algorithms developed\nin the quest of achieving the limits, in particular two-round algorithms via\ngraph-splitting, semi-definite programming, (linearized) belief propagation,\nclassical/nonbacktracking spectral methods and graph powering. Extensions to\nother block models, such as geometric block models, and a few open problems are\nalso discussed. \n\n"}
{"id": "1703.10663", "contents": "Title: Near Perfect Protein Multi-Label Classification with Deep Neural\n  Networks Abstract: Artificial neural networks (ANNs) have gained a well-deserved popularity\namong machine learning tools upon their recent successful applications in\nimage- and sound processing and classification problems. ANNs have also been\napplied for predicting the family or function of a protein, knowing its residue\nsequence. Here we present two new ANNs with multi-label classification ability,\nshowing impressive accuracy when classifying protein sequences into 698 UniProt\nfamilies (AUC=99.99%) and 983 Gene Ontology classes (AUC=99.45%). \n\n"}
{"id": "1703.10717", "contents": "Title: BEGAN: Boundary Equilibrium Generative Adversarial Networks Abstract: We propose a new equilibrium enforcing method paired with a loss derived from\nthe Wasserstein distance for training auto-encoder based Generative Adversarial\nNetworks. This method balances the generator and discriminator during training.\nAdditionally, it provides a new approximate convergence measure, fast and\nstable training and high visual quality. We also derive a way of controlling\nthe trade-off between image diversity and visual quality. We focus on the image\ngeneration task, setting a new milestone in visual quality, even at higher\nresolutions. This is achieved while using a relatively simple model\narchitecture and a standard training procedure. \n\n"}
{"id": "1704.00003", "contents": "Title: Spectral Methods for Nonparametric Models Abstract: Nonparametric models are versatile, albeit computationally expensive, tool\nfor modeling mixture models. In this paper, we introduce spectral methods for\nthe two most popular nonparametric models: the Indian Buffet Process (IBP) and\nthe Hierarchical Dirichlet Process (HDP). We show that using spectral methods\nfor the inference of nonparametric models are computationally and statistically\nefficient. In particular, we derive the lower-order moments of the IBP and the\nHDP, propose spectral algorithms for both models, and provide reconstruction\nguarantees for the algorithms. For the HDP, we further show that applying\nhierarchical models on dataset with hierarchical structure, which can be solved\nwith the generalized spectral HDP, produces better solutions to that of flat\nmodels regarding likelihood performance. \n\n"}
{"id": "1704.01255", "contents": "Title: Linear Additive Markov Processes Abstract: We introduce LAMP: the Linear Additive Markov Process. Transitions in LAMP\nmay be influenced by states visited in the distant history of the process, but\nunlike higher-order Markov processes, LAMP retains an efficient\nparametrization. LAMP also allows the specific dependence on history to be\nlearned efficiently from data. We characterize some theoretical properties of\nLAMP, including its steady-state and mixing time. We then give an algorithm\nbased on alternating minimization to learn LAMP models from data. Finally, we\nperform a series of real-world experiments to show that LAMP is more powerful\nthan first-order Markov processes, and even holds its own against deep\nsequential models (LSTMs) with a negligible increase in parameter complexity. \n\n"}
{"id": "1704.01474", "contents": "Title: Convolutional Neural Networks for Page Segmentation of Historical\n  Document Images Abstract: This paper presents a Convolutional Neural Network (CNN) based page\nsegmentation method for handwritten historical document images. We consider\npage segmentation as a pixel labeling problem, i.e., each pixel is classified\nas one of the predefined classes. Traditional methods in this area rely on\ncarefully hand-crafted features or large amounts of prior knowledge. In\ncontrast, we propose to learn features from raw image pixels using a CNN. While\nmany researchers focus on developing deep CNN architectures to solve different\nproblems, we train a simple CNN with only one convolution layer. We show that\nthe simple architecture achieves competitive results against other deep\narchitectures on different public datasets. Experiments also demonstrate the\neffectiveness and superiority of the proposed method compared to previous\nmethods. \n\n"}
{"id": "1704.01858", "contents": "Title: An Online Hierarchical Algorithm for Extreme Clustering Abstract: Many modern clustering methods scale well to a large number of data items, N,\nbut not to a large number of clusters, K. This paper introduces PERCH, a new\nnon-greedy algorithm for online hierarchical clustering that scales to both\nmassive N and K--a problem setting we term extreme clustering. Our algorithm\nefficiently routes new data points to the leaves of an incrementally-built\ntree. Motivated by the desire for both accuracy and speed, our approach\nperforms tree rotations for the sake of enhancing subtree purity and\nencouraging balancedness. We prove that, under a natural separability\nassumption, our non-greedy algorithm will produce trees with perfect dendrogram\npurity regardless of online data arrival order. Our experiments demonstrate\nthat PERCH constructs more accurate trees than other tree-building clustering\nalgorithms and scales well with both N and K, achieving a higher quality\nclustering than the strongest flat clustering competitor in nearly half the\ntime. \n\n"}
{"id": "1704.02304", "contents": "Title: It Takes (Only) Two: Adversarial Generator-Encoder Networks Abstract: We present a new autoencoder-type architecture that is trainable in an\nunsupervised mode, sustains both generation and inference, and has the quality\nof conditional and unconditional samples boosted by adversarial learning.\nUnlike previous hybrids of autoencoders and adversarial networks, the\nadversarial game in our approach is set up directly between the encoder and the\ngenerator, and no external mappings are trained in the process of learning. The\ngame objective compares the divergences of each of the real and the generated\ndata distributions with the prior distribution in the latent space. We show\nthat direct generator-vs-encoder game leads to a tight coupling of the two\ncomponents, resulting in samples and reconstructions of a comparable quality to\nsome recently-proposed more complex architectures. \n\n"}
{"id": "1704.02882", "contents": "Title: Dynamic Safe Interruptibility for Decentralized Multi-Agent\n  Reinforcement Learning Abstract: In reinforcement learning, agents learn by performing actions and observing\ntheir outcomes. Sometimes, it is desirable for a human operator to\n\\textit{interrupt} an agent in order to prevent dangerous situations from\nhappening. Yet, as part of their learning process, agents may link these\ninterruptions, that impact their reward, to specific states and deliberately\navoid them. The situation is particularly challenging in a multi-agent context\nbecause agents might not only learn from their own past interruptions, but also\nfrom those of other agents. Orseau and Armstrong defined \\emph{safe\ninterruptibility} for one learner, but their work does not naturally extend to\nmulti-agent systems. This paper introduces \\textit{dynamic safe\ninterruptibility}, an alternative definition more suited to decentralized\nlearning problems, and studies this notion in two learning frameworks:\n\\textit{joint action learners} and \\textit{independent learners}. We give\nrealistic sufficient conditions on the learning algorithm to enable dynamic\nsafe interruptibility in the case of joint action learners, yet show that these\nconditions are not sufficient for independent learners. We show however that if\nagents can detect interruptions, it is possible to prune the observations to\nensure dynamic safe interruptibility even for independent learners. \n\n"}
{"id": "1704.03058", "contents": "Title: CERN: Confidence-Energy Recurrent Network for Group Activity Recognition Abstract: This work is about recognizing human activities occurring in videos at\ndistinct semantic levels, including individual actions, interactions, and group\nactivities. The recognition is realized using a two-level hierarchy of Long\nShort-Term Memory (LSTM) networks, forming a feed-forward deep architecture,\nwhich can be trained end-to-end. In comparison with existing architectures of\nLSTMs, we make two key contributions giving the name to our approach as\nConfidence-Energy Recurrent Network -- CERN. First, instead of using the common\nsoftmax layer for prediction, we specify a novel energy layer (EL) for\nestimating the energy of our predictions. Second, rather than finding the\ncommon minimum-energy class assignment, which may be numerically unstable under\nuncertainty, we specify that the EL additionally computes the p-values of the\nsolutions, and in this way estimates the most confident energy minimum. The\nevaluation on the Collective Activity and Volleyball datasets demonstrates: (i)\nadvantages of our two contributions relative to the common softmax and\nenergy-minimization formulations and (ii) a superior performance relative to\nthe state-of-the-art approaches. \n\n"}
{"id": "1704.03144", "contents": "Title: Parametric Gaussian Process Regression for Big Data Abstract: This work introduces the concept of parametric Gaussian processes (PGPs),\nwhich is built upon the seemingly self-contradictory idea of making Gaussian\nprocesses parametric. Parametric Gaussian processes, by construction, are\ndesigned to operate in \"big data\" regimes where one is interested in\nquantifying the uncertainty associated with noisy data. The proposed\nmethodology circumvents the well-established need for stochastic variational\ninference, a scalable algorithm for approximating posterior distributions. The\neffectiveness of the proposed approach is demonstrated using an illustrative\nexample with simulated data and a benchmark dataset in the airline industry\nwith approximately 6 million records. \n\n"}
{"id": "1704.03581", "contents": "Title: P\\'olya Urn Latent Dirichlet Allocation: a doubly sparse massively\n  parallel sampler Abstract: Latent Dirichlet Allocation (LDA) is a topic model widely used in natural\nlanguage processing and machine learning. Most approaches to training the model\nrely on iterative algorithms, which makes it difficult to run LDA on big\ncorpora that are best analyzed in parallel and distributed computational\nenvironments. Indeed, current approaches to parallel inference either don't\nconverge to the correct posterior or require storage of large dense matrices in\nmemory. We present a novel sampler that overcomes both problems, and we show\nthat this sampler is faster, both empirically and theoretically, than previous\nGibbs samplers for LDA. We do so by employing a novel P\\'olya-urn-based\napproximation in the sparse partially collapsed sampler for LDA. We prove that\nthe approximation error vanishes with data size, making our algorithm\nasymptotically exact, a property of importance for large-scale topic models. In\naddition, we show, via an explicit example, that - contrary to popular belief\nin the topic modeling literature - partially collapsed samplers can be more\nefficient than fully collapsed samplers. We conclude by comparing the\nperformance of our algorithm with that of other approaches on well-known\ncorpora. \n\n"}
{"id": "1704.03809", "contents": "Title: A Neural Parametric Singing Synthesizer Abstract: We present a new model for singing synthesis based on a modified version of\nthe WaveNet architecture. Instead of modeling raw waveform, we model features\nproduced by a parametric vocoder that separates the influence of pitch and\ntimbre. This allows conveniently modifying pitch to match any target melody,\nfacilitates training on more modest dataset sizes, and significantly reduces\ntraining and generation times. Our model makes frame-wise predictions using\nmixture density outputs rather than categorical outputs in order to reduce the\nrequired parameter count. As we found overfitting to be an issue with the\nrelatively small datasets used in our experiments, we propose a method to\nregularize the model and make the autoregressive generation process more robust\nto prediction errors. Using a simple multi-stream architecture, harmonic,\naperiodic and voiced/unvoiced components can all be predicted in a coherent\nmanner. We compare our method to existing parametric statistical and\nstate-of-the-art concatenative methods using quantitative metrics and a\nlistening test. While naive implementations of the autoregressive generation\nalgorithm tend to be inefficient, using a smart algorithm we can greatly speed\nup the process and obtain a system that's competitive in both speed and\nquality. \n\n"}
{"id": "1704.04567", "contents": "Title: Asynchronous Parallel Empirical Variance Guided Algorithms for the\n  Thresholding Bandit Problem Abstract: This paper considers the multi-armed thresholding bandit problem --\nidentifying all arms whose expected rewards are above a predefined threshold\nvia as few pulls (or rounds) as possible -- proposed by Locatelli et al. [2016]\nrecently. Although the proposed algorithm in Locatelli et al. [2016] achieves\nthe optimal round complexity in a certain sense, there still remain unsolved\nissues. This paper proposes an asynchronous parallel thresholding algorithm and\nits parameter-free version to improve the efficiency and the applicability. On\none hand, the proposed two algorithms use the empirical variance to guide the\npull decision at each round, and significantly improve the round complexity of\nthe \"optimal\" algorithm when all arms have bounded high order moments. The\nproposed algorithms can be proven to be optimal. On the other hand, most bandit\nalgorithms assume that the reward can be observed immediately after the pull or\nthe next decision would not be made before all rewards are observed. Our\nproposed asynchronous parallel algorithms allow making the choice of the next\npull with unobserved rewards from earlier pulls, which avoids such an\nunrealistic assumption and significantly improves the identification process.\nOur theoretical analysis justifies the effectiveness and the efficiency of\nproposed asynchronous parallel algorithms. \n\n"}
{"id": "1704.05147", "contents": "Title: O$^2$TD: (Near)-Optimal Off-Policy TD Learning Abstract: Temporal difference learning and Residual Gradient methods are the most\nwidely used temporal difference based learning algorithms; however, it has been\nshown that none of their objective functions is optimal w.r.t approximating the\ntrue value function $V$. Two novel algorithms are proposed to approximate the\ntrue value function $V$. This paper makes the following contributions: (1) A\nbatch algorithm that can help find the approximate optimal off-policy\nprediction of the true value function $V$. (2) A linear computational cost (per\nstep) near-optimal algorithm that can learn from a collection of off-policy\nsamples. (3) A new perspective of the emphatic temporal difference learning\nwhich bridges the gap between off-policy optimality and off-policy stability. \n\n"}
{"id": "1704.05193", "contents": "Title: Accelerated Distributed Dual Averaging over Evolving Networks of Growing\n  Connectivity Abstract: We consider the problem of accelerating distributed optimization in\nmulti-agent networks by sequentially adding edges. Specifically, we extend the\ndistributed dual averaging (DDA) subgradient algorithm to evolving networks of\ngrowing connectivity and analyze the corresponding improvement in convergence\nrate. It is known that the convergence rate of DDA is influenced by the\nalgebraic connectivity of the underlying network, where better connectivity\nleads to faster convergence. However, the impact of network topology design on\nthe convergence rate of DDA has not been fully understood. In this paper, we\nbegin by designing network topologies via edge selection and scheduling. For\nedge selection, we determine the best set of candidate edges that achieves the\noptimal tradeoff between the growth of network connectivity and the usage of\nnetwork resources. The dynamics of network evolution is then incurred by edge\nscheduling. Further, we provide a tractable approach to analyze the improvement\nin the convergence rate of DDA induced by the growth of network connectivity.\nOur analysis reveals the connection between network topology design and the\nconvergence rate of DDA, and provides quantitative evaluation of DDA\nacceleration for distributed optimization that is absent in the existing\nanalysis. Lastly, numerical experiments show that DDA can be significantly\naccelerated using a sequence of well-designed networks, and our theoretical\npredictions are well matched to its empirical convergence behavior. \n\n"}
{"id": "1704.08165", "contents": "Title: A Generalization of Convolutional Neural Networks to Graph-Structured\n  Data Abstract: This paper introduces a generalization of Convolutional Neural Networks\n(CNNs) from low-dimensional grid data, such as images, to graph-structured\ndata. We propose a novel spatial convolution utilizing a random walk to uncover\nthe relations within the input, analogous to the way the standard convolution\nuses the spatial neighborhood of a pixel on the grid. The convolution has an\nintuitive interpretation, is efficient and scalable and can also be used on\ndata with varying graph structure. Furthermore, this generalization can be\napplied to many standard regression or classification problems, by learning the\nthe underlying graph. We empirically demonstrate the performance of the\nproposed CNN on MNIST, and challenge the state-of-the-art on Merck molecular\nactivity data set. \n\n"}
{"id": "1705.01204", "contents": "Title: Spectral clustering in the dynamic stochastic block model Abstract: In the present paper, we studied a Dynamic Stochastic Block Model (DSBM)\nunder the assumptions that the connection probabilities, as functions of time,\nare smooth and that at most $s$ nodes can switch their class memberships\nbetween two consecutive time points. We estimate the edge probability tensor by\na kernel-type procedure and extract the group memberships of the nodes by\nspectral clustering. The procedure is computationally viable, adaptive to the\nunknown smoothness of the functional connection probabilities, to the rate $s$\nof membership switching and to the unknown number of clusters. In addition, it\nis accompanied by non-asymptotic guarantees for the precision of estimation and\nclustering. \n\n"}
{"id": "1705.03419", "contents": "Title: Learning Deep Networks from Noisy Labels with Dropout Regularization Abstract: Large datasets often have unreliable labels-such as those obtained from\nAmazon's Mechanical Turk or social media platforms-and classifiers trained on\nmislabeled datasets often exhibit poor performance. We present a simple,\neffective technique for accounting for label noise when training deep neural\nnetworks. We augment a standard deep network with a softmax layer that models\nthe label noise statistics. Then, we train the deep network and noise model\njointly via end-to-end stochastic gradient descent on the (perhaps mislabeled)\ndataset. The augmented model is overdetermined, so in order to encourage the\nlearning of a non-trivial noise model, we apply dropout regularization to the\nweights of the noise model during training. Numerical experiments on noisy\nversions of the CIFAR-10 and MNIST datasets show that the proposed dropout\ntechnique outperforms state-of-the-art methods. \n\n"}
{"id": "1705.03439", "contents": "Title: Frequentist Consistency of Variational Bayes Abstract: A key challenge for modern Bayesian statistics is how to perform scalable\ninference of posterior distributions. To address this challenge, variational\nBayes (VB) methods have emerged as a popular alternative to the classical\nMarkov chain Monte Carlo (MCMC) methods. VB methods tend to be faster while\nachieving comparable predictive performance. However, there are few theoretical\nresults around VB. In this paper, we establish frequentist consistency and\nasymptotic normality of VB methods. Specifically, we connect VB methods to\npoint estimates based on variational approximations, called frequentist\nvariational approximations, and we use the connection to prove a variational\nBernstein-von Mises theorem. The theorem leverages the theoretical\ncharacterizations of frequentist variational approximations to understand\nasymptotic properties of VB. In summary, we prove that (1) the VB posterior\nconverges to the Kullback-Leibler (KL) minimizer of a normal distribution,\ncentered at the truth and (2) the corresponding variational expectation of the\nparameter is consistent and asymptotically normal. As applications of the\ntheorem, we derive asymptotic properties of VB posteriors in Bayesian mixture\nmodels, Bayesian generalized linear mixed models, and Bayesian stochastic block\nmodels. We conduct a simulation study to illustrate these theoretical results. \n\n"}
{"id": "1705.04524", "contents": "Title: Long-term Blood Pressure Prediction with Deep Recurrent Neural Networks Abstract: Existing methods for arterial blood pressure (BP) estimation directly map the\ninput physiological signals to output BP values without explicitly modeling the\nunderlying temporal dependencies in BP dynamics. As a result, these models\nsuffer from accuracy decay over a long time and thus require frequent\ncalibration. In this work, we address this issue by formulating BP estimation\nas a sequence prediction problem in which both the input and target are\ntemporal sequences. We propose a novel deep recurrent neural network (RNN)\nconsisting of multilayered Long Short-Term Memory (LSTM) networks, which are\nincorporated with (1) a bidirectional structure to access larger-scale context\ninformation of input sequence, and (2) residual connections to allow gradients\nin deep RNN to propagate more effectively. The proposed deep RNN model was\ntested on a static BP dataset, and it achieved root mean square error (RMSE) of\n3.90 and 2.66 mmHg for systolic BP (SBP) and diastolic BP (DBP) prediction\nrespectively, surpassing the accuracy of traditional BP prediction models. On a\nmulti-day BP dataset, the deep RNN achieved RMSE of 3.84, 5.25, 5.80 and 5.81\nmmHg for the 1st day, 2nd day, 4th day and 6th month after the 1st day SBP\nprediction, and 1.80, 4.78, 5.0, 5.21 mmHg for corresponding DBP prediction,\nrespectively, which outperforms all previous models with notable improvement.\nThe experimental results suggest that modeling the temporal dependencies in BP\ndynamics significantly improves the long-term BP prediction accuracy. \n\n"}
{"id": "1705.05091", "contents": "Title: Bandit Regret Scaling with the Effective Loss Range Abstract: We study how the regret guarantees of nonstochastic multi-armed bandits can\nbe improved, if the effective range of the losses in each round is small (e.g.\nthe maximal difference between two losses in a given round). Despite a recent\nimpossibility result, we show how this can be made possible under certain mild\nadditional assumptions, such as availability of rough estimates of the losses,\nor advance knowledge of the loss of a single, possibly unspecified arm. Along\nthe way, we develop a novel technique which might be of independent interest,\nto convert any multi-armed bandit algorithm with regret depending on the loss\nrange, to an algorithm with regret depending only on the effective range, while\navoiding predictably bad arms altogether. \n\n"}
{"id": "1705.05524", "contents": "Title: Learning Hard Alignments with Variational Inference Abstract: There has recently been significant interest in hard attention models for\ntasks such as object recognition, visual captioning and speech recognition.\nHard attention can offer benefits over soft attention such as decreased\ncomputational cost, but training hard attention models can be difficult because\nof the discrete latent variables they introduce. Previous work used REINFORCE\nand Q-learning to approach these issues, but those methods can provide\nhigh-variance gradient estimates and be slow to train. In this paper, we tackle\nthe problem of learning hard attention for a sequential task using variational\ninference methods, specifically the recently introduced VIMCO and NVIL.\nFurthermore, we propose a novel baseline that adapts VIMCO to this setting. We\ndemonstrate our method on a phoneme recognition task in clean and noisy\nenvironments and show that our method outperforms REINFORCE, with the\ndifference being greater for a more complicated task. \n\n"}
{"id": "1705.07120", "contents": "Title: VAE with a VampPrior Abstract: Many different methods to train deep generative models have been introduced\nin the past. In this paper, we propose to extend the variational auto-encoder\n(VAE) framework with a new type of prior which we call \"Variational Mixture of\nPosteriors\" prior, or VampPrior for short. The VampPrior consists of a mixture\ndistribution (e.g., a mixture of Gaussians) with components given by\nvariational posteriors conditioned on learnable pseudo-inputs. We further\nextend this prior to a two layer hierarchical model and show that this\narchitecture with a coupled prior and posterior, learns significantly better\nmodels. The model also avoids the usual local optima issues related to useless\nlatent dimensions that plague VAEs. We provide empirical studies on six\ndatasets, namely, static and binary MNIST, OMNIGLOT, Caltech 101 Silhouettes,\nFrey Faces and Histopathology patches, and show that applying the hierarchical\nVampPrior delivers state-of-the-art results on all datasets in the unsupervised\npermutation invariant setting and the best results or comparable to SOTA\nmethods for the approach with convolutional networks. \n\n"}
{"id": "1705.07819", "contents": "Title: Regularizing deep networks using efficient layerwise adversarial\n  training Abstract: Adversarial training has been shown to regularize deep neural networks in\naddition to increasing their robustness to adversarial examples. However, its\nimpact on very deep state of the art networks has not been fully investigated.\nIn this paper, we present an efficient approach to perform adversarial training\nby perturbing intermediate layer activations and study the use of such\nperturbations as a regularizer during training. We use these perturbations to\ntrain very deep models such as ResNets and show improvement in performance both\non adversarial and original test data. Our experiments highlight the benefits\nof perturbing intermediate layer activations compared to perturbing only the\ninputs. The results on CIFAR-10 and CIFAR-100 datasets show the merits of the\nproposed adversarial training approach. Additional results on WideResNets show\nthat our approach provides significant improvement in classification accuracy\nfor a given base model, outperforming dropout and other base models of larger\nsize. \n\n"}
{"id": "1705.08415", "contents": "Title: Supervised Community Detection with Line Graph Neural Networks Abstract: Traditionally, community detection in graphs can be solved using spectral\nmethods or posterior inference under probabilistic graphical models. Focusing\non random graph families such as the stochastic block model, recent research\nhas unified both approaches and identified both statistical and computational\ndetection thresholds in terms of the signal-to-noise ratio. By recasting\ncommunity detection as a node-wise classification problem on graphs, we can\nalso study it from a learning perspective. We present a novel family of Graph\nNeural Networks (GNNs) for solving community detection problems in a supervised\nlearning setting. We show that, in a data-driven manner and without access to\nthe underlying generative models, they can match or even surpass the\nperformance of the belief propagation algorithm on binary and multi-class\nstochastic block models, which is believed to reach the computational\nthreshold. In particular, we propose to augment GNNs with the non-backtracking\noperator defined on the line graph of edge adjacencies. Our models also achieve\ngood performance on real-world datasets. In addition, we perform the first\nanalysis of the optimization landscape of training linear GNNs for community\ndetection problems, demonstrating that under certain simplifications and\nassumptions, the loss values at local and global minima are not far apart. \n\n"}
{"id": "1705.08858", "contents": "Title: Audio-replay attack detection countermeasures Abstract: This paper presents the Speech Technology Center (STC) replay attack\ndetection systems proposed for Automatic Speaker Verification Spoofing and\nCountermeasures Challenge 2017. In this study we focused on comparison of\ndifferent spoofing detection approaches. These were GMM based methods, high\nlevel features extraction with simple classifier and deep learning frameworks.\nExperiments performed on the development and evaluation parts of the challenge\ndataset demonstrated stable efficiency of deep learning approaches in case of\nchanging acoustic conditions. At the same time SVM classifier with high level\nfeatures provided a substantial input in the efficiency of the resulting STC\nsystems according to the fusion systems results. \n\n"}
{"id": "1705.09236", "contents": "Title: Asynchronous Parallel Bayesian Optimisation via Thompson Sampling Abstract: We design and analyse variations of the classical Thompson sampling (TS)\nprocedure for Bayesian optimisation (BO) in settings where function evaluations\nare expensive, but can be performed in parallel. Our theoretical analysis shows\nthat a direct application of the sequential Thompson sampling algorithm in\neither synchronous or asynchronous parallel settings yields a surprisingly\npowerful result: making $n$ evaluations distributed among $M$ workers is\nessentially equivalent to performing $n$ evaluations in sequence. Further, by\nmodeling the time taken to complete a function evaluation, we show that, under\na time constraint, asynchronously parallel TS achieves asymptotically lower\nregret than both the synchronous and sequential versions. These results are\ncomplemented by an experimental analysis, showing that asynchronous TS\noutperforms a suite of existing parallel BO algorithms in simulations and in a\nhyper-parameter tuning application in convolutional neural networks. In\naddition to these, the proposed procedure is conceptually and computationally\nmuch simpler than existing work for parallel BO. \n\n"}
{"id": "1705.10813", "contents": "Title: Large Linear Multi-output Gaussian Process Learning Abstract: Gaussian processes (GPs), or distributions over arbitrary functions in a\ncontinuous domain, can be generalized to the multi-output case: a linear model\nof coregionalization (LMC) is one approach. LMCs estimate and exploit\ncorrelations across the multiple outputs. While model estimation can be\nperformed efficiently for single-output GPs, these assume stationarity, but in\nthe multi-output case the cross-covariance interaction is not stationary. We\npropose Large Linear GP (LLGP), which circumvents the need for stationarity by\ninducing structure in the LMC kernel through a common grid of inputs shared\nbetween outputs, enabling optimization of GP hyperparameters for\nmulti-dimensional outputs and low-dimensional inputs. When applied to synthetic\ntwo-dimensional and real time series data, we find our theoretical improvement\nrelative to the current solutions for multi-output GPs is realized with LLGP\nreducing training time while improving or maintaining predictive mean accuracy.\nMoreover, by using a direct likelihood approximation rather than a variational\none, model confidence estimates are significantly improved. \n\n"}
{"id": "1705.10817", "contents": "Title: Dynamics Based Features For Graph Classification Abstract: Numerous social, medical, engineering and biological challenges can be framed\nas graph-based learning tasks. Here, we propose a new feature based approach to\nnetwork classification. We show how dynamics on a network can be useful to\nreveal patterns about the organization of the components of the underlying\ngraph where the process takes place. We define generalized assortativities on\nnetworks and use them as generalized features across multiple time scales.\nThese features turn out to be suitable signatures for discriminating between\ndifferent classes of networks. Our method is evaluated empirically on\nestablished network benchmarks. We also introduce a new dataset of human brain\nnetworks (connectomes) and use it to evaluate our method. Results reveal that\nour dynamics based features are competitive and often outperform state of the\nart accuracies. \n\n"}
{"id": "1706.00051", "contents": "Title: Deep Generative Adversarial Networks for Compressed Sensing Automates\n  MRI Abstract: Magnetic resonance image (MRI) reconstruction is a severely ill-posed linear\ninverse task demanding time and resource intensive computations that can\nsubstantially trade off {\\it accuracy} for {\\it speed} in real-time imaging. In\naddition, state-of-the-art compressed sensing (CS) analytics are not cognizant\nof the image {\\it diagnostic quality}. To cope with these challenges we put\nforth a novel CS framework that permeates benefits from generative adversarial\nnetworks (GAN) to train a (low-dimensional) manifold of diagnostic-quality MR\nimages from historical patients. Leveraging a mixture of least-squares (LS)\nGANs and pixel-wise $\\ell_1$ cost, a deep residual network with skip\nconnections is trained as the generator that learns to remove the {\\it\naliasing} artifacts by projecting onto the manifold. LSGAN learns the texture\ndetails, while $\\ell_1$ controls the high-frequency noise. A multilayer\nconvolutional neural network is then jointly trained based on diagnostic\nquality images to discriminate the projection quality. The test phase performs\nfeed-forward propagation over the generator network that demands a very low\ncomputational overhead. Extensive evaluations are performed on a large\ncontrast-enhanced MR dataset of pediatric patients. In particular, images rated\nbased on expert radiologists corroborate that GANCS retrieves high contrast\nimages with detailed texture relative to conventional CS, and pixel-wise\nschemes. In addition, it offers reconstruction under a few milliseconds, two\norders of magnitude faster than state-of-the-art CS-MRI schemes. \n\n"}
{"id": "1706.00119", "contents": "Title: Bayesian fairness Abstract: We consider the problem of how decision making can be fair when the\nunderlying probabilistic model of the world is not known with certainty. We\nargue that recent notions of fairness in machine learning need to explicitly\nincorporate parameter uncertainty, hence we introduce the notion of {\\em\nBayesian fairness} as a suitable candidate for fair decision rules. Using\nbalance, a definition of fairness introduced by Kleinberg et al (2016), we show\nhow a Bayesian perspective can lead to well-performing, fair decision rules\neven under high uncertainty. \n\n"}
{"id": "1706.00550", "contents": "Title: On Unifying Deep Generative Models Abstract: Deep generative models have achieved impressive success in recent years.\nGenerative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), as\nemerging families for generative model learning, have largely been considered\nas two distinct paradigms and received extensive independent studies\nrespectively. This paper aims to establish formal connections between GANs and\nVAEs through a new formulation of them. We interpret sample generation in GANs\nas performing posterior inference, and show that GANs and VAEs involve\nminimizing KL divergences of respective posterior and inference distributions\nwith opposite directions, extending the two learning phases of classic\nwake-sleep algorithm, respectively. The unified view provides a powerful tool\nto analyze a diverse set of existing model variants, and enables to transfer\ntechniques across research lines in a principled way. For example, we apply the\nimportance weighting method in VAE literatures for improved GAN learning, and\nenhance VAEs with an adversarial mechanism that leverages generated samples.\nExperiments show generality and effectiveness of the transferred techniques. \n\n"}
{"id": "1706.00856", "contents": "Title: Multiple Kernel Learning and Automatic Subspace Relevance Determination\n  for High-dimensional Neuroimaging Data Abstract: Alzheimer's disease is a major cause of dementia. Its diagnosis requires\naccurate biomarkers that are sensitive to disease stages. In this respect, we\nregard probabilistic classification as a method of designing a probabilistic\nbiomarker for disease staging. Probabilistic biomarkers naturally support the\ninterpretation of decisions and evaluation of uncertainty associated with them.\nIn this paper, we obtain probabilistic biomarkers via Gaussian Processes.\nGaussian Processes enable probabilistic kernel machines that offer flexible\nmeans to accomplish Multiple Kernel Learning. Exploiting this flexibility, we\npropose a new variation of Automatic Relevance Determination and tackle the\nchallenges of high dimensionality through multiple kernels. Our research\nresults demonstrate that the Gaussian Process models are competitive with or\nbetter than the well-known Support Vector Machine in terms of classification\nperformance even in the cases of single kernel learning. Extending the basic\nscheme towards the Multiple Kernel Learning, we improve the efficacy of the\nGaussian Process models and their interpretability in terms of the known\nanatomical correlates of the disease. For instance, the disease pathology\nstarts in and around the hippocampus and entorhinal cortex. Through the use of\nGaussian Processes and Multiple Kernel Learning, we have automatically and\nefficiently determined those portions of neuroimaging data. In addition to\ntheir interpretability, our Gaussian Process models are competitive with recent\ndeep learning solutions under similar settings. \n\n"}
{"id": "1706.01214", "contents": "Title: Inconsistent Node Flattening for Improving Top-down Hierarchical\n  Classification Abstract: Large-scale classification of data where classes are structurally organized\nin a hierarchy is an important area of research. Top-down approaches that\nexploit the hierarchy during the learning and prediction phase are efficient\nfor large scale hierarchical classification. However, accuracy of top-down\napproaches is poor due to error propagation i.e., prediction errors made at\nhigher levels in the hierarchy cannot be corrected at lower levels. One of the\nmain reason behind errors at the higher levels is the presence of inconsistent\nnodes that are introduced due to the arbitrary process of creating these\nhierarchies by domain experts. In this paper, we propose two different\ndata-driven approaches (local and global) for hierarchical structure\nmodification that identifies and flattens inconsistent nodes present within the\nhierarchy. Our extensive empirical evaluation of the proposed approaches on\nseveral image and text datasets with varying distribution of features, classes\nand training instances per class shows improved classification performance over\ncompeting hierarchical modification approaches. Specifically, we see an\nimprovement upto 7% in Macro-F1 score with our approach over best TD baseline.\nSOURCE CODE: http://www.cs.gmu.edu/~mlbio/InconsistentNodeFlattening \n\n"}
{"id": "1706.01604", "contents": "Title: Hyperplane Clustering Via Dual Principal Component Pursuit Abstract: We extend the theoretical analysis of a recently proposed single subspace\nlearning algorithm, called Dual Principal Component Pursuit (DPCP), to the case\nwhere the data are drawn from of a union of hyperplanes. To gain insight into\nthe properties of the $\\ell_1$ non-convex problem associated with DPCP, we\ndevelop a geometric analysis of a closely related continuous optimization\nproblem. Then transferring this analysis to the discrete problem, our results\nstate that as long as the hyperplanes are sufficiently separated, the dominant\nhyperplane is sufficiently dominant and the points are uniformly distributed\ninside the associated hyperplanes, then the non-convex DPCP problem has a\nunique global solution, equal to the normal vector of the dominant hyperplane.\nThis suggests the correctness of a sequential hyperplane learning algorithm\nbased on DPCP. A thorough experimental evaluation reveals that hyperplane\nlearning schemes based on DPCP dramatically improve over the state-of-the-art\nmethods for the case of synthetic data, while are competitive to the\nstate-of-the-art in the case of 3D plane clustering for Kinect data. \n\n"}
{"id": "1706.01686", "contents": "Title: Limitations on Variance-Reduction and Acceleration Schemes for Finite\n  Sum Optimization Abstract: We study the conditions under which one is able to efficiently apply\nvariance-reduction and acceleration schemes on finite sum optimization\nproblems. First, we show that, perhaps surprisingly, the finite sum structure\nby itself, is not sufficient for obtaining a complexity bound of\n$\\tilde{\\cO}((n+L/\\mu)\\ln(1/\\epsilon))$ for $L$-smooth and $\\mu$-strongly\nconvex individual functions - one must also know which individual function is\nbeing referred to by the oracle at each iteration. Next, we show that for a\nbroad class of first-order and coordinate-descent finite sum algorithms\n(including, e.g., SDCA, SVRG, SAG), it is not possible to get an `accelerated'\ncomplexity bound of $\\tilde{\\cO}((n+\\sqrt{n L/\\mu})\\ln(1/\\epsilon))$, unless\nthe strong convexity parameter is given explicitly. Lastly, we show that when\nthis class of algorithms is used for minimizing $L$-smooth and convex finite\nsums, the optimal complexity bound is $\\tilde{\\cO}(n+L/\\epsilon)$, assuming\nthat (on average) the same update rule is used in every iteration, and\n$\\tilde{\\cO}(n+\\sqrt{nL/\\epsilon})$, otherwise. \n\n"}
{"id": "1706.02952", "contents": "Title: TIP: Typifying the Interpretability of Procedures Abstract: We provide a novel notion of what it means to be interpretable, looking past\nthe usual association with human understanding. Our key insight is that\ninterpretability is not an absolute concept and so we define it relative to a\ntarget model, which may or may not be a human. We define a framework that\nallows for comparing interpretable procedures by linking them to important\npractical aspects such as accuracy and robustness. We characterize many of the\ncurrent state-of-the-art interpretable methods in our framework portraying its\ngeneral applicability. Finally, principled interpretable strategies are\nproposed and empirically evaluated on synthetic data, as well as on the largest\npublic olfaction dataset that was made recently available \\cite{olfs}. We also\nexperiment on MNIST with a simple target model and different oracle models of\nvarying complexity. This leads to the insight that the improvement in the\ntarget model is not only a function of the oracle model's performance, but also\nits relative complexity with respect to the target model. Further experiments\non CIFAR-10, a real manufacturing dataset and FICO dataset showcase the benefit\nof our methods over Knowledge Distillation when the target models are simple\nand the complex model is a neural network. \n\n"}
{"id": "1706.03369", "contents": "Title: On the Sampling Problem for Kernel Quadrature Abstract: The standard Kernel Quadrature method for numerical integration with random\npoint sets (also called Bayesian Monte Carlo) is known to converge in root mean\nsquare error at a rate determined by the ratio $s/d$, where $s$ and $d$ encode\nthe smoothness and dimension of the integrand. However, an empirical\ninvestigation reveals that the rate constant $C$ is highly sensitive to the\ndistribution of the random points. In contrast to standard Monte Carlo\nintegration, for which optimal importance sampling is well-understood, the\nsampling distribution that minimises $C$ for Kernel Quadrature does not admit a\nclosed form. This paper argues that the practical choice of sampling\ndistribution is an important open problem. One solution is considered; a novel\nautomatic approach based on adaptive tempering and sequential Monte Carlo.\nEmpirical results demonstrate a dramatic reduction in integration error of up\nto 4 orders of magnitude can be achieved with the proposed method. \n\n"}
{"id": "1706.03692", "contents": "Title: SEVEN: Deep Semi-supervised Verification Networks Abstract: Verification determines whether two samples belong to the same class or not,\nand has important applications such as face and fingerprint verification, where\nthousands or millions of categories are present but each category has scarce\nlabeled examples, presenting two major challenges for existing deep learning\nmodels. We propose a deep semi-supervised model named SEmi-supervised\nVErification Network (SEVEN) to address these challenges. The model consists of\ntwo complementary components. The generative component addresses the lack of\nsupervision within each category by learning general salient structures from a\nlarge amount of data across categories. The discriminative component exploits\nthe learned general features to mitigate the lack of supervision within\ncategories, and also directs the generative component to find more informative\nstructures of the whole data manifold. The two components are tied together in\nSEVEN to allow an end-to-end training of the two components. Extensive\nexperiments on four verification tasks demonstrate that SEVEN significantly\noutperforms other state-of-the-art deep semi-supervised techniques when labeled\ndata are in short supply. Furthermore, SEVEN is competitive with fully\nsupervised baselines trained with a larger amount of labeled data. It indicates\nthe importance of the generative component in SEVEN. \n\n"}
{"id": "1706.04097", "contents": "Title: Provable Alternating Gradient Descent for Non-negative Matrix\n  Factorization with Strong Correlations Abstract: Non-negative matrix factorization is a basic tool for decomposing data into\nthe feature and weight matrices under non-negativity constraints, and in\npractice is often solved in the alternating minimization framework. However, it\nis unclear whether such algorithms can recover the ground-truth feature matrix\nwhen the weights for different features are highly correlated, which is common\nin applications. This paper proposes a simple and natural alternating gradient\ndescent based algorithm, and shows that with a mild initialization it provably\nrecovers the ground-truth in the presence of strong correlations. In most\ninteresting cases, the correlation can be in the same order as the highest\npossible. Our analysis also reveals its several favorable features including\nrobustness to noise. We complement our theoretical results with empirical\nstudies on semi-synthetic datasets, demonstrating its advantage over several\npopular methods in recovering the ground-truth. \n\n"}
{"id": "1706.05350", "contents": "Title: L2 Regularization versus Batch and Weight Normalization Abstract: Batch Normalization is a commonly used trick to improve the training of deep\nneural networks. These neural networks use L2 regularization, also called\nweight decay, ostensibly to prevent overfitting. However, we show that L2\nregularization has no regularizing effect when combined with normalization.\nInstead, regularization has an influence on the scale of weights, and thereby\non the effective learning rate. We investigate this dependence, both in theory,\nand experimentally. We show that popular optimization methods such as ADAM only\npartially eliminate the influence of normalization on the learning rate. This\nleads to a discussion on other ways to mitigate this issue. \n\n"}
{"id": "1706.05378", "contents": "Title: A framework for Multi-A(rmed)/B(andit) testing with online FDR control Abstract: We propose an alternative framework to existing setups for controlling false\nalarms when multiple A/B tests are run over time. This setup arises in many\npractical applications, e.g. when pharmaceutical companies test new treatment\noptions against control pills for different diseases, or when internet\ncompanies test their default webpages versus various alternatives over time.\nOur framework proposes to replace a sequence of A/B tests by a sequence of\nbest-arm MAB instances, which can be continuously monitored by the data\nscientist. When interleaving the MAB tests with an an online false discovery\nrate (FDR) algorithm, we can obtain the best of both worlds: low sample\ncomplexity and any time online FDR control. Our main contributions are: (i) to\npropose reasonable definitions of a null hypothesis for MAB instances; (ii) to\ndemonstrate how one can derive an always-valid sequential p-value that allows\ncontinuous monitoring of each MAB test; and (iii) to show that using rejection\nthresholds of online-FDR algorithms as the confidence levels for the MAB\nalgorithms results in both sample-optimality, high power and low FDR at any\npoint in time. We run extensive simulations to verify our claims, and also\nreport results on real data collected from the New Yorker Cartoon Caption\ncontest. \n\n"}
{"id": "1706.06060", "contents": "Title: Consistent feature attribution for tree ensembles Abstract: Note that a newer expanded version of this paper is now available at:\narXiv:1802.03888\n  It is critical in many applications to understand what features are important\nfor a model, and why individual predictions were made. For tree ensemble\nmethods these questions are usually answered by attributing importance values\nto input features, either globally or for a single prediction. Here we show\nthat current feature attribution methods are inconsistent, which means changing\nthe model to rely more on a given feature can actually decrease the importance\nassigned to that feature. To address this problem we develop fast exact\nsolutions for SHAP (SHapley Additive exPlanation) values, which were recently\nshown to be the unique additive feature attribution method based on conditional\nexpectations that is both consistent and locally accurate. We integrate these\nimprovements into the latest version of XGBoost, demonstrate the\ninconsistencies of current methods, and show how using SHAP values results in\nsignificantly improved supervised clustering performance. Feature importance\nvalues are a key part of understanding widely used models such as gradient\nboosting trees and random forests, so improvements to them have broad practical\nimplications. \n\n"}
{"id": "1706.06341", "contents": "Title: SPLBoost: An Improved Robust Boosting Algorithm Based on Self-paced\n  Learning Abstract: It is known that Boosting can be interpreted as a gradient descent technique\nto minimize an underlying loss function. Specifically, the underlying loss\nbeing minimized by the traditional AdaBoost is the exponential loss, which is\nproved to be very sensitive to random noise/outliers. Therefore, several\nBoosting algorithms, e.g., LogitBoost and SavageBoost, have been proposed to\nimprove the robustness of AdaBoost by replacing the exponential loss with some\ndesigned robust loss functions. In this work, we present a new way to robustify\nAdaBoost, i.e., incorporating the robust learning idea of Self-paced Learning\n(SPL) into Boosting framework. Specifically, we design a new robust Boosting\nalgorithm based on SPL regime, i.e., SPLBoost, which can be easily implemented\nby slightly modifying off-the-shelf Boosting packages. Extensive experiments\nand a theoretical characterization are also carried out to illustrate the\nmerits of the proposed SPLBoost. \n\n"}
{"id": "1706.07535", "contents": "Title: Efficient Approximate Solutions to Mutual Information Based Global\n  Feature Selection Abstract: Mutual Information (MI) is often used for feature selection when developing\nclassifier models. Estimating the MI for a subset of features is often\nintractable. We demonstrate, that under the assumptions of conditional\nindependence, MI between a subset of features can be expressed as the\nConditional Mutual Information (CMI) between pairs of features. But selecting\nfeatures with the highest CMI turns out to be a hard combinatorial problem. In\nthis work, we have applied two unique global methods, Truncated Power Method\n(TPower) and Low Rank Bilinear Approximation (LowRank), to solve the feature\nselection problem. These algorithms provide very good approximations to the\nNP-hard CMI based feature selection problem. We experimentally demonstrate the\neffectiveness of these procedures across multiple datasets and compare them\nwith existing MI based global and iterative feature selection procedures. \n\n"}
{"id": "1706.07896", "contents": "Title: Reservoir Computing on the Hypersphere Abstract: Reservoir Computing (RC) refers to a Recurrent Neural Networks (RNNs)\nframework, frequently used for sequence learning and time series prediction.\nThe RC system consists of a random fixed-weight RNN (the input-hidden reservoir\nlayer) and a classifier (the hidden-output readout layer). Here we focus on the\nsequence learning problem, and we explore a different approach to RC. More\nspecifically, we remove the non-linear neural activation function, and we\nconsider an orthogonal reservoir acting on normalized states on the unit\nhypersphere. Surprisingly, our numerical results show that the system's memory\ncapacity exceeds the dimensionality of the reservoir, which is the upper bound\nfor the typical RC approach based on Echo State Networks (ESNs). We also show\nhow the proposed system can be applied to symmetric cryptography problems, and\nwe include a numerical implementation. \n\n"}
{"id": "1706.08359", "contents": "Title: GPU-acceleration for Large-scale Tree Boosting Abstract: In this paper, we present a novel massively parallel algorithm for\naccelerating the decision tree building procedure on GPUs (Graphics Processing\nUnits), which is a crucial step in Gradient Boosted Decision Tree (GBDT) and\nrandom forests training. Previous GPU based tree building algorithms are based\non parallel multi-scan or radix sort to find the exact tree split, and thus\nsuffer from scalability and performance issues. We show that using a histogram\nbased algorithm to approximately find the best split is more efficient and\nscalable on GPU. By identifying the difference between classical GPU-based\nimage histogram construction and the feature histogram construction in decision\ntree training, we develop a fast feature histogram building kernel on GPU with\ncarefully designed computational and memory access sequence to reduce atomic\nupdate conflict and maximize GPU utilization. Our algorithm can be used as a\ndrop-in replacement for histogram construction in popular tree boosting systems\nto improve their scalability. As an example, to train GBDT on epsilon dataset,\nour method using a main-stream GPU is 7-8 times faster than histogram based\nalgorithm on CPU in LightGBM and 25 times faster than the exact-split finding\nalgorithm in XGBoost on a dual-socket 28-core Xeon server, while achieving\nsimilar prediction accuracy. \n\n"}
{"id": "1706.08936", "contents": "Title: Fast Algorithms for Learning Latent Variables in Graphical Models Abstract: We study the problem of learning latent variables in Gaussian graphical\nmodels. Existing methods for this problem assume that the precision matrix of\nthe observed variables is the superposition of a sparse and a low-rank\ncomponent. In this paper, we focus on the estimation of the low-rank component,\nwhich encodes the effect of marginalization over the latent variables. We\nintroduce fast, proper learning algorithms for this problem. In contrast with\nexisting approaches, our algorithms are manifestly non-convex. We support their\nefficacy via a rigorous theoretical analysis, and show that our algorithms\nmatch the best possible in terms of sample complexity, while achieving\ncomputational speed-ups over existing methods. We complement our theory with\nseveral numerical experiments. \n\n"}
{"id": "1707.00622", "contents": "Title: Rank Determination for Low-Rank Data Completion Abstract: Recently, fundamental conditions on the sampling patterns have been obtained\nfor finite completability of low-rank matrices or tensors given the\ncorresponding ranks. In this paper, we consider the scenario where the rank is\nnot given and we aim to approximate the unknown rank based on the location of\nsampled entries and some given completion. We consider a number of data models,\nincluding single-view matrix, multi-view matrix, CP tensor, tensor-train tensor\nand Tucker tensor. For each of these data models, we provide an upper bound on\nthe rank when an arbitrary low-rank completion is given. We characterize these\nbounds both deterministically, i.e., with probability one given that the\nsampling pattern satisfies certain combinatorial properties, and\nprobabilistically, i.e., with high probability given that the sampling\nprobability is above some threshold. Moreover, for both single-view matrix and\nCP tensor, we are able to show that the obtained upper bound is exactly equal\nto the unknown rank if the lowest-rank completion is given. Furthermore, we\nprovide numerical experiments for the case of single-view matrix, where we use\nnuclear norm minimization to find a low-rank completion of the sampled data and\nwe observe that in most of the cases the proposed upper bound on the rank is\nequal to the true rank. \n\n"}
{"id": "1707.03269", "contents": "Title: Q-Learning Algorithm for VoLTE Closed-Loop Power Control in Indoor Small\n  Cells Abstract: We propose a reinforcement learning (RL) based closed loop power control\nalgorithm for the downlink of the voice over LTE (VoLTE) radio bearer for an\nindoor environment served by small cells. The main contributions of our paper\nare to 1) use RL to solve performance tuning problems in an indoor cellular\nnetwork for voice bearers and 2) show that our derived lower bound loss in\neffective signal to interference plus noise ratio due to neighboring cell\nfailure is sufficient for VoLTE power control purposes in practical cellular\nnetworks. In our simulation, the proposed RL-based power control algorithm\nsignificantly improves both voice retainability and mean opinion score compared\nto current industry standards. The improvement is due to maintaining an\neffective downlink signal to interference plus noise ratio against adverse\nnetwork operational issues and faults. \n\n"}
{"id": "1707.03718", "contents": "Title: LinkNet: Exploiting Encoder Representations for Efficient Semantic\n  Segmentation Abstract: Pixel-wise semantic segmentation for visual scene understanding not only\nneeds to be accurate, but also efficient in order to find any use in real-time\napplication. Existing algorithms even though are accurate but they do not focus\non utilizing the parameters of neural network efficiently. As a result they are\nhuge in terms of parameters and number of operations; hence slow too. In this\npaper, we propose a novel deep neural network architecture which allows it to\nlearn without any significant increase in number of parameters. Our network\nuses only 11.5 million parameters and 21.2 GFLOPs for processing an image of\nresolution 3x640x360. It gives state-of-the-art performance on CamVid and\ncomparable results on Cityscapes dataset. We also compare our networks\nprocessing time on NVIDIA GPU and embedded system device with existing\nstate-of-the-art architectures for different image resolutions. \n\n"}
{"id": "1707.03821", "contents": "Title: Process Monitoring on Sequences of System Call Count Vectors Abstract: We introduce a methodology for efficient monitoring of processes running on\nhosts in a corporate network. The methodology is based on collecting streams of\nsystem calls produced by all or selected processes on the hosts, and sending\nthem over the network to a monitoring server, where machine learning algorithms\nare used to identify changes in process behavior due to malicious activity,\nhardware failures, or software errors. The methodology uses a sequence of\nsystem call count vectors as the data format which can handle large and varying\nvolumes of data.\n  Unlike previous approaches, the methodology introduced in this paper is\nsuitable for distributed collection and processing of data in large corporate\nnetworks. We evaluate the methodology both in a laboratory setting on a\nreal-life setup and provide statistics characterizing performance and accuracy\nof the methodology. \n\n"}
{"id": "1707.03858", "contents": "Title: Gradient Coding from Cyclic MDS Codes and Expander Graphs Abstract: Gradient coding is a technique for straggler mitigation in distributed\nlearning. In this paper we design novel gradient codes using tools from\nclassical coding theory, namely, cyclic MDS codes, which compare favorably with\nexisting solutions, both in the applicable range of parameters and in the\ncomplexity of the involved algorithms. Second, we introduce an approximate\nvariant of the gradient coding problem, in which we settle for approximate\ngradient computation instead of the exact one. This approach enables graceful\ndegradation, i.e., the $\\ell_2$ error of the approximate gradient is a\ndecreasing function of the number of stragglers. Our main result is that\nnormalized adjacency matrices of expander graphs yield excellent approximate\ngradient codes, which enable significantly less computation compared to exact\ngradient coding, and guarantee faster convergence than trivial solutions under\nstandard assumptions. We experimentally test our approach on Amazon EC2, and\nshow that the generalization error of approximate gradient coding is very close\nto the full gradient while requiring significantly less computation from the\nworkers. \n\n"}
{"id": "1707.06213", "contents": "Title: Analysis of $p$-Laplacian Regularization in Semi-Supervised Learning Abstract: We investigate a family of regression problems in a semi-supervised setting.\nThe task is to assign real-valued labels to a set of $n$ sample points,\nprovided a small training subset of $N$ labeled points. A goal of\nsemi-supervised learning is to take advantage of the (geometric) structure\nprovided by the large number of unlabeled data when assigning labels. We\nconsider random geometric graphs, with connection radius $\\epsilon(n)$, to\nrepresent the geometry of the data set. Functionals which model the task reward\nthe regularity of the estimator function and impose or reward the agreement\nwith the training data. Here we consider the discrete $p$-Laplacian\nregularization.\n  We investigate asymptotic behavior when the number of unlabeled points\nincreases, while the number of training points remains fixed. We uncover a\ndelicate interplay between the regularizing nature of the functionals\nconsidered and the nonlocality inherent to the graph constructions. We\nrigorously obtain almost optimal ranges on the scaling of $\\epsilon(n)$ for the\nasymptotic consistency to hold. We prove that the minimizers of the discrete\nfunctionals in random setting converge uniformly to the desired continuum\nlimit. Furthermore we discover that for the standard model used there is a\nrestrictive upper bound on how quickly $\\epsilon(n)$ must converge to zero as\n$n \\to \\infty$. We introduce a new model which is as simple as the original\nmodel, but overcomes this restriction. \n\n"}
{"id": "1707.07012", "contents": "Title: Learning Transferable Architectures for Scalable Image Recognition Abstract: Developing neural network image classification models often requires\nsignificant architecture engineering. In this paper, we study a method to learn\nthe model architectures directly on the dataset of interest. As this approach\nis expensive when the dataset is large, we propose to search for an\narchitectural building block on a small dataset and then transfer the block to\na larger dataset. The key contribution of this work is the design of a new\nsearch space (the \"NASNet search space\") which enables transferability. In our\nexperiments, we search for the best convolutional layer (or \"cell\") on the\nCIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking\ntogether more copies of this cell, each with their own parameters to design a\nconvolutional architecture, named \"NASNet architecture\". We also introduce a\nnew regularization technique called ScheduledDropPath that significantly\nimproves generalization in the NASNet models. On CIFAR-10 itself, NASNet\nachieves 2.4% error rate, which is state-of-the-art. On ImageNet, NASNet\nachieves, among the published works, state-of-the-art accuracy of 82.7% top-1\nand 96.2% top-5 on ImageNet. Our model is 1.2% better in top-1 accuracy than\nthe best human-invented architectures while having 9 billion fewer FLOPS - a\nreduction of 28% in computational demand from the previous state-of-the-art\nmodel. When evaluated at different levels of computational cost, accuracies of\nNASNets exceed those of the state-of-the-art human-designed models. For\ninstance, a small version of NASNet also achieves 74% top-1 accuracy, which is\n3.1% better than equivalently-sized, state-of-the-art models for mobile\nplatforms. Finally, the learned features by NASNet used with the Faster-RCNN\nframework surpass state-of-the-art by 4.0% achieving 43.1% mAP on the COCO\ndataset. \n\n"}
{"id": "1707.08238", "contents": "Title: A Nearly Instance Optimal Algorithm for Top-k Ranking under the\n  Multinomial Logit Model Abstract: We study the active learning problem of top-$k$ ranking from multi-wise\ncomparisons under the popular multinomial logit model. Our goal is to identify\nthe top-$k$ items with high probability by adaptively querying sets for\ncomparisons and observing the noisy output of the most preferred item from each\ncomparison. To achieve this goal, we design a new active ranking algorithm\nwithout using any information about the underlying items' preference scores. We\nalso establish a matching lower bound on the sample complexity even when the\nset of preference scores is given to the algorithm. These two results together\nshow that the proposed algorithm is nearly instance optimal (similar to\ninstance optimal [FLN03], but up to polylog factors). Our work extends the\nexisting literature on rank aggregation in three directions. First, instead of\nstudying a static problem with fixed data, we investigate the top-$k$ ranking\nproblem in an active learning setting. Second, we show our algorithm is nearly\ninstance optimal, which is a much stronger theoretical guarantee. Finally, we\nextend the pairwise comparison to the multi-wise comparison, which has not been\nfully explored in ranking literature. \n\n"}
{"id": "1707.08552", "contents": "Title: A Robust Multi-Batch L-BFGS Method for Machine Learning Abstract: This paper describes an implementation of the L-BFGS method designed to deal\nwith two adversarial situations. The first occurs in distributed computing\nenvironments where some of the computational nodes devoted to the evaluation of\nthe function and gradient are unable to return results on time. A similar\nchallenge occurs in a multi-batch approach in which the data points used to\ncompute function and gradients are purposely changed at each iteration to\naccelerate the learning process. Difficulties arise because L-BFGS employs\ngradient differences to update the Hessian approximations, and when these\ngradients are computed using different data points the updating process can be\nunstable. This paper shows how to perform stable quasi-Newton updating in the\nmulti-batch setting, studies the convergence properties for both convex and\nnonconvex functions, and illustrates the behavior of the algorithm in a\ndistributed computing platform on binary classification logistic regression and\nneural network training problems that arise in machine learning. \n\n"}
{"id": "1707.09157", "contents": "Title: Efficient Algorithms for Non-convex Isotonic Regression through\n  Submodular Optimization Abstract: We consider the minimization of submodular functions subject to ordering\nconstraints. We show that this optimization problem can be cast as a convex\noptimization problem on a space of uni-dimensional measures, with ordering\nconstraints corresponding to first-order stochastic dominance. We propose new\ndiscretization schemes that lead to simple and efficient algorithms based on\nzero-th, first, or higher order oracles; these algorithms also lead to\nimprovements without isotonic constraints. Finally, our experiments show that\nnon-convex loss functions can be much more robust to outliers for isotonic\nregression, while still leading to an efficient optimization problem. \n\n"}
{"id": "1707.09457", "contents": "Title: Men Also Like Shopping: Reducing Gender Bias Amplification using\n  Corpus-level Constraints Abstract: Language is increasingly being used to define rich visual recognition\nproblems with supporting image collections sourced from the web. Structured\nprediction models are used in these tasks to take advantage of correlations\nbetween co-occurring labels and visual input but risk inadvertently encoding\nsocial biases found in web corpora. In this work, we study data and models\nassociated with multilabel object classification and visual semantic role\nlabeling. We find that (a) datasets for these tasks contain significant gender\nbias and (b) models trained on these datasets further amplify existing bias.\nFor example, the activity cooking is over 33% more likely to involve females\nthan males in a training set, and a trained model further amplifies the\ndisparity to 68% at test time. We propose to inject corpus-level constraints\nfor calibrating existing structured prediction models and design an algorithm\nbased on Lagrangian relaxation for collective inference. Our method results in\nalmost no performance loss for the underlying recognition task but decreases\nthe magnitude of bias amplification by 47.5% and 40.5% for multilabel\nclassification and visual semantic role labeling, respectively. \n\n"}
{"id": "1707.09971", "contents": "Title: Spectral Method and Regularized MLE Are Both Optimal for Top-$K$ Ranking Abstract: This paper is concerned with the problem of top-$K$ ranking from pairwise\ncomparisons. Given a collection of $n$ items and a few pairwise comparisons\nacross them, one wishes to identify the set of $K$ items that receive the\nhighest ranks. To tackle this problem, we adopt the logistic parametric model\n--- the Bradley-Terry-Luce model, where each item is assigned a latent\npreference score, and where the outcome of each pairwise comparison depends\nsolely on the relative scores of the two items involved. Recent works have made\nsignificant progress towards characterizing the performance (e.g. the mean\nsquare error for estimating the scores) of several classical methods, including\nthe spectral method and the maximum likelihood estimator (MLE). However, where\nthey stand regarding top-$K$ ranking remains unsettled.\n  We demonstrate that under a natural random sampling model, the spectral\nmethod alone, or the regularized MLE alone, is minimax optimal in terms of the\nsample complexity --- the number of paired comparisons needed to ensure exact\ntop-$K$ identification, for the fixed dynamic range regime. This is\naccomplished via optimal control of the entrywise error of the score estimates.\nWe complement our theoretical studies by numerical experiments, confirming that\nboth methods yield low entrywise errors for estimating the underlying scores.\nOur theory is established via a novel leave-one-out trick, which proves\neffective for analyzing both iterative and non-iterative procedures. Along the\nway, we derive an elementary eigenvector perturbation bound for probability\ntransition matrices, which parallels the Davis-Kahan $\\sin\\Theta$ theorem for\nsymmetric matrices. This also allows us to close the gap between the $\\ell_2$\nerror upper bound for the spectral method and the minimax lower limit. \n\n"}
{"id": "1708.00185", "contents": "Title: Tensorial Recurrent Neural Networks for Longitudinal Data Analysis Abstract: Traditional Recurrent Neural Networks assume vectorized data as inputs.\nHowever many data from modern science and technology come in certain structures\nsuch as tensorial time series data. To apply the recurrent neural networks for\nthis type of data, a vectorisation process is necessary, while such a\nvectorisation leads to the loss of the precise information of the spatial or\nlongitudinal dimensions. In addition, such a vectorized data is not an optimum\nsolution for learning the representation of the longitudinal data. In this\npaper, we propose a new variant of tensorial neural networks which directly\ntake tensorial time series data as inputs. We call this new variant as\nTensorial Recurrent Neural Network (TRNN). The proposed TRNN is based on tensor\nTucker decomposition. \n\n"}
{"id": "1708.00524", "contents": "Title: Using millions of emoji occurrences to learn any-domain representations\n  for detecting sentiment, emotion and sarcasm Abstract: NLP tasks are often limited by scarcity of manually annotated data. In social\nmedia sentiment analysis and related tasks, researchers have therefore used\nbinarized emoticons and specific hashtags as forms of distant supervision. Our\npaper shows that by extending the distant supervision to a more diverse set of\nnoisy labels, the models can learn richer representations. Through emoji\nprediction on a dataset of 1246 million tweets containing one of 64 common\nemojis we obtain state-of-the-art performance on 8 benchmark datasets within\nsentiment, emotion and sarcasm detection using a single pretrained model. Our\nanalyses confirm that the diversity of our emotional labels yield a performance\nimprovement over previous distant supervision approaches. \n\n"}
{"id": "1708.00955", "contents": "Title: Hamiltonian Monte Carlo with Energy Conserving Subsampling Abstract: Hamiltonian Monte Carlo (HMC) samples efficiently from high-dimensional\nposterior distributions with proposed parameter draws obtained by iterating on\na discretized version of the Hamiltonian dynamics. The iterations make HMC\ncomputationally costly, especially in problems with large datasets, since it is\nnecessary to compute posterior densities and their derivatives with respect to\nthe parameters. Naively computing the Hamiltonian dynamics on a subset of the\ndata causes HMC to lose its key ability to generate distant parameter proposals\nwith high acceptance probability. The key insight in our article is that\nefficient subsampling HMC for the parameters is possible if both the dynamics\nand the acceptance probability are computed from the same data subsample in\neach complete HMC iteration. We show that this is possible to do in a\nprincipled way in a HMC-within-Gibbs framework where the subsample is updated\nusing a pseudo marginal MH step and the parameters are then updated using an\nHMC step, based on the current subsample. We show that our subsampling methods\nare fast and compare favorably to two popular sampling algorithms that utilize\ngradient estimates from data subsampling. We also explore the current\nlimitations of subsampling HMC algorithms by varying the quality of the\nvariance reducing control variates used in the estimators of the posterior\ndensity and its gradients. \n\n"}
{"id": "1708.01289", "contents": "Title: Independently Controllable Factors Abstract: It has been postulated that a good representation is one that disentangles\nthe underlying explanatory factors of variation. However, it remains an open\nquestion what kind of training framework could potentially achieve that.\nWhereas most previous work focuses on the static setting (e.g., with images),\nwe postulate that some of the causal factors could be discovered if the learner\nis allowed to interact with its environment. The agent can experiment with\ndifferent actions and observe their effects. More specifically, we hypothesize\nthat some of these factors correspond to aspects of the environment which are\nindependently controllable, i.e., that there exists a policy and a learnable\nfeature for each such aspect of the environment, such that this policy can\nyield changes in that feature with minimal changes to other features that\nexplain the statistical variations in the observed data. We propose a specific\nobjective function to find such factors and verify experimentally that it can\nindeed disentangle independently controllable aspects of the environment\nwithout any extrinsic reward signal. \n\n"}
{"id": "1708.01519", "contents": "Title: A Latent Variable Model for Two-Dimensional Canonical Correlation\n  Analysis and its Variational Inference Abstract: Describing the dimension reduction (DR) techniques by means of probabilistic\nmodels has recently been given special attention. Probabilistic models, in\naddition to a better interpretability of the DR methods, provide a framework\nfor further extensions of such algorithms. One of the new approaches to the\nprobabilistic DR methods is to preserving the internal structure of data. It is\nmeant that it is not necessary that the data first be converted from the matrix\nor tensor format to the vector format in the process of dimensionality\nreduction. In this paper, a latent variable model for matrix-variate data for\ncanonical correlation analysis (CCA) is proposed. Since in general there is not\nany analytical maximum likelihood solution for this model, we present two\napproaches for learning the parameters. The proposed methods are evaluated\nusing the synthetic data in terms of convergence and quality of mappings. Also,\nreal data set is employed for assessing the proposed methods with several\nprobabilistic and none-probabilistic CCA based approaches. The results confirm\nthe superiority of the proposed methods with respect to the competing\nalgorithms. Moreover, this model can be considered as a framework for further\nextensions. \n\n"}
{"id": "1708.04692", "contents": "Title: GANs for Biological Image Synthesis Abstract: In this paper, we propose a novel application of Generative Adversarial\nNetworks (GAN) to the synthesis of cells imaged by fluorescence microscopy.\nCompared to natural images, cells tend to have a simpler and more geometric\nglobal structure that facilitates image generation. However, the correlation\nbetween the spatial pattern of different fluorescent proteins reflects\nimportant biological functions, and synthesized images have to capture these\nrelationships to be relevant for biological applications. We adapt GANs to the\ntask at hand and propose new models with casual dependencies between image\nchannels that can generate multi-channel images, which would be impossible to\nobtain experimentally. We evaluate our approach using two independent\ntechniques and compare it against sensible baselines. Finally, we demonstrate\nthat by interpolating across the latent space we can mimic the known changes in\nprotein localization that occur through time during the cell cycle, allowing us\nto predict temporal evolution from static images. \n\n"}
{"id": "1708.04729", "contents": "Title: Deconvolutional Paragraph Representation Learning Abstract: Learning latent representations from long text sequences is an important\nfirst step in many natural language processing applications. Recurrent Neural\nNetworks (RNNs) have become a cornerstone for this challenging task. However,\nthe quality of sentences during RNN-based decoding (reconstruction) decreases\nwith the length of the text. We propose a sequence-to-sequence, purely\nconvolutional and deconvolutional autoencoding framework that is free of the\nabove issue, while also being computationally efficient. The proposed method is\nsimple, easy to implement and can be leveraged as a building block for many\napplications. We show empirically that compared to RNNs, our framework is\nbetter at reconstructing and correcting long paragraphs. Quantitative\nevaluation on semi-supervised text classification and summarization tasks\ndemonstrate the potential for better utilization of long unlabeled text data. \n\n"}
{"id": "1708.05629", "contents": "Title: Learning to Transfer Abstract: Transfer learning borrows knowledge from a source domain to facilitate\nlearning in a target domain. Two primary issues to be addressed in transfer\nlearning are what and how to transfer. For a pair of domains, adopting\ndifferent transfer learning algorithms results in different knowledge\ntransferred between them. To discover the optimal transfer learning algorithm\nthat maximally improves the learning performance in the target domain,\nresearchers have to exhaustively explore all existing transfer learning\nalgorithms, which is computationally intractable. As a trade-off, a sub-optimal\nalgorithm is selected, which requires considerable expertise in an ad-hoc way.\nMeanwhile, it is widely accepted in educational psychology that human beings\nimprove transfer learning skills of deciding what to transfer through\nmeta-cognitive reflection on inductive transfer learning practices. Motivated\nby this, we propose a novel transfer learning framework known as Learning to\nTransfer (L2T) to automatically determine what and how to transfer are the best\nby leveraging previous transfer learning experiences. We establish the L2T\nframework in two stages: 1) we first learn a reflection function encrypting\ntransfer learning skills from experiences; and 2) we infer what and how to\ntransfer for a newly arrived pair of domains by optimizing the reflection\nfunction. Extensive experiments demonstrate the L2T's superiority over several\nstate-of-the-art transfer learning algorithms and its effectiveness on\ndiscovering more transferable knowledge. \n\n"}
{"id": "1708.08042", "contents": "Title: Imbalanced Malware Images Classification: a CNN based Approach Abstract: Deep convolutional neural networks (CNNs) can be applied to malware binary\ndetection via image classification. The performance, however, is degraded due\nto the imbalance of malware families (classes). To mitigate this issue, we\npropose a simple yet effective weighted softmax loss which can be employed as\nthe final layer of deep CNNs. The original softmax loss is weighted, and the\nweight value can be determined according to class size. A scaling parameter is\nalso included in computing the weight. Proper selection of this parameter is\nstudied and an empirical option is suggested. The weighted loss aims at\nalleviating the impact of data imbalance in an end-to-end learning fashion. To\nvalidate the efficacy, we deploy the proposed weighted loss in a pre-trained\ndeep CNN model and fine-tune it to achieve promising results on malware images\nclassification. Extensive experiments also demonstrate that the new loss\nfunction can well fit other typical CNNs, yielding an improved classification\nperformance. \n\n"}
{"id": "1708.08311", "contents": "Title: Deep Learning Sparse Ternary Projections for Compressed Sensing of\n  Images Abstract: Compressed sensing (CS) is a sampling theory that allows reconstruction of\nsparse (or compressible) signals from an incomplete number of measurements,\nusing of a sensing mechanism implemented by an appropriate projection matrix.\nThe CS theory is based on random Gaussian projection matrices, which satisfy\nrecovery guarantees with high probability; however, sparse ternary {0, -1, +1}\nprojections are more suitable for hardware implementation. In this paper, we\npresent a deep learning approach to obtain very sparse ternary projections for\ncompressed sensing. Our deep learning architecture jointly learns a pair of a\nprojection matrix and a reconstruction operator in an end-to-end fashion. The\nexperimental results on real images demonstrate the effectiveness of the\nproposed approach compared to state-of-the-art methods, with significant\nadvantage in terms of complexity. \n\n"}
{"id": "1708.08487", "contents": "Title: On denoising autoencoders trained to minimise binary cross-entropy Abstract: Denoising autoencoders (DAEs) are powerful deep learning models used for\nfeature extraction, data generation and network pre-training. DAEs consist of\nan encoder and decoder which may be trained simultaneously to minimise a loss\n(function) between an input and the reconstruction of a corrupted version of\nthe input. There are two common loss functions used for training autoencoders,\nthese include the mean-squared error (MSE) and the binary cross-entropy (BCE).\nWhen training autoencoders on image data a natural choice of loss function is\nBCE, since pixel values may be normalised to take values in [0,1] and the\ndecoder model may be designed to generate samples that take values in (0,1). We\nshow theoretically that DAEs trained to minimise BCE may be used to take\ngradient steps in the data space towards regions of high probability under the\ndata-generating distribution. Previously this had only been shown for DAEs\ntrained using MSE. As a consequence of the theory, iterative application of a\ntrained DAE moves a data sample from regions of low probability to regions of\nhigher probability under the data-generating distribution. Firstly, we validate\nthe theory by showing that novel data samples, consistent with the training\ndata, may be synthesised when the initial data samples are random noise.\nSecondly, we motivate the theory by showing that initial data samples\nsynthesised via other methods may be improved via iterative application of a\ntrained DAE to those initial samples. \n\n"}
{"id": "1708.08705", "contents": "Title: Multi-Layer Convolutional Sparse Modeling: Pursuit and Dictionary\n  Learning Abstract: The recently proposed Multi-Layer Convolutional Sparse Coding (ML-CSC) model,\nconsisting of a cascade of convolutional sparse layers, provides a new\ninterpretation of Convolutional Neural Networks (CNNs). Under this framework,\nthe computation of the forward pass in a CNN is equivalent to a pursuit\nalgorithm aiming to estimate the nested sparse representation vectors -- or\nfeature maps -- from a given input signal. Despite having served as a pivotal\nconnection between CNNs and sparse modeling, a deeper understanding of the\nML-CSC is still lacking: there are no pursuit algorithms that can serve this\nmodel exactly, nor are there conditions to guarantee a non-empty model. While\none can easily obtain signals that approximately satisfy the ML-CSC\nconstraints, it remains unclear how to simply sample from the model and, more\nimportantly, how one can train the convolutional filters from real data.\n  In this work, we propose a sound pursuit algorithm for the ML-CSC model by\nadopting a projection approach. We provide new and improved bounds on the\nstability of the solution of such pursuit and we analyze different practical\nalternatives to implement this in practice. We show that the training of the\nfilters is essential to allow for non-trivial signals in the model, and we\nderive an online algorithm to learn the dictionaries from real data,\neffectively resulting in cascaded sparse convolutional layers. Last, but not\nleast, we demonstrate the applicability of the ML-CSC model for several\napplications in an unsupervised setting, providing competitive results. Our\nwork represents a bridge between matrix factorization, sparse dictionary\nlearning and sparse auto-encoders, and we analyze these connections in detail. \n\n"}
{"id": "1709.00291", "contents": "Title: Asymptotic Bias of Stochastic Gradient Search Abstract: The asymptotic behavior of the stochastic gradient algorithm with a biased\ngradient estimator is analyzed. Relying on arguments based on the dynamic\nsystem theory (chain-recurrence) and the differential geometry (Yomdin theorem\nand Lojasiewicz inequality), tight bounds on the asymptotic bias of the\niterates generated by such an algorithm are derived. The obtained results hold\nunder mild conditions and cover a broad class of high-dimensional nonlinear\nalgorithms. Using these results, the asymptotic properties of the\npolicy-gradient (reinforcement) learning and adaptive population Monte Carlo\nsampling are studied. Relying on the same results, the asymptotic behavior of\nthe recursive maximum split-likelihood estimation in hidden Markov models is\nanalyzed, too. \n\n"}
{"id": "1709.00483", "contents": "Title: Iteratively Linearized Reweighted Alternating Direction Method of\n  Multipliers for a Class of Nonconvex Problems Abstract: In this paper, we consider solving a class of nonconvex and nonsmooth\nproblems frequently appearing in signal processing and machine learning\nresearch. The traditional alternating direction method of multipliers\nencounters troubles in both mathematics and computations in solving the\nnonconvex and nonsmooth subproblem. In view of this, we propose a reweighted\nalternating direction method of multipliers. In this algorithm, all subproblems\nare convex and easy to solve. We also provide several guarantees for the\nconvergence and prove that the algorithm globally converges to a critical point\nof an auxiliary function with the help of the Kurdyka-{\\L}ojasiewicz property.\nSeveral numerical results are presented to demonstrate the efficiency of the\nproposed algorithm. \n\n"}
{"id": "1709.00503", "contents": "Title: Mean Actor Critic Abstract: We propose a new algorithm, Mean Actor-Critic (MAC), for discrete-action\ncontinuous-state reinforcement learning. MAC is a policy gradient algorithm\nthat uses the agent's explicit representation of all action values to estimate\nthe gradient of the policy, rather than using only the actions that were\nactually executed. We prove that this approach reduces variance in the policy\ngradient estimate relative to traditional actor-critic methods. We show\nempirical results on two control domains and on six Atari games, where MAC is\ncompetitive with state-of-the-art policy search algorithms. \n\n"}
{"id": "1709.01620", "contents": "Title: Deep Learning Techniques for Music Generation -- A Survey Abstract: This paper is a survey and an analysis of different ways of using deep\nlearning (deep artificial neural networks) to generate musical content. We\npropose a methodology based on five dimensions for our analysis:\n  Objective - What musical content is to be generated? Examples are: melody,\npolyphony, accompaniment or counterpoint. - For what destination and for what\nuse? To be performed by a human(s) (in the case of a musical score), or by a\nmachine (in the case of an audio file).\n  Representation - What are the concepts to be manipulated? Examples are:\nwaveform, spectrogram, note, chord, meter and beat. - What format is to be\nused? Examples are: MIDI, piano roll or text. - How will the representation be\nencoded? Examples are: scalar, one-hot or many-hot.\n  Architecture - What type(s) of deep neural network is (are) to be used?\nExamples are: feedforward network, recurrent network, autoencoder or generative\nadversarial networks.\n  Challenge - What are the limitations and open challenges? Examples are:\nvariability, interactivity and creativity.\n  Strategy - How do we model and control the process of generation? Examples\nare: single-step feedforward, iterative feedforward, sampling or input\nmanipulation.\n  For each dimension, we conduct a comparative analysis of various models and\ntechniques and we propose some tentative multidimensional typology. This\ntypology is bottom-up, based on the analysis of many existing deep-learning\nbased systems for music generation selected from the relevant literature. These\nsystems are described and are used to exemplify the various choices of\nobjective, representation, architecture, challenge and strategy. The last\nsection includes some discussion and some prospects. \n\n"}
{"id": "1709.01870", "contents": "Title: Clustering of Data with Missing Entries using Non-convex Fusion\n  Penalties Abstract: The presence of missing entries in data often creates challenges for pattern\nrecognition algorithms. Traditional algorithms for clustering data assume that\nall the feature values are known for every data point. We propose a method to\ncluster data in the presence of missing information. Unlike conventional\nclustering techniques where every feature is known for each point, our\nalgorithm can handle cases where a few feature values are unknown for every\npoint. For this more challenging problem, we provide theoretical guarantees for\nclustering using a $\\ell_0$ fusion penalty based optimization problem.\nFurthermore, we propose an algorithm to solve a relaxation of this problem\nusing saturating non-convex fusion penalties. It is observed that this\nalgorithm produces solutions that degrade gradually with an increase in the\nfraction of missing feature values. We demonstrate the utility of the proposed\nmethod using a simulated dataset, the Wine dataset and also an under-sampled\ncardiac MRI dataset. It is shown that the proposed method is a promising\nclustering technique for datasets with large fractions of missing entries. \n\n"}
{"id": "1709.02956", "contents": "Title: Deep Residual Networks and Weight Initialization Abstract: Residual Network (ResNet) is the state-of-the-art architecture that realizes\nsuccessful training of really deep neural network. It is also known that good\nweight initialization of neural network avoids problem of vanishing/exploding\ngradients. In this paper, simplified models of ResNets are analyzed. We argue\nthat goodness of ResNet is correlated with the fact that ResNets are relatively\ninsensitive to choice of initial weights. We also demonstrate how batch\nnormalization improves backpropagation of deep ResNets without tuning initial\nvalues of weights. \n\n"}
{"id": "1709.03528", "contents": "Title: GIANT: Globally Improved Approximate Newton Method for Distributed\n  Optimization Abstract: For distributed computing environment, we consider the empirical risk\nminimization problem and propose a distributed and communication-efficient\nNewton-type optimization method. At every iteration, each worker locally finds\nan Approximate NewTon (ANT) direction, which is sent to the main driver. The\nmain driver, then, averages all the ANT directions received from workers to\nform a {\\it Globally Improved ANT} (GIANT) direction. GIANT is highly\ncommunication efficient and naturally exploits the trade-offs between local\ncomputations and global communications in that more local computations result\nin fewer overall rounds of communications. Theoretically, we show that GIANT\nenjoys an improved convergence rate as compared with first-order methods and\nexisting distributed Newton-type methods. Further, and in sharp contrast with\nmany existing distributed Newton-type methods, as well as popular first-order\nmethods, a highly advantageous practical feature of GIANT is that it only\ninvolves one tuning parameter. We conduct large-scale experiments on a computer\ncluster and, empirically, demonstrate the superior performance of GIANT. \n\n"}
{"id": "1709.04004", "contents": "Title: Adaptive Exploration-Exploitation Tradeoff for Opportunistic Bandits Abstract: In this paper, we propose and study opportunistic bandits - a new variant of\nbandits where the regret of pulling a suboptimal arm varies under different\nenvironmental conditions, such as network load or produce price. When the\nload/price is low, so is the cost/regret of pulling a suboptimal arm (e.g.,\ntrying a suboptimal network configuration). Therefore, intuitively, we could\nexplore more when the load/price is low and exploit more when the load/price is\nhigh. Inspired by this intuition, we propose an Adaptive Upper-Confidence-Bound\n(AdaUCB) algorithm to adaptively balance the exploration-exploitation tradeoff\nfor opportunistic bandits. We prove that AdaUCB achieves $O(\\log T)$ regret\nwith a smaller coefficient than the traditional UCB algorithm. Furthermore,\nAdaUCB achieves $O(1)$ regret with respect to $T$ if the exploration cost is\nzero when the load level is below a certain threshold. Last, based on both\nsynthetic data and real-world traces, experimental results show that AdaUCB\nsignificantly outperforms other bandit algorithms, such as UCB and TS (Thompson\nSampling), under large load/price fluctuations. \n\n"}
{"id": "1709.04744", "contents": "Title: Subspace Clustering using Ensembles of $K$-Subspaces Abstract: Subspace clustering is the unsupervised grouping of points lying near a union\nof low-dimensional linear subspaces. Algorithms based directly on geometric\nproperties of such data tend to either provide poor empirical performance, lack\ntheoretical guarantees, or depend heavily on their initialization. We present a\nnovel geometric approach to the subspace clustering problem that leverages\nensembles of the K-subspaces (KSS) algorithm via the evidence accumulation\nclustering framework. Our algorithm, referred to as ensemble K-subspaces\n(EKSS), forms a co-association matrix whose (i,j)th entry is the number of\ntimes points i and j are clustered together by several runs of KSS with random\ninitializations. We prove general recovery guarantees for any algorithm that\nforms an affinity matrix with entries close to a monotonic transformation of\npairwise absolute inner products. We then show that a specific instance of EKSS\nresults in an affinity matrix with entries of this form, and hence our proposed\nalgorithm can provably recover subspaces under similar conditions to\nstate-of-the-art algorithms. The finding is, to the best of our knowledge, the\nfirst recovery guarantee for evidence accumulation clustering and for KSS\nvariants. We show on synthetic data that our method performs well in the\ntraditionally challenging settings of subspaces with large intersection,\nsubspaces with small principal angles, and noisy data. Finally, we evaluate our\nalgorithm on six common benchmark datasets and show that unlike existing\nmethods, EKSS achieves excellent empirical performance when there are both a\nsmall and large number of points per subspace. \n\n"}
{"id": "1709.05454", "contents": "Title: Statistical inference on random dot product graphs: a survey Abstract: The random dot product graph (RDPG) is an independent-edge random graph that\nis analytically tractable and, simultaneously, either encompasses or can\nsuccessfully approximate a wide range of random graphs, from relatively simple\nstochastic block models to complex latent position graphs. In this survey\npaper, we describe a comprehensive paradigm for statistical inference on random\ndot product graphs, a paradigm centered on spectral embeddings of adjacency and\nLaplacian matrices. We examine the analogues, in graph inference, of several\ncanonical tenets of classical Euclidean inference: in particular, we summarize\na body of existing results on the consistency and asymptotic normality of the\nadjacency and Laplacian spectral embeddings, and the role these spectral\nembeddings can play in the construction of single- and multi-sample hypothesis\ntests for graph data. We investigate several real-world applications, including\ncommunity detection and classification in large social networks and the\ndetermination of functional and biologically relevant network properties from\nan exploratory data analysis of the Drosophila connectome. We outline requisite\nbackground and current open problems in spectral graph inference. \n\n"}
{"id": "1709.05750", "contents": "Title: Adaptive Laplace Mechanism: Differential Privacy Preservation in Deep\n  Learning Abstract: In this paper, we focus on developing a novel mechanism to preserve\ndifferential privacy in deep neural networks, such that: (1) The privacy budget\nconsumption is totally independent of the number of training steps; (2) It has\nthe ability to adaptively inject noise into features based on the contribution\nof each to the output; and (3) It could be applied in a variety of different\ndeep neural networks. To achieve this, we figure out a way to perturb affine\ntransformations of neurons, and loss functions used in deep neural networks. In\naddition, our mechanism intentionally adds \"more noise\" into features which are\n\"less relevant\" to the model output, and vice-versa. Our theoretical analysis\nfurther derives the sensitivities and error bounds of our mechanism. Rigorous\nexperiments conducted on MNIST and CIFAR-10 datasets show that our mechanism is\nhighly effective and outperforms existing solutions. \n\n"}
{"id": "1709.05964", "contents": "Title: Why Pay More When You Can Pay Less: A Joint Learning Framework for\n  Active Feature Acquisition and Classification Abstract: We consider the problem of active feature acquisition, where we sequentially\nselect the subset of features in order to achieve the maximum prediction\nperformance in the most cost-effective way. In this work, we formulate this\nactive feature acquisition problem as a reinforcement learning problem, and\nprovide a novel framework for jointly learning both the RL agent and the\nclassifier (environment). We also introduce a more systematic way of encoding\nsubsets of features that can properly handle innate challenge with missing\nentries in active feature acquisition problems, that uses the orderless\nLSTM-based set encoding mechanism that readily fits in the joint learning\nframework. We evaluate our model on a carefully designed synthetic dataset for\nthe active feature acquisition as well as several real datasets such as\nelectric health record (EHR) datasets, on which it outperforms all baselines in\nterms of prediction performance as well feature acquisition cost. \n\n"}
{"id": "1709.06622", "contents": "Title: Distributed Training Large-Scale Deep Architectures Abstract: Scale of data and scale of computation infrastructures together enable the\ncurrent deep learning renaissance. However, training large-scale deep\narchitectures demands both algorithmic improvement and careful system\nconfiguration. In this paper, we focus on employing the system approach to\nspeed up large-scale training. Via lessons learned from our routine\nbenchmarking effort, we first identify bottlenecks and overheads that hinter\ndata parallelism. We then devise guidelines that help practitioners to\nconfigure an effective system and fine-tune parameters to achieve desired\nspeedup. Specifically, we develop a procedure for setting minibatch size and\nchoosing computation algorithms. We also derive lemmas for determining the\nquantity of key components such as the number of GPUs and parameter servers.\nExperiments and examples show that these guidelines help effectively speed up\nlarge-scale deep learning training. \n\n"}
{"id": "1709.08294", "contents": "Title: Learning Context-Sensitive Convolutional Filters for Text Processing Abstract: Convolutional neural networks (CNNs) have recently emerged as a popular\nbuilding block for natural language processing (NLP). Despite their success,\nmost existing CNN models employed in NLP share the same learned (and static)\nset of filters for all input sentences. In this paper, we consider an approach\nof using a small meta network to learn context-sensitive convolutional filters\nfor text processing. The role of meta network is to abstract the contextual\ninformation of a sentence or document into a set of input-aware filters. We\nfurther generalize this framework to model sentence pairs, where a\nbidirectional filter generation mechanism is introduced to encapsulate\nco-dependent sentence representations. In our benchmarks on four different\ntasks, including ontology classification, sentiment analysis, answer sentence\nselection, and paraphrase identification, our proposed model, a modified CNN\nwith context-sensitive filters, consistently outperforms the standard CNN and\nattention-based CNN baselines. By visualizing the learned context-sensitive\nfilters, we further validate and rationalize the effectiveness of proposed\nframework. \n\n"}
{"id": "1709.08519", "contents": "Title: Enhanced Quantum Synchronization via Quantum Machine Learning Abstract: We study the quantum synchronization between a pair of two-level systems\ninside two coupled cavities. By using a digital-analog decomposition of the\nmaster equation that rules the system dynamics, we show that this approach\nleads to quantum synchronization between both two-level systems. Moreover, we\ncan identify in this digital-analog block decomposition the fundamental\nelements of a quantum machine learning protocol, in which the agent and the\nenvironment (learning units) interact through a mediating system, namely, the\nregister. If we can additionally equip this algorithm with a classical feedback\nmechanism, which consists of projective measurements in the register,\nreinitialization of the register state and local conditional operations on the\nagent and environment subspace, a powerful and flexible quantum machine\nlearning protocol emerges. Indeed, numerical simulations show that this\nprotocol enhances the synchronization process, even when every subsystem\nexperience different loss/decoherence mechanisms, and give us the flexibility\nto choose the synchronization state. Finally, we propose an implementation\nbased on current technologies in superconducting circuits. \n\n"}
{"id": "1709.09844", "contents": "Title: Distance-based Confidence Score for Neural Network Classifiers Abstract: The reliable measurement of confidence in classifiers' predictions is very\nimportant for many applications and is, therefore, an important part of\nclassifier design. Yet, although deep learning has received tremendous\nattention in recent years, not much progress has been made in quantifying the\nprediction confidence of neural network classifiers. Bayesian models offer a\nmathematically grounded framework to reason about model uncertainty, but\nusually come with prohibitive computational costs. In this paper we propose a\nsimple, scalable method to achieve a reliable confidence score, based on the\ndata embedding derived from the penultimate layer of the network. We\ninvestigate two ways to achieve desirable embeddings, by using either a\ndistance-based loss or Adversarial Training. We then test the benefits of our\nmethod when used for classification error prediction, weighting an ensemble of\nclassifiers, and novelty detection. In all tasks we show significant\nimprovement over traditional, commonly used confidence scores. \n\n"}
{"id": "1709.10029", "contents": "Title: Sparse High-Dimensional Regression: Exact Scalable Algorithms and Phase\n  Transitions Abstract: We present a novel binary convex reformulation of the sparse regression\nproblem that constitutes a new duality perspective. We devise a new cutting\nplane method and provide evidence that it can solve to provable optimality the\nsparse regression problem for sample sizes n and number of regressors p in the\n100,000s, that is two orders of magnitude better than the current state of the\nart, in seconds. The ability to solve the problem for very high dimensions\nallows us to observe new phase transition phenomena. Contrary to traditional\ncomplexity theory which suggests that the difficulty of a problem increases\nwith problem size, the sparse regression problem has the property that as the\nnumber of samples $n$ increases the problem becomes easier in that the solution\nrecovers 100% of the true signal, and our approach solves the problem extremely\nfast (in fact faster than Lasso), while for small number of samples n, our\napproach takes a larger amount of time to solve the problem, but importantly\nthe optimal solution provides a statistically more relevant regressor. We argue\nthat our exact sparse regression approach presents a superior alternative over\nheuristic methods available at present. \n\n"}
{"id": "1710.00499", "contents": "Title: Online control of the false discovery rate with decaying memory Abstract: In the online multiple testing problem, p-values corresponding to different\nnull hypotheses are observed one by one, and the decision of whether or not to\nreject the current hypothesis must be made immediately, after which the next\np-value is observed. Alpha-investing algorithms to control the false discovery\nrate (FDR), formulated by Foster and Stine, have been generalized and applied\nto many settings, including quality-preserving databases in science and\nmultiple A/B or multi-armed bandit tests for internet commerce. This paper\nimproves the class of generalized alpha-investing algorithms (GAI) in four\nways: (a) we show how to uniformly improve the power of the entire class of\nmonotone GAI procedures by awarding more alpha-wealth for each rejection,\ngiving a win-win resolution to a recent dilemma raised by Javanmard and\nMontanari, (b) we demonstrate how to incorporate prior weights to indicate\ndomain knowledge of which hypotheses are likely to be non-null, (c) we allow\nfor differing penalties for false discoveries to indicate that some hypotheses\nmay be more important than others, (d) we define a new quantity called the\ndecaying memory false discovery rate (mem-FDR) that may be more meaningful for\ntruly temporal applications, and which alleviates problems that we describe and\nrefer to as \"piggybacking\" and \"alpha-death\". Our GAI++ algorithms incorporate\nall four generalizations simultaneously, and reduce to more powerful variants\nof earlier algorithms when the weights and decay are all set to unity. Finally,\nwe also describe a simple method to derive new online FDR rules based on an\nestimated false discovery proportion. \n\n"}
{"id": "1710.01278", "contents": "Title: Dilated Convolutions for Modeling Long-Distance Genomic Dependencies Abstract: We consider the task of detecting regulatory elements in the human genome\ndirectly from raw DNA. Past work has focused on small snippets of DNA, making\nit difficult to model long-distance dependencies that arise from DNA's\n3-dimensional conformation. In order to study long-distance dependencies, we\ndevelop and release a novel dataset for a larger-context modeling task. Using\nthis new data set we model long-distance interactions using dilated\nconvolutional neural networks, and compare them to standard convolutions and\nrecurrent neural networks. We show that dilated convolutions are effective at\nmodeling the locations of regulatory markers in the human genome, such as\ntranscription factor binding sites, histone modifications, and DNAse\nhypersensitivity sites. \n\n"}
{"id": "1710.02971", "contents": "Title: Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE,\n  and node2vec Abstract: Since the invention of word2vec, the skip-gram model has significantly\nadvanced the research of network embedding, such as the recent emergence of the\nDeepWalk, LINE, PTE, and node2vec approaches. In this work, we show that all of\nthe aforementioned models with negative sampling can be unified into the matrix\nfactorization framework with closed forms. Our analysis and proofs reveal that:\n(1) DeepWalk empirically produces a low-rank transformation of a network's\nnormalized Laplacian matrix; (2) LINE, in theory, is a special case of DeepWalk\nwhen the size of vertices' context is set to one; (3) As an extension of LINE,\nPTE can be viewed as the joint factorization of multiple networks' Laplacians;\n(4) node2vec is factorizing a matrix related to the stationary distribution and\ntransition probability tensor of a 2nd-order random walk. We further provide\nthe theoretical connections between skip-gram based network embedding\nalgorithms and the theory of graph Laplacian. Finally, we present the NetMF\nmethod as well as its approximation algorithm for computing network embedding.\nOur method offers significant improvements over DeepWalk and LINE for\nconventional network mining tasks. This work lays the theoretical foundation\nfor skip-gram based network embedding methods, leading to a better\nunderstanding of latent network representation learning. \n\n"}
{"id": "1710.03285", "contents": "Title: Coresets for Dependency Networks Abstract: Many applications infer the structure of a probabilistic graphical model from\ndata to elucidate the relationships between variables. But how can we train\ngraphical models on a massive data set? In this paper, we show how to construct\ncoresets -compressed data sets which can be used as proxy for the original data\nand have provably bounded worst case error- for Gaussian dependency networks\n(DNs), i.e., cyclic directed graphical models over Gaussians, where the parents\nof each variable are its Markov blanket. Specifically, we prove that Gaussian\nDNs admit coresets of size independent of the size of the data set.\nUnfortunately, this does not extend to DNs over members of the exponential\nfamily in general. As we will prove, Poisson DNs do not admit small coresets.\nDespite this worst-case result, we will provide an argument why our coreset\nconstruction for DNs can still work well in practice on count data. To\ncorroborate our theoretical results, we empirically evaluated the resulting\nCore DNs on real data sets. The results \n\n"}
{"id": "1710.03740", "contents": "Title: Mixed Precision Training Abstract: Deep neural networks have enabled progress in a wide variety of applications.\nGrowing the size of the neural network typically results in improved accuracy.\nAs model sizes grow, the memory and compute requirements for training these\nmodels also increases. We introduce a technique to train deep neural networks\nusing half precision floating point numbers. In our technique, weights,\nactivations and gradients are stored in IEEE half-precision format.\nHalf-precision floating numbers have limited numerical range compared to\nsingle-precision numbers. We propose two techniques to handle this loss of\ninformation. Firstly, we recommend maintaining a single-precision copy of the\nweights that accumulates the gradients after each optimizer step. This\nsingle-precision copy is rounded to half-precision format during training.\nSecondly, we propose scaling the loss appropriately to handle the loss of\ninformation with half-precision gradients. We demonstrate that this approach\nworks for a wide variety of models including convolution neural networks,\nrecurrent neural networks and generative adversarial networks. This technique\nworks for large scale models with more than 100 million parameters trained on\nlarge datasets. Using this approach, we can reduce the memory consumption of\ndeep learning models by nearly 2x. In future processors, we can also expect a\nsignificant computation speedup using half-precision hardware units. \n\n"}
{"id": "1710.04806", "contents": "Title: Deep Learning for Case-Based Reasoning through Prototypes: A Neural\n  Network that Explains Its Predictions Abstract: Deep neural networks are widely used for classification. These deep models\noften suffer from a lack of interpretability -- they are particularly difficult\nto understand because of their non-linear nature. As a result, neural networks\nare often treated as \"black box\" models, and in the past, have been trained\npurely to optimize the accuracy of predictions. In this work, we create a novel\nnetwork architecture for deep learning that naturally explains its own\nreasoning for each prediction. This architecture contains an autoencoder and a\nspecial prototype layer, where each unit of that layer stores a weight vector\nthat resembles an encoded training input. The encoder of the autoencoder allows\nus to do comparisons within the latent space, while the decoder allows us to\nvisualize the learned prototypes. The training objective has four terms: an\naccuracy term, a term that encourages every prototype to be similar to at least\none encoded input, a term that encourages every encoded input to be close to at\nleast one prototype, and a term that encourages faithful reconstruction by the\nautoencoder. The distances computed in the prototype layer are used as part of\nthe classification process. Since the prototypes are learned during training,\nthe learned network naturally comes with explanations for each prediction, and\nthe explanations are loyal to what the network actually computes. \n\n"}
{"id": "1710.05092", "contents": "Title: Dropout as a Low-Rank Regularizer for Matrix Factorization Abstract: Regularization for matrix factorization (MF) and approximation problems has\nbeen carried out in many different ways. Due to its popularity in deep\nlearning, dropout has been applied also for this class of problems. Despite its\nsolid empirical performance, the theoretical properties of dropout as a\nregularizer remain quite elusive for this class of problems. In this paper, we\npresent a theoretical analysis of dropout for MF, where Bernoulli random\nvariables are used to drop columns of the factors. We demonstrate the\nequivalence between dropout and a fully deterministic model for MF in which the\nfactors are regularized by the sum of the product of squared Euclidean norms of\nthe columns. Additionally, we inspect the case of a variable sized\nfactorization and we prove that dropout achieves the global minimum of a convex\napproximation problem with (squared) nuclear norm regularization. As a result,\nwe conclude that dropout can be used as a low-rank regularizer with data\ndependent singular-value thresholding. \n\n"}
{"id": "1710.05488", "contents": "Title: A Geometric View of Optimal Transportation and Generative Model Abstract: In this work, we show the intrinsic relations between optimal transportation\nand convex geometry, especially the variational approach to solve Alexandrov\nproblem: constructing a convex polytope with prescribed face normals and\nvolumes. This leads to a geometric interpretation to generative models, and\nleads to a novel framework for generative models. By using the optimal\ntransportation view of GAN model, we show that the discriminator computes the\nKantorovich potential, the generator calculates the transportation map. For a\nlarge class of transportation costs, the Kantorovich potential can give the\noptimal transportation map by a close-form formula. Therefore, it is sufficient\nto solely optimize the discriminator. This shows the adversarial competition\ncan be avoided, and the computational architecture can be simplified.\nPreliminary experimental results show the geometric method outperforms WGAN for\napproximating probability measures with multiple clusters in low dimensional\nspace. \n\n"}
{"id": "1710.05512", "contents": "Title: The Feeling of Success: Does Touch Sensing Help Predict Grasp Outcomes? Abstract: A successful grasp requires careful balancing of the contact forces. Deducing\nwhether a particular grasp will be successful from indirect measurements, such\nas vision, is therefore quite challenging, and direct sensing of contacts\nthrough touch sensing provides an appealing avenue toward more successful and\nconsistent robotic grasping. However, in order to fully evaluate the value of\ntouch sensing for grasp outcome prediction, we must understand how touch\nsensing can influence outcome prediction accuracy when combined with other\nmodalities. Doing so using conventional model-based techniques is exceptionally\ndifficult. In this work, we investigate the question of whether touch sensing\naids in predicting grasp outcomes within a multimodal sensing framework that\ncombines vision and touch. To that end, we collected more than 9,000 grasping\ntrials using a two-finger gripper equipped with GelSight high-resolution\ntactile sensors on each finger, and evaluated visuo-tactile deep neural network\nmodels to directly predict grasp outcomes from either modality individually,\nand from both modalities together. Our experimental results indicate that\nincorporating tactile readings substantially improve grasping performance. \n\n"}
{"id": "1710.05758", "contents": "Title: TensorQuant - A Simulation Toolbox for Deep Neural Network Quantization Abstract: Recent research implies that training and inference of deep neural networks\n(DNN) can be computed with low precision numerical representations of the\ntraining/test data, weights and gradients without a general loss in accuracy.\nThe benefit of such compact representations is twofold: they allow a\nsignificant reduction of the communication bottleneck in distributed DNN\ntraining and faster neural network implementations on hardware accelerators\nlike FPGAs. Several quantization methods have been proposed to map the original\n32-bit floating point problem to low-bit representations. While most related\npublications validate the proposed approach on a single DNN topology, it\nappears to be evident, that the optimal choice of the quantization method and\nnumber of coding bits is topology dependent. To this end, there is no general\ntheory available, which would allow users to derive the optimal quantization\nduring the design of a DNN topology. In this paper, we present a quantization\ntool box for the TensorFlow framework. TensorQuant allows a transparent\nquantization simulation of existing DNN topologies during training and\ninference. TensorQuant supports generic quantization methods and allows\nexperimental evaluation of the impact of the quantization on single layers as\nwell as on the full topology. In a first series of experiments with\nTensorQuant, we show an analysis of fix-point quantizations of popular CNN\ntopologies. \n\n"}
{"id": "1710.06081", "contents": "Title: Boosting Adversarial Attacks with Momentum Abstract: Deep neural networks are vulnerable to adversarial examples, which poses\nsecurity concerns on these algorithms due to the potentially severe\nconsequences. Adversarial attacks serve as an important surrogate to evaluate\nthe robustness of deep learning models before they are deployed. However, most\nof existing adversarial attacks can only fool a black-box model with a low\nsuccess rate. To address this issue, we propose a broad class of momentum-based\niterative algorithms to boost adversarial attacks. By integrating the momentum\nterm into the iterative process for attacks, our methods can stabilize update\ndirections and escape from poor local maxima during the iterations, resulting\nin more transferable adversarial examples. To further improve the success rates\nfor black-box attacks, we apply momentum iterative algorithms to an ensemble of\nmodels, and show that the adversarially trained models with a strong defense\nability are also vulnerable to our black-box attacks. We hope that the proposed\nmethods will serve as a benchmark for evaluating the robustness of various deep\nmodels and defense methods. With this method, we won the first places in NIPS\n2017 Non-targeted Adversarial Attack and Targeted Adversarial Attack\ncompetitions. \n\n"}
{"id": "1710.06276", "contents": "Title: Smooth and Sparse Optimal Transport Abstract: Entropic regularization is quickly emerging as a new standard in optimal\ntransport (OT). It enables to cast the OT computation as a differentiable and\nunconstrained convex optimization problem, which can be efficiently solved\nusing the Sinkhorn algorithm. However, entropy keeps the transportation plan\nstrictly positive and therefore completely dense, unlike unregularized OT. This\nlack of sparsity can be problematic in applications where the transportation\nplan itself is of interest. In this paper, we explore regularizing the primal\nand dual OT formulations with a strongly convex term, which corresponds to\nrelaxing the dual and primal constraints with smooth approximations. We show\nhow to incorporate squared $2$-norm and group lasso regularizations within that\nframework, leading to sparse and group-sparse transportation plans. On the\ntheoretical side, we bound the approximation error introduced by regularizing\nthe primal and dual formulations. Our results suggest that, for the regularized\nprimal, the approximation error can often be smaller with squared $2$-norm than\nwith entropic regularization. We showcase our proposed framework on the task of\ncolor transfer. \n\n"}
{"id": "1710.08873", "contents": "Title: Robust Photometric Stereo via Dictionary Learning Abstract: Photometric stereo is a method that seeks to reconstruct the normal vectors\nof an object from a set of images of the object illuminated under different\nlight sources. While effective in some situations, classical photometric stereo\nrelies on a diffuse surface model that cannot handle objects with complex\nreflectance patterns, and it is sensitive to non-idealities in the images. In\nthis work, we propose a novel approach to photometric stereo that relies on\ndictionary learning to produce robust normal vector reconstructions.\nSpecifically, we develop two formulations for applying dictionary learning to\nphotometric stereo. We propose a model that applies dictionary learning to\nregularize and reconstruct the normal vectors from the images under the classic\nLambertian reflectance model. We then generalize this model to explicitly model\nnon-Lambertian objects. We investigate both approaches through extensive\nexperimentation on synthetic and real benchmark datasets and observe\nstate-of-the-art performance compared to existing robust photometric stereo\nmethods. \n\n"}
{"id": "1710.09220", "contents": "Title: The Heterogeneous Ensembles of Standard Classification Algorithms\n  (HESCA): the Whole is Greater than the Sum of its Parts Abstract: Building classification models is an intrinsically practical exercise that\nrequires many design decisions prior to deployment. We aim to provide some\nguidance in this decision making process. Specifically, given a classification\nproblem with real valued attributes, we consider which classifier or family of\nclassifiers should one use. Strong contenders are tree based homogeneous\nensembles, support vector machines or deep neural networks. All three families\nof model could claim to be state-of-the-art, and yet it is not clear when one\nis preferable to the others. Our extensive experiments with over 200 data sets\nfrom two distinct archives demonstrate that, rather than choose a single family\nand expend computing resources on optimising that model, it is significantly\nbetter to build simpler versions of classifiers from each family and ensemble.\nWe show that the Heterogeneous Ensembles of Standard Classification Algorithms\n(HESCA), which ensembles based on error estimates formed on the train data, is\nsignificantly better (in terms of error, balanced error, negative log\nlikelihood and area under the ROC curve) than its individual components,\npicking the component that is best on train data, and a support vector machine\ntuned over 1089 different parameter configurations. We demonstrate HESCA+,\nwhich contains a deep neural network, a support vector machine and two decision\ntree forests, is significantly better than its components, picking the best\ncomponent, and HESCA. We analyse the results further and find that HESCA and\nHESCA+ are of particular value when the train set size is relatively small and\nthe problem has multiple classes. HESCA is a fast approach that is, on average,\nas good as state-of-the-art classifiers, whereas HESCA+ is significantly better\nthan average and represents a strong benchmark for future research. \n\n"}
{"id": "1710.09508", "contents": "Title: Reparameterizing the Birkhoff Polytope for Variational Permutation\n  Inference Abstract: Many matching, tracking, sorting, and ranking problems require probabilistic\nreasoning about possible permutations, a set that grows factorially with\ndimension. Combinatorial optimization algorithms may enable efficient point\nestimation, but fully Bayesian inference poses a severe challenge in this\nhigh-dimensional, discrete space. To surmount this challenge, we start with the\nusual step of relaxing a discrete set (here, of permutation matrices) to its\nconvex hull, which here is the Birkhoff polytope: the set of all\ndoubly-stochastic matrices. We then introduce two novel transformations: first,\nan invertible and differentiable stick-breaking procedure that maps\nunconstrained space to the Birkhoff polytope; second, a map that rounds points\ntoward the vertices of the polytope. Both transformations include a temperature\nparameter that, in the limit, concentrates the densities on permutation\nmatrices. We then exploit these transformations and reparameterization\ngradients to introduce variational inference over permutation matrices, and we\ndemonstrate its utility in a series of experiments. \n\n"}
{"id": "1710.09768", "contents": "Title: From Distance Correlation to Multiscale Graph Correlation Abstract: Understanding and developing a correlation measure that can detect general\ndependencies is not only imperative to statistics and machine learning, but\nalso crucial to general scientific discovery in the big data age. In this\npaper, we establish a new framework that generalizes distance correlation --- a\ncorrelation measure that was recently proposed and shown to be universally\nconsistent for dependence testing against all joint distributions of finite\nmoments --- to the Multiscale Graph Correlation (MGC). By utilizing the\ncharacteristic functions and incorporating the nearest neighbor machinery, we\nformalize the population version of local distance correlations, define the\noptimal scale in a given dependency, and name the optimal local correlation as\nMGC. The new theoretical framework motivates a theoretically sound Sample MGC\nand allows a number of desirable properties to be proved, including the\nuniversal consistency, convergence and almost unbiasedness of the sample\nversion. The advantages of MGC are illustrated via a comprehensive set of\nsimulations with linear, nonlinear, univariate, multivariate, and noisy\ndependencies, where it loses almost no power in monotone dependencies while\nachieving better performance in general dependencies, compared to distance\ncorrelation and other popular methods. \n\n"}
{"id": "1710.10121", "contents": "Title: Beyond Finite Layer Neural Networks: Bridging Deep Architectures and\n  Numerical Differential Equations Abstract: In our work, we bridge deep neural network design with numerical differential\nequations. We show that many effective networks, such as ResNet, PolyNet,\nFractalNet and RevNet, can be interpreted as different numerical\ndiscretizations of differential equations. This finding brings us a brand new\nperspective on the design of effective deep architectures. We can take\nadvantage of the rich knowledge in numerical analysis to guide us in designing\nnew and potentially more effective deep networks. As an example, we propose a\nlinear multi-step architecture (LM-architecture) which is inspired by the\nlinear multi-step method solving ordinary differential equations. The\nLM-architecture is an effective structure that can be used on any ResNet-like\nnetworks. In particular, we demonstrate that LM-ResNet and LM-ResNeXt (i.e. the\nnetworks obtained by applying the LM-architecture on ResNet and ResNeXt\nrespectively) can achieve noticeably higher accuracy than ResNet and ResNeXt on\nboth CIFAR and ImageNet with comparable numbers of trainable parameters. In\nparticular, on both CIFAR and ImageNet, LM-ResNet/LM-ResNeXt can significantly\ncompress ($>50$\\%) the original networks while maintaining a similar\nperformance. This can be explained mathematically using the concept of modified\nequation from numerical analysis. Last but not least, we also establish a\nconnection between stochastic control and noise injection in the training\nprocess which helps to improve generalization of the networks. Furthermore, by\nrelating stochastic training strategy with stochastic dynamic system, we can\neasily apply stochastic training to the networks with the LM-architecture. As\nan example, we introduced stochastic depth to LM-ResNet and achieve significant\nimprovement over the original LM-ResNet on CIFAR10. \n\n"}
{"id": "1710.10348", "contents": "Title: Multi-level Residual Networks from Dynamical Systems View Abstract: Deep residual networks (ResNets) and their variants are widely used in many\ncomputer vision applications and natural language processing tasks. However,\nthe theoretical principles for designing and training ResNets are still not\nfully understood. Recently, several points of view have emerged to try to\ninterpret ResNet theoretically, such as unraveled view, unrolled iterative\nestimation and dynamical systems view. In this paper, we adopt the dynamical\nsystems point of view, and analyze the lesioning properties of ResNet both\ntheoretically and experimentally. Based on these analyses, we additionally\npropose a novel method for accelerating ResNet training. We apply the proposed\nmethod to train ResNets and Wide ResNets for three image classification\nbenchmarks, reducing training time by more than 40% with superior or on-par\naccuracy. \n\n"}
{"id": "1710.10381", "contents": "Title: Partitioning Relational Matrices of Similarities or Dissimilarities\n  using the Value of Information Abstract: In this paper, we provide an approach to clustering relational matrices whose\nentries correspond to either similarities or dissimilarities between objects.\nOur approach is based on the value of information, a parameterized,\ninformation-theoretic criterion that measures the change in costs associated\nwith changes in information. Optimizing the value of information yields a\ndeterministic annealing style of clustering with many benefits. For instance,\ninvestigators avoid needing to a priori specify the number of clusters, as the\npartitions naturally undergo phase changes, during the annealing process,\nwhereby the number of clusters changes in a data-driven fashion. The\nglobal-best partition can also often be identified. \n\n"}
{"id": "1710.10720", "contents": "Title: Globally Optimal Symbolic Regression Abstract: In this study we introduce a new technique for symbolic regression that\nguarantees global optimality. This is achieved by formulating a mixed integer\nnon-linear program (MINLP) whose solution is a symbolic mathematical expression\nof minimum complexity that explains the observations. We demonstrate our\napproach by rediscovering Kepler's law on planetary motion using exoplanet data\nand Galileo's pendulum periodicity equation using experimental data. \n\n"}
{"id": "1710.10903", "contents": "Title: Graph Attention Networks Abstract: We present graph attention networks (GATs), novel neural network\narchitectures that operate on graph-structured data, leveraging masked\nself-attentional layers to address the shortcomings of prior methods based on\ngraph convolutions or their approximations. By stacking layers in which nodes\nare able to attend over their neighborhoods' features, we enable (implicitly)\nspecifying different weights to different nodes in a neighborhood, without\nrequiring any kind of costly matrix operation (such as inversion) or depending\non knowing the graph structure upfront. In this way, we address several key\nchallenges of spectral-based graph neural networks simultaneously, and make our\nmodel readily applicable to inductive as well as transductive problems. Our GAT\nmodels have achieved or matched state-of-the-art results across four\nestablished transductive and inductive graph benchmarks: the Cora, Citeseer and\nPubmed citation network datasets, as well as a protein-protein interaction\ndataset (wherein test graphs remain unseen during training). \n\n"}
{"id": "1710.11004", "contents": "Title: Denoising random forests Abstract: This paper proposes a novel type of random forests called a denoising random\nforests that are robust against noises contained in test samples. Such\nnoise-corrupted samples cause serious damage to the estimation performances of\nrandom forests, since unexpected child nodes are often selected and the leaf\nnodes that the input sample reaches are sometimes far from those for a clean\nsample. Our main idea for tackling this problem originates from a binary\nindicator vector that encodes a traversal path of a sample in the forest. Our\nproposed method effectively employs this vector by introducing denoising\nautoencoders into random forests. A denoising autoencoder can be trained with\nindicator vectors produced from clean and noisy input samples, and non-leaf\nnodes where incorrect decisions are made can be identified by comparing the\ninput and output of the trained denoising autoencoder. Multiple traversal paths\nwith respect to the nodes with incorrect decisions caused by the noises can\nthen be considered for the estimation. \n\n"}
{"id": "1710.11469", "contents": "Title: Conditional Variance Penalties and Domain Shift Robustness Abstract: When training a deep neural network for image classification, one can broadly\ndistinguish between two types of latent features of images that will drive the\nclassification. We can divide latent features into (i) \"core\" or \"conditionally\ninvariant\" features $X^\\text{core}$ whose distribution $X^\\text{core}\\vert Y$,\nconditional on the class $Y$, does not change substantially across domains and\n(ii) \"style\" features $X^{\\text{style}}$ whose distribution $X^{\\text{style}}\n\\vert Y$ can change substantially across domains. Examples for style features\ninclude position, rotation, image quality or brightness but also more complex\nones like hair color, image quality or posture for images of persons. Our goal\nis to minimize a loss that is robust under changes in the distribution of these\nstyle features. In contrast to previous work, we assume that the domain itself\nis not observed and hence a latent variable.\n  We do assume that we can sometimes observe a typically discrete identifier or\n\"$\\mathrm{ID}$ variable\". In some applications we know, for example, that two\nimages show the same person, and $\\mathrm{ID}$ then refers to the identity of\nthe person. The proposed method requires only a small fraction of images to\nhave $\\mathrm{ID}$ information. We group observations if they share the same\nclass and identifier $(Y,\\mathrm{ID})=(y,\\mathrm{id})$ and penalize the\nconditional variance of the prediction or the loss if we condition on\n$(Y,\\mathrm{ID})$. Using a causal framework, this conditional variance\nregularization (CoRe) is shown to protect asymptotically against shifts in the\ndistribution of the style variables. Empirically, we show that the CoRe penalty\nimproves predictive accuracy substantially in settings where domain changes\noccur in terms of image quality, brightness and color while we also look at\nmore complex changes such as changes in movement and posture. \n\n"}
{"id": "1711.01012", "contents": "Title: Policy Optimization by Genetic Distillation Abstract: Genetic algorithms have been widely used in many practical optimization\nproblems. Inspired by natural selection, operators, including mutation,\ncrossover and selection, provide effective heuristics for search and black-box\noptimization. However, they have not been shown useful for deep reinforcement\nlearning, possibly due to the catastrophic consequence of parameter crossovers\nof neural networks. Here, we present Genetic Policy Optimization (GPO), a new\ngenetic algorithm for sample-efficient deep policy optimization. GPO uses\nimitation learning for policy crossover in the state space and applies policy\ngradient methods for mutation. Our experiments on MuJoCo tasks show that GPO as\na genetic algorithm is able to provide superior performance over the\nstate-of-the-art policy gradient methods and achieves comparable or higher\nsample efficiency. \n\n"}
{"id": "1711.01244", "contents": "Title: Meta-Learning by Adjusting Priors Based on Extended PAC-Bayes Theory Abstract: In meta-learning an agent extracts knowledge from observed tasks, aiming to\nfacilitate learning of novel future tasks. Under the assumption that future\ntasks are 'related' to previous tasks, the accumulated knowledge should be\nlearned in a way which captures the common structure across learned tasks,\nwhile allowing the learner sufficient flexibility to adapt to novel aspects of\nnew tasks. We present a framework for meta-learning that is based on\ngeneralization error bounds, allowing us to extend various PAC-Bayes bounds to\nmeta-learning. Learning takes place through the construction of a distribution\nover hypotheses based on the observed tasks, and its utilization for learning a\nnew task. Thus, prior knowledge is incorporated through setting an\nexperience-dependent prior for novel tasks. We develop a gradient-based\nalgorithm which minimizes an objective function derived from the bounds and\ndemonstrate its effectiveness numerically with deep neural networks. In\naddition to establishing the improved performance available through\nmeta-learning, we demonstrate the intuitive way by which prior information is\nmanifested at different levels of the network. \n\n"}
{"id": "1711.01569", "contents": "Title: Double Q($\\sigma$) and Q($\\sigma, \\lambda$): Unifying Reinforcement\n  Learning Control Algorithms Abstract: Temporal-difference (TD) learning is an important field in reinforcement\nlearning. Sarsa and Q-Learning are among the most used TD algorithms. The\nQ($\\sigma$) algorithm (Sutton and Barto (2017)) unifies both. This paper\nextends the Q($\\sigma$) algorithm to an online multi-step algorithm Q($\\sigma,\n\\lambda$) using eligibility traces and introduces Double Q($\\sigma$) as the\nextension of Q($\\sigma$) to double learning. Experiments suggest that the new\nQ($\\sigma, \\lambda$) algorithm can outperform the classical TD control methods\nSarsa($\\lambda$), Q($\\lambda$) and Q($\\sigma$). \n\n"}
{"id": "1711.03038", "contents": "Title: Recency-weighted Markovian inference Abstract: We describe a Markov latent state space (MLSS) model, where the latent state\ndistribution is a decaying mixture over multiple past states. We present a\nsimple sampling algorithm that allows to approximate such high-order MLSS with\nfixed time and memory costs. \n\n"}
{"id": "1711.03656", "contents": "Title: p-FP: Extraction, Classification, and Prediction of Website Fingerprints\n  with Deep Learning Abstract: Recent advances in learning Deep Neural Network (DNN) architectures have\nreceived a great deal of attention due to their ability to outperform\nstate-of-the-art classifiers across a wide range of applications, with little\nor no feature engineering. In this paper, we broadly study the applicability of\ndeep learning to website fingerprinting. We show that unsupervised DNNs can be\nused to extract low-dimensional feature vectors that improve the performance of\nstate-of-the-art website fingerprinting attacks. When used as classifiers, we\nshow that they can match or exceed performance of existing attacks across a\nrange of application scenarios, including fingerprinting Tor website traces,\nfingerprinting search engine queries over Tor, defeating fingerprinting\ndefenses, and fingerprinting TLS-encrypted websites. Finally, we show that DNNs\ncan be used to predict the fingerprintability of a website based on its\ncontents, achieving 99% accuracy on a data set of 4500 website downloads. \n\n"}
{"id": "1711.03674", "contents": "Title: Breast density classification with deep convolutional neural networks Abstract: Breast density classification is an essential part of breast cancer\nscreening. Although a lot of prior work considered this problem as a task for\nlearning algorithms, to our knowledge, all of them used small and not\nclinically realistic data both for training and evaluation of their models. In\nthis work, we explore the limits of this task with a data set coming from over\n200,000 breast cancer screening exams. We use this data to train and evaluate a\nstrong convolutional neural network classifier. In a reader study, we find that\nour model can perform this task comparably to a human expert. \n\n"}
{"id": "1711.03946", "contents": "Title: Bayesian Paragraph Vectors Abstract: Word2vec (Mikolov et al., 2013) has proven to be successful in natural\nlanguage processing by capturing the semantic relationships between different\nwords. Built on top of single-word embeddings, paragraph vectors (Le and\nMikolov, 2014) find fixed-length representations for pieces of text with\narbitrary lengths, such as documents, paragraphs, and sentences. In this work,\nwe propose a novel interpretation for neural-network-based paragraph vectors by\ndeveloping an unsupervised generative model whose maximum likelihood solution\ncorresponds to traditional paragraph vectors. This probabilistic formulation\nallows us to go beyond point estimates of parameters and to perform Bayesian\nposterior inference. We find that the entropy of paragraph vectors decreases\nwith the length of documents, and that information about posterior uncertainty\nimproves performance in supervised learning tasks such as sentiment analysis\nand paraphrase detection. \n\n"}
{"id": "1711.03985", "contents": "Title: Applications of Deep Learning and Reinforcement Learning to Biological\n  Data Abstract: Rapid advances of hardware-based technologies during the past decades have\nopened up new possibilities for Life scientists to gather multimodal data in\nvarious application domains (e.g., Omics, Bioimaging, Medical Imaging, and\n[Brain/Body]-Machine Interfaces), thus generating novel opportunities for\ndevelopment of dedicated data intensive machine learning techniques. Overall,\nrecent research in Deep learning (DL), Reinforcement learning (RL), and their\ncombination (Deep RL) promise to revolutionize Artificial Intelligence. The\ngrowth in computational power accompanied by faster and increased data storage\nand declining computing costs have already allowed scientists in various fields\nto apply these techniques on datasets that were previously intractable for\ntheir size and complexity. This review article provides a comprehensive survey\non the application of DL, RL, and Deep RL techniques in mining Biological data.\nIn addition, we compare performances of DL techniques when applied to different\ndatasets across various application domains. Finally, we outline open issues in\nthis challenging research area and discuss future development perspectives. \n\n"}
{"id": "1711.04329", "contents": "Title: Medical Diagnosis From Laboratory Tests by Combining Generative and\n  Discriminative Learning Abstract: A primary goal of computational phenotype research is to conduct medical\ndiagnosis. In hospital, physicians rely on massive clinical data to make\ndiagnosis decisions, among which laboratory tests are one of the most important\nresources. However, the longitudinal and incomplete nature of laboratory test\ndata casts a significant challenge on its interpretation and usage, which may\nresult in harmful decisions by both human physicians and automatic diagnosis\nsystems. In this work, we take advantage of deep generative models to deal with\nthe complex laboratory tests. Specifically, we propose an end-to-end\narchitecture that involves a deep generative variational recurrent neural\nnetworks (VRNN) to learn robust and generalizable features, and a\ndiscriminative neural network (NN) model to learn diagnosis decision making,\nand the two models are trained jointly. Our experiments are conducted on a\ndataset involving 46,252 patients, and the 50 most frequent tests are used to\npredict the 50 most common diagnoses. The results show that our model, VRNN+NN,\nsignificantly (p<0.001) outperforms other baseline models. Moreover, we\ndemonstrate that the representations learned by the joint training are more\ninformative than those learned by pure generative models. Finally, we find that\nour model offers a surprisingly good imputation for missing values. \n\n"}
{"id": "1711.04837", "contents": "Title: Improving Factor-Based Quantitative Investing by Forecasting Company\n  Fundamentals Abstract: On a periodic basis, publicly traded companies are required to report\nfundamentals: financial data such as revenue, operating income, debt, among\nothers. These data points provide some insight into the financial health of a\ncompany. Academic research has identified some factors, i.e. computed features\nof the reported data, that are known through retrospective analysis to\noutperform the market average. Two popular factors are the book value\nnormalized by market capitalization (book-to-market) and the operating income\nnormalized by the enterprise value (EBIT/EV). In this paper: we first show\nthrough simulation that if we could (clairvoyantly) select stocks using factors\ncalculated on future fundamentals (via oracle), then our portfolios would far\noutperform a standard factor approach. Motivated by this analysis, we train\ndeep neural networks to forecast future fundamentals based on a trailing\n5-years window. Quantitative analysis demonstrates a significant improvement in\nMSE over a naive strategy. Moreover, in retrospective analysis using an\nindustry-grade stock portfolio simulator (backtester), we show an improvement\nin compounded annual return to 17.1% (MLP) vs 14.4% for a standard factor\nmodel. \n\n"}
{"id": "1711.04955", "contents": "Title: Scalable Peaceman-Rachford Splitting Method with Proximal Terms Abstract: Along with developing of Peaceman-Rachford Splittling Method (PRSM), many\nbatch algorithms based on it have been studied very deeply. But almost no\nalgorithm focused on the performance of stochastic version of PRSM. In this\npaper, we propose a new stochastic algorithm based on PRSM, prove its\nconvergence rate in ergodic sense, and test its performance on both artificial\nand real data. We show that our proposed algorithm, Stochastic Scalable PRSM\n(SS-PRSM), enjoys the $O(1/K)$ convergence rate, which is the same as those\nnewest stochastic algorithms that based on ADMM but faster than general\nStochastic ADMM (which is $O(1/\\sqrt{K})$). Our algorithm also owns wide\nflexibility, outperforms many state-of-the-art stochastic algorithms coming\nfrom ADMM, and has low memory cost in large-scale splitting optimization\nproblems. \n\n"}
{"id": "1711.05225", "contents": "Title: CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep\n  Learning Abstract: We develop an algorithm that can detect pneumonia from chest X-rays at a\nlevel exceeding practicing radiologists. Our algorithm, CheXNet, is a 121-layer\nconvolutional neural network trained on ChestX-ray14, currently the largest\npublicly available chest X-ray dataset, containing over 100,000 frontal-view\nX-ray images with 14 diseases. Four practicing academic radiologists annotate a\ntest set, on which we compare the performance of CheXNet to that of\nradiologists. We find that CheXNet exceeds average radiologist performance on\nthe F1 metric. We extend CheXNet to detect all 14 diseases in ChestX-ray14 and\nachieve state of the art results on all 14 diseases. \n\n"}
{"id": "1711.05363", "contents": "Title: Kernel Conditional Exponential Family Abstract: A nonparametric family of conditional distributions is introduced, which\ngeneralizes conditional exponential families using functional parameters in a\nsuitable RKHS. An algorithm is provided for learning the generalized natural\nparameter, and consistency of the estimator is established in the well\nspecified case. In experiments, the new method generally outperforms a\ncompeting approach with consistency guarantees, and is competitive with a deep\nconditional density model on datasets that exhibit abrupt transitions and\nheteroscedasticity. \n\n"}
{"id": "1711.05376", "contents": "Title: Sliced Wasserstein Distance for Learning Gaussian Mixture Models Abstract: Gaussian mixture models (GMM) are powerful parametric tools with many\napplications in machine learning and computer vision. Expectation maximization\n(EM) is the most popular algorithm for estimating the GMM parameters. However,\nEM guarantees only convergence to a stationary point of the log-likelihood\nfunction, which could be arbitrarily worse than the optimal solution. Inspired\nby the relationship between the negative log-likelihood function and the\nKullback-Leibler (KL) divergence, we propose an alternative formulation for\nestimating the GMM parameters using the sliced Wasserstein distance, which\ngives rise to a new algorithm. Specifically, we propose minimizing the\nsliced-Wasserstein distance between the mixture model and the data distribution\nwith respect to the GMM parameters. In contrast to the KL-divergence, the\nenergy landscape for the sliced-Wasserstein distance is more well-behaved and\ntherefore more suitable for a stochastic gradient descent scheme to obtain the\noptimal GMM parameters. We show that our formulation results in parameter\nestimates that are more robust to random initializations and demonstrate that\nit can estimate high-dimensional data distributions more faithfully than the EM\nalgorithm. \n\n"}
{"id": "1711.05597", "contents": "Title: Advances in Variational Inference Abstract: Many modern unsupervised or semi-supervised machine learning algorithms rely\non Bayesian probabilistic models. These models are usually intractable and thus\nrequire approximate inference. Variational inference (VI) lets us approximate a\nhigh-dimensional Bayesian posterior with a simpler variational distribution by\nsolving an optimization problem. This approach has been successfully used in\nvarious models and large-scale applications. In this review, we give an\noverview of recent trends in variational inference. We first introduce standard\nmean field variational inference, then review recent advances focusing on the\nfollowing aspects: (a) scalable VI, which includes stochastic approximations,\n(b) generic VI, which extends the applicability of VI to a large class of\notherwise intractable models, such as non-conjugate models, (c) accurate VI,\nwhich includes variational models beyond the mean field approximation or with\natypical divergences, and (d) amortized VI, which implements the inference over\nlocal latent variables with inference networks. Finally, we provide a summary\nof promising future research directions. \n\n"}
{"id": "1711.06756", "contents": "Title: Deep supervised learning using local errors Abstract: Error backpropagation is a highly effective mechanism for learning\nhigh-quality hierarchical features in deep networks. Updating the features or\nweights in one layer, however, requires waiting for the propagation of error\nsignals from higher layers. Learning using delayed and non-local errors makes\nit hard to reconcile backpropagation with the learning mechanisms observed in\nbiological neural networks as it requires the neurons to maintain a memory of\nthe input long enough until the higher-layer errors arrive. In this paper, we\npropose an alternative learning mechanism where errors are generated locally in\neach layer using fixed, random auxiliary classifiers. Lower layers could thus\nbe trained independently of higher layers and training could either proceed\nlayer by layer, or simultaneously in all layers using local error information.\nWe address biological plausibility concerns such as weight symmetry\nrequirements and show that the proposed learning mechanism based on fixed,\nbroad, and random tuning of each neuron to the classification categories\noutperforms the biologically-motivated feedback alignment learning technique on\nthe MNIST, CIFAR10, and SVHN datasets, approaching the performance of standard\nbackpropagation. Our approach highlights a potential biological mechanism for\nthe supervised, or task-dependent, learning of feature hierarchies. In\naddition, we show that it is well suited for learning deep networks in custom\nhardware where it can drastically reduce memory traffic and data communication\noverheads. \n\n"}
{"id": "1711.06798", "contents": "Title: MorphNet: Fast & Simple Resource-Constrained Structure Learning of Deep\n  Networks Abstract: We present MorphNet, an approach to automate the design of neural network\nstructures. MorphNet iteratively shrinks and expands a network, shrinking via a\nresource-weighted sparsifying regularizer on activations and expanding via a\nuniform multiplicative factor on all layers. In contrast to previous\napproaches, our method is scalable to large networks, adaptable to specific\nresource constraints (e.g. the number of floating-point operations per\ninference), and capable of increasing the network's performance. When applied\nto standard network architectures on a wide variety of datasets, our approach\ndiscovers novel structures in each domain, obtaining higher performance while\nrespecting the resource constraint. \n\n"}
{"id": "1711.07099", "contents": "Title: Compression-Based Regularization with an Application to Multi-Task\n  Learning Abstract: This paper investigates, from information theoretic grounds, a learning\nproblem based on the principle that any regularity in a given dataset can be\nexploited to extract compact features from data, i.e., using fewer bits than\nneeded to fully describe the data itself, in order to build meaningful\nrepresentations of a relevant content (multiple labels). We begin by\nintroducing the noisy lossy source coding paradigm with the log-loss fidelity\ncriterion which provides the fundamental tradeoffs between the\n\\emph{cross-entropy loss} (average risk) and the information rate of the\nfeatures (model complexity). Our approach allows an information theoretic\nformulation of the \\emph{multi-task learning} (MTL) problem which is a\nsupervised learning framework in which the prediction models for several\nrelated tasks are learned jointly from common representations to achieve better\ngeneralization performance. Then, we present an iterative algorithm for\ncomputing the optimal tradeoffs and its global convergence is proven provided\nthat some conditions hold. An important property of this algorithm is that it\nprovides a natural safeguard against overfitting, because it minimizes the\naverage risk taking into account a penalization induced by the model\ncomplexity. Remarkably, empirical results illustrate that there exists an\noptimal information rate minimizing the \\emph{excess risk} which depends on the\nnature and the amount of available training data. An application to\nhierarchical text categorization is also investigated, extending previous\nworks. \n\n"}
{"id": "1711.07831", "contents": "Title: On Breast Cancer Detection: An Application of Machine Learning\n  Algorithms on the Wisconsin Diagnostic Dataset Abstract: This paper presents a comparison of six machine learning (ML) algorithms:\nGRU-SVM (Agarap, 2017), Linear Regression, Multilayer Perceptron (MLP), Nearest\nNeighbor (NN) search, Softmax Regression, and Support Vector Machine (SVM) on\nthe Wisconsin Diagnostic Breast Cancer (WDBC) dataset (Wolberg, Street, &\nMangasarian, 1992) by measuring their classification test accuracy and their\nsensitivity and specificity values. The said dataset consists of features which\nwere computed from digitized images of FNA tests on a breast mass (Wolberg,\nStreet, & Mangasarian, 1992). For the implementation of the ML algorithms, the\ndataset was partitioned in the following fashion: 70% for training phase, and\n30% for the testing phase. The hyper-parameters used for all the classifiers\nwere manually assigned. Results show that all the presented ML algorithms\nperformed well (all exceeded 90% test accuracy) on the classification task. The\nMLP algorithm stands out among the implemented algorithms with a test accuracy\nof ~99.04%. \n\n"}
{"id": "1711.07871", "contents": "Title: Autoencoder Node Saliency: Selecting Relevant Latent Representations Abstract: The autoencoder is an artificial neural network model that learns hidden\nrepresentations of unlabeled data. With a linear transfer function it is\nsimilar to the principal component analysis (PCA). While both methods use\nweight vectors for linear transformations, the autoencoder does not come with\nany indication similar to the eigenvalues in PCA that are paired with the\neigenvectors. We propose a novel supervised node saliency (SNS) method that\nranks the hidden nodes by comparing class distributions of latent\nrepresentations against a fixed reference distribution. The latent\nrepresentations of a hidden node can be described using a one-dimensional\nhistogram. We apply normalized entropy difference (NED) to measure the\n\"interestingness\" of the histograms, and conclude a property for NED values to\nidentify a good classifying node. By applying our methods to real data sets, we\ndemonstrate the ability of SNS to explain what the trained autoencoders have\nlearned. \n\n"}
{"id": "1711.08132", "contents": "Title: Locally Smoothed Neural Networks Abstract: Convolutional Neural Networks (CNN) and the locally connected layer are\nlimited in capturing the importance and relations of different local receptive\nfields, which are often crucial for tasks such as face verification, visual\nquestion answering, and word sequence prediction. To tackle the issue, we\npropose a novel locally smoothed neural network (LSNN) in this paper. The main\nidea is to represent the weight matrix of the locally connected layer as the\nproduct of the kernel and the smoother, where the kernel is shared over\ndifferent local receptive fields, and the smoother is for determining the\nimportance and relations of different local receptive fields. Specifically, a\nmulti-variate Gaussian function is utilized to generate the smoother, for\nmodeling the location relations among different local receptive fields.\nFurthermore, the content information can also be leveraged by setting the mean\nand precision of the Gaussian function according to the content. Experiments on\nsome variant of MNIST clearly show our advantages over CNN and locally\nconnected layer. \n\n"}
{"id": "1711.08442", "contents": "Title: From Monte Carlo to Las Vegas: Improving Restricted Boltzmann Machine\n  Training Through Stopping Sets Abstract: We propose a Las Vegas transformation of Markov Chain Monte Carlo (MCMC)\nestimators of Restricted Boltzmann Machines (RBMs). We denote our approach\nMarkov Chain Las Vegas (MCLV). MCLV gives statistical guarantees in exchange\nfor random running times. MCLV uses a stopping set built from the training data\nand has maximum number of Markov chain steps K (referred as MCLV-K). We present\na MCLV-K gradient estimator (LVS-K) for RBMs and explore the correspondence and\ndifferences between LVS-K and Contrastive Divergence (CD-K), with LVS-K\nsignificantly outperforming CD-K training RBMs over the MNIST dataset,\nindicating MCLV to be a promising direction in learning generative models. \n\n"}
{"id": "1711.08833", "contents": "Title: Deep Learning for Real-Time Crime Forecasting and its Ternarization Abstract: Real-time crime forecasting is important. However, accurate prediction of\nwhen and where the next crime will happen is difficult. No known physical model\nprovides a reasonable approximation to such a complex system. Historical crime\ndata are sparse in both space and time and the signal of interests is weak. In\nthis work, we first present a proper representation of crime data. We then\nadapt the spatial temporal residual network on the well represented data to\npredict the distribution of crime in Los Angeles at the scale of hours in\nneighborhood-sized parcels. These experiments as well as comparisons with\nseveral existing approaches to prediction demonstrate the superiority of the\nproposed model in terms of accuracy. Finally, we present a ternarization\ntechnique to address the resource consumption issue for its deployment in real\nworld. This work is an extension of our short conference proceeding paper [Wang\net al, Arxiv 1707.03340]. \n\n"}
{"id": "1711.09176", "contents": "Title: Selling to a No-Regret Buyer Abstract: We consider the problem of a single seller repeatedly selling a single item\nto a single buyer (specifically, the buyer has a value drawn fresh from known\ndistribution $D$ in every round). Prior work assumes that the buyer is fully\nrational and will perfectly reason about how their bids today affect the\nseller's decisions tomorrow. In this work we initiate a different direction:\nthe buyer simply runs a no-regret learning algorithm over possible bids. We\nprovide a fairly complete characterization of optimal auctions for the seller\nin this domain. Specifically:\n  - If the buyer bids according to EXP3 (or any \"mean-based\" learning\nalgorithm), then the seller can extract expected revenue arbitrarily close to\nthe expected welfare. This auction is independent of the buyer's valuation $D$,\nbut somewhat unnatural as it is sometimes in the buyer's interest to overbid. -\nThere exists a learning algorithm $\\mathcal{A}$ such that if the buyer bids\naccording to $\\mathcal{A}$ then the optimal strategy for the seller is simply\nto post the Myerson reserve for $D$ every round. - If the buyer bids according\nto EXP3 (or any \"mean-based\" learning algorithm), but the seller is restricted\nto \"natural\" auction formats where overbidding is dominated (e.g. Generalized\nFirst-Price or Generalized Second-Price), then the optimal strategy for the\nseller is a pay-your-bid format with decreasing reserves over time. Moreover,\nthe seller's optimal achievable revenue is characterized by a linear program,\nand can be unboundedly better than the best truthful auction yet simultaneously\nunboundedly worse than the expected welfare. \n\n"}
{"id": "1711.10058", "contents": "Title: Dependent relevance determination for smooth and structured sparse\n  regression Abstract: In many problem settings, parameter vectors are not merely sparse but\ndependent in such a way that non-zero coefficients tend to cluster together. We\nrefer to this form of dependency as \"region sparsity.\" Classical sparse\nregression methods, such as the lasso and automatic relevance determination\n(ARD), which model parameters as independent a priori, and therefore do not\nexploit such dependencies. Here we introduce a hierarchical model for smooth,\nregion-sparse weight vectors and tensors in a linear regression setting. Our\napproach represents a hierarchical extension of the relevance determination\nframework, where we add a transformed Gaussian process to model the\ndependencies between the prior variances of regression weights. We combine this\nwith a structured model of the prior variances of Fourier coefficients, which\neliminates unnecessary high frequencies. The resulting prior encourages weights\nto be region-sparse in two different bases simultaneously. We develop Laplace\napproximation and Monte Carlo Markov Chain (MCMC) sampling to provide efficient\ninference for the posterior. Furthermore, a two-stage convex relaxation of the\nLaplace approximation approach is also provided to relax the inevitable\nnon-convexity during the optimization. We finally show substantial improvements\nover comparable methods for both simulated and real datasets from brain\nimaging. \n\n"}
{"id": "1711.10433", "contents": "Title: Parallel WaveNet: Fast High-Fidelity Speech Synthesis Abstract: The recently-developed WaveNet architecture is the current state of the art\nin realistic speech synthesis, consistently rated as more natural sounding for\nmany different languages than any previous system. However, because WaveNet\nrelies on sequential generation of one audio sample at a time, it is poorly\nsuited to today's massively parallel computers, and therefore hard to deploy in\na real-time production setting. This paper introduces Probability Density\nDistillation, a new method for training a parallel feed-forward network from a\ntrained WaveNet with no significant difference in quality. The resulting system\nis capable of generating high-fidelity speech samples at more than 20 times\nfaster than real-time, and is deployed online by Google Assistant, including\nserving multiple English and Japanese voices. \n\n"}
{"id": "1711.10462", "contents": "Title: Plan, Attend, Generate: Planning for Sequence-to-Sequence Models Abstract: We investigate the integration of a planning mechanism into\nsequence-to-sequence models using attention. We develop a model which can plan\nahead in the future when it computes its alignments between input and output\nsequences, constructing a matrix of proposed future alignments and a commitment\nvector that governs whether to follow or recompute the plan. This mechanism is\ninspired by the recently proposed strategic attentive reader and writer (STRAW)\nmodel for Reinforcement Learning. Our proposed model is end-to-end trainable\nusing primarily differentiable operations. We show that it outperforms a strong\nbaseline on character-level translation tasks from WMT'15, the algorithmic task\nof finding Eulerian circuits of graphs, and question generation from the text.\nOur analysis demonstrates that the model computes qualitatively intuitive\nalignments, converges faster than the baselines, and achieves superior\nperformance with fewer parameters. \n\n"}
{"id": "1711.10856", "contents": "Title: Semi-Supervised and Active Few-Shot Learning with Prototypical Networks Abstract: We consider the problem of semi-supervised few-shot classification where a\nclassifier needs to adapt to new tasks using a few labeled examples and\n(potentially many) unlabeled examples. We propose a clustering approach to the\nproblem. The features extracted with Prototypical Networks are clustered using\n$K$-means with the few labeled examples guiding the clustering process. We note\nthat in many real-world applications the adaptation performance can be\nsignificantly improved by requesting the few labels through user feedback. We\ndemonstrate good performance of the active adaptation strategy using image\ndata. \n\n"}
{"id": "1711.11581", "contents": "Title: Outlier-robust moment-estimation via sum-of-squares Abstract: We develop efficient algorithms for estimating low-degree moments of unknown\ndistributions in the presence of adversarial outliers. The guarantees of our\nalgorithms improve in many cases significantly over the best previous ones,\nobtained in recent works of Diakonikolas et al, Lai et al, and Charikar et al.\nWe also show that the guarantees of our algorithms match information-theoretic\nlower-bounds for the class of distributions we consider. These improved\nguarantees allow us to give improved algorithms for independent component\nanalysis and learning mixtures of Gaussians in the presence of outliers.\n  Our algorithms are based on a standard sum-of-squares relaxation of the\nfollowing conceptually-simple optimization problem: Among all distributions\nwhose moments are bounded in the same way as for the unknown distribution, find\nthe one that is closest in statistical distance to the empirical distribution\nof the adversarially-corrupted sample. \n\n"}
{"id": "1712.00499", "contents": "Title: Prediction-Constrained Topic Models for Antidepressant Recommendation Abstract: Supervisory signals can help topic models discover low-dimensional data\nrepresentations that are more interpretable for clinical tasks. We propose a\nframework for training supervised latent Dirichlet allocation that balances two\ngoals: faithful generative explanations of high-dimensional data and accurate\nprediction of associated class labels. Existing approaches fail to balance\nthese goals by not properly handling a fundamental asymmetry: the intended task\nis always predicting labels from data, not data from labels. Our new\nprediction-constrained objective trains models that predict labels from heldout\ndata well while also producing good generative likelihoods and interpretable\ntopic-word parameters. In a case study on predicting depression medications\nfrom electronic health records, we demonstrate improved recommendations\ncompared to previous supervised topic models and high- dimensional logistic\nregression from words alone. \n\n"}
{"id": "1712.00725", "contents": "Title: Sentiment Classification using Images and Label Embeddings Abstract: In this project we analysed how much semantic information images carry, and\nhow much value image data can add to sentiment analysis of the text associated\nwith the images. To better understand the contribution from images, we compared\nmodels which only made use of image data, models which only made use of text\ndata, and models which combined both data types. We also analysed if this\napproach could help sentiment classifiers generalize to unknown sentiments. \n\n"}
{"id": "1712.00891", "contents": "Title: Data Dropout in Arbitrary Basis for Deep Network Regularization Abstract: An important problem in training deep networks with high capacity is to\nensure that the trained network works well when presented with new inputs\noutside the training dataset. Dropout is an effective regularization technique\nto boost the network generalization in which a random subset of the elements of\nthe given data and the extracted features are set to zero during the training\nprocess. In this paper, a new randomized regularization technique in which we\nwithhold a random part of the data without necessarily turning off the\nneurons/data-elements is proposed. In the proposed method, of which the\nconventional dropout is shown to be a special case, random data dropout is\nperformed in an arbitrary basis, hence the designation Generalized Dropout. We\nalso present a framework whereby the proposed technique can be applied\nefficiently to convolutional neural networks. The presented numerical\nexperiments demonstrate that the proposed technique yields notable performance\ngain. Generalized Dropout provides new insight into the idea of dropout, shows\nthat we can achieve different performance gains by using different bases\nmatrices, and opens up a new research question as of how to choose optimal\nbases matrices that achieve maximal performance gain. \n\n"}
{"id": "1712.01727", "contents": "Title: OL\\'E: Orthogonal Low-rank Embedding, A Plug and Play Geometric Loss for\n  Deep Learning Abstract: Deep neural networks trained using a softmax layer at the top and the\ncross-entropy loss are ubiquitous tools for image classification. Yet, this\ndoes not naturally enforce intra-class similarity nor inter-class margin of the\nlearned deep representations. To simultaneously achieve these two goals,\ndifferent solutions have been proposed in the literature, such as the pairwise\nor triplet losses. However, such solutions carry the extra task of selecting\npairs or triplets, and the extra computational burden of computing and learning\nfor many combinations of them. In this paper, we propose a plug-and-play loss\nterm for deep networks that explicitly reduces intra-class variance and\nenforces inter-class margin simultaneously, in a simple and elegant geometric\nmanner. For each class, the deep features are collapsed into a learned linear\nsubspace, or union of them, and inter-class subspaces are pushed to be as\northogonal as possible. Our proposed Orthogonal Low-rank Embedding (OL\\'E) does\nnot require carefully crafting pairs or triplets of samples for training, and\nworks standalone as a classification loss, being the first reported deep metric\nlearning framework of its kind. Because of the improved margin between features\nof different classes, the resulting deep networks generalize better, are more\ndiscriminative, and more robust. We demonstrate improved classification\nperformance in general object recognition, plugging the proposed loss term into\nexisting off-the-shelf architectures. In particular, we show the advantage of\nthe proposed loss in the small data/model scenario, and we significantly\nadvance the state-of-the-art on the Stanford STL-10 benchmark. \n\n"}
{"id": "1712.02154", "contents": "Title: Guided Labeling using Convolutional Neural Networks Abstract: Over the last couple of years, deep learning and especially convolutional\nneural networks have become one of the work horses of computer vision. One\nlimiting factor for the applicability of supervised deep learning to more areas\nis the need for large, manually labeled datasets. In this paper we propose an\neasy to implement method we call guided labeling, which automatically\ndetermines which samples from an unlabeled dataset should be labeled. We show\nthat using this procedure, the amount of samples that need to be labeled is\nreduced considerably in comparison to labeling images arbitrarily. \n\n"}
{"id": "1712.02950", "contents": "Title: CycleGAN, a Master of Steganography Abstract: CycleGAN (Zhu et al. 2017) is one recent successful approach to learn a\ntransformation between two image distributions. In a series of experiments, we\ndemonstrate an intriguing property of the model: CycleGAN learns to \"hide\"\ninformation about a source image into the images it generates in a nearly\nimperceptible, high-frequency signal. This trick ensures that the generator can\nrecover the original sample and thus satisfy the cyclic consistency\nrequirement, while the generated image remains realistic. We connect this\nphenomenon with adversarial attacks by viewing CycleGAN's training procedure as\ntraining a generator of adversarial examples and demonstrate that the cyclic\nconsistency loss causes CycleGAN to be especially vulnerable to adversarial\nattacks. \n\n"}
{"id": "1712.03298", "contents": "Title: Neumann Optimizer: A Practical Optimization Algorithm for Deep Neural\n  Networks Abstract: Progress in deep learning is slowed by the days or weeks it takes to train\nlarge models. The natural solution of using more hardware is limited by\ndiminishing returns, and leads to inefficient use of additional resources. In\nthis paper, we present a large batch, stochastic optimization algorithm that is\nboth faster than widely used algorithms for fixed amounts of computation, and\nalso scales up substantially better as more computational resources become\navailable. Our algorithm implicitly computes the inverse Hessian of each\nmini-batch to produce descent directions; we do so without either an explicit\napproximation to the Hessian or Hessian-vector products. We demonstrate the\neffectiveness of our algorithm by successfully training large ImageNet models\n(Inception-V3, Resnet-50, Resnet-101 and Inception-Resnet-V2) with mini-batch\nsizes of up to 32000 with no loss in validation error relative to current\nbaselines, and no increase in the total number of steps. At smaller mini-batch\nsizes, our optimizer improves the validation error in these models by 0.8-0.9%.\nAlternatively, we can trade off this accuracy to reduce the number of training\nsteps needed by roughly 10-30%. Our work is practical and easily usable by\nothers -- only one hyperparameter (learning rate) needs tuning, and\nfurthermore, the algorithm is as computationally cheap as the commonly used\nAdam optimizer. \n\n"}
{"id": "1712.04407", "contents": "Title: Logo Synthesis and Manipulation with Clustered Generative Adversarial\n  Networks Abstract: Designing a logo for a new brand is a lengthy and tedious back-and-forth\nprocess between a designer and a client. In this paper we explore to what\nextent machine learning can solve the creative task of the designer. For this,\nwe build a dataset -- LLD -- of 600k+ logos crawled from the world wide web.\nTraining Generative Adversarial Networks (GANs) for logo synthesis on such\nmulti-modal data is not straightforward and results in mode collapse for some\nstate-of-the-art methods. We propose the use of synthetic labels obtained\nthrough clustering to disentangle and stabilize GAN training. We are able to\ngenerate a high diversity of plausible logos and we demonstrate latent space\nexploration techniques to ease the logo design task in an interactive manner.\nMoreover, we validate the proposed clustered GAN training on CIFAR 10,\nachieving state-of-the-art Inception scores when using synthetic labels\nobtained via clustering the features of an ImageNet classifier. GANs can cope\nwith multi-modal data by means of synthetic labels achieved through clustering,\nand our results show the creative potential of such techniques for logo\nsynthesis and manipulation. Our dataset and models will be made publicly\navailable at https://data.vision.ee.ethz.ch/cvl/lld/. \n\n"}
{"id": "1712.04567", "contents": "Title: Practical Bayesian optimization in the presence of outliers Abstract: Inference in the presence of outliers is an important field of research as\noutliers are ubiquitous and may arise across a variety of problems and domains.\nBayesian optimization is method that heavily relies on probabilistic inference.\nThis allows outstanding sample efficiency because the probabilistic machinery\nprovides a memory of the whole optimization process. However, that virtue\nbecomes a disadvantage when the memory is populated with outliers, inducing\nbias in the estimation. In this paper, we present an empirical evaluation of\nBayesian optimization methods in the presence of outliers. The empirical\nevidence shows that Bayesian optimization with robust regression often produces\nsuboptimal results. We then propose a new algorithm which combines robust\nregression (a Gaussian process with Student-t likelihood) with outlier\ndiagnostics to classify data points as outliers or inliers. By using an\nscheduler for the classification of outliers, our method is more efficient and\nhas better convergence over the standard robust regression. Furthermore, we\nshow that even in controlled situations with no expected outliers, our method\nis able to produce better results. \n\n"}
{"id": "1712.05790", "contents": "Title: Deep Burst Denoising Abstract: Noise is an inherent issue of low-light image capture, one which is\nexacerbated on mobile devices due to their narrow apertures and small sensors.\nOne strategy for mitigating noise in a low-light situation is to increase the\nshutter time of the camera, thus allowing each photosite to integrate more\nlight and decrease noise variance. However, there are two downsides of long\nexposures: (a) bright regions can exceed the sensor range, and (b) camera and\nscene motion will result in blurred images. Another way of gathering more light\nis to capture multiple short (thus noisy) frames in a \"burst\" and intelligently\nintegrate the content, thus avoiding the above downsides. In this paper, we use\nthe burst-capture strategy and implement the intelligent integration via a\nrecurrent fully convolutional deep neural net (CNN). We build our novel,\nmultiframe architecture to be a simple addition to any single frame denoising\nmodel, and design to handle an arbitrary number of noisy input frames. We show\nthat it achieves state of the art denoising results on our burst dataset,\nimproving on the best published multi-frame techniques, such as VBM4D and\nFlexISP. Finally, we explore other applications of image enhancement by\nintegrating content from multiple frames and demonstrate that our DNN\narchitecture generalizes well to image super-resolution. \n\n"}
{"id": "1712.06096", "contents": "Title: Efficient B-mode Ultrasound Image Reconstruction from Sub-sampled RF\n  Data using Deep Learning Abstract: In portable, three dimensional, and ultra-fast ultrasound imaging systems,\nthere is an increasing demand for the reconstruction of high quality images\nfrom a limited number of radio-frequency (RF) measurements due to receiver (Rx)\nor transmit (Xmit) event sub-sampling. However, due to the presence of side\nlobe artifacts from RF sub-sampling, the standard beamformer often produces\nblurry images with less contrast, which are unsuitable for diagnostic purposes.\nExisting compressed sensing approaches often require either hardware changes or\ncomputationally expensive algorithms, but their quality improvements are\nlimited. To address this problem, here we propose a novel deep learning\napproach that directly interpolates the missing RF data by utilizing redundancy\nin the Rx-Xmit plane. Our extensive experimental results using sub-sampled RF\ndata from a multi-line acquisition B-mode system confirm that the proposed\nmethod can effectively reduce the data rate without sacrificing image quality. \n\n"}
{"id": "1712.06424", "contents": "Title: Learning to Write Stylized Chinese Characters by Reading a Handful of\n  Examples Abstract: Automatically writing stylized Chinese characters is an attractive yet\nchallenging task due to its wide applicabilities. In this paper, we propose a\nnovel framework named Style-Aware Variational Auto-Encoder (SA-VAE) to flexibly\ngenerate Chinese characters. Specifically, we propose to capture the different\ncharacteristics of a Chinese character by disentangling the latent features\ninto content-related and style-related components. Considering of the complex\nshapes and structures, we incorporate the structure information as prior\nknowledge into our framework to guide the generation. Our framework shows a\npowerful one-shot/low-shot generalization ability by inferring the style\ncomponent given a character with unseen style. To the best of our knowledge,\nthis is the first attempt to learn to write new-style Chinese characters by\nobserving only one or a few examples. Extensive experiments demonstrate its\neffectiveness in generating different stylized Chinese characters by fusing the\nfeature vectors corresponding to different contents and styles, which is of\nsignificant importance in real-world applications. \n\n"}
{"id": "1712.06745", "contents": "Title: Efficient Algorithms for Searching the Minimum Information Partition in\n  Integrated Information Theory Abstract: The ability to integrate information in the brain is considered to be an\nessential property for cognition and consciousness. Integrated Information\nTheory (IIT) hypothesizes that the amount of integrated information ($\\Phi$) in\nthe brain is related to the level of consciousness. IIT proposes that to\nquantify information integration in a system as a whole, integrated information\nshould be measured across the partition of the system at which information loss\ncaused by partitioning is minimized, called the Minimum Information Partition\n(MIP). The computational cost for exhaustively searching for the MIP grows\nexponentially with system size, making it difficult to apply IIT to real neural\ndata. It has been previously shown that if a measure of $\\Phi$ satisfies a\nmathematical property, submodularity, the MIP can be found in a polynomial\norder by an optimization algorithm. However, although the first version of\n$\\Phi$ is submodular, the later versions are not. In this study, we empirically\nexplore to what extent the algorithm can be applied to the non-submodular\nmeasures of $\\Phi$ by evaluating the accuracy of the algorithm in simulated\ndata and real neural data. We find that the algorithm identifies the MIP in a\nnearly perfect manner even for the non-submodular measures. Our results show\nthat the algorithm allows us to measure $\\Phi$ in large systems within a\npractical amount of time. \n\n"}
{"id": "1712.07203", "contents": "Title: Discovery of Shifting Patterns in Sequence Classification Abstract: In this paper, we investigate the multi-variate sequence classification\nproblem from a multi-instance learning perspective. Real-world sequential data\ncommonly show discriminative patterns only at specific time periods. For\ninstance, we can identify a cropland during its growing season, but it looks\nsimilar to a barren land after harvest or before planting. Besides, even within\nthe same class, the discriminative patterns can appear in different periods of\nsequential data. Due to such property, these discriminative patterns are also\nreferred to as shifting patterns. The shifting patterns in sequential data\nseverely degrade the performance of traditional classification methods without\nsufficient training data.\n  We propose a novel sequence classification method by automatically mining\nshifting patterns from multi-variate sequence. The method employs a\nmulti-instance learning approach to detect shifting patterns while also\nmodeling temporal relationships within each multi-instance bag by an LSTM model\nto further improve the classification performance. We extensively evaluate our\nmethod on two real-world applications - cropland mapping and affective state\nrecognition. The experiments demonstrate the superiority of our proposed method\nin sequence classification performance and in detecting discriminative shifting\npatterns. \n\n"}
{"id": "1712.07233", "contents": "Title: Hyperparameters Optimization in Deep Convolutional Neural Network /\n  Bayesian Approach with Gaussian Process Prior Abstract: Convolutional Neural Network is known as ConvNet have been extensively used\nin many complex machine learning tasks. However, hyperparameters optimization\nis one of a crucial step in developing ConvNet architectures, since the\naccuracy and performance are reliant on the hyperparameters. This multilayered\narchitecture parameterized by a set of hyperparameters such as the number of\nconvolutional layers, number of fully connected dense layers & neurons, the\nprobability of dropout implementation, learning rate. Hence the searching the\nhyperparameter over the hyperparameter space are highly difficult to build such\ncomplex hierarchical architecture. Many methods have been proposed over the\ndecade to explore the hyperparameter space and find the optimum set of\nhyperparameter values. Reportedly, Gird search and Random search are said to be\ninefficient and extremely expensive, due to a large number of hyperparameters\nof the architecture. Hence, Sequential model-based Bayesian Optimization is a\npromising alternative technique to address the extreme of the unknown cost\nfunction. The recent study on Bayesian Optimization by Snoek in nine\nconvolutional network parameters is achieved the lowerest error report in the\nCIFAR-10 benchmark. This article is intended to provide the overview of the\nmathematical concept behind the Bayesian Optimization over a Gaussian prior. \n\n"}
{"id": "1801.00318", "contents": "Title: Towards Building an Intelligent Anti-Malware System: A Deep Learning\n  Approach using Support Vector Machine (SVM) for Malware Classification Abstract: Effective and efficient mitigation of malware is a long-time endeavor in the\ninformation security community. The development of an anti-malware system that\ncan counteract an unknown malware is a prolific activity that may benefit\nseveral sectors. We envision an intelligent anti-malware system that utilizes\nthe power of deep learning (DL) models. Using such models would enable the\ndetection of newly-released malware through mathematical generalization. That\nis, finding the relationship between a given malware $x$ and its corresponding\nmalware family $y$, $f: x \\mapsto y$. To accomplish this feat, we used the\nMalimg dataset (Nataraj et al., 2011) which consists of malware images that\nwere processed from malware binaries, and then we trained the following DL\nmodels 1 to classify each malware family: CNN-SVM (Tang, 2013), GRU-SVM\n(Agarap, 2017), and MLP-SVM. Empirical evidence has shown that the GRU-SVM\nstands out among the DL models with a predictive accuracy of ~84.92%. This\nstands to reason for the mentioned model had the relatively most sophisticated\narchitecture design among the presented models. The exploration of an even more\noptimal DL-SVM model is the next stage towards the engineering of an\nintelligent anti-malware system. \n\n"}
{"id": "1801.02642", "contents": "Title: Boundary Optimizing Network (BON) Abstract: Despite all the success that deep neural networks have seen in classifying\ncertain datasets, the challenge of finding optimal solutions that generalize\nstill remains. In this paper, we propose the Boundary Optimizing Network (BON),\na new approach to generalization for deep neural networks when used for\nsupervised learning. Given a classification network, we propose to use a\ncollaborative generative network that produces new synthetic data points in the\nform of perturbations of original data points. In this way, we create a data\nsupport around each original data point which prevents decision boundaries from\npassing too close to the original data points, i.e. prevents overfitting. We\nshow that BON improves convergence on CIFAR-10 using the state-of-the-art\nDensenet. We do however observe that the generative network suffers from\ncatastrophic forgetting during training, and we therefore propose to use a\nvariation of Memory Aware Synapses to optimize the generative network (called\nBON++). On the Iris dataset, we visualize the effect of BON++ when the\ngenerator does not suffer from catastrophic forgetting and conclude that the\napproach has the potential to create better boundaries in a higher dimensional\nspace. \n\n"}
{"id": "1801.02901", "contents": "Title: Convexification of Neural Graph Abstract: Traditionally, most complex intelligence architectures are extremely\nnon-convex, which could not be well performed by convex optimization. However,\nthis paper decomposes complex structures into three types of nodes: operators,\nalgorithms and functions. Iteratively, propagating from node to node along\nedge, we prove that \"regarding the tree-structured neural graph, it is nearly\nconvex in each variable, when the other variables are fixed.\" In fact, the\nnon-convex properties stem from circles and functions, which could be\ntransformed to be convex with our proposed \\textit{\\textbf{scale mechanism}}.\nExperimentally, we justify our theoretical analysis by two practical\napplications. \n\n"}
{"id": "1801.03558", "contents": "Title: Inference Suboptimality in Variational Autoencoders Abstract: Amortized inference allows latent-variable models trained via variational\nlearning to scale to large datasets. The quality of approximate inference is\ndetermined by two factors: a) the capacity of the variational distribution to\nmatch the true posterior and b) the ability of the recognition network to\nproduce good variational parameters for each datapoint. We examine approximate\ninference in variational autoencoders in terms of these factors. We find that\ndivergence from the true posterior is often due to imperfect recognition\nnetworks, rather than the limited complexity of the approximating distribution.\nWe show that this is due partly to the generator learning to accommodate the\nchoice of approximation. Furthermore, we show that the parameters used to\nincrease the expressiveness of the approximation play a role in generalizing\ninference rather than simply improving the complexity of the approximation. \n\n"}
{"id": "1801.03911", "contents": "Title: Stochastic Learning of Nonstationary Kernels for Natural Language\n  Modeling Abstract: Natural language processing often involves computations with semantic or\nsyntactic graphs to facilitate sophisticated reasoning based on structural\nrelationships. While convolution kernels provide a powerful tool for comparing\ngraph structure based on node (word) level relationships, they are difficult to\ncustomize and can be computationally expensive. We propose a generalization of\nconvolution kernels, with a nonstationary model, for better expressibility of\nnatural languages in supervised settings. For a scalable learning of the\nparameters introduced with our model, we propose a novel algorithm that\nleverages stochastic sampling on k-nearest neighbor graphs, along with\napproximations based on locality-sensitive hashing. We demonstrate the\nadvantages of our approach on a challenging real-world (structured inference)\nproblem of automatically extracting biological models from the text of\nscientific papers. \n\n"}
{"id": "1801.04055", "contents": "Title: A3T: Adversarially Augmented Adversarial Training Abstract: Recent research showed that deep neural networks are highly sensitive to\nso-called adversarial perturbations, which are tiny perturbations of the input\ndata purposely designed to fool a machine learning classifier. Most\nclassification models, including deep learning models, are highly vulnerable to\nadversarial attacks. In this work, we investigate a procedure to improve\nadversarial robustness of deep neural networks through enforcing representation\ninvariance. The idea is to train the classifier jointly with a discriminator\nattached to one of its hidden layer and trained to filter the adversarial\nnoise. We perform preliminary experiments to test the viability of the approach\nand to compare it to other standard adversarial training methods. \n\n"}
{"id": "1801.04813", "contents": "Title: Predicting Movie Genres Based on Plot Summaries Abstract: This project explores several Machine Learning methods to predict movie\ngenres based on plot summaries. Naive Bayes, Word2Vec+XGBoost and Recurrent\nNeural Networks are used for text classification, while K-binary\ntransformation, rank method and probabilistic classification with learned\nprobability threshold are employed for the multi-label problem involved in the\ngenre tagging task.Experiments with more than 250,000 movies show that\nemploying the Gated Recurrent Units (GRU) neural networks for the probabilistic\nclassification with learned probability threshold approach achieves the best\nresult on the test set. The model attains a Jaccard Index of 50.0%, a F-score\nof 0.56, and a hit rate of 80.5%. \n\n"}
{"id": "1801.05574", "contents": "Title: Brenier approach for optimal transportation between a quasi-discrete\n  measure and a discrete measure Abstract: Correctly estimating the discrepancy between two data distributions has\nalways been an important task in Machine Learning. Recently, Cuturi proposed\nthe Sinkhorn distance which makes use of an approximate Optimal Transport cost\nbetween two distributions as a distance to describe distribution discrepancy.\nAlthough it has been successfully adopted in various machine learning\napplications (e.g. in Natural Language Processing and Computer Vision) since\nthen, the Sinkhorn distance also suffers from two unnegligible limitations. The\nfirst one is that the Sinkhorn distance only gives an approximation of the real\nWasserstein distance, the second one is the `divide by zero' problem which\noften occurs during matrix scaling when setting the entropy regularization\ncoefficient to a small value. In this paper, we introduce a new Brenier\napproach for calculating a more accurate Wasserstein distance between two\ndiscrete distributions, this approach successfully avoids the two limitations\nshown above for Sinkhorn distance and gives an alternative way for estimating\ndistribution discrepancy. \n\n"}
{"id": "1801.06879", "contents": "Title: Bayesian Deep Convolutional Encoder-Decoder Networks for Surrogate\n  Modeling and Uncertainty Quantification Abstract: We are interested in the development of surrogate models for uncertainty\nquantification and propagation in problems governed by stochastic PDEs using a\ndeep convolutional encoder-decoder network in a similar fashion to approaches\nconsidered in deep learning for image-to-image regression tasks. Since normal\nneural networks are data intensive and cannot provide predictive uncertainty,\nwe propose a Bayesian approach to convolutional neural nets. A recently\nintroduced variational gradient descent algorithm based on Stein's method is\nscaled to deep convolutional networks to perform approximate Bayesian inference\non millions of uncertain network parameters. This approach achieves state of\nthe art performance in terms of predictive accuracy and uncertainty\nquantification in comparison to other approaches in Bayesian neural networks as\nwell as techniques that include Gaussian processes and ensemble methods even\nwhen the training data size is relatively small. To evaluate the performance of\nthis approach, we consider standard uncertainty quantification benchmark\nproblems including flow in heterogeneous media defined in terms of limited\ndata-driven permeability realizations. The performance of the surrogate model\ndeveloped is very good even though there is no underlying structure shared\nbetween the input (permeability) and output (flow/pressure) fields as is often\nthe case in the image-to-image regression models used in computer vision\nproblems. Studies are performed with an underlying stochastic input\ndimensionality up to $4,225$ where most other uncertainty quantification\nmethods fail. Uncertainty propagation tasks are considered and the predictive\noutput Bayesian statistics are compared to those obtained with Monte Carlo\nestimates. \n\n"}
{"id": "1801.07194", "contents": "Title: Optimizing Prediction Intervals by Tuning Random Forest via\n  Meta-Validation Abstract: Recent studies have shown that tuning prediction models increases prediction\naccuracy and that Random Forest can be used to construct prediction intervals.\nHowever, to our best knowledge, no study has investigated the need to, and the\nmanner in which one can, tune Random Forest for optimizing prediction intervals\n{ this paper aims to fill this gap. We explore a tuning approach that combines\nan effectively exhaustive search with a validation technique on a single Random\nForest parameter. This paper investigates which, out of eight validation\ntechniques, are beneficial for tuning, i.e., which automatically choose a\nRandom Forest configuration constructing prediction intervals that are reliable\nand with a smaller width than the default configuration. Additionally, we\npresent and validate three meta-validation techniques to determine which are\nbeneficial, i.e., those which automatically chose a beneficial validation\ntechnique. This study uses data from our industrial partner (Keymind Inc.) and\nthe Tukutuku Research Project, related to post-release defect prediction and\nWeb application effort estimation, respectively. Results from our study\nindicate that: i) the default configuration is frequently unreliable, ii) most\nof the validation techniques, including previously successfully adopted ones\nsuch as 50/50 holdout and bootstrap, are counterproductive in most of the\ncases, and iii) the 75/25 holdout meta-validation technique is always\nbeneficial; i.e., it avoids the likely counterproductive effects of validation\ntechniques. \n\n"}
{"id": "1801.07292", "contents": "Title: Convergence of Value Aggregation for Imitation Learning Abstract: Value aggregation is a general framework for solving imitation learning\nproblems. Based on the idea of data aggregation, it generates a policy sequence\nby iteratively interleaving policy optimization and evaluation in an online\nlearning setting. While the existence of a good policy in the policy sequence\ncan be guaranteed non-asymptotically, little is known about the convergence of\nthe sequence or the performance of the last policy. In this paper, we debunk\nthe common belief that value aggregation always produces a convergent policy\nsequence with improving performance. Moreover, we identify a critical stability\ncondition for convergence and provide a tight non-asymptotic bound on the\nperformance of the last policy. These new theoretical insights let us stabilize\nproblems with regularization, which removes the inconvenient process of\nidentifying the best policy in the policy sequence in stochastic problems. \n\n"}
{"id": "1801.07353", "contents": "Title: Flexible Deep Neural Network Processing Abstract: The recent success of Deep Neural Networks (DNNs) has drastically improved\nthe state of the art for many application domains. While achieving high\naccuracy performance, deploying state-of-the-art DNNs is a challenge since they\ntypically require billions of expensive arithmetic computations. In addition,\nDNNs are typically deployed in ensemble to boost accuracy performance, which\nfurther exacerbates the system requirements. This computational overhead is an\nissue for many platforms, e.g. data centers and embedded systems, with tight\nlatency and energy budgets. In this article, we introduce flexible DNNs\nensemble processing technique, which achieves large reduction in average\ninference latency while incurring small to negligible accuracy drop. Our\ntechnique is flexible in that it allows for dynamic adaptation between quality\nof results (QoR) and execution runtime. We demonstrate the effectiveness of the\ntechnique on AlexNet and ResNet-50 using the ImageNet dataset. This technique\ncan also easily handle other types of networks. \n\n"}
{"id": "1801.09326", "contents": "Title: Sparse and Low-rank Tensor Estimation via Cubic Sketchings Abstract: In this paper, we propose a general framework for sparse and low-rank tensor\nestimation from cubic sketchings. A two-stage non-convex implementation is\ndeveloped based on sparse tensor decomposition and thresholded gradient\ndescent, which ensures exact recovery in the noiseless case and stable recovery\nin the noisy case with high probability. The non-asymptotic analysis sheds\nlight on an interplay between optimization error and statistical error. The\nproposed procedure is shown to be rate-optimal under certain conditions. As a\ntechnical by-product, novel high-order concentration inequalities are derived\nfor studying high-moment sub-Gaussian tensors. An interesting tensor\nformulation illustrates the potential application to high-order interaction\npursuit in high-dimensional linear regression. \n\n"}
{"id": "1801.09597", "contents": "Title: Deep Reinforcement Learning using Capsules in Advanced Game Environments Abstract: Reinforcement Learning (RL) is a research area that has blossomed\ntremendously in recent years and has shown remarkable potential for artificial\nintelligence based opponents in computer games. This success is primarily due\nto vast capabilities of Convolutional Neural Networks (ConvNet), enabling\nalgorithms to extract useful information from noisy environments. Capsule\nNetwork (CapsNet) is a recent introduction to the Deep Learning algorithm group\nand has only barely begun to be explored. The network is an architecture for\nimage classification, with superior performance for classification of the MNIST\ndataset. CapsNets have not been explored beyond image classification.\n  This thesis introduces the use of CapsNet for Q-Learning based game\nalgorithms. To successfully apply CapsNet in advanced game play, three main\ncontributions follow. First, the introduction of four new game environments as\nframeworks for RL research with increasing complexity, namely Flash RL, Deep\nLine Wars, Deep RTS, and Deep Maze. These environments fill the gap between\nrelatively simple and more complex game environments available for RL research\nand are in the thesis used to test and explore the CapsNet behavior.\n  Second, the thesis introduces a generative modeling approach to produce\nartificial training data for use in Deep Learning models including CapsNets. We\nempirically show that conditional generative modeling can successfully generate\ngame data of sufficient quality to train a Deep Q-Network well.\n  Third, we show that CapsNet is a reliable architecture for Deep Q-Learning\nbased algorithms for game AI. A capsule is a group of neurons that determine\nthe presence of objects in the data and is in the literature shown to increase\nthe robustness of training and predictions while lowering the amount training\ndata needed. It should, therefore, be ideally suited for game plays. \n\n"}
{"id": "1801.10130", "contents": "Title: Spherical CNNs Abstract: Convolutional Neural Networks (CNNs) have become the method of choice for\nlearning problems involving 2D planar images. However, a number of problems of\nrecent interest have created a demand for models that can analyze spherical\nimages. Examples include omnidirectional vision for drones, robots, and\nautonomous cars, molecular regression problems, and global weather and climate\nmodelling. A naive application of convolutional networks to a planar projection\nof the spherical signal is destined to fail, because the space-varying\ndistortions introduced by such a projection will make translational weight\nsharing ineffective.\n  In this paper we introduce the building blocks for constructing spherical\nCNNs. We propose a definition for the spherical cross-correlation that is both\nexpressive and rotation-equivariant. The spherical correlation satisfies a\ngeneralized Fourier theorem, which allows us to compute it efficiently using a\ngeneralized (non-commutative) Fast Fourier Transform (FFT) algorithm. We\ndemonstrate the computational efficiency, numerical accuracy, and effectiveness\nof spherical CNNs applied to 3D model recognition and atomization energy\nregression. \n\n"}
{"id": "1801.10199", "contents": "Title: A novel methodology on distributed representations of proteins using\n  their interacting ligands Abstract: The effective representation of proteins is a crucial task that directly\naffects the performance of many bioinformatics problems. Related proteins\nusually bind to similar ligands. Chemical characteristics of ligands are known\nto capture the functional and mechanistic properties of proteins suggesting\nthat a ligand based approach can be utilized in protein representation. In this\nstudy, we propose SMILESVec, a SMILES-based method to represent ligands and a\nnovel method to compute similarity of proteins by describing them based on\ntheir ligands. The proteins are defined utilizing the word-embeddings of the\nSMILES strings of their ligands. The performance of the proposed protein\ndescription method is evaluated in protein clustering task using TransClust and\nMCL algorithms. Two other protein representation methods that utilize protein\nsequence, BLAST and ProtVec, and two compound fingerprint based protein\nrepresentation methods are compared. We showed that ligand-based protein\nrepresentation, which uses only SMILES strings of the ligands that proteins\nbind to, performs as well as protein-sequence based representation methods in\nprotein clustering. The results suggest that ligand-based protein description\ncan be an alternative to the traditional sequence or structure based\nrepresentation of proteins and this novel approach can be applied to different\nbioinformatics problems such as prediction of new protein-ligand interactions\nand protein function annotation. \n\n"}
{"id": "1801.10395", "contents": "Title: Probabilistic Recurrent State-Space Models Abstract: State-space models (SSMs) are a highly expressive model class for learning\npatterns in time series data and for system identification. Deterministic\nversions of SSMs (e.g. LSTMs) proved extremely successful in modeling complex\ntime series data. Fully probabilistic SSMs, however, are often found hard to\ntrain, even for smaller problems. To overcome this limitation, we propose a\nnovel model formulation and a scalable training algorithm based on doubly\nstochastic variational inference and Gaussian processes. In contrast to\nexisting work, the proposed variational approximation allows one to fully\ncapture the latent state temporal correlations. These correlations are the key\nto robust training. The effectiveness of the proposed PR-SSM is evaluated on a\nset of real-world benchmark datasets in comparison to state-of-the-art\nprobabilistic model learning methods. Scalability and robustness are\ndemonstrated on a high dimensional problem. \n\n"}
{"id": "1802.02212", "contents": "Title: Classification and Disease Localization in Histopathology Using Only\n  Global Labels: A Weakly-Supervised Approach Abstract: Analysis of histopathology slides is a critical step for many diagnoses, and\nin particular in oncology where it defines the gold standard. In the case of\ndigital histopathological analysis, highly trained pathologists must review\nvast whole-slide-images of extreme digital resolution ($100,000^2$ pixels)\nacross multiple zoom levels in order to locate abnormal regions of cells, or in\nsome cases single cells, out of millions. The application of deep learning to\nthis problem is hampered not only by small sample sizes, as typical datasets\ncontain only a few hundred samples, but also by the generation of ground-truth\nlocalized annotations for training interpretable classification and\nsegmentation models. We propose a method for disease localization in the\ncontext of weakly supervised learning, where only image-level labels are\navailable during training. Even without pixel-level annotations, we are able to\ndemonstrate performance comparable with models trained with strong annotations\non the Camelyon-16 lymph node metastases detection challenge. We accomplish\nthis through the use of pre-trained deep convolutional networks, feature\nembedding, as well as learning via top instances and negative evidence, a\nmultiple instance learning technique from the field of semantic segmentation\nand object detection. \n\n"}
{"id": "1802.02290", "contents": "Title: Spectral Image Visualization Using Generative Adversarial Networks Abstract: Spectral images captured by satellites and radio-telescopes are analyzed to\nobtain information about geological compositions distributions, distant asters\nas well as undersea terrain. Spectral images usually contain tens to hundreds\nof continuous narrow spectral bands and are widely used in various fields. But\nthe vast majority of those image signals are beyond the visible range, which\ncalls for special visualization technique. The visualizations of spectral\nimages shall convey as much information as possible from the original signal\nand facilitate image interpretation. However, most of the existing visualizatio\nmethods display spectral images in false colors, which contradict with human's\nexperience and expectation. In this paper, we present a novel visualization\ngenerative adversarial network (GAN) to display spectral images in natural\ncolors. To achieve our goal, we propose a loss function which consists of an\nadversarial loss and a structure loss. The adversarial loss pushes our solution\nto the natural image distribution using a discriminator network that is trained\nto differentiate between false-color images and natural-color images. We also\nuse a cycle loss as the structure constraint to guarantee structure\nconsistency. Experimental results show that our method is able to generate\nstructure-preserved and natural-looking visualizations. \n\n"}
{"id": "1802.03133", "contents": "Title: Batch Kalman Normalization: Towards Training Deep Neural Networks with\n  Micro-Batches Abstract: As an indispensable component, Batch Normalization (BN) has successfully\nimproved the training of deep neural networks (DNNs) with mini-batches, by\nnormalizing the distribution of the internal representation for each hidden\nlayer. However, the effectiveness of BN would diminish with scenario of\nmicro-batch (e.g., less than 10 samples in a mini-batch), since the estimated\nstatistics in a mini-batch are not reliable with insufficient samples. In this\npaper, we present a novel normalization method, called Batch Kalman\nNormalization (BKN), for improving and accelerating the training of DNNs,\nparticularly under the context of micro-batches. Specifically, unlike the\nexisting solutions treating each hidden layer as an isolated system, BKN treats\nall the layers in a network as a whole system, and estimates the statistics of\na certain layer by considering the distributions of all its preceding layers,\nmimicking the merits of Kalman Filtering. BKN has two appealing properties.\nFirst, it enables more stable training and faster convergence compared to\nprevious works. Second, training DNNs using BKN performs substantially better\nthan those using BN and its variants, especially when very small mini-batches\nare presented. On the image classification benchmark of ImageNet, using BKN\npowered networks we improve upon the best-published model-zoo results: reaching\n74.0% top-1 val accuracy for InceptionV2. More importantly, using BKN achieves\nthe comparable accuracy with extremely smaller batch size, such as 64 times\nsmaller on CIFAR-10/100 and 8 times smaller on ImageNet. \n\n"}
{"id": "1802.03689", "contents": "Title: Dual Control Memory Augmented Neural Networks for Treatment\n  Recommendations Abstract: Machine-assisted treatment recommendations hold a promise to reduce physician\ntime and decision errors. We formulate the task as a sequence-to-sequence\nprediction model that takes the entire time-ordered medical history as input,\nand predicts a sequence of future clinical procedures and medications. It is\nbuilt on the premise that an effective treatment plan may have long-term\ndependencies from previous medical history. We approach the problem by using a\nmemory-augmented neural network, in particular, by leveraging the recent\ndifferentiable neural computer that consists of a neural controller and an\nexternal memory module. But differing from the original model, we use dual\ncontrollers, one for encoding the history followed by another for decoding the\ntreatment sequences. In the encoding phase, the memory is updated as new input\nis read; at the end of this phase, the memory holds not only the medical\nhistory but also the information about the current illness. During the decoding\nphase, the memory is write-protected. The decoding controller generates a\ntreatment sequence, one treatment option at a time. The resulting dual\ncontroller write-protected memory-augmented neural network is demonstrated on\nthe MIMIC-III dataset on two tasks: procedure prediction and medication\nprescription. The results show improved performance over both traditional\nbag-of-words and sequence-to-sequence methods. \n\n"}
{"id": "1802.03753", "contents": "Title: Sample Efficient Deep Reinforcement Learning for Dialogue Systems with\n  Large Action Spaces Abstract: In spoken dialogue systems, we aim to deploy artificial intelligence to build\nautomated dialogue agents that can converse with humans. A part of this effort\nis the policy optimisation task, which attempts to find a policy describing how\nto respond to humans, in the form of a function taking the current state of the\ndialogue and returning the response of the system. In this paper, we\ninvestigate deep reinforcement learning approaches to solve this problem.\nParticular attention is given to actor-critic methods, off-policy reinforcement\nlearning with experience replay, and various methods aimed at reducing the bias\nand variance of estimators. When combined, these methods result in the\npreviously proposed ACER algorithm that gave competitive results in gaming\nenvironments. These environments however are fully observable and have a\nrelatively small action set so in this paper we examine the application of ACER\nto dialogue policy optimisation. We show that this method beats the current\nstate-of-the-art in deep learning approaches for spoken dialogue systems. This\nnot only leads to a more sample efficient algorithm that can train faster, but\nalso allows us to apply the algorithm in more difficult environments than\nbefore. We thus experiment with learning in a very large action space, which\nhas two orders of magnitude more actions than previously considered. We find\nthat ACER trains significantly faster than the current state-of-the-art. \n\n"}
{"id": "1802.03800", "contents": "Title: Drug response prediction by ensemble learning and drug-induced gene\n  expression signatures Abstract: Chemotherapeutic response of cancer cells to a given compound is one of the\nmost fundamental information one requires to design anti-cancer drugs. Recent\nadvances in producing large drug screens against cancer cell lines provided an\nopportunity to apply machine learning methods for this purpose. In addition to\ncytotoxicity databases, considerable amount of drug-induced gene expression\ndata has also become publicly available. Following this, several methods that\nexploit omics data were proposed to predict drug activity on cancer cells.\nHowever, due to the complexity of cancer drug mechanisms, none of the existing\nmethods are perfect. One possible direction, therefore, is to combine the\nstrengths of both the methods and the databases for improved performance. We\ndemonstrate that integrating a large number of predictions by the proposed\nmethod improves the performance for this task. The predictors in the ensemble\ndiffer in several aspects such as the method itself, the number of tasks method\nconsiders (multi-task vs. single-task) and the subset of data considered\n(sub-sampling). We show that all these different aspects contribute to the\nsuccess of the final ensemble. In addition, we attempt to use the drug screen\ndata together with two novel signatures produced from the drug-induced gene\nexpression profiles of cancer cell lines. Finally, we evaluate the method\npredictions by in vitro experiments in addition to the tests on data sets.The\npredictions of the methods, the signatures and the software are available from\n\\url{http://mtan.etu.edu.tr/drug-response-prediction/}. \n\n"}
{"id": "1802.03900", "contents": "Title: Q-learning with Nearest Neighbors Abstract: We consider model-free reinforcement learning for infinite-horizon discounted\nMarkov Decision Processes (MDPs) with a continuous state space and unknown\ntransition kernel, when only a single sample path under an arbitrary policy of\nthe system is available. We consider the Nearest Neighbor Q-Learning (NNQL)\nalgorithm to learn the optimal Q function using nearest neighbor regression\nmethod. As the main contribution, we provide tight finite sample analysis of\nthe convergence rate. In particular, for MDPs with a $d$-dimensional state\nspace and the discounted factor $\\gamma \\in (0,1)$, given an arbitrary sample\npath with \"covering time\" $ L $, we establish that the algorithm is guaranteed\nto output an $\\varepsilon$-accurate estimate of the optimal Q-function using\n$\\tilde{O}\\big(L/(\\varepsilon^3(1-\\gamma)^7)\\big)$ samples. For instance, for a\nwell-behaved MDP, the covering time of the sample path under the purely random\npolicy scales as $ \\tilde{O}\\big(1/\\varepsilon^d\\big),$ so the sample\ncomplexity scales as $\\tilde{O}\\big(1/\\varepsilon^{d+3}\\big).$ Indeed, we\nestablish a lower bound that argues that the dependence of $\n\\tilde{\\Omega}\\big(1/\\varepsilon^{d+2}\\big)$ is necessary. \n\n"}
{"id": "1802.04034", "contents": "Title: Lipschitz-Margin Training: Scalable Certification of Perturbation\n  Invariance for Deep Neural Networks Abstract: High sensitivity of neural networks against malicious perturbations on inputs\ncauses security concerns. To take a steady step towards robust classifiers, we\naim to create neural network models provably defended from perturbations. Prior\ncertification work requires strong assumptions on network structures and\nmassive computational costs, and thus the range of their applications was\nlimited. From the relationship between the Lipschitz constants and prediction\nmargins, we present a computationally efficient calculation technique to\nlower-bound the size of adversarial perturbations that can deceive networks,\nand that is widely applicable to various complicated networks. Moreover, we\npropose an efficient training procedure that robustifies networks and\nsignificantly improves the provably guarded areas around data points. In\nexperimental evaluations, our method showed its ability to provide a\nnon-trivial guarantee and enhance robustness for even large networks. \n\n"}
{"id": "1802.04346", "contents": "Title: Gaining Free or Low-Cost Transparency with Interpretable Partial\n  Substitute Abstract: This work addresses the situation where a black-box model with good\npredictive performance is chosen over its interpretable competitors, and we\nshow interpretability is still achievable in this case. Our solution is to find\nan interpretable substitute on a subset of data where the black-box model is\noverkill or nearly overkill while leaving the rest to the black-box. This\ntransparency is obtained at minimal cost or no cost of the predictive\nperformance. Under this framework, we develop a Hybrid Rule Sets (HyRS) model\nthat uses decision rules to capture the subspace of data where the rules are as\naccurate or almost as accurate as the black-box provided. To train a HyRS, we\ndevise an efficient search algorithm that iteratively finds the optimal model\nand exploits theoretically grounded strategies to reduce computation. Our\nframework is agnostic to the black-box during training. Experiments on\nstructured and text data show that HyRS obtains an effective trade-off between\ntransparency and interpretability. \n\n"}
{"id": "1802.04364", "contents": "Title: Junction Tree Variational Autoencoder for Molecular Graph Generation Abstract: We seek to automate the design of molecules based on specific chemical\nproperties. In computational terms, this task involves continuous embedding and\ngeneration of molecular graphs. Our primary contribution is the direct\nrealization of molecular graphs, a task previously approached by generating\nlinear SMILES strings instead of graphs. Our junction tree variational\nautoencoder generates molecular graphs in two phases, by first generating a\ntree-structured scaffold over chemical substructures, and then combining them\ninto a molecule with a graph message passing network. This approach allows us\nto incrementally expand molecules while maintaining chemical validity at every\nstep. We evaluate our model on multiple tasks ranging from molecular generation\nto optimization. Across these tasks, our model outperforms previous\nstate-of-the-art baselines by a significant margin. \n\n"}
{"id": "1802.04412", "contents": "Title: Efficient Exploration through Bayesian Deep Q-Networks Abstract: We study reinforcement learning (RL) in high dimensional episodic Markov\ndecision processes (MDP). We consider value-based RL when the optimal Q-value\nis a linear function of d-dimensional state-action feature representation. For\ninstance, in deep-Q networks (DQN), the Q-value is a linear function of the\nfeature representation layer (output layer). We propose two algorithms, one\nbased on optimism, LINUCB, and another based on posterior sampling, LINPSRL. We\nguarantee frequentist and Bayesian regret upper bounds of O(d sqrt{T}) for\nthese two algorithms, where T is the number of episodes. We extend these\nmethods to deep RL and propose Bayesian deep Q-networks (BDQN), which uses an\nefficient Thompson sampling algorithm for high dimensional RL. We deploy the\ndouble DQN (DDQN) approach, and instead of learning the last layer of Q-network\nusing linear regression, we use Bayesian linear regression, resulting in an\napproximated posterior over Q-function. This allows us to directly incorporate\nthe uncertainty over the Q-function and deploy Thompson sampling on the learned\nposterior distribution resulting in efficient exploration/exploitation\ntrade-off. We empirically study the behavior of BDQN on a wide range of Atari\ngames. Since BDQN carries out more efficient exploration and exploitation, it\nis able to reach higher return substantially faster compared to DDQN. \n\n"}
{"id": "1802.05380", "contents": "Title: Active Feature Acquisition with Supervised Matrix Completion Abstract: Feature missing is a serious problem in many applications, which may lead to\nlow quality of training data and further significantly degrade the learning\nperformance. While feature acquisition usually involves special devices or\ncomplex process, it is expensive to acquire all feature values for the whole\ndataset. On the other hand, features may be correlated with each other, and\nsome values may be recovered from the others. It is thus important to decide\nwhich features are most informative for recovering the other features as well\nas improving the learning performance. In this paper, we try to train an\neffective classification model with least acquisition cost by jointly\nperforming active feature querying and supervised matrix completion. When\ncompleting the feature matrix, a novel target function is proposed to\nsimultaneously minimize the reconstruction error on observed entries and the\nsupervised loss on training data. When querying the feature value, the most\nuncertain entry is actively selected based on the variance of previous\niterations. In addition, a bi-objective optimization method is presented for\ncost-aware active selection when features bear different acquisition costs. The\neffectiveness of the proposed approach is well validated by both theoretical\nanalysis and experimental study. \n\n"}
{"id": "1802.06383", "contents": "Title: Efficient Gaussian Process Classification Using Polya-Gamma Data\n  Augmentation Abstract: We propose a scalable stochastic variational approach to GP classification\nbuilding on Polya-Gamma data augmentation and inducing points. Unlike former\napproaches, we obtain closed-form updates based on natural gradients that lead\nto efficient optimization. We evaluate the algorithm on real-world datasets\ncontaining up to 11 million data points and demonstrate that it is up to two\norders of magnitude faster than the state-of-the-art while being competitive in\nterms of prediction performance. \n\n"}
{"id": "1802.06875", "contents": "Title: LSALSA: Accelerated Source Separation via Learned Sparse Coding Abstract: We propose an efficient algorithm for the generalized sparse coding (SC)\ninference problem. The proposed framework applies to both the single dictionary\nsetting, where each data point is represented as a sparse combination of the\ncolumns of one dictionary matrix, as well as the multiple dictionary setting as\ngiven in morphological component analysis (MCA), where the goal is to separate\na signal into additive parts such that each part has distinct sparse\nrepresentation within a corresponding dictionary. Both the SC task and its\ngeneralization via MCA have been cast as $\\ell_1$-regularized least-squares\noptimization problems. To accelerate traditional acquisition of sparse codes,\nwe propose a deep learning architecture that constitutes a trainable\ntime-unfolded version of the Split Augmented Lagrangian Shrinkage Algorithm\n(SALSA), a special case of the Alternating Direction Method of Multipliers\n(ADMM). We empirically validate both variants of the algorithm, that we refer\nto as LSALSA (learned-SALSA), on image vision tasks and demonstrate that at\ninference our networks achieve vast improvements in terms of the running time,\nthe quality of estimated sparse codes, and visual clarity on both classic SC\nand MCA problems. Finally, we present a theoretical framework for analyzing\nLSALSA network: we show that the proposed approach exactly implements a\ntruncated ADMM applied to a new, learned cost function with curvature modified\nby one of the learned parameterized matrices. We extend a very recent\nStochastic Alternating Optimization analysis framework to show that a gradient\ndescent step along this learned loss landscape is equivalent to a modified\ngradient descent step along the original loss landscape. In this framework, the\nacceleration achieved by LSALSA could potentially be explained by the network's\nability to learn a correction to the gradient direction of steeper descent. \n\n"}
{"id": "1802.08241", "contents": "Title: Hessian-based Analysis of Large Batch Training and Robustness to\n  Adversaries Abstract: Large batch size training of Neural Networks has been shown to incur accuracy\nloss when trained with the current methods. The exact underlying reasons for\nthis are still not completely understood. Here, we study large batch size\ntraining through the lens of the Hessian operator and robust optimization. In\nparticular, we perform a Hessian based study to analyze exactly how the\nlandscape of the loss function changes when training with large batch size. We\ncompute the true Hessian spectrum, without approximation, by back-propagating\nthe second derivative. Extensive experiments on multiple networks show that\nsaddle-points are not the cause for generalization gap of large batch size\ntraining, and the results consistently show that large batch converges to\npoints with noticeably higher Hessian spectrum. Furthermore, we show that\nrobust training allows one to favor flat areas, as points with large Hessian\nspectrum show poor robustness to adversarial perturbation. We further study\nthis relationship, and provide empirical and theoretical proof that the inner\nloop for robust training is a saddle-free optimization problem \\textit{almost\neverywhere}. We present detailed experiments with five different network\narchitectures, including a residual network, tested on MNIST, CIFAR-10, and\nCIFAR-100 datasets. We have open sourced our method which can be accessed at\n[1]. \n\n"}
{"id": "1802.08714", "contents": "Title: Deep Multi-View Spatial-Temporal Network for Taxi Demand Prediction Abstract: Taxi demand prediction is an important building block to enabling intelligent\ntransportation systems in a smart city. An accurate prediction model can help\nthe city pre-allocate resources to meet travel demand and to reduce empty taxis\non streets which waste energy and worsen the traffic congestion. With the\nincreasing popularity of taxi requesting services such as Uber and Didi Chuxing\n(in China), we are able to collect large-scale taxi demand data continuously.\nHow to utilize such big data to improve the demand prediction is an interesting\nand critical real-world problem. Traditional demand prediction methods mostly\nrely on time series forecasting techniques, which fail to model the complex\nnon-linear spatial and temporal relations. Recent advances in deep learning\nhave shown superior performance on traditionally challenging tasks such as\nimage classification by learning the complex features and correlations from\nlarge-scale data. This breakthrough has inspired researchers to explore deep\nlearning techniques on traffic prediction problems. However, existing methods\non traffic prediction have only considered spatial relation (e.g., using CNN)\nor temporal relation (e.g., using LSTM) independently. We propose a Deep\nMulti-View Spatial-Temporal Network (DMVST-Net) framework to model both spatial\nand temporal relations. Specifically, our proposed model consists of three\nviews: temporal view (modeling correlations between future demand values with\nnear time points via LSTM), spatial view (modeling local spatial correlation\nvia local CNN), and semantic view (modeling correlations among regions sharing\nsimilar temporal patterns). Experiments on large-scale real taxi demand data\ndemonstrate effectiveness of our approach over state-of-the-art methods. \n\n"}
{"id": "1802.09129", "contents": "Title: Multi-Evidence Filtering and Fusion for Multi-Label Classification,\n  Object Detection and Semantic Segmentation Based on Weakly Supervised\n  Learning Abstract: Supervised object detection and semantic segmentation require object or even\npixel level annotations. When there exist image level labels only, it is\nchallenging for weakly supervised algorithms to achieve accurate predictions.\nThe accuracy achieved by top weakly supervised algorithms is still\nsignificantly lower than their fully supervised counterparts. In this paper, we\npropose a novel weakly supervised curriculum learning pipeline for multi-label\nobject recognition, detection and semantic segmentation. In this pipeline, we\nfirst obtain intermediate object localization and pixel labeling results for\nthe training images, and then use such results to train task-specific deep\nnetworks in a fully supervised manner. The entire process consists of four\nstages, including object localization in the training images, filtering and\nfusing object instances, pixel labeling for the training images, and\ntask-specific network training. To obtain clean object instances in the\ntraining images, we propose a novel algorithm for filtering, fusing and\nclassifying object instances collected from multiple solution mechanisms. In\nthis algorithm, we incorporate both metric learning and density-based\nclustering to filter detected object instances. Experiments show that our\nweakly supervised pipeline achieves state-of-the-art results in multi-label\nimage classification as well as weakly supervised object detection and very\ncompetitive results in weakly supervised semantic segmentation on MS-COCO,\nPASCAL VOC 2007 and PASCAL VOC 2012. \n\n"}
{"id": "1802.09707", "contents": "Title: Understanding and Enhancing the Transferability of Adversarial Examples Abstract: State-of-the-art deep neural networks are known to be vulnerable to\nadversarial examples, formed by applying small but malicious perturbations to\nthe original inputs. Moreover, the perturbations can \\textit{transfer across\nmodels}: adversarial examples generated for a specific model will often mislead\nother unseen models. Consequently the adversary can leverage it to attack\ndeployed systems without any query, which severely hinder the application of\ndeep learning, especially in the areas where security is crucial. In this work,\nwe systematically study how two classes of factors that might influence the\ntransferability of adversarial examples. One is about model-specific factors,\nincluding network architecture, model capacity and test accuracy. The other is\nthe local smoothness of loss function for constructing adversarial examples.\nBased on these understanding, a simple but effective strategy is proposed to\nenhance transferability. We call it variance-reduced attack, since it utilizes\nthe variance-reduced gradient to generate adversarial example. The\neffectiveness is confirmed by a variety of experiments on both CIFAR-10 and\nImageNet datasets. \n\n"}
{"id": "1802.09732", "contents": "Title: Online learning with kernel losses Abstract: We present a generalization of the adversarial linear bandits framework,\nwhere the underlying losses are kernel functions (with an associated\nreproducing kernel Hilbert space) rather than linear functions. We study a\nversion of the exponential weights algorithm and bound its regret in this\nsetting. Under conditions on the eigendecay of the kernel we provide a sharp\ncharacterization of the regret for this algorithm. When we have polynomial\neigendecay $\\mu_j \\le \\mathcal{O}(j^{-\\beta})$, we find that the regret is\nbounded by $\\mathcal{R}_n \\le \\mathcal{O}(n^{\\beta/(2(\\beta-1))})$; while under\nthe assumption of exponential eigendecay $\\mu_j \\le \\mathcal{O}(e^{-\\beta j\n})$, we get an even tighter bound on the regret $\\mathcal{R}_n \\le\n\\mathcal{O}(n^{1/2}\\log(n)^{1/2})$. We also study the full information setting\nwhen the underlying losses are kernel functions and present an adapted\nexponential weights algorithm and a conditional gradient descent algorithm. \n\n"}
{"id": "1802.09914", "contents": "Title: High-Dimensional Vector Semantics Abstract: In this paper we explore the \"vector semantics\" problem from the perspective\nof \"almost orthogonal\" property of high-dimensional random vectors. We show\nthat this intriguing property can be used to \"memorize\" random vectors by\nsimply adding them, and we provide an efficient probabilistic solution to the\nset membership problem. Also, we discuss several applications to word context\nvector embeddings, document sentences similarity, and spam filtering. \n\n"}
{"id": "1802.10116", "contents": "Title: Generalized Byzantine-tolerant SGD Abstract: We propose three new robust aggregation rules for distributed synchronous\nStochastic Gradient Descent~(SGD) under a general Byzantine failure model. The\nattackers can arbitrarily manipulate the data transferred between the servers\nand the workers in the parameter server~(PS) architecture. We prove the\nByzantine resilience properties of these aggregation rules. Empirical analysis\nshows that the proposed techniques outperform current approaches for realistic\nuse cases and Byzantine attack scenarios. \n\n"}
{"id": "1802.10558", "contents": "Title: Exactly Robust Kernel Principal Component Analysis Abstract: Robust principal component analysis (RPCA) can recover low-rank matrices when\nthey are corrupted by sparse noises. In practice, many matrices are, however,\nof high-rank and hence cannot be recovered by RPCA. We propose a novel method\ncalled robust kernel principal component analysis (RKPCA) to decompose a\npartially corrupted matrix as a sparse matrix plus a high or full-rank matrix\nwith low latent dimensionality. RKPCA can be applied to many problems such as\nnoise removal and subspace clustering and is still the only unsupervised\nnonlinear method robust to sparse noises. Our theoretical analysis shows that,\nwith high probability, RKPCA can provide high recovery accuracy. The\noptimization of RKPCA involves nonconvex and indifferentiable problems. We\npropose two nonconvex optimization algorithms for RKPCA. They are alternating\ndirection method of multipliers with backtracking line search and proximal\nlinearized minimization with adaptive step size. Comparative studies in noise\nremoval and robust subspace clustering corroborate the effectiveness and\nsuperiority of RKPCA. \n\n"}
{"id": "1802.10582", "contents": "Title: Evaluating Overfit and Underfit in Models of Network Community Structure Abstract: A common data mining task on networks is community detection, which seeks an\nunsupervised decomposition of a network into structural groups based on\nstatistical regularities in the network's connectivity. Although many methods\nexist, the No Free Lunch theorem for community detection implies that each\nmakes some kind of tradeoff, and no algorithm can be optimal on all inputs.\nThus, different algorithms will over or underfit on different inputs, finding\nmore, fewer, or just different communities than is optimal, and evaluation\nmethods that use a metadata partition as a ground truth will produce misleading\nconclusions about general accuracy. Here, we present a broad evaluation of over\nand underfitting in community detection, comparing the behavior of 16\nstate-of-the-art community detection algorithms on a novel and structurally\ndiverse corpus of 406 real-world networks. We find that (i) algorithms vary\nwidely both in the number of communities they find and in their corresponding\ncomposition, given the same input, (ii) algorithms can be clustered into\ndistinct high-level groups based on similarities of their outputs on real-world\nnetworks, and (iii) these differences induce wide variation in accuracy on link\nprediction and link description tasks. We introduce a new diagnostic for\nevaluating overfitting and underfitting in practice, and use it to roughly\ndivide community detection methods into general and specialized learning\nalgorithms. Across methods and inputs, Bayesian techniques based on the\nstochastic block model and a minimum description length approach to\nregularization represent the best general learning approach, but can be\noutperformed under specific circumstances. These results introduce both a\ntheoretically principled approach to evaluate over and underfitting in models\nof network community structure and a realistic benchmark by which new methods\nmay be evaluated and compared. \n\n"}
{"id": "1803.00156", "contents": "Title: Autoencoding topology Abstract: The problem of learning a manifold structure on a dataset is framed in terms\nof a generative model, to which we use ideas behind autoencoders (namely\nadversarial/Wasserstein autoencoders) to fit deep neural networks. From a\nmachine learning perspective, the resulting structure, an atlas of a manifold,\nmay be viewed as a combination of dimensionality reduction and \"fuzzy\"\nclustering. \n\n"}
{"id": "1803.01229", "contents": "Title: GAN-based Synthetic Medical Image Augmentation for increased CNN\n  Performance in Liver Lesion Classification Abstract: Deep learning methods, and in particular convolutional neural networks\n(CNNs), have led to an enormous breakthrough in a wide range of computer vision\ntasks, primarily by using large-scale annotated datasets. However, obtaining\nsuch datasets in the medical domain remains a challenge. In this paper, we\npresent methods for generating synthetic medical images using recently\npresented deep learning Generative Adversarial Networks (GANs). Furthermore, we\nshow that generated medical images can be used for synthetic data augmentation,\nand improve the performance of CNN for medical image classification. Our novel\nmethod is demonstrated on a limited dataset of computed tomography (CT) images\nof 182 liver lesions (53 cysts, 64 metastases and 65 hemangiomas). We first\nexploit GAN architectures for synthesizing high quality liver lesion ROIs. Then\nwe present a novel scheme for liver lesion classification using CNN. Finally,\nwe train the CNN using classic data augmentation and our synthetic data\naugmentation and compare performance. In addition, we explore the quality of\nour synthesized examples using visualization and expert assessment. The\nclassification performance using only classic data augmentation yielded 78.6%\nsensitivity and 88.4% specificity. By adding the synthetic data augmentation\nthe results increased to 85.7% sensitivity and 92.4% specificity. We believe\nthat this approach to synthetic data augmentation can generalize to other\nmedical classification applications and thus support radiologists' efforts to\nimprove diagnosis. \n\n"}
{"id": "1803.01422", "contents": "Title: DAGs with NO TEARS: Continuous Optimization for Structure Learning Abstract: Estimating the structure of directed acyclic graphs (DAGs, also known as\nBayesian networks) is a challenging problem since the search space of DAGs is\ncombinatorial and scales superexponentially with the number of nodes. Existing\napproaches rely on various local heuristics for enforcing the acyclicity\nconstraint. In this paper, we introduce a fundamentally different strategy: We\nformulate the structure learning problem as a purely \\emph{continuous}\noptimization problem over real matrices that avoids this combinatorial\nconstraint entirely. This is achieved by a novel characterization of acyclicity\nthat is not only smooth but also exact. The resulting problem can be\nefficiently solved by standard numerical algorithms, which also makes\nimplementation effortless. The proposed method outperforms existing ones,\nwithout imposing any structural assumptions on the graph such as bounded\ntreewidth or in-degree. Code implementing the proposed algorithm is open-source\nand publicly available at https://github.com/xunzheng/notears. \n\n"}
{"id": "1803.01442", "contents": "Title: Stochastic Activation Pruning for Robust Adversarial Defense Abstract: Neural networks are known to be vulnerable to adversarial examples. Carefully\nchosen perturbations to real images, while imperceptible to humans, induce\nmisclassification and threaten the reliability of deep learning systems in the\nwild. To guard against adversarial examples, we take inspiration from game\ntheory and cast the problem as a minimax zero-sum game between the adversary\nand the model. In general, for such games, the optimal strategy for both\nplayers requires a stochastic policy, also known as a mixed strategy. In this\nlight, we propose Stochastic Activation Pruning (SAP), a mixed strategy for\nadversarial defense. SAP prunes a random subset of activations (preferentially\npruning those with smaller magnitude) and scales up the survivors to\ncompensate. We can apply SAP to pretrained networks, including adversarially\ntrained models, without fine-tuning, providing robustness against adversarial\nexamples. Experiments demonstrate that SAP confers robustness against attacks,\nincreasing accuracy and preserving calibration. \n\n"}
{"id": "1803.02043", "contents": "Title: Online Deep Learning: Growing RBM on the fly Abstract: We propose a novel online learning algorithm for Restricted Boltzmann\nMachines (RBM), namely, the Online Generative Discriminative Restricted\nBoltzmann Machine (OGD-RBM), that provides the ability to build and adapt the\nnetwork architecture of RBM according to the statistics of streaming data. The\nOGD-RBM is trained in two phases: (1) an online generative phase for\nunsupervised feature representation at the hidden layer and (2) a\ndiscriminative phase for classification. The online generative training begins\nwith zero neurons in the hidden layer, adds and updates the neurons to adapt to\nstatistics of streaming data in a single pass unsupervised manner, resulting in\na feature representation best suited to the data. The discriminative phase is\nbased on stochastic gradient descent and associates the represented features to\nthe class labels. We demonstrate the OGD-RBM on a set of multi-category and\nbinary classification problems for data sets having varying degrees of\nclass-imbalance. We first apply the OGD-RBM algorithm on the multi-class MNIST\ndataset to characterize the network evolution. We demonstrate that the online\ngenerative phase converges to a stable, concise network architecture, wherein\nindividual neurons are inherently discriminative to the class labels despite\nunsupervised training. We then benchmark OGD-RBM performance to other machine\nlearning, neural network and ClassRBM techniques for credit scoring\napplications using 3 public non-stationary two-class credit datasets with\nvarying degrees of class-imbalance. We report that OGD-RBM improves accuracy by\n2.5-3% over batch learning techniques while requiring at least 24%-70% fewer\nneurons and fewer training samples. This online generative training approach\ncan be extended greedily to multiple layers for training Deep Belief Networks\nin non-stationary data mining applications without the need for a priori fixed\narchitectures. \n\n"}
{"id": "1803.02108", "contents": "Title: HexaConv Abstract: The effectiveness of Convolutional Neural Networks stems in large part from\ntheir ability to exploit the translation invariance that is inherent in many\nlearning problems. Recently, it was shown that CNNs can exploit other\ninvariances, such as rotation invariance, by using group convolutions instead\nof planar convolutions. However, for reasons of performance and ease of\nimplementation, it has been necessary to limit the group convolution to\ntransformations that can be applied to the filters without interpolation. Thus,\nfor images with square pixels, only integer translations, rotations by\nmultiples of 90 degrees, and reflections are admissible.\n  Whereas the square tiling provides a 4-fold rotational symmetry, a hexagonal\ntiling of the plane has a 6-fold rotational symmetry. In this paper we show how\none can efficiently implement planar convolution and group convolution over\nhexagonal lattices, by re-using existing highly optimized convolution routines.\nWe find that, due to the reduced anisotropy of hexagonal filters, planar\nHexaConv provides better accuracy than planar convolution with square filters,\ngiven a fixed parameter budget. Furthermore, we find that the increased degree\nof symmetry of the hexagonal grid increases the effectiveness of group\nconvolutions, by allowing for more parameter sharing. We show that our method\nsignificantly outperforms conventional CNNs on the AID aerial scene\nclassification dataset, even outperforming ImageNet pre-trained models. \n\n"}
{"id": "1803.04062", "contents": "Title: Pseudo-task Augmentation: From Deep Multitask Learning to Intratask\n  Sharing---and Back Abstract: Deep multitask learning boosts performance by sharing learned structure\nacross related tasks. This paper adapts ideas from deep multitask learning to\nthe setting where only a single task is available. The method is formalized as\npseudo-task augmentation, in which models are trained with multiple decoders\nfor each task. Pseudo-tasks simulate the effect of training towards\nclosely-related tasks drawn from the same universe. In a suite of experiments,\npseudo-task augmentation is shown to improve performance on single-task\nlearning problems. When combined with multitask learning, further improvements\nare achieved, including state-of-the-art performance on the CelebA dataset,\nshowing that pseudo-task augmentation and multitask learning have complementary\nvalue. All in all, pseudo-task augmentation is a broadly applicable and\nefficient way to boost performance in deep learning systems. \n\n"}
{"id": "1803.04189", "contents": "Title: Noise2Noise: Learning Image Restoration without Clean Data Abstract: We apply basic statistical reasoning to signal reconstruction by machine\nlearning -- learning to map corrupted observations to clean signals -- with a\nsimple and powerful conclusion: it is possible to learn to restore images by\nonly looking at corrupted examples, at performance at and sometimes exceeding\ntraining using clean data, without explicit image priors or likelihood models\nof the corruption. In practice, we show that a single model learns photographic\nnoise removal, denoising synthetic Monte Carlo images, and reconstruction of\nundersampled MRI scans -- all corrupted by different processes -- based on\nnoisy data only. \n\n"}
{"id": "1803.04209", "contents": "Title: High Throughput Synchronous Distributed Stochastic Gradient Descent Abstract: We introduce a new, high-throughput, synchronous, distributed, data-parallel,\nstochastic-gradient-descent learning algorithm. This algorithm uses amortized\ninference in a compute-cluster-specific, deep, generative, dynamical model to\nperform joint posterior predictive inference of the mini-batch gradient\ncomputation times of all worker-nodes in a parallel computing cluster. We show\nthat a synchronous parameter server can, by utilizing such a model, choose an\noptimal cutoff time beyond which mini-batch gradient messages from slow workers\nare ignored that maximizes overall mini-batch gradient computations per second.\nIn keeping with earlier findings we observe that, under realistic conditions,\neagerly discarding the mini-batch gradient computations of stragglers not only\nincreases throughput but actually increases the overall rate of convergence as\na function of wall-clock time by virtue of eliminating idleness. The principal\nnovel contribution and finding of this work goes beyond this by demonstrating\nthat using the predicted run-times from a generative model of cluster worker\nperformance to dynamically adjust the cutoff improves substantially over the\nstatic-cutoff prior art, leading to, among other things, significantly reduced\ndeep neural net training times on large computer clusters. \n\n"}
{"id": "1803.04765", "contents": "Title: Deep k-Nearest Neighbors: Towards Confident, Interpretable and Robust\n  Deep Learning Abstract: Deep neural networks (DNNs) enable innovative applications of machine\nlearning like image recognition, machine translation, or malware detection.\nHowever, deep learning is often criticized for its lack of robustness in\nadversarial settings (e.g., vulnerability to adversarial inputs) and general\ninability to rationalize its predictions. In this work, we exploit the\nstructure of deep learning to enable new learning-based inference and decision\nstrategies that achieve desirable properties such as robustness and\ninterpretability. We take a first step in this direction and introduce the Deep\nk-Nearest Neighbors (DkNN). This hybrid classifier combines the k-nearest\nneighbors algorithm with representations of the data learned by each layer of\nthe DNN: a test input is compared to its neighboring training points according\nto the distance that separates them in the representations. We show the labels\nof these neighboring points afford confidence estimates for inputs outside the\nmodel's training manifold, including on malicious inputs like adversarial\nexamples--and therein provides protections against inputs that are outside the\nmodels understanding. This is because the nearest neighbors can be used to\nestimate the nonconformity of, i.e., the lack of support for, a prediction in\nthe training data. The neighbors also constitute human-interpretable\nexplanations of predictions. We evaluate the DkNN algorithm on several\ndatasets, and show the confidence estimates accurately identify inputs outside\nthe model, and that the explanations provided by nearest neighbors are\nintuitive and useful in understanding model failures. \n\n"}
{"id": "1803.04837", "contents": "Title: Learning the Joint Representation of Heterogeneous Temporal Events for\n  Clinical Endpoint Prediction Abstract: The availability of a large amount of electronic health records (EHR)\nprovides huge opportunities to improve health care service by mining these\ndata. One important application is clinical endpoint prediction, which aims to\npredict whether a disease, a symptom or an abnormal lab test will happen in the\nfuture according to patients' history records. This paper develops deep\nlearning techniques for clinical endpoint prediction, which are effective in\nmany practical applications. However, the problem is very challenging since\npatients' history records contain multiple heterogeneous temporal events such\nas lab tests, diagnosis, and drug administrations. The visiting patterns of\ndifferent types of events vary significantly, and there exist complex nonlinear\nrelationships between different events. In this paper, we propose a novel model\nfor learning the joint representation of heterogeneous temporal events. The\nmodel adds a new gate to control the visiting rates of different events which\neffectively models the irregular patterns of different events and their\nnonlinear correlations. Experiment results with real-world clinical data on the\ntasks of predicting death and abnormal lab tests prove the effectiveness of our\nproposed approach over competitive baselines. \n\n"}
{"id": "1803.05070", "contents": "Title: A Multi-Modal Approach to Infer Image Affect Abstract: The group affect or emotion in an image of people can be inferred by\nextracting features about both the people in the picture and the overall makeup\nof the scene. The state-of-the-art on this problem investigates a combination\nof facial features, scene extraction and even audio tonality. This paper\ncombines three additional modalities, namely, human pose, text-based tagging\nand CNN extracted features / predictions. To the best of our knowledge, this is\nthe first time all of the modalities were extracted using deep neural networks.\nWe evaluate the performance of our approach against baselines and identify\ninsights throughout this paper. \n\n"}
{"id": "1803.05397", "contents": "Title: Redundancy Techniques for Straggler Mitigation in Distributed\n  Optimization and Learning Abstract: Performance of distributed optimization and learning systems is bottlenecked\nby \"straggler\" nodes and slow communication links, which significantly delay\ncomputation. We propose a distributed optimization framework where the dataset\nis \"encoded\" to have an over-complete representation with built-in redundancy,\nand the straggling nodes in the system are dynamically left out of the\ncomputation at every iteration, whose loss is compensated by the embedded\nredundancy. We show that oblivious application of several popular optimization\nalgorithms on encoded data, including gradient descent, L-BFGS, proximal\ngradient under data parallelism, and coordinate descent under model\nparallelism, converge to either approximate or exact solutions of the original\nproblem when stragglers are treated as erasures. These convergence results are\ndeterministic, i.e., they establish sample path convergence for arbitrary\nsequences of delay patterns or distributions on the nodes, and are independent\nof the tail behavior of the delay distribution. We demonstrate that equiangular\ntight frames have desirable properties as encoding matrices, and propose\nefficient mechanisms for encoding large-scale data. We implement the proposed\ntechnique on Amazon EC2 clusters, and demonstrate its performance over several\nlearning problems, including matrix factorization, LASSO, ridge regression and\nlogistic regression, and compare the proposed method with uncoded,\nasynchronous, and data replication strategies. \n\n"}
{"id": "1803.06024", "contents": "Title: Deep Learning Reconstruction of Ultra-Short Pulses Abstract: Ultra-short laser pulses with femtosecond to attosecond pulse duration are\nthe shortest systematic events humans can create. Characterization (amplitude\nand phase) of these pulses is a key ingredient in ultrafast science, e.g.,\nexploring chemical reactions and electronic phase transitions. Here, we propose\nand demonstrate, numerically and experimentally, the first deep neural network\ntechnique to reconstruct ultra-short optical pulses. We anticipate that this\napproach will extend the range of ultrashort laser pulses that can be\ncharacterized, e.g., enabling to diagnose very weak attosecond pulses. \n\n"}
{"id": "1803.06727", "contents": "Title: Aggregating Strategies for Long-term Forecasting Abstract: The article is devoted to investigating the application of aggregating\nalgorithms to the problem of the long-term forecasting. We examine the classic\naggregating algorithms based on the exponential reweighing. For the general\nVovk's aggregating algorithm we provide its generalization for the long-term\nforecasting. For the special basic case of Vovk's algorithm we provide its two\nmodifications for the long-term forecasting. The first one is theoretically\nclose to an optimal algorithm and is based on replication of independent\ncopies. It provides the time-independent regret bound with respect to the best\nexpert in the pool. The second one is not optimal but is more practical and has\n$O(\\sqrt{T})$ regret bound, where $T$ is the length of the game. \n\n"}
{"id": "1803.06969", "contents": "Title: Comparing Dynamics: Deep Neural Networks versus Glassy Systems Abstract: We analyze numerically the training dynamics of deep neural networks (DNN) by\nusing methods developed in statistical physics of glassy systems. The two main\nissues we address are (1) the complexity of the loss landscape and of the\ndynamics within it, and (2) to what extent DNNs share similarities with glassy\nsystems. Our findings, obtained for different architectures and datasets,\nsuggest that during the training process the dynamics slows down because of an\nincreasingly large number of flat directions. At large times, when the loss is\napproaching zero, the system diffuses at the bottom of the landscape. Despite\nsome similarities with the dynamics of mean-field glassy systems, in\nparticular, the absence of barrier crossing, we find distinctive dynamical\nbehaviors in the two cases, showing that the statistical properties of the\ncorresponding loss and energy landscapes are different. In contrast, when the\nnetwork is under-parametrized we observe a typical glassy behavior, thus\nsuggesting the existence of different phases depending on whether the network\nis under-parametrized or over-parametrized. \n\n"}
{"id": "1803.06978", "contents": "Title: Improving Transferability of Adversarial Examples with Input Diversity Abstract: Though CNNs have achieved the state-of-the-art performance on various vision\ntasks, they are vulnerable to adversarial examples --- crafted by adding\nhuman-imperceptible perturbations to clean images. However, most of the\nexisting adversarial attacks only achieve relatively low success rates under\nthe challenging black-box setting, where the attackers have no knowledge of the\nmodel structure and parameters. To this end, we propose to improve the\ntransferability of adversarial examples by creating diverse input patterns.\nInstead of only using the original images to generate adversarial examples, our\nmethod applies random transformations to the input images at each iteration.\nExtensive experiments on ImageNet show that the proposed attack method can\ngenerate adversarial examples that transfer much better to different networks\nthan existing baselines. By evaluating our method against top defense solutions\nand official baselines from NIPS 2017 adversarial competition, the enhanced\nattack reaches an average success rate of 73.0%, which outperforms the top-1\nattack submission in the NIPS competition by a large margin of 6.6%. We hope\nthat our proposed attack strategy can serve as a strong benchmark baseline for\nevaluating the robustness of networks to adversaries and the effectiveness of\ndifferent defense methods in the future. Code is available at\nhttps://github.com/cihangxie/DI-2-FGSM. \n\n"}
{"id": "1803.07068", "contents": "Title: D$^2$: Decentralized Training over Decentralized Data Abstract: While training a machine learning model using multiple workers, each of which\ncollects data from their own data sources, it would be most useful when the\ndata collected from different workers can be {\\em unique} and {\\em different}.\nIronically, recent analysis of decentralized parallel stochastic gradient\ndescent (D-PSGD) relies on the assumption that the data hosted on different\nworkers are {\\em not too different}. In this paper, we ask the question: {\\em\nCan we design a decentralized parallel stochastic gradient descent algorithm\nthat is less sensitive to the data variance across workers?} In this paper, we\npresent D$^2$, a novel decentralized parallel stochastic gradient descent\nalgorithm designed for large data variance \\xr{among workers} (imprecisely,\n\"decentralized\" data). The core of D$^2$ is a variance blackuction extension of\nthe standard D-PSGD algorithm, which improves the convergence rate from\n$O\\left({\\sigma \\over \\sqrt{nT}} + {(n\\zeta^2)^{\\frac{1}{3}} \\over\nT^{2/3}}\\right)$ to $O\\left({\\sigma \\over \\sqrt{nT}}\\right)$ where $\\zeta^{2}$\ndenotes the variance among data on different workers. As a result, D$^2$ is\nrobust to data variance among workers. We empirically evaluated D$^2$ on image\nclassification tasks where each worker has access to only the data of a limited\nset of labels, and find that D$^2$ significantly outperforms D-PSGD. \n\n"}
{"id": "1803.07192", "contents": "Title: Diagnostic Classification Of Lung Nodules Using 3D Neural Networks Abstract: Lung cancer is the leading cause of cancer-related death worldwide. Early\ndiagnosis of pulmonary nodules in Computed Tomography (CT) chest scans provides\nan opportunity for designing effective treatment and making financial and care\nplans. In this paper, we consider the problem of diagnostic classification\nbetween benign and malignant lung nodules in CT images, which aims to learn a\ndirect mapping from 3D images to class labels. To achieve this goal, four\ntwo-pathway Convolutional Neural Networks (CNN) are proposed, including a basic\n3D CNN, a novel multi-output network, a 3D DenseNet, and an augmented 3D\nDenseNet with multi-outputs. These four networks are evaluated on the public\nLIDC-IDRI dataset and outperform most existing methods. In particular, the 3D\nmulti-output DenseNet (MoDenseNet) achieves the state-of-the-art classification\naccuracy on the task of end-to-end lung nodule diagnosis. In addition, the\nnetworks pretrained on the LIDC-IDRI dataset can be further extended to handle\nsmaller datasets using transfer learning. This is demonstrated on our dataset\nwith encouraging prediction accuracy in lung nodule classification. \n\n"}
{"id": "1803.07445", "contents": "Title: MLtuner: System Support for Automatic Machine Learning Tuning Abstract: MLtuner automatically tunes settings for training tunables (such as the\nlearning rate, the momentum, the mini-batch size, and the data staleness bound)\nthat have a significant impact on large-scale machine learning (ML)\nperformance. Traditionally, these tunables are set manually, which is\nunsurprisingly error-prone and difficult to do without extensive domain\nknowledge. MLtuner uses efficient snapshotting, branching, and\noptimization-guided online trial-and-error to find good initial settings as\nwell as to re-tune settings during execution. Experiments show that MLtuner can\nrobustly find and re-tune tunable settings for a variety of ML applications,\nincluding image classification (for 3 models and 2 datasets), video\nclassification, and matrix factorization. Compared to state-of-the-art ML\nauto-tuning approaches, MLtuner is more robust for large problems and over an\norder of magnitude faster. \n\n"}
{"id": "1803.07658", "contents": "Title: Graph-based regularization for regression problems with alignment and\n  highly-correlated designs Abstract: Sparse models for high-dimensional linear regression and machine learning\nhave received substantial attention over the past two decades. Model selection,\nor determining which features or covariates are the best explanatory variables,\nis critical to the interpretability of a learned model. Much of the current\nliterature assumes that covariates are only mildly correlated. However, in many\nmodern applications covariates are highly correlated and do not exhibit key\nproperties (such as the restricted eigenvalue condition, restricted isometry\nproperty, or other related assumptions). This work considers a high-dimensional\nregression setting in which a graph governs both correlations among the\ncovariates and the similarity among regression coefficients -- meaning there is\n\\emph{alignment} between the covariates and regression coefficients. Using side\ninformation about the strength of correlations among features, we form a graph\nwith edge weights corresponding to pairwise covariances. This graph is used to\ndefine a graph total variation regularizer that promotes similar weights for\ncorrelated features.\n  This work shows how the proposed graph-based regularization yields\nmean-squared error guarantees for a broad range of covariance graph structures.\nThese guarantees are optimal for many specific covariance graphs, including\nblock and lattice graphs. Our proposed approach outperforms other methods for\nhighly-correlated design in a variety of experiments on synthetic data and real\nbiochemistry data. \n\n"}
{"id": "1803.07821", "contents": "Title: Multi-view Metric Learning in Vector-valued Kernel Spaces Abstract: We consider the problem of metric learning for multi-view data and present a\nnovel method for learning within-view as well as between-view metrics in\nvector-valued kernel spaces, as a way to capture multi-modal structure of the\ndata. We formulate two convex optimization problems to jointly learn the metric\nand the classifier or regressor in kernel feature spaces. An iterative\nthree-step multi-view metric learning algorithm is derived from the\noptimization problems. In order to scale the computation to large training\nsets, a block-wise Nystr{\\\"o}m approximation of the multi-view kernel matrix is\nintroduced. We justify our approach theoretically and experimentally, and show\nits performance on real-world datasets against relevant state-of-the-art\nmethods. \n\n"}
{"id": "1803.08367", "contents": "Title: Gradient Descent Quantizes ReLU Network Features Abstract: Deep neural networks are often trained in the over-parametrized regime (i.e.\nwith far more parameters than training examples), and understanding why the\ntraining converges to solutions that generalize remains an open problem.\nSeveral studies have highlighted the fact that the training procedure, i.e.\nmini-batch Stochastic Gradient Descent (SGD) leads to solutions that have\nspecific properties in the loss landscape. However, even with plain Gradient\nDescent (GD) the solutions found in the over-parametrized regime are pretty\ngood and this phenomenon is poorly understood.\n  We propose an analysis of this behavior for feedforward networks with a ReLU\nactivation function under the assumption of small initialization and learning\nrate and uncover a quantization effect: The weight vectors tend to concentrate\nat a small number of directions determined by the input data. As a consequence,\nwe show that for given input data there are only finitely many, \"simple\"\nfunctions that can be obtained, independent of the network size. This puts\nthese functions in analogy to linear interpolations (for given input data there\nare finitely many triangulations, which each determine a function by linear\ninterpolation). We ask whether this analogy extends to the generalization\nproperties - while the usual distribution-independent generalization property\ndoes not hold, it could be that for e.g. smooth functions with bounded second\nderivative an approximation property holds which could \"explain\" generalization\nof networks (of unbounded size) to unseen inputs. \n\n"}
{"id": "1803.08416", "contents": "Title: Demystifying Deep Learning: A Geometric Approach to Iterative\n  Projections Abstract: Parametric approaches to Learning, such as deep learning (DL), are highly\npopular in nonlinear regression, in spite of their extremely difficult training\nwith their increasing complexity (e.g. number of layers in DL). In this paper,\nwe present an alternative semi-parametric framework which foregoes the\nordinarily required feedback, by introducing the novel idea of geometric\nregularization. We show that certain deep learning techniques such as residual\nnetwork (ResNet) architecture are closely related to our approach. Hence, our\ntechnique can be used to analyze these types of deep learning. Moreover, we\npresent preliminary results which confirm that our approach can be easily\ntrained to obtain complex structures. \n\n"}
{"id": "1803.08591", "contents": "Title: End-to-End Learning for the Deep Multivariate Probit Model Abstract: The multivariate probit model (MVP) is a popular classic model for studying\nbinary responses of multiple entities. Nevertheless, the computational\nchallenge of learning the MVP model, given that its likelihood involves\nintegrating over a multidimensional constrained space of latent variables,\nsignificantly limits its application in practice. We propose a flexible deep\ngeneralization of the classic MVP, the Deep Multivariate Probit Model (DMVP),\nwhich is an end-to-end learning scheme that uses an efficient parallel sampling\nprocess of the multivariate probit model to exploit GPU-boosted deep neural\nnetworks. We present both theoretical and empirical analysis of the convergence\nbehavior of DMVP's sampling process with respect to the resolution of the\ncorrelation structure. We provide convergence guarantees for DMVP and our\nempirical analysis demonstrates the advantages of DMVP's sampling compared with\nstandard MCMC-based methods. We also show that when applied to multi-entity\nmodelling problems, which are natural DMVP applications, DMVP trains faster\nthan classical MVP, by at least an order of magnitude, captures rich\ncorrelations among entities, and further improves the joint likelihood of\nentities compared with several competitive models. \n\n"}
{"id": "1803.08661", "contents": "Title: Bayesian Optimization with Expensive Integrands Abstract: We propose a Bayesian optimization algorithm for objective functions that are\nsums or integrals of expensive-to-evaluate functions, allowing noisy\nevaluations. These objective functions arise in multi-task Bayesian\noptimization for tuning machine learning hyperparameters, optimization via\nsimulation, and sequential design of experiments with random environmental\nconditions. Our method is average-case optimal by construction when a single\nevaluation of the integrand remains within our evaluation budget. Achieving\nthis one-step optimality requires solving a challenging value of information\noptimization problem, for which we provide a novel efficient\ndiscretization-free computational method. We also provide consistency proofs\nfor our method in both continuum and discrete finite domains for objective\nfunctions that are sums. In numerical experiments comparing against previous\nstate-of-the-art methods, including those that also leverage sum or integral\nstructure, our method performs as well or better across a wide range of\nproblems and offers significant improvements when evaluations are noisy or the\nintegrand varies smoothly in the integrated variables. \n\n"}
{"id": "1803.09180", "contents": "Title: Unsupervised Domain Adaptation: from Simulation Engine to the RealWorld Abstract: Large-scale labeled training datasets have enabled deep neural networks to\nexcel on a wide range of benchmark vision tasks. However, in many applications\nit is prohibitively expensive or time-consuming to obtain large quantities of\nlabeled data. To cope with limited labeled training data, many have attempted\nto directly apply models trained on a large-scale labeled source domain to\nanother sparsely labeled target domain. Unfortunately, direct transfer across\ndomains often performs poorly due to domain shift and dataset bias. Domain\nadaptation is the machine learning paradigm that aims to learn a model from a\nsource domain that can perform well on a different (but related) target domain.\nIn this paper, we summarize and compare the latest unsupervised domain\nadaptation methods in computer vision applications. We classify the non-deep\napproaches into sample re-weighting and intermediate subspace transformation\ncategories, while the deep strategy includes discrepancy-based methods,\nadversarial generative models, adversarial discriminative models and\nreconstruction-based methods. We also discuss some potential directions. \n\n"}
{"id": "1803.09533", "contents": "Title: Deep Representation for Patient Visits from Electronic Health Records Abstract: We show how to learn low-dimensional representations (embeddings) of patient\nvisits from the corresponding electronic health record (EHR) where\nInternational Classification of Diseases (ICD) diagnosis codes are removed. We\nexpect that these embeddings will be useful for the construction of predictive\nstatistical models anticipated to drive personalized medicine and improve\nhealthcare quality. These embeddings are learned using a deep neural network\ntrained to predict ICD diagnosis categories. We show that our embeddings\ncapture relevant clinical informations and can be used directly as input to\nstandard machine learning algorithms like multi-output classifiers for ICD code\nprediction. We also show that important medical informations correspond to\nparticular directions in our embedding space. \n\n"}
{"id": "1803.10049", "contents": "Title: Fast Parametric Learning with Activation Memorization Abstract: Neural networks trained with backpropagation often struggle to identify\nclasses that have been observed a small number of times. In applications where\nmost class labels are rare, such as language modelling, this can become a\nperformance bottleneck. One potential remedy is to augment the network with a\nfast-learning non-parametric model which stores recent activations and class\nlabels into an external memory. We explore a simplified architecture where we\ntreat a subset of the model parameters as fast memory stores. This can help\nretain information over longer time intervals than a traditional memory, and\ndoes not require additional space or compute. In the case of image\nclassification, we display faster binding of novel classes on an Omniglot image\ncurriculum task. We also show improved performance for word-based language\nmodels on news reports (GigaWord), books (Project Gutenberg) and Wikipedia\narticles (WikiText-103) --- the latter achieving a state-of-the-art perplexity\nof 29.2. \n\n"}
{"id": "1803.10122", "contents": "Title: World Models Abstract: We explore building generative neural network models of popular reinforcement\nlearning environments. Our world model can be trained quickly in an\nunsupervised manner to learn a compressed spatial and temporal representation\nof the environment. By using features extracted from the world model as inputs\nto an agent, we can train a very compact and simple policy that can solve the\nrequired task. We can even train our agent entirely inside of its own\nhallucinated dream generated by its world model, and transfer this policy back\ninto the actual environment.\n  An interactive version of this paper is available at\nhttps://worldmodels.github.io/ \n\n"}
{"id": "1803.10586", "contents": "Title: Stochastic Variational Inference with Gradient Linearization Abstract: Variational inference has experienced a recent surge in popularity owing to\nstochastic approaches, which have yielded practical tools for a wide range of\nmodel classes. A key benefit is that stochastic variational inference obviates\nthe tedious process of deriving analytical expressions for closed-form variable\nupdates. Instead, one simply needs to derive the gradient of the log-posterior,\nwhich is often much easier. Yet for certain model classes, the log-posterior\nitself is difficult to optimize using standard gradient techniques. One such\nexample are random field models, where optimization based on gradient\nlinearization has proven popular, since it speeds up convergence significantly\nand can avoid poor local optima. In this paper we propose stochastic\nvariational inference with gradient linearization (SVIGL). It is similarly\nconvenient as standard stochastic variational inference - all that is required\nis a local linearization of the energy gradient. Its benefit over stochastic\nvariational inference with conventional gradient methods is a clear improvement\nin convergence speed, while yielding comparable or even better variational\napproximations in terms of KL divergence. We demonstrate the benefits of SVIGL\nin three applications: Optical flow estimation, Poisson-Gaussian denoising, and\n3D surface reconstruction. \n\n"}
{"id": "1804.00021", "contents": "Title: Hierarchical Transfer Convolutional Neural Networks for Image\n  Classification Abstract: In this paper, we address the issue of how to enhance the generalization\nperformance of convolutional neural networks (CNN) in the early learning stage\nfor image classification. This is motivated by real-time applications that\nrequire the generalization performance of CNN to be satisfactory within limited\ntraining time. In order to achieve this, a novel hierarchical transfer CNN\nframework is proposed. It consists of a group of shallow CNNs and a cloud CNN,\nwhere the shallow CNNs are trained firstly and then the first layers of the\ntrained shallow CNNs are used to initialize the first layer of the cloud CNN.\nThis method will boost the generalization performance of the cloud CNN\nsignificantly, especially during the early stage of training. Experiments using\nCIFAR-10 and ImageNet datasets are performed to examine the proposed method.\nResults demonstrate the improvement of testing accuracy is 12% on average and\nas much as 20% for the CIFAR-10 case while 5% testing accuracy improvement for\nthe ImageNet case during the early stage of learning. It is also shown that\nuniversal improvements of testing accuracy are obtained across different\nsettings of dropout and number of shallow CNNs. \n\n"}
{"id": "1804.00236", "contents": "Title: Recognizing Challenging Handwritten Annotations with Fully Convolutional\n  Networks Abstract: This paper introduces a very challenging dataset of historic German documents\nand evaluates Fully Convolutional Neural Network (FCNN) based methods to locate\nhandwritten annotations of any kind in these documents. The handwritten\nannotations can appear in form of underlines and text by using various writing\ninstruments, e.g., the use of pencils makes the data more challenging. We train\nand evaluate various end-to-end semantic segmentation approaches and report the\nresults. The task is to classify the pixels of documents into two classes:\nbackground and handwritten annotation. The best model achieves a mean\nIntersection over Union (IoU) score of 95.6% on the test documents of the\npresented dataset. We also present a comparison of different strategies used\nfor data augmentation and training on our presented dataset. For evaluation, we\nuse the Layout Analysis Evaluator for the ICDAR 2017 Competition on Layout\nAnalysis for Challenging Medieval Manuscripts. \n\n"}
{"id": "1804.00379", "contents": "Title: Recall Traces: Backtracking Models for Efficient Reinforcement Learning Abstract: In many environments only a tiny subset of all states yield high reward. In\nthese cases, few of the interactions with the environment provide a relevant\nlearning signal. Hence, we may want to preferentially train on those\nhigh-reward states and the probable trajectories leading to them. To this end,\nwe advocate for the use of a backtracking model that predicts the preceding\nstates that terminate at a given high-reward state. We can train a model which,\nstarting from a high value state (or one that is estimated to have high value),\npredicts and sample for which the (state, action)-tuples may have led to that\nhigh value state. These traces of (state, action) pairs, which we refer to as\nRecall Traces, sampled from this backtracking model starting from a high value\nstate, are informative as they terminate in good states, and hence we can use\nthese traces to improve a policy. We provide a variational interpretation for\nthis idea and a practical algorithm in which the backtracking model samples\nfrom an approximate posterior distribution over trajectories which lead to\nlarge rewards. Our method improves the sample efficiency of both on- and\noff-policy RL algorithms across several environments and tasks. \n\n"}
{"id": "1804.00722", "contents": "Title: Hierarchical Novelty Detection for Visual Object Recognition Abstract: Deep neural networks have achieved impressive success in large-scale visual\nobject recognition tasks with a predefined set of classes. However, recognizing\nobjects of novel classes unseen during training still remains challenging. The\nproblem of detecting such novel classes has been addressed in the literature,\nbut most prior works have focused on providing simple binary or regressive\ndecisions, e.g., the output would be \"known,\" \"novel,\" or corresponding\nconfidence intervals. In this paper, we study more informative novelty\ndetection schemes based on a hierarchical classification framework. For an\nobject of a novel class, we aim for finding its closest super class in the\nhierarchical taxonomy of known classes. To this end, we propose two different\napproaches termed top-down and flatten methods, and their combination as well.\nThe essential ingredients of our methods are confidence-calibrated classifiers,\ndata relabeling, and the leave-one-out strategy for modeling novel classes\nunder the hierarchical taxonomy. Furthermore, our method can generate a\nhierarchical embedding that leads to improved generalized zero-shot learning\nperformance in combination with other commonly-used semantic embeddings. \n\n"}
{"id": "1804.00925", "contents": "Title: Correlated discrete data generation using adversarial training Abstract: Generative Adversarial Networks (GAN) have shown great promise in tasks like\nsynthetic image generation, image inpainting, style transfer, and anomaly\ndetection. However, generating discrete data is a challenge. This work presents\nan adversarial training based correlated discrete data (CDD) generation model.\nIt also details an approach for conditional CDD generation. The results of our\napproach are presented over two datasets; job-seeking candidates skill set\n(private dataset) and MNIST (public dataset). From quantitative and qualitative\nanalysis of these results, we show that our model performs better as it\nleverages inherent correlation in the data, than an existing model that\noverlooks correlation. \n\n"}
{"id": "1804.01118", "contents": "Title: Synthesizing Programs for Images using Reinforced Adversarial Learning Abstract: Advances in deep generative networks have led to impressive results in recent\nyears. Nevertheless, such models can often waste their capacity on the minutiae\nof datasets, presumably due to weak inductive biases in their decoders. This is\nwhere graphics engines may come in handy since they abstract away low-level\ndetails and represent images as high-level programs. Current methods that\ncombine deep learning and renderers are limited by hand-crafted likelihood or\ndistance functions, a need for large amounts of supervision, or difficulties in\nscaling their inference algorithms to richer datasets. To mitigate these\nissues, we present SPIRAL, an adversarially trained agent that generates a\nprogram which is executed by a graphics engine to interpret and sample images.\nThe goal of this agent is to fool a discriminator network that distinguishes\nbetween real and rendered data, trained with a distributed reinforcement\nlearning setup without any supervision. A surprising finding is that using the\ndiscriminator's output as a reward signal is the key to allow the agent to make\nmeaningful progress at matching the desired output rendering. To the best of\nour knowledge, this is the first demonstration of an end-to-end, unsupervised\nand adversarial inverse graphics agent on challenging real world (MNIST,\nOmniglot, CelebA) and synthetic 3D datasets. \n\n"}
{"id": "1804.01947", "contents": "Title: Sliced-Wasserstein Autoencoder: An Embarrassingly Simple Generative\n  Model Abstract: In this paper we study generative modeling via autoencoders while using the\nelegant geometric properties of the optimal transport (OT) problem and the\nWasserstein distances. We introduce Sliced-Wasserstein Autoencoders (SWAE),\nwhich are generative models that enable one to shape the distribution of the\nlatent space into any samplable probability distribution without the need for\ntraining an adversarial network or defining a closed-form for the distribution.\nIn short, we regularize the autoencoder loss with the sliced-Wasserstein\ndistance between the distribution of the encoded training samples and a\npredefined samplable distribution. We show that the proposed formulation has an\nefficient numerical solution that provides similar capabilities to Wasserstein\nAutoencoders (WAE) and Variational Autoencoders (VAE), while benefiting from an\nembarrassingly simple implementation. \n\n"}
{"id": "1804.02528", "contents": "Title: ANNETT-O: An Ontology for Describing Artificial Neural Network\n  Evaluation, Topology and Training Abstract: Deep learning models, while effective and versatile, are becoming\nincreasingly complex, often including multiple overlapping networks of\narbitrary depths, multiple objectives and non-intuitive training methodologies.\nThis makes it increasingly difficult for researchers and practitioners to\ndesign, train and understand them. In this paper we present ANNETT-O, a\nmuch-needed, generic and computer-actionable vocabulary for researchers and\npractitioners to describe their deep learning configurations, training\nprocedures and experiments. The proposed ontology focuses on topological,\ntraining and evaluation aspects of complex deep neural configurations, while\nkeeping peripheral entities more succinct. Knowledge bases implementing\nANNETT-O can support a wide variety of queries, providing relevant insights to\nusers. In addition to a detailed description of the ontology, we demonstrate\nits suitability to the task via a number of hypothetical use-cases of\nincreasing complexity. \n\n"}
{"id": "1804.02704", "contents": "Title: Discovering Process Maps from Event Streams Abstract: Automated process discovery is a class of process mining methods that allow\nanalysts to extract business process models from event logs. Traditional\nprocess discovery methods extract process models from a snapshot of an event\nlog stored in its entirety. In some scenarios, however, events keep coming with\na high arrival rate to the extent that it is impractical to store the entire\nevent log and to continuously re-discover a process model from scratch. Such\nscenarios require online process discovery approaches. Given an event stream\nproduced by the execution of a business process, the goal of an online process\ndiscovery method is to maintain a continuously updated model of the process\nwith a bounded amount of memory while at the same time achieving similar\naccuracy as offline methods. However, existing online discovery approaches\nrequire relatively large amounts of memory to achieve levels of accuracy\ncomparable to that of offline methods. Therefore, this paper proposes an\napproach that addresses this limitation by mapping the problem of online\nprocess discovery to that of cache memory management, and applying well-known\ncache replacement policies to the problem of online process discovery. The\napproach has been implemented in .NET, experimentally integrated with the Minit\nprocess mining tool and comparatively evaluated against an existing baseline\nusing real-life datasets. \n\n"}
{"id": "1804.04732", "contents": "Title: Multimodal Unsupervised Image-to-Image Translation Abstract: Unsupervised image-to-image translation is an important and challenging\nproblem in computer vision. Given an image in the source domain, the goal is to\nlearn the conditional distribution of corresponding images in the target\ndomain, without seeing any pairs of corresponding images. While this\nconditional distribution is inherently multimodal, existing approaches make an\noverly simplified assumption, modeling it as a deterministic one-to-one\nmapping. As a result, they fail to generate diverse outputs from a given source\ndomain image. To address this limitation, we propose a Multimodal Unsupervised\nImage-to-image Translation (MUNIT) framework. We assume that the image\nrepresentation can be decomposed into a content code that is domain-invariant,\nand a style code that captures domain-specific properties. To translate an\nimage to another domain, we recombine its content code with a random style code\nsampled from the style space of the target domain. We analyze the proposed\nframework and establish several theoretical results. Extensive experiments with\ncomparisons to the state-of-the-art approaches further demonstrates the\nadvantage of the proposed framework. Moreover, our framework allows users to\ncontrol the style of translation outputs by providing an example style image.\nCode and pretrained models are available at https://github.com/nvlabs/MUNIT \n\n"}
{"id": "1804.05018", "contents": "Title: Comparatives, Quantifiers, Proportions: A Multi-Task Model for the\n  Learning of Quantities from Vision Abstract: The present work investigates whether different quantification mechanisms\n(set comparison, vague quantification, and proportional estimation) can be\njointly learned from visual scenes by a multi-task computational model. The\nmotivation is that, in humans, these processes underlie the same cognitive,\nnon-symbolic ability, which allows an automatic estimation and comparison of\nset magnitudes. We show that when information about lower-complexity tasks is\navailable, the higher-level proportional task becomes more accurate than when\nperformed in isolation. Moreover, the multi-task model is able to generalize to\nunseen combinations of target/non-target objects. Consistently with behavioral\nevidence showing the interference of absolute number in the proportional task,\nthe multi-task model no longer works when asked to provide the number of target\nobjects in the scene. \n\n"}
{"id": "1804.05020", "contents": "Title: A Deep Learning Approach to Fast, Format-Agnostic Detection of Malicious\n  Web Content Abstract: Malicious web content is a serious problem on the Internet today. In this\npaper we propose a deep learning approach to detecting malevolent web pages.\nWhile past work on web content detection has relied on syntactic parsing or on\nemulation of HTML and Javascript to extract features, our approach operates\ndirectly on a language-agnostic stream of tokens extracted directly from static\nHTML files with a simple regular expression. This makes it fast enough to\noperate in high-frequency data contexts like firewalls and web proxies, and\nallows it to avoid the attack surface exposure of complex parsing and emulation\ncode. Unlike well-known approaches such as bag-of-words models, which ignore\nspatial information, our neural network examines content at hierarchical\nspatial scales, allowing our model to capture locality and yielding superior\naccuracy compared to bag-of-words baselines. Our proposed architecture achieves\na 97.5% detection rate at a 0.1% false positive rate, and classifies\nsmall-batched web pages at a rate of over 100 per second on commodity hardware.\nThe speed and accuracy of our approach makes it appropriate for deployment to\nendpoints, firewalls, and web proxies. \n\n"}
{"id": "1804.06218", "contents": "Title: Hierarchical correlation reconstruction with missing data, for example\n  for biology-inspired neuron Abstract: Machine learning often needs to model density from a multidimensional data\nsample, including correlations between coordinates. Additionally, we often have\nmissing data case: that data points can miss values for some of coordinates.\nThis article adapts rapid parametric density estimation approach for this\npurpose: modelling density as a linear combination of orthonormal functions,\nfor which $L^2$ optimization says that (independently) estimated coefficient\nfor a given function is just average over the sample of value of this function.\nHierarchical correlation reconstruction first models probability density for\neach separate coordinate using all its appearances in data sample, then adds\ncorrections from independently modelled pairwise correlations using all samples\nhaving both coordinates, and so on independently adding correlations for\ngrowing numbers of variables using often decreasing evidence in data sample. A\nbasic application of such modelled multidimensional density can be imputation\nof missing coordinates: by inserting known coordinates to the density, and\ntaking expected values for the missing coordinates, or even their entire joint\nprobability distribution. Presented method can be compared with cascade\ncorrelations approach, offering several advantages in flexibility and accuracy.\nIt can be also used as artificial neuron: maximizing prediction capabilities\nfor only local behavior - modelling and predicting local connections. \n\n"}
{"id": "1804.07010", "contents": "Title: Forward-Backward Stochastic Neural Networks: Deep Learning of\n  High-dimensional Partial Differential Equations Abstract: Classical numerical methods for solving partial differential equations suffer\nfrom the curse dimensionality mainly due to their reliance on meticulously\ngenerated spatio-temporal grids. Inspired by modern deep learning based\ntechniques for solving forward and inverse problems associated with partial\ndifferential equations, we circumvent the tyranny of numerical discretization\nby devising an algorithm that is scalable to high-dimensions. In particular, we\napproximate the unknown solution by a deep neural network which essentially\nenables us to benefit from the merits of automatic differentiation. To train\nthe aforementioned neural network we leverage the well-known connection between\nhigh-dimensional partial differential equations and forward-backward stochastic\ndifferential equations. In fact, independent realizations of a standard\nBrownian motion will act as training data. We test the effectiveness of our\napproach for a couple of benchmark problems spanning a number of scientific\ndomains including Black-Scholes-Barenblatt and Hamilton-Jacobi-Bellman\nequations, both in 100-dimensions. \n\n"}
{"id": "1804.07237", "contents": "Title: Multi-view Hybrid Embedding: A Divide-and-Conquer Approach Abstract: We present a novel cross-view classification algorithm where the gallery and\nprobe data come from different views. A popular approach to tackle this problem\nis the multi-view subspace learning (MvSL) that aims to learn a latent subspace\nshared by multi-view data. Despite promising results obtained on some\napplications, the performance of existing methods deteriorates dramatically\nwhen the multi-view data is sampled from nonlinear manifolds or suffers from\nheavy outliers. To circumvent this drawback, motivated by the\nDivide-and-Conquer strategy, we propose Multi-view Hybrid Embedding (MvHE), a\nunique method of dividing the problem of cross-view classification into three\nsubproblems and building one model for each subproblem. Specifically, the first\nmodel is designed to remove view discrepancy, whereas the second and third\nmodels attempt to discover the intrinsic nonlinear structure and to increase\ndiscriminability in intra-view and inter-view samples respectively. The kernel\nextension is conducted to further boost the representation power of MvHE.\nExtensive experiments are conducted on four benchmark datasets. Our methods\ndemonstrate overwhelming advantages against the state-of-the-art MvSL based\ncross-view classification approaches in terms of classification accuracy and\nrobustness. \n\n"}
{"id": "1804.07275", "contents": "Title: Deep Triplet Ranking Networks for One-Shot Recognition Abstract: Despite the breakthroughs achieved by deep learning models in conventional\nsupervised learning scenarios, their dependence on sufficient labeled training\ndata in each class prevents effective applications of these deep models in\nsituations where labeled training instances for a subset of novel classes are\nvery sparse -- in the extreme case only one instance is available for each\nclass. To tackle this natural and important challenge, one-shot learning, which\naims to exploit a set of well labeled base classes to build classifiers for the\nnew target classes that have only one observed instance per class, has recently\nreceived increasing attention from the research community. In this paper we\npropose a novel end-to-end deep triplet ranking network to perform one-shot\nlearning. The proposed approach learns class universal image embeddings on the\nwell labeled base classes under a triplet ranking loss, such that the instances\nfrom new classes can be categorized based on their similarity with the one-shot\ninstances in the learned embedding space. Moreover, our approach can naturally\nincorporate the available one-shot instances from the new classes into the\nembedding learning process to improve the triplet ranking model. We conduct\nexperiments on two popular datasets for one-shot learning. The results show the\nproposed approach achieves better performance than the state-of-the- art\ncomparison methods. \n\n"}
{"id": "1804.07645", "contents": "Title: One-Shot Learning using Mixture of Variational Autoencoders: a\n  Generalization Learning approach Abstract: Deep learning, even if it is very successful nowadays, traditionally needs\nvery large amounts of labeled data to perform excellent on the classification\ntask. In an attempt to solve this problem, the one-shot learning paradigm,\nwhich makes use of just one labeled sample per class and prior knowledge,\nbecomes increasingly important. In this paper, we propose a new one-shot\nlearning method, dubbed MoVAE (Mixture of Variational AutoEncoders), to perform\nclassification. Complementary to prior studies, MoVAE represents a shift of\nparadigm in comparison with the usual one-shot learning methods, as it does not\nuse any prior knowledge. Instead, it starts from zero knowledge and one labeled\nsample per class. Afterward, by using unlabeled data and the generalization\nlearning concept (in a way, more as humans do), it is capable to gradually\nimprove by itself its performance. Even more, if there are no unlabeled data\navailable MoVAE can still perform well in one-shot learning classification. We\ndemonstrate empirically the efficiency of our proposed approach on three\ndatasets, i.e. the handwritten digits (MNIST), fashion products\n(Fashion-MNIST), and handwritten characters (Omniglot), showing that MoVAE\noutperforms state-of-the-art one-shot learning algorithms. \n\n"}
{"id": "1804.08071", "contents": "Title: Decoupled Networks Abstract: Inner product-based convolution has been a central component of convolutional\nneural networks (CNNs) and the key to learning visual representations. Inspired\nby the observation that CNN-learned features are naturally decoupled with the\nnorm of features corresponding to the intra-class variation and the angle\ncorresponding to the semantic difference, we propose a generic decoupled\nlearning framework which models the intra-class variation and semantic\ndifference independently. Specifically, we first reparametrize the inner\nproduct to a decoupled form and then generalize it to the decoupled convolution\noperator which serves as the building block of our decoupled networks. We\npresent several effective instances of the decoupled convolution operator. Each\ndecoupled operator is well motivated and has an intuitive geometric\ninterpretation. Based on these decoupled operators, we further propose to\ndirectly learn the operator from data. Extensive experiments show that such\ndecoupled reparameterization renders significant performance gain with easier\nconvergence and stronger robustness. \n\n"}
{"id": "1804.08450", "contents": "Title: Decorrelated Batch Normalization Abstract: Batch Normalization (BN) is capable of accelerating the training of deep\nmodels by centering and scaling activations within mini-batches. In this work,\nwe propose Decorrelated Batch Normalization (DBN), which not just centers and\nscales activations but whitens them. We explore multiple whitening techniques,\nand find that PCA whitening causes a problem we call stochastic axis swapping,\nwhich is detrimental to learning. We show that ZCA whitening does not suffer\nfrom this problem, permitting successful learning. DBN retains the desirable\nqualities of BN and further improves BN's optimization efficiency and\ngeneralization ability. We design comprehensive experiments to show that DBN\ncan improve the performance of BN on multilayer perceptrons and convolutional\nneural networks. Furthermore, we consistently improve the accuracy of residual\nnetworks on CIFAR-10, CIFAR-100, and ImageNet. \n\n"}
{"id": "1804.08619", "contents": "Title: State Distribution-aware Sampling for Deep Q-learning Abstract: A critical and challenging problem in reinforcement learning is how to learn\nthe state-action value function from the experience replay buffer and\nsimultaneously keep sample efficiency and faster convergence to a high quality\nsolution. In prior works, transitions are uniformly sampled at random from the\nreplay buffer or sampled based on their priority measured by\ntemporal-difference (TD) error. However, these approaches do not fully take\ninto consideration the intrinsic characteristics of transition distribution in\nthe state space and could result in redundant and unnecessary TD updates,\nslowing down the convergence of the learning procedure. To overcome this\nproblem, we propose a novel state distribution-aware sampling method to balance\nthe replay times for transitions with skew distribution, which takes into\naccount both the occurrence frequencies of transitions and the uncertainty of\nstate-action values. Consequently, our approach could reduce the unnecessary TD\nupdates and increase the TD updates for state-action value with more\nuncertainty, making the experience replay more effective and efficient.\nExtensive experiments are conducted on both classic control tasks and Atari\n2600 games based on OpenAI gym platform and the experimental results\ndemonstrate the effectiveness of our approach in comparison with the standard\nDQN approach. \n\n"}
{"id": "1804.08796", "contents": "Title: Block-Structure Based Time-Series Models For Graph Sequences Abstract: Although the computational and statistical trade-off for modeling single\ngraphs, for instance, using block models is relatively well understood,\nextending such results to sequences of graphs has proven to be difficult. In\nthis work, we take a step in this direction by proposing two models for graph\nsequences that capture: (a) link persistence between nodes across time, and (b)\ncommunity persistence of each node across time. In the first model, we assume\nthat the latent community of each node does not change over time, and in the\nsecond model we relax this assumption suitably. For both of these proposed\nmodels, we provide statistically and computationally efficient inference\nalgorithms, whose unique feature is that they leverage community detection\nmethods that work on single graphs. We also provide experimental results\nvalidating the suitability of our models and methods on synthetic and real\ninstances. \n\n"}
{"id": "1804.09401", "contents": "Title: Generative Temporal Models with Spatial Memory for Partially Observed\n  Environments Abstract: In model-based reinforcement learning, generative and temporal models of\nenvironments can be leveraged to boost agent performance, either by tuning the\nagent's representations during training or via use as part of an explicit\nplanning mechanism. However, their application in practice has been limited to\nsimplistic environments, due to the difficulty of training such models in\nlarger, potentially partially-observed and 3D environments. In this work we\nintroduce a novel action-conditioned generative model of such challenging\nenvironments. The model features a non-parametric spatial memory system in\nwhich we store learned, disentangled representations of the environment.\nLow-dimensional spatial updates are computed using a state-space model that\nmakes use of knowledge on the prior dynamics of the moving agent, and\nhigh-dimensional visual observations are modelled with a Variational\nAuto-Encoder. The result is a scalable architecture capable of performing\ncoherent predictions over hundreds of time steps across a range of partially\nobserved 2D and 3D environments. \n\n"}
{"id": "1804.09530", "contents": "Title: Strong Baselines for Neural Semi-supervised Learning under Domain Shift Abstract: Novel neural models have been proposed in recent years for learning under\ndomain shift. Most models, however, only evaluate on a single task, on\nproprietary datasets, or compare to weak baselines, which makes comparison of\nmodels difficult. In this paper, we re-evaluate classic general-purpose\nbootstrapping approaches in the context of neural networks under domain shifts\nvs. recent neural approaches and propose a novel multi-task tri-training method\nthat reduces the time and space complexity of classic tri-training. Extensive\nexperiments on two benchmarks are negative: while our novel method establishes\na new state-of-the-art for sentiment analysis, it does not fare consistently\nthe best. More importantly, we arrive at the somewhat surprising conclusion\nthat classic tri-training, with some additions, outperforms the state of the\nart. We conclude that classic approaches constitute an important and strong\nbaseline. \n\n"}
{"id": "1804.10500", "contents": "Title: Deep Reinforcement Learning to Acquire Navigation Skills for\n  Wheel-Legged Robots in Complex Environments Abstract: Mobile robot navigation in complex and dynamic environments is a challenging\nbut important problem. Reinforcement learning approaches fail to solve these\ntasks efficiently due to reward sparsities, temporal complexities and\nhigh-dimensionality of sensorimotor spaces which are inherent in such problems.\nWe present a novel approach to train action policies to acquire navigation\nskills for wheel-legged robots using deep reinforcement learning. The policy\nmaps height-map image observations to motor commands to navigate to a target\nposition while avoiding obstacles. We propose to acquire the multifaceted\nnavigation skill by learning and exploiting a number of manageable navigation\nbehaviors. We also introduce a domain randomization technique to improve the\nversatility of the training samples. We demonstrate experimentally a\nsignificant improvement in terms of data-efficiency, success rate, robustness\nagainst irrelevant sensory data, and also the quality of the maneuver skills. \n\n"}
{"id": "1804.11067", "contents": "Title: Staircase Network: structural language identification via hierarchical\n  attentive units Abstract: Language recognition system is typically trained directly to optimize\nclassification error on the target language labels, without using the external,\nor meta-information in the estimation of the model parameters. However labels\nare not independent of each other, there is a dependency enforced by, for\nexample, the language family, which affects negatively on classification. The\nother external information sources (e.g. audio encoding, telephony or video\nspeech) can also decrease classification accuracy. In this paper, we attempt to\nsolve these issues by constructing a deep hierarchical neural network, where\ndifferent levels of meta-information are encapsulated by attentive prediction\nunits and also embedded into the training progress. The proposed method learns\nauxiliary tasks to obtain robust internal representation and to construct a\nvariant of attentive units within the hierarchical model. The final result is\nthe structural prediction of the target language and a closely related language\nfamily. The algorithm reflects a \"staircase\" way of learning in both its\narchitecture and training, advancing from the fundamental audio encoding to the\nlanguage family level and finally to the target language level. This process\nnot only improves generalization but also tackles the issues of imbalanced\nclass priors and channel variability in the deep neural network model. Our\nexperimental findings show that the proposed architecture outperforms the\nstate-of-the-art i-vector approaches on both small and big language corpora by\na significant margin. \n\n"}
{"id": "1805.00361", "contents": "Title: Ultra Power-Efficient CNN Domain Specific Accelerator with 9.3TOPS/Watt\n  for Mobile and Embedded Applications Abstract: Computer vision performances have been significantly improved in recent years\nby Convolutional Neural Networks(CNN). Currently, applications using CNN\nalgorithms are deployed mainly on general purpose hardwares, such as CPUs, GPUs\nor FPGAs. However, power consumption, speed, accuracy, memory footprint, and\ndie size should all be taken into consideration for mobile and embedded\napplications. Domain Specific Architecture (DSA) for CNN is the efficient and\npractical solution for CNN deployment and implementation. We designed and\nproduced a 28nm Two-Dimensional CNN-DSA accelerator with an ultra\npower-efficient performance of 9.3TOPS/Watt and with all processing done in the\ninternal memory instead of outside DRAM. It classifies 224x224 RGB image inputs\nat more than 140fps with peak power consumption at less than 300mW and an\naccuracy comparable to the VGG benchmark. The CNN-DSA accelerator is\nreconfigurable to support CNN model coefficients of various layer sizes and\nlayer types, including convolution, depth-wise convolution, short-cut\nconnections, max pooling, and ReLU. Furthermore, in order to better support\nreal-world deployment for various application scenarios, especially with\nlow-end mobile and embedded platforms and MCUs (Microcontroller Units), we also\ndesigned algorithms to fully utilize the CNN-DSA accelerator efficiently by\nreducing the dependency on external accelerator computation resources,\nincluding implementation of Fully-Connected (FC) layers within the accelerator\nand compression of extracted features from the CNN-DSA accelerator. Live demos\nwith our CNN-DSA accelerator on mobile and embedded systems show its\ncapabilities to be widely and practically applied in the real world. \n\n"}
{"id": "1805.00705", "contents": "Title: Investigating Audio, Visual, and Text Fusion Methods for End-to-End\n  Automatic Personality Prediction Abstract: We propose a tri-modal architecture to predict Big Five personality trait\nscores from video clips with different channels for audio, text, and video\ndata. For each channel, stacked Convolutional Neural Networks are employed. The\nchannels are fused both on decision-level and by concatenating their respective\nfully connected layers. It is shown that a multimodal fusion approach\noutperforms each single modality channel, with an improvement of 9.4\\% over the\nbest individual modality (video). Full backpropagation is also shown to be\nbetter than a linear combination of modalities, meaning complex interactions\nbetween modalities can be leveraged to build better models. Furthermore, we can\nsee the prediction relevance of each modality for each trait. The described\nmodel can be used to increase the emotional intelligence of virtual agents. \n\n"}
{"id": "1805.00784", "contents": "Title: Markov Chain Neural Networks Abstract: In this work we present a modified neural network model which is capable to\nsimulate Markov Chains. We show how to express and train such a network, how to\nensure given statistical properties reflected in the training data and we\ndemonstrate several applications where the network produces non-deterministic\noutcomes. One example is a random walker model, e.g. useful for simulation of\nBrownian motions or a natural Tic-Tac-Toe network which ensures\nnon-deterministic game behavior. \n\n"}
{"id": "1805.01033", "contents": "Title: Unsupervised Learning using Pretrained CNN and Associative Memory Bank Abstract: Deep Convolutional features extracted from a comprehensive labeled dataset,\ncontain substantial representations which could be effectively used in a new\ndomain. Despite the fact that generic features achieved good results in many\nvisual tasks, fine-tuning is required for pretrained deep CNN models to be more\neffective and provide state-of-the-art performance. Fine tuning using the\nbackpropagation algorithm in a supervised setting, is a time and resource\nconsuming process. In this paper, we present a new architecture and an approach\nfor unsupervised object recognition that addresses the above mentioned problem\nwith fine tuning associated with pretrained CNN-based supervised deep learning\napproaches while allowing automated feature extraction. Unlike existing works,\nour approach is applicable to general object recognition tasks. It uses a\npretrained (on a related domain) CNN model for automated feature extraction\npipelined with a Hopfield network based associative memory bank for storing\npatterns for classification purposes. The use of associative memory bank in our\nframework allows eliminating backpropagation while providing competitive\nperformance on an unseen dataset. \n\n"}
{"id": "1805.01367", "contents": "Title: Open Loop Execution of Tree-Search Algorithms, extended version Abstract: In the context of tree-search stochastic planning algorithms where a\ngenerative model is available, we consider on-line planning algorithms building\ntrees in order to recommend an action. We investigate the question of avoiding\nre-planning in subsequent decision steps by directly using sub-trees as action\nrecommender. Firstly, we propose a method for open loop control via a new\nalgorithm taking the decision of re-planning or not at each time step based on\nan analysis of the statistics of the sub-tree. Secondly, we show that the\nprobability of selecting a suboptimal action at any depth of the tree can be\nupper bounded and converges towards zero. Moreover, this upper bound decays in\na logarithmic way between subsequent depths. This leads to a distinction\nbetween node-wise optimality and state-wise optimality. Finally, we empirically\ndemonstrate that our method achieves a compromise between loss of performance\nand computational gain. \n\n"}
{"id": "1805.01889", "contents": "Title: t-PINE: Tensor-based Predictable and Interpretable Node Embeddings Abstract: Graph representations have increasingly grown in popularity during the last\nyears. Existing representation learning approaches explicitly encode network\nstructure. Despite their good performance in downstream processes (e.g., node\nclassification, link prediction), there is still room for improvement in\ndifferent aspects, like efficacy, visualization, and interpretability. In this\npaper, we propose, t-PINE, a method that addresses these limitations. Contrary\nto baseline methods, which generally learn explicit graph representations by\nsolely using an adjacency matrix, t-PINE avails a multi-view information graph,\nthe adjacency matrix represents the first view, and a nearest neighbor\nadjacency, computed over the node features, is the second view, in order to\nlearn explicit and implicit node representations, using the Canonical Polyadic\n(a.k.a. CP) decomposition. We argue that the implicit and the explicit mapping\nfrom a higher-dimensional to a lower-dimensional vector space is the key to\nlearn more useful, highly predictable, and gracefully interpretable\nrepresentations. Having good interpretable representations provides a good\nguidance to understand how each view contributes to the representation learning\nprocess. In addition, it helps us to exclude unrelated dimensions. Extensive\nexperiments show that t-PINE drastically outperforms baseline methods by up to\n158.6% with respect to Micro-F1, in several multi-label classification\nproblems, while it has high visualization and interpretability utility. \n\n"}
{"id": "1805.02070", "contents": "Title: Deep Reinforcement Learning for Playing 2.5D Fighting Games Abstract: Deep reinforcement learning has shown its success in game playing. However,\n2.5D fighting games would be a challenging task to handle due to ambiguity in\nvisual appearances like height or depth of the characters. Moreover, actions in\nsuch games typically involve particular sequential action orders, which also\nmakes the network design very difficult. Based on the network of Asynchronous\nAdvantage Actor-Critic (A3C), we create an OpenAI-gym-like gaming environment\nwith the game of Little Fighter 2 (LF2), and present a novel A3C+ network for\nlearning RL agents. The introduced model includes a Recurrent Info network,\nwhich utilizes game-related info features with recurrent layers to observe\ncombo skills for fighting. In the experiments, we consider LF2 in different\nsettings, which successfully demonstrates the use of our proposed model for\nlearning 2.5D fighting games. \n\n"}
{"id": "1805.02855", "contents": "Title: Tile2Vec: Unsupervised representation learning for spatially distributed\n  data Abstract: Geospatial analysis lacks methods like the word vector representations and\npre-trained networks that significantly boost performance across a wide range\nof natural language and computer vision tasks. To fill this gap, we introduce\nTile2Vec, an unsupervised representation learning algorithm that extends the\ndistributional hypothesis from natural language -- words appearing in similar\ncontexts tend to have similar meanings -- to spatially distributed data. We\ndemonstrate empirically that Tile2Vec learns semantically meaningful\nrepresentations on three datasets. Our learned representations significantly\nimprove performance in downstream classification tasks and, similar to word\nvectors, visual analogies can be obtained via simple arithmetic in the latent\nspace. \n\n"}
{"id": "1805.03096", "contents": "Title: Fast Feature Extraction with CNNs with Pooling Layers Abstract: In recent years, many publications showed that convolutional neural network\nbased features can have a superior performance to engineered features. However,\nnot much effort was taken so far to extract local features efficiently for a\nwhole image. In this paper, we present an approach to compute patch-based local\nfeature descriptors efficiently in presence of pooling and striding layers for\nwhole images at once. Our approach is generic and can be applied to nearly all\nexisting network architectures. This includes networks for all local feature\nextraction tasks like camera calibration, Patchmatching, optical flow\nestimation and stereo matching. In addition, our approach can be applied to\nother patch-based approaches like sliding window object detection and\nrecognition. We complete our paper with a speed benchmark of popular CNN based\nfeature extraction approaches applied on a whole image, with and without our\nspeedup, and example code (for Torch) that shows how an arbitrary CNN\narchitecture can be easily converted by our approach. \n\n"}
{"id": "1805.03779", "contents": "Title: k-Space Deep Learning for Accelerated MRI Abstract: The annihilating filter-based low-rank Hankel matrix approach (ALOHA) is one\nof the state-of-the-art compressed sensing approaches that directly\ninterpolates the missing k-space data using low-rank Hankel matrix completion.\nThe success of ALOHA is due to the concise signal representation in the k-space\ndomain thanks to the duality between structured low-rankness in the k-space\ndomain and the image domain sparsity. Inspired by the recent mathematical\ndiscovery that links convolutional neural networks to Hankel matrix\ndecomposition using data-driven framelet basis, here we propose a fully\ndata-driven deep learning algorithm for k-space interpolation. Our network can\nbe also easily applied to non-Cartesian k-space trajectories by simply adding\nan additional regridding layer. Extensive numerical experiments show that the\nproposed deep learning method consistently outperforms the existing\nimage-domain deep learning approaches. \n\n"}
{"id": "1805.03887", "contents": "Title: Scaling associative classification for very large datasets Abstract: Supervised learning algorithms are nowadays successfully scaling up to\ndatasets that are very large in volume, leveraging the potential of in-memory\ncluster-computing Big Data frameworks. Still, massive datasets with a number of\nlarge-domain categorical features are a difficult challenge for any classifier.\nMost off-the-shelf solutions cannot cope with this problem. In this work we\nintroduce DAC, a Distributed Associative Classifier. DAC exploits ensemble\nlearning to distribute the training of an associative classifier among parallel\nworkers and improve the final quality of the model. Furthermore, it adopts\nseveral novel techniques to reach high scalability without sacrificing quality,\namong which a preventive pruning of classification rules in the extraction\nphase based on Gini impurity. We ran experiments on Apache Spark, on a real\nlarge-scale dataset with more than 4 billion records and 800 million distinct\ncategories. The results showed that DAC improves on a state-of-the-art solution\nin both prediction quality and execution time. Since the generated model is\nhuman-readable, it can not only classify new records, but also allow\nunderstanding both the logic behind the prediction and the properties of the\nmodel, becoming a useful aid for decision makers. \n\n"}
{"id": "1805.04193", "contents": "Title: An Unsupervised Clustering-Based Short-Term Solar Forecasting\n  Methodology Using Multi-Model Machine Learning Blending Abstract: Solar forecasting accuracy is affected by weather conditions, and weather\nawareness forecasting models are expected to improve the performance. However,\nit may not be available and reliable to classify different forecasting tasks by\nusing only meteorological weather categorization. In this paper, an\nunsupervised clustering-based (UC-based) solar forecasting methodology is\ndeveloped for short-term (1-hour-ahead) global horizontal irradiance (GHI)\nforecasting. This methodology consists of three parts: GHI time series\nunsupervised clustering, pattern recognition, and UC-based forecasting. The\ndaily GHI time series is first clustered by an Optimized Cross-validated\nClUsteRing (OCCUR) method, which determines the optimal number of clusters and\nbest clustering results. Then, support vector machine pattern recognition\n(SVM-PR) is adopted to recognize the category of a certain day using the first\nfew hours' data in the forecasting stage. GHI forecasts are generated by the\nmost suitable models in different clusters, which are built by a two-layer\nMachine learning based Multi-Model (M3) forecasting framework. The developed\nUC-based methodology is validated by using 1-year of data with six solar\nfeatures. Numerical results show that (i) UC-based models outperform non-UC\n(all-in-one) models with the same M3 architecture by approximately 20%; (ii)\nM3-based models also outperform the single-algorithm machine learning (SAML)\nmodels by approximately 20%. \n\n"}
{"id": "1805.04246", "contents": "Title: Convex Programming Based Spectral Clustering Abstract: Clustering is a fundamental task in data analysis, and spectral clustering\nhas been recognized as a promising approach to it. Given a graph describing the\nrelationship between data, spectral clustering explores the underlying cluster\nstructure in two stages. The first stage embeds the nodes of the graph in real\nspace, and the second stage groups the embedded nodes into several clusters.\nThe use of the $k$-means method in the grouping stage is currently standard\npractice. We present a spectral clustering algorithm that uses convex\nprogramming in the grouping stage and study how well it works. This algorithm\nis designed based on the following observation. If a graph is well-clustered,\nthen the nodes with the largest degree in each cluster can be found by\ncomputing an enclosing ellipsoid of the nodes embedded in real space, and the\nclusters can be identified by using those nodes. We show that, for\nwell-clustered graphs, the algorithm can find clusters of nodes with minimal\nconductance. We also give an experimental assessment of the algorithm's\nperformance. \n\n"}
{"id": "1805.04756", "contents": "Title: Improving Predictive Uncertainty Estimation using Dropout -- Hamiltonian\n  Monte Carlo Abstract: Estimating predictive uncertainty is crucial for many computer vision tasks,\nfrom image classification to autonomous driving systems. Hamiltonian Monte\nCarlo (HMC) is an sampling method for performing Bayesian inference. On the\nother hand, Dropout regularization has been proposed as an approximate model\naveraging technique that tends to improve generalization in large scale models\nsuch as deep neural networks. Although, HMC provides convergence guarantees for\nmost standard Bayesian models, it does not handle discrete parameters arising\nfrom Dropout regularization. In this paper, we present a robust methodology for\nimproving predictive uncertainty in classification problems, based on Dropout\nand Hamiltonian Monte Carlo. Even though Dropout induces a non-smooth energy\nfunction with no such convergence guarantees, the resulting discretization of\nthe Hamiltonian proves empirical success. The proposed method allows to\neffectively estimate the predictive accuracy and to provide better\ngeneralization for difficult test examples. \n\n"}
{"id": "1805.04938", "contents": "Title: The Global Optimization Geometry of Shallow Linear Neural Networks Abstract: We examine the squared error loss landscape of shallow linear neural\nnetworks. We show---with significantly milder assumptions than previous\nworks---that the corresponding optimization problems have benign geometric\nproperties: there are no spurious local minima and the Hessian at every saddle\npoint has at least one negative eigenvalue. This means that at every saddle\npoint there is a directional negative curvature which algorithms can utilize to\nfurther decrease the objective value. These geometric properties imply that\nmany local search algorithms (such as the gradient descent which is widely\nutilized for training neural networks) can provably solve the training problem\nwith global convergence. \n\n"}
{"id": "1805.04958", "contents": "Title: Accelerating Message Passing for MAP with Benders Decomposition Abstract: We introduce a novel mechanism to tighten the local polytope relaxation for\nMAP inference in Markov random fields with low state space variables. We\nconsider a surjection of the variables to a set of hyper-variables and apply\nthe local polytope relaxation over these hyper-variables. The state space of\neach individual hyper-variable is constructed to be enumerable while the vector\nproduct of pairs is not easily enumerable making message passing inference\nintractable.\n  To circumvent the difficulty of enumerating the vector product of state\nspaces of hyper-variables we introduce a novel Benders decomposition approach.\nThis produces an upper envelope describing the message constructed from affine\nfunctions of the individual variables that compose the hyper-variable receiving\nthe message. The envelope is tight at the minimizers which are shared by the\ntrue message. Benders rows are constructed to be Pareto optimal and are\ngenerated using an efficient procedure targeted for binary problems. \n\n"}
{"id": "1805.05826", "contents": "Title: A Purely End-to-end System for Multi-speaker Speech Recognition Abstract: Recently, there has been growing interest in multi-speaker speech\nrecognition, where the utterances of multiple speakers are recognized from\ntheir mixture. Promising techniques have been proposed for this task, but\nearlier works have required additional training data such as isolated source\nsignals or senone alignments for effective learning. In this paper, we propose\na new sequence-to-sequence framework to directly decode multiple label\nsequences from a single speech sequence by unifying source separation and\nspeech recognition functions in an end-to-end manner. We further propose a new\nobjective function to improve the contrast between the hidden vectors to avoid\ngenerating similar hypotheses. Experimental results show that the model is\ndirectly able to learn a mapping from a speech mixture to multiple label\nsequences, achieving 83.1 % relative improvement compared to a model trained\nwithout the proposed objective. Interestingly, the results are comparable to\nthose produced by previous end-to-end works featuring explicit separation and\nrecognition modules. \n\n"}
{"id": "1805.06095", "contents": "Title: Semi-Blind Inference of Topologies and Dynamical Processes over Graphs Abstract: Network science provides valuable insights across numerous disciplines\nincluding sociology, biology, neuroscience and engineering. A task of major\npractical importance in these application domains is inferring the network\nstructure from noisy observations at a subset of nodes. Available methods for\ntopology inference typically assume that the process over the network is\nobserved at all nodes. However, application-specific constraints may prevent\nacquiring network-wide observations. Alleviating the limited flexibility of\nexisting approaches, this work advocates structural models for graph processes\nand develops novel algorithms for joint inference of the network topology and\nprocesses from partial nodal observations. Structural equation models (SEMs)\nand structural vector autoregressive models (SVARMs) have well-documented\nmerits in identifying even directed topologies of complex graphs; while SEMs\ncapture contemporaneous causal dependencies among nodes, SVARMs further account\nfor time-lagged influences. This paper develops algorithms that iterate between\ninferring directed graphs that \"best\" fit the data, and estimating the network\nprocesses at reduced computational complexity by leveraging tools related to\nKalman smoothing. To further accommodate delay-sensitive applications, an\nonline joint inference approach is put forth that even tracks time-evolving\ntopologies. Furthermore, conditions for identifying the network topology given\npartial observations are specified. It is proved that the required number of\nobservations for unique identification reduces significantly when the network\nstructure is sparse. Numerical tests with synthetic as well as real datasets\ncorroborate the effectiveness of the novel approach. \n\n"}
{"id": "1805.06539", "contents": "Title: Beyond Structural Causal Models: Causal Constraints Models Abstract: Structural Causal Models (SCMs) provide a popular causal modeling framework.\nIn this work, we show that SCMs are not flexible enough to give a complete\ncausal representation of dynamical systems at equilibrium. Instead, we propose\na generalization of the notion of an SCM, that we call Causal Constraints Model\n(CCM), and prove that CCMs do capture the causal semantics of such systems. We\nshow how CCMs can be constructed from differential equations and initial\nconditions and we illustrate our ideas further on a simple but ubiquitous\n(bio)chemical reaction. Our framework also allows to model functional laws,\nsuch as the ideal gas law, in a sensible and intuitive way. \n\n"}
{"id": "1805.06660", "contents": "Title: Single Shot Active Learning using Pseudo Annotators Abstract: Standard myopic active learning assumes that human annotations are always\nobtainable whenever new samples are selected. This, however, is unrealistic in\nmany real-world applications where human experts are not readily available at\nall times. In this paper, we consider the single shot setting: all the required\nsamples should be chosen in a single shot and no human annotation can be\nexploited during the selection process. We propose a new method, Active\nLearning through Random Labeling (ALRL), which substitutes single human\nannotator for multiple, what we will refer to as, pseudo annotators. These\npseudo annotators always provide uniform and random labels whenever new\nunlabeled samples are queried. This random labeling enables standard active\nlearning algorithms to also exhibit the exploratory behavior needed for single\nshot active learning. The exploratory behavior is further enhanced by selecting\nthe most representative sample via minimizing nearest neighbor distance between\nunlabeled samples and queried samples. Experiments on real-world datasets\ndemonstrate that the proposed method outperforms several state-of-the-art\napproaches. \n\n"}
{"id": "1805.07476", "contents": "Title: Two geometric input transformation methods for fast online reinforcement\n  learning with neural nets Abstract: We apply neural nets with ReLU gates in online reinforcement learning. Our\ngoal is to train these networks in an incremental manner, without the\ncomputationally expensive experience replay. By studying how individual neural\nnodes behave in online training, we recognize that the global nature of ReLU\ngates can cause undesirable learning interference in each node's learning\nbehavior. We propose reducing such interferences with two efficient input\ntransformation methods that are geometric in nature and match well the\ngeometric property of ReLU gates. The first one is tile coding, a classic\nbinary encoding scheme originally designed for local generalization based on\nthe topological structure of the input space. The second one (EmECS) is a new\nmethod we introduce; it is based on geometric properties of convex sets and\ntopological embedding of the input space into the boundary of a convex set. We\ndiscuss the behavior of the network when it operates on the transformed inputs.\nWe also compare it experimentally with some neural nets that do not use the\nsame input transformations, and with the classic algorithm of tile coding plus\na linear function approximator, and on several online reinforcement learning\ntasks, we show that the neural net with tile coding or EmECS can achieve not\nonly faster learning but also more accurate approximations. Our results\nstrongly suggest that geometric input transformation of this type can be\neffective for interference reduction and takes us a step closer to fully\nincremental reinforcement learning with neural nets. \n\n"}
{"id": "1805.07616", "contents": "Title: Do Neural Network Cross-Modal Mappings Really Bridge Modalities? Abstract: Feed-forward networks are widely used in cross-modal applications to bridge\nmodalities by mapping distributed vectors of one modality to the other, or to a\nshared space. The predicted vectors are then used to perform e.g., retrieval or\nlabeling. Thus, the success of the whole system relies on the ability of the\nmapping to make the neighborhood structure (i.e., the pairwise similarities) of\nthe predicted vectors akin to that of the target vectors. However, whether this\nis achieved has not been investigated yet. Here, we propose a new similarity\nmeasure and two ad hoc experiments to shed light on this issue. In three\ncross-modal benchmarks we learn a large number of language-to-vision and\nvision-to-language neural network mappings (up to five layers) using a rich\ndiversity of image and text features and loss functions. Our results reveal\nthat, surprisingly, the neighborhood structure of the predicted vectors\nconsistently resembles more that of the input vectors than that of the target\nvectors. In a second experiment, we further show that untrained nets do not\nsignificantly disrupt the neighborhood (i.e., semantic) structure of the input\nvectors. \n\n"}
{"id": "1805.07722", "contents": "Title: Task-Agnostic Meta-Learning for Few-shot Learning Abstract: Meta-learning approaches have been proposed to tackle the few-shot learning\nproblem.Typically, a meta-learner is trained on a variety of tasks in the hopes\nof being generalizable to new tasks. However, the generalizability on new tasks\nof a meta-learner could be fragile when it is over-trained on existing tasks\nduring meta-training phase. In other words, the initial model of a meta-learner\ncould be too biased towards existing tasks to adapt to new tasks, especially\nwhen only very few examples are available to update the model. To avoid a\nbiased meta-learner and improve its generalizability, we propose a novel\nparadigm of Task-Agnostic Meta-Learning (TAML) algorithms. Specifically, we\npresent an entropy-based approach that meta-learns an unbiased initial model\nwith the largest uncertainty over the output labels by preventing it from\nover-performing in classification tasks. Alternatively, a more general\ninequality-minimization TAML is presented for more ubiquitous scenarios by\ndirectly minimizing the inequality of initial losses beyond the classification\ntasks wherever a suitable loss can be defined.Experiments on benchmarked\ndatasets demonstrate that the proposed approaches outperform compared\nmeta-learning algorithms in both few-shot classification and reinforcement\nlearning tasks. \n\n"}
{"id": "1805.07909", "contents": "Title: Quickshift++: Provably Good Initializations for Sample-Based Mean Shift Abstract: We provide initial seedings to the Quick Shift clustering algorithm, which\napproximate the locally high-density regions of the data. Such seedings act as\nmore stable and expressive cluster-cores than the singleton modes found by\nQuick Shift. We establish statistical consistency guarantees for this\nmodification. We then show strong clustering performance on real datasets as\nwell as promising applications to image segmentation. \n\n"}
{"id": "1805.08166", "contents": "Title: Learning to Optimize Tensor Programs Abstract: We introduce a learning-based framework to optimize tensor programs for deep\nlearning workloads. Efficient implementations of tensor operators, such as\nmatrix multiplication and high dimensional convolution, are key enablers of\neffective deep learning systems. However, existing systems rely on manually\noptimized libraries such as cuDNN where only a narrow range of server class\nGPUs are well-supported. The reliance on hardware-specific operator libraries\nlimits the applicability of high-level graph optimizations and incurs\nsignificant engineering costs when deploying to new hardware targets. We use\nlearning to remove this engineering burden. We learn domain-specific\nstatistical cost models to guide the search of tensor operator implementations\nover billions of possible program variants. We further accelerate the search by\neffective model transfer across workloads. Experimental results show that our\nframework delivers performance competitive with state-of-the-art hand-tuned\nlibraries for low-power CPU, mobile GPU, and server-class GPU. \n\n"}
{"id": "1805.08289", "contents": "Title: Measuring and regularizing networks in function space Abstract: To optimize a neural network one often thinks of optimizing its parameters,\nbut it is ultimately a matter of optimizing the function that maps inputs to\noutputs. Since a change in the parameters might serve as a poor proxy for the\nchange in the function, it is of some concern that primacy is given to\nparameters but that the correspondence has not been tested. Here, we show that\nit is simple and computationally feasible to calculate distances between\nfunctions in a $L^2$ Hilbert space. We examine how typical networks behave in\nthis space, and compare how parameter $\\ell^2$ distances compare to function\n$L^2$ distances between various points of an optimization trajectory. We find\nthat the two distances are nontrivially related. In particular, the\n$L^2/\\ell^2$ ratio decreases throughout optimization, reaching a steady value\naround when test error plateaus. We then investigate how the $L^2$ distance\ncould be applied directly to optimization. We first propose that in multitask\nlearning, one can avoid catastrophic forgetting by directly limiting how much\nthe input/output function changes between tasks. Secondly, we propose a new\nlearning rule that constrains the distance a network can travel through\n$L^2$-space in any one update. This allows new examples to be learned in a way\nthat minimally interferes with what has previously been learned. These\napplications demonstrate how one can measure and regularize function distances\ndirectly, without relying on parameters or local approximations like loss\ncurvature. \n\n"}
{"id": "1805.08571", "contents": "Title: On Coresets for Logistic Regression Abstract: Coresets are one of the central methods to facilitate the analysis of large\ndata sets. We continue a recent line of research applying the theory of\ncoresets to logistic regression. First, we show a negative result, namely, that\nno strongly sublinear sized coresets exist for logistic regression. To deal\nwith intractable worst-case instances we introduce a complexity measure\n$\\mu(X)$, which quantifies the hardness of compressing a data set for logistic\nregression. $\\mu(X)$ has an intuitive statistical interpretation that may be of\nindependent interest. For data sets with bounded $\\mu(X)$-complexity, we show\nthat a novel sensitivity sampling scheme produces the first provably sublinear\n$(1\\pm\\varepsilon)$-coreset. We illustrate the performance of our method by\ncomparing to uniform sampling as well as to state of the art methods in the\narea. The experiments are conducted on real world benchmark data for logistic\nregression. \n\n"}
{"id": "1805.08610", "contents": "Title: Optimization, fast and slow: optimally switching between local and\n  Bayesian optimization Abstract: We develop the first Bayesian Optimization algorithm, BLOSSOM, which selects\nbetween multiple alternative acquisition functions and traditional local\noptimization at each step. This is combined with a novel stopping condition\nbased on expected regret. This pairing allows us to obtain the best\ncharacteristics of both local and Bayesian optimization, making efficient use\nof function evaluations while yielding superior convergence to the global\nminimum on a selection of optimization problems, and also halting optimization\nonce a principled and intuitive stopping condition has been fulfilled. \n\n"}
{"id": "1805.08704", "contents": "Title: Replicating Active Appearance Model by Generator Network Abstract: A recent Cell paper [Chang and Tsao, 2017] reports an interesting discovery.\nFor the face stimuli generated by a pre-trained active appearance model (AAM),\nthe responses of neurons in the areas of the primate brain that are responsible\nfor face recognition exhibit strong linear relationship with the shape\nvariables and appearance variables of the AAM that generates the face stimuli.\nIn this paper, we show that this behavior can be replicated by a deep\ngenerative model called the generator network, which assumes that the observed\nsignals are generated by latent random variables via a top-down convolutional\nneural network. Specifically, we learn the generator network from the face\nimages generated by a pre-trained AAM model using variational auto-encoder, and\nwe show that the inferred latent variables of the learned generator network\nhave strong linear relationship with the shape and appearance variables of the\nAAM model that generates the face images. Unlike the AAM model that has an\nexplicit shape model where the shape variables generate the control points or\nlandmarks, the generator network has no such shape model and shape variables.\nYet the generator network can learn the shape knowledge in the sense that some\nof the latent variables of the learned generator network capture the shape\nvariations in the face images generated by AAM. \n\n"}
{"id": "1805.08974", "contents": "Title: Do Better ImageNet Models Transfer Better? Abstract: Transfer learning is a cornerstone of computer vision, yet little work has\nbeen done to evaluate the relationship between architecture and transfer. An\nimplicit hypothesis in modern computer vision research is that models that\nperform better on ImageNet necessarily perform better on other vision tasks.\nHowever, this hypothesis has never been systematically tested. Here, we compare\nthe performance of 16 classification networks on 12 image classification\ndatasets. We find that, when networks are used as fixed feature extractors or\nfine-tuned, there is a strong correlation between ImageNet accuracy and\ntransfer accuracy ($r = 0.99$ and $0.96$, respectively). In the former setting,\nwe find that this relationship is very sensitive to the way in which networks\nare trained on ImageNet; many common forms of regularization slightly improve\nImageNet accuracy but yield penultimate layer features that are much worse for\ntransfer learning. Additionally, we find that, on two small fine-grained image\nclassification datasets, pretraining on ImageNet provides minimal benefits,\nindicating the learned features from ImageNet do not transfer well to\nfine-grained tasks. Together, our results show that ImageNet architectures\ngeneralize well across datasets, but ImageNet features are less general than\npreviously suggested. \n\n"}
{"id": "1805.08975", "contents": "Title: Particle Filter Networks with Application to Visual Localization Abstract: Particle filtering is a powerful approach to sequential state estimation and\nfinds application in many domains, including robot localization, object\ntracking, etc. To apply particle filtering in practice, a critical challenge is\nto construct probabilistic system models, especially for systems with complex\ndynamics or rich sensory inputs such as camera images. This paper introduces\nthe Particle Filter Network (PFnet), which encodes both a system model and a\nparticle filter algorithm in a single neural network. The PF-net is fully\ndifferentiable and trained end-to-end from data. Instead of learning a generic\nsystem model, it learns a model optimized for the particle filter algorithm. We\napply the PF-net to a visual localization task, in which a robot must localize\nin a rich 3-D world, using only a schematic 2-D floor map. In simulation\nexperiments, PF-net consistently outperforms alternative learning\narchitectures, as well as a traditional model-based method, under a variety of\nsensor inputs. Further, PF-net generalizes well to new, unseen environments. \n\n"}
{"id": "1805.09039", "contents": "Title: Amortized Context Vector Inference for Sequence-to-Sequence Networks Abstract: Neural attention (NA) has become a key component of sequence-to-sequence\nmodels that yield state-of-the-art performance in as hard tasks as abstractive\ndocument summarization (ADS) and video captioning (VC). NA mechanisms perform\ninference of context vectors; these constitute weighted sums of deterministic\ninput sequence encodings, adaptively sourced over long temporal horizons.\nInspired from recent work in the field of amortized variational inference\n(AVI), in this work we consider treating the context vectors generated by\nsoft-attention (SA) models as latent variables, with approximate finite mixture\nmodel posteriors inferred via AVI. We posit that this formulation may yield\nstronger generalization capacity, in line with the outcomes of existing\napplications of AVI to deep networks. To illustrate our method, we implement it\nand experimentally evaluate it considering challenging ADS, VC, and MT\nbenchmarks. This way, we exhibit its improved effectiveness over\nstate-of-the-art alternatives. \n\n"}
{"id": "1805.09044", "contents": "Title: Representation Balancing MDPs for Off-Policy Policy Evaluation Abstract: We study the problem of off-policy policy evaluation (OPPE) in RL. In\ncontrast to prior work, we consider how to estimate both the individual policy\nvalue and average policy value accurately. We draw inspiration from recent work\nin causal reasoning, and propose a new finite sample generalization error bound\nfor value estimates from MDP models. Using this upper bound as an objective, we\ndevelop a learning algorithm of an MDP model with a balanced representation,\nand show that our approach can yield substantially lower MSE in common\nsynthetic benchmarks and a HIV treatment simulation domain. \n\n"}
{"id": "1805.09156", "contents": "Title: Matrix Co-completion for Multi-label Classification with Missing\n  Features and Labels Abstract: We consider a challenging multi-label classification problem where both\nfeature matrix $\\X$ and label matrix $\\Y$ have missing entries. An existing\nmethod concatenated $\\X$ and $\\Y$ as $[\\X; \\Y]$ and applied a matrix completion\n(MC) method to fill the missing entries, under the assumption that $[\\X; \\Y]$\nis of low-rank. However, since entries of $\\Y$ take binary values in the\nmulti-label setting, it is unlikely that $\\Y$ is of low-rank. Moreover, such\nassumption implies a linear relationship between $\\X$ and $\\Y$ which may not\nhold in practice. In this paper, we consider a latent matrix $\\Z$ that produces\nthe probability $\\sigma(Z_{ij})$ of generating label $Y_{ij}$, where\n$\\sigma(\\cdot)$ is nonlinear. Considering label correlation, we assume $[\\X;\n\\Z]$ is of low-rank, and propose an MC algorithm based on subgradient descent\nnamed co-completion (COCO) motivated by elastic net and one-bit MC. We give a\ntheoretical bound on the recovery effect of COCO and demonstrate its practical\nusefulness through experiments. \n\n"}
{"id": "1805.09293", "contents": "Title: Learning to Optimize Contextually Constrained Problems for Real-Time\n  Decision-Generation Abstract: The topic of learning to solve optimization problems has received interest\nfrom both the operations research and machine learning communities. In this\nwork, we combine techniques from both fields to address the problem of learning\nto generate decisions to instances of continuous optimization problems where\nthe feasible set varies with contextual features. We propose a novel framework\nfor training a generative model to estimate optimal decisions by combining\ninterior point methods and adversarial learning, which we further embed within\nan data generation algorithm. Decisions generated by our model satisfy\nin-sample and out-of-sample optimality guarantees. Finally, we investigate case\nstudies in portfolio optimization and personalized treatment design,\ndemonstrating that our approach yields advantages over predict-then-optimize\nand supervised deep learning techniques, respectively. \n\n"}
{"id": "1805.09501", "contents": "Title: AutoAugment: Learning Augmentation Policies from Data Abstract: Data augmentation is an effective technique for improving the accuracy of\nmodern image classifiers. However, current data augmentation implementations\nare manually designed. In this paper, we describe a simple procedure called\nAutoAugment to automatically search for improved data augmentation policies. In\nour implementation, we have designed a search space where a policy consists of\nmany sub-policies, one of which is randomly chosen for each image in each\nmini-batch. A sub-policy consists of two operations, each operation being an\nimage processing function such as translation, rotation, or shearing, and the\nprobabilities and magnitudes with which the functions are applied. We use a\nsearch algorithm to find the best policy such that the neural network yields\nthe highest validation accuracy on a target dataset. Our method achieves\nstate-of-the-art accuracy on CIFAR-10, CIFAR-100, SVHN, and ImageNet (without\nadditional data). On ImageNet, we attain a Top-1 accuracy of 83.5% which is\n0.4% better than the previous record of 83.1%. On CIFAR-10, we achieve an error\nrate of 1.5%, which is 0.6% better than the previous state-of-the-art.\nAugmentation policies we find are transferable between datasets. The policy\nlearned on ImageNet transfers well to achieve significant improvements on other\ndatasets, such as Oxford Flowers, Caltech-101, Oxford-IIT Pets, FGVC Aircraft,\nand Stanford Cars. \n\n"}
{"id": "1805.09781", "contents": "Title: Efficient Inference in Multi-task Cox Process Models Abstract: We generalize the log Gaussian Cox process (LGCP) framework to model multiple\ncorrelated point data jointly. The observations are treated as realizations of\nmultiple LGCPs, whose log intensities are given by linear combinations of\nlatent functions drawn from Gaussian process priors. The combination\ncoefficients are also drawn from Gaussian processes and can incorporate\nadditional dependencies. We derive closed-form expressions for the moments of\nthe intensity functions and develop an efficient variational inference\nalgorithm that is orders of magnitude faster than competing deterministic and\nstochastic approximations of multivariate LGCP, coregionalization models, and\nmulti-task permanental processes. Our approach outperforms these benchmarks in\nmultiple problems, offering the current state of the art in modeling\nmultivariate point processes. \n\n"}
{"id": "1805.10309", "contents": "Title: Learning Self-Imitating Diverse Policies Abstract: The success of popular algorithms for deep reinforcement learning, such as\npolicy-gradients and Q-learning, relies heavily on the availability of an\ninformative reward signal at each timestep of the sequential decision-making\nprocess. When rewards are only sparsely available during an episode, or a\nrewarding feedback is provided only after episode termination, these algorithms\nperform sub-optimally due to the difficultly in credit assignment.\nAlternatively, trajectory-based policy optimization methods, such as\ncross-entropy method and evolution strategies, do not require per-timestep\nrewards, but have been found to suffer from high sample complexity by\ncompleting forgoing the temporal nature of the problem. Improving the\nefficiency of RL algorithms in real-world problems with sparse or episodic\nrewards is therefore a pressing need. In this work, we introduce a\nself-imitation learning algorithm that exploits and explores well in the sparse\nand episodic reward settings. We view each policy as a state-action visitation\ndistribution and formulate policy optimization as a divergence minimization\nproblem. We show that with Jensen-Shannon divergence, this divergence\nminimization problem can be reduced into a policy-gradient algorithm with\nshaped rewards learned from experience replays. Experimental results indicate\nthat our algorithm works comparable to existing algorithms in environments with\ndense rewards, and significantly better in environments with sparse and\nepisodic rewards. We then discuss limitations of self-imitation learning, and\npropose to solve them by using Stein variational policy gradient descent with\nthe Jensen-Shannon kernel to learn multiple diverse policies. We demonstrate\nits effectiveness on a challenging variant of continuous-control MuJoCo\nlocomotion tasks. \n\n"}
{"id": "1805.10927", "contents": "Title: Scalable and Robust Community Detection with Randomized Sketching Abstract: This article explores and analyzes the unsupervised clustering of large\npartially observed graphs. We propose a scalable and provable randomized\nframework for clustering graphs generated from the stochastic block model. The\nclustering is first applied to a sub-matrix of the graph's adjacency matrix\nassociated with a reduced graph sketch constructed using random sampling. Then,\nthe clusters of the full graph are inferred based on the clusters extracted\nfrom the sketch using a correlation-based retrieval step. Uniform random node\nsampling is shown to improve the computational complexity over clustering of\nthe full graph when the cluster sizes are balanced. A new random degree-based\nnode sampling algorithm is presented which significantly improves upon the\nperformance of the clustering algorithm even when clusters are unbalanced. This\nframework improves the phase transitions for matrix-decomposition-based\nclustering with regard to computational complexity and minimum cluster size,\nwhich are shown to be nearly dimension-free in the low inter-cluster\nconnectivity regime. A third sampling technique is shown to improve balance by\nrandomly sampling nodes based on spatial distribution. We provide analysis and\nnumerical results using a convex clustering algorithm based on matrix\ncompletion. \n\n"}
{"id": "1805.10939", "contents": "Title: Optimal ridge penalty for real-world high-dimensional data can be zero\n  or negative due to the implicit ridge regularization Abstract: A conventional wisdom in statistical learning is that large models require\nstrong regularization to prevent overfitting. Here we show that this rule can\nbe violated by linear regression in the underdetermined $n\\ll p$ situation\nunder realistic conditions. Using simulations and real-life high-dimensional\ndata sets, we demonstrate that an explicit positive ridge penalty can fail to\nprovide any improvement over the minimum-norm least squares estimator.\nMoreover, the optimal value of ridge penalty in this situation can be negative.\nThis happens when the high-variance directions in the predictor space can\npredict the response variable, which is often the case in the real-world\nhigh-dimensional data. In this regime, low-variance directions provide an\nimplicit ridge regularization and can make any further positive ridge penalty\ndetrimental. We prove that augmenting any linear model with random covariates\nand using minimum-norm estimator is asymptotically equivalent to adding the\nridge penalty. We use a spiked covariance model as an analytically tractable\nexample and prove that the optimal ridge penalty in this case is negative when\n$n\\ll p$. \n\n"}
{"id": "1805.11182", "contents": "Title: GESF: A Universal Discriminative Mapping Mechanism for Graph\n  Representation Learning Abstract: Graph embedding is a central problem in social network analysis and many\nother applications, aiming to learn the vector representation for each node.\nWhile most existing approaches need to specify the neighborhood and the\ndependence form to the neighborhood, which may significantly degrades the\nflexibility of representation, we propose a novel graph node embedding method\n(namely GESF) via the set function technique. Our method can 1) learn an\narbitrary form of representation function from neighborhood, 2) automatically\ndecide the significance of neighbors at different distances, and 3) be applied\nto heterogeneous graph embedding, which may contain multiple types of nodes.\nTheoretical guarantee for the representation capability of our method has been\nproved for general homogeneous and heterogeneous graphs and evaluation results\non benchmark data sets show that the proposed GESF outperforms the\nstate-of-the-art approaches on producing node vectors for classification tasks. \n\n"}
{"id": "1805.11191", "contents": "Title: Learning From Less Data: Diversified Subset Selection and Active\n  Learning in Image Classification Tasks Abstract: Supervised machine learning based state-of-the-art computer vision techniques\nare in general data hungry and pose the challenges of not having adequate\ncomputing resources and of high costs involved in human labeling efforts.\nTraining data subset selection and active learning techniques have been\nproposed as possible solutions to these challenges respectively. A special\nclass of subset selection functions naturally model notions of diversity,\ncoverage and representation and they can be used to eliminate redundancy and\nthus lend themselves well for training data subset selection. They can also\nhelp improve the efficiency of active learning in further reducing human\nlabeling efforts by selecting a subset of the examples obtained using the\nconventional uncertainty sampling based techniques. In this work we empirically\ndemonstrate the effectiveness of two diversity models, namely the\nFacility-Location and Disparity-Min models for training-data subset selection\nand reducing labeling effort. We do this for a variety of computer vision tasks\nincluding Gender Recognition, Scene Recognition and Object Recognition. Our\nresults show that subset selection done in the right way can add 2-3% in\naccuracy on existing baselines, particularly in the case of less training data.\nThis allows the training of complex machine learning models (like Convolutional\nNeural Networks) with much less training data while incurring minimal\nperformance loss. \n\n"}
{"id": "1805.11504", "contents": "Title: Capturing Variabilities from Computed Tomography Images with Generative\n  Adversarial Networks Abstract: With the advent of Deep Learning (DL) techniques, especially Generative\nAdversarial Networks (GANs), data augmentation and generation are quickly\nevolving domains that have raised much interest recently. However, the DL\ntechniques are data demanding and since, medical data is not easily accessible,\nthey suffer from data insufficiency. To deal with this limitation, different\ndata augmentation techniques are used. Here, we propose a novel unsupervised\ndata-driven approach for data augmentation that can generate 2D Computed\nTomography (CT) images using a simple GAN. The generated CT images have good\nglobal and local features of a real CT image and can be used to augment the\ntraining datasets for effective learning. In this proof-of-concept study, we\nshow that our proposed solution using GANs is able to capture some of the\nglobal and local CT variabilities. Our network is able to generate visually\nrealistic CT images and we aim to further enhance its output by scaling it to a\nhigher resolution and potentially from 2D to 3D. \n\n"}
{"id": "1805.11640", "contents": "Title: K-Beam Minimax: Efficient Optimization for Deep Adversarial Learning Abstract: Minimax optimization plays a key role in adversarial training of machine\nlearning algorithms, such as learning generative models, domain adaptation,\nprivacy preservation, and robust learning. In this paper, we demonstrate the\nfailure of alternating gradient descent in minimax optimization problems due to\nthe discontinuity of solutions of the inner maximization. To address this, we\npropose a new epsilon-subgradient descent algorithm that addresses this problem\nby simultaneously tracking K candidate solutions. Practically, the algorithm\ncan find solutions that previous saddle-point algorithms cannot find, with only\na sublinear increase of complexity in K. We analyze the conditions under which\nthe algorithm converges to the true solution in detail. A significant\nimprovement in stability and convergence speed of the algorithm is observed in\nsimple representative problems, GAN training, and domain-adaptation problems. \n\n"}
{"id": "1805.11916", "contents": "Title: On the Spectrum of Random Features Maps of High Dimensional Data Abstract: Random feature maps are ubiquitous in modern statistical machine learning,\nwhere they generalize random projections by means of powerful, yet often\ndifficult to analyze nonlinear operators. In this paper, we leverage the\n\"concentration\" phenomenon induced by random matrix theory to perform a\nspectral analysis on the Gram matrix of these random feature maps, here for\nGaussian mixture models of simultaneously large dimension and size. Our results\nare instrumental to a deeper understanding on the interplay of the nonlinearity\nand the statistics of the data, thereby allowing for a better tuning of random\nfeature-based techniques. \n\n"}
{"id": "1805.12218", "contents": "Title: Convolutional Embedded Networks for Population Scale Clustering and\n  Bio-ancestry Inferencing Abstract: The study of genetic variants can help find correlating population groups to\nidentify cohorts that are predisposed to common diseases and explain\ndifferences in disease susceptibility and how patients react to drugs. Machine\nlearning algorithms are increasingly being applied to identify interacting GVs\nto understand their complex phenotypic traits. Since the performance of a\nlearning algorithm not only depends on the size and nature of the data but also\non the quality of underlying representation, deep neural networks can learn\nnon-linear mappings that allow transforming GVs data into more clustering and\nclassification friendly representations than manual feature selection. In this\npaper, we proposed convolutional embedded networks in which we combine two DNN\narchitectures called convolutional embedded clustering and convolutional\nautoencoder classifier for clustering individuals and predicting geographic\nethnicity based on GVs, respectively. We employed CAE-based representation\nlearning on 95 million GVs from the 1000 genomes and Simons genome diversity\nprojects. Quantitative and qualitative analyses with a focus on accuracy and\nscalability show that our approach outperforms state-of-the-art approaches such\nas VariantSpark and ADMIXTURE. In particular, CEC can cluster targeted\npopulation groups in 22 hours with an adjusted rand index of 0.915, the\nnormalized mutual information of 0.92, and the clustering accuracy of 89%.\nContrarily, the CAE classifier can predict the geographic ethnicity of unknown\nsamples with an F1 and Mathews correlation coefficient(MCC) score of 0.9004 and\n0.8245, respectively. To provide interpretations of the predictions, we\nidentify significant biomarkers using gradient boosted trees(GBT) and SHAP.\nOverall, our approach is transparent and faster than the baseline methods, and\nscalable for 5% to 100% of the full human genome. \n\n"}
{"id": "1805.12243", "contents": "Title: Novel Video Prediction for Large-scale Scene using Optical Flow Abstract: Making predictions of future frames is a critical challenge in autonomous\ndriving research. Most of the existing methods for video prediction attempt to\ngenerate future frames in simple and fixed scenes. In this paper, we propose a\nnovel and effective optical flow conditioned method for the task of video\nprediction with an application to complex urban scenes. In contrast with\nprevious work, the prediction model only requires video sequences and optical\nflow sequences for training and testing. Our method uses the rich\nspatial-temporal features in video sequences. The method takes advantage of the\nmotion information extracting from optical flow maps between neighbor images as\nwell as previous images. Empirical evaluations on the KITTI dataset and the\nCityscapes dataset demonstrate the effectiveness of our method. \n\n"}
{"id": "1806.00007", "contents": "Title: Multi-Layered Gradient Boosting Decision Trees Abstract: Multi-layered representation is believed to be the key ingredient of deep\nneural networks especially in cognitive tasks like computer vision. While\nnon-differentiable models such as gradient boosting decision trees (GBDTs) are\nthe dominant methods for modeling discrete or tabular data, they are hard to\nincorporate with such representation learning ability. In this work, we propose\nthe multi-layered GBDT forest (mGBDTs), with an explicit emphasis on exploring\nthe ability to learn hierarchical representations by stacking several layers of\nregression GBDTs as its building block. The model can be jointly trained by a\nvariant of target propagation across layers, without the need to derive\nback-propagation nor differentiability. Experiments and visualizations\nconfirmed the effectiveness of the model in terms of performance and\nrepresentation learning ability. \n\n"}
{"id": "1806.00064", "contents": "Title: Efficient Low-rank Multimodal Fusion with Modality-Specific Factors Abstract: Multimodal research is an emerging field of artificial intelligence, and one\nof the main research problems in this field is multimodal fusion. The fusion of\nmultimodal data is the process of integrating multiple unimodal representations\ninto one compact multimodal representation. Previous research in this field has\nexploited the expressiveness of tensors for multimodal representation. However,\nthese methods often suffer from exponential increase in dimensions and in\ncomputational complexity introduced by transformation of input into tensor. In\nthis paper, we propose the Low-rank Multimodal Fusion method, which performs\nmultimodal fusion using low-rank tensors to improve efficiency. We evaluate our\nmodel on three different tasks: multimodal sentiment analysis, speaker trait\nanalysis, and emotion recognition. Our model achieves competitive results on\nall these tasks while drastically reducing computational complexity. Additional\nexperiments also show that our model can perform robustly for a wide range of\nlow-rank settings, and is indeed much more efficient in both training and\ninference compared to other methods that utilize tensor representations. \n\n"}
{"id": "1806.00088", "contents": "Title: PeerNets: Exploiting Peer Wisdom Against Adversarial Attacks Abstract: Deep learning systems have become ubiquitous in many aspects of our lives.\nUnfortunately, it has been shown that such systems are vulnerable to\nadversarial attacks, making them prone to potential unlawful uses. Designing\ndeep neural networks that are robust to adversarial attacks is a fundamental\nstep in making such systems safer and deployable in a broader variety of\napplications (e.g. autonomous driving), but more importantly is a necessary\nstep to design novel and more advanced architectures built on new computational\nparadigms rather than marginally building on the existing ones. In this paper\nwe introduce PeerNets, a novel family of convolutional networks alternating\nclassical Euclidean convolutions with graph convolutions to harness information\nfrom a graph of peer samples. This results in a form of non-local forward\npropagation in the model, where latent features are conditioned on the global\nstructure induced by the graph, that is up to 3 times more robust to a variety\nof white- and black-box adversarial attacks compared to conventional\narchitectures with almost no drop in accuracy. \n\n"}
{"id": "1806.00176", "contents": "Title: Reparameterization Gradient for Non-differentiable Models Abstract: We present a new algorithm for stochastic variational inference that targets\nat models with non-differentiable densities. One of the key challenges in\nstochastic variational inference is to come up with a low-variance estimator of\nthe gradient of a variational objective. We tackle the challenge by\ngeneralizing the reparameterization trick, one of the most effective techniques\nfor addressing the variance issue for differentiable models, so that the trick\nworks for non-differentiable models as well. Our algorithm splits the space of\nlatent variables into regions where the density of the variables is\ndifferentiable, and their boundaries where the density may fail to be\ndifferentiable. For each differentiable region, the algorithm applies the\nstandard reparameterization trick and estimates the gradient restricted to the\nregion. For each potentially non-differentiable boundary, it uses a form of\nmanifold sampling and computes the direction for variational parameters that,\nif followed, would increase the boundary's contribution to the variational\nobjective. The sum of all the estimates becomes the gradient estimate of our\nalgorithm. Our estimator enjoys the reduced variance of the reparameterization\ngradient while remaining unbiased even for non-differentiable models. The\nexperiments with our preliminary implementation confirm the benefit of reduced\nvariance and unbiasedness. \n\n"}
{"id": "1806.00428", "contents": "Title: A Classification approach towards Unsupervised Learning of Visual\n  Representations Abstract: In this paper, we present a technique for unsupervised learning of visual\nrepresentations. Specifically, we train a model for foreground and background\nclassification task, in the process of which it learns visual representations.\nForeground and background patches for training come af- ter mining for such\npatches from hundreds and thousands of unlabelled videos available on the web\nwhich we ex- tract using a proposed patch extraction algorithm. With- out using\nany supervision, with just using 150, 000 unla- belled videos and the PASCAL\nVOC 2007 dataset, we train a object recognition model that achieves 45.3 mAP\nwhich is close to the best performing unsupervised feature learn- ing technique\nwhereas better than many other proposed al- gorithms. The code for patch\nextraction is implemented in Matlab and available open source at the following\nlink . \n\n"}
{"id": "1806.00512", "contents": "Title: Structurally Sparsified Backward Propagation for Faster Long Short-Term\n  Memory Training Abstract: Exploiting sparsity enables hardware systems to run neural networks faster\nand more energy-efficiently. However, most prior sparsity-centric optimization\ntechniques only accelerate the forward pass of neural networks and usually\nrequire an even longer training process with iterative pruning and retraining.\nWe observe that artificially inducing sparsity in the gradients of the gates in\nan LSTM cell has little impact on the training quality. Further, we can enforce\nstructured sparsity in the gate gradients to make the LSTM backward pass up to\n45% faster than the state-of-the-art dense approach and 168% faster than the\nstate-of-the-art sparsifying method on modern GPUs. Though the structured\nsparsifying method can impact the accuracy of a model, this performance gap can\nbe eliminated by mixing our sparse training method and the standard dense\ntraining method. Experimental results show that the mixed method can achieve\ncomparable results in a shorter time span than using purely dense training. \n\n"}
{"id": "1806.00589", "contents": "Title: Efficient Entropy for Policy Gradient with Multidimensional Action Space Abstract: In recent years, deep reinforcement learning has been shown to be adept at\nsolving sequential decision processes with high-dimensional state spaces such\nas in the Atari games. Many reinforcement learning problems, however, involve\nhigh-dimensional discrete action spaces as well as high-dimensional state\nspaces. This paper considers entropy bonus, which is used to encourage\nexploration in policy gradient. In the case of high-dimensional action spaces,\ncalculating the entropy and its gradient requires enumerating all the actions\nin the action space and running forward and backpropagation for each action,\nwhich may be computationally infeasible. We develop several novel unbiased\nestimators for the entropy bonus and its gradient. We apply these estimators to\nseveral models for the parameterized policies, including Independent Sampling,\nCommNet, Autoregressive with Modified MDP, and Autoregressive with LSTM.\nFinally, we test our algorithms on two environments: a multi-hunter\nmulti-rabbit grid game and a multi-agent multi-arm bandit problem. The results\nshow that our entropy estimators substantially improve performance with\nmarginal additional computational cost. \n\n"}
{"id": "1806.00949", "contents": "Title: Private PAC learning implies finite Littlestone dimension Abstract: We show that every approximately differentially private learning algorithm\n(possibly improper) for a class $H$ with Littlestone dimension~$d$ requires\n$\\Omega\\bigl(\\log^*(d)\\bigr)$ examples. As a corollary it follows that the\nclass of thresholds over $\\mathbb{N}$ can not be learned in a private manner;\nthis resolves open question due to [Bun et al., 2015, Feldman and Xiao, 2015].\nWe leave as an open question whether every class with a finite Littlestone\ndimension can be learned by an approximately differentially private algorithm. \n\n"}
{"id": "1806.01182", "contents": "Title: Online Reciprocal Recommendation with Theoretical Performance Guarantees Abstract: A reciprocal recommendation problem is one where the goal of learning is not\njust to predict a user's preference towards a passive item (e.g., a book), but\nto recommend the targeted user on one side another user from the other side\nsuch that a mutual interest between the two exists. The problem thus is sharply\ndifferent from the more traditional items-to-users recommendation, since a good\nmatch requires meeting the preferences of both users. We initiate a rigorous\ntheoretical investigation of the reciprocal recommendation task in a specific\nframework of sequential learning. We point out general limitations, formulate\nreasonable assumptions enabling effective learning and, under these\nassumptions, we design and analyze a computationally efficient algorithm that\nuncovers mutual likes at a pace comparable to those achieved by a clearvoyant\nalgorithm knowing all user preferences in advance. Finally, we validate our\nalgorithm against synthetic and real-world datasets, showing improved empirical\nperformance over simple baselines. \n\n"}
{"id": "1806.01427", "contents": "Title: Analysis of DAWNBench, a Time-to-Accuracy Machine Learning Performance\n  Benchmark Abstract: Researchers have proposed hardware, software, and algorithmic optimizations\nto improve the computational performance of deep learning. While some of these\noptimizations perform the same operations faster (e.g., increasing GPU clock\nspeed), many others modify the semantics of the training procedure (e.g.,\nreduced precision), and can impact the final model's accuracy on unseen data.\nDue to a lack of standard evaluation criteria that considers these trade-offs,\nit is difficult to directly compare these optimizations. To address this\nproblem, we recently introduced DAWNBench, a benchmark competition focused on\nend-to-end training time to achieve near-state-of-the-art accuracy on an unseen\ndataset---a combined metric called time-to-accuracy (TTA). In this work, we\nanalyze the entries from DAWNBench, which received optimized submissions from\nmultiple industrial groups, to investigate the behavior of TTA as a metric as\nwell as trends in the best-performing entries. We show that TTA has a low\ncoefficient of variation and that models optimized for TTA generalize nearly as\nwell as those trained using standard methods. Additionally, even though\nDAWNBench entries were able to train ImageNet models in under 3 minutes, we\nfind they still underutilize hardware capabilities such as Tensor Cores.\nFurthermore, we find that distributed entries can spend more than half of their\ntime on communication. We show similar findings with entries to the MLPERF v0.5\nbenchmark. \n\n"}
{"id": "1806.01793", "contents": "Title: Gradient-based Filter Design for the Dual-tree Wavelet Transform Abstract: The wavelet transform has seen success when incorporated into neural network\narchitectures, such as in wavelet scattering networks. More recently, it has\nbeen shown that the dual-tree complex wavelet transform can provide better\nrepresentations than the standard transform. With this in mind, we extend our\nprevious method for learning filters for the 1D and 2D wavelet transforms into\nthe dual-tree domain. We show that with few modifications to our original\nmodel, we can learn directional filters that leverage the properties of the\ndual-tree wavelet transform. \n\n"}
{"id": "1806.01822", "contents": "Title: Relational recurrent neural networks Abstract: Memory-based neural networks model temporal data by leveraging an ability to\nremember information for long periods. It is unclear, however, whether they\nalso have an ability to perform complex relational reasoning with the\ninformation they remember. Here, we first confirm our intuitions that standard\nmemory architectures may struggle at tasks that heavily involve an\nunderstanding of the ways in which entities are connected -- i.e., tasks\ninvolving relational reasoning. We then improve upon these deficits by using a\nnew memory module -- a \\textit{Relational Memory Core} (RMC) -- which employs\nmulti-head dot product attention to allow memories to interact. Finally, we\ntest the RMC on a suite of tasks that may profit from more capable relational\nreasoning across sequential information, and show large gains in RL domains\n(e.g. Mini PacMan), program evaluation, and language modeling, achieving\nstate-of-the-art results on the WikiText-103, Project Gutenberg, and GigaWord\ndatasets. \n\n"}
{"id": "1806.02046", "contents": "Title: Implicit regularization and solution uniqueness in over-parameterized\n  matrix sensing Abstract: We consider whether algorithmic choices in over-parameterized linear matrix\nfactorization introduce implicit regularization. We focus on noiseless matrix\nsensing over rank-$r$ positive semi-definite (PSD) matrices in $\\mathbb{R}^{n\n\\times n}$, with a sensing mechanism that satisfies restricted isometry\nproperties (RIP). The algorithm we study is \\emph{factored gradient descent},\nwhere we model the low-rankness and PSD constraints with the factorization\n$UU^\\top$, for $U \\in \\mathbb{R}^{n \\times r}$. Surprisingly, recent work\nargues that the choice of $r \\leq n$ is not pivotal: even setting $U \\in\n\\mathbb{R}^{n \\times n}$ is sufficient for factored gradient descent to find\nthe rank-$r$ solution, which suggests that operating over the factors leads to\nan implicit regularization. In this contribution, we provide a different\nperspective to the problem of implicit regularization. We show that under\ncertain conditions, the PSD constraint by itself is sufficient to lead to a\nunique rank-$r$ matrix recovery, without implicit or explicit low-rank\nregularization. \\emph{I.e.}, under assumptions, the set of PSD matrices, that\nare consistent with the observed data, is a singleton, regardless of the\nalgorithm used. \n\n"}
{"id": "1806.02389", "contents": "Title: Not All Attributes are Created Equal: $d_{\\mathcal{X}}$-Private\n  Mechanisms for Linear Queries Abstract: Differential privacy provides strong privacy guarantees simultaneously\nenabling useful insights from sensitive datasets. However, it provides the same\nlevel of protection for all elements (individuals and attributes) in the data.\nThere are practical scenarios where some data attributes need more/less\nprotection than others. In this paper, we consider $d_{\\mathcal{X}}$-privacy,\nan instantiation of the privacy notion introduced in\n\\cite{chatzikokolakis2013broadening}, which allows this flexibility by\nspecifying a separate privacy budget for each pair of elements in the data\ndomain. We describe a systematic procedure to tailor any existing\ndifferentially private mechanism that assumes a query set and a sensitivity\nvector as input into its $d_{\\mathcal{X}}$-private variant, specifically\nfocusing on linear queries. Our proposed meta procedure has broad applications\nas linear queries form the basis of a range of data analysis and machine\nlearning algorithms, and the ability to define a more flexible privacy budget\nacross the data domain results in improved privacy/utility tradeoff in these\napplications. We propose several $d_{\\mathcal{X}}$-private mechanisms, and\nprovide theoretical guarantees on the trade-off between utility and privacy. We\nalso experimentally demonstrate the effectiveness of our procedure, by\nevaluating our proposed $d_{\\mathcal{X}}$-private Laplace mechanism on both\nsynthetic and real datasets using a set of randomly generated linear queries. \n\n"}
{"id": "1806.02450", "contents": "Title: A Finite Time Analysis of Temporal Difference Learning With Linear\n  Function Approximation Abstract: Temporal difference learning (TD) is a simple iterative algorithm used to\nestimate the value function corresponding to a given policy in a Markov\ndecision process. Although TD is one of the most widely used algorithms in\nreinforcement learning, its theoretical analysis has proved challenging and few\nguarantees on its statistical efficiency are available. In this work, we\nprovide a simple and explicit finite time analysis of temporal difference\nlearning with linear function approximation. Except for a few key insights, our\nanalysis mirrors standard techniques for analyzing stochastic gradient descent\nalgorithms, and therefore inherits the simplicity and elegance of that\nliterature. Final sections of the paper show how all of our main results extend\nto the study of TD learning with eligibility traces, known as TD($\\lambda$),\nand to Q-learning applied in high-dimensional optimal stopping problems. \n\n"}
{"id": "1806.02485", "contents": "Title: Stochastic Block Models are a Discrete Surface Tension Abstract: Networks, which represent agents and interactions between them, arise in\nmyriad applications throughout the sciences, engineering, and even the\nhumanities. To understand large-scale structure in a network, a common task is\nto cluster a network's nodes into sets called \"communities\", such that there\nare dense connections within communities but sparse connections between them. A\npopular and statistically principled method to perform such clustering is to\nuse a family of generative models known as stochastic block models (SBMs). In\nthis paper, we show that maximum likelihood estimation in an SBM is a network\nanalog of a well-known continuum surface-tension problem that arises from an\napplication in metallurgy. To illustrate the utility of this relationship, we\nimplement network analogs of three surface-tension algorithms, with which we\nsuccessfully recover planted community structure in synthetic networks and\nwhich yield fascinating insights on empirical networks that we construct from\nhyperspectral videos. \n\n"}
{"id": "1806.02612", "contents": "Title: Dimensionality-Driven Learning with Noisy Labels Abstract: Datasets with significant proportions of noisy (incorrect) class labels\npresent challenges for training accurate Deep Neural Networks (DNNs). We\npropose a new perspective for understanding DNN generalization for such\ndatasets, by investigating the dimensionality of the deep representation\nsubspace of training samples. We show that from a dimensionality perspective,\nDNNs exhibit quite distinctive learning styles when trained with clean labels\nversus when trained with a proportion of noisy labels. Based on this finding,\nwe develop a new dimensionality-driven learning strategy, which monitors the\ndimensionality of subspaces during training and adapts the loss function\naccordingly. We empirically demonstrate that our approach is highly tolerant to\nsignificant proportions of noisy labels, and can effectively learn\nlow-dimensional local subspaces that capture the data distribution. \n\n"}
{"id": "1806.02817", "contents": "Title: Probabilistic Model-Agnostic Meta-Learning Abstract: Meta-learning for few-shot learning entails acquiring a prior over previous\ntasks and experiences, such that new tasks be learned from small amounts of\ndata. However, a critical challenge in few-shot learning is task ambiguity:\neven when a powerful prior can be meta-learned from a large number of prior\ntasks, a small dataset for a new task can simply be too ambiguous to acquire a\nsingle model (e.g., a classifier) for that task that is accurate. In this\npaper, we propose a probabilistic meta-learning algorithm that can sample\nmodels for a new task from a model distribution. Our approach extends\nmodel-agnostic meta-learning, which adapts to new tasks via gradient descent,\nto incorporate a parameter distribution that is trained via a variational lower\nbound. At meta-test time, our algorithm adapts via a simple procedure that\ninjects noise into gradient descent, and at meta-training time, the model is\ntrained such that this stochastic adaptation procedure produces samples from\nthe approximate model posterior. Our experimental results show that our method\ncan sample plausible classifiers and regressors in ambiguous few-shot learning\nproblems. We also show how reasoning about ambiguity can also be used for\ndownstream active learning problems. \n\n"}
{"id": "1806.02901", "contents": "Title: Probabilistic FastText for Multi-Sense Word Embeddings Abstract: We introduce Probabilistic FastText, a new model for word embeddings that can\ncapture multiple word senses, sub-word structure, and uncertainty information.\nIn particular, we represent each word with a Gaussian mixture density, where\nthe mean of a mixture component is given by the sum of n-grams. This\nrepresentation allows the model to share statistical strength across sub-word\nstructures (e.g. Latin roots), producing accurate representations of rare,\nmisspelt, or even unseen words. Moreover, each component of the mixture can\ncapture a different word sense. Probabilistic FastText outperforms both\nFastText, which has no probabilistic model, and dictionary-level probabilistic\nembeddings, which do not incorporate subword structures, on several\nword-similarity benchmarks, including English RareWord and foreign language\ndatasets. We also achieve state-of-art performance on benchmarks that measure\nability to discern different meanings. Thus, the proposed model is the first to\nachieve multi-sense representations while having enriched semantics on rare\nwords. \n\n"}
{"id": "1806.03000", "contents": "Title: Noise-adding Methods of Saliency Map as Series of Higher Order Partial\n  Derivative Abstract: SmoothGrad and VarGrad are techniques that enhance the empirical quality of\nstandard saliency maps by adding noise to input. However, there were few works\nthat provide a rigorous theoretical interpretation of those methods. We\nanalytically formalize the result of these noise-adding methods. As a result,\nwe observe two interesting results from the existing noise-adding methods.\nFirst, SmoothGrad does not make the gradient of the score function smooth.\nSecond, VarGrad is independent of the gradient of the score function. We\nbelieve that our findings provide a clue to reveal the relationship between\nlocal explanation methods of deep neural networks and higher-order partial\nderivatives of the score function. \n\n"}
{"id": "1806.03316", "contents": "Title: Adversarial Meta-Learning Abstract: Meta-learning enables a model to learn from very limited data to undertake a\nnew task. In this paper, we study the general meta-learning with adversarial\nsamples. We present a meta-learning algorithm, ADML (ADversarial Meta-Learner),\nwhich leverages clean and adversarial samples to optimize the initialization of\na learning model in an adversarial manner. ADML leads to the following\ndesirable properties: 1) it turns out to be very effective even in the cases\nwith only clean samples; 2) it is robust to adversarial samples, i.e., unlike\nother meta-learning algorithms, it only leads to a minor performance\ndegradation when there are adversarial samples; 3) it sheds light on tackling\nthe cases with limited and even contaminated samples. It has been shown by\nextensive experimental results that ADML consistently outperforms three\nrepresentative meta-learning algorithms in the cases involving adversarial\nsamples, on two widely-used image datasets, MiniImageNet and CIFAR100, in terms\nof both accuracy and robustness. \n\n"}
{"id": "1806.04245", "contents": "Title: Learning to Speed Up Structured Output Prediction Abstract: Predicting structured outputs can be computationally onerous due to the\ncombinatorially large output spaces. In this paper, we focus on reducing the\nprediction time of a trained black-box structured classifier without losing\naccuracy. To do so, we train a speedup classifier that learns to mimic a\nblack-box classifier under the learning-to-search approach. As the structured\nclassifier predicts more examples, the speedup classifier will operate as a\nlearned heuristic to guide search to favorable regions of the output space. We\npresent a mistake bound for the speedup classifier and identify inference\nsituations where it can independently make correct judgments without input\nfeatures. We evaluate our method on the task of entity and relation extraction\nand show that the speedup classifier outperforms even greedy search in terms of\nspeed without loss of accuracy. \n\n"}
{"id": "1806.04326", "contents": "Title: Differentiable Compositional Kernel Learning for Gaussian Processes Abstract: The generalization properties of Gaussian processes depend heavily on the\nchoice of kernel, and this choice remains a dark art. We present the Neural\nKernel Network (NKN), a flexible family of kernels represented by a neural\nnetwork. The NKN architecture is based on the composition rules for kernels, so\nthat each unit of the network corresponds to a valid kernel. It can compactly\napproximate compositional kernel structures such as those used by the Automatic\nStatistician (Lloyd et al., 2014), but because the architecture is\ndifferentiable, it is end-to-end trainable with gradient-based optimization. We\nshow that the NKN is universal for the class of stationary kernels. Empirically\nwe demonstrate pattern discovery and extrapolation abilities of NKN on several\ntasks that depend crucially on identifying the underlying structure, including\ntime series and texture extrapolation, as well as Bayesian optimization. \n\n"}
{"id": "1806.04339", "contents": "Title: When Will Gradient Methods Converge to Max-margin Classifier under ReLU\n  Models? Abstract: We study the implicit bias of gradient descent methods in solving a binary\nclassification problem over a linearly separable dataset. The classifier is\ndescribed by a nonlinear ReLU model and the objective function adopts the\nexponential loss function. We first characterize the landscape of the loss\nfunction and show that there can exist spurious asymptotic local minima besides\nasymptotic global minima. We then show that gradient descent (GD) can converge\nto either a global or a local max-margin direction, or may diverge from the\ndesired max-margin direction in a general context. For stochastic gradient\ndescent (SGD), we show that it converges in expectation to either the global or\nthe local max-margin direction if SGD converges. We further explore the\nimplicit bias of these algorithms in learning a multi-neuron network under\ncertain stationary conditions, and show that the learned classifier maximizes\nthe margins of each sample pattern partition under the ReLU activation. \n\n"}
{"id": "1806.04562", "contents": "Title: Multi-Agent Deep Reinforcement Learning with Human Strategies Abstract: Deep learning has enabled traditional reinforcement learning methods to deal\nwith high-dimensional problems. However, one of the disadvantages of deep\nreinforcement learning methods is the limited exploration capacity of learning\nagents. In this paper, we introduce an approach that integrates human\nstrategies to increase the exploration capacity of multiple deep reinforcement\nlearning agents. We also report the development of our own multi-agent\nenvironment called Multiple Tank Defence to simulate the proposed approach. The\nresults show the significant performance improvement of multiple agents that\nhave learned cooperatively with human strategies. This implies that there is a\ncritical need for human intellect teamed with machines to solve complex\nproblems. In addition, the success of this simulation indicates that our\nmulti-agent environment can be used as a testbed platform to develop and\nvalidate other multi-agent control algorithms. \n\n"}
{"id": "1806.05009", "contents": "Title: Tree Edit Distance Learning via Adaptive Symbol Embeddings Abstract: Metric learning has the aim to improve classification accuracy by learning a\ndistance measure which brings data points from the same class closer together\nand pushes data points from different classes further apart. Recent research\nhas demonstrated that metric learning approaches can also be applied to trees,\nsuch as molecular structures, abstract syntax trees of computer programs, or\nsyntax trees of natural language, by learning the cost function of an edit\ndistance, i.e. the costs of replacing, deleting, or inserting nodes in a tree.\nHowever, learning such costs directly may yield an edit distance which violates\nmetric axioms, is challenging to interpret, and may not generalize well. In\nthis contribution, we propose a novel metric learning approach for trees which\nwe call embedding edit distance learning (BEDL) and which learns an edit\ndistance indirectly by embedding the tree nodes as vectors, such that the\nEuclidean distance between those vectors supports class discrimination. We\nlearn such embeddings by reducing the distance to prototypical trees from the\nsame class and increasing the distance to prototypical trees from different\nclasses. In our experiments, we show that BEDL improves upon the\nstate-of-the-art in metric learning for trees on six benchmark data sets,\nranging from computer science over biomedical data to a natural-language\nprocessing data set containing over 300,000 nodes. \n\n"}
{"id": "1806.05178", "contents": "Title: Generating Sentences Using a Dynamic Canvas Abstract: We introduce the Attentive Unsupervised Text (W)riter (AUTR), which is a word\nlevel generative model for natural language. It uses a recurrent neural network\nwith a dynamic attention and canvas memory mechanism to iteratively construct\nsentences. By viewing the state of the memory at intermediate stages and where\nthe model is placing its attention, we gain insight into how it constructs\nsentences. We demonstrate that AUTR learns a meaningful latent representation\nfor each sentence, and achieves competitive log-likelihood lower bounds whilst\nbeing computationally efficient. It is effective at generating and\nreconstructing sentences, as well as imputing missing words. \n\n"}
{"id": "1806.05357", "contents": "Title: Deep Multi-Output Forecasting: Learning to Accurately Predict Blood\n  Glucose Trajectories Abstract: In many forecasting applications, it is valuable to predict not only the\nvalue of a signal at a certain time point in the future, but also the values\nleading up to that point. This is especially true in clinical applications,\nwhere the future state of the patient can be less important than the patient's\noverall trajectory. This requires multi-step forecasting, a forecasting variant\nwhere one aims to predict multiple values in the future simultaneously.\nStandard methods to accomplish this can propagate error from prediction to\nprediction, reducing quality over the long term. In light of these challenges,\nwe propose multi-output deep architectures for multi-step forecasting in which\nwe explicitly model the distribution of future values of the signal over a\nprediction horizon. We apply these techniques to the challenging and clinically\nrelevant task of blood glucose forecasting. Through a series of experiments on\na real-world dataset consisting of 550K blood glucose measurements, we\ndemonstrate the effectiveness of our proposed approaches in capturing the\nunderlying signal dynamics. Compared to existing shallow and deep methods, we\nfind that our proposed approaches improve performance individually and capture\ncomplementary information, leading to a large improvement over the baseline\nwhen combined (4.87 vs. 5.31 absolute percentage error (APE)). Overall, the\nresults suggest the efficacy of our proposed approach in predicting blood\nglucose level and multi-step forecasting more generally. \n\n"}
{"id": "1806.05512", "contents": "Title: NetScore: Towards Universal Metrics for Large-scale Performance Analysis\n  of Deep Neural Networks for Practical On-Device Edge Usage Abstract: Much of the focus in the design of deep neural networks has been on improving\naccuracy, leading to more powerful yet highly complex network architectures\nthat are difficult to deploy in practical scenarios, particularly on edge\ndevices such as mobile and other consumer devices given their high\ncomputational and memory requirements. As a result, there has been a recent\ninterest in the design of quantitative metrics for evaluating deep neural\nnetworks that accounts for more than just model accuracy as the sole indicator\nof network performance. In this study, we continue the conversation towards\nuniversal metrics for evaluating the performance of deep neural networks for\npractical on-device edge usage. In particular, we propose a new balanced metric\ncalled NetScore, which is designed specifically to provide a quantitative\nassessment of the balance between accuracy, computational complexity, and\nnetwork architecture complexity of a deep neural network, which is important\nfor on-device edge operation. In what is one of the largest comparative\nanalysis between deep neural networks in literature, the NetScore metric, the\ntop-1 accuracy metric, and the popular information density metric were compared\nacross a diverse set of 60 different deep convolutional neural networks for\nimage classification on the ImageNet Large Scale Visual Recognition Challenge\n(ILSVRC 2012) dataset. The evaluation results across these three metrics for\nthis diverse set of networks are presented in this study to act as a reference\nguide for practitioners in the field. The proposed NetScore metric, along with\nthe other tested metrics, are by no means perfect, but the hope is to push the\nconversation towards better universal metrics for evaluating deep neural\nnetworks for use in practical on-device edge scenarios to help guide\npractitioners in model design for such scenarios. \n\n"}
{"id": "1806.05779", "contents": "Title: Deep Learning Approximation: Zero-Shot Neural Network Speedup Abstract: Neural networks offer high-accuracy solutions to a range of problems, but are\ncostly to run in production systems because of computational and memory\nrequirements during a forward pass. Given a trained network, we propose a\ntechique called Deep Learning Approximation to build a faster network in a tiny\nfraction of the time required for training by only manipulating the network\nstructure and coefficients without requiring re-training or access to the\ntraining data. Speedup is achieved by by applying a sequential series of\nindependent optimizations that reduce the floating-point operations (FLOPs)\nrequired to perform a forward pass. First, lossless optimizations are applied,\nfollowed by lossy approximations using singular value decomposition (SVD) and\nlow-rank matrix decomposition. The optimal approximation is chosen by weighing\nthe relative accuracy loss and FLOP reduction according to a single parameter\nspecified by the user. On PASCAL VOC 2007 with the YOLO network, we show an\nend-to-end 2x speedup in a network forward pass with a 5% drop in mAP that can\nbe re-gained by finetuning. \n\n"}
{"id": "1806.05824", "contents": "Title: Three dimensional Deep Learning approach for remote sensing image\n  classification Abstract: Recently, a variety of approaches has been enriching the field of Remote\nSensing (RS) image processing and analysis. Unfortunately, existing methods\nremain limited faced to the rich spatio-spectral content of today's large\ndatasets. It would seem intriguing to resort to Deep Learning (DL) based\napproaches at this stage with regards to their ability to offer accurate\nsemantic interpretation of the data. However, the specificity introduced by the\ncoexistence of spectral and spatial content in the RS datasets widens the scope\nof the challenges presented to adapt DL methods to these contexts. Therefore,\nthe aim of this paper is firstly to explore the performance of DL architectures\nfor the RS hyperspectral dataset classification and secondly to introduce a new\nthree-dimensional DL approach that enables a joint spectral and spatial\ninformation process. A set of three-dimensional schemes is proposed and\nevaluated. Experimental results based on well knownhyperspectral datasets\ndemonstrate that the proposed method is able to achieve a better classification\nrate than state of the art methods with lower computational costs. \n\n"}
{"id": "1806.06122", "contents": "Title: Fairness Under Composition Abstract: Algorithmic fairness, and in particular the fairness of scoring and\nclassification algorithms, has become a topic of increasing social concern and\nhas recently witnessed an explosion of research in theoretical computer\nscience, machine learning, statistics, the social sciences, and law. Much of\nthe literature considers the case of a single classifier (or scoring function)\nused once, in isolation. In this work, we initiate the study of the fairness\nproperties of systems composed of algorithms that are fair in isolation; that\nis, we study fairness under composition. We identify pitfalls of naive\ncomposition and give general constructions for fair composition, demonstrating\nboth that classifiers that are fair in isolation do not necessarily compose\ninto fair systems and also that seemingly unfair components may be carefully\ncombined to construct fair systems. We focus primarily on the individual\nfairness setting proposed in [Dwork, Hardt, Pitassi, Reingold, Zemel, 2011],\nbut also extend our results to a large class of group fairness definitions\npopular in the recent literature, exhibiting several cases in which group\nfairness definitions give misleading signals under composition. \n\n"}
{"id": "1806.06384", "contents": "Title: Multi-variable LSTM neural network for autoregressive exogenous model Abstract: In this paper, we propose multi-variable LSTM capable of accurate forecasting\nand variable importance interpretation for time series with exogenous\nvariables. Current attention mechanism in recurrent neural networks mostly\nfocuses on the temporal aspect of data and falls short of characterizing\nvariable importance. To this end, the multi-variable LSTM equipped with\ntensorized hidden states is developed to learn hidden states for individual\nvariables, which give rise to our mixture temporal and variable attention.\nBased on such attention mechanism, we infer and quantify variable importance.\nExtensive experiments using real datasets with Granger-causality test and the\nsynthetic dataset with ground truth demonstrate the prediction performance and\ninterpretability of multi-variable LSTM in comparison to a variety of\nbaselines. It exhibits the prospect of multi-variable LSTM as an end-to-end\nframework for both forecasting and knowledge discovery. \n\n"}
{"id": "1806.06408", "contents": "Title: Gated Path Planning Networks Abstract: Value Iteration Networks (VINs) are effective differentiable path planning\nmodules that can be used by agents to perform navigation while still\nmaintaining end-to-end differentiability of the entire architecture. Despite\ntheir effectiveness, they suffer from several disadvantages including training\ninstability, random seed sensitivity, and other optimization problems. In this\nwork, we reframe VINs as recurrent-convolutional networks which demonstrates\nthat VINs couple recurrent convolutions with an unconventional max-pooling\nactivation. From this perspective, we argue that standard gated recurrent\nupdate equations could potentially alleviate the optimization issues plaguing\nVIN. The resulting architecture, which we call the Gated Path Planning Network,\nis shown to empirically outperform VIN on a variety of metrics such as learning\nspeed, hyperparameter sensitivity, iteration count, and even generalization.\nFurthermore, we show that this performance gap is consistent across different\nmaze transition types, maze sizes and even show success on a challenging 3D\nenvironment, where the planner is only provided with first-person RGB images. \n\n"}
{"id": "1806.06908", "contents": "Title: Designing Optimal Binary Rating Systems Abstract: Modern online platforms rely on effective rating systems to learn about\nitems. We consider the optimal design of rating systems that collect binary\nfeedback after transactions. We make three contributions. First, we formalize\nthe performance of a rating system as the speed with which it recovers the true\nunderlying ranking on items (in a large deviations sense), accounting for both\nitems' underlying match rates and the platform's preferences. Second, we\nprovide an efficient algorithm to compute the binary feedback system that\nyields the highest such performance. Finally, we show how this theoretical\nperspective can be used to empirically design an implementable, approximately\noptimal rating system, and validate our approach using real-world experimental\ndata collected on Amazon Mechanical Turk. \n\n"}
{"id": "1806.07373", "contents": "Title: Few-Shot Segmentation Propagation with Guided Networks Abstract: Learning-based methods for visual segmentation have made progress on\nparticular types of segmentation tasks, but are limited by the necessary\nsupervision, the narrow definitions of fixed tasks, and the lack of control\nduring inference for correcting errors. To remedy the rigidity and annotation\nburden of standard approaches, we address the problem of few-shot segmentation:\ngiven few image and few pixel supervision, segment any images accordingly. We\npropose guided networks, which extract a latent task representation from any\namount of supervision, and optimize our architecture end-to-end for fast,\naccurate few-shot segmentation. Our method can switch tasks without further\noptimization and quickly update when given more guidance. We report the first\nresults for segmentation from one pixel per concept and show real-time\ninteractive video segmentation. Our unified approach propagates pixel\nannotations across space for interactive segmentation, across time for video\nsegmentation, and across scenes for semantic segmentation. Our guided segmentor\nis state-of-the-art in accuracy for the amount of annotation and time. See\nhttp://github.com/shelhamer/revolver for code, models, and more details. \n\n"}
{"id": "1806.07385", "contents": "Title: Detecting and interpreting myocardial infarction using fully\n  convolutional neural networks Abstract: Objective: We aim to provide an algorithm for the detection of myocardial\ninfarction that operates directly on ECG data without any preprocessing and to\ninvestigate its decision criteria. Approach: We train an ensemble of fully\nconvolutional neural networks on the PTB ECG dataset and apply state-of-the-art\nattribution methods. Main results: Our classifier reaches 93.3% sensitivity and\n89.7% specificity evaluated using 10-fold cross-validation with sampling based\non patients. The presented method outperforms state-of-the-art approaches and\nreaches the performance level of human cardiologists for detection of\nmyocardial infarction. We are able to discriminate channel-specific regions\nthat contribute most significantly to the neural network's decision.\nInterestingly, the network's decision is influenced by signs also recognized by\nhuman cardiologists as indicative of myocardial infarction. Significance: Our\nresults demonstrate the high prospects of algorithmic ECG analysis for future\nclinical applications considering both its quantitative performance as well as\nthe possibility of assessing decision criteria on a per-example basis, which\nenhances the comprehensibility of the approach. \n\n"}
{"id": "1806.07409", "contents": "Title: Built-in Vulnerabilities to Imperceptible Adversarial Perturbations Abstract: Designing models that are robust to small adversarial perturbations of their\ninputs has proven remarkably difficult. In this work we show that the reverse\nproblem---making models more vulnerable---is surprisingly easy. After\npresenting some proofs of concept on MNIST, we introduce a generic tilting\nattack that injects vulnerabilities into the linear layers of pre-trained\nnetworks by increasing their sensitivity to components of low variance in the\ntraining data without affecting their performance on test data. We illustrate\nthis attack on a multilayer perceptron trained on SVHN and use it to design a\nstand-alone adversarial module which we call a steganogram decoder. Finally, we\nshow on CIFAR-10 that a poisoning attack with a poisoning rate as low as 0.1%\ncan induce vulnerabilities to chosen imperceptible backdoor signals in\nstate-of-the-art networks. Beyond their practical implications, these different\nresults shed new light on the nature of the adversarial example phenomenon. \n\n"}
{"id": "1806.07492", "contents": "Title: On the Learning of Deep Local Features for Robust Face Spoofing\n  Detection Abstract: Biometrics emerged as a robust solution for security systems. However, given\nthe dissemination of biometric applications, criminals are developing\ntechniques to circumvent them by simulating physical or behavioral traits of\nlegal users (spoofing attacks). Despite face being a promising characteristic\ndue to its universality, acceptability and presence of cameras almost\neverywhere, face recognition systems are extremely vulnerable to such frauds\nsince they can be easily fooled with common printed facial photographs.\nState-of-the-art approaches, based on Convolutional Neural Networks (CNNs),\npresent good results in face spoofing detection. However, these methods do not\nconsider the importance of learning deep local features from each facial\nregion, even though it is known from face recognition that each facial region\npresents different visual aspects, which can also be exploited for face\nspoofing detection. In this work we propose a novel CNN architecture trained in\ntwo steps for such task. Initially, each part of the neural network learns\nfeatures from a given facial region. Afterwards, the whole model is fine-tuned\non the whole facial images. Results show that such pre-training step allows the\nCNN to learn different local spoofing cues, improving the performance and the\nconvergence speed of the final model, outperforming the state-of-the-art\napproaches. \n\n"}
{"id": "1806.08235", "contents": "Title: Semi-supervised Seizure Prediction with Generative Adversarial Networks Abstract: In this article, we propose an approach that can make use of not only labeled\nEEG signals but also the unlabeled ones which is more accessible. We also\nsuggest the use of data fusion to further improve the seizure prediction\naccuracy. Data fusion in our vision includes EEG signals, cardiogram signals,\nbody temperature and time. We use the short-time Fourier transform on 28-s EEG\nwindows as a pre-processing step. A generative adversarial network (GAN) is\ntrained in an unsupervised manner where information of seizure onset is\ndisregarded. The trained Discriminator of the GAN is then used as feature\nextractor. Features generated by the feature extractor are classified by two\nfully-connected layers (can be replaced by any classifier) for the labeled EEG\nsignals. This semi-supervised seizure prediction method achieves area under the\noperating characteristic curve (AUC) of 77.68% and 75.47% for the CHBMIT scalp\nEEG dataset and the Freiburg Hospital intracranial EEG dataset, respectively.\nUnsupervised training without the need of labeling is important because not\nonly it can be performed in real-time during EEG signal recording, but also it\ndoes not require feature engineering effort for each patient. \n\n"}
{"id": "1806.08462", "contents": "Title: Stochastic Wasserstein Autoencoder for Probabilistic Sentence Generation Abstract: The variational autoencoder (VAE) imposes a probabilistic distribution\n(typically Gaussian) on the latent space and penalizes the Kullback--Leibler\n(KL) divergence between the posterior and prior. In NLP, VAEs are extremely\ndifficult to train due to the problem of KL collapsing to zero. One has to\nimplement various heuristics such as KL weight annealing and word dropout in a\ncarefully engineered manner to successfully train a VAE for text. In this\npaper, we propose to use the Wasserstein autoencoder (WAE) for probabilistic\nsentence generation, where the encoder could be either stochastic or\ndeterministic. We show theoretically and empirically that, in the original WAE,\nthe stochastically encoded Gaussian distribution tends to become a Dirac-delta\nfunction, and we propose a variant of WAE that encourages the stochasticity of\nthe encoder. Experimental results show that the latent space learned by WAE\nexhibits properties of continuity and smoothness as in VAEs, while\nsimultaneously achieving much higher BLEU scores for sentence reconstruction. \n\n"}
{"id": "1806.08829", "contents": "Title: Diffusion Scattering Transforms on Graphs Abstract: Stability is a key aspect of data analysis. In many applications, the natural\nnotion of stability is geometric, as illustrated for example in computer\nvision. Scattering transforms construct deep convolutional representations\nwhich are certified stable to input deformations. This stability to\ndeformations can be interpreted as stability with respect to changes in the\nmetric structure of the domain. In this work, we show that scattering\ntransforms can be generalized to non-Euclidean domains using diffusion\nwavelets, while preserving a notion of stability with respect to metric changes\nin the domain, measured with diffusion maps. The resulting representation is\nstable to metric perturbations of the domain while being able to capture\n\"high-frequency\" information, akin to the Euclidean Scattering. \n\n"}
{"id": "1806.08915", "contents": "Title: DALEX: explainers for complex predictive models Abstract: Predictive modeling is invaded by elastic, yet complex methods such as neural\nnetworks or ensembles (model stacking, boosting or bagging). Such methods are\nusually described by a large number of parameters or hyper parameters - a price\nthat one needs to pay for elasticity. The very number of parameters makes\nmodels hard to understand. This paper describes a consistent collection of\nexplainers for predictive models, a.k.a. black boxes. Each explainer is a\ntechnique for exploration of a black box model. Presented approaches are\nmodel-agnostic, what means that they extract useful information from any\npredictive method despite its internal structure. Each explainer is linked with\na specific aspect of a model. Some are useful in decomposing predictions, some\nserve better in understanding performance, while others are useful in\nunderstanding importance and conditional responses of a particular variable.\nEvery explainer presented in this paper works for a single model or for a\ncollection of models. In the latter case, models can be compared against each\nother. Such comparison helps to find strengths and weaknesses of different\napproaches and gives additional possibilities for model validation. Presented\nexplainers are implemented in the DALEX package for R. They are based on a\nuniform standardized grammar of model exploration which may be easily extended.\nThe current implementation supports the most popular frameworks for\nclassification and regression. \n\n"}
{"id": "1806.09186", "contents": "Title: Detection based Defense against Adversarial Examples from the\n  Steganalysis Point of View Abstract: Deep Neural Networks (DNNs) have recently led to significant improvements in\nmany fields. However, DNNs are vulnerable to adversarial examples which are\nsamples with imperceptible perturbations while dramatically misleading the\nDNNs. Moreover, adversarial examples can be used to perform an attack on\nvarious kinds of DNN based systems, even if the adversary has no access to the\nunderlying model. Many defense methods have been proposed, such as obfuscating\ngradients of the networks or detecting adversarial examples. However it is\nproved out that these defense methods are not effective or cannot resist\nsecondary adversarial attacks. In this paper, we point out that steganalysis\ncan be applied to adversarial examples detection, and propose a method to\nenhance steganalysis features by estimating the probability of modifications\ncaused by adversarial attacks. Experimental results show that the proposed\nmethod can accurately detect adversarial examples. Moreover, secondary\nadversarial attacks cannot be directly performed to our method because our\nmethod is not based on a neural network but based on high-dimensional\nartificial features and FLD (Fisher Linear Discriminant) ensemble. \n\n"}
{"id": "1806.09300", "contents": "Title: Improving Chemical Autoencoder Latent Space and Molecular De novo\n  Generation Diversity with Heteroencoders Abstract: Chemical autoencoders are attractive models as they combine chemical space\nnavigation with possibilities for de-novo molecule generation in areas of\ninterest. This enables them to produce focused chemical libraries around a\nsingle lead compound for employment early in a drug discovery project. Here it\nis shown that the choice of chemical representation, such as SMILES strings,\nhas a large influence on the properties of the latent space. It is further\nexplored to what extent translating between different chemical representations\ninfluences the latent space similarity to the SMILES strings or circular\nfingerprints. By employing SMILES enumeration for either the encoder or\ndecoder, it is found that the decoder has the largest influence on the\nproperties of the latent space. Training a sequence to sequence heteroencoder\nbased on recurrent neural networks(RNNs) with long short-term memory cells\n(LSTM) to predict different enumerated SMILES strings from the same canonical\nSMILES string gives the largest similarity between latent space distance and\nmolecular similarity measured as circular fingerprints similarity. Using the\noutput from the bottleneck in QSAR modelling of five molecular datasets shows\nthat heteroencoder derived vectors markedly outperforms autoencoder derived\nvectors as well as models built using ECFP4 fingerprints, underlining the\nincreased chemical relevance of the latent space. However, the use of\nenumeration during training of the decoder leads to a markedly increase in the\nrate of decoding to a different molecules than encoded, a tendency that can be\ncounteracted with more complex network architectures. \n\n"}
{"id": "1806.09504", "contents": "Title: Interpreting Embedding Models of Knowledge Bases: A Pedagogical Approach Abstract: Knowledge bases are employed in a variety of applications from natural\nlanguage processing to semantic web search; alas, in practice their usefulness\nis hurt by their incompleteness. Embedding models attain state-of-the-art\naccuracy in knowledge base completion, but their predictions are notoriously\nhard to interpret. In this paper, we adapt \"pedagogical approaches\" (from the\nliterature on neural networks) so as to interpret embedding models by\nextracting weighted Horn rules from them. We show how pedagogical approaches\nhave to be adapted to take upon the large-scale relational aspects of knowledge\nbases and show experimentally their strengths and weaknesses. \n\n"}
{"id": "1806.09542", "contents": "Title: Mapping Unparalleled Clinical Professional and Consumer Languages with\n  Embedding Alignment Abstract: Mapping and translating professional but arcane clinical jargons to consumer\nlanguage is essential to improve the patient-clinician communication.\nResearchers have used the existing biomedical ontologies and consumer health\nvocabulary dictionary to translate between the languages. However, such\napproaches are limited by expert efforts to manually build the dictionary,\nwhich is hard to be generalized and scalable. In this work, we utilized the\nembeddings alignment method for the word mapping between unparalleled clinical\nprofessional and consumer language embeddings. To map semantically similar\nwords in two different word embeddings, we first independently trained word\nembeddings on both the corpus with abundant clinical professional terms and the\nother with mainly healthcare consumer terms. Then, we aligned the embeddings by\nthe Procrustes algorithm. We also investigated the approach with the\nadversarial training with refinement. We evaluated the quality of the alignment\nthrough the similar words retrieval both by computing the model precision and\nas well as judging qualitatively by human. We show that the Procrustes\nalgorithm can be performant for the professional consumer language embeddings\nalignment, whereas adversarial training with refinement may find some relations\nbetween two languages. \n\n"}
{"id": "1806.09602", "contents": "Title: A Machine-learning framework for automatic reference-free quality\n  assessment in MRI Abstract: Magnetic resonance (MR) imaging offers a wide variety of imaging techniques.\nA large amount of data is created per examination which needs to be checked for\nsufficient quality in order to derive a meaningful diagnosis. This is a manual\nprocess and therefore time- and cost-intensive. Any imaging artifacts\noriginating from scanner hardware, signal processing or induced by the patient\nmay reduce the image quality and complicate the diagnosis or any image\npost-processing. Therefore, the assessment or the ensurance of sufficient image\nquality in an automated manner is of high interest. Usually no reference image\nis available or difficult to define. Therefore, classical reference-based\napproaches are not applicable. Model observers mimicking the human observers\n(HO) can assist in this task. Thus, we propose a new machine-learning-based\nreference-free MR image quality assessment framework which is trained on\nHO-derived labels to assess MR image quality immediately after each\nacquisition. We include the concept of active learning and present an efficient\nblinded reading platform to reduce the effort in the HO labeling procedure.\nDerived image features and the applied classifiers (support-vector-machine,\ndeep neural network) are investigated for a cohort of 250 patients. The MR\nimage quality assessment framework can achieve a high test accuracy of 93.7$\\%$\nfor estimating quality classes on a 5-point Likert-scale. The proposed MR image\nquality assessment framework is able to provide an accurate and efficient\nquality estimation which can be used as a prospective quality assurance\nincluding automatic acquisition adaptation or guided MR scanner operation,\nand/or as a retrospective quality assessment including support of diagnostic\ndecisions or quality control in cohort studies. \n\n"}
{"id": "1806.09856", "contents": "Title: Dropout-based Active Learning for Regression Abstract: Active learning is relevant and challenging for high-dimensional regression\nmodels when the annotation of the samples is expensive. Yet most of the\nexisting sampling methods cannot be applied to large-scale problems, consuming\ntoo much time for data processing. In this paper, we propose a fast active\nlearning algorithm for regression, tailored for neural network models. It is\nbased on uncertainty estimation from stochastic dropout output of the network.\nExperiments on both synthetic and real-world datasets show comparable or better\nperformance (depending on the accuracy metric) as compared to the baselines.\nThis approach can be generalized to other deep learning architectures. It can\nbe used to systematically improve a machine-learning model as it offers a\ncomputationally efficient way of sampling additional data. \n\n"}
{"id": "1806.09888", "contents": "Title: Towards an understanding of CNNs: analysing the recovery of activation\n  pathways via Deep Convolutional Sparse Coding Abstract: Deep Convolutional Sparse Coding (D-CSC) is a framework reminiscent of deep\nconvolutional neural networks (DCNNs), but by omitting the learning of the\ndictionaries one can more transparently analyse the role of the activation\nfunction and its ability to recover activation paths through the layers.\nPapyan, Romano, and Elad conducted an analysis of such an architecture,\ndemonstrated the relationship with DCNNs and proved conditions under which the\nD-CSC is guaranteed to recover specific activation paths. A technical\ninnovation of their work highlights that one can view the efficacy of the ReLU\nnonlinear activation function of a DCNN through a new variant of the tensor's\nsparsity, referred to as stripe-sparsity. Using this they proved that\nrepresentations with an activation density proportional to the ambient\ndimension of the data are recoverable. We extend their uniform guarantees to a\nmodified model and prove that with high probability the true activation is\ntypically possible to recover for a greater density of activations per layer.\nOur extension follows from incorporating the prior work on one step\nthresholding by Schnass and Vandergheynst. \n\n"}
{"id": "1806.10230", "contents": "Title: Guided evolutionary strategies: Augmenting random search with surrogate\n  gradients Abstract: Many applications in machine learning require optimizing a function whose\ntrue gradient is unknown, but where surrogate gradient information (directions\nthat may be correlated with, but not necessarily identical to, the true\ngradient) is available instead. This arises when an approximate gradient is\neasier to compute than the full gradient (e.g. in meta-learning or unrolled\noptimization), or when a true gradient is intractable and is replaced with a\nsurrogate (e.g. in certain reinforcement learning applications, or when using\nsynthetic gradients). We propose Guided Evolutionary Strategies, a method for\noptimally using surrogate gradient directions along with random search. We\ndefine a search distribution for evolutionary strategies that is elongated\nalong a guiding subspace spanned by the surrogate gradients. This allows us to\nestimate a descent direction which can then be passed to a first-order\noptimizer. We analytically and numerically characterize the tradeoffs that\nresult from tuning how strongly the search distribution is stretched along the\nguiding subspace, and we use this to derive a setting of the hyperparameters\nthat works well across problems. Finally, we apply our method to example\nproblems, demonstrating an improvement over both standard evolutionary\nstrategies and first-order methods (that directly follow the surrogate\ngradient). We provide a demo of Guided ES at\nhttps://github.com/brain-research/guided-evolutionary-strategies \n\n"}
{"id": "1806.10308", "contents": "Title: Matrix Completion from Non-Uniformly Sampled Entries Abstract: In this paper, we consider matrix completion from non-uniformly sampled\nentries including fully observed and partially observed columns. Specifically,\nwe assume that a small number of columns are randomly selected and fully\nobserved, and each remaining column is partially observed with uniform\nsampling. To recover the unknown matrix, we first recover its column space from\nthe fully observed columns. Then, for each partially observed column, we\nrecover it by finding a vector which lies in the recovered column space and\nconsists of the observed entries. When the unknown $m\\times n$ matrix is\nlow-rank, we show that our algorithm can exactly recover it from merely\n$\\Omega(rn\\ln n)$ entries, where $r$ is the rank of the matrix. Furthermore,\nfor a noisy low-rank matrix, our algorithm computes a low-rank approximation of\nthe unknown matrix and enjoys an additive error bound measured by Frobenius\nnorm. Experimental results on synthetic datasets verify our theoretical claims\nand demonstrate the effectiveness of our proposed algorithm. \n\n"}
{"id": "1806.10648", "contents": "Title: Uncoupled isotonic regression via minimum Wasserstein deconvolution Abstract: Isotonic regression is a standard problem in shape-constrained estimation\nwhere the goal is to estimate an unknown nondecreasing regression function $f$\nfrom independent pairs $(x_i, y_i)$ where $\\mathbb{E}[y_i]=f(x_i), i=1, \\ldots\nn$. While this problem is well understood both statistically and\ncomputationally, much less is known about its uncoupled counterpart where one\nis given only the unordered sets $\\{x_1, \\ldots, x_n\\}$ and $\\{y_1, \\ldots,\ny_n\\}$. In this work, we leverage tools from optimal transport theory to derive\nminimax rates under weak moments conditions on $y_i$ and to give an efficient\nalgorithm achieving optimal rates. Both upper and lower bounds employ\nmoment-matching arguments that are also pertinent to learning mixtures of\ndistributions and deconvolution. \n\n"}
{"id": "1806.10787", "contents": "Title: How To Extract Fashion Trends From Social Media? A Robust Object\n  Detector With Support For Unsupervised Learning Abstract: With the proliferation of social media, fashion inspired from celebrities,\nreputed designers as well as fashion influencers has shortened the cycle of\nfashion design and manufacturing. However, with the explosion of fashion\nrelated content and large number of user generated fashion photos, it is an\narduous task for fashion designers to wade through social media photos and\ncreate a digest of trending fashion. This necessitates deep parsing of fashion\nphotos on social media to localize and classify multiple fashion items from a\ngiven fashion photo. While object detection competitions such as MSCOCO have\nthousands of samples for each of the object categories, it is quite difficult\nto get large labeled datasets for fast fashion items. Moreover,\nstate-of-the-art object detectors do not have any functionality to ingest large\namount of unlabeled data available on social media in order to fine tune object\ndetectors with labeled datasets. In this work, we show application of a generic\nobject detector, that can be pretrained in an unsupervised manner, on 24\ncategories from recently released Open Images V4 dataset. We first train the\nbase architecture of the object detector using unsupervisd learning on 60K\nunlabeled photos from 24 categories gathered from social media, and then\nsubsequently fine tune it on 8.2K labeled photos from Open Images V4 dataset.\nOn 300 X 300 image inputs, we achieve 72.7% mAP on a test dataset of 2.4K\nphotos while performing 11% to 17% better as compared to the state-of-the-art\nobject detectors. We show that this improvement is due to our choice of\narchitecture that lets us do unsupervised learning and that performs\nsignificantly better in identifying small objects. \n\n"}
{"id": "1806.11212", "contents": "Title: Proxy Fairness Abstract: We consider the problem of improving fairness when one lacks access to a\ndataset labeled with protected groups, making it difficult to take advantage of\nstrategies that can improve fairness but require protected group labels, either\nat training or runtime. To address this, we investigate improving fairness\nmetrics for proxy groups, and test whether doing so results in improved\nfairness for the true sensitive groups. Results on benchmark and real-world\ndatasets demonstrate that such a proxy fairness strategy can work well in\npractice. However, we caution that the effectiveness likely depends on the\nchoice of fairness metric, as well as how aligned the proxy groups are with the\ntrue protected groups in terms of the constrained model parameters. \n\n"}
{"id": "1806.11302", "contents": "Title: Generate the corresponding Image from Text Description using Modified\n  GAN-CLS Algorithm Abstract: Synthesizing images or texts automatically is a useful research area in the\nartificial intelligence nowadays. Generative adversarial networks (GANs), which\nare proposed by Goodfellow in 2014, make this task to be done more efficiently\nby using deep neural networks. We consider generating corresponding images from\nan input text description using a GAN. In this paper, we analyze the GAN-CLS\nalgorithm, which is a kind of advanced method of GAN proposed by Scott Reed in\n2016. First, we find the problem with this algorithm through inference. Then we\ncorrect the GAN-CLS algorithm according to the inference by modifying the\nobjective function of the model. Finally, we do the experiments on the\nOxford-102 dataset and the CUB dataset. As a result, our modified algorithm can\ngenerate images which are more plausible than the GAN-CLS algorithm in some\ncases. Also, some of the generated images match the input texts better. \n\n"}
{"id": "1806.11536", "contents": "Title: An Exact Quantized Decentralized Gradient Descent Algorithm Abstract: We consider the problem of decentralized consensus optimization, where the\nsum of $n$ smooth and strongly convex functions are minimized over $n$\ndistributed agents that form a connected network. In particular, we consider\nthe case that the communicated local decision variables among nodes are\nquantized in order to alleviate the communication bottleneck in distributed\noptimization. We propose the Quantized Decentralized Gradient Descent (QDGD)\nalgorithm, in which nodes update their local decision variables by combining\nthe quantized information received from their neighbors with their local\ninformation. We prove that under standard strong convexity and smoothness\nassumptions for the objective function, QDGD achieves a vanishing mean solution\nerror under customary conditions for quantizers. To the best of our knowledge,\nthis is the first algorithm that achieves vanishing consensus error in the\npresence of quantization noise. Moreover, we provide simulation results that\nshow tight agreement between our derived theoretical convergence rate and the\nnumerical results. \n\n"}
{"id": "1807.00251", "contents": "Title: Trust-Region Algorithms for Training Responses: Machine Learning Methods\n  Using Indefinite Hessian Approximations Abstract: Machine learning (ML) problems are often posed as highly nonlinear and\nnonconvex unconstrained optimization problems. Methods for solving ML problems\nbased on stochastic gradient descent are easily scaled for very large problems\nbut may involve fine-tuning many hyper-parameters. Quasi-Newton approaches\nbased on the limited-memory Broyden-Fletcher-Goldfarb-Shanno (BFGS) update\ntypically do not require manually tuning hyper-parameters but suffer from\napproximating a potentially indefinite Hessian with a positive-definite matrix.\nHessian-free methods leverage the ability to perform Hessian-vector\nmultiplication without needing the entire Hessian matrix, but each iteration's\ncomplexity is significantly greater than quasi-Newton methods. In this paper we\npropose an alternative approach for solving ML problems based on a quasi-Newton\ntrust-region framework for solving large-scale optimization problems that allow\nfor indefinite Hessian approximations. Numerical experiments on a standard\ntesting data set show that with a fixed computational time budget, the proposed\nmethods achieve better results than the traditional limited-memory BFGS and the\nHessian-free methods. \n\n"}
{"id": "1807.00297", "contents": "Title: Exponential Convergence of the Deep Neural Network Approximation for\n  Analytic Functions Abstract: We prove that for analytic functions in low dimension, the convergence rate\nof the deep neural network approximation is exponential. \n\n"}
{"id": "1807.00448", "contents": "Title: Speeding up the Metabolism in E-commerce by Reinforcement Mechanism\n  Design Abstract: In a large E-commerce platform, all the participants compete for impressions\nunder the allocation mechanism of the platform. Existing methods mainly focus\non the short-term return based on the current observations instead of the\nlong-term return. In this paper, we formally establish the lifecycle model for\nproducts, by defining the introduction, growth, maturity and decline stages and\ntheir transitions throughout the whole life period. Based on such model, we\nfurther propose a reinforcement learning based mechanism design framework for\nimpression allocation, which incorporates the first principal component based\npermutation and the novel experiences generation method, to maximize short-term\nas well as long-term return of the platform. With the power of trial-and-error,\nit is possible to optimize impression allocation strategies globally which is\ncontribute to the healthy development of participants and the platform itself.\nWe evaluate our algorithm on a simulated environment built based on one of the\nlargest E-commerce platforms, and a significant improvement has been achieved\nin comparison with the baseline solutions. \n\n"}
{"id": "1807.00583", "contents": "Title: Sample Efficient Semantic Segmentation using Rotation Equivariant\n  Convolutional Networks Abstract: We propose a semantic segmentation model that exploits rotation and\nreflection symmetries. We demonstrate significant gains in sample efficiency\ndue to increased weight sharing, as well as improvements in robustness to\nsymmetry transformations. The group equivariant CNN framework is extended for\nsegmentation by introducing a new equivariant (G->Z2)-convolution that\ntransforms feature maps on a group to planar feature maps. Also, equivariant\ntransposed convolution is formulated for up-sampling in an encoder-decoder\nnetwork. To demonstrate improvements in sample efficiency we evaluate on\nmultiple data regimes of a rotation-equivariant segmentation task: cancer\nmetastases detection in histopathology images. We further show the\neffectiveness of exploiting more symmetries by varying the size of the group. \n\n"}
{"id": "1807.00737", "contents": "Title: Learning Goal-Oriented Visual Dialog via Tempered Policy Gradient Abstract: Learning goal-oriented dialogues by means of deep reinforcement learning has\nrecently become a popular research topic. However, commonly used policy-based\ndialogue agents often end up focusing on simple utterances and suboptimal\npolicies. To mitigate this problem, we propose a class of novel\ntemperature-based extensions for policy gradient methods, which are referred to\nas Tempered Policy Gradients (TPGs). On a recent AI-testbed, i.e., the\nGuessWhat?! game, we achieve significant improvements with two innovations. The\nfirst one is an extension of the state-of-the-art solutions with Seq2Seq and\nMemory Network structures that leads to an improvement of 7%. The second one is\nthe application of our newly developed TPG methods, which improves the\nperformance additionally by around 5% and, even more importantly, helps produce\nmore convincing utterances. \n\n"}
{"id": "1807.01011", "contents": "Title: A First Analysis of Kernels for Kriging-based Optimization in\n  Hierarchical Search Spaces Abstract: Many real-world optimization problems require significant resources for\nobjective function evaluations. This is a challenge to evolutionary algorithms,\nas it limits the number of available evaluations. One solution are surrogate\nmodels, which replace the expensive objective. A particular issue in this\ncontext are hierarchical variables. Hierarchical variables only influence the\nobjective function if other variables satisfy some condition. We study how this\nkind of hierarchical structure can be integrated into the model based\noptimization framework. We discuss an existing kernel and propose alternatives.\nAn artificial test function is used to investigate how different kernels and\nassumptions affect model quality and search performance. \n\n"}
{"id": "1807.01298", "contents": "Title: Generalized Bilinear Deep Convolutional Neural Networks for Multimodal\n  Biometric Identification Abstract: In this paper, we propose to employ a bank of modality-dedicated\nConvolutional Neural Networks (CNNs), fuse, train, and optimize them together\nfor person classification tasks. A modality-dedicated CNN is used for each\nmodality to extract modality-specific features. We demonstrate that, rather\nthan spatial fusion at the convolutional layers, the fusion can be performed on\nthe outputs of the fully-connected layers of the modality-specific CNNs without\nany loss of performance and with significant reduction in the number of\nparameters. We show that, using multiple CNNs with multimodal fusion at the\nfeature-level, we significantly outperform systems that use unimodal\nrepresentation. We study weighted feature, bilinear, and compact bilinear\nfeature-level fusion algorithms for multimodal biometric person identification.\nFinally, We propose generalized compact bilinear fusion algorithm to deploy\nboth the weighted feature fusion and compact bilinear schemes. We provide the\nresults for the proposed algorithms on three challenging databases: CMU\nMulti-PIE, BioCop, and BIOMDATA. \n\n"}
{"id": "1807.02033", "contents": "Title: Consistent Generative Query Networks Abstract: Stochastic video prediction models take in a sequence of image frames, and\ngenerate a sequence of consecutive future image frames. These models typically\ngenerate future frames in an autoregressive fashion, which is slow and requires\nthe input and output frames to be consecutive. We introduce a model that\novercomes these drawbacks by generating a latent representation from an\narbitrary set of frames that can then be used to simultaneously and efficiently\nsample temporally consistent frames at arbitrary time-points. For example, our\nmodel can \"jump\" and directly sample frames at the end of the video, without\nsampling intermediate frames. Synthetic video evaluations confirm substantial\ngains in speed and functionality without loss in fidelity. We also apply our\nframework to a 3D scene reconstruction dataset. Here, our model is conditioned\non camera location and can sample consistent sets of images for what an\noccluded region of a 3D scene might look like, even if there are multiple\npossibilities for what that region might contain. Reconstructions and videos\nare available at https://bit.ly/2O4Pc4R. \n\n"}
{"id": "1807.02089", "contents": "Title: Linear Bandits with Stochastic Delayed Feedback Abstract: Stochastic linear bandits are a natural and well-studied model for structured\nexploration/exploitation problems and are widely used in applications such as\nonline marketing and recommendation. One of the main challenges faced by\npractitioners hoping to apply existing algorithms is that usually the feedback\nis randomly delayed and delays are only partially observable. For example,\nwhile a purchase is usually observable some time after the display, the\ndecision of not buying is never explicitly sent to the system. In other words,\nthe learner only observes delayed positive events. We formalize this problem as\na novel stochastic delayed linear bandit and propose ${\\tt OTFLinUCB}$ and\n${\\tt OTFLinTS}$, two computationally efficient algorithms able to integrate\nnew information as it becomes available and to deal with the permanently\ncensored feedback. We prove optimal $\\tilde O(\\smash{d\\sqrt{T}})$ bounds on the\nregret of the first algorithm and study the dependency on delay-dependent\nparameters. Our model, assumptions and results are validated by experiments on\nsimulated and real data. \n\n"}
{"id": "1807.02128", "contents": "Title: Adaptive Path-Integral Autoencoder: Representation Learning and Planning\n  for Dynamical Systems Abstract: We present a representation learning algorithm that learns a low-dimensional\nlatent dynamical system from high-dimensional \\textit{sequential} raw data,\ne.g., video. The framework builds upon recent advances in amortized inference\nmethods that use both an inference network and a refinement procedure to output\nsamples from a variational distribution given an observation sequence, and\ntakes advantage of the duality between control and inference to approximately\nsolve the intractable inference problem using the path integral control\napproach. The learned dynamical model can be used to predict and plan the\nfuture states; we also present the efficient planning method that exploits the\nlearned low-dimensional latent dynamics. Numerical experiments show that the\nproposed path-integral control based variational inference method leads to\ntighter lower bounds in statistical model learning of sequential data. The\nsupplementary video: https://youtu.be/xCp35crUoLQ \n\n"}
{"id": "1807.02234", "contents": "Title: Distributed Self-Paced Learning in Alternating Direction Method of\n  Multipliers Abstract: Self-paced learning (SPL) mimics the cognitive process of humans, who\ngenerally learn from easy samples to hard ones. One key issue in SPL is the\ntraining process required for each instance weight depends on the other samples\nand thus cannot easily be run in a distributed manner in a large-scale dataset.\nIn this paper, we reformulate the self-paced learning problem into a\ndistributed setting and propose a novel Distributed Self-Paced Learning method\n(DSPL) to handle large-scale datasets. Specifically, both the model and\ninstance weights can be optimized in parallel for each batch based on a\nconsensus alternating direction method of multipliers. We also prove the\nconvergence of our algorithm under mild conditions. Extensive experiments on\nboth synthetic and real datasets demonstrate that our approach is superior to\nthose of existing methods. \n\n"}
{"id": "1807.02264", "contents": "Title: Variance Reduction for Reinforcement Learning in Input-Driven\n  Environments Abstract: We consider reinforcement learning in input-driven environments, where an\nexogenous, stochastic input process affects the dynamics of the system. Input\nprocesses arise in many applications, including queuing systems, robotics\ncontrol with disturbances, and object tracking. Since the state dynamics and\nrewards depend on the input process, the state alone provides limited\ninformation for the expected future returns. Therefore, policy gradient methods\nwith standard state-dependent baselines suffer high variance during training.\nWe derive a bias-free, input-dependent baseline to reduce this variance, and\nanalytically show its benefits over state-dependent baselines. We then propose\na meta-learning approach to overcome the complexity of learning a baseline that\ndepends on a long sequence of inputs. Our experimental results show that across\nenvironments from queuing systems, computer networks, and MuJoCo robotic\nlocomotion, input-dependent baselines consistently improve training stability\nand result in better eventual policies. \n\n"}
{"id": "1807.02609", "contents": "Title: Anytime Neural Prediction via Slicing Networks Vertically Abstract: The pioneer deep neural networks (DNNs) have emerged to be deeper or wider\nfor improving their accuracy in various applications of artificial\nintelligence. However, DNNs are often too heavy to deploy in practice, and it\nis often required to control their architectures dynamically given computing\nresource budget, i.e., anytime prediction. While most existing approaches have\nfocused on training multiple shallow sub-networks jointly, we study training\nthin sub-networks instead. To this end, we first build many inclusive thin\nsub-networks (of the same depth) under a minor modification of existing\nmulti-branch DNNs, and found that they can significantly outperform the\nstate-of-art dense architecture for anytime prediction. This is remarkable due\nto their simplicity and effectiveness, but training many thin sub-networks\njointly faces a new challenge on training complexity. To address the issue, we\nalso propose a novel DNN architecture by forcing a certain sparsity pattern on\nmulti-branch network parameters, making them train efficiently for the purpose\nof anytime prediction. In our experiments on the ImageNet dataset, its\nsub-networks have up to $43.3\\%$ smaller sizes (FLOPs) compared to those of the\nstate-of-art anytime model with respect to the same accuracy. Finally, we also\npropose an alternative task under the proposed architecture using a\nhierarchical taxonomy, which brings a new angle for anytime prediction. \n\n"}
{"id": "1807.02802", "contents": "Title: Revisiting Distillation and Incremental Classifier Learning Abstract: One of the key differences between the learning mechanism of humans and\nArtificial Neural Networks (ANNs) is the ability of humans to learn one task at\na time. ANNs, on the other hand, can only learn multiple tasks simultaneously.\nAny attempts at learning new tasks incrementally cause them to completely\nforget about previous tasks. This lack of ability to learn incrementally,\ncalled Catastrophic Forgetting, is considered a major hurdle in building a true\nAI system. In this paper, our goal is to isolate the truly effective existing\nideas for incremental learning from those that only work under certain\nconditions. To this end, we first thoroughly analyze the current state of the\nart (iCaRL) method for incremental learning and demonstrate that the good\nperformance of the system is not because of the reasons presented in the\nexisting literature. We conclude that the success of iCaRL is primarily due to\nknowledge distillation and recognize a key limitation of knowledge\ndistillation, i.e, it often leads to bias in classifiers. Finally, we propose a\ndynamic threshold moving algorithm that is able to successfully remove this\nbias. We demonstrate the effectiveness of our algorithm on CIFAR100 and MNIST\ndatasets showing near-optimal results. Our implementation is available at\nhttps://github.com/Khurramjaved96/incremental-learning. \n\n"}
{"id": "1807.02839", "contents": "Title: Hierarchical stochastic graphlet embedding for graph-based pattern\n  recognition Abstract: Despite being very successful within the pattern recognition and machine\nlearning community, graph-based methods are often unusable because of the lack\nof mathematical operations defined in graph domain. Graph embedding, which maps\ngraphs to a vectorial space, has been proposed as a way to tackle these\ndifficulties enabling the use of standard machine learning techniques. However,\nit is well known that graph embedding functions usually suffer from the loss of\nstructural information. In this paper, we consider the hierarchical structure\nof a graph as a way to mitigate this loss of information. The hierarchical\nstructure is constructed by topologically clustering the graph nodes, and\nconsidering each cluster as a node in the upper hierarchical level. Once this\nhierarchical structure is constructed, we consider several configurations to\ndefine the mapping into a vector space given a classical graph embedding, in\nparticular, we propose to make use of the Stochastic Graphlet Embedding (SGE).\nBroadly speaking, SGE produces a distribution of uniformly sampled low to high\norder graphlets as a way to embed graphs into the vector space. In what\nfollows, the coarse-to-fine structure of a graph hierarchy and the statistics\nfetched by the SGE complements each other and includes important structural\ninformation with varied contexts. Altogether, these two techniques\nsubstantially cope with the usual information loss involved in graph embedding\ntechniques, obtaining a more robust graph representation. This fact has been\ncorroborated through a detailed experimental evaluation on various benchmark\ngraph datasets, where we outperform the state-of-the-art methods. \n\n"}
{"id": "1807.02892", "contents": "Title: Automated labeling of bugs and tickets using attention-based mechanisms\n  in recurrent neural networks Abstract: We explore solutions for automated labeling of content in bug trackers and\ncustomer support systems. In order to do that, we classify content in terms of\nseveral criteria, such as priority or product area. In the first part of the\npaper, we provide an overview of existing methods used for text classification.\nThese methods fall into two categories - the ones that rely on neural networks\nand the ones that don't. We evaluate results of several solutions of both\nkinds. In the second part of the paper we present our own recurrent neural\nnetwork solution based on hierarchical attention paradigm. It consists of\nseveral Hierarchical Attention network blocks with varying Gated Recurrent Unit\ncell sizes and a complementary shallow network that goes alongside. Lastly, we\nevaluate above-mentioned methods when predicting fields from two datasets -\nArch Linux bug tracker and Chromium bug tracker. Our contributions include a\ncomprehensive benchmark between a variety of methods on relevant datasets; a\nnovel solution that outperforms previous generation methods; and two new\ndatasets that are made public for further research. \n\n"}
{"id": "1807.03126", "contents": "Title: Estimating Bicycle Route Attractivity from Image Data Abstract: This master thesis focuses on practical application of Convolutional Neural\nNetwork models on the task of road labeling with bike attractivity score. We\nstart with an abstraction of real world locations into nodes and scored edges\nin partially annotated dataset. We enhance information available about each\nedge with photographic data from Google Street View service and with additional\nneighborhood information from Open Street Map database. We teach a model on\nthis enhanced dataset and experiment with ImageNet Large Scale Visual\nRecognition Competition. We try different dataset enhancing techniques as well\nas various model architectures to improve road scoring. We also make use of\ntransfer learning to use features from a task with rich dataset of ImageNet\ninto our task with smaller number of images, to prevent model overfitting. \n\n"}
{"id": "1807.03133", "contents": "Title: Outfit Generation and Style Extraction via Bidirectional LSTM and\n  Autoencoder Abstract: When creating an outfit, style is a criterion in selecting each fashion item.\nThis means that style can be regarded as a feature of the overall outfit.\nHowever, in various previous studies on outfit generation, there have been few\nmethods focusing on global information obtained from an outfit. To address this\ndeficiency, we have incorporated an unsupervised style extraction module into a\nmodel to learn outfits. Using the style information of an outfit as a whole,\nthe proposed model succeeded in generating outfits more flexibly without\nrequiring additional information. Moreover, the style information extracted by\nthe proposed model is easy to interpret. The proposed model was evaluated on\ntwo human-generated outfit datasets. In a fashion item prediction task (missing\nprediction task), the proposed model outperformed a baseline method. In a style\nextraction task, the proposed model extracted some easily distinguishable\nstyles. In an outfit generation task, the proposed model generated an outfit\nwhile controlling its styles. This capability allows us to generate fashionable\noutfits according to various preferences. \n\n"}
{"id": "1807.03142", "contents": "Title: Faster Bounding Box Annotation for Object Detection in Indoor Scenes Abstract: This paper proposes an approach for rapid bounding box annotation for object\ndetection datasets. The procedure consists of two stages: The first step is to\nannotate a part of the dataset manually, and the second step proposes\nannotations for the remaining samples using a model trained with the first\nstage annotations. We experimentally study which first/second stage split\nminimizes to total workload. In addition, we introduce a new fully labeled\nobject detection dataset collected from indoor scenes. Compared to other indoor\ndatasets, our collection has more class categories, different backgrounds,\nlighting conditions, occlusion and high intra-class differences. We train deep\nlearning based object detectors with a number of state-of-the-art models and\ncompare them in terms of speed and accuracy. The fully annotated dataset is\nreleased freely available for the research community. \n\n"}
{"id": "1807.03148", "contents": "Title: Deep Spatio-Temporal Random Fields for Efficient Video Segmentation Abstract: In this work we introduce a time- and memory-efficient method for structured\nprediction that couples neuron decisions across both space at time. We show\nthat we are able to perform exact and efficient inference on a densely\nconnected spatio-temporal graph by capitalizing on recent advances on deep\nGaussian Conditional Random Fields (GCRFs). Our method, called VideoGCRF is (a)\nefficient, (b) has a unique global minimum, and (c) can be trained end-to-end\nalongside contemporary deep networks for video understanding. We experiment\nwith multiple connectivity patterns in the temporal domain, and present\nempirical improvements over strong baselines on the tasks of both semantic and\ninstance segmentation of videos. \n\n"}
{"id": "1807.03149", "contents": "Title: Learning models for visual 3D localization with implicit mapping Abstract: We consider learning based methods for visual localization that do not\nrequire the construction of explicit maps in the form of point clouds or\nvoxels. The goal is to learn an implicit representation of the environment at a\nhigher, more abstract level. We propose to use a generative approach based on\nGenerative Query Networks (GQNs, Eslami et al. 2018), asking the following\nquestions: 1) Can GQN capture more complex scenes than those it was originally\ndemonstrated on? 2) Can GQN be used for localization in those scenes? To study\nthis approach we consider procedurally generated Minecraft worlds, for which we\ncan generate images of complex 3D scenes along with camera pose coordinates. We\nfirst show that GQNs, enhanced with a novel attention mechanism can capture the\nstructure of 3D scenes in Minecraft, as evidenced by their samples. We then\napply the models to the localization problem, comparing the results to a\ndiscriminative baseline, and comparing the ways each approach captures the task\nuncertainty. \n\n"}
{"id": "1807.03610", "contents": "Title: Window Opening Model using Deep Learning Methods Abstract: Occupant behavior (OB) and in particular window openings need to be\nconsidered in building performance simulation (BPS), in order to realistically\nmodel the indoor climate and energy consumption for heating ventilation and air\nconditioning (HVAC). However, the proposed OB window opening models are often\nbiased towards the over-represented class where windows remained closed. In\naddition, they require tuning for each occupant which can not be efficiently\nscaled to the increased number of occupants. This paper presents a window\nopening model for commercial buildings using deep learning methods. The model\nis trained using data from occupants from an office building in Germany. In\ntotal the model is evaluated using almost 20 mio. data points from 3\nindependent buildings, located in Aachen, Frankfurt and Philadelphia.\nEventually, the results of 3100 core hours of model development are summarized,\nwhich makes this study the largest of its kind in window states modeling.\nAdditionally, the practical potential of the proposed model was tested by\nincorporating it in the Modelica-based thermal building simulation. The\nresulting evaluation accuracy and F1 scores on the office buildings ranged\nbetween 86-89 % and 0.53-0.65 respectively. The performance dropped around 15 %\npoints in case of sparse input data, while the F1 score remained high. \n\n"}
{"id": "1807.03653", "contents": "Title: Handling Incomplete Heterogeneous Data using VAEs Abstract: Variational autoencoders (VAEs), as well as other generative models, have\nbeen shown to be efficient and accurate for capturing the latent structure of\nvast amounts of complex high-dimensional data. However, existing VAEs can still\nnot directly handle data that are heterogenous (mixed continuous and discrete)\nor incomplete (with missing data at random), which is indeed common in\nreal-world applications. In this paper, we propose a general framework to\ndesign VAEs suitable for fitting incomplete heterogenous data. The proposed\nHI-VAE includes likelihood models for real-valued, positive real valued,\ninterval, categorical, ordinal and count data, and allows accurate estimation\n(and potentially imputation) of missing data. Furthermore, HI-VAE presents\ncompetitive predictive performance in supervised tasks, outperforming\nsupervised models when trained on incomplete data. \n\n"}
{"id": "1807.04065", "contents": "Title: Recurrent Neural Networks with Flexible Gates using Kernel Activation\n  Functions Abstract: Gated recurrent neural networks have achieved remarkable results in the\nanalysis of sequential data. Inside these networks, gates are used to control\nthe flow of information, allowing to model even very long-term dependencies in\nthe data. In this paper, we investigate whether the original gate equation (a\nlinear projection followed by an element-wise sigmoid) can be improved. In\nparticular, we design a more flexible architecture, with a small number of\nadaptable parameters, which is able to model a wider range of gating functions\nthan the classical one. To this end, we replace the sigmoid function in the\nstandard gate with a non-parametric formulation extending the recently proposed\nkernel activation function (KAF), with the addition of a residual\nskip-connection. A set of experiments on sequential variants of the MNIST\ndataset shows that the adoption of this novel gate allows to improve accuracy\nwith a negligible cost in terms of computational power and with a large\nspeed-up in the number of training iterations. \n\n"}
{"id": "1807.04188", "contents": "Title: A Hardware-Software Blueprint for Flexible Deep Learning Specialization Abstract: Specialized Deep Learning (DL) acceleration stacks, designed for a specific\nset of frameworks, model architectures, operators, and data types, offer the\nallure of high performance while sacrificing flexibility. Changes in\nalgorithms, models, operators, or numerical systems threaten the viability of\nspecialized hardware accelerators. We propose VTA, a programmable deep learning\narchitecture template designed to be extensible in the face of evolving\nworkloads. VTA achieves this flexibility via a parametrizable architecture,\ntwo-level ISA, and a JIT compiler. The two-level ISA is based on (1) a task-ISA\nthat explicitly orchestrates concurrent compute and memory tasks and (2) a\nmicrocode-ISA which implements a wide variety of operators with single-cycle\ntensor-tensor operations. Next, we propose a runtime system equipped with a JIT\ncompiler for flexible code-generation and heterogeneous execution that enables\neffective use of the VTA architecture. VTA is integrated and open-sourced into\nApache TVM, a state-of-the-art deep learning compilation stack that provides\nflexibility for diverse models and divergent hardware backends. We propose a\nflow that performs design space exploration to generate a customized hardware\narchitecture and software operator library that can be leveraged by mainstream\nlearning frameworks. We demonstrate our approach by deploying optimized deep\nlearning models used for object classification and style transfer on edge-class\nFPGAs. \n\n"}
{"id": "1807.04439", "contents": "Title: Will it Blend? Composing Value Functions in Reinforcement Learning Abstract: An important property for lifelong-learning agents is the ability to combine\nexisting skills to solve unseen tasks. In general, however, it is unclear how\nto compose skills in a principled way. We provide a \"recipe\" for optimal value\nfunction composition in entropy-regularised reinforcement learning (RL) and\nthen extend this to the standard RL setting. Composition is demonstrated in a\nvideo game environment, where an agent with an existing library of policies is\nable to solve new tasks without the need for further learning. \n\n"}
{"id": "1807.04734", "contents": "Title: Scalable Convolutional Dictionary Learning with Constrained Recurrent\n  Sparse Auto-encoders Abstract: Given a convolutional dictionary underlying a set of observed signals, can a\ncarefully designed auto-encoder recover the dictionary in the presence of\nnoise? We introduce an auto-encoder architecture, termed constrained recurrent\nsparse auto-encoder (CRsAE), that answers this question in the affirmative.\nGiven an input signal and an approximate dictionary, the encoder finds a sparse\napproximation using FISTA. The decoder reconstructs the signal by applying the\ndictionary to the output of the encoder. The encoder and decoder in CRsAE\nparallel the sparse-coding and dictionary update steps in optimization-based\nalternating-minimization schemes for dictionary learning. As such, the\nparameters of the encoder and decoder are not independent, a constraint which\nwe enforce for the first time. We derive the back-propagation algorithm for\nCRsAE. CRsAE is a framework for blind source separation that, only knowing the\nnumber of sources (dictionary elements), and assuming sparsely-many can\noverlap, is able to separate them. We demonstrate its utility in the context of\nspike sorting, a source separation problem in computational neuroscience. We\ndemonstrate the ability of CRsAE to recover the underlying dictionary and\ncharacterize its sensitivity as a function of SNR. \n\n"}
{"id": "1807.04855", "contents": "Title: A feature agnostic approach for glaucoma detection in OCT volumes Abstract: Optical coherence tomography (OCT) based measurements of retinal layer\nthickness, such as the retinal nerve fibre layer (RNFL) and the ganglion cell\nwith inner plexiform layer (GCIPL) are commonly used for the diagnosis and\nmonitoring of glaucoma. Previously, machine learning techniques have utilized\nsegmentation-based imaging features such as the peripapillary RNFL thickness\nand the cup-to-disc ratio. Here, we propose a deep learning technique that\nclassifies eyes as healthy or glaucomatous directly from raw, unsegmented OCT\nvolumes of the optic nerve head (ONH) using a 3D Convolutional Neural Network\n(CNN). We compared the accuracy of this technique with various feature-based\nmachine learning algorithms and demonstrated the superiority of the proposed\ndeep learning based method.\n  Logistic regression was found to be the best performing classical machine\nlearning technique with an AUC of 0.89. In direct comparison, the deep learning\napproach achieved a substantially higher AUC of 0.94 with the additional\nadvantage of providing insight into which regions of an OCT volume are\nimportant for glaucoma detection.\n  Computing Class Activation Maps (CAM), we found that the CNN identified\nneuroretinal rim and optic disc cupping as well as the lamina cribrosa (LC) and\nits surrounding areas as the regions significantly associated with the glaucoma\nclassification. These regions anatomically correspond to the well established\nand commonly used clinical markers for glaucoma diagnosis such as increased cup\nvolume, cup diameter, and neuroretinal rim thinning at the superior and\ninferior segments. \n\n"}
{"id": "1807.05827", "contents": "Title: Remember and Forget for Experience Replay Abstract: Experience replay (ER) is a fundamental component of off-policy deep\nreinforcement learning (RL). ER recalls experiences from past iterations to\ncompute gradient estimates for the current policy, increasing data-efficiency.\nHowever, the accuracy of such updates may deteriorate when the policy diverges\nfrom past behaviors and can undermine the performance of ER. Many algorithms\nmitigate this issue by tuning hyper-parameters to slow down policy changes. An\nalternative is to actively enforce the similarity between policy and the\nexperiences in the replay memory. We introduce Remember and Forget Experience\nReplay (ReF-ER), a novel method that can enhance RL algorithms with\nparameterized policies. ReF-ER (1) skips gradients computed from experiences\nthat are too unlikely with the current policy and (2) regulates policy changes\nwithin a trust region of the replayed behaviors. We couple ReF-ER with\nQ-learning, deterministic policy gradient and off-policy gradient methods. We\nfind that ReF-ER consistently improves the performance of continuous-action,\noff-policy RL on fully observable benchmarks and partially observable flow\ncontrol problems. \n\n"}
{"id": "1807.05979", "contents": "Title: Lesion Analysis and Diagnosis with Mask-RCNN Abstract: This project applies Mask R-CNN method to ISIC 2018 challenge tasks: lesion\nboundary segmentation (task1), lesion attributes detection (task 2), lesion\ndiagnosis (task 3), a solution to the latter is using a trained model for task\n1 and a simple voting procedure. \n\n"}
{"id": "1807.06160", "contents": "Title: Layer-wise Relevance Propagation for Explainable Recommendations Abstract: In this paper, we tackle the problem of explanations in a deep-learning based\nmodel for recommendations by leveraging the technique of layer-wise relevance\npropagation. We use a Deep Convolutional Neural Network to extract relevant\nfeatures from the input images before identifying similarity between the images\nin feature space. Relationships between the images are identified by the model\nand layer-wise relevance propagation is used to infer pixel-level details of\nthe images that may have significantly informed the model's choice. We evaluate\nour method on an Amazon products dataset and demonstrate the efficacy of our\napproach. \n\n"}
{"id": "1807.06699", "contents": "Title: Adaptive Neural Trees Abstract: Deep neural networks and decision trees operate on largely separate\nparadigms; typically, the former performs representation learning with\npre-specified architectures, while the latter is characterised by learning\nhierarchies over pre-specified features with data-driven architectures. We\nunite the two via adaptive neural trees (ANTs) that incorporates representation\nlearning into edges, routing functions and leaf nodes of a decision tree, along\nwith a backpropagation-based training algorithm that adaptively grows the\narchitecture from primitive modules (e.g., convolutional layers). We\ndemonstrate that, whilst achieving competitive performance on classification\nand regression datasets, ANTs benefit from (i) lightweight inference via\nconditional computation, (ii) hierarchical separation of features useful to the\ntask e.g. learning meaningful class associations, such as separating natural\nvs. man-made objects, and (iii) a mechanism to adapt the architecture to the\nsize and complexity of the training dataset. \n\n"}
{"id": "1807.07215", "contents": "Title: Machine Learning Classifiers Do Not Improve the Prediction of Academic\n  Risk: Evidence from Australia Abstract: Machine learning methods tend to outperform traditional statistical models at\nprediction. In the prediction of academic achievement, ML models have not shown\nsubstantial improvement over logistic regression. So far, these results have\nalmost entirely focused on college achievement, due to the availability of\nadministrative datasets, and have contained relatively small sample sizes by ML\nstandards. In this article we apply popular machine learning models to a large\ndataset ($n=1.2$ million) containing primary and middle school performance on a\nstandardized test given annually to Australian students. We show that machine\nlearning models do not outperform logistic regression for detecting students\nwho will perform in the `below standard' band of achievement upon sitting their\nnext test, even in a large-$n$ setting. \n\n"}
{"id": "1807.07603", "contents": "Title: Doubly Stochastic Adversarial Autoencoder Abstract: Any autoencoder network can be turned into a generative model by imposing an\narbitrary prior distribution on its hidden code vector. Variational Autoencoder\n(VAE) [2] uses a KL divergence penalty to impose the prior, whereas Adversarial\nAutoencoder (AAE) [1] uses {\\it generative adversarial networks} GAN [3]. GAN\ntrades the complexities of {\\it sampling} algorithms with the complexities of\n{\\it searching} Nash equilibrium in minimax games. Such minimax architectures\nget trained with the help of data examples and gradients flowing through a\ngenerator and an adversary. A straightforward modification of AAE is to replace\nthe adversary with the maximum mean discrepancy (MMD) test [4-5]. This\nreplacement leads to a new type of probabilistic autoencoder, which is also\ndiscussed in our paper. We propose a novel probabilistic autoencoder in which\nthe adversary of AAE is replaced with a space of {\\it stochastic} functions.\nThis replacement introduces a new source of randomness, which can be considered\nas a continuous control for encouraging {\\it explorations}. This prevents the\nadversary from fitting too closely to the generator and therefore leads to a\nmore diverse set of generated samples. \n\n"}
{"id": "1807.07998", "contents": "Title: Convolutional Neural Networks Analyzed via Inverse Problem Theory and\n  Sparse Representations Abstract: Inverse problems in imaging such as denoising, deblurring, superresolution\n(SR) have been addressed for many decades. In recent years, convolutional\nneural networks (CNNs) have been widely used for many inverse problem areas.\nAlthough their indisputable success, CNNs are not mathematically validated as\nto how and what they learn. In this paper, we prove that during training, CNN\nelements solve for inverse problems which are optimum solutions stored as CNN\nneuron filters. We discuss the necessity of mutual coherence between CNN layer\nelements in order for a network to converge to the optimum solution. We prove\nthat required mutual coherence can be provided by the usage of residual\nlearning and skip connections. We have set rules over training sets and depth\nof networks for better convergence, i.e. performance. \n\n"}
{"id": "1807.08706", "contents": "Title: Contrastive Explanations for Reinforcement Learning in terms of Expected\n  Consequences Abstract: Machine Learning models become increasingly proficient in complex tasks.\nHowever, even for experts in the field, it can be difficult to understand what\nthe model learned. This hampers trust and acceptance, and it obstructs the\npossibility to correct the model. There is therefore a need for transparency of\nmachine learning models. The development of transparent classification models\nhas received much attention, but there are few developments for achieving\ntransparent Reinforcement Learning (RL) models. In this study we propose a\nmethod that enables a RL agent to explain its behavior in terms of the expected\nconsequences of state transitions and outcomes. First, we define a translation\nof states and actions to a description that is easier to understand for human\nusers. Second, we developed a procedure that enables the agent to obtain the\nconsequences of a single action, as well as its entire policy. The method\ncalculates contrasts between the consequences of a policy derived from a user\nquery, and of the learned policy of the agent. Third, a format for generating\nexplanations was constructed. A pilot survey study was conducted to explore\npreferences of users for different explanation properties. Results indicate\nthat human users tend to favor explanations about policy rather than about\nsingle actions. \n\n"}
{"id": "1807.08844", "contents": "Title: Lesion segmentation using U-Net network Abstract: This paper explains the method used in the segmentation challenge (Task 1) in\nthe International Skin Imaging Collaboration's (ISIC) Skin Lesion Analysis\nTowards Melanoma Detection challenge held in 2018. We have trained a U-Net\nnetwork to perform the segmentation. The key elements for the training were\nfirst to adjust the loss function to incorporate unbalanced proportion of\nbackground and second to perform post-processing operation to adjust the\ncontour of the prediction. \n\n"}
{"id": "1807.08934", "contents": "Title: SAAGs: Biased Stochastic Variance Reduction Methods for Large-scale\n  Learning Abstract: Stochastic approximation is one of the effective approach to deal with the\nlarge-scale machine learning problems and the recent research has focused on\nreduction of variance, caused by the noisy approximations of the gradients. In\nthis paper, we have proposed novel variants of SAAG-I and II (Stochastic\nAverage Adjusted Gradient) (Chauhan et al. 2017), called SAAG-III and IV,\nrespectively. Unlike SAAG-I, starting point is set to average of previous epoch\nin SAAG-III, and unlike SAAG-II, the snap point and starting point are set to\naverage and last iterate of previous epoch in SAAG-IV, respectively. To\ndetermine the step size, we have used Stochastic Backtracking-Armijo line\nSearch (SBAS) which performs line search only on selected mini-batch of data\npoints. Since backtracking line search is not suitable for large-scale problems\nand the constants used to find the step size, like Lipschitz constant, are not\nalways available so SBAS could be very effective in such cases. We have\nextended SAAGs (I, II, III and IV) to solve non-smooth problems and designed\ntwo update rules for smooth and non-smooth problems. Moreover, our theoretical\nresults have proved linear convergence of SAAG-IV for all the four combinations\nof smoothness and strong-convexity, in expectation. Finally, our experimental\nstudies have proved the efficacy of proposed methods against the state-of-art\ntechniques. \n\n"}
{"id": "1807.09427", "contents": "Title: Multi-Agent Reinforcement Learning: A Report on Challenges and\n  Approaches Abstract: Reinforcement Learning (RL) is a learning paradigm concerned with learning to\ncontrol a system so as to maximize an objective over the long term. This\napproach to learning has received immense interest in recent times and success\nmanifests itself in the form of human-level performance on games like\n\\textit{Go}. While RL is emerging as a practical component in real-life\nsystems, most successes have been in Single Agent domains. This report will\ninstead specifically focus on challenges that are unique to Multi-Agent Systems\ninteracting in mixed cooperative and competitive environments. The report\nconcludes with advances in the paradigm of training Multi-Agent Systems called\n\\textit{Decentralized Actor, Centralized Critic}, based on an extension of MDPs\ncalled \\textit{Decentralized Partially Observable MDP}s, which has seen a\nrenewed interest lately. \n\n"}
{"id": "1807.09586", "contents": "Title: Perturb and Combine to Identify Influential Spreaders in Real-World\n  Networks Abstract: Some of the most effective influential spreader detection algorithms are\nunstable to small perturbations of the network structure. Inspired by bagging\nin Machine Learning, we propose the first Perturb and Combine (P&C) procedure\nfor networks. It (1) creates many perturbed versions of a given graph, (2)\napplies a node scoring function separately to each graph, and (3) combines the\nresults. Experiments conducted on real-world networks of various sizes with the\nk-core, generalized k-core, and PageRank algorithms reveal that P&C brings\nsubstantial improvements. Moreover, this performance boost can be obtained at\nalmost no extra cost through parallelization. Finally, a bias-variance analysis\nsuggests that P&C works mainly by reducing bias, and that therefore, it should\nbe capable of improving the performance of all vertex scoring functions,\nincluding stable ones. \n\n"}
{"id": "1807.09705", "contents": "Title: Limitations of the Lipschitz constant as a defense against adversarial\n  examples Abstract: Several recent papers have discussed utilizing Lipschitz constants to limit\nthe susceptibility of neural networks to adversarial examples. We analyze\nrecently proposed methods for computing the Lipschitz constant. We show that\nthe Lipschitz constant may indeed enable adversarially robust neural networks.\nHowever, the methods currently employed for computing it suffer from\ntheoretical and practical limitations. We argue that addressing this\nshortcoming is a promising direction for future research into certified\nadversarial defenses. \n\n"}
{"id": "1807.09979", "contents": "Title: Bayesian Optimal Design of Experiments For Inferring The Statistical\n  Expectation Of A Black-Box Function Abstract: Bayesian optimal design of experiments (BODE) has been successful in\nacquiring information about a quantity of interest (QoI) which depends on a\nblack-box function. BODE is characterized by sequentially querying the function\nat specific designs selected by an infill-sampling criterion. However, most\ncurrent BODE methods operate in specific contexts like optimization, or\nlearning a universal representation of the black-box function. The objective of\nthis paper is to design a BODE for estimating the statistical expectation of a\nphysical response surface. This QoI is omnipresent in uncertainty propagation\nand design under uncertainty problems. Our hypothesis is that an optimal BODE\nshould be maximizing the expected information gain in the QoI. We represent the\ninformation gain from a hypothetical experiment as the Kullback-Liebler (KL)\ndivergence between the prior and the posterior probability distributions of the\nQoI. The prior distribution of the QoI is conditioned on the observed data and\nthe posterior distribution of the QoI is conditioned on the observed data and a\nhypothetical experiment. The main contribution of this paper is the derivation\nof a semi-analytic mathematical formula for the expected information gain about\nthe statistical expectation of a physical response. The developed BODE is\nvalidated on synthetic functions with varying number of input-dimensions. We\ndemonstrate the performance of the methodology on a steel wire manufacturing\nproblem. \n\n"}
{"id": "1807.10096", "contents": "Title: Towards a Deep Unified Framework for Nuclear Reactor Perturbation\n  Analysis Abstract: In this paper, we take the first steps towards a novel unified framework for\nthe analysis of perturbations in both the Time and Frequency domains. The\nidentification of type and source of such perturbations is fundamental for\nmonitoring reactor cores and guarantee safety while running at nominal\nconditions. A 3D Convolutional Neural Network (3D-CNN) was employed to analyse\nperturbations happening in the frequency domain, such as an absorber of\nvariable strength or propagating perturbation. Recurrent neural networks (RNN),\nspecifically Long Short-Term Memory (LSTM) networks were used to study signal\nsequences related to perturbations induced in the time domain, including the\nvibrations of fuel assemblies and the fluctuations of thermal-hydraulic\nparameters at the inlet of the reactor coolant loops. 512 dimensional\nrepresentations were extracted from the 3D-CNN and LSTM architectures, and used\nas input to a fused multi-sigmoid classification layer to recognise the\nperturbation type. If the perturbation is in the frequency domain, a separate\nfully-connected layer utilises said representations to regress the coordinates\nof its source. The results showed that the perturbation type can be recognised\nwith high accuracy in all cases, and frequency domain scenario sources can be\nlocalised with high precision. \n\n"}
{"id": "1807.10225", "contents": "Title: Medical Image Synthesis for Data Augmentation and Anonymization using\n  Generative Adversarial Networks Abstract: Data diversity is critical to success when training deep learning models.\nMedical imaging data sets are often imbalanced as pathologic findings are\ngenerally rare, which introduces significant challenges when training deep\nlearning models. In this work, we propose a method to generate synthetic\nabnormal MRI images with brain tumors by training a generative adversarial\nnetwork using two publicly available data sets of brain MRI. We demonstrate two\nunique benefits that the synthetic images provide. First, we illustrate\nimproved performance on tumor segmentation by leveraging the synthetic images\nas a form of data augmentation. Second, we demonstrate the value of generative\nmodels as an anonymization tool, achieving comparable tumor segmentation\nresults when trained on the synthetic data versus when trained on real subject\ndata. Together, these results offer a potential solution to two of the largest\nchallenges facing machine learning in medical imaging, namely the small\nincidence of pathological findings, and the restrictions around sharing of\npatient data. \n\n"}
{"id": "1807.10570", "contents": "Title: Embedded Implementation of a Deep Learning Smile Detector Abstract: In this paper we study the real time deployment of deep learning algorithms\nin low resource computational environments. As the use case, we compare the\naccuracy and speed of neural networks for smile detection using different\nneural network architectures and their system level implementation on NVidia\nJetson embedded platform. We also propose an asynchronous multithreading scheme\nfor parallelizing the pipeline. Within this framework, we experimentally\ncompare thirteen widely used network topologies. The experiments show that low\ncomplexity architectures can achieve almost equal performance as larger ones,\nwith a fraction of computation required. \n\n"}
{"id": "1807.10584", "contents": "Title: Uncertainty and Interpretability in Convolutional Neural Networks for\n  Semantic Segmentation of Colorectal Polyps Abstract: Convolutional Neural Networks (CNNs) are propelling advances in a range of\ndifferent computer vision tasks such as object detection and object\nsegmentation. Their success has motivated research in applications of such\nmodels for medical image analysis. If CNN-based models are to be helpful in a\nmedical context, they need to be precise, interpretable, and uncertainty in\npredictions must be well understood. In this paper, we develop and evaluate\nrecent advances in uncertainty estimation and model interpretability in the\ncontext of semantic segmentation of polyps from colonoscopy images. We evaluate\nand enhance several architectures of Fully Convolutional Networks (FCNs) for\nsemantic segmentation of colorectal polyps and provide a comparison between\nthese models. Our highest performing model achieves a 76.06\\% mean IOU accuracy\non the EndoScene dataset, a considerable improvement over the previous\nstate-of-the-art. \n\n"}
{"id": "1807.10588", "contents": "Title: A Modality-Adaptive Method for Segmenting Brain Tumors and\n  Organs-at-Risk in Radiation Therapy Planning Abstract: In this paper we present a method for simultaneously segmenting brain tumors\nand an extensive set of organs-at-risk for radiation therapy planning of\nglioblastomas. The method combines a contrast-adaptive generative model for\nwhole-brain segmentation with a new spatial regularization model of tumor shape\nusing convolutional restricted Boltzmann machines. We demonstrate\nexperimentally that the method is able to adapt to image acquisitions that\ndiffer substantially from any available training data, ensuring its\napplicability across treatment sites; that its tumor segmentation accuracy is\ncomparable to that of the current state of the art; and that it captures most\norgans-at-risk sufficiently well for radiation therapy planning purposes. The\nproposed method may be a valuable step towards automating the delineation of\nbrain tumors and organs-at-risk in glioblastoma patients undergoing radiation\ntherapy. \n\n"}
{"id": "1807.10600", "contents": "Title: A post-processing method to improve the white matter hyperintensity\n  segmentation accuracy for randomly-initialized U-net Abstract: White matter hyperintensity (WMH) is commonly found in elder individuals and\nappears to be associated with brain diseases. U-net is a convolutional network\nthat has been widely used for biomedical image segmentation. Recently, U-net\nhas been successfully applied to WMH segmentation. Random initialization is\nusally used to initialize the model weights in the U-net. However, the model\nmay coverage to different local optima with different randomly initialized\nweights. We find a combination of thresholding and averaging the outputs of\nU-nets with different random initializations can largely improve the WMH\nsegmentation accuracy. Based on this observation, we propose a post-processing\ntechnique concerning the way how averaging and thresholding are conducted.\nSpecifically, we first transfer the score maps from three U-nets to binary\nmasks via thresholding and then average those binary masks to obtain the final\nWMH segmentation. Both quantitative analysis (via the Dice similarity\ncoefficient) and qualitative analysis (via visual examinations) reveal the\nsuperior performance of the proposed method. This post-processing technique is\nindependent of the model used. As such, it can also be applied to situations\nwhere other deep learning models are employed, especially when random\ninitialization is adopted and pre-training is unavailable. \n\n"}
{"id": "1807.11620", "contents": "Title: K-medoids Clustering of Data Sequences with Composite Distributions Abstract: This paper studies clustering of data sequences using the k-medoids\nalgorithm. All the data sequences are assumed to be generated from\n\\emph{unknown} continuous distributions, which form clusters with each cluster\ncontaining a composite set of closely located distributions (based on a certain\ndistance metric between distributions). The maximum intra-cluster distance is\nassumed to be smaller than the minimum inter-cluster distance, and both values\nare assumed to be known. The goal is to group the data sequences together if\ntheir underlying generative distributions (which are unknown) belong to one\ncluster. Distribution distance metrics based k-medoids algorithms are proposed\nfor known and unknown number of distribution clusters. Upper bounds on the\nerror probability and convergence results in the large sample regime are also\nprovided. It is shown that the error probability decays exponentially fast as\nthe number of samples in each data sequence goes to infinity. The error\nexponent has a simple form regardless of the distance metric applied when\ncertain conditions are satisfied. In particular, the error exponent is\ncharacterized when either the Kolmogrov-Smirnov distance or the maximum mean\ndiscrepancy are used as the distance metric. Simulation results are provided to\nvalidate the analysis. \n\n"}
{"id": "1807.11697", "contents": "Title: Multimodal Deep Domain Adaptation Abstract: Typically a classifier trained on a given dataset (source domain) does not\nperforms well if it is tested on data acquired in a different setting (target\ndomain). This is the problem that domain adaptation (DA) tries to overcome and,\nwhile it is a well explored topic in computer vision, it is largely ignored in\nrobotic vision where usually visual classification methods are trained and\ntested in the same domain. Robots should be able to deal with unknown\nenvironments, recognize objects and use them in the correct way, so it is\nimportant to explore the domain adaptation scenario also in this context. The\ngoal of the project is to define a benchmark and a protocol for multi-modal\ndomain adaptation that is valuable for the robot vision community. With this\npurpose some of the state-of-the-art DA methods are selected: Deep Adaptation\nNetwork (DAN), Domain Adversarial Training of Neural Network (DANN), Automatic\nDomain Alignment Layers (AutoDIAL) and Adversarial Discriminative Domain\nAdaptation (ADDA). Evaluations have been done using different data types: RGB\nonly, depth only and RGB-D over the following datasets, designed for the\nrobotic community: RGB-D Object Dataset (ROD), Web Object Dataset (WOD),\nAutonomous Robot Indoor Dataset (ARID), Big Berkeley Instance Recognition\nDataset (BigBIRD) and Active Vision Dataset. Although progresses have been made\non the formulation of effective adaptation algorithms and more realistic object\ndatasets are available, the results obtained show that, training a sufficiently\ngood object classifier, especially in the domain adaptation scenario, is still\nan unsolved problem. Also the best way to combine depth with RGB informations\nto improve the performance is a point that needs to be investigated more. \n\n"}
{"id": "1808.00232", "contents": "Title: Off-Policy Evaluation and Learning from Logged Bandit Feedback: Error\n  Reduction via Surrogate Policy Abstract: When learning from a batch of logged bandit feedback, the discrepancy between\nthe policy to be learned and the off-policy training data imposes statistical\nand computational challenges. Unlike classical supervised learning and online\nlearning settings, in batch contextual bandit learning, one only has access to\na collection of logged feedback from the actions taken by a historical policy,\nand expect to learn a policy that takes good actions in possibly unseen\ncontexts. Such a batch learning setting is ubiquitous in online and interactive\nsystems, such as ad platforms and recommendation systems. Existing approaches\nbased on inverse propensity weights, such as Inverse Propensity Scoring (IPS)\nand Policy Optimizer for Exponential Models (POEM), enjoy unbiasedness but\noften suffer from large mean squared error. In this work, we introduce a new\napproach named Maximum Likelihood Inverse Propensity Scoring (MLIPS) for batch\nlearning from logged bandit feedback. Instead of using the given historical\npolicy as the proposal in inverse propensity weights, we estimate a maximum\nlikelihood surrogate policy based on the logged action-context pairs, and then\nuse this surrogate policy as the proposal. We prove that MLIPS is\nasymptotically unbiased, and moreover, has a smaller nonasymptotic mean squared\nerror than IPS. Such an error reduction phenomenon is somewhat surprising as\nthe estimated surrogate policy is less accurate than the given historical\npolicy. Results on multi-label classification problems and a large- scale ad\nplacement dataset demonstrate the empirical effectiveness of MLIPS.\nFurthermore, the proposed surrogate policy technique is complementary to\nexisting error reduction techniques, and when combined, is able to consistently\nboost the performance of several widely used approaches. \n\n"}
{"id": "1808.00529", "contents": "Title: Open Category Detection with PAC Guarantees Abstract: Open category detection is the problem of detecting \"alien\" test instances\nthat belong to categories or classes that were not present in the training\ndata. In many applications, reliably detecting such aliens is central to\nensuring the safety and accuracy of test set predictions. Unfortunately, there\nare no algorithms that provide theoretical guarantees on their ability to\ndetect aliens under general assumptions. Further, while there are algorithms\nfor open category detection, there are few empirical results that directly\nreport alien detection rates. Thus, there are significant theoretical and\nempirical gaps in our understanding of open category detection. In this paper,\nwe take a step toward addressing this gap by studying a simple, but\npractically-relevant variant of open category detection. In our setting, we are\nprovided with a \"clean\" training set that contains only the target categories\nof interest and an unlabeled \"contaminated\" training set that contains a\nfraction $\\alpha$ of alien examples. Under the assumption that we know an upper\nbound on $\\alpha$, we develop an algorithm with PAC-style guarantees on the\nalien detection rate, while aiming to minimize false alarms. Empirical results\non synthetic and standard benchmark datasets demonstrate the regimes in which\nthe algorithm can be effective and provide a baseline for further advancements. \n\n"}
{"id": "1808.00783", "contents": "Title: The Quest for the Golden Activation Function Abstract: Deep Neural Networks have been shown to be beneficial for a variety of tasks,\nin particular allowing for end-to-end learning and reducing the requirement for\nmanual design decisions. However, still many parameters have to be chosen in\nadvance, also raising the need to optimize them. One important, but often\nignored system parameter is the selection of a proper activation function.\nThus, in this paper we target to demonstrate the importance of activation\nfunctions in general and show that for different tasks different activation\nfunctions might be meaningful. To avoid the manual design or selection of\nactivation functions, we build on the idea of genetic algorithms to learn the\nbest activation function for a given task. In addition, we introduce two new\nactivation functions, ELiSH and HardELiSH, which can easily be incorporated in\nour framework. In this way, we demonstrate for three different image\nclassification benchmarks that different activation functions are learned, also\nshowing improved results compared to typically used baselines. \n\n"}
{"id": "1808.01357", "contents": "Title: A recurrent multi-scale approach to RBG-D Object Recognition Abstract: Technological development aims to produce generations of increasingly\nefficient robots able to perform complex tasks. This requires considerable\nefforts, from the scientific community, to find new algorithms that solve\ncomputer vision problems, such as object recognition. The diffusion of RGB-D\ncameras directed the study towards the research of new architectures able to\nexploit the RGB and Depth information. The project that is developed in this\nthesis concerns the realization of a new end-to-end architecture for the\nrecognition of RGB-D objects called RCFusion. Our method generates compact and\nhighly discriminative multi-modal features by combining complementary RGB and\ndepth information representing different levels of abstraction. We evaluate our\nmethod on standard object recognition datasets, RGB-D Object Dataset and\nJHUIT-50. The experiments performed show that our method outperforms the\nexisting approaches and establishes new state-of-the-art results for both\ndatasets. \n\n"}
{"id": "1808.02316", "contents": "Title: Modelling hidden structure of signals in group data analysis with\n  modified (Lr, 1) and block-term decompositions Abstract: This work is devoted to elaboration on the idea to use block term\ndecomposition for group data analysis and to raise the possibility of modelling\ngroup activity with (Lr, 1) and Tucker blocks. A new generalization of block\ntensor decomposition was considered in application to group data analysis.\nSuggested approach was evaluated on multilabel classification task for a set of\nimages. This contribution also reports results of investigation on clustering\nwith proposed tensor models in comparison with known matrix models, namely\ncommon orthogonal basis extraction and group independent component analysis. \n\n"}
{"id": "1808.02513", "contents": "Title: Rethinking Numerical Representations for Deep Neural Networks Abstract: With ever-increasing computational demand for deep learning, it is critical\nto investigate the implications of the numeric representation and precision of\nDNN model weights and activations on computational efficiency. In this work, we\nexplore unconventional narrow-precision floating-point representations as it\nrelates to inference accuracy and efficiency to steer the improved design of\nfuture DNN platforms. We show that inference using these custom numeric\nrepresentations on production-grade DNNs, including GoogLeNet and VGG, achieves\nan average speedup of 7.6x with less than 1% degradation in inference accuracy\nrelative to a state-of-the-art baseline platform representing the most\nsophisticated hardware using single-precision floating point. To facilitate the\nuse of such customized precision, we also present a novel technique that\ndrastically reduces the time required to derive the optimal precision\nconfiguration. \n\n"}
{"id": "1808.02610", "contents": "Title: L-Shapley and C-Shapley: Efficient Model Interpretation for Structured\n  Data Abstract: We study instancewise feature importance scoring as a method for model\ninterpretation. Any such method yields, for each predicted instance, a vector\nof importance scores associated with the feature vector. Methods based on the\nShapley score have been proposed as a fair way of computing feature\nattributions of this kind, but incur an exponential complexity in the number of\nfeatures. This combinatorial explosion arises from the definition of the\nShapley value and prevents these methods from being scalable to large data sets\nand complex models. We focus on settings in which the data have a graph\nstructure, and the contribution of features to the target variable is\nwell-approximated by a graph-structured factorization. In such settings, we\ndevelop two algorithms with linear complexity for instancewise feature\nimportance scoring. We establish the relationship of our methods to the Shapley\nvalue and another closely related concept known as the Myerson value from\ncooperative game theory. We demonstrate on both language and image data that\nour algorithms compare favorably with other methods for model interpretation. \n\n"}
{"id": "1808.03030", "contents": "Title: Policy Optimization as Wasserstein Gradient Flows Abstract: Policy optimization is a core component of reinforcement learning (RL), and\nmost existing RL methods directly optimize parameters of a policy based on\nmaximizing the expected total reward, or its surrogate. Though often achieving\nencouraging empirical success, its underlying mathematical principle on {\\em\npolicy-distribution} optimization is unclear. We place policy optimization into\nthe space of probability measures, and interpret it as Wasserstein gradient\nflows. On the probability-measure space, under specified circumstances, policy\noptimization becomes a convex problem in terms of distribution optimization. To\nmake optimization feasible, we develop efficient algorithms by numerically\nsolving the corresponding discrete gradient flows. Our technique is applicable\nto several RL settings, and is related to many state-of-the-art\npolicy-optimization algorithms. Empirical results verify the effectiveness of\nour framework, often obtaining better performance compared to related\nalgorithms. \n\n"}
{"id": "1808.03591", "contents": "Title: How Complex is your classification problem? A survey on measuring\n  classification complexity Abstract: Characteristics extracted from the training datasets of classification\nproblems have proven to be effective predictors in a number of meta-analyses.\nAmong them, measures of classification complexity can be used to estimate the\ndifficulty in separating the data points into their expected classes.\nDescriptors of the spatial distribution of the data and estimates of the shape\nand size of the decision boundary are among the known measures for this\ncharacterization. This information can support the formulation of new\ndata-driven pre-processing and pattern recognition techniques, which can in\nturn be focused on challenges highlighted by such characteristics of the\nproblems. This paper surveys and analyzes measures which can be extracted from\nthe training datasets in order to characterize the complexity of the respective\nclassification problems. Their use in recent literature is also reviewed and\ndiscussed, allowing to prospect opportunities for future work in the area.\nFinally, descriptions are given on an R package named Extended Complexity\nLibrary (ECoL) that implements a set of complexity measures and is made\npublicly available. \n\n"}
{"id": "1808.03856", "contents": "Title: Neural Importance Sampling Abstract: We propose to use deep neural networks for generating samples in Monte Carlo\nintegration. Our work is based on non-linear independent components estimation\n(NICE), which we extend in numerous ways to improve performance and enable its\napplication to integration problems. First, we introduce piecewise-polynomial\ncoupling transforms that greatly increase the modeling power of individual\ncoupling layers. Second, we propose to preprocess the inputs of neural networks\nusing one-blob encoding, which stimulates localization of computation and\nimproves inference. Third, we derive a gradient-descent-based optimization for\nthe KL and the $\\chi^2$ divergence for the specific application of Monte Carlo\nintegration with unnormalized stochastic estimates of the target distribution.\nOur approach enables fast and accurate inference and efficient sample\ngeneration independently of the dimensionality of the integration domain. We\nshow its benefits on generating natural images and in two applications to\nlight-transport simulation: first, we demonstrate learning of joint\npath-sampling densities in the primary sample space and importance sampling of\nmulti-dimensional path prefixes thereof. Second, we use our technique to\nextract conditional directional densities driven by the product of incident\nillumination and the BSDF in the rendering equation, and we leverage the\ndensities for path guiding. In all applications, our approach yields on-par or\nhigher performance than competing techniques at equal sample count. \n\n"}
{"id": "1808.04256", "contents": "Title: CT Super-resolution GAN Constrained by the Identical, Residual, and\n  Cycle Learning Ensemble(GAN-CIRCLE) Abstract: Computed tomography (CT) is widely used in screening, diagnosis, and\nimage-guided therapy for both clinical and research purposes. Since CT involves\nionizing radiation, an overarching thrust of related technical research is\ndevelopment of novel methods enabling ultrahigh quality imaging with fine\nstructural details while reducing the X-ray radiation. In this paper, we\npresent a semi-supervised deep learning approach to accurately recover\nhigh-resolution (HR) CT images from low-resolution (LR) counterparts.\nSpecifically, with the generative adversarial network (GAN) as the building\nblock, we enforce the cycle-consistency in terms of the Wasserstein distance to\nestablish a nonlinear end-to-end mapping from noisy LR input images to denoised\nand deblurred HR outputs. We also include the joint constraints in the loss\nfunction to facilitate structural preservation. In this deep imaging process,\nwe incorporate deep convolutional neural network (CNN), residual learning, and\nnetwork in network techniques for feature extraction and restoration. In\ncontrast to the current trend of increasing network depth and complexity to\nboost the CT imaging performance, which limit its real-world applications by\nimposing considerable computational and memory overheads, we apply a parallel\n$1\\times1$ CNN to compress the output of the hidden layer and optimize the\nnumber of layers and the number of filters for each convolutional layer.\nQuantitative and qualitative evaluations demonstrate that our proposed model is\naccurate, efficient and robust for super-resolution (SR) image restoration from\nnoisy LR input images. In particular, we validate our composite SR networks on\nthree large-scale CT datasets, and obtain promising results as compared to the\nother state-of-the-art methods. \n\n"}
{"id": "1808.04308", "contents": "Title: Explaining the Unique Nature of Individual Gait Patterns with Deep\n  Learning Abstract: Machine learning (ML) techniques such as (deep) artificial neural networks\n(DNN) are solving very successfully a plethora of tasks and provide new\npredictive models for complex physical, chemical, biological and social\nsystems. However, in most cases this comes with the disadvantage of acting as a\nblack box, rarely providing information about what made them arrive at a\nparticular prediction. This black box aspect of ML techniques can be\nproblematic especially in medical diagnoses, so far hampering a clinical\nacceptance. The present paper studies the uniqueness of individual gait\npatterns in clinical biomechanics using DNNs. By attributing portions of the\nmodel predictions back to the input variables (ground reaction forces and\nfull-body joint angles), the Layer-Wise Relevance Propagation (LRP) technique\nreliably demonstrates which variables at what time windows of the gait cycle\nare most relevant for the characterisation of gait patterns from a certain\nindividual. By measuring the time-resolved contribution of each input variable\nto the prediction of ML techniques such as DNNs, our method describes the first\ngeneral framework that enables to understand and interpret non-linear ML\nmethods in (biomechanical) gait analysis and thereby supplies a powerful tool\nfor analysis, diagnosis and treatment of human gait. \n\n"}
{"id": "1808.04433", "contents": "Title: Out of the Black Box: Properties of deep neural networks and their\n  applications Abstract: Deep neural networks are powerful machine learning approaches that have\nexhibited excellent results on many classification tasks. However, they are\nconsidered as black boxes and some of their properties remain to be formalized.\nIn the context of image recognition, it is still an arduous task to understand\nwhy an image is recognized or not. In this study, we formalize some properties\nshared by eight state-of-the-art deep neural networks in order to grasp the\nprinciples allowing a given deep neural network to classify an image. Our\nresults, tested on these eight networks, show that an image can be sub-divided\ninto several regions (patches) responding at different degrees of probability\n(local property). With the same patch, some locations in the image can answer\ntwo (or three) orders of magnitude higher than other locations (spatial\nproperty). Some locations are activators and others inhibitors\n(activation-inhibition property). The repetition of the same patch can increase\n(or decrease) the probability of recognition of an object (cumulative\nproperty). Furthermore, we propose a new approach called Deepception that\nexploits these properties to deceive a deep neural network. We obtain for the\nVGG-VDD-19 neural network a fooling ratio of 88\\%. Thanks to our\n\"Psychophysics\" approach, no prior knowledge on the networks architectures is\nrequired. \n\n"}
{"id": "1808.04550", "contents": "Title: SciSports: Learning football kinematics through two-dimensional tracking\n  data Abstract: SciSports is a Dutch startup company specializing in football analytics. This\npaper describes a joint research effort with SciSports, during the Study Group\nMathematics with Industry 2018 at Eindhoven, the Netherlands. The main\nchallenge that we addressed was to automatically process empirical football\nplayers' trajectories, in order to extract useful information from them. The\ndata provided to us was two-dimensional positional data during entire matches.\nWe developed methods based on Newtonian mechanics and the Kalman filter,\nGenerative Adversarial Nets and Variational Autoencoders. In addition, we\ntrained a discriminator network to recognize and discern different movement\npatterns of players. The Kalman-filter approach yields an interpretable model,\nin which a small number of player-dependent parameters can be fit; in theory\nthis could be used to distinguish among players. The\nGenerative-Adversarial-Nets approach appears promising in theory, and some\ninitial tests showed an improvement with respect to the baseline, but the\nlimits in time and computational power meant that we could not fully explore\nit. We also trained a Discriminator network to distinguish between two players\nbased on their trajectories; after training, the network managed to distinguish\nbetween some pairs of players, but not between others. After training, the\nVariational Autoencoders generated trajectories that are difficult to\ndistinguish, visually, from the data. These experiments provide an indication\nthat deep generative models can learn the underlying structure and statistics\nof football players' trajectories. This can serve as a starting point for\ndetermining player qualities based on such trajectory data. \n\n"}
{"id": "1808.05163", "contents": "Title: A Simple Convolutional Generative Network for Next Item Recommendation Abstract: Convolutional Neural Networks (CNNs) have been recently introduced in the\ndomain of session-based next item recommendation. An ordered collection of past\nitems the user has interacted with in a session (or sequence) are embedded into\na 2-dimensional latent matrix, and treated as an image. The convolution and\npooling operations are then applied to the mapped item embeddings. In this\npaper, we first examine the typical session-based CNN recommender and show that\nboth the generative model and network architecture are suboptimal when modeling\nlong-range dependencies in the item sequence. To address the issues, we\nintroduce a simple, but very effective generative model that is capable of\nlearning high-level representation from both short- and long-range item\ndependencies. The network architecture of the proposed model is formed of a\nstack of \\emph{holed} convolutional layers, which can efficiently increase the\nreceptive fields without relying on the pooling operation. Another contribution\nis the effective use of residual block structure in recommender systems, which\ncan ease the optimization for much deeper networks. The proposed generative\nmodel attains state-of-the-art accuracy with less training time in the next\nitem recommendation task. It accordingly can be used as a powerful\nrecommendation baseline to beat in future, especially when there are long\nsequences of user feedback. \n\n"}
{"id": "1808.06394", "contents": "Title: Faster Support Vector Machines Abstract: The time complexity of support vector machines (SVMs) prohibits training on\nhuge data sets with millions of data points. Recently, multilevel approaches to\ntrain SVMs have been developed to allow for time-efficient training on huge\ndata sets. While regular SVMs perform the entire training in one -- time\nconsuming -- optimization step, multilevel SVMs first build a hierarchy of\nproblems decreasing in size that resemble the original problem and then train\nan SVM model for each hierarchy level, benefiting from the solved models of\nprevious levels. We present a faster multilevel support vector machine that\nuses a label propagation algorithm to construct the problem hierarchy.\nExtensive experiments indicate that our approach is up to orders of magnitude\nfaster than the previous fastest algorithm while having comparable\nclassification quality. For example, already one of our sequential solvers is\non average a factor 15 faster than the parallel ThunderSVM algorithm, while\nhaving similar classification quality. \n\n"}
{"id": "1808.06797", "contents": "Title: zoNNscan : a boundary-entropy index for zone inspection of neural models Abstract: The training of deep neural network classifiers results in decision\nboundaries which geometry is still not well understood. This is in direct\nrelation with classification problems such as so called adversarial examples.\nWe introduce zoNNscan, an index that is intended to inform on the boundary\nuncertainty (in terms of the presence of other classes) around one given input\ndatapoint. It is based on confidence entropy, and is implemented through\nsampling in the multidimensional ball surrounding that input. We detail the\nzoNNscan index, give an algorithm for approximating it, and finally illustrate\nits benefits on four applications, including two important problems for the\nadoption of deep networks in critical systems: adversarial examples and corner\ncase inputs. We highlight that zoNNscan exhibits significantly higher values\nthan for standard inputs in those two problem classes. \n\n"}
{"id": "1808.06809", "contents": "Title: Are You Tampering With My Data? Abstract: We propose a novel approach towards adversarial attacks on neural networks\n(NN), focusing on tampering the data used for training instead of generating\nattacks on trained models. Our network-agnostic method creates a backdoor\nduring training which can be exploited at test time to force a neural network\nto exhibit abnormal behaviour. We demonstrate on two widely used datasets\n(CIFAR-10 and SVHN) that a universal modification of just one pixel per image\nfor all the images of a class in the training set is enough to corrupt the\ntraining procedure of several state-of-the-art deep neural networks causing the\nnetworks to misclassify any images to which the modification is applied. Our\naim is to bring to the attention of the machine learning community, the\npossibility that even learning-based methods that are personally trained on\npublic datasets can be subject to attacks by a skillful adversary. \n\n"}
{"id": "1808.07018", "contents": "Title: Hypernetwork Knowledge Graph Embeddings Abstract: Knowledge graphs are graphical representations of large databases of facts,\nwhich typically suffer from incompleteness. Inferring missing relations (links)\nbetween entities (nodes) is the task of link prediction. A recent\nstate-of-the-art approach to link prediction, ConvE, implements a convolutional\nneural network to extract features from concatenated subject and relation\nvectors. Whilst results are impressive, the method is unintuitive and poorly\nunderstood. We propose a hypernetwork architecture that generates simplified\nrelation-specific convolutional filters that (i) outperforms ConvE and all\nprevious approaches across standard datasets; and (ii) can be framed as tensor\nfactorization and thus set within a well established family of factorization\nmodels for link prediction. We thus demonstrate that convolution simply offers\na convenient computational means of introducing sparsity and parameter tying to\nfind an effective trade-off between non-linear expressiveness and the number of\nparameters to learn. \n\n"}
{"id": "1808.07412", "contents": "Title: Ithemal: Accurate, Portable and Fast Basic Block Throughput Estimation\n  using Deep Neural Networks Abstract: Predicting the number of clock cycles a processor takes to execute a block of\nassembly instructions in steady state (the throughput) is important for both\ncompiler designers and performance engineers. Building an analytical model to\ndo so is especially complicated in modern x86-64 Complex Instruction Set\nComputer (CISC) machines with sophisticated processor microarchitectures in\nthat it is tedious, error prone, and must be performed from scratch for each\nprocessor generation. In this paper we present Ithemal, the first tool which\nlearns to predict the throughput of a set of instructions. Ithemal uses a\nhierarchical LSTM--based approach to predict throughput based on the opcodes\nand operands of instructions in a basic block. We show that Ithemal is more\naccurate than state-of-the-art hand-written tools currently used in compiler\nbackends and static machine code analyzers. In particular, our model has less\nthan half the error of state-of-the-art analytical models (LLVM's llvm-mca and\nIntel's IACA). Ithemal is also able to predict these throughput values just as\nfast as the aforementioned tools, and is easily ported across a variety of\nprocessor microarchitectures with minimal developer effort. \n\n"}
{"id": "1808.07632", "contents": "Title: DOPING: Generative Data Augmentation for Unsupervised Anomaly Detection\n  with GAN Abstract: Recently, the introduction of the generative adversarial network (GAN) and\nits variants has enabled the generation of realistic synthetic samples, which\nhas been used for enlarging training sets. Previous work primarily focused on\ndata augmentation for semi-supervised and supervised tasks. In this paper, we\ninstead focus on unsupervised anomaly detection and propose a novel generative\ndata augmentation framework optimized for this task. In particular, we propose\nto oversample infrequent normal samples - normal samples that occur with small\nprobability, e.g., rare normal events. We show that these samples are\nresponsible for false positives in anomaly detection. However, oversampling of\ninfrequent normal samples is challenging for real-world high-dimensional data\nwith multimodal distributions. To address this challenge, we propose to use a\nGAN variant known as the adversarial autoencoder (AAE) to transform the\nhigh-dimensional multimodal data distributions into low-dimensional unimodal\nlatent distributions with well-defined tail probability. Then, we\nsystematically oversample at the `edge' of the latent distributions to increase\nthe density of infrequent normal samples. We show that our oversampling\npipeline is a unified one: it is generally applicable to datasets with\ndifferent complex data distributions. To the best of our knowledge, our method\nis the first data augmentation technique focused on improving performance in\nunsupervised anomaly detection. We validate our method by demonstrating\nconsistent improvements across several real-world datasets. \n\n"}
{"id": "1808.07945", "contents": "Title: Maximal Jacobian-based Saliency Map Attack Abstract: The Jacobian-based Saliency Map Attack is a family of adversarial attack\nmethods for fooling classification models, such as deep neural networks for\nimage classification tasks. By saturating a few pixels in a given image to\ntheir maximum or minimum values, JSMA can cause the model to misclassify the\nresulting adversarial image as a specified erroneous target class. We propose\ntwo variants of JSMA, one which removes the requirement to specify a target\nclass, and another that additionally does not need to specify whether to only\nincrease or decrease pixel intensities. Our experiments highlight the\ncompetitive speeds and qualities of these variants when applied to datasets of\nhand-written digits and natural scenes. \n\n"}
{"id": "1808.08023", "contents": "Title: A Jointly Learned Context-Aware Place of Interest Embedding for Trip\n  Recommendations Abstract: Trip recommendation is an important location-based service that helps relieve\nusers from the time and efforts for trip planning. It aims to recommend a\nsequence of places of interest (POIs) for a user to visit that maximizes the\nuser's satisfaction. When adding a POI to a recommended trip, it is essential\nto understand the context of the recommendation, including the POI popularity,\nother POIs co-occurring in the trip, and the preferences of the user. These\ncontextual factors are learned separately in existing studies, while in\nreality, they impact jointly on a user's choice of a POI to visit. In this\nstudy, we propose a POI embedding model to jointly learn the impact of these\ncontextual factors. We call the learned POI embedding a context-aware POI\nembedding. To showcase the effectiveness of this embedding, we apply it to\ngenerate trip recommendations given a user and a time budget. We propose two\ntrip recommendation algorithms based on our context-aware POI embedding. The\nfirst algorithm finds the exact optimal trip by transforming and solving the\ntrip recommendation problem as an integer linear programming problem. To\nachieve a high computation efficiency, the second algorithm finds a\nheuristically optimal trip based on adaptive large neighborhood search. We\nperform extensive experiments on real datasets. The results show that our\nproposed algorithms consistently outperform state-of-the-art algorithms in trip\nrecommendation quality, with an advantage of up to 43% in F1-score. \n\n"}
{"id": "1808.08097", "contents": "Title: Memory Time Span in LSTMs for Multi-Speaker Source Separation Abstract: With deep learning approaches becoming state-of-the-art in many speech (as\nwell as non-speech) related machine learning tasks, efforts are being taken to\ndelve into the neural networks which are often considered as a black box. In\nthis paper it is analyzed how recurrent neural network (RNNs) cope with\ntemporal dependencies by determining the relevant memory time span in a long\nshort-term memory (LSTM) cell. This is done by leaking the state variable with\na controlled lifetime and evaluating the task performance. This technique can\nbe used for any task to estimate the time span the LSTM exploits in that\nspecific scenario. The focus in this paper is on the task of separating\nspeakers from overlapping speech. We discern two effects: A long term effect,\nprobably due to speaker characterization and a short term effect, probably\nexploiting phone-size formant tracks. \n\n"}
{"id": "1808.08782", "contents": "Title: Amobee at IEST 2018: Transfer Learning from Language Models Abstract: This paper describes the system developed at Amobee for the WASSA 2018\nimplicit emotions shared task (IEST). The goal of this task was to predict the\nemotion expressed by missing words in tweets without an explicit mention of\nthose words. We developed an ensemble system consisting of language models\ntogether with LSTM-based networks containing a CNN attention mechanism. Our\napproach represents a novel use of language models (specifically trained on a\nlarge Twitter dataset) to predict and classify emotions. Our system reached 1st\nplace with a macro $\\text{F}_1$ score of 0.7145. \n\n"}
{"id": "1808.09819", "contents": "Title: Approximate Exploration through State Abstraction Abstract: Although exploration in reinforcement learning is well understood from a\ntheoretical point of view, provably correct methods remain impractical. In this\npaper we study the interplay between exploration and approximation, what we\ncall approximate exploration. Our main goal is to further our theoretical\nunderstanding of pseudo-count based exploration bonuses (Bellemare et al.,\n2016), a practical exploration scheme based on density modelling. As a warm-up,\nwe quantify the performance of an exploration algorithm, MBIE-EB (Strehl and\nLittman, 2008), when explicitly combined with state aggregation. This allows us\nto confirm that, as might be expected, approximation allows the agent to trade\noff between learning speed and quality of the learned policy. Next, we show how\na given density model can be related to an abstraction and that the\ncorresponding pseudo-count bonus can act as a substitute in MBIE-EB combined\nwith this abstraction, but may lead to either under- or over-exploration. Then,\nwe show that a given density model also defines an implicit abstraction, and\nfind a surprising mismatch between pseudo-counts derived either implicitly or\nexplicitly. Finally we derive a new pseudo-count bonus alleviating this issue. \n\n"}
{"id": "1808.09856", "contents": "Title: Application of Machine Learning in Rock Facies Classification with\n  Physics-Motivated Feature Augmentation Abstract: With recent progress in algorithms and the availability of massive amounts of\ncomputation power, application of machine learning techniques is becoming a hot\ntopic in the oil and gas industry. One of the most promising aspects to apply\nmachine learning to the upstream field is the rock facies classification in\nreservoir characterization, which is crucial in determining the net pay\nthickness of reservoirs, thus a definitive factor in drilling decision making\nprocess. For complex machine learning tasks like facies classification, feature\nengineering is often critical. This paper shows the inclusion of\nphysics-motivated feature interaction in feature augmentation can further\nimprove the capability of machine learning in rock facies classification. We\ndemonstrate this approach with the SEG 2016 machine learning contest dataset\nand the top winning algorithms. The improvement is roboust and can be $\\sim5\\%$\nbetter than current existing best F-1 score, where F-1 is an evaluation metric\nused to quantify average prediction accuracy. \n\n"}
{"id": "1808.09935", "contents": "Title: Attention-based Neural Text Segmentation Abstract: Text segmentation plays an important role in various Natural Language\nProcessing (NLP) tasks like summarization, context understanding, document\nindexing and document noise removal. Previous methods for this task require\nmanual feature engineering, huge memory requirements and large execution times.\nTo the best of our knowledge, this paper is the first one to present a novel\nsupervised neural approach for text segmentation. Specifically, we propose an\nattention-based bidirectional LSTM model where sentence embeddings are learned\nusing CNNs and the segments are predicted based on contextual information. This\nmodel can automatically handle variable sized context information. Compared to\nthe existing competitive baselines, the proposed model shows a performance\nimprovement of ~7% in WinDiff score on three benchmark datasets. \n\n"}
{"id": "1808.10013", "contents": "Title: The implicit fairness criterion of unconstrained learning Abstract: We clarify what fairness guarantees we can and cannot expect to follow from\nunconstrained machine learning. Specifically, we characterize when\nunconstrained learning on its own implies group calibration, that is, the\noutcome variable is conditionally independent of group membership given the\nscore. We show that under reasonable conditions, the deviation from satisfying\ngroup calibration is upper bounded by the excess risk of the learned score\nrelative to the Bayes optimal score function. A lower bound confirms the\noptimality of our upper bound. Moreover, we prove that as the excess risk of\nthe learned score decreases, it strongly violates separation and independence,\ntwo other standard fairness criteria.\n  Our results show that group calibration is the fairness criterion that\nunconstrained learning implicitly favors. On the one hand, this means that\ncalibration is often satisfied on its own without the need for active\nintervention, albeit at the cost of violating other criteria that are at odds\nwith calibration. On the other hand, it suggests that we should be satisfied\nwith calibration as a fairness criterion only if we are at ease with the use of\nunconstrained machine learning in a given application. \n\n"}
{"id": "1808.10632", "contents": "Title: A novel extension of Generalized Low-Rank Approximation of Matrices\n  based on multiple-pairs of transformations Abstract: Dimensionality reduction is a main step in the learning process which plays\nan essential role in many applications. The most popular methods in this field\nlike SVD, PCA, and LDA, only can be applied to data with vector format. This\nmeans that for higher order data like matrices or more generally tensors, data\nshould be fold to the vector format. So, in this approach, the spatial\nrelations of features are not considered and also the probability of\nover-fitting is increased. Due to these issues, in recent years some methods\nlike Generalized low-rank approximation of matrices (GLRAM) and Multilinear PCA\n(MPCA) are proposed which deal with the data in their own format. So, in these\nmethods, the spatial relationships of features are preserved and the\nprobability of overfitting could be fallen. Also, their time and space\ncomplexities are less than vector-based ones. However, because of the fewer\nparameters, the search space in a multilinear approach is much smaller than the\nsearch space of the vector-based approach. To overcome this drawback of\nmultilinear methods like GLRAM, we proposed a new method which is a general\nform of GLRAM and by preserving the merits of it have a larger search space.\nExperimental results confirm the quality of the proposed method. Also, applying\nthis approach to the other multilinear dimensionality reduction methods like\nMPCA and MLDA is straightforward. \n\n"}
{"id": "1808.10692", "contents": "Title: APES: a Python toolbox for simulating reinforcement learning\n  environments Abstract: Assisted by neural networks, reinforcement learning agents have been able to\nsolve increasingly complex tasks over the last years. The simulation\nenvironment in which the agents interact is an essential component in any\nreinforcement learning problem. The environment simulates the dynamics of the\nagents' world and hence provides feedback to their actions in terms of state\nobservations and external rewards. To ease the design and simulation of such\nenvironments this work introduces $\\texttt{APES}$, a highly customizable and\nopen source package in Python to create 2D grid-world environments for\nreinforcement learning problems. $\\texttt{APES}$ equips agents with algorithms\nto simulate any field of vision, it allows the creation and positioning of\nitems and rewards according to user-defined rules, and supports the interaction\nof multiple agents. \n\n"}
{"id": "1809.00338", "contents": "Title: Look Across Elapse: Disentangled Representation Learning and\n  Photorealistic Cross-Age Face Synthesis for Age-Invariant Face Recognition Abstract: Despite the remarkable progress in face recognition related technologies,\nreliably recognizing faces across ages still remains a big challenge. The\nappearance of a human face changes substantially over time, resulting in\nsignificant intra-class variations. As opposed to current techniques for\nage-invariant face recognition, which either directly extract age-invariant\nfeatures for recognition, or first synthesize a face that matches target age\nbefore feature extraction, we argue that it is more desirable to perform both\ntasks jointly so that they can leverage each other. To this end, we propose a\ndeep Age-Invariant Model (AIM) for face recognition in the wild with three\ndistinct novelties. First, AIM presents a novel unified deep architecture\njointly performing cross-age face synthesis and recognition in a mutual\nboosting way. Second, AIM achieves continuous face rejuvenation/aging with\nremarkable photorealistic and identity-preserving properties, avoiding the\nrequirement of paired data and the true age of testing samples. Third, we\ndevelop effective and novel training strategies for end-to-end learning the\nwhole deep architecture, which generates powerful age-invariant face\nrepresentations explicitly disentangled from the age variation. Moreover, we\npropose a new large-scale Cross-Age Face Recognition (CAFR) benchmark dataset\nto facilitate existing efforts and push the frontiers of age-invariant face\nrecognition research. Extensive experiments on both our CAFR and several other\ncross-age datasets (MORPH, CACD and FG-NET) demonstrate the superiority of the\nproposed AIM model over the state-of-the-arts. Benchmarking our model on one of\nthe most popular unconstrained face recognition datasets IJB-C additionally\nverifies the promising generalizability of AIM in recognizing faces in the\nwild. \n\n"}
{"id": "1809.00811", "contents": "Title: A Deep Learning Spatiotemporal Prediction Framework for Mobile\n  Crowdsourced Services Abstract: This papers presents a deep learning-based framework to predict crowdsourced\nservice availability spatially and temporally. A novel two-stage prediction\nmodel is introduced based on historical spatio-temporal traces of mobile\ncrowdsourced services. The prediction model first clusters mobile crowdsourced\nservices into regions. The availability prediction of a mobile crowdsourced\nservice at a certain location and time is then formulated as a classification\nproblem. To determine the availability duration of predicted mobile\ncrowdsourced services, we formulate a forecasting task of time series using the\nGramian Angular Field. We validated the effectiveness of the proposed framework\nthrough multiple experiments. \n\n"}
{"id": "1809.00862", "contents": "Title: Handwriting styles: benchmarks and evaluation metrics Abstract: Evaluating the style of handwriting generation is a challenging problem,\nsince it is not well defined. It is a key component in order to develop in\ndeveloping systems with more personalized experiences with humans. In this\npaper, we propose baseline benchmarks, in order to set anchors to estimate the\nrelative quality of different handwriting style methods. This will be done\nusing deep learning techniques, which have shown remarkable results in\ndifferent machine learning tasks, learning classification, regression, and most\nrelevant to our work, generating temporal sequences. We discuss the challenges\nassociated with evaluating our methods, which is related to evaluation of\ngenerative models in general. We then propose evaluation metrics, which we find\nrelevant to this problem, and we discuss how we evaluate the evaluation\nmetrics. In this study, we use IRON-OFF dataset. To the best of our knowledge,\nthere is no work done before in generating handwriting (either in terms of\nmethodology or the performance metrics), our in exploring styles using this\ndataset. \n\n"}
{"id": "1809.00946", "contents": "Title: Twin-GAN -- Unpaired Cross-Domain Image Translation with Weight-Sharing\n  GANs Abstract: We present a framework for translating unlabeled images from one domain into\nanalog images in another domain. We employ a progressively growing\nskip-connected encoder-generator structure and train it with a GAN loss for\nrealistic output, a cycle consistency loss for maintaining same-domain\ntranslation identity, and a semantic consistency loss that encourages the\nnetwork to keep the input semantic features in the output. We apply our\nframework on the task of translating face images, and show that it is capable\nof learning semantic mappings for face images with no supervised one-to-one\nimage mapping. \n\n"}
{"id": "1809.01000", "contents": "Title: Bayesian Outdoor Defect Detection Abstract: We introduce a Bayesian defect detector to facilitate the defect detection on\nthe motion blurred images on rough texture surfaces. To enhance the accuracy of\nBayesian detection on removing non-defect pixels, we develop a class of\nreflected non-local prior distributions, which is constructed by using the mode\nof a distribution to subtract its density. The reflected non-local priors\nforces the Bayesian detector to approach 0 at the non-defect locations. We\nconduct experiments studies to demonstrate the superior performance of the\nBayesian detector in eliminating the non-defect points. We implement the\nBayesian detector in the motion blurred drone images, in which the detector\nsuccessfully identifies the hail damages on the rough surface and substantially\nenhances the accuracy of the entire defect detection pipeline. \n\n"}
{"id": "1809.01090", "contents": "Title: Graph Convolutional Neural Networks based on Quantum Vertex Saliency Abstract: This paper proposes a new Quantum Spatial Graph Convolutional Neural Network\n(QSGCNN) model that can directly learn a classification function for graphs of\narbitrary sizes. Unlike state-of-the-art Graph Convolutional Neural Network\n(GCNN) models, the proposed QSGCNN model incorporates the process of\nidentifying transitive aligned vertices between graphs, and transforms\narbitrary sized graphs into fixed-sized aligned vertex grid structures. In\norder to learn representative graph characteristics, a new quantum spatial\ngraph convolution is proposed and employed to extract multi-scale vertex\nfeatures, in terms of quantum information propagation between grid vertices of\neach graph. Since the quantum spatial convolution preserves the grid structures\nof the input vertices (i.e., the convolution layer does not change the original\nspatial sequence of vertices), the proposed QSGCNN model allows to directly\nemploy the traditional convolutional neural network architecture to further\nlearn from the global graph topology, providing an end-to-end deep learning\narchitecture that integrates the graph representation and learning in the\nquantum spatial graph convolution layer and the traditional convolutional layer\nfor graph classifications. We demonstrate the effectiveness of the proposed\nQSGCNN model in relation to existing state-of-the-art methods. The proposed\nQSGCNN model addresses the shortcomings of information loss and imprecise\ninformation representation arising in existing GCN models associated with the\nuse of SortPooling or SumPooling layers. Experiments on benchmark graph\nclassification datasets demonstrate the effectiveness of the proposed QSGCNN\nmodel. \n\n"}
{"id": "1809.01465", "contents": "Title: Deep Bilevel Learning Abstract: We present a novel regularization approach to train neural networks that\nenjoys better generalization and test error than standard stochastic gradient\ndescent. Our approach is based on the principles of cross-validation, where a\nvalidation set is used to limit the model overfitting. We formulate such\nprinciples as a bilevel optimization problem. This formulation allows us to\ndefine the optimization of a cost on the validation set subject to another\noptimization on the training set. The overfitting is controlled by introducing\nweights on each mini-batch in the training set and by choosing their values so\nthat they minimize the error on the validation set. In practice, these weights\ndefine mini-batch learning rates in a gradient descent update equation that\nfavor gradients with better generalization capabilities. Because of its\nsimplicity, this approach can be integrated with other regularization methods\nand training schemes. We evaluate extensively our proposed algorithm on several\nneural network architectures and datasets, and find that it consistently\nimproves the generalization of the model, especially when labels are noisy. \n\n"}
{"id": "1809.01733", "contents": "Title: Deep Joint Source-Channel Coding for Wireless Image Transmission Abstract: We propose a joint source and channel coding (JSCC) technique for wireless\nimage transmission that does not rely on explicit codes for either compression\nor error correction; instead, it directly maps the image pixel values to the\ncomplex-valued channel input symbols. We parameterize the encoder and decoder\nfunctions by two convolutional neural networks (CNNs), which are trained\njointly, and can be considered as an autoencoder with a non-trainable layer in\nthe middle that represents the noisy communication channel. Our results show\nthat the proposed deep JSCC scheme outperforms digital transmission\nconcatenating JPEG or JPEG2000 compression with a capacity achieving channel\ncode at low signal-to-noise ratio (SNR) and channel bandwidth values in the\npresence of additive white Gaussian noise (AWGN). More strikingly, deep JSCC\ndoes not suffer from the ``cliff effect'', and it provides a graceful\nperformance degradation as the channel SNR varies with respect to the SNR value\nassumed during training. In the case of a slow Rayleigh fading channel, deep\nJSCC learns noise resilient coded representations and significantly outperforms\nseparation-based digital communication at all SNR and channel bandwidth values. \n\n"}
{"id": "1809.02497", "contents": "Title: Sparse Kernel PCA for Outlier Detection Abstract: In this paper, we propose a new method to perform Sparse Kernel Principal\nComponent Analysis (SKPCA) and also mathematically analyze the validity of\nSKPCA. We formulate SKPCA as a constrained optimization problem with elastic\nnet regularization (Hastie et al.) in kernel feature space and solve it. We\nconsider outlier detection (where KPCA is employed) as an application for\nSKPCA, using the RBF kernel. We test it on 5 real-world datasets and show that\nby using just 4% (or even less) of the principal components (PCs), where each\nPC has on average less than 12% non-zero elements in the worst case among all 5\ndatasets, we are able to nearly match and in 3 datasets even outperform KPCA.\nWe also compare the performance of our method with a recently proposed method\nfor SKPCA by Wang et al. and show that our method performs better in terms of\nboth accuracy and sparsity. We also provide a novel probabilistic proof to\njustify the existence of sparse solutions for KPCA using the RBF kernel. To the\nbest of our knowledge, this is the first attempt at theoretically analyzing the\nvalidity of SKPCA. \n\n"}
{"id": "1809.02652", "contents": "Title: Are You Sure You Want To Do That? Classification with Verification Abstract: Classification systems typically act in isolation, meaning they are required\nto implicitly memorize the characteristics of all candidate classes in order to\nclassify. The cost of this is increased memory usage and poor sample\nefficiency. We propose a model which instead verifies using reference images\nduring the classification process, reducing the burden of memorization. The\nmodel uses iterative nondifferentiable queries in order to classify an image.\nWe demonstrate that such a model is feasible to train and can match baseline\naccuracy while being more parameter efficient. However, we show that finding\nthe correct balance between image recognition and verification is essential to\npushing the model towards desired behavior, suggesting that a pipeline of\nrecognition followed by verification is a more promising approach. \n\n"}
{"id": "1809.03113", "contents": "Title: Certified Adversarial Robustness with Additive Noise Abstract: The existence of adversarial data examples has drawn significant attention in\nthe deep-learning community; such data are seemingly minimally perturbed\nrelative to the original data, but lead to very different outputs from a\ndeep-learning algorithm. Although a significant body of work on developing\ndefensive models has been considered, most such models are heuristic and are\noften vulnerable to adaptive attacks. Defensive methods that provide\ntheoretical robustness guarantees have been studied intensively, yet most fail\nto obtain non-trivial robustness when a large-scale model and data are present.\nTo address these limitations, we introduce a framework that is scalable and\nprovides certified bounds on the norm of the input manipulation for\nconstructing adversarial examples. We establish a connection between robustness\nagainst adversarial perturbation and additive random noise, and propose a\ntraining strategy that can significantly improve the certified bounds. Our\nevaluation on MNIST, CIFAR-10 and ImageNet suggests that the proposed method is\nscalable to complicated models and large data sets, while providing competitive\nrobustness to state-of-the-art provable defense methods. \n\n"}
{"id": "1809.03316", "contents": "Title: Hierarchical Video Understanding Abstract: We introduce a hierarchical architecture for video understanding that\nexploits the structure of real world actions by capturing targets at different\nlevels of granularity. We design the model such that it first learns simpler\ncoarse-grained tasks, and then moves on to learn more fine-grained targets. The\nmodel is trained with a joint loss on different granularity levels. We\ndemonstrate empirical results on the recent release of Something-Something\ndataset, which provides a hierarchy of targets, namely coarse-grained action\ngroups, fine-grained action categories, and captions. Experiments suggest that\nmodels that exploit targets at different levels of granularity achieve better\nperformance on all levels. \n\n"}
{"id": "1809.03322", "contents": "Title: Guiding the Creation of Deep Learning-based Object Detectors Abstract: Object detection is a computer vision field that has applications in several\ncontexts ranging from biomedicine and agriculture to security. In the last\nyears, several deep learning techniques have greatly improved object detection\nmodels. Among those techniques, we can highlight the YOLO approach, that allows\nthe construction of accurate models that can be employed in real-time\napplications. However, as most deep learning techniques, YOLO has a steep\nlearning curve and creating models using this approach might be challenging for\nnon-expert users. In this work, we tackle this problem by constructing a suite\nof Jupyter notebooks that democratizes the construction of object detection\nmodels using YOLO. The suitability of our approach has been proven with a\ndataset of stomata images where we have achieved a mAP of 90.91%. \n\n"}
{"id": "1809.03385", "contents": "Title: SPASS: Scientific Prominence Active Search System with Deep Image\n  Captioning Network Abstract: Planetary exploration missions with Mars rovers are complicated, which\ngenerally require elaborated task planning by human experts, from the path to\ntake to the images to capture. NASA has been using this process to acquire over\n22 million images from the planet Mars. In order to improve the degree of\nautomation and thus efficiency in this process, we propose a system for\nplanetary rovers to actively search for prominence of prespecified scientific\nfeatures in captured images. Scientists can prespecify such search tasks in\nnatural language and upload them to a rover, on which the deployed system\nconstantly captions captured images with a deep image captioning network and\ncompare the auto-generated captions to the prespecified search tasks by certain\nmetrics so as to prioritize those images for transmission. As a beneficial side\neffect, the proposed system can also be deployed to ground-based planetary data\nsystems as a content-based search engine. \n\n"}
{"id": "1809.03470", "contents": "Title: ViZDoom Competitions: Playing Doom from Pixels Abstract: This paper presents the first two editions of Visual Doom AI Competition,\nheld in 2016 and 2017. The challenge was to create bots that compete in a\nmulti-player deathmatch in a first-person shooter (FPS) game, Doom. The bots\nhad to make their decisions based solely on visual information, i.e., a raw\nscreen buffer. To play well, the bots needed to understand their surroundings,\nnavigate, explore, and handle the opponents at the same time. These aspects,\ntogether with the competitive multi-agent aspect of the game, make the\ncompetition a unique platform for evaluating the state of the art reinforcement\nlearning algorithms. The paper discusses the rules, solutions, results, and\nstatistics that give insight into the agents' behaviors. Best-performing agents\nare described in more detail. The results of the competition lead to the\nconclusion that, although reinforcement learning can produce capable Doom bots,\nthey still are not yet able to successfully compete against humans in this\ngame. The paper also revisits the ViZDoom environment, which is a flexible,\neasy to use, and efficient 3D platform for research for vision-based\nreinforcement learning, based on a well-recognized first-person perspective\ngame Doom. \n\n"}
{"id": "1809.04270", "contents": "Title: MotherNets: Rapid Deep Ensemble Learning Abstract: Ensembles of deep neural networks significantly improve generalization\naccuracy. However, training neural network ensembles requires a large amount of\ncomputational resources and time. State-of-the-art approaches either train all\nnetworks from scratch leading to prohibitive training cost that allows only\nvery small ensemble sizes in practice, or generate ensembles by training a\nmonolithic architecture, which results in lower model diversity and decreased\nprediction accuracy. We propose MotherNets to enable higher accuracy and\npractical training cost for large and diverse neural network ensembles: A\nMotherNet captures the structural similarity across some or all members of a\ndeep neural network ensemble which allows us to share data movement and\ncomputation costs across these networks. We first train a single or a small set\nof MotherNets and, subsequently, we generate the target ensemble networks by\ntransferring the function from the trained MotherNet(s). Then, we continue to\ntrain these ensemble networks, which now converge drastically faster compared\nto training from scratch. MotherNets handle ensembles with diverse\narchitectures by clustering ensemble networks of similar architecture and\ntraining a separate MotherNet for every cluster. MotherNets also use clustering\nto control the accuracy vs. training cost tradeoff. We show that compared to\nstate-of-the-art approaches such as Snapshot Ensembles, Knowledge Distillation,\nand TreeNets, MotherNets provide a new Pareto frontier for the\naccuracy-training cost tradeoff. Crucially, training cost and accuracy\nimprovements continue to scale as we increase the ensemble size (2 to 3 percent\nreduced absolute test error rate and up to 35 percent faster training compared\nto Snapshot Ensembles). We verify these benefits over numerous neural network\narchitectures and large data sets. \n\n"}
{"id": "1809.04379", "contents": "Title: Bayesian Semi-supervised Learning with Graph Gaussian Processes Abstract: We propose a data-efficient Gaussian process-based Bayesian approach to the\nsemi-supervised learning problem on graphs. The proposed model shows extremely\ncompetitive performance when compared to the state-of-the-art graph neural\nnetworks on semi-supervised learning benchmark experiments, and outperforms the\nneural networks in active learning experiments where labels are scarce.\nFurthermore, the model does not require a validation data set for early\nstopping to control over-fitting. Our model can be viewed as an instance of\nempirical distribution regression weighted locally by network connectivity. We\nfurther motivate the intuitive construction of the model with a Bayesian linear\nmodel interpretation where the node features are filtered by an operator\nrelated to the graph Laplacian. The method can be easily implemented by\nadapting off-the-shelf scalable variational inference algorithms for Gaussian\nprocesses. \n\n"}
{"id": "1809.04683", "contents": "Title: SAFE: A Neural Survival Analysis Model for Fraud Early Detection Abstract: Many online platforms have deployed anti-fraud systems to detect and prevent\nfraudulent activities. However, there is usually a gap between the time that a\nuser commits a fraudulent action and the time that the user is suspended by the\nplatform. How to detect fraudsters in time is a challenging problem. Most of\nthe existing approaches adopt classifiers to predict fraudsters given their\nactivity sequences along time. The main drawback of classification models is\nthat the prediction results between consecutive timestamps are often\ninconsistent. In this paper, we propose a survival analysis based fraud early\ndetection model, SAFE, which maps dynamic user activities to survival\nprobabilities that are guaranteed to be monotonically decreasing along time.\nSAFE adopts recurrent neural network (RNN) to handle user activity sequences\nand directly outputs hazard values at each timestamp, and then, survival\nprobability derived from hazard values is deployed to achieve consistent\npredictions. Because we only observe the user suspended time instead of the\nfraudulent activity time in the training data, we revise the loss function of\nthe regular survival model to achieve fraud early detection. Experimental\nresults on two real world datasets demonstrate that SAFE outperforms both the\nsurvival analysis model and recurrent neural network model alone as well as\nstate-of-the-art fraud early detection approaches. \n\n"}
{"id": "1809.04747", "contents": "Title: Geodesic Clustering in Deep Generative Models Abstract: Deep generative models are tremendously successful in learning\nlow-dimensional latent representations that well-describe the data. These\nrepresentations, however, tend to much distort relationships between points,\ni.e. pairwise distances tend to not reflect semantic similarities well. This\nrenders unsupervised tasks, such as clustering, difficult when working with the\nlatent representations. We demonstrate that taking the geometry of the\ngenerative model into account is sufficient to make simple clustering\nalgorithms work well over latent representations. Leaning on the recent finding\nthat deep generative models constitute stochastically immersed Riemannian\nmanifolds, we propose an efficient algorithm for computing geodesics (shortest\npaths) and computing distances in the latent space, while taking its distortion\ninto account. We further propose a new architecture for modeling uncertainty in\nvariational autoencoders, which is essential for understanding the geometry of\ndeep generative models. Experiments show that the geodesic distance is very\nlikely to reflect the internal structure of the data. \n\n"}
{"id": "1809.05165", "contents": "Title: Defensive Dropout for Hardening Deep Neural Networks under Adversarial\n  Attacks Abstract: Deep neural networks (DNNs) are known vulnerable to adversarial attacks. That\nis, adversarial examples, obtained by adding delicately crafted distortions\nonto original legal inputs, can mislead a DNN to classify them as any target\nlabels. This work provides a solution to hardening DNNs under adversarial\nattacks through defensive dropout. Besides using dropout during training for\nthe best test accuracy, we propose to use dropout also at test time to achieve\nstrong defense effects. We consider the problem of building robust DNNs as an\nattacker-defender two-player game, where the attacker and the defender know\neach others' strategies and try to optimize their own strategies towards an\nequilibrium. Based on the observations of the effect of test dropout rate on\ntest accuracy and attack success rate, we propose a defensive dropout algorithm\nto determine an optimal test dropout rate given the neural network model and\nthe attacker's strategy for generating adversarial examples.We also investigate\nthe mechanism behind the outstanding defense effects achieved by the proposed\ndefensive dropout. Comparing with stochastic activation pruning (SAP), another\ndefense method through introducing randomness into the DNN model, we find that\nour defensive dropout achieves much larger variances of the gradients, which is\nthe key for the improved defense effects (much lower attack success rate). For\nexample, our defensive dropout can reduce the attack success rate from 100% to\n13.89% under the currently strongest attack i.e., C&W attack on MNIST dataset. \n\n"}
{"id": "1809.05476", "contents": "Title: Hardware-Aware Machine Learning: Modeling and Optimization Abstract: Recent breakthroughs in Deep Learning (DL) applications have made DL models a\nkey component in almost every modern computing system. The increased popularity\nof DL applications deployed on a wide-spectrum of platforms have resulted in a\nplethora of design challenges related to the constraints introduced by the\nhardware itself. What is the latency or energy cost for an inference made by a\nDeep Neural Network (DNN)? Is it possible to predict this latency or energy\nconsumption before a model is trained? If yes, how can machine learners take\nadvantage of these models to design the hardware-optimal DNN for deployment?\nFrom lengthening battery life of mobile devices to reducing the runtime\nrequirements of DL models executing in the cloud, the answers to these\nquestions have drawn significant attention.\n  One cannot optimize what isn't properly modeled. Therefore, it is important\nto understand the hardware efficiency of DL models during serving for making an\ninference, before even training the model. This key observation has motivated\nthe use of predictive models to capture the hardware performance or energy\nefficiency of DL applications. Furthermore, DL practitioners are challenged\nwith the task of designing the DNN model, i.e., of tuning the hyper-parameters\nof the DNN architecture, while optimizing for both accuracy of the DL model and\nits hardware efficiency. Therefore, state-of-the-art methodologies have\nproposed hardware-aware hyper-parameter optimization techniques. In this paper,\nwe provide a comprehensive assessment of state-of-the-art work and selected\nresults on the hardware-aware modeling and optimization for DL applications. We\nalso highlight several open questions that are poised to give rise to novel\nhardware-aware designs in the next few years, as DL applications continue to\nsignificantly impact associated hardware systems and platforms. \n\n"}
{"id": "1809.05822", "contents": "Title: Aesthetic-based Clothing Recommendation Abstract: Recently, product images have gained increasing attention in clothing\nrecommendation since the visual appearance of clothing products has a\nsignificant impact on consumers' decision. Most existing methods rely on\nconventional features to represent an image, such as the visual features\nextracted by convolutional neural networks (CNN features) and the\nscale-invariant feature transform algorithm (SIFT features), color histograms,\nand so on. Nevertheless, one important type of features, the \\emph{aesthetic\nfeatures}, is seldom considered. It plays a vital role in clothing\nrecommendation since a users' decision depends largely on whether the clothing\nis in line with her aesthetics, however the conventional image features cannot\nportray this directly. To bridge this gap, we propose to introduce the\naesthetic information, which is highly relevant with user preference, into\nclothing recommender systems. To achieve this, we first present the aesthetic\nfeatures extracted by a pre-trained neural network, which is a brain-inspired\ndeep structure trained for the aesthetic assessment task. Considering that the\naesthetic preference varies significantly from user to user and by time, we\nthen propose a new tensor factorization model to incorporate the aesthetic\nfeatures in a personalized manner. We conduct extensive experiments on\nreal-world datasets, which demonstrate that our approach can capture the\naesthetic preference of users and significantly outperform several\nstate-of-the-art recommendation methods. \n\n"}
{"id": "1809.05896", "contents": "Title: Classifying Process Instances Using Recurrent Neural Networks Abstract: Process Mining consists of techniques where logs created by operative systems\nare transformed into process models. In process mining tools it is often\ndesired to be able to classify ongoing process instances, e.g., to predict how\nlong the process will still require to complete, or to classify process\ninstances to different classes based only on the activities that have occurred\nin the process instance thus far. Recurrent neural networks and its subclasses,\nsuch as Gated Recurrent Unit (GRU) and Long Short-Term Memory (LSTM), have been\ndemonstrated to be able to learn relevant temporal features for subsequent\nclassification tasks. In this paper we apply recurrent neural networks to\nclassifying process instances. The proposed model is trained in a supervised\nfashion using labeled process instances extracted from event log traces. This\nis the first time we know of GRU having been used in classifying business\nprocess instances. Our main experimental results shows that GRU outperforms\nLSTM remarkably in training time while giving almost identical accuracies to\nLSTM models. Additional contributions of our paper are improving the\nclassification model training time by filtering infrequent activities, which is\na technique commonly used, e.g., in Natural Language Processing (NLP). \n\n"}
{"id": "1809.06219", "contents": "Title: Ensemble learning with 3D convolutional neural networks for\n  connectome-based prediction Abstract: The specificty and sensitivity of resting state functional MRI (rs-fMRI)\nmeasurements depend on pre-processing choices, such as the parcellation scheme\nused to define regions of interest (ROIs). In this study, we critically\nevaluate the effect of brain parcellations on machine learning models applied\nto rs-fMRI data. Our experiments reveal a remarkable trend: On average, models\nwith stochastic parcellations consistently perform as well as models with\nwidely used atlases at the same spatial scale. We thus propose an ensemble\nlearning strategy to combine the predictions from models trained on\nconnectivity data extracted using different (e.g., stochastic) parcellations.\nWe further present an implementation of our ensemble learning strategy with a\nnovel 3D Convolutional Neural Network (CNN) approach. The proposed CNN approach\ntakes advantage of the full-resolution 3D spatial structure of rs-fMRI data and\nfits non-linear predictive models. Our ensemble CNN framework overcomes the\nlimitations of traditional machine learning models for connectomes that often\nrely on region-based summary statistics and/or linear models. We showcase our\napproach on a classification (autism patients versus healthy controls) and a\nregression problem (prediction of subject's age), and report promising results. \n\n"}
{"id": "1809.06222", "contents": "Title: GANs for Medical Image Analysis Abstract: Generative Adversarial Networks (GANs) and their extensions have carved open\nmany exciting ways to tackle well known and challenging medical image analysis\nproblems such as medical image de-noising, reconstruction, segmentation, data\nsimulation, detection or classification. Furthermore, their ability to\nsynthesize images at unprecedented levels of realism also gives hope that the\nchronic scarcity of labeled data in the medical field can be resolved with the\nhelp of these generative models. In this review paper, a broad overview of\nrecent literature on GANs for medical applications is given, the shortcomings\nand opportunities of the proposed methods are thoroughly discussed and\npotential future work is elaborated. We review the most relevant papers\npublished until the submission date. For quick access, important details such\nas the underlying method, datasets and performance are tabulated. An\ninteractive visualization which categorizes all papers to keep the review\nalive, is available at\nhttp://livingreview.in.tum.de/GANs_for_Medical_Applications. \n\n"}
{"id": "1809.06227", "contents": "Title: Improving Reinforcement Learning Based Image Captioning with Natural\n  Language Prior Abstract: Recently, Reinforcement Learning (RL) approaches have demonstrated advanced\nperformance in image captioning by directly optimizing the metric used for\ntesting. However, this shaped reward introduces learning biases, which reduces\nthe readability of generated text. In addition, the large sample space makes\ntraining unstable and slow. To alleviate these issues, we propose a simple\ncoherent solution that constrains the action space using an n-gram language\nprior. Quantitative and qualitative evaluations on benchmarks show that RL with\nthe simple add-on module performs favorably against its counterpart in terms of\nboth readability and speed of convergence. Human evaluation results show that\nour model is more human readable and graceful. The implementation will become\npublicly available upon the acceptance of the paper. \n\n"}
{"id": "1809.06573", "contents": "Title: Runtime Monitoring Neuron Activation Patterns Abstract: For using neural networks in safety critical domains, it is important to know\nif a decision made by a neural network is supported by prior similarities in\ntraining. We propose runtime neuron activation pattern monitoring - after the\nstandard training process, one creates a monitor by feeding the training data\nto the network again in order to store the neuron activation patterns in\nabstract form. In operation, a classification decision over an input is further\nsupplemented by examining if a pattern similar (measured by Hamming distance)\nto the generated pattern is contained in the monitor. If the monitor does not\ncontain any pattern similar to the generated pattern, it raises a warning that\nthe decision is not based on the training data. Our experiments show that, by\nadjusting the similarity-threshold for activation patterns, the monitors can\nreport a significant portion of misclassfications to be not supported by\ntraining with a small false-positive rate, when evaluated on a test set. \n\n"}
{"id": "1809.07258", "contents": "Title: DPPy: Sampling DPPs with Python Abstract: Determinantal point processes (DPPs) are specific probability distributions\nover clouds of points that are used as models and computational tools across\nphysics, probability, statistics, and more recently machine learning. Sampling\nfrom DPPs is a challenge and therefore we present DPPy, a Python toolbox that\ngathers known exact and approximate sampling algorithms for both finite and\ncontinuous DPPs. The project is hosted on GitHub and equipped with an extensive\ndocumentation. \n\n"}
{"id": "1809.07276", "contents": "Title: Music Mood Detection Based On Audio And Lyrics With Deep Neural Net Abstract: We consider the task of multimodal music mood prediction based on the audio\nsignal and the lyrics of a track. We reproduce the implementation of\ntraditional feature engineering based approaches and propose a new model based\non deep learning. We compare the performance of both approaches on a database\ncontaining 18,000 tracks with associated valence and arousal values and show\nthat our approach outperforms classical models on the arousal detection task,\nand that both approaches perform equally on the valence prediction task. We\nalso compare the a posteriori fusion with fusion of modalities optimized\nsimultaneously with each unimodal model, and observe a significant improvement\nof valence prediction. We release part of our database for comparison purposes. \n\n"}
{"id": "1809.08530", "contents": "Title: Provably Correct Automatic Subdifferentiation for Qualified Programs Abstract: The Cheap Gradient Principle (Griewank 2008) --- the computational cost of\ncomputing the gradient of a scalar-valued function is nearly the same (often\nwithin a factor of $5$) as that of simply computing the function itself --- is\nof central importance in optimization; it allows us to quickly obtain (high\ndimensional) gradients of scalar loss functions which are subsequently used in\nblack box gradient-based optimization procedures. The current state of affairs\nis markedly different with regards to computing subderivatives: widely used ML\nlibraries, including TensorFlow and PyTorch, do not correctly compute\n(generalized) subderivatives even on simple examples. This work considers the\nquestion: is there a Cheap Subgradient Principle? Our main result shows that,\nunder certain restrictions on our library of nonsmooth functions (standard in\nnonlinear programming), provably correct generalized subderivatives can be\ncomputed at a computational cost that is within a (dimension-free) factor of\n$6$ of the cost of computing the scalar function itself. \n\n"}
{"id": "1809.09035", "contents": "Title: FeatureAnalytics: An approach to derive relevant attributes for\n  analyzing Android Malware Abstract: Ever increasing number of Android malware, has always been a concern for\ncybersecurity professionals. Even though plenty of anti-malware solutions\nexist, a rational and pragmatic approach for the same is rare and has to be\ninspected further. In this paper, we propose a novel two-set feature selection\napproach based on Rough Set and Statistical Test named as RSST to extract\nrelevant system calls. To address the problem of higher dimensional attribute\nset, we derived suboptimal system call space by applying the proposed feature\nselection method to maximize the separability between malware and benign\nsamples. Comprehensive experiments conducted on a dataset consisting of 3500\nsamples with 30 RSST derived essential system calls resulted in an accuracy of\n99.9%, Area Under Curve (AUC) of 1.0, with 1% False Positive Rate (FPR).\nHowever, other feature selectors (Information Gain, CFsSubsetEval, ChiSquare,\nFreqSel and Symmetric Uncertainty) used in the domain of malware analysis\nresulted in the accuracy of 95.5% with 8.5% FPR. Besides, empirical analysis of\nRSST derived system calls outperform other attributes such as permissions,\nopcodes, API, methods, call graphs, Droidbox attributes and network traces. \n\n"}
{"id": "1809.09147", "contents": "Title: Better Safe than Sorry: Evidence Accumulation Allows for Safe\n  Reinforcement Learning Abstract: In the real world, agents often have to operate in situations with incomplete\ninformation, limited sensing capabilities, and inherently stochastic\nenvironments, making individual observations incomplete and unreliable.\nMoreover, in many situations it is preferable to delay a decision rather than\nrun the risk of making a bad decision. In such situations it is necessary to\naggregate information before taking an action; however, most state of the art\nreinforcement learning (RL) algorithms are biased towards taking actions\n\\textit{at every time step}, even if the agent is not particularly confident in\nits chosen action. This lack of caution can lead the agent to make critical\nmistakes, regardless of prior experience and acclimation to the environment.\nMotivated by theories of dynamic resolution of uncertainty during decision\nmaking in biological brains, we propose a simple accumulator module which\naccumulates evidence in favor of each possible decision, encodes uncertainty as\na dynamic competition between actions, and acts on the environment only when it\nis sufficiently confident in the chosen action. The agent makes no decision by\ndefault, and the burden of proof to make a decision falls on the policy to\naccrue evidence strongly in favor of a single decision. Our results show that\nthis accumulator module achieves near-optimal performance on a simple guessing\ngame, far outperforming deep recurrent networks using traditional, forced\naction selection policies. \n\n"}
{"id": "1809.09307", "contents": "Title: Utilizing Class Information for Deep Network Representation Shaping Abstract: Statistical characteristics of deep network representations, such as sparsity\nand correlation, are known to be relevant to the performance and\ninterpretability of deep learning. When a statistical characteristic is\ndesired, often an adequate regularizer can be designed and applied during the\ntraining phase. Typically, such a regularizer aims to manipulate a statistical\ncharacteristic over all classes together. For classification tasks, however, it\nmight be advantageous to enforce the desired characteristic per class such that\ndifferent classes can be better distinguished. Motivated by the idea, we design\ntwo class-wise regularizers that explicitly utilize class information:\nclass-wise Covariance Regularizer (cw-CR) and class-wise Variance Regularizer\n(cw-VR). cw-CR targets to reduce the covariance of representations calculated\nfrom the same class samples for encouraging feature independence. cw-VR is\nsimilar, but variance instead of covariance is targeted to improve feature\ncompactness. For the sake of completeness, their counterparts without using\nclass information, Covariance Regularizer (CR) and Variance Regularizer (VR),\nare considered together. The four regularizers are conceptually simple and\ncomputationally very efficient, and the visualization shows that the\nregularizers indeed perform distinct representation shaping. In terms of\nclassification performance, significant improvements over the baseline and\nL1/L2 weight regularization methods were found for 21 out of 22 tasks over\npopular benchmark datasets. In particular, cw-VR achieved the best performance\nfor 13 tasks including ResNet-32/110. \n\n"}
{"id": "1809.10121", "contents": "Title: Safely Learning to Control the Constrained Linear Quadratic Regulator Abstract: We study the constrained linear quadratic regulator with unknown dynamics,\naddressing the tension between safety and exploration in data-driven control\ntechniques. We present a framework which allows for system identification\nthrough persistent excitation, while maintaining safety by guaranteeing the\nsatisfaction of state and input constraints. This framework involves a novel\nmethod for synthesizing robust constraint-satisfying feedback controllers,\nleveraging newly developed tools from system level synthesis. We connect\nstatistical results with cost sub-optimality bounds to give non-asymptotic\nguarantees on both estimation and controller performance. \n\n"}
{"id": "1809.10200", "contents": "Title: Compressing the Input for CNNs with the First-Order Scattering Transform Abstract: We study the first-order scattering transform as a candidate for reducing the\nsignal processed by a convolutional neural network (CNN). We show theoretical\nand empirical evidence that in the case of natural images and sufficiently\nsmall translation invariance, this transform preserves most of the signal\ninformation needed for classification while substantially reducing the spatial\nresolution and total signal size. We demonstrate that cascading a CNN with this\nrepresentation performs on par with ImageNet classification models, commonly\nused in downstream tasks, such as the ResNet-50. We subsequently apply our\ntrained hybrid ImageNet model as a base model on a detection system, which has\ntypically larger image inputs. On Pascal VOC and COCO detection tasks we\ndemonstrate improvements in the inference speed and training memory consumption\ncompared to models trained directly on the input image. \n\n"}
{"id": "1809.10239", "contents": "Title: Empty Cities: Image Inpainting for a Dynamic-Object-Invariant Space Abstract: In this paper we present an end-to-end deep learning framework to turn images\nthat show dynamic content, such as vehicles or pedestrians, into realistic\nstatic frames. This objective encounters two main challenges: detecting all the\ndynamic objects, and inpainting the static occluded background with plausible\nimagery. The second problem is approached with a conditional generative\nadversarial model that, taking as input the original dynamic image and its\ndynamic/static binary mask, is capable of generating the final static image.\nThe former challenge is addressed by the use of a convolutional network that\nlearns a multi-class semantic segmentation of the image.\n  These generated images can be used for applications such as augmented reality\nor vision-based robot localization purposes. To validate our approach, we show\nboth qualitative and quantitative comparisons against other state-of-the-art\ninpainting methods by removing the dynamic objects and hallucinating the static\nstructure behind them. Furthermore, to demonstrate the potential of our\nresults, we carry out pilot experiments that show the benefits of our proposal\nfor visual place recognition. \n\n"}
{"id": "1809.10243", "contents": "Title: Segmentation of Skin Lesions and their Attributes Using Multi-Scale\n  Convolutional Neural Networks and Domain Specific Augmentations Abstract: Computer-aided diagnosis systems for classification of different type of skin\nlesions have been an active field of research in recent decades. It has been\nshown that introducing lesions and their attributes masks into lesion\nclassification pipeline can greatly improve the performance. In this paper, we\npropose a framework by incorporating transfer learning for segmenting lesions\nand their attributes based on the convolutional neural networks. The proposed\nframework is based on the encoder-decoder architecture which utilizes a variety\nof pre-trained networks in the encoding path and generates the prediction map\nby combining multi-scale information in decoding path using a pyramid pooling\nmanner. To address the lack of training data and increase the proposed model\ngeneralization, an extensive set of novel domain-specific augmentation routines\nhave been applied to simulate the real variations in dermoscopy images.\nFinally, by performing broad experiments on three different data sets obtained\nfrom International Skin Imaging Collaboration archive (ISIC2016, ISIC2017, and\nISIC2018 challenges data sets), we show that the proposed method outperforms\nother state-of-the-art approaches for ISIC2016 and ISIC2017 segmentation task\nand achieved the first rank on the leader-board of ISIC2018 attribute detection\ntask. \n\n"}
{"id": "1809.10326", "contents": "Title: Boosting Trust Region Policy Optimization by Normalizing Flows Policy Abstract: We propose to improve trust region policy search with normalizing flows\npolicy. We illustrate that when the trust region is constructed by KL\ndivergence constraints, normalizing flows policy generates samples far from the\n'center' of the previous policy iterate, which potentially enables better\nexploration and helps avoid bad local optima. Through extensive comparisons, we\nshow that the normalizing flows policy significantly improves upon baseline\narchitectures especially on high-dimensional tasks with complex dynamics. \n\n"}
{"id": "1809.10333", "contents": "Title: Using Autoencoders To Learn Interesting Features For Detecting\n  Surveillance Aircraft Abstract: This paper explores using a Long short-term memory (LSTM) based sequence\nautoencoder to learn interesting features for detecting surveillance aircraft\nusing ADS-B flight data. An aircraft periodically broadcasts ADS-B (Automatic\nDependent Surveillance - Broadcast) data to ground receivers. The ability of\nLSTM networks to model varying length time series data and remember\ndependencies that span across events makes it an ideal candidate for\nimplementing a sequence autoencoder for ADS-B data because of its possible\nvariable length time series, irregular sampling and dependencies that span\nacross events. \n\n"}
{"id": "1809.10858", "contents": "Title: Efficiently testing local optimality and escaping saddles for ReLU\n  networks Abstract: We provide a theoretical algorithm for checking local optimality and escaping\nsaddles at nondifferentiable points of empirical risks of two-layer ReLU\nnetworks. Our algorithm receives any parameter value and returns: local\nminimum, second-order stationary point, or a strict descent direction. The\npresence of $M$ data points on the nondifferentiability of the ReLU divides the\nparameter space into at most $2^M$ regions, which makes analysis difficult. By\nexploiting polyhedral geometry, we reduce the total computation down to one\nconvex quadratic program (QP) for each hidden node, $O(M)$ (in)equality tests,\nand one (or a few) nonconvex QP. For the last QP, we show that our specific\nproblem can be solved efficiently, in spite of nonconvexity. In the benign\ncase, we solve one equality constrained QP, and we prove that projected\ngradient descent solves it exponentially fast. In the bad case, we have to\nsolve a few more inequality constrained QPs, but we prove that the time\ncomplexity is exponential only in the number of inequality constraints. Our\nexperiments show that either benign case or bad case with very few inequality\nconstraints occurs, implying that our algorithm is efficient in most cases. \n\n"}
{"id": "1810.00150", "contents": "Title: Directional Analysis of Stochastic Gradient Descent via von Mises-Fisher\n  Distributions in Deep learning Abstract: Although stochastic gradient descent (SGD) is a driving force behind the\nrecent success of deep learning, our understanding of its dynamics in a\nhigh-dimensional parameter space is limited. In recent years, some researchers\nhave used the stochasticity of minibatch gradients, or the signal-to-noise\nratio, to better characterize the learning dynamics of SGD. Inspired from these\nwork, we here analyze SGD from a geometrical perspective by inspecting the\nstochasticity of the norms and directions of minibatch gradients. We propose a\nmodel of the directional concentration for minibatch gradients through von\nMises-Fisher (VMF) distribution, and show that the directional uniformity of\nminibatch gradients increases over the course of SGD. We empirically verify our\nresult using deep convolutional networks and observe a higher correlation\nbetween the gradient stochasticity and the proposed directional uniformity than\nthat against the gradient norm stochasticity, suggesting that the directional\nstatistics of minibatch gradients is a major factor behind SGD. \n\n"}
{"id": "1810.00299", "contents": "Title: Training Behavior of Sparse Neural Network Topologies Abstract: Improvements in the performance of deep neural networks have often come\nthrough the design of larger and more complex networks. As a result, fast\nmemory is a significant limiting factor in our ability to improve network\nperformance. One approach to overcoming this limit is the design of sparse\nneural networks, which can be both very large and efficiently trained. In this\npaper we experiment training on sparse neural network topologies. We test\npruning-based topologies, which are derived from an initially dense network\nwhose connections are pruned, as well as RadiX-Nets, a class of network\ntopologies with proven connectivity and sparsity properties. Results show that\nsparse networks obtain accuracies comparable to dense networks, but extreme\nlevels of sparsity cause instability in training, which merits further study. \n\n"}
{"id": "1810.00361", "contents": "Title: Using State Predictions for Value Regularization in Curiosity Driven\n  Deep Reinforcement Learning Abstract: Learning in sparse reward settings remains a challenge in Reinforcement\nLearning, which is often addressed by using intrinsic rewards. One promising\nstrategy is inspired by human curiosity, requiring the agent to learn to\npredict the future. In this paper a curiosity-driven agent is extended to use\nthese predictions directly for training. To achieve this, the agent predicts\nthe value function of the next state at any point in time. Subsequently, the\nconsistency of this prediction with the current value function is measured,\nwhich is then used as a regularization term in the loss function of the\nalgorithm. Experiments were made on grid-world environments as well as on a 3D\nnavigation task, both with sparse rewards. In the first case the extended agent\nis able to learn significantly faster than the baselines. \n\n"}
{"id": "1810.00393", "contents": "Title: Deep, Skinny Neural Networks are not Universal Approximators Abstract: In order to choose a neural network architecture that will be effective for a\nparticular modeling problem, one must understand the limitations imposed by\neach of the potential options. These limitations are typically described in\nterms of information theoretic bounds, or by comparing the relative complexity\nneeded to approximate example functions between different architectures. In\nthis paper, we examine the topological constraints that the architecture of a\nneural network imposes on the level sets of all the functions that it is able\nto approximate. This approach is novel for both the nature of the limitations\nand the fact that they are independent of network depth for a broad family of\nactivation functions. \n\n"}
{"id": "1810.00471", "contents": "Title: Identifying Bias in AI using Simulation Abstract: Machine learned models exhibit bias, often because the datasets used to train\nthem are biased. This presents a serious problem for the deployment of such\ntechnology, as the resulting models might perform poorly on populations that\nare minorities within the training set and ultimately present higher risks to\nthem. We propose to use high-fidelity computer simulations to interrogate and\ndiagnose biases within ML classifiers. We present a framework that leverages\nBayesian parameter search to efficiently characterize the high dimensional\nfeature space and more quickly identify weakness in performance. We apply our\napproach to an example domain, face detection, and show that it can be used to\nhelp identify demographic biases in commercial face application programming\ninterfaces (APIs). \n\n"}
{"id": "1810.00500", "contents": "Title: One Network to Solve All ROIs: Deep Learning CT for Any ROI using\n  Differentiated Backprojection Abstract: Computed tomography for region-of-interest (ROI) reconstruction has\nadvantages of reducing X-ray radiation dose and using a small detector.\nHowever, standard analytic reconstruction methods suffer from severe cupping\nartifacts, and existing model-based iterative reconstruction methods require\nextensive computations. Recently, we proposed a deep neural network to learn\nthe cupping artifact, but the network is not well generalized for different\nROIs due to the singularities in the corrupted images. Therefore, there is an\nincreasing demand for a neural network that works well for any ROI sizes. In\nthis paper, two types of neural networks are designed. The first type learns\nROI size-specific cupping artifacts from the analytic reconstruction images,\nwhereas the second type network is to learn to invert the finite Hilbert\ntransform from the truncated differentiated backprojection (DBP) data. Their\ngeneralizability for any ROI sizes is then examined. Experimental results show\nthat the new type of neural network significantly outperforms the existing\niterative methods for any ROI size in spite of significantly reduced run-time\ncomplexity. Since the proposed method consistently surpasses existing methods\nfor any ROIs, it can be used as a general CT reconstruction engine for many\npractical applications without compromising possible detector truncation. \n\n"}
{"id": "1810.00668", "contents": "Title: Wronging a Right: Generating Better Errors to Improve Grammatical Error\n  Detection Abstract: Grammatical error correction, like other machine learning tasks, greatly\nbenefits from large quantities of high quality training data, which is\ntypically expensive to produce. While writing a program to automatically\ngenerate realistic grammatical errors would be difficult, one could learn the\ndistribution of naturallyoccurring errors and attempt to introduce them into\nother datasets. Initial work on inducing errors in this way using statistical\nmachine translation has shown promise; we investigate cheaply constructing\nsynthetic samples, given a small corpus of human-annotated data, using an\noff-the-rack attentive sequence-to-sequence model and a straight-forward\npost-processing procedure. Our approach yields error-filled artificial data\nthat helps a vanilla bi-directional LSTM to outperform the previous state of\nthe art at grammatical error detection, and a previously introduced model to\ngain further improvements of over 5% $F_{0.5}$ score. When attempting to\ndetermine if a given sentence is synthetic, a human annotator at best achieves\n39.39 $F_1$ score, indicating that our model generates mostly human-like\ninstances. \n\n"}
{"id": "1810.00737", "contents": "Title: Risk-Averse Stochastic Convex Bandit Abstract: Motivated by applications in clinical trials and finance, we study the\nproblem of online convex optimization (with bandit feedback) where the decision\nmaker is risk-averse. We provide two algorithms to solve this problem. The\nfirst one is a descent-type algorithm which is easy to implement. The second\nalgorithm, which combines the ellipsoid method and a center point device,\nachieves (almost) optimal regret bounds with respect to the number of rounds.\nTo the best of our knowledge this is the first attempt to address risk-aversion\nin the online convex bandit problem. \n\n"}
{"id": "1810.01875", "contents": "Title: Relaxed Quantization for Discretized Neural Networks Abstract: Neural network quantization has become an important research area due to its\ngreat impact on deployment of large models on resource constrained devices. In\norder to train networks that can be effectively discretized without loss of\nperformance, we introduce a differentiable quantization procedure.\nDifferentiability can be achieved by transforming continuous distributions over\nthe weights and activations of the network to categorical distributions over\nthe quantization grid. These are subsequently relaxed to continuous surrogates\nthat can allow for efficient gradient-based optimization. We further show that\nstochastic rounding can be seen as a special case of the proposed approach and\nthat under this formulation the quantization grid itself can also be optimized\nwith gradient descent. We experimentally validate the performance of our method\non MNIST, CIFAR 10 and Imagenet classification. \n\n"}
{"id": "1810.02923", "contents": "Title: Adaptive Geo-Topological Independence Criterion Abstract: Testing two potentially multivariate variables for statistical dependence on\nthe basis finite samples is a fundamental statistical challenge. Here we\nexplore a family of tests that adapt to the complexity of the relationship\nbetween the variables, promising robust power across scenarios. Building on the\ndistance correlation, we introduce a family of adaptive independence criteria\nbased on nonlinear monotonic transformations of distances. We show that these\ncriteria, like the distance correlation and RKHS-based criteria, provide\ndependence indicators. We propose a class of adaptive (multi-threshold) test\nstatistics, which form the basis for permutation tests. These tests empirically\noutperform some of the established tests in average and worst-case statistical\nsensitivity across a range of univariate and multivariate relationships, offer\nuseful insights to the data and may deserve further exploration. \n\n"}
{"id": "1810.02927", "contents": "Title: Scaling All-Goals Updates in Reinforcement Learning Using Convolutional\n  Neural Networks Abstract: Being able to reach any desired location in the environment can be a valuable\nasset for an agent. Learning a policy to navigate between all pairs of states\nindividually is often not feasible. An all-goals updating algorithm uses each\ntransition to learn Q-values towards all goals simultaneously and off-policy.\nHowever the expensive numerous updates in parallel limited the approach to\nsmall tabular cases so far. To tackle this problem we propose to use\nconvolutional network architectures to generate Q-values and updates for a\nlarge number of goals at once. We demonstrate the accuracy and generalization\nqualities of the proposed method on randomly generated mazes and Sokoban\npuzzles. In the case of on-screen goal coordinates the resulting mapping from\nframes to distance-maps directly informs the agent about which places are\nreachable and in how many steps. As an example of application we show that\nreplacing the random actions in epsilon-greedy exploration by several actions\ntowards feasible goals generates better exploratory trajectories on Montezuma's\nRevenge and Super Mario All-Stars games. \n\n"}
{"id": "1810.03052", "contents": "Title: Deep convolutional Gaussian processes Abstract: We propose deep convolutional Gaussian processes, a deep Gaussian process\narchitecture with convolutional structure. The model is a principled Bayesian\nframework for detecting hierarchical combinations of local features for image\nclassification. We demonstrate greatly improved image classification\nperformance compared to current Gaussian process approaches on the MNIST and\nCIFAR-10 datasets. In particular, we improve CIFAR-10 accuracy by over 10\npercentage points. \n\n"}
{"id": "1810.03292", "contents": "Title: Sanity Checks for Saliency Maps Abstract: Saliency methods have emerged as a popular tool to highlight features in an\ninput deemed relevant for the prediction of a learned model. Several saliency\nmethods have been proposed, often guided by visual appeal on image data. In\nthis work, we propose an actionable methodology to evaluate what kinds of\nexplanations a given method can and cannot provide. We find that reliance,\nsolely, on visual assessment can be misleading. Through extensive experiments\nwe show that some existing saliency methods are independent both of the model\nand of the data generating process. Consequently, methods that fail the\nproposed tests are inadequate for tasks that are sensitive to either data or\nmodel, such as, finding outliers in the data, explaining the relationship\nbetween inputs and outputs that the model learned, and debugging the model. We\ninterpret our findings through an analogy with edge detection in images, a\ntechnique that requires neither training data nor model. Theory in the case of\na linear model and a single-layer convolutional neural network supports our\nexperimental findings. \n\n"}
{"id": "1810.03307", "contents": "Title: Local Explanation Methods for Deep Neural Networks Lack Sensitivity to\n  Parameter Values Abstract: Explaining the output of a complicated machine learning model like a deep\nneural network (DNN) is a central challenge in machine learning. Several\nproposed local explanation methods address this issue by identifying what\ndimensions of a single input are most responsible for a DNN's output. The goal\nof this work is to assess the sensitivity of local explanations to DNN\nparameter values. Somewhat surprisingly, we find that DNNs with\nrandomly-initialized weights produce explanations that are both visually and\nquantitatively similar to those produced by DNNs with learned weights. Our\nconjecture is that this phenomenon occurs because these explanations are\ndominated by the lower level features of a DNN, and that a DNN's architecture\nprovides a strong prior which significantly affects the representations learned\nat these lower layers. NOTE: This work is now subsumed by our recent\nmanuscript, Sanity Checks for Saliency Maps (to appear NIPS 2018), where we\nexpand on findings and address concerns raised in Sundararajan et. al. (2018). \n\n"}
{"id": "1810.03505", "contents": "Title: CINIC-10 is not ImageNet or CIFAR-10 Abstract: In this brief technical report we introduce the CINIC-10 dataset as a plug-in\nextended alternative for CIFAR-10. It was compiled by combining CIFAR-10 with\nimages selected and downsampled from the ImageNet database. We present the\napproach to compiling the dataset, illustrate the example images for different\nclasses, give pixel distributions for each part of the repository, and give\nsome standard benchmarks for well known models. Details for download, usage,\nand compilation can be found in the associated github repository. \n\n"}
{"id": "1810.03913", "contents": "Title: Analyzing the Noise Robustness of Deep Neural Networks Abstract: Deep neural networks (DNNs) are vulnerable to maliciously generated\nadversarial examples. These examples are intentionally designed by making\nimperceptible perturbations and often mislead a DNN into making an incorrect\nprediction. This phenomenon means that there is significant risk in applying\nDNNs to safety-critical applications, such as driverless cars. To address this\nissue, we present a visual analytics approach to explain the primary cause of\nthe wrong predictions introduced by adversarial examples. The key is to analyze\nthe datapaths of the adversarial examples and compare them with those of the\nnormal examples. A datapath is a group of critical neurons and their\nconnections. To this end, we formulate the datapath extraction as a subset\nselection problem and approximately solve it based on back-propagation. A\nmulti-level visualization consisting of a segmented DAG (layer level), an Euler\ndiagram (feature map level), and a heat map (neuron level), has been designed\nto help experts investigate datapaths from the high-level layers to the\ndetailed neuron activations. Two case studies are conducted that demonstrate\nthe promise of our approach in support of explaining the working mechanism of\nadversarial examples. \n\n"}
{"id": "1810.03964", "contents": "Title: Rate-Accuracy Trade-Off In Video Classification With Deep Convolutional\n  Neural Networks Abstract: Advanced video classification systems decode video frames to derive the\nnecessary texture and motion representations for ingestion and analysis by\nspatio-temporal deep convolutional neural networks (CNNs). However, when\nconsidering visual Internet-of-Things applications, surveillance systems and\nsemantic crawlers of large video repositories, the video capture and the\nCNN-based semantic analysis parts do not tend to be co-located. This\nnecessitates the transport of compressed video over networks and incurs\nsignificant overhead in bandwidth and energy consumption, thereby significantly\nundermining the deployment potential of such systems. In this paper, we\ninvestigate the trade-off between the encoding bitrate and the achievable\naccuracy of CNN-based video classification models that directly ingest\nAVC/H.264 and HEVC encoded videos. Instead of retaining entire compressed video\nbitstreams and applying complex optical flow calculations prior to CNN\nprocessing, we only retain motion vector and select texture information at\nsignificantly-reduced bitrates and apply no additional processing prior to CNN\ningestion. Based on three CNN architectures and two action recognition\ndatasets, we achieve 11%-94% saving in bitrate with marginal effect on\nclassification accuracy. A model-based selection between multiple CNNs\nincreases these savings further, to the point where, if up to 7% loss of\naccuracy can be tolerated, video classification can take place with as little\nas 3 kbps for the transport of the required compressed video information to the\nsystem implementing the CNN models. \n\n"}
{"id": "1810.03969", "contents": "Title: A Generative Adversarial Model for Right Ventricle Segmentation Abstract: The clinical management of several cardiovascular conditions, such as\npulmonary hypertension, require the assessment of the right ventricular (RV)\nfunction. This work addresses the fully automatic and robust access to one of\nthe key RV biomarkers, its ejection fraction, from the gold standard imaging\nmodality, MRI. The problem becomes the accurate segmentation of the RV blood\npool from cine MRI sequences. This work proposes a solution based on Fully\nConvolutional Neural Networks (FCNN), where our first contribution is the\noptimal combination of three concepts (the convolution Gated Recurrent Units\n(GRU), the Generative Adversarial Networks (GAN), and the L1 loss function)\nthat achieves an improvement of 0.05 and 3.49 mm in Dice Index and Hausdorff\nDistance respectively with respect to the baseline FCNN. This improvement is\nthen doubled by our second contribution, the ROI-GAN, that sets two GANs to\ncooperate working at two fields of view of the image, its full resolution and\nthe region of interest (ROI). Our rationale here is to better guide the FCNN\nlearning by combining global (full resolution) and local Region Of Interest\n(ROI) features. The study is conducted in a large in-house dataset of $\\sim$\n23.000 segmented MRI slices, and its generality is verified in a publicly\navailable dataset. \n\n"}
{"id": "1810.03982", "contents": "Title: Deep Decoder: Concise Image Representations from Untrained\n  Non-convolutional Networks Abstract: Deep neural networks, in particular convolutional neural networks, have\nbecome highly effective tools for compressing images and solving inverse\nproblems including denoising, inpainting, and reconstruction from few and noisy\nmeasurements. This success can be attributed in part to their ability to\nrepresent and generate natural images well. Contrary to classical tools such as\nwavelets, image-generating deep neural networks have a large number of\nparameters---typically a multiple of their output dimension---and need to be\ntrained on large datasets. In this paper, we propose an untrained simple image\nmodel, called the deep decoder, which is a deep neural network that can\ngenerate natural images from very few weight parameters. The deep decoder has a\nsimple architecture with no convolutions and fewer weight parameters than the\noutput dimensionality. This underparameterization enables the deep decoder to\ncompress images into a concise set of network weights, which we show is on par\nwith wavelet-based thresholding. Further, underparameterization provides a\nbarrier to overfitting, allowing the deep decoder to have state-of-the-art\nperformance for denoising. The deep decoder is simple in the sense that each\nlayer has an identical structure that consists of only one upsampling unit,\npixel-wise linear combination of channels, ReLU activation, and channelwise\nnormalization. This simplicity makes the network amenable to theoretical\nanalysis, and it sheds light on the aspects of neural networks that enable them\nto form effective signal representations. \n\n"}
{"id": "1810.04247", "contents": "Title: Feature Selection using Stochastic Gates Abstract: Feature selection problems have been extensively studied for linear\nestimation, for instance, Lasso, but less emphasis has been placed on feature\nselection for non-linear functions. In this study, we propose a method for\nfeature selection in high-dimensional non-linear function estimation problems.\nThe new procedure is based on minimizing the $\\ell_0$ norm of the vector of\nindicator variables that represent if a feature is selected or not. Our\napproach relies on the continuous relaxation of Bernoulli distributions, which\nallows our model to learn the parameters of the approximate Bernoulli\ndistributions via gradient descent. This general framework simultaneously\nminimizes a loss function while selecting relevant features. Furthermore, we\nprovide an information-theoretic justification of incorporating Bernoulli\ndistribution into our approach and demonstrate the potential of the approach on\nsynthetic and real-life applications. \n\n"}
{"id": "1810.04714", "contents": "Title: Training Generative Adversarial Networks with Binary Neurons by\n  End-to-end Backpropagation Abstract: We propose the BinaryGAN, a novel generative adversarial network (GAN) that\nuses binary neurons at the output layer of the generator. We employ the\nsigmoid-adjusted straight-through estimators to estimate the gradients for the\nbinary neurons and train the whole network by end-to-end backpropogation. The\nproposed model is able to directly generate binary-valued predictions at test\ntime. We implement such a model to generate binarized MNIST digits and\nexperimentally compare the performance for different types of binary neurons,\nGAN objectives and network architectures. Although the results are still\npreliminary, we show that it is possible to train a GAN that has binary neurons\nand that the use of gradient estimators can be a promising direction for\nmodeling discrete distributions with GANs. For reproducibility, the source code\nis available at https://github.com/salu133445/binarygan . \n\n"}
{"id": "1810.04719", "contents": "Title: Fully Supervised Speaker Diarization Abstract: In this paper, we propose a fully supervised speaker diarization approach,\nnamed unbounded interleaved-state recurrent neural networks (UIS-RNN). Given\nextracted speaker-discriminative embeddings (a.k.a. d-vectors) from input\nutterances, each individual speaker is modeled by a parameter-sharing RNN,\nwhile the RNN states for different speakers interleave in the time domain. This\nRNN is naturally integrated with a distance-dependent Chinese restaurant\nprocess (ddCRP) to accommodate an unknown number of speakers. Our system is\nfully supervised and is able to learn from examples where time-stamped speaker\nlabels are annotated. We achieved a 7.6% diarization error rate on NIST SRE\n2000 CALLHOME, which is better than the state-of-the-art method using spectral\nclustering. Moreover, our method decodes in an online fashion while most\nstate-of-the-art systems rely on offline clustering. \n\n"}
{"id": "1810.04754", "contents": "Title: Efficient Tensor Decomposition with Boolean Factors Abstract: Tensor decomposition has been extensively used as a tool for exploratory\nanalysis. Motivated by neuroscience applications, we study tensor decomposition\nwith Boolean factors. The resulting optimization problem is challenging due to\nthe non-convex objective and the combinatorial constraints. We propose Binary\nMatching Pursuit (BMP), a novel generalization of the matching pursuit strategy\nto decompose the tensor efficiently. BMP iteratively searches for atoms in a\ngreedy fashion. The greedy atom search step is solved efficiently via a\nMAXCUT-like boolean quadratic program. We prove that BMP is guaranteed to\nconverge sublinearly to the optimal solution and recover the factors under mild\nidentifiability conditions. Experiments demonstrate the superior performance of\nour method over baselines on synthetic and real datasets. We also showcase the\napplication of BMP in quantifying neural interactions underlying\nhigh-resolution spatiotemporal ECoG recordings. \n\n"}
{"id": "1810.05206", "contents": "Title: MeshAdv: Adversarial Meshes for Visual Recognition Abstract: Highly expressive models such as deep neural networks (DNNs) have been widely\napplied to various applications. However, recent studies show that DNNs are\nvulnerable to adversarial examples, which are carefully crafted inputs aiming\nto mislead the predictions. Currently, the majority of these studies have\nfocused on perturbation added to image pixels, while such manipulation is not\nphysically realistic. Some works have tried to overcome this limitation by\nattaching printable 2D patches or painting patterns onto surfaces, but can be\npotentially defended because 3D shape features are intact. In this paper, we\npropose meshAdv to generate \"adversarial 3D meshes\" from objects that have rich\nshape features but minimal textural variation. To manipulate the shape or\ntexture of the objects, we make use of a differentiable renderer to compute\naccurate shading on the shape and propagate the gradient. Extensive experiments\nshow that the generated 3D meshes are effective in attacking both classifiers\nand object detectors. We evaluate the attack under different viewpoints. In\naddition, we design a pipeline to perform black-box attack on a photorealistic\nrenderer with unknown rendering parameters. \n\n"}
{"id": "1810.05221", "contents": "Title: MDGAN: Boosting Anomaly Detection Using \\\\Multi-Discriminator Generative\n  Adversarial Networks Abstract: Anomaly detection is often considered a challenging field of machine learning\ndue to the difficulty of obtaining anomalous samples for training and the need\nto obtain a sufficient amount of training data. In recent years, autoencoders\nhave been shown to be effective anomaly detectors that train only on \"normal\"\ndata. Generative adversarial networks (GANs) have been used to generate\nadditional training samples for classifiers, thus making them more accurate and\nrobust. However, in anomaly detection GANs are only used to reconstruct\nexisting samples rather than to generate additional ones. This stems both from\nthe small amount and lack of diversity of anomalous data in most domains. In\nthis study we propose MDGAN, a novel GAN architecture for improving anomaly\ndetection through the generation of additional samples. Our approach uses two\ndiscriminators: a dense network for determining whether the generated samples\nare of sufficient quality (i.e., valid) and an autoencoder that serves as an\nanomaly detector. MDGAN enables us to reconcile two conflicting goals: 1)\ngenerate high-quality samples that can fool the first discriminator, and 2)\ngenerate samples that can eventually be effectively reconstructed by the second\ndiscriminator, thus improving its performance. Empirical evaluation on a\ndiverse set of datasets demonstrates the merits of our approach. \n\n"}
{"id": "1810.05222", "contents": "Title: Efficient Augmentation via Data Subsampling Abstract: Data augmentation is commonly used to encode invariances in learning methods.\nHowever, this process is often performed in an inefficient manner, as\nartificial examples are created by applying a number of transformations to all\npoints in the training set. The resulting explosion of the dataset size can be\nan issue in terms of storage and training costs, as well as in selecting and\ntuning the optimal set of transformations to apply. In this work, we\ndemonstrate that it is possible to significantly reduce the number of data\npoints included in data augmentation while realizing the same accuracy and\ninvariance benefits of augmenting the entire dataset. We propose a novel set of\nsubsampling policies, based on model influence and loss, that can achieve a 90%\nreduction in augmentation set size while maintaining the accuracy gains of\nstandard data augmentation. \n\n"}
{"id": "1810.05247", "contents": "Title: Real-time Faulted Line Localization and PMU Placement in Power Systems\n  through Convolutional Neural Networks Abstract: Diverse fault types, fast re-closures, and complicated transient states after\na fault event make real-time fault location in power grids challenging.\nExisting localization techniques in this area rely on simplistic assumptions,\nsuch as static loads, or require much higher sampling rates or total\nmeasurement availability. This paper proposes a faulted line localization\nmethod based on a Convolutional Neural Network (CNN) classifier using bus\nvoltages. Unlike prior data-driven methods, the proposed classifier is based on\nfeatures with physical interpretations that improve the robustness of the\nlocation performance. The accuracy of our CNN based localization tool is\ndemonstrably superior to other machine learning classifiers in the literature.\nTo further improve the location performance, a joint phasor measurement units\n(PMU) placement strategy is proposed and validated against other methods. A\nsignificant aspect of our methodology is that under very low observability (7%\nof buses), the algorithm is still able to localize the faulted line to a small\nneighborhood with high probability. The performance of our scheme is validated\nthrough simulations of faults of various types in the IEEE 39-bus and 68-bus\npower systems under varying uncertain conditions, system observability, and\nmeasurement quality. \n\n"}
{"id": "1810.05724", "contents": "Title: Unpaired High-Resolution and Scalable Style Transfer Using Generative\n  Adversarial Networks Abstract: Neural networks have proven their capabilities by outperforming many other\napproaches on regression or classification tasks on various kinds of data.\nOther astonishing results have been achieved using neural nets as data\ngenerators, especially in settings of generative adversarial networks (GANs).\nOne special application is the field of image domain translations. Here, the\ngoal is to take an image with a certain style (e.g. a photography) and\ntransform it into another one (e.g. a painting). If such a task is performed\nfor unpaired training examples, the corresponding GAN setting is complex, the\nneural networks are large, and this leads to a high peak memory consumption\nduring, both, training and evaluation phase. This sets a limit to the highest\nprocessable image size. We address this issue by the idea of not processing the\nwhole image at once, but to train and evaluate the domain translation on the\nlevel of overlapping image subsamples. This new approach not only enables us to\ntranslate high-resolution images that otherwise cannot be processed by the\nneural network at once, but also allows us to work with comparably small neural\nnetworks and with limited hardware resources. Additionally, the number of\nimages required for the training process is significantly reduced. We present\nhigh-quality results on images with a total resolution of up to over 50\nmegapixels and emonstrate that our method helps to preserve local image details\nwhile it also keeps global consistency. \n\n"}
{"id": "1810.05731", "contents": "Title: Image Super-Resolution Using VDSR-ResNeXt and SRCGAN Abstract: Over the past decade, many Super Resolution techniques have been developed\nusing deep learning. Among those, generative adversarial networks (GAN) and\nvery deep convolutional networks (VDSR) have shown promising results in terms\nof HR image quality and computational speed. In this paper, we propose two\napproaches based on these two algorithms: VDSR-ResNeXt, which is a deep\nmulti-branch convolutional network inspired by VDSR and ResNeXt; and SRCGAN,\nwhich is a conditional GAN that explicitly passes class labels as input to the\nGAN. The two methods were implemented on common SR benchmark datasets for both\nquantitative and qualitative assessment. \n\n"}
{"id": "1810.05732", "contents": "Title: A Novel Domain Adaptation Framework for Medical Image Segmentation Abstract: We propose a segmentation framework that uses deep neural networks and\nintroduce two innovations. First, we describe a biophysics-based domain\nadaptation method. Second, we propose an automatic method to segment white and\ngray matter, and cerebrospinal fluid, in addition to tumorous tissue. Regarding\nour first innovation, we use a domain adaptation framework that combines a\nnovel multispecies biophysical tumor growth model with a generative adversarial\nmodel to create realistic looking synthetic multimodal MR images with known\nsegmentation. Regarding our second innovation, we propose an automatic approach\nto enrich available segmentation data by computing the segmentation for healthy\ntissues. This segmentation, which is done using diffeomorphic image\nregistration between the BraTS training data and a set of prelabeled atlases,\nprovides more information for training and reduces the class imbalance problem.\nOur overall approach is not specific to any particular neural network and can\nbe used in conjunction with existing solutions. We demonstrate the performance\nimprovement using a 2D U-Net for the BraTS'18 segmentation challenge. Our\nbiophysics based domain adaptation achieves better results, as compared to the\nexisting state-of-the-art GAN model used to create synthetic data for training. \n\n"}
{"id": "1810.05749", "contents": "Title: Graph HyperNetworks for Neural Architecture Search Abstract: Neural architecture search (NAS) automatically finds the best task-specific\nneural network topology, outperforming many manual architecture designs.\nHowever, it can be prohibitively expensive as the search requires training\nthousands of different networks, while each can last for hours. In this work,\nwe propose the Graph HyperNetwork (GHN) to amortize the search cost: given an\narchitecture, it directly generates the weights by running inference on a graph\nneural network. GHNs model the topology of an architecture and therefore can\npredict network performance more accurately than regular hypernetworks and\npremature early stopping. To perform NAS, we randomly sample architectures and\nuse the validation accuracy of networks with GHN generated weights as the\nsurrogate search signal. GHNs are fast -- they can search nearly 10 times\nfaster than other random search methods on CIFAR-10 and ImageNet. GHNs can be\nfurther extended to the anytime prediction setting, where they have found\nnetworks with better speed-accuracy tradeoff than the state-of-the-art manual\ndesigns. \n\n"}
{"id": "1810.06060", "contents": "Title: Distributed learning of deep neural network over multiple agents Abstract: In domains such as health care and finance, shortage of labeled data and\ncomputational resources is a critical issue while developing machine learning\nalgorithms. To address the issue of labeled data scarcity in training and\ndeployment of neural network-based systems, we propose a new technique to train\ndeep neural networks over several data sources. Our method allows for deep\nneural networks to be trained using data from multiple entities in a\ndistributed fashion. We evaluate our algorithm on existing datasets and show\nthat it obtains performance which is similar to a regular neural network\ntrained on a single machine. We further extend it to incorporate\nsemi-supervised learning when training with few labeled samples, and analyze\nany security concerns that may arise. Our algorithm paves the way for\ndistributed training of deep neural networks in data sensitive applications\nwhen raw data may not be shared directly. \n\n"}
{"id": "1810.06544", "contents": "Title: Deep Imitative Models for Flexible Inference, Planning, and Control Abstract: Imitation Learning (IL) is an appealing approach to learn desirable\nautonomous behavior. However, directing IL to achieve arbitrary goals is\ndifficult. In contrast, planning-based algorithms use dynamics models and\nreward functions to achieve goals. Yet, reward functions that evoke desirable\nbehavior are often difficult to specify. In this paper, we propose Imitative\nModels to combine the benefits of IL and goal-directed planning. Imitative\nModels are probabilistic predictive models of desirable behavior able to plan\ninterpretable expert-like trajectories to achieve specified goals. We derive\nfamilies of flexible goal objectives, including constrained goal regions,\nunconstrained goal sets, and energy-based goals. We show that our method can\nuse these objectives to successfully direct behavior. Our method substantially\noutperforms six IL approaches and a planning-based approach in a dynamic\nsimulated autonomous driving task, and is efficiently learned from expert\ndemonstrations without online data collection. We also show our approach is\nrobust to poorly specified goals, such as goals on the wrong side of the road. \n\n"}
{"id": "1810.06665", "contents": "Title: Stop Illegal Comments: A Multi-Task Deep Learning Approach Abstract: Deep learning methods are often difficult to apply in the legal domain due to\nthe large amount of labeled data required by deep learning methods. A recent\nnew trend in the deep learning community is the application of multi-task\nmodels that enable single deep neural networks to perform more than one task at\nthe same time, for example classification and translation tasks. These powerful\nnovel models are capable of transferring knowledge among different tasks or\ntraining sets and therefore could open up the legal domain for many deep\nlearning applications. In this paper, we investigate the transfer learning\ncapabilities of such a multi-task model on a classification task on the\npublicly available Kaggle toxic comment dataset for classifying illegal\ncomments and we can report promising results. \n\n"}
{"id": "1810.06801", "contents": "Title: Quasi-hyperbolic momentum and Adam for deep learning Abstract: Momentum-based acceleration of stochastic gradient descent (SGD) is widely\nused in deep learning. We propose the quasi-hyperbolic momentum algorithm (QHM)\nas an extremely simple alteration of momentum SGD, averaging a plain SGD step\nwith a momentum step. We describe numerous connections to and identities with\nother algorithms, and we characterize the set of two-state optimization\nalgorithms that QHM can recover. Finally, we propose a QH variant of Adam\ncalled QHAdam, and we empirically demonstrate that our algorithms lead to\nsignificantly improved training in a variety of settings, including a new\nstate-of-the-art result on WMT16 EN-DE. We hope that these empirical results,\ncombined with the conceptual and practical simplicity of QHM and QHAdam, will\nspur interest from both practitioners and researchers. Code is immediately\navailable. \n\n"}
{"id": "1810.06803", "contents": "Title: Co-manifold learning with missing data Abstract: Representation learning is typically applied to only one mode of a data\nmatrix, either its rows or columns. Yet in many applications, there is an\nunderlying geometry to both the rows and the columns. We propose utilizing this\ncoupled structure to perform co-manifold learning: uncovering the underlying\ngeometry of both the rows and the columns of a given matrix, where we focus on\na missing data setting. Our unsupervised approach consists of three components.\nWe first solve a family of optimization problems to estimate a complete matrix\nat multiple scales of smoothness. We then use this collection of smooth matrix\nestimates to compute pairwise distances on the rows and columns based on a new\nmulti-scale metric that implicitly introduces a coupling between the rows and\nthe columns. Finally, we construct row and column representations from these\nmulti-scale metrics. We demonstrate that our approach outperforms competing\nmethods in both data visualization and clustering. \n\n"}
{"id": "1810.07168", "contents": "Title: An empirical evaluation of imbalanced data strategies from a\n  practitioner's point of view Abstract: This paper evaluates six strategies for mitigating imbalanced data:\noversampling, undersampling, ensemble methods, specialized algorithms, class\nweight adjustments, and a no-mitigation approach referred to as the baseline.\nThese strategies were tested on 58 real-life binary imbalanced datasets with\nimbalance rates ranging from 3 to 120. We conducted a comparative analysis of\n10 under-sampling algorithms, 5 over-sampling algorithms, 2 ensemble methods,\nand 3 specialized algorithms across eight different performance metrics:\naccuracy, area under the ROC curve (AUC), balanced accuracy, F1-measure,\nG-mean, Matthew's correlation coefficient, precision, and recall. Additionally,\nwe assessed the six strategies on altered datasets, derived from real-life\ndata, with both low (3) and high (100 or 300) imbalance ratios (IR).\n  The principal finding indicates that the effectiveness of each strategy\nsignificantly varies depending on the metric used. The paper also examines a\nselection of newer algorithms within the categories of specialized algorithms,\noversampling, and ensemble methods. The findings suggest that the current\nhierarchy of best-performing strategies for each metric is unlikely to change\nwith the introduction of newer algorithms. \n\n"}
{"id": "1810.08305", "contents": "Title: Open Vocabulary Learning on Source Code with a Graph-Structured Cache Abstract: Machine learning models that take computer program source code as input\ntypically use Natural Language Processing (NLP) techniques. However, a major\nchallenge is that code is written using an open, rapidly changing vocabulary\ndue to, e.g., the coinage of new variable and method names. Reasoning over such\na vocabulary is not something for which most NLP methods are designed. We\nintroduce a Graph-Structured Cache to address this problem; this cache contains\na node for each new word the model encounters with edges connecting each word\nto its occurrences in the code. We find that combining this graph-structured\ncache strategy with recent Graph-Neural-Network-based models for supervised\nlearning on code improves the models' performance on a code completion task and\na variable naming task --- with over $100\\%$ relative improvement on the latter\n--- at the cost of a moderate increase in computation time. \n\n"}
{"id": "1810.08732", "contents": "Title: Named Entity Recognition on Twitter for Turkish using Semi-supervised\n  Learning with Word Embeddings Abstract: Recently, due to the increasing popularity of social media, the necessity for\nextracting information from informal text types, such as microblog texts, has\ngained significant attention. In this study, we focused on the Named Entity\nRecognition (NER) problem on informal text types for Turkish. We utilized a\nsemi-supervised learning approach based on neural networks. We applied a fast\nunsupervised method for learning continuous representations of words in vector\nspace. We made use of these obtained word embeddings, together with language\nindependent features that are engineered to work better on informal text types,\nfor generating a Turkish NER system on microblog texts. We evaluated our\nTurkish NER system on Twitter messages and achieved better F-score performances\nthan the published results of previously proposed NER systems on Turkish\ntweets. Since we did not employ any language dependent features, we believe\nthat our method can be easily adapted to microblog texts in other\nmorphologically rich languages. \n\n"}
{"id": "1810.08907", "contents": "Title: Understanding the Acceleration Phenomenon via High-Resolution\n  Differential Equations Abstract: Gradient-based optimization algorithms can be studied from the perspective of\nlimiting ordinary differential equations (ODEs). Motivated by the fact that\nexisting ODEs do not distinguish between two fundamentally different\nalgorithms---Nesterov's accelerated gradient method for strongly convex\nfunctions (NAG-SC) and Polyak's heavy-ball method---we study an alternative\nlimiting process that yields high-resolution ODEs. We show that these ODEs\npermit a general Lyapunov function framework for the analysis of convergence in\nboth continuous and discrete time. We also show that these ODEs are more\naccurate surrogates for the underlying algorithms; in particular, they not only\ndistinguish between NAG-SC and Polyak's heavy-ball method, but they allow the\nidentification of a term that we refer to as \"gradient correction\" that is\npresent in NAG-SC but not in the heavy-ball method and is responsible for the\nqualitative difference in convergence of the two methods. We also use the\nhigh-resolution ODE framework to study Nesterov's accelerated gradient method\nfor (non-strongly) convex functions, uncovering a hitherto unknown\nresult---that NAG-C minimizes the squared gradient norm at an inverse cubic\nrate. Finally, by modifying the high-resolution ODE of NAG-C, we obtain a\nfamily of new optimization methods that are shown to maintain the accelerated\nconvergence rates of NAG-C for smooth convex functions. \n\n"}
{"id": "1810.09440", "contents": "Title: Deep multi-survey classification of variable stars Abstract: During the last decade, a considerable amount of effort has been made to\nclassify variable stars using different machine learning techniques. Typically,\nlight curves are represented as vectors of statistical descriptors or features\nthat are used to train various algorithms. These features demand big\ncomputational powers that can last from hours to days, making impossible to\ncreate scalable and efficient ways of automatically classifying variable stars.\nAlso, light curves from different surveys cannot be integrated and analyzed\ntogether when using features, because of observational differences. For\nexample, having variations in cadence and filters, feature distributions become\nbiased and require expensive data-calibration models. The vast amount of data\nthat will be generated soon make necessary to develop scalable machine learning\narchitectures without expensive integration techniques. Convolutional Neural\nNetworks have shown impressing results in raw image classification and\nrepresentation within the machine learning literature. In this work, we present\na novel Deep Learning model for light curve classification, mainly based on\nconvolutional units. Our architecture receives as input the differences between\ntime and magnitude of light curves. It captures the essential classification\npatterns regardless of cadence and filter. In addition, we introduce a novel\ndata augmentation schema for unevenly sampled time series. We test our method\nusing three different surveys: OGLE-III; Corot; and VVV, which differ in\nfilters, cadence, and area of the sky. We show that besides the benefit of\nscalability, our model obtains state of the art levels accuracy in light curve\nclassification benchmarks. \n\n"}
{"id": "1810.09650", "contents": "Title: One Bit Matters: Understanding Adversarial Examples as the Abuse of\n  Redundancy Abstract: Despite the great success achieved in machine learning (ML), adversarial\nexamples have caused concerns with regards to its trustworthiness: A small\nperturbation of an input results in an arbitrary failure of an otherwise\nseemingly well-trained ML model. While studies are being conducted to discover\nthe intrinsic properties of adversarial examples, such as their transferability\nand universality, there is insufficient theoretic analysis to help understand\nthe phenomenon in a way that can influence the design process of ML\nexperiments. In this paper, we deduce an information-theoretic model which\nexplains adversarial attacks as the abuse of feature redundancies in ML\nalgorithms. We prove that feature redundancy is a necessary condition for the\nexistence of adversarial examples. Our model helps to explain some major\nquestions raised in many anecdotal studies on adversarial examples. Our theory\nis backed up by empirical measurements of the information content of benign and\nadversarial examples on both image and text datasets. Our measurements show\nthat typical adversarial examples introduce just enough redundancy to overflow\nthe decision making of an ML model trained on corresponding benign examples. We\nconclude with actionable recommendations to improve the robustness of machine\nlearners against adversarial examples. \n\n"}
{"id": "1810.09746", "contents": "Title: On PAC-Bayesian Bounds for Random Forests Abstract: Existing guarantees in terms of rigorous upper bounds on the generalization\nerror for the original random forest algorithm, one of the most frequently used\nmachine learning methods, are unsatisfying. We discuss and evaluate various\nPAC-Bayesian approaches to derive such bounds. The bounds do not require\nadditional hold-out data, because the out-of-bag samples from the bagging in\nthe training process can be exploited. A random forest predicts by taking a\nmajority vote of an ensemble of decision trees. The first approach is to bound\nthe error of the vote by twice the error of the corresponding Gibbs classifier\n(classifying with a single member of the ensemble selected at random). However,\nthis approach does not take into account the effect of averaging out of errors\nof individual classifiers when taking the majority vote. This effect provides a\nsignificant boost in performance when the errors are independent or negatively\ncorrelated, but when the correlations are strong the advantage from taking the\nmajority vote is small. The second approach based on PAC-Bayesian C-bounds\ntakes dependencies between ensemble members into account, but it requires\nestimating correlations between the errors of the individual classifiers. When\nthe correlations are high or the estimation is poor, the bounds degrade. In our\nexperiments, we compute generalization bounds for random forests on various\nbenchmark data sets. Because the individual decision trees already perform\nwell, their predictions are highly correlated and the C-bounds do not lead to\nsatisfactory results. For the same reason, the bounds based on the analysis of\nGibbs classifiers are typically superior and often reasonably tight. Bounds\nbased on a validation set coming at the cost of a smaller training set gave\nbetter performance guarantees, but worse performance in most experiments. \n\n"}
{"id": "1810.09751", "contents": "Title: Analysis of Atomistic Representations Using Weighted Skip-Connections Abstract: In this work, we extend the SchNet architecture by using weighted skip\nconnections to assemble the final representation. This enables us to study the\nrelative importance of each interaction block for property prediction. We\ndemonstrate on both the QM9 and MD17 dataset that their relative weighting\ndepends strongly on the chemical composition and configurational degrees of\nfreedom of the molecules which opens the path towards a more detailed\nunderstanding of machine learning models for molecules. \n\n"}
{"id": "1810.10222", "contents": "Title: Universal Language Model Fine-Tuning with Subword Tokenization for\n  Polish Abstract: Universal Language Model for Fine-tuning [arXiv:1801.06146] (ULMFiT) is one\nof the first NLP methods for efficient inductive transfer learning.\nUnsupervised pretraining results in improvements on many NLP tasks for English.\nIn this paper, we describe a new method that uses subword tokenization to adapt\nULMFiT to languages with high inflection. Our approach results in a new\nstate-of-the-art for the Polish language, taking first place in Task 3 of\nPolEval'18. After further training, our final model outperformed the second\nbest model by 35%. We have open-sourced our pretrained models and code. \n\n"}
{"id": "1810.10325", "contents": "Title: Multi-Stage Reinforcement Learning For Object Detection Abstract: We present a reinforcement learning approach for detecting objects within an\nimage. Our approach performs a step-wise deformation of a bounding box with the\ngoal of tightly framing the object. It uses a hierarchical tree-like\nrepresentation of predefined region candidates, which the agent can zoom in on.\nThis reduces the number of region candidates that must be evaluated so that the\nagent can afford to compute new feature maps before each step to enhance\ndetection quality. We compare an approach that is based purely on zoom actions\nwith one that is extended by a second refinement stage to fine-tune the\nbounding box after each zoom step. We also improve the fitting ability by\nallowing for different aspect ratios of the bounding box. Finally, we propose\ndifferent reward functions to lead to a better guidance of the agent while\nfollowing its search trajectories. Experiments indicate that each of these\nextensions leads to more correct detections. The best performing approach\ncomprises a zoom stage and a refinement stage, uses aspect-ratio modifying\nactions and is trained using a combination of three different reward metrics. \n\n"}
{"id": "1810.10329", "contents": "Title: Vehicle classification using ResNets, localisation and\n  spatially-weighted pooling Abstract: We investigate whether ResNet architectures can outperform more traditional\nConvolutional Neural Networks on the task of fine-grained vehicle\nclassification. We train and test ResNet-18, ResNet-34 and ResNet-50 on the\nComprehensive Cars dataset without pre-training on other datasets. We then\nmodify the networks to use Spatially Weighted Pooling. Finally, we add a\nlocalisation step before the classification process, using a network based on\nResNet-50. We find that using Spatially Weighted Pooling and localisation both\nimprove classification accuracy of ResNet50. Spatially Weighted Pooling\nincreases accuracy by 1.5 percent points and localisation increases accuracy by\n3.4 percent points. Using both increases accuracy by 3.7 percent points giving\na top-1 accuracy of 96.351\\% on the Comprehensive Cars dataset. Our method\nachieves higher accuracy than a range of methods including those that use\ntraditional CNNs. However, our method does not perform quite as well as\npre-trained networks that use Spatially Weighted Pooling. \n\n"}
{"id": "1810.10333", "contents": "Title: Memorization in Overparameterized Autoencoders Abstract: The ability of deep neural networks to generalize well in the\noverparameterized regime has become a subject of significant research interest.\nWe show that overparameterized autoencoders exhibit memorization, a form of\ninductive bias that constrains the functions learned through the optimization\nprocess to concentrate around the training examples, although the network could\nin principle represent a much larger function class. In particular, we prove\nthat single-layer fully-connected autoencoders project data onto the\n(nonlinear) span of the training examples. In addition, we show that deep\nfully-connected autoencoders learn a map that is locally contractive at the\ntraining examples, and hence iterating the autoencoder results in convergence\nto the training examples. Finally, we prove that depth is necessary and provide\nempirical evidence that it is also sufficient for memorization in convolutional\nautoencoders. Understanding this inductive bias may shed light on the\ngeneralization properties of overparametrized deep neural networks that are\ncurrently unexplained by classical statistical theory. \n\n"}
{"id": "1810.10358", "contents": "Title: Implicit Modeling with Uncertainty Estimation for Intravoxel Incoherent\n  Motion Imaging Abstract: Intravoxel incoherent motion (IVIM) imaging allows contrast-agent free in\nvivo perfusion quantification with magnetic resonance imaging (MRI). However,\nits use is limited by typically low accuracy due to low signal-to-noise ratio\n(SNR) at large gradient encoding magnitudes as well as dephasing artefacts\ncaused by subject motion, which is particularly challenging in fetal MRI. To\nmitigate this problem, we propose an implicit IVIM signal acquisition model\nwith which we learn full posterior distribution of perfusion parameters using\nartificial neural networks. This posterior then encapsulates the uncertainty of\nthe inferred parameter estimates, which we validate herein via numerical\nexperiments with rejection-based Bayesian sampling. Compared to\nstate-of-the-art IVIM estimation method of segmented least-squares fitting, our\nproposed approach improves parameter estimation accuracy by 65% on synthetic\nanisotropic perfusion data. On paired rescans of in vivo fetal MRI, our method\nincreases repeatability of parameter estimation in placenta by 46%. \n\n"}
{"id": "1810.10627", "contents": "Title: Streaming Graph Neural Networks Abstract: Graphs are essential representations of many real-world data such as social\nnetworks. Recent years have witnessed the increasing efforts made to extend the\nneural network models to graph-structured data. These methods, which are\nusually known as the graph neural networks, have been applied to advance many\ngraphs related tasks such as reasoning dynamics of the physical system, graph\nclassification, and node classification. Most of the existing graph neural\nnetwork models have been designed for static graphs, while many real-world\ngraphs are inherently dynamic. For example, social networks are naturally\nevolving as new users joining and new relations being created. Current graph\nneural network models cannot utilize the dynamic information in dynamic graphs.\nHowever, the dynamic information has been proven to enhance the performance of\nmany graph analytic tasks such as community detection and link prediction.\nHence, it is necessary to design dedicated graph neural networks for dynamic\ngraphs. In this paper, we propose DGNN, a new {\\bf D}ynamic {\\bf G}raph {\\bf\nN}eural {\\bf N}etwork model, which can model the dynamic information as the\ngraph evolving. In particular, the proposed framework can keep updating node\ninformation by capturing the sequential information of edges (interactions),\nthe time intervals between edges and information propagation coherently.\nExperimental results on various dynamic graphs demonstrate the effectiveness of\nthe proposed framework. \n\n"}
{"id": "1810.11066", "contents": "Title: Automating Generation of Low Precision Deep Learning Operators Abstract: State of the art deep learning models have made steady progress in the fields\nof computer vision and natural language processing, at the expense of growing\nmodel sizes and computational complexity. Deploying these models on low power\nand mobile devices poses a challenge due to their limited compute capabilities\nand strict energy budgets. One solution that has generated significant research\ninterest is deploying highly quantized models that operate on low precision\ninputs and weights less than eight bits, trading off accuracy for performance.\nThese models have a significantly reduced memory footprint (up to 32x\nreduction) and can replace multiply-accumulates with bitwise operations during\ncompute intensive convolution and fully connected layers.\n  Most deep learning frameworks rely on highly engineered linear algebra\nlibraries such as ATLAS or Intel's MKL to implement efficient deep learning\noperators. To date, none of the popular deep learning directly support low\nprecision operators, partly due to a lack of optimized low precision libraries.\nIn this paper we introduce a work flow to quickly generate high performance low\nprecision deep learning operators for arbitrary precision that target multiple\nCPU architectures and include optimizations such as memory tiling and\nvectorization. We present an extensive case study on low power ARM Cortex-A53\nCPU, and show how we can generate 1-bit, 2-bit convolutions with speedups up to\n16x over an optimized 16-bit integer baseline and 2.3x better than handwritten\nimplementations. \n\n"}
{"id": "1810.11730", "contents": "Title: Low-shot Learning via Covariance-Preserving Adversarial Augmentation\n  Networks Abstract: Deep neural networks suffer from over-fitting and catastrophic forgetting\nwhen trained with small data. One natural remedy for this problem is data\naugmentation, which has been recently shown to be effective. However, previous\nworks either assume that intra-class variances can always be generalized to new\nclasses, or employ naive generation methods to hallucinate finite examples\nwithout modeling their latent distributions. In this work, we propose\nCovariance-Preserving Adversarial Augmentation Networks to overcome existing\nlimits of low-shot learning. Specifically, a novel Generative Adversarial\nNetwork is designed to model the latent distribution of each novel class given\nits related base counterparts. Since direct estimation of novel classes can be\ninductively biased, we explicitly preserve covariance information as the\n`variability' of base examples during the generation process. Empirical results\nshow that our model can generate realistic yet diverse examples, leading to\nsubstantial improvements on the ImageNet benchmark over the state of the art. \n\n"}
{"id": "1810.11908", "contents": "Title: Mean-field theory of graph neural networks in graph partitioning Abstract: A theoretical performance analysis of the graph neural network (GNN) is\npresented. For classification tasks, the neural network approach has the\nadvantage in terms of flexibility that it can be employed in a data-driven\nmanner, whereas Bayesian inference requires the assumption of a specific model.\nA fundamental question is then whether GNN has a high accuracy in addition to\nthis flexibility. Moreover, whether the achieved performance is predominately a\nresult of the backpropagation or the architecture itself is a matter of\nconsiderable interest. To gain a better insight into these questions, a\nmean-field theory of a minimal GNN architecture is developed for the graph\npartitioning problem. This demonstrates a good agreement with numerical\nexperiments. \n\n"}
{"id": "1810.12770", "contents": "Title: Explicit Feedbacks Meet with Implicit Feedbacks : A Combined Approach\n  for Recommendation System Abstract: Recommender systems recommend items more accurately by analyzing users'\npotential interest on different brands' items. In conjunction with users'\nrating similarity, the presence of users' implicit feedbacks like clicking\nitems, viewing items specifications, watching videos etc. have been proved to\nbe helpful for learning users' embedding, that helps better rating prediction\nof users. Most existing recommender systems focus on modeling of ratings and\nimplicit feedbacks ignoring users' explicit feedbacks. Explicit feedbacks can\nbe used to validate the reliability of the particular users and can be used to\nlearn about the users' characteristic. Users' characteristic mean what type of\nreviewers they are. In this paper, we explore three different models for\nrecommendation with more accuracy focusing on users' explicit feedbacks and\nimplicit feedbacks. First one is RHC-PMF that predicts users' rating more\naccurately based on user's three explicit feedbacks (rating, helpfulness score\nand centrality) and second one is RV-PMF, where user's implicit feedback (view\nrelationship) is considered. Last one is RHCV-PMF, where both type of feedbacks\nare considered. In this model users' explicit feedbacks' similarity indicate\nthe similarity of their reliability and characteristic and implicit feedback's\nsimilarity indicates their preference similarity. Extensive experiments on real\nworld dataset, i.e. Amazon.com online review dataset shows that our models\nperform better compare to base-line models in term of users' rating prediction.\nRHCV-PMF model also performs better rating prediction compare to baseline\nmodels for cold start users and cold start items. \n\n"}
{"id": "1810.12997", "contents": "Title: An Online-Learning Approach to Inverse Optimization Abstract: In this paper, we demonstrate how to learn the objective function of a\ndecision-maker while only observing the problem input data and the\ndecision-maker's corresponding decisions over multiple rounds. We present exact\nalgorithms for this online version of inverse optimization which converge at a\nrate of $ \\mathcal{O}(1/\\sqrt{T}) $ in the number of observations~$T$ and\ncompare their further properties. Especially, they all allow taking decisions\nwhich are essentially as good as those of the observed decision-maker already\nafter relatively few iterations, but are suited best for different settings\neach. Our approach is based on online learning and works for linear objectives\nover arbitrary feasible sets for which we have a linear optimization oracle. As\nsuch, it generalizes previous approaches based on KKT-system decomposition and\ndualization. We also introduce several generalizations, such as the approximate\nlearning of non-linear objective functions, dynamically changing as well as\nparameterized objectives and the case of suboptimal observed decisions. When\napplied to the stochastic offline case, our algorithms are able to give\nguarantees on the quality of the learned objectives in expectation. Finally, we\nshow the effectiveness and possible applications of our methods in indicative\ncomputational experiments. \n\n"}
{"id": "1810.13098", "contents": "Title: Low-Rank Embedding of Kernels in Convolutional Neural Networks under\n  Random Shuffling Abstract: Although the convolutional neural networks (CNNs) have become popular for\nvarious image processing and computer vision task recently, it remains a\nchallenging problem to reduce the storage cost of the parameters for\nresource-limited platforms. In the previous studies, tensor decomposition (TD)\nhas achieved promising compression performance by embedding the kernel of a\nconvolutional layer into a low-rank subspace. However the employment of TD is\nnaively on the kernel or its specified variants. Unlike the conventional\napproaches, this paper shows that the kernel can be embedded into more general\nor even random low-rank subspaces. We demonstrate this by compressing the\nconvolutional layers via randomly-shuffled tensor decomposition (RsTD) for a\nstandard classification task using CIFAR-10. In addition, we analyze how the\nspatial similarity of the training data influences the low-rank structure of\nthe kernels. The experimental results show that the CNN can be significantly\ncompressed even if the kernels are randomly shuffled. Furthermore, the\nRsTD-based method yields more stable classification accuracy than the\nconventional TD-based methods in a large range of compression ratios. \n\n"}
{"id": "1810.13118", "contents": "Title: SplineNets: Continuous Neural Decision Graphs Abstract: We present SplineNets, a practical and novel approach for using conditioning\nin convolutional neural networks (CNNs). SplineNets are continuous\ngeneralizations of neural decision graphs, and they can dramatically reduce\nruntime complexity and computation costs of CNNs, while maintaining or even\nincreasing accuracy. Functions of SplineNets are both dynamic (i.e.,\nconditioned on the input) and hierarchical (i.e., conditioned on the\ncomputational path). SplineNets employ a unified loss function with a desired\nlevel of smoothness over both the network and decision parameters, while\nallowing for sparse activation of a subset of nodes for individual samples. In\nparticular, we embed infinitely many function weights (e.g. filters) on smooth,\nlow dimensional manifolds parameterized by compact B-splines, which are indexed\nby a position parameter. Instead of sampling from a categorical distribution to\npick a branch, samples choose a continuous position to pick a function weight.\nWe further show that by maximizing the mutual information between spline\npositions and class labels, the network can be optimally utilized and\nspecialized for classification tasks. Experiments show that our approach can\nsignificantly increase the accuracy of ResNets with negligible cost in speed,\nmatching the precision of a 110 level ResNet with a 32 level SplineNet. \n\n"}
{"id": "1810.13373", "contents": "Title: Analyzing biological and artificial neural networks: challenges with\n  opportunities for synergy? Abstract: Deep neural networks (DNNs) transform stimuli across multiple processing\nstages to produce representations that can be used to solve complex tasks, such\nas object recognition in images. However, a full understanding of how they\nachieve this remains elusive. The complexity of biological neural networks\nsubstantially exceeds the complexity of DNNs, making it even more challenging\nto understand the representations that they learn. Thus, both machine learning\nand computational neuroscience are faced with a shared challenge: how can we\nanalyze their representations in order to understand how they solve complex\ntasks?\n  We review how data-analysis concepts and techniques developed by\ncomputational neuroscientists can be useful for analyzing representations in\nDNNs, and in turn, how recently developed techniques for analysis of DNNs can\nbe useful for understanding representations in biological neural networks. We\nexplore opportunities for synergy between the two fields, such as the use of\nDNNs as in-silico model systems for neuroscience, and how this synergy can lead\nto new hypotheses about the operating principles of biological neural networks. \n\n"}
{"id": "1811.00112", "contents": "Title: Generating Photo-Realistic Training Data to Improve Face Recognition\n  Accuracy Abstract: In this paper we investigate the feasibility of using synthetic data to\naugment face datasets. In particular, we propose a novel generative adversarial\nnetwork (GAN) that can disentangle identity-related attributes from\nnon-identity-related attributes. This is done by training an embedding network\nthat maps discrete identity labels to an identity latent space that follows a\nsimple prior distribution, and training a GAN conditioned on samples from that\ndistribution. Our proposed GAN allows us to augment face datasets by generating\nboth synthetic images of subjects in the training set and synthetic images of\nnew subjects not in the training set. By using recent advances in GAN training,\nwe show that the synthetic images generated by our model are photo-realistic,\nand that training with augmented datasets can indeed increase the accuracy of\nface recognition models as compared with models trained with real images alone. \n\n"}
{"id": "1811.00596", "contents": "Title: Variational Dropout via Empirical Bayes Abstract: We study the Automatic Relevance Determination procedure applied to deep\nneural networks. We show that ARD applied to Bayesian DNNs with Gaussian\napproximate posterior distributions leads to a variational bound similar to\nthat of variational dropout, and in the case of a fixed dropout rate,\nobjectives are exactly the same. Experimental results show that the two\napproaches yield comparable results in practice even when the dropout rates are\ntrained. This leads to an alternative Bayesian interpretation of dropout and\nmitigates some of the theoretical issues that arise with the use of improper\npriors in the variational dropout model. Additionally, we explore the use of\nthe hierarchical priors in ARD and show that it helps achieve higher sparsity\nfor the same accuracy. \n\n"}
{"id": "1811.00648", "contents": "Title: Prediction Error Meta Classification in Semantic Segmentation: Detection\n  via Aggregated Dispersion Measures of Softmax Probabilities Abstract: We present a method that \"meta\" classifies whether seg-ments predicted by a\nsemantic segmentation neural networkintersect with the ground truth. For this\npurpose, we employ measures of dispersion for predicted pixel-wise class\nprobability distributions, like classification entropy, that yield heat maps of\nthe input scene's size. We aggregate these dispersion measures segment-wise and\nderive metrics that are well-correlated with the segment-wise IoU of prediction\nand ground truth. This procedure yields an almost plug and play post-processing\ntool to rate the prediction quality of semantic segmentation networks on\nsegment level. This is especially relevant for monitoring neural networks in\nonline applications like automated driving or medical imaging where reliability\nis of utmost importance. In our tests, we use publicly available\nstate-of-the-art networks trained on the Cityscapes dataset and the BraTS2017\ndataset and analyze the predictive power of different metrics as well as\ndifferent sets of metrics. To this end, we compute logistic LASSO regression\nfits for the task of classifying IoU=0 vs. IoU>0 per segment and obtain AUROC\nvalues of up to 91.55%. We complement these tests with linear regression fits\nto predict the segment-wise IoU and obtain prediction standard deviations of\ndown to 0.130 as well as $R^2$ values of up to 84.15%. We show that these\nresults clearly outperform standard approaches. \n\n"}
{"id": "1811.01054", "contents": "Title: Minimax Estimation of Neural Net Distance Abstract: An important class of distance metrics proposed for training generative\nadversarial networks (GANs) is the integral probability metric (IPM), in which\nthe neural net distance captures the practical GAN training via two neural\nnetworks. This paper investigates the minimax estimation problem of the neural\nnet distance based on samples drawn from the distributions. We develop the\nfirst known minimax lower bound on the estimation error of the neural net\ndistance, and an upper bound tighter than an existing bound on the estimator\nerror for the empirical neural net distance. Our lower and upper bounds match\nnot only in the order of the sample size but also in terms of the norm of the\nparameter matrices of neural networks, which justifies the empirical neural net\ndistance as a good approximation of the true neural net distance for training\nGANs in practice. \n\n"}
{"id": "1811.01135", "contents": "Title: Content preserving text generation with attribute controls Abstract: In this work, we address the problem of modifying textual attributes of\nsentences. Given an input sentence and a set of attribute labels, we attempt to\ngenerate sentences that are compatible with the conditioning information. To\nensure that the model generates content compatible sentences, we introduce a\nreconstruction loss which interpolates between auto-encoding and\nback-translation loss components. We propose an adversarial loss to enforce\ngenerated samples to be attribute compatible and realistic. Through\nquantitative, qualitative and human evaluations we demonstrate that our model\nis capable of generating fluent sentences that better reflect the conditioning\ninformation compared to prior methods. We further demonstrate that the model is\ncapable of simultaneously controlling multiple attributes. \n\n"}
{"id": "1811.01249", "contents": "Title: Dynamic Feature Acquisition Using Denoising Autoencoders Abstract: In real-world scenarios, different features have different acquisition costs\nat test-time which necessitates cost-aware methods to optimize the cost and\nperformance trade-off. This paper introduces a novel and scalable approach for\ncost-aware feature acquisition at test-time. The method incrementally asks for\nfeatures based on the available context that are known feature values. The\nproposed method is based on sensitivity analysis in neural networks and density\nestimation using denoising autoencoders with binary representation layers. In\nthe proposed architecture, a denoising autoencoder is used to handle unknown\nfeatures (i.e., features that are yet to be acquired), and the sensitivity of\npredictions with respect to each unknown feature is used as a context-dependent\nmeasure of informativeness. We evaluated the proposed method on eight different\nreal-world datasets as well as one synthesized dataset and compared its\nperformance with several other approaches in the literature. According to the\nresults, the suggested method is capable of efficiently acquiring features at\ntest-time in a cost- and context-aware fashion. \n\n"}
{"id": "1811.01506", "contents": "Title: Theoretical and Experimental Analysis on the Generalizability of\n  Distribution Regression Network Abstract: There is emerging interest in performing regression between distributions. In\ncontrast to prediction on single instances, these machine learning methods can\nbe useful for population-based studies or on problems that are inherently\nstatistical in nature. The recently proposed distribution regression network\n(DRN) has shown superior performance for the distribution-to-distribution\nregression task compared to conventional neural networks. However, in Kou et\nal. (2018) and some other works on distribution regression, there is a lack of\ncomprehensive comparative study on both theoretical basis and generalization\nabilities of the methods. We derive some mathematical properties of DRN and\nqualitatively compare it to conventional neural networks. We also perform\ncomprehensive experiments to study the generalizability of distribution\nregression models, by studying their robustness to limited training data, data\nsampling noise and task difficulty. DRN consistently outperforms conventional\nneural networks, requiring fewer training data and maintaining robust\nperformance with noise. Furthermore, the theoretical properties of DRN can be\nused to provide some explanation on the ability of DRN to achieve better\ngeneralization performance than conventional neural networks. \n\n"}
{"id": "1811.01558", "contents": "Title: Stochastic Modified Equations and Dynamics of Stochastic Gradient\n  Algorithms I: Mathematical Foundations Abstract: We develop the mathematical foundations of the stochastic modified equations\n(SME) framework for analyzing the dynamics of stochastic gradient algorithms,\nwhere the latter is approximated by a class of stochastic differential\nequations with small noise parameters. We prove that this approximation can be\nunderstood mathematically as an weak approximation, which leads to a number of\nprecise and useful results on the approximations of stochastic gradient descent\n(SGD), momentum SGD and stochastic Nesterov's accelerated gradient method in\nthe general setting of stochastic objectives. We also demonstrate through\nexplicit calculations that this continuous-time approach can uncover important\nanalytical insights into the stochastic gradient algorithms under consideration\nthat may not be easy to obtain in a purely discrete-time setting. \n\n"}
{"id": "1811.01686", "contents": "Title: GEMRank: Global Entity Embedding For Collaborative Filtering Abstract: Recently, word embedding algorithms have been applied to map the entities of\nrecommender systems, such as users and items, to new feature spaces using\ntextual element-context relations among them. Unlike many other domains, this\napproach has not achieved a desired performance in collaborative filtering\nproblems, probably due to unavailability of appropriate textual data. In this\npaper we propose a new recommendation framework, called GEMRank that can be\napplied when the user-item matrix is the sole available souce of information.\nIt uses the concept of profile co-occurrence for defining relations among\nentities and applies a factorization method for embedding the users and items.\nGEMRank then feeds the extracted representations to a neural network model to\npredict user-item like/dislike relations which the final recommendations are\nmade based on. We evaluated GEMRank in an extensive set of experiments against\nstate of the art recommendation methods. The results show that GEMRank\nsignificantly outperforms the baseline algorithms in a variety of data sets\nwith different degrees of density. \n\n"}
{"id": "1811.01715", "contents": "Title: Multi-armed Bandits with Compensation Abstract: We propose and study the known-compensation multi-arm bandit (KCMAB) problem,\nwhere a system controller offers a set of arms to many short-term players for\n$T$ steps. In each step, one short-term player arrives to the system. Upon\narrival, the player aims to select an arm with the current best average reward\nand receives a stochastic reward associated with the arm. In order to\nincentivize players to explore other arms, the controller provides a proper\npayment compensation to players. The objective of the controller is to maximize\nthe total reward collected by players while minimizing the compensation. We\nfirst provide a compensation lower bound $\\Theta(\\sum_i {\\Delta_i\\log T\\over\nKL_i})$, where $\\Delta_i$ and $KL_i$ are the expected reward gap and\nKullback-Leibler (KL) divergence between distributions of arm $i$ and the best\narm, respectively. We then analyze three algorithms to solve the KCMAB problem,\nand obtain their regrets and compensations. We show that the algorithms all\nachieve $O(\\log T)$ regret and $O(\\log T)$ compensation that match the\ntheoretical lower bound. Finally, we present experimental results to\ndemonstrate the performance of the algorithms. \n\n"}
{"id": "1811.02067", "contents": "Title: Sample Compression, Support Vectors, and Generalization in Deep Learning Abstract: Even though Deep Neural Networks (DNNs) are widely celebrated for their\npractical performance, they possess many intriguing properties related to depth\nthat are difficult to explain both theoretically and intuitively. Understanding\nhow weights in deep networks coordinate together across layers to form useful\nlearners has proven challenging, in part because the repeated composition of\nnonlinearities has proved intractable. This paper presents a reparameterization\nof DNNs as a linear function of a feature map that is locally independent of\nthe weights. This feature map transforms depth-dependencies into simple tensor\nproducts and maps each input to a discrete subset of the feature space. Then,\nusing a max-margin assumption, the paper develops a sample compression\nrepresentation of the neural network in terms of the discrete activation state\nof neurons induced by s ``support vectors\". The paper shows that the number of\nsupport vectors s relates with learning guarantees for neural networks through\nsample compression bounds, yielding a sample complexity of O(ns/epsilon) for\nnetworks with n neurons. Finally, the number of support vectors s is found to\nmonotonically increase with width and label noise but decrease with depth. \n\n"}
{"id": "1811.02228", "contents": "Title: Kernel Exponential Family Estimation via Doubly Dual Embedding Abstract: We investigate penalized maximum log-likelihood estimation for exponential\nfamily distributions whose natural parameter resides in a reproducing kernel\nHilbert space. Key to our approach is a novel technique, doubly dual embedding,\nthat avoids computation of the partition function. This technique also allows\nthe development of a flexible sampling strategy that amortizes the cost of\nMonte-Carlo sampling in the inference stage. The resulting estimator can be\neasily generalized to kernel conditional exponential families. We establish a\nconnection between kernel exponential family estimation and MMD-GANs, revealing\na new perspective for understanding GANs. Compared to the score matching based\nestimators, the proposed method improves both memory and time efficiency while\nenjoying stronger statistical properties, such as fully capturing smoothness in\nits statistical convergence rate while the score matching estimator appears to\nsaturate. Finally, we show that the proposed estimator empirically outperforms\nstate-of-the-art \n\n"}
{"id": "1811.02384", "contents": "Title: Robust Bhattacharyya bound linear discriminant analysis through adaptive\n  algorithm Abstract: In this paper, we propose a novel linear discriminant analysis criterion via\nthe Bhattacharyya error bound estimation based on a novel L1-norm (L1BLDA) and\nL2-norm (L2BLDA). Both L1BLDA and L2BLDA maximize the between-class scatters\nwhich are measured by the weighted pairwise distances of class means and\nmeanwhile minimize the within-class scatters under the L1-norm and L2-norm,\nrespectively. The proposed models can avoid the small sample size (SSS) problem\nand have no rank limit that may encounter in LDA. It is worth mentioning that,\nthe employment of L1-norm gives a robust performance of L1BLDA, and L1BLDA is\nsolved through an effective non-greedy alternating direction method of\nmultipliers (ADMM), where all the projection vectors can be obtained once for\nall. In addition, the weighting constants of L1BLDA and L2BLDA between the\nbetween-class and within-class terms are determined by the involved data set,\nwhich makes our L1BLDA and L2BLDA adaptive. The experimental results on both\nbenchmark data sets as well as the handwritten digit databases demonstrate the\neffectiveness of the proposed methods. \n\n"}
{"id": "1811.02471", "contents": "Title: Convolutional LSTMs for Cloud-Robust Segmentation of Remote Sensing\n  Imagery Abstract: Clouds frequently cover the Earth's surface and pose an omnipresent challenge\nto optical Earth observation methods. The vast majority of remote sensing\napproaches either selectively choose single cloud-free observations or employ a\npre-classification strategy to identify and mask cloudy pixels. We follow a\ndifferent strategy and treat cloud coverage as noise that is inherent to the\nobserved satellite data. In prior work, we directly employed a straightforward\n\\emph{convolutional long short-term memory} network for vegetation\nclassification without explicit cloud filtering and achieved state-of-the-art\nclassification accuracies. In this work, we investigate this cloud-robustness\nfurther by visualizing internal cell activations and performing an ablation\nexperiment on datasets of different cloud coverage. In the visualizations of\nnetwork states, we identified some cells in which modulation and input gates\nclosed on cloudy pixels. This indicates that the network has internalized a\ncloud-filtering mechanism without being specifically trained on cloud labels.\nOverall, our results question the necessity of sophisticated pre-processing\npipelines for multi-temporal deep learning approaches. \n\n"}
{"id": "1811.02628", "contents": "Title: Learning Bone Suppression from Dual Energy Chest X-rays using\n  Adversarial Networks Abstract: Suppressing bones on chest X-rays such as ribs and clavicle is often expected\nto improve pathologies classification. These bones can interfere with a broad\nrange of diagnostic tasks on pulmonary disease except for musculoskeletal\nsystem. Current conventional method for acquisition of bone suppressed X-rays\nis dual energy imaging, which captures two radiographs at a very short interval\nwith different energy levels; however, the patient is exposed to radiation\ntwice and the artifacts arise due to heartbeats between two shots. In this\npaper, we introduce a deep generative model trained to predict bone suppressed\nimages on single energy chest X-rays, analyzing a finite set of previously\nacquired dual energy chest X-rays. Since the relatively small amount of data is\navailable, such approach relies on the methodology maximizing the data\nutilization. Here we integrate the following two approaches. First, we use a\nconditional generative adversarial network that complements the traditional\nregression method minimizing the pairwise image difference. Second, we use Haar\n2D wavelet decomposition to offer a perceptual guideline in frequency details\nto allow the model to converge quickly and efficiently. As a result, we achieve\nstate-of-the-art performance on bone suppression as compared to the existing\napproaches with dual energy chest X-rays. \n\n"}
{"id": "1811.02642", "contents": "Title: Computational Histological Staining and Destaining of Prostate Core\n  Biopsy RGB Images with Generative Adversarial Neural Networks Abstract: Histopathology tissue samples are widely available in two states:\nparaffin-embedded unstained and non-paraffin-embedded stained whole slide RGB\nimages (WSRI). Hematoxylin and eosin stain (H&E) is one of the principal stains\nin histology but suffers from several shortcomings related to tissue\npreparation, staining protocols, slowness and human error. We report two novel\napproaches for training machine learning models for the computational H&E\nstaining and destaining of prostate core biopsy RGB images. The staining model\nuses a conditional generative adversarial network that learns hierarchical\nnon-linear mappings between whole slide RGB image (WSRI) pairs of prostate core\nbiopsy before and after H&E staining. The trained staining model can then\ngenerate computationally H&E-stained prostate core WSRIs using previously\nunseen non-stained biopsy images as input. The destaining model, by learning\nmappings between an H&E stained WSRI and a non-stained WSRI of the same biopsy,\ncan computationally destain previously unseen H&E-stained images. Structural\nand anatomical details of prostate tissue and colors, shapes, geometries,\nlocations of nuclei, stroma, vessels, glands and other cellular components were\ngenerated by both models with structural similarity indices of 0.68 (staining)\nand 0.84 (destaining). The proposed staining and destaining models can engender\ncomputational H&E staining and destaining of WSRI biopsies without additional\nequipment and devices. \n\n"}
{"id": "1811.02656", "contents": "Title: Quaternion Convolutional Neural Networks for Heterogeneous Image\n  Processing Abstract: Convolutional neural networks (CNN) have recently achieved state-of-the-art\nresults in various applications. In the case of image recognition, an ideal\nmodel has to learn independently of the training data, both local dependencies\nbetween the three components (R,G,B) of a pixel, and the global relations\ndescribing edges or shapes, making it efficient with small or heterogeneous\ndatasets. Quaternion-valued convolutional neural networks (QCNN) solved this\nproblematic by introducing multidimensional algebra to CNN. This paper proposes\nto explore the fundamental reason of the success of QCNN over CNN, by\ninvestigating the impact of the Hamilton product on a color image\nreconstruction task performed from a gray-scale only training. By learning\nindependently both internal and external relations and with less parameters\nthan real valued convolutional encoder-decoder (CAE), quaternion convolutional\nencoder-decoders (QCAE) perfectly reconstructed unseen color images while CAE\nproduced worst and gray-scale versions. \n\n"}
{"id": "1811.02659", "contents": "Title: Machine Learning Algorithms for Classification of Microcirculation\n  Images from Septic and Non-Septic Patients Abstract: Sepsis is a life-threatening disease and one of the major causes of death in\nhospitals. Imaging of microcirculatory dysfunction is a promising approach for\nautomated diagnosis of sepsis. We report a machine learning classifier capable\nof distinguishing non-septic and septic images from dark field microcirculation\nvideos of patients. The classifier achieves an accuracy of 89.45%. The area\nunder the receiver operating characteristics of the classifier was 0.92, the\nprecision was 0.92 and the recall was 0.84. Codes representing the learned\nfeature space of trained classifier were visualized using t-SNE embedding and\nwere separable and distinguished between images from critically ill and\nnon-septic patients. Using an unsupervised convolutional autoencoder,\nindependent of the clinical diagnosis, we also report clustering of learned\nfeatures from a compressed representation associated with healthy images and\nthose with microcirculatory dysfunction. The feature space used by our trained\nclassifier to distinguish between images from septic and non-septic patients\nhas potential diagnostic application. \n\n"}
{"id": "1811.02661", "contents": "Title: MAMMO: A Deep Learning Solution for Facilitating Radiologist-Machine\n  Collaboration in Breast Cancer Diagnosis Abstract: With an aging and growing population, the number of women requiring either\nscreening or symptomatic mammograms is increasing. To reduce the number of\nmammograms that need to be read by a radiologist while keeping the diagnostic\naccuracy the same or better than current clinical practice, we develop Man and\nMachine Mammography Oracle (MAMMO) - a clinical decision support system capable\nof triaging mammograms into those that can be confidently classified by a\nmachine and those that cannot be, thus requiring the reading of a radiologist.\nThe first component of MAMMO is a novel multi-view convolutional neural network\n(CNN) with multi-task learning (MTL). MTL enables the CNN to learn the\nradiological assessments known to be associated with cancer, such as breast\ndensity, conspicuity, suspicion, etc., in addition to learning the primary task\nof cancer diagnosis. We show that MTL has two advantages: 1) learning refined\nfeature representations associated with cancer improves the classification\nperformance of the diagnosis task and 2) issuing radiological assessments\nprovides an additional layer of model interpretability that a radiologist can\nuse to debug and scrutinize the diagnoses provided by the CNN. The second\ncomponent of MAMMO is a triage network, which takes as input the radiological\nassessment and diagnostic predictions of the first network's MTL outputs and\ndetermines which mammograms can be correctly and confidently diagnosed by the\nCNN and which mammograms cannot, thus needing to be read by a radiologist.\nResults obtained on a private dataset of 8,162 patients show that MAMMO reduced\nthe number of radiologist readings by 42.8% while improving the overall\ndiagnostic accuracy in comparison to readings done by radiologists alone. We\nanalyze the triage of patients decided by MAMMO to gain a better understanding\nof what unique mammogram characteristics require radiologists' expertise. \n\n"}
{"id": "1811.02662", "contents": "Title: Similarity Learning with Higher-Order Graph Convolutions for Brain\n  Network Analysis Abstract: Learning a similarity metric has gained much attention recently, where the\ngoal is to learn a function that maps input patterns to a target space while\npreserving the semantic distance in the input space. While most related work\nfocused on images, we focus instead on learning a similarity metric for\nneuroimages, such as fMRI and DTI images. We propose an end-to-end similarity\nlearning framework called Higher-order Siamese GCN for multi-subject fMRI data\nanalysis. The proposed framework learns the brain network representations via a\nsupervised metric-based approach with siamese neural networks using two graph\nconvolutional networks as the twin networks. Our proposed framework performs\nhigher-order convolutions by incorporating higher-order proximity in graph\nconvolutional networks to characterize and learn the community structure in\nbrain connectivity networks. To the best of our knowledge, this is the first\ncommunity-preserving similarity learning framework for multi-subject brain\nnetwork analysis. Experimental results on four real fMRI datasets demonstrate\nthe potential use cases of the proposed framework for multi-subject brain\nanalysis in health and neuropsychiatric disorders. Our proposed approach\nachieves an average AUC gain of 75% compared to PCA, an average AUC gain of\n65.5% compared to Spectral Embedding, and an average AUC gain of 24.3% compared\nto S-GCN across the four datasets, indicating promising application in clinical\ninvestigation and brain disease diagnosis. \n\n"}
{"id": "1811.02667", "contents": "Title: Band Selection from Hyperspectral Images Using Attention-based\n  Convolutional Neural Networks Abstract: This paper introduces new attention-based convolutional neural networks for\nselecting bands from hyperspectral images. The proposed approach re-uses\nconvolutional activations at different depths, identifying the most informative\nregions of the spectrum with the help of gating mechanisms. Our attention\ntechniques are modular and easy to implement, and they can be seamlessly\ntrained end-to-end using gradient descent. Our rigorous experiments showed that\ndeep models equipped with the attention mechanism deliver high-quality\nclassification, and repeatedly identify significant bands in the training data,\npermitting the creation of refined and extremely compact sets that retain the\nmost meaningful features. \n\n"}
{"id": "1811.02756", "contents": "Title: Bayesian State Estimation for Unobservable Distribution Systems via Deep\n  Learning Abstract: The problem of state estimation for unobservable distribution systems is\nconsidered. A deep learning approach to Bayesian state estimation is proposed\nfor real-time applications. The proposed technique consists of distribution\nlearning of stochastic power injection, a Monte Carlo technique for the\ntraining of a deep neural network for state estimation, and a Bayesian bad-data\ndetection and filtering algorithm. Structural characteristics of the deep\nneural networks are investigated. Simulations illustrate the accuracy of\nBayesian state estimation for unobservable systems and demonstrate the benefit\nof employing a deep neural network. Numerical results show the robustness of\nBayesian state estimation against modeling and estimation errors and the\npresence of bad and missing data. Comparing with pseudo-measurement techniques,\ndirect Bayesian state estimation via deep learning neural network outperforms\nexisting benchmarks. \n\n"}
{"id": "1811.02783", "contents": "Title: YASENN: Explaining Neural Networks via Partitioning Activation Sequences Abstract: We introduce a novel approach to feed-forward neural network interpretation\nbased on partitioning the space of sequences of neuron activations. In line\nwith this approach, we propose a model-specific interpretation method, called\nYASENN. Our method inherits many advantages of model-agnostic distillation,\nsuch as an ability to focus on the particular input region and to express an\nexplanation in terms of features different from those observed by a neural\nnetwork. Moreover, examination of distillation error makes the method\napplicable to the problems with low tolerance to interpretation mistakes.\nTechnically, YASENN distills the network with an ensemble of layer-wise\ngradient boosting decision trees and encodes the sequences of neuron\nactivations with leaf indices. The finite number of unique codes induces a\npartitioning of the input space. Each partition may be described in a variety\nof ways, including examination of an interpretable model (e.g. a logistic\nregression or a decision tree) trained to discriminate between objects of those\npartitions. Our experiments provide an intuition behind the method and\ndemonstrate revealed artifacts in neural network decision making. \n\n"}
{"id": "1811.03259", "contents": "Title: Bias and Generalization in Deep Generative Models: An Empirical Study Abstract: In high dimensional settings, density estimation algorithms rely crucially on\ntheir inductive bias. Despite recent empirical success, the inductive bias of\ndeep generative models is not well understood. In this paper we propose a\nframework to systematically investigate bias and generalization in deep\ngenerative models of images. Inspired by experimental methods from cognitive\npsychology, we probe each learning algorithm with carefully designed training\ndatasets to characterize when and how existing models generate novel attributes\nand their combinations. We identify similarities to human psychology and verify\nthat these patterns are consistent across commonly used models and\narchitectures. \n\n"}
{"id": "1811.03305", "contents": "Title: BAR: Bayesian Activity Recognition using variational inference Abstract: Uncertainty estimation in deep neural networks is essential for designing\nreliable and robust AI systems. Applications such as video surveillance for\nidentifying suspicious activities are designed with deep neural networks\n(DNNs), but DNNs do not provide uncertainty estimates. Capturing reliable\nuncertainty estimates in safety and security critical applications will help to\nestablish trust in the AI system. Our contribution is to apply Bayesian deep\nlearning framework to visual activity recognition application and quantify\nmodel uncertainty along with principled confidence. We utilize the stochastic\nvariational inference technique while training the Bayesian DNNs to infer the\napproximate posterior distribution around model parameters and perform Monte\nCarlo sampling on the posterior of model parameters to obtain the predictive\ndistribution. We show that the Bayesian inference applied to DNNs provide\nreliable confidence measures for visual activity recognition task as compared\nto conventional DNNs. We also show that our method improves the visual activity\nrecognition precision-recall AUC by 6.2% compared to non-Bayesian baseline. We\nevaluate our models on Moments-In-Time (MiT) activity recognition dataset by\nselecting a subset of in- and out-of-distribution video samples. \n\n"}
{"id": "1811.03717", "contents": "Title: Fast determinantal point processes via distortion-free intermediate\n  sampling Abstract: Given a fixed $n\\times d$ matrix $\\mathbf{X}$, where $n\\gg d$, we study the\ncomplexity of sampling from a distribution over all subsets of rows where the\nprobability of a subset is proportional to the squared volume of the\nparallelepiped spanned by the rows (a.k.a. a determinantal point process). In\nthis task, it is important to minimize the preprocessing cost of the procedure\n(performed once) as well as the sampling cost (performed repeatedly). To that\nend, we propose a new determinantal point process algorithm which has the\nfollowing two properties, both of which are novel: (1) a preprocessing step\nwhich runs in time $O(\\text{number-of-non-zeros}(\\mathbf{X})\\cdot\\log\nn)+\\text{poly}(d)$, and (2) a sampling step which runs in $\\text{poly}(d)$\ntime, independent of the number of rows $n$. We achieve this by introducing a\nnew regularized determinantal point process (R-DPP), which serves as an\nintermediate distribution in the sampling procedure by reducing the number of\nrows from $n$ to $\\text{poly}(d)$. Crucially, this intermediate distribution\ndoes not distort the probabilities of the target sample. Our key novelty in\ndefining the R-DPP is the use of a Poisson random variable for controlling the\nprobabilities of different subset sizes, leading to new determinantal formulas\nsuch as the normalization constant for this distribution. Our algorithm has\napplications in many diverse areas where determinantal point processes have\nbeen used, such as machine learning, stochastic optimization, data\nsummarization and low-rank matrix reconstruction. \n\n"}
{"id": "1811.03850", "contents": "Title: MD-GAN: Multi-Discriminator Generative Adversarial Networks for\n  Distributed Datasets Abstract: A recent technical breakthrough in the domain of machine learning is the\ndiscovery and the multiple applications of Generative Adversarial Networks\n(GANs). Those generative models are computationally demanding, as a GAN is\ncomposed of two deep neural networks, and because it trains on large datasets.\nA GAN is generally trained on a single server.\n  In this paper, we address the problem of distributing GANs so that they are\nable to train over datasets that are spread on multiple workers. MD-GAN is\nexposed as the first solution for this problem: we propose a novel learning\nprocedure for GANs so that they fit this distributed setup. We then compare the\nperformance of MD-GAN to an adapted version of Federated Learning to GANs,\nusing the MNIST and CIFAR10 datasets. MD-GAN exhibits a reduction by a factor\nof two of the learning complexity on each worker node, while providing better\nperformances than federated learning on both datasets. We finally discuss the\npractical implications of distributing GANs. \n\n"}
{"id": "1811.03963", "contents": "Title: Deep Compression of Sum-Product Networks on Tensor Networks Abstract: Sum-product networks (SPNs) represent an emerging class of neural networks\nwith clear probabilistic semantics and superior inference speed over graphical\nmodels. This work reveals a strikingly intimate connection between SPNs and\ntensor networks, thus leading to a highly efficient representation that we call\ntensor SPNs (tSPNs). For the first time, through mapping an SPN onto a tSPN and\nemploying novel optimization techniques, we demonstrate remarkable parameter\ncompression with negligible loss in accuracy. \n\n"}
{"id": "1811.03980", "contents": "Title: A Methodology for Automatic Selection of Activation Functions to Design\n  Hybrid Deep Neural Networks Abstract: Activation functions influence behavior and performance of DNNs. Nonlinear\nactivation functions, like Rectified Linear Units (ReLU), Exponential Linear\nUnits (ELU) and Scaled Exponential Linear Units (SELU), outperform the linear\ncounterparts. However, selecting an appropriate activation function is a\nchallenging problem, as it affects the accuracy and the complexity of the given\nDNN. In this paper, we propose a novel methodology to automatically select the\nbest-possible activation function for each layer of a given DNN, such that the\noverall DNN accuracy, compared to considering only one type of activation\nfunction for the whole DNN, is improved. However, an associated scientific\nchallenge in exploring all the different configurations of activation functions\nwould be time and resource-consuming. Towards this, our methodology identifies\nthe Evaluation Points during learning to evaluate the accuracy in an\nintermediate step of training and to perform early termination by checking the\naccuracy gradient of the learning curve. This helps in significantly reducing\nthe exploration time during training. Moreover, our methodology selects, for\neach layer, the dropout rate that optimizes the accuracy. Experiments show that\nwe are able to achieve on average 7% to 15% Relative Error Reduction on MNIST,\nCIFAR-10 and CIFAR-100 benchmarks, with limited performance and power penalty\non GPUs. \n\n"}
{"id": "1811.04022", "contents": "Title: Convolutional neural networks in phase space and inverse problems Abstract: We study inverse problems consisting on determining medium properties using\nthe responses to probing waves from the machine learning point of view. Based\non the understanding of propagation of waves and their nonlinear interactions,\nwe construct a deep convolutional neural network in which the parameters are\nused to classify and reconstruct the coefficients of nonlinear wave equations\nthat model the medium properties. Furthermore, for given approximation\naccuracy, we obtain the depth and number of units of the network and their\nquantitative dependence on the complexity of the medium. \n\n"}
{"id": "1811.04393", "contents": "Title: Gaussian-Induced Convolution for Graphs Abstract: Learning representation on graph plays a crucial role in numerous tasks of\npattern recognition. Different from grid-shaped images/videos, on which local\nconvolution kernels can be lattices, however, graphs are fully coordinate-free\non vertices and edges. In this work, we propose a Gaussian-induced convolution\n(GIC) framework to conduct local convolution filtering on irregular graphs.\nSpecifically, an edge-induced Gaussian mixture model is designed to encode\nvariations of subgraph region by integrating edge information into weighted\nGaussian models, each of which implicitly characterizes one component of\nsubgraph variations. In order to coarsen a graph, we derive a vertex-induced\nGaussian mixture model to cluster vertices dynamically according to the\nconnection of edges, which is approximately equivalent to the weighted graph\ncut. We conduct our multi-layer graph convolution network on several public\ndatasets of graph classification. The extensive experiments demonstrate that\nour GIC is effective and can achieve the state-of-the-art results. \n\n"}
{"id": "1811.05042", "contents": "Title: Exploiting Local Feature Patterns for Unsupervised Domain Adaptation Abstract: Unsupervised domain adaptation methods aim to alleviate performance\ndegradation caused by domain-shift by learning domain-invariant\nrepresentations. Existing deep domain adaptation methods focus on holistic\nfeature alignment by matching source and target holistic feature distributions,\nwithout considering local features and their multi-mode statistics. We show\nthat the learned local feature patterns are more generic and transferable and a\nfurther local feature distribution matching enables fine-grained feature\nalignment. In this paper, we present a method for learning domain-invariant\nlocal feature patterns and jointly aligning holistic and local feature\nstatistics. Comparisons to the state-of-the-art unsupervised domain adaptation\nmethods on two popular benchmark datasets demonstrate the superiority of our\napproach and its effectiveness on alleviating negative transfer. \n\n"}
{"id": "1811.05157", "contents": "Title: Recurrent Multi-Graph Neural Networks for Travel Cost Prediction Abstract: Origin-destination (OD) matrices are often used in urban planning, where a\ncity is partitioned into regions and an element (i, j) in an OD matrix records\nthe cost (e.g., travel time, fuel consumption, or travel speed) from region i\nto region j. In this paper, we partition a day into multiple intervals, e.g.,\n96 15-min intervals and each interval is associated with an OD matrix which\nrepresents the costs in the interval; and we consider sparse and stochastic OD\nmatrices, where the elements represent stochastic but not deterministic costs\nand some elements are missing due to lack of data between two regions. We solve\nthe sparse, stochastic OD matrix forecasting problem. Given a sequence of\nhistorical OD matrices that are sparse, we aim at predicting future OD matrices\nwith no empty elements. We propose a generic learning framework to solve the\nproblem by dealing with sparse matrices via matrix factorization and two graph\nconvolutional neural networks and capturing temporal dynamics via recurrent\nneural network. Empirical studies using two taxi datasets from different\ncountries verify the effectiveness of the proposed framework. \n\n"}
{"id": "1811.05544", "contents": "Title: An Introductory Survey on Attention Mechanisms in NLP Problems Abstract: First derived from human intuition, later adapted to machine translation for\nautomatic token alignment, attention mechanism, a simple method that can be\nused for encoding sequence data based on the importance score each element is\nassigned, has been widely applied to and attained significant improvement in\nvarious tasks in natural language processing, including sentiment\nclassification, text summarization, question answering, dependency parsing,\netc. In this paper, we survey through recent works and conduct an introductory\nsummary of the attention mechanism in different NLP problems, aiming to provide\nour readers with basic knowledge on this widely used method, discuss its\ndifferent variants for different tasks, explore its association with other\ntechniques in machine learning, and examine methods for evaluating its\nperformance. \n\n"}
{"id": "1811.05646", "contents": "Title: Fast Distribution Grid Line Outage Identification with $\\mu$PMU Abstract: The growing integration of distributed energy resources (DERs) in urban\ndistribution grids raises various reliability issues due to DER's uncertain and\ncomplex behaviors. With a large-scale DER penetration, traditional outage\ndetection methods, which rely on customers making phone calls and smart meters'\n\"last gasp\" signals, will have limited performance, because the renewable\ngenerators can supply powers after line outages and many urban grids are mesh\nso line outages do not affect power supply. To address these drawbacks, we\npropose a data-driven outage monitoring approach based on the stochastic time\nseries analysis from micro phasor measurement unit ($\\mu$PMU). Specifically, we\nprove via power flow analysis that the dependency of time-series voltage\nmeasurements exhibits significant statistical changes after line outages. This\nmakes the theory on optimal change-point detection suitable to identify line\noutages via $\\mu$PMUs with fast and accurate sampling. However, existing change\npoint detection methods require post-outage voltage distribution unknown in\ndistribution systems. Therefore, we design a maximum likelihood-based method to\ndirectly learn the distribution parameters from $\\mu$PMU data. We prove that\nthe estimated parameters-based detection still achieves the optimal\nperformance, making it extremely useful for distribution grid outage\nidentifications. Simulation results show highly accurate outage identification\nin eight distribution grids with 14 configurations with and without DERs using\n$\\mu$PMU data. \n\n"}
{"id": "1811.06524", "contents": "Title: Exploiting Class Learnability in Noisy Data Abstract: In many domains, collecting sufficient labeled training data for supervised\nmachine learning requires easily accessible but noisy sources, such as\ncrowdsourcing services or tagged Web data. Noisy labels occur frequently in\ndata sets harvested via these means, sometimes resulting in entire classes of\ndata on which learned classifiers generalize poorly. For real world\napplications, we argue that it can be beneficial to avoid training on such\nclasses entirely. In this work, we aim to explore the classes in a given data\nset, and guide supervised training to spend time on a class proportional to its\nlearnability. By focusing the training process, we aim to improve model\ngeneralization on classes with a strong signal. To that end, we develop an\nonline algorithm that works in conjunction with classifier and training\nalgorithm, iteratively selecting training data for the classifier based on how\nwell it appears to generalize on each class. Testing our approach on a variety\nof data sets, we show our algorithm learns to focus on classes for which the\nmodel has low generalization error relative to strong baselines, yielding a\nclassifier with good performance on learnable classes. \n\n"}
{"id": "1811.06665", "contents": "Title: Spatial-temporal Multi-Task Learning for Within-field Cotton Yield\n  Prediction Abstract: Understanding and accurately predicting within-field spatial variability of\ncrop yield play a key role in site-specific management of crop inputs such as\nirrigation water and fertilizer for optimized crop production. However, such a\ntask is challenged by the complex interaction between crop growth and\nenvironmental and managerial factors, such as climate, soil conditions,\ntillage, and irrigation. In this paper, we present a novel Spatial-temporal\nMulti-Task Learning algorithms for within-field crop yield prediction in west\nTexas from 2001 to 2003. This algorithm integrates multiple heterogeneous data\nsources to learn different features simultaneously, and to aggregate\nspatial-temporal features by introducing a weighted regularizer to the loss\nfunctions. Our comprehensive experimental results consistently outperform the\nresults of other conventional methods, and suggest a promising approach, which\nimproves the landscape of crop prediction research fields. \n\n"}
{"id": "1811.06930", "contents": "Title: Pre-training Graph Neural Networks with Kernels Abstract: Many machine learning techniques have been proposed in the last few years to\nprocess data represented in graph-structured form. Graphs can be used to model\nseveral scenarios, from molecules and materials to RNA secondary structures.\nSeveral kernel functions have been defined on graphs that coupled with\nkernelized learning algorithms, have shown state-of-the-art performances on\nmany tasks. Recently, several definitions of Neural Networks for Graph (GNNs)\nhave been proposed, but their accuracy is not yet satisfying. In this paper, we\npropose a task-independent pre-training methodology that allows a GNN to learn\nthe representation induced by state-of-the-art graph kernels. Then, the\nsupervised learning phase will fine-tune this representation for the task at\nhand. The proposed technique is agnostic on the adopted GNN architecture and\nkernel function, and shows consistent improvements in the predictive\nperformance of GNNs in our preliminary experimental results. \n\n"}
{"id": "1811.06981", "contents": "Title: Learned Video Compression Abstract: We present a new algorithm for video coding, learned end-to-end for the\nlow-latency mode. In this setting, our approach outperforms all existing video\ncodecs across nearly the entire bitrate range. To our knowledge, this is the\nfirst ML-based method to do so.\n  We evaluate our approach on standard video compression test sets of varying\nresolutions, and benchmark against all mainstream commercial codecs, in the\nlow-latency mode. On standard-definition videos, relative to our algorithm,\nHEVC/H.265, AVC/H.264 and VP9 typically produce codes up to 60% larger. On\nhigh-definition 1080p videos, H.265 and VP9 typically produce codes up to 20%\nlarger, and H.264 up to 35% larger. Furthermore, our approach does not suffer\nfrom blocking artifacts and pixelation, and thus produces videos that are more\nvisually pleasing.\n  We propose two main contributions. The first is a novel architecture for\nvideo compression, which (1) generalizes motion estimation to perform any\nlearned compensation beyond simple translations, (2) rather than strictly\nrelying on previously transmitted reference frames, maintains a state of\narbitrary information learned by the model, and (3) enables jointly compressing\nall transmitted signals (such as optical flow and residual).\n  Secondly, we present a framework for ML-based spatial rate control: namely, a\nmechanism for assigning variable bitrates across space for each frame. This is\na critical component for video coding, which to our knowledge had not been\ndeveloped within a machine learning setting. \n\n"}
{"id": "1811.07023", "contents": "Title: An Infinite Parade of Giraffes: Expressive Augmentation and Complexity\n  Layers for Cartoon Drawing Abstract: In this paper, we explore creative image generation constrained by small\ndata. To partially automate the creation of cartoon sketches consistent with a\nspecific designer's style, where acquiring a very large original image set is\nimpossible or cost prohibitive, we exploit domain specific knowledge for a huge\nreduction in original image requirements, creating an effectively infinite\nnumber of cartoon giraffes from just nine original drawings. We introduce\n\"expressive augmentations\" for cartoon sketches, mathematical transformations\nthat create broad domain appropriate variation, far beyond the usual affine\ntransformations, and we show that chained GANs models trained on the temporal\nstages of drawing or \"complexity layers\" can effectively add character\nappropriate details and finish new drawings in the designer's style.\n  We discuss the application of these tools in design processes for textiles,\ngraphics, architectural elements and interior design. \n\n"}
{"id": "1811.07073", "contents": "Title: Semi-Supervised Semantic Image Segmentation with Self-correcting\n  Networks Abstract: Building a large image dataset with high-quality object masks for semantic\nsegmentation is costly and time consuming. In this paper, we introduce a\nprincipled semi-supervised framework that only uses a small set of fully\nsupervised images (having semantic segmentation labels and box labels) and a\nset of images with only object bounding box labels (we call it the weak set).\nOur framework trains the primary segmentation model with the aid of an\nancillary model that generates initial segmentation labels for the weak set and\na self-correction module that improves the generated labels during training\nusing the increasingly accurate primary model. We introduce two variants of the\nself-correction module using either linear or convolutional functions.\nExperiments on the PASCAL VOC 2012 and Cityscape datasets show that our models\ntrained with a small fully supervised set perform similar to, or better than,\nmodels trained with a large fully supervised set while requiring ~7x less\nannotation effort. \n\n"}
{"id": "1811.07745", "contents": "Title: Reinforcement Learning with A* and a Deep Heuristic Abstract: A* is a popular path-finding algorithm, but it can only be applied to those\ndomains where a good heuristic function is known. Inspired by recent methods\ncombining Deep Neural Networks (DNNs) and trees, this study demonstrates how to\ntrain a heuristic represented by a DNN and combine it with A*. This new\nalgorithm which we call aleph-star can be used efficiently in domains where the\ninput to the heuristic could be processed by a neural network. We compare\naleph-star to N-Step Deep Q-Learning (DQN Mnih et al. 2013) in a driving\nsimulation with pixel-based input, and demonstrate significantly better\nperformance in this scenario. \n\n"}
{"id": "1811.08382", "contents": "Title: Locally Private Gaussian Estimation Abstract: We study a basic private estimation problem: each of $n$ users draws a single\ni.i.d. sample from an unknown Gaussian distribution, and the goal is to\nestimate the mean of this Gaussian distribution while satisfying local\ndifferential privacy for each user. Informally, local differential privacy\nrequires that each data point is individually and independently privatized\nbefore it is passed to a learning algorithm. Locally private Gaussian\nestimation is therefore difficult because the data domain is unbounded: users\nmay draw arbitrarily different inputs, but local differential privacy\nnonetheless mandates that different users have (worst-case) similar privatized\noutput distributions. We provide both adaptive two-round solutions and\nnonadaptive one-round solutions for locally private Gaussian estimation. We\nthen partially match these upper bounds with an information-theoretic lower\nbound. This lower bound shows that our accuracy guarantees are tight up to\nlogarithmic factors for all sequentially interactive\n$(\\varepsilon,\\delta)$-locally private protocols. \n\n"}
{"id": "1811.08577", "contents": "Title: How the Softmax Output is Misleading for Evaluating the Strength of\n  Adversarial Examples Abstract: Even before deep learning architectures became the de facto models for\ncomplex computer vision tasks, the softmax function was, given its elegant\nproperties, already used to analyze the predictions of feedforward neural\nnetworks. Nowadays, the output of the softmax function is also commonly used to\nassess the strength of adversarial examples: malicious data points designed to\nfail machine learning models during the testing phase. However, in this paper,\nwe show that it is possible to generate adversarial examples that take\nadvantage of some properties of the softmax function, leading to undesired\noutcomes when interpreting the strength of the adversarial examples at hand.\nSpecifically, we argue that the output of the softmax function is a poor\nindicator when the strength of an adversarial example is analyzed and that this\nindicator can be easily tricked by already existing methods for adversarial\nexample generation. \n\n"}
{"id": "1811.08737", "contents": "Title: SpotTune: Transfer Learning through Adaptive Fine-tuning Abstract: Transfer learning, which allows a source task to affect the inductive bias of\nthe target task, is widely used in computer vision. The typical way of\nconducting transfer learning with deep neural networks is to fine-tune a model\npre-trained on the source task using data from the target task. In this paper,\nwe propose an adaptive fine-tuning approach, called SpotTune, which finds the\noptimal fine-tuning strategy per instance for the target data. In SpotTune,\ngiven an image from the target task, a policy network is used to make routing\ndecisions on whether to pass the image through the fine-tuned layers or the\npre-trained layers. We conduct extensive experiments to demonstrate the\neffectiveness of the proposed approach. Our method outperforms the traditional\nfine-tuning approach on 12 out of 14 standard datasets.We also compare SpotTune\nwith other state-of-the-art fine-tuning strategies, showing superior\nperformance. On the Visual Decathlon datasets, our method achieves the highest\nscore across the board without bells and whistles. \n\n"}
{"id": "1811.08990", "contents": "Title: Markov Chain Block Coordinate Descent Abstract: The method of block coordinate gradient descent (BCD) has been a powerful\nmethod for large-scale optimization. This paper considers the BCD method that\nsuccessively updates a series of blocks selected according to a Markov chain.\nThis kind of block selection is neither i.i.d. random nor cyclic. On the other\nhand, it is a natural choice for some applications in distributed optimization\nand Markov decision process, where i.i.d. random and cyclic selections are\neither infeasible or very expensive. By applying mixing-time properties of a\nMarkov chain, we prove convergence of Markov chain BCD for minimizing Lipschitz\ndifferentiable functions, which can be nonconvex. When the functions are convex\nand strongly convex, we establish both sublinear and linear convergence rates,\nrespectively. We also present a method of Markov chain inertial BCD. Finally,\nwe discuss potential applications. \n\n"}
{"id": "1811.09083", "contents": "Title: Learning Goal Embeddings via Self-Play for Hierarchical Reinforcement\n  Learning Abstract: In hierarchical reinforcement learning a major challenge is determining\nappropriate low-level policies. We propose an unsupervised learning scheme,\nbased on asymmetric self-play from Sukhbaatar et al. (2018), that automatically\nlearns a good representation of sub-goals in the environment and a low-level\npolicy that can execute them. A high-level policy can then direct the lower one\nby generating a sequence of continuous sub-goal vectors. We evaluate our model\nusing Mazebase and Mujoco environments, including the challenging AntGather\ntask. Visualizations of the sub-goal embeddings reveal a logical decomposition\nof tasks within the environment. Quantitatively, our approach obtains\ncompelling performance gains over non-hierarchical approaches. \n\n"}
{"id": "1811.09350", "contents": "Title: Predicting Diabetes Disease Evolution Using Financial Records and\n  Recurrent Neural Networks Abstract: Managing patients with chronic diseases is a major and growing healthcare\nchallenge in several countries. A chronic condition, such as diabetes, is an\nillness that lasts a long time and does not go away, and often leads to the\npatient's health gradually getting worse. While recent works involve raw\nelectronic health record (EHR) from hospitals, this work uses only financial\nrecords from health plan providers (medical claims) to predict diabetes disease\nevolution with a self-attentive recurrent neural network. The use of financial\ndata is due to the possibility of being an interface to international\nstandards, as the records standard encodes medical procedures. The main goal\nwas to assess high risk diabetics, so we predict records related to diabetes\nacute complications such as amputations and debridements, revascularization and\nhemodialysis. Our work succeeds to anticipate complications between 60 to 240\ndays with an area under ROC curve ranging from 0.81 to 0.94. In this paper we\ndescribe the first half of a work-in-progress developed within a health plan\nprovider with ROC curve ranging from 0.81 to 0.83. This assessment will give\nhealthcare providers the chance to intervene earlier and head off\nhospitalizations. We are aiming to deliver personalized predictions and\npersonalized recommendations to individual patients, with the goal of improving\noutcomes and reducing costs \n\n"}
{"id": "1811.09539", "contents": "Title: State of the Art in Fair ML: From Moral Philosophy and Legislation to\n  Fair Classifiers Abstract: Machine learning is becoming an ever present part in our lives as many\ndecisions, e.g. to lend a credit, are no longer made by humans but by machine\nlearning algorithms. However those decisions are often unfair and\ndiscriminating individuals belonging to protected groups based on race or\ngender. With the recent General Data Protection Regulation (GDPR) coming into\neffect, new awareness has been raised for such issues and with computer\nscientists having such a large impact on peoples lives it is necessary that\nactions are taken to discover and prevent discrimination. This work aims to\ngive an introduction into discrimination, legislative foundations to counter it\nand strategies to detect and prevent machine learning algorithms from showing\nsuch behavior. \n\n"}
{"id": "1811.09716", "contents": "Title: Robustness via curvature regularization, and vice versa Abstract: State-of-the-art classifiers have been shown to be largely vulnerable to\nadversarial perturbations. One of the most effective strategies to improve\nrobustness is adversarial training. In this paper, we investigate the effect of\nadversarial training on the geometry of the classification landscape and\ndecision boundaries. We show in particular that adversarial training leads to a\nsignificant decrease in the curvature of the loss surface with respect to\ninputs, leading to a drastically more \"linear\" behaviour of the network. Using\na locally quadratic approximation, we provide theoretical evidence on the\nexistence of a strong relation between large robustness and small curvature. To\nfurther show the importance of reduced curvature for improving the robustness,\nwe propose a new regularizer that directly minimizes curvature of the loss\nsurface, and leads to adversarial robustness that is on par with adversarial\ntraining. Besides being a more efficient and principled alternative to\nadversarial training, the proposed regularizer confirms our claims on the\nimportance of exhibiting quasi-linear behavior in the vicinity of data points\nin order to achieve robustness. \n\n"}
{"id": "1811.09757", "contents": "Title: Physics-Informed CoKriging: A Gaussian-Process-Regression-Based\n  Multifidelity Method for Data-Model Convergence Abstract: In this work, we propose a new Gaussian process regression (GPR)-based\nmultifidelity method: physics-informed CoKriging (CoPhIK). In CoKriging-based\nmultifidelity methods, the quantities of interest are modeled as linear\ncombinations of multiple parameterized stationary Gaussian processes (GPs), and\nthe hyperparameters of these GPs are estimated from data via optimization. In\nCoPhIK, we construct a GP representing low-fidelity data using physics-informed\nKriging (PhIK), and model the discrepancy between low- and high-fidelity data\nusing a parameterized GP with hyperparameters identified via optimization. Our\napproach reduces the cost of optimization for inferring hyperparameters by\nincorporating partial physical knowledge. We prove that the physical\nconstraints in the form of deterministic linear operators are satisfied up to\nan error bound. Furthermore, we combine CoPhIK with a greedy active learning\nalgorithm for guiding the selection of additional observation locations. The\nefficiency and accuracy of CoPhIK are demonstrated for reconstructing the\npartially observed modified Branin function, reconstructing the sparsely\nobserved state of a steady state heat transport problem, and learning a\nconservative tracer distribution from sparse tracer concentration measurements. \n\n"}
{"id": "1811.09834", "contents": "Title: Efficient Video Understanding via Layered Multi Frame-Rate Analysis Abstract: One of the greatest challenges in the design of a real-time perception system\nfor autonomous driving vehicles and drones is the conflicting requirement of\nsafety (high prediction accuracy) and efficiency. Traditional approaches use a\nsingle frame rate for the entire system. Motivated by the observation that the\nlack of robustness against environmental factors is the major weakness of\ncompact ConvNet architectures, we propose a dual frame-rate system that brings\nin the best of both worlds: A modulator stream that executes an expensive\nmodels robust to environmental factors at a low frame rate to extract slowly\nchanging features describing the environment, and a prediction stream that\nexecutes a light-weight model at real-time to extract transient signals that\ndescribes particularities of the current frame. The advantage of our design is\nvalidated by our extensive empirical study, showing that our solution leads to\nconsistent improvements using a variety of backbone architecture choice and\ninput resolutions. These findings suggest multiple frame-rate systems as a\npromising direction in designing efficient perception for autonomous agents. \n\n"}
{"id": "1811.09955", "contents": "Title: Online Newton Step Algorithm with Estimated Gradient Abstract: Online learning with limited information feedback (bandit) tries to solve the\nproblem where an online learner receives partial feedback information from the\nenvironment in the course of learning. Under this setting, Flaxman et al.[8]\nextended Zinkevich's classical Online Gradient Descent (OGD) algorithm [29] by\nproposing the Online Gradient Descent with Expected Gradient (OGDEG) algorithm.\nSpecifically, it uses a simple trick to approximate the gradient of the loss\nfunction $f_t$ by evaluating it at a single point and bounds the expected\nregret as $\\mathcal{O}(T^{5/6})$ [8], where the number of rounds is $T$.\nMeanwhile, past research efforts have shown that compared with the first-order\nalgorithms, second-order online learning algorithms such as Online Newton Step\n(ONS) [11] can significantly accelerate the convergence rate of traditional\nonline learning algorithms. Motivated by this, this paper aims to exploit the\nsecond-order information to speed up the convergence of the OGDEG algorithm. In\nparticular, we extend the ONS algorithm with the trick of expected gradient and\ndevelop a novel second-order online learning algorithm, i.e., Online Newton\nStep with Expected Gradient (ONSEG). Theoretically, we show that the proposed\nONSEG algorithm significantly reduces the expected regret of OGDEG algorithm\nfrom $\\mathcal{O}(T^{5/6})$ to $\\mathcal{O}(T^{2/3})$ in the bandit feedback\nscenario. Empirically, we further demonstrate the advantages of the proposed\nalgorithm on multiple real-world datasets. \n\n"}
{"id": "1811.10052", "contents": "Title: An overview of deep learning in medical imaging focusing on MRI Abstract: What has happened in machine learning lately, and what does it mean for the\nfuture of medical image analysis? Machine learning has witnessed a tremendous\namount of attention over the last few years. The current boom started around\n2009 when so-called deep artificial neural networks began outperforming other\nestablished models on a number of important benchmarks. Deep neural networks\nare now the state-of-the-art machine learning models across a variety of areas,\nfrom image analysis to natural language processing, and widely deployed in\nacademia and industry. These developments have a huge potential for medical\nimaging technology, medical data analysis, medical diagnostics and healthcare\nin general, slowly being realized. We provide a short overview of recent\nadvances and some associated challenges in machine learning applied to medical\nimage processing and image analysis. As this has become a very broad and fast\nexpanding field we will not survey the entire landscape of applications, but\nput particular focus on deep learning in MRI.\n  Our aim is threefold: (i) give a brief introduction to deep learning with\npointers to core references; (ii) indicate how deep learning has been applied\nto the entire MRI processing chain, from acquisition to image retrieval, from\nsegmentation to disease prediction; (iii) provide a starting point for people\ninterested in experimenting and perhaps contributing to the field of machine\nlearning for medical imaging by pointing out good educational resources,\nstate-of-the-art open-source code, and interesting sources of data and problems\nrelated medical imaging. \n\n"}
{"id": "1811.10112", "contents": "Title: A Model-Based Reinforcement Learning Approach for a Rare Disease\n  Diagnostic Task Abstract: In this work, we present our various contributions to the objective of\nbuilding a decision support tool for the diagnosis of rare diseases. Our goal\nis to achieve a state of knowledge where the uncertainty about the patient's\ndisease is below a predetermined threshold. We aim to reach such states while\nminimizing the average number of medical tests to perform. In doing so, we take\ninto account the need, in many medical applications, to avoid, as much as\npossible, any misdiagnosis. To solve this optimization task, we investigate\nseveral reinforcement learning algorithm and make them operable in our\nhigh-dimensional and sparse-reward setting. We also present a way to combine\nexpert knowledge, expressed as conditional probabilities, with real clinical\ndata. This is crucial because the scarcity of data in the field of rare\ndiseases prevents any approach based solely on clinical data. Finally we show\nthat it is possible to integrate the ontological information about symptoms\nwhile remaining in our probabilistic reasoning. It enables our decision support\ntool to process information given at different level of precision by the user. \n\n"}
{"id": "1811.10158", "contents": "Title: Reinforcement Learning for Uplift Modeling Abstract: Uplift modeling aims to directly model the incremental impact of a treatment\non an individual response. In this work, we address the problem from a new\nangle and reformulate it as a Markov Decision Process (MDP). We conducted\nextensive experiments on both a synthetic dataset and real-world scenarios, and\nshowed that our method can achieve significant improvement over previous\nmethods. \n\n"}
{"id": "1811.10797", "contents": "Title: Node Embedding with Adaptive Similarities for Scalable Learning over\n  Graphs Abstract: Node embedding is the task of extracting informative and descriptive features\nover the nodes of a graph. The importance of node embeddings for graph\nanalytics, as well as learning tasks such as node classification, link\nprediction and community detection, has led to increased interest on the\nproblem leading to a number of recent advances. Much like PCA in the feature\ndomain, node embedding is an inherently \\emph{unsupervised} task; in lack of\nmetadata used for validation, practical methods may require standardization and\nlimiting the use of tunable hyperparameters. Finally, node embedding methods\nare faced with maintaining scalability in the face of large-scale real-world\ngraphs of ever-increasing sizes. In the present work, we propose an adaptive\nnode embedding framework that adjusts the embedding process to a given\nunderlying graph, in a fully unsupervised manner. To achieve this, we adopt the\nnotion of a tunable node similarity matrix that assigns weights on paths of\ndifferent length. The design of the multilength similarities ensures that the\nresulting embeddings also inherit interpretable spectral properties. The\nproposed model is carefully studied, interpreted, and numerically evaluated\nusing stochastic block models. Moreover, an algorithmic scheme is proposed for\ntraining the model parameters effieciently and in an unsupervised manner. We\nperform extensive node classification, link prediction, and clustering\nexperiments on many real world graphs from various domains, and compare with\nstate-of-the-art scalable and unsupervised node embedding alternatives. The\nproposed method enjoys superior performance in many cases, while also yielding\ninterpretable information on the underlying structure of the graph. \n\n"}
{"id": "1811.10869", "contents": "Title: Efficient non-uniform quantizer for quantized neural network targeting\n  reconfigurable hardware Abstract: Convolutional Neural Networks (CNN) has become more popular choice for\nvarious tasks such as computer vision, speech recognition and natural language\nprocessing. Thanks to their large computational capability and throughput, GPUs\n,which are not power efficient and therefore does not suit low power systems\nsuch as mobile devices, are the most common platform for both training and\ninferencing tasks. Recent studies has shown that FPGAs can provide a good\nalternative to GPUs as a CNN accelerator, due to their re-configurable nature,\nlow power and small latency. In order for FPGA-based accelerators outperform\nGPUs in inference task, both the parameters of the network and the activations\nmust be quantized. While most works use uniform quantizers for both parameters\nand activations, it is not always the optimal one, and a non-uniform quantizer\nneed to be considered. In this work we introduce a custom hardware-friendly\napproach to implement non-uniform quantizers. In addition, we use a single\nscale integer representation of both parameters and activations, for both\ntraining and inference. The combined method yields a hardware efficient\nnon-uniform quantizer, fit for real-time applications. We have tested our\nmethod on CIFAR-10 and CIFAR-100 image classification datasets with ResNet-18\nand VGG-like architectures, and saw little degradation in accuracy. \n\n"}
{"id": "1811.11163", "contents": "Title: Class-Distinct and Class-Mutual Image Generation with GANs Abstract: Class-conditional extensions of generative adversarial networks (GANs), such\nas auxiliary classifier GAN (AC-GAN) and conditional GAN (cGAN), have garnered\nattention owing to their ability to decompose representations into class labels\nand other factors and to boost the training stability. However, a limitation is\nthat they assume that each class is separable and ignore the relationship\nbetween classes even though class overlapping frequently occurs in a real-world\nscenario when data are collected on the basis of diverse or ambiguous criteria.\nTo overcome this limitation, we address a novel problem called class-distinct\nand class-mutual image generation, in which the goal is to construct a\ngenerator that can capture between-class relationships and generate an image\nselectively conditioned on the class specificity. To solve this problem without\nadditional supervision, we propose classifier's posterior GAN (CP-GAN), in\nwhich we redesign the generator input and the objective function of AC-GAN for\nclass-overlapping data. Precisely, we incorporate the classifier's posterior\ninto the generator input and optimize the generator so that the classifier's\nposterior of generated data corresponds with that of real data. We demonstrate\nthe effectiveness of CP-GAN using both controlled and real-world\nclass-overlapping data with a model configuration analysis and comparative\nstudy. Our code is available at https://github.com/takuhirok/CP-GAN/. \n\n"}
{"id": "1811.11210", "contents": "Title: Calibrating Uncertainties in Object Localization Task Abstract: In many safety-critical applications such as autonomous driving and surgical\nrobots, it is desirable to obtain prediction uncertainties from object\ndetection modules to help support safe decision-making. Specifically, such\nmodules need to estimate the probability of each predicted object in a given\nregion and the confidence interval for its bounding box. While recent Bayesian\ndeep learning methods provide a principled way to estimate this uncertainty,\nthe estimates for the bounding boxes obtained using these methods are\nuncalibrated. In this paper, we address this problem for the single-object\nlocalization task by adapting an existing technique for calibrating regression\nmodels. We show, experimentally, that the resulting calibrated model obtains\nmore reliable uncertainty estimates. \n\n"}
{"id": "1811.12239", "contents": "Title: Counterfactual Learning from Human Proofreading Feedback for Semantic\n  Parsing Abstract: In semantic parsing for question-answering, it is often too expensive to\ncollect gold parses or even gold answers as supervision signals. We propose to\nconvert model outputs into a set of human-understandable statements which allow\nnon-expert users to act as proofreaders, providing error markings as learning\nsignals to the parser. Because model outputs were suggested by a historic\nsystem, we operate in a counterfactual, or off-policy, learning setup. We\nintroduce new estimators which can effectively leverage the given feedback and\nwhich avoid known degeneracies in counterfactual learning, while still being\napplicable to stochastic gradient optimization for neural semantic parsing.\nFurthermore, we discuss how our feedback collection method can be seamlessly\nintegrated into deployed virtual personal assistants that embed a semantic\nparser. Our work is the first to show that semantic parsers can be improved\nsignificantly by counterfactual learning from logged human feedback data. \n\n"}
{"id": "1811.12488", "contents": "Title: Leveraging Deep Stein's Unbiased Risk Estimator for Unsupervised X-ray\n  Denoising Abstract: Among the plethora of techniques devised to curb the prevalence of noise in\nmedical images, deep learning based approaches have shown the most promise.\nHowever, one critical limitation of these deep learning based denoisers is the\nrequirement of high-quality noiseless ground truth images that are difficult to\nobtain in many medical imaging applications such as X-rays. To circumvent this\nissue, we leverage recently proposed approach of [7] that incorporates Stein's\nUnbiased Risk Estimator (SURE) to train a deep convolutional neural network\nwithout requiring denoised ground truth X-ray data. Our experimental results\ndemonstrate the effectiveness of SURE based approach for denoising X-ray\nimages. \n\n"}
{"id": "1812.00417", "contents": "Title: Snorkel DryBell: A Case Study in Deploying Weak Supervision at\n  Industrial Scale Abstract: Labeling training data is one of the most costly bottlenecks in developing\nmachine learning-based applications. We present a first-of-its-kind study\nshowing how existing knowledge resources from across an organization can be\nused as weak supervision in order to bring development time and cost down by an\norder of magnitude, and introduce Snorkel DryBell, a new weak supervision\nmanagement system for this setting. Snorkel DryBell builds on the Snorkel\nframework, extending it in three critical aspects: flexible, template-based\ningestion of diverse organizational knowledge, cross-feature production\nserving, and scalable, sampling-free execution. On three classification tasks\nat Google, we find that Snorkel DryBell creates classifiers of comparable\nquality to ones trained with tens of thousands of hand-labeled examples,\nconverts non-servable organizational resources to servable models for an\naverage 52% performance improvement, and executes over millions of data points\nin tens of minutes. \n\n"}
{"id": "1812.00463", "contents": "Title: Personalizing Intervention Probabilities By Pooling Abstract: In many mobile health interventions, treatments should only be delivered in a\nparticular context, for example when a user is currently stressed, walking or\nsedentary. Even in an optimal context, concerns about user burden can restrict\nwhich treatments are sent. To diffuse the treatment delivery over times when a\nuser is in a desired context, it is critical to predict the future number of\ntimes the context will occur. The focus of this paper is on whether\npersonalization can improve predictions in these settings. Though the variance\nbetween individuals' behavioral patterns suggest that personalization should be\nuseful, the amount of individual-level data limits its capabilities. Thus, we\ninvestigate several methods which pool data across users to overcome these\ndeficiencies and find that pooling lowers the overall error rate relative to\nboth personalized and batch approaches. \n\n"}
{"id": "1812.00547", "contents": "Title: Semi-supervised Rare Disease Detection Using Generative Adversarial\n  Network Abstract: Rare diseases affect a relatively small number of people, which limits\ninvestment in research for treatments and cures. Developing an efficient method\nfor rare disease detection is a crucial first step towards subsequent clinical\nresearch. In this paper, we present a semi-supervised learning framework for\nrare disease detection using generative adversarial networks. Our method takes\nadvantage of the large amount of unlabeled data for disease detection and\nachieves the best results in terms of precision-recall score compared to\nbaseline techniques. \n\n"}
{"id": "1812.00804", "contents": "Title: Deep Inverse Optimization Abstract: Given a set of observations generated by an optimization process, the goal of\ninverse optimization is to determine likely parameters of that process. We cast\ninverse optimization as a form of deep learning. Our method, called deep\ninverse optimization, is to unroll an iterative optimization process and then\nuse backpropagation to learn parameters that generate the observations. We\ndemonstrate that by backpropagating through the interior point algorithm we can\nlearn the coefficients determining the cost vector and the constraints,\nindependently or jointly, for both non-parametric and parametric linear\nprograms, starting from one or multiple observations. With this approach,\ninverse optimization can leverage concepts and algorithms from deep learning. \n\n"}
{"id": "1812.00877", "contents": "Title: Automatic lesion boundary detection in dermoscopy Abstract: This manuscript addresses the problem of the automatic lesion boundary\ndetection in dermoscopy, using deep neural networks. An approach is based on\nthe adaptation of the U-net convolutional neural network with skip connections\nfor lesion boundary segmentation task. I hope this paper could serve, to some\nextent, as an experiment of using deep convolutional networks in biomedical\nsegmentation task and as a guideline of the boundary detection benchmark,\ninspiring further attempts and researches. \n\n"}
{"id": "1812.01074", "contents": "Title: Distilling Information from a Flood: A Possibility for the Use of\n  Meta-Analysis and Systematic Review in Machine Learning Research Abstract: The current flood of information in all areas of machine learning research,\nfrom computer vision to reinforcement learning, has made it difficult to make\naggregate scientific inferences. It can be challenging to distill a myriad of\nsimilar papers into a set of useful principles, to determine which new\nmethodologies to use for a particular application, and to be confident that one\nhas compared against all relevant related work when developing new ideas.\nHowever, such a rapidly growing body of research literature is a problem that\nother fields have already faced - in particular, medicine and epidemiology. In\nthose fields, systematic reviews and meta-analyses have been used exactly for\ndealing with these issues and it is not uncommon for entire journals to be\ndedicated to such analyses. Here, we suggest the field of machine learning\nmight similarly benefit from meta-analysis and systematic review, and we\nencourage further discussion and development along this direction. \n\n"}
{"id": "1812.01106", "contents": "Title: Improving Traffic Safety Through Video Analysis in Jakarta, Indonesia Abstract: This project presents the results of a partnership between the Data Science\nfor Social Good fellowship, Jakarta Smart City and Pulse Lab Jakarta to create\na video analysis pipeline for the purpose of improving traffic safety in\nJakarta. The pipeline transforms raw traffic video footage into databases that\nare ready to be used for traffic analysis. By analyzing these patterns, the\ncity of Jakarta will better understand how human behavior and built\ninfrastructure contribute to traffic challenges and safety risks. The results\nof this work should also be broadly applicable to smart city initiatives around\nthe globe as they improve urban planning and sustainability through data\nscience approaches. \n\n"}
{"id": "1812.01608", "contents": "Title: Generating High Fidelity Images with Subscale Pixel Networks and\n  Multidimensional Upscaling Abstract: The unconditional generation of high fidelity images is a longstanding\nbenchmark for testing the performance of image decoders. Autoregressive image\nmodels have been able to generate small images unconditionally, but the\nextension of these methods to large images where fidelity can be more readily\nassessed has remained an open problem. Among the major challenges are the\ncapacity to encode the vast previous context and the sheer difficulty of\nlearning a distribution that preserves both global semantic coherence and\nexactness of detail. To address the former challenge, we propose the Subscale\nPixel Network (SPN), a conditional decoder architecture that generates an image\nas a sequence of sub-images of equal size. The SPN compactly captures\nimage-wide spatial dependencies and requires a fraction of the memory and the\ncomputation required by other fully autoregressive models. To address the\nlatter challenge, we propose to use Multidimensional Upscaling to grow an image\nin both size and depth via intermediate stages utilising distinct SPNs. We\nevaluate SPNs on the unconditional generation of CelebAHQ of size 256 and of\nImageNet from size 32 to 256. We achieve state-of-the-art likelihood results in\nmultiple settings, set up new benchmark results in previously unexplored\nsettings and are able to generate very high fidelity large scale samples on the\nbasis of both datasets. \n\n"}
{"id": "1812.01662", "contents": "Title: Feed-Forward Neural Networks Need Inductive Bias to Learn Equality\n  Relations Abstract: Basic binary relations such as equality and inequality are fundamental to\nrelational data structures. Neural networks should learn such relations and\ngeneralise to new unseen data. We show in this study, however, that this\ngeneralisation fails with standard feed-forward networks on binary vectors.\nEven when trained with maximal training data, standard networks do not reliably\ndetect equality.We introduce differential rectifier (DR) units that we add to\nthe network in different configurations. The DR units create an inductive bias\nin the networks, so that they do learn to generalise, even from small numbers\nof examples and we have not found any negative effect of their inclusion in the\nnetwork. Given the fundamental nature of these relations, we hypothesize that\nfeed-forward neural network learning benefits from inductive bias in other\nrelations as well. Consequently, the further development of suitable inductive\nbiases will be beneficial to many tasks in relational learning with neural\nnetworks. \n\n"}
{"id": "1812.01681", "contents": "Title: Deep Bayesian Self-Training Abstract: Supervised Deep Learning has been highly successful in recent years,\nachieving state-of-the-art results in most tasks. However, with the ongoing\nuptake of such methods in industrial applications, the requirement for large\namounts of annotated data is often a challenge. In most real world problems,\nmanual annotation is practically intractable due to time/labour constraints,\nthus the development of automated and adaptive data annotation systems is\nhighly sought after. In this paper, we propose both a (i) Deep Bayesian\nSelf-Training methodology for automatic data annotation, by leveraging\npredictive uncertainty estimates using variational inference and modern Neural\nNetwork architectures, as well as (ii) a practical adaptation procedure for\nhandling high label variability between different dataset distributions through\nclustering of Neural Network latent variable representations. An experimental\nstudy on both public and private datasets is presented illustrating the\nsuperior performance of the proposed approach over standard Self-Training\nbaselines, highlighting the importance of predictive uncertainty estimates in\nsafety-critical domains. \n\n"}
{"id": "1812.01690", "contents": "Title: General-to-Detailed GAN for Infrequent Class Medical Images Abstract: Deep learning has significant potential for medical imaging. However, since\nthe incident rate of each disease varies widely, the frequency of classes in a\nmedical image dataset is imbalanced, leading to poor accuracy for such\ninfrequent classes. One possible solution is data augmentation of infrequent\nclasses using synthesized images created by Generative Adversarial Networks\n(GANs), but conventional GANs also require certain amount of images to learn.\nTo overcome this limitation, here we propose General-to-detailed GAN (GDGAN),\nserially connected two GANs, one for general labels and the other for detailed\nlabels. GDGAN produced diverse medical images, and the network trained with an\naugmented dataset outperformed other networks using existing methods with\nrespect to Area-Under-Curve (AUC) of Receiver Operating Characteristic (ROC)\ncurve. \n\n"}
{"id": "1812.01710", "contents": "Title: GANtruth - an unpaired image-to-image translation method for driving\n  scenarios Abstract: Synthetic image translation has significant potentials in autonomous\ntransportation systems. That is due to the expense of data collection and\nannotation as well as the unmanageable diversity of real-words situations. The\nmain issue with unpaired image-to-image translation is the ill-posed nature of\nthe problem. In this work, we propose a novel method for constraining the\noutput space of unpaired image-to-image translation. We make the assumption\nthat the environment of the source domain is known (e.g. synthetically\ngenerated), and we propose to explicitly enforce preservation of the\nground-truth labels on the translated images.\n  We experiment on preserving ground-truth information such as semantic\nsegmentation, disparity, and instance segmentation. We show significant\nevidence that our method achieves improved performance over the\nstate-of-the-art model of UNIT for translating images from SYNTHIA to\nCityscapes. The generated images are perceived as more realistic in human\nsurveys and outperforms UNIT when used in a domain adaptation scenario for\nsemantic segmentation. \n\n"}
{"id": "1812.01711", "contents": "Title: A Graph-CNN for 3D Point Cloud Classification Abstract: Graph convolutional neural networks (Graph-CNNs) extend traditional CNNs to\nhandle data that is supported on a graph. Major challenges when working with\ndata on graphs are that the support set (the vertices of the graph) do not\ntypically have a natural ordering, and in general, the topology of the graph is\nnot regular (i.e., vertices do not all have the same number of neighbors).\nThus, Graph-CNNs have huge potential to deal with 3D point cloud data which has\nbeen obtained from sampling a manifold. In this paper, we develop a Graph-CNN\nfor classifying 3D point cloud data, called PointGCN. The architecture combines\nlocalized graph convolutions with two types of graph downsampling operations\n(also known as pooling). By the effective exploration of the point cloud local\nstructure using the Graph-CNN, the proposed architecture achieves competitive\nperformance on the 3D object classification benchmark ModelNet, and our\narchitecture is more stable than competing schemes. \n\n"}
{"id": "1812.01712", "contents": "Title: Multiview Based 3D Scene Understanding On Partial Point Sets Abstract: Deep learning within the context of point clouds has gained much research\ninterest in recent years mostly due to the promising results that have been\nachieved on a number of challenging benchmarks, such as 3D shape recognition\nand scene semantic segmentation. In many realistic settings however, snapshots\nof the environment are often taken from a single view, which only contains a\npartial set of the scene due to the field of view restriction of commodity\ncameras. 3D scene semantic understanding on partial point clouds is considered\nas a challenging task. In this work, we propose a processing approach for 3D\npoint cloud data based on a multiview representation of the existing 360{\\deg}\npoint clouds. By fusing the original 360{\\deg} point clouds and their\ncorresponding 3D multiview representations as input data, a neural network is\nable to recognize partial point sets while improving the general performance on\ncomplete point sets, resulting in an overall increase of 31.9% and 4.3% in\nsegmentation accuracy for partial and complete scene semantic understanding,\nrespectively. This method can also be applied in a wider 3D recognition context\nsuch as 3D part segmentation. \n\n"}
{"id": "1812.01713", "contents": "Title: FineFool: Fine Object Contour Attack via Attention Abstract: Machine learning models have been shown vulnerable to adversarial attacks\nlaunched by adversarial examples which are carefully crafted by attacker to\ndefeat classifiers. Deep learning models cannot escape the attack either. Most\nof adversarial attack methods are focused on success rate or perturbations\nsize, while we are more interested in the relationship between adversarial\nperturbation and the image itself. In this paper, we put forward a novel\nadversarial attack based on contour, named FineFool. Finefool not only has\nbetter attack performance compared with other state-of-art white-box attacks in\naspect of higher attack success rate and smaller perturbation, but also capable\nof visualization the optimal adversarial perturbation via attention on object\ncontour. To the best of our knowledge, Finefool is for the first time combines\nthe critical feature of the original clean image with the optimal perturbations\nin a visible manner. Inspired by the correlations between adversarial\nperturbations and object contour, slighter perturbations is produced via\nfocusing on object contour features, which is more imperceptible and difficult\nto be defended, especially network add-on defense methods with the trade-off\nbetween perturbations filtering and contour feature loss. Compared with\nexisting state-of-art attacks, extensive experiments are conducted to show that\nFinefool is capable of efficient attack against defensive deep models. \n\n"}
{"id": "1812.01718", "contents": "Title: Deep Learning for Classical Japanese Literature Abstract: Much of machine learning research focuses on producing models which perform\nwell on benchmark tasks, in turn improving our understanding of the challenges\nassociated with those tasks. From the perspective of ML researchers, the\ncontent of the task itself is largely irrelevant, and thus there have\nincreasingly been calls for benchmark tasks to more heavily focus on problems\nwhich are of social or cultural relevance. In this work, we introduce\nKuzushiji-MNIST, a dataset which focuses on Kuzushiji (cursive Japanese), as\nwell as two larger, more challenging datasets, Kuzushiji-49 and\nKuzushiji-Kanji. Through these datasets, we wish to engage the machine learning\ncommunity into the world of classical Japanese literature. Dataset available at\nhttps://github.com/rois-codh/kmnist \n\n"}
{"id": "1812.01719", "contents": "Title: Knowing what you know in brain segmentation using Bayesian deep neural\n  networks Abstract: In this paper, we describe a Bayesian deep neural network (DNN) for\npredicting FreeSurfer segmentations of structural MRI volumes, in minutes\nrather than hours. The network was trained and evaluated on a large dataset (n\n= 11,480), obtained by combining data from more than a hundred different sites,\nand also evaluated on another completely held-out dataset (n = 418). The\nnetwork was trained using a novel spike-and-slab dropout-based variational\ninference approach. We show that, on these datasets, the proposed Bayesian DNN\noutperforms previously proposed methods, in terms of the similarity between the\nsegmentation predictions and the FreeSurfer labels, and the usefulness of the\nestimate uncertainty of these predictions. In particular, we demonstrated that\nthe prediction uncertainty of this network at each voxel is a good indicator of\nwhether the network has made an error and that the uncertainty across the whole\nbrain can predict the manual quality control ratings of a scan. The proposed\nBayesian DNN method should be applicable to any new network architecture for\naddressing the segmentation problem. \n\n"}
{"id": "1812.01735", "contents": "Title: Learning Cheap and Novel Flight Itineraries Abstract: We consider the problem of efficiently constructing cheap and novel round\ntrip flight itineraries by combining legs from different airlines. We analyse\nthe factors that contribute towards the price of such itineraries and find that\nmany result from the combination of just 30% of airlines and that the closer\nthe departure of such itineraries is to the user's search date the more likely\nthey are to be cheaper than the tickets from one airline. We use these insights\nto formulate the problem as a trade-off between the recall of cheap itinerary\nconstructions and the costs associated with building them.\n  We propose a supervised learning solution with location embeddings which\nachieves an AUC=80.48, a substantial improvement over simpler baselines. We\ndiscuss various practical considerations for dealing with the staleness and the\nstability of the model and present the design of the machine learning pipeline.\nFinally, we present an analysis of the model's performance in production and\nits impact on Skyscanner's users. \n\n"}
{"id": "1812.01739", "contents": "Title: Benchmarking Keyword Spotting Efficiency on Neuromorphic Hardware Abstract: Using Intel's Loihi neuromorphic research chip and ABR's Nengo Deep Learning\ntoolkit, we analyze the inference speed, dynamic power consumption, and energy\ncost per inference of a two-layer neural network keyword spotter trained to\nrecognize a single phrase. We perform comparative analyses of this keyword\nspotter running on more conventional hardware devices including a CPU, a GPU,\nNvidia's Jetson TX1, and the Movidius Neural Compute Stick. Our results\nindicate that for this inference application, Loihi outperforms all of these\nalternatives on an energy cost per inference basis while maintaining equivalent\ninference accuracy. Furthermore, an analysis of tradeoffs between network size,\ninference speed, and energy cost indicates that Loihi's comparative advantage\nover other low-power computing devices improves for larger networks. \n\n"}
{"id": "1812.01965", "contents": "Title: Training Competitive Binary Neural Networks from Scratch Abstract: Convolutional neural networks have achieved astonishing results in different\napplication areas. Various methods that allow us to use these models on mobile\nand embedded devices have been proposed. Especially binary neural networks are\na promising approach for devices with low computational power. However,\ntraining accurate binary models from scratch remains a challenge. Previous work\noften uses prior knowledge from full-precision models and complex training\nstrategies. In our work, we focus on increasing the performance of binary\nneural networks without such prior knowledge and a much simpler training\nstrategy. In our experiments we show that we are able to achieve\nstate-of-the-art results on standard benchmark datasets. Further, to the best\nof our knowledge, we are the first to successfully adopt a network architecture\nwith dense connections for binary networks, which lets us improve the\nstate-of-the-art even further. \n\n"}
{"id": "1812.02207", "contents": "Title: Better Trees: An empirical study on hyperparameter tuning of\n  classification decision tree induction algorithms Abstract: Machine learning algorithms often contain many hyperparameters (HPs) whose\nvalues affect the predictive performance of the induced models in intricate\nways. Due to the high number of possibilities for these HP configurations and\ntheir complex interactions, it is common to use optimization techniques to find\nsettings that lead to high predictive performance. However, insights into\nefficiently exploring this vast space of configurations and dealing with the\ntrade-off between predictive and runtime performance remain challenging.\nFurthermore, there are cases where the default HPs fit the suitable\nconfiguration. Additionally, for many reasons, including model validation and\nattendance to new legislation, there is an increasing interest in interpretable\nmodels, such as those created by the Decision Tree (DT) induction algorithms.\nThis paper provides a comprehensive approach for investigating the effects of\nhyperparameter tuning for the two DT induction algorithms most often used, CART\nand C4.5. DT induction algorithms present high predictive performance and\ninterpretable classification models, though many HPs need to be adjusted.\nExperiments were carried out with different tuning strategies to induce models\nand to evaluate HPs' relevance using 94 classification datasets from OpenML.\nThe experimental results point out that different HP profiles for the tuning of\neach algorithm provide statistically significant improvements in most of the\ndatasets for CART, but only in one-third for C4.5. Although different\nalgorithms may present different tuning scenarios, the tuning techniques\ngenerally required few evaluations to find accurate solutions. Furthermore, the\nbest technique for all the algorithms was the IRACE. Finally, we found out that\ntuning a specific small subset of HPs is a good alternative for achieving\noptimal predictive performance. \n\n"}
{"id": "1812.02256", "contents": "Title: Relative Entropy Regularized Policy Iteration Abstract: We present an off-policy actor-critic algorithm for Reinforcement Learning\n(RL) that combines ideas from gradient-free optimization via stochastic search\nwith learned action-value function. The result is a simple procedure consisting\nof three steps: i) policy evaluation by estimating a parametric action-value\nfunction; ii) policy improvement via the estimation of a local non-parametric\npolicy; and iii) generalization by fitting a parametric policy. Each step can\nbe implemented in different ways, giving rise to several algorithm variants.\nOur algorithm draws on connections to existing literature on black-box\noptimization and 'RL as an inference' and it can be seen either as an extension\nof the Maximum a Posteriori Policy Optimisation algorithm (MPO) [Abdolmaleki et\nal., 2018a], or as an extension of Trust Region Covariance Matrix Adaptation\nEvolutionary Strategy (CMA-ES) [Abdolmaleki et al., 2017b; Hansen et al., 1997]\nto a policy iteration scheme. Our comparison on 31 continuous control tasks\nfrom parkour suite [Heess et al., 2017], DeepMind control suite [Tassa et al.,\n2018] and OpenAI Gym [Brockman et al., 2016] with diverse properties, limited\namount of compute and a single set of hyperparameters, demonstrate the\neffectiveness of our method and the state of art results. Videos, summarizing\nresults, can be found at goo.gl/HtvJKR . \n\n"}
{"id": "1812.02271", "contents": "Title: Teacher-Student Compression with Generative Adversarial Networks Abstract: More accurate machine learning models often demand more computation and\nmemory at test time, making them difficult to deploy on CPU- or\nmemory-constrained devices. Teacher-student compression (TSC), also known as\ndistillation, alleviates this burden by training a less expensive student model\nto mimic the expensive teacher model while maintaining most of the original\naccuracy. However, when fresh data is unavailable for the compression task, the\nteacher's training data is typically reused, leading to suboptimal compression.\nIn this work, we propose to augment the compression dataset with synthetic data\nfrom a generative adversarial network (GAN) designed to approximate the\ntraining data distribution. Our GAN-assisted TSC (GAN-TSC) significantly\nimproves student accuracy for expensive models such as large random forests and\ndeep neural networks on both tabular and image datasets. Building on these\nresults, we propose a comprehensive metric---the TSC Score---to evaluate the\nquality of synthetic datasets based on their induced TSC performance. The TSC\nScore captures both data diversity and class affinity, and we illustrate its\nbenefits over the popular Inception Score in the context of image\nclassification. \n\n"}
{"id": "1812.02395", "contents": "Title: Time-Discounting Convolution for Event Sequences with Ambiguous\n  Timestamps Abstract: This paper proposes a method for modeling event sequences with ambiguous\ntimestamps, a time-discounting convolution. Unlike in ordinary time series,\ntime intervals are not constant, small time-shifts have no significant effect,\nand inputting timestamps or time durations into a model is not effective. The\ncriteria that we require for the modeling are providing robustness against\ntime-shifts or timestamps uncertainty as well as maintaining the essential\ncapabilities of time-series models, i.e., forgetting meaningless past\ninformation and handling infinite sequences. The proposed method handles them\nwith a convolutional mechanism across time with specific parameterizations,\nwhich efficiently represents the event dependencies in a time-shift invariant\nmanner while discounting the effect of past events, and a dynamic pooling\nmechanism, which provides robustness against the uncertainty in timestamps and\nenhances the time-discounting capability by dynamically changing the pooling\nwindow size. In our learning algorithm, the decaying and dynamic pooling\nmechanisms play critical roles in handling infinite and variable length\nsequences. Numerical experiments on real-world event sequences with ambiguous\ntimestamps and ordinary time series demonstrated the advantages of our method. \n\n"}
{"id": "1812.02538", "contents": "Title: Energy Efficiency in Reinforcement Learning for Wireless Sensor Networks Abstract: As sensor networks for health monitoring become more prevalent, so will the\nneed to control their usage and consumption of energy. This paper presents a\nmethod which leverages the algorithm's performance and energy consumption. By\nutilising Reinforcement Learning (RL) techniques, we provide an adaptive\nframework, which continuously performs weak training in an energy-aware system.\nWe motivate this using a realistic example of residential localisation based on\nReceived Signal Strength (RSS). The method is cheap in terms of work-hours,\ncalibration and energy usage. It achieves this by utilising other sensors\navailable in the environment. These other sensors provide weak labels, which\nare then used to employ the State-Action-Reward-State-Action (SARSA) algorithm\nand train the model over time. Our approach is evaluated on a simulated\nlocalisation environment and validated on a widely available pervasive health\ndataset which facilitates realistic residential localisation using RSS. We show\nthat our method is cheaper to implement and requires less effort, whilst at the\nsame time providing a performance enhancement and energy savings over time. \n\n"}
{"id": "1812.03190", "contents": "Title: Deep-RBF Networks Revisited: Robust Classification with Rejection Abstract: One of the main drawbacks of deep neural networks, like many other\nclassifiers, is their vulnerability to adversarial attacks. An important reason\nfor their vulnerability is assigning high confidence to regions with few or\neven no feature points. By feature points, we mean a nonlinear transformation\nof the input space extracting a meaningful representation of the input data. On\nthe other hand, deep-RBF networks assign high confidence only to the regions\ncontaining enough feature points, but they have been discounted due to the\nwidely-held belief that they have the vanishing gradient problem. In this\npaper, we revisit the deep-RBF networks by first giving a general formulation\nfor them, and then proposing a family of cost functions thereof inspired by\nmetric learning. In the proposed deep-RBF learning algorithm, the vanishing\ngradient problem does not occur. We make these networks robust to adversarial\nattack by adding the reject option to their output layer. Through several\nexperiments on the MNIST dataset, we demonstrate that our proposed method not\nonly achieves significant classification accuracy but is also very resistant to\nvarious adversarial attacks. \n\n"}
{"id": "1812.03889", "contents": "Title: Regularization by architecture: A deep prior approach for inverse\n  problems Abstract: The present paper studies so-called deep image prior (DIP) techniques in the\ncontext of ill-posed inverse problems. DIP networks have been recently\nintroduced for applications in image processing; also first experimental\nresults for applying DIP to inverse problems have been reported. This paper\naims at discussing different interpretations of DIP and to obtain analytic\nresults for specific network designs and linear operators. The main\ncontribution is to introduce the idea of viewing these approaches as the\noptimization of Tikhonov functionals rather than optimizing networks. Besides\ntheoretical results, we present numerical verifications. \n\n"}
{"id": "1812.04152", "contents": "Title: Duelling Bandits with Weak Regret in Adversarial Environments Abstract: Research on the multi-armed bandit problem has studied the trade-off of\nexploration and exploitation in depth. However, there are numerous applications\nwhere the cardinal absolute-valued feedback model (e.g. ratings from one to\nfive) is not suitable. This has motivated the formulation of the duelling\nbandits problem, where the learner picks a pair of actions and observes a noisy\nbinary feedback, indicating a relative preference between the two. There exist\na multitude of different settings and interpretations of the problem for two\nreasons. First, due to the absence of a total order of actions, there is no\nnatural definition of the best action. Existing work either explicitly assumes\nthe existence of a linear order, or uses a custom definition for the winner.\nSecond, there are multiple reasonable notions of regret to measure the\nlearner's performance. Most prior work has been focussing on the\n$\\textit{strong regret}$, which averages the quality of the two actions picked.\nThis work focusses on the $\\textit{weak regret}$, which is based on the quality\nof the better of the two actions selected. Weak regret is the more appropriate\nperformance measure when the pair's inferior action has no significant\ndetrimental effect on the pair's quality.\n  We study the duelling bandits problem in the adversarial setting. We provide\nan algorithm which has theoretical guarantees in both the utility-based\nsetting, which implies a total order, and the unrestricted setting. For the\nlatter, we work with the $\\textit{Borda winner}$, finding the action maximising\nthe probability of winning against an action sampled uniformly at random. The\nthesis concludes with experimental results based on both real-world data and\nsynthetic data, showing the algorithm's performance and limitations. \n\n"}
{"id": "1812.04529", "contents": "Title: On the Ineffectiveness of Variance Reduced Optimization for Deep\n  Learning Abstract: The application of stochastic variance reduction to optimization has shown\nremarkable recent theoretical and practical success. The applicability of these\ntechniques to the hard non-convex optimization problems encountered during\ntraining of modern deep neural networks is an open problem. We show that naive\napplication of the SVRG technique and related approaches fail, and explore why. \n\n"}
{"id": "1812.04634", "contents": "Title: On the Curved Geometry of Accelerated Optimization Abstract: In this work we propose a differential geometric motivation for Nesterov's\naccelerated gradient method (AGM) for strongly-convex problems. By considering\nthe optimization procedure as occurring on a Riemannian manifold with a natural\nstructure, The AGM method can be seen as the proximal point method applied in\nthis curved space. This viewpoint can also be extended to the continuous time\ncase, where the accelerated gradient method arises from the natural\nblock-implicit Euler discretization of an ODE on the manifold. We provide an\nanalysis of the convergence rate of this ODE for quadratic objectives. \n\n"}
{"id": "1812.05248", "contents": "Title: Shortcut Matrix Product States and its applications Abstract: Matrix Product States (MPS), also known as Tensor Train (TT) decomposition in\nmathematics, has been proposed originally for describing an (especially\none-dimensional) quantum system, and recently has found applications in various\napplications such as compressing high-dimensional data, supervised kernel\nlinear classifier, and unsupervised generative modeling. However, when applied\nto systems which are not defined on one-dimensional lattices, a serious\ndrawback of the MPS is the exponential decay of the correlations, which limits\nits power in capturing long-range dependences among variables in the system. To\nalleviate this problem, we propose to introduce long-range interactions, which\nact as shortcuts, to MPS, resulting in a new model \\textit{ Shortcut Matrix\nProduct States} (SMPS). When chosen properly, the shortcuts can decrease\nsignificantly the correlation length of the MPS, while preserving the\ncomputational efficiency. We develop efficient training methods of SMPS for\nvarious tasks, establish some of their mathematical properties, and show how to\nfind a good location to add shortcuts. Finally, using extensive numerical\nexperiments we evaluate its performance in a variety of applications, including\nfunction fitting, partition function calculation of $2-$d Ising model, and\nunsupervised generative modeling of handwritten digits, to illustrate its\nadvantages over vanilla matrix product states. \n\n"}
{"id": "1812.05737", "contents": "Title: Effectiveness of Hierarchical Softmax in Large Scale Classification\n  Tasks Abstract: Typically, Softmax is used in the final layer of a neural network to get a\nprobability distribution for output classes. But the main problem with Softmax\nis that it is computationally expensive for large scale data sets with large\nnumber of possible outputs. To approximate class probability efficiently on\nsuch large scale data sets we can use Hierarchical Softmax. LSHTC datasets were\nused to study the performance of the Hierarchical Softmax. LSHTC datasets have\nlarge number of categories. In this paper we evaluate and report the\nperformance of normal Softmax Vs Hierarchical Softmax on LSHTC datasets. This\nevaluation used macro f1 score as a performance measure. The observation was\nthat the performance of Hierarchical Softmax degrades as the number of classes\nincrease. \n\n"}
{"id": "1812.06181", "contents": "Title: Efficient Interpretation of Deep Learning Models Using Graph Structure\n  and Cooperative Game Theory: Application to ASD Biomarker Discovery Abstract: Discovering imaging biomarkers for autism spectrum disorder (ASD) is critical\nto help explain ASD and predict or monitor treatment outcomes. Toward this end,\ndeep learning classifiers have recently been used for identifying ASD from\nfunctional magnetic resonance imaging (fMRI) with higher accuracy than\ntraditional learning strategies. However, a key challenge with deep learning\nmodels is understanding just what image features the network is using, which\ncan in turn be used to define the biomarkers. Current methods extract\nbiomarkers, i.e., important features, by looking at how the prediction changes\nif \"ignoring\" one feature at a time. In this work, we go beyond looking at only\nindividual features by using Shapley value explanation (SVE) from cooperative\ngame theory. Cooperative game theory is advantageous here because it directly\nconsiders the interaction between features and can be applied to any machine\nlearning method, making it a novel, more accurate way of determining\ninstance-wise biomarker importance from deep learning models. A barrier to\nusing SVE is its computational complexity: $2^N$ given $N$ features. We\nexplicitly reduce the complexity of SVE computation by two approaches based on\nthe underlying graph structure of the input data: 1) only consider the\ncentralized coalition of each feature; 2) a hierarchical pipeline which first\nclusters features into small communities, then applies SVE in each community.\nMonte Carlo approximation can be used for large permutation sets. We first\nvalidate our methods on the MNIST dataset and compare to human perception.\nNext, to insure plausibility of our biomarker results, we train a Random Forest\n(RF) to classify ASD/control subjects from fMRI and compare SVE results to\nstandard RF-based feature importance. Finally, we show initial results on\nranked fMRI biomarkers using SVE on a deep learning classifier for the\nASD/control dataset. \n\n"}
{"id": "1812.06303", "contents": "Title: Multi-Tasking Genetic Algorithm (MTGA) for Fuzzy System Optimization Abstract: Multi-task learning uses auxiliary data or knowledge from relevant tasks to\nfacilitate the learning in a new task. Multi-task optimization applies\nmulti-task learning to optimization to study how to effectively and efficiently\ntackle multiple optimization problems simultaneously. Evolutionary\nmulti-tasking, or multi-factorial optimization, is an emerging subfield of\nmulti-task optimization, which integrates evolutionary computation and\nmulti-task learning. This paper proposes a novel and easy-to-implement\nmulti-tasking genetic algorithm (MTGA), which copes well with significantly\ndifferent optimization tasks by estimating and using the bias among them.\nComparative studies with eight state-of-the-art single- and multi-task\napproaches in the literature on nine benchmarks demonstrated that on average\nthe MTGA outperformed all of them, and had lower computational cost than six of\nthem. Based on the MTGA, a simultaneous optimization strategy for fuzzy system\ndesign is also proposed. Experiments on simultaneous optimization of type-1 and\ninterval type-2 fuzzy logic controllers for couple-tank water level control\ndemonstrated that the MTGA can find better fuzzy logic controllers than other\napproaches. \n\n"}
{"id": "1812.06408", "contents": "Title: Human Pose and Path Estimation from Aerial Video using Dynamic\n  Classifier Selection Abstract: We consider the problem of estimating human pose and trajectory by an aerial\nrobot with a monocular camera in near real time. We present a preliminary\nsolution whose distinguishing feature is a dynamic classifier selection\narchitecture. In our solution, each video frame is corrected for perspective\nusing projective transformation. Then, two alternative feature sets are used:\n(i) Histogram of Oriented Gradients (HOG) of the silhouette, (ii) Convolutional\nNeural Network (CNN) features of the RGB image. The features (HOG or CNN) are\nclassified using a dynamic classifier. A class is defined as a pose-viewpoint\npair, and a total of 64 classes are defined to represent a forward walking and\nturning gait sequence. Our solution provides three main advantages: (i)\nClassification is efficient due to dynamic selection (4-class vs. 64-class\nclassification). (ii) Classification errors are confined to neighbors of the\ntrue view-points. (iii) The robust temporal relationship between poses is used\nto resolve the left-right ambiguities of human silhouettes. Experiments\nconducted on both fronto-parallel videos and aerial videos confirm our solution\ncan achieve accurate pose and trajectory estimation for both scenarios. We\nfound using HOG features provides higher accuracy than using CNN features. For\nexample, applying the HOG-based variant of our scheme to the 'walking on a\nfigure 8-shaped path' dataset (1652 frames) achieved estimation accuracies of\n99.6% for viewpoints and 96.2% for number of poses. \n\n"}
{"id": "1812.06707", "contents": "Title: Not Using the Car to See the Sidewalk: Quantifying and Controlling the\n  Effects of Context in Classification and Segmentation Abstract: Importance of visual context in scene understanding tasks is well recognized\nin the computer vision community. However, to what extent the computer vision\nmodels for image classification and semantic segmentation are dependent on the\ncontext to make their predictions is unclear. A model overly relying on context\nwill fail when encountering objects in context distributions different from\ntraining data and hence it is important to identify these dependencies before\nwe can deploy the models in the real-world. We propose a method to quantify the\nsensitivity of black-box vision models to visual context by editing images to\nremove selected objects and measuring the response of the target models. We\napply this methodology on two tasks, image classification and semantic\nsegmentation, and discover undesirable dependency between objects and context,\nfor example that \"sidewalk\" segmentation relies heavily on \"cars\" being present\nin the image. We propose an object removal based data augmentation solution to\nmitigate this dependency and increase the robustness of classification and\nsegmentation models to contextual variations. Our experiments show that the\nproposed data augmentation helps these models improve the performance in\nout-of-context scenarios, while preserving the performance on regular data. \n\n"}
{"id": "1812.07051", "contents": "Title: Unsupervised Single Image Dehazing Using Dark Channel Prior Loss Abstract: Single image dehazing is a critical stage in many modern-day autonomous\nvision applications. Early prior-based methods often involved a time-consuming\nminimization of a hand-crafted energy function. Recent learning-based\napproaches utilize the representational power of deep neural networks (DNNs) to\nlearn the underlying transformation between hazy and clear images. Due to\ninherent limitations in collecting matching clear and hazy images, these\nmethods resort to training on synthetic data; constructed from indoor images\nand corresponding depth information. This may result in a possible domain shift\nwhen treating outdoor scenes. We propose a completely unsupervised method of\ntraining via minimization of the well-known, Dark Channel Prior (DCP) energy\nfunction. Instead of feeding the network with synthetic data, we solely use\nreal-world outdoor images and tune the network's parameters by directly\nminimizing the DCP. Although our \"Deep DCP\" technique can be regarded as a fast\napproximator of DCP, it actually improves its results significantly. This\nsuggests an additional regularization obtained via the network and learning\nprocess. Experiments show that our method performs on par with large-scale\nsupervised methods. \n\n"}
{"id": "1812.07060", "contents": "Title: Channel-wise pruning of neural networks with tapering resource\n  constraint Abstract: Neural network pruning is an important step in design process of efficient\nneural networks for edge devices with limited computational power. Pruning is a\nform of knowledge transfer from the weights of the original network to a\nsmaller target subnetwork. We propose a new method for compute-constrained\nstructured channel-wise pruning of convolutional neural networks. The method\niteratively fine-tunes the network, while gradually tapering the computation\nresources available to the pruned network via a holonomic constraint in the\nmethod of Lagrangian multipliers framework. An explicit and adaptive automatic\ncontrol over the rate of tapering is provided. The trainable parameters of our\npruning method are separate from the weights of the neural network, which\nallows us to avoid the interference with the neural network solver (e.g. avoid\nthe direct dependence of pruning speed on neural network learning rates). Our\nmethod combines the `rigoristic' approach by the direct application of\nconstrained optimization, avoiding the pitfalls of ADMM-based methods, like\ntheir need to define the target amount of resources for each pruning run, and\ndirect dependence of pruning speed and priority of pruning on the relative\nscale of weights between layers. For VGG-16 @ ILSVRC-2012, we achieve reduction\nof 15.47 -> 3.87 GMAC with only 1% top-1 accuracy reduction (68.4% -> 67.4%).\nFor AlexNet @ ILSVRC-2012, we achieve 0.724 -> 0.411 GMAC with 1% top-1\naccuracy reduction (56.8% -> 55.8%). \n\n"}
{"id": "1812.07101", "contents": "Title: Application of Deep Learning in Fundus Image Processing for Ophthalmic\n  Diagnosis -- A Review Abstract: An overview of the applications of deep learning in ophthalmic diagnosis\nusing retinal fundus images is presented. We also review various retinal image\ndatasets that can be used for deep learning purposes. Applications of deep\nlearning for segmentation of optic disk, blood vessels and retinal layer as\nwell as detection of lesions are reviewed. Recent deep learning models for\nclassification of diseases such as age-related macular degeneration,\nglaucoma,diabetic macular edema and diabetic retinopathy are also reported. \n\n"}
{"id": "1812.07102", "contents": "Title: Deep Learning with Attention to Predict Gestational Age of the Fetal\n  Brain Abstract: Fetal brain imaging is a cornerstone of prenatal screening and early\ndiagnosis of congenital anomalies. Knowledge of fetal gestational age is the\nkey to the accurate assessment of brain development. This study develops an\nattention-based deep learning model to predict gestational age of the fetal\nbrain. The proposed model is an end-to-end framework that combines key insights\nfrom multi-view MRI including axial, coronal, and sagittal views. The model\nalso uses age-activated weakly-supervised attention maps to enable\nrotation-invariant localization of the fetal brain among background noise. We\nevaluate our methods on the collected fetal brain MRI cohort with a large age\ndistribution from 125 to 273 days. Our extensive experiments show age\nprediction performance with R2 = 0.94 using multi-view MRI and attention. \n\n"}
{"id": "1812.07484", "contents": "Title: Efficient Autotuning of Hyperparameters in Approximate Nearest Neighbor\n  Search Abstract: Approximate nearest neighbor algorithms are used to speed up nearest neighbor\nsearch in a wide array of applications. However, current indexing methods\nfeature several hyperparameters that need to be tuned to reach an acceptable\naccuracy--speed trade-off. A grid search in the parameter space is often\nimpractically slow due to a time-consuming index-building procedure. Therefore,\nwe propose an algorithm for automatically tuning the hyperparameters of\nindexing methods based on randomized space-partitioning trees. In particular,\nwe present results using randomized k-d trees, random projection trees and\nrandomized PCA trees. The tuning algorithm adds minimal overhead to the\nindex-building process but is able to find the optimal hyperparameters\naccurately. We demonstrate that the algorithm is significantly faster than\nexisting approaches, and that the indexing methods used are competitive with\nthe state-of-the-art methods in query time while being faster to build. \n\n"}
{"id": "1812.07676", "contents": "Title: Molecular Dynamics with Neural-Network Potentials Abstract: Molecular dynamics simulations are an important tool for describing the\nevolution of a chemical system with time. However, these simulations are\ninherently held back either by the prohibitive cost of accurate electronic\nstructure theory computations or the limited accuracy of classical empirical\nforce fields. Machine learning techniques can help to overcome these\nlimitations by providing access to potential energies, forces and other\nmolecular properties modeled directly after an electronic structure reference\nat only a fraction of the original computational cost. The present text\ndiscusses several practical aspects of conducting machine learning driven\nmolecular dynamics simulations. First, we study the efficient selection of\nreference data points on the basis of an active learning inspired adaptive\nsampling scheme. This is followed by the analysis of a machine-learning based\nmodel for simulating molecular dipole moments in the framework of predicting\ninfrared spectra via molecular dynamics simulations. Finally, we show that\nmachine learning models can offer valuable aid in understanding chemical\nsystems beyond a simple prediction of quantities. \n\n"}
{"id": "1812.07738", "contents": "Title: Max-Diversity Distributed Learning: Theory and Algorithms Abstract: We study the risk performance of distributed learning for the regularization\nempirical risk minimization with fast convergence rate, substantially improving\nthe error analysis of the existing divide-and-conquer based distributed\nlearning. An interesting theoretical finding is that the larger the diversity\nof each local estimate is, the tighter the risk bound is. This theoretical\nanalysis motivates us to devise an effective maxdiversity distributed learning\nalgorithm (MDD). Experimental results show that MDD can outperform the existing\ndivide-andconquer methods but with a bit more time. Theoretical analysis and\nempirical results demonstrate that our proposed MDD is sound and effective. \n\n"}
{"id": "1812.07809", "contents": "Title: Found in Translation: Learning Robust Joint Representations by Cyclic\n  Translations Between Modalities Abstract: Multimodal sentiment analysis is a core research area that studies speaker\nsentiment expressed from the language, visual, and acoustic modalities. The\ncentral challenge in multimodal learning involves inferring joint\nrepresentations that can process and relate information from these modalities.\nHowever, existing work learns joint representations by requiring all modalities\nas input and as a result, the learned representations may be sensitive to noisy\nor missing modalities at test time. With the recent success of sequence to\nsequence (Seq2Seq) models in machine translation, there is an opportunity to\nexplore new ways of learning joint representations that may not require all\ninput modalities at test time. In this paper, we propose a method to learn\nrobust joint representations by translating between modalities. Our method is\nbased on the key insight that translation from a source to a target modality\nprovides a method of learning joint representations using only the source\nmodality as input. We augment modality translations with a cycle consistency\nloss to ensure that our joint representations retain maximal information from\nall modalities. Once our translation model is trained with paired multimodal\ndata, we only need data from the source modality at test time for final\nsentiment prediction. This ensures that our model remains robust from\nperturbations or missing information in the other modalities. We train our\nmodel with a coupled translation-prediction objective and it achieves new\nstate-of-the-art results on multimodal sentiment analysis datasets: CMU-MOSI,\nICT-MMMO, and YouTube. Additional experiments show that our model learns\nincreasingly discriminative joint representations with more input modalities\nwhile maintaining robustness to missing or perturbed modalities. \n\n"}
{"id": "1812.08040", "contents": "Title: Credibility evaluation of income data with hierarchical correlation\n  reconstruction Abstract: In situations like tax declarations or analyzes of household budgets we would\nlike to automatically evaluate credibility of exogenous variable (declared\nincome) based on some available (endogenous) variables - we want to build a\nmodel and train it on provided data sample to predict (conditional) probability\ndistribution of exogenous variable based on values of endogenous variables.\nUsing Polish household budget survey data there will be discussed simple and\nsystematic adaptation of hierarchical correlation reconstruction (HCR)\ntechnique for this purpose, which allows to combine interpretability of\nstatistics with modelling of complex densities like in machine learning. For\ncredibility evaluation we normalize marginal distribution of predicted variable\nto $\\rho\\approx 1$ uniform distribution on $[0,1]$ using empirical distribution\nfunction $(x=EDF(y)\\in[0,1])$, then model density of its conditional\ndistribution $(\\textrm{Pr}(x_0|x_1 x_2\\ldots))$ as a linear combination of\northonormal polynomials using coefficients modelled as linear combinations of\nfeatures of the remaining variables. These coefficients can be calculated\nindependently, have similar interpretation as cumulants, additionally allowing\nto directly reconstruct probability distribution. Values corresponding to high\npredicted density can be considered as credible, while low density suggests\ndisagreement with statistics of data sample, for example to mark for manual\nverification a chosen percentage of data points evaluated as the least\ncredible. \n\n"}
{"id": "1812.08113", "contents": "Title: On The Chain Rule Optimal Transport Distance Abstract: We define a novel class of distances between statistical multivariate\ndistributions by modeling an optimal transport problem on their marginals with\nrespect to a ground distance defined on their conditionals. These new distances\nare metrics whenever the ground distance between the marginals is a metric,\ngeneralize both the Wasserstein distances between discrete measures and a\nrecently introduced metric distance between statistical mixtures, and provide\nan upper bound for jointly convex distances between statistical mixtures. By\nentropic regularization of the optimal transport, we obtain a fast\ndifferentiable Sinkhorn-type distance. We experimentally evaluate our new\nfamily of distances by quantifying the upper bounds of several jointly convex\ndistances between statistical mixtures, and by proposing a novel efficient\nmethod to learn Gaussian mixture models (GMMs) by simplifying kernel density\nestimators with respect to our distance. Our GMM learning technique\nexperimentally improves significantly over the EM implementation of {\\tt\nsklearn} on the {\\tt MNIST} and {\\tt Fashion MNIST} datasets. \n\n"}
{"id": "1812.08555", "contents": "Title: Adversarial Signal Denoising with Encoder-Decoder Networks Abstract: The presence of noise is common in signal processing regardless the signal\ntype. Deep neural networks have shown good performance in noise removal,\nespecially on the image domain. In this work, we consider deep neural networks\nas a denoising tool where our focus is on one dimensional signals. We introduce\nan encoder-decoder architecture to denoise signals, represented by a sequence\nof measurements. Instead of relying only on the standard reconstruction error\nto train the encoder-decoder network, we treat the task of denoising as\ndistribution alignment between the clean and noisy signals. Then, we propose an\nadversarial learning formulation where the goal is to align the clean and noisy\nsignal latent representation given that both signals pass through the encoder.\nIn our approach, the discriminator has the role of detecting whether the latent\nrepresentation comes from clean or noisy signals. We evaluate on\nelectrocardiogram and motion signal denoising; and show better performance than\nlearning-based and non-learning approaches. \n\n"}
{"id": "1812.08861", "contents": "Title: Animating Arbitrary Objects via Deep Motion Transfer Abstract: This paper introduces a novel deep learning framework for image animation.\nGiven an input image with a target object and a driving video sequence\ndepicting a moving object, our framework generates a video in which the target\nobject is animated according to the driving sequence. This is achieved through\na deep architecture that decouples appearance and motion information. Our\nframework consists of three main modules: (i) a Keypoint Detector unsupervisely\ntrained to extract object keypoints, (ii) a Dense Motion prediction network for\ngenerating dense heatmaps from sparse keypoints, in order to better encode\nmotion information and (iii) a Motion Transfer Network, which uses the motion\nheatmaps and appearance information extracted from the input image to\nsynthesize the output frames. We demonstrate the effectiveness of our method on\nseveral benchmark datasets, spanning a wide variety of object appearances, and\nshow that our approach outperforms state-of-the-art image animation and video\ngeneration methods. Our source code is publicly available. \n\n"}
{"id": "1812.08997", "contents": "Title: Stochastic Doubly Robust Gradient Abstract: When training a machine learning model with observational data, it is often\nencountered that some values are systemically missing. Learning from the\nincomplete data in which the missingness depends on some covariates may lead to\nbiased estimation of parameters and even harm the fairness of decision outcome.\nThis paper proposes how to adjust the causal effect of covariates on the\nmissingness when training models using stochastic gradient descent (SGD).\nInspired by the design of doubly robust estimator and its theoretical property\nof double robustness, we introduce stochastic doubly robust gradient (SDRG)\nconsisting of two models: weight-corrected gradients for inverse propensity\nscore weighting and per-covariate control variates for regression adjustment.\nAlso, we identify the connection between double robustness and variance\nreduction in SGD by demonstrating the SDRG algorithm with a unifying framework\nfor variance reduced SGD. The performance of our approach is empirically tested\nby showing the convergence in training image classifiers with several examples\nof missing data. \n\n"}
{"id": "1812.09520", "contents": "Title: Universal Supervised Learning for Individual Data Abstract: Universal supervised learning is considered from an information theoretic\npoint of view following the universal prediction approach, see Merhav and Feder\n(1998). We consider the standard supervised \"batch\" learning where prediction\nis done on a test sample once the entire training data is observed, and the\nindividual setting where the features and labels, both in the training and\ntest, are specific individual quantities. The information theoretic approach\nnaturally uses the self-information loss or log-loss. Our results provide\nuniversal learning schemes that compete with a \"genie\" (or reference) that\nknows the true test label. In particular, it is demonstrated that the main\nproposed scheme, termed Predictive Normalized Maximum Likelihood (pNML), is a\nrobust learning solution that outperforms the current leading approach based on\nEmpirical Risk Minimization (ERM). Furthermore, the pNML construction provides\na pointwise indication for the learnability of the specific test challenge with\nthe given training examples \n\n"}
{"id": "1812.10252", "contents": "Title: Optimizing Market Making using Multi-Agent Reinforcement Learning Abstract: In this paper, reinforcement learning is applied to the problem of optimizing\nmarket making. A multi-agent reinforcement learning framework is used to\noptimally place limit orders that lead to successful trades. The framework\nconsists of two agents. The macro-agent optimizes on making the decision to\nbuy, sell, or hold an asset. The micro-agent optimizes on placing limit orders\nwithin the limit order book. For the context of this paper, the proposed\nframework is applied and studied on the Bitcoin cryptocurrency market. The goal\nof this paper is to show that reinforcement learning is a viable strategy that\ncan be applied to complex problems (with complex environments) such as market\nmaking. \n\n"}
{"id": "1812.10962", "contents": "Title: A Variational Topological Neural Model for Cascade-based Diffusion in\n  Networks Abstract: Many works have been proposed in the literature to capture the dynamics of\ndiffusion in networks. While some of them define graphical markovian models to\nextract temporal relationships between node infections in networks, others\nconsider diffusion episodes as sequences of infections via recurrent neural\nmodels. In this paper we propose a model at the crossroads of these two\nextremes, which embeds the history of diffusion in infected nodes as hidden\ncontinuous states. Depending on the trajectory followed by the content before\nreaching a given node, the distribution of influence probabilities may vary.\nHowever, content trajectories are usually hidden in the data, which induces\nchallenging learning problems. We propose a topological recurrent neural model\nwhich exhibits good experimental performances for diffusion modelling and\nprediction. \n\n"}
{"id": "1812.11295", "contents": "Title: Monocular 3D Pose Recovery via Nonconvex Sparsity with Theoretical\n  Analysis Abstract: For recovering 3D object poses from 2D images, a prevalent method is to\npre-train an over-complete dictionary $\\mathcal D=\\{B_i\\}_i^D$ of 3D basis\nposes. During testing, the detected 2D pose $Y$ is matched to dictionary by $Y\n\\approx \\sum_i M_i B_i$ where $\\{M_i\\}_i^D=\\{c_i \\Pi R_i\\}$, by estimating the\nrotation $R_i$, projection $\\Pi$ and sparse combination coefficients $c \\in\n\\mathbb R_{+}^D$. In this paper, we propose non-convex regularization $H(c)$ to\nlearn coefficients $c$, including novel leaky capped $\\ell_1$-norm\nregularization (LCNR), \\begin{align*} H(c)=\\alpha \\sum_{i } \\min(|c_i|,\\tau)+\n\\beta \\sum_{i } \\max(| c_i|,\\tau), \\end{align*} where $0\\leq \\beta \\leq \\alpha$\nand $0<\\tau$ is a certain threshold, so the invalid components smaller than\n$\\tau$ are composed with larger regularization and other valid components with\nsmaller regularization. We propose a multi-stage optimizer with convex\nrelaxation and ADMM. We prove that the estimation error $\\mathcal L(l)$ decays\nw.r.t. the stages $l$, \\begin{align*} Pr\\left(\\mathcal L(l) < \\rho^{l-1}\n\\mathcal L(0) + \\delta \\right) \\geq 1- \\epsilon, \\end{align*} where $0< \\rho\n<1, 0<\\delta, 0<\\epsilon \\ll 1$. Experiments on large 3D human datasets like\nH36M are conducted to support our improvement upon previous approaches. To the\nbest of our knowledge, this is the first theoretical analysis in this line of\nresearch, to understand how the recovery error is affected by fundamental\nfactors, e.g. dictionary size, observation noises, optimization times. We\ncharacterize the trade-off between speed and accuracy towards real-time\ninference in applications. \n\n"}
{"id": "1812.11440", "contents": "Title: Brain MRI super-resolution using 3D generative adversarial networks Abstract: In this work we propose an adversarial learning approach to generate high\nresolution MRI scans from low resolution images. The architecture, based on the\nSRGAN model, adopts 3D convolutions to exploit volumetric information. For the\ndiscriminator, the adversarial loss uses least squares in order to stabilize\nthe training. For the generator, the loss function is a combination of a least\nsquares adversarial loss and a content term based on mean square error and\nimage gradients in order to improve the quality of the generated images. We\nexplore different solutions for the upsampling phase. We present promising\nresults that improve classical interpolation, showing the potential of the\napproach for 3D medical imaging super-resolution. Source code available at\nhttps://github.com/imatge-upc/3D-GAN-superresolution \n\n"}
{"id": "1901.00130", "contents": "Title: Realizing data features by deep nets Abstract: This paper considers the power of deep neural networks (deep nets for short)\nin realizing data features. Based on refined covering number estimates, we find\nthat, to realize some complex data features, deep nets can improve the\nperformances of shallow neural networks (shallow nets for short) without\nrequiring additional capacity costs. This verifies the advantage of deep nets\nin realizing complex features. On the other hand, to realize some simple data\nfeature like the smoothness, we prove that, up to a logarithmic factor, the\napproximation rate of deep nets is asymptotically identical to that of shallow\nnets, provided that the depth is fixed. This exhibits a limitation of deep nets\nin realizing simple features. \n\n"}
{"id": "1901.00248", "contents": "Title: A Survey on Multi-output Learning Abstract: Multi-output learning aims to simultaneously predict multiple outputs given\nan input. It is an important learning problem due to the pressing need for\nsophisticated decision making in real-world applications. Inspired by big data,\nthe 4Vs characteristics of multi-output imposes a set of challenges to\nmulti-output learning, in terms of the volume, velocity, variety and veracity\nof the outputs. Increasing number of works in the literature have been devoted\nto the study of multi-output learning and the development of novel approaches\nfor addressing the challenges encountered. However, it lacks a comprehensive\noverview on different types of challenges of multi-output learning brought by\nthe characteristics of the multiple outputs and the techniques proposed to\novercome the challenges. This paper thus attempts to fill in this gap to\nprovide a comprehensive review on this area. We first introduce different\nstages of the life cycle of the output labels. Then we present the paradigm on\nmulti-output learning, including its myriads of output structures, definitions\nof its different sub-problems, model evaluation metrics and popular data\nrepositories used in the study. Subsequently, we review a number of\nstate-of-the-art multi-output learning methods, which are categorized based on\nthe challenges. \n\n"}
{"id": "1901.00398", "contents": "Title: Judge the Judges: A Large-Scale Evaluation Study of Neural Language\n  Models for Online Review Generation Abstract: We conduct a large-scale, systematic study to evaluate the existing\nevaluation methods for natural language generation in the context of generating\nonline product reviews. We compare human-based evaluators with a variety of\nautomated evaluation procedures, including discriminative evaluators that\nmeasure how well machine-generated text can be distinguished from human-written\ntext, as well as word overlap metrics that assess how similar the generated\ntext compares to human-written references. We determine to what extent these\ndifferent evaluators agree on the ranking of a dozen of state-of-the-art\ngenerators for online product reviews. We find that human evaluators do not\ncorrelate well with discriminative evaluators, leaving a bigger question of\nwhether adversarial accuracy is the correct objective for natural language\ngeneration. In general, distinguishing machine-generated text is challenging\neven for human evaluators, and human decisions correlate better with lexical\noverlaps. We find lexical diversity an intriguing metric that is indicative of\nthe assessments of different evaluators. A post-experiment survey of\nparticipants provides insights into how to evaluate and improve the quality of\nnatural language generation systems. \n\n"}
{"id": "1901.00861", "contents": "Title: Mapping Informal Settlements in Developing Countries using Machine\n  Learning and Low Resolution Multi-spectral Data Abstract: Informal settlements are home to the most socially and economically\nvulnerable people on the planet. In order to deliver effective economic and\nsocial aid, non-government organizations (NGOs), such as the United Nations\nChildren's Fund (UNICEF), require detailed maps of the locations of informal\nsettlements. However, data regarding informal and formal settlements is\nprimarily unavailable and if available is often incomplete. This is due, in\npart, to the cost and complexity of gathering data on a large scale. To address\nthese challenges, we, in this work, provide three contributions. 1) A brand new\nmachine learning data-set, purposely developed for informal settlement\ndetection. 2) We show that it is possible to detect informal settlements using\nfreely available low-resolution (LR) data, in contrast to previous studies that\nuse very-high resolution (VHR) satellite and aerial imagery, something that is\ncost-prohibitive for NGOs. 3) We demonstrate two effective classification\nschemes on our curated data set, one that is cost-efficient for NGOs and\nanother that is cost-prohibitive for NGOs, but has additional utility. We\nintegrate these schemes into a semi-automated pipeline that converts either a\nLR or VHR satellite image into a binary map that encodes the locations of\ninformal settlements. \n\n"}
{"id": "1901.02217", "contents": "Title: Tree Tensor Networks for Generative Modeling Abstract: Matrix product states (MPS), a tensor network designed for one-dimensional\nquantum systems, has been recently proposed for generative modeling of natural\ndata (such as images) in terms of `Born machine'. However, the exponential\ndecay of correlation in MPS restricts its representation power heavily for\nmodeling complex data such as natural images. In this work, we push forward the\neffort of applying tensor networks to machine learning by employing the Tree\nTensor Network (TTN) which exhibits balanced performance in expressibility and\nefficient training and sampling. We design the tree tensor network to utilize\nthe 2-dimensional prior of the natural images and develop sweeping learning and\nsampling algorithms which can be efficiently implemented utilizing Graphical\nProcessing Units (GPU). We apply our model to random binary patterns and the\nbinary MNIST datasets of handwritten digits. We show that TTN is superior to\nMPS for generative modeling in keeping correlation of pixels in natural images,\nas well as giving better log-likelihood scores in standard datasets of\nhandwritten digits. We also compare its performance with state-of-the-art\ngenerative models such as the Variational AutoEncoders, Restricted Boltzmann\nmachines, and PixelCNN. Finally, we discuss the future development of Tensor\nNetwork States in machine learning problems. \n\n"}
{"id": "1901.02324", "contents": "Title: Learning with Fenchel-Young Losses Abstract: Over the past decades, numerous loss functions have been been proposed for a\nvariety of supervised learning tasks, including regression, classification,\nranking, and more generally structured prediction. Understanding the core\nprinciples and theoretical properties underpinning these losses is key to\nchoose the right loss for the right problem, as well as to create new losses\nwhich combine their strengths. In this paper, we introduce Fenchel-Young\nlosses, a generic way to construct a convex loss function for a regularized\nprediction function. We provide an in-depth study of their properties in a very\nbroad setting, covering all the aforementioned supervised learning tasks, and\nrevealing new connections between sparsity, generalized entropies, and\nseparation margins. We show that Fenchel-Young losses unify many well-known\nloss functions and allow to create useful new ones easily. Finally, we derive\nefficient predictive and training algorithms, making Fenchel-Young losses\nappealing both in theory and practice. \n\n"}
{"id": "1901.02352", "contents": "Title: Auto-weighted Mutli-view Sparse Reconstructive Embedding Abstract: With the development of multimedia era, multi-view data is generated in\nvarious fields. Contrast with those single-view data, multi-view data brings\nmore useful information and should be carefully excavated. Therefore, it is\nessential to fully exploit the complementary information embedded in multiple\nviews to enhance the performances of many tasks. Especially for those\nhigh-dimensional data, how to develop a multi-view dimension reduction\nalgorithm to obtain the low-dimensional representations is of vital importance\nbut chanllenging. In this paper, we propose a novel multi-view dimensional\nreduction algorithm named Auto-weighted Mutli-view Sparse Reconstructive\nEmbedding (AMSRE) to deal with this problem. AMSRE fully exploits the sparse\nreconstructive correlations between features from multiple views. Furthermore,\nit is equipped with an auto-weighted technique to treat multiple views\ndiscriminatively according to their contributions. Various experiments have\nverified the excellent performances of the proposed AMSRE. \n\n"}
{"id": "1901.02511", "contents": "Title: Multi-stream CNN based Video Semantic Segmentation for Automated Driving Abstract: Majority of semantic segmentation algorithms operate on a single frame even\nin the case of videos. In this work, the goal is to exploit temporal\ninformation within the algorithm model for leveraging motion cues and temporal\nconsistency. We propose two simple high-level architectures based on Recurrent\nFCN (RFCN) and Multi-Stream FCN (MSFCN) networks. In case of RFCN, a recurrent\nnetwork namely LSTM is inserted between the encoder and decoder. MSFCN combines\nthe encoders of different frames into a fused encoder via 1x1 channel-wise\nconvolution. We use a ResNet50 network as the baseline encoder and construct\nthree networks namely MSFCN of order 2 & 3 and RFCN of order 2. MSFCN-3\nproduces the best results with an accuracy improvement of 9% and 15% for\nHighway and New York-like city scenarios in the SYNTHIA-CVPR'16 dataset using\nmean IoU metric. MSFCN-3 also produced 11% and 6% for SegTrack V2 and DAVIS\ndatasets over the baseline FCN network. We also designed an efficient version\nof MSFCN-2 and RFCN-2 using weight sharing among the two encoders. The\nefficient MSFCN-2 provided an improvement of 11% and 5% for KITTI and SYNTHIA\nwith negligible increase in computational complexity compared to the baseline\nversion. \n\n"}
{"id": "1901.02871", "contents": "Title: The Lingering of Gradients: Theory and Applications Abstract: Classically, the time complexity of a first-order method is estimated by its\nnumber of gradient computations. In this paper, we study a more refined\ncomplexity by taking into account the `lingering' of gradients: once a gradient\nis computed at $x_k$, the additional time to compute gradients at\n$x_{k+1},x_{k+2},\\dots$ may be reduced.\n  We show how this improves the running time of several first-order methods.\nFor instance, if the `additional time' scales linearly with respect to the\ntraveled distance, then the `convergence rate' of gradient descent can be\nimproved from $1/T$ to $\\exp(-T^{1/3})$. On the application side, we solve a\nhypothetical revenue management problem on the Yahoo! Front Page Today Module\nwith 4.6m users to $10^{-6}$ error using only 6 passes of the dataset; and\nsolve a real-life support vector machine problem to an accuracy that is two\norders of magnitude better comparing to the state-of-the-art algorithm. \n\n"}
{"id": "1901.03665", "contents": "Title: A Biologically Inspired Visual Working Memory for Deep Networks Abstract: The ability to look multiple times through a series of pose-adjusted glimpses\nis fundamental to human vision. This critical faculty allows us to understand\nhighly complex visual scenes. Short term memory plays an integral role in\naggregating the information obtained from these glimpses and informing our\ninterpretation of the scene. Computational models have attempted to address\nglimpsing and visual attention but have failed to incorporate the notion of\nmemory. We introduce a novel, biologically inspired visual working memory\narchitecture that we term the Hebb-Rosenblatt memory. We subsequently introduce\na fully differentiable Short Term Attentive Working Memory model (STAWM) which\nuses transformational attention to learn a memory over each image it sees. The\nstate of our Hebb-Rosenblatt memory is embedded in STAWM as the weights space\nof a layer. By projecting different queries through this layer we can obtain\ngoal-oriented latent representations for tasks including classification and\nvisual reconstruction. Our model obtains highly competitive classification\nperformance on MNIST and CIFAR-10. As demonstrated through the CelebA dataset,\nto perform reconstruction the model learns to make a sequence of updates to a\ncanvas which constitute a parts-based representation. Classification with the\nself supervised representation obtained from MNIST is shown to be in line with\nthe state of the art models (none of which use a visual attention mechanism).\nFinally, we show that STAWM can be trained under the dual constraints of\nclassification and reconstruction to provide an interpretable visual sketchpad\nwhich helps open the 'black-box' of deep learning. \n\n"}
{"id": "1901.03674", "contents": "Title: On the Global Convergence of Imitation Learning: A Case for Linear\n  Quadratic Regulator Abstract: We study the global convergence of generative adversarial imitation learning\nfor linear quadratic regulators, which is posed as minimax optimization. To\naddress the challenges arising from non-convex-concave geometry, we analyze the\nalternating gradient algorithm and establish its Q-linear rate of convergence\nto a unique saddle point, which simultaneously recovers the globally optimal\npolicy and reward function. We hope our results may serve as a small step\ntowards understanding and taming the instability in imitation learning as well\nas in more general non-convex-concave alternating minimax optimization that\narises from reinforcement learning and generative adversarial learning. \n\n"}
{"id": "1901.03707", "contents": "Title: Neumann Networks for Inverse Problems in Imaging Abstract: Many challenging image processing tasks can be described by an ill-posed\nlinear inverse problem: deblurring, deconvolution, inpainting, compressed\nsensing, and superresolution all lie in this framework. Traditional inverse\nproblem solvers minimize a cost function consisting of a data-fit term, which\nmeasures how well an image matches the observations, and a regularizer, which\nreflects prior knowledge and promotes images with desirable properties like\nsmoothness. Recent advances in machine learning and image processing have\nillustrated that it is often possible to learn a regularizer from training data\nthat can outperform more traditional regularizers. We present an end-to-end,\ndata-driven method of solving inverse problems inspired by the Neumann series,\nwhich we call a Neumann network. Rather than unroll an iterative optimization\nalgorithm, we truncate a Neumann series which directly solves the linear\ninverse problem with a data-driven nonlinear regularizer. The Neumann network\narchitecture outperforms traditional inverse problem solution methods,\nmodel-free deep learning approaches, and state-of-the-art unrolled iterative\nmethods on standard datasets. Finally, when the images belong to a union of\nsubspaces and under appropriate assumptions on the forward model, we prove\nthere exists a Neumann network configuration that well-approximates the optimal\noracle estimator for the inverse problem and demonstrate empirically that the\ntrained Neumann network has the form predicted by theory. \n\n"}
{"id": "1901.03719", "contents": "Title: Non-Parametric Inference Adaptive to Intrinsic Dimension Abstract: We consider non-parametric estimation and inference of conditional moment\nmodels in high dimensions. We show that even when the dimension $D$ of the\nconditioning variable is larger than the sample size $n$, estimation and\ninference is feasible as long as the distribution of the conditioning variable\nhas small intrinsic dimension $d$, as measured by locally low doubling\nmeasures. Our estimation is based on a sub-sampled ensemble of the $k$-nearest\nneighbors ($k$-NN) $Z$-estimator. We show that if the intrinsic dimension of\nthe covariate distribution is equal to $d$, then the finite sample estimation\nerror of our estimator is of order $n^{-1/(d+2)}$ and our estimate is\n$n^{1/(d+2)}$-asymptotically normal, irrespective of $D$. The sub-sampling size\nrequired for achieving these results depends on the unknown intrinsic dimension\n$d$. We propose an adaptive data-driven approach for choosing this parameter\nand prove that it achieves the desired rates. We discuss extensions and\napplications to heterogeneous treatment effect estimation. \n\n"}
{"id": "1901.03912", "contents": "Title: Real-time Joint Object Detection and Semantic Segmentation Network for\n  Automated Driving Abstract: Convolutional Neural Networks (CNN) are successfully used for various visual\nperception tasks including bounding box object detection, semantic\nsegmentation, optical flow, depth estimation and visual SLAM. Generally these\ntasks are independently explored and modeled. In this paper, we present a joint\nmulti-task network design for learning object detection and semantic\nsegmentation simultaneously. The main motivation is to achieve real-time\nperformance on a low power embedded SOC by sharing of encoder for both the\ntasks. We construct an efficient architecture using a small ResNet10 like\nencoder which is shared for both decoders. Object detection uses YOLO v2 like\ndecoder and semantic segmentation uses FCN8 like decoder. We evaluate the\nproposed network in two public datasets (KITTI, Cityscapes) and in our private\nfisheye camera dataset, and demonstrate that joint network provides the same\naccuracy as that of separate networks. We further optimize the network to\nachieve 30 fps for 1280x384 resolution image. \n\n"}
{"id": "1901.04055", "contents": "Title: Gradient Boosted Feature Selection Abstract: A feature selection algorithm should ideally satisfy four conditions:\nreliably extract relevant features; be able to identify non-linear feature\ninteractions; scale linearly with the number of features and dimensions; allow\nthe incorporation of known sparsity structure. In this work we propose a novel\nfeature selection algorithm, Gradient Boosted Feature Selection (GBFS), which\nsatisfies all four of these requirements. The algorithm is flexible, scalable,\nand surprisingly straight-forward to implement as it is based on a modification\nof Gradient Boosted Trees. We evaluate GBFS on several real world data sets and\nshow that it matches or out-performs other state of the art feature selection\nalgorithms. Yet it scales to larger data set sizes and naturally allows for\ndomain-specific side information. \n\n"}
{"id": "1901.04195", "contents": "Title: Integrating Learning and Reasoning with Deep Logic Models Abstract: Deep learning is very effective at jointly learning feature representations\nand classification models, especially when dealing with high dimensional input\npatterns. Probabilistic logic reasoning, on the other hand, is capable to take\nconsistent and robust decisions in complex environments. The integration of\ndeep learning and logic reasoning is still an open-research problem and it is\nconsidered to be the key for the development of real intelligent agents. This\npaper presents Deep Logic Models, which are deep graphical models integrating\ndeep learning and logic reasoning both for learning and inference. Deep Logic\nModels create an end-to-end differentiable architecture, where deep learners\nare embedded into a network implementing a continuous relaxation of the logic\nknowledge. The learning process allows to jointly learn the weights of the deep\nlearners and the meta-parameters controlling the high-level reasoning. The\nexperimental results show that the proposed methodology overtakes the\nlimitations of the other approaches that have been proposed to bridge deep\nlearning and reasoning. \n\n"}
{"id": "1901.04555", "contents": "Title: Music Artist Classification with Convolutional Recurrent Neural Networks Abstract: Previous attempts at music artist classification use frame level audio\nfeatures which summarize frequency content within short intervals of time.\nComparatively, more recent music information retrieval tasks take advantage of\ntemporal structure in audio spectrograms using deep convolutional and recurrent\nmodels. This paper revisits artist classification with this new framework and\nempirically explores the impacts of incorporating temporal structure in the\nfeature representation. To this end, an established classification\narchitecture, a Convolutional Recurrent Neural Network (CRNN), is applied to\nthe artist20 music artist identification dataset under a comprehensive set of\nconditions. These include audio clip length, which is a novel contribution in\nthis work, and previously identified considerations such as dataset split and\nfeature level. Our results improve upon baseline works, verify the influence of\nthe producer effect on classification performance and demonstrate the\ntrade-offs between audio length and training set size. The best performing\nmodel achieves an average F1 score of 0.937 across three independent trials\nwhich is a substantial improvement over the corresponding baseline under\nsimilar conditions. Additionally, to showcase the effectiveness of the CRNN's\nfeature extraction capabilities, we visualize audio samples at the model's\nbottleneck layer demonstrating that learned representations segment into\nclusters belonging to their respective artists. \n\n"}
{"id": "1901.04653", "contents": "Title: Normalized Flat Minima: Exploring Scale Invariant Definition of Flat\n  Minima for Neural Networks using PAC-Bayesian Analysis Abstract: The notion of flat minima has played a key role in the generalization studies\nof deep learning models. However, existing definitions of the flatness are\nknown to be sensitive to the rescaling of parameters. The issue suggests that\nthe previous definitions of the flatness might not be a good measure of\ngeneralization, because generalization is invariant to such rescalings. In this\npaper, from the PAC-Bayesian perspective, we scrutinize the discussion\nconcerning the flat minima and introduce the notion of normalized flat minima,\nwhich is free from the known scale dependence issues. Additionally, we\nhighlight the scale dependence of existing matrix-norm based generalization\nerror bounds similar to the existing flat minima definitions. Our modified\nnotion of the flatness does not suffer from the insufficiency, either,\nsuggesting it might provide better hierarchy in the hypothesis class. \n\n"}
{"id": "1901.04704", "contents": "Title: DeepCF: A Unified Framework of Representation Learning and Matching\n  Function Learning in Recommender System Abstract: In general, recommendation can be viewed as a matching problem, i.e., match\nproper items for proper users. However, due to the huge semantic gap between\nusers and items, it's almost impossible to directly match users and items in\ntheir initial representation spaces. To solve this problem, many methods have\nbeen studied, which can be generally categorized into two types, i.e.,\nrepresentation learning-based CF methods and matching function learning-based\nCF methods. Representation learning-based CF methods try to map users and items\ninto a common representation space. In this case, the higher similarity between\na user and an item in that space implies they match better. Matching function\nlearning-based CF methods try to directly learn the complex matching function\nthat maps user-item pairs to matching scores. Although both methods are well\ndeveloped, they suffer from two fundamental flaws, i.e., the limited\nexpressiveness of dot product and the weakness in capturing low-rank relations\nrespectively. To this end, we propose a general framework named DeepCF, short\nfor Deep Collaborative Filtering, to combine the strengths of the two types of\nmethods and overcome such flaws. Extensive experiments on four publicly\navailable datasets demonstrate the effectiveness of the proposed DeepCF\nframework. \n\n"}
{"id": "1901.05582", "contents": "Title: CodeX: Bit-Flexible Encoding for Streaming-based FPGA Acceleration of\n  DNNs Abstract: This paper proposes CodeX, an end-to-end framework that facilitates encoding,\nbitwidth customization, fine-tuning, and implementation of neural networks on\nFPGA platforms. CodeX incorporates nonlinear encoding to the computation flow\nof neural networks to save memory. The encoded features demand significantly\nlower storage compared to the raw full-precision activation values; therefore,\nthe execution flow of CodeX hardware engine is completely performed within the\nFPGA using on-chip streaming buffers with no access to the off-chip DRAM. We\nfurther propose a fully-automated algorithm inspired by reinforcement learning\nwhich determines the customized encoding bitwidth across network layers. CodeX\nfull-stack framework comprises of a compiler which takes a high-level Python\ndescription of an arbitrary neural network architecture. The compiler then\ninstantiates the corresponding elements from CodeX Hardware library for FPGA\nimplementation. Proof-of-concept evaluations on MNIST, SVHN, and CIFAR-10\ndatasets demonstrate an average of 4.65x throughput improvement compared to\nstand-alone weight encoding. We further compare CodeX with six existing\nfull-precision DNN accelerators on ImageNet, showing an average of 3.6x and\n2.54x improvement in throughput and performance-per-watt, respectively. \n\n"}
{"id": "1901.05808", "contents": "Title: AuxNet: Auxiliary tasks enhanced Semantic Segmentation for Automated\n  Driving Abstract: Decision making in automated driving is highly specific to the environment\nand thus semantic segmentation plays a key role in recognizing the objects in\nthe environment around the car. Pixel level classification once considered a\nchallenging task which is now becoming mature to be productized in a car.\nHowever, semantic annotation is time consuming and quite expensive. Synthetic\ndatasets with domain adaptation techniques have been used to alleviate the lack\nof large annotated datasets. In this work, we explore an alternate approach of\nleveraging the annotations of other tasks to improve semantic segmentation.\nRecently, multi-task learning became a popular paradigm in automated driving\nwhich demonstrates joint learning of multiple tasks improves overall\nperformance of each tasks. Motivated by this, we use auxiliary tasks like depth\nestimation to improve the performance of semantic segmentation task. We propose\nadaptive task loss weighting techniques to address scale issues in multi-task\nloss functions which become more crucial in auxiliary tasks. We experimented on\nautomotive datasets including SYNTHIA and KITTI and obtained 3% and 5%\nimprovement in accuracy respectively. \n\n"}
{"id": "1901.06033", "contents": "Title: Continuous Hierarchical Representations with Poincar\\'e Variational\n  Auto-Encoders Abstract: The variational auto-encoder (VAE) is a popular method for learning a\ngenerative model and embeddings of the data. Many real datasets are\nhierarchically structured. However, traditional VAEs map data in a Euclidean\nlatent space which cannot efficiently embed tree-like structures. Hyperbolic\nspaces with negative curvature can. We therefore endow VAEs with a Poincar\\'e\nball model of hyperbolic geometry as a latent space and rigorously derive the\nnecessary methods to work with two main Gaussian generalisations on that space.\nWe empirically show better generalisation to unseen data than the Euclidean\ncounterpart, and can qualitatively and quantitatively better recover\nhierarchical structures. \n\n"}
{"id": "1901.06075", "contents": "Title: Splitting Methods for Convex Bi-Clustering and Co-Clustering Abstract: Co-Clustering, the problem of simultaneously identifying clusters across\nmultiple aspects of a data set, is a natural generalization of clustering to\nhigher-order structured data. Recent convex formulations of bi-clustering and\ntensor co-clustering, which shrink estimated centroids together using a convex\nfusion penalty, allow for global optimality guarantees and precise theoretical\nanalysis, but their computational properties have been less well studied. In\nthis note, we present three efficient operator-splitting methods for the convex\nco-clustering problem: a standard two-block ADMM, a Generalized ADMM which\navoids an expensive tensor Sylvester equation in the primal update, and a\nthree-block ADMM based on the operator splitting scheme of Davis and Yin.\nTheoretical complexity analysis suggests, and experimental evidence confirms,\nthat the Generalized ADMM is far more efficient for large problems. \n\n"}
{"id": "1901.06314", "contents": "Title: Physics-Constrained Deep Learning for High-dimensional Surrogate\n  Modeling and Uncertainty Quantification without Labeled Data Abstract: Surrogate modeling and uncertainty quantification tasks for PDE systems are\nmost often considered as supervised learning problems where input and output\ndata pairs are used for training. The construction of such emulators is by\ndefinition a small data problem which poses challenges to deep learning\napproaches that have been developed to operate in the big data regime. Even in\ncases where such models have been shown to have good predictive capability in\nhigh dimensions, they fail to address constraints in the data implied by the\nPDE model. This paper provides a methodology that incorporates the governing\nequations of the physical model in the loss/likelihood functions. The resulting\nphysics-constrained, deep learning models are trained without any labeled data\n(e.g. employing only input data) and provide comparable predictive responses\nwith data-driven models while obeying the constraints of the problem at hand.\nThis work employs a convolutional encoder-decoder neural network approach as\nwell as a conditional flow-based generative model for the solution of PDEs,\nsurrogate model construction, and uncertainty quantification tasks. The\nmethodology is posed as a minimization problem of the reverse Kullback-Leibler\n(KL) divergence between the model predictive density and the reference\nconditional density, where the later is defined as the Boltzmann-Gibbs\ndistribution at a given inverse temperature with the underlying potential\nrelating to the PDE system of interest. The generalization capability of these\nmodels to out-of-distribution input is considered. Quantification and\ninterpretation of the predictive uncertainty is provided for a number of\nproblems. \n\n"}
{"id": "1901.06413", "contents": "Title: Differentially Private High Dimensional Sparse Covariance Matrix\n  Estimation Abstract: In this paper, we study the problem of estimating the covariance matrix under\ndifferential privacy, where the underlying covariance matrix is assumed to be\nsparse and of high dimensions. We propose a new method, called DP-Thresholding,\nto achieve a non-trivial $\\ell_2$-norm based error bound, which is\nsignificantly better than the existing ones from adding noise directly to the\nempirical covariance matrix. We also extend the $\\ell_2$-norm based error bound\nto a general $\\ell_w$-norm based one for any $1\\leq w\\leq \\infty$, and show\nthat they share the same upper bound asymptotically. Our approach can be easily\nextended to local differential privacy. Experiments on the synthetic datasets\nshow consistent results with our theoretical claims. \n\n"}
{"id": "1901.06494", "contents": "Title: Writer Independent Offline Signature Recognition Using Ensemble Learning Abstract: The area of Handwritten Signature Verification has been broadly researched in\nthe last decades, but remains an open research problem. In offline (static)\nsignature verification, the dynamic information of the signature writing\nprocess is lost, and it is difficult to design good feature extractors that can\ndistinguish genuine signatures and skilled forgeries. This verification task is\neven harder in writer independent scenarios which is undeniably fiscal for\nrealistic cases. In this paper, we have proposed an Ensemble model for offline\nwriter, independent signature verification task with Deep learning. We have\nused two CNNs for feature extraction, after that RGBT for classification &\nStacking to generate final prediction vector. We have done extensive\nexperiments on various datasets from various sources to maintain a variance in\nthe dataset. We have achieved the state of the art performance on various\ndatasets. \n\n"}
{"id": "1901.07061", "contents": "Title: Prior Information Guided Regularized Deep Learning for Cell Nucleus\n  Detection Abstract: Cell nuclei detection is a challenging research topic because of limitations\nin cellular image quality and diversity of nuclear morphology, i.e. varying\nnuclei shapes, sizes, and overlaps between multiple cell nuclei. This has been\na topic of enduring interest with promising recent success shown by deep\nlearning methods. These methods train Convolutional Neural Networks (CNNs) with\na training set of input images and known, labeled nuclei locations. Many such\nmethods are supplemented by spatial or morphological processing. Using a set of\ncanonical cell nuclei shapes, prepared with the help of a domain expert, we\ndevelop a new approach that we call Shape Priors with Convolutional Neural\nNetworks (SP-CNN). We further extend the network to introduce a shape prior\n(SP) layer and then allowing it to become trainable (i.e. optimizable). We call\nthis network tunable SP-CNN (TSP-CNN). In summary, we present new network\nstructures that can incorporate 'expected behavior' of nucleus shapes via two\ncomponents: learnable layers that perform the nucleus detection and a fixed\nprocessing part that guides the learning with prior information. Analytically,\nwe formulate two new regularization terms that are targeted at: 1) learning the\nshapes, 2) reducing false positives while simultaneously encouraging detection\ninside the cell nucleus boundary. Experimental results on two challenging\ndatasets reveal that the proposed SP-CNN and TSP-CNN can outperform\nstate-of-the-art alternatives. \n\n"}
{"id": "1901.07132", "contents": "Title: Universal Rules for Fooling Deep Neural Networks based Text\n  Classification Abstract: Recently, deep learning based natural language processing techniques are\nbeing extensively used to deal with spam mail, censorship evaluation in social\nnetworks, among others. However, there is only a couple of works evaluating the\nvulnerabilities of such deep neural networks. Here, we go beyond attacks to\ninvestigate, for the first time, universal rules, i.e., rules that are sample\nagnostic and therefore could turn any text sample in an adversarial one. In\nfact, the universal rules do not use any information from the method itself (no\ninformation from the method, gradient information or training dataset\ninformation is used), making them black-box universal attacks. In other words,\nthe universal rules are sample and method agnostic. By proposing a\ncoevolutionary optimization algorithm we show that it is possible to create\nuniversal rules that can automatically craft imperceptible adversarial samples\n(only less than five perturbations which are close to misspelling are inserted\nin the text sample). A comparison with a random search algorithm further\njustifies the strength of the method. Thus, universal rules for fooling\nnetworks are here shown to exist. Hopefully, the results from this work will\nimpact the development of yet more sample and model agnostic attacks as well as\ntheir defenses, culminating in perhaps a new age for artificial intelligence. \n\n"}
{"id": "1901.07355", "contents": "Title: Optical Flow augmented Semantic Segmentation networks for Automated\n  Driving Abstract: Motion is a dominant cue in automated driving systems. Optical flow is\ntypically computed to detect moving objects and to estimate depth using\ntriangulation. In this paper, our motivation is to leverage the existing dense\noptical flow to improve the performance of semantic segmentation. To provide a\nsystematic study, we construct four different architectures which use RGB only,\nflow only, RGBF concatenated and two-stream RGB + flow. We evaluate these\nnetworks on two automotive datasets namely Virtual KITTI and Cityscapes using\nthe state-of-the-art flow estimator FlowNet v2. We also make use of the ground\ntruth optical flow in Virtual KITTI to serve as an ideal estimator and a\nstandard Farneback optical flow algorithm to study the effect of noise. Using\nthe flow ground truth in Virtual KITTI, two-stream architecture achieves the\nbest results with an improvement of 4% IoU. As expected, there is a large\nimprovement for moving objects like trucks, vans and cars with 38%, 28% and 6%\nincrease in IoU. FlowNet produces an improvement of 2.4% in average IoU with\nlarger improvement in the moving objects corresponding to 26%, 11% and 5% in\ntrucks, vans and cars. In Cityscapes, flow augmentation provided an improvement\nfor moving objects like motorcycle and train with an increase of 17% and 7% in\nIoU. \n\n"}
{"id": "1901.07417", "contents": "Title: On Connected Sublevel Sets in Deep Learning Abstract: This paper shows that every sublevel set of the loss function of a class of\ndeep over-parameterized neural nets with piecewise linear activation functions\nis connected and unbounded. This implies that the loss has no bad local valleys\nand all of its global minima are connected within a unique and potentially very\nlarge global valley. \n\n"}
{"id": "1901.07868", "contents": "Title: Constant Time Graph Neural Networks Abstract: The recent advancements in graph neural networks (GNNs) have led to\nstate-of-the-art performances in various applications, including\nchemo-informatics, question-answering systems, and recommender systems.\nHowever, scaling up these methods to huge graphs, such as social networks and\nWeb graphs, remains a challenge. In particular, the existing methods for\naccelerating GNNs either are not theoretically guaranteed in terms of the\napproximation error or incur at least a linear time computation cost. In this\nstudy, we reveal the query complexity of the uniform node sampling scheme for\nMessage Passing Neural Networks, including GraphSAGE, graph attention networks\n(GATs), and graph convolutional networks (GCNs). Surprisingly, our analysis\nreveals that the complexity of the node sampling method is completely\nindependent of the number of the nodes, edges, and neighbors of the input and\ndepends only on the error tolerance and confidence probability while providing\na theoretical guarantee for the approximation error. To the best of our\nknowledge, this is the first paper to provide a theoretical guarantee of\napproximation for GNNs within constant time. Through experiments with synthetic\nand real-world datasets, we investigated the speed and precision of the node\nsampling scheme and validated our theoretical results. \n\n"}
{"id": "1901.08244", "contents": "Title: Measurements of Three-Level Hierarchical Structure in the Outliers in\n  the Spectrum of Deepnet Hessians Abstract: We consider deep classifying neural networks. We expose a structure in the\nderivative of the logits with respect to the parameters of the model, which is\nused to explain the existence of outliers in the spectrum of the Hessian.\nPrevious works decomposed the Hessian into two components, attributing the\noutliers to one of them, the so-called Covariance of gradients. We show this\nterm is not a Covariance but a second moment matrix, i.e., it is influenced by\nmeans of gradients. These means possess an additive two-way structure that is\nthe source of the outliers in the spectrum. This structure can be used to\napproximate the principal subspace of the Hessian using certain \"averaging\"\noperations, avoiding the need for high-dimensional eigenanalysis. We\ncorroborate this claim across different datasets, architectures and sample\nsizes. \n\n"}
{"id": "1901.08256", "contents": "Title: Large-Batch Training for LSTM and Beyond Abstract: Large-batch training approaches have enabled researchers to utilize\nlarge-scale distributed processing and greatly accelerate deep-neural net (DNN)\ntraining. For example, by scaling the batch size from 256 to 32K, researchers\nhave been able to reduce the training time of ResNet50 on ImageNet from 29\nhours to 2.2 minutes (Ying et al., 2018). In this paper, we propose a new\napproach called linear-epoch gradual-warmup (LEGW) for better large-batch\ntraining. With LEGW, we are able to conduct large-batch training for both CNNs\nand RNNs with the Sqrt Scaling scheme. LEGW enables Sqrt Scaling scheme to be\nuseful in practice and as a result we achieve much better results than the\nLinear Scaling learning rate scheme. For LSTM applications, we are able to\nscale the batch size by a factor of 64 without losing accuracy and without\ntuning the hyper-parameters. For CNN applications, LEGW is able to achieve the\nsame accuracy even as we scale the batch size to 32K. LEGW works better than\nprevious large-batch auto-tuning techniques. LEGW achieves a 5.3X average\nspeedup over the baselines for four LSTM-based applications on the same\nhardware. We also provide some theoretical explanations for LEGW. \n\n"}
{"id": "1901.08394", "contents": "Title: Application of Decision Rules for Handling Class Imbalance in Semantic\n  Segmentation Abstract: As part of autonomous car driving systems, semantic segmentation is an\nessential component to obtain a full understanding of the car's environment.\nOne difficulty, that occurs while training neural networks for this purpose, is\nclass imbalance of training data. Consequently, a neural network trained on\nunbalanced data in combination with maximum a-posteriori classification may\neasily ignore classes that are rare in terms of their frequency in the dataset.\nHowever, these classes are often of highest interest. We approach such\npotential misclassifications by weighting the posterior class probabilities\nwith the prior class probabilities which in our case are the inverse\nfrequencies of the corresponding classes in the training dataset. More\nprecisely, we adopt a localized method by computing the priors pixel-wise such\nthat the impact can be analyzed at pixel level as well. In our experiments, we\ntrain one network from scratch using a proprietary dataset containing 20,000\nannotated frames of video sequences recorded from street scenes. The evaluation\non our test set shows an increase of average recall with regard to instances of\npedestrians and info signs by $25\\%$ and $23.4\\%$, respectively. In addition,\nwe significantly reduce the non-detection rate for instances of the same\nclasses by $61\\%$ and $38\\%$. \n\n"}
{"id": "1901.08669", "contents": "Title: SAGA with Arbitrary Sampling Abstract: We study the problem of minimizing the average of a very large number of\nsmooth functions, which is of key importance in training supervised learning\nmodels. One of the most celebrated methods in this context is the SAGA\nalgorithm. Despite years of research on the topic, a general-purpose version of\nSAGA---one that would include arbitrary importance sampling and minibatching\nschemes---does not exist. We remedy this situation and propose a general and\nflexible variant of SAGA following the {\\em arbitrary sampling} paradigm. We\nperform an iteration complexity analysis of the method, largely possible due to\nthe construction of new stochastic Lyapunov functions. We establish linear\nconvergence rates in the smooth and strongly convex regime, and under a\nquadratic functional growth condition (i.e., in a regime not assuming strong\nconvexity). Our rates match those of the primal-dual method Quartz for which an\narbitrary sampling analysis is available, which makes a significant step\ntowards closing the gap in our understanding of complexity of primal and dual\nmethods for finite sum problems. \n\n"}
{"id": "1901.08755", "contents": "Title: SecureBoost: A Lossless Federated Learning Framework Abstract: The protection of user privacy is an important concern in machine learning,\nas evidenced by the rolling out of the General Data Protection Regulation\n(GDPR) in the European Union (EU) in May 2018. The GDPR is designed to give\nusers more control over their personal data, which motivates us to explore\nmachine learning frameworks for data sharing that do not violate user privacy.\nTo meet this goal, in this paper, we propose a novel lossless\nprivacy-preserving tree-boosting system known as SecureBoost in the setting of\nfederated learning. SecureBoost first conducts entity alignment under a\nprivacy-preserving protocol and then constructs boosting trees across multiple\nparties with a carefully designed encryption strategy. This federated learning\nsystem allows the learning process to be jointly conducted over multiple\nparties with common user samples but different feature sets, which corresponds\nto a vertically partitioned data set. An advantage of SecureBoost is that it\nprovides the same level of accuracy as the non-privacy-preserving approach\nwhile at the same time, reveals no information of each private data provider.\nWe show that the SecureBoost framework is as accurate as other non-federated\ngradient tree-boosting algorithms that require centralized data and thus it is\nhighly scalable and practical for industrial applications such as credit risk\nanalysis. To this end, we discuss information leakage during the protocol\nexecution and propose ways to provably reduce it. \n\n"}
{"id": "1901.08987", "contents": "Title: Dynamical Isometry and a Mean Field Theory of LSTMs and GRUs Abstract: Training recurrent neural networks (RNNs) on long sequence tasks is plagued\nwith difficulties arising from the exponential explosion or vanishing of\nsignals as they propagate forward or backward through the network. Many\ntechniques have been proposed to ameliorate these issues, including various\nalgorithmic and architectural modifications. Two of the most successful RNN\narchitectures, the LSTM and the GRU, do exhibit modest improvements over\nvanilla RNN cells, but they still suffer from instabilities when trained on\nvery long sequences. In this work, we develop a mean field theory of signal\npropagation in LSTMs and GRUs that enables us to calculate the time scales for\nsignal propagation as well as the spectral properties of the state-to-state\nJacobians. By optimizing these quantities in terms of the initialization\nhyperparameters, we derive a novel initialization scheme that eliminates or\nreduces training instabilities. We demonstrate the efficacy of our\ninitialization scheme on multiple sequence tasks, on which it enables\nsuccessful training while a standard initialization either fails completely or\nis orders of magnitude slower. We also observe a beneficial effect on\ngeneralization performance using this new initialization. \n\n"}
{"id": "1901.09097", "contents": "Title: Driver Distraction Identification with an Ensemble of Convolutional\n  Neural Networks Abstract: The World Health Organization (WHO) reported 1.25 million deaths yearly due\nto road traffic accidents worldwide and the number has been continuously\nincreasing over the last few years. Nearly fifth of these accidents are caused\nby distracted drivers. Existing work of distracted driver detection is\nconcerned with a small set of distractions (mostly, cell phone usage).\nUnreliable ad-hoc methods are often used.In this paper, we present the first\npublicly available dataset for driver distraction identification with more\ndistraction postures than existing alternatives. In addition, we propose a\nreliable deep learning-based solution that achieves a 90% accuracy. The system\nconsists of a genetically-weighted ensemble of convolutional neural networks,\nwe show that a weighted ensemble of classifiers using a genetic algorithm\nyields in a better classification confidence. We also study the effect of\ndifferent visual elements in distraction detection by means of face and hand\nlocalizations, and skin segmentation. Finally, we present a thinned version of\nour ensemble that could achieve 84.64% classification accuracy and operate in a\nreal-time environment. \n\n"}
{"id": "1901.09149", "contents": "Title: Escaping Saddle Points with Adaptive Gradient Methods Abstract: Adaptive methods such as Adam and RMSProp are widely used in deep learning\nbut are not well understood. In this paper, we seek a crisp, clean and precise\ncharacterization of their behavior in nonconvex settings. To this end, we first\nprovide a novel view of adaptive methods as preconditioned SGD, where the\npreconditioner is estimated in an online manner. By studying the preconditioner\non its own, we elucidate its purpose: it rescales the stochastic gradient noise\nto be isotropic near stationary points, which helps escape saddle points.\nFurthermore, we show that adaptive methods can efficiently estimate the\naforementioned preconditioner. By gluing together these two components, we\nprovide the first (to our knowledge) second-order convergence result for any\nadaptive method. The key insight from our analysis is that, compared to SGD,\nadaptive methods escape saddle points faster, and can converge faster overall\nto second-order stationary points. \n\n"}
{"id": "1901.09203", "contents": "Title: ACNN: a Full Resolution DCNN for Medical Image Segmentation Abstract: Deep Convolutional Neural Networks (DCNNs) are used extensively in medical\nimage segmentation and hence 3D navigation for robot-assisted Minimally\nInvasive Surgeries (MISs). However, current DCNNs usually use down sampling\nlayers for increasing the receptive field and gaining abstract semantic\ninformation. These down sampling layers decrease the spatial dimension of\nfeature maps, which can be detrimental to image segmentation. Atrous\nconvolution is an alternative for the down sampling layer. It increases the\nreceptive field whilst maintains the spatial dimension of feature maps. In this\npaper, a method for effective atrous rate setting is proposed to achieve the\nlargest and fully-covered receptive field with a minimum number of atrous\nconvolutional layers. Furthermore, a new and full resolution DCNN - Atrous\nConvolutional Neural Network (ACNN), which incorporates cascaded atrous\nII-blocks, residual learning and Instance Normalization (IN) is proposed.\nApplication results of the proposed ACNN to Magnetic Resonance Imaging (MRI)\nand Computed Tomography (CT) image segmentation demonstrate that the proposed\nACNN can achieve higher segmentation Intersection over Unions (IoUs) than U-Net\nand Deeplabv3+, but with reduced trainable parameters. \n\n"}
{"id": "1901.09335", "contents": "Title: Augment your batch: better training with larger batches Abstract: Large-batch SGD is important for scaling training of deep neural networks.\nHowever, without fine-tuning hyperparameter schedules, the generalization of\nthe model may be hampered. We propose to use batch augmentation: replicating\ninstances of samples within the same batch with different data augmentations.\nBatch augmentation acts as a regularizer and an accelerator, increasing both\ngeneralization and performance scaling. We analyze the effect of batch\naugmentation on gradient variance and show that it empirically improves\nconvergence for a wide variety of deep neural networks and datasets. Our\nresults show that batch augmentation reduces the number of necessary SGD\nupdates to achieve the same accuracy as the state-of-the-art. Overall, this\nsimple yet effective method enables faster training and better generalization\nby allowing more computational resources to be used concurrently. \n\n"}
{"id": "1901.09344", "contents": "Title: Stochastic Approximation of Smooth and Strongly Convex Functions: Beyond\n  the $O(1/T)$ Convergence Rate Abstract: Stochastic approximation (SA) is a classical approach for stochastic convex\noptimization. Previous studies have demonstrated that the convergence rate of\nSA can be improved by introducing either smoothness or strong convexity\ncondition. In this paper, we make use of smoothness and strong convexity\nsimultaneously to boost the convergence rate. Let $\\lambda$ be the modulus of\nstrong convexity, $\\kappa$ be the condition number, $F_*$ be the minimal risk,\nand $\\alpha>1$ be some small constant. First, we demonstrate that, in\nexpectation, an $O(1/[\\lambda T^\\alpha] + \\kappa F_*/T)$ risk bound is\nattainable when $T = \\Omega(\\kappa^\\alpha)$. Thus, when $F_*$ is small, the\nconvergence rate could be faster than $O(1/[\\lambda T])$ and approaches\n$O(1/[\\lambda T^\\alpha])$ in the ideal case. Second, to further benefit from\nsmall risk, we show that, in expectation, an $O(1/2^{T/\\kappa}+F_*)$ risk bound\nis achievable. Thus, the excess risk reduces exponentially until reaching\n$O(F_*)$, and if $F_*=0$, we obtain a global linear convergence. Finally, we\nemphasize that our proof is constructive and each risk bound is equipped with\nan efficient stochastic algorithm attaining that bound. \n\n"}
{"id": "1901.09387", "contents": "Title: Imitation Learning from Imperfect Demonstration Abstract: Imitation learning (IL) aims to learn an optimal policy from demonstrations.\nHowever, such demonstrations are often imperfect since collecting optimal ones\nis costly. To effectively learn from imperfect demonstrations, we propose a\nnovel approach that utilizes confidence scores, which describe the quality of\ndemonstrations. More specifically, we propose two confidence-based IL methods,\nnamely two-step importance weighting IL (2IWIL) and generative adversarial IL\nwith imperfect demonstration and confidence (IC-GAIL). We show that confidence\nscores given only to a small portion of sub-optimal demonstrations\nsignificantly improve the performance of IL both theoretically and empirically. \n\n"}
{"id": "1901.09451", "contents": "Title: Bias in Bios: A Case Study of Semantic Representation Bias in a\n  High-Stakes Setting Abstract: We present a large-scale study of gender bias in occupation classification, a\ntask where the use of machine learning may lead to negative outcomes on\npeoples' lives. We analyze the potential allocation harms that can result from\nsemantic representation bias. To do so, we study the impact on occupation\nclassification of including explicit gender indicators---such as first names\nand pronouns---in different semantic representations of online biographies.\nAdditionally, we quantify the bias that remains when these indicators are\n\"scrubbed,\" and describe proxy behavior that occurs in the absence of explicit\ngender indicators. As we demonstrate, differences in true positive rates\nbetween genders are correlated with existing gender imbalances in occupations,\nwhich may compound these imbalances. \n\n"}
{"id": "1901.09453", "contents": "Title: On Learning Invariant Representation for Domain Adaptation Abstract: Due to the ability of deep neural nets to learn rich representations, recent\nadvances in unsupervised domain adaptation have focused on learning\ndomain-invariant features that achieve a small error on the source domain. The\nhope is that the learnt representation, together with the hypothesis learnt\nfrom the source domain, can generalize to the target domain. In this paper, we\nfirst construct a simple counterexample showing that, contrary to common\nbelief, the above conditions are not sufficient to guarantee successful domain\nadaptation. In particular, the counterexample exhibits \\emph{conditional\nshift}: the class-conditional distributions of input features change between\nsource and target domains. To give a sufficient condition for domain\nadaptation, we propose a natural and interpretable generalization upper bound\nthat explicitly takes into account the aforementioned shift. Moreover, we shed\nnew light on the problem by proving an information-theoretic lower bound on the\njoint error of \\emph{any} domain adaptation method that attempts to learn\ninvariant representations. Our result characterizes a fundamental tradeoff\nbetween learning invariant representations and achieving small joint error on\nboth domains when the marginal label distributions differ from source to\ntarget. Finally, we conduct experiments on real-world datasets that corroborate\nour theoretical findings. We believe these insights are helpful in guiding the\nfuture design of domain adaptation and representation learning algorithms. \n\n"}
{"id": "1901.09532", "contents": "Title: Target Tracking for Contextual Bandits: Application to Demand Side\n  Management Abstract: We propose a contextual-bandit approach for demand side management by\noffering price incentives. More precisely, a target mean consumption is set at\neach round and the mean consumption is modeled as a complex function of the\ndistribution of prices sent and of some contextual variables such as the\ntemperature, weather, and so on. The performance of our strategies is measured\nin quadratic losses through a regret criterion. We offer $T^{2/3}$ upper bounds\non this regret (up to poly-logarithmic terms)---and even faster rates under\nstronger assumptions---for strategies inspired by standard strategies for\ncontextual bandits (like LinUCB, see Li et al., 2010). Simulations on a real\ndata set gathered by UK Power Networks, in which price incentives were offered,\nshow that our strategies are effective and may indeed manage demand response by\nsuitably picking the price levels. \n\n"}
{"id": "1901.09557", "contents": "Title: Out-of-Sample Testing for GANs Abstract: We propose a new method to evaluate GANs, namely EvalGAN. EvalGAN relies on a\ntest set to directly measure the reconstruction quality in the original sample\nspace (no auxiliary networks are necessary), and it also computes the\n(log)likelihood for the reconstructed samples in the test set. Further, EvalGAN\nis agnostic to the GAN algorithm and the dataset. We decided to test it on\nthree state-of-the-art GANs over the well-known CIFAR-10 and CelebA datasets. \n\n"}
{"id": "1901.09680", "contents": "Title: The Intrinsic Scale of Networks is Small Abstract: We define the intrinsic scale at which a network begins to reveal its\nidentity as the scale at which subgraphs in the network (created by a random\nwalk) are distinguishable from similar sized subgraphs in a perturbed copy of\nthe network. We conduct an extensive study of intrinsic scale for several\nnetworks, ranging from structured (e.g. road networks) to ad-hoc and\nunstructured (e.g. crowd sourced information networks), to biological. We find:\n(a) The intrinsic scale is surprisingly small (7-20 vertices), even though the\nnetworks are many orders of magnitude larger. (b) The intrinsic scale\nquantifies ``structure'' in a network -- networks which are explicitly\nconstructed for specific tasks have smaller intrinsic scale. (c) The structure\nat different scales can be fragile (easy to disrupt) or robust. \n\n"}
{"id": "1901.09764", "contents": "Title: CollaGAN : Collaborative GAN for Missing Image Data Imputation Abstract: In many applications requiring multiple inputs to obtain a desired output, if\nany of the input data is missing, it often introduces large amounts of bias.\nAlthough many techniques have been developed for imputing missing data, the\nimage imputation is still difficult due to complicated nature of natural\nimages. To address this problem, here we proposed a novel framework for missing\nimage data imputation, called Collaborative Generative Adversarial Network\n(CollaGAN). CollaGAN converts an image imputation problem to a multi-domain\nimages-to-image translation task so that a single generator and discriminator\nnetwork can successfully estimate the missing data using the remaining clean\ndata set. We demonstrate that CollaGAN produces the images with a higher visual\nquality compared to the existing competing approaches in various image\nimputation tasks. \n\n"}
{"id": "1901.09813", "contents": "Title: Analogies Explained: Towards Understanding Word Embeddings Abstract: Word embeddings generated by neural network methods such as word2vec (W2V)\nare well known to exhibit seemingly linear behaviour, e.g. the embeddings of\nanalogy \"woman is to queen as man is to king\" approximately describe a\nparallelogram. This property is particularly intriguing since the embeddings\nare not trained to achieve it. Several explanations have been proposed, but\neach introduces assumptions that do not hold in practice. We derive a\nprobabilistically grounded definition of paraphrasing that we re-interpret as\nword transformation, a mathematical description of \"$w_x$ is to $w_y$\". From\nthese concepts we prove existence of linear relationships between W2V-type\nembeddings that underlie the analogical phenomenon, identifying explicit error\nterms. \n\n"}
{"id": "1901.09839", "contents": "Title: Interpreting Deep Neural Networks Through Variable Importance Abstract: While the success of deep neural networks (DNNs) is well-established across a\nvariety of domains, our ability to explain and interpret these methods is\nlimited. Unlike previously proposed local methods which try to explain\nparticular classification decisions, we focus on global interpretability and\nask a universally applicable question: given a trained model, which features\nare the most important? In the context of neural networks, a feature is rarely\nimportant on its own, so our strategy is specifically designed to leverage\npartial covariance structures and incorporate variable dependence into feature\nranking. Our methodological contributions in this paper are two-fold. First, we\npropose an effect size analogue for DNNs that is appropriate for applications\nwith highly collinear predictors (ubiquitous in computer vision). Second, we\nextend the recently proposed \"RelATive cEntrality\" (RATE) measure (Crawford et\nal., 2019) to the Bayesian deep learning setting. RATE applies an information\ntheoretic criterion to the posterior distribution of effect sizes to assess\nfeature significance. We apply our framework to three broad application areas:\ncomputer vision, natural language processing, and social science. \n\n"}
{"id": "1901.09993", "contents": "Title: Label Efficient Semi-Supervised Learning via Graph Filtering Abstract: Graph-based methods have been demonstrated as one of the most effective\napproaches for semi-supervised learning, as they can exploit the connectivity\npatterns between labeled and unlabeled data samples to improve learning\nperformance. However, existing graph-based methods either are limited in their\nability to jointly model graph structures and data features, such as the\nclassical label propagation methods, or require a considerable amount of\nlabeled data for training and validation due to high model complexity, such as\nthe recent neural-network-based methods. In this paper, we address label\nefficient semi-supervised learning from a graph filtering perspective.\nSpecifically, we propose a graph filtering framework that injects graph\nsimilarity into data features by taking them as signals on the graph and\napplying a low-pass graph filter to extract useful data representations for\nclassification, where label efficiency can be achieved by conveniently\nadjusting the strength of the graph filter. Interestingly, this framework\nunifies two seemingly very different methods -- label propagation and graph\nconvolutional networks. Revisiting them under the graph filtering framework\nleads to new insights that improve their modeling capabilities and reduce model\ncomplexity. Experiments on various semi-supervised classification tasks on four\ncitation networks and one knowledge graph and one semi-supervised regression\ntask for zero-shot image recognition validate our findings and proposals. \n\n"}
{"id": "1901.10024", "contents": "Title: Cross-Domain Image Manipulation by Demonstration Abstract: In this work we propose a model that can manipulate individual visual\nattributes of objects in a real scene using examples of how respective\nattribute manipulations affect the output of a simulation. As an example, we\ntrain our model to manipulate the expression of a human face using\nnonphotorealistic 3D renders of a face with varied expression. Our model\nmanages to preserve all other visual attributes of a real face, such as head\norientation, even though this and other attributes are not labeled in either\nreal or synthetic domain. Since our model learns to manipulate a specific\nproperty in isolation using only \"synthetic demonstrations\" of such\nmanipulations without explicitly provided labels, it can be applied to shape,\ntexture, lighting, and other properties that are difficult to measure or\nrepresent as real-valued vectors. We measure the degree to which our model\npreserves other attributes of a real image when a single specific attribute is\nmanipulated. We use digit datasets to analyze how discrepancy in attribute\ndistributions affects the performance of our model, and demonstrate results in\na far more difficult setting: learning to manipulate real human faces using\nnonphotorealistic 3D renders. \n\n"}
{"id": "1901.10053", "contents": "Title: Towards Fair Deep Clustering With Multi-State Protected Variables Abstract: Fair clustering under the disparate impact doctrine requires that population\nof each protected group should be approximately equal in every cluster.\nPrevious work investigated a difficult-to-scale pre-processing step for\n$k$-center and $k$-median style algorithms for the special case of this problem\nwhen the number of protected groups is two. In this work, we consider a more\ngeneral and practical setting where there can be many protected groups. To this\nend, we propose Deep Fair Clustering, which learns a discriminative but fair\ncluster assignment function. The experimental results on three public datasets\nwith different types of protected attribute show that our approach can steadily\nimprove the degree of fairness while only having minor loss in terms of\nclustering quality. \n\n"}
{"id": "1901.10277", "contents": "Title: High-Quality Self-Supervised Deep Image Denoising Abstract: We describe a novel method for training high-quality image denoising models\nbased on unorganized collections of corrupted images. The training does not\nneed access to clean reference images, or explicit pairs of corrupted images,\nand can thus be applied in situations where such data is unacceptably expensive\nor impossible to acquire. We build on a recent technique that removes the need\nfor reference data by employing networks with a \"blind spot\" in the receptive\nfield, and significantly improve two key aspects: image quality and training\nefficiency. Our result quality is on par with state-of-the-art neural network\ndenoisers in the case of i.i.d. additive Gaussian noise, and not far behind\nwith Poisson and impulse noise. We also successfully handle cases where\nparameters of the noise model are variable and/or unknown in both training and\nevaluation data. \n\n"}
{"id": "1901.10310", "contents": "Title: Robust Learning from Untrusted Sources Abstract: Modern machine learning methods often require more data for training than a\nsingle expert can provide. Therefore, it has become a standard procedure to\ncollect data from external sources, e.g. via crowdsourcing. Unfortunately, the\nquality of these sources is not always guaranteed. As additional complications,\nthe data might be stored in a distributed way, or might even have to remain\nprivate. In this work, we address the question of how to learn robustly in such\nscenarios. Studying the problem through the lens of statistical learning\ntheory, we derive a procedure that allows for learning from all available\nsources, yet automatically suppresses irrelevant or corrupted data. We show by\nextensive experiments that our method provides significant improvements over\nalternative approaches from robust statistics and distributed optimization. \n\n"}
{"id": "1901.10435", "contents": "Title: A Deep Learning Framework for Assessing Physical Rehabilitation\n  Exercises Abstract: Computer-aided assessment of physical rehabilitation entails evaluation of\npatient performance in completing prescribed rehabilitation exercises, based on\nprocessing movement data captured with a sensory system. Despite the essential\nrole of rehabilitation assessment toward improved patient outcomes and reduced\nhealthcare costs, existing approaches lack versatility, robustness, and\npractical relevance. In this paper, we propose a deep learning-based framework\nfor automated assessment of the quality of physical rehabilitation exercises.\nThe main components of the framework are metrics for quantifying movement\nperformance, scoring functions for mapping the performance metrics into\nnumerical scores of movement quality, and deep neural network models for\ngenerating quality scores of input movements via supervised learning. The\nproposed performance metric is defined based on the log-likelihood of a\nGaussian mixture model, and encodes low-dimensional data representation\nobtained with a deep autoencoder network. The proposed deep spatio-temporal\nneural network arranges data into temporal pyramids, and exploits the spatial\ncharacteristics of human movements by using sub-networks to process joint\ndisplacements of individual body parts. The presented framework is validated\nusing a dataset of ten rehabilitation exercises. The significance of this work\nis that it is the first that implements deep neural networks for assessment of\nrehabilitation performance. \n\n"}
{"id": "1901.10517", "contents": "Title: Reparameterizable Subset Sampling via Continuous Relaxations Abstract: Many machine learning tasks require sampling a subset of items from a\ncollection based on a parameterized distribution. The Gumbel-softmax trick can\nbe used to sample a single item, and allows for low-variance reparameterized\ngradients with respect to the parameters of the underlying distribution.\nHowever, stochastic optimization involving subset sampling is typically not\nreparameterizable. To overcome this limitation, we define a continuous\nrelaxation of subset sampling that provides reparameterization gradients by\ngeneralizing the Gumbel-max trick. We use this approach to sample subsets of\nfeatures in an instance-wise feature selection task for model interpretability,\nsubsets of neighbors to implement a deep stochastic k-nearest neighbors model,\nand sub-sequences of neighbors to implement parametric t-SNE by directly\ncomparing the identities of local neighbors. We improve performance in all\nthese tasks by incorporating subset sampling in end-to-end training. \n\n"}
{"id": "1901.10621", "contents": "Title: Enhanced Variational Inference with Dyadic Transformation Abstract: Variational autoencoder is a powerful deep generative model with variational\ninference. The practice of modeling latent variables in the VAE's original\nformulation as normal distributions with a diagonal covariance matrix limits\nthe flexibility to match the true posterior distribution. We propose a new\ntransformation, dyadic transformation (DT), that can model a multivariate\nnormal distribution. DT is a single-stage transformation with low computational\nrequirements. We demonstrate empirically on MNIST dataset that DT enhances the\nposterior flexibility and attains competitive results compared to other VAE\nenhancements. \n\n"}
{"id": "1901.11078", "contents": "Title: Real-world Mapping of Gaze Fixations Using Instance Segmentation for\n  Road Construction Safety Applications Abstract: Research studies have shown that a large proportion of hazards remain\nunrecognized, which expose construction workers to unanticipated safety risks.\nRecent studies have also found that a strong correlation exists between viewing\npatterns of workers, captured using eye-tracking devices, and their hazard\nrecognition performance. Therefore, it is important to analyze the viewing\npatterns of workers to gain a better understanding of their hazard recognition\nperformance. This paper proposes a method that can automatically map the gaze\nfixations collected using a wearable eye-tracker to the predefined areas of\ninterests. The proposed method detects these areas or objects (i.e., hazards)\nof interests through a computer vision-based segmentation technique and\ntransfer learning. The mapped fixation data is then used to analyze the viewing\nbehaviors of workers and compute their attention distribution. The proposed\nmethod is implemented on an under construction road as a case study to evaluate\nthe performance of the proposed method. \n\n"}
{"id": "1901.11224", "contents": "Title: Lower Bounds for Smooth Nonconvex Finite-Sum Optimization Abstract: Smooth finite-sum optimization has been widely studied in both convex and\nnonconvex settings. However, existing lower bounds for finite-sum optimization\nare mostly limited to the setting where each component function is (strongly)\nconvex, while the lower bounds for nonconvex finite-sum optimization remain\nlargely unsolved. In this paper, we study the lower bounds for smooth nonconvex\nfinite-sum optimization, where the objective function is the average of $n$\nnonconvex component functions. We prove tight lower bounds for the complexity\nof finding $\\epsilon$-suboptimal point and $\\epsilon$-approximate stationary\npoint in different settings, for a wide regime of the smallest eigenvalue of\nthe Hessian of the objective function (or each component function). Given our\nlower bounds, we can show that existing algorithms including KatyushaX\n(Allen-Zhu, 2018), Natasha (Allen-Zhu, 2017), RapGrad (Lan and Yang, 2018) and\nStagewiseKatyusha (Chen and Yang, 2018) have achieved optimal Incremental\nFirst-order Oracle (IFO) complexity (i.e., number of IFO calls) up to logarithm\nfactors for nonconvex finite-sum optimization. We also point out potential ways\nto further improve these complexity results, in terms of making stronger\nassumptions or by a different convergence analysis. \n\n"}
{"id": "1901.11275", "contents": "Title: A Theory of Regularized Markov Decision Processes Abstract: Many recent successful (deep) reinforcement learning algorithms make use of\nregularization, generally based on entropy or Kullback-Leibler divergence. We\npropose a general theory of regularized Markov Decision Processes that\ngeneralizes these approaches in two directions: we consider a larger class of\nregularizers, and we consider the general modified policy iteration approach,\nencompassing both policy iteration and value iteration. The core building\nblocks of this theory are a notion of regularized Bellman operator and the\nLegendre-Fenchel transform, a classical tool of convex optimization. This\napproach allows for error propagation analyses of general algorithmic schemes\nof which (possibly variants of) classical algorithms such as Trust Region\nPolicy Optimization, Soft Q-learning, Stochastic Actor Critic or Dynamic Policy\nProgramming are special cases. This also draws connections to proximal convex\noptimization, especially to Mirror Descent. \n\n"}
{"id": "1901.11384", "contents": "Title: Learning to navigate image manifolds induced by generative adversarial\n  networks for unsupervised video generation Abstract: In this work, we introduce a two-step framework for generative modeling of\ntemporal data. Specifically, the generative adversarial networks (GANs) setting\nis employed to generate synthetic scenes of moving objects. To do so, we\npropose a two-step training scheme within which: a generator of static frames\nis trained first. Afterwards, a recurrent model is trained with the goal of\nproviding a sequence of inputs to the previously trained frames generator, thus\nyielding scenes which look natural. The adversarial setting is employed in both\ntraining steps. However, with the aim of avoiding known training instabilities\nin GANs, a multiple discriminator approach is used to train both models.\nResults in the studied video dataset indicate that, by employing such an\napproach, the recurrent part is able to learn how to coherently navigate the\nimage manifold induced by the frames generator, thus yielding more\nnatural-looking scenes. \n\n"}
{"id": "1901.11503", "contents": "Title: Contrasting Exploration in Parameter and Action Space: A Zeroth-Order\n  Optimization Perspective Abstract: Black-box optimizers that explore in parameter space have often been shown to\noutperform more sophisticated action space exploration methods developed\nspecifically for the reinforcement learning problem. We examine these black-box\nmethods closely to identify situations in which they are worse than action\nspace exploration methods and those in which they are superior. Through simple\ntheoretical analyses, we prove that complexity of exploration in parameter\nspace depends on the dimensionality of parameter space, while complexity of\nexploration in action space depends on both the dimensionality of action space\nand horizon length. This is also demonstrated empirically by comparing simple\nexploration methods on several model problems, including Contextual Bandit,\nLinear Regression and Reinforcement Learning in continuous control. \n\n"}
