{"id": "0704.3424", "contents": "Title: A New Proof of Pappus's Theorem Abstract: Any stretching of Ringel's non-Pappus pseudoline arrangement when projected\ninto the Euclidean plane, implicitly contains a particular arrangement of nine\ntriangles. This arrangement has a complex constraint involving the sines of its\nangles. These constraints cannot be satisfied by any projection of the initial\narrangement. This is sufficient to prove Pappus's theorem. The derivation of\nthe constraint is via systems of inequalities arising from the polar\ncoordinates of the lines. These systems are linear in r for any given theta,\nand their solubility can be analysed in terms of the signs of determinants. The\nevaluation of the determinants is via a normal form for sums of products of\nsines, giving a powerful system of trigonometric identities. The particular\nresult is generalized to arrangements derived from three edge connected totally\ncyclic directed graphs, conjectured to be sufficient for a complete analysis of\nangle constraining arrangements of lines, and thus a full response to Ringel's\nslope conjecture. These methods are generally applicable to the realizability\nproblem for rank 3 oriented matroids. \n\n"}
{"id": "0706.3520", "contents": "Title: On probabilities for separating sets of order statistics Abstract: Consider a set of order statistics that arise from sorting samples from two\ndifferent populations, each with their own, possibly different distribution\nfunction. The probability that these order statistics fall in disjoint, ordered\nintervals, and that of the smallest statistics, a certain number come from the\nfirst populations, are given in terms of the two distribution functions. The\nresult is applied to computing the joint probability of the number of\nrejections and the number of false rejections for the Benjamini-Hochberg false\ndiscovery rate procedure. \n\n"}
{"id": "0707.4558", "contents": "Title: Open Problems in Algebraic Statistics Abstract: Algebraic statistics is concerned with the study of probabilistic models and\ntechniques for statistical inference using methods from algebra and geometry.\nThis article presents a list of open mathematical problems in this emerging\nfield, with main emphasis on graphical models with hidden variables, maximum\nlikelihood estimation, and multivariate Gaussian distributions. This article is\nbased on a lecture presented at the IMA in Minneapolis during the 2006/07\nprogram on Applications of Algebraic Geometry. \n\n"}
{"id": "0709.3535", "contents": "Title: Maximum Likelihood Estimation in Latent Class Models For Contingency\n  Table Data Abstract: Statistical models with latent structure have a history going back to the\n1950s and have seen widespread use in the social sciences and, more recently,\nin computational biology and in machine learning. Here we study the basic\nlatent class model proposed originally by the sociologist Paul F. Lazarfeld for\ncategorical variables, and we explain its geometric structure. We draw\nparallels between the statistical and geometric properties of latent class\nmodels and we illustrate geometrically the causes of many problems associated\nwith maximum likelihood estimation and related statistical inference. In\nparticular, we focus on issues of non-identifiability and determination of the\nmodel dimension, of maximization of the likelihood function and on the effect\nof symmetric data. We illustrate these phenomena with a variety of synthetic\nand real-life tables, of different dimension and complexity. Much of the\nmotivation for this work stems from the \"100 Swiss Francs\" problem, which we\nintroduce and describe in detail. \n\n"}
{"id": "0711.1612", "contents": "Title: Enhancing Sparsity by Reweighted L1 Minimization Abstract: It is now well understood that (1) it is possible to reconstruct sparse\nsignals exactly from what appear to be highly incomplete sets of linear\nmeasurements and (2) that this can be done by constrained L1 minimization. In\nthis paper, we study a novel method for sparse signal recovery that in many\nsituations outperforms L1 minimization in the sense that substantially fewer\nmeasurements are needed for exact recovery. The algorithm consists of solving a\nsequence of weighted L1-minimization problems where the weights used for the\nnext iteration are computed from the value of the current solution. We present\na series of experiments demonstrating the remarkable performance and broad\napplicability of this algorithm in the areas of sparse signal recovery,\nstatistical estimation, error correction and image processing. Interestingly,\nsuperior gains are also achieved when our method is applied to recover signals\nwith assumed near-sparsity in overcomplete representations--not by reweighting\nthe L1 norm of the coefficient sequence as is common, but by reweighting the L1\nnorm of the transformed object. An immediate consequence is the possibility of\nhighly efficient data acquisition protocols by improving on a technique known\nas compressed sensing. \n\n"}
{"id": "0802.0529", "contents": "Title: The distribution of the maximum of a first order moving average: the\n  discrete case Abstract: We give the distribution of $M_n$, the maximum of a sequence of $n$\nobservations from a moving average of order 1. Solutions are first given in\nterms of repeated integrals and then for the case where the underlying\nindependent random variables are discrete. When the correlation is positive, $$\nP(M_n \\max^n_{i=1} X_i \\leq x) = \\sum_{j=1}^\\infty \\beta_{jx} \\nu_{jx}^{n}\n\\approx B_{x} r{1x}^{n} $$ where $\\{\\nu_{jx}\\}$ are the eigenvalues of a\ncertain matrix, $r_{1x}$ is the maximum magnitude of the eigenvalues, and $I$\ndepends on the number of possible values of the underlying random variables.\nThe eigenvalues do not depend on $x$ only on its range. \n\n"}
{"id": "0802.2377", "contents": "Title: Higher-Order Properties of Analytic Wavelets Abstract: The influence of higher-order wavelet properties on the analytic wavelet\ntransform behavior is investigated, and wavelet functions offering advantageous\nperformance are identified. This is accomplished through detailed investigation\nof the generalized Morse wavelets, a two-parameter family of exactly analytic\ncontinuous wavelets. The degree of time/frequency localization, the existence\nof a mapping between scale and frequency, and the bias involved in estimating\nproperties of modulated oscillatory signals, are proposed as important\nconsiderations. Wavelet behavior is found to be strongly impacted by the degree\nof asymmetry of the wavelet in both the frequency and the time domain, as\nquantified by the third central moments. A particular subset of the generalized\nMorse wavelets, recognized as deriving from an inhomogeneous Airy function,\nemerge as having particularly desirable properties. These \"Airy wavelets\"\nsubstantially outperform the only approximately analytic Morlet wavelets for\nhigh time localization. Special cases of the generalized Morse wavelets are\nexamined, revealing a broad range of behaviors which can be matched to the\ncharacteristics of a signal. \n\n"}
{"id": "0802.2581", "contents": "Title: A Localization Approach to Improve Iterative Proportional Scaling in\n  Gaussian Graphical Models Abstract: We discuss an efficient implementation of the iterative proportional scaling\nprocedure in the multivariate Gaussian graphical models. We show that the\ncomputational cost can be reduced by localization of the update procedure in\neach iterative step by using the structure of a decomposable model obtained by\ntriangulation of the graph associated with the model. Some numerical\nexperiments demonstrate the competitive performance of the proposed algorithm. \n\n"}
{"id": "0804.2413", "contents": "Title: Bayesian Inference on Mixtures of Distributions Abstract: This survey covers state-of-the-art Bayesian techniques for the estimation of\nmixtures. It complements the earlier Marin, Mengersen and Robert (2005) by\nstudying new types of distributions, the multinomial, latent class and t\ndistributions. It also exhibits closed form solutions for Bayesian inference in\nsome discrete setups. Lastly, it sheds a new light on the computation of Bayes\nfactors via the approximation of Chib (1995). \n\n"}
{"id": "0805.0774", "contents": "Title: Splitting Polytopes Abstract: A split of a polytope $P$ is a (regular) subdivision with exactly two maximal\ncells. It turns out that each weight function on the vertices of $P$ admits a\nunique decomposition as a linear combination of weight functions corresponding\nto the splits of $P$ (with a split prime remainder). This generalizes a result\nof Bandelt and Dress [Adv. Math. 92 (1992)] on the decomposition of finite\nmetric spaces.\n  Introducing the concept of compatibility of splits gives rise to a finite\nsimplicial complex associated with any polytope $P$, the split complex of $P$.\nComplete descriptions of the split complexes of all hypersimplices are\nobtained. Moreover, it is shown that these complexes arise as subcomplexes of\nthe tropical (pre-)Grassmannians of Speyer and Sturmfels [Adv. Geom. 4 (2004)]. \n\n"}
{"id": "0806.3474", "contents": "Title: Information field theory for cosmological perturbation reconstruction\n  and non-linear signal analysis Abstract: We develop information field theory (IFT) as a means of Bayesian inference on\nspatially distributed signals, the information fields. A didactical approach is\nattempted. Starting from general considerations on the nature of measurements,\nsignals, noise, and their relation to a physical reality, we derive the\ninformation Hamiltonian, the source field, propagator, and interaction terms.\nFree IFT reproduces the well known Wiener-filter theory. Interacting IFT can be\ndiagrammatically expanded, for which we provide the Feynman rules in position-,\nFourier-, and spherical harmonics space, and the Boltzmann-Shannon information\nmeasure. The theory should be applicable in many fields. However, here, two\ncosmological signal recovery problems are discussed in their IFT-formulation.\n1) Reconstruction of the cosmic large-scale structure matter distribution from\ndiscrete galaxy counts in incomplete galaxy surveys within a simple model of\ngalaxy formation. We show that a Gaussian signal, which should resemble the\ninitial density perturbations of the Universe, observed with a strongly\nnon-linear, incomplete and Poissonian-noise affected response, as the processes\nof structure and galaxy formation and observations provide, can be\nreconstructed thanks to the virtue of a response-renormalization flow equation.\n2) We design a filter to detect local non-linearities in the cosmic microwave\nbackground, which are predicted from some Early-Universe inflationary\nscenarios, and expected due to measurement imperfections. This filter is the\noptimal Bayes' estimator up to linear order in the non-linearity parameter and\ncan be used even to construct sky maps of non-linearities in the data. \n\n"}
{"id": "0806.3769", "contents": "Title: Improved testing inference in mixed linear models Abstract: Mixed linear models are commonly used in repeated measures studies. They\naccount for the dependence amongst observations obtained from the same\nexperimental unit. Oftentimes, the number of observations is small, and it is\nthus important to use inference strategies that incorporate small sample\ncorrections. In this paper, we develop modified versions of the likelihood\nratio test for fixed effects inference in mixed linear models. In particular,\nwe derive a Bartlett correction to such a test and also to a test obtained from\na modified profile likelihood function. Our results generalize those in Zucker\net al. (Journal of the Royal Statistical Society B, 2000, 62, 827-838) by\nallowing the parameter of interest to be vector-valued. Additionally, our\nBartlett corrections allow for random effects nonlinear covariance matrix\nstructure. We report numerical evidence which shows that the proposed tests\ndisplay superior finite sample behavior relative to the standard likelihood\nratio test. An application is also presented and discussed. \n\n"}
{"id": "0807.1106", "contents": "Title: Principal components analysis for sparsely observed correlated\n  functional data using a kernel smoothing approach Abstract: In this paper, we consider the problem of estimating the covariance kernel\nand its eigenvalues and eigenfunctions from sparse, irregularly observed, noise\ncorrupted and (possibly) correlated functional data. We present a method based\non pre-smoothing of individual sample curves through an appropriate kernel. We\nshow that the naive empirical covariance of the pre-smoothed sample curves\ngives highly biased estimator of the covariance kernel along its diagonal. We\nattend to this problem by estimating the diagonal and off-diagonal parts of the\ncovariance kernel separately. We then present a practical and efficient method\nfor choosing the bandwidth for the kernel by using an approximation to the\nleave-one-curve-out cross validation score. We prove that under standard\nregularity conditions on the covariance kernel and assuming i.i.d. samples, the\nrisk of our estimator, under $L^2$ loss, achieves the optimal nonparametric\nrate when the number of measurements per curve is bounded. We also show that\neven when the sample curves are correlated in such a way that the noiseless\ndata has a separable covariance structure, the proposed method is still\nconsistent and we quantify the role of this correlation in the risk of the\nestimator. \n\n"}
{"id": "0809.0974", "contents": "Title: Least Squares and Shrinkage Estimation under Bimonotonicity Constraints Abstract: In this paper we describe active set type algorithms for minimization of a\nsmooth function under general order constraints, an important case being\nfunctions on the set of bimonotone r-by-s matrices. These algorithms can be\nused, for instance, to estimate a bimonotone regression function via least\nsquares or (a smooth approximation of) least absolute deviations. Another\napplication is shrinkage estimation in image denoising or, more generally,\nregression problems with two ordinal factors after representing the data in a\nsuitable basis which is indexed by pairs (i,j) in {1,...,r}x{1,...,s}. Various\nnumerical examples illustrate our methods. \n\n"}
{"id": "0809.4694", "contents": "Title: Tropical Convex Hull Computations Abstract: This is a survey on tropical polytopes from the combinatorial point of view\nand with a focus on algorithms. Tropical convexity is interesting because it\nrelates a number of combinatorial concepts including ordinary convexity,\nmonomial ideals, subdivisions of products of simplices, matroid theory, finite\nmetric spaces, and the tropical Grassmannians. The relationship between these\ntopics is explained via one running example throughout the whole paper. The\nfinal section explains how the new version 2.9.4 of the software system\npolymake can be used to compute with tropical polytopes. \n\n"}
{"id": "0901.1378", "contents": "Title: A cautionary tale on the efficiency of some adaptive Monte Carlo schemes Abstract: There is a growing interest in the literature for adaptive Markov chain Monte\nCarlo methods based on sequences of random transition kernels $\\{P_n\\}$ where\nthe kernel $P_n$ is allowed to have an invariant distribution $\\pi_n$ not\nnecessarily equal to the distribution of interest $\\pi$ (target distribution).\nThese algorithms are designed such that as $n\\to\\infty$, $P_n$ converges to\n$P$, a kernel that has the correct invariant distribution $\\pi$. Typically, $P$\nis a kernel with good convergence properties, but one that cannot be directly\nimplemented. It is then expected that the algorithm will inherit the good\nconvergence properties of $P$. The equi-energy sampler of [Ann. Statist. 34\n(2006) 1581--1619] is an example of this type of adaptive MCMC. We show in this\npaper that the asymptotic variance of this type of adaptive MCMC is always at\nleast as large as the asymptotic variance of the Markov chain with transition\nkernel $P$. We also show by simulation that the difference can be substantial. \n\n"}
{"id": "0903.3002", "contents": "Title: Learning with Structured Sparsity Abstract: This paper investigates a new learning formulation called structured\nsparsity, which is a natural extension of the standard sparsity concept in\nstatistical learning and compressive sensing. By allowing arbitrary structures\non the feature set, this concept generalizes the group sparsity idea that has\nbecome popular in recent years. A general theory is developed for learning with\nstructured sparsity, based on the notion of coding complexity associated with\nthe structure. It is shown that if the coding complexity of the target signal\nis small, then one can achieve improved performance by using coding complexity\nregularization methods, which generalize the standard sparse regularization.\nMoreover, a structured greedy algorithm is proposed to efficiently solve the\nstructured sparsity problem. It is shown that the greedy algorithm\napproximately solves the coding complexity optimization problem under\nappropriate conditions. Experiments are included to demonstrate the advantage\nof structured sparsity over standard sparsity on some real applications. \n\n"}
{"id": "0903.5223", "contents": "Title: Maximum entropy Gaussian approximation for the number of integer points\n  and volumes of polytopes Abstract: We describe a maximum entropy approach for computing volumes and counting\ninteger points in polyhedra. To estimate the number of points from a particular\nset X in R^n in a polyhedron P in R^n, by solving a certain entropy\nmaximization problem, we construct a probability distribution on the set X such\nthat a) the probability mass function is constant on the intersection of P and\nX and b) the expectation of the distribution lies in P. This allows us to apply\nCentral Limit Theorem type arguments to deduce computationally efficient\napproximations for the number of integer points, volumes, and the number of 0-1\nvectors in the polytope. As an application, we obtain asymptotic formulas for\nvolumes of multi-index transportation polytopes and for the number of multi-way\ncontingency tables. \n\n"}
{"id": "0904.2052", "contents": "Title: Least Squares estimation of two ordered monotone regression curves Abstract: In this paper, we consider the problem of finding the Least Squares\nestimators of two isotonic regression curves $g^\\circ_1$ and $g^\\circ_2$ under\nthe additional constraint that they are ordered; e.g., $g^\\circ_1 \\le\ng^\\circ_2$. Given two sets of $n$ data points $y_1, ..., y_n$ and $z_1,\n>...,z_n$ observed at (the same) design points, the estimates of the true\ncurves are obtained by minimizing the weighted Least Squares criterion $L_2(a,\nb) = \\sum_{j=1}^n (y_j - a_j)^2 w_{1,j}+ \\sum_{j=1}^n (z_j - b_j)^2 w_{2,j}$\nover the class of pairs of vectors $(a, b) \\in \\mathbb{R}^n \\times \\mathbb{R}^n\n$ such that $a_1 \\le a_2 \\le ...\\le a_n $, $b_1 \\le b_2 \\le ...\\le b_n $, and\n$a_i \\le b_i, i=1, ...,n$. The characterization of the estimators is\nestablished. To compute these estimators, we use an iterative projected\nsubgradient algorithm, where the projection is performed with a \"generalized\"\npool-adjacent-violaters algorithm (PAVA), a byproduct of this work. Then, we\napply the estimation method to real data from mechanical engineering. \n\n"}
{"id": "0905.2979", "contents": "Title: Extreme deconvolution: Inferring complete distribution functions from\n  noisy, heterogeneous and incomplete observations Abstract: We generalize the well-known mixtures of Gaussians approach to density\nestimation and the accompanying Expectation--Maximization technique for finding\nthe maximum likelihood parameters of the mixture to the case where each data\npoint carries an individual $d$-dimensional uncertainty covariance and has\nunique missing data properties. This algorithm reconstructs the\nerror-deconvolved or \"underlying\" distribution function common to all samples,\neven when the individual data points are samples from different distributions,\nobtained by convolving the underlying distribution with the heteroskedastic\nuncertainty distribution of the data point and projecting out the missing data\ndirections. We show how this basic algorithm can be extended with conjugate\npriors on all of the model parameters and a \"split-and-merge\" procedure\ndesigned to avoid local maxima of the likelihood. We demonstrate the full\nmethod by applying it to the problem of inferring the three-dimensional\nvelocity distribution of stars near the Sun from noisy two-dimensional,\ntransverse velocity measurements from the Hipparcos satellite. \n\n"}
{"id": "0906.3501", "contents": "Title: Semiparametric modeling of autonomous nonlinear dynamical systems with\n  applications Abstract: In this paper, we propose a semi-parametric model for autonomous nonlinear\ndynamical systems and devise an estimation procedure for model fitting. This\nmodel incorporates subject-specific effects and can be viewed as a nonlinear\nsemi-parametric mixed effects model. We also propose a computationally\nefficient model selection procedure. We prove consistency of the proposed\nestimator under suitable regularity conditions. We show by simulation studies\nthat the proposed estimation as well as model selection procedures can\nefficiently handle sparse and noisy measurements. Finally, we apply the\nproposed method to a plant growth data used to study growth displacement rates\nwithin meristems of maize roots under two different experimental conditions. \n\n"}
{"id": "0907.4728", "contents": "Title: A survey of cross-validation procedures for model selection Abstract: Used to estimate the risk of an estimator or to perform model selection,\ncross-validation is a widespread strategy because of its simplicity and its\napparent universality. Many results exist on the model selection performances\nof cross-validation procedures. This survey intends to relate these results to\nthe most recent advances of model selection theory, with a particular emphasis\non distinguishing empirical statements from rigorous theoretical results. As a\nconclusion, guidelines are provided for choosing the best cross-validation\nprocedure according to the particular features of the problem in hand. \n\n"}
{"id": "0909.3052", "contents": "Title: Cross-Validation for Unsupervised Learning Abstract: Cross-validation (CV) is a popular method for model-selection. Unfortunately,\nit is not immediately obvious how to apply CV to unsupervised or exploratory\ncontexts. This thesis discusses some extensions of cross-validation to\nunsupervised learning, specifically focusing on the problem of choosing how\nmany principal components to keep. We introduce the latent factor model, define\nan objective criterion, and show how CV can be used to estimate the intrinsic\ndimensionality of a data set. Through both simulation and theory, we\ndemonstrate that cross-validation is a valuable tool for unsupervised learning. \n\n"}
{"id": "0910.2477", "contents": "Title: An asymptotic formula for the number of non-negative integer matrices\n  with prescribed row and column sums Abstract: We count mxn non-negative integer matrices (contingency tables) with\nprescribed row and column sums (margins). For a wide class of smooth margins we\nestablish a computationally efficient asymptotic formula approximating the\nnumber of matrices within a relative error which approaches 0 as m and n grow. \n\n"}
{"id": "0910.4558", "contents": "Title: Effect of indirect dependencies on \"A mutual information minimization\n  approach for a class of nonlinear recurrent separating systems\" Abstract: In a recent paper [4], Duarte and Jutten investigated the Blind Source\nSeparation (BSS) problem, for the nonlinear mixing model that they introduced\nin that paper. They proposed to solve this problem by using\ninformation-theoretic tools, more precisely by minimizing the mutual\ninformation (MI) of the outputs of the separating structure. When applying the\nMI approach to BSS problems, one usually determines the analytical expressions\nof the derivatives of the MI with respect to the parameters of the considered\nseparating model. In the literature, these calculations were mainly reported\nfor linear mixtures up to now. They are more complex for nonlinear mixtures,\ndue to dependencies between the considered quantities. Moreover, the notations\ncommonly employed by the BSS community in such calculations may become\nmisleading when using them for nonlinear mixtures, due to the above-mentioned\ndependencies. We claim that the calculations reported in [4] contain an error,\nbecause they did not take into account all these dependencies. In this\ndocument, we therefore explain this phenomenon, by showing the effect of\nindirect dependencies on the application of the MI approach to the mixing and\nseparating models considered in [4]. We thus introduce a corrected expression\nof the gradient of the considered BSS criterion based on MI. This correct\ngradient may then e.g. be used to optimize the adaptive coefficients of the\nconsidered separating system by means of the well-known gradient descent\nalgorithm. As explained hereafter, this investigation has some similarities\nwith an analysis that we previously reported in another arXiv document [3].\nHowever, these two investigations concern different problems (mixture and\nseparating structure, mathematical tools: see paper). \n\n"}
{"id": "0910.5761", "contents": "Title: Which graphical models are difficult to learn? Abstract: We consider the problem of learning the structure of Ising models (pairwise\nbinary Markov random fields) from i.i.d. samples. While several methods have\nbeen proposed to accomplish this task, their relative merits and limitations\nremain somewhat obscure. By analyzing a number of concrete examples, we show\nthat low-complexity algorithms systematically fail when the Markov random field\ndevelops long-range correlations. More precisely, this phenomenon appears to be\nrelated to the Ising model phase transition (although it does not coincide with\nit). \n\n"}
{"id": "0911.0221", "contents": "Title: Limit theorems for some adaptive MCMC algorithms with subgeometric\n  kernels: Part II Abstract: We prove a central limit theorem for a general class of adaptive Markov Chain\nMonte Carlo algorithms driven by sub-geometrically ergodic Markov kernels. We\ndiscuss in detail the special case of stochastic approximation. We use the\nresult to analyze the asymptotic behavior of an adaptive version of the\nMetropolis Adjusted Langevin algorithm with a heavy tailed target density. \n\n"}
{"id": "0911.0522", "contents": "Title: Can the Adaptive Metropolis Algorithm Collapse Without the Covariance\n  Lower Bound? Abstract: The Adaptive Metropolis (AM) algorithm is based on the symmetric random-walk\nMetropolis algorithm. The proposal distribution has the following\ntime-dependent covariance matrix at step $n+1$ \\[\n  S_n = Cov(X_1,...,X_n) + \\epsilon I, \\] that is, the sample covariance matrix\nof the history of the chain plus a (small) constant $\\epsilon>0$ multiple of\nthe identity matrix $I$. The lower bound on the eigenvalues of $S_n$ induced by\nthe factor $\\epsilon I$ is theoretically convenient, but practically\ncumbersome, as a good value for the parameter $\\epsilon$ may not always be easy\nto choose. This article considers variants of the AM algorithm that do not\nexplicitly bound the eigenvalues of $S_n$ away from zero. The behaviour of\n$S_n$ is studied in detail, indicating that the eigenvalues of $S_n$ do not\ntend to collapse to zero in general. \n\n"}
{"id": "0911.1164", "contents": "Title: Kernel estimators of asymptotic variance for adaptive Markov chain Monte\n  Carlo Abstract: We study the asymptotic behavior of kernel estimators of asymptotic variances\n(or long-run variances) for a class of adaptive Markov chains. The convergence\nis studied both in $L^p$ and almost surely. The results also apply to Markov\nchains and improve on the existing literature by imposing weaker conditions. We\nillustrate the results with applications to the $\\operatorname {GARCH}(1,1)$\nMarkov model and to an adaptive MCMC algorithm for Bayesian logistic\nregression. \n\n"}
{"id": "0911.4106", "contents": "Title: Revisiting the hexagonal lattice: on optimal lattice circle packing Abstract: In this note we give a simple proof of the classical fact that the hexagonal\nlattice gives the highest density circle packing among all lattices in $R^2$.\nWith the benefit of hindsight, we show that the problem can be restricted to\nthe important class of well-rounded lattices, on which the density function\ntakes a particularly simple form. Our proof emphasizes the role of well-rounded\nlattices for discrete optimization problems. \n\n"}
{"id": "0911.4982", "contents": "Title: More bounds on the diameters of convex polytopes Abstract: Finding a good bound on the maximal edge diameter $\\Delta(d,n)$ of a polytope\nin terms of its dimension $d$ and the number of its facets $n$ is one of the\nbasic open questions in polytope theory \\cite{BG}. Although some bounds are\nknown, the behaviour of the function $\\Delta(d,n)$ is largely unknown. The\nHirsch conjecture, formulated in 1957 and reported in \\cite{GD}, states that\n$\\Delta(d,n)$ is linear in $n$ and $d$: $\\Delta(d,n) \\leq n-d$. The conjecture\nis known to hold in small dimensions, i.e., for $d \\leq 3$ \\cite{VK}, along\nwith other specific pairs of $d$ and $n$ (Table \\ref{before}). However, the\nasymptotic behaviour of $\\Delta(d,n)$ is not well understood: the best upper\nbound -- due to Kalai and Kleitman -- is quasi-polynomial \\cite{GKDK}.\n  In this article we will show that $\\Delta(4,12)=7$ and present strong\nevidence for $\\Delta(5,12)=\\Delta(6,13)=7$. The first of these new values is of\nparticular interest since it indicates that the Hirsch bound is not sharp in\ndimension 4. \n\n"}
{"id": "0912.5200", "contents": "Title: Penalized Composite Quasi-Likelihood for Ultrahigh-Dimensional Variable\n  Selection Abstract: In high-dimensional model selection problems, penalized simple least-square\napproaches have been extensively used. This paper addresses the question of\nboth robustness and efficiency of penalized model selection methods, and\nproposes a data-driven weighted linear combination of convex loss functions,\ntogether with weighted $L_1$-penalty. It is completely data-adaptive and does\nnot require prior knowledge of the error distribution. The weighted\n$L_1$-penalty is used both to ensure the convexity of the penalty term and to\nameliorate the bias caused by the $L_1$-penalty. In the setting with\ndimensionality much larger than the sample size, we establish a strong oracle\nproperty of the proposed method that possesses both the model selection\nconsistency and estimation efficiency for the true non-zero coefficients. As\nspecific examples, we introduce a robust method of composite L1-L2, and optimal\ncomposite quantile method and evaluate their performance in both simulated and\nreal data examples. \n\n"}
{"id": "1001.2185", "contents": "Title: Improved estimators for dispersion models with dispersion covariates Abstract: In this paper we discuss improved estimators for the regression and the\ndispersion parameters in an extended class of dispersion models (J{\\o}rgensen,\n1996). This class extends the regular dispersion models by letting the\ndispersion parameter vary throughout the observations, and contains the\ndispersion models as particular case. General formulae for the second-order\nbias are obtained explicitly in dispersion models with dispersion covariates,\nwhich generalize previous results by Botter and Cordeiro (1998), Cordeiro and\nMcCullagh (1991), Cordeiro and Vasconcellos (1999), and Paula (1992). The\npractical use of the formulae is that we can derive closed-form expressions for\nthe second-order biases of the maximum likelihood estimators of the regression\nand dispersion parameters when the information matrix has a closed-form.\nVarious expressions for the second-order biases are given for special models.\nThe formulae have advantages for numerical purposes because they require only a\nsupplementary weighted linear regression. We also compare these bias-corrected\nestimators with two different estimators which are also bias-free to the\nsecond-order that are based on bootstrap methods. These estimators are compared\nby simulation. \n\n"}
{"id": "1001.2187", "contents": "Title: Skewness of maximum likelihood estimators in dispersion models Abstract: We introduce the dispersion models with a regression structure to extend the\ngeneralized linear models, the exponential family nonlinear models (Cordeiro\nand Paula, 1989) and the proper dispersion models (J{\\o}rgensen, 1997a). We\nprovide a matrix expression for the skewness of the maximum likelihood\nestimators of the regression parameters in dispersion models. The formula is\nsuitable for computer implementation and can be applied for several important\nsubmodels discussed in the literature. Expressions for the skewness of the\nmaximum likelihood estimators of the precision and dispersion parameters are\nalso derived. In particular, our results extend previous formulas obtained by\nCordeiro and Cordeiro (2001) and Cavalcanti et al. (2009). A simulation study\nis perfomed to show the practice importance of our results. \n\n"}
{"id": "1001.2797", "contents": "Title: Adaptive Gibbs samplers Abstract: We consider various versions of adaptive Gibbs and Metropolis within-Gibbs\nsamplers, which update their selection probabilities (and perhaps also their\nproposal distributions) on the fly during a run, by learning as they go in an\nattempt to optimise the algorithm. We present a cautionary example of how even\na simple-seeming adaptive Gibbs sampler may fail to converge. We then present\nvarious positive results guaranteeing convergence of adaptive Gibbs samplers\nunder certain conditions. \n\n"}
{"id": "1002.1994", "contents": "Title: Probabilistic Recovery of Multiple Subspaces in Point Clouds by\n  Geometric lp Minimization Abstract: We assume data independently sampled from a mixture distribution on the unit\nball of the D-dimensional Euclidean space with K+1 components: the first\ncomponent is a uniform distribution on that ball representing outliers and the\nother K components are uniform distributions along K d-dimensional linear\nsubspaces restricted to that ball. We study both the simultaneous recovery of\nall K underlying subspaces and the recovery of the best l0 subspace (i.e., with\nlargest number of points) by minimizing the lp-averaged distances of data\npoints from d-dimensional subspaces of the D-dimensional space. Unlike other lp\nminimization problems, this minimization is non-convex for all p>0 and thus\nrequires different methods for its analysis. We show that if 0<p <= 1, then\nboth all underlying subspaces and the best l0 subspace can be precisely\nrecovered by lp minimization with overwhelming probability. This result extends\nto additive homoscedastic uniform noise around the subspaces (i.e., uniform\ndistribution in a strip around them) and near recovery with an error\nproportional to the noise level. On the other hand, if K>1 and p>1, then we\nshow that both all underlying subspaces and the best l0 subspace cannot be\nrecovered and even nearly recovered. Further relaxations are also discussed. We\nuse the results of this paper for partially justifying recent effective\nalgorithms for modeling data by mixtures of multiple subspaces as well as for\ndiscussing the effect of using variants of lp minimizations in RANSAC-type\nstrategies for single subspace recovery. \n\n"}
{"id": "1002.2684", "contents": "Title: On computational tools for Bayesian data analysis Abstract: While Robert and Rousseau (2010) addressed the foundational aspects of\nBayesian analysis, the current chapter details its practical aspects through a\nreview of the computational methods available for approximating Bayesian\nprocedures. Recent innovations like Monte Carlo Markov chain, sequential Monte\nCarlo methods and more recently Approximate Bayesian Computation techniques\nhave considerably increased the potential for Bayesian applications and they\nhave also opened new avenues for Bayesian inference, first and foremost\nBayesian model choice. \n\n"}
{"id": "1003.6039", "contents": "Title: Stein couplings for normal approximation Abstract: In this article we propose a general framework for normal approximation using\nStein's method. We introduce the new concept of Stein couplings and we show\nthat it lies at the heart of popular approaches such as the local approach,\nexchangeable pairs, size biasing and many other approaches. We prove several\ntheorems with which normal approximation for the Wasserstein and Kolmogorov\nmetrics becomes routine once a Stein coupling is found. To illustrate the\nversatility of our framework we give applications in Hoeffding's combinatorial\ncentral limit theorem, functionals in the classic occupancy scheme,\nneighbourhood statistics of point patterns with fixed number of points and\nfunctionals of the components of randomly chosen vertices of sub-critical\nErdos-Renyi random graphs. In all these cases, we use new, non-standard\ncouplings. \n\n"}
{"id": "1004.2287", "contents": "Title: An empirical comparative study of approximate methods for binary\n  graphical models; application to the search of associations among causes of\n  death in French death certificates Abstract: Looking for associations among multiple variables is a topical issue in\nstatistics due to the increasing amount of data encountered in biology,\nmedicine and many other domains involving statistical applications. Graphical\nmodels have recently gained popularity for this purpose in the statistical\nliterature. Following the ideas of the LASSO procedure designed for the linear\nregression framework, recent developments dealing with graphical model\nselection have been based on $\\ell_1$-penalization. In the binary case,\nhowever, exact inference is generally very slow or even intractable because of\nthe form of the so-called log-partition function. Various approximate methods\nhave recently been proposed in the literature and the main objective of this\npaper is to compare them. Through an extensive simulation study, we show that a\nsimple modification of a method relying on a Gaussian approximation achieves\ngood performance and is very fast. We present a real application in which we\nsearch for associations among causes of death recorded on French death\ncertificates. \n\n"}
{"id": "1004.2910", "contents": "Title: Conservative Hypothesis Tests and Confidence Intervals using Importance\n  Sampling Abstract: Importance sampling is a common technique for Monte Carlo approximation,\nincluding Monte Carlo approximation of p-values. Here it is shown that a simple\ncorrection of the usual importance sampling p-values creates valid p-values,\nmeaning that a hypothesis test created by rejecting the null when the p-value\nis <= alpha will also have a type I error rate <= alpha. This correction uses\nthe importance weight of the original observation, which gives valuable\ndiagnostic information under the null hypothesis. Using the corrected p-values\ncan be crucial for multiple testing and also in problems where evaluating the\naccuracy of importance sampling approximations is difficult. Inverting the\ncorrected p-values provides a useful way to create Monte Carlo confidence\nintervals that maintain the nominal significance level and use only a single\nMonte Carlo sample. Several applications are described, including accelerated\nmultiple testing for a large neurophysiological dataset and exact conditional\ninference for a logistic regression model with nuisance parameters. \n\n"}
{"id": "1005.0598", "contents": "Title: The pentagram map and Y-patterns Abstract: The pentagram map, introduced by R. Schwartz, is defined by the following\nconstruction: given a polygon as input, draw all of its \"shortest\" diagonals,\nand output the smaller polygon which they cut out. We employ the machinery of\ncluster algebras to obtain explicit formulas for the iterates of the pentagram\nmap. \n\n"}
{"id": "1007.1684", "contents": "Title: Spectral clustering and the high-dimensional stochastic blockmodel Abstract: Networks or graphs can easily represent a diverse set of data sources that\nare characterized by interacting units or actors. Social networks, representing\npeople who communicate with each other, are one example. Communities or\nclusters of highly connected actors form an essential feature in the structure\nof several empirical networks. Spectral clustering is a popular and\ncomputationally feasible method to discover these communities. The stochastic\nblockmodel [Social Networks 5 (1983) 109--137] is a social network model with\nwell-defined communities; each node is a member of one community. For a network\ngenerated from the Stochastic Blockmodel, we bound the number of nodes\n\"misclustered\" by spectral clustering. The asymptotic results in this paper are\nthe first clustering results that allow the number of clusters in the model to\ngrow with the number of nodes, hence the name high-dimensional. In order to\nstudy spectral clustering under the stochastic blockmodel, we first show that\nunder the more general latent space model, the eigenvectors of the normalized\ngraph Laplacian asymptotically converge to the eigenvectors of a \"population\"\nnormalized graph Laplacian. Aside from the implication for spectral clustering,\nthis provides insight into a graph visualization technique. Our method of\nstudying the eigenvectors of random matrices is original. \n\n"}
{"id": "1007.3424", "contents": "Title: Bacterial Community Reconstruction Using A Single Sequencing Reaction Abstract: Bacteria are the unseen majority on our planet, with millions of species and\ncomprising most of the living protoplasm. While current methods enable in-depth\nstudy of a small number of communities, a simple tool for breadth studies of\nbacterial population composition in a large number of samples is lacking. We\npropose a novel approach for reconstruction of the composition of an unknown\nmixture of bacteria using a single Sanger-sequencing reaction of the mixture.\nThis method is based on compressive sensing theory, which deals with\nreconstruction of a sparse signal using a small number of measurements.\nUtilizing the fact that in many cases each bacterial community is comprised of\na small subset of the known bacterial species, we show the feasibility of this\napproach for determining the composition of a bacterial mixture. Using\nsimulations, we show that sequencing a few hundred base-pairs of the 16S rRNA\ngene sequence may provide enough information for reconstruction of mixtures\ncontaining tens of species, out of tens of thousands, even in the presence of\nrealistic measurement noise. Finally, we show initial promising results when\napplying our method for the reconstruction of a toy experimental mixture with\nfive species. Our approach may have a potential for a practical and efficient\nway for identifying bacterial species compositions in biological samples. \n\n"}
{"id": "1007.4013", "contents": "Title: Quasi-concave density estimation Abstract: Maximum likelihood estimation of a log-concave probability density is\nformulated as a convex optimization problem and shown to have an equivalent\ndual formulation as a constrained maximum Shannon entropy problem. Closely\nrelated maximum Renyi entropy estimators that impose weaker concavity\nrestrictions on the fitted density are also considered, notably a minimum\nHellinger discrepancy estimator that constrains the reciprocal of the\nsquare-root of the density to be concave. A limiting form of these estimators\nconstrains solutions to the class of quasi-concave densities. \n\n"}
{"id": "1007.4148", "contents": "Title: Reconstruction of a Low-rank Matrix in the Presence of Gaussian Noise Abstract: In this paper we study the problem of reconstruction of a low-rank matrix\nobserved with additive Gaussian noise. First we show that under mild\nassumptions (about the prior distribution of the signal matrix) we can restrict\nour attention to reconstruction methods that are based on the singular value\ndecomposition of the observed matrix and act only on its singular values\n(preserving the singular vectors). Then we determine the effect of noise on the\nSVD of low-rank matrices by building a connection between matrix reconstruction\nproblem and spiked population model in random matrix theory. Based on this\nknowledge, we propose a new reconstruction method, called RMT, that is designed\nto reverse the effect of the noise on the singular values of the signal matrix\nand adjust for its effect on the singular vectors. With an extensive simulation\nstudy we show that the proposed method outperform even oracle versions of both\nsoft and hard thresholding methods and closely matches the performance of a\ngeneral oracle scheme. \n\n"}
{"id": "1008.1355", "contents": "Title: Control Variates for Reversible MCMC Samplers Abstract: A general methodology is introduced for the construction and effective\napplication of control variates to estimation problems involving data from\nreversible MCMC samplers. We propose the use of a specific class of functions\nas control variates, and we introduce a new, consistent estimator for the\nvalues of the coefficients of the optimal linear combination of these\nfunctions. The form and proposed construction of the control variates is\nderived from our solution of the Poisson equation associated with a specific\nMCMC scenario. The new estimator, which can be applied to the same MCMC sample,\nis derived from a novel, finite-dimensional, explicit representation for the\noptimal coefficients. The resulting variance-reduction methodology is primarily\napplicable when the simulated data are generated by a conjugate random-scan\nGibbs sampler. MCMC examples of Bayesian inference problems demonstrate that\nthe corresponding reduction in the estimation variance is significant, and that\nin some cases it can be quite dramatic. Extensions of this methodology in\nseveral directions are given, including certain families of Metropolis-Hastings\nsamplers and hybrid Metropolis-within-Gibbs algorithms. Corresponding\nsimulation examples are presented illustrating the utility of the proposed\nmethods. All methodological and asymptotic arguments are rigorously justified\nunder easily verifiable and essentially minimal conditions. \n\n"}
{"id": "1008.4937", "contents": "Title: Bounds on generalized Frobenius numbers Abstract: Let $N \\geq 2$ and let $1 < a_1 < ... < a_N$ be relatively prime integers.\nThe Frobenius number of this $N$-tuple is defined to be the largest positive\ninteger that has no representation as $\\sum_{i=1}^N a_i x_i$ where\n$x_1,...,x_N$ are non-negative integers. More generally, the $s$-Frobenius\nnumber is defined to be the largest positive integer that has precisely $s$\ndistinct representations like this. We use techniques from the Geometry of\nNumbers to give upper and lower bounds on the $s$-Frobenius number for any\nnonnegative integer $s$. \n\n"}
{"id": "1009.1926", "contents": "Title: Robust Bayesian variable selection with sub-harmonic priors Abstract: This paper studies Bayesian variable selection in linear models with general\nspherically symmetric error distributions. We propose sub-harmonic priors which\narise as a class of mixtures of Zellner's g-priors for which the Bayes factors\nare independent of the underlying error distribution, as long as it is in the\nspherically symmetric class. Because of this invariance to spherically\nsymmetric error distribution, we refer to our method as a robust Bayesian\nvariable selection method. We demonstrate that our Bayes factors have model\nselection consistency and are coherent. We also develop Laplace approximations\nto Bayes factors for a number of recently studied mixtures of g-priors that\nhave recently appeared in the literature (including our own) for Gaussian\nerrors. These approximations, in each case, are given by the Gaussian Bayes\nfactor based on BIC times a simple rational function of the prior's\nhyper-parameters and the R^2's for the respective models. We also extend model\nselection consistency for several g-prior based Bayes factor methods for\nGaussian errors to the entire class of spherically symmetric error\ndistributions. Additionally we demonstrate that our class of sub-harmonic\npriors are the only ones within a large class of mixtures of g-priors studied\nin the literature which are robust in our sense. A simulation study and an\nanalysis of two real data sets indicates good performance of our robust Bayes\nfactors relative to BIC and to other mixture of g-prior based methods. \n\n"}
{"id": "1009.3954", "contents": "Title: Infinite Bar-Joint Frameworks, Crystals and Operator Theory Abstract: A theory of flexibility and rigidity is developed for general infinite\nbar-joint frameworks (G,p). Determinations of nondeformability through\nvanishing flexibility are obtained as well as sufficient conditions for\ndeformability. Forms of infinitesimal flexibility are defined in terms of the\noperator theory of the associated infinite rigidity matrix R(G,p). The\nmatricial symbol function of an abstract crystal framework is introduced, being\nthe matrix-valued function on the $d$-torus representing R(G,p) as a Hilbert\nspace operator. The symbol function is related to infinitesimal flexibility,\ndeformability and isostaticity. Various generic abstract crystal frameworks\nwhich are in Maxwellian equilibrium, such as certain 4-regular planar\nframeworks, are proven to be square-summably infinitesimally rigid as well as\nsmoothly deformable in infinitely many ways. The symbol function of a\nthree-dimensional crystal framework determines the infinitesimal wave flexes in\nmodels for the low energy vibrational modes (RUMs) in material crystals. For\ncrystal frameworks with inversion symmetry it is shown that the RUMS appear in\nsurfaces, generalising a result of F. Wegner for tetrahedral crystals. \n\n"}
{"id": "1009.5689", "contents": "Title: Square-Root Lasso: Pivotal Recovery of Sparse Signals via Conic\n  Programming Abstract: We propose a pivotal method for estimating high-dimensional sparse linear\nregression models, where the overall number of regressors $p$ is large,\npossibly much larger than $n$, but only $s$ regressors are significant. The\nmethod is a modification of the lasso, called the square-root lasso. The method\nis pivotal in that it neither relies on the knowledge of the standard deviation\n$\\sigma$ or nor does it need to pre-estimate $\\sigma$. Moreover, the method\ndoes not rely on normality or sub-Gaussianity of noise. It achieves near-oracle\nperformance, attaining the convergence rate $\\sigma \\{(s/n)\\log p\\}^{1/2}$ in\nthe prediction norm, and thus matching the performance of the lasso with known\n$\\sigma$. These performance results are valid for both Gaussian and\nnon-Gaussian errors, under some mild moment restrictions. We formulate the\nsquare-root lasso as a solution to a convex conic programming problem, which\nallows us to implement the estimator using efficient algorithmic methods, such\nas interior-point and first-order methods. \n\n"}
{"id": "1009.6158", "contents": "Title: Polytopes with Special Simplices Abstract: For a polytope P a simplex S with vertex set V(S) is called a special simplex\nif every facet of P contains all but exactly one vertex of S. For such\npolytopes P with face complex F(P) containing a special simplex the subcomplex\nF(P) / V(S) of all faces not containing vertices of S is the boundary of a\npolytope Q - the basis polytope of P. If additionally the dimension of the\naffine basis space of F(P) / V(S) equals dim(Q), we call P meek; otherwise we\ncall P wild. We give a full combinatorial classification and techniques for\ngeometric construction of the class of meek polytopes with special simplices.\nWe show that every wild polytope P' with special simplex can be constructed out\nof a particular meek one P by intersecting P with particular hyperplanes. It is\nnon-trivial to find all these hyperplanes for an arbitrary basis polytope; we\ngive an exact description for 2-basis polytopes. Furthermore we show that the\nf-vector of each wild polytope with special simplex is component wise bounded\nabove by the f-vector of a particular meek one which can be computed\nexplicitly. Finally, we discuss the n-cube as a non-trivial example of a wild\npolytope with special simplex and prove that its basis polytope is the zonotope\ngiven by the Minkowski sum of the (n-1)-cube and the vector (1,...,1).\nPolytopes with special simplex have applications on Ehrhart theory, toric rings\nand were just used by Francisco Santos to construct a counter-example\ndisproving the Hirsch conjecture. \n\n"}
{"id": "1010.3390", "contents": "Title: Local shrinkage rules, Levy processes, and regularized regression Abstract: We use Levy processes to generate joint prior distributions, and therefore\npenalty functions, for a location parameter as p grows large. This generalizes\nthe class of local-global shrinkage rules based on scale mixtures of normals,\nilluminates new connections among disparate methods, and leads to new results\nfor computing posterior means and modes under a wide class of priors. We extend\nthis framework to large-scale regularized regression problems where p>n, and\nprovide comparisons with other methodologies. \n\n"}
{"id": "1011.0057", "contents": "Title: Discussion of \"Riemann manifold Langevin and Hamiltonian Monte Carlo\n  methods'' by M. Girolami and B. Calderhead Abstract: This technical report is the union of two contributions to the discussion of\nthe Read Paper \"Riemann manifold Langevin and Hamiltonian Monte Carlo methods\"\nby B. Calderhead and M. Girolami, presented in front of the Royal Statistical\nSociety on October 13th 2010 and to appear in the Journal of the Royal\nStatistical Society Series B. The first comment establishes a parallel and\npossible interactions with Adaptive Monte Carlo methods. The second comment\nexposes a detailed study of Riemannian Manifold Hamiltonian Monte Carlo (RMHMC)\nfor a weakly identifiable model presenting a strong ridge in its geometry. \n\n"}
{"id": "1011.1170", "contents": "Title: Interacting Multiple Try Algorithms with Different Proposal\n  Distributions Abstract: We propose a new class of interacting Markov chain Monte Carlo (MCMC)\nalgorithms designed for increasing the efficiency of a modified multiple-try\nMetropolis (MTM) algorithm. The extension with respect to the existing MCMC\nliterature is twofold. The sampler proposed extends the basic MTM algorithm by\nallowing different proposal distributions in the multiple-try generation step.\nWe exploit the structure of the MTM algorithm with different proposal\ndistributions to naturally introduce an interacting MTM mechanism (IMTM) that\nexpands the class of population Monte Carlo methods. We show the validity of\nthe algorithm and discuss the choice of the selection weights and of the\ndifferent proposals. We provide numerical studies which show that the new\nalgorithm can perform better than the basic MTM algorithm and that the\ninteraction mechanism allows the IMTM to efficiently explore the state space. \n\n"}
{"id": "1011.1253", "contents": "Title: Coupling optional P\\'olya trees and the two sample problem Abstract: Testing and characterizing the difference between two data samples is of\nfundamental interest in statistics. Existing methods such as Kolmogorov-Smirnov\nand Cramer-von-Mises tests do not scale well as the dimensionality increases\nand provides no easy way to characterize the difference should it exist. In\nthis work, we propose a theoretical framework for inference that addresses\nthese challenges in the form of a prior for Bayesian nonparametric analysis.\nThe new prior is constructed based on a random-partition-and-assignment\nprocedure similar to the one that defines the standard optional P\\'olya tree\ndistribution, but has the ability to generate multiple random distributions\njointly. These random probability distributions are allowed to \"couple\", that\nis to have the same conditional distribution, on subsets of the sample space.\nWe show that this \"coupling optional P\\'olya tree\" prior provides a convenient\nand effective way for both the testing of two sample difference and the\nlearning of the underlying structure of the difference. In addition, we discuss\nsome practical issues in the computational implementation of this prior and\nprovide several numerical examples to demonstrate its work. \n\n"}
{"id": "1011.6256", "contents": "Title: Nuclear norm penalization and optimal rates for noisy low rank matrix\n  completion Abstract: This paper deals with the trace regression model where $n$ entries or linear\ncombinations of entries of an unknown $m_1\\times m_2$ matrix $A_0$ corrupted by\nnoise are observed. We propose a new nuclear norm penalized estimator of $A_0$\nand establish a general sharp oracle inequality for this estimator for\narbitrary values of $n,m_1,m_2$ under the condition of isometry in expectation.\nThen this method is applied to the matrix completion problem. In this case, the\nestimator admits a simple explicit form and we prove that it satisfies oracle\ninequalities with faster rates of convergence than in the previous works. They\nare valid, in particular, in the high-dimensional setting $m_1m_2\\gg n$. We\nshow that the obtained rates are optimal up to logarithmic factors in a minimax\nsense and also derive, for any fixed matrix $A_0$, a non-minimax lower bound on\nthe rate of convergence of our estimator, which coincides with the upper bound\nup to a constant factor. Finally, we show that our procedure provides an exact\nrecovery of the rank of $A_0$ with probability close to 1. We also discuss the\nstatistical learning setting where there is no underlying model determined by\n$A_0$ and the aim is to find the best trace regression model approximating the\ndata. \n\n"}
{"id": "1012.3795", "contents": "Title: Estimating Networks With Jumps Abstract: We study the problem of estimating a temporally varying coefficient and\nvarying structure (VCVS) graphical model underlying nonstationary time series\ndata, such as social states of interacting individuals or microarray expression\nprofiles of gene networks, as opposed to i.i.d. data from an invariant model\nwidely considered in current literature of structural estimation. In\nparticular, we consider the scenario in which the model evolves in a piece-wise\nconstant fashion. We propose a procedure that minimizes the so-called TESLA\nloss (i.e., temporally smoothed L1 regularized regression), which allows\njointly estimating the partition boundaries of the VCVS model and the\ncoefficient of the sparse precision matrix on each block of the partition. A\nhighly scalable proximal gradient method is proposed to solve the resultant\nconvex optimization problem; and the conditions for sparsistent estimation and\nthe convergence rate of both the partition boundaries and the network structure\nare established for the first time for such estimators. \n\n"}
{"id": "1101.5091", "contents": "Title: Why approximate Bayesian computational (ABC) methods cannot handle model\n  choice problems Abstract: Approximate Bayesian computation (ABC), also known as likelihood-free\nmethods, have become a favourite tool for the analysis of complex stochastic\nmodels, primarily in population genetics but also in financial analyses. We\nadvocated in Grelaud et al. (2009) the use of ABC for Bayesian model choice in\nthe specific case of Gibbs random fields (GRF), relying on a sufficiency\nproperty mainly enjoyed by GRFs to show that the approach was legitimate.\nDespite having previously suggested the use of ABC for model choice in a wider\nrange of models in the DIY ABC software (Cornuet et al., 2008), we present\ntheoretical evidence that the general use of ABC for model choice is fraught\nwith danger in the sense that no amount of computation, however large, can\nguarantee a proper approximation of the posterior probabilities of the models\nunder comparison. \n\n"}
{"id": "1101.5838", "contents": "Title: Adaptive Gibbs samplers and related MCMC methods Abstract: We consider various versions of adaptive Gibbs and Metropolis-within-Gibbs\nsamplers, which update their selection probabilities (and perhaps also their\nproposal distributions) on the fly during a run by learning as they go in an\nattempt to optimize the algorithm. We present a cautionary example of how even\na simple-seeming adaptive Gibbs sampler may fail to converge. We then present\nvarious positive results guaranteeing convergence of adaptive Gibbs samplers\nunder certain conditions. \n\n"}
{"id": "1103.0542", "contents": "Title: Optimal scaling and diffusion limits for the Langevin algorithm in high\n  dimensions Abstract: The Metropolis-adjusted Langevin (MALA) algorithm is a sampling algorithm\nwhich makes local moves by incorporating information about the gradient of the\nlogarithm of the target density. In this paper we study the efficiency of MALA\non a natural class of target measures supported on an infinite dimensional\nHilbert space. These natural measures have density with respect to a Gaussian\nrandom field measure and arise in many applications such as Bayesian\nnonparametric statistics and the theory of conditioned diffusions. We prove\nthat, started in stationarity, a suitably interpolated and scaled version of\nthe Markov chain corresponding to MALA converges to an infinite dimensional\ndiffusion process. Our results imply that, in stationarity, the MALA algorithm\napplied to an N-dimensional approximation of the target will take\n$\\mathcal{O}(N^{1/3})$ steps to explore the invariant measure, comparing\nfavorably with the Random Walk Metropolis which was recently shown to require\n$\\mathcal{O}(N)$ steps when applied to the same class of problems. \n\n"}
{"id": "1103.1914", "contents": "Title: Crystal frameworks, symmetry and affinely periodic flexes Abstract: Symmetry equations are obtained for the rigidity matrices associated with\nvarious forms of infinitesimal flexibility for an idealised bond-node crystal\nframework $\\C$ in $\\bR^d$. These equations are used to derive symmetry-adapted\nMaxwell-Calladine counting formulae for periodic self-stresses and affinely\nperiodic infinitesimal mechanisms. The symmetry equations also lead to general\nFowler-Guest formulae connecting the character lists of subrepresentations of\nthe crystallographic space and point groups which are associated with bonds,\nnodes, stresses, flexes and rigid motions. A new derivation is also given for\nthe Borcea-Streinu rigidity matrix and the correspondence between its nullspace\nand the space of affinely periodic infinitesimal flexes. \n\n"}
{"id": "1103.2731", "contents": "Title: The brick polytope of a sorting network Abstract: The associahedron is a polytope whose graph is the graph of flips on\ntriangulations of a convex polygon. Pseudotriangulations and\nmultitriangulations generalize triangulations in two different ways, which have\nbeen unified by Pilaud and Pocchiola in their study of flip graphs on\npseudoline arrangements with contacts supported by a given sorting network.\n  In this paper, we construct the brick polytope of a sorting network, obtained\nas the convex hull of the brick vectors associated to each pseudoline\narrangement supported by the network. We combinatorially characterize the\nvertices of this polytope, describe its faces, and decompose it as a Minkowski\nsum of matroid polytopes.\n  Our brick polytopes include Hohlweg and Lange's many realizations of the\nassociahedron, which arise as brick polytopes for certain well-chosen sorting\nnetworks. We furthermore discuss the brick polytopes of sorting networks\nsupporting pseudoline arrangements which correspond to multitriangulations of\nconvex polygons: our polytopes only realize subgraphs of the flip graphs on\nmultitriangulations and they cannot appear as projections of a hypothetical\nmultiassociahedron. \n\n"}
{"id": "1103.2967", "contents": "Title: An Inductive Construction of (2,1)-tight Graphs Abstract: The simple graphs $G=(V,E)$ that satisfy $|E'|\\leq 2|V'|-l$ for any subgraph\n(and for $l=1,2,3$) are the $(2,l)$-sparse graphs. Those that also satisfy\n$|E|=2|V|-l$ are the $(2,l)$-tight graphs. These can be characterised by their\ndecompositions into two edge disjoint spanning subgraphs of various types. The\nHenneberg--Laman theorem characterises $(2,3)$-tight graphs inductively in\nterms of two simple moves, known as the Henneberg moves. Recently this has been\nextended, via the addition of a graph extension move, to the case of\n$(2,2)$-tight graphs. Here an alternative characterisation is provided by means\nof vertex-to-$K_4$ and edge-to-$K_3$ moves, and this is extended to the\n$(2,1)$-tight graphs by addition of an edge joining move. Similar\ncharacterisations of $(2,l)$-sparse graphs are also provided. \n\n"}
{"id": "1103.3508", "contents": "Title: Approximating Probability Densities by Iterated Laplace Approximations Abstract: The Laplace approximation is an old, but frequently used method to\napproximate integrals for Bayesian calculations. In this paper we develop an\nextension of the Laplace approximation, by applying it iteratively to the\nresidual, i.e., the difference between the current approximation and the true\nfunction. The final approximation is thus a linear combination of multivariate\nnormal densities, where the coefficients are chosen to achieve a good fit to\nthe target distribution. We illustrate on real and artificial examples that the\nproposed procedure is a computationally efficient alternative to current\napproaches for approximation of multivariate probability densities. The\nR-package iterLap implementing the methods described in this article is\navailable from the CRAN servers. \n\n"}
{"id": "1103.5202", "contents": "Title: Fast Learning Rate of lp-MKL and its Minimax Optimality Abstract: In this paper, we give a new sharp generalization bound of lp-MKL which is a\ngeneralized framework of multiple kernel learning (MKL) and imposes\nlp-mixed-norm regularization instead of l1-mixed-norm regularization. We\nutilize localization techniques to obtain the sharp learning rate. The bound is\ncharacterized by the decay rate of the eigenvalues of the associated kernels. A\nlarger decay rate gives a faster convergence rate. Furthermore, we give the\nminimax learning rate on the ball characterized by lp-mixed-norm in the product\nspace. Then we show that our derived learning rate of lp-MKL achieves the\nminimax optimal rate on the lp-mixed-norm ball. \n\n"}
{"id": "1104.0354", "contents": "Title: Low-rank Matrix Recovery from Errors and Erasures Abstract: This paper considers the recovery of a low-rank matrix from an observed\nversion that simultaneously contains both (a) erasures: most entries are not\nobserved, and (b) errors: values at a constant fraction of (unknown) locations\nare arbitrarily corrupted. We provide a new unified performance guarantee on\nwhen the natural convex relaxation of minimizing rank plus support succeeds in\nexact recovery. Our result allows for the simultaneous presence of random and\ndeterministic components in both the error and erasure patterns. On the one\nhand, corollaries obtained by specializing this one single result in different\nways recover (up to poly-log factors) all the existing works in matrix\ncompletion, and sparse and low-rank matrix recovery. On the other hand, our\nresults also provide the first guarantees for (a) recovery when we observe a\nvanishing fraction of entries of a corrupted matrix, and (b) deterministic\nmatrix completion. \n\n"}
{"id": "1104.0401", "contents": "Title: The second Voronoi conjecture on parallelohedra for zonotopes Abstract: We prove the second Voronoi conjecture on parallelohedra for zonotope. We\nshow that for a given face-to-face tiling of d-dimensional Euclidean space into\nparallel copies of zonotope Z there are d vectors, connecting centers of\nzonotopes with common facet, that are basis of the correspondent lattice of the\ntiling. \n\n"}
{"id": "1104.1204", "contents": "Title: Planar Cycle Covering Graphs Abstract: We describe a new variational lower-bound on the minimum energy configuration\nof a planar binary Markov Random Field (MRF). Our method is based on adding\nauxiliary nodes to every face of a planar embedding of the graph in order to\ncapture the effect of unary potentials. A ground state of the resulting\napproximation can be computed efficiently by reduction to minimum-weight\nperfect matching. We show that optimization of variational parameters achieves\nthe same lower-bound as dual-decomposition into the set of all cycles of the\noriginal graph. We demonstrate that our variational optimization converges\nquickly and provides high-quality solutions to hard combinatorial problems\n10-100x faster than competing algorithms that optimize the same bound. \n\n"}
{"id": "1105.0760", "contents": "Title: Variational Bayes approach for model aggregation in unsupervised\n  classification with Markovian dependency Abstract: We consider a binary unsupervised classification problem where each\nobservation is associated with an unobserved label that we want to retrieve.\nMore precisely, we assume that there are two groups of observation: normal and\nabnormal. The `normal' observations are coming from a known distribution\nwhereas the distribution of the `abnormal' observations is unknown. Several\nmodels have been developed to fit this unknown distribution. In this paper, we\npropose an alternative based on a mixture of Gaussian distributions. The\ninference is done within a variational Bayesian framework and our aim is to\ninfer the posterior probability of belonging to the class of interest. To this\nend, it makes no sense to estimate the mixture component number since each\nmixture model provides more or less relevant information to the posterior\nprobability estimation. By computing a weighted average (named aggregated\nestimator) over the model collection, Bayesian Model Averaging (BMA) is one way\nof combining models in order to account for information provided by each model.\nThe aim is then the estimation of the weights and the posterior probability for\none specific model. In this work, we derive optimal approximations of these\nquantities from the variational theory and propose other approximations of the\nweights. To perform our method, we consider that the data are dependent\n(Markovian dependency) and hence we consider a Hidden Markov Model. A\nsimulation study is carried out to evaluate the accuracy of the estimates in\nterms of classification. We also present an application to the analysis of\npublic health surveillance systems. \n\n"}
{"id": "1105.0841", "contents": "Title: Generalized Frobenius numbers: Bounds and average behavior Abstract: We produce new upper and lower bounds for the s-Frobenius number by relating\nit to the so called s-covering radius of a certain convex body with respect to\na certain lattice; this generalizes a well-known theorem of R. Kannan for the\nclassical Frobenius number. Using these bounds, we obtain results on the\naverage behavior of the s-Frobenius number, extending analogous recent\ninvestigations for the classical Frobenius number by a variety of authors. We\nalso derive bounds on the s-covering radius, an interesting geometric quantity\nin its own right. \n\n"}
{"id": "1105.1475", "contents": "Title: Pivotal estimation via square-root Lasso in nonparametric regression Abstract: We propose a self-tuning $\\sqrt{\\mathrm {Lasso}}$ method that simultaneously\nresolves three important practical problems in high-dimensional regression\nanalysis, namely it handles the unknown scale, heteroscedasticity and (drastic)\nnon-Gaussianity of the noise. In addition, our analysis allows for badly\nbehaved designs, for example, perfectly collinear regressors, and generates\nsharp bounds even in extreme cases, such as the infinite variance case and the\nnoiseless case, in contrast to Lasso. We establish various nonasymptotic bounds\nfor $\\sqrt{\\mathrm {Lasso}}$ including prediction norm rate and sparsity. Our\nanalysis is based on new impact factors that are tailored for bounding\nprediction norm. In order to cover heteroscedastic non-Gaussian noise, we rely\non moderate deviation theory for self-normalized sums to achieve Gaussian-like\nresults under weak conditions. Moreover, we derive bounds on the performance of\nordinary least square (ols) applied to the model selected by $\\sqrt{\\mathrm\n{Lasso}}$ accounting for possible misspecification of the selected model. Under\nmild conditions, the rate of convergence of ols post $\\sqrt{\\mathrm {Lasso}}$\nis as good as $\\sqrt{\\mathrm {Lasso}}$'s rate. As an application, we consider\nthe use of $\\sqrt{\\mathrm {Lasso}}$ and ols post $\\sqrt{\\mathrm {Lasso}}$ as\nestimators of nuisance parameters in a generic semiparametric problem\n(nonlinear moment condition or $Z$-problem), resulting in a construction of\n$\\sqrt{n}$-consistent and asymptotically normal estimators of the main\nparameters. \n\n"}
{"id": "1105.4823", "contents": "Title: Simulation in Statistics Abstract: Simulation has become a standard tool in statistics because it may be the\nonly tool available for analysing some classes of probabilistic models. We\nreview in this paper simulation tools that have been specifically derived to\naddress statistical challenges and, in particular, recent advances in the areas\nof adaptive Markov chain Monte Carlo (MCMC) algorithms, and approximate\nBayesian calculation (ABC) algorithms. \n\n"}
{"id": "1106.1622", "contents": "Title: Large-Scale Convex Minimization with a Low-Rank Constraint Abstract: We address the problem of minimizing a convex function over the space of\nlarge matrices with low rank. While this optimization problem is hard in\ngeneral, we propose an efficient greedy algorithm and derive its formal\napproximation guarantees. Each iteration of the algorithm involves\n(approximately) finding the left and right singular vectors corresponding to\nthe largest singular value of a certain matrix, which can be calculated in\nlinear time. This leads to an algorithm which can scale to large matrices\narising in several applications such as matrix completion for collaborative\nfiltering and robust low rank matrix approximation. \n\n"}
{"id": "1106.1916", "contents": "Title: Threshold estimation based on a p-value framework in dose-response and\n  regression settings Abstract: We use p-values to identify the threshold level at which a regression\nfunction takes off from its baseline value, a problem motivated by applications\nin toxicological and pharmacological dose-response studies and environmental\nstatistics. We study the problem in two sampling settings: one where multiple\nresponses can be obtained at a number of different covariate-levels and the\nother the standard regression setting involving limited number of response\nvalues at each covariate. Our procedure involves testing the hypothesis that\nthe regression function is at its baseline at each covariate value and then\ncomputing the potentially approximate p-value of the test. An estimate of the\nthreshold is obtained by fitting a piecewise constant function with a single\njump discontinuity, otherwise known as a stump, to these observed p-values, as\nthey behave in markedly different ways on the two sides of the threshold. The\nestimate is shown to be consistent and its finite sample properties are studied\nthrough simulations. Our approach is computationally simple and extends to the\nestimation of the baseline value of the regression function, heteroscedastic\nerrors and to time-series. It is illustrated on some real data applications. \n\n"}
{"id": "1106.2525", "contents": "Title: Uniform Stability of a Particle Approximation of the Optimal Filter\n  Derivative Abstract: Sequential Monte Carlo methods, also known as particle methods, are a widely\nused set of computational tools for inference in non-linear non-Gaussian\nstate-space models. In many applications it may be necessary to compute the\nsensitivity, or derivative, of the optimal filter with respect to the static\nparameters of the state-space model; for instance, in order to obtain maximum\nlikelihood model parameters of interest, or to compute the optimal controller\nin an optimal control problem. In Poyiadjis et al. [2011] an original particle\nalgorithm to compute the filter derivative was proposed and it was shown using\nnumerical examples that the particle estimate was numerically stable in the\nsense that it did not deteriorate over time. In this paper we substantiate this\nclaim with a detailed theoretical study. Lp bounds and a central limit theorem\nfor this particle approximation of the filter derivative are presented. It is\nfurther shown that under mixing conditions these Lp bounds and the asymptotic\nvariance characterized by the central limit theorem are uniformly bounded with\nrespect to the time index. We demon- strate the performance predicted by theory\nwith several numerical examples. We also use the particle approximation of the\nfilter derivative to perform online maximum likelihood parameter estimation for\na stochastic volatility model. \n\n"}
{"id": "1106.5544", "contents": "Title: Multi-parameter projection theorems with applications to sums-products\n  and finite point configurations in the Euclidean setting Abstract: In this paper we study multi-parameter projection theorems for fractal sets.\nWith the help of these estimates, we recover results about the size of $A \\cdot\nA+...+A \\cdot A$, where $A$ is a subset of the real line of a given Hausdorff\ndimension, $A+A=\\{a+a': a,a' \\in A \\}$ and $A \\cdot A=\\{a \\cdot a': a,a' \\in\nA\\}$. We also use projection results and inductive arguments to show that if a\nHausdorff dimension of a subset of ${\\Bbb R}^d$ is sufficiently large, then the\n${k+1 \\choose 2}$-dimensional Lebesgue measure of the set of $k$-simplexes\ndetermined by this set is positive. The sharpness of these results and\nconnection with number theoretic estimates is also discussed. \n\n"}
{"id": "1106.5941", "contents": "Title: Split Hamiltonian Monte Carlo Abstract: We show how the Hamiltonian Monte Carlo algorithm can sometimes be speeded up\nby \"splitting\" the Hamiltonian in a way that allows much of the movement around\nthe state space to be done at low computational cost. One context where this is\npossible is when the log density of the distribution of interest (the potential\nenergy function) can be written as the log of a Gaussian density, which is a\nquadratic function, plus a slowly varying function. Hamiltonian dynamics for\nquadratic energy functions can be analytically solved. With the splitting\ntechnique, only the slowly-varying part of the energy needs to be handled\nnumerically, and this can be done with a larger stepsize (and hence fewer\nsteps) than would be necessary with a direct simulation of the dynamics.\nAnother context where splitting helps is when the most important terms of the\npotential energy function and its gradient can be evaluated quickly, with only\na slowly-varying part requiring costly computations. With splitting, the quick\nportion can be handled with a small stepsize, while the costly portion uses a\nlarger stepsize. We show that both of these splitting approaches can reduce the\ncomputational cost of sampling from the posterior distribution for a logistic\nregression model, using either a Gaussian approximation centered on the\nposterior mode, or a Hamiltonian split into a term that depends on only a small\nnumber of critical cases, and another term that involves the larger number of\ncases whose influence on the posterior distribution is small. Supplemental\nmaterials for this paper are available online. \n\n"}
{"id": "1107.5959", "contents": "Title: Expectation-Propagation for Likelihood-Free Inference Abstract: Many models of interest in the natural and social sciences have no\nclosed-form likelihood function, which means that they cannot be treated using\nthe usual techniques of statistical inference. In the case where such models\ncan be efficiently simulated, Bayesian inference is still possible thanks to\nthe Approximate Bayesian Computation (ABC) algorithm. Although many refinements\nhave been suggested, ABC inference is still far from routine. ABC is often\nexcruciatingly slow due to very low acceptance rates. In addition, ABC requires\nintroducing a vector of \"summary statistics\", the choice of which is relatively\narbitrary, and often require some trial and error, making the whole process\nquite laborious for the user.\n  We introduce in this work the EP-ABC algorithm, which is an adaptation to the\nlikelihood-free context of the variational approximation algorithm known as\nExpectation Propagation (Minka, 2001). The main advantage of EP-ABC is that it\nis faster by a few orders of magnitude than standard algorithms, while\nproducing an overall approximation error which is typically negligible. A\nsecond advantage of EP-ABC is that it replaces the usual global ABC constraint\non the vector of summary statistics computed on the whole dataset, by n local\nconstraints of the form that apply separately to each data-point. As a\nconsequence, it is often possible to do away with summary statistics entirely.\nIn that case, EP-ABC approximates directly the evidence (marginal likelihood)\nof the model.\n  Comparisons are performed in three real-world applications which are typical\nof likelihood-free inference, including one application in neuroscience which\nis novel, and possibly too challenging for standard ABC techniques. \n\n"}
{"id": "1109.2279", "contents": "Title: The Bayesian Bridge Abstract: We propose the Bayesian bridge estimator for regularized regression and\nclassification. Two key mixture representations for the Bayesian bridge model\nare developed: (1) a scale mixture of normals with respect to an alpha-stable\nrandom variable; and (2) a mixture of Bartlett--Fejer kernels (or triangle\ndensities) with respect to a two-component mixture of gamma random variables.\nBoth lead to MCMC methods for posterior simulation, and these methods turn out\nto have complementary domains of maximum efficiency. The first representation\nis a well known result due to West (1987), and is the better choice for\ncollinear design matrices. The second representation is new, and is more\nefficient for orthogonal problems, largely because it avoids the need to deal\nwith exponentially tilted stable random variables. It also provides insight\ninto the multimodality of the joint posterior distribution, a feature of the\nbridge model that is notably absent under ridge or lasso-type priors. We prove\na theorem that extends this representation to a wider class of densities\nrepresentable as scale mixtures of betas, and provide an explicit inversion\nformula for the mixing distribution. The connections with slice sampling and\nscale mixtures of normals are explored. On the practical side, we find that the\nBayesian bridge model outperforms its classical cousin in estimation and\nprediction across a variety of data sets, both simulated and real. We also show\nthat the MCMC for fitting the bridge model exhibits excellent mixing\nproperties, particularly for the global scale parameter. This makes for a\nfavorable contrast with analogous MCMC algorithms for other sparse Bayesian\nmodels. All methods described in this paper are implemented in the R package\nBayesBridge. An extensive set of simulation results are provided in two\nsupplemental files. \n\n"}
{"id": "1109.6179", "contents": "Title: On maximal S-free sets and the Helly number for the family of S-convex\n  sets Abstract: We study two combinatorial parameters, which we denote by f(S) and h(S),\nassociated to an arbitrary set S \\subseteq R^d, where d \\in N. In the\nnondegenerate situation, f(S) is the largest possible number of facets of a\nd-dimensional polyhedron L such that the interior of L is disjoint with S and L\nis inclusion-maximal with respect to this property. The parameter h(S) is the\nHelly number of the family of all sets that can be given as the intersection of\nS with a convex subset of R^d. We obtain the inequality f(S) \\le h(S) for an\narbitrary S and the equality f(S)=h(S) for every discrete S. Furthermore,\nmotivated by research in integer and mixed-integer optimization, we show that\n2^d is the sharp upper bound on f(S) in the case S = (Z^d \\times R^n) \\cap C,\nwhere n \\ge 0 and C \\subseteq R^{d+n} is convex. The presented material\ngeneralizes and unifies results of various authors, including the result h(Z^d)\n= 2^d of Doignon, the related result f(Z^d)=2^d of Lov\\'asz and the inequality\nf(Z^d \\cap C) \\le 2^d, which has recently been proved for every convex set C\n\\subseteq R^d by Dey & Mor\\'an. \n\n"}
{"id": "1109.6779", "contents": "Title: Stability properties of some particle filters Abstract: Under multiplicative drift and other regularity conditions, it is established\nthat the asymptotic variance associated with a particle filter approximation of\nthe prediction filter is bounded uniformly in time, and the nonasymptotic,\nrelative variance associated with a particle approximation of the normalizing\nconstant is bounded linearly in time. The conditions are demonstrated to hold\nfor some hidden Markov models on noncompact state spaces. The particle\nstability results are obtained by proving $v$-norm multiplicative stability and\nexponential moment results for the underlying Feynman-Kac formulas. \n\n"}
{"id": "1110.0721", "contents": "Title: Properties and applications of Fisher distribution on the rotation group Abstract: We study properties of Fisher distribution (von Mises-Fisher distribution,\nmatrix Langevin distribution) on the rotation group SO(3). In particular we\napply the holonomic gradient descent, introduced by Nakayama et al. (2011), and\na method of series expansion for evaluating the normalizing constant of the\ndistribution and for computing the maximum likelihood estimate. The rotation\ngroup can be identified with the Stiefel manifold of two orthonormal vectors.\nTherefore from the viewpoint of statistical modeling, it is of interest to\ncompare Fisher distributions on these manifolds. We illustrate the difference\nwith an example of near-earth objects data. \n\n"}
{"id": "1110.2894", "contents": "Title: Two algorithms for fitting constrained marginal models Abstract: We study in detail the two main algorithms which have been considered for\nfitting constrained marginal models to discrete data, one based on Lagrange\nmultipliers and the other on a regression model. We show that the updates\nproduced by the two methods are identical, but that the Lagrangian method is\nmore efficient in the case of identically distributed observations. We provide\na generalization of the regression algorithm for modelling the effect of\nexogenous individual-level covariates, a context in which the use of the\nLagrangian algorithm would be infeasible for even moderate sample sizes. An\nextension of the method to likelihood-based estimation under $L_1$-penalties is\nalso considered. \n\n"}
{"id": "1110.4539", "contents": "Title: Markov Equivalences for Subclasses of Loopless Mixed Graphs Abstract: In this paper we discuss four problems regarding Markov equivalences for\nsubclasses of loopless mixed graphs. We classify these four problems as finding\nconditions for internal Markov equivalence, which is Markov equivalence within\na subclass, for external Markov equivalence, which is Markov equivalence\nbetween subclasses, for representational Markov equivalence, which is the\npossibility of a graph from a subclass being Markov equivalent to a graph from\nanother subclass, and finding algorithms to generate a graph from a certain\nsubclass that is Markov equivalent to a given graph. We particularly focus on\nthe class of maximal ancestral graphs and its subclasses, namely regression\ngraphs, bidirected graphs, undirected graphs, and directed acyclic graphs, and\npresent novel results for representational Markov equivalence and algorithms. \n\n"}
{"id": "1111.2932", "contents": "Title: Belt diameter of $\\Pi$-zonotopes Abstract: We prove that any d-dimensional zonotope obtained from permutahedron by\ndeleting zone vectors has belt diameter at most 3. Moreover if d is not greater\nthan 6 then its belt diameter is bounded from above by 2. Also we show that\nthese bounds are sharp. As a consequence we show that diameter of the edge\ngraph of dual polytope for such zonotopes is not greater then 4 and 3\nrespectively. \n\n"}
{"id": "1112.0801", "contents": "Title: Different forms of metric characterizations of classes of Banach spaces Abstract: For each sequence X of finite-dimensional Banach spaces there exists a\nsequence H of finite connected nweighted graphs with maximum degree 3 such that\nthe following conditions on a Banach space Y are equivalent: (1) Y admits\nuniformly isomorphic embeddings of elements of the sequence X. (2) Y admits\nuniformly bilipschitz embeddings of elements of the sequence H. \n\n"}
{"id": "1112.3086", "contents": "Title: Test-space characterizations of some classes of Banach spaces Abstract: Let $\\mathcal{P}$ be a class of Banach spaces and let\n$T=\\{T_\\alpha\\}_{\\alpha\\in A}$ be a set of metric spaces. We say that $T$ is a\nset of {\\it test-spaces} for $\\mathcal{P}$ if the following two conditions are\nequivalent: (1) $X\\notin\\mathcal{P}$; (2) The spaces $\\{T_\\alpha\\}_{\\alpha\\in\nA}$ admit uniformly bilipschitz embeddings into $X$.\n  The first part of the paper is devoted to a simplification of the proof of\nthe following test-space characterization obtained in M.I. Ostrovskii\n[Different forms of metric characterizations of classes of Banach spaces,\nHouston J. Math., to appear]:\n  For each sequence $\\{X_m\\}_{m=1}^\\infty$ of finite-dimensional Banach spaces\nthere is a sequence $\\{H_n\\}_{n=1}^\\infty$ of finite connected unweighted\ngraphs with maximum degree 3 such that the following conditions on a Banach\nspace $Y$ are equivalent:\n  (A) $Y$ admits uniformly isomorphic embeddings of $\\{X_m\\}_{m=1}^\\infty$;\n  (B) $Y$ admits uniformly bilipschitz embeddings of $\\{H_n\\}_{n=1}^\\infty$.\n  The second part of the paper is devoted to the case when\n$\\{X_m\\}_{m=1}^\\infty$ is an increasing sequence of spaces. It is shown that in\nthis case the class of spaces given by (A) can be characterized using one\ntest-space, which can be chosen to be an infinite graph with maximum degree 3. \n\n"}
{"id": "1112.5969", "contents": "Title: Probabilities of exoplanet signals from posterior samplings Abstract: Estimating the marginal likelihoods is an essential feature of model\nselection in the Bayesian context. It is especially crucial to have good\nestimates when assessing the number of planets orbiting stars when the models\nexplain the noisy data with different numbers of Keplerian signals. We\nintroduce a simple method for approximating the marginal likelihoods in\npractice when a statistically representative sample from the parameter\nposterior density is available.\n  We use our truncated posterior mixture estimate to receive accurate model\nprobabilities for models with differing number of Keplerian signals in radial\nvelocity data. We test this estimate in simple scenarios to assess its accuracy\nand rate of convergence in practice when the corresponding estimates calculated\nusing deviance information criterion can be applied to receive trustworthy\nresults for reliable comparison. As a test case, we determine the posterior\nprobability of a planet orbiting HD 3651 given Lick and Keck radial velocity\ndata.\n  The posterior mixture estimate appears to be a simple and an accurate way of\ncalculating marginal integrals from posterior samples. We show, that it can be\nused to estimate the marginal integrals reliably in practice, given a suitable\nselection of parameter \\lambda, that controls its accuracy and convergence\nrate. It is also more accurate than the one block Metropolis-Hastings estimate\nand can be used in any application because it is not based on assumptions on\nthe nature of the posterior density nor the amount of data or parameters in the\nstatistical model. \n\n"}
{"id": "1201.0306", "contents": "Title: Alternating Linearization for Structured Regularization Problems Abstract: We adapt the alternating linearization method for proximal decomposition to\nstructured regularization problems, in particular, to the generalized lasso\nproblems. The method is related to two well-known operator splitting methods,\nthe Douglas--Rachford and the Peaceman--Rachford method, but it has descent\nproperties with respect to the objective function. This is achieved by\nemploying a special update test, which decides whether it is beneficial to make\na Peaceman--Rachford step, any of the two possible Douglas--Rachford steps, or\nnone. The convergence mechanism of the method is related to that of bundle\nmethods of nonsmooth optimization. We also discuss implementation for very\nlarge problems, with the use of specialized algorithms and sparse data\nstructures. Finally, we present numerical results for several synthetic and\nreal-world examples, including a three-dimensional fused lasso problem, which\nillustrate the scalability, efficacy, and accuracy of the method. \n\n"}
{"id": "1201.1893", "contents": "Title: Discussions on Fernhead and Prangle (2012) Abstract: Two contributions to the discussion of Fearnhead P. and D. Prangle (2012).\nConstructing summary statistics for approximate Bayesian computation:\nSemi-automatic approx- imate Bayesian computation, J. Roy. Statist. Soc. B, 74\n(3). \n\n"}
{"id": "1201.6205", "contents": "Title: Gale transform of a starshaped sphere Abstract: Gale transform is a simple but powerful tool in convex geometry. In\nparticular, the use of Gale transform is the main argument in the\nclassification of polytopes with few vertices. Many books and documents cover\nthe definition of Gale transform and its main properties related to convex\npolytopes. But it seems that there does not exist document studying the Gale\ntransform of more general objects, such that triangulation of spheres. In this\npaper, we study the properties of the Gale transform of a large class of such\nspheres called starshaped spheres. \n\n"}
{"id": "1202.1377", "contents": "Title: Statistical significance in high-dimensional linear models Abstract: We propose a method for constructing p-values for general hypotheses in a\nhigh-dimensional linear model. The hypotheses can be local for testing a single\nregression parameter or they may be more global involving several up to all\nparameters. Furthermore, when considering many hypotheses, we show how to\nadjust for multiple testing taking dependence among the p-values into account.\nOur technique is based on Ridge estimation with an additional correction term\ndue to a substantial projection bias in high dimensions. We prove strong error\ncontrol for our p-values and provide sufficient conditions for detection: for\nthe former, we do not make any assumption on the size of the true underlying\nregression coefficients while regarding the latter, our procedure might not be\noptimal in terms of power. We demonstrate the method in simulated examples and\na real data application. \n\n"}
{"id": "1202.5504", "contents": "Title: Convex Equipartitions via Equivariant Obstruction Theory Abstract: We describe a regular cell complex model for the configuration space\nF(\\R^d,n). Based on this, we use Equivariant Obstruction Theory to prove the\nprime power case of the conjecture by Nandakumar and Ramana Rao that every\npolygon can be partitioned into n convex parts of equal area and perimeter. \n\n"}
{"id": "1203.5181", "contents": "Title: $k$-MLE: A fast algorithm for learning statistical mixture models Abstract: We describe $k$-MLE, a fast and efficient local search algorithm for learning\nfinite statistical mixtures of exponential families such as Gaussian mixture\nmodels. Mixture models are traditionally learned using the\nexpectation-maximization (EM) soft clustering technique that monotonically\nincreases the incomplete (expected complete) likelihood. Given prescribed\nmixture weights, the hard clustering $k$-MLE algorithm iteratively assigns data\nto the most likely weighted component and update the component models using\nMaximum Likelihood Estimators (MLEs). Using the duality between exponential\nfamilies and Bregman divergences, we prove that the local convergence of the\ncomplete likelihood of $k$-MLE follows directly from the convergence of a dual\nadditively weighted Bregman hard clustering. The inner loop of $k$-MLE can be\nimplemented using any $k$-means heuristic like the celebrated Lloyd's batched\nor Hartigan's greedy swap updates. We then show how to update the mixture\nweights by minimizing a cross-entropy criterion that implies to update weights\nby taking the relative proportion of cluster points, and reiterate the mixture\nparameter update and mixture weight update processes until convergence. Hard EM\nis interpreted as a special case of $k$-MLE when both the component update and\nthe weight update are performed successively in the inner loop. To initialize\n$k$-MLE, we propose $k$-MLE++, a careful initialization of $k$-MLE guaranteeing\nprobabilistically a global bound on the best possible complete likelihood. \n\n"}
{"id": "1204.0316", "contents": "Title: Subsampling Extremes: From Block Maxima to Smooth Tail Estimation Abstract: We study a new estimator for the tail index of a distribution in the Frechet\ndomain of attraction that arises naturally by computing subsample maxima. This\nestimator is equivalent to taking a U-statistic over a Hill estimator with two\norder statistics. The estimator presents multiple advantages over the Hill\nestimator. In particular, it has asymptotically smooth sample paths as a\nfunction of the threshold k, making it considerably more stable than the Hill\nestimator. The estimator also admits a simple and intuitive threshold selection\nrule that does not require fitting a second-order model. Journal of\nMultivariate Analysis, 130, 2014 \n\n"}
{"id": "1204.1349", "contents": "Title: Periodic Rigidity on a Variable Torus Using Inductive Constructions Abstract: In this paper we prove a recursive characterisation of generic rigidity for\nframeworks periodic with respect to a partially variable lattice. We follow the\napproach of modelling periodic frameworks as frameworks on a torus and use the\nlanguage of gain graphs for the finite counterpart of a periodic graph. In this\nsetting we employ variants of the Henneberg operations used frequently in\nrigidity theory. \n\n"}
{"id": "1204.4313", "contents": "Title: Containment problems for polytopes and spectrahedra Abstract: We study the computational question whether a given polytope or spectrahedron\n$S_A$ (as given by the positive semidefiniteness region of a linear matrix\npencil $A(x)$) is contained in another one $S_B$.\n  First we classify the computational complexity, extending results on the\npolytope/polytope-case by Gritzmann and Klee to the\npolytope/spectrahedron-case. For various restricted containment problems,\nNP-hardness is shown.\n  We then study in detail semidefinite conditions to certify containment,\nbuilding upon work by Ben-Tal, Nemirovski and Helton, Klep, McCullough. In\nparticular, we discuss variations of a sufficient semidefinite condition to\ncertify containment of a spectrahedron in a spectrahedron. It is shown that\nthese sufficient conditions even provide exact semidefinite characterizations\nfor containment in several important cases, including containment of a\nspectrahedron in a polyhedron. Moreover, in the case of bounded $S_A$ the\ncriteria will always succeed in certifying containment of some scaled\nspectrahedron $\\nu S_A$ in $S_B$. \n\n"}
{"id": "1204.4494", "contents": "Title: Rotation Sampling for Functional Data Abstract: This paper addresses the survey estimation of a population mean in continuous\ntime. For this purpose we extend the rotation sampling method to functional\ndata. In contrast to conventional rotation designs that select the sample\nbefore the survey, our approach randomizes each sample replacement and thus\nallows for adaptive sampling. Using Markov chain theory, we evaluate the\ncovariance structure and the integrated squared error [ISE] of the related\nHorvitz-Thompson estimator. Our sampling designs decrease the mean ISE by\nsuitably reallocating the sample across population strata during replacements.\nThey also reduce the variance of the ISE by increasing the frequency or the\nintensity of replacements. To investigate the benefits of using both current\nand past measurements in the estimation, we develop a new composite estimator.\nIn an application to electricity usage data, our rotation method outperforms\nfixed panels and conventional rotation samples. Because of the weak temporal\ndependence of the data, the composite estimator only slightly improves upon the\nHorvitz-Thompson estimator. \n\n"}
{"id": "1205.5723", "contents": "Title: An asymptotic approximation for the permanent of a doubly stochastic\n  matrix Abstract: A determinantal approximation is obtained for the permanent of a doubly\nstochastic matrix. For moderate-deviation matrix sequences, the asymptotic\nrelative error is of order $O(n^{-1})$. \n\n"}
{"id": "1206.0867", "contents": "Title: Testing linear hypotheses in high-dimensional regressions Abstract: For a multivariate linear model, Wilk's likelihood ratio test (LRT)\nconstitutes one of the cornerstone tools. However, the computation of its\nquantiles under the null or the alternative requires complex analytic\napproximations and more importantly, these distributional approximations are\nfeasible only for moderate dimension of the dependent variable, say $p\\le 20$.\nOn the other hand, assuming that the data dimension $p$ as well as the number\n$q$ of regression variables are fixed while the sample size $n$ grows, several\nasymptotic approximations are proposed in the literature for Wilk's $\\bLa$\nincluding the widely used chi-square approximation. In this paper, we consider\nnecessary modifications to Wilk's test in a high-dimensional context,\nspecifically assuming a high data dimension $p$ and a large sample size $n$.\nBased on recent random matrix theory, the correction we propose to Wilk's test\nis asymptotically Gaussian under the null and simulations demonstrate that the\ncorrected LRT has very satisfactory size and power, surely in the large $p$ and\nlarge $n$ context, but also for moderately large data dimensions like $p=30$ or\n$p=50$. As a byproduct, we give a reason explaining why the standard chi-square\napproximation fails for high-dimensional data. We also introduce a new\nprocedure for the classical multiple sample significance test in MANOVA which\nis valid for high-dimensional data. \n\n"}
{"id": "1206.1270", "contents": "Title: Factoring nonnegative matrices with linear programs Abstract: This paper describes a new approach, based on linear programming, for\ncomputing nonnegative matrix factorizations (NMFs). The key idea is a\ndata-driven model for the factorization where the most salient features in the\ndata are used to express the remaining features. More precisely, given a data\nmatrix X, the algorithm identifies a matrix C such that X approximately equals\nCX and some linear constraints. The constraints are chosen to ensure that the\nmatrix C selects features; these features can then be used to find a low-rank\nNMF of X. A theoretical analysis demonstrates that this approach has guarantees\nsimilar to those of the recent NMF algorithm of Arora et al. (2012). In\ncontrast with this earlier work, the proposed method extends to more general\nnoise models and leads to efficient, scalable algorithms. Experiments with\nsynthetic and real datasets provide evidence that the new approach is also\nsuperior in practice. An optimized C++ implementation can factor a\nmultigigabyte matrix in a matter of minutes. \n\n"}
{"id": "1206.6367", "contents": "Title: A comparison of the discrete Kolmogorov-Smirnov statistic and the\n  Euclidean distance Abstract: Goodness-of-fit tests gauge whether a given set of observations is consistent\n(up to expected random fluctuations) with arising as independent and\nidentically distributed (i.i.d.) draws from a user-specified probability\ndistribution known as the \"model.\" The standard gauges involve the discrepancy\nbetween the model and the empirical distribution of the observed draws. Some\nmeasures of discrepancy are cumulative; others are not. The most popular\ncumulative measure is the Kolmogorov-Smirnov statistic; when all probability\ndistributions under consideration are discrete, a natural noncumulative measure\nis the Euclidean distance between the model and the empirical distributions. In\nthe present paper, both mathematical analysis and its illustration via various\ndata sets indicate that the Kolmogorov-Smirnov statistic tends to be more\npowerful than the Euclidean distance when there is a natural ordering for the\nvalues that the draws can take -- that is, when the data is ordinal -- whereas\nthe Euclidean distance is more reliable and more easily understood than the\nKolmogorov-Smirnov statistic when there is no natural ordering (or partial\norder) -- that is, when the data is nominal. \n\n"}
{"id": "1207.4747", "contents": "Title: Block-Coordinate Frank-Wolfe Optimization for Structural SVMs Abstract: We propose a randomized block-coordinate variant of the classic Frank-Wolfe\nalgorithm for convex optimization with block-separable constraints. Despite its\nlower iteration cost, we show that it achieves a similar convergence rate in\nduality gap as the full Frank-Wolfe algorithm. We also show that, when applied\nto the dual structural support vector machine (SVM) objective, this yields an\nonline algorithm that has the same low iteration complexity as primal\nstochastic subgradient methods. However, unlike stochastic subgradient methods,\nthe block-coordinate Frank-Wolfe algorithm allows us to compute the optimal\nstep-size and yields a computable duality gap guarantee. Our experiments\nindicate that this simple algorithm outperforms competing structural SVM\nsolvers. \n\n"}
{"id": "1207.5355", "contents": "Title: Estimating the granularity coefficient of a Potts-Markov random field\n  within an MCMC algorithm Abstract: This paper addresses the problem of estimating the Potts parameter B jointly\nwith the unknown parameters of a Bayesian model within a Markov chain Monte\nCarlo (MCMC) algorithm. Standard MCMC methods cannot be applied to this problem\nbecause performing inference on B requires computing the intractable\nnormalizing constant of the Potts model. In the proposed MCMC method the\nestimation of B is conducted using a likelihood-free Metropolis-Hastings\nalgorithm. Experimental results obtained for synthetic data show that\nestimating B jointly with the other unknown parameters leads to estimation\nresults that are as good as those obtained with the actual value of B. On the\nother hand, assuming that the value of B is known can degrade estimation\nperformance significantly if this value is incorrect. To illustrate the\ninterest of this method, the proposed algorithm is successfully applied to real\nbidimensional SAR and tridimensional ultrasound images. \n\n"}
{"id": "1207.6606", "contents": "Title: Weighted sampling, Maximum Likelihood and minimum divergence estimators Abstract: This paper explores Maximum Likelihood in parametric models in the context of\nSanov type Large Deviation Probabilities. MLE in parametric models under\nweighted sampling is shown to be associated with the minimization of a specific\ndivergence criterion defined with respect to the distribution of the weights.\nSome properties of the resulting inferential procedure are presented; Bahadur\nefficiency of tests are also considered in this context. \n\n"}
{"id": "1208.1439", "contents": "Title: Structure results for multiple tilings in 3D Abstract: We study multiple tilings of 3-dimensional Euclidean space by a convex body.\nIn a multiple tiling, a convex body $P$ is translated with a discrete multiset\n$\\Lambda$ in such a way that each point of the space gets covered exactly $k$\ntimes, except perhaps the translated copies of the boundary of $P$. It is known\nthat all possible multiple tilers in 3D are zonotopes. In 2D it was known by\nthe work of M. Kolountzakis that, unless $P$ is a parallelogram, the multiset\nof translation vectors $\\Lambda$ must be a finite union of translated lattices\n(also known as quasi periodic sets). In that work [Kolountzakis, 2002], the\nauthor asked whether the same quasi-periodic structure on the translation\nvectors would be true in 3D. Here we prove that this conclusion is indeed true\nfor 3D.\n  Namely, we show that if $P$ is a convex multiple tiler in 3D, with a discrete\nmultiset $\\Lambda$ of translation vectors, then $\\Lambda$ has to be a finite\nunion of translated lattices, unless $P$ belongs to a special class of\nzonotopes. This exceptional class consists of two-flat zonotopes $P$, defined\nby the Minkowski sum of $n+m$ line segments that lie in the union of two\ndifferent two-dimensional subspaces $H_1$ and $H_2$. Equivalently, a two-flat\nzonotope $P$ may be thought of as the Minkowski sum of two 2-dimensional\nsymmetric polygons one of which may degenerate into a single line segment. It\nturns out that rational two-flat zonotopes admit a multiple tiling with an\naperiodic (non-quasi-periodic) set of translation vectors $\\Lambda$. We note\nthat it may be quite difficult to offer a visualization of these 3-dimensional\nnon-quasi-periodic tilings, and that we discovered them by using Fourier\nmethods. \n\n"}
{"id": "1208.3553", "contents": "Title: The Dependence of Routine Bayesian Model Selection Methods on Irrelevant\n  Alternatives Abstract: Bayesian methods - either based on Bayes Factors or BIC - are now widely used\nfor model selection. One property that might reasonably be demanded of any\nmodel selection method is that if a model ${M}_{1}$ is preferred to a model\n${M}_{0}$, when these two models are expressed as members of one model class\n$\\mathbb{M}$, this preference is preserved when they are embedded in a\ndifferent class $\\mathbb{M}'$. However, we illustrate in this paper that with\nthe usual implementation of these common Bayesian procedures this property does\nnot hold true even approximately. We therefore contend that to use these\nmethods it is first necessary for there to exist a \"natural\" embedding class.\nWe argue that in any context like the one illustrated in our running example of\nBayesian model selection of binary phylogenetic trees there is no such\nembedding. \n\n"}
{"id": "1209.0012", "contents": "Title: Residual variance and the signal-to-noise ratio in high-dimensional\n  linear models Abstract: Residual variance and the signal-to-noise ratio are important quantities in\nmany statistical models and model fitting procedures. They play an important\nrole in regression diagnostics, in determining the performance limits in\nestimation and prediction problems, and in shrinkage parameter selection in\nmany popular regularized regression methods for high-dimensional data analysis.\nWe propose new estimators for the residual variance, the l2-signal strength,\nand the signal-to-noise ratio that are consistent and asymptotically normal in\nhigh-dimensional linear models with Gaussian predictors and errors, where the\nnumber of predictors d is proportional to the number of observations n.\nExisting results on residual variance estimation in high-dimensional linear\nmodels depend on sparsity in the underlying signal. Our results require no\nsparsity assumptions and imply that the residual variance may be consistently\nestimated even when d > n and the underlying signal itself is non-estimable.\nBasic numerical work suggests that some of the distributional assumptions made\nfor our theoretical results may be relaxed. \n\n"}
{"id": "1209.5712", "contents": "Title: The degree of point configurations: Ehrhart theory, Tverberg points and\n  almost neighborly polytopes Abstract: The degree of a point configuration is defined as the maximal codimension of\nits interior faces. This concept is motivated from a corresponding\nEhrhart-theoretic notion for lattice polytopes and is related to neighborly\npolytopes and the generalized lower bound theorem and, by Gale duality, to\nTverberg theory.\n  The main results of this paper are a complete classification of point\nconfigurations of degree 1, as well as a structure result on point\nconfigurations whose degree is less than a third of the dimension. Statements\nand proofs involve the novel notion of a weak Cayley decomposition, and imply\nthat the m-core of a set S of n points in R^r is contained in the set of\nTverberg points of order 3m-2(n-r) of S. \n\n"}
{"id": "1209.5908", "contents": "Title: Correlated variables in regression: clustering and sparse estimation Abstract: We consider estimation in a high-dimensional linear model with strongly\ncorrelated variables. We propose to cluster the variables first and do\nsubsequent sparse estimation such as the Lasso for cluster-representatives or\nthe group Lasso based on the structure from the clusters. Regarding the first\nstep, we present a novel and bottom-up agglomerative clustering algorithm based\non canonical correlations, and we show that it finds an optimal solution and is\nstatistically consistent. We also present some theoretical arguments that\ncanonical correlation based clustering leads to a better-posed compatibility\nconstant for the design matrix which ensures identifiability and an oracle\ninequality for the group Lasso. Furthermore, we discuss circumstances where\ncluster-representatives and using the Lasso as subsequent estimator leads to\nimproved results for prediction and detection of variables. We complement the\ntheoretical analysis with various empirical results. \n\n"}
{"id": "1211.2264", "contents": "Title: Calibrated Elastic Regularization in Matrix Completion Abstract: This paper concerns the problem of matrix completion, which is to estimate a\nmatrix from observations in a small subset of indices. We propose a calibrated\nspectrum elastic net method with a sum of the nuclear and Frobenius penalties\nand develop an iterative algorithm to solve the convex minimization problem.\nThe iterative algorithm alternates between imputing the missing entries in the\nincomplete matrix by the current guess and estimating the matrix by a scaled\nsoft-thresholding singular value decomposition of the imputed matrix until the\nresulting matrix converges. A calibration step follows to correct the bias\ncaused by the Frobenius penalty. Under proper coherence conditions and for\nsuitable penalties levels, we prove that the proposed estimator achieves an\nerror bound of nearly optimal order and in proportion to the noise level. This\nprovides a unified analysis of the noisy and noiseless matrix completion\nproblems. Simulation results are presented to compare our proposal with\nprevious ones. \n\n"}
{"id": "1211.2442", "contents": "Title: Testing goodness-of-fit of random graph models Abstract: Random graphs are matrices with independent 0, 1 elements with probabilities\ndetermined by a small number of parameters. One of the oldest model is the\nRasch model where the odds are ratios of positive numbers scaling the rows and\ncolumns. Later Persi Diaconis with his coworkers rediscovered the model for\nsymmetric matrices and called the model beta. Here we give goodnes-of-fit tests\nfor the model and extend the model to a version of the block model introduced\nby Holland, Laskey, and Leinhard. \n\n"}
{"id": "1211.4483", "contents": "Title: Computational aspects of Bayesian spectral density estimation Abstract: Gaussian time-series models are often specified through their spectral\ndensity. Such models present several computational challenges, in particular\nbecause of the non-sparse nature of the covariance matrix. We derive a fast\napproximation of the likelihood for such models. We propose to sample from the\napproximate posterior (that is, the prior times the approximate likelihood),\nand then to recover the exact posterior through importance sampling. We show\nthat the variance of the importance sampling weights vanishes as the sample\nsize goes to infinity. We explain why the approximate posterior may typically\nmulti-modal, and we derive a Sequential Monte Carlo sampler based on an\nannealing sequence in order to sample from that target distribution.\nPerformance of the overall approach is evaluated on simulated and real\ndatasets. In addition, for one real world dataset, we provide some numerical\nevidence that a Bayesian approach to semi-parametric estimation of spectral\ndensity may provide more reasonable results than its Frequentist counter-parts. \n\n"}
{"id": "1212.0634", "contents": "Title: Better subset regression Abstract: To find efficient screening methods for high dimensional linear regression\nmodels, this paper studies the relationship between model fitting and screening\nperformance. Under a sparsity assumption, we show that a subset that includes\nthe true submodel always yields smaller residual sum of squares (i.e., has\nbetter model fitting) than all that do not in a general asymptotic setting.\nThis indicates that, for screening important variables, we could follow a\n\"better fitting, better screening\" rule, i.e., pick a \"better\" subset that has\nbetter model fitting. To seek such a better subset, we consider the\noptimization problem associated with best subset regression. An EM algorithm,\ncalled orthogonalizing subset screening, and its accelerating version are\nproposed for searching for the best subset. Although the two algorithms cannot\nguarantee that a subset they yield is the best, their monotonicity property\nmakes the subset have better model fitting than initial subsets generated by\npopular screening methods, and thus the subset can have better screening\nperformance asymptotically. Simulation results show that our methods are very\ncompetitive in high dimensional variable screening even for finite sample\nsizes. \n\n"}
{"id": "1212.2834", "contents": "Title: Dictionary Subselection Using an Overcomplete Joint Sparsity Model Abstract: Many natural signals exhibit a sparse representation, whenever a suitable\ndescribing model is given. Here, a linear generative model is considered, where\nmany sparsity-based signal processing techniques rely on such a simplified\nmodel. As this model is often unknown for many classes of the signals, we need\nto select such a model based on the domain knowledge or using some exemplar\nsignals. This paper presents a new exemplar based approach for the linear model\n(called the dictionary) selection, for such sparse inverse problems. The\nproblem of dictionary selection, which has also been called the dictionary\nlearning in this setting, is first reformulated as a joint sparsity model. The\njoint sparsity model here differs from the standard joint sparsity model as it\nconsiders an overcompleteness in the representation of each signal, within the\nrange of selected subspaces. The new dictionary selection paradigm is examined\nwith some synthetic and realistic simulations. \n\n"}
{"id": "1212.3721", "contents": "Title: Approximate continuous-discrete filters for the estimation of diffusion\n  processes from partial and noisy observations Abstract: In this paper, an alternative approximation to the innovation method is\nintroduced for the parameter estimation of diffusion processes from partial and\nnoisy observations. This is based on a convergent approximation to the first\ntwo conditional moments of the innovation process through approximate\ncontinuous-discrete filters of minimum variance. It is shown that, for finite\nsamples, the resulting approximate estimators converge to the exact one when\nthe error of the approximate filters decreases. For an increasing number of\nobservations, the estimators are asymptotically normal distributed and their\nbias decreases when the above mentioned error does it. A simulation study is\nprovided to illustrate the performance of the new estimators. The results show\nthat, with respect to the conventional approximate estimators, the new ones\nsignificantly enhance the parameter estimation of the test equations. The\nproposed estimators are intended for the recurrent practical situation where a\nnonlinear stochastic system should be identified from a reduced number of\npartial and noisy observations distant in time. \n\n"}
{"id": "1212.4463", "contents": "Title: Young's lattice and dihedral symmetries revisited: M\\\"obius strips and\n  metric geometry Abstract: A cascade of dihedral symmetries is hidden in Young's lattice of integer\npartitions. In fact, for each integer N>2 the Hasse graph of the subposet\nconsisting of the partitions with maximal hook length strictly less than N has\nthe dihedral group of order 2N as its symmetry group. Here a new interpretation\nof those Hasse graphs is presented, namely as the 1-skeleta of the injective\nhulls of certain finite metric spaces. \n\n"}
{"id": "1212.4696", "contents": "Title: Combinatorics of flag simplicial 3-polytopes Abstract: In the focus of this paper is the operation of edge contraction. One can show\nthat simplicial 3-polytope is flag iff contraction of any its edge gives\nsimplicial 3-polytope. Our main result states that any flag simplicial\n3-polytope can be reduced to octahedron by sequence of edge contractions. Using\nthis operation we introduce a partial order on the set of flag simplicial\n3-polytopes and study Hasse graph of corresponding poset. We estimate input and\noutput degrees of vertices of this Hasse graph. \n\n"}
{"id": "1212.5156", "contents": "Title: Nonparametric ridge estimation Abstract: We study the problem of estimating the ridges of a density function. Ridge\nestimation is an extension of mode finding and is useful for understanding the\nstructure of a density. It can also be used to find hidden structure in point\ncloud data. We show that, under mild regularity conditions, the ridges of the\nkernel density estimator consistently estimate the ridges of the true density.\nWhen the data are noisy measurements of a manifold, we show that the ridges are\nclose and topologically similar to the hidden manifold. To find the estimated\nridges in practice, we adapt the modified mean-shift algorithm proposed by\nOzertem and Erdogmus [J. Mach. Learn. Res. 12 (2011) 1249-1286]. Some numerical\nexperiments verify that the algorithm is accurate. \n\n"}
{"id": "1301.3611", "contents": "Title: Jitter-Adaptive Dictionary Learning - Application to Multi-Trial\n  Neuroelectric Signals Abstract: Dictionary Learning has proven to be a powerful tool for many image\nprocessing tasks, where atoms are typically defined on small image patches. As\na drawback, the dictionary only encodes basic structures. In addition, this\napproach treats patches of different locations in one single set, which means a\nloss of information when features are well-aligned across signals. This is the\ncase, for instance, in multi-trial magneto- or electroencephalography (M/EEG).\nLearning the dictionary on the entire signals could make use of the alignement\nand reveal higher-level features. In this case, however, small missalignements\nor phase variations of features would not be compensated for. In this paper, we\npropose an extension to the common dictionary learning framework to overcome\nthese limitations by allowing atoms to adapt their position across signals. The\nmethod is validated on simulated and real neuroelectric data. \n\n"}
{"id": "1301.3928", "contents": "Title: Importance sampling for weighted binary random matrices with specified\n  margins Abstract: A sequential importance sampling algorithm is developed for the distribution\nthat results when a matrix of independent, but not identically distributed,\nBernoulli random variables is conditioned on a given sequence of row and column\nsums. This conditional distribution arises in a variety of applications and\nincludes as a special case the uniform distribution over zero-one tables with\nspecified margins. The algorithm uses dynamic programming to combine hard\nmargin constraints, combinatorial approximations, and additional non-uniform\nweighting in a principled way to give state-of-the-art results. \n\n"}
{"id": "1301.4194", "contents": "Title: Financial Portfolio Optimization: Computationally guided agents to\n  investigate, analyse and invest!? Abstract: Financial portfolio optimization is a widely studied problem in mathematics,\nstatistics, financial and computational literature. It adheres to determining\nan optimal combination of weights associated with financial assets held in a\nportfolio. In practice, it faces challenges by virtue of varying math.\nformulations, parameters, business constraints and complex financial\ninstruments. Empirical nature of data is no longer one-sided; thereby\nreflecting upside and downside trends with repeated yet unidentifiable cyclic\nbehaviours potentially caused due to high frequency volatile movements in asset\ntrades. Portfolio optimization under such circumstances is theoretically and\ncomputationally challenging. This work presents a novel mechanism to reach an\noptimal solution by encoding a variety of optimal solutions in a solution bank\nto guide the search process for the global investment objective formulation. It\nconceptualizes the role of individual solver agents that contribute optimal\nsolutions to a bank of solutions, a super-agent solver that learns from the\nsolution bank, and, thus reflects a knowledge-based computationally guided\nagents approach to investigate, analyse and reach to optimal solution for\ninformed investment decisions.\n  Conceptual understanding of classes of solver agents that represent varying\nproblem formulations and, mathematically oriented deterministic solvers along\nwith stochastic-search driven evolutionary and swarm-intelligence based\ntechniques for optimal weights are discussed. Algorithmic implementation is\npresented by an enhanced neighbourhood generation mechanism in Simulated\nAnnealing algorithm. A framework for inclusion of heuristic knowledge and human\nexpertise from financial literature related to investment decision making\nprocess is reflected via introduction of controlled perturbation strategies\nusing a decision matrix for neighbourhood generation. \n\n"}
{"id": "1301.5054", "contents": "Title: A Note on Probabilistic Models over Strings: the Linear Algebra Approach Abstract: Probabilistic models over strings have played a key role in developing\nmethods allowing indels to be treated as phylogenetically informative events.\nThere is an extensive literature on using automata and transducers on\nphylogenies to do inference on these probabilistic models, in which an\nimportant theoretical question in the field is the complexity of computing the\nnormalization of a class of string-valued graphical models. This question has\nbeen investigated using tools from combinatorics, dynamic programming, and\ngraph theory, and has practical applications in Bayesian phylogenetics. In this\nwork, we revisit this theoretical question from a different point of view,\nbased on linear algebra. The main contribution is a new proof of a known result\non the complexity of inference on TKF91, a well-known probabilistic model over\nstrings. Our proof uses a different approach based on classical linear algebra\nresults, and is in some cases easier to extend to other models. The proving\nmethod also has consequences on the implementation and complexity of inference\nalgorithms. \n\n"}
{"id": "1301.5088", "contents": "Title: Piecewise Linear Multilayer Perceptrons and Dropout Abstract: We propose a new type of hidden layer for a multilayer perceptron, and\ndemonstrate that it obtains the best reported performance for an MLP on the\nMNIST dataset. \n\n"}
{"id": "1301.7212", "contents": "Title: Multiscale Change-Point Inference Abstract: We introduce a new estimator SMUCE (simultaneous multiscale change-point\nestimator) for the change-point problem in exponential family regression. An\nunknown step function is estimated by minimizing the number of change-points\nover the acceptance region of a multiscale test at a level \\alpha. The\nprobability of overestimating the true number of change-points K is controlled\nby the asymptotic null distribution of the multiscale test statistic. Further,\nwe derive exponential bounds for the probability of underestimating K. By\nbalancing these quantities, \\alpha will be chosen such that the probability of\ncorrectly estimating K is maximized. All results are even non-asymptotic for\nthe normal case. Based on the aforementioned bounds, we construct\nasymptotically honest confidence sets for the unknown step function and its\nchange-points. At the same time, we obtain exponential bounds for estimating\nthe change-point locations which for example yield the minimax rate O(1/n) up\nto a log term. Finally, SMUCE asymptotically achieves the optimal detection\nrate of vanishing signals. We illustrate how dynamic programming techniques can\nbe employed for efficient computation of estimators and confidence regions. The\nperformance of the proposed multiscale approach is illustrated by simulations\nand in two cutting-edge applications from genetic engineering and photoemission\nspectroscopy. \n\n"}
{"id": "1302.0315", "contents": "Title: Sparse Multiple Kernel Learning with Geometric Convergence Rate Abstract: In this paper, we study the problem of sparse multiple kernel learning (MKL),\nwhere the goal is to efficiently learn a combination of a fixed small number of\nkernels from a large pool that could lead to a kernel classifier with a small\nprediction error. We develop an efficient algorithm based on the greedy\ncoordinate descent algorithm, that is able to achieve a geometric convergence\nrate under appropriate conditions. The convergence rate is achieved by\nmeasuring the size of functional gradients by an empirical $\\ell_2$ norm that\ndepends on the empirical data distribution. This is in contrast to previous\nalgorithms that use a functional norm to measure the size of gradients, which\nis independent from the data samples. We also establish a generalization error\nbound of the learned sparse kernel classifier using the technique of local\nRademacher complexity. \n\n"}
{"id": "1302.2068", "contents": "Title: Efficiency for Regularization Parameter Selection in Penalized\n  Likelihood Estimation of Misspecified Models Abstract: It has been shown that AIC-type criteria are asymptotically efficient\nselectors of the tuning parameter in non-concave penalized regression methods\nunder the assumption that the population variance is known or that a consistent\nestimator is available. We relax this assumption to prove that AIC itself is\nasymptotically efficient and we study its performance in finite samples. In\nclassical regression, it is known that AIC tends to select overly complex\nmodels when the dimension of the maximum candidate model is large relative to\nthe sample size. Simulation studies suggest that AIC suffers from the same\nshortcomings when used in penalized regression. We therefore propose the use of\nthe classical corrected AIC (AICc) as an alternative and prove that it\nmaintains the desired asymptotic properties. To broaden our results, we further\nprove the efficiency of AIC for penalized likelihood methods in the context of\ngeneralized linear models with no dispersion parameter. Similar results exist\nin the literature but only for a restricted set of candidate models. By\nemploying results from the classical literature on maximum-likelihood\nestimation in misspecified models, we are able to establish this result for a\ngeneral set of candidate models. We use simulations to assess the performance\nof AIC and AICc, as well as that of other selectors, in finite samples for both\nSCAD-penalized and Lasso regressions and a real data example is considered. \n\n"}
{"id": "1303.3598", "contents": "Title: The Hirsch conjecture holds for normal flag complexes Abstract: Using an intuition from metric geometry, we prove that any flag and normal\nsimplicial complex satisfies the non-revisiting path conjecture. As a\nconsequence, the diameter of its facet-ridge graph is smaller than the number\nof vertices minus the dimension, as in the Hirsch conjecture. This proves the\nHirsch conjecture for all flag polytopes, and more generally, for all\n(connected) flag homology manifolds. \n\n"}
{"id": "1303.3775", "contents": "Title: Approximation for the Distribution of Three-dimensional Discrete Scan\n  Statistic Abstract: We consider the discrete three dimensional scan statistics. Viewed as the\nmaximum of an 1-dependent stationary r.v.'s sequence, we provide approximations\nand error bounds for the probability distribution of the three dimensional scan\nstatistics. Importance sampling algorithm is used to obtains sharp bounds for\nthe simulation error. Simulation results and comparisons with other\napproximations are presented for the binomial and Poisson models. \n\n"}
{"id": "1303.4172", "contents": "Title: Margins, Shrinkage, and Boosting Abstract: This manuscript shows that AdaBoost and its immediate variants can produce\napproximate maximum margin classifiers simply by scaling step size choices with\na fixed small constant. In this way, when the unscaled step size is an optimal\nchoice, these results provide guarantees for Friedman's empirically successful\n\"shrinkage\" procedure for gradient boosting (Friedman, 2000). Guarantees are\nalso provided for a variety of other step sizes, affirming the intuition that\nincreasingly regularized line searches provide improved margin guarantees. The\nresults hold for the exponential loss and similar losses, most notably the\nlogistic loss. \n\n"}
{"id": "1303.5145", "contents": "Title: Node-Based Learning of Multiple Gaussian Graphical Models Abstract: We consider the problem of estimating high-dimensional Gaussian graphical\nmodels corresponding to a single set of variables under several distinct\nconditions. This problem is motivated by the task of recovering transcriptional\nregulatory networks on the basis of gene expression data {containing\nheterogeneous samples, such as different disease states, multiple species, or\ndifferent developmental stages}. We assume that most aspects of the conditional\ndependence networks are shared, but that there are some structured differences\nbetween them. Rather than assuming that similarities and differences between\nnetworks are driven by individual edges, we take a node-based approach, which\nin many cases provides a more intuitive interpretation of the network\ndifferences. We consider estimation under two distinct assumptions: (1)\ndifferences between the K networks are due to individual nodes that are\nperturbed across conditions, or (2) similarities among the K networks are due\nto the presence of common hub nodes that are shared across all K networks.\nUsing a row-column overlap norm penalty function, we formulate two convex\noptimization problems that correspond to these two assumptions. We solve these\nproblems using an alternating direction method of multipliers algorithm, and we\nderive a set of necessary and sufficient conditions that allows us to decompose\nthe problem into independent subproblems so that our algorithm can be scaled to\nhigh-dimensional settings. Our proposal is illustrated on synthetic data, a\nwebpage data set, and a brain cancer gene expression data set. \n\n"}
{"id": "1303.5846", "contents": "Title: Smoothness and singularities of the perfect form and the second Voronoi\n  compactification of ${\\mathcal A}_g$ Abstract: We study the cones in the first Voronoi or perfect cone decomposition of\nquadratic forms with respect to the question which of these cones are basic or\nsimplicial. As a consequence we deduce that the singular locus of the moduli\nstack ${\\mathcal A_g^{\\mathop{Perf}}}$, the toroidal compactification of the\nmoduli space of principally polarized abelian varieties of dimension $g$ given\nby this decomposition, has codimension $10$ if $g \\geq 4$. Moreover we describe\nthe non-simplicial locus in codimension $10$. We also show that the second\nVoronoi compactification ${\\mathcal A_{g}^{\\mathop{Vor}}}$ has singularities in\ncodimension $3$ for $g\\geq 5$. \n\n"}
{"id": "1303.6223", "contents": "Title: Random Intersection Trees Abstract: Finding interactions between variables in large and high-dimensional datasets\nis often a serious computational challenge. Most approaches build up\ninteraction sets incrementally, adding variables in a greedy fashion. The\ndrawback is that potentially informative high-order interactions may be\noverlooked. Here, we propose at an alternative approach for classification\nproblems with binary predictor variables, called Random Intersection Trees. It\nworks by starting with a maximal interaction that includes all variables, and\nthen gradually removing variables if they fail to appear in randomly chosen\nobservations of a class of interest. We show that informative interactions are\nretained with high probability, and the computational complexity of our\nprocedure is of order $p^\\kappa$ for a value of $\\kappa$ that can reach values\nas low as 1 for very sparse data; in many more general settings, it will still\nbeat the exponent $s$ obtained when using a brute force search constrained to\norder $s$ interactions. In addition, by using some new ideas based on min-wise\nhash schemes, we are able to further reduce the computational cost.\nInteractions found by our algorithm can be used for predictive modelling in\nvarious forms, but they are also often of interest in their own right as useful\ncharacterisations of what distinguishes a certain class from others. \n\n"}
{"id": "1303.7318", "contents": "Title: Approximate Inference for Observation Driven Time Series Models with\n  Intractable Likelihoods Abstract: In the following article we consider approximate Bayesian parameter inference\nfor observation driven time series models. Such statistical models appear in a\nwide variety of applications, including econometrics and applied mathematics.\nThis article considers the scenario where the likelihood function cannot be\nevaluated point-wise; in such cases, one cannot perform exact statistical\ninference, including parameter estimation, which often requires advanced\ncomputational algorithms, such as Markov chain Monte Carlo (MCMC). We introduce\na new approximation based upon approximate Bayesian computation (ABC). Under\nsome conditions, we show that as $n\\rightarrow\\infty$, with $n$ the length of\nthe time series, the ABC posterior has, almost surely, a maximum \\emph{a\nposteriori} (MAP) estimator of the parameters which is different from the true\nparameter. However, a noisy ABC MAP, which perturbs the original data,\nasymptotically converges to the true parameter, almost surely. In order to draw\nstatistical inference, for the ABC approximation adopted, standard MCMC\nalgorithms can have acceptance probabilities that fall at an exponential rate\nin $n$ and slightly more advanced algorithms can mix poorly. We develop a new\nand improved MCMC kernel, which is based upon an exact approximation of a\nmarginal algorithm, whose cost per-iteration is random but the expected cost,\nfor good performance, is shown to be $\\mathcal{O}(n^2)$ per-iteration. \n\n"}
{"id": "1304.0682", "contents": "Title: Sparse Signal Processing with Linear and Nonlinear Observations: A\n  Unified Shannon-Theoretic Approach Abstract: We derive fundamental sample complexity bounds for recovering sparse and\nstructured signals for linear and nonlinear observation models including sparse\nregression, group testing, multivariate regression and problems with missing\nfeatures. In general, sparse signal processing problems can be characterized in\nterms of the following Markovian property. We are given a set of $N$ variables\n$X_1,X_2,\\ldots,X_N$, and there is an unknown subset of variables $S \\subset\n\\{1,\\ldots,N\\}$ that are relevant for predicting outcomes $Y$. More\nspecifically, when $Y$ is conditioned on $\\{X_n\\}_{n\\in S}$ it is conditionally\nindependent of the other variables, $\\{X_n\\}_{n \\not \\in S}$. Our goal is to\nidentify the set $S$ from samples of the variables $X$ and the associated\noutcomes $Y$. We characterize this problem as a version of the noisy channel\ncoding problem. Using asymptotic information theoretic analyses, we establish\nmutual information formulas that provide sufficient and necessary conditions on\nthe number of samples required to successfully recover the salient variables.\nThese mutual information expressions unify conditions for both linear and\nnonlinear observations. We then compute sample complexity bounds for the\naforementioned models, based on the mutual information expressions in order to\ndemonstrate the applicability and flexibility of our results in general sparse\nsignal processing models. \n\n"}
{"id": "1304.3285", "contents": "Title: Scaling the Indian Buffet Process via Submodular Maximization Abstract: Inference for latent feature models is inherently difficult as the inference\nspace grows exponentially with the size of the input data and number of latent\nfeatures. In this work, we use Kurihara & Welling (2008)'s\nmaximization-expectation framework to perform approximate MAP inference for\nlinear-Gaussian latent feature models with an Indian Buffet Process (IBP)\nprior. This formulation yields a submodular function of the features that\ncorresponds to a lower bound on the model evidence. By adding a constant to\nthis function, we obtain a nonnegative submodular function that can be\nmaximized via a greedy algorithm that obtains at least a one-third\napproximation to the optimal solution. Our inference method scales linearly\nwith the size of the input data, and we show the efficacy of our method on the\nlargest datasets currently analyzed using an IBP model. \n\n"}
{"id": "1304.7186", "contents": "Title: Neighborly and almost neighborly configurations, and their duals Abstract: This thesis presents new applications of Gale duality to the study of\npolytopes, point configurations and oriented matroids with extremal\ncombinatorial properties.\n  The first part of the thesis explores construction techniques for neighborly\npolytopes and oriented matroids. First, we provide a new interpretation of\nShemer's classical Sewing Construction for neighborly polytopes in terms of\nlexicographic extensions of oriented matroids. This allows us to provide a\nsimplified proof and to generalize it to oriented matroids in two ways: the\nExtended Sewing Construction and the Gale Sewing Construction. Estimating the\nnumber of polytopes constructed with the later, we can provide new lower bounds\nfor the number of combinatorial types of neighborly polytopes that even improve\nthe current best bounds for the number of polytopes. The combination of both\nnew techniques also allows us to construct many non-realizable neighborly\noriented matroids.\n  The degree of a point configuration is the maximal codimension of its\ninterior faces. The second part of the thesis presents various results on the\ncombinatorial structure of point configurations whose degree is small compared\nto their dimension; specifically, those whose degree is smaller than [(d+1)/2],\nthe degree of neighborly polytopes. The study of this problem comes motivated\nby Ehrhart theory, where a notion equivalent to the degree - for lattice\npolytopes - has been widely studied during the last years. In addition, the\nstudy of the degree is also related to the Generalized Lower Bound Theorem for\nsimplicial polytopes, with Cayley polytopes and with Tverberg theory. Among\nother results, we present a complete combinatorial classification for point\nconfigurations of degree 1. Moreover, we show combinatorial restrictions for\nconfigurations of small degree in terms of the novel concepts of weak Cayley\nconfigurations and codegree decompositions. \n\n"}
{"id": "1304.7198", "contents": "Title: Evidential Value in ANOVA Results in Favor of Fabrication Abstract: Some scientific publications are under suspicion of fabrication of data.\nSince humans are bad random number generators, there might be some evidential\nvalue in favor of fabrication in the statistical results as presented in such\npapers. In line with Uri Simonsohn (2012, 2013) we study the evidential value\nof the results of an ANOVA study in favor of the hypothesis of a dependence\nstructure in the underlying data. \n\n"}
{"id": "1306.0113", "contents": "Title: Trust, but verify: benefits and pitfalls of least-squares refitting in\n  high dimensions Abstract: Least-squares refitting is widely used in high dimensional regression to\nreduce the prediction bias of l1-penalized estimators (e.g., Lasso and\nSquare-Root Lasso). We present theoretical and numerical results that provide\nnew insights into the benefits and pitfalls of least-squares refitting. In\nparticular, we consider both prediction and estimation, and we pay close\nattention to the effects of correlations in the design matrices of linear\nregression models, since these correlations - although often neglected - are\ncrucial in the context of linear regression, especially in high dimensional\ncontexts. First, we demonstrate that the benefit of least-squares refitting\nstrongly depends on the setting and task under consideration: least-squares\nrefitting can be beneficial even for settings with highly correlated design\nmatrices but is not advisable for all settings, and least-squares refitting can\nbe beneficial for estimation but performs better for prediction. Finally, we\nintroduce a criterion that indicates whether least-squares refitting is\nadvisable for a specific setting and task under consideration, and we conduct a\nthorough simulation study involving the Lasso to show the usefulness of this\ncriterion. \n\n"}
{"id": "1306.0308", "contents": "Title: Probabilistic Solutions to Differential Equations and their Application\n  to Riemannian Statistics Abstract: We study a probabilistic numerical method for the solution of both boundary\nand initial value problems that returns a joint Gaussian process posterior over\nthe solution. Such methods have concrete value in the statistics on Riemannian\nmanifolds, where non-analytic ordinary differential equations are involved in\nvirtually all computations. The probabilistic formulation permits marginalising\nthe uncertainty of the numerical solution such that statistics are less\nsensitive to inaccuracies. This leads to new Riemannian algorithms for mean\nvalue computations and principal geodesic analysis. Marginalisation also means\nresults can be less precise than point estimates, enabling a noticeable\nspeed-up over the state of the art. Our approach is an argument for a wider\npoint that uncertainty caused by numerical calculations should be tracked\nthroughout the pipeline of machine learning algorithms. \n\n"}
{"id": "1306.1043", "contents": "Title: Structural Intervention Distance (SID) for Evaluating Causal Graphs Abstract: Causal inference relies on the structure of a graph, often a directed acyclic\ngraph (DAG). Different graphs may result in different causal inference\nstatements and different intervention distributions. To quantify such\ndifferences, we propose a (pre-) distance between DAGs, the structural\nintervention distance (SID). The SID is based on a graphical criterion only and\nquantifies the closeness between two DAGs in terms of their corresponding\ncausal inference statements. It is therefore well-suited for evaluating graphs\nthat are used for computing interventions. Instead of DAGs it is also possible\nto compare CPDAGs, completed partially directed acyclic graphs that represent\nMarkov equivalence classes. Since it differs significantly from the popular\nStructural Hamming Distance (SHD), the SID constitutes a valuable additional\nmeasure. We discuss properties of this distance and provide an efficient\nimplementation with software code available on the first author's homepage (an\nR package is under construction). \n\n"}
{"id": "1306.6462", "contents": "Title: On the Convergence of Adaptive Sequential Monte Carlo Methods Abstract: In several implementations of Sequential Monte Carlo (SMC) methods it is\nnatural, and important in terms of algorithmic efficiency, to exploit the\ninformation of the history of the samples to optimally tune their subsequent\npropagations. In this article we provide a carefully formulated asymptotic\ntheory for a class of such \\emph{adaptive} SMC methods. The theoretical\nframework developed here will cover, under assumptions, several commonly used\nSMC algorithms. There are only limited results about the theoretical\nunderpinning of such adaptive methods: we will bridge this gap by providing a\nweak law of large numbers (WLLN) and a central limit theorem (CLT) for some of\nthese algorithms. The latter seems to be the first result of its kind in the\nliterature and provides a formal justification of algorithms used in many real\ndata context. We establish that for a general class of adaptive SMC algorithms\nthe asymptotic variance of the estimators from the adaptive SMC method is\n\\emph{identical} to a so-called `perfect' SMC algorithm which uses ideal\nproposal kernels. Our results are supported by application on a complex\nhigh-dimensional posterior distribution associated with the Navier-Stokes\nmodel, where adapting high-dimensional parameters of the proposal kernels is\ncritical for the efficiency of the algorithm. \n\n"}
{"id": "1307.0164", "contents": "Title: Sparse Principal Component Analysis for High Dimensional Vector\n  Autoregressive Models Abstract: We study sparse principal component analysis for high dimensional vector\nautoregressive time series under a doubly asymptotic framework, which allows\nthe dimension $d$ to scale with the series length $T$. We treat the transition\nmatrix of time series as a nuisance parameter and directly apply sparse\nprincipal component analysis on multivariate time series as if the data are\nindependent. We provide explicit non-asymptotic rates of convergence for\nleading eigenvector estimation and extend this result to principal subspace\nestimation. Our analysis illustrates that the spectral norm of the transition\nmatrix plays an essential role in determining the final rates. We also\ncharacterize sufficient conditions under which sparse principal component\nanalysis attains the optimal parametric rate. Our theoretical results are\nbacked up by thorough numerical studies. \n\n"}
{"id": "1307.4145", "contents": "Title: A Safe Screening Rule for Sparse Logistic Regression Abstract: The l1-regularized logistic regression (or sparse logistic regression) is a\nwidely used method for simultaneous classification and feature selection.\nAlthough many recent efforts have been devoted to its efficient implementation,\nits application to high dimensional data still poses significant challenges. In\nthis paper, we present a fast and effective sparse logistic regression\nscreening rule (Slores) to identify the 0 components in the solution vector,\nwhich may lead to a substantial reduction in the number of features to be\nentered to the optimization. An appealing feature of Slores is that the data\nset needs to be scanned only once to run the screening and its computational\ncost is negligible compared to that of solving the sparse logistic regression\nproblem. Moreover, Slores is independent of solvers for sparse logistic\nregression, thus Slores can be integrated with any existing solver to improve\nthe efficiency. We have evaluated Slores using high-dimensional data sets from\ndifferent applications. Extensive experimental results demonstrate that Slores\noutperforms the existing state-of-the-art screening rules and the efficiency of\nsolving sparse logistic regression is improved by one magnitude in general. \n\n"}
{"id": "1307.6887", "contents": "Title: Sequential Transfer in Multi-armed Bandit with Finite Set of Models Abstract: Learning from prior tasks and transferring that experience to improve future\nperformance is critical for building lifelong learning agents. Although results\nin supervised and reinforcement learning show that transfer may significantly\nimprove the learning performance, most of the literature on transfer is focused\non batch learning tasks. In this paper we study the problem of\n\\textit{sequential transfer in online learning}, notably in the multi-armed\nbandit framework, where the objective is to minimize the cumulative regret over\na sequence of tasks by incrementally transferring knowledge from prior tasks.\nWe introduce a novel bandit algorithm based on a method-of-moments approach for\nthe estimation of the possible tasks and derive regret bounds for it. \n\n"}
{"id": "1307.7721", "contents": "Title: Geodesic PCA in the Wasserstein space Abstract: We introduce the method of Geodesic Principal Component Analysis (GPCA) on\nthe space of probability measures on the line, with finite second moment,\nendowed with the Wasserstein metric. We discuss the advantages of this\napproach, over a standard functional PCA of probability densities in the\nHilbert space of square-integrable functions. We establish the consistency of\nthe method by showing that the empirical GPCA converges to its population\ncounterpart, as the sample size tends to infinity. A key property in the study\nof GPCA is the isometry between the Wasserstein space and a closed convex\nsubset of the space of square-integrable functions, with respect to an\nappropriate measure. Therefore, we consider the general problem of PCA in a\nclosed convex subset of a separable Hilbert space, which serves as basis for\nthe analysis of GPCA and also has interest in its own right. We provide\nillustrative examples on simple statistical models, to show the benefits of\nthis approach for data analysis. The method is also applied to a real dataset\nof population pyramids. \n\n"}
{"id": "1308.2853", "contents": "Title: When are Overcomplete Topic Models Identifiable? Uniqueness of Tensor\n  Tucker Decompositions with Structured Sparsity Abstract: Overcomplete latent representations have been very popular for unsupervised\nfeature learning in recent years. In this paper, we specify which overcomplete\nmodels can be identified given observable moments of a certain order. We\nconsider probabilistic admixture or topic models in the overcomplete regime,\nwhere the number of latent topics can greatly exceed the size of the observed\nword vocabulary. While general overcomplete topic models are not identifiable,\nwe establish generic identifiability under a constraint, referred to as topic\npersistence. Our sufficient conditions for identifiability involve a novel set\nof \"higher order\" expansion conditions on the topic-word matrix or the\npopulation structure of the model. This set of higher-order expansion\nconditions allow for overcomplete models, and require the existence of a\nperfect matching from latent topics to higher order observed words. We\nestablish that random structured topic models are identifiable w.h.p. in the\novercomplete regime. Our identifiability results allows for general\n(non-degenerate) distributions for modeling the topic proportions, and thus, we\ncan handle arbitrarily correlated topics in our framework. Our identifiability\nresults imply uniqueness of a class of tensor decompositions with structured\nsparsity which is contained in the class of Tucker decompositions, but is more\ngeneral than the Candecomp/Parafac (CP) decomposition. \n\n"}
{"id": "1308.5712", "contents": "Title: The Generalized Mean Information Coefficient Abstract: Reshef & Reshef recently published a paper in which they present a method\ncalled the Maximal Information Coefficient (MIC) that can detect all forms of\nstatistical dependence between pairs of variables as sample size goes to\ninfinity. While this method has been praised by some, it has also been\ncriticized for its lack of power in finite samples. We seek to modify MIC so\nthat it has higher power in detecting associations for limited sample sizes.\nHere we present the Generalized Mean Information Coefficient (GMIC), a\ngeneralization of MIC which incorporates a tuning parameter that can be used to\nmodify the complexity of the association favored by the measure. We define GMIC\nand prove it maintains several key asymptotic properties of MIC. Its increased\npower over MIC is demonstrated using a simulation of eight different functional\nrelationships at sixty different noise levels. The results are compared to the\nPearson correlation, distance correlation, and MIC. Simulation results suggest\nthat while generally GMIC has slightly lower power than the distance\ncorrelation measure, it achieves higher power than MIC for many forms of\nunderlying association. For some functional relationships, GMIC surpasses all\nother statistics calculated. Preliminary results suggest choosing a moderate\nvalue of the tuning parameter for GMIC will yield a test that is robust across\nunderlying relationships. GMIC is a promising new method that mitigates the\npower issues suffered by MIC, at the possible expense of equitability.\nNonetheless, distance correlation was in our simulations more powerful for many\nforms of underlying relationships. At a minimum, this work motivates further\nconsideration of maximal information-based nonparametric exploration (MINE)\nmethods as statistical tests of independence. \n\n"}
{"id": "1309.0302", "contents": "Title: Unmixing Incoherent Structures of Big Data by Randomized or Greedy\n  Decomposition Abstract: Learning big data by matrix decomposition always suffers from expensive\ncomputation, mixing of complicated structures and noise. In this paper, we\nstudy more adaptive models and efficient algorithms that decompose a data\nmatrix as the sum of semantic components with incoherent structures. We firstly\nintroduce \"GO decomposition (GoDec)\", an alternating projection method\nestimating the low-rank part $L$ and the sparse part $S$ from data matrix\n$X=L+S+G$ corrupted by noise $G$. Two acceleration strategies are proposed to\nobtain scalable unmixing algorithm on big data: 1) Bilateral random projection\n(BRP) is developed to speed up the update of $L$ in GoDec by a closed-form\nbuilt from left and right random projections of $X-S$ in lower dimensions; 2)\nGreedy bilateral (GreB) paradigm updates the left and right factors of $L$ in a\nmutually adaptive and greedy incremental manner, and achieve significant\nimprovement in both time and sample complexities. Then we proposes three\nnontrivial variants of GoDec that generalizes GoDec to more general data type\nand whose fast algorithms can be derived from the two strategies...... \n\n"}
{"id": "1309.3271", "contents": "Title: How to combine correlated data sets -- A Bayesian hyperparameter matrix\n  method Abstract: We construct a \"hyperparameter matrix\" statistical method for performing the\njoint analyses of multiple correlated astronomical data sets, in which the\nweights of data sets are determined by their own statistical properties. This\nmethod is a generalization of the hyperparameter method constructed by Lahav et\nal. (2000) and Hobson, Bridle, & Lahav (2002) which was designed to combine\nindependent data sets. The advantage of our method is to treat correlations\nbetween multiple data sets and gives appropriate relevant weights of multiple\ndata sets with mutual correlations. We define a new \"element-wise\" product,\nwhich greatly simplifies the likelihood function with hyperparameter matrix. We\nrigorously prove the simplified formula of the joint likelihood and show that\nit recovers the original hyperparameter method in the limit of no covariance\nbetween data sets. We then illustrate the method by applying it to a\ndemonstrative toy model of fitting a straight line to two sets of data. We show\nthat the hyperparameter matrix method can detect unaccounted systematic errors\nor underestimated errors in the data sets. Additionally, the ratio of Bayes'\nfactors provides a distinct indicator of the necessity of including\nhyperparameters. Our example shows that the likelihood we construct for joint\nanalyses of correlated data sets can be widely applied to many astrophysical\nsystems. \n\n"}
{"id": "1309.6473", "contents": "Title: On nonnegative unbiased estimators Abstract: We study the existence of algorithms generating almost surely nonnegative\nunbiased estimators. We show that given a nonconstant real-valued function $f$\nand a sequence of unbiased estimators of $\\lambda\\in\\mathbb{R}$, there is no\nalgorithm yielding almost surely nonnegative unbiased estimators of\n$f(\\lambda)\\in\\mathbb{R}^+$. The study is motivated by pseudo-marginal Monte\nCarlo algorithms that rely on such nonnegative unbiased estimators. These\nmethods allow \"exact inference\" in intractable models, in the sense that\nintegrals with respect to a target distribution can be estimated without any\nsystematic error, even though the associated probability density function\ncannot be evaluated pointwise. We discuss the consequences of our results on\nthe applicability of pseudo-marginal algorithms and thus on the possibility of\nexact inference in intractable models. We illustrate our study with particular\nchoices of functions $f$ corresponding to known challenges in statistics, such\nas exact simulation of diffusions, inference in large datasets and doubly\nintractable distributions. \n\n"}
{"id": "1309.6699", "contents": "Title: Finite Sample Properties of Adaptive Markov Chains via Curvature Abstract: Adaptive Markov chains are an important class of Monte Carlo methods for\nsampling from probability distributions. The time evolution of adaptive\nalgorithms depends on past samples, and thus these algorithms are\nnon-Markovian. Although there has been previous work establishing conditions\nfor their ergodicity, not much is known theoretically about their finite sample\nproperties. In this paper, using a notion of discrete Ricci curvature for\nMarkov kernels introduced by Ollivier, we establish concentration inequalities\nand finite sample bounds for a class of adaptive Markov chains. After\nestablishing some general results, we give quantitative bounds for\n`multi-level' adaptive algorithms such as the equi-energy sampler. We also\nprovide the first rigorous proofs that the finite sample properties of an\nequi-energy sampler are superior to those of related parallel tempering and\nMetropolis-Hastings samplers after a learning period comparable to their mixing\ntimes. \n\n"}
{"id": "1309.7064", "contents": "Title: Stable Intersections of Tropical Varieties Abstract: We give several characterizations of stable intersections of tropical cycles\nand establish their fundamental properties. We prove that the stable\nintersection of two tropical varieties is the tropicalization of the\nintersection of the classical varieties after a generic rescaling. A proof of\nBernstein's theorem follows from this. We prove that the tropical intersection\nring of tropical cycle fans is isomorphic to McMullen's polytope algebra. It\nfollows that every tropical cycle fan is a linear combination of pure powers of\ntropical hypersurfaces, which are always realizable. We prove that every stable\nintersection of constant coefficient tropical varieties defined by prime ideals\nis connected through codimension one. We also give an example of a realizable\ntropical variety that is connected through codimension one but whose stable\nintersection with a hyperplane is not. \n\n"}
{"id": "1310.1297", "contents": "Title: Spectral Clustering for Divide-and-Conquer Graph Matching Abstract: We present a parallelized bijective graph matching algorithm that leverages\nseeds and is designed to match very large graphs. Our algorithm combines\nspectral graph embedding with existing state-of-the-art seeded graph matching\nprocedures. We justify our approach by proving that modestly correlated, large\nstochastic block model random graphs are correctly matched utilizing very few\nseeds through our divide-and-conquer procedure. We also demonstrate the\neffectiveness of our approach in matching very large graphs in simulated and\nreal data examples, showing up to a factor of 8 improvement in runtime with\nminimal sacrifice in accuracy. \n\n"}
{"id": "1310.1560", "contents": "Title: Shapes of polyhedra, mixed volumes, and hyperbolic geometry Abstract: We are generalizing to higher dimensions the Bavard-Ghys construction of the\nhyperbolic metric on the space of polygons with fixed directions of edges.\n  The space of convex d-dimensional polyhedra with fixed directions of facet\nnormals has a decomposition into type cones that correspond to different\ncombinatorial types of polyhedra. This decomposition is a subfan of the\nsecondary fan of a vector configuration and can be analyzed with the help of\nGale diagrams.\n  We construct a family of quadratic forms on each of the type cones using the\ntheory of mixed volumes. The Alexandrov-Fenchel inequalities ensure that these\nforms have exactly one positive eigenvalue. This introduces a piecewise\nhyperbolic structure on the space of similarity classes of polyhedra with fixed\ndirections of facet normals. We show that some of the dihedral angles on the\nboundary of the resulting cone-manifold are equal to \\pi/2. \n\n"}
{"id": "1310.2409", "contents": "Title: Discriminative Relational Topic Models Abstract: Many scientific and engineering fields involve analyzing network data. For\ndocument networks, relational topic models (RTMs) provide a probabilistic\ngenerative process to describe both the link structure and document contents,\nand they have shown promise on predicting network structures and discovering\nlatent topic representations. However, existing RTMs have limitations in both\nthe restricted model expressiveness and incapability of dealing with imbalanced\nnetwork data. To expand the scope and improve the inference accuracy of RTMs,\nthis paper presents three extensions: 1) unlike the common link likelihood with\na diagonal weight matrix that allows the-same-topic interactions only, we\ngeneralize it to use a full weight matrix that captures all pairwise topic\ninteractions and is applicable to asymmetric networks; 2) instead of doing\nstandard Bayesian inference, we perform regularized Bayesian inference\n(RegBayes) with a regularization parameter to deal with the imbalanced link\nstructure issue in common real networks and improve the discriminative ability\nof learned latent representations; and 3) instead of doing variational\napproximation with strict mean-field assumptions, we present collapsed Gibbs\nsampling algorithms for the generalized relational topic models by exploring\ndata augmentation without making restricting assumptions. Under the generic\nRegBayes framework, we carefully investigate two popular discriminative loss\nfunctions, namely, the logistic log-loss and the max-margin hinge loss.\nExperimental results on several real network datasets demonstrate the\nsignificance of these extensions on improving the prediction performance, and\nthe time efficiency can be dramatically improved with a simple fast\napproximation method. \n\n"}
{"id": "1310.7529", "contents": "Title: Successive Nonnegative Projection Algorithm for Robust Nonnegative Blind\n  Source Separation Abstract: In this paper, we propose a new fast and robust recursive algorithm for\nnear-separable nonnegative matrix factorization, a particular nonnegative blind\nsource separation problem. This algorithm, which we refer to as the successive\nnonnegative projection algorithm (SNPA), is closely related to the popular\nsuccessive projection algorithm (SPA), but takes advantage of the nonnegativity\nconstraint in the decomposition. We prove that SNPA is more robust than SPA and\ncan be applied to a broader class of nonnegative matrices. This is illustrated\non some synthetic data sets, and on a real-world hyperspectral image. \n\n"}
{"id": "1310.8608", "contents": "Title: Lorentzian Coxeter systems and Boyd-Maxwell ball packings Abstract: In the recent study of infinite root systems, fractal patterns of ball\npackings were observed while visualizing roots in affine space. In this paper,\nwe show that the observed fractals are exactly the ball packings described by\nBoyd and Maxwell. This correspondence is a corollary of a more fundamental\nresult: Given a geometric representation of a Coxeter group in a Lorentz space,\nthe set of limit directions of weights equals the set of limit roots.\nAdditionally, we use Coxeter complexes to describe tangency graphs of the\ncorresponding Boyd--Maxwell ball packings. Finally, we enumerate all the\nCoxeter systems that generate Boyd-Maxwell ball packings. \n\n"}
{"id": "1311.0701", "contents": "Title: On Fast Dropout and its Applicability to Recurrent Networks Abstract: Recurrent Neural Networks (RNNs) are rich models for the processing of\nsequential data. Recent work on advancing the state of the art has been focused\non the optimization or modelling of RNNs, mostly motivated by adressing the\nproblems of the vanishing and exploding gradients. The control of overfitting\nhas seen considerably less attention. This paper contributes to that by\nanalyzing fast dropout, a recent regularization method for generalized linear\nmodels and neural networks from a back-propagation inspired perspective. We\nshow that fast dropout implements a quadratic form of an adaptive,\nper-parameter regularizer, which rewards large weights in the light of\nunderfitting, penalizes them for overconfident predictions and vanishes at\nminima of an unregularized training loss. The derivatives of that regularizer\nare exclusively based on the training error signal. One consequence of this is\nthe absense of a global weight attractor, which is particularly appealing for\nRNNs, since the dynamics are not biased towards a certain regime. We positively\ntest the hypothesis that this improves the performance of RNNs on four musical\ndata sets. \n\n"}
{"id": "1311.2335", "contents": "Title: A First-Order Algorithm for the A-Optimal Experimental Design Problem: A\n  Mathematical Programming Approach Abstract: We develop and analyse a first-order algorithm for the A-optimal experimental\ndesign problem. The problem is first presented as a special case of a\nparametric family of optimal design problems for which duality results and\noptimality conditions are given. Then, two first-order (Frank-Wolfe type)\nalgorithms are presented, accompanied by a detailed time-complexity analysis of\nthe algorithms and computational results on various sized problems. \n\n"}
{"id": "1311.2965", "contents": "Title: Derived subdivisions make every PL sphere polytopal Abstract: We give a simple proof that some iterated derived subdivision of every PL\nsphere is combinatorially equivalent to the boundary of a simplicial polytope,\nthereby resolving a problem of Billera (personal communication). \n\n"}
{"id": "1311.3350", "contents": "Title: Sequential Tests of Multiple Hypotheses Controlling False Discovery and\n  Nondiscovery Rates Abstract: We propose a general and flexible procedure for testing multiple hypotheses\nabout sequential (or streaming) data that simultaneously controls both the\nfalse discovery rate (FDR) and false nondiscovery rate (FNR) under minimal\nassumptions about the data streams which may differ in distribution, dimension,\nand be dependent. All that is needed is a test statistic for each data stream\nthat controls the conventional type I and II error probabilities, and no\ninformation or assumptions are required about the joint distribution of the\nstatistics or data streams. The procedure can be used with sequential, group\nsequential, truncated, or other sampling schemes. The procedure is a natural\nextension of Benjamini and Hochberg's (1995) widely-used fixed sample size\nprocedure to the domain of sequential data, with the added benefit of\nsimultaneous FDR and FNR control that sequential sampling affords. We prove the\nprocedure's error control and give some tips for implementation in commonly\nencountered testing situations. \n\n"}
{"id": "1311.4291", "contents": "Title: Minimum $n$-Rank Approximation via Iterative Hard Thresholding Abstract: The problem of recovering a low $n$-rank tensor is an extension of sparse\nrecovery problem from the low dimensional space (matrix space) to the high\ndimensional space (tensor space) and has many applications in computer vision\nand graphics such as image inpainting and video inpainting. In this paper, we\nconsider a new tensor recovery model, named as minimum $n$-rank approximation\n(MnRA), and propose an appropriate iterative hard thresholding algorithm with\ngiving the upper bound of the $n$-rank in advance. The convergence analysis of\nthe proposed algorithm is also presented. Particularly, we show that for the\nnoiseless case, the linear convergence with rate $\\frac{1}{2}$ can be obtained\nfor the proposed algorithm under proper conditions. Additionally, combining an\neffective heuristic for determining $n$-rank, we can also apply the proposed\nalgorithm to solve MnRA when $n$-rank is unknown in advance. Some preliminary\nnumerical results on randomly generated and real low $n$-rank tensor completion\nproblems are reported, which show the efficiency of the proposed algorithms. \n\n"}
{"id": "1311.4924", "contents": "Title: Robust Compressed Sensing Under Matrix Uncertainties Abstract: Compressed sensing (CS) shows that a signal having a sparse or compressible\nrepresentation can be recovered from a small set of linear measurements. In\nclassical CS theory, the sampling matrix and representation matrix are assumed\nto be known exactly in advance. However, uncertainties exist due to sampling\ndistortion, finite grids of the parameter space of dictionary, etc. In this\npaper, we take a generalized sparse signal model, which simultaneously\nconsiders the sampling and representation matrix uncertainties. Based on the\nnew signal model, a new optimization model for robust sparse signal\nreconstruction is proposed. This optimization model can be deduced with\nstochastic robust approximation analysis. Both convex relaxation and greedy\nalgorithms are used to solve the optimization problem. For the convex\nrelaxation method, a sufficient condition for recovery by convex relaxation is\ngiven; For the greedy algorithm, it is realized by the introduction of a\npre-processing of the sensing matrix and the measurements. In numerical\nexperiments, both simulated data and real-life ECG data based results show that\nthe proposed method has a better performance than the current methods. \n\n"}
{"id": "1311.5777", "contents": "Title: Exact simulation of the sample paths of a diffusion with a finite\n  entrance boundary Abstract: Diffusion processes arise in many fields, and so simulating the path of a\ndiffusion is an important problem. It is usually necessary to make some sort of\napproximation via model-discretization, but a recently introduced class of\nalgorithms, known as the exact algorithm and based on retrospective rejection\nsampling ideas, obviate the need for such discretization. In this paper I\nextend the exact algorithm to apply to a class of diffusions with a finite\nentrance boundary. The key innovation is that for these models the Bessel\nprocess is a more suitable candidate process than the more usually chosen\nBrownian motion. The algorithm is illustrated by an application to a general\ndiffusion model of population growth, where it simulates paths efficiently,\nwhile previous algorithms are impracticable. \n\n"}
{"id": "1311.7455", "contents": "Title: Semi-Penalized Inference with Direct False Discovery Rate Control in\n  High-Dimensions Abstract: We propose a new method, semi-penalized inference with direct false discovery\nrate control (SPIDR), for variable selection and confidence interval\nconstruction in high-dimensional linear regression. SPIDR first uses a\nsemi-penalized approach to constructing estimators of the regression\ncoefficients. We show that the SPIDR estimator is ideal in the sense that it\nequals an ideal least squares estimator with high probability under a sparsity\nand other suitable conditions. Consequently, the SPIDR estimator is\nasymptotically normal. Based on this distributional result, SPIDR determines\nthe selection rule by directly controlling false discovery rate. This provides\nan explicit assessment of the selection error. This also naturally leads to\nconfidence intervals for the selected coefficients with a proper confidence\nstatement. We conduct simulation studies to evaluate its finite sample\nperformance and demonstrate its application on a breast cancer gene expression\ndata set. Our simulation studies and data example suggest that SPIDR is a\nuseful method for high-dimensional statistical inference in practice. \n\n"}
{"id": "1312.0209", "contents": "Title: Bipartite Rigidity Abstract: We develop a bipartite rigidity theory for bipartite graphs parallel to the\nclassical rigidity theory for general graphs, and define for two positive\nintegers $k,l$ the notions of $(k,l)$-rigid and $(k,l)$-stress free bipartite\ngraphs. This theory coincides with the study of Babson--Novik's balanced\nshifting restricted to graphs. We establish bipartite analogs of the cone,\ncontraction, deletion, and gluing lemmas, and apply these results to derive a\nbipartite analog of the rigidity criterion for planar graphs. Our result\nasserts that for a planar bipartite graph $G$ its balanced shifting, $G^b$,\ndoes not contain $K_{3,3}$; equivalently, planar bipartite graphs are\ngenerically $(2,2)$-stress free. We also discuss potential applications of this\ntheory to Jockusch's cubical lower bound conjecture and to upper bound\nconjectures for embedded simplicial complexes. \n\n"}
{"id": "1312.1931", "contents": "Title: Multi-frame denoising of high speed optical coherence tomography data\n  using inter-frame and intra-frame priors Abstract: Optical coherence tomography (OCT) is an important interferometric diagnostic\ntechnique which provides cross-sectional views of the subsurface microstructure\nof biological tissues. However, the imaging quality of high-speed OCT is\nlimited due to the large speckle noise. To address this problem, this paper\nproposes a multi-frame algorithmic method to denoise OCT volume.\nMathematically, we build an optimization model which forces the temporally\nregistered frames to be low rank, and the gradient in each frame to be sparse,\nunder logarithmic image formation and noise variance constraints. Besides, a\nconvex optimization algorithm based on the augmented Lagrangian method is\nderived to solve the above model. The results reveal that our approach\noutperforms the other methods in terms of both speckle noise suppression and\ncrucial detail preservation. \n\n"}
{"id": "1312.2050", "contents": "Title: Consistency of spectral clustering in stochastic block models Abstract: We analyze the performance of spectral clustering for community extraction in\nstochastic block models. We show that, under mild conditions, spectral\nclustering applied to the adjacency matrix of the network can consistently\nrecover hidden communities even when the order of the maximum expected degree\nis as small as $\\log n$, with $n$ the number of nodes. This result applies to\nsome popular polynomial time spectral clustering algorithms and is further\nextended to degree corrected stochastic block models using a spherical\n$k$-median spectral clustering method. A key component of our analysis is a\ncombinatorial bound on the spectrum of binary random matrices, which is sharper\nthan the conventional matrix Bernstein inequality and may be of independent\ninterest. \n\n"}
{"id": "1401.1436", "contents": "Title: Accelerating ABC methods using Gaussian processes Abstract: Approximate Bayesian computation (ABC) methods are used to approximate\nposterior distributions using simulation rather than likelihood calculations.\nWe introduce Gaussian process (GP) accelerated ABC, which we show can\nsignificantly reduce the number of simulations required. As computational\nresource is usually the main determinant of accuracy in ABC, GP-accelerated\nmethods can thus enable more accurate inference in some models. GP models of\nthe unknown log-likelihood function are used to exploit continuity and\nsmoothness, reducing the required computation. We use a sequence of models that\nincrease in accuracy, using intermediate models to rule out regions of the\nparameter space as implausible. The methods will not be suitable for all\nproblems, but when they can be used, can result in significant computational\nsavings. For the Ricker model, we are able to achieve accurate approximations\nto the posterior distribution using a factor of 100 fewer simulator evaluations\nthan comparable Monte Carlo approaches, and for a population genetics model we\nare able to approximate the exact posterior for the first time. \n\n"}
{"id": "1401.1605", "contents": "Title: Fast nonparametric clustering of structured time-series Abstract: In this publication, we combine two Bayesian non-parametric models: the\nGaussian Process (GP) and the Dirichlet Process (DP). Our innovation in the GP\nmodel is to introduce a variation on the GP prior which enables us to model\nstructured time-series data, i.e. data containing groups where we wish to model\ninter- and intra-group variability. Our innovation in the DP model is an\nimplementation of a new fast collapsed variational inference procedure which\nenables us to optimize our variationala pproximation significantly faster than\nstandard VB approaches. In a biological time series application we show how our\nmodel better captures salient features of the data, leading to better\nconsistency with existing biological classifications, while the associated\ninference algorithm provides a twofold speed-up over EM-based variational\ninference. \n\n"}
{"id": "1401.4408", "contents": "Title: Embedding Graphs under Centrality Constraints for Network Visualization Abstract: Visual rendering of graphs is a key task in the mapping of complex network\ndata. Although most graph drawing algorithms emphasize aesthetic appeal,\ncertain applications such as travel-time maps place more importance on\nvisualization of structural network properties. The present paper advocates two\ngraph embedding approaches with centrality considerations to comply with node\nhierarchy. The problem is formulated first as one of constrained\nmulti-dimensional scaling (MDS), and it is solved via block coordinate descent\niterations with successive approximations and guaranteed convergence to a KKT\npoint. In addition, a regularization term enforcing graph smoothness is\nincorporated with the goal of reducing edge crossings. A second approach\nleverages the locally-linear embedding (LLE) algorithm which assumes that the\ngraph encodes data sampled from a low-dimensional manifold. Closed-form\nsolutions to the resulting centrality-constrained optimization problems are\ndetermined yielding meaningful embeddings. Experimental results demonstrate the\nefficacy of both approaches, especially for visualizing large networks on the\norder of thousands of nodes. \n\n"}
{"id": "1401.8257", "contents": "Title: Online Clustering of Bandits Abstract: We introduce a novel algorithmic approach to content recommendation based on\nadaptive clustering of exploration-exploitation (\"bandit\") strategies. We\nprovide a sharp regret analysis of this algorithm in a standard stochastic\nnoise setting, demonstrate its scalability properties, and prove its\neffectiveness on a number of artificial and real-world datasets. Our\nexperiments show a significant increase in prediction performance over\nstate-of-the-art methods for bandit problems. \n\n"}
{"id": "1402.3805", "contents": "Title: Cutting convex polytopes by hyperplanes Abstract: Cutting a polytope is a very natural way to produce new classes of\ninteresting polytopes. Moreover, it has been very enlightening to explore which\nalgebraic and combinatorial properties of the orignial polytope are hereditary\nto its subpolytopes obtained by a cut. In this work, we put our attention to\nall the seperating hyperplanes for some given polytope (integral and convex)\nand study the existence and classification of such hyperplanes.\n  We prove the exitence of seperating hyperplanes for the order and chain\npolytopes for any finite posets that are not a single chain; prove there are no\nsuch hyperplanes for any Birkhoff polytopes. Moreover, we give a complete\nseperating hyperplane classification for the unit cube and its subpolytopes\nobtained by one cut, together with some partial classification results for\norder and chain polytopes. \n\n"}
{"id": "1402.5257", "contents": "Title: Conditional Multilevel Monte Carlo Simulation of Groundwater Flow in the\n  Culebra Dolomite at the Waste Isolation Pilot Plant (WIPP) Site Abstract: We extended the multilevel Monte of Carlo (MLMC) approach to simulation of\ngroundwater flow in porous media by incorporating direct measurements of medium\nproperties. Numerical simulations of Waste Isolation Pilot Plant (WIPP)\nrepository in southeastern New Mexico are performed to test the performance of\nthe conditional MLMC technique. The log-transmissivity of WIPP site is modeled\nas the conditional random fields which honor exact field values at a few\nlocations. The conditional random fields are generated through the modified\ncirculant embedding methods in (Dietrich and Newsam, 1996). We also study\neffects of a combination of the conditional MLMC accompanied by antithetic\nvariates. The main quantity of interest is the time of radionuclides travelling\nfrom the center of repository to the site boundary. Numerical examples are\npresented to demonstrate the cost-effectiveness of the multilevel approach in\ncomparison to the standard Monte Carlo (MC) simulation. \n\n"}
{"id": "1402.5473", "contents": "Title: Scaling Nonparametric Bayesian Inference via Subsample-Annealing Abstract: We describe an adaptation of the simulated annealing algorithm to\nnonparametric clustering and related probabilistic models. This new algorithm\nlearns nonparametric latent structure over a growing and constantly churning\nsubsample of training data, where the portion of data subsampled can be\ninterpreted as the inverse temperature beta(t) in an annealing schedule. Gibbs\nsampling at high temperature (i.e., with a very small subsample) can more\nquickly explore sketches of the final latent state by (a) making longer jumps\naround latent space (as in block Gibbs) and (b) lowering energy barriers (as in\nsimulated annealing). We prove subsample annealing speeds up mixing time N^2 ->\nN in a simple clustering model and exp(N) -> N in another class of models,\nwhere N is data size. Empirically subsample-annealing outperforms naive Gibbs\nsampling in accuracy-per-wallclock time, and can scale to larger datasets and\ndeeper hierarchical models. We demonstrate improved inference on million-row\nsubsamples of US Census data and network log data and a 307-row hospital rating\ndataset, using a Pitman-Yor generalization of the Cross Categorization model. \n\n"}
{"id": "1403.0437", "contents": "Title: Volumes of convex lattice polytopes and a question of V. I. Arnold Abstract: We show by a direct construction that there are at least\n$\\exp\\{cV^{(d-1)/(d+1)}\\}$ convex lattice polytopes in $\\mathbb{R}^d$ of volume\n$V$ that are different in the sense that none of them can be carried to an\nother one by a lattice preserving affine transformation. This is achieved by\nconsidering the family $\\mathcal{P}^d(r)$ (to be defined in the text) of convex\nlattice polytopes whose volumes are between $0$ and $r^d/d!$. Namely we prove\nthat for $P \\in \\mathcal{P}^d(r)$, $d!\\mathrm{vol\\;} P$ takes all possible\ninteger values between $cr^{d-1}$ and $r^d$ where $c>0$ is a constant depending\nonly on $d$. \n\n"}
{"id": "1403.1166", "contents": "Title: Mathematical optimization for packing problems Abstract: During the last few years several new results on packing problems were\nobtained using a blend of tools from semidefinite optimization, polynomial\noptimization, and harmonic analysis. We survey some of these results and the\ntechniques involved, concentrating on geometric packing problems such as the\nsphere-packing problem or the problem of packing regular tetrahedra in R^3. \n\n"}
{"id": "1403.1502", "contents": "Title: Limit Directions for Lorentzian Coxeter Systems Abstract: Every Coxeter group admits a geometric representation as a group generated by\nreflections in a real vector space. In the projective representation space,\nlimit directions are limits of injective sequences in the orbit of some base\npoint. Limit roots are limit directions that can be obtained starting from\nsimple roots. In this article, we study the limit directions arising from any\npoint when the representation space is a Lorentz space. In particular, we\ncharacterize the light-like limit directions using eigenvectors of\ninfinite-order elements. This provides a spectral perspective on limit roots,\nallowing for efficient computations. Moreover, we describe the space-like limit\ndirections in terms of the projective Coxeter arrangement. \n\n"}
{"id": "1403.2940", "contents": "Title: Efficient Bayesian inference for long memory processes Abstract: In forecasting problems it is important to know whether or not recent events\nrepresent a regime change (low long-term predictive potential), or rather a\nlocal manifestation of longer term effects (potentially higher predictive\npotential). Mathematically, a key question is about whether the underlying\nstochastic process exhibits \"memory\", and if so whether the memory is \"long\" in\na precise sense. Being able to detect or rule out such effects can have a\nprofound impact on speculative investment (e.g., in financial markets) and\ninform public policy (e.g., characterising the size and timescales of the earth\nsystem's response to the anthropogenic CO2 perturbation). Most previous work on\ninference of long memory effects is frequentist in nature. Here we provide a\nsystematic treatment of Bayesian inference for long memory processes via the\nAutoregressive Fractional Integrated Moving Average (ARFIMA) model. In\nparticular, we provide a new approximate likelihood for efficient parameter\ninference, and show how nuisance parameters (e.g., short memory effects) can be\nintegrated over in order to focus on long memory parameters and hypothesis\ntesting more directly than ever before. We illustrate our new methodology on\nboth synthetic and observational data, with favorable comparison to the\nstandard estimators. \n\n"}
{"id": "1403.2963", "contents": "Title: Strong rules for nonconvex penalties and their implications for\n  efficient algorithms in high-dimensional regression Abstract: We consider approaches for improving the efficiency of algorithms for fitting\nnonconvex penalized regression models such as SCAD and MCP in high dimensions.\nIn particular, we develop rules for discarding variables during cyclic\ncoordinate descent. This dimension reduction leads to a substantial improvement\nin the speed of these algorithms for high-dimensional problems. The rules we\npropose here eliminate a substantial fraction of the variables from the\ncoordinate descent algorithm. Violations are quite rare, especially in the\nlocally convex region of the solution path, and furthermore, may be easily\ndetected and corrected by checking the Karush-Kuhn-Tucker conditions. We extend\nthese rules to generalized linear models, as well as to other nonconvex\npenalties such as the $\\ell_2$-stabilized Mnet penalty, group MCP, and group\nSCAD. We explore three variants of the coordinate decent algorithm that\nincorporate these rules and study the efficiency of these algorithms in fitting\nmodels to both simulated data and on real data from a genome-wide association\nstudy. \n\n"}
{"id": "1403.4359", "contents": "Title: Pre-processing for approximate Bayesian computation in image analysis Abstract: Most of the existing algorithms for approximate Bayesian computation (ABC)\nassume that it is feasible to simulate pseudo-data from the model at each\niteration. However, the computational cost of these simulations can be\nprohibitive for high dimensional data. An important example is the Potts model,\nwhich is commonly used in image analysis. Images encountered in real world\napplications can have millions of pixels, therefore scalability is a major\nconcern. We apply ABC with a synthetic likelihood to the hidden Potts model\nwith additive Gaussian noise. Using a pre-processing step, we fit a binding\nfunction to model the relationship between the model parameters and the\nsynthetic likelihood parameters. Our numerical experiments demonstrate that the\nprecomputed binding function dramatically improves the scalability of ABC,\nreducing the average runtime required for model fitting from 71 hours to only 7\nminutes. We also illustrate the method by estimating the smoothing parameter\nfor remotely sensed satellite imagery. Without precomputation, Bayesian\ninference is impractical for datasets of that scale. \n\n"}
{"id": "1403.4547", "contents": "Title: A note on the combinatorial structure of finite and locally finite\n  simplicial complexes of nonpositive curvature Abstract: We investigate the collapsibility of systolic finite simplicial complexes of\narbitrary dimension. The main tool we use in the proof is discrete Morse\ntheory. We shall consider a convex subcomplex of the complex and project any\nsimplex of the complex onto a ball around this convex subcomplex. These\nprojections will induce a convenient gradient matching on the complex. Besides\nwe analyze the combinatorial structure of both CAT(0) and systolic locally\nfinite simplicial complexes of arbitrary dimensions. We will show that both\nsuch complexes possess an arborescent structure. Along the way we make use of\ncertain well known results regarding systolic geometry. \n\n"}
{"id": "1403.6295", "contents": "Title: Asymptotic Properties of Minimum S-Divergence Estimator for Discrete\n  Models Abstract: Robust inference based on the minimization of statistical divergences has\nproved to be a useful alternative to the classical techniques based on maximum\nlikelihood and related methods. Recently Ghosh et al. (2013) proposed a general\nclass of divergence measures, namely the S-Divergence Family and discussed its\nusefulness in robust parametric estimation through some numerical\nillustrations. In this present paper, we develop the asymptotic properties of\nthe proposed minimum S-Divergence estimators under discrete models. \n\n"}
{"id": "1403.6573", "contents": "Title: Exact Inference for Gaussian Process Regression in case of Big Data with\n  the Cartesian Product Structure Abstract: Approximation algorithms are widely used in many engineering problems. To\nobtain a data set for approximation a factorial design of experiments is often\nused. In such case the size of the data set can be very large. Therefore, one\nof the most popular algorithms for approximation - Gaussian Process regression\n- can be hardly applied due to its computational complexity. In this paper a\nnew approach for Gaussian Process regression in case of factorial design of\nexperiments is proposed. It allows to efficiently compute exact inference and\nhandle large multidimensional data sets. The proposed algorithm provides fast\nand accurate approximation and also handles anisotropic data. \n\n"}
{"id": "1403.6706", "contents": "Title: Beyond L2-Loss Functions for Learning Sparse Models Abstract: Incorporating sparsity priors in learning tasks can give rise to simple, and\ninterpretable models for complex high dimensional data. Sparse models have\nfound widespread use in structure discovery, recovering data from corruptions,\nand a variety of large scale unsupervised and supervised learning problems.\nAssuming the availability of sufficient data, these methods infer dictionaries\nfor sparse representations by optimizing for high-fidelity reconstruction. In\nmost scenarios, the reconstruction quality is measured using the squared\nEuclidean distance, and efficient algorithms have been developed for both batch\nand online learning cases. However, new application domains motivate looking\nbeyond conventional loss functions. For example, robust loss functions such as\n$\\ell_1$ and Huber are useful in learning outlier-resilient models, and the\nquantile loss is beneficial in discovering structures that are the\nrepresentative of a particular quantile. These new applications motivate our\nwork in generalizing sparse learning to a broad class of convex loss functions.\nIn particular, we consider the class of piecewise linear quadratic (PLQ) cost\nfunctions that includes Huber, as well as $\\ell_1$, quantile, Vapnik, hinge\nloss, and smoothed variants of these penalties. We propose an algorithm to\nlearn dictionaries and obtain sparse codes when the data reconstruction\nfidelity is measured using any smooth PLQ cost function. We provide convergence\nguarantees for the proposed algorithm, and demonstrate the convergence behavior\nusing empirical experiments. Furthermore, we present three case studies that\nrequire the use of PLQ cost functions: (i) robust image modeling, (ii) tag\nrefinement for image annotation and retrieval and (iii) computing empirical\nconfidence limits for subspace clustering. \n\n"}
{"id": "1404.1361", "contents": "Title: Learning the Conditional Independence Structure of Stationary Time\n  Series: A Multitask Learning Approach Abstract: We propose a method for inferring the conditional independence graph (CIG) of\na high-dimensional Gaussian vector time series (discrete-time process) from a\nfinite-length observation. By contrast to existing approaches, we do not rely\non a parametric process model (such as, e.g., an autoregressive model) for the\nobserved random process. Instead, we only require certain smoothness properties\n(in the Fourier domain) of the process. The proposed inference scheme works\neven for sample sizes much smaller than the number of scalar process components\nif the true underlying CIG is sufficiently sparse. A theoretical performance\nanalysis provides conditions which guarantee that the probability of the\nproposed inference method to deliver a wrong CIG is below a prescribed value.\nThese conditions imply lower bounds on the sample size such that the new method\nis consistent asymptotically. Some numerical experiments validate our\ntheoretical performance analysis and demonstrate superior performance of our\nscheme compared to an existing (parametric) approach in case of model mismatch. \n\n"}
{"id": "1404.4414", "contents": "Title: Probit transformation for nonparametric kernel estimation of the copula\n  density Abstract: Copula modelling has become ubiquitous in modern statistics. Here, the\nproblem of nonparametrically estimating a copula density is addressed. Arguably\nthe most popular nonparametric density estimator, the kernel estimator is not\nsuitable for the unit-square-supported copula densities, mainly because it is\nheavily affected by boundary bias issues. In addition, most common copulas\nadmit unbounded densities, and kernel methods are not consistent in that case.\nIn this paper, a kernel-type copula density estimator is proposed. It is based\non the idea of transforming the uniform marginals of the copula density into\nnormal distributions via the probit function, estimating the density in the\ntransformed domain, which can be accomplished without boundary problems, and\nobtaining an estimate of the copula density through back-transformation.\nAlthough natural, a raw application of this procedure was, however, seen not to\nperform very well in the earlier literature. Here, it is shown that, if\ncombined with local likelihood density estimation methods, the idea yields very\ngood and easy to implement estimators, fixing boundary issues in a natural way\nand able to cope with unbounded copula densities. The asymptotic properties of\nthe suggested estimators are derived, and a practical way of selecting the\ncrucially important smoothing parameters is devised. Finally, extensive\nsimulation studies and a real data analysis evidence their excellent\nperformance compared to their main competitors. \n\n"}
{"id": "1404.4667", "contents": "Title: Subspace Learning and Imputation for Streaming Big Data Matrices and\n  Tensors Abstract: Extracting latent low-dimensional structure from high-dimensional data is of\nparamount importance in timely inference tasks encountered with `Big Data'\nanalytics. However, increasingly noisy, heterogeneous, and incomplete datasets\nas well as the need for {\\em real-time} processing of streaming data pose major\nchallenges to this end. In this context, the present paper permeates benefits\nfrom rank minimization to scalable imputation of missing data, via tracking\nlow-dimensional subspaces and unraveling latent (possibly multi-way) structure\nfrom \\emph{incomplete streaming} data. For low-rank matrix data, a subspace\nestimator is proposed based on an exponentially-weighted least-squares\ncriterion regularized with the nuclear norm. After recasting the non-separable\nnuclear norm into a form amenable to online optimization, real-time algorithms\nwith complementary strengths are developed and their convergence is established\nunder simplifying technical assumptions. In a stationary setting, the\nasymptotic estimates obtained offer the well-documented performance guarantees\nof the {\\em batch} nuclear-norm regularized estimator. Under the same unifying\nframework, a novel online (adaptive) algorithm is developed to obtain multi-way\ndecompositions of \\emph{low-rank tensors} with missing entries, and perform\nimputation as a byproduct. Simulated tests with both synthetic as well as real\nInternet and cardiac magnetic resonance imagery (MRI) data confirm the efficacy\nof the proposed algorithms, and their superior performance relative to\nstate-of-the-art alternatives. \n\n"}
{"id": "1404.4812", "contents": "Title: Beyond Bell's Theorem II: Scenarios with arbitrary causal structure Abstract: It has recently been found that Bell scenarios are only a small subclass of\ninteresting setups for studying the non-classical features of quantum theory\nwithin spacetime. We find that it is possible to talk about classical\ncorrelations, quantum correlations and other kinds of correlations on any\ndirected acyclic graph, and this captures various extensions of Bell scenarios\nwhich have been considered in the literature. From a conceptual point of view,\nthe main feature of our approach is its high level of unification: while the\nnotions of source, choice of setting and measurement play all seemingly\ndifferent roles in a Bell scenario, our formalism shows that they are all\ninstances of the same concept of \"event\".\n  Our work can also be understood as a contribution to the subject of causal\ninference with latent variables. Among other things, we introduce hidden\nBayesian networks as a generalization of hidden Markov models. \n\n"}
{"id": "1404.5609", "contents": "Title: Controlling the false discovery rate via knockoffs Abstract: In many fields of science, we observe a response variable together with a\nlarge number of potential explanatory variables, and would like to be able to\ndiscover which variables are truly associated with the response. At the same\ntime, we need to know that the false discovery rate (FDR) - the expected\nfraction of false discoveries among all discoveries - is not too high, in order\nto assure the scientist that most of the discoveries are indeed true and\nreplicable. This paper introduces the knockoff filter, a new variable selection\nprocedure controlling the FDR in the statistical linear model whenever there\nare at least as many observations as variables. This method achieves exact FDR\ncontrol in finite sample settings no matter the design or covariates, the\nnumber of variables in the model, or the amplitudes of the unknown regression\ncoefficients, and does not require any knowledge of the noise level. As the\nname suggests, the method operates by manufacturing knockoff variables that are\ncheap - their construction does not require any new data - and are designed to\nmimic the correlation structure found within the existing variables, in a way\nthat allows for accurate FDR control, beyond what is possible with\npermutation-based methods. The method of knockoffs is very general and\nflexible, and can work with a broad class of test statistics. We test the\nmethod in combination with statistics from the Lasso for sparse regression, and\nobtain empirical results showing that the resulting method has far more power\nthan existing selection rules when the proportion of null variables is high. \n\n"}
{"id": "1404.6909", "contents": "Title: Establishing some order amongst exact approximations of MCMCs Abstract: Exact approximations of Markov chain Monte Carlo (MCMC) algorithms are a\ngeneral emerging class of sampling algorithms. One of the main ideas behind\nexact approximations consists of replacing intractable quantities required to\nrun standard MCMC algorithms, such as the target probability density in a\nMetropolis-Hastings algorithm, with estimators. Perhaps surprisingly, such\napproximations lead to powerful algorithms which are exact in the sense that\nthey are guaranteed to have correct limiting distributions. In this paper we\ndiscover a general framework which allows one to compare, or order, performance\nmeasures of two implementations of such algorithms. In particular, we establish\nan order with respect to the mean acceptance probability, the first\nautocorrelation coefficient, the asymptotic variance and the right spectral\ngap. The key notion to guarantee the ordering is that of the convex order\nbetween estimators used to implement the algorithms. We believe that our convex\norder condition is close to optimal, and this is supported by a counter-example\nwhich shows that a weaker variance order is not sufficient. The convex order\nplays a central role by allowing us to construct a martingale coupling which\nenables the comparison of performance measures of Markov chain with differing\ninvariant distributions, contrary to existing results. We detail applications\nof our result by identifying extremal distributions within given classes of\napproximations, by showing that averaging replicas improves performance in a\nmonotonic fashion and that stratification is guaranteed to improve performance\nfor the standard implementation of the Approximate Bayesian Computation (ABC)\nMCMC method. \n\n"}
{"id": "1404.6909", "contents": "Title: Establishing some order amongst exact approximations of MCMCs Abstract: Exact approximations of Markov chain Monte Carlo (MCMC) algorithms are a\ngeneral emerging class of sampling algorithms. One of the main ideas behind\nexact approximations consists of replacing intractable quantities required to\nrun standard MCMC algorithms, such as the target probability density in a\nMetropolis-Hastings algorithm, with estimators. Perhaps surprisingly, such\napproximations lead to powerful algorithms which are exact in the sense that\nthey are guaranteed to have correct limiting distributions. In this paper we\ndiscover a general framework which allows one to compare, or order, performance\nmeasures of two implementations of such algorithms. In particular, we establish\nan order with respect to the mean acceptance probability, the first\nautocorrelation coefficient, the asymptotic variance and the right spectral\ngap. The key notion to guarantee the ordering is that of the convex order\nbetween estimators used to implement the algorithms. We believe that our convex\norder condition is close to optimal, and this is supported by a counter-example\nwhich shows that a weaker variance order is not sufficient. The convex order\nplays a central role by allowing us to construct a martingale coupling which\nenables the comparison of performance measures of Markov chain with differing\ninvariant distributions, contrary to existing results. We detail applications\nof our result by identifying extremal distributions within given classes of\napproximations, by showing that averaging replicas improves performance in a\nmonotonic fashion and that stratification is guaranteed to improve performance\nfor the standard implementation of the Approximate Bayesian Computation (ABC)\nMCMC method. \n\n"}
{"id": "1405.0042", "contents": "Title: Learning with incremental iterative regularization Abstract: Within a statistical learning setting, we propose and study an iterative\nregularization algorithm for least squares defined by an incremental gradient\nmethod. In particular, we show that, if all other parameters are fixed a\npriori, the number of passes over the data (epochs) acts as a regularization\nparameter, and prove strong universal consistency, i.e. almost sure convergence\nof the risk, as well as sharp finite sample bounds for the iterates. Our\nresults are a step towards understanding the effect of multiple epochs in\nstochastic gradient techniques in machine learning and rely on integrating\nstatistical and optimization results. \n\n"}
{"id": "1405.0913", "contents": "Title: A Brief Review of Optimal Scaling of the Main MCMC Approaches and\n  Optimal Scaling of Additive TMCMC Under Non-Regular Cases Abstract: Very recently, Transformation based Markov Chain Monte Carlo (TMCMC) was\nproposed by Dutta and Bhattcharya (2013) as a much efficient alternative to the\nMetropolis-Hastings algorithm, Random Walk Metropolis (RWM) algorithm,\nespecially in high dimensions. The main advantage of this algorithm is that it\nsimultaneously updates all components of a high dimensional parameter by some\nappropriate deterministic transformation of a single random variable, thereby\nreducing time complexity and enhancing the acceptance rate. The optimal scaling\nof the additive TMCMC approach has already been studied for the Gaussian\nproposal density by Dey and Bhattacharya(2013). In this paper, we discuss\ndiffusion-based optimal scaling behavior for non-Gaussian proposal densities -\nin particular, uniform, Student's t and Cauchy proposals. We also consider\ndiffusion based optimal scaling for non-Gaussian proposals when the target\ndensity is discontinuous. In the case of the Random Walk metropolis (RWM)\nalgorithm these non-regular situations have been studied by Neal and Roberts\n(2011) in terms of expected squared jumping distance (ESJD), but the diffusion\nbased approach has not been considered. Although we could not formally prove\nour diffusion result for the Cauchy proposal, simulation based results led us\nto a conjecture that the diffusion result still holds for the Cauchy case. We\ncompare our diffusion based TMCMC approach with that of ESJD based RWM approach\nfor the very challenging Cauchy proposal case, showing that our former approach\nclearly outperforms the latter. \n\n"}
{"id": "1405.4525", "contents": "Title: Bootstrap-based model selection criteria for beta regressions Abstract: The Akaike information criterion (AIC) is a model selection criterion widely\nused in practical applications. The AIC is an estimator of the log-likelihood\nexpected value, and measures the discrepancy between the true model and the\nestimated model. In small samples the AIC is biased and tends to select\noverparameterized models. To circumvent that problem, we propose two new\nselection criteria, namely: the bootstrapped likelihood quasi-CV (BQCV) and its\n632QCV variant. We use Monte Carlo simulation to compare the finite sample\nperformances of the two proposed criteria to those of the AIC and its\nvariations that use the bootstrapped log-likelihood in the class of varying\ndispersion beta regressions. The numerical evidence shows that the proposed\nmodel selection criteria perform well in small samples. We also present and\ndiscuss and empirical application. \n\n"}
{"id": "1405.6531", "contents": "Title: Gaussian Random Functional Dynamic Spatio-Temporal Modeling of Discrete\n  Time Spatial Time Series Data Abstract: Discrete time spatial time series data arise routinely in meteorological and\nenvironmental studies. Inference and prediction associated with them are mostly\ncarried out using any of the several variants of the linear state space model\nthat are collectively called linear dynamic spatio-temporal models (LDSTMs).\nHowever, real world environmental processes are highly complex and are seldom\nrepresentable by models with such simple linear structure. Hence, nonlinear\ndynamic spatio-temporal models (NLDSTMs) based on the idea of nonlinear\nobservational and evolutionary equation have been proposed as an alternative.\nHowever, in that case, the caveat lies in selecting the specific form of\nnonlinearity from a large class of potentially appropriate nonlinear functions.\nMoreover, modeling by NLDSTMs requires precise knowledge about the dynamics\nunderlying the data. In this article, we address this problem by introducing\nthe Gaussian random functional dynamic spatio-temporal model (GRFDSTM). Unlike\nthe LDSTMs or NLDSTMs, in GRFDSTM both the functions governing the\nobservational and evolutionary equations are composed of Gaussian random\nfunctions. We exhibit many interesting theoretical properties of the GRFDSTM\nand demonstrate how model fitting and prediction can be carried out coherently\nin a Bayesian framework. We also conduct an extensive simulation study and\napply our model to a real, SO2 pollution data over Europe. The results are\nhighly encouraging. \n\n"}
{"id": "1406.3521", "contents": "Title: Exact prior-free probabilistic inference on the heritability coefficient\n  in a linear mixed model Abstract: Linear mixed-effect models with two variance components are often used when\nvariability comes from two sources. In genetics applications, variation in\nobserved traits can be attributed to biological and environmental effects, and\nthe heritability coefficient is a fundamental quantity that measures the\nproportion of total variability due to the biological effect. We propose a new\ninferential model approach which yields exact prior-free probabilistic\ninference on the heritability coefficient. In particular we construct exact\nconfidence intervals and demonstrate numerically our method's efficiency\ncompared to that of existing methods. \n\n"}
{"id": "1406.4993", "contents": "Title: Divide-and-Conquer with Sequential Monte Carlo Abstract: We propose a novel class of Sequential Monte Carlo (SMC) algorithms,\nappropriate for inference in probabilistic graphical models. This class of\nalgorithms adopts a divide-and-conquer approach based upon an auxiliary\ntree-structured decomposition of the model of interest, turning the overall\ninferential task into a collection of recursively solved sub-problems. The\nproposed method is applicable to a broad class of probabilistic graphical\nmodels, including models with loops. Unlike a standard SMC sampler, the\nproposed Divide-and-Conquer SMC employs multiple independent populations of\nweighted particles, which are resampled, merged, and propagated as the method\nprogresses. We illustrate empirically that this approach can outperform\nstandard methods in terms of the accuracy of the posterior expectation and\nmarginal likelihood approximations. Divide-and-Conquer SMC also opens up novel\nparallel implementation options and the possibility of concentrating the\ncomputational effort on the most challenging sub-problems. We demonstrate its\nperformance on a Markov random field and on a hierarchical logistic regression\nproblem. \n\n"}
{"id": "1406.5015", "contents": "Title: Small cancellation labellings of some infinite graphs and applications Abstract: We construct small cancellation labellings for some infinite sequences of\nfinite graphs of bounded degree. We use them to define infinite graphical small\ncancellation presentations of groups. This technique allows us to provide\nexamples of groups with exotic properties:\n  - We construct the first examples of finitely generated coarsely non-amenable\ngroups (that is, groups without Guoliang Yu's Property A) that are coarsely\nembeddable into a Hilbert space. Moreover, our groups act properly on CAT(0)\ncubical complexes.\n  - We construct the first examples of finitely generated groups, with\nexpanders embedded isometrically into their Cayley graphs - in contrast,in the\ncase of the Gromov monster expanders are not even coarsely embedded.\n  We present further applications. \n\n"}
{"id": "1406.5550", "contents": "Title: ViDaExpert: user-friendly tool for nonlinear visualization and analysis\n  of multidimensional vectorial data Abstract: ViDaExpert is a tool for visualization and analysis of multidimensional\nvectorial data. ViDaExpert is able to work with data tables of \"object-feature\"\ntype that might contain numerical feature values as well as textual labels for\nrows (objects) and columns (features). ViDaExpert implements several\nstatistical methods such as standard and weighted Principal Component Analysis\n(PCA) and the method of elastic maps (non-linear version of PCA), Linear\nDiscriminant Analysis (LDA), multilinear regression, K-Means clustering, a\nvariant of decision tree construction algorithm. Equipped with several\nuser-friendly dialogs for configuring data point representations (size, shape,\ncolor) and fast 3D viewer, ViDaExpert is a handy tool allowing to construct an\ninteractive 3D-scene representing a table of data in multidimensional space and\nperform its quick and insightfull statistical analysis, from basic to advanced\nmethods. \n\n"}
{"id": "1406.5663", "contents": "Title: Asymptotic theory for density ridges Abstract: The large sample theory of estimators for density modes is well understood.\nIn this paper we consider density ridges, which are a higher-dimensional\nextension of modes. Modes correspond to zero-dimensional, local high-density\nregions in point clouds. Density ridges correspond to $s$-dimensional, local\nhigh-density regions in point clouds. We establish three main results. First we\nshow that under appropriate regularity conditions, the local variation of the\nestimated ridge can be approximated by an empirical process. Second, we show\nthat the distribution of the estimated ridge converges to a Gaussian process.\nThird, we establish that the bootstrap leads to valid confidence sets for\ndensity ridges. \n\n"}
{"id": "1406.7648", "contents": "Title: Bayesian Network Constraint-Based Structure Learning Algorithms:\n  Parallel and Optimised Implementations in the bnlearn R Package Abstract: It is well known in the literature that the problem of learning the structure\nof Bayesian networks is very hard to tackle: its computational complexity is\nsuper-exponential in the number of nodes in the worst case and polynomial in\nmost real-world scenarios.\n  Efficient implementations of score-based structure learning benefit from past\nand current research in optimisation theory, which can be adapted to the task\nby using the network score as the objective function to maximise. This is not\ntrue for approaches based on conditional independence tests, called\nconstraint-based learning algorithms. The only optimisation in widespread use,\nbacktracking, leverages the symmetries implied by the definitions of\nneighbourhood and Markov blanket.\n  In this paper we illustrate how backtracking is implemented in recent\nversions of the bnlearn R package, and how it degrades the stability of\nBayesian network structure learning for little gain in terms of speed. As an\nalternative, we describe a software architecture and framework that can be used\nto parallelise constraint-based structure learning algorithms (also implemented\nin bnlearn) and we demonstrate its performance using four reference networks\nand two real-world data sets from genetics and systems biology. We show that on\nmodern multi-core or multiprocessor hardware parallel implementations are\npreferable over backtracking, which was developed when single-processor\nmachines were the norm. \n\n"}
{"id": "1407.5241", "contents": "Title: Influential Feature PCA for high dimensional clustering Abstract: We consider a clustering problem where we observe feature vectors $X_i \\in\nR^p$, $i = 1, 2, \\ldots, n$, from $K$ possible classes. The class labels are\nunknown and the main interest is to estimate them. We are primarily interested\nin the modern regime of $p \\gg n$, where classical clustering methods face\nchallenges.\n  We propose Influential Features PCA (IF-PCA) as a new clustering procedure.\nIn IF-PCA, we select a small fraction of features with the largest\nKolmogorov-Smirnov (KS) scores, where the threshold is chosen by adapting the\nrecent notion of Higher Criticism, obtain the first $(K-1)$ left singular\nvectors of the post-selection normalized data matrix, and then estimate the\nlabels by applying the classical k-means to these singular vectors. It can be\nseen that IF-PCA is a tuning free clustering method.\n  We apply IF-PCA to $10$ gene microarray data sets. The method has competitive\nperformance in clustering. Especially, in three of the data sets, the error\nrates of IF-PCA are only $29\\%$ or less of the error rates by other methods. We\nhave also rediscovered a phenomenon on empirical null by \\cite{Efron} on\nmicroarray data.\n  With delicate analysis, especially post-selection eigen-analysis, we derive\ntight probability bounds on the Kolmogorov-Smirnov statistics and show that\nIF-PCA yields clustering consistency in a broad context. The clustering problem\nis connected to the problems of sparse PCA and low-rank matrix recovery, but it\nis different in important ways. We reveal an interesting phase transition\nphenomenon associated with these problems and identify the range of interest\nfor each. \n\n"}
{"id": "1407.5614", "contents": "Title: A spectral lower bound for the divisorial gonality of metric graphs Abstract: Let $\\Gamma$ be a compact metric graph, and denote by $\\Delta$ the Laplace\noperator on $\\Gamma$ with the first non-trivial eigenvalue $\\lambda_1$. We\nprove the following Yang-Li-Yau type inequality on divisorial gonality\n$\\gamma_{div}$ of $\\Gamma$. There is a universal constant $C$ such that\n\\[\\gamma_{div}(\\Gamma) \\geq C \\frac{\\mu(\\Gamma) .\n\\ell_{\\min}^{\\mathrm{geo}}(\\Gamma). \\lambda_1(\\Gamma)}{d_{\\max}},\\] where the\nvolume $\\mu(\\Gamma)$ is the total length of the edges in $\\Gamma$,\n$\\ell_{\\min}^{\\mathrm{geo}}$ is the minimum length of all the geodesic paths\nbetween points of $\\Gamma$ of valence different from two, and $d_{\\max}$ is the\nlargest valence of points of $\\Gamma$. Along the way, we also establish\ndiscrete versions of the above inequality concerning finite simple graph models\nof $\\Gamma$ and their spectral gaps. \n\n"}
{"id": "1407.7969", "contents": "Title: Automated Machine Learning on Big Data using Stochastic Algorithm Tuning Abstract: We introduce a means of automating machine learning (ML) for big data tasks,\nby performing scalable stochastic Bayesian optimisation of ML algorithm\nparameters and hyper-parameters. More often than not, the critical tuning of ML\nalgorithm parameters has relied on domain expertise from experts, along with\nlaborious hand-tuning, brute search or lengthy sampling runs. Against this\nbackground, Bayesian optimisation is finding increasing use in automating\nparameter tuning, making ML algorithms accessible even to non-experts. However,\nthe state of the art in Bayesian optimisation is incapable of scaling to the\nlarge number of evaluations of algorithm performance required to fit realistic\nmodels to complex, big data. We here describe a stochastic, sparse, Bayesian\noptimisation strategy to solve this problem, using many thousands of noisy\nevaluations of algorithm performance on subsets of data in order to effectively\ntrain algorithms for big data. We provide a comprehensive benchmarking of\npossible sparsification strategies for Bayesian optimisation, concluding that a\nNystrom approximation offers the best scaling and performance for real tasks.\nOur proposed algorithm demonstrates substantial improvement over the state of\nthe art in tuning the parameters of a Gaussian Process time series prediction\ntask on real, big data. \n\n"}
{"id": "1408.2698", "contents": "Title: Approximate D-optimal Experimental Design with Simultaneous Size and\n  Cost Constraints Abstract: Consider an experiment with a finite set of design points representing\npermissible trial conditions. Suppose that each trial is associated with a cost\nthat depends on the selected design point. In this paper, we study the problem\nof constructing an approximate D-optimal experimental design with simultaneous\nrestrictions on the size and on the total cost. For the problem of\nsize-and-cost constrained D-optimality, we formulate an equivalence theorem and\nrules for the removal of redundant design points. We also propose a simple\nmonotonically convergent \"barycentric\" algorithm that allows us to numerically\ncompute a size-and-cost constrained approximate D-optimal design. \n\n"}
{"id": "1408.2773", "contents": "Title: On Integration Methods Based on Scrambled Nets of Arbitrary Size Abstract: We consider the problem of evaluating $I(\\varphi):=\\int_{[0,1)^s}\\varphi(x)\ndx$ for a function $\\varphi \\in L^2[0,1)^{s}$. In situations where $I(\\varphi)$\ncan be approximated by an estimate of the form\n$N^{-1}\\sum_{n=0}^{N-1}\\varphi(x^n)$, with $\\{x^n\\}_{n=0}^{N-1}$ a point set in\n$[0,1)^s$, it is now well known that the $O_P(N^{-1/2})$ Monte Carlo\nconvergence rate can be improved by taking for $\\{x^n\\}_{n=0}^{N-1}$ the first\n$N=\\lambda b^m$ points, $\\lambda\\in\\{1,\\dots,b-1\\}$, of a scrambled\n$(t,s)$-sequence in base $b\\geq 2$. In this paper we derive a bound for the\nvariance of scrambled net quadrature rules which is of order $o(N^{-1})$\nwithout any restriction on $N$. As a corollary, this bound allows us to provide\nsimple conditions to get, for any pattern of $N$, an integration error of size\n$o_P(N^{-1/2})$ for functions that depend on the quadrature size $N$. Notably,\nwe establish that sequential quasi-Monte Carlo (M. Gerber and N. Chopin, 2015,\n\\emph{J. R. Statist. Soc. B, to appear.}) reaches the $o_P(N^{-1/2})$\nconvergence rate for any values of $N$. In a numerical study, we show that for\nscrambled net quadrature rules we can relax the constraint on $N$ without any\nloss of efficiency when the integrand $\\varphi$ is a discontinuous function\nwhile, for sequential quasi-Monte Carlo, taking $N=\\lambda b^m$ may only\nprovide moderate gains. \n\n"}
{"id": "1408.3467", "contents": "Title: Evaluating Visual Properties via Robust HodgeRank Abstract: Nowadays, how to effectively evaluate visual properties has become a popular\ntopic for fine-grained visual comprehension. In this paper we study the problem\nof how to estimate such visual properties from a ranking perspective with the\nhelp of the annotators from online crowdsourcing platforms. The main challenges\nof our task are two-fold. On one hand, the annotations often contain\ncontaminated information, where a small fraction of label flips might ruin the\nglobal ranking of the whole dataset. On the other hand, considering the large\ndata capacity, the annotations are often far from being complete. What is\nworse, there might even exist imbalanced annotations where a small subset of\nsamples are frequently annotated. Facing such challenges, we propose a robust\nranking framework based on the principle of Hodge decomposition of imbalanced\nand incomplete ranking data. According to the HodgeRank theory, we find that\nthe major source of the contamination comes from the cyclic ranking component\nof the Hodge decomposition. This leads us to an outlier detection formulation\nas sparse approximations of the cyclic ranking projection. Taking a step\nfurther, it facilitates a novel outlier detection model as Huber's LASSO in\nrobust statistics. Moreover, simple yet scalable algorithms are developed based\non Linearized Bregman Iteration to achieve an even less biased estimator.\nStatistical consistency of outlier detection is established in both cases under\nnearly the same conditions. Our studies are supported by experiments with both\nsimulated examples and real-world data. The proposed framework provides us a\npromising tool for robust ranking with large scale crowdsourcing data arising\nfrom computer vision. \n\n"}
{"id": "1408.4344", "contents": "Title: Optimal scaling for the pseudo-marginal random walk Metropolis:\n  insensitivity to the noise generating mechanism Abstract: We examine the optimal scaling and the efficiency of the pseudo-marginal\nrandom walk Metropolis algorithm using a recently-derived result on the\nlimiting efficiency as the dimension, $d\\rightarrow \\infty$. We prove that the\noptimal scaling for a given target varies by less than $20\\%$ across a wide\nrange of distributions for the noise in the estimate of the target, and that\nany scaling that is within $20\\%$ of the optimal one will be at least $70\\%$\nefficient. We demonstrate that this phenomenon occurs even outside the range of\ndistributions for which we rigorously prove it. We then conduct a simulation\nstudy on an example with $d=10$ where importance sampling is used to estimate\nthe target density; we also examine results available from an existing\nsimulations study with $d=5$ and where a particle filter was used. Our key\nconclusions are found to hold in these examples also. \n\n"}
{"id": "1409.1910", "contents": "Title: Symmetries of hyperbolic 4-manifolds Abstract: In this paper, for each finite group $G$, we construct explicitly a\nnon-compact complete finite-volume arithmetic hyperbolic $4$-manifold $M$ such\nthat $\\mathrm{Isom}\\,M \\cong G$, or $\\mathrm{Isom}^{+}\\,M \\cong G$. In order to\ndo so, we use essentially the geometry of Coxeter polytopes in the hyperbolic\n$4$-space, on one hand, and the combinatorics of simplicial complexes, on the\nother. \n\n"}
{"id": "1409.3287", "contents": "Title: Metric dimensions of minor excluded graphs and minor exclusion in groups Abstract: An infinite graph G is minor excluded if there is a finite graph that is not\na minor of G. We prove that minor excluded graphs have finite Assouad-Nagata\ndimension and study minor exclusion for Cayley graphs of finitely generated\ngroups. Our main results and observations are: (1) minor exclusion is not a\ngroup property: it depends on the choice of generating set; (2) a group with\none end has a generating set for which the Cayley graph is not minor excluded;\n(3) there are groups that are not minor excluded for any set of generators; (4)\nminor exclusion is preserved under free products; and (5) virtually free groups\nare minor excluded for any choice of finite generating set. \n\n"}
{"id": "1409.5191", "contents": "Title: Hamiltonian Monte Carlo Without Detailed Balance Abstract: We present a method for performing Hamiltonian Monte Carlo that largely\neliminates sample rejection for typical hyperparameters. In situations that\nwould normally lead to rejection, instead a longer trajectory is computed until\na new state is reached that can be accepted. This is achieved using Markov\nchain transitions that satisfy the fixed point equation, but do not satisfy\ndetailed balance. The resulting algorithm significantly suppresses the random\nwalk behavior and wasted function evaluations that are typically the\nconsequence of update rejection. We demonstrate a greater than factor of two\nimprovement in mixing time on three test problems. We release the source code\nas Python and MATLAB packages. \n\n"}
{"id": "1409.6326", "contents": "Title: Characterisations of algebraic properties of groups in terms of harmonic\n  functions Abstract: We prove various results connecting structural or algebraic properties of\ngraphs and groups to conditions on their spaces of harmonic functions. In\nparticular: we show that a group with a finitely supported symmetric measure\nhas a finite-dimensional space of harmonic functions if and only if it is\nvirtually cyclic; we present a new proof of a result of V. Trofimov that an\ninfinite vertex-transitive graph admits a non-constant harmonic function; we\ngive a new proof of a result of T. Ceccherini-Silberstein, M. Coornaert and J.\nDodziuk that the Laplacian on an infinite, connected, locally finite graph is\nsurjective; and we show that the positive harmonic functions on a non-virtually\nnilpotent linear group span an infinite-dimensional space. \n\n"}
{"id": "1409.8565", "contents": "Title: Sparse CCA: Adaptive Estimation and Computational Barriers Abstract: Canonical correlation analysis is a classical technique for exploring the\nrelationship between two sets of variables. It has important applications in\nanalyzing high dimensional datasets originated from genomics, imaging and other\nfields. This paper considers adaptive minimax and computationally tractable\nestimation of leading sparse canonical coefficient vectors in high dimensions.\nFirst, we establish separate minimax estimation rates for canonical coefficient\nvectors of each set of random variables under no structural assumption on\nmarginal covariance matrices. Second, we propose a computationally feasible\nestimator to attain the optimal rates adaptively under an additional sample\nsize condition. Finally, we show that a sample size condition of this kind is\nneeded for any randomized polynomial-time estimator to be consistent, assuming\nhardness of certain instances of the Planted Clique detection problem. The\nresult is faithful to the Gaussian models used in the paper. As a byproduct, we\nobtain the first computational lower bounds for sparse PCA under the Gaussian\nsingle spiked covariance model. \n\n"}
{"id": "1410.0247", "contents": "Title: A Practical Scheme and Fast Algorithm to Tune the Lasso With Optimality\n  Guarantees Abstract: We introduce a novel scheme for choosing the regularization parameter in\nhigh-dimensional linear regression with Lasso. This scheme, inspired by\nLepski's method for bandwidth selection in non-parametric regression, is\nequipped with both optimal finite-sample guarantees and a fast algorithm. In\nparticular, for any design matrix such that the Lasso has low sup-norm error\nunder an \"oracle choice\" of the regularization parameter, we show that our\nmethod matches the oracle performance up to a small constant factor, and show\nthat it can be implemented by performing simple tests along a single Lasso\npath. By applying the Lasso to simulated and real data, we find that our novel\nscheme can be faster and more accurate than standard schemes such as\nCross-Validation. \n\n"}
{"id": "1410.0389", "contents": "Title: Learning to Transfer Privileged Information Abstract: We introduce a learning framework called learning using privileged\ninformation (LUPI) to the computer vision field. We focus on the prototypical\ncomputer vision problem of teaching computers to recognize objects in images.\nWe want the computers to be able to learn faster at the expense of providing\nextra information during training time. As additional information about the\nimage data, we look at several scenarios that have been studied in computer\nvision before: attributes, bounding boxes and image tags. The information is\nprivileged as it is available at training time but not at test time. We explore\ntwo maximum-margin techniques that are able to make use of this additional\nsource of information, for binary and multiclass object classification. We\ninterpret these methods as learning easiness and hardness of the objects in the\nprivileged space and then transferring this knowledge to train a better\nclassifier in the original space. We provide a thorough analysis and comparison\nof information transfer from privileged to the original data spaces for both\nLUPI methods. Our experiments show that incorporating privileged information\ncan improve the classification accuracy. Finally, we conduct user studies to\nunderstand which samples are easy and which are hard for human learning, and\nexplore how this information is related to easy and hard samples when learning\na classifier. \n\n"}
{"id": "1410.3155", "contents": "Title: Sample Size Dependent Species Models Abstract: Motivated by the fundamental problem of measuring species diversity, this\npaper introduces the concept of a cluster structure to define an exchangeable\ncluster probability function that governs the joint distribution of a random\ncount and its exchangeable random partitions. A cluster structure, naturally\narising from a completely random measure mixed Poisson process, allows the\nprobability distribution of the random partitions of a subset of a sample to be\ndependent on the sample size, a distinct and motivated feature that differs it\nfrom a partition structure. A generalized negative binomial process model is\nproposed to generate a cluster structure, where in the prior the number of\nclusters is finite and Poisson distributed, and the cluster sizes follow a\ntruncated negative binomial distribution. We construct a nonparametric Bayesian\nestimator of Simpson's index of diversity under the generalized negative\nbinomial process. We illustrate our results through the analysis of two real\nsequencing count datasets. \n\n"}
{"id": "1410.3886", "contents": "Title: Tighter Low-rank Approximation via Sampling the Leveraged Element Abstract: In this work, we propose a new randomized algorithm for computing a low-rank\napproximation to a given matrix. Taking an approach different from existing\nliterature, our method first involves a specific biased sampling, with an\nelement being chosen based on the leverage scores of its row and column, and\nthen involves weighted alternating minimization over the factored form of the\nintended low-rank matrix, to minimize error only on these samples. Our method\ncan leverage input sparsity, yet produce approximations in {\\em spectral} (as\nopposed to the weaker Frobenius) norm; this combines the best aspects of\notherwise disparate current results, but with a dependence on the condition\nnumber $\\kappa = \\sigma_1/\\sigma_r$. In particular we require $O(nnz(M) +\n\\frac{n\\kappa^2 r^5}{\\epsilon^2})$ computations to generate a rank-$r$\napproximation to $M$ in spectral norm. In contrast, the best existing method\nrequires $O(nnz(M)+ \\frac{nr^2}{\\epsilon^4})$ time to compute an approximation\nin Frobenius norm. Besides the tightness in spectral norm, we have a better\ndependence on the error $\\epsilon$. Our method is naturally and highly\nparallelizable.\n  Our new approach enables two extensions that are interesting on their own.\nThe first is a new method to directly compute a low-rank approximation (in\nefficient factored form) to the product of two given matrices; it computes a\nsmall random set of entries of the product, and then executes weighted\nalternating minimization (as before) on these. The sampling strategy is\ndifferent because now we cannot access leverage scores of the product matrix\n(but instead have to work with input matrices). The second extension is an\nimproved algorithm with smaller communication complexity for the distributed\nPCA setting (where each server has small set of rows of the matrix, and want to\ncompute low rank approximation with small amount of communication with other\nservers). \n\n"}
{"id": "1410.4217", "contents": "Title: Sequential Importance Sampling for Two-dimensional Ising Models Abstract: In recent years, sequential importance sampling (SIS) has been well developed\nfor sampling contingency tables with linear constraints. In this paper, we\napply SIS procedure to 2-dimensional Ising models, which give observations of\n0-1 tables and include both linear and quadratic constraints. We show how to\ncompute bounds for specific cells by solving linear programming (LP) problems\nover cut polytopes to reduce rejections. The computational results, which\nincludes both simulations and real data analysis, suggest that our method\nperforms very well for sparse tables and when the 1's are spread out: the\ncomputational times are short, the acceptance rates are high, and if proper\ntests are used then in most cases our conclusions are theoretically reasonable. \n\n"}
{"id": "1410.5014", "contents": "Title: Optimal Two-Step Prediction in Regression Abstract: High-dimensional prediction typically comprises two steps: variable selection\nand subsequent least-squares refitting on the selected variables. However, the\nstandard variable selection procedures, such as the lasso, hinge on tuning\nparameters that need to be calibrated. Cross-validation, the most popular\ncalibration scheme, is computationally costly and lacks finite sample\nguarantees. In this paper, we introduce an alternative scheme, easy to\nimplement and both computationally and theoretically efficient. \n\n"}
{"id": "1410.8229", "contents": "Title: Two New Approaches to Compressed Sensing Exhibiting Both Robust Sparse\n  Recovery and the Grouping Effect Abstract: In this paper we introduce a new optimization formulation for sparse\nregression and compressed sensing, called CLOT (Combined L-One and Two),\nwherein the regularizer is a convex combination of the $\\ell_1$- and\n$\\ell_2$-norms. This formulation differs from the Elastic Net (EN) formulation,\nin which the regularizer is a convex combination of the $\\ell_1$- and\n$\\ell_2$-norm squared. It is shown that, in the context of compressed sensing,\nthe EN formulation does not achieve robust recovery of sparse vectors, whereas\nthe new CLOT formulation achieves robust recovery. Also, like EN but unlike\nLASSO, the CLOT formulation achieves the grouping effect, wherein coefficients\nof highly correlated columns of the measurement (or design) matrix are assigned\nroughly comparable values. It is already known LASSO does not have the grouping\neffect. Therefore the CLOT formulation combines the best features of both LASSO\n(robust sparse recovery) and EN (grouping effect).\n  The CLOT formulation is a special case of another one called SGL (Sparse\nGroup LASSO) which was introduced into the literature previously, but without\nany analysis of either the grouping effect or robust sparse recovery. It is\nshown here that SGL achieves robust sparse recovery, and also achieves a\nversion of the grouping effect in that coefficients of highly correlated\ncolumns belonging to the same group of the measurement (or design) matrix are\nassigned roughly comparable values. \n\n"}
{"id": "1411.4257", "contents": "Title: Choosing the number of clusters in a finite mixture model using an exact\n  Integrated Completed Likelihood criterion Abstract: The integrated completed likelihood (ICL) criterion has proven to be a very\npopular approach in model-based clustering through automatically choosing the\nnumber of clusters in a mixture model. This approach effectively maximises the\ncomplete data likelihood, thereby including the allocation of observations to\nclusters in the model selection criterion. However for practical implementation\none needs to introduce an approximation in order to estimate the ICL. Our\ncontribution here is to illustrate that through the use of conjugate priors one\ncan derive an exact expression for ICL and so avoiding any approximation.\nMoreover, we illustrate how one can find both the number of clusters and the\nbest allocation of observations in one algorithmic framework. The performance\nof our algorithm is presented on several simulated and real examples. \n\n"}
{"id": "1411.6591", "contents": "Title: A Latent Source Model for Online Collaborative Filtering Abstract: Despite the prevalence of collaborative filtering in recommendation systems,\nthere has been little theoretical development on why and how well it works,\nespecially in the \"online\" setting, where items are recommended to users over\ntime. We address this theoretical gap by introducing a model for online\nrecommendation systems, cast item recommendation under the model as a learning\nproblem, and analyze the performance of a cosine-similarity collaborative\nfiltering method. In our model, each of $n$ users either likes or dislikes each\nof $m$ items. We assume there to be $k$ types of users, and all the users of a\ngiven type share a common string of probabilities determining the chance of\nliking each item. At each time step, we recommend an item to each user, where a\nkey distinction from related bandit literature is that once a user consumes an\nitem (e.g., watches a movie), then that item cannot be recommended to the same\nuser again. The goal is to maximize the number of likable items recommended to\nusers over time. Our main result establishes that after nearly $\\log(km)$\ninitial learning time steps, a simple collaborative filtering algorithm\nachieves essentially optimal performance without knowing $k$. The algorithm has\nan exploitation step that uses cosine similarity and two types of exploration\nsteps, one to explore the space of items (standard in the literature) and the\nother to explore similarity between users (novel to this work). \n\n"}
{"id": "1411.6669", "contents": "Title: Optimizing The Integrator Step Size for Hamiltonian Monte Carlo Abstract: Hamiltonian Monte Carlo can provide powerful inference in complex statistical\nproblems, but ultimately its performance is sensitive to various tuning\nparameters. In this paper we use the underlying geometry of Hamiltonian Monte\nCarlo to construct a universal optimization criteria for tuning the step size\nof the symplectic integrator crucial to any implementation of the algorithm as\nwell as diagnostics to monitor for any signs of invalidity. An immediate\noutcome of this result is that the suggested target average acceptance\nprobability of 0.651 can be relaxed to $0.6 \\lesssim a \\lesssim 0.9$ with\nlarger values more robust in practice. \n\n"}
{"id": "1412.0753", "contents": "Title: Convex clustering via $\\ell_1$ fusion penalization Abstract: We study the large sample behavior of a convex clustering framework, which\nminimizes the sample within cluster sum of squares under an~$\\ell_1$ fusion\nconstraint on the cluster centroids. This recently proposed approach has been\ngaining in popularity, however, its asymptotic properties have remained mostly\nunknown. Our analysis is based on a novel representation of the sample\nclustering procedure as a sequence of cluster splits determined by a sequence\nof maximization problems. We use this representation to provide a simple and\nintuitive formulation for the population clustering procedure. We then\ndemonstrate that the sample procedure consistently estimates its population\nanalog, and derive the corresponding rates of convergence. The proof conducts a\ncareful simultaneous analysis of a collection of M-estimation problems, whose\ncardinality grows together with the sample size. Based on the new perspectives\ngained from the asymptotic investigation, we propose a key post-processing\nmodification of the original clustering framework. We show, both theoretically\nand empirically, that the resulting approach can be successfully used to\nestimate the number of clusters in the population. Using simulated data, we\ncompare the proposed method with existing number of clusters and modality\nassessment approaches, and obtain encouraging results. We also demonstrate the\napplicability of our clustering method for the detection of cellular\nsubpopulations in a single-cell virology study. \n\n"}
{"id": "1412.2129", "contents": "Title: An iterative step-function estimator for graphons Abstract: Exchangeable graphs arise via a sampling procedure from measurable functions\nknown as graphons. A natural estimation problem is how well we can recover a\ngraphon given a single graph sampled from it. One general framework for\nestimating a graphon uses step-functions obtained by partitioning the nodes of\nthe graph according to some clustering algorithm. We propose an iterative\nstep-function estimator (ISFE) that, given an initial partition, iteratively\nclusters nodes based on their edge densities with respect to the previous\niteration's partition. We analyze ISFE and demonstrate its performance in\ncomparison with other graphon estimation techniques. \n\n"}
{"id": "1412.2183", "contents": "Title: Reduced-Rank Covariance Estimation in Vector Autoregressive Modeling Abstract: We consider reduced-rank modeling of the white noise covariance matrix in a\nlarge dimensional vector autoregressive (VAR) model. We first propose the\nreduced-rank covariance estimator under the setting where independent\nobservations are available. We derive the reduced-rank estimator based on a\nlatent variable model for the vector observation and give the analytical form\nof its maximum likelihood estimate. Simulation results show that the\nreduced-rank covariance estimator outperforms two competing covariance\nestimators for estimating large dimensional covariance matrices from\nindependent observations. Then we describe how to integrate the proposed\nreduced-rank estimator into the fitting of large dimensional VAR models, where\nwe consider two scenarios that require different model fitting procedures. In\nthe VAR modeling context, our reduced-rank covariance estimator not only\nprovides interpretable descriptions of the dependence structure of VAR\nprocesses but also leads to improvement in model-fitting and forecasting over\nunrestricted covariance estimators. Two real data examples are presented to\nillustrate these fitting procedures. \n\n"}
{"id": "1412.3013", "contents": "Title: Efficient Bayesian inference for stochastic volatility models with\n  ensemble MCMC methods Abstract: In this paper, we introduce efficient ensemble Markov Chain Monte Carlo\n(MCMC) sampling methods for Bayesian computations in the univariate stochastic\nvolatility model. We compare the performance of our ensemble MCMC methods with\nan improved version of a recent sampler of Kastner and Fruwirth-Schnatter\n(2014). We show that ensemble samplers are more efficient than this state of\nthe art sampler by a factor of about 3.1, on a data set simulated from the\nstochastic volatility model. This performance gain is achieved without the\nensemble MCMC sampler relying on the assumption that the latent process is\nlinear and Gaussian, unlike the sampler of Kastner and Fruwirth-Schnatter. \n\n"}
{"id": "1412.3315", "contents": "Title: A generalization of the discrete version of Minkowski's fundamental\n  theorem Abstract: One of the most fruitful results from Minkowski's geometric viewpoint on\nnumber theory is his so called 1st Fundamental Theorem. It provides an optimal\nupper bound for the volume of an o-symmetric convex body whose only interior\nlattice point is the origin. Minkowski also obtained a discrete analog by\nproving optimal upper bounds on the number of lattice points in the boundary of\nsuch convex bodies. Whereas the volume inequality has been generalized to any\nnumber of interior lattice points already by van der Corput in the 1930s, a\ncorresponding result for the discrete case remained to be proven. Our main\ncontribution is a corresponding optimal relation between the number of boundary\nand interior lattice points of an o-symmetric convex body. The proof relies on\na congruence argument and a difference set estimate from additive\ncombinatorics. \n\n"}
{"id": "1412.4128", "contents": "Title: Expanded Alternating Optimization of Nonconvex Functions with\n  Applications to Matrix Factorization and Penalized Regression Abstract: We propose a general technique for improving alternating optimization (AO) of\nnonconvex functions. Starting from the solution given by AO, we conduct another\nsequence of searches over subspaces that are both meaningful to the\noptimization problem at hand and different from those used by AO. To\ndemonstrate the utility of our approach, we apply it to the matrix\nfactorization (MF) algorithm for recommender systems and the coordinate descent\nalgorithm for penalized regression (PR), and show meaningful improvements using\nboth real-world (for MF) and simulated (for PR) data sets. Moreover, we\ndemonstrate for MF that, by constructing search spaces customized to the given\ndata set, we can significantly increase the convergence rate of our technique. \n\n"}
{"id": "1412.5122", "contents": "Title: Fast computation of Tukey trimmed regions and median in dimension $p>2$ Abstract: Given data in $\\mathbb{R}^{p}$, a Tukey $\\kappa$-trimmed region is the set of\nall points that have at least Tukey depth $\\kappa$ w.r.t. the data. As they are\nvisual, affine equivariant and robust, Tukey regions are useful tools in\nnonparametric multivariate analysis. While these regions are easily defined and\ninterpreted, their practical use in applications has been impeded so far by the\nlack of efficient computational procedures in dimension $p > 2$. We construct\ntwo novel algorithms to compute a Tukey $\\kappa$-trimmed region, a na\\\"{i}ve\none and a more sophisticated one that is much faster than known algorithms.\nFurther, a strict bound on the number of facets of a Tukey region is derived.\nIn a large simulation study the novel fast algorithm is compared with the\nna\\\"{i}ve one, which is slower and by construction exact, yielding in every\ncase the same correct results. Finally, the approach is extended to an\nalgorithm that calculates the innermost Tukey region and its barycenter, the\nTukey median. \n\n"}
{"id": "1412.5244", "contents": "Title: Learning unbiased features Abstract: A key element in transfer learning is representation learning; if\nrepresentations can be developed that expose the relevant factors underlying\nthe data, then new tasks and domains can be learned readily based on mappings\nof these salient factors. We propose that an important aim for these\nrepresentations are to be unbiased. Different forms of representation learning\ncan be derived from alternative definitions of unwanted bias, e.g., bias to\nparticular tasks, domains, or irrelevant underlying data dimensions. One very\nuseful approach to estimating the amount of bias in a representation comes from\nmaximum mean discrepancy (MMD) [5], a measure of distance between probability\ndistributions. We are not the first to suggest that MMD can be a useful\ncriterion in developing representations that apply across multiple domains or\ntasks [1]. However, in this paper we describe a number of novel applications of\nthis criterion that we have devised, all based on the idea of developing\nunbiased representations. These formulations include: a standard domain\nadaptation framework; a method of learning invariant representations; an\napproach based on noise-insensitive autoencoders; and a novel form of\ngenerative model. \n\n"}
{"id": "1412.6980", "contents": "Title: Adam: A Method for Stochastic Optimization Abstract: We introduce Adam, an algorithm for first-order gradient-based optimization\nof stochastic objective functions, based on adaptive estimates of lower-order\nmoments. The method is straightforward to implement, is computationally\nefficient, has little memory requirements, is invariant to diagonal rescaling\nof the gradients, and is well suited for problems that are large in terms of\ndata and/or parameters. The method is also appropriate for non-stationary\nobjectives and problems with very noisy and/or sparse gradients. The\nhyper-parameters have intuitive interpretations and typically require little\ntuning. Some connections to related algorithms, on which Adam was inspired, are\ndiscussed. We also analyze the theoretical convergence properties of the\nalgorithm and provide a regret bound on the convergence rate that is comparable\nto the best known results under the online convex optimization framework.\nEmpirical results demonstrate that Adam works well in practice and compares\nfavorably to other stochastic optimization methods. Finally, we discuss AdaMax,\na variant of Adam based on the infinity norm. \n\n"}
{"id": "1412.7210", "contents": "Title: Denoising autoencoder with modulated lateral connections learns\n  invariant representations of natural images Abstract: Suitable lateral connections between encoder and decoder are shown to allow\nhigher layers of a denoising autoencoder (dAE) to focus on invariant\nrepresentations. In regular autoencoders, detailed information needs to be\ncarried through the highest layers but lateral connections from encoder to\ndecoder relieve this pressure. It is shown that abstract invariant features can\nbe translated to detailed reconstructions when invariant features are allowed\nto modulate the strength of the lateral connection. Three dAE structures with\nmodulated and additive lateral connections, and without lateral connections\nwere compared in experiments using real-world images. The experiments verify\nthat adding modulated lateral connections to the model 1) improves the accuracy\nof the probability model for inputs, as measured by denoising performance; 2)\nresults in representations whose degree of invariance grows faster towards the\nhigher layers; and 3) supports the formation of diverse invariant poolings. \n\n"}
{"id": "1412.8200", "contents": "Title: A generalized FKG-inequality for compositions Abstract: We prove a Fortuin-Kasteleyn-Ginibre-type inequality for the lattice of\ncompositions of the integer n with at most r parts. As an immediate application\nwe get a wide generalization of the classical Alexandrov-Fenchel inequality for\nmixed volumes and of Teissier's inequality for mixed covolumes. \n\n"}
{"id": "1412.8283", "contents": "Title: Lines, betweenness and metric spaces Abstract: A classic theorem of Euclidean geometry asserts that any noncollinear set of\n$n$ points in the plane determines at least $n$ distinct lines. Chen and\nChv\\'atal conjectured that this holds for an arbitrary finite metric space,\nwith a certain natural definition of lines in a metric space.\n  We prove that in any metric space with $n$ points, either there is a line\ncontaining all the points or there are at least $\\Omega(\\sqrt{n})$ lines. This\nis the first polynomial lower bound on the number of lines in general finite\nmetric spaces. In the more general setting of pseudometric betweenness, we\nprove a corresponding bound of $\\Omega(n^{2/5})$ lines. When the metric space\nis induced by a connected graph, we prove that either there is a line\ncontaining all the points or there are $\\Omega(n^{4/7})$ lines, improving the\nprevious $\\Omega(n^{2/7})$ bound. We also prove that the number of lines in an\n$n$-point metric space is at least $n / 5w$, where $w$ is the number of\ndifferent distances in the space, and we give an $\\Omega(n^{4/3})$ lower bound\non the number of lines in metric spaces induced by graphs with constant\ndiameter, as well as spaces where all the positive distances are from \\{1, 2,\n3\\}. \n\n"}
{"id": "1501.00426", "contents": "Title: A conical approach to Laurent expansions for multivariate meromorphic\n  germs with linear poles Abstract: We use convex polyhedral cones to study a large class of multivariate\nmeromorphic germs, namely those with linear poles, which naturally arise in\nvarious contexts in mathematics and physics. We express such a germ as a sum of\na holomorphic germ and a linear combination of special non-holomorphic germs\ncalled polar germs. In analyzing the supporting cones -- cones that reflect the\npole structure of the polar germs -- we obtain a geometric criterion for the\nnon-holomorphicity of linear combinations of polar germs. This yields the\nuniqueness of the above sum when required to be supported on a suitable family\nof cones and assigns a Laurent expansion to the germ. Laurent expansions\nprovide various decompositions of such germs and thereby a uniformized proof of\nknown results on decompositions of rational fractions. These Laurent expansions\nalso yield new concepts on the space of such germs, all of which are\nindependent of the choice of the specific Laurent expansion. These include a\ngeneralization of Jeffrey-Kirwan's residue, a filtered residue and a coproduct\nin the space of such germs. When applied to exponential sums on rational convex\npolyhedral cones, the filtered residue yields back exponential integrals. \n\n"}
{"id": "1501.02056", "contents": "Title: Sequential Kernel Herding: Frank-Wolfe Optimization for Particle\n  Filtering Abstract: Recently, the Frank-Wolfe optimization algorithm was suggested as a procedure\nto obtain adaptive quadrature rules for integrals of functions in a reproducing\nkernel Hilbert space (RKHS) with a potentially faster rate of convergence than\nMonte Carlo integration (and \"kernel herding\" was shown to be a special case of\nthis procedure). In this paper, we propose to replace the random sampling step\nin a particle filter by Frank-Wolfe optimization. By optimizing the position of\nthe particles, we can obtain better accuracy than random or quasi-Monte Carlo\nsampling. In applications where the evaluation of the emission probabilities is\nexpensive (such as in robot localization), the additional computational cost to\ngenerate the particles through optimization can be justified. Experiments on\nstandard synthetic examples as well as on a robot localization task indicate\nindeed an improvement of accuracy over random and quasi-Monte Carlo sampling. \n\n"}
{"id": "1501.03796", "contents": "Title: The Fast Convergence of Incremental PCA Abstract: We consider a situation in which we see samples in $\\mathbb{R}^d$ drawn\ni.i.d. from some distribution with mean zero and unknown covariance A. We wish\nto compute the top eigenvector of A in an incremental fashion - with an\nalgorithm that maintains an estimate of the top eigenvector in O(d) space, and\nincrementally adjusts the estimate with each new data point that arrives. Two\nclassical such schemes are due to Krasulina (1969) and Oja (1983). We give\nfinite-sample convergence rates for both. \n\n"}
{"id": "1501.07904", "contents": "Title: Expanders are order diameter non-hyperbolic Abstract: We show that expander graphs must have Gromov-hyperbolicity at least\nproportional to their diameter, with a constant of proportionality depending\nonly on the expansion constant and maximal degree. In other words, expanders\ncontain geodesic triangles which are $\\Omega(\\mathop{\\rm diam} \\Gamma)$-thick. \n\n"}
{"id": "1502.00318", "contents": "Title: Setting the stage for data science: integration of data management\n  skills in introductory and second courses in statistics Abstract: Many have argued that statistics students need additional facility to express\nstatistical computations. By introducing students to commonplace tools for data\nmanagement, visualization, and reproducible analysis in data science and\napplying these to real-world scenarios, we prepare them to think statistically.\nIn an era of increasingly big data, it is imperative that students develop\ndata-related capacities, beginning with the introductory course. We believe\nthat the integration of these precursors to data science into our\ncurricula-early and often-will help statisticians be part of the dialogue\nregarding \"Big Data\" and \"Big Questions\". \n\n"}
{"id": "1502.02347", "contents": "Title: Local and Global Inference for High Dimensional Nonparanormal Graphical\n  Models Abstract: This paper proposes a unified framework to quantify local and global\ninferential uncertainty for high dimensional nonparanormal graphical models. In\nparticular, we consider the problems of testing the presence of a single edge\nand constructing a uniform confidence subgraph. Due to the presence of unknown\nmarginal transformations, we propose a pseudo likelihood based inferential\napproach. In sharp contrast to the existing high dimensional score test method,\nour method is free of tuning parameters given an initial estimator, and extends\nthe scope of the existing likelihood based inferential framework. Furthermore,\nwe propose a U-statistic multiplier bootstrap method to construct the\nconfidence subgraph. We show that the constructed subgraph is contained in the\ntrue graph with probability greater than a given nominal level. Compared with\nexisting methods for constructing confidence subgraphs, our method does not\nrely on Gaussian or sub-Gaussian assumptions. The theoretical properties of the\nproposed inferential methods are verified by thorough numerical experiments and\nreal data analysis. \n\n"}
{"id": "1502.02501", "contents": "Title: A CLT for an improved subspace estimator with observations of increasing\n  dimensions Abstract: This paper deals with subspace estimation in the small sample size regime,\nwhere the number of samples is comparable in magnitude with the observation\ndimension. The traditional estimators, mostly based on the sample correlation\nmatrix, are known to perform well as long as the number of available samples is\nmuch larger than the observation dimension. However, in the small sample size\nregime, the performance degrades. Recently, based on random matrix theory\nresults, a new subspace estimator was introduced, which was shown to be\nconsistent in the asymptotic regime where the number of samples and the\nobservation dimension converge to infinity at the same rate. In practice, this\nestimator outperforms the traditional ones even for certain scenarios where the\nobservation dimension is small and of the same order of magnitude as the number\nof samples. In this paper, we address a performance analysis of this recent\nestimator, by proving a central limit theorem in the above asymptotic regime.\nWe propose an accurate approximation of the mean square error, which can be\nevaluated numerically. \n\n"}
{"id": "1502.03391", "contents": "Title: Fast Embedding for JOFC Using the Raw Stress Criterion Abstract: The Joint Optimization of Fidelity and Commensurability (JOFC) manifold\nmatching methodology embeds an omnibus dissimilarity matrix consisting of\nmultiple dissimilarities on the same set of objects. One approach to this\nembedding optimizes the preservation of fidelity to each individual\ndissimilarity matrix together with commensurability of each given observation\nacross modalities via iterative majorization of a raw stress error criterion by\nsuccessive Guttman transforms. In this paper, we exploit the special structure\ninherent to JOFC to exactly and efficiently compute the successive Guttman\ntransforms, and as a result we are able to greatly speed up the JOFC procedure\nfor both in-sample and out-of-sample embedding. We demonstrate the scalability\nof our implementation on both real and simulated data examples. \n\n"}
{"id": "1502.06069", "contents": "Title: Multilevel ensemble Kalman filtering Abstract: This work embeds a multilevel Monte Carlo sampling strategy into the Monte\nCarlo step of the ensemble Kalman filter (EnKF) in the setting of finite\ndimensional signal evolution and noisy discrete-time observations. The signal\ndynamics is assumed to be governed by a stochastic differential equation (SDE),\nand a hierarchy of time grids is introduced for multilevel numerical\nintegration of that SDE. The resulting multilevel EnKF is proved to\nasymptotically outperform EnKF in terms of computational cost versus\napproximation accuracy. The theoretical results are illustrated numerically. \n\n"}
{"id": "1502.06919", "contents": "Title: Low Rank Matrix Completion with Exponential Family Noise Abstract: The matrix completion problem consists in reconstructing a matrix from a\nsample of entries, possibly observed with noise. A popular class of estimator,\nknown as nuclear norm penalized estimators, are based on minimizing the sum of\na data fitting term and a nuclear norm penalization. Here, we investigate the\ncase where the noise distribution belongs to the exponential family and is\nsub-exponential. Our framework alllows for a general sampling scheme. We first\nconsider an estimator defined as the minimizer of the sum of a log-likelihood\nterm and a nuclear norm penalization and prove an upper bound on the Frobenius\nprediction risk. The rate obtained improves on previous works on matrix\ncompletion for exponential family. When the sampling distribution is known, we\npropose another estimator and prove an oracle inequality w.r.t. the\nKullback-Leibler prediction risk, which translates immediatly into an upper\nbound on the Frobenius prediction risk. Finally, we show that all the rates\nobtained are minimax optimal up to a logarithmic factor. \n\n"}
{"id": "1502.07989", "contents": "Title: Statistical Methods and Computing for Big Data Abstract: Big data are data on a massive scale in terms of volume, intensity, and\ncomplexity that exceed the capacity of standard software tools. They present\nopportunities as well as challenges to statisticians. The role of computational\nstatisticians in scientific discovery from big data analyses has been\nunder-recognized even by peer statisticians. This article reviews recent\nmethodological and software developments in statistics that address the big\ndata challenges. Methodologies are grouped into three classes:\nsubsampling-based, divide and conquer, and sequential updating for stream data.\nSoftware review focuses on the open source R and R packages, covering recent\ntools that help break the barriers of computer memory and computing power. Some\nof the tools are illustrated in a case study with a logistic regression for the\nchance of airline delay. \n\n"}
{"id": "1502.08009", "contents": "Title: Second-order Quantile Methods for Experts and Combinatorial Games Abstract: We aim to design strategies for sequential decision making that adjust to the\ndifficulty of the learning problem. We study this question both in the setting\nof prediction with expert advice, and for more general combinatorial decision\ntasks. We are not satisfied with just guaranteeing minimax regret rates, but we\nwant our algorithms to perform significantly better on easy data. Two popular\nways to formalize such adaptivity are second-order regret bounds and quantile\nbounds. The underlying notions of 'easy data', which may be paraphrased as \"the\nlearning problem has small variance\" and \"multiple decisions are useful\", are\nsynergetic. But even though there are sophisticated algorithms that exploit one\nof the two, no existing algorithm is able to adapt to both.\n  In this paper we outline a new method for obtaining such adaptive algorithms,\nbased on a potential function that aggregates a range of learning rates (which\nare essential tuning parameters). By choosing the right prior we construct\nefficient algorithms and show that they reap both benefits by proving the first\nbounds that are both second-order and incorporate quantiles. \n\n"}
{"id": "1503.00024", "contents": "Title: Influence Maximization with Bandits Abstract: We consider the problem of \\emph{influence maximization}, the problem of\nmaximizing the number of people that become aware of a product by finding the\n`best' set of `seed' users to expose the product to. Most prior work on this\ntopic assumes that we know the probability of each user influencing each other\nuser, or we have data that lets us estimate these influences. However, this\ninformation is typically not initially available or is difficult to obtain. To\navoid this assumption, we adopt a combinatorial multi-armed bandit paradigm\nthat estimates the influence probabilities as we sequentially try different\nseed sets. We establish bounds on the performance of this procedure under the\nexisting edge-level feedback as well as a novel and more realistic node-level\nfeedback. Beyond our theoretical results, we describe a practical\nimplementation and experimentally demonstrate its efficiency and effectiveness\non four real datasets. \n\n"}
{"id": "1503.00038", "contents": "Title: Sequential Feature Explanations for Anomaly Detection Abstract: In many applications, an anomaly detection system presents the most anomalous\ndata instance to a human analyst, who then must determine whether the instance\nis truly of interest (e.g. a threat in a security setting). Unfortunately, most\nanomaly detectors provide no explanation about why an instance was considered\nanomalous, leaving the analyst with no guidance about where to begin the\ninvestigation. To address this issue, we study the problems of computing and\nevaluating sequential feature explanations (SFEs) for anomaly detectors. An SFE\nof an anomaly is a sequence of features, which are presented to the analyst one\nat a time (in order) until the information contained in the highlighted\nfeatures is enough for the analyst to make a confident judgement about the\nanomaly. Since analyst effort is related to the amount of information that they\nconsider in an investigation, an explanation's quality is related to the number\nof features that must be revealed to attain confidence. One of our main\ncontributions is to present a novel framework for large scale quantitative\nevaluations of SFEs, where the quality measure is based on analyst effort. To\ndo this we construct anomaly detection benchmarks from real data sets along\nwith artificial experts that can be simulated for evaluation. Our second\ncontribution is to evaluate several novel explanation approaches within the\nframework and on traditional anomaly detection benchmarks, offering several\ninsights into the approaches. \n\n"}
{"id": "1503.00778", "contents": "Title: Simple, Efficient, and Neural Algorithms for Sparse Coding Abstract: Sparse coding is a basic task in many fields including signal processing,\nneuroscience and machine learning where the goal is to learn a basis that\nenables a sparse representation of a given set of data, if one exists. Its\nstandard formulation is as a non-convex optimization problem which is solved in\npractice by heuristics based on alternating minimization. Re- cent work has\nresulted in several algorithms for sparse coding with provable guarantees, but\nsomewhat surprisingly these are outperformed by the simple alternating\nminimization heuristics. Here we give a general framework for understanding\nalternating minimization which we leverage to analyze existing heuristics and\nto design new ones also with provable guarantees. Some of these algorithms seem\nimplementable on simple neural architectures, which was the original motivation\nof Olshausen and Field (1997a) in introducing sparse coding. We also give the\nfirst efficient algorithm for sparse coding that works almost up to the\ninformation theoretic limit for sparse recovery on incoherent dictionaries. All\nprevious algorithms that approached or surpassed this limit run in time\nexponential in some natural parameter. Finally, our algorithms improve upon the\nsample complexity of existing approaches. We believe that our analysis\nframework will have applications in other settings where simple iterative\nalgorithms are used. \n\n"}
{"id": "1503.00966", "contents": "Title: Sequential Monte Carlo as Approximate Sampling: bounds, adaptive\n  resampling via $\\infty$-ESS, and an application to Particle Gibbs Abstract: Sequential Monte Carlo (SMC) algorithms were originally designed for\nestimating intractable conditional expectations within state-space models, but\nare now routinely used to generate approximate samples in the context of\ngeneral-purpose Bayesian inference. In particular, SMC algorithms are often\nused as subroutines within larger Monte Carlo schemes, and in this context, the\ndemands placed on SMC are different: control of mean-squared error is\ninsufficient---one needs to control the divergence from the target distribution\ndirectly. Towards this goal, we introduce the conditional adaptive resampling\nparticle filter, building on the work of Gordon, Salmond, and Smith (1993),\nAndrieu, Doucet, and Holenstein (2010), and Whiteley, Lee, and Heine (2016). By\ncontrolling a novel notion of effective sample size, the $\\infty$-ESS, we\nestablish the efficiency of the resulting SMC sampling algorithm, providing an\nadaptive resampling extension of the work of Andrieu, Lee, and Vihola (2013).\nWe apply our results to arrive at new divergence bounds for SMC samplers with\nadaptive resampling as well as an adaptive resampling version of the Particle\nGibbs algorithm with the same geometric-ergodicity guarantees as its\nnonadaptive counterpart. \n\n"}
{"id": "1503.01401", "contents": "Title: Quantifying Uncertainty in Stochastic Models with Parametric Variability Abstract: We present a method to quantify uncertainty in the predictions made by\nsimulations of mathematical models that can be applied to a broad class of\nstochastic, discrete, and differential equation models. Quantifying uncertainty\nis crucial for determining how accurate the model predictions are and\nidentifying which input parameters affect the outputs of interest. Most of the\nexisting methods for uncertainty quantification require many samples to\ngenerate accurate results, are unable to differentiate where the uncertainty is\ncoming from (e.g., parameters or model assumptions), or require a lot of\ncomputational resources. Our approach addresses these challenges and\nopportunities by allowing different types of uncertainty, that is, uncertainty\nin input parameters as well as uncertainty created through stochastic model\ncomponents. This is done by combining the Karhunen-Loeve decomposition,\npolynomial chaos expansion, and Bayesian Gaussian process regression to create\na statistical surrogate for the stochastic model. The surrogate separates the\nanalysis of variation arising through stochastic simulation and variation\narising through uncertainty in the model parameterization. We illustrate our\napproach by quantifying the uncertainty in a stochastic ordinary differential\nequation epidemic model. Specifically, we estimate four quantities of interest\nfor the epidemic model and show agreement between the surrogate and the actual\nmodel results. \n\n"}
{"id": "1503.02533", "contents": "Title: A Smoothed Dual Approach for Variational Wasserstein Problems Abstract: Variational problems that involve Wasserstein distances have been recently\nproposed to summarize and learn from probability measures. Despite being\nconceptually simple, such problems are computationally challenging because they\ninvolve minimizing over quantities (Wasserstein distances) that are themselves\nhard to compute. We show that the dual formulation of Wasserstein variational\nproblems introduced recently by Carlier et al. (2014) can be regularized using\nan entropic smoothing, which leads to smooth, differentiable, convex\noptimization problems that are simpler to implement and numerically more\nstable. We illustrate the versatility of this approach by applying it to the\ncomputation of Wasserstein barycenters and gradient flows of spacial\nregularization functionals. \n\n"}
{"id": "1503.04363", "contents": "Title: Fast calculation of boundary crossing probabilities for Poisson\n  processes Abstract: The boundary crossing probability of a Poisson process with $n$ jumps is a\nfundamental quantity with numerous applications. We present a fast $O(n^2 \\log\nn)$ algorithm to calculate this probability for arbitrary upper and lower\nboundaries. \n\n"}
{"id": "1503.06452", "contents": "Title: Unsupervised model compression for multilayer bootstrap networks Abstract: Recently, multilayer bootstrap network (MBN) has demonstrated promising\nperformance in unsupervised dimensionality reduction. It can learn compact\nrepresentations in standard data sets, i.e. MNIST and RCV1. However, as a\nbootstrap method, the prediction complexity of MBN is high. In this paper, we\npropose an unsupervised model compression framework for this general problem of\nunsupervised bootstrap methods. The framework compresses a large unsupervised\nbootstrap model into a small model by taking the bootstrap model and its\napplication together as a black box and learning a mapping function from the\ninput of the bootstrap model to the output of the application by a supervised\nlearner. To specialize the framework, we propose a new technique, named\ncompressive MBN. It takes MBN as the unsupervised bootstrap model and deep\nneural network (DNN) as the supervised learner. Our initial result on MNIST\nshowed that compressive MBN not only maintains the high prediction accuracy of\nMBN but also is over thousands of times faster than MBN at the prediction\nstage. Our result suggests that the new technique integrates the effectiveness\nof MBN on unsupervised learning and the effectiveness and efficiency of DNN on\nsupervised learning together for the effectiveness and efficiency of\ncompressive MBN on unsupervised learning. \n\n"}
{"id": "1503.07259", "contents": "Title: Multilevel Sequential Monte Carlo Samplers Abstract: In this article we consider the approximation of expectations w.r.t.\nprobability distributions associated to the solution of partial differential\nequations (PDEs); this scenario appears routinely in Bayesian inverse problems.\nIn practice, one often has to solve the associated PDE numerically, using, for\ninstance finite element methods and leading to a discretisation bias, with the\nstep-size level $h_L$. In addition, the expectation cannot be computed\nanalytically and one often resorts to Monte Carlo methods. In the context of\nthis problem, it is known that the introduction of the multilevel Monte Carlo\n(MLMC) method can reduce the amount of computational effort to estimate\nexpectations, for a given level of error. This is achieved via a telescoping\nidentity associated to a Monte Carlo approximation of a sequence of probability\ndistributions with discretisation levels $\\infty>h_0>h_1\\cdots>h_L$. In many\npractical problems of interest, one cannot achieve an i.i.d. sampling of the\nassociated sequence of probability distributions. A sequential Monte Carlo\n(SMC) version of the MLMC method is introduced to deal with this problem. It is\nshown that under appropriate assumptions, the attractive property of a\nreduction of the amount of computational effort to estimate expectations, for a\ngiven level of error, can be maintained within the SMC context. \n\n"}
{"id": "1503.08195", "contents": "Title: Of Quantiles and Expectiles: Consistent Scoring Functions, Choquet\n  Representations, and Forecast Rankings Abstract: In the practice of point prediction, it is desirable that forecasters receive\na directive in the form of a statistical functional, such as the mean or a\nquantile of the predictive distribution. When evaluating and comparing\ncompeting forecasts, it is then critical that the scoring function used for\nthese purposes be consistent for the functional at hand, in the sense that the\nexpected score is minimized when following the directive.\n  We show that any scoring function that is consistent for a quantile or an\nexpectile functional, respectively, can be represented as a mixture of extremal\nscoring functions that form a linearly parameterized family. Scoring functions\nfor the mean value and probability forecasts of binary events constitute\nimportant examples. The quantile and expectile functionals along with the\nrespective extremal scoring functions admit appealing economic interpretations\nin terms of thresholds in decision making.\n  The Choquet type mixture representations give rise to simple checks of\nwhether a forecast dominates another in the sense that it is preferable under\nany consistent scoring function. In empirical settings it suffices to compare\nthe average scores for only a finite number of extremal elements. Plots of the\naverage scores with respect to the extremal scoring functions, which we call\nMurphy diagrams, permit detailed comparisons of the relative merits of\ncompeting forecasts. \n\n"}
{"id": "1503.08340", "contents": "Title: Statistical Properties of Convex Clustering Abstract: In this manuscript, we study the statistical properties of convex clustering.\nWe establish that convex clustering is closely related to single linkage\nhierarchical clustering and $k$-means clustering. In addition, we derive the\nrange of tuning parameter for convex clustering that yields a non-trivial\nsolution. We also provide an unbiased estimate of the degrees of freedom, and\nprovide a finite sample bound for the prediction error for convex clustering.\nWe compare convex clustering to some traditional clustering methods in\nsimulation studies. \n\n"}
{"id": "1504.01036", "contents": "Title: Quantum jumps of normal polytopes Abstract: We introduce a partial order on the set of all normal polytopes in R^d. This\nposet NPol(d) is a natural discrete counterpart of the continuum of convex\ncompact sets in R^d, ordered by inclusion, and exhibits a remarkably rich\ncombinatorial structure. We derive various arithmetic bounds on elementary\nrelations in NPol(d), called \"quantum jumps\". The existence of extremal objects\nin NPol(d) is a challenge of number theoretical flavor, leading to interesting\nclasses of normal polytopes: minimal, maximal, spherical. Minimal elements in\nNPol(5) have played a critical role in disproving various covering conjectures\nfor normal polytopes in the 1990s. Here we report on the first examples of\nmaximal elements in NPol(4) and NPol(5), found by a combination of the\ndeveloped theory, random generation, and extensive computer search. \n\n"}
{"id": "1504.04093", "contents": "Title: Extending approximate Bayesian computation methods to high dimensions\n  via a Gaussian copula model Abstract: Approximate Bayesian computation (ABC) refers to a family of inference\nmethods used in the Bayesian analysis of complex models where evaluation of the\nlikelihood is difficult. Conventional ABC methods often suffer from the curse\nof dimensionality, and a marginal adjustment strategy was recently introduced\nin the literature to improve the performance of ABC algorithms in\nhigh-dimensional problems. The marginal adjustment approach is extended using a\nGaussian copula approximation. The method first estimates the bivariate\nposterior for each pair of parameters separately using a 2-dimensional Gaussian\ncopula, and then combines these estimates together to estimate the joint\nposterior. The approximation works well in large sample settings when the\nposterior is approximately normal, but also works well in many cases which are\nfar from that situation due to the nonparametric estimation of the marginal\nposterior distributions. If each bivariate posterior distribution can be well\nestimated with a low-dimensional ABC analysis then this Gaussian copula method\ncan extend ABC methods to problems of high dimension. The method also results\nin an analytic expression for the approximate posterior which is useful for\nmany purposes such as approximation of the likelihood itself. This method is\nillustrated with several examples. \n\n"}
{"id": "1504.05229", "contents": "Title: Poisson Matrix Recovery and Completion Abstract: We extend the theory of low-rank matrix recovery and completion to the case\nwhen Poisson observations for a linear combination or a subset of the entries\nof a matrix are available, which arises in various applications with count\ndata. We consider the usual matrix recovery formulation through maximum\nlikelihood with proper constraints on the matrix $M$ of size $d_1$-by-$d_2$,\nand establish theoretical upper and lower bounds on the recovery error. Our\nbounds for matrix completion are nearly optimal up to a factor on the order of\n$\\mathcal{O}(\\log(d_1 d_2))$. These bounds are obtained by combing techniques\nfor compressed sensing for sparse vectors with Poisson noise and for analyzing\nlow-rank matrices, as well as adapting the arguments used for one-bit matrix\ncompletion \\cite{davenport20121} (although these two problems are different in\nnature) and the adaptation requires new techniques exploiting properties of the\nPoisson likelihood function and tackling the difficulties posed by the locally\nsub-Gaussian characteristic of the Poisson distribution. Our results highlight\na few important distinctions of the Poisson case compared to the prior work\nincluding having to impose a minimum signal-to-noise requirement on each\nobserved entry and a gap in the upper and lower bounds. We also develop a set\nof efficient iterative algorithms and demonstrate their good performance on\nsynthetic examples and real data. \n\n"}
{"id": "1504.05665", "contents": "Title: Rebuilding Factorized Information Criterion: Asymptotically Accurate\n  Marginal Likelihood Abstract: Factorized information criterion (FIC) is a recently developed approximation\ntechnique for the marginal log-likelihood, which provides an automatic model\nselection framework for a few latent variable models (LVMs) with tractable\ninference algorithms. This paper reconsiders FIC and fills theoretical gaps of\nprevious FIC studies. First, we reveal the core idea of FIC that allows\ngeneralization for a broader class of LVMs, including continuous LVMs, in\ncontrast to previous FICs, which are applicable only to binary LVMs. Second, we\ninvestigate the model selection mechanism of the generalized FIC. Our analysis\nprovides a formal justification of FIC as a model selection criterion for LVMs\nand also a systematic procedure for pruning redundant latent variables that\nhave been removed heuristically in previous studies. Third, we provide an\ninterpretation of FIC as a variational free energy and uncover a few\npreviously-unknown their relationships. A demonstrative study on Bayesian\nprincipal component analysis is provided and numerical experiments support our\ntheoretical results. \n\n"}
{"id": "1504.07245", "contents": "Title: Approximate Bayesian Computation for Forward Modeling in Cosmology Abstract: Bayesian inference is often used in cosmology and astrophysics to derive\nconstraints on model parameters from observations. This approach relies on the\nability to compute the likelihood of the data given a choice of model\nparameters. In many practical situations, the likelihood function may however\nbe unavailable or intractable due to non-gaussian errors, non-linear\nmeasurements processes, or complex data formats such as catalogs and maps. In\nthese cases, the simulation of mock data sets can often be made through forward\nmodeling. We discuss how Approximate Bayesian Computation (ABC) can be used in\nthese cases to derive an approximation to the posterior constraints using\nsimulated data sets. This technique relies on the sampling of the parameter\nset, a distance metric to quantify the difference between the observation and\nthe simulations and summary statistics to compress the information in the data.\nWe first review the principles of ABC and discuss its implementation using a\nPopulation Monte-Carlo (PMC) algorithm and the Mahalanobis distance metric. We\ntest the performance of the implementation using a Gaussian toy model. We then\napply the ABC technique to the practical case of the calibration of image\nsimulations for wide field cosmological surveys. We find that the ABC analysis\nis able to provide reliable parameter constraints for this problem and is\ntherefore a promising technique for other applications in cosmology and\nastrophysics. Our implementation of the ABC PMC method is made available via a\npublic code release. \n\n"}
{"id": "1504.07389", "contents": "Title: Building Classifiers to Predict the Start of Glucose-Lowering\n  Pharmacotherapy Using Belgian Health Expenditure Data Abstract: Early diagnosis is important for type 2 diabetes (T2D) to improve patient\nprognosis, prevent complications and reduce long-term treatment costs. We\npresent a novel risk profiling approach based exclusively on health expenditure\ndata that is available to Belgian mutual health insurers. We used expenditure\ndata related to drug purchases and medical provisions to construct models that\npredict whether a patient will start glucose-lowering pharmacotherapy in the\ncoming years, based on that patient's recent medical expenditure history. The\ndesign and implementation of the modeling strategy are discussed in detail and\nseveral learning methods are benchmarked for our application. Our best\nperforming model obtains between 74.9% and 76.8% area under the ROC curve,\nwhich is comparable to state-of-the-art risk prediction approaches for T2D\nbased on questionnaires. In contrast to other methods, our approach can be\nimplemented on a population-wide scale at virtually no extra operational cost.\nPossibly, our approach can be further improved by additional information about\nsome risk factors of T2D that is unavailable in health expenditure data. \n\n"}
{"id": "1505.01164", "contents": "Title: Achieving a Hyperlocal Housing Price Index: Overcoming Data Sparsity by\n  Bayesian Dynamical Modeling of Multiple Data Streams Abstract: Understanding how housing values evolve over time is important to policy\nmakers, consumers and real estate professionals. Existing methods for\nconstructing housing indices are computed at a coarse spatial granularity, such\nas metropolitan regions, which can mask or distort price dynamics apparent in\nlocal markets, such as neighborhoods and census tracts. A challenge in moving\nto estimates at, for example, the census tract level is the sparsity of\nspatiotemporally localized house sales observations. Our work aims at\naddressing this challenge by leveraging observations from multiple census\ntracts discovered to have correlated valuation dynamics. Our proposed Bayesian\nnonparametric approach builds on the framework of latent factor models to\nenable a flexible, data-driven method for inferring the clustering of\ncorrelated census tracts. We explore methods for scalability and\nparallelizability of computations, yielding a housing valuation index at the\nlevel of census tract rather than zip code, and on a monthly basis rather than\nquarterly. Our analysis is provided on a large Seattle metropolitan housing\ndataset. \n\n"}
{"id": "1505.02023", "contents": "Title: Tests for separability in nonparametric covariance operators of random\n  surfaces Abstract: The assumption of separability of the covariance operator for a random image\nor hypersurface can be of substantial use in applications, especially in\nsituations where the accurate estimation of the full covariance structure is\nunfeasible, either for computational reasons, or due to a small sample size.\nHowever, inferential tools to verify this assumption are somewhat lacking in\nhigh-dimensional or functional {data analysis} settings, where this assumption\nis most relevant. We propose here to test separability by focusing on\n$K$-dimensional projections of the difference between the covariance operator\nand a nonparametric separable approximation. The subspace we project onto is\none generated by the eigenfunctions of the covariance operator estimated under\nthe separability hypothesis, negating the need to ever estimate the full\nnon-separable covariance. We show that the rescaled difference of the sample\ncovariance operator with its separable approximation is asymptotically\nGaussian. As a by-product of this result, we derive asymptotically pivotal\ntests under Gaussian assumptions, and propose bootstrap methods for\napproximating the distribution of the test statistics. We probe the finite\nsample performance through simulations studies, and present an application to\nlog-spectrogram images from a phonetic linguistics dataset. \n\n"}
{"id": "1505.02514", "contents": "Title: Orthogonal colorings of the sphere Abstract: An orthogonal coloring of the two-dimensional unit sphere $\\mathbb{S}^2$, is\na partition of $\\mathbb{S}^2$ into parts such that no part contains a pair of\northogonal points, that is, a pair of points at spherical distance $\\pi/2$\napart. It is a well-known result that an orthogonal coloring of $\\mathbb{S}^2$\nrequires at least four parts, and orthogonal colorings with exactly four parts\ncan easily be constructed from a regular octahedron centered at the origin. An\nintriguing question is whether or not every orthogonal 4-coloring of\n$\\mathbb{S}^2$ is such an octahedral coloring. In this paper we address this\nquestion and show that if every color class has a non-empty interior, then the\ncoloring is octahedral. Some related results are also given. \n\n"}
{"id": "1505.02556", "contents": "Title: Extending Bayesian analysis of circular data to comparison of multiple\n  groups Abstract: Circular data are data measured in angles and occur in a variety of\nscientific disciplines. Bayesian methods promise to allow for flexible analysis\nof circular data. Three existing MCMC methods (Gibbs, Metropolis-Hastings, and\nRejection) for a single group of circular data were extended to be used in a\nbetween-subjects design, providing a novel procedure to compare groups of\ncircular data. Investigating the performance of the methods by simulation\nstudy, all methods were found to overestimate the concentration parameter of\nthe posterior, while coverage was reasonable. The rejection sampler performed\nbest. In future research, the MCMC method may be extended to include\ncovariates, or a within-subjects design. \n\n"}
{"id": "1505.04898", "contents": "Title: Heterogeneous Change Point Inference Abstract: We propose HSMUCE (heterogeneous simultaneous multiscale change-point\nestimator) for the detection of multiple change-points of the signal in a\nheterogeneous gaussian regression model. A piecewise constant function is\nestimated by minimizing the number of change-points over the acceptance region\nof a multiscale test which locally adapts to changes in the variance. The\nmultiscale test is a combination of local likelihood ratio tests which are\nproperly calibrated by scale dependent critical values in order to keep a\nglobal nominal level alpha, even for finite samples. We show that HSMUCE\ncontrols the error of over- and underestimation of the number of change-points.\nTo this end, new deviation bounds for F-type statistics are derived. Moreover,\nwe obtain confidence sets for the whole signal. All results are non-asymptotic\nand uniform over a large class of heterogeneous change-point models. HSMUCE is\nfast to compute, achieves the optimal detection rate and estimates the number\nof change-points at almost optimal accuracy for vanishing signals, while still\nbeing robust. We compare HSMUCE with several state of the art methods in\nsimulations and analyse current recordings of a transmembrane protein in the\nbacterial outer membrane with pronounced heterogeneity for its states. An\nR-package is available online. \n\n"}
{"id": "1505.06145", "contents": "Title: On minimum spanning tree-like metric spaces Abstract: We attempt to shed new light on the notion of 'tree-like' metric spaces by\nfocusing on an approach that does not use the four-point condition. Our key\nquestion is: Given metric space $M$ on $n$ points, when does a fully labelled\npositive-weighted tree $T$ exist on the same $n$ vertices that precisely\nrealises $M$ using its shortest path metric? We prove that if a spanning tree\nrepresentation, $T$, of $M$ exists, then it is isomorphic to the unique minimum\nspanning tree in the weighted complete graph associated with $M$, and we\nintroduce a fourth-point condition that is necessary and sufficient to ensure\nthe existence of $T$ whenever each distance in $M$ is unique. In other words, a\nfinite median graph, in which each geodesic distance is distinct, is simply a\ntree. Provided that the tie-breaking assumption holds, the fourth-point\ncondition serves as a criterion for measuring the goodness-of-fit of the\nminimum spanning tree to $M$, i.e., the spanning tree-likeness of $M$. It is\nalso possible to evaluate the spanning path-likeness of $M$. These quantities\ncan be measured in $O(n^4)$ and $O(n^3)$ time, respectively. \n\n"}
{"id": "1506.00053", "contents": "Title: Efficient Bayesian experimentation using an expected information gain\n  lower bound Abstract: Experimental design is crucial for inference where limitations in the data\ncollection procedure are present due to cost or other restrictions. Optimal\nexperimental designs determine parameters that in some appropriate sense make\nthe data the most informative possible. In a Bayesian setting this is\ntranslated to updating to the best possible posterior. Information theoretic\narguments have led to the formation of the expected information gain as a\ndesign criterion. This can be evaluated mainly by Monte Carlo sampling and\nmaximized by using stochastic approximation methods, both known for being\ncomputationally expensive tasks. We propose a framework where a lower bound of\nthe expected information gain is used as an alternative design criterion. In\naddition to alleviating the computational burden, this also addresses issues\nconcerning estimation bias. The problem of permeability inference in a large\ncontaminated area is used to demonstrate the validity of our approach where we\nemploy the massively parallel version of the multiphase multicomponent\nsimulator TOUGH2 to simulate contaminant transport and a Polynomial Chaos\napproximation of the forward model that further accelerates the objective\nfunction evaluations. The proposed methodology is demonstrated to a setting\nwhere field measurements are available. \n\n"}
{"id": "1506.00138", "contents": "Title: Efficient Computation of Gaussian Likelihoods for Stationary Markov\n  Random Field Models Abstract: Rue and Held (2005) proposed a method for efficiently computing the Gaussian\nlikelihood for stationary Markov random field models, when the data locations\nfall on a complete regular grid, and the model has no additive error term. The\ncalculations rely on the availability of the covariances. We prove a theorem\ngiving the rate of convergence of a spectral method of computing the\ncovariances, establishing that the error decays faster than any polynomial in\nthe size of the computing grid. We extend the exact likelihood calculations to\nthe case of non-rectangular domains and missing values on the interior of the\ngrid and to the case when an additive uncorrelated error term (nugget) is\npresent in the model. We also give an alternative formulation of the likelihood\nthat has a smaller memory burden, parts of which can be computed in parallel.\nWe show in simulations that using the exact likelihood can give far better\nparameter estimates than using standard Markov random field approximations.\nHaving access to the exact likelihood allows for model comparisons via\nlikelihood ratios on large datasets, so as an application of the methods, we\ncompare several state-of-the-art methods for large spatial datasets on an\naerosol optical thickness dataset. We find that simple block independent\nlikelihood and composite likelihood methods outperform stochastic partial\ndifferential equation approximations in terms of computation time and returning\nparameter estimates that nearly maximize the likelihood. \n\n"}
{"id": "1506.00886", "contents": "Title: Nilprogressions and groups with moderate growth Abstract: We show that doubling at some large scale in a Cayley graph implies uniform\ndoubling at all subsequent scales. The proof is based on the structure theorem\nfor approximate subgroups proved by Green, Tao and the first author. We also\ngive a number of applications to the geometry and spectrum of finite Cayley\ngraphs. For example, we show that a finite group has moderate growth in the\nsense of Diaconis and Saloff-Coste if and only if its diameter is larger than a\nfixed power of the cardinality of the group. We call such groups almost flat\nand show that they have a subgroup of bounded index admitting a cyclic quotient\nof comparable diameter. We also give bounds on the Cheeger constant, first\neigenvalue of the Laplacian, and mixing time. This can be seen as a\nfinite-group version of Gromov's theorem on groups with polynomial growth. It\nalso improves on a result of Lackenby regarding property (tau) in towers of\ncoverings. Another consequence is a universal upper bound on the diameter of\nall finite simple groups, independent of the CFSG. \n\n"}
{"id": "1506.03481", "contents": "Title: On the Asymptotic Efficiency of Approximate Bayesian Computation\n  Estimators Abstract: Many statistical applications involve models for which it is difficult to\nevaluate the likelihood, but from which it is relatively easy to sample.\nApproximate Bayesian computation is a likelihood-free method for implementing\nBayesian inference in such cases. We present results on the asymptotic variance\nof estimators obtained using approximate Bayesian computation in a large-data\nlimit. Our key assumption is that the data are summarized by a\nfixed-dimensional summary statistic that obeys a central limit theorem. We\nprove asymptotic normality of the mean of the approximate Bayesian computation\nposterior. This result also shows that, in terms of asymptotic variance, we\nshould use a summary statistic that is the same dimension as the parameter\nvector, p; and that any summary statistic of higher dimension can be reduced,\nthrough a linear transformation, to dimension p in a way that can only reduce\nthe asymptotic variance of the posterior mean. We look at how the Monte Carlo\nerror of an importance sampling algorithm that samples from the approximate\nBayesian computation posterior affects the accuracy of estimators. We give\nconditions on the importance sampling proposal distribution such that the\nvariance of the estimator will be the same order as that of the maximum\nlikelihood estimator based on the summary statistics used. This suggests an\niterative importance sampling algorithm, which we evaluate empirically on a\nstochastic volatility model. \n\n"}
{"id": "1506.04915", "contents": "Title: Bayesian nonparametric inference for discovery probabilities: credible\n  intervals and large sample asymptotics Abstract: Given a sample of size $n$ from a population of individuals belonging to\ndifferent species with unknown proportions, a popular problem of practical\ninterest consists in making inference on the probability $D_{n}(l)$ that the\n$(n+1)$-th draw coincides with a species with frequency $l$ in the sample, for\nany $l=0,1,\\ldots,n$. This paper contributes to the methodology of Bayesian\nnonparametric inference for $D_{n}(l)$. Specifically, under the general\nframework of Gibbs-type priors we show how to derive credible intervals for a\nBayesian nonparametric estimation of $D_{n}(l)$, and we investigate the large\n$n$ asymptotic behaviour of such an estimator. Of particular interest are\nspecial cases of our results obtained under the specification of the two\nparameter Poisson--Dirichlet prior and the normalized generalized Gamma prior,\nwhich are two of the most commonly used Gibbs-type priors. With respect to\nthese two prior specifications, the proposed results are illustrated through a\nsimulation study and a benchmark Expressed Sequence Tags dataset. To the best\nour knowledge, this illustration provides the first comparative study between\nthe two parameter Poisson--Dirichlet prior and the normalized generalized Gamma\nprior in the context of Bayesian nonparemetric inference for $D_{n}(l)$. \n\n"}
{"id": "1506.05741", "contents": "Title: Accelerated dimension-independent adaptive Metropolis Abstract: This work considers black-box Bayesian inference over high-dimensional\nparameter spaces. The well-known adaptive Metropolis (AM) algorithm of (Haario\netal. 2001) is extended herein to scale asymptotically uniformly with respect\nto the underlying parameter dimension for Gaussian targets, by respecting the\nvariance of the target. The resulting algorithm, referred to as the\ndimension-independent adaptive Metropolis (DIAM) algorithm, also shows improved\nperformance with respect to adaptive Metropolis on non-Gaussian targets. This\nalgorithm is further improved, and the possibility of probing high-dimensional\ntargets is enabled, via GPU-accelerated numerical libraries and periodically\nsynchronized concurrent chains (justified a posteriori). Asymptotically in\ndimension, this GPU implementation exhibits a factor of four improvement versus\na competitive CPU-based Intel MKL parallel version alone. Strong scaling to\nconcurrent chains is exhibited, through a combination of longer time per sample\nbatch (weak scaling) and yet fewer necessary samples to convergence. The\nalgorithm performance is illustrated on several Gaussian and non-Gaussian\ntarget examples, in which the dimension may be in excess of one thousand. \n\n"}
{"id": "1506.07405", "contents": "Title: Global Convergence of a Grassmannian Gradient Descent Algorithm for\n  Subspace Estimation Abstract: It has been observed in a variety of contexts that gradient descent methods\nhave great success in solving low-rank matrix factorization problems, despite\nthe relevant problem formulation being non-convex. We tackle a particular\ninstance of this scenario, where we seek the $d$-dimensional subspace spanned\nby a streaming data matrix. We apply the natural first order incremental\ngradient descent method, constraining the gradient method to the Grassmannian.\nIn this paper, we propose an adaptive step size scheme that is greedy for the\nnoiseless case, that maximizes the improvement of our metric of convergence at\neach data index $t$, and yields an expected improvement for the noisy case. We\nshow that, with noise-free data, this method converges from any random\ninitialization to the global minimum of the problem. For noisy data, we provide\nthe expected convergence rate of the proposed algorithm per iteration. \n\n"}
{"id": "1506.07540", "contents": "Title: Global Optimality in Tensor Factorization, Deep Learning, and Beyond Abstract: Techniques involving factorization are found in a wide range of applications\nand have enjoyed significant empirical success in many fields. However, common\nto a vast majority of these problems is the significant disadvantage that the\nassociated optimization problems are typically non-convex due to a multilinear\nform or other convexity destroying transformation. Here we build on ideas from\nconvex relaxations of matrix factorizations and present a very general\nframework which allows for the analysis of a wide range of non-convex\nfactorization problems - including matrix factorization, tensor factorization,\nand deep neural network training formulations. We derive sufficient conditions\nto guarantee that a local minimum of the non-convex optimization problem is a\nglobal minimum and show that if the size of the factorized variables is large\nenough then from any initialization it is possible to find a global minimizer\nusing a purely local descent algorithm. Our framework also provides a partial\ntheoretical justification for the increasingly common use of Rectified Linear\nUnits (ReLUs) in deep neural networks and offers guidance on deep network\narchitectures and regularization strategies to facilitate efficient\noptimization. \n\n"}
{"id": "1507.00803", "contents": "Title: Model-assisted design of experiments in the presence of network\n  correlated outcomes Abstract: We consider the problem of how to assign treatment in a randomized\nexperiment, in which the correlation among the outcomes is informed by a\nnetwork available pre-intervention. Working within the potential outcome causal\nframework, we develop a class of models that posit such a correlation structure\namong the outcomes. Then we leverage these models to develop restricted\nrandomization strategies for allocating treatment optimally, by minimizing the\nmean square error of the estimated average treatment effect. Analytical\ndecompositions of the mean square error, due both to the model and to the\nrandomization distribution, provide insights into aspects of the optimal\ndesigns. In particular, the analysis suggests new notions of balance based on\nspecific network quantities, in addition to classical covariate balance. The\nresulting balanced, optimal restricted randomization strategies are still\ndesign unbiased, in situations where the model used to derive them does not\nhold. We illustrate how the proposed treatment allocation strategies improve on\nallocations that ignore the network structure, with extensive simulations. \n\n"}
{"id": "1507.00919", "contents": "Title: Rare Event Simulation and Splitting for Discontinuous Random Variables Abstract: Multilevel Splitting methods, also called Sequential Monte-Carlo or\n\\emph{Subset Simulation}, are widely used methods for estimating extreme\nprobabilities of the form $P[S(\\mathbf{U}) > q]$ where $S$ is a deterministic\nreal-valued function and $\\mathbf{U}$ can be a random finite- or\ninfinite-dimensional vector. Very often, $X := S(\\mathbf{U})$ is supposed to be\na continuous random variable and a lot of theoretical results on the\nstatistical behaviour of the estimator are now derived with this hypothesis.\nHowever, as soon as some threshold effect appears in $S$ and/or $\\mathbf{U}$ is\ndiscrete or mixed discrete/continuous this assumption does not hold any more\nand the estimator is not consistent.\n  In this paper, we study the impact of discontinuities in the \\emph{cdf} of\n$X$ and present three unbiased \\emph{corrected} estimators to handle them.\nThese estimators do not require to know in advance if $X$ is actually\ndiscontinuous or not and become all equal if $X$ is continuous. Especially, one\nof them has the same statistical properties in any case. Efficiency is shown on\na 2-D diffusive process as well as on the \\emph{Boolean SATisfiability problem}\n(SAT). \n\n"}
{"id": "1507.00996", "contents": "Title: A New Approach to Probabilistic Programming Inference Abstract: We introduce and demonstrate a new approach to inference in expressive\nprobabilistic programming languages based on particle Markov chain Monte Carlo.\nOur approach is simple to implement and easy to parallelize. It applies to\nTuring-complete probabilistic programming languages and supports accurate\ninference in models that make use of complex control flow, including stochastic\nrecursion. It also includes primitives from Bayesian nonparametric statistics.\nOur experiments show that this approach can be more efficient than previously\nintroduced single-site Metropolis-Hastings methods. \n\n"}
{"id": "1507.04398", "contents": "Title: On the use of reproducing kernel Hilbert spaces in functional\n  classification Abstract: The H\\'ajek-Feldman dichotomy establishes that two Gaussian measures are\neither mutually absolutely continuous with respect to each other (and hence\nthere is a Radon-Nikodym density for each measure with respect to the other\none) or mutually singular. Unlike the case of finite dimensional Gaussian\nmeasures, there are non-trivial examples of both situations when dealing with\nGaussian stochastic processes. This paper provides:\n  (a) Explicit expressions for the optimal (Bayes) rule and the minimal\nclassification error probability in several relevant problems of supervised\nbinary classification of mutually absolutely continuous Gaussian processes. The\napproach relies on some classical results in the theory of Reproducing Kernel\nHilbert Spaces (RKHS).\n  (b) An interpretation, in terms of mutual singularity, for the \"near perfect\nclassification\" phenomenon described by Delaigle and Hall (2012). We show that\nthe asymptotically optimal rule proposed by these authors can be identified\nwith the sequence of optimal rules for an approximating sequence of\nclassification problems in the absolutely continuous case.\n  (c) A new model-based method for variable selection in binary classification\nproblems, which arises in a very natural way from the explicit knowledge of the\nRN-derivatives and the underlying RKHS structure. Different classifiers might\nbe used from the selected variables. In particular, the classical, linear\nfinite-dimensional Fisher rule turns out to be consistent under some standard\nconditions on the underlying functional model. \n\n"}
{"id": "1507.05367", "contents": "Title: Structured Sparsity: Discrete and Convex approaches Abstract: Compressive sensing (CS) exploits sparsity to recover sparse or compressible\nsignals from dimensionality reducing, non-adaptive sensing mechanisms. Sparsity\nis also used to enhance interpretability in machine learning and statistics\napplications: While the ambient dimension is vast in modern data analysis\nproblems, the relevant information therein typically resides in a much lower\ndimensional space. However, many solutions proposed nowadays do not leverage\nthe true underlying structure. Recent results in CS extend the simple sparsity\nidea to more sophisticated {\\em structured} sparsity models, which describe the\ninterdependency between the nonzero components of a signal, allowing to\nincrease the interpretability of the results and lead to better recovery\nperformance. In order to better understand the impact of structured sparsity,\nin this chapter we analyze the connections between the discrete models and\ntheir convex relaxations, highlighting their relative advantages. We start with\nthe general group sparse model and then elaborate on two important special\ncases: the dispersive and the hierarchical models. For each, we present the\nmodels in their discrete nature, discuss how to solve the ensuing discrete\nproblems and then describe convex relaxations. We also consider more general\nstructures as defined by set functions and present their convex proxies.\nFurther, we discuss efficient optimization solutions for structured sparsity\nproblems and illustrate structured sparsity in action via three applications. \n\n"}
{"id": "1507.07024", "contents": "Title: A multiscale strategy for Bayesian inference using transport maps Abstract: In many inverse problems, model parameters cannot be precisely determined\nfrom observational data. Bayesian inference provides a mechanism for capturing\nthe resulting parameter uncertainty, but typically at a high computational\ncost. This work introduces a multiscale decomposition that exploits conditional\nindependence across scales, when present in certain classes of inverse\nproblems, to decouple Bayesian inference into two stages: (1) a computationally\ntractable coarse-scale inference problem; and (2) a mapping of the\nlow-dimensional coarse-scale posterior distribution into the original\nhigh-dimensional parameter space. This decomposition relies on a\ncharacterization of the non-Gaussian joint distribution of coarse- and\nfine-scale quantities via optimal transport maps. We demonstrate our approach\non a sequence of inverse problems arising in subsurface flow, using the\nmultiscale finite element method to discretize the steady state pressure\nequation. We compare the multiscale strategy with full-dimensional Markov chain\nMonte Carlo on a problem of moderate dimension (100 parameters) and then use it\nto infer a conductivity field described by over 10,000 parameters. \n\n"}
{"id": "1507.08396", "contents": "Title: Tag-Weighted Topic Model For Large-scale Semi-Structured Documents Abstract: To date, there have been massive Semi-Structured Documents (SSDs) during the\nevolution of the Internet. These SSDs contain both unstructured features (e.g.,\nplain text) and metadata (e.g., tags). Most previous works focused on modeling\nthe unstructured text, and recently, some other methods have been proposed to\nmodel the unstructured text with specific tags. To build a general model for\nSSDs remains an important problem in terms of both model fitness and\nefficiency. We propose a novel method to model the SSDs by a so-called\nTag-Weighted Topic Model (TWTM). TWTM is a framework that leverages both the\ntags and words information, not only to learn the document-topic and topic-word\ndistributions, but also to infer the tag-topic distributions for text mining\ntasks. We present an efficient variational inference method with an EM\nalgorithm for estimating the model parameters. Meanwhile, we propose three\nlarge-scale solutions for our model under the MapReduce distributed computing\nplatform for modeling large-scale SSDs. The experimental results show the\neffectiveness, efficiency and the robustness by comparing our model with the\nstate-of-the-art methods in document modeling, tags prediction and text\nclassification. We also show the performance of the three distributed solutions\nin terms of time and accuracy on document modeling. \n\n"}
{"id": "1508.00459", "contents": "Title: Unsupervised Learning in Genome Informatics Abstract: With different genomes available, unsupervised learning algorithms are\nessential in learning genome-wide biological insights. Especially, the\nfunctional characterization of different genomes is essential for us to\nunderstand lives. In this book chapter, we review the state-of-the-art\nunsupervised learning algorithms for genome informatics from DNA to MicroRNA.\n  DNA (DeoxyriboNucleic Acid) is the basic component of genomes. A significant\nfraction of DNA regions (transcription factor binding sites) are bound by\nproteins (transcription factors) to regulate gene expression at different\ndevelopment stages in different tissues. To fully understand genetics, it is\nnecessary of us to apply unsupervised learning algorithms to learn and infer\nthose DNA regions. Here we review several unsupervised learning methods for\ndeciphering the genome-wide patterns of those DNA regions.\n  MicroRNA (miRNA), a class of small endogenous non-coding RNA (RiboNucleic\nacid) species, regulate gene expression post-transcriptionally by forming\nimperfect base-pair with the target sites primarily at the 3$'$ untranslated\nregions of the messenger RNAs. Since the 1993 discovery of the first miRNA\n\\emph{let-7} in worms, a vast amount of studies have been dedicated to\nfunctionally characterizing the functional impacts of miRNA in a network\ncontext to understand complex diseases such as cancer. Here we review several\nrepresentative unsupervised learning frameworks on inferring miRNA regulatory\nnetwork by exploiting the static sequence-based information pertinent to the\nprior knowledge of miRNA targeting and the dynamic information of miRNA\nactivities implicated by the recently available large data compendia, which\ninterrogate genome-wide expression profiles of miRNAs and/or mRNAs across\nvarious cell conditions. \n\n"}
{"id": "1508.01126", "contents": "Title: A subsampled double bootstrap for massive data Abstract: The bootstrap is a popular and powerful method for assessing precision of\nestimators and inferential methods. However, for massive datasets which are\nincreasingly prevalent, the bootstrap becomes prohibitively costly in\ncomputation and its feasibility is questionable even with modern parallel\ncomputing platforms. Recently Kleiner, Talwalkar, Sarkar, and Jordan (2014)\nproposed a method called BLB (Bag of Little Bootstraps) for massive data which\nis more computationally scalable with little sacrifice of statistical accuracy.\nBuilding on BLB and the idea of fast double bootstrap, we propose a new\nresampling method, the subsampled double bootstrap, for both independent data\nand time series data. We establish consistency of the subsampled double\nbootstrap under mild conditions for both independent and dependent cases.\nMethodologically, the subsampled double bootstrap is superior to BLB in terms\nof running time, more sample coverage and automatic implementation with less\ntuning parameters for a given time budget. Its advantage relative to BLB and\nbootstrap is also demonstrated in numerical simulations and a data\nillustration. \n\n"}
{"id": "1508.04384", "contents": "Title: Experimental observation of controllable kinetic constraints in a cold\n  atomic gas Abstract: Many-body systems relaxing to equilibrium can exhibit complex dynamics even\nif their steady state is trivial. At low temperatures or high densities their\nevolution is often dominated by steric hindrances affecting particle motion\n[1,2,3]. Local rearrangements are highly constrained, giving rise to collective\n- and often slow - relaxation.This dynamics can be difficult to analyse from\nfirst principles, but the essential physical ingredients are captured by\nidealized lattice models with so- called kinetic constraints [4]. Here we\nexperimentally realize a many-body system exhibiting manifest kinetic\nconstraints and measure its dynamical properties. In the cold Rydberg gas used\nin our experiments, the nature of the constraints can be tailored through the\ndetuning of the excitation lasers from resonance [5,6,7,8], which controls\nwhether the system undergoes correlated or anti- correlated dynamics. Our\nresults confirm recent theoretical predictions [5,6], and highlight the analogy\nbetween the dynamics of interacting Rydberg gases and that of soft-matter\nsystems. \n\n"}
{"id": "1508.04521", "contents": "Title: Simulated Tempering and Swapping on Mean-Field Models Abstract: Simulated and parallel tempering are families of Markov Chain Monte Carlo\nalgorithms where a temperature parameter is varied during the simulation to\novercome bottlenecks to convergence due to multimodality.\n  In this work we introduce and analyze the convergence for a set of new\ntempering distributions which we call \\textit{entropy dampening}. For\nasymmetric exponential distributions and the mean field Ising model with and\nexternal field simulated tempering is known to converge slowly. We show that\ntempering with entropy dampening distributions mixes in polynomial time for\nthese models.\n  Examining slow mixing times of tempering more closely, we show that for the\nmean-field 3-state ferromagnetic Potts model, tempering converges slowly\nregardless of the temperature schedule chosen. On the other hand, tempering\nwith entropy dampening distributions converges in polynomial time to\nstationarity. Finally we show that the slow mixing can be very expensive\npractically. In particular, the mixing time of simulated tempering is an\nexponential factor longer than the mixing time at the fixed temperature. \n\n"}
{"id": "1508.05170", "contents": "Title: Adaptive Online Learning Abstract: We propose a general framework for studying adaptive regret bounds in the\nonline learning framework, including model selection bounds and data-dependent\nbounds. Given a data- or model-dependent bound we ask, \"Does there exist some\nalgorithm achieving this bound?\" We show that modifications to recently\nintroduced sequential complexity measures can be used to answer this question\nby providing sufficient conditions under which adaptive rates can be achieved.\nIn particular each adaptive rate induces a set of so-called offset complexity\nmeasures, and obtaining small upper bounds on these quantities is sufficient to\ndemonstrate achievability. A cornerstone of our analysis technique is the use\nof one-sided tail inequalities to bound suprema of offset random processes.\n  Our framework recovers and improves a wide variety of adaptive bounds\nincluding quantile bounds, second-order data-dependent bounds, and small loss\nbounds. In addition we derive a new type of adaptive bound for online linear\noptimization based on the spectral norm, as well as a new online PAC-Bayes\ntheorem that holds for countably infinite sets. \n\n"}
{"id": "1508.06235", "contents": "Title: Clustering With Side Information: From a Probabilistic Model to a\n  Deterministic Algorithm Abstract: In this paper, we propose a model-based clustering method (TVClust) that\nrobustly incorporates noisy side information as soft-constraints and aims to\nseek a consensus between side information and the observed data. Our method is\nbased on a nonparametric Bayesian hierarchical model that combines the\nprobabilistic model for the data instance and the one for the side-information.\nAn efficient Gibbs sampling algorithm is proposed for posterior inference.\nUsing the small-variance asymptotics of our probabilistic model, we then derive\na new deterministic clustering algorithm (RDP-means). It can be viewed as an\nextension of K-means that allows for the inclusion of side information and has\nthe additional property that the number of clusters does not need to be\nspecified a priori. Empirical studies have been carried out to compare our work\nwith many constrained clustering algorithms from the literature on both a\nvariety of data sets and under a variety of conditions such as using noisy side\ninformation and erroneous k values. The results of our experiments show strong\nresults for our probabilistic and deterministic approaches under these\nconditions when compared to other algorithms in the literature. \n\n"}
{"id": "1509.00151", "contents": "Title: Learning A Task-Specific Deep Architecture For Clustering Abstract: While sparse coding-based clustering methods have shown to be successful,\ntheir bottlenecks in both efficiency and scalability limit the practical usage.\nIn recent years, deep learning has been proved to be a highly effective,\nefficient and scalable feature learning tool. In this paper, we propose to\nemulate the sparse coding-based clustering pipeline in the context of deep\nlearning, leading to a carefully crafted deep model benefiting from both. A\nfeed-forward network structure, named TAGnet, is constructed based on a\ngraph-regularized sparse coding algorithm. It is then trained with\ntask-specific loss functions from end to end. We discover that connecting deep\nlearning to sparse coding benefits not only the model performance, but also its\ninitialization and interpretation. Moreover, by introducing auxiliary\nclustering tasks to the intermediate feature hierarchy, we formulate DTAGnet\nand obtain a further performance boost. Extensive experiments demonstrate that\nthe proposed model gains remarkable margins over several state-of-the-art\nmethods. \n\n"}
{"id": "1509.00349", "contents": "Title: Transitional annealed adaptive slice sampling for Gaussian process\n  hyper-parameter estimation Abstract: Surrogate models have become ubiquitous in science and engineering for their\ncapability of emulating expensive computer codes, necessary to model and\ninvestigate complex phenomena. Bayesian emulators based on Gaussian processes\nadequately quantify the uncertainty that results from the cost of the original\nsimulator, and thus the inability to evaluate it on the whole input space.\nHowever, it is common in the literature that only a partial Bayesian analysis\nis carried out, whereby the underlying hyper-parameters are estimated via\ngradient-free optimisation or genetic algorithms, to name a few methods. On the\nother hand, maximum a posteriori (MAP) estimation could discard important\nregions of the hyper-parameter space. In this paper, we carry out a more\ncomplete Bayesian inference, that combines Slice Sampling with some recently\ndeveloped Sequential Monte Carlo samplers. The resulting algorithm improves the\nmixing in the sampling through delayed-rejection, the inclusion of an annealing\nscheme akin to Asymptotically Independent Markov Sampling and parallelisation\nvia Transitional Markov Chain Monte Carlo. Examples related to the estimation\nof Gaussian process hyper-parameters are presented. For the purpose of\nreproducibility, further development, and use in other applications, the code\nto generate the examples in this paper is freely available for download at\nhttp://github.com/agarbuno/ta2s2_codes \n\n"}
{"id": "1509.00711", "contents": "Title: The generic minimal rigidity of a partially triangulated torus Abstract: A simple graph is $3$-rigid if its generic bar-joint frameworks in $R^3$ are\ninfinitesimally rigid. Necessary and sufficient conditions are obtained for the\nminimal $3$-rigidity of a simple graph which is obtained from the $1$-skeleton\nof a triangulated torus by the deletion of edges interior to a triangulated\ndisc. \n\n"}
{"id": "1509.00899", "contents": "Title: A robust approach for estimating change-points in the mean of an AR(p)\n  process Abstract: We consider the problem of change-points estimation in the mean of an AR(p)\nprocess. Taking into account the dependence structure does not allow us to use\nthe approach of the independent case. Especially, the dynamic programming\nalgorithm giving the optimal solution in the independent case cannot be used\nanymore. We propose a two-step method, based on the preliminary robust (to the\nchange-points) estimation of the autoregression parameters. Then, we propose to\nfollow the classical approach, by plugging this estimator in the criterion used\nfor change-point estimation, which is equivalent to decorrelate the series\nusing the estimated autoregression parameters. We show that the asymptotic\nproperties of these change-point location and mean estimators are the same as\nthose of the classical estimators in the independent framework. The same\nplug-in approach is then used to approximate the modified BIC and choose the\nnumber of segments, and to derive a heuristic BIC criterion to select both the\nnumber of changes and the order of the autoregression. Finally, we show, in the\nsimulation section, that for finite sample size taking into account the\ndependence structure improves the statistical performance of the change-point\nestimators and of the selection criterion. \n\n"}
{"id": "1509.01168", "contents": "Title: Semi-described and semi-supervised learning with Gaussian processes Abstract: Propagating input uncertainty through non-linear Gaussian process (GP)\nmappings is intractable. This hinders the task of training GPs using uncertain\nand partially observed inputs. In this paper we refer to this task as\n\"semi-described learning\". We then introduce a GP framework that solves both,\nthe semi-described and the semi-supervised learning problems (where missing\nvalues occur in the outputs). Auto-regressive state space simulation is also\nrecognised as a special case of semi-described learning. To achieve our goal we\ndevelop variational methods for handling semi-described inputs in GPs, and\ncouple them with algorithms that allow for imputing the missing values while\ntreating the uncertainty in a principled, Bayesian manner. Extensive\nexperiments on simulated and real-world data study the problems of iterative\nforecasting and regression/classification with missing values. The results\nsuggest that the principled propagation of uncertainty stemming from our\nframework can significantly improve performance in these tasks. \n\n"}
{"id": "1509.01631", "contents": "Title: Stochastic gradient variational Bayes for gamma approximating\n  distributions Abstract: While stochastic variational inference is relatively well known for scaling\ninference in Bayesian probabilistic models, related methods also offer ways to\ncircumnavigate the approximation of analytically intractable expectations. The\nkey challenge in either setting is controlling the variance of gradient\nestimates: recent work has shown that for continuous latent variables,\nparticularly multivariate Gaussians, this can be achieved by using the gradient\nof the log posterior. In this paper we apply the same idea to gamma distributed\nlatent variables given gamma variational distributions, enabling\nstraightforward \"black box\" variational inference in models where sparsity and\nnon-negativity are appropriate. We demonstrate the method on a recently\nproposed gamma process model for network data, as well as a novel sparse factor\nanalysis. We outperform generic sampling algorithms and the approach of using\nGaussian variational distributions on transformed variables. \n\n"}
{"id": "1509.02216", "contents": "Title: Fuzzy Jets Abstract: Collimated streams of particles produced in high energy physics experiments\nare organized using clustering algorithms to form jets. To construct jets, the\nexperimental collaborations based at the Large Hadron Collider (LHC) primarily\nuse agglomerative hierarchical clustering schemes known as sequential\nrecombination. We propose a new class of algorithms for clustering jets that\nuse infrared and collinear safe mixture models. These new algorithms, known as\nfuzzy jets, are clustered using maximum likelihood techniques and can\ndynamically determine various properties of jets like their size. We show that\nthe fuzzy jet size adds additional information to conventional jet tagging\nvariables. Furthermore, we study the impact of pileup and show that with some\nslight modifications to the algorithm, fuzzy jets can be stable up to high\npileup interaction multiplicities. \n\n"}
{"id": "1509.02254", "contents": "Title: Mixed Ehrhart polynomials Abstract: For lattice polytopes $P_1,\\ldots, P_k \\subseteq \\mathbb{R}^d$, Bihan (2014)\nintroduced the discrete mixed volume $\\mathrm{DMV}(P_1,\\dots,P_k)$ in analogy\nto the classical mixed volume. In this note we initiate the study of the\nassociated mixed Ehrhart polynomial $\\mathrm{ME}_{P_1,\\dots,P_k}(n) =\n\\mathrm{DMV}(nP_1,\\dots,nP_k)$. We study properties of this polynomial and we\ngive interpretations for some of its coefficients in terms of (discrete) mixed\nvolumes. Bihan (2014) showed that the discrete mixed volume is always\nnon-negative. Our investigations yield simpler proofs for certain special\ncases. We also introduce and study the associated mixed $h^*$-vector. We show\nthat for large enough dilates $r P_1, \\ldots, rP_k$ the corresponding mixed\n$h^*$-polynomial has only real roots and as a consequence the mixed\n$h^*$-vector becomes non-negative. \n\n"}
{"id": "1509.02735", "contents": "Title: Containment Problems for Projections of Polyhedra and Spectrahedra Abstract: Spectrahedra are affine sections of the cone of positive semidefinite\nmatrices which form a rich class of convex bodies that properly contains that\nof polyhedra. While the class of polyhedra is closed under linear projections,\nthe class of spectrahedra is not. In this paper we investigate the problem of\ndeciding containment of projections of polyhedra and spectrahedra based on\nprevious works on containment of spectrahedra. The main concern is to study\nthese containment problems by formulating them as polynomial nonnegativity\nproblems. This allows to state hierarchies of (sufficient) semidefinite\nconditions by applying (and proving) sophisticated Positivstellens\\\"atze. We\nalso extend results on a solitary sufficient condition for containment of\nspectrahedra coming from the polyhedral situation as well as connections to the\ntheory of (completely) positive linear maps. \n\n"}
{"id": "1509.02999", "contents": "Title: Complex spherical codes with three inner products Abstract: Let $X$ be a finite set in a complex sphere of $d$ dimension. Let $D(X)$ be\nthe set of usual inner products of two distinct vectors in $X$. A set $X$ is\ncalled a complex spherical $s$-code if the cardinality of $D(X)$ is $s$ and\n$D(X)$ contains an imaginary number. We would like to classify the largest\npossible $s$-codes for given dimension $d$. In this paper, we consider the\nproblem for the case $s=3$. Roy and Suda (2014) gave a certain upper bound for\nthe cardinalities of $3$-codes. A $3$-code $X$ is said to be tight if $X$\nattains the bound. We show that there exists no tight $3$-code except for\ndimensions $1$, $2$. Moreover we make an algorithm to classify the largest\n$3$-codes by considering representations of oriented graphs. By this algorithm,\nthe largest $3$-codes are classified for dimensions $1$, $2$, $3$ with a\ncurrent computer. \n\n"}
{"id": "1509.04640", "contents": "Title: Dynamic Poisson Factorization Abstract: Models for recommender systems use latent factors to explain the preferences\nand behaviors of users with respect to a set of items (e.g., movies, books,\nacademic papers). Typically, the latent factors are assumed to be static and,\ngiven these factors, the observed preferences and behaviors of users are\nassumed to be generated without order. These assumptions limit the explorative\nand predictive capabilities of such models, since users' interests and item\npopularity may evolve over time. To address this, we propose dPF, a dynamic\nmatrix factorization model based on the recent Poisson factorization model for\nrecommendations. dPF models the time evolving latent factors with a Kalman\nfilter and the actions with Poisson distributions. We derive a scalable\nvariational inference algorithm to infer the latent factors. Finally, we\ndemonstrate dPF on 10 years of user click data from arXiv.org, one of the\nlargest repository of scientific papers and a formidable source of information\nabout the behavior of scientists. Empirically we show performance improvement\nover both static and, more recently proposed, dynamic recommendation models. We\nalso provide a thorough exploration of the inferred posteriors over the latent\nvariables. \n\n"}
{"id": "1509.04704", "contents": "Title: Central limit theorems for network driven sampling Abstract: Respondent-Driven Sampling is a popular technique for sampling hidden\npopulations. This paper models Respondent-Driven Sampling as a Markov process\nindexed by a tree. Our main results show that the Volz-Heckathorn estimator is\nasymptotically normal below a critical threshold. The key technical\ndifficulties stem from (i) the dependence between samples and (ii) the tree\nstructure which characterizes the dependence. The theorems allow the growth\nrate of the tree to exceed one and suggest that this growth rate should not be\ntoo large. To illustrate the usefulness of these results beyond their obvious\nuse, an example shows that in certain cases the sample average is preferable to\ninverse probability weighting. We provide a test statistic to distinguish\nbetween these two cases. \n\n"}
{"id": "1509.06428", "contents": "Title: Large-Scale Mode Identification and Data-Driven Sciences Abstract: Bump-hunting or mode identification is a fundamental problem that arises in\nalmost every scientific field of data-driven discovery. Surprisingly, very few\ndata modeling tools are available for automatic (not requiring manual\ncase-by-base investigation), objective (not subjective), and nonparametric (not\nbased on restrictive parametric model assumptions) mode discovery, which can\nscale to large data sets. This article introduces LPMode--an algorithm based on\na new theory for detecting multimodality of a probability density. We apply\nLPMode to answer important research questions arising in various fields from\nenvironmental science, ecology, econometrics, analytical chemistry to astronomy\nand cancer genomics. \n\n"}
{"id": "1509.06459", "contents": "Title: Stochastic gradient descent methods for estimation with large data sets Abstract: We develop methods for parameter estimation in settings with large-scale data\nsets, where traditional methods are no longer tenable. Our methods rely on\nstochastic approximations, which are computationally efficient as they maintain\none iterate as a parameter estimate, and successively update that iterate based\non a single data point. When the update is based on a noisy gradient, the\nstochastic approximation is known as standard stochastic gradient descent,\nwhich has been fundamental in modern applications with large data sets.\nAdditionally, our methods are numerically stable because they employ implicit\nupdates of the iterates. Intuitively, an implicit update is a shrinked version\nof a standard one, where the shrinkage factor depends on the observed Fisher\ninformation at the corresponding data point. This shrinkage prevents numerical\ndivergence of the iterates, which can be caused either by excess noise or\noutliers. Our sgd package in R offers the most extensive and robust\nimplementation of stochastic gradient descent methods. We demonstrate that sgd\ndominates alternative software in runtime for several estimation problems with\nmassive data sets. Our applications include the wide class of generalized\nlinear models as well as M-estimation for robust regression. \n\n"}
{"id": "1509.08775", "contents": "Title: Error Bounds for Sequential Monte Carlo Samplers for Multimodal\n  Distributions Abstract: In this paper, we provide bounds on the asymptotic variance for a class of\nsequential Monte Carlo (SMC) samplers designed for approximating multimodal\ndistributions. Such methods combine standard SMC methods and Markov chain Monte\nCarlo (MCMC) kernels. Our bounds improve upon previous results, and unlike some\nearlier work, they also apply in the case when the MCMC kernels can move\nbetween the modes. We apply our results to the Potts model from statistical\nphysics. In this case, the problem of sharp peaks is encountered. Earlier\nmethods, such as parallel tempering, are only able to sample from it at an\nexponential (in an important parameter of the model) cost. We propose a\nsequence of interpolating distributions called interpolation to independence,\nand show that the SMC sampler based on it is able to sample from this target\ndistribution at a polynomial cost. We believe that our method is generally\napplicable to many other distributions as well. \n\n"}
{"id": "1509.08783", "contents": "Title: Colorful theorems for strong convexity Abstract: We prove two colorful Carath\\'eodory theorems for strongly convex hulls,\ngeneralizing the colorful Carat\\'eodory theorem for ordinary convexity by Imre\nB\\'ar\\'any, the non-colorful Carath\\'eodory theorem for strongly convex hulls\nby the second author, and the \"very colorful theorems\" by the first author and\nothers. We also investigate if the assumption of a \"generating convex set\" is\nreally needed in such results and try to give a topological criterion for one\nconvex body to be a Minkowski summand of another. \n\n"}
{"id": "1510.00861", "contents": "Title: A Geometric View of Posterior Approximation Abstract: Although Bayesian methods are robust and principled, their application in\npractice could be limited since they typically rely on computationally\nintensive Markov Chain Monte Carlo algorithms for their implementation. One\npossible solution is to find a fast approximation of posterior distribution and\nuse it for statistical inference. For commonly used approximation methods, such\nas Laplace and variational free energy, the objective is mainly defined in\nterms of computational convenience as opposed to a true distance measure\nbetween the target and approximating distributions. In this paper, we provide a\ngeometric view of posterior approximation based on a valid distance measure\nderived from ambient Fisher geometry. Our proposed framework is easily\ngeneralizable and can inspire a new class of methods for approximate Bayesian\ninference. \n\n"}
{"id": "1510.02577", "contents": "Title: Asymptotic Analysis of the Random-Walk Metropolis Algorithm on Ridged\n  Densities Abstract: In this paper we study the asymptotic behavior of the Random-Walk Metropolis\nalgorithm on probability densities with two different `scales', where most of\nthe probability mass is distributed along certain key directions with the\n`orthogonal' directions containing relatively less mass. Such class of\nprobability measures arise in various applied contexts including Bayesian\ninverse problems where the posterior measure concentrates on a sub-manifold\nwhen the noise variance goes to zero. When the target measure concentrates on a\nlinear sub-manifold, we derive analytically a diffusion limit for the\nRandom-Walk Metropolis Markov chain as the scale parameter goes to zero. In\ncontrast to the existing works on scaling limits, our limiting Stochastic\nDifferential Equation does not in general have a constant diffusion\ncoefficient. Our results show that in some cases, the usual practice of\nadapting the step-size to control the acceptance probability might be\nsub-optimal as the optimal acceptance probability is zero (in the limit). \n\n"}
{"id": "1510.02948", "contents": "Title: Construction of fullerenes Abstract: We present an infinite series of operations on fullerenes generalizing the\nEndo-Kroto operation, such that each combinatorial fullerene is obtained from\nthe dodecahedron by a sequence of such operations. We prove that these\noperations are invertible in the proper sense, and are compositions of\n(1;4,5)-, (1;5,5)-, (2,6;4,5)-, (2,6;5,5)-, (2,6;5,6)-, (2,7;5,5)-, and\n(2,7;5,6)-truncations, where each truncation increases the number of hexagons\nby one. \n\n"}
{"id": "1510.04073", "contents": "Title: Convex hulls of random walks, hyperplane arrangements, and Weyl chambers Abstract: We give an explicit formula for the probability that the convex hull of an\n$n$-step random walk in $R^d$ does not contain the origin, under the assumption\nthat the distribution of increments of the walk is centrally symmetric and puts\nno mass on affine hyperplanes. This extends the formula by Sparre Andersen\n(1949) for the probability that such random walk in dimension one stays\npositive. Our result is distribution-free, that is, the probability does not\ndepend on the distribution of increments.\n  This probabilistic problem is shown to be equivalent to either of the two\ngeometric ones: 1) Find the number of Weyl chambers of type $B_n$ intersected\nby a generic linear subspace of $R^n$ of codimension $d$; 2) Find the conic\nintrinsic volumes of a Weyl chamber of type $B_n$. We solve the first geometric\nproblem using the theory of hyperplane arrangements. A by-product of our method\nis a new simple proof of the general formula by Klivans and Swartz (2011)\nrelating the coefficients of the characteristic polynomial of a linear\nhyperplane arrangement to the conic intrinsic volumes of the chambers\nconstituting its complement.\n  We obtain analogous distribution-free results for Weyl chambers of type\n$A_{n-1}$ (yielding the probability of absorption of the origin by the convex\nhull of a generic random walk bridge), type $D_n$, and direct products of Weyl\nchambers (yielding the absorption probability for the joint convex hull of\nseveral random walks or bridges). The simplest case of products of the form\n$B_1\\times \\dots \\times B_1$ recovers the Wendel formula (1962) for the\nprobability that the convex hull of an i.i.d. multidimensional sample chosen\nfrom a centrally symmetric distribution does not contain the origin.\n  We also give an asymptotic analysis of the obtained absorption probabilities\nas $n \\to \\infty$, in both cases of fixed and increasing dimension $d$. \n\n"}
{"id": "1510.04514", "contents": "Title: Mixture Models: Building a Parameter Space Abstract: Despite the flexibility and popularity of mixture models, their associated\nparameter spaces are often difficult to represent due to fundamental\nidentification problems. This paper looks at a novel way of representing such a\nspace for general mixtures of exponential families, where the parameters are\nidentifiable, interpretable, and, due to a tractable geometric structure, the\nspace allows fast computational algorithms to be constructed. \n\n"}
{"id": "1510.04977", "contents": "Title: Multilevel particle filter Abstract: In this paper the filtering of partially observed diffusions, with\ndiscrete-time observations, is considered. It is assumed that only biased\napproximations of the diffusion can be obtained, for choice of an accuracy\nparameter indexed by $l$. A multilevel estimator is proposed, consisting of a\ntelescopic sum of increment estimators associated to the successive levels. The\nwork associated to $\\mathcal{O}(\\varepsilon^2)$ mean-square error between the\nmultilevel estimator and average with respect to the filtering distribution is\nshown to scale optimally, for example as $\\mathcal{O}(\\varepsilon^{-2})$ for\noptimal rates of convergence of the underlying diffusion approximation. The\nmethod is illustrated on some toy examples as well as estimation of interest\nrate based on real S&P 500 stock price data. \n\n"}
{"id": "1510.05466", "contents": "Title: A Sparse Multi-Scale Algorithm for Dense Optimal Transport Abstract: Discrete optimal transport solvers do not scale well on dense large problems\nsince they do not explicitly exploit the geometric structure of the cost\nfunction. In analogy to continuous optimal transport we provide a framework to\nverify global optimality of a discrete transport plan locally. This allows\nconstruction of an algorithm to solve large dense problems by considering a\nsequence of sparse problems instead. The algorithm lends itself to being\ncombined with a hierarchical multi-scale scheme. Any existing discrete solver\ncan be used as internal black-box.Several cost functions, including the noisy\nsquared Euclidean distance, are explicitly detailed. We observe a significant\nreduction of run-time and memory requirements. \n\n"}
{"id": "1510.09072", "contents": "Title: Palindromic Bernoulli distributions Abstract: We introduce and study a subclass of joint Bernoulli distributions which has\nthe palindromic property. For such distributions the vector of joint\nprobabilities is unchanged when the order of the elements is reversed. We prove\nfor binary variables that the palindromic property is equivalent to zero\nconstraints on all odd-order interaction parameters, be it in parameterizations\nwhich are log-linear, linear or multivariate logistic. In particular, we derive\nthe one-to-one parametric transformations for these three types of model\nspecifications and give simple closed forms of maximum likelihood estimates.\nSome special cases and a case study are described. \n\n"}
{"id": "1511.00108", "contents": "Title: Recursive computation for evaluating the exact $p$-values of temporal\n  and spatial scan statistics Abstract: Let $V$ be a finite set of indices, and let $B_i$, $i=1,\\ldots,m$, be subsets\nof $V$ such that $V=\\bigcup_{i=1}^{m}B_i$. Let $X_i$, $i\\in V$, be independent\nrandom variables, and let $X_{B_i}=(X_j)_{j\\in B_i}$. In this paper, we propose\na recursive computation method to calculate the conditional expectation\n$E\\bigl[\\prod_{i=1}^m\\chi_i(X_{B_i}) \\,|\\, N\\bigr]$ with $N=\\sum_{i\\in V}X_i$\ngiven, where $\\chi_i$ is an arbitrary function. Our method is based on the\nrecursive summation/integration technique using the Markov property in\nstatistics. To extract the Markov property, we define an undirected graph whose\ncliques are $B_j$, and obtain its chordal extension, from which we present the\nexpressions of the recursive formula. This methodology works for a class of\ndistributions including the Poisson distribution (that is, the conditional\ndistribution is the multinomial). This problem is motivated from the evaluation\nof the multiplicity-adjusted $p$-value of scan statistics in spatial\nepidemiology. As an illustration of the approach, we present the real data\nanalyses to detect temporal and spatial clustering. \n\n"}
{"id": "1511.00146", "contents": "Title: Faster Stochastic Variational Inference using Proximal-Gradient Methods\n  with General Divergence Functions Abstract: Several recent works have explored stochastic gradient methods for\nvariational inference that exploit the geometry of the variational-parameter\nspace. However, the theoretical properties of these methods are not\nwell-understood and these methods typically only apply to\nconditionally-conjugate models. We present a new stochastic method for\nvariational inference which exploits the geometry of the variational-parameter\nspace and also yields simple closed-form updates even for non-conjugate models.\nWe also give a convergence-rate analysis of our method and many other previous\nmethods which exploit the geometry of the space. Our analysis generalizes\nexisting convergence results for stochastic mirror-descent on non-convex\nobjectives by using a more general class of divergence functions. Beyond giving\na theoretical justification for a variety of recent methods, our experiments\nshow that new algorithms derived in this framework lead to state of the art\nresults on a variety of problems. Further, due to its generality, we expect\nthat our theoretical analysis could also apply to other applications. \n\n"}
{"id": "1511.03334", "contents": "Title: Goodness of fit tests for high-dimensional linear models Abstract: In this work we propose a framework for constructing goodness of fit tests in\nboth low and high-dimensional linear models. We advocate applying regression\nmethods to the scaled residuals following either an ordinary least squares or\nLasso fit to the data, and using some proxy for prediction error as the final\ntest statistic. We call this family Residual Prediction (RP) tests. We show\nthat simulation can be used to obtain the critical values for such tests in the\nlow-dimensional setting, and demonstrate using both theoretical results and\nextensive numerical studies that some form of the parametric bootstrap can do\nthe same when the high-dimensional linear model is under consideration. We show\nthat RP tests can be used to test for significance of groups or individual\nvariables as special cases, and here they compare favourably with state of the\nart methods, but we also argue that they can be designed to test for as diverse\nmodel misspecifications as heteroscedasticity and nonlinearity. \n\n"}
{"id": "1511.04128", "contents": "Title: A Widely Linear Complex Autoregressive Process of Order One Abstract: We propose a simple stochastic process for modeling improper or noncircular\ncomplex-valued signals. The process is a natural extension of a complex-valued\nautoregressive process, extended to include a widely linear autoregressive\nterm. This process can then capture elliptical, as opposed to circular,\nstochastic oscillations in a bivariate signal. The process is order one and is\nmore parsimonious than alternative stochastic modeling approaches in the\nliterature. We provide conditions for stationarity, and derive the form of the\ncovariance and relation sequence of this model. We describe how parameter\nestimation can be efficiently performed both in the time and frequency domain.\nWe demonstrate the practical utility of the process in capturing elliptical\noscillations that are naturally present in seismic signals. \n\n"}
{"id": "1511.04514", "contents": "Title: Sparse Nonlinear Regression: Parameter Estimation and Asymptotic\n  Inference Abstract: We study parameter estimation and asymptotic inference for sparse nonlinear\nregression. More specifically, we assume the data are given by $y = f( x^\\top\n\\beta^* ) + \\epsilon$, where $f$ is nonlinear. To recover $\\beta^*$, we propose\nan $\\ell_1$-regularized least-squares estimator. Unlike classical linear\nregression, the corresponding optimization problem is nonconvex because of the\nnonlinearity of $f$. In spite of the nonconvexity, we prove that under mild\nconditions, every stationary point of the objective enjoys an optimal\nstatistical rate of convergence. In addition, we provide an efficient algorithm\nthat provably converges to a stationary point. We also access the uncertainty\nof the obtained estimator. Specifically, based on any stationary point of the\nobjective, we construct valid hypothesis tests and confidence intervals for the\nlow dimensional components of the high-dimensional parameter $\\beta^*$.\nDetailed numerical results are provided to back up our theory. \n\n"}
{"id": "1511.04992", "contents": "Title: The Correlated Pseudo-Marginal Method Abstract: The pseudo-marginal algorithm is a popular variant of the\nMetropolis--Hastings scheme which allows us to sample asymptotically from a\ntarget probability density $\\pi$, when we are only able to estimate an\nunnormalized version of $\\pi$ pointwise unbiasedly. It has found numerous\napplications in Bayesian statistics as there are many scenarios where the\nlikelihood function is intractable but can be estimated unbiasedly using Monte\nCarlo samples. Using many samples will typically result in averages computed\nunder this chain with lower asymptotic variances than the corresponding\naverages that use fewer samples. For a fixed computing time, it has been shown\nin several recent contributions that an efficient implementation of the\npseudo-marginal method requires the variance of the log-likelihood ratio\nestimator appearing in the acceptance probability of the algorithm to be of\norder 1, which in turn usually requires scaling the number $N$ of Monte Carlo\nsamples linearly with the number $T$ of data points. We propose a modification\nof the pseudo-marginal algorithm, termed the correlated pseudo-marginal\nalgorithm, which is based on a novel log-likelihood ratio estimator computed\nusing the difference of two positively correlated log-likelihood estimators. We\nshow that the parameters of this scheme can be selected such that the variance\nof this estimator is order $1$ as $N,T\\rightarrow\\infty$ whenever\n$N/T\\rightarrow 0$. By combining these results with the Bernstein-von Mises\ntheorem, we provide an analysis of the performance of the correlated\npseudo-marginal algorithm in the large $T$ regime. In our numerical examples,\nthe efficiency of computations is increased relative to the standard\npseudo-marginal algorithm by more than 20 fold for values of $T$ of a few\nhundreds to more than 100 fold for values of $T$ of around 10,000-20,000. \n\n"}
{"id": "1511.05355", "contents": "Title: A fixed-point approach to barycenters in Wasserstein space Abstract: Let $\\mathcal{P}_{2,ac}$ be the set of Borel probabilities on $\\mathbb{R}^d$\nwith finite second moment and absolutely continuous with respect to Lebesgue\nmeasure. We consider the problem of finding the barycenter (or Fr\\'echet mean)\nof a finite set of probabilities $\\nu_1,\\ldots,\\nu_k \\in \\mathcal{P}_{2,ac}$\nwith respect to the $L_2-$Wasserstein metric. For this task we introduce an\noperator on $\\mathcal{P}_{2,ac}$ related to the optimal transport maps pushing\nforward any $\\mu \\in \\mathcal{P}_{2,ac}$ to $\\nu_1,\\ldots,\\nu_k$. Under very\ngeneral conditions we prove that the barycenter must be a fixed point for this\noperator and introduce an iterative procedure which consistently approximates\nthe barycenter. The procedure allows effective computation of barycenters in\nany location-scatter family, including the Gaussian case. In such cases the\nbarycenter must belong to the family, thus it is characterized by its mean and\ncovariance matrix. While its mean is just the weighted mean of the means of the\nprobabilities, the covariance matrix is characterized in terms of their\ncovariance matrices $\\Sigma_1,\\dots,\\Sigma_k$ through a nonlinear matrix\nequation. The performance of the iterative procedure in this case is\nillustrated through numerical simulations, which show fast convergence towards\nthe barycenter. \n\n"}
{"id": "1511.07130", "contents": "Title: Parallel Predictive Entropy Search for Batch Global Optimization of\n  Expensive Objective Functions Abstract: We develop parallel predictive entropy search (PPES), a novel algorithm for\nBayesian optimization of expensive black-box objective functions. At each\niteration, PPES aims to select a batch of points which will maximize the\ninformation gain about the global maximizer of the objective. Well known\nstrategies exist for suggesting a single evaluation point based on previous\nobservations, while far fewer are known for selecting batches of points to\nevaluate in parallel. The few batch selection schemes that have been studied\nall resort to greedy methods to compute an optimal batch. To the best of our\nknowledge, PPES is the first non-greedy batch Bayesian optimization strategy.\nWe demonstrate the benefit of this approach in optimization performance on both\nsynthetic and real world applications, including problems in machine learning,\nrocket science and robotics. \n\n"}
{"id": "1511.07289", "contents": "Title: Fast and Accurate Deep Network Learning by Exponential Linear Units\n  (ELUs) Abstract: We introduce the \"exponential linear unit\" (ELU) which speeds up learning in\ndeep neural networks and leads to higher classification accuracies. Like\nrectified linear units (ReLUs), leaky ReLUs (LReLUs) and parametrized ReLUs\n(PReLUs), ELUs alleviate the vanishing gradient problem via the identity for\npositive values. However, ELUs have improved learning characteristics compared\nto the units with other activation functions. In contrast to ReLUs, ELUs have\nnegative values which allows them to push mean unit activations closer to zero\nlike batch normalization but with lower computational complexity. Mean shifts\ntoward zero speed up learning by bringing the normal gradient closer to the\nunit natural gradient because of a reduced bias shift effect. While LReLUs and\nPReLUs have negative values, too, they do not ensure a noise-robust\ndeactivation state. ELUs saturate to a negative value with smaller inputs and\nthereby decrease the forward propagated variation and information. Therefore,\nELUs code the degree of presence of particular phenomena in the input, while\nthey do not quantitatively model the degree of their absence. In experiments,\nELUs lead not only to faster learning, but also to significantly better\ngeneralization performance than ReLUs and LReLUs on networks with more than 5\nlayers. On CIFAR-100 ELUs networks significantly outperform ReLU networks with\nbatch normalization while batch normalization does not improve ELU networks.\nELU networks are among the top 10 reported CIFAR-10 results and yield the best\npublished result on CIFAR-100, without resorting to multi-view evaluation or\nmodel averaging. On ImageNet, ELU networks considerably speed up learning\ncompared to a ReLU network with the same architecture, obtaining less than 10%\nclassification error for a single crop, single model network. \n\n"}
{"id": "1511.07827", "contents": "Title: Stopping criteria for boosting automatic experimental design using\n  real-time fMRI with Bayesian optimization Abstract: Bayesian optimization has been proposed as a practical and efficient tool\nthrough which to tune parameters in many difficult settings. Recently, such\ntechniques have been combined with real-time fMRI to propose a novel framework\nwhich turns on its head the conventional functional neuroimaging approach. This\nclosed-loop method automatically designs the optimal experiment to evoke a\ndesired target brain pattern. One of the challenges associated with extending\nsuch methods to real-time brain imaging is the need for adequate stopping\ncriteria, an aspect of Bayesian optimization which has received limited\nattention. In light of high scanning costs and limited attentional capacities\nof subjects an accurate and reliable stopping criteria is essential. In order\nto address this issue we propose and empirically study the performance of two\nstopping criteria. \n\n"}
{"id": "1511.08684", "contents": "Title: The complement of the figure-eight knot geometrically bounds Abstract: We show that some hyperbolic 3-manifolds which are tessellated by copies of\nthe regular ideal hyperbolic tetrahedron embed geodesically in a complete,\nfinite volume, hyperbolic 4-manifold. This allows us to prove that the\ncomplement of the figure-eight knot geometrically bounds a complete, finite\nvolume hyperbolic 4-manifold. This the first example of geometrically bounding\nhyperbolic knot complement and, amongst known examples of geometrically\nbounding manifolds, the one with the smallest volume. \n\n"}
{"id": "1512.01022", "contents": "Title: Unbiased estimators and multilevel Monte Carlo Abstract: Multilevel Monte Carlo (MLMC) and unbiased estimators recently proposed by\nMcLeish (Monte Carlo Methods Appl., 2011) and Rhee and Glynn (Oper. Res., 2015)\nare closely related. This connection is elaborated by presenting a new general\nclass of unbiased estimators, which admits previous debiasing schemes as\nspecial cases. New lower variance estimators are proposed, which are stratified\nversions of earlier unbiased schemes. Under general conditions, essentially\nwhen MLMC admits the canonical square root Monte Carlo error rate, the proposed\nnew schemes are shown to be asymptotically as efficient as MLMC, both in terms\nof variance and cost. The experiments demonstrate that the variance reduction\nprovided by the new schemes can be substantial. \n\n"}
{"id": "1512.01139", "contents": "Title: Kalman-based Stochastic Gradient Method with Stop Condition and\n  Insensitivity to Conditioning Abstract: Modern proximal and stochastic gradient descent (SGD) methods are believed to\nefficiently minimize large composite objective functions, but such methods have\ntwo algorithmic challenges: (1) a lack of fast or justified stop conditions,\nand (2) sensitivity to the objective function's conditioning. In response to\nthe first challenge, modern proximal and SGD methods guarantee convergence only\nafter multiple epochs, but such a guarantee renders proximal and SGD methods\ninfeasible when the number of component functions is very large or infinite. In\nresponse to the second challenge, second order SGD methods have been developed,\nbut they are marred by the complexity of their analysis. In this work, we\naddress these challenges on the limited, but important, linear regression\nproblem by introducing and analyzing a second order proximal/SGD method based\non Kalman Filtering (kSGD). Through our analysis, we show kSGD is\nasymptotically optimal, develop a fast algorithm for very large, infinite or\nstreaming data sources with a justified stop condition, prove that kSGD is\ninsensitive to the problem's conditioning, and develop a unique approach for\nanalyzing the complex second order dynamics. Our theoretical results are\nsupported by numerical experiments on three regression problems (linear,\nnonparametric wavelet, and logistic) using three large publicly available\ndatasets. Moreover, our analysis and experiments lay a foundation for embedding\nkSGD in multiple epoch algorithms, extending kSGD to other problem classes, and\ndeveloping parallel and low memory kSGD implementations. \n\n"}
{"id": "1512.03385", "contents": "Title: Deep Residual Learning for Image Recognition Abstract: Deeper neural networks are more difficult to train. We present a residual\nlearning framework to ease the training of networks that are substantially\ndeeper than those used previously. We explicitly reformulate the layers as\nlearning residual functions with reference to the layer inputs, instead of\nlearning unreferenced functions. We provide comprehensive empirical evidence\nshowing that these residual networks are easier to optimize, and can gain\naccuracy from considerably increased depth. On the ImageNet dataset we evaluate\nresidual nets with a depth of up to 152 layers---8x deeper than VGG nets but\nstill having lower complexity. An ensemble of these residual nets achieves\n3.57% error on the ImageNet test set. This result won the 1st place on the\nILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100\nand 1000 layers.\n  The depth of representations is of central importance for many visual\nrecognition tasks. Solely due to our extremely deep representations, we obtain\na 28% relative improvement on the COCO object detection dataset. Deep residual\nnets are foundations of our submissions to ILSVRC & COCO 2015 competitions,\nwhere we also won the 1st places on the tasks of ImageNet detection, ImageNet\nlocalization, COCO detection, and COCO segmentation. \n\n"}
{"id": "1512.03472", "contents": "Title: On the chromatic numbers of small-dimensional Euclidean spaces Abstract: The paper is devoted to the study of graph sequence G_n = (V_n, E_n) where\nV_n is the set of all vectors v in R^n with coordinates from {-1, 0, 1} such\nthat |v| = sqrt(3), and E_n consists of all pairs of vertices with the scalar\nproduct 1. We find exactly the independence number of G_n. As a corollary we\nget some new lower bounds of chi(\\R^n) and chi(\\Q^n) for small values of n. \n\n"}
{"id": "1512.04093", "contents": "Title: Multiple Change-point Detection: a Selective Overview Abstract: Very long and noisy sequence data arise from biological sciences to social\nscience including high throughput data in genomics and stock prices in\neconometrics. Often such data are collected in order to identify and understand\nshifts in trend, e.g., from a bull market to a bear market in finance or from a\nnormal number of chromosome copies to an excessive number of chromosome copies\nin genetics. Thus, identifying multiple change points in a long, possibly very\nlong, sequence is an important problem. In this article, we review both\nclassical and new multiple change-point detection strategies. Considering the\nlong history and the extensive literature on the change-point detection, we\nprovide an in-depth discussion on a normal mean change-point model from aspects\nof regression analysis, hypothesis testing, consistency and inference. In\nparticular, we present a strategy to gather and aggregate local information for\nchange-point detection that has become the cornerstone of several emerging\nmethods because of its attractiveness in both computational and theoretical\nproperties. \n\n"}
{"id": "1601.00863", "contents": "Title: Coordinate Friendly Structures, Algorithms and Applications Abstract: This paper focuses on coordinate update methods, which are useful for solving\nproblems involving large or high-dimensional datasets. They decompose a problem\ninto simple subproblems, where each updates one, or a small block of, variables\nwhile fixing others. These methods can deal with linear and nonlinear mappings,\nsmooth and nonsmooth functions, as well as convex and nonconvex problems. In\naddition, they are easy to parallelize.\n  The great performance of coordinate update methods depends on solving simple\nsub-problems. To derive simple subproblems for several new classes of\napplications, this paper systematically studies coordinate-friendly operators\nthat perform low-cost coordinate updates.\n  Based on the discovered coordinate friendly operators, as well as operator\nsplitting techniques, we obtain new coordinate update algorithms for a variety\nof problems in machine learning, image processing, as well as sub-areas of\noptimization. Several problems are treated with coordinate update for the first\ntime in history. The obtained algorithms are scalable to large instances\nthrough parallel and even asynchronous computing. We present numerical examples\nto illustrate how effective these algorithms are. \n\n"}
{"id": "1601.01142", "contents": "Title: Streaming Gibbs Sampling for LDA Model Abstract: Streaming variational Bayes (SVB) is successful in learning LDA models in an\nonline manner. However previous attempts toward developing online Monte-Carlo\nmethods for LDA have little success, often by having much worse perplexity than\ntheir batch counterparts. We present a streaming Gibbs sampling (SGS) method,\nan online extension of the collapsed Gibbs sampling (CGS). Our empirical study\nshows that SGS can reach similar perplexity as CGS, much better than SVB. Our\ndistributed version of SGS, DSGS, is much more scalable than SVB mainly because\nthe updates' communication complexity is small. \n\n"}
{"id": "1601.08130", "contents": "Title: Rigidity and reconstruction for graphs Abstract: We present measure theoretic rigidity for graphs of first Betti number b>1 in\nterms of measures on the boundary of a 2b-regular tree, that we make explicit\nin terms of the edge-adjacency and closed-walk structure of the graph. We prove\nthat edge-reconstruction of the entire graph is equivalent to that of the\n\"closed walk lengths\". \n\n"}
{"id": "1602.01557", "contents": "Title: An ensemble diversity approach to supervised binary hashing Abstract: Binary hashing is a well-known approach for fast approximate nearest-neighbor\nsearch in information retrieval. Much work has focused on affinity-based\nobjective functions involving the hash functions or binary codes. These\nobjective functions encode neighborhood information between data points and are\noften inspired by manifold learning algorithms. They ensure that the hash\nfunctions differ from each other through constraints or penalty terms that\nencourage codes to be orthogonal or dissimilar across bits, but this couples\nthe binary variables and complicates the already difficult optimization. We\npropose a much simpler approach: we train each hash function (or bit)\nindependently from each other, but introduce diversity among them using\ntechniques from classifier ensembles. Surprisingly, we find that not only is\nthis faster and trivially parallelizable, but it also improves over the more\ncomplex, coupled objective function, and achieves state-of-the-art precision\nand recall in experiments with image retrieval. \n\n"}
{"id": "1602.02282", "contents": "Title: Ladder Variational Autoencoders Abstract: Variational Autoencoders are powerful models for unsupervised learning.\nHowever deep models with several layers of dependent stochastic variables are\ndifficult to train which limits the improvements obtained using these highly\nexpressive models. We propose a new inference model, the Ladder Variational\nAutoencoder, that recursively corrects the generative distribution by a data\ndependent approximate likelihood in a process resembling the recently proposed\nLadder Network. We show that this model provides state of the art predictive\nlog-likelihood and tighter log-likelihood lower bound compared to the purely\nbottom-up inference in layered Variational Autoencoders and other generative\nmodels. We provide a detailed analysis of the learned hierarchical latent\nrepresentation and show that our new inference model is qualitatively different\nand utilizes a deeper more distributed hierarchy of latent variables. Finally,\nwe observe that batch normalization and deterministic warm-up (gradually\nturning on the KL-term) are crucial for training variational models with many\nstochastic layers. \n\n"}
{"id": "1602.02450", "contents": "Title: Loss factorization, weakly supervised learning and label noise\n  robustness Abstract: We prove that the empirical risk of most well-known loss functions factors\ninto a linear term aggregating all labels with a term that is label free, and\ncan further be expressed by sums of the loss. This holds true even for\nnon-smooth, non-convex losses and in any RKHS. The first term is a (kernel)\nmean operator --the focal quantity of this work-- which we characterize as the\nsufficient statistic for the labels. The result tightens known generalization\nbounds and sheds new light on their interpretation.\n  Factorization has a direct application on weakly supervised learning. In\nparticular, we demonstrate that algorithms like SGD and proximal methods can be\nadapted with minimal effort to handle weak supervision, once the mean operator\nhas been estimated. We apply this idea to learning with asymmetric noisy\nlabels, connecting and extending prior work. Furthermore, we show that most\nlosses enjoy a data-dependent (by the mean operator) form of noise robustness,\nin contrast with known negative results. \n\n"}
{"id": "1602.02466", "contents": "Title: Overfitting hidden Markov models with an unknown number of states Abstract: This paper presents new theory and methodology for the Bayesian estimation of\noverfitted hidden Markov models, with finite state space. The goal is then to\nachieve posterior emptying of extra states. A prior configuration is\nconstructed which favours configurations where the hidden Markov chain remains\nergodic although it empties out some of the states. Asymptotic posterior\nconvergence rates are proven theoretically, and demonstrated with a large\nsample simulation. The problem of overfitted HMMs is then considered in the\ncontext of smaller sample sizes, and due to computational and mixing issues two\nalternative prior structures are studied, one commonly used in practice, and a\nmixture of the two priors. The Prior Parallel Tempering approach of van Havre\n(2015) is also extended to HMMs to allow MCMC estimation of the complex\nposterior space. A replicate simulation study and an in-depth exploration is\nperformed to compare the three priors with hyperparameters chosen according to\nthe asymptotic constraints alongside less informative alternatives. \n\n"}
{"id": "1602.02681", "contents": "Title: Macdonald's solid-angle sum for real dilations of rational polygons Abstract: The solid-angle sum $A_{\\mathcal{P}} (t)$ of a rational polytope\n${\\mathcal{P}} \\subset \\mathbb{R}^d$, with $t \\in \\mathbb{Z}$ was first\ninvestigated by I.G. Macdonald. Using our Fourier-analytic methods, we are able\nto establish an explicit formula for $A_{\\mathcal{P}} (t)$, for any real\ndilation $t$ and any rational polygon ${\\mathcal{P}} \\subset \\mathbb{R}^2$. Our\nformulation sheds additional light on previous results, for lattice-point\nenumerating functions of triangles, which are usually confined to the case of\ninteger dilations. Our approach differs from that of Hardy and Littlewood in\n1992, but offers an alternate point of view for enumerating weighted lattice\npoints in real dilations of real triangles. \n\n"}
{"id": "1602.03574", "contents": "Title: A knockoff filter for high-dimensional selective inference Abstract: This paper develops a framework for testing for associations in a possibly\nhigh-dimensional linear model where the number of features/variables may far\nexceed the number of observational units. In this framework, the observations\nare split into two groups, where the first group is used to screen for a set of\npotentially relevant variables, whereas the second is used for inference over\nthis reduced set of variables; we also develop strategies for leveraging\ninformation from the first part of the data at the inference step for greater\npower. In our work, the inferential step is carried out by applying the\nrecently introduced knockoff filter, which creates a knockoff copy-a fake\nvariable serving as a control-for each screened variable. We prove that this\nprocedure controls the directional false discovery rate (FDR) in the reduced\nmodel controlling for all screened variables; this says that our\nhigh-dimensional knockoff procedure 'discovers' important variables as well as\nthe directions (signs) of their effects, in such a way that the expected\nproportion of wrongly chosen signs is below the user-specified level (thereby\ncontrolling a notion of Type S error averaged over the selected set). This\nresult is non-asymptotic, and holds for any distribution of the original\nfeatures and any values of the unknown regression coefficients, so that\ninference is not calibrated under hypothesized values of the effect sizes. We\ndemonstrate the performance of our general and flexible approach through\nnumerical studies, showing more power than existing alternatives. Finally, we\napply our method to a genome-wide association study to find locations on the\ngenome that are possibly associated with a continuous phenotype. \n\n"}
{"id": "1602.03807", "contents": "Title: Variational Inference for Sparse and Undirected Models Abstract: Undirected graphical models are applied in genomics, protein structure\nprediction, and neuroscience to identify sparse interactions that underlie\ndiscrete data. Although Bayesian methods for inference would be favorable in\nthese contexts, they are rarely used because they require doubly intractable\nMonte Carlo sampling. Here, we develop a framework for scalable Bayesian\ninference of discrete undirected models based on two new methods. The first is\nPersistent VI, an algorithm for variational inference of discrete undirected\nmodels that avoids doubly intractable MCMC and approximations of the partition\nfunction. The second is Fadeout, a reparameterization approach for variational\ninference under sparsity-inducing priors that captures a posteriori\ncorrelations between parameters and hyperparameters with noncentered\nparameterizations. We find that, together, these methods for variational\ninference substantially improve learning of sparse undirected graphical models\nin simulated and real problems from physics and biology. \n\n"}
{"id": "1602.05023", "contents": "Title: An introduction to sampling via measure transport Abstract: We present the fundamentals of a measure transport approach to sampling. The\nidea is to construct a deterministic coupling---i.e., a transport map---between\na complex \"target\" probability measure of interest and a simpler reference\nmeasure. Given a transport map, one can generate arbitrarily many independent\nand unweighted samples from the target simply by pushing forward reference\nsamples through the map. We consider two different and complementary scenarios:\nfirst, when only evaluations of the unnormalized target density are available,\nand second, when the target distribution is known only through a finite\ncollection of samples. We show that in both settings the desired transports can\nbe characterized as the solutions of variational problems. We then address\npractical issues associated with the optimization--based construction of\ntransports: choosing finite-dimensional parameterizations of the map, enforcing\nmonotonicity, quantifying the error of approximate transports, and refining\napproximate transports by enriching the corresponding approximation spaces.\nApproximate transports can also be used to \"Gaussianize\" complex distributions\nand thus precondition conventional asymptotically exact sampling schemes. We\nplace the measure transport approach in broader context, describing connections\nwith other optimization--based samplers, with inference and density estimation\nschemes using optimal transport, and with alternative transformation--based\napproaches to simulation. We also sketch current work aimed at the construction\nof transport maps in high dimensions, exploiting essential features of the\ntarget distribution (e.g., conditional independence, low-rank structure). The\napproaches and algorithms presented here have direct applications to Bayesian\ncomputation and to broader problems of stochastic simulation. \n\n"}
{"id": "1602.05125", "contents": "Title: Locally Stationary Functional Time Series Abstract: The literature on time series of functional data has focused on processes of\nwhich the probabilistic law is either constant over time or constant up to its\nsecond-order structure. Especially for long stretches of data it is desirable\nto be able to weaken this assumption. This paper introduces a framework that\nwill enable meaningful statistical inference of functional data of which the\ndynamics change over time. We put forward the concept of local stationarity in\nthe functional setting and establish a class of processes that have a\nfunctional time-varying spectral representation. Subsequently, we derive\nconditions that allow for fundamental results from nonstationary multivariate\ntime series to carry over to the function space. In particular, time-varying\nfunctional ARMA processes are investigated and shown to be functional locally\nstationary according to the proposed definition. As a side-result, we establish\na Cram\\'er representation for an important class of weakly stationary\nfunctional processes. Important in our context is the notion of a time-varying\nspectral density operator of which the properties are studied and uniqueness is\nderived. Finally, we provide a consistent nonparametric estimator of this\noperator and show it is asymptotically Gaussian using a weaker tightness\ncriterion than what is usually deemed necessary. \n\n"}
{"id": "1602.05455", "contents": "Title: Heterogeneity Adjustment with Applications to Graphical Model Inference Abstract: Heterogeneity is an unwanted variation when analyzing aggregated datasets\nfrom multiple sources. Though different methods have been proposed for\nheterogeneity adjustment, no systematic theory exists to justify these methods.\nIn this work, we propose a generic framework named ALPHA (short for Adaptive\nLow-rank Principal Heterogeneity Adjustment) to model, estimate, and adjust\nheterogeneity from the original data. Once the heterogeneity is adjusted, we\nare able to remove the biases of batch effects and to enhance the inferential\npower by aggregating the homogeneous residuals from multiple sources. Under a\npervasive assumption that the latent heterogeneity factors simultaneously\naffect a large fraction of observed variables, we provide a rigorous theory to\njustify the proposed framework. Our framework also allows the incorporation of\ninformative covariates and appeals to the \"Bless of Dimensionality\". As an\nillustrative application of this generic framework, we consider a problem of\nestimating high-dimensional precision matrix for graphical model inference\nbased on multiple datasets. We also provide thorough numerical studies on both\nsynthetic datasets and a brain imaging dataset to demonstrate the efficacy of\nthe developed theory and methods. \n\n"}
{"id": "1602.07630", "contents": "Title: Online Dual Coordinate Ascent Learning Abstract: The stochastic dual coordinate-ascent (S-DCA) technique is a useful\nalternative to the traditional stochastic gradient-descent algorithm for\nsolving large-scale optimization problems due to its scalability to large data\nsets and strong theoretical guarantees. However, the available S-DCA\nformulation is limited to finite sample sizes and relies on performing multiple\npasses over the same data. This formulation is not well-suited for online\nimplementations where data keep streaming in. In this work, we develop an {\\em\nonline} dual coordinate-ascent (O-DCA) algorithm that is able to respond to\nstreaming data and does not need to revisit the past data. This feature embeds\nthe resulting construction with continuous adaptation, learning, and tracking\nabilities, which are particularly attractive for online learning scenarios. \n\n"}
{"id": "1602.07839", "contents": "Title: Tight bounds on discrete quantitative Helly numbers Abstract: Given a subset S of R^n, let c(S,k) be the smallest number t such that\nwhenever finitely many convex sets have exactly k common points in S, there\nexist at most t of these sets that already have exactly k common points in S.\nFor S = Z^n, this number was introduced by Aliev et al. [2014] who gave an\nexplicit bound showing that c(Z^n,k) = O(k) holds for every fixed n. Recently,\nChestnut et al. [2015] improved this to c(Z^n,k) = O(k (log log k)(log\nk)^{-1/3} ) and provided the lower bound c(Z^n,k) = Omega(k^{(n-1)/(n+1)}).\n  We provide a combinatorial description of c(S,k) in terms of polytopes with\nvertices in S and use it to improve the previously known bounds as follows: We\nstrengthen the bound of Aliev et al. [2014] by a constant factor and extend it\nto general discrete sets S. We close the gap for Z^n by showing that c(Z^n,k) =\nTheta(k^{(n-1)/(n+1)}) holds for every fixed n. Finally, we determine the exact\nvalues of c(Z^n,k) for all k <= 4. \n\n"}
{"id": "1602.08593", "contents": "Title: Fourier transforms of polytopes, solid angle sums, and discrete volume Abstract: Given a real closed polytope $P$, we first describe the Fourier transform of\nits indicator function by using iterations of Stokes' theorem. We then use the\nensuing Fourier transform formulations, together with the Poisson summation\nformula, to give a new algorithm to count fractionally-weighted lattice points\ninside the one-parameter family of all real dilates of $P$. The combinatorics\nof the face poset of $P$ plays a central role in the description of the Fourier\ntransform of $P$. We also obtain a closed form for the codimension-1\ncoefficient that appears in an expansion of this sum in powers of the real\ndilation parameter $t$. This closed form generalizes some known results about\nthe Macdonald solid-angle polynomial, which is the analogous expression\ntraditionally obtained by requiring that $t$ assumes only integer values.\nAlthough most of the present methodology applies to all real polytopes, a\nparticularly nice application is to the study of all real dilates of integer\n(and rational) polytopes. \n\n"}
{"id": "1603.01136", "contents": "Title: Multilevel Sequential Monte Carlo Samplers for Normalizing Constants Abstract: This article considers the sequential Monte Carlo (SMC) approximation of\nratios of normalizing constants associated to posterior distributions which in\nprinciple rely on continuum models. Therefore, the Monte Carlo estimation error\nand the discrete approximation error must be balanced. A multilevel strategy is\nutilized to substantially reduce the cost to obtain a given error level in the\napproximation as compared to standard estimators. Two estimators are considered\nand relative variance bounds are given. The theoretical results are numerically\nillustrated for the example of identifying a parametrized permeability in an\nelliptic equation given point-wise observations of the pressure. \n\n"}
{"id": "1603.02834", "contents": "Title: Inference and rare event simulation for stopped Markov processes via\n  reverse-time sequential Monte Carlo Abstract: We present a sequential Monte Carlo algorithm for Markov chain trajectories\nwith proposals constructed in reverse time, which is advantageous when paths\nare conditioned to end in a rare set. The reverse time proposal distribution is\nconstructed by approximating the ratio of Green's functions in Nagasawa's\nformula. Conditioning arguments can be used to interpret these ratios as\nlow-dimensional conditional sampling distributions of some coordinates of the\nprocess given the others. Hence the difficulty in designing SMC proposals in\nhigh dimension is greatly reduced. We illustrate our method on estimating an\noverflow probability in a queueing model, the probability that a diffusion\nfollows a narrowing corridor, and the initial location of an infection in an\nepidemic model on a network. \n\n"}
{"id": "1603.05296", "contents": "Title: Exact Clustering of Weighted Graphs via Semidefinite Programming Abstract: As a model problem for clustering, we consider the densest k-disjoint-clique\nproblem of partitioning a weighted complete graph into k disjoint subgraphs\nsuch that the sum of the densities of these subgraphs is maximized. We\nestablish that such subgraphs can be recovered from the solution of a\nparticular semidefinite relaxation with high probability if the input graph is\nsampled from a distribution of clusterable graphs. Specifically, the\nsemidefinite relaxation is exact if the graph consists of k large disjoint\nsubgraphs, corresponding to clusters, with weight concentrated within these\nsubgraphs, plus a moderate number of outliers. Further, we establish that if\nnoise is weakly obscuring these clusters, i.e, the between-cluster edges are\nassigned very small weights, then we can recover significantly smaller\nclusters. For example, we show that in approximately sparse graphs, where the\nbetween-cluster weights tend to zero as the size n of the graph tends to\ninfinity, we can recover clusters of size polylogarithmic in n. Empirical\nevidence from numerical simulations is also provided to support these\ntheoretical phase transitions to perfect recovery of the cluster structure. \n\n"}
{"id": "1603.08232", "contents": "Title: The block-Poisson estimator for optimally tuned exact subsampling MCMC Abstract: Speeding up Markov Chain Monte Carlo (MCMC) for datasets with many\nobservations by data subsampling has recently received considerable attention.\nA pseudo-marginal MCMC method is proposed that estimates the likelihood by data\nsubsampling using a block-Poisson estimator. The estimator is a product of\nPoisson estimators, allowing us to update a single block of subsample\nindicators in each MCMC iteration so that a desired correlation is achieved\nbetween the logs of successive likelihood estimates. This is important since\npseudo-marginal MCMC with positively correlated likelihood estimates can use\nsubstantially smaller subsamples without adversely affecting the sampling\nefficiency. The block-Poisson estimator is unbiased but not necessarily\npositive, so the algorithm runs the MCMC on the absolute value of the\nlikelihood estimator and uses an importance sampling correction to obtain\nconsistent estimates of the posterior mean of any function of the parameters.\nOur article derives guidelines to select the optimal tuning parameters for our\nmethod and shows that it compares very favourably to regular MCMC without\nsubsampling, and to two other recently proposed exact subsampling approaches in\nthe literature. \n\n"}
{"id": "1604.00872", "contents": "Title: Geometrically Tempered Hamiltonian Monte Carlo Abstract: Hamiltonian Monte Carlo (HMC) has become routinely used for sampling from\nposterior distributions. Its extension Riemann manifold HMC (RMHMC) modifies\nthe proposal kernel through distortion of local distances by a Riemannian\nmetric. The performance depends critically on the choice of metric, with the\nFisher information providing the standard choice. In this article, we propose a\nnew class of metrics aimed at improving HMC's performance on multi-modal target\ndistributions. We refer to the proposed approach as geometrically tempered HMC\n(GTHMC) due to its connection to other tempering methods. We establish a\ngeometric theory behind RMHMC to motivate GTHMC and characterize its\ntheoretical properties. Moreover, we develop a novel variable step size\nintegrator for simulating Hamiltonian dynamics to improve on the usual\nSt\\\"{o}rmer-Verlet integrator which suffers from numerical instability in GTHMC\nsettings. We illustrate GTHMC through simulations, demonstrating generality and\nsubstantial gains over standard HMC implementations in terms of effective\nsample sizes. \n\n"}
{"id": "1604.03192", "contents": "Title: Scalar-on-Image Regression via the Soft-Thresholded Gaussian Process Abstract: The focus of this work is on spatial variable selection for scalar-on-image\nregression. We propose a new class of Bayesian nonparametric models,\nsoft-thresholded Gaussian processes and develop the efficient posterior\ncomputation algorithms. Theoretically, soft-thresholded Gaussian processes\nprovide large prior support for the spatially varying coefficients that enjoy\npiecewise smoothness, sparsity and continuity, characterizing the important\nfeatures of imaging data. Also, under some mild regularity conditions, the\nsoft-thresholded Gaussian process leads to the posterior consistency for both\nparameter estimation and variable selection for scalar-on-image regression,\neven when the number of true predictors is larger than the sample size. The\nproposed method is illustrated via simulations, compared numerically with\nexisting alternatives and applied to Electroencephalography (EEG) study of\nalcoholism. \n\n"}
{"id": "1604.07087", "contents": "Title: Optimal Estimation of Slope Vector in High-dimensional Linear\n  Transformation Model Abstract: In a linear transformation model, there exists an unknown monotone nonlinear\ntransformation function such that the transformed response variable and the\npredictor variables satisfy a linear regression model. In this paper, we\npresent CENet, a new method for estimating the slope vector and simultaneously\nperforming variable selection in the high-dimensional sparse linear\ntransformation model. CENet is the solution to a convex optimization problem\nand can be computed efficiently from an algorithm with guaranteed convergence\nto the global optimum. We show that under a pairwise elliptical distribution\nassumption on each predictor-transformed-response pair and some regularity\nconditions, CENet attains the same optimal rate of convergence as the best\nregression method in the high-dimensional sparse linear regression model. To\nthe best of our limited knowledge, this is the first such result in the\nliterature. We demonstrate the empirical performance of CENet on both simulated\nand real datasets. We also discuss the connection of CENet with some nonlinear\nregression/multivariate methods proposed in the literature. \n\n"}
{"id": "1604.07706", "contents": "Title: Distributed Clustering of Linear Bandits in Peer to Peer Networks Abstract: We provide two distributed confidence ball algorithms for solving linear\nbandit problems in peer to peer networks with limited communication\ncapabilities. For the first, we assume that all the peers are solving the same\nlinear bandit problem, and prove that our algorithm achieves the optimal\nasymptotic regret rate of any centralised algorithm that can instantly\ncommunicate information between the peers. For the second, we assume that there\nare clusters of peers solving the same bandit problem within each cluster, and\nwe prove that our algorithm discovers these clusters, while achieving the\noptimal asymptotic regret rate within each one. Through experiments on several\nreal-world datasets, we demonstrate the performance of proposed algorithms\ncompared to the state-of-the-art. \n\n"}
{"id": "1604.08153", "contents": "Title: Classifying Options for Deep Reinforcement Learning Abstract: In this paper we combine one method for hierarchical reinforcement learning -\nthe options framework - with deep Q-networks (DQNs) through the use of\ndifferent \"option heads\" on the policy network, and a supervisory network for\nchoosing between the different options. We utilise our setup to investigate the\neffects of architectural constraints in subtasks with positive and negative\ntransfer, across a range of network capacities. We empirically show that our\naugmented DQN has lower sample complexity when simultaneously learning subtasks\nwith negative transfer, without degrading performance when learning subtasks\nwith positive transfer. \n\n"}
{"id": "1604.08859", "contents": "Title: The Z-loss: a shift and scale invariant classification loss belonging to\n  the Spherical Family Abstract: Despite being the standard loss function to train multi-class neural\nnetworks, the log-softmax has two potential limitations. First, it involves\ncomputations that scale linearly with the number of output classes, which can\nrestrict the size of problems we are able to tackle with current hardware.\nSecond, it remains unclear how close it matches the task loss such as the top-k\nerror rate or other non-differentiable evaluation metrics which we aim to\noptimize ultimately. In this paper, we introduce an alternative classification\nloss function, the Z-loss, which is designed to address these two issues.\nUnlike the log-softmax, it has the desirable property of belonging to the\nspherical loss family (Vincent et al., 2015), a class of loss functions for\nwhich training can be performed very efficiently with a complexity independent\nof the number of output classes. We show experimentally that it significantly\noutperforms the other spherical loss functions previously investigated.\nFurthermore, we show on a word language modeling task that it also outperforms\nthe log-softmax with respect to certain ranking scores, such as top-k scores,\nsuggesting that the Z-loss has the flexibility to better match the task loss.\nThese qualities thus makes the Z-loss an appealing candidate to train very\nefficiently large output networks such as word-language models or other extreme\nclassification problems. On the One Billion Word (Chelba et al., 2014) dataset,\nwe are able to train a model with the Z-loss 40 times faster than the\nlog-softmax and more than 4 times faster than the hierarchical softmax. \n\n"}
{"id": "1605.00031", "contents": "Title: Deep Convolutional Neural Networks on Cartoon Functions Abstract: Wiatowski and B\\\"olcskei, 2015, proved that deformation stability and\nvertical translation invariance of deep convolutional neural network-based\nfeature extractors are guaranteed by the network structure per se rather than\nthe specific convolution kernels and non-linearities. While the translation\ninvariance result applies to square-integrable functions, the deformation\nstability bound holds for band-limited functions only. Many signals of\npractical relevance (such as natural images) exhibit, however, sharp and curved\ndiscontinuities and are, hence, not band-limited. The main contribution of this\npaper is a deformation stability result that takes these structural properties\ninto account. Specifically, we establish deformation stability bounds for the\nclass of cartoon functions introduced by Donoho, 2001. \n\n"}
{"id": "1605.00278", "contents": "Title: Particle Smoothing for Hidden Diffusion Processes: Adaptive Path\n  Integral Smoother Abstract: Particle smoothing methods are used for inference of stochastic processes\nbased on noisy observations. Typically, the estimation of the marginal\nposterior distribution given all observations is cumbersome and computational\nintensive. In this paper, we propose a simple algorithm based on path integral\ncontrol theory to estimate the smoothing distribution of continuous-time\ndiffusion processes with partial observations. In particular, we use an\nadaptive importance sampling method to improve the effective sampling size of\nthe posterior over processes given the observations and the reliability of the\nestimation of the marginals. This is achieved by estimating a feedback\ncontroller to sample efficiently from the joint smoothing distributions. We\ncompare the results with estimations obtained from the standard Forward\nFilter/Backward Simulator for two diffusion processes of different complexity.\nWe show that the proposed method gives more reliable estimations than the\nstandard FFBSi when the smoothing distribution is poorly represented by the\nfilter distribution. \n\n"}
{"id": "1605.01116", "contents": "Title: An evaluation of randomized machine learning methods for redundant data:\n  Predicting short and medium-term suicide risk from administrative records and\n  risk assessments Abstract: Accurate prediction of suicide risk in mental health patients remains an open\nproblem. Existing methods including clinician judgments have acceptable\nsensitivity, but yield many false positives. Exploiting administrative data has\na great potential, but the data has high dimensionality and redundancies in the\nrecording processes. We investigate the efficacy of three most effective\nrandomized machine learning techniques random forests, gradient boosting\nmachines, and deep neural nets with dropout in predicting suicide risk. Using a\ncohort of mental health patients from a regional Australian hospital, we\ncompare the predictive performance with popular traditional approaches\nclinician judgments based on a checklist, sparse logistic regression and\ndecision trees. The randomized methods demonstrated robustness against data\nredundancies and superior predictive performance on AUC and F-measure. \n\n"}
{"id": "1605.02105", "contents": "Title: Distributed Learning with Infinitely Many Hypotheses Abstract: We consider a distributed learning setup where a network of agents\nsequentially access realizations of a set of random variables with unknown\ndistributions. The network objective is to find a parametrized distribution\nthat best describes their joint observations in the sense of the\nKullback-Leibler divergence. Apart from recent efforts in the literature, we\nanalyze the case of countably many hypotheses and the case of a continuum of\nhypotheses. We provide non-asymptotic bounds for the concentration rate of the\nagents' beliefs around the correct hypothesis in terms of the number of agents,\nthe network parameters, and the learning abilities of the agents. Additionally,\nwe provide a novel motivation for a general set of distributed Non-Bayesian\nupdate rules as instances of the distributed stochastic mirror descent\nalgorithm. \n\n"}
{"id": "1605.02366", "contents": "Title: A zonotope and a product of two simplices with disconnected flip graphs Abstract: We give an example of a three-dimensional zonotope whose set of tight\nzonotopal tilings is not connected by flips. Using this, we show that the set\nof triangulations of $\\Delta^4 \\times \\Delta^n$ is not connected by flips for\nlarge $n$. Our proof makes use of a non-explicit probabilistic construction. \n\n"}
{"id": "1605.04039", "contents": "Title: Cross-Domain Visual Matching via Generalized Similarity Measure and\n  Feature Learning Abstract: Cross-domain visual data matching is one of the fundamental problems in many\nreal-world vision tasks, e.g., matching persons across ID photos and\nsurveillance videos. Conventional approaches to this problem usually involves\ntwo steps: i) projecting samples from different domains into a common space,\nand ii) computing (dis-)similarity in this space based on a certain distance.\nIn this paper, we present a novel pairwise similarity measure that advances\nexisting models by i) expanding traditional linear projections into affine\ntransformations and ii) fusing affine Mahalanobis distance and Cosine\nsimilarity by a data-driven combination. Moreover, we unify our similarity\nmeasure with feature representation learning via deep convolutional neural\nnetworks. Specifically, we incorporate the similarity measure matrix into the\ndeep architecture, enabling an end-to-end way of model optimization. We\nextensively evaluate our generalized similarity model in several challenging\ncross-domain matching tasks: person re-identification under different views and\nface verification over different modalities (i.e., faces from still images and\nvideos, older and younger faces, and sketch and photo portraits). The\nexperimental results demonstrate superior performance of our model over other\nstate-of-the-art methods. \n\n"}
{"id": "1605.04465", "contents": "Title: Monotone Retargeting for Unsupervised Rank Aggregation with Object\n  Features Abstract: Learning the true ordering between objects by aggregating a set of expert\nopinion rank order lists is an important and ubiquitous problem in many\napplications ranging from social choice theory to natural language processing\nand search aggregation. We study the problem of unsupervised rank aggregation\nwhere no ground truth ordering information in available, neither about the true\npreference ordering between any set of objects nor about the quality of\nindividual rank lists. Aggregating the often inconsistent and poor quality rank\nlists in such an unsupervised manner is a highly challenging problem, and\nstandard consensus-based methods are often ill-defined, and difficult to solve.\nIn this manuscript we propose a novel framework to bypass these issues by using\nobject attributes to augment the standard rank aggregation framework. We design\nalgorithms that learn joint models on both rank lists and object features to\nobtain an aggregated rank ordering that is more accurate and robust, and also\nhelps weed out rank lists of dubious validity. We validate our techniques on\nsynthetic datasets where our algorithm is able to estimate the true rank\nordering even when the rank lists are corrupted. Experiments on three real\ndatasets, MQ2008, MQ2008 and OHSUMED, show that using object features can\nresult in significant improvement in performance over existing rank aggregation\nmethods that do not use object information. Furthermore, when at least some of\nthe rank lists are of high quality, our methods are able to effectively exploit\ntheir high expertise to output an aggregated rank ordering of great accuracy. \n\n"}
{"id": "1605.04565", "contents": "Title: Hierarchical Models for Independence Structures of Networks Abstract: We introduce a new family of network models, called hierarchical network\nmodels, that allow us to represent in an explicit manner the stochastic\ndependence among the dyads (random ties) of the network. In particular, each\nmember of this family can be associated with a graphical model defining\nconditional independence clauses among the dyads of the network, called the\ndependency graph. Every network model with dyadic independence assumption can\nbe generalized to construct members of this new family. Using this new\nframework, we generalize the Erd\\\"os-R\\'enyi and beta-models to create\nhierarchical Erd\\\"os-R\\'enyi and beta-models. We describe various methods for\nparameter estimation as well as simulation studies for models with sparse\ndependency graphs. \n\n"}
{"id": "1605.04963", "contents": "Title: Multilevel Particle Filters: Normalizing Constant Estimation Abstract: In this article we introduce two new estimates of the normalizing constant\n(or marginal likelihood) for partially observed diffusion (POD) processes, with\ndiscrete observations. One estimate is biased but non-negative and the other is\nunbiased but not almost surely non-negative. Our method uses the multilevel\nparticle filter of Jasra et al (2015). We show that, under assumptions, for\nEuler discretized PODs and a given $\\varepsilon>0$. in order to obtain a mean\nsquare error (MSE) of $\\mathcal{O}(\\varepsilon^2)$ one requires a work of\n$\\mathcal{O}(\\varepsilon^{-2.5})$ for our new estimates versus a standard\nparticle filter that requires a work of $\\mathcal{O}(\\varepsilon^{-3})$. Our\ntheoretical results are supported by numerical simulations. \n\n"}
{"id": "1605.05537", "contents": "Title: ABC random forests for Bayesian parameter inference Abstract: This preprint has been reviewed and recommended by Peer Community In\nEvolutionary Biology (http://dx.doi.org/10.24072/pci.evolbiol.100036).\nApproximate Bayesian computation (ABC) has grown into a standard methodology\nthat manages Bayesian inference for models associated with intractable\nlikelihood functions. Most ABC implementations require the preliminary\nselection of a vector of informative statistics summarizing raw data.\nFurthermore, in almost all existing implementations, the tolerance level that\nseparates acceptance from rejection of simulated parameter values needs to be\ncalibrated. We propose to conduct likelihood-free Bayesian inferences about\nparameters with no prior selection of the relevant components of the summary\nstatistics and bypassing the derivation of the associated tolerance level. The\napproach relies on the random forest methodology of Breiman (2001) applied in a\n(non parametric) regression setting. We advocate the derivation of a new random\nforest for each component of the parameter vector of interest. When compared\nwith earlier ABC solutions, this method offers significant gains in terms of\nrobustness to the choice of the summary statistics, does not depend on any type\nof tolerance level, and is a good trade-off in term of quality of point\nestimator precision and credible interval estimations for a given computing\ntime. We illustrate the performance of our methodological proposal and compare\nit with earlier ABC methods on a Normal toy example and a population genetics\nexample dealing with human population evolution. All methods designed here have\nbeen incorporated in the R package abcrf (version 1.7) available on CRAN. \n\n"}
{"id": "1605.06181", "contents": "Title: Variational hybridization and transformation for large inaccurate\n  noisy-or networks Abstract: Variational inference provides approximations to the computationally\nintractable posterior distribution in Bayesian networks. A prominent medical\napplication of noisy-or Bayesian network is to infer potential diseases given\nobserved symptoms. Previous studies focus on approximating a handful of\ncomplicated pathological cases using variational transformation. Our goal is to\nuse variational transformation as part of a novel hybridized inference for\nserving reliable and real time diagnosis at web scale. We propose a hybridized\ninference that allows variational parameters to be estimated without disease\nposteriors or priors, making the inference faster and much of its computation\nrecyclable. In addition, we propose a transformation ranking algorithm that is\nvery stable to large variances in network prior probabilities, a common issue\nthat arises in medical applications of Bayesian networks. In experiments, we\nperform comparative study on a large real life medical network and scalability\nstudy on a much larger (36,000x) synthesized network. \n\n"}
{"id": "1605.07244", "contents": "Title: Optimal Estimation of Co-heritability in High-dimensional Linear Models Abstract: Co-heritability is an important concept that characterizes the genetic\nassociations within pairs of quantitative traits. There has been significant\nrecent interest in estimating the co-heritability based on data from the\ngenome-wide association studies (GWAS). This paper introduces two measures of\nco-heritability in the high-dimensional linear model framework, including the\ninner product of the two regression vectors and a normalized inner product by\ntheir lengths. Functional de-biased estimators (FDEs) are developed to estimate\nthese two co-heritability measures. In addition, estimators of quadratic\nfunctionals of the regression vectors are proposed. Both theoretical and\nnumerical properties of the estimators are investigated. In particular, minimax\nrates of convergence are established and the proposed estimators of the inner\nproduct, the quadratic functionals and the normalized inner product are shown\nto be rate-optimal. Simulation results show that the FDEs significantly\noutperform the naive plug-in estimates. The FDEs are also applied to analyze a\nyeast segregant data set with multiple traits to estimate heritability and\nco-heritability among the traits. \n\n"}
{"id": "1605.07604", "contents": "Title: Posterior Dispersion Indices Abstract: Probabilistic modeling is cyclical: we specify a model, infer its posterior,\nand evaluate its performance. Evaluation drives the cycle, as we revise our\nmodel based on how it performs. This requires a metric. Traditionally,\npredictive accuracy prevails. Yet, predictive accuracy does not tell the whole\nstory. We propose to evaluate a model through posterior dispersion. The idea is\nto analyze how each datapoint fares in relation to posterior uncertainty around\nthe hidden structure. We propose a family of posterior dispersion indices (PDI)\nthat capture this idea. A PDI identifies rich patterns of model mismatch in\nthree real data examples: voting preferences, supermarket shopping, and\npopulation genetics. \n\n"}
{"id": "1605.08346", "contents": "Title: Distributed Sequence Memory of Multidimensional Inputs in Recurrent\n  Networks Abstract: Recurrent neural networks (RNNs) have drawn interest from machine learning\nresearchers because of their effectiveness at preserving past inputs for\ntime-varying data processing tasks. To understand the success and limitations\nof RNNs, it is critical that we advance our analysis of their fundamental\nmemory properties. We focus on echo state networks (ESNs), which are RNNs with\nsimple memoryless nodes and random connectivity. In most existing analyses, the\nshort-term memory (STM) capacity results conclude that the ESN network size\nmust scale linearly with the input size for unstructured inputs. The main\ncontribution of this paper is to provide general results characterizing the STM\ncapacity for linear ESNs with multidimensional input streams when the inputs\nhave common low-dimensional structure: sparsity in a basis or significant\nstatistical dependence between inputs. In both cases, we show that the number\nof nodes in the network must scale linearly with the information rate and\npoly-logarithmically with the ambient input dimension. The analysis relies on\nadvanced applications of random matrix theory and results in explicit\nnon-asymptotic bounds on the recovery error. Taken together, this analysis\nprovides a significant step forward in our understanding of the STM properties\nin RNNs. \n\n"}
{"id": "1605.08375", "contents": "Title: Generalization Properties and Implicit Regularization for Multiple\n  Passes SGM Abstract: We study the generalization properties of stochastic gradient methods for\nlearning with convex loss functions and linearly parameterized functions. We\nshow that, in the absence of penalizations or constraints, the stability and\napproximation properties of the algorithm can be controlled by tuning either\nthe step-size or the number of passes over the data. In this view, these\nparameters can be seen to control a form of implicit regularization. Numerical\nresults complement the theoretical findings. \n\n"}
{"id": "1605.08961", "contents": "Title: A simple and provable algorithm for sparse diagonal CCA Abstract: Given two sets of variables, derived from a common set of samples, sparse\nCanonical Correlation Analysis (CCA) seeks linear combinations of a small\nnumber of variables in each set, such that the induced canonical variables are\nmaximally correlated. Sparse CCA is NP-hard.\n  We propose a novel combinatorial algorithm for sparse diagonal CCA, i.e.,\nsparse CCA under the additional assumption that variables within each set are\nstandardized and uncorrelated. Our algorithm operates on a low rank\napproximation of the input data and its computational complexity scales\nlinearly with the number of input variables. It is simple to implement, and\nparallelizable. In contrast to most existing approaches, our algorithm\nadministers precise control on the sparsity of the extracted canonical vectors,\nand comes with theoretical data-dependent global approximation guarantees, that\nhinge on the spectrum of the input data. Finally, it can be straightforwardly\nadapted to other constrained variants of CCA enforcing structure beyond\nsparsity.\n  We empirically evaluate the proposed scheme and apply it on a real\nneuroimaging dataset to investigate associations between brain activity and\nbehavior measurements. \n\n"}
{"id": "1605.09445", "contents": "Title: An estimator for Poisson means whose relative error distribution is\n  known Abstract: Suppose that $X_1,X_2,\\ldots$ are a stream of independent, identically\ndistributed Poisson random variables with mean $\\mu$. This work presents a new\nestimate $\\mu_k$ for $\\mu$ with the property that the distribution of the\nrelative error in the estimate ($(\\hat \\mu_k/\\mu) - 1$) is known, and does not\ndepend on $\\mu$ in any way. This enables the construction of simple exact\nconfidence intervals for the estimate, as well as a means of obtaining fast\napproximation algorithms for high dimensional integration using TPA. The new\nestimate requires a random number of Poisson draws, and so is best suited to\nMonte Carlo applications. As an example of such an application, the method is\napplied to obtain an exact confidence interval for the normalizing constant of\nthe Ising model. \n\n"}
{"id": "1606.00068", "contents": "Title: Quantifying the probable approximation error of probabilistic inference\n  programs Abstract: This paper introduces a new technique for quantifying the approximation error\nof a broad class of probabilistic inference programs, including ones based on\nboth variational and Monte Carlo approaches. The key idea is to derive a\nsubjective bound on the symmetrized KL divergence between the distribution\nachieved by an approximate inference program and its true target distribution.\nThe bound's validity (and subjectivity) rests on the accuracy of two auxiliary\nprobabilistic programs: (i) a \"reference\" inference program that defines a gold\nstandard of accuracy and (ii) a \"meta-inference\" program that answers the\nquestion \"what internal random choices did the original approximate inference\nprogram probably make given that it produced a particular result?\" The paper\nincludes empirical results on inference problems drawn from linear regression,\nDirichlet process mixture modeling, HMMs, and Bayesian networks. The\nexperiments show that the technique is robust to the quality of the reference\ninference program and that it can detect implementation bugs that are not\napparent from predictive performance. \n\n"}
{"id": "1606.01016", "contents": "Title: On Coupling Particle Filter Trajectories Abstract: Particle filters are a powerful and flexible tool for performing inference on\nstate-space models. They involve a collection of samples evolving over time\nthrough a combination of sampling and re-sampling steps. The re-sampling step\nis necessary to ensure that weight degeneracy is avoided. In several situations\nof statistical interest, it is important to be able to compare the estimates\nproduced by two different particle filters; consequently, being able to\nefficiently couple two particle filter trajectories is often of paramount\nimportance. In this text, we propose several ways to do so. In particular, we\nleverage ideas from the optimal transportation literature. In general, though,\ncomputing the optimal transport map is extremely computationally expensive; to\ndeal with this, we introduce computationally tractable approximations to\noptimal transport couplings. We demonstrate that our resulting algorithms for\ncoupling two particle filter trajectories often perform orders of magnitude\nmore efficiently than more standard approaches. \n\n"}
{"id": "1606.01156", "contents": "Title: Coupling of Particle Filters Abstract: Particle filters provide Monte Carlo approximations of intractable quantities\nsuch as point-wise evaluations of the likelihood in state space models. In many\nscenarios, the interest lies in the comparison of these quantities as some\nparameter or input varies. To facilitate such comparisons, we introduce and\nstudy methods to couple two particle filters in such a way that the correlation\nbetween the two underlying particle systems is increased. The motivation stems\nfrom the classic variance reduction technique of positively correlating two\nestimators. The key challenge in constructing such a coupling stems from the\ndiscontinuity of the resampling step of the particle filter. As our first\ncontribution, we consider coupled resampling algorithms. Within bootstrap\nparticle filters, they improve the precision of finite-difference estimators of\nthe score vector and boost the performance of particle marginal\nMetropolis--Hastings algorithms for parameter inference. The second\ncontribution arises from the use of these coupled resampling schemes within\nconditional particle filters, allowing for unbiased estimators of smoothing\nfunctionals. The result is a new smoothing strategy that operates by averaging\na number of independent and unbiased estimators, which allows for 1)\nstraightforward parallelization and 2) the construction of accurate error\nestimates. Neither of the above is possible with existing particle smoothers. \n\n"}
{"id": "1606.02679", "contents": "Title: Learning Power Spectrum Maps from Quantized Power Measurements Abstract: Power spectral density (PSD) maps providing the distribution of RF power\nacross space and frequency are constructed using power measurements collected\nby a network of low-cost sensors. By introducing linear compression and\nquantization to a small number of bits, sensor measurements can be communicated\nto the fusion center with minimal bandwidth requirements. Strengths of data-\nand model-driven approaches are combined to develop estimators capable of\nincorporating multiple forms of spectral and propagation prior information\nwhile fitting the rapid variations of shadow fading across space. To this end,\nnovel nonparametric and semiparametric formulations are investigated. It is\nshown that PSD maps can be obtained using support vector machine-type solvers.\nIn addition to batch approaches, an online algorithm attuned to real-time\noperation is developed. Numerical tests assess the performance of the novel\nalgorithms. \n\n"}
{"id": "1606.02931", "contents": "Title: Bayesian Estimation and Comparison of Moment Condition Models Abstract: In this paper we consider the problem of inference in statistical models\ncharacterized by moment restrictions by casting the problem within the\nExponentially Tilted Empirical Likelihood (ETEL) framework. Because the ETEL\nfunction has a well defined probabilistic interpretation and plays the role of\na nonparametric likelihood, a fully Bayesian semiparametric framework can be\ndeveloped. We establish a number of powerful results surrounding the Bayesian\nETEL framework in such models. One major concern driving our work is the\npossibility of misspecification. To accommodate this possibility, we show how\nthe moment conditions can be reexpressed in terms of additional nuisance\nparameters and that, even under misspecification, the Bayesian ETEL posterior\ndistribution satisfies a Bernstein-von Mises result. A second key contribution\nof the paper is the development of a framework based on marginal likelihoods\nand Bayes factors to compare models defined by different moment conditions.\nComputation of the marginal likelihoods is by the method of Chib (1995) as\nextended to Metropolis-Hastings samplers in Chib and Jeliazkov (2001). We\nestablish the model selection consistency of the marginal likelihood and show\nthat the marginal likelihood favors the model with the minimum number of\nparameters and the maximum number of valid moment restrictions. When the models\nare misspecified, the marginal likelihood model selection procedure selects the\nmodel that is closer to the (unknown) true data generating process in terms of\nthe Kullback-Leibler divergence. The ideas and results in this paper provide a\nfurther broadening of the theoretical underpinning and value of the Bayesian\nETEL framework with likely far-reaching practical consequences. The discussion\nis illuminated through several examples. \n\n"}
{"id": "1606.03259", "contents": "Title: New Upper Bounds for Equiangular Lines by Pillar Decomposition Abstract: We derive a procedure for computing an upper bound on the number of\nequiangular lines in various Euclidean vector spaces by generalizing the\nclassical pillar decomposition developed by (Lemmens and Seidel, 1973); namely,\nwe use linear algebra and combinatorial arguments to bound the number of\nvectors within an equiangular set which have inner products of certain signs\nwith a negative clique. After projection and rescaling, such sets are also\ncertain spherical two-distance sets, and semidefinite programming techniques\nmay be used to bound the size. Applying our method, we prove new relative\nbounds for the angle arccos(1/5). Experiments show that our relative bounds for\nall possible angles are considerably less than the known SDP bounds for a range\nof larger dimension r. Our computational results also show an explicit bound on\nthe size of a set of equiangular lines regardless of angle, which is strictly\nless than the well-known Gerzon's bound if r+2 is not a square of an odd\nnumber. \n\n"}
{"id": "1606.04938", "contents": "Title: Two double poset polytopes Abstract: To every poset P, Stanley (1986) associated two polytopes, the order polytope\nand the chain polytope, whose geometric properties reflect the combinatorial\nqualities of P. This construction allows for deep insights into combinatorics\nby way of geometry and vice versa. Malvenuto and Reutenauer (2011) introduced\n'double posets', that is, (finite) sets equipped with two partial orders, as a\ngeneralization of Stanley's labelled posets. Many combinatorial constructions\ncan be naturally phrased in terms of double posets. We introduce the 'double\norder polytope' and the 'double chain polytope' and we amply demonstrate that\nthey geometrically capture double posets, i.e., the interaction between the two\npartial orders. We describe the facial structures, Ehrhart polynomials, and\nvolumes of these polytopes in terms of the combinatorics of double posets. We\nalso describe a curious connection to Geissinger's valuation polytopes and we\ncharacterize 2-level polytopes among our double poset polytopes.\n  Fulkerson's 'anti-blocking' polytopes from combinatorial optimization subsume\nstable set polytopes of graphs and chain polytopes of posets. We determine the\ngeometry of Minkowski- and Cayley sums of anti-blocking polytopes. In\nparticular, we describe a canonical subdivision of Minkowski sums of\nanti-blocking polytopes that facilitates the computation of Ehrhart\n(quasi-)polynomials and volumes. This also yields canonical triangulations of\ndouble poset polytopes.\n  Finally, we investigate the affine semigroup rings associated to double poset\npolytopes. We show that they have quadratic Groebner bases, which gives an\nalgebraic description of the unimodular flag triangulations described in the\nfirst part. \n\n"}
{"id": "1606.06246", "contents": "Title: High-dimensional changepoint estimation via sparse projection Abstract: Changepoints are a very common feature of Big Data that arrive in the form of\na data stream. In this paper, we study high-dimensional time series in which,\nat certain time points, the mean structure changes in a sparse subset of the\ncoordinates. The challenge is to borrow strength across the coordinates in\norder to detect smaller changes than could be observed in any individual\ncomponent series. We propose a two-stage procedure called `inspect' for\nestimation of the changepoints: first, we argue that a good projection\ndirection can be obtained as the leading left singular vector of the matrix\nthat solves a convex optimisation problem derived from the CUSUM transformation\nof the time series. We then apply an existing univariate changepoint estimation\nalgorithm to the projected series. Our theory provides strong guarantees on\nboth the number of estimated changepoints and the rates of convergence of their\nlocations, and our numerical studies validate its highly competitive empirical\nperformance for a wide range of data generating mechanisms. Software\nimplementing the methodology is available in the R package\n`InspectChangepoint'. \n\n"}
{"id": "1606.06746", "contents": "Title: Approximate Recovery in Changepoint Problems, from $\\ell_2$ Estimation\n  Error Rates Abstract: In the 1-dimensional multiple changepoint detection problem, we prove that\nany procedure with a fast enough $\\ell_2$ error rate, in terms of its\nestimation of the underlying piecewise constant mean vector, automatically has\nan (approximate) changepoint screening property---specifically, each true jump\nin the underlying mean vector has an estimated jump nearby. We also show, again\nassuming only knowledge of the $\\ell_2$ error rate, that a simple\npost-processing step can be used to eliminate spurious estimated changepoints,\nand thus delivers an (approximate) changepoint recovery\nproperty---specifically, in addition to the screening property described above,\nwe are assured that each estimated jump has a true jump nearby. As a special\ncase, we focus on the application of these results to the 1-dimensional fused\nlasso, i.e., 1-dimensional total variation denoising, and compare the\nimplications with existing results from the literature. We also study\nextensions to related problems, such as changepoint detection over graphs. \n\n"}
{"id": "1606.06956", "contents": "Title: Statistics of topological RNA structures Abstract: In this paper we study properties of topological RNA structures, i.e.~RNA\ncontact structures with cross-serial interactions that are filtered by their\ntopological genus. RNA secondary structures within this framework are\ntopological structures having genus zero. We derive a new bivariate generating\nfunction whose singular expansion allows us to analyze the distributions of\narcs, stacks, hairpin- , interior- and multi-loops. We then extend this\nanalysis to H-type pseudoknots, kissing hairpins as well as $3$-knots and\ncompute their respective expectation values. Finally we discuss our results and\nput them into context with data obtained by uniform sampling structures of\nfixed genus. \n\n"}
{"id": "1606.07845", "contents": "Title: Robust and scalable Bayesian analysis of spatial neural tuning function\n  data Abstract: A common analytical problem in neuroscience is the interpretation of neural\nactivity with respect to sensory input or behavioral output. This is typically\nachieved by regressing measured neural activity against known stimuli or\nbehavioral variables to produce a \"tuning function\" for each neuron.\nUnfortunately, because this approach handles neurons individually, it cannot\ntake advantage of simultaneous measurements from spatially adjacent neurons\nthat often have similar tuning properties. On the other hand, sharing\ninformation between adjacent neurons can errantly degrade estimates of tuning\nfunctions across space if there are sharp discontinuities in tuning between\nnearby neurons. In this paper, we develop a computationally efficient block\nGibbs sampler that effectively pools information between neurons to de-noise\ntuning function estimates while simultaneously preserving sharp discontinuities\nthat might exist in the organization of tuning across space. This method is\nfully Bayesian and its computational cost per iteration scales\nsub-quadratically with total parameter dimensionality. We demonstrate the\nrobustness and scalability of this approach by applying it to both real and\nsynthetic datasets. In particular, an application to data from the spinal cord\nillustrates that the proposed methods can dramatically decrease the\nexperimental time required to accurately estimate tuning functions. \n\n"}
{"id": "1606.07855", "contents": "Title: Probabilistic Forecasting and Simulation of Electricity Markets via\n  Online Dictionary Learning Abstract: The problem of probabilistic forecasting and online simulation of real-time\nelectricity market with stochastic generation and demand is considered. By\nexploiting the parametric structure of the direct current optimal power flow, a\nnew technique based on online dictionary learning (ODL) is proposed. The ODL\napproach incorporates real-time measurements and historical traces to produce\nforecasts of joint and marginal probability distributions of future locational\nmarginal prices, power flows, and dispatch levels, conditional on the system\nstate at the time of forecasting. Compared with standard Monte Carlo simulation\ntechniques, the ODL approach offers several orders of magnitude improvement in\ncomputation time, making it feasible for online forecasting of market\noperations. Numerical simulations on large and moderate size power systems\nillustrate its performance and complexity features and its potential as a tool\nfor system operators. \n\n"}
{"id": "1606.07892", "contents": "Title: Large-Scale Kernel Methods for Independence Testing Abstract: Representations of probability measures in reproducing kernel Hilbert spaces\nprovide a flexible framework for fully nonparametric hypothesis tests of\nindependence, which can capture any type of departure from independence,\nincluding nonlinear associations and multivariate interactions. However, these\napproaches come with an at least quadratic computational cost in the number of\nobservations, which can be prohibitive in many applications. Arguably, it is\nexactly in such large-scale datasets that capturing any type of dependence is\nof interest, so striking a favourable tradeoff between computational efficiency\nand test performance for kernel independence tests would have a direct impact\non their applicability in practice. In this contribution, we provide an\nextensive study of the use of large-scale kernel approximations in the context\nof independence testing, contrasting block-based, Nystrom and random Fourier\nfeature approaches. Through a variety of synthetic data experiments, it is\ndemonstrated that our novel large scale methods give comparable performance\nwith existing methods whilst using significantly less computation time and\nmemory. \n\n"}
{"id": "1607.00071", "contents": "Title: An Operator Theoretic Approach to Nonparametric Mixture Models Abstract: When estimating finite mixture models, it is common to make assumptions on\nthe mixture components, such as parametric assumptions. In this work, we make\nno distributional assumptions on the mixture components and instead assume that\nobservations from the mixture model are grouped, such that observations in the\nsame group are known to be drawn from the same mixture component. We precisely\ncharacterize the number of observations $n$ per group needed for the mixture\nmodel to be identifiable, as a function of the number $m$ of mixture\ncomponents. In addition to our assumption-free analysis, we also study the\nsettings where the mixture components are either linearly independent or\njointly irreducible. Furthermore, our analysis considers two kinds of\nidentifiability -- where the mixture model is the simplest one explaining the\ndata, and where it is the only one. As an application of these results, we\nprecisely characterize identifiability of multinomial mixture models. Our\nanalysis relies on an operator-theoretic framework that associates mixture\nmodels in the grouped-sample setting with certain infinite-dimensional tensors.\nBased on this framework, we introduce general spectral algorithms for\nrecovering the mixture components and illustrate their use on a synthetic data\nset. \n\n"}
{"id": "1607.00798", "contents": "Title: The Finiteness Threshold Width of Lattice Polytopes Abstract: We prove that in each dimension $d$ there is a constant $w^\\infty(d)\\in\n\\mathbb{N}$ such that for every $n\\in \\mathbb{N}$ all but finitely many\n$d$-polytopes with $n$ lattice points have width at most $w^\\infty(d)$. We call\n$w^\\infty(d)$ the finiteness threshold width and show that $d-2 \\le\nw^\\infty(d)\\le O^*\\left( d^{4/3}\\right)$.\n  Blanco and Santos determined the value $w^\\infty(3)=1$. Here, we establish\n$w^\\infty(4)=2$. This implies, in particular, that there are only finitely many\nempty $4$-simplices of width larger than two.\n  The main tool in our proofs is the study of $d$-dimensional lifts of hollow\n$(d-1)$-polytopes. \n\n"}
{"id": "1607.02188", "contents": "Title: Whole-brain substitute CT generation using Markov random field mixture\n  models Abstract: Computed tomography (CT) equivalent information is needed for attenuation\ncorrection in PET imaging and for dose planning in radiotherapy. Prior work has\nshown that Gaussian mixture models can be used to generate a substitute CT\n(s-CT) image from a specific set of MRI modalities. This work introduces a more\nflexible class of mixture models for s-CT generation, that incorporates spatial\ndependency in the data through a Markov random field prior on the latent field\nof class memberships associated with a mixture model. Furthermore, the mixture\ndistributions are extended from Gaussian to normal inverse Gaussian (NIG),\nallowing heavier tails and skewness. The amount of data needed to train a model\nfor s-CT generation is of the order of 100 million voxels. The computational\nefficiency of the parameter estimation and prediction methods are hence\nparamount, especially when spatial dependency is included in the models. A\nstochastic Expectation Maximization (EM) gradient algorithm is proposed in\norder to tackle this challenge. The advantages of the spatial model and NIG\ndistributions are evaluated with a cross-validation study based on data from 14\npatients. The study show that the proposed model enhances the predictive\nquality of the s-CT images by reducing the mean absolute error with 17.9%.\nAlso, the distribution of CT values conditioned on the MR images are better\nexplained by the proposed model as evaluated using continuous ranked\nprobability scores. \n\n"}
{"id": "1607.03050", "contents": "Title: Learning a metric for class-conditional KNN Abstract: Naive Bayes Nearest Neighbour (NBNN) is a simple and effective framework\nwhich addresses many of the pitfalls of K-Nearest Neighbour (KNN)\nclassification. It has yielded competitive results on several computer vision\nbenchmarks. Its central tenet is that during NN search, a query is not compared\nto every example in a database, ignoring class information. Instead, NN\nsearches are performed within each class, generating a score per class. A key\nproblem with NN techniques, including NBNN, is that they fail when the data\nrepresentation does not capture perceptual (e.g.~class-based) similarity. NBNN\ncircumvents this by using independent engineered descriptors (e.g.~SIFT). To\nextend its applicability outside of image-based domains, we propose to learn a\nmetric which captures perceptual similarity. Similar to how Neighbourhood\nComponents Analysis optimizes a differentiable form of KNN classification, we\npropose \"Class Conditional\" metric learning (CCML), which optimizes a soft form\nof the NBNN selection rule. Typical metric learning algorithms learn either a\nglobal or local metric. However, our proposed method can be adjusted to a\nparticular level of locality by tuning a single parameter. An empirical\nevaluation on classification and retrieval tasks demonstrates that our proposed\nmethod clearly outperforms existing learned distance metrics across a variety\nof image and non-image datasets. \n\n"}
{"id": "1607.03188", "contents": "Title: The Zig-Zag Process and Super-Efficient Sampling for Bayesian Analysis\n  of Big Data Abstract: Standard MCMC methods can scale poorly to big data settings due to the need\nto evaluate the likelihood at each iteration. There have been a number of\napproximate MCMC algorithms that use sub-sampling ideas to reduce this\ncomputational burden, but with the drawback that these algorithms no longer\ntarget the true posterior distribution. We introduce a new family of Monte\nCarlo methods based upon a multi-dimensional version of the Zig-Zag process of\n(Bierkens, Roberts, 2017), a continuous time piecewise deterministic Markov\nprocess. While traditional MCMC methods are reversible by construction (a\nproperty which is known to inhibit rapid convergence) the Zig-Zag process\noffers a flexible non-reversible alternative which we observe to often have\nfavourable convergence properties. We show how the Zig-Zag process can be\nsimulated without discretisation error, and give conditions for the process to\nbe ergodic. Most importantly, we introduce a sub-sampling version of the\nZig-Zag process that is an example of an {\\em exact approximate scheme}, i.e.\nthe resulting approximate process still has the posterior as its stationary\ndistribution. Furthermore, if we use a control-variate idea to reduce the\nvariance of our unbiased estimator, then the Zig-Zag process can be\nsuper-efficient: after an initial pre-processing step, essentially independent\nsamples from the posterior distribution are obtained at a computational cost\nwhich does not depend on the size of the data. \n\n"}
{"id": "1607.03188", "contents": "Title: The Zig-Zag Process and Super-Efficient Sampling for Bayesian Analysis\n  of Big Data Abstract: Standard MCMC methods can scale poorly to big data settings due to the need\nto evaluate the likelihood at each iteration. There have been a number of\napproximate MCMC algorithms that use sub-sampling ideas to reduce this\ncomputational burden, but with the drawback that these algorithms no longer\ntarget the true posterior distribution. We introduce a new family of Monte\nCarlo methods based upon a multi-dimensional version of the Zig-Zag process of\n(Bierkens, Roberts, 2017), a continuous time piecewise deterministic Markov\nprocess. While traditional MCMC methods are reversible by construction (a\nproperty which is known to inhibit rapid convergence) the Zig-Zag process\noffers a flexible non-reversible alternative which we observe to often have\nfavourable convergence properties. We show how the Zig-Zag process can be\nsimulated without discretisation error, and give conditions for the process to\nbe ergodic. Most importantly, we introduce a sub-sampling version of the\nZig-Zag process that is an example of an {\\em exact approximate scheme}, i.e.\nthe resulting approximate process still has the posterior as its stationary\ndistribution. Furthermore, if we use a control-variate idea to reduce the\nvariance of our unbiased estimator, then the Zig-Zag process can be\nsuper-efficient: after an initial pre-processing step, essentially independent\nsamples from the posterior distribution are obtained at a computational cost\nwhich does not depend on the size of the data. \n\n"}
{"id": "1607.03592", "contents": "Title: Cluster Sampling Filters for Non-Gaussian Data Assimilation Abstract: This paper presents a fully non-Gaussian version of the Hamiltonian Monte\nCarlo (HMC) sampling filter. The Gaussian prior assumption in the original HMC\nfilter is relaxed. Specifically, a clustering step is introduced after the\nforecast phase of the filter, and the prior density function is estimated by\nfitting a Gaussian Mixture Model (GMM) to the prior ensemble. Using the data\nlikelihood function, the posterior density is then formulated as a mixture\ndensity, and is sampled using a HMC approach (or any other scheme capable of\nsampling multimodal densities in high-dimensional subspaces). The main filter\ndeveloped herein is named \"cluster HMC sampling filter\" (ClHMC). A multi-chain\nversion of the ClHMC filter, namely MC-ClHMC is also proposed to guarantee that\nsamples are taken from the vicinities of all probability modes of the\nformulated posterior. The new methodologies are tested using a\nquasi-geostrophic (QG) model with double-gyre wind forcing and bi-harmonic\nfriction. Numerical results demonstrate the usefulness of using GMMs to relax\nthe Gaussian prior assumption in the HMC filtering paradigm. \n\n"}
{"id": "1607.03792", "contents": "Title: Kernel Density Estimation for Dynamical Systems Abstract: We study the density estimation problem with observations generated by\ncertain dynamical systems that admit a unique underlying invariant Lebesgue\ndensity. Observations drawn from dynamical systems are not independent and\nmoreover, usual mixing concepts may not be appropriate for measuring the\ndependence among these observations. By employing the $\\mathcal{C}$-mixing\nconcept to measure the dependence, we conduct statistical analysis on the\nconsistency and convergence of the kernel density estimator. Our main results\nare as follows: First, we show that with properly chosen bandwidth, the kernel\ndensity estimator is universally consistent under $L_1$-norm; Second, we\nestablish convergence rates for the estimator with respect to several classes\nof dynamical systems under $L_1$-norm. In the analysis, the density function\n$f$ is only assumed to be H\\\"{o}lder continuous which is a weak assumption in\nthe literature of nonparametric density estimation and also more realistic in\nthe dynamical system context. Last but not least, we prove that the same\nconvergence rates of the estimator under $L_\\infty$-norm and $L_1$-norm can be\nachieved when the density function is H\\\"{o}lder continuous, compactly\nsupported and bounded. The bandwidth selection problem of the kernel density\nestimator for dynamical system is also discussed in our study via numerical\nsimulations. \n\n"}
{"id": "1607.03854", "contents": "Title: The Partially Observable Hidden Markov Model and its Application to\n  Keystroke Dynamics Abstract: The partially observable hidden Markov model is an extension of the hidden\nMarkov Model in which the hidden state is conditioned on an independent Markov\nchain. This structure is motivated by the presence of discrete metadata, such\nas an event type, that may partially reveal the hidden state but itself\nemanates from a separate process. Such a scenario is encountered in keystroke\ndynamics whereby a user's typing behavior is dependent on the text that is\ntyped. Under the assumption that the user can be in either an active or passive\nstate of typing, the keyboard key names are event types that partially reveal\nthe hidden state due to the presence of relatively longer time intervals\nbetween words and sentences than between letters of a word. Using five public\ndatasets, the proposed model is shown to consistently outperform other anomaly\ndetectors, including the standard HMM, in biometric identification and\nverification tasks and is generally preferred over the HMM in a Monte Carlo\ngoodness of fit test. \n\n"}
{"id": "1607.05241", "contents": "Title: Imitation Learning with Recurrent Neural Networks Abstract: We present a novel view that unifies two frameworks that aim to solve\nsequential prediction problems: learning to search (L2S) and recurrent neural\nnetworks (RNN). We point out equivalences between elements of the two\nframeworks. By complementing what is missing from one framework comparing to\nthe other, we introduce a more advanced imitation learning framework that, on\none hand, augments L2S s notion of search space and, on the other hand,\nenhances RNNs training procedure to be more robust to compounding errors\narising from training on highly correlated examples. \n\n"}
{"id": "1607.05981", "contents": "Title: Fitting logistic multilevel models with crossed random effects via\n  Bayesian Integrated Nested Laplace Approximations: a simulation study Abstract: Fitting cross-classified multilevel models with binary response is\nchallenging. In this setting a promising method is Bayesian inference through\nIntegrated Nested Laplace Approximations (INLA), which performs well in several\nlatent variable models. Therefore we devise a systematic simulation study to\nassess the performance of INLA with cross-classified logistic data under\ndifferent scenarios defined by the magnitude of the random effects variances,\nthe number of observations, the number of clusters, and the degree of\ncross-classification. In the simulations INLA is systematically compared with\nthe popular method of Maximum Likelihood via Laplace Approximation. By an\napplication to the classical salamander mating data, we compare INLA with the\nbest performing methods. Given the computational speed and the generally good\nperformance, INLA turns out to be a valuable method for fitting the considered\ncross-classified models. \n\n"}
{"id": "1607.06364", "contents": "Title: Distributed Supervised Learning using Neural Networks Abstract: Distributed learning is the problem of inferring a function in the case where\ntraining data is distributed among multiple geographically separated sources.\nParticularly, the focus is on designing learning strategies with low\ncomputational requirements, in which communication is restricted only to\nneighboring agents, with no reliance on a centralized authority. In this\nthesis, we analyze multiple distributed protocols for a large number of neural\nnetwork architectures. The first part of the thesis is devoted to a definition\nof the problem, followed by an extensive overview of the state-of-the-art.\nNext, we introduce different strategies for a relatively simple class of single\nlayer neural networks, where a linear output layer is preceded by a nonlinear\nlayer, whose weights are stochastically assigned in the beginning of the\nlearning process. We consider both batch and sequential learning, with\nhorizontally and vertically partitioned data. In the third part, we consider\ninstead the more complex problem of semi-supervised distributed learning, where\neach agent is provided with an additional set of unlabeled training samples. We\npropose two different algorithms based on diffusion processes for linear\nsupport vector machines and kernel ridge regression. Subsequently, the fourth\npart extends the discussion to learning with time-varying data (e.g.\ntime-series) using recurrent neural networks. We consider two different\nfamilies of networks, namely echo state networks (extending the algorithms\nintroduced in the second part), and spline adaptive filters. Overall, the\nalgorithms presented throughout the thesis cover a wide range of possible\npractical applications, and lead the way to numerous future extensions, which\nare briefly summarized in the conclusive chapter. \n\n"}
{"id": "1607.07814", "contents": "Title: Minkowski complexes and convex threshold dimension Abstract: For a collection of convex bodies $P_1,\\dots,P_n \\subset \\mathbb{R}^d$\ncontaining the origin, a Minkowski complex is given by those subsets whose\nMinkowski sum does not contain a fixed basepoint. Every simplicial complex can\nbe realized as a Minkowski complex and for convex bodies on the real line, this\nrecovers the class of threshold complexes. The purpose of this note is the\nstudy of the convex threshold dimension of a complex, that is, the smallest\ndimension in which it can be realized as a Minkowski complex. In particular, we\nshow that the convex threshold dimension can be arbitrarily large. This is\nrelated to work of Chv\\'atal and Hammer (1977) regarding forbidden subgraphs of\nthreshold graphs. We also show that convexity is crucial this context. \n\n"}
{"id": "1608.00948", "contents": "Title: The bootstrap, covariance matrices and PCA in moderate and\n  high-dimensions Abstract: We consider the properties of the bootstrap as a tool for inference\nconcerning the eigenvalues of a sample covariance matrix computed from an\n$n\\times p$ data matrix $X$. We focus on the modern framework where $p/n$ is\nnot close to 0 but remains bounded as $n$ and $p$ tend to infinity.\n  Through a mix of numerical and theoretical considerations, we show that the\nbootstrap is not in general a reliable inferential tool in the setting we\nconsider. However, in the case where the population covariance matrix is\nwell-approximated by a finite rank matrix, the bootstrap performs as it does in\nfinite dimension. \n\n"}
{"id": "1608.01234", "contents": "Title: Fast Algorithms for Demixing Sparse Signals from Nonlinear Observations Abstract: We study the problem of demixing a pair of sparse signals from noisy,\nnonlinear observations of their superposition. Mathematically, we consider a\nnonlinear signal observation model, $y_i = g(a_i^Tx) + e_i, \\ i=1,\\ldots,m$,\nwhere $x = \\Phi w+\\Psi z$ denotes the superposition signal, $\\Phi$ and $\\Psi$\nare orthonormal bases in $\\mathbb{R}^n$, and $w, z\\in\\mathbb{R}^n$ are sparse\ncoefficient vectors of the constituent signals, and $e_i$ represents the noise.\nMoreover, $g$ represents a nonlinear link function, and $a_i\\in\\mathbb{R}^n$ is\nthe $i$-th row of the measurement matrix, $A\\in\\mathbb{R}^{m\\times n}$.\nProblems of this nature arise in several applications ranging from astronomy,\ncomputer vision, and machine learning. In this paper, we make some concrete\nalgorithmic progress for the above demixing problem. Specifically, we consider\ntwo scenarios: (i) the case when the demixing procedure has no knowledge of the\nlink function, and (ii) the case when the demixing algorithm has perfect\nknowledge of the link function. In both cases, we provide fast algorithms for\nrecovery of the constituents $w$ and $z$ from the observations. Moreover, we\nsupport these algorithms with a rigorous theoretical analysis, and derive\n(nearly) tight upper bounds on the sample complexity of the proposed algorithms\nfor achieving stable recovery of the component signals. We also provide a range\nof numerical simulations to illustrate the performance of the proposed\nalgorithms on both real and synthetic signals and images. \n\n"}
{"id": "1608.04117", "contents": "Title: The Importance of Skip Connections in Biomedical Image Segmentation Abstract: In this paper, we study the influence of both long and short skip connections\non Fully Convolutional Networks (FCN) for biomedical image segmentation. In\nstandard FCNs, only long skip connections are used to skip features from the\ncontracting path to the expanding path in order to recover spatial information\nlost during downsampling. We extend FCNs by adding short skip connections, that\nare similar to the ones introduced in residual networks, in order to build very\ndeep FCNs (of hundreds of layers). A review of the gradient flow confirms that\nfor a very deep FCN it is beneficial to have both long and short skip\nconnections. Finally, we show that a very deep FCN can achieve\nnear-to-state-of-the-art results on the EM dataset without any further\npost-processing. \n\n"}
{"id": "1608.04667", "contents": "Title: Medical image denoising using convolutional denoising autoencoders Abstract: Image denoising is an important pre-processing step in medical image\nanalysis. Different algorithms have been proposed in past three decades with\nvarying denoising performances. More recently, having outperformed all\nconventional methods, deep learning based models have shown a great promise.\nThese methods are however limited for requirement of large training sample size\nand high computational costs. In this paper we show that using small sample\nsize, denoising autoencoders constructed using convolutional layers can be used\nfor efficient denoising of medical images. Heterogeneous images can be combined\nto boost sample size for increased denoising performance. Simplest of networks\ncan reconstruct images with corruption levels so high that noise and signal are\nnot differentiable to human eye. \n\n"}
{"id": "1608.06373", "contents": "Title: A General Method to Determine Limiting Optimal Shapes for\n  Edge-Isoperimetric Inequalities Abstract: For a general family of graphs on $\\mathbb{Z}^n$, we translate the\nedge-isoperimetric problem into a continuous isoperimetric problem in\n$\\mathbb{R}^n$. We then solve the continuous isoperimetric problem using the\nBrunn-Minkowski inequality and Minkowski's theorem on Mixed Volumes. This\ntranslation allows us to conclude, under a reasonable assumption about the\ndiscrete problem, that the shapes of the optimal sets in the discrete problem\napproach the shape of the optimal set in the continuous problem as the size of\nthe set grows. The solution is the zonotope defined as the Minkowski sum of the\nedges of the original graph.\n  We demonstrate the efficacy of this method by revisiting some previously\nsolved classical edge-isoperimetric problems. We then apply our method to some\ndiscrete isoperimetric problems which had not previously been solved. The\ncomplexity of those solutions suggest that it would be quite difficult to find\nthem using discrete methods only. \n\n"}
{"id": "1608.08468", "contents": "Title: Sparse Bayesian time-varying covariance estimation in many dimensions Abstract: We address the curse of dimensionality in dynamic covariance estimation by\nmodeling the underlying co-volatility dynamics of a time series vector through\nlatent time-varying stochastic factors. The use of a global-local shrinkage\nprior for the elements of the factor loadings matrix pulls loadings on\nsuperfluous factors towards zero. To demonstrate the merits of the proposed\nframework, the model is applied to simulated data as well as to daily\nlog-returns of 300 S&P 500 members. Our approach yields precise correlation\nestimates, strong implied minimum variance portfolio performance and superior\nforecasting accuracy in terms of log predictive scores when compared to typical\nbenchmarks. \n\n"}
{"id": "1608.08666", "contents": "Title: Online state and parameter estimation in Dynamic Generalised Linear\n  Models Abstract: Inference for streaming time-series is tightly coupled with the problem of\nBayesian on-line state and parameter inference. In this paper we will introduce\nDynamic Generalised Linear Models, the class of models often chosen to model\ncontinuous and discrete time-series data. We will look at three different\napproaches which allow on-line estimation and analyse the results when applied\nto different real world datasets related to inference for streaming data.\nSufficient statistics based methods delay known problems, such as particle\nimpoverishment, especially when applied to long running time-series, while\nproviding reasonable parameter estimations when compared to exact methods, such\nas Particle Marginal Metropolis-Hastings. State and observation forecasts will\nalso be analysed as a performance metric. By benchmarking against a \"gold\nstandard\" (off-line) method, we can better understand the performance of\non-line methods in challenging real-world scenarios. \n\n"}
{"id": "1608.08814", "contents": "Title: Importance Sampling and Necessary Sample Size: an Information Theory\n  Approach Abstract: Importance sampling approximates expectations with respect to a target\nmeasure by using samples from a proposal measure. The performance of the method\nover large classes of test functions depends heavily on the closeness between\nboth measures. We derive a general bound that needs to hold for importance\nsampling to be successful, and relates the $f$-divergence between the target\nand the proposal to the sample size. The bound is deduced from a new and simple\ninformation theory paradigm for the study of importance sampling. As examples\nof the general theory we give necessary conditions on the sample size in terms\nof the Kullback-Leibler and $\\chi^2$ divergences, and the total variation and\nHellinger distances. Our approach is non-asymptotic, and its generality allows\nto tell apart the relative merits of these metrics. Unsurprisingly, the\nnon-symmetric divergences give sharper bounds than total variation or\nHellinger. Our results extend existing necessary conditions -and complement\nsufficient ones- on the sample size required for importance sampling. \n\n"}
{"id": "1609.00368", "contents": "Title: Ten Steps of EM Suffice for Mixtures of Two Gaussians Abstract: The Expectation-Maximization (EM) algorithm is a widely used method for\nmaximum likelihood estimation in models with latent variables. For estimating\nmixtures of Gaussians, its iteration can be viewed as a soft version of the\nk-means clustering algorithm. Despite its wide use and applications, there are\nessentially no known convergence guarantees for this method. We provide global\nconvergence guarantees for mixtures of two Gaussians with known covariance\nmatrices. We show that the population version of EM, where the algorithm is\ngiven access to infinitely many samples from the mixture, converges\ngeometrically to the correct mean vectors, and provide simple, closed-form\nexpressions for the convergence rate. As a simple illustration, we show that,\nin one dimension, ten steps of the EM algorithm initialized at infinity result\nin less than 1\\% error estimation of the means. In the finite sample regime, we\nshow that, under a random initialization, $\\tilde{O}(d/\\epsilon^2)$ samples\nsuffice to compute the unknown vectors to within $\\epsilon$ in Mahalanobis\ndistance, where $d$ is the dimension. In particular, the error rate of the EM\nbased estimator is $\\tilde{O}\\left(\\sqrt{d \\over n}\\right)$ where $n$ is the\nnumber of samples, which is optimal up to logarithmic factors. \n\n"}
{"id": "1609.02541", "contents": "Title: Importance sampling type estimators based on approximate marginal MCMC Abstract: We consider importance sampling (IS) type weighted estimators based on Markov\nchain Monte Carlo (MCMC) targeting an approximate marginal of the target\ndistribution. In the context of Bayesian latent variable models, the MCMC\ntypically operates on the hyperparameters, and the subsequent weighting may be\nbased on IS or sequential Monte Carlo (SMC), but allows for multilevel\ntechniques as well. The IS approach provides a natural alternative to delayed\nacceptance (DA) pseudo-marginal/particle MCMC, and has many advantages over DA,\nincluding a straightforward parallelisation and additional flexibility in MCMC\nimplementation. We detail minimal conditions which ensure strong consistency of\nthe suggested estimators, and provide central limit theorems with expressions\nfor asymptotic variances. We demonstrate how our method can make use of SMC in\nthe state space models context, using Laplace approximations and\ntime-discretised diffusions. Our experimental results are promising and show\nthat the IS type approach can provide substantial gains relative to an\nanalogous DA scheme, and is often competitive even without parallelisation. \n\n"}
{"id": "1609.04558", "contents": "Title: Statistical Inference in a Directed Network Model with Covariates Abstract: Networks are often characterized by node heterogeneity for which nodes\nexhibit different degrees of interaction and link homophily for which nodes\nsharing common features tend to associate with each other. In this paper, we\npropose a new directed network model to capture the former via node-specific\nparametrization and the latter by incorporating covariates. In particular, this\nmodel quantifies the extent of heterogeneity in terms of outgoingness and\nincomingness of each node by different parameters, thus allowing the number of\nheterogeneity parameters to be twice the number of nodes. We study the maximum\nlikelihood estimation of the model and establish the uniform consistency and\nasymptotic normality of the resulting estimators. Numerical studies demonstrate\nour theoretical findings and a data analysis confirms the usefulness of our\nmodel. \n\n"}
{"id": "1609.04789", "contents": "Title: Coherence Pursuit: Fast, Simple, and Robust Principal Component Analysis Abstract: This paper presents a remarkably simple, yet powerful, algorithm termed\nCoherence Pursuit (CoP) to robust Principal Component Analysis (PCA). As\ninliers lie in a low dimensional subspace and are mostly correlated, an inlier\nis likely to have strong mutual coherence with a large number of data points.\nBy contrast, outliers either do not admit low dimensional structures or form\nsmall clusters. In either case, an outlier is unlikely to bear strong\nresemblance to a large number of data points. Given that, CoP sets an outlier\napart from an inlier by comparing their coherence with the rest of the data\npoints. The mutual coherences are computed by forming the Gram matrix of the\nnormalized data points. Subsequently, the sought subspace is recovered from the\nspan of the subset of the data points that exhibit strong coherence with the\nrest of the data. As CoP only involves one simple matrix multiplication, it is\nsignificantly faster than the state-of-the-art robust PCA algorithms. We derive\nanalytical performance guarantees for CoP under different models for the\ndistributions of inliers and outliers in both noise-free and noisy settings.\nCoP is the first robust PCA algorithm that is simultaneously non-iterative,\nprovably robust to both unstructured and structured outliers, and can tolerate\na large number of unstructured outliers. \n\n"}
{"id": "1609.05615", "contents": "Title: Discussion of \"Fast Approximate Inference for Arbitrarily Large\n  Semiparametric Regression Models via Message Passing\" Abstract: Discussion paper on \"Fast Approximate Inference for Arbitrarily Large\nSemiparametric Regression Models via Message Passing\" by Wand\n[arXiv:1602.07412]. \n\n"}
{"id": "1609.06972", "contents": "Title: Minimal completely asymmetric (4; n)-regular matchstick graphs Abstract: A matchstick graph is a graph drawn with straight edges in the plane such\nthat the edges have unit length, and non-adjacent edges do not intersect. We\ncall a matchstick graph $(m;n)$-regular if every vertex has only degree $m$ or\n$n$. In this article we present the latest known $(4;n)$-regular matchstick\ngraphs for $4\\leq n\\leq11$ with a minimum number of vertices and a completely\nasymmetric structure. We call a matchstick graph completely asymmetric, if the\nfollowing conditions are complied. 1) The graph is rigid. 2) The graph has no\npoint, rotational or mirror symmetry. 3) The graph has an asymmetric outer\nshape. 4) The graph can not be decomposed into rigid subgraphs and rearrange to\na similar graph which contradicts to any of the other conditions. \n\n"}
{"id": "1609.08596", "contents": "Title: $h^\\ast$-polynomials of zonotopes Abstract: The Ehrhart polynomial of a lattice polytope $P$ encodes information about\nthe number of integer lattice points in positive integral dilates of $P$. The\n$h^\\ast$-polynomial of $P$ is the numerator polynomial of the generating\nfunction of its Ehrhart polynomial. A zonotope is any projection of a higher\ndimensional cube. We give a combinatorial description of the\n$h^\\ast$-polynomial of a lattice zonotope in terms of refined descent\nstatistics of permutations and prove that the $h^\\ast$-polynomial of every\nlattice zonotope has only real roots and therefore unimodal coefficients.\nFurthermore, we present a closed formula for the $h^\\ast$-polynomial of a\nzonotope in matroidal terms which is analogous to a result by Stanley (1991) on\nthe Ehrhart polynomial. Our results hold not only for $h^\\ast$-polynomials but\ncarry over to general combinatorial positive valuations. Moreover, we give a\ncomplete description of the convex hull of all $h^\\ast$-polynomials of\nzonotopes in a given dimension: it is a simplicial cone spanned by refined\nEulerian polynomials. \n\n"}
{"id": "1609.08828", "contents": "Title: Geometry of mutation classes of rank $3$ quivers Abstract: We present a geometric realization for all mutation classes of quivers of\nrank $3$ with real weights. This realization is via linear reflection groups\nfor acyclic mutation classes and via groups generated by $\\pi$-rotations for\nthe cyclic ones. The geometric behavior of the model turns out to be controlled\nby the Markov constant $p^2+q^2+r^2-pqr$, where $p,q,r$ are the elements of\nexchange matrix. We also classify skew-symmetric mutation-finite real $3\\times\n3$ matrices and explore the structure of acyclic representatives in finite and\ninfinite mutation classes. \n\n"}
{"id": "1609.09272", "contents": "Title: A New Algorithm for Circulant Rational Covariance Extension and\n  Applications to Finite-interval Smoothing Abstract: The partial stochastic realization of periodic processes from finite\ncovariance data has recently been solved by Lindquist and Picci based on convex\noptimization of a generalized entropy functional. The meaning and the role of\nthis criterion have an unclear origin. In this paper we propose a solution\nbased on a nonlinear generalization of the classical Yule-Walker type equations\nand on a new iterative algorithm which is shown to converge to the same\n(unique) solution of the variational problem. This provides a conceptual link\nto the variational principles and at the same time yields a robust algorithm\nwhich can for example be successfully applied to finite-interval smoothing\nproblems providing a simpler procedure if compared with the classical\nRiccati-based calculations. \n\n"}
{"id": "1610.00494", "contents": "Title: One-Trial Correction of Legacy AI Systems and Stochastic Separation\n  Theorems Abstract: We consider the problem of efficient \"on the fly\" tuning of existing, or {\\it\nlegacy}, Artificial Intelligence (AI) systems. The legacy AI systems are\nallowed to be of arbitrary class, albeit the data they are using for computing\ninterim or final decision responses should posses an underlying structure of a\nhigh-dimensional topological real vector space. The tuning method that we\npropose enables dealing with errors without the need to re-train the system.\nInstead of re-training a simple cascade of perceptron nodes is added to the\nlegacy system. The added cascade modulates the AI legacy system's decisions. If\napplied repeatedly, the process results in a network of modulating rules\n\"dressing up\" and improving performance of existing AI systems. Mathematical\nrationale behind the method is based on the fundamental property of measure\nconcentration in high dimensional spaces. The method is illustrated with an\nexample of fine-tuning a deep convolutional network that has been pre-trained\nto detect pedestrians in images. \n\n"}
{"id": "1610.01353", "contents": "Title: Confidence regions for high-dimensional generalized linear models under\n  sparsity Abstract: We study asymptotically normal estimation and confidence regions for\nlow-dimensional parameters in high-dimensional sparse models. Our approach is\nbased on the $\\ell_1$-penalized M-estimator which is used for construction of a\nbias corrected estimator. We show that the proposed estimator is asymptotically\nnormal, under a sparsity assumption on the high-dimensional parameter,\nsmoothness conditions on the expected loss and an entropy condition. This leads\nto uniformly valid confidence regions and hypothesis testing for\nlow-dimensional parameters. The present approach is different in that it allows\nfor treatment of loss functions that we not sufficiently differentiable, such\nas quantile loss, Huber loss or hinge loss functions. We also provide new\nresults for estimation of the inverse Fisher information matrix, which is\nnecessary for the construction of the proposed estimator. We formulate our\nresults for general models under high-level conditions, but investigate these\nconditions in detail for generalized linear models and provide mild sufficient\nconditions. As particular examples, we investigate the case of quantile loss\nand Huber loss in linear regression and demonstrate the performance of the\nestimators in a simulation study and on real datasets from genome-wide\nassociation studies. We further investigate the case of logistic regression and\nillustrate the performance of the estimator on simulated and real data. \n\n"}
{"id": "1610.02106", "contents": "Title: Numerical approximation of the Frobenius-Perron operator using the\n  finite volume method Abstract: We develop a finite-dimensional approximation of the Frobenius-Perron\noperator using the finite volume method applied to the continuity equation for\nthe evolution of probability. A Courant-Friedrichs-Lewy condition ensures that\nthe approximation satisfies the Markov property, while existing convergence\ntheory for the finite volume method guarantees convergence of the discrete\noperator to the continuous operator as mesh size tends to zero. Properties of\nthe approximation are demonstrated in a computed example of sequential\ninference for the state of a low-dimensional mechanical system when\nobservations give rise to multi-modal distributions. \n\n"}
{"id": "1610.03774", "contents": "Title: Parallelizing Stochastic Gradient Descent for Least Squares Regression:\n  mini-batching, averaging, and model misspecification Abstract: This work characterizes the benefits of averaging schemes widely used in\nconjunction with stochastic gradient descent (SGD). In particular, this work\nprovides a sharp analysis of: (1) mini-batching, a method of averaging many\nsamples of a stochastic gradient to both reduce the variance of the stochastic\ngradient estimate and for parallelizing SGD and (2) tail-averaging, a method\ninvolving averaging the final few iterates of SGD to decrease the variance in\nSGD's final iterate. This work presents non-asymptotic excess risk bounds for\nthese schemes for the stochastic approximation problem of least squares\nregression.\n  Furthermore, this work establishes a precise problem-dependent extent to\nwhich mini-batch SGD yields provable near-linear parallelization speedups over\nSGD with batch size one. This allows for understanding learning rate versus\nbatch size tradeoffs for the final iterate of an SGD method. These results are\nthen utilized in providing a highly parallelizable SGD method that obtains the\nminimax risk with nearly the same number of serial updates as batch gradient\ndescent, improving significantly over existing SGD methods. A non-asymptotic\nanalysis of communication efficient parallelization schemes such as\nmodel-averaging/parameter mixing methods is then provided.\n  Finally, this work sheds light on some fundamental differences in SGD's\nbehavior when dealing with agnostic noise in the (non-realizable) least squares\nregression problem. In particular, the work shows that the stepsizes that\nensure minimax risk for the agnostic case must be a function of the noise\nproperties.\n  This paper builds on the operator view of analyzing SGD methods, introduced\nby Defossez and Bach (2015), followed by developing a novel analysis in\nbounding these operators to characterize the excess risk. These techniques are\nof broader interest in analyzing computational aspects of stochastic\napproximation. \n\n"}
{"id": "1610.05108", "contents": "Title: The xyz algorithm for fast interaction search in high-dimensional data Abstract: When performing regression on a dataset with $p$ variables, it is often of\ninterest to go beyond using main linear effects and include interactions as\nproducts between individual variables. For small-scale problems, these\ninteractions can be computed explicitly but this leads to a computational\ncomplexity of at least $\\mathcal{O}(p^2)$ if done naively. This cost can be\nprohibitive if $p$ is very large. We introduce a new randomised algorithm that\nis able to discover interactions with high probability and under mild\nconditions has a runtime that is subquadratic in $p$. We show that strong\ninteractions can be discovered in almost linear time, whilst finding weaker\ninteractions requires $\\mathcal{O}(p^\\alpha)$ operations for $1 < \\alpha < 2$\ndepending on their strength. The underlying idea is to transform interaction\nsearch into a closestpair problem which can be solved efficiently in\nsubquadratic time. The algorithm is called $\\mathit{xyz}$ and is implemented in\nthe language R. We demonstrate its efficiency for application to genome-wide\nassociation studies, where more than $10^{11}$ interactions can be screened in\nunder $280$ seconds with a single-core $1.2$ GHz CPU. \n\n"}
{"id": "1610.05247", "contents": "Title: Black-box Importance Sampling Abstract: Importance sampling is widely used in machine learning and statistics, but\nits power is limited by the restriction of using simple proposals for which the\nimportance weights can be tractably calculated. We address this problem by\nstudying black-box importance sampling methods that calculate importance\nweights for samples generated from any unknown proposal or black-box mechanism.\nOur method allows us to use better and richer proposals to solve difficult\nproblems, and (somewhat counter-intuitively) also has the additional benefit of\nimproving the estimation accuracy beyond typical importance sampling. Both\ntheoretical and empirical analyses are provided. \n\n"}
{"id": "1610.05350", "contents": "Title: How Well Do Local Algorithms Solve Semidefinite Programs? Abstract: Several probabilistic models from high-dimensional statistics and machine\nlearning reveal an intriguing --and yet poorly understood-- dichotomy. Either\nsimple local algorithms succeed in estimating the object of interest, or even\nsophisticated semi-definite programming (SDP) relaxations fail.\n  In order to explore this phenomenon, we study a classical SDP relaxation of\nthe minimum graph bisection problem, when applied to Erd\\H{o}s-Renyi random\ngraphs with bounded average degree $d>1$, and obtain several types of results.\nFirst, we use a dual witness construction (using the so-called non-backtracking\nmatrix of the graph) to upper bound the SDP value. Second, we prove that a\nsimple local algorithm approximately solves the SDP to within a factor\n$2d^2/(2d^2+d-1)$ of the upper bound. In particular, the local algorithm is at\nmost $8/9$ suboptimal, and $1+O(1/d)$ suboptimal for large degree.\n  We then analyze a more sophisticated local algorithm, which aggregates\ninformation according to the harmonic measure on the limiting Galton-Watson\n(GW) tree. The resulting lower bound is expressed in terms of the conductance\nof the GW tree and matches surprisingly well the empirically determined SDP\nvalues on large-scale Erd\\H{o}s-Renyi graphs.\n  We finally consider the planted partition model. In this case, purely local\nalgorithms are known to fail, but they do succeed if a small amount of side\ninformation is available. Our results imply quantitative bounds on the\nthreshold for partial recovery using SDP in this model. \n\n"}
{"id": "1610.05792", "contents": "Title: Big Batch SGD: Automated Inference using Adaptive Batch Sizes Abstract: Classical stochastic gradient methods for optimization rely on noisy gradient\napproximations that become progressively less accurate as iterates approach a\nsolution. The large noise and small signal in the resulting gradients makes it\ndifficult to use them for adaptive stepsize selection and automatic stopping.\nWe propose alternative \"big batch\" SGD schemes that adaptively grow the batch\nsize over time to maintain a nearly constant signal-to-noise ratio in the\ngradient approximation. The resulting methods have similar convergence rates to\nclassical SGD, and do not require convexity of the objective. The high fidelity\ngradients enable automated learning rate selection and do not require stepsize\ndecay. Big batch methods are thus easily automated and can run with little or\nno oversight. \n\n"}
{"id": "1610.07755", "contents": "Title: Global rigidity of generic frameworks on the cylinder Abstract: We show that a generic framework $(G,p)$ on the cylinder is globally rigid if\nand only if $G$ is a complete graph on at most four vertices or $G$ is both\nredundantly rigid and $2$-connected. To prove the theorem we also derive a new\nrecursive construction of circuits in the simple $(2,2)$-sparse matroid, and a\ncharacterisation of rigidity for generic frameworks on the cylinder when a\nsingle designated vertex is allowed to move off the cylinder. \n\n"}
{"id": "1610.08621", "contents": "Title: Estimator Augmentation with Applications in High-Dimensional Group\n  Inference Abstract: To make inference about a group of parameters on high-dimensional data, we\ndevelop the method of estimator augmentation for the block Lasso, which is\ndefined via the block norm. By augmenting a block Lasso estimator $\\hat{\\beta}$\nwith the subgradient $S$ of the block norm evaluated at $\\hat{\\beta}$, we\nderive a closed-form density for the joint distribution of $(\\hat{\\beta},S)$\nunder a high-dimensional setting. This allows us to draw from an estimated\nsampling distribution of $\\hat{\\beta}$, or more generally any function of\n$(\\hat{\\beta},S)$, by Monte Carlo algorithms. We demonstrate the application of\nestimator augmentation in group inference with the group Lasso and a de-biased\ngroup Lasso constructed as a function of $(\\hat{\\beta},S)$. Our numerical\nresults show that importance sampling via estimator augmentation can be orders\nof magnitude more efficient than parametric bootstrap in estimating tail\nprobabilities for significance tests. This work also brings new insights into\nthe geometry of the sample space and the solution uniqueness of the block\nLasso. \n\n"}
{"id": "1610.08735", "contents": "Title: Stratification of patient trajectories using covariate latent variable\n  models Abstract: Standard models assign disease progression to discrete categories or stages\nbased on well-characterized clinical markers. However, such a system is\npotentially at odds with our understanding of the underlying biology, which in\nhighly complex systems may support a (near-)continuous evolution of disease\nfrom inception to terminal state. To learn such a continuous disease score one\ncould infer a latent variable from dynamic \"omics\" data such as RNA-seq that\ncorrelates with an outcome of interest such as survival time. However, such\nanalyses may be confounded by additional data such as clinical covariates\nmeasured in electronic health records (EHRs). As a solution to this we\nintroduce covariate latent variable models, a novel type of latent variable\nmodel that learns a low-dimensional data representation in the presence of two\n(asymmetric) views of the same data source. We apply our model to TCGA\ncolorectal cancer RNA-seq data and demonstrate how incorporating\nmicrosatellite-instability (MSI) status as an external covariate allows us to\nidentify genes that stratify patients on an immune-response trajectory.\nFinally, we propose an extension termed Covariate Gaussian Process Latent\nVariable Models for learning nonparametric, nonlinear representations. An R\npackage implementing variational inference for covariate latent variable models\nis available at http://github.com/kieranrcampbell/clvm. \n\n"}
{"id": "1610.09075", "contents": "Title: Missing Data Imputation for Supervised Learning Abstract: Missing data imputation can help improve the performance of prediction models\nin situations where missing data hide useful information. This paper compares\nmethods for imputing missing categorical data for supervised classification\ntasks. We experiment on two machine learning benchmark datasets with missing\ncategorical data, comparing classifiers trained on non-imputed (i.e., one-hot\nencoded) or imputed data with different levels of additional missing-data\nperturbation. We show imputation methods can increase predictive accuracy in\nthe presence of missing-data perturbation, which can actually improve\nprediction accuracy by regularizing the classifier. We achieve the\nstate-of-the-art on the Adult dataset with missing-data perturbation and\nk-nearest-neighbors (k-NN) imputation. \n\n"}
{"id": "1610.09735", "contents": "Title: Community detection with nodal information Abstract: Community detection is one of the fundamental problems in the study of\nnetwork data. Most existing community detection approaches only consider edge\ninformation as inputs, and the output could be suboptimal when nodal\ninformation is available. In such cases, it is desirable to leverage nodal\ninformation for the improvement of community detection accuracy. Towards this\ngoal, we propose a flexible network model incorporating nodal information, and\ndevelop likelihood-based inference methods. For the proposed methods, we\nestablish favorable asymptotic properties as well as efficient algorithms for\ncomputation. Numerical experiments show the effectiveness of our methods in\nutilizing nodal information across a variety of simulated and real network data\nsets. \n\n"}
{"id": "1610.09788", "contents": "Title: Pseudo-marginal Metropolis--Hastings using averages of unbiased\n  estimators Abstract: We consider a pseudo-marginal Metropolis--Hastings kernel $P_m$ that is\nconstructed using an average of $m$ exchangeable random variables, as well as\nan analogous kernel $P_s$ that averages $s<m$ of these same random variables.\nUsing an embedding technique to facilitate comparisons, we show that the\nasymptotic variances of ergodic averages associated with $P_m$ are lower\nbounded in terms of those associated with $P_s$. We show that the bound\nprovided is tight and disprove a conjecture that when the random variables to\nbe averaged are independent, the asymptotic variance under $P_m$ is never less\nthan $s/m$ times the variance under $P_s$. The conjecture does, however, hold\nwhen considering continuous-time Markov chains. These results imply that if the\ncomputational cost of the algorithm is proportional to $m$, it is often better\nto set $m=1$. We provide intuition as to why these findings differ so markedly\nfrom recent results for pseudo-marginal kernels employing particle filter\napproximations. Our results are exemplified through two simulation studies; in\nthe first the computational cost is effectively proportional to $m$ and in the\nsecond there is a considerable start-up cost at each iteration. \n\n"}
{"id": "1611.00114", "contents": "Title: Faces of highest weight modules and the universal Weyl polyhedron Abstract: Let $V$ be a highest weight module over a Kac-Moody algebra $\\mathfrak{g}$,\nand let conv $V$ denote the convex hull of its weights. We determine the\ncombinatorial isomorphism type of conv $V$, i.e. we completely classify the\nfaces and their inclusions. In the special case where $\\mathfrak{g}$ is\nsemisimple, this brings closure to a question studied by Cellini-Marietti [IMRN\n2015] for the adjoint representation, and by Khare [J. Algebra 2016; Trans.\nAmer. Math. Soc. 2017] for most modules. The determination of faces of\nfinite-dimensional modules up to the Weyl group action and some of their\ninclusions also appears in previous work of Satake [Ann. of Math. 1960],\nBorel-Tits [IHES Publ. Math. 1965], Vinberg [Izv. Akad. Nauk 1990], and\nCasselman [Austral. Math. Soc. 1997].\n  For any subset of the simple roots, we introduce a remarkable convex cone\nwhich we call the universal Weyl polyhedron, which controls the convex hulls of\nall modules parabolically induced from the corresponding Levi factor. Namely,\nthe combinatorial isomorphism type of the cone stores the classification of\nfaces for all such highest weight modules, as well as how faces degenerate as\nthe highest weight gets increasingly singular. To our knowledge, this cone is\nnew in finite and infinite type.\n  We further answer a question of Michel Brion, by showing that the\nlocalization of conv $V$ along a face is always the convex hull of the weights\nof a parabolically induced module. Finally, as we determine the inclusion\nrelations between faces representation-theoretically from the set of weights,\nwithout recourse to convexity, we answer a similar question for highest weight\nmodules over symmetrizable quantum groups. \n\n"}
{"id": "1611.01182", "contents": "Title: Golden-Ratio-Based Rectangular Tilings Abstract: A golden-ratio-based rectangular tiling of the first quadrant of the\nEuclidean plane is constructed by drawing vertical and horizontal grid lines\nwhich are located at all even powers of $\\phi$ along one axis, and at all odd\npowers of $\\phi$ on the other axis. The vertices of the rectangles formed by\nthese lines can be connected by rays starting at the origin having slopes that\nare odd powers of $\\phi$. A refinement of this tiling results in the familiar\none with horizontal and vertical grid lines at every power of $\\phi$ along each\naxis. Geometric proofs of the convergence of several known power series' in\n$\\phi$ are provided. \n\n"}
{"id": "1611.01238", "contents": "Title: Corrected Bayesian information criterion for stochastic block models Abstract: Estimating the number of communities is one of the fundamental problems in\ncommunity detection. We re-examine the Bayesian paradigm for stochastic block\nmodels and propose a \"corrected Bayesian information criterion\",to determine\nthe number of communities and show that the proposed estimator is consistent\nunder mild conditions. The proposed criterion improves those used in Wang and\nBickel (2016) and Saldana et al. (2017) which tend to underestimate and\noverestimate the number of communities, respectively. Along the way, we\nestablish the Wilks theorem for stochastic block models. Moreover, we show\nthat, to obtain the consistency of model selection for stochastic block models,\nwe need a so-called \"consistency condition\". We also provide sufficient\nconditions for both homogenous networks and non-homogenous networks. The\nresults are further extended to degree corrected stochastic block models.\nNumerical studies demonstrate our theoretical results. \n\n"}
{"id": "1611.01504", "contents": "Title: Estimating Causal Direction and Confounding of Two Discrete Variables Abstract: We propose a method to classify the causal relationship between two discrete\nvariables given only the joint distribution of the variables, acknowledging\nthat the method is subject to an inherent baseline error. We assume that the\ncausal system is acyclicity, but we do allow for hidden common causes. Our\nalgorithm presupposes that the probability distributions $P(C)$ of a cause $C$\nis independent from the probability distribution $P(E\\mid C)$ of the\ncause-effect mechanism. While our classifier is trained with a Bayesian\nassumption of flat hyperpriors, we do not make this assumption about our test\ndata. This work connects to recent developments on the identifiability of\ncausal models over continuous variables under the assumption of \"independent\nmechanisms\". Carefully-commented Python notebooks that reproduce all our\nexperiments are available online at\nhttp://vision.caltech.edu/~kchalupk/code.html. \n\n"}
{"id": "1611.02163", "contents": "Title: Unrolled Generative Adversarial Networks Abstract: We introduce a method to stabilize Generative Adversarial Networks (GANs) by\ndefining the generator objective with respect to an unrolled optimization of\nthe discriminator. This allows training to be adjusted between using the\noptimal discriminator in the generator's objective, which is ideal but\ninfeasible in practice, and using the current value of the discriminator, which\nis often unstable and leads to poor solutions. We show how this technique\nsolves the common problem of mode collapse, stabilizes training of GANs with\ncomplex recurrent generators, and increases diversity and coverage of the data\ndistribution by the generator. \n\n"}
{"id": "1611.03033", "contents": "Title: On the Diffusion Geometry of Graph Laplacians and Applications Abstract: We study directed, weighted graphs $G=(V,E)$ and consider the (not\nnecessarily symmetric) averaging operator $$ (\\mathcal{L}u)(i) = -\\sum_{j\n\\sim_{} i}{p_{ij} (u(j) - u(i))},$$ where $p_{ij}$ are normalized edge weights.\nGiven a vertex $i \\in V$, we define the diffusion distance to a set $B \\subset\nV$ as the smallest number of steps $d_{B}(i) \\in \\mathbb{N}$ required for half\nof all random walks started in $i$ and moving randomly with respect to the\nweights $p_{ij}$ to visit $B$ within $d_{B}(i)$ steps. Our main result is that\nthe eigenfunctions interact nicely with this notion of distance. In particular,\nif $u$ satisfies $\\mathcal{L}u = \\lambda u$ on $V$ and $$ B = \\left\\{ i \\in V:\n- \\varepsilon \\leq u(i) \\leq \\varepsilon \\right\\} \\neq \\emptyset,$$ then, for\nall $i \\in V$, $$ d_{B}(i) \\log{\\left( \\frac{1}{|1-\\lambda|} \\right) } \\geq\n\\log{\\left( \\frac{ |u(i)| }{\\|u\\|_{L^{\\infty}}} \\right)} -\n\\log{\\left(\\frac{1}{2} + \\varepsilon\\right)}.$$ $d_B(i)$ is a remarkably good\napproximation of $|u|$ in the sense of having very high correlation. The result\nimplies that the classical one-dimensional spectral embedding preserves\nparticular aspects of geometry in the presence of clustered data. We also give\na continuous variant of the result which has a connection to the hot spots\nconjecture. \n\n"}
{"id": "1611.03112", "contents": "Title: Multiple imputation of multilevel missing data: An introduction to the R\n  package pan Abstract: The treatment of missing data can be difficult in multilevel research because\nstate-of-the-art procedures such as multiple imputation (MI) may require\nadvanced statistical knowledge or a high degree of familiarity with certain\nstatistical software. In the missing data literature, pan has been recommended\nfor MI of multilevel data. In this article, we provide an introduction to MI of\nmultilevel missing data using the R package pan, and we discuss its\npossibilities and limitations in accommodating typical questions in multilevel\nresearch. In order to make pan more accessible to applied researchers, we make\nuse of the mitml package, which provides a user-friendly interface to the pan\npackage and several tools for managing and analyzing multiply imputed data\nsets. We illustrate the use of pan and mitml with two empirical examples that\nrepresent common applications of multilevel models, and we discuss how these\nprocedures may be used in conjunction with other software. \n\n"}
{"id": "1611.03146", "contents": "Title: The Control of the False Discovery Rate in Fixed Sequence Multiple\n  Testing Abstract: Controlling the false discovery rate (FDR) is a powerful approach to multiple\ntesting. In many applications, the tested hypotheses have an inherent\nhierarchical structure. In this paper, we focus on the fixed sequence structure\nwhere the testing order of the hypotheses has been strictly specified in\nadvance. We are motivated to study such a structure, since it is the most basic\nof hierarchical structures, yet it is often seen in real applications such as\nstatistical process control and streaming data analysis. We first consider a\nconventional fixed sequence method that stops testing once an acceptance\noccurs, and develop such a method controlling the FDR under both arbitrary and\nnegative dependencies. The method under arbitrary dependency is shown to be\nunimprovable without losing control of the FDR and unlike existing FDR methods;\nit cannot be improved even by restricting to the usual positive regression\ndependence on subset (PRDS) condition. To account for any potential mistakes in\nthe ordering of the tests, we extend the conventional fixed sequence method to\none that allows more but a given number of acceptances. Simulation studies show\nthat the proposed procedures can be powerful alternatives to existing FDR\ncontrolling procedures. The proposed procedures are illustrated through a real\ndata set from a microarray experiment. \n\n"}
{"id": "1611.03177", "contents": "Title: A Note on Random Walks with Absorbing barriers and Sequential Monte\n  Carlo Methods Abstract: In this article we consider importance sampling (IS) and sequential Monte\nCarlo (SMC) methods in the context of 1-dimensional random walks with absorbing\nbarriers. In particular, we develop a very precise variance analysis for\nseveral IS and SMC procedures. We take advantage of some explicit spectral\nformulae available for these models to derive sharp and explicit estimates;\nthis provides stability properties of the associated normalized Feynman-Kac\nsemigroups. Our analysis allows one to compare the variance of SMC and IS\ntechniques for these models. The work in this article, is one of the few to\nconsider an in-depth analysis of an SMC method for a particular model-type as\nwell as variance comparison of SMC algorithms. \n\n"}
{"id": "1611.04831", "contents": "Title: The Power of Normalization: Faster Evasion of Saddle Points Abstract: A commonly used heuristic in non-convex optimization is Normalized Gradient\nDescent (NGD) - a variant of gradient descent in which only the direction of\nthe gradient is taken into account and its magnitude ignored. We analyze this\nheuristic and show that with carefully chosen parameters and noise injection,\nthis method can provably evade saddle points. We establish the convergence of\nNGD to a local minimum, and demonstrate rates which improve upon the fastest\nknown first order algorithm due to Ge e al. (2015).\n  The effectiveness of our method is demonstrated via an application to the\nproblem of online tensor decomposition; a task for which saddle point evasion\nis known to result in convergence to global minima. \n\n"}
{"id": "1611.05201", "contents": "Title: Multiscale inference for multivariate deconvolution Abstract: In this paper we provide new methodology for inference of the geometric\nfeatures of a multivariate density in deconvolution. Our approach is based on\nmultiscale tests to detect significant directional derivatives of the unknown\ndensity at arbitrary points in arbitrary directions. The multiscale method is\nused to identify regions of monotonicity and to construct a general procedure\nfor the detection of modes of the multivariate density. Moreover, as an\nimportant application a significance test for the presence of a local maximum\nat a pre-specified point is proposed. The performance of the new methods is\ninvestigated from a theoretical point of view and the finite sample properties\nare illustrated by means of a small simulation study. \n\n"}
{"id": "1611.05934", "contents": "Title: Increasing the Interpretability of Recurrent Neural Networks Using\n  Hidden Markov Models Abstract: As deep neural networks continue to revolutionize various application\ndomains, there is increasing interest in making these powerful models more\nunderstandable and interpretable, and narrowing down the causes of good and bad\npredictions. We focus on recurrent neural networks, state of the art models in\nspeech recognition and translation. Our approach to increasing interpretability\nis by combining a long short-term memory (LSTM) model with a hidden Markov\nmodel (HMM), a simpler and more transparent model. We add the HMM state\nprobabilities to the output layer of the LSTM, and then train the HMM and LSTM\neither sequentially or jointly. The LSTM can make use of the information from\nthe HMM, and fill in the gaps when the HMM is not performing well. A small\nhybrid model usually performs better than a standalone LSTM of the same size,\nespecially on smaller data sets. We test the algorithms on text data and\nmedical time series data, and find that the LSTM and HMM learn complementary\ninformation about the features in the text. \n\n"}
{"id": "1611.06882", "contents": "Title: Learning From Graph Neighborhoods Using LSTMs Abstract: Many prediction problems can be phrased as inferences over local\nneighborhoods of graphs. The graph represents the interaction between entities,\nand the neighborhood of each entity contains information that allows the\ninferences or predictions. We present an approach for applying machine learning\ndirectly to such graph neighborhoods, yielding predicitons for graph nodes on\nthe basis of the structure of their local neighborhood and the features of the\nnodes in it. Our approach allows predictions to be learned directly from\nexamples, bypassing the step of creating and tuning an inference model or\nsummarizing the neighborhoods via a fixed set of hand-crafted features. The\napproach is based on a multi-level architecture built from Long Short-Term\nMemory neural nets (LSTMs); the LSTMs learn how to summarize the neighborhood\nfrom data. We demonstrate the effectiveness of the proposed technique on a\nsynthetic example and on real-world data related to crowdsourced grading,\nBitcoin transactions, and Wikipedia edit reversions. \n\n"}
{"id": "1611.07051", "contents": "Title: Time Series Structure Discovery via Probabilistic Program Synthesis Abstract: There is a widespread need for techniques that can discover structure from\ntime series data. Recently introduced techniques such as Automatic Bayesian\nCovariance Discovery (ABCD) provide a way to find structure within a single\ntime series by searching through a space of covariance kernels that is\ngenerated using a simple grammar. While ABCD can identify a broad class of\ntemporal patterns, it is difficult to extend and can be brittle in practice.\nThis paper shows how to extend ABCD by formulating it in terms of probabilistic\nprogram synthesis. The key technical ideas are to (i) represent models using\nabstract syntax trees for a domain-specific probabilistic language, and (ii)\nrepresent the time series model prior, likelihood, and search strategy using\nprobabilistic programs in a sufficiently expressive language. The final\nprobabilistic program is written in under 70 lines of probabilistic code in\nVenture. The paper demonstrates an application to time series clustering that\ninvolves a non-parametric extension to ABCD, experiments for interpolation and\nextrapolation on real-world econometric data, and improvements in accuracy over\nboth non-parametric and standard regression baselines. \n\n"}
{"id": "1611.09252", "contents": "Title: Fast Mixing Random Walks and Regularity of Incompressible Vector Fields Abstract: We show sufficient conditions under which the \\textsc{BallWalk} algorithm\nmixes fast in a bounded connected subset of $\\Real^n$. In particular, we show\nfast mixing if the space is the transformation of a convex space under a smooth\nincompressible flow. Construction of such smooth flows is in turn reduced to\nthe study of the regularity of the solution of the Dirichlet problem for\nLaplace's equation. \n\n"}
{"id": "1612.00516", "contents": "Title: Canonical Correlation Analysis for Analyzing Sequences of Medical\n  Billing Codes Abstract: We propose using canonical correlation analysis (CCA) to generate features\nfrom sequences of medical billing codes. Applying this novel use of CCA to a\ndatabase of medical billing codes for patients with diverticulitis, we first\ndemonstrate that the CCA embeddings capture meaningful relationships among the\ncodes. We then generate features from these embeddings and establish their\nusefulness in predicting future elective surgery for diverticulitis, an\nimportant marker in efforts for reducing costs in healthcare. \n\n"}
{"id": "1612.00796", "contents": "Title: Overcoming catastrophic forgetting in neural networks Abstract: The ability to learn tasks in a sequential fashion is crucial to the\ndevelopment of artificial intelligence. Neural networks are not, in general,\ncapable of this and it has been widely thought that catastrophic forgetting is\nan inevitable feature of connectionist models. We show that it is possible to\novercome this limitation and train networks that can maintain expertise on\ntasks which they have not experienced for a long time. Our approach remembers\nold tasks by selectively slowing down learning on the weights important for\nthose tasks. We demonstrate our approach is scalable and effective by solving a\nset of classification tasks based on the MNIST hand written digit dataset and\nby learning several Atari 2600 games sequentially. \n\n"}
{"id": "1612.00877", "contents": "Title: Bayesian sparse multiple regression for simultaneous rank reduction and\n  variable selection Abstract: We develop a Bayesian methodology aimed at simultaneously estimating low-rank\nand row-sparse matrices in a high-dimensional multiple-response linear\nregression model. We consider a carefully devised shrinkage prior on the matrix\nof regression coefficients which obviates the need to specify a prior on the\nrank, and shrinks the regression matrix towards low-rank and row-sparse\nstructures. We provide theoretical support to the proposed methodology by\nproving minimax optimality of the posterior mean under the prediction risk in\nultra-high dimensional settings where the number of predictors can grow\nsub-exponentially relative to the sample size. A one-step post-processing\nscheme induced by group lasso penalties on the rows of the estimated\ncoefficient matrix is proposed for variable selection, with default choices of\ntuning parameters. We additionally provide an estimate of the rank using a\nnovel optimization function achieving dimension reduction in the covariate\nspace. We exhibit the performance of the proposed methodology in an extensive\nsimulation study and a real data example. \n\n"}
{"id": "1612.01453", "contents": "Title: A note and a short survey on supporting lines of compact convex sets in\n  the plane Abstract: After surveying some known properties of compact convex sets in the plane, we\ngive a two rigorous proofs of the general feeling that supporting lines can be\nslide-turned slowly and continuously. Targeting a wide readership, our\ntreatment is elementary on purpose. \n\n"}
{"id": "1612.01872", "contents": "Title: Simulation from quasi-stationary distributions on reducible state spaces Abstract: Quasi-stationary distributions (QSDs)arise from stochastic processes that\nexhibit transient equilibrium behaviour on the way to absorption QSDs are often\nmathematically intractable and even drawing samples from them is not\nstraightforward. In this paper the framework of Sequential Monte Carlo samplers\nis utilized to simulate QSDs and several novel resampling techniques are\nproposed to accommodate models with reducible state spaces, with particular\nfocus on preserving particle diversity on discrete spaces. Finally an approach\nis considered to estimate eigenvalues associated with QSDs, such as the decay\nparameter. \n\n"}
{"id": "1612.02358", "contents": "Title: A-optimal encoding weights for nonlinear inverse problems, with\n  applications to the Helmholtz inverse problem Abstract: The computational cost of solving an inverse problem governed by PDEs, using\nmultiple experiments, increases linearly with the number of experiments. A\nrecently proposed method to decrease this cost uses only a small number of\nrandom linear combinations of all experiments for solving the inverse problem.\nThis approach applies to inverse problems where the PDE solution depends\nlinearly on the right-hand side function that models the experiment. As this\nmethod is stochastic in essence, the quality of the obtained reconstructions\ncan vary, in particular when only a small number of combinations are used. We\ndevelop a Bayesian formulation for the definition and computation of encoding\nweights that lead to a parameter reconstruction with the least uncertainty. We\ncall these weights A-optimal encoding weights. Our framework applies to inverse\nproblems where the governing PDE is nonlinear with respect to the inversion\nparameter field. We formulate the problem in infinite dimensions and follow the\noptimize-then-discretize approach, devoting special attention to the\ndiscretization and the choice of numerical methods in order to achieve a\ncomputational cost that is independent of the parameter discretization. We\nelaborate our method for a Helmholtz inverse problem, and derive the\nadjoint-based expressions for the gradient of the objective function of the\noptimization problem for finding the A-optimal encoding weights. The proposed\nmethod is potentially attractive for real-time monitoring applications, where\none can invest the effort to compute optimal weights offline, to later solve an\ninverse problem repeatedly, over time, at a fraction of the initial cost. \n\n"}
{"id": "1612.02589", "contents": "Title: Multi-source Transfer Learning with Convolutional Neural Networks for\n  Lung Pattern Analysis Abstract: Early diagnosis of interstitial lung diseases is crucial for their treatment,\nbut even experienced physicians find it difficult, as their clinical\nmanifestations are similar. In order to assist with the diagnosis,\ncomputer-aided diagnosis (CAD) systems have been developed. These commonly rely\non a fixed scale classifier that scans CT images, recognizes textural lung\npatterns and generates a map of pathologies. In a previous study, we proposed a\nmethod for classifying lung tissue patterns using a deep convolutional neural\nnetwork (CNN), with an architecture designed for the specific problem. In this\nstudy, we present an improved method for training the proposed network by\ntransferring knowledge from the similar domain of general texture\nclassification. Six publicly available texture databases are used to pretrain\nnetworks with the proposed architecture, which are then fine-tuned on the lung\ntissue data. The resulting CNNs are combined in an ensemble and their fused\nknowledge is compressed back to a network with the original architecture. The\nproposed approach resulted in an absolute increase of about 2% in the\nperformance of the proposed CNN. The results demonstrate the potential of\ntransfer learning in the field of medical image analysis, indicate the textural\nnature of the problem and show that the method used for training a network can\nbe as important as designing its architecture. \n\n"}
{"id": "1612.02712", "contents": "Title: Scalable Influence Maximization for Multiple Products in Continuous-Time\n  Diffusion Networks Abstract: A typical viral marketing model identifies influential users in a social\nnetwork to maximize a single product adoption assuming unlimited user\nattention, campaign budgets, and time. In reality, multiple products need\ncampaigns, users have limited attention, convincing users incurs costs, and\nadvertisers have limited budgets and expect the adoptions to be maximized soon.\nFacing these user, monetary, and timing constraints, we formulate the problem\nas a submodular maximization task in a continuous-time diffusion model under\nthe intersection of a matroid and multiple knapsack constraints. We propose a\nrandomized algorithm estimating the user influence in a network\n($|\\mathcal{V}|$ nodes, $|\\mathcal{E}|$ edges) to an accuracy of $\\epsilon$\nwith $n=\\mathcal{O}(1/\\epsilon^2)$ randomizations and\n$\\tilde{\\mathcal{O}}(n|\\mathcal{E}|+n|\\mathcal{V}|)$ computations. By\nexploiting the influence estimation algorithm as a subroutine, we develop an\nadaptive threshold greedy algorithm achieving an approximation factor $k_a/(2+2\nk)$ of the optimal when $k_a$ out of the $k$ knapsack constraints are active.\nExtensive experiments on networks of millions of nodes demonstrate that the\nproposed algorithms achieve the state-of-the-art in terms of effectiveness and\nscalability. \n\n"}
{"id": "1612.03663", "contents": "Title: Analysis and Optimization of Loss Functions for Multiclass, Top-k, and\n  Multilabel Classification Abstract: Top-k error is currently a popular performance measure on large scale image\nclassification benchmarks such as ImageNet and Places. Despite its wide\nacceptance, our understanding of this metric is limited as most of the previous\nresearch is focused on its special case, the top-1 error. In this work, we\nexplore two directions that shed more light on the top-k error. First, we\nprovide an in-depth analysis of established and recently proposed single-label\nmulticlass methods along with a detailed account of efficient optimization\nalgorithms for them. Our results indicate that the softmax loss and the smooth\nmulticlass SVM are surprisingly competitive in top-k error uniformly across all\nk, which can be explained by our analysis of multiclass top-k calibration.\nFurther improvements for a specific k are possible with a number of proposed\ntop-k loss functions. Second, we use the top-k methods to explore the\ntransition from multiclass to multilabel learning. In particular, we find that\nit is possible to obtain effective multilabel classifiers on Pascal VOC using a\nsingle label per image for training, while the gap between multiclass and\nmultilabel methods on MS COCO is more significant. Finally, our contribution of\nefficient algorithms for training with the considered top-k and multilabel loss\nfunctions is of independent interest. \n\n"}
{"id": "1612.03839", "contents": "Title: Tensor Decompositions via Two-Mode Higher-Order SVD (HOSVD) Abstract: Tensor decompositions have rich applications in statistics and machine\nlearning, and developing efficient, accurate algorithms for the problem has\nreceived much attention recently. Here, we present a new method built on\nKruskal's uniqueness theorem to decompose symmetric, nearly orthogonally\ndecomposable tensors. Unlike the classical higher-order singular value\ndecomposition which unfolds a tensor along a single mode, we consider\nunfoldings along two modes and use rank-1 constraints to characterize the\nunderlying components. This tensor decomposition method provably handles a\ngreater level of noise compared to previous methods and achieves a high\nestimation accuracy. Numerical results demonstrate that our algorithm is robust\nto various noise distributions and that it performs especially favorably as the\norder increases. \n\n"}
{"id": "1612.04021", "contents": "Title: Generative Adversarial Parallelization Abstract: Generative Adversarial Networks have become one of the most studied\nframeworks for unsupervised learning due to their intuitive formulation. They\nhave also been shown to be capable of generating convincing examples in limited\ndomains, such as low-resolution images. However, they still prove difficult to\ntrain in practice and tend to ignore modes of the data generating distribution.\nQuantitatively capturing effects such as mode coverage and more generally the\nquality of the generative model still remain elusive. We propose Generative\nAdversarial Parallelization, a framework in which many GANs or their variants\nare trained simultaneously, exchanging their discriminators. This eliminates\nthe tight coupling between a generator and discriminator, leading to improved\nconvergence and improved coverage of modes. We also propose an improved variant\nof the recently proposed Generative Adversarial Metric and show how it can\nscore individual GANs or their collections under the GAP model. \n\n"}
{"id": "1612.04093", "contents": "Title: Modified Cholesky Riemann Manifold Hamiltonian Monte Carlo: Exploiting\n  Sparsity for Fast Sampling of High-dimensional Targets Abstract: Riemann manifold Hamiltonian Monte Carlo (RMHMC) has the potential to produce\nhigh-quality Markov chain Monte Carlo-output even for very challenging target\ndistributions. To this end, a symmetric positive definite scaling matrix for\nRMHMC, which derives, via a modified Cholesky factorization, from the\npotentially indefinite negative Hessian of the target log-density is proposed.\nThe methodology is able to exploit the sparsity of the Hessian, stemming from\nconditional independence modeling assumptions, and thus admit fast\nimplementation of RMHMC even for high-dimensional target distributions.\nMoreover, the methodology can exploit log-concave conditional target densities,\noften encountered in Bayesian hierarchical models, for faster sampling and more\nstraight forward tuning. The proposed methodology is compared to alternatives\nfor some challenging targets, and is illustrated by applying a state space\nmodel to real data. \n\n"}
{"id": "1612.04271", "contents": "Title: BayesBD: An R Package for Bayesian Inference on Image Boundaries Abstract: We present the BayesBD package providing Bayesian inference for boundaries of\nnoisy images. The BayesBD package implements flexible Gaussian process priors\nindexed by the circle to recover the boundary in a binary or Gaussian noised\nimage, with the benefits of guaranteed geometric restrictions on the estimated\nboundary, (nearly) minimax optimal and smoothness adaptive convergence rates,\nand convenient joint inferences under certain assumptions. The core sampling\ntasks for our model have linear complexity, and our implementation in c++ using\npackages Rcpp and RcppArmadillo is computationally efficient. Users can access\nthe full functionality of the package in both Rgui and the corresponding shiny\napplication. Additionally, the package includes numerous utility functions to\naid users in data preparation and analysis of results. We compare BayesBD with\nselected existing packages using both simulations and real data applications,\nand demonstrate the excellent performance and flexibility of BayesBD even when\nthe observation contains complicated structural information that may violate\nits assumptions. \n\n"}
{"id": "1612.04467", "contents": "Title: On Procedures Controlling the FDR for Testing Hierarchically Ordered\n  Hypotheses Abstract: Complex large-scale studies, such as those related to microarray data and\nfMRI studies, often involve testing multiple hierarchically ordered hypotheses.\nHowever, most existing false discovery rate (FDR) controlling procedures do not\nexploit the inherent hierarchical structure among the tested hypotheses. In\nthis paper, we first present a generalized stepwise procedure which generalizes\nthe usual stepwise procedure to the case where each hypothesis is tested with a\ndifferent set of critical constants. This procedure is helpful in creating a\ngeneral framework under which our hierarchical testing procedures are\ndeveloped. Then, we present several hierarchical testing procedures which\ncontrol the FDR under various forms of dependence such as positive dependence\nand block dependence. Our simulation studies show that these proposed methods\ncan be more powerful in some situations than alternative methods such as\nYekutieli's hierarchical testing procedure (Yekutieli, \\emph{JASA} \\textbf{103}\n(2008) 309-316). Finally, we apply our proposed procedures to a real data set\ninvolving abundances of microbes in different ecological environments. \n\n"}
{"id": "1701.01325", "contents": "Title: Outlier Detection for Text Data : An Extended Version Abstract: The problem of outlier detection is extremely challenging in many domains\nsuch as text, in which the attribute values are typically non-negative, and\nmost values are zero. In such cases, it often becomes difficult to separate the\noutliers from the natural variations in the patterns in the underlying data. In\nthis paper, we present a matrix factorization method, which is naturally able\nto distinguish the anomalies with the use of low rank approximations of the\nunderlying data. Our iterative algorithm TONMF is based on block coordinate\ndescent (BCD) framework. We define blocks over the term-document matrix such\nthat the function becomes solvable. Given most recently updated values of other\nmatrix blocks, we always update one block at a time to its optimal. Our\napproach has significant advantages over traditional methods for text outlier\ndetection. Finally, we present experimental results illustrating the\neffectiveness of our method over competing methods. \n\n"}
{"id": "1701.02789", "contents": "Title: Identifying Best Interventions through Online Importance Sampling Abstract: Motivated by applications in computational advertising and systems biology,\nwe consider the problem of identifying the best out of several possible soft\ninterventions at a source node $V$ in an acyclic causal directed graph, to\nmaximize the expected value of a target node $Y$ (located downstream of $V$).\nOur setting imposes a fixed total budget for sampling under various\ninterventions, along with cost constraints on different types of interventions.\nWe pose this as a best arm identification bandit problem with $K$ arms where\neach arm is a soft intervention at $V,$ and leverage the information leakage\namong the arms to provide the first gap dependent error and simple regret\nbounds for this problem. Our results are a significant improvement over the\ntraditional best arm identification results. We empirically show that our\nalgorithms outperform the state of the art in the Flow Cytometry data-set, and\nalso apply our algorithm for model interpretation of the Inception-v3 deep net\nthat classifies images. \n\n"}
{"id": "1701.03772", "contents": "Title: Additive Partially Linear Models for Massive Heterogeneous Data Abstract: We consider an additive partially linear framework for modelling massive\nheterogeneous data. The major goal is to extract multiple common features\nsimultaneously across all sub-populations while exploring heterogeneity of each\nsub-population. We propose an aggregation type of estimators for the\ncommonality parameters that possess the asymptotic optimal bounds and the\nasymptotic distributions as if there were no heterogeneity. This oracle result\nholds when the number of sub-populations does not grow too fast and the tuning\nparameters are selected carefully. A plug-in estimator for the heterogeneity\nparameter is further constructed, and shown to possess the asymptotic\ndistribution as if the commonality information were available. Furthermore, we\ndevelop a heterogeneity test for the linear components and a homogeneity test\nfor the non-linear components accordingly. The performance of the proposed\nmethods is evaluated via simulation studies and an application to the Medicare\nProvider Utilization and Payment data. \n\n"}
{"id": "1701.05146", "contents": "Title: On parameter estimation with the Wasserstein distance Abstract: Statistical inference can be performed by minimizing, over the parameter\nspace, the Wasserstein distance between model distributions and the empirical\ndistribution of the data. We study asymptotic properties of such minimum\nWasserstein distance estimators, complementing results derived by Bassetti,\nBodini and Regazzini in 2006. In particular, our results cover the misspecified\nsetting, in which the data-generating process is not assumed to be part of the\nfamily of distributions described by the model. Our results are motivated by\nrecent applications of minimum Wasserstein estimators to complex generative\nmodels. We discuss some difficulties arising in the approximation of these\nestimators and illustrate their behavior in several numerical experiments. Two\nof our examples are taken from the literature on approximate Bayesian\ncomputation and have likelihood functions that are not analytically tractable.\nTwo other examples involve misspecified models. \n\n"}
{"id": "1701.05892", "contents": "Title: Bayesian Static Parameter Estimation for Partially Observed Diffusions\n  via Multilevel Monte Carlo Abstract: In this article we consider static Bayesian parameter estimation for\npartially observed diffusions that are discretely observed. We work under the\nassumption that one must resort to discretizing the underlying diffusion\nprocess, for instance using the Euler-Maruyama method. Given this assumption,\nwe show how one can use Markov chain Monte Carlo (MCMC) and particularly\nparticle MCMC [Andrieu, C., Doucet, A. and Holenstein, R. (2010). Particle\nMarkov chain Monte Carlo methods (with discussion). J. R. Statist. Soc. Ser. B,\n72, 269--342] to implement a new approximation of the multilevel (ML) Monte\nCarlo (MC) collapsing sum identity. Our approach comprises constructing an\napproximate coupling of the posterior density of the joint distribution over\nparameter and hidden variables at two different discretization levels and then\ncorrecting by an importance sampling method. The variance of the weights are\nindependent of the length of the observed data set. The utility of such a\nmethod is that, for a prescribed level of mean square error, the cost of this\nMLMC method is provably less than i.i.d. sampling from the posterior associated\nto the most precise discretization. However the method here comprises using\nonly known and efficient simulation methodologies. The theoretical results are\nillustrated by inference of the parameters of two prototypical processes given\nnoisy partial observations of the process: the first is an Ornstein Uhlenbeck\nprocess and the second is a more general Langevin equation. \n\n"}
{"id": "1701.06597", "contents": "Title: Iterative Thresholding for Demixing Structured Superpositions in High\n  Dimensions Abstract: We consider the demixing problem of two (or more) high-dimensional vectors\nfrom nonlinear observations when the number of such observations is far less\nthan the ambient dimension of the underlying vectors. Specifically, we\ndemonstrate an algorithm that stably estimate the underlying components under\ngeneral \\emph{structured sparsity} assumptions on these components.\nSpecifically, we show that for certain types of structured superposition\nmodels, our method provably recovers the components given merely $n =\n\\mathcal{O}(s)$ samples where $s$ denotes the number of nonzero entries in the\nunderlying components. Moreover, our method achieves a fast (linear)\nconvergence rate, and also exhibits fast (near-linear) per-iteration complexity\nfor certain types of structured models. We also provide a range of simulations\nto illustrate the performance of the proposed algorithm. \n\n"}
{"id": "1701.06619", "contents": "Title: Bayesian Inference in the Presence of Intractable Normalizing Functions Abstract: Models with intractable normalizing functions arise frequently in statistics.\nCommon examples of such models include exponential random graph models for\nsocial networks and Markov point processes for ecology and disease modeling.\nInference for these models is complicated because the normalizing functions of\ntheir probability distributions include the parameters of interest. In Bayesian\nanalysis they result in so-called doubly intractable posterior distributions\nwhich pose significant computational challenges. Several Monte Carlo methods\nhave emerged in recent years to address Bayesian inference for such models. We\nprovide a framework for understanding the algorithms and elucidate connections\namong them. Through multiple simulated and real data examples, we compare and\ncontrast the computational and statistical efficiency of these algorithms and\ndiscuss their theoretical bases. Our study provides practical recommendations\nfor practitioners along with directions for future research for MCMC\nmethodologists. \n\n"}
{"id": "1702.00204", "contents": "Title: Bayesian model selection for the latent position cluster model for\n  Social Networks Abstract: The latent position cluster model is a popular model for the statistical\nanalysis of network data. This model assumes that there is an underlying latent\nspace in which the actors follow a finite mixture distribution. Moreover,\nactors which are close in this latent space are more likely to be tied by an\nedge. This is an appealing approach since it allows the model to cluster actors\nwhich consequently provides the practitioner with useful qualitative\ninformation. However, exploring the uncertainty in the number of underlying\nlatent components in the mixture distribution is a complex task. The current\nstate-of-the-art is to use an approximate form of BIC for this purpose, where\nan approximation of the log-likelihood is used instead of the true\nlog-likelihood which is unavailable. The main contribution of this paper is to\nshow that through the use of conjugate prior distributions it is possible to\nanalytically integrate out almost all of the model parameters, leaving a\nposterior distribution which depends on the allocation vector of the mixture\nmodel. This enables posterior inference over the number of components in the\nlatent mixture distribution without using trans- dimensional MCMC algorithms\nsuch as reversible jump MCMC. Our approach is compared with the\nstate-of-the-art latentnet (Krivitsky & Handcock 2015) and VBLPCM\n(Salter-Townshend & Murphy 2013) packages. \n\n"}
{"id": "1702.00428", "contents": "Title: Malliavin-based Multilevel Monte Carlo Estimators for Densities of\n  Max-stable Processes Abstract: We introduce a class of unbiased Monte Carlo estimators for the multivariate\ndensity of max-stable fields generated by Gaussian processes. Our estimators\ntake advantage of recent results on exact simulation of max-stable fields\ncombined with identities studied in the Malliavin calculus literature and ideas\ndeveloped in the multilevel Monte Carlo literature. Our approach allows\nestimating multivariate densities of max-stable fields with precision\n$\\varepsilon $ at a computational cost of order $O\\left( \\varepsilon ^{-2}\\log\n\\log \\log \\left( 1/\\varepsilon \\right) \\right) $. \n\n"}
{"id": "1702.00763", "contents": "Title: Natasha: Faster Non-Convex Stochastic Optimization Via Strongly\n  Non-Convex Parameter Abstract: Given a nonconvex function that is an average of $n$ smooth functions, we\ndesign stochastic first-order methods to find its approximate stationary\npoints. The convergence of our new methods depends on the smallest (negative)\neigenvalue $-\\sigma$ of the Hessian, a parameter that describes how nonconvex\nthe function is.\n  Our methods outperform known results for a range of parameter $\\sigma$, and\ncan be used to find approximate local minima. Our result implies an interesting\ndichotomy: there exists a threshold $\\sigma_0$ so that the currently fastest\nmethods for $\\sigma>\\sigma_0$ and for $\\sigma<\\sigma_0$ have different\nbehaviors: the former scales with $n^{2/3}$ and the latter scales with\n$n^{3/4}$. \n\n"}
{"id": "1702.01185", "contents": "Title: Basis Adaptive Sample Efficient Polynomial Chaos (BASE-PC) Abstract: For a large class of orthogonal basis functions, there has been a recent\nidentification of expansion methods for computing accurate, stable\napproximations of a quantity of interest. This paper presents, within the\ncontext of uncertainty quantification, a practical implementation using basis\nadaptation, and coherence motivated sampling, which under assumptions has\nsatisfying guarantees. This implementation is referred to as Basis Adaptive\nSample Efficient Polynomial Chaos (BASE-PC). A key component of this is the use\nof anisotropic polynomial order which admits evolving global bases for\napproximation in an efficient manner, leading to consistently stable\napproximation for a practical class of smooth functionals. This fully adaptive,\nnon-intrusive method, requires no a priori information of the solution, and has\nsatisfying theoretical guarantees of recovery. A key contribution to stability\nis the use of a presented correction sampling for coherence-optimal sampling in\norder to improve stability and accuracy within the adaptive basis scheme.\nTheoretically, the method may dramatically reduce the impact of dimensionality\nin function approximation, and numerically the method is demonstrated to\nperform well on problems with dimension up to 1000. \n\n"}
{"id": "1702.01906", "contents": "Title: Affiliation networks with an increasing degree sequence Abstract: Affiliation network is one kind of two-mode social network with two different\nsets of nodes (namely, a set of actors and a set of social events) and edges\nrepresenting the affiliation of the actors with the social events. Although a\nnumber of statistical models are proposed to analyze affiliation networks, the\nasymptotic behaviors of the estimator are still unknown or have not been\nproperly explored. In this paper, we study an affiliation model with the degree\nsequence as the exclusively natural sufficient statistic in the exponential\nfamily distributions. We establish the uniform consistency and asymptotic\nnormality of the maximum likelihood estimator when the numbers of actors and\nevents both go to infinity. Simulation studies and a real data example\ndemonstrate our theoretical results. \n\n"}
{"id": "1702.02715", "contents": "Title: A Fast and Scalable Joint Estimator for Learning Multiple Related Sparse\n  Gaussian Graphical Models Abstract: Estimating multiple sparse Gaussian Graphical Models (sGGMs) jointly for many\nrelated tasks (large $K$) under a high-dimensional (large $p$) situation is an\nimportant task. Most previous studies for the joint estimation of multiple\nsGGMs rely on penalized log-likelihood estimators that involve expensive and\ndifficult non-smooth optimizations. We propose a novel approach, FASJEM for\n\\underline{fa}st and \\underline{s}calable \\underline{j}oint\nstructure-\\underline{e}stimation of \\underline{m}ultiple sGGMs at a large\nscale. As the first study of joint sGGM using the Elementary Estimator\nframework, our work has three major contributions: (1) We solve FASJEM through\nan entry-wise manner which is parallelizable. (2) We choose a proximal\nalgorithm to optimize FASJEM. This improves the computational efficiency from\n$O(Kp^3)$ to $O(Kp^2)$ and reduces the memory requirement from $O(Kp^2)$ to\n$O(K)$. (3) We theoretically prove that FASJEM achieves a consistent estimation\nwith a convergence rate of $O(\\log(Kp)/n_{tot})$. On several synthetic and four\nreal-world datasets, FASJEM shows significant improvements over baselines on\naccuracy, computational complexity, and memory costs. \n\n"}
{"id": "1702.03056", "contents": "Title: Sparse modeling approach to analytical continuation of imaginary-time\n  quantum Monte Carlo data Abstract: A new approach of solving the ill-conditioned inverse problem for analytical\ncontinuation is proposed. The root of the problem lies in the fact that even\ntiny noise of imaginary-time input data has a serious impact on the inferred\nreal-frequency spectra. By means of a modern regularization technique, we\neliminate redundant degrees of freedom that essentially carry the noise,\nleaving only relevant information unaffected by the noise. The resultant\nspectrum is represented with minimal bases and thus a stable analytical\ncontinuation is achieved. This framework further provides a tool for analyzing\nto what extent the Monte Carlo data need to be accurate to resolve details of\nan expected spectral function. \n\n"}
{"id": "1702.03126", "contents": "Title: Multilevel rejection sampling for approximate Bayesian computation Abstract: Likelihood-free methods, such as approximate Bayesian computation, are\npowerful tools for practical inference problems with intractable likelihood\nfunctions. Markov chain Monte Carlo and sequential Monte Carlo variants of\napproximate Bayesian computation can be effective techniques for sampling\nposterior distributions in an approximate Bayesian computation setting.\nHowever, without careful consideration of convergence criteria and selection of\nproposal kernels, such methods can lead to very biased inference or\ncomputationally inefficient sampling. In contrast, rejection sampling for\napproximate Bayesian computation, despite being computationally intensive,\nresults in independent, identically distributed samples from the approximated\nposterior. An alternative method is proposed for the acceleration of\nlikelihood-free Bayesian inference that applies multilevel Monte Carlo variance\nreduction techniques directly to rejection sampling. The resulting method\nretains the accuracy advantages of rejection sampling while significantly\nimproving the computational efficiency. \n\n"}
{"id": "1702.04031", "contents": "Title: Maximum likelihood estimation in Gaussian models under total positivity Abstract: We analyze the problem of maximum likelihood estimation for Gaussian\ndistributions that are multivariate totally positive of order two (MTP2). By\nexploiting connections to phylogenetics and single-linkage clustering, we give\na simple proof that the maximum likelihood estimator (MLE) for such\ndistributions exists based on at least 2 observations, irrespective of the\nunderlying dimension. Slawski and Hein, who first proved this result, also\nprovided empirical evidence showing that the MTP2 constraint serves as an\nimplicit regularizer and leads to sparsity in the estimated inverse covariance\nmatrix, determining what we name the ML graph. We show that we can find an\nupper bound for the ML graph by adding edges corresponding to correlations in\nexcess of those explained by the maximum weight spanning forest of the\ncorrelation matrix. Moreover, we provide globally convergent coordinate descent\nalgorithms for calculating the MLE under the MTP2 constraint which are\nstructurally similar to iterative proportional scaling. We conclude the paper\nwith a discussion of signed MTP2 distributions. \n\n"}
{"id": "1702.07186", "contents": "Title: Stability of Topic Modeling via Matrix Factorization Abstract: Topic models can provide us with an insight into the underlying latent\nstructure of a large corpus of documents. A range of methods have been proposed\nin the literature, including probabilistic topic models and techniques based on\nmatrix factorization. However, in both cases, standard implementations rely on\nstochastic elements in their initialization phase, which can potentially lead\nto different results being generated on the same corpus when using the same\nparameter values. This corresponds to the concept of \"instability\" which has\npreviously been studied in the context of $k$-means clustering. In many\napplications of topic modeling, this problem of instability is not considered\nand topic models are treated as being definitive, even though the results may\nchange considerably if the initialization process is altered. In this paper we\ndemonstrate the inherent instability of popular topic modeling approaches,\nusing a number of new measures to assess stability. To address this issue in\nthe context of matrix factorization for topic modeling, we propose the use of\nensemble learning strategies. Based on experiments performed on annotated text\ncorpora, we show that a K-Fold ensemble strategy, combining both ensembles and\nstructured initialization, can significantly reduce instability, while\nsimultaneously yielding more accurate topic models. \n\n"}
{"id": "1702.07800", "contents": "Title: On the Origin of Deep Learning Abstract: This paper is a review of the evolutionary history of deep learning models.\nIt covers from the genesis of neural networks when associationism modeling of\nthe brain is studied, to the models that dominate the last decade of research\nin deep learning like convolutional neural networks, deep belief networks, and\nrecurrent neural networks. In addition to a review of these models, this paper\nprimarily focuses on the precedents of the models above, examining how the\ninitial ideas are assembled to construct the early models and how these\npreliminary models are developed into their current forms. Many of these\nevolutionary paths last more than half a century and have a diversity of\ndirections. For example, CNN is built on prior knowledge of biological vision\nsystem; DBN is evolved from a trade-off of modeling power and computation\ncomplexity of graphical models and many nowadays models are neural counterparts\nof ancient linear models. This paper reviews these evolutionary paths and\noffers a concise thought flow of how these models are developed, and aims to\nprovide a thorough background for deep learning. More importantly, along with\nthe path, this paper summarizes the gist behind these milestones and proposes\nmany directions to guide the future research of deep learning. \n\n"}
{"id": "1702.08343", "contents": "Title: Approximate Inference with Amortised MCMC Abstract: We propose a novel approximate inference algorithm that approximates a target\ndistribution by amortising the dynamics of a user-selected MCMC sampler. The\nidea is to initialise MCMC using samples from an approximation network, apply\nthe MCMC operator to improve these samples, and finally use the samples to\nupdate the approximation network thereby improving its quality. This provides a\nnew generic framework for approximate inference, allowing us to deploy highly\ncomplex, or implicitly defined approximation families with intractable\ndensities, including approximations produced by warping a source of randomness\nthrough a deep neural network. Experiments consider image modelling with deep\ngenerative models as a challenging test for the method. Deep models trained\nusing amortised MCMC are shown to generate realistic looking samples as well as\nproducing diverse imputations for images with regions of missing pixels. \n\n"}
{"id": "1702.08359", "contents": "Title: Dynamic Word Embeddings Abstract: We present a probabilistic language model for time-stamped text data which\ntracks the semantic evolution of individual words over time. The model\nrepresents words and contexts by latent trajectories in an embedding space. At\neach moment in time, the embedding vectors are inferred from a probabilistic\nversion of word2vec [Mikolov et al., 2013]. These embedding vectors are\nconnected in time through a latent diffusion process. We describe two scalable\nvariational inference algorithms--skip-gram smoothing and skip-gram\nfiltering--that allow us to train the model jointly over all times; thus\nlearning on all data while simultaneously allowing word and context vectors to\ndrift. Experimental results on three different corpora demonstrate that our\ndynamic model infers word embedding trajectories that are more interpretable\nand lead to higher predictive likelihoods than competing methods that are based\non static models trained separately on time slices. \n\n"}
{"id": "1702.08835", "contents": "Title: Deep Forest Abstract: Current deep learning models are mostly build upon neural networks, i.e.,\nmultiple layers of parameterized differentiable nonlinear modules that can be\ntrained by backpropagation. In this paper, we explore the possibility of\nbuilding deep models based on non-differentiable modules. We conjecture that\nthe mystery behind the success of deep neural networks owes much to three\ncharacteristics, i.e., layer-by-layer processing, in-model feature\ntransformation and sufficient model complexity. We propose the gcForest\napproach, which generates \\textit{deep forest} holding these characteristics.\nThis is a decision tree ensemble approach, with much less hyper-parameters than\ndeep neural networks, and its model complexity can be automatically determined\nin a data-dependent way. Experiments show that its performance is quite robust\nto hyper-parameter settings, such that in most cases, even across different\ndata from different domains, it is able to get excellent performance by using\nthe same default setting. This study opens the door of deep learning based on\nnon-differentiable modules, and exhibits the possibility of constructing deep\nmodels without using backpropagation. \n\n"}
{"id": "1703.00144", "contents": "Title: Theoretical Properties for Neural Networks with Weight Matrices of Low\n  Displacement Rank Abstract: Recently low displacement rank (LDR) matrices, or so-called structured\nmatrices, have been proposed to compress large-scale neural networks. Empirical\nresults have shown that neural networks with weight matrices of LDR matrices,\nreferred as LDR neural networks, can achieve significant reduction in space and\ncomputational complexity while retaining high accuracy. We formally study LDR\nmatrices in deep learning. First, we prove the universal approximation property\nof LDR neural networks with a mild condition on the displacement operators. We\nthen show that the error bounds of LDR neural networks are as efficient as\ngeneral neural networks with both single-layer and multiple-layer structure.\nFinally, we propose back-propagation based training algorithm for general LDR\nneural networks. \n\n"}
{"id": "1703.00368", "contents": "Title: Approximate Computational Approaches for Bayesian Sensor Placement in\n  High Dimensions Abstract: Since the cost of installing and maintaining sensors is usually high, sensor\nlocations are always strategically selected. For those aiming at inferring\ncertain quantities of interest (QoI), it is desirable to explore the dependency\nbetween sensor measurements and QoI. One of the most popular metric for the\ndependency is mutual information which naturally measures how much information\nabout one variable can be obtained given the other. However, computing mutual\ninformation is always challenging, and the result is unreliable in high\ndimension. In this paper, we propose an approach to find an approximate lower\nbound of mutual information and compute it in a lower dimension. Then, sensors\nare placed where highest mutual information (lower bound) is achieved and QoI\nis inferred via Bayes rule given sensor measurements. In addition, Bayesian\noptimization is introduced to provide a continuous mutual information surface\nover the domain and thus reduce the number of evaluations. A chemical release\naccident is simulated where multiple sensors are placed to locate the source of\nthe release. The result shows that the proposed approach is both effective and\nefficient in inferring QoI. \n\n"}
{"id": "1703.01234", "contents": "Title: A Bayesian computer model analysis of Robust Bayesian analyses Abstract: We harness the power of Bayesian emulation techniques, designed to aid the\nanalysis of complex computer models, to examine the structure of complex\nBayesian analyses themselves. These techniques facilitate robust Bayesian\nanalyses and/or sensitivity analyses of complex problems, and hence allow\nglobal exploration of the impacts of choices made in both the likelihood and\nprior specification. We show how previously intractable problems in robustness\nstudies can be overcome using emulation techniques, and how these methods allow\nother scientists to quickly extract approximations to posterior results\ncorresponding to their own particular subjective specification. The utility and\nflexibility of our method is demonstrated on a reanalysis of a real application\nwhere Bayesian methods were employed to capture beliefs about river flow. We\ndiscuss the obvious extensions and directions of future research that such an\napproach opens up. \n\n"}
{"id": "1703.01541", "contents": "Title: Soft-DTW: a Differentiable Loss Function for Time-Series Abstract: We propose in this paper a differentiable learning loss between time series,\nbuilding upon the celebrated dynamic time warping (DTW) discrepancy. Unlike the\nEuclidean distance, DTW can compare time series of variable size and is robust\nto shifts or dilatations across the time dimension. To compute DTW, one\ntypically solves a minimal-cost alignment problem between two time series using\ndynamic programming. Our work takes advantage of a smoothed formulation of DTW,\ncalled soft-DTW, that computes the soft-minimum of all alignment costs. We show\nin this paper that soft-DTW is a differentiable loss function, and that both\nits value and gradient can be computed with quadratic time/space complexity\n(DTW has quadratic time but linear space complexity). We show that this\nregularization is particularly well suited to average and cluster time series\nunder the DTW geometry, a task for which our proposal significantly outperforms\nexisting baselines. Next, we propose to tune the parameters of a machine that\noutputs time series by minimizing its fit with ground-truth labels in a\nsoft-DTW sense. \n\n"}
{"id": "1703.02156", "contents": "Title: On the Limits of Learning Representations with Label-Based Supervision Abstract: Advances in neural network based classifiers have transformed automatic\nfeature learning from a pipe dream of stronger AI to a routine and expected\nproperty of practical systems. Since the emergence of AlexNet every winning\nsubmission of the ImageNet challenge has employed end-to-end representation\nlearning, and due to the utility of good representations for transfer learning,\nrepresentation learning has become as an important and distinct task from\nsupervised learning. At present, this distinction is inconsequential, as\nsupervised methods are state-of-the-art in learning transferable\nrepresentations. But recent work has shown that generative models can also be\npowerful agents of representation learning. Will the representations learned\nfrom these generative methods ever rival the quality of those from their\nsupervised competitors? In this work, we argue in the affirmative, that from an\ninformation theoretic perspective, generative models have greater potential for\nrepresentation learning. Based on several experimentally validated assumptions,\nwe show that supervised learning is upper bounded in its capacity for\nrepresentation learning in ways that certain generative models, such as\nGenerative Adversarial Networks (GANs) are not. We hope that our analysis will\nprovide a rigorous motivation for further exploration of generative\nrepresentation learning. \n\n"}
{"id": "1703.02596", "contents": "Title: Customer Lifetime Value Prediction Using Embeddings Abstract: We describe the Customer LifeTime Value (CLTV) prediction system deployed at\nASOS.com, a global online fashion retailer. CLTV prediction is an important\nproblem in e-commerce where an accurate estimate of future value allows\nretailers to effectively allocate marketing spend, identify and nurture high\nvalue customers and mitigate exposure to losses. The system at ASOS provides\ndaily estimates of the future value of every customer and is one of the\ncornerstones of the personalised shopping experience. The state of the art in\nthis domain uses large numbers of handcrafted features and ensemble regressors\nto forecast value, predict churn and evaluate customer loyalty. Recently,\ndomains including language, vision and speech have shown dramatic advances by\nreplacing handcrafted features with features that are learned automatically\nfrom data. We detail the system deployed at ASOS and show that learning feature\nrepresentations is a promising extension to the state of the art in CLTV\nmodelling. We propose a novel way to generate embeddings of customers, which\naddresses the issue of the ever changing product catalogue and obtain a\nsignificant improvement over an exhaustive set of handcrafted features. \n\n"}
{"id": "1703.03165", "contents": "Title: Perturbation Bootstrap in Adaptive Lasso Abstract: The Adaptive Lasso(Alasso) was proposed by Zou [\\textit{J. Amer. Statist.\nAssoc. \\textbf{101} (2006) 1418-1429}] as a modification of the Lasso for the\npurpose of simultaneous variable selection and estimation of the parameters in\na linear regression model. Zou (2006) established that the Alasso estimator is\nvariable-selection consistent as well as asymptotically Normal in the indices\ncorresponding to the nonzero regression coefficients in certain\nfixed-dimensional settings. In an influential paper, Minnier, Tian and Cai\n[\\textit{J. Amer. Statist. Assoc. \\textbf{106} (2011) 1371-1382}] proposed a\nperturbation bootstrap method and established its distributional consistency\nfor the Alasso estimator in the fixed-dimensional setting. In this paper,\nhowever, we show that this (naive) perturbation bootstrap fails to achieve\nsecond order correctness in approximating the distribution of the Alasso\nestimator. We propose a modification to the perturbation bootstrap objective\nfunction and show that a suitably studentized version of our modified\nperturbation bootstrap Alasso estimator achieves second-order correctness even\nwhen the dimension of the model is allowed to grow to infinity with the sample\nsize. As a consequence, inferences based on the modified perturbation bootstrap\nwill be more accurate than the inferences based on the oracle Normal\napproximation. We give simulation studies demonstrating good finite-sample\nproperties of our modified perturbation bootstrap method as well as an\nillustration of our method on a real data set. \n\n"}
{"id": "1703.03680", "contents": "Title: Strong convergence rates of probabilistic integrators for ordinary\n  differential equations Abstract: Probabilistic integration of a continuous dynamical system is a way of\nsystematically introducing model error, at scales no larger than errors\nintroduced by standard numerical discretisation, in order to enable thorough\nexploration of possible responses of the system to inputs. It is thus a\npotentially useful approach in a number of applications such as forward\nuncertainty quantification, inverse problems, and data assimilation. We extend\nthe convergence analysis of probabilistic integrators for deterministic\nordinary differential equations, as proposed by Conrad et al.\\ (\\textit{Stat.\\\nComput.}, 2017), to establish mean-square convergence in the uniform norm on\ndiscrete- or continuous-time solutions under relaxed regularity assumptions on\nthe driving vector fields and their induced flows. Specifically, we show that\nrandomised high-order integrators for globally Lipschitz flows and randomised\nEuler integrators for dissipative vector fields with polynomially-bounded local\nLipschitz constants all have the same mean-square convergence rate as their\ndeterministic counterparts, provided that the variance of the integration noise\nis not of higher order than the corresponding deterministic integrator. These\nand similar results are proven for probabilistic integrators where the random\nperturbations may be state-dependent, non-Gaussian, or non-centred random\nvariables. \n\n"}
{"id": "1703.03888", "contents": "Title: Segmentation of skin lesions based on fuzzy classification of pixels and\n  histogram thresholding Abstract: This paper proposes an innovative method for segmentation of skin lesions in\ndermoscopy images developed by the authors, based on fuzzy classification of\npixels and histogram thresholding. \n\n"}
{"id": "1703.04389", "contents": "Title: Bayesian Optimization with Gradients Abstract: Bayesian optimization has been successful at global optimization of\nexpensive-to-evaluate multimodal objective functions. However, unlike most\noptimization methods, Bayesian optimization typically does not use derivative\ninformation. In this paper we show how Bayesian optimization can exploit\nderivative information to decrease the number of objective function evaluations\nrequired for good performance. In particular, we develop a novel Bayesian\noptimization algorithm, the derivative-enabled knowledge-gradient (dKG), for\nwhich we show one-step Bayes-optimality, asymptotic consistency, and greater\none-step value of information than is possible in the derivative-free setting.\nOur procedure accommodates noisy and incomplete derivative information, comes\nin both sequential and batch forms, and can optionally reduce the computational\ncost of inference through automatically selected retention of a single\ndirectional derivative. We also compute the d-KG acquisition function and its\ngradient using a novel fast discretization-free technique. We show d-KG\nprovides state-of-the-art performance compared to a wide range of optimization\nprocedures with and without gradients, on benchmarks including logistic\nregression, deep learning, kernel learning, and k-nearest neighbors. \n\n"}
{"id": "1703.05984", "contents": "Title: A Tutorial on Bridge Sampling Abstract: The marginal likelihood plays an important role in many areas of Bayesian\nstatistics such as parameter estimation, model comparison, and model averaging.\nIn most applications, however, the marginal likelihood is not analytically\ntractable and must be approximated using numerical methods. Here we provide a\ntutorial on bridge sampling (Bennett, 1976; Meng & Wong, 1996), a reliable and\nrelatively straightforward sampling method that allows researchers to obtain\nthe marginal likelihood for models of varying complexity. First, we introduce\nbridge sampling and three related sampling methods using the beta-binomial\nmodel as a running example. We then apply bridge sampling to estimate the\nmarginal likelihood for the Expectancy Valence (EV) model---a popular model for\nreinforcement learning. Our results indicate that bridge sampling provides\naccurate estimates for both a single participant and a hierarchical version of\nthe EV model. We conclude that bridge sampling is an attractive method for\nmathematical psychologists who typically aim to approximate the marginal\nlikelihood for a limited set of possibly high-dimensional models. \n\n"}
{"id": "1703.06098", "contents": "Title: Multilevel linear models, Gibbs samplers and multigrid decompositions Abstract: We study the convergence properties of the Gibbs Sampler in the context of\nposterior distributions arising from Bayesian analysis of conditionally\nGaussian hierarchical models. We develop a multigrid approach to derive\nanalytic expressions for the convergence rates of the algorithm for various\nwidely used model structures, including nested and crossed random effects. Our\nresults apply to multilevel models with an arbitrary number of layers in the\nhierarchy, while most previous work was limited to the two-level nested case.\nThe theoretical results provide explicit and easy-to-implement guidelines to\noptimize practical implementations of the Gibbs Sampler, such as indications on\nwhich parametrization to choose (e.g. centred and non-centred), which\nconstraint to impose to guarantee statistical identifiability, and which\nparameters to monitor in the diagnostic process. Simulations suggest that the\nresults are informative also in the context of non-Gaussian distributions and\nmore general MCMC schemes, such as gradient-based ones.implementation of Gibbs\nsamplers on conditionally Gaussian hierarchical models. \n\n"}
{"id": "1703.06359", "contents": "Title: Fully symmetric kernel quadrature Abstract: Kernel quadratures and other kernel-based approximation methods typically\nsuffer from prohibitive cubic time and quadratic space complexity in the number\nof function evaluations. The problem arises because a system of linear\nequations needs to be solved. In this article we show that the weights of a\nkernel quadrature rule can be computed efficiently and exactly for up to tens\nof millions of nodes if the kernel, integration domain, and measure are fully\nsymmetric and the node set is a union of fully symmetric sets. This is based on\nthe observations that in such a setting there are only as many distinct weights\nas there are fully symmetric sets and that these weights can be solved from a\nlinear system of equations constructed out of row sums of certain submatrices\nof the full kernel matrix. We present several numerical examples that show\nfeasibility, both for a large number of nodes and in high dimensions, of the\ndeveloped fully symmetric kernel quadrature rules. Most prominent of the fully\nsymmetric kernel quadrature rules we propose are those that use sparse grids. \n\n"}
{"id": "1703.06686", "contents": "Title: Copula Index for Detecting Dependence and Monotonicity between\n  Stochastic Signals Abstract: This paper introduces a nonparametric copula-based index for detecting the\nstrength and monotonicity structure of linear and nonlinear statistical\ndependence between pairs of random variables or stochastic signals. Our index,\ntermed Copula Index for Detecting Dependence and Monotonicity (CIM), satisfies\nseveral desirable properties of measures of association, including Renyi's\nproperties, the data processing inequality (DPI), and consequently\nself-equitability. Synthetic data simulations reveal that the statistical power\nof CIM compares favorably to other state-of-the-art measures of association\nthat are proven to satisfy the DPI. Simulation results with real-world data\nreveal the CIM's unique ability to detect the monotonicity structure among\nstochastic signals to find interesting dependencies in large datasets.\nAdditionally, simulations show that the CIM shows favorable performance to\nestimators of mutual information when discovering Markov network structure. \n\n"}
{"id": "1703.06857", "contents": "Title: On the Limitation of Convolutional Neural Networks in Recognizing\n  Negative Images Abstract: Convolutional Neural Networks (CNNs) have achieved state-of-the-art\nperformance on a variety of computer vision tasks, particularly visual\nclassification problems, where new algorithms reported to achieve or even\nsurpass the human performance. In this paper, we examine whether CNNs are\ncapable of learning the semantics of training data. To this end, we evaluate\nCNNs on negative images, since they share the same structure and semantics as\nregular images and humans can classify them correctly. Our experimental results\nindicate that when training on regular images and testing on negative images,\nthe model accuracy is significantly lower than when it is tested on regular\nimages. This leads us to the conjecture that current training methods do not\neffectively train models to generalize the concepts. We then introduce the\nnotion of semantic adversarial examples - transformed inputs that semantically\nrepresent the same objects, but the model does not classify them correctly -\nand present negative images as one class of such inputs. \n\n"}
{"id": "1703.07047", "contents": "Title: High-Resolution Breast Cancer Screening with Multi-View Deep\n  Convolutional Neural Networks Abstract: Advances in deep learning for natural images have prompted a surge of\ninterest in applying similar techniques to medical images. The majority of the\ninitial attempts focused on replacing the input of a deep convolutional neural\nnetwork with a medical image, which does not take into consideration the\nfundamental differences between these two types of images. Specifically, fine\ndetails are necessary for detection in medical images, unlike in natural images\nwhere coarse structures matter most. This difference makes it inadequate to use\nthe existing network architectures developed for natural images, because they\nwork on heavily downscaled images to reduce the memory requirements. This hides\ndetails necessary to make accurate predictions. Additionally, a single exam in\nmedical imaging often comes with a set of views which must be fused in order to\nreach a correct conclusion. In our work, we propose to use a multi-view deep\nconvolutional neural network that handles a set of high-resolution medical\nimages. We evaluate it on large-scale mammography-based breast cancer screening\n(BI-RADS prediction) using 886,000 images. We focus on investigating the impact\nof the training set size and image size on the prediction accuracy. Our results\nhighlight that performance increases with the size of training set, and that\nthe best performance can only be achieved using the original resolution. In the\nreader study, performed on a random subset of the test set, we confirmed the\nefficacy of our model, which achieved performance comparable to a committee of\nradiologists when presented with the same data. \n\n"}
{"id": "1704.00153", "contents": "Title: Computations of volumes and Ehrhart series in four candidates elections Abstract: We describe several experimental results obtained in four candidates social\nchoice elections. These include the Condorcet and Borda paradoxes, as well as\nthe Condorcet efficiency of plurality voting with runoff. The computations are\ndone by Normaliz. It finds precise probabilities as volumes of polytopes and\ncounting functions encoded as Ehrhart series of polytopes. \n\n"}
{"id": "1704.01255", "contents": "Title: Linear Additive Markov Processes Abstract: We introduce LAMP: the Linear Additive Markov Process. Transitions in LAMP\nmay be influenced by states visited in the distant history of the process, but\nunlike higher-order Markov processes, LAMP retains an efficient\nparametrization. LAMP also allows the specific dependence on history to be\nlearned efficiently from data. We characterize some theoretical properties of\nLAMP, including its steady-state and mixing time. We then give an algorithm\nbased on alternating minimization to learn LAMP models from data. Finally, we\nperform a series of real-world experiments to show that LAMP is more powerful\nthan first-order Markov processes, and even holds its own against deep\nsequential models (LSTMs) with a negligible increase in parameter complexity. \n\n"}
{"id": "1704.01312", "contents": "Title: On Generalization and Regularization in Deep Learning Abstract: Why do large neural network generalize so well on complex tasks such as image\nclassification or speech recognition? What exactly is the role regularization\nfor them? These are arguably among the most important open questions in machine\nlearning today. In a recent and thought provoking paper [C. Zhang et al.]\nseveral authors performed a number of numerical experiments that hint at the\nneed for novel theoretical concepts to account for this phenomenon. The paper\nstirred quit a lot of excitement among the machine learning community but at\nthe same time it created some confusion as discussions on OpenReview.net\ntestifies. The aim of this pedagogical paper is to make this debate accessible\nto a wider audience of data scientists without advanced theoretical knowledge\nin statistical learning. The focus here is on explicit mathematical definitions\nand on a discussion of relevant concepts, not on proofs for which we provide\nreferences. \n\n"}
{"id": "1704.02030", "contents": "Title: Using stacking to average Bayesian predictive distributions Abstract: The widely recommended procedure of Bayesian model averaging is flawed in the\nM-open setting in which the true data-generating process is not one of the\ncandidate models being fit. We take the idea of stacking from the point\nestimation literature and generalize to the combination of predictive\ndistributions, extending the utility function to any proper scoring rule, using\nPareto smoothed importance sampling to efficiently compute the required\nleave-one-out posterior distributions and regularization to get more stability.\nWe compare stacking of predictive distributions to several alternatives:\nstacking of means, Bayesian model averaging (BMA), pseudo-BMA using AIC-type\nweighting, and a variant of pseudo-BMA that is stabilized using the Bayesian\nbootstrap. Based on simulations and real-data applications, we recommend\nstacking of predictive distributions, with BB-pseudo-BMA as an approximate\nalternative when computation cost is an issue. \n\n"}
{"id": "1704.02161", "contents": "Title: ReLayNet: Retinal Layer and Fluid Segmentation of Macular Optical\n  Coherence Tomography using Fully Convolutional Network Abstract: Optical coherence tomography (OCT) is used for non-invasive diagnosis of\ndiabetic macular edema assessing the retinal layers. In this paper, we propose\na new fully convolutional deep architecture, termed ReLayNet, for end-to-end\nsegmentation of retinal layers and fluid masses in eye OCT scans. ReLayNet uses\na contracting path of convolutional blocks (encoders) to learn a hierarchy of\ncontextual features, followed by an expansive path of convolutional blocks\n(decoders) for semantic segmentation. ReLayNet is trained to optimize a joint\nloss function comprising of weighted logistic regression and Dice overlap loss.\nThe framework is validated on a publicly available benchmark dataset with\ncomparisons against five state-of-the-art segmentation methods including two\ndeep learning based approaches to substantiate its effectiveness. \n\n"}
{"id": "1704.02531", "contents": "Title: Three Skewed Matrix Variate Distributions Abstract: Three-way data can be conveniently modelled by using matrix variate\ndistributions. Although there has been a lot of work for the matrix variate\nnormal distribution, there is little work in the area of matrix skew\ndistributions. Three matrix variate distributions that incorporate skewness, as\nwell as other flexible properties such as concentration, are discussed.\nEquivalences to multivariate analogues are presented, and moment generating\nfunctions are derived. Maximum likelihood parameter estimation is discussed,\nand simulated data is used for illustration. \n\n"}
{"id": "1704.03817", "contents": "Title: MAGAN: Margin Adaptation for Generative Adversarial Networks Abstract: We propose the Margin Adaptation for Generative Adversarial Networks (MAGANs)\nalgorithm, a novel training procedure for GANs to improve stability and\nperformance by using an adaptive hinge loss function. We estimate the\nappropriate hinge loss margin with the expected energy of the target\ndistribution, and derive principled criteria for when to update the margin. We\nprove that our method converges to its global optimum under certain\nassumptions. Evaluated on the task of unsupervised image generation, the\nproposed training procedure is simple yet robust on a diverse set of data, and\nachieves qualitative and quantitative improvements compared to the\nstate-of-the-art. \n\n"}
{"id": "1704.05098", "contents": "Title: Statistical inference for high dimensional regression via Constrained\n  Lasso Abstract: In this paper, we propose a new method for estimation and constructing\nconfidence intervals for low-dimensional components in a high-dimensional\nmodel. The proposed estimator, called Constrained Lasso (CLasso) estimator, is\nobtained by simultaneously solving two estimating equations---one imposing a\nzero-bias constraint for the low-dimensional parameter and the other forming an\n$\\ell_1$-penalized procedure for the high-dimensional nuisance parameter. By\ncarefully choosing the zero-bias constraint, the resulting estimator of the low\ndimensional parameter is shown to admit an asymptotically normal limit\nattaining the Cram\\'{e}r-Rao lower bound in a semiparametric sense. We propose\na tuning-free iterative algorithm for implementing the CLasso. We show that\nwhen the algorithm is initialized at the Lasso estimator, the de-sparsified\nestimator proposed in van de Geer et al. [\\emph{Ann. Statist.} {\\bf 42} (2014)\n1166--1202] is asymptotically equivalent to the first iterate of the algorithm.\nWe analyse the asymptotic properties of the CLasso estimator and show the\nglobally linear convergence of the algorithm. We also demonstrate encouraging\nempirical performance of the CLasso through numerical studies. \n\n"}
{"id": "1704.06017", "contents": "Title: PAFit: an R Package for the Non-Parametric Estimation of Preferential\n  Attachment and Node Fitness in Temporal Complex Networks Abstract: Many real-world systems are profitably described as complex networks that\ngrow over time. Preferential attachment and node fitness are two simple growth\nmechanisms that not only explain certain structural properties commonly\nobserved in real-world systems, but are also tied to a number of applications\nin modeling and inference. While there are statistical packages for estimating\nvarious parametric forms of the preferential attachment function, there is no\nsuch package implementing non-parametric estimation procedures. The\nnon-parametric approach to the estimation of the preferential attachment\nfunction allows for comparatively finer-grained investigations of the\n`rich-get-richer' phenomenon that could lead to novel insights in the search to\nexplain certain nonstandard structural properties observed in real-world\nnetworks. This paper introduces the R package PAFit, which implements\nnon-parametric procedures for estimating the preferential attachment function\nand node fitnesses in a growing network, as well as a number of functions for\ngenerating complex networks from these two mechanisms. The main computational\npart of the package is implemented in C++ with OpenMP to ensure scalability to\nlarge-scale networks. We first introduce the main functionalities of PAFit\nthrough simulated examples, and then use the package to analyze a collaboration\nnetwork between scientists in the field of complex networks. The results\nindicate the joint presence of `rich-get-richer' and `fit-get-richer' phenomena\nin the collaboration network. The estimated attachment function is observed to\nbe near-linear, which we interpret as meaning that the chance an author gets a\nnew collaborator is proportional to their current number of collaborators.\nFurthermore, the estimated author fitnesses reveal a host of familiar faces\nfrom the complex networks community among the field's topmost fittest network\nscientists. \n\n"}
{"id": "1704.06374", "contents": "Title: Recalibration: A post-processing method for approximate Bayesian\n  computation Abstract: A new recalibration post-processing method is presented to improve the\nquality of the posterior approximation when using Approximate Bayesian\nComputation (ABC) algorithms. Recalibration may be used in conjunction with\nexisting post-processing methods, such as regression-adjustments. In addition,\nthis work extends and strengthens the links between ABC and indirect inference\nalgorithms, allowing more extensive use of misspecified auxiliary models in the\nABC context. The method is illustrated using simulated examples to demonstrate\nthe effects of recalibration under various conditions, and through an\napplication to an analysis of stereological extremes both with and without the\nuse of auxiliary models. Code to implement recalibration post-processing is\navailable in the R package, abctools. \n\n"}
{"id": "1704.08488", "contents": "Title: Optimal client recommendation for market makers in illiquid financial\n  products Abstract: The process of liquidity provision in financial markets can result in\nprolonged exposure to illiquid instruments for market makers. In this case,\nwhere a proprietary position is not desired, pro-actively targeting the right\nclient who is likely to be interested can be an effective means to offset this\nposition, rather than relying on commensurate interest arising through natural\ndemand. In this paper, we consider the inference of a client profile for the\npurpose of corporate bond recommendation, based on typical recorded information\navailable to the market maker. Given a historical record of corporate bond\ntransactions and bond meta-data, we use a topic-modelling analogy to develop a\nprobabilistic technique for compiling a curated list of client recommendations\nfor a particular bond that needs to be traded, ranked by probability of\ninterest. We show that a model based on Latent Dirichlet Allocation offers\npromising performance to deliver relevant recommendations for sales traders. \n\n"}
{"id": "1705.00166", "contents": "Title: On the convergence of Hamiltonian Monte Carlo Abstract: This paper discusses the irreducibility and geometric ergodicity of the\nHamiltonian Monte Carlo (HMC) algorithm. We consider cases where the number of\nsteps of the symplectic integrator is either fixed or random. Under mild\nconditions on the potential $\\F$ associated with target distribution $\\pi$, we\nfirst show that the Markov kernel associated to the HMC algorithm is\nirreducible and recurrent. Under more stringent conditions, we then establish\nthat the Markov kernel is Harris recurrent. Finally, we provide verifiable\nconditions on $\\F$ under which the HMC sampler is geometrically ergodic. \n\n"}
{"id": "1705.00607", "contents": "Title: Determinantal Point Processes for Mini-Batch Diversification Abstract: We study a mini-batch diversification scheme for stochastic gradient descent\n(SGD). While classical SGD relies on uniformly sampling data points to form a\nmini-batch, we propose a non-uniform sampling scheme based on the Determinantal\nPoint Process (DPP). The DPP relies on a similarity measure between data points\nand gives low probabilities to mini-batches which contain redundant data, and\nhigher probabilities to mini-batches with more diverse data. This\nsimultaneously balances the data and leads to stochastic gradients with lower\nvariance. We term this approach Diversified Mini-Batch SGD (DM-SGD). We show\nthat regular SGD and a biased version of stratified sampling emerge as special\ncases. Furthermore, DM-SGD generalizes stratified sampling to cases where no\ndiscrete features exist to bin the data into groups. We show experimentally\nthat our method results more interpretable and diverse features in unsupervised\nsetups, and in better classification accuracies in supervised setups. \n\n"}
{"id": "1705.00841", "contents": "Title: Bayes Shrinkage at GWAS scale: Convergence and Approximation Theory of a\n  Scalable MCMC Algorithm for the Horseshoe Prior Abstract: The horseshoe prior is frequently employed in Bayesian analysis of\nhigh-dimensional models, and has been shown to achieve minimax optimal risk\nproperties when the truth is sparse. While optimization-based algorithms for\nthe extremely popular Lasso and elastic net procedures can scale to dimension\nin the hundreds of thousands, algorithms for the horseshoe that use Markov\nchain Monte Carlo (MCMC) for computation are limited to problems an order of\nmagnitude smaller. This is due to high computational cost per step and growth\nof the variance of time-averaging estimators as a function of dimension. We\npropose two new MCMC algorithms for computation in these models that have\nimproved performance compared to existing alternatives. One of the algorithms\nalso approximates an expensive matrix product to give orders of magnitude\nspeedup in high-dimensional applications. We prove that the exact algorithm is\ngeometrically ergodic, and give guarantees for the accuracy of the approximate\nalgorithm using perturbation theory. Versions of the approximation algorithm\nthat gradually decrease the approximation error as the chain extends are shown\nto be exact. The scalability of the algorithm is illustrated in simulations\nwith problem size as large as $N=5,000$ observations and $p=50,000$ predictors,\nand an application to a genome-wide association study with $N=2,267$ and\n$p=98,385$. The empirical results also show that the new algorithm yields\nestimates with lower mean squared error, intervals with better coverage, and\nelucidates features of the posterior that were often missed by previous\nalgorithms in high dimensions, including bimodality of posterior marginals\nindicating uncertainty about which covariates belong in the model. \n\n"}
{"id": "1705.01715", "contents": "Title: Directed Networks with a Differentially Private Bi-degree Sequence Abstract: Although a lot of approaches are developed to release network data with a\ndifferentially privacy guarantee, inference using noisy data in many network\nmodels is still unknown or not properly explored. In this paper, we release the\nbi-degree sequences of directed networks using the Laplace mechanism and use\nthe $p_0$ model for inferring the degree parameters. The $p_0$ model is an\nexponential random graph model with the bi-degree sequence as its exclusively\nsufficient statistic. We show that the estimator of the parameter without the\ndenoised process is asymptotically consistent and normally distributed. This is\ncontrast sharply with some known results that valid inference such as the\nexistence and consistency of the estimator needs the denoised process. Along\nthe way, a new phenomenon is revealed in which an additional variance factor\nappears in the asymptotic variance of the estimator when the noise becomes\nlarge. Further, we propose an efficient algorithm for finding the closet point\nlying in the set of all graphical bi-degree sequences under the global $L_1$\noptimization problem. Numerical studies demonstrate our theoretical findings. \n\n"}
{"id": "1705.01830", "contents": "Title: On distinct cross-ratios and related growth problems Abstract: It is shown that for a finite set $A$ of four or more complex numbers, the\ncardinality of the set $C[A]$ of all cross-ratios generated by quadruples of\npair-wise distinct elements of $A$ is $|C[A]|\\gg\n|A|^{2+\\frac{2}{11}}\\log^{-\\frac{6}{11}} |A|$ and without the logarithmic\nfactor in the real case. The set $C=C[A]$ always grows under both addition and\nmultiplication. The cross-ratio arises, in particular, in the study of the open\nquestion of the minimum number of triangle areas, with two vertices in a given\nnon-collinear finite point set in the plane and the third one at the fixed\norigin. The above distinct cross-ratio bound implies a new lower bound for the\nlatter question, and enables one to show growth of the set\n$\\sin(A-A),\\;A\\subset \\mathbb R/\\pi\\mathbb Z$ under multiplication. It seems\nreasonable to conjecture that more-fold product, as well as sum sets of this\nset or $C$ continue growing ad infinitum. \n\n"}
{"id": "1705.03297", "contents": "Title: Semiparametric spectral modeling of the Drosophila connectome Abstract: We present semiparametric spectral modeling of the complete larval Drosophila\nmushroom body connectome. Motivated by a thorough exploratory data analysis of\nthe network via Gaussian mixture modeling (GMM) in the adjacency spectral\nembedding (ASE) representation space, we introduce the latent structure model\n(LSM) for network modeling and inference. LSM is a generalization of the\nstochastic block model (SBM) and a special case of the random dot product graph\n(RDPG) latent position model, and is amenable to semiparametric GMM in the ASE\nrepresentation space. The resulting connectome code derived via semiparametric\nGMM composed with ASE captures latent connectome structure and elucidates\nbiologically relevant neuronal properties. \n\n"}
{"id": "1705.03335", "contents": "Title: Dimensions of sets which uniformly avoid arithmetic progressions Abstract: We provide estimates for the dimensions of sets in $\\mathbb{R}$ which\nuniformly avoid finite arithmetic progressions. More precisely, we say $F$\nuniformly avoids arithmetic progressions of length $k \\geq 3$ if there is an\n$\\epsilon>0$ such that one cannot find an arithmetic progression of length $k$\nand gap length $\\Delta>0$ inside the $\\epsilon \\Delta$ neighbourhood of $F$.\nOur main result is an explicit upper bound for the Assouad (and thus Hausdorff)\ndimension of such sets in terms of $k$ and $\\epsilon$. In the other direction,\nwe provide examples of sets which uniformly avoid arithmetic progressions of a\ngiven length but still have relatively large Hausdorff dimension.\n  We also consider higher dimensional analogues of these problems, where\narithmetic progressions are replaced with arithmetic patches lying in a\nhyperplane. As a consequence we obtain a discretised version of a `reverse\nKakeya problem': we show that if the dimension of a set in $\\mathbb{R}^d$ is\nsufficiently large, then it closely approximates arithmetic progressions in\nevery direction. \n\n"}
{"id": "1705.03566", "contents": "Title: Spatial Random Sampling: A Structure-Preserving Data Sketching Tool Abstract: Random column sampling is not guaranteed to yield data sketches that preserve\nthe underlying structures of the data and may not sample sufficiently from\nless-populated data clusters. Also, adaptive sampling can often provide\naccurate low rank approximations, yet may fall short of producing descriptive\ndata sketches, especially when the cluster centers are linearly dependent.\nMotivated by that, this paper introduces a novel randomized column sampling\ntool dubbed Spatial Random Sampling (SRS), in which data points are sampled\nbased on their proximity to randomly sampled points on the unit sphere. The\nmost compelling feature of SRS is that the corresponding probability of\nsampling from a given data cluster is proportional to the surface area the\ncluster occupies on the unit sphere, independently from the size of the cluster\npopulation. Although it is fully randomized, SRS is shown to provide\ndescriptive and balanced data representations. The proposed idea addresses a\npressing need in data science and holds potential to inspire many novel\napproaches for analysis of big data. \n\n"}
{"id": "1705.03953", "contents": "Title: Convex equipartitions of colored point sets Abstract: We show that any $d$-colored set of points in general position in\n$\\mathbb{R}^d$ can be partitioned into $n$ subsets with disjoint convex hulls\nsuch that the set of points and all color classes are partitioned as evenly as\npossible. This extends results by Holmsen, Kyn\\v{c}l & Valculescu (2017) and\nestablishes a special case of their general conjecture. Our proof utilizes a\nresult obtained independently by Sober\\'on and by Karasev in 2010, on\nsimultaneous equipartitions of $d$ continuous measures in $\\mathbb{R}^d$ by $n$\nconvex regions. This gives a convex partition of $\\mathbb{R}^d$ with the\ndesired properties, except that points may lie on the boundaries of the\nregions. In order to resolve the ambiguous assignment of these points, we set\nup a network flow problem. The equipartition of the continuous measures gives a\nfractional flow. The existence of an integer flow then yields the desired\npartition of the point set. \n\n"}
{"id": "1705.04524", "contents": "Title: Long-term Blood Pressure Prediction with Deep Recurrent Neural Networks Abstract: Existing methods for arterial blood pressure (BP) estimation directly map the\ninput physiological signals to output BP values without explicitly modeling the\nunderlying temporal dependencies in BP dynamics. As a result, these models\nsuffer from accuracy decay over a long time and thus require frequent\ncalibration. In this work, we address this issue by formulating BP estimation\nas a sequence prediction problem in which both the input and target are\ntemporal sequences. We propose a novel deep recurrent neural network (RNN)\nconsisting of multilayered Long Short-Term Memory (LSTM) networks, which are\nincorporated with (1) a bidirectional structure to access larger-scale context\ninformation of input sequence, and (2) residual connections to allow gradients\nin deep RNN to propagate more effectively. The proposed deep RNN model was\ntested on a static BP dataset, and it achieved root mean square error (RMSE) of\n3.90 and 2.66 mmHg for systolic BP (SBP) and diastolic BP (DBP) prediction\nrespectively, surpassing the accuracy of traditional BP prediction models. On a\nmulti-day BP dataset, the deep RNN achieved RMSE of 3.84, 5.25, 5.80 and 5.81\nmmHg for the 1st day, 2nd day, 4th day and 6th month after the 1st day SBP\nprediction, and 1.80, 4.78, 5.0, 5.21 mmHg for corresponding DBP prediction,\nrespectively, which outperforms all previous models with notable improvement.\nThe experimental results suggest that modeling the temporal dependencies in BP\ndynamics significantly improves the long-term BP prediction accuracy. \n\n"}
{"id": "1705.04579", "contents": "Title: Exponential Ergodicity of the Bouncy Particle Sampler Abstract: Non-reversible Markov chain Monte Carlo schemes based on piecewise\ndeterministic Markov processes have been recently introduced in applied\nprobability, automatic control, physics and statistics. Although these\nalgorithms demonstrate experimentally good performance and are accordingly\nincreasingly used in a wide range of applications, geometric ergodicity results\nfor such schemes have only been established so far under very restrictive\nassumptions. We give here verifiable conditions on the target distribution\nunder which the Bouncy Particle Sampler algorithm introduced in \\cite{P_dW_12}\nis geometrically ergodic. This holds whenever the target satisfies a curvature\ncondition and has tails decaying at least as fast as an exponential and at most\nas fast as a Gaussian distribution. This allows us to provide a central limit\ntheorem for the associated ergodic averages. When the target has tails thinner\nthan a Gaussian distribution, we propose an original modification of this\nscheme that is geometrically ergodic. For thick-tailed target distributions,\nsuch as $t$-distributions, we extend the idea pioneered in \\cite{J_G_12} in a\nrandom walk Metropolis context. We apply a change of variable to obtain a\ntransformed target satisfying the tail conditions for geometric ergodicity. By\nsampling the transformed target using the Bouncy Particle Sampler and mapping\nback the Markov process to the original parameterization, we obtain a\ngeometrically ergodic algorithm. \n\n"}
{"id": "1705.04715", "contents": "Title: A catalog of 4-regular and (2;4)-regular matchstick graphs Abstract: The first part (page 1 - 7) of this article presents the currently known\nexamples of 4-regular matchstick graphs with 63 - 70 vertices. The second part\n(page 8 - 15) presents the currently known examples of $(2;4)$-regular\nmatchstick graphs with less than 42 vertices which contain only two vertices of\ndegree 2. \n\n"}
{"id": "1705.05907", "contents": "Title: Machine Learning Molecular Dynamics for the Simulation of Infrared\n  Spectra Abstract: Machine learning has emerged as an invaluable tool in many research areas. In\nthe present work, we harness this power to predict highly accurate molecular\ninfrared spectra with unprecedented computational efficiency. To account for\nvibrational anharmonic and dynamical effects -- typically neglected by\nconventional quantum chemistry approaches -- we base our machine learning\nstrategy on ab initio molecular dynamics simulations. While these simulations\nare usually extremely time consuming even for small molecules, we overcome\nthese limitations by leveraging the power of a variety of machine learning\ntechniques, not only accelerating simulations by several orders of magnitude,\nbut also greatly extending the size of systems that can be treated. To this\nend, we develop a molecular dipole moment model based on environment dependent\nneural network charges and combine it with the neural network potentials of\nBehler and Parrinello. Contrary to the prevalent big data philosophy, we are\nable to obtain very accurate machine learning models for the prediction of\ninfrared spectra based on only a few hundreds of electronic structure reference\npoints. This is made possible through the introduction of a fully automated\nsampling scheme and the use of molecular forces during neural network potential\ntraining. We demonstrate the power of our machine learning approach by applying\nit to model the infrared spectra of a methanol molecule, n-alkanes containing\nup to 200 atoms and the protonated alanine tripeptide, which at the same time\nrepresents the first application of machine learning techniques to simulate the\ndynamics of a peptide. In all these case studies we find excellent agreement\nbetween the infrared spectra predicted via machine learning models and the\nrespective theoretical and experimental spectra. \n\n"}
{"id": "1705.06168", "contents": "Title: Two-Sample Tests for Large Random Graphs Using Network Statistics Abstract: We consider a two-sample hypothesis testing problem, where the distributions\nare defined on the space of undirected graphs, and one has access to only one\nobservation from each model. A motivating example for this problem is comparing\nthe friendship networks on Facebook and LinkedIn. The practical approach to\nsuch problems is to compare the networks based on certain network statistics.\nIn this paper, we present a general principle for two-sample hypothesis testing\nin such scenarios without making any assumption about the network generation\nprocess. The main contribution of the paper is a general formulation of the\nproblem based on concentration of network statistics, and consequently, a\nconsistent two-sample test that arises as the natural solution for this\nproblem. We also show that the proposed test is minimax optimal for certain\nnetwork statistics. \n\n"}
{"id": "1705.07136", "contents": "Title: Softmax Q-Distribution Estimation for Structured Prediction: A\n  Theoretical Interpretation for RAML Abstract: Reward augmented maximum likelihood (RAML), a simple and effective learning\nframework to directly optimize towards the reward function in structured\nprediction tasks, has led to a number of impressive empirical successes. RAML\nincorporates task-specific reward by performing maximum-likelihood updates on\ncandidate outputs sampled according to an exponentiated payoff distribution,\nwhich gives higher probabilities to candidates that are close to the reference\noutput. While RAML is notable for its simplicity, efficiency, and its\nimpressive empirical successes, the theoretical properties of RAML, especially\nthe behavior of the exponentiated payoff distribution, has not been examined\nthoroughly. In this work, we introduce softmax Q-distribution estimation, a\nnovel theoretical interpretation of RAML, which reveals the relation between\nRAML and Bayesian decision theory. The softmax Q-distribution can be regarded\nas a smooth approximation of the Bayes decision boundary, and the Bayes\ndecision rule is achieved by decoding with this Q-distribution. We further show\nthat RAML is equivalent to approximately estimating the softmax Q-distribution,\nwith the temperature $\\tau$ controlling approximation error. We perform two\nexperiments, one on synthetic data of multi-class classification and one on\nreal data of image captioning, to demonstrate the relationship between RAML and\nthe proposed softmax Q-distribution estimation method, verifying our\ntheoretical analysis. Additional experiments on three structured prediction\ntasks with rewards defined on sequential (named entity recognition), tree-based\n(dependency parsing) and irregular (machine translation) structures show\nnotable improvements over maximum likelihood baselines. \n\n"}
{"id": "1705.07959", "contents": "Title: Comparison of statistical sampling methods with ScannerBit, the GAMBIT\n  scanning module Abstract: We introduce ScannerBit, the statistics and sampling module of the public,\nopen-source global fitting framework GAMBIT. ScannerBit provides a standardised\ninterface to different sampling algorithms, enabling the use and comparison of\nmultiple computational methods for inferring profile likelihoods, Bayesian\nposteriors, and other statistical quantities. The current version offers\nrandom, grid, raster, nested sampling, differential evolution, Markov Chain\nMonte Carlo (MCMC) and ensemble Monte Carlo samplers. We also announce the\nrelease of a new standalone differential evolution sampler, Diver, and describe\nits design, usage and interface to ScannerBit. We subject Diver and three other\nsamplers (the nested sampler MultiNest, the MCMC GreAT, and the native\nScannerBit implementation of the ensemble Monte Carlo algorithm T-Walk) to a\nbattery of statistical tests. For this we use a realistic physical likelihood\nfunction, based on the scalar singlet model of dark matter. We examine the\nperformance of each sampler as a function of its adjustable settings, and the\ndimensionality of the sampling problem. We evaluate performance on four\nmetrics: optimality of the best fit found, completeness in exploring the\nbest-fit region, number of likelihood evaluations, and total runtime. For\nBayesian posterior estimation at high resolution, T-Walk provides the most\naccurate and timely mapping of the full parameter space. For profile likelihood\nanalysis in less than about ten dimensions, we find that Diver and MultiNest\nscore similarly in terms of best fit and speed, outperforming GreAT and T-Walk;\nin ten or more dimensions, Diver substantially outperforms the other three\nsamplers on all metrics. \n\n"}
{"id": "1705.08415", "contents": "Title: Supervised Community Detection with Line Graph Neural Networks Abstract: Traditionally, community detection in graphs can be solved using spectral\nmethods or posterior inference under probabilistic graphical models. Focusing\non random graph families such as the stochastic block model, recent research\nhas unified both approaches and identified both statistical and computational\ndetection thresholds in terms of the signal-to-noise ratio. By recasting\ncommunity detection as a node-wise classification problem on graphs, we can\nalso study it from a learning perspective. We present a novel family of Graph\nNeural Networks (GNNs) for solving community detection problems in a supervised\nlearning setting. We show that, in a data-driven manner and without access to\nthe underlying generative models, they can match or even surpass the\nperformance of the belief propagation algorithm on binary and multi-class\nstochastic block models, which is believed to reach the computational\nthreshold. In particular, we propose to augment GNNs with the non-backtracking\noperator defined on the line graph of edge adjacencies. Our models also achieve\ngood performance on real-world datasets. In addition, we perform the first\nanalysis of the optimization landscape of training linear GNNs for community\ndetection problems, demonstrating that under certain simplifications and\nassumptions, the loss values at local and global minima are not far apart. \n\n"}
{"id": "1705.08527", "contents": "Title: Causal inference for social network data Abstract: We describe semiparametric estimation and inference for causal effects using\nobservational data from a single social network. Our asymptotic results are the\nfirst to allow for dependence of each observation on a growing number of other\nunits as sample size increases. In addition, while previous methods have\nimplicitly permitted only one of two possible sources of dependence among\nsocial network observations, we allow for both dependence due to transmission\nof information across network ties and for dependence due to latent\nsimilarities among nodes sharing ties. We propose new causal effects that are\nspecifically of interest in social network settings, such as interventions on\nnetwork ties and network structure. We use our methods to reanalyze an\ninfluential and controversial study that estimated causal peer effects of\nobesity using social network data from the Framingham Heart Study; after\naccounting for network structure we find no evidence for causal peer effects. \n\n"}
{"id": "1705.09199", "contents": "Title: Non-parametric estimation of Jensen-Shannon Divergence in Generative\n  Adversarial Network training Abstract: Generative Adversarial Networks (GANs) have become a widely popular framework\nfor generative modelling of high-dimensional datasets. However their training\nis well-known to be difficult. This work presents a rigorous statistical\nanalysis of GANs providing straight-forward explanations for common training\npathologies such as vanishing gradients. Furthermore, it proposes a new\ntraining objective, Kernel GANs, and demonstrates its practical effectiveness\non large-scale real-world data sets. A key element in the analysis is the\ndistinction between training with respect to the (unknown) data distribution,\nand its empirical counterpart. To overcome issues in GAN training, we pursue\nthe idea of smoothing the Jensen-Shannon Divergence (JSD) by incorporating\nnoise in the input distributions of the discriminator. As we show, this\neffectively leads to an empirical version of the JSD in which the true and the\ngenerator densities are replaced by kernel density estimates, which leads to\nKernel GANs. \n\n"}
{"id": "1705.09944", "contents": "Title: Learning Data Manifolds with a Cutting Plane Method Abstract: We consider the problem of classifying data manifolds where each manifold\nrepresents invariances that are parameterized by continuous degrees of freedom.\nConventional data augmentation methods rely upon sampling large numbers of\ntraining examples from these manifolds; instead, we propose an iterative\nalgorithm called M_{CP} based upon a cutting-plane approach that efficiently\nsolves a quadratic semi-infinite programming problem to find the maximum margin\nsolution. We provide a proof of convergence as well as a polynomial bound on\nthe number of iterations required for a desired tolerance in the objective\nfunction. The efficiency and performance of M_{CP} are demonstrated in\nhigh-dimensional simulations and on image manifolds generated from the ImageNet\ndataset. Our results indicate that M_{CP} is able to rapidly learn good\nclassifiers and shows superior generalization performance compared with\nconventional maximum margin methods using data augmentation methods. \n\n"}
{"id": "1705.10941", "contents": "Title: Spectral Norm Regularization for Improving the Generalizability of Deep\n  Learning Abstract: We investigate the generalizability of deep learning based on the sensitivity\nto input perturbation. We hypothesize that the high sensitivity to the\nperturbation of data degrades the performance on it. To reduce the sensitivity\nto perturbation, we propose a simple and effective regularization method,\nreferred to as spectral norm regularization, which penalizes the high spectral\nnorm of weight matrices in neural networks. We provide supportive evidence for\nthe abovementioned hypothesis by experimentally confirming that the models\ntrained using spectral norm regularization exhibit better generalizability than\nother baseline methods. \n\n"}
{"id": "1706.00598", "contents": "Title: Dynamic Steerable Blocks in Deep Residual Networks Abstract: Filters in convolutional networks are typically parameterized in a pixel\nbasis, that does not take prior knowledge about the visual world into account.\nWe investigate the generalized notion of frames designed with image properties\nin mind, as alternatives to this parametrization. We show that frame-based\nResNets and Densenets can improve performance on Cifar-10+ consistently, while\nhaving additional pleasant properties like steerability. By exploiting these\ntransformation properties explicitly, we arrive at dynamic steerable blocks.\nThey are an extension of residual blocks, that are able to seamlessly transform\nfilters under pre-defined transformations, conditioned on the input at training\nand inference time. Dynamic steerable blocks learn the degree of invariance\nfrom data and locally adapt filters, allowing them to apply a different\ngeometrical variant of the same filter to each location of the feature map.\nWhen evaluated on the Berkeley Segmentation contour detection dataset, our\napproach outperforms all competing approaches that do not utilize pre-training.\nOur results highlight the benefits of image-based regularization to deep\nnetworks. \n\n"}
{"id": "1706.00689", "contents": "Title: Fast approximate Bayesian inference for stable differential equation\n  models Abstract: Inference for mechanistic models is challenging because of nonlinear\ninteractions between model parameters and a lack of identifiability. Here we\nfocus on a specific class of mechanistic models, which we term stable\ndifferential equations. The dynamics in these models are approximately linear\naround a stable fixed point of the system. We exploit this property to develop\nfast approximate methods for posterior inference. We illustrate our approach\nusing simulated data on a mechanistic neuroscience model with EEG data. More\ngenerally, stable differential equation models and the corresponding inference\nmethods are useful for analysis of stationary time-series data. Compared to the\nexisting state-of-the art, our methods are several orders of magnitude faster,\nand are particularly suited to analysis of long time-series (>10,000\ntime-points) and models of moderate dimension (10-50 state variables and 10-50\nparameters.) \n\n"}
{"id": "1706.00856", "contents": "Title: Multiple Kernel Learning and Automatic Subspace Relevance Determination\n  for High-dimensional Neuroimaging Data Abstract: Alzheimer's disease is a major cause of dementia. Its diagnosis requires\naccurate biomarkers that are sensitive to disease stages. In this respect, we\nregard probabilistic classification as a method of designing a probabilistic\nbiomarker for disease staging. Probabilistic biomarkers naturally support the\ninterpretation of decisions and evaluation of uncertainty associated with them.\nIn this paper, we obtain probabilistic biomarkers via Gaussian Processes.\nGaussian Processes enable probabilistic kernel machines that offer flexible\nmeans to accomplish Multiple Kernel Learning. Exploiting this flexibility, we\npropose a new variation of Automatic Relevance Determination and tackle the\nchallenges of high dimensionality through multiple kernels. Our research\nresults demonstrate that the Gaussian Process models are competitive with or\nbetter than the well-known Support Vector Machine in terms of classification\nperformance even in the cases of single kernel learning. Extending the basic\nscheme towards the Multiple Kernel Learning, we improve the efficacy of the\nGaussian Process models and their interpretability in terms of the known\nanatomical correlates of the disease. For instance, the disease pathology\nstarts in and around the hippocampus and entorhinal cortex. Through the use of\nGaussian Processes and Multiple Kernel Learning, we have automatically and\nefficiently determined those portions of neuroimaging data. In addition to\ntheir interpretability, our Gaussian Process models are competitive with recent\ndeep learning solutions under similar settings. \n\n"}
{"id": "1706.01738", "contents": "Title: Ehrhart tensor polynomials Abstract: The notion of Ehrhart tensor polynomials, a natural generalization of the\nEhrhart polynomial of a lattice polytope, was recently introduced by Ludwig and\nSilverstein. We initiate a study of their coefficients. In the vector and\nmatrix cases, we give Pick-type formulas in terms of triangulations of a\nlattice polygon. As our main tool, we introduce $h^r$-tensor polynomials,\nextending the notion of the Ehrhart $h^\\ast$-polynomial, and, for matrices,\ninvestigate their coefficients for positive semidefiniteness. In contrast to\nthe usual $h^\\ast$-polynomial, the coefficients are in general not monotone\nwith respect to inclusion. Nevertheless, we are able to prove positive\nsemidefiniteness in dimension two. Based on computational results, we\nconjecture positive semidefiniteness of the coefficients in higher dimensions.\nFurthermore, we generalize Hibi's palindromic theorem for reflexive polytopes\nto $h^r$-tensor polynomials and discuss possible future research directions. \n\n"}
{"id": "1706.02515", "contents": "Title: Self-Normalizing Neural Networks Abstract: Deep Learning has revolutionized vision via convolutional neural networks\n(CNNs) and natural language processing via recurrent neural networks (RNNs).\nHowever, success stories of Deep Learning with standard feed-forward neural\nnetworks (FNNs) are rare. FNNs that perform well are typically shallow and,\ntherefore cannot exploit many levels of abstract representations. We introduce\nself-normalizing neural networks (SNNs) to enable high-level abstract\nrepresentations. While batch normalization requires explicit normalization,\nneuron activations of SNNs automatically converge towards zero mean and unit\nvariance. The activation function of SNNs are \"scaled exponential linear units\"\n(SELUs), which induce self-normalizing properties. Using the Banach fixed-point\ntheorem, we prove that activations close to zero mean and unit variance that\nare propagated through many network layers will converge towards zero mean and\nunit variance -- even under the presence of noise and perturbations. This\nconvergence property of SNNs allows to (1) train deep networks with many\nlayers, (2) employ strong regularization, and (3) to make learning highly\nrobust. Furthermore, for activations not close to unit variance, we prove an\nupper and lower bound on the variance, thus, vanishing and exploding gradients\nare impossible. We compared SNNs on (a) 121 tasks from the UCI machine learning\nrepository, on (b) drug discovery benchmarks, and on (c) astronomy tasks with\nstandard FNNs and other machine learning methods such as random forests and\nsupport vector machines. SNNs significantly outperformed all competing FNN\nmethods at 121 UCI tasks, outperformed all competing methods at the Tox21\ndataset, and set a new record at an astronomy data set. The winning SNN\narchitectures are often very deep. Implementations are available at:\ngithub.com/bioinf-jku/SNNs. \n\n"}
{"id": "1706.03301", "contents": "Title: Neural networks and rational functions Abstract: Neural networks and rational functions efficiently approximate each other. In\nmore detail, it is shown here that for any ReLU network, there exists a\nrational function of degree $O(\\text{polylog}(1/\\epsilon))$ which is\n$\\epsilon$-close, and similarly for any rational function there exists a ReLU\nnetwork of size $O(\\text{polylog}(1/\\epsilon))$ which is $\\epsilon$-close. By\ncontrast, polynomials need degree $\\Omega(\\text{poly}(1/\\epsilon))$ to\napproximate even a single ReLU. When converting a ReLU network to a rational\nfunction as above, the hidden constants depend exponentially on the number of\nlayers, which is shown to be tight; in other words, a compositional\nrepresentation can be beneficial even for rational functions. \n\n"}
{"id": "1706.03369", "contents": "Title: On the Sampling Problem for Kernel Quadrature Abstract: The standard Kernel Quadrature method for numerical integration with random\npoint sets (also called Bayesian Monte Carlo) is known to converge in root mean\nsquare error at a rate determined by the ratio $s/d$, where $s$ and $d$ encode\nthe smoothness and dimension of the integrand. However, an empirical\ninvestigation reveals that the rate constant $C$ is highly sensitive to the\ndistribution of the random points. In contrast to standard Monte Carlo\nintegration, for which optimal importance sampling is well-understood, the\nsampling distribution that minimises $C$ for Kernel Quadrature does not admit a\nclosed form. This paper argues that the practical choice of sampling\ndistribution is an important open problem. One solution is considered; a novel\nautomatic approach based on adaptive tempering and sequential Monte Carlo.\nEmpirical results demonstrate a dramatic reduction in integration error of up\nto 4 orders of magnitude can be achieved with the proposed method. \n\n"}
{"id": "1706.03400", "contents": "Title: A Prototype Knockoff Filter for Group Selection with FDR Control Abstract: In many applications, we need to study a linear regression model that\nconsists of a response variable and a large number of potential explanatory\nvariables and determine which variables are truly associated with the response.\nIn 2015, Barber and Candes introduced a new variable selection procedure called\nthe knockoff filter to control the false discovery rate (FDR) and proved that\nthis method achieves exact FDR control. In this paper, we propose a prototype\nknockoff filter for group selection by extending the Reid-Tibshirani prototype\nmethod. Our prototype knockoff filter improves the computational efficiency and\nstatistical power of the Reid-Tibshirani prototype method when it is applied\nfor group selection. In some cases when the group features are spanned by one\nor a few hidden factors, we demonstrate that the PCA prototype knockoff filter\noutperforms the Dai-Barber group knockoff filter. We present several numerical\nexperiments to compare our prototype knockoff filter with the Reid-Tibshirani\nprototype method and the group knockoff filter. We have also conducted some\nanalysis of the knockoff filter. Our analysis reveals that some knockoff path\nmethod statistics, including the Lasso path statistic, may lead to loss of\npower for certain design matrices and a specially designed response even if\ntheir signal strengths are still relatively strong. \n\n"}
{"id": "1706.03850", "contents": "Title: Adversarial Feature Matching for Text Generation Abstract: The Generative Adversarial Network (GAN) has achieved great success in\ngenerating realistic (real-valued) synthetic data. However, convergence issues\nand difficulties dealing with discrete data hinder the applicability of GAN to\ntext. We propose a framework for generating realistic text via adversarial\ntraining. We employ a long short-term memory network as generator, and a\nconvolutional network as discriminator. Instead of using the standard objective\nof GAN, we propose matching the high-dimensional latent feature distributions\nof real and synthetic sentences, via a kernelized discrepancy metric. This\neases adversarial training by alleviating the mode-collapsing problem. Our\nexperiments show superior performance in quantitative evaluation, and\ndemonstrate that our model can generate realistic-looking sentences. \n\n"}
{"id": "1706.04032", "contents": "Title: Modified Hamiltonian Monte Carlo for Bayesian inference Abstract: The Hamiltonian Monte Carlo (HMC) method has been recognized as a powerful\nsampling tool in computational statistics. We show that performance of HMC can\nbe significantly improved by incorporating importance sampling and an\nirreversible part of the dynamics into a chain. This is achieved by replacing\nHamiltonians in the Metropolis test with modified Hamiltonians, and a complete\nmomentum update with a partial momentum refreshment. We call the resulting\ngeneralized HMC importance sampler---Mix & Match Hamiltonian Monte Carlo\n(MMHMC). The method is irreversible by construction and further benefits from\n(i) the efficient algorithms for computation of modified Hamiltonians; (ii) the\nimplicit momentum update procedure and (iii) the multi-stage splitting\nintegrators specially derived for the methods sampling with modified\nHamiltonians. MMHMC has been implemented, tested on the popular statistical\nmodels and compared in sampling efficiency with HMC, Riemann Manifold\nHamiltonian Monte Carlo, Generalized Hybrid Monte Carlo, Generalized Shadow\nHybrid Monte Carlo, Metropolis Adjusted Langevin Algorithm and Random Walk\nMetropolis-Hastings. To make a fair comparison, we propose a metric that\naccounts for correlations among samples and weights, and can be readily used\nfor all methods which generate such samples. The experiments reveal the\nsuperiority of MMHMC over popular sampling techniques, especially in solving\nhigh dimensional problems. \n\n"}
{"id": "1706.05069", "contents": "Title: Generalization for Adaptively-chosen Estimators via Stable Median Abstract: Datasets are often reused to perform multiple statistical analyses in an\nadaptive way, in which each analysis may depend on the outcomes of previous\nanalyses on the same dataset. Standard statistical guarantees do not account\nfor these dependencies and little is known about how to provably avoid\noverfitting and false discovery in the adaptive setting. We consider a natural\nformalization of this problem in which the goal is to design an algorithm that,\ngiven a limited number of i.i.d.~samples from an unknown distribution, can\nanswer adaptively-chosen queries about that distribution.\n  We present an algorithm that estimates the expectations of $k$ arbitrary\nadaptively-chosen real-valued estimators using a number of samples that scales\nas $\\sqrt{k}$. The answers given by our algorithm are essentially as accurate\nas if fresh samples were used to evaluate each estimator. In contrast, prior\nwork yields error guarantees that scale with the worst-case sensitivity of each\nestimator. We also give a version of our algorithm that can be used to verify\nanswers to such queries where the sample complexity depends logarithmically on\nthe number of queries $k$ (as in the reusable holdout technique).\n  Our algorithm is based on a simple approximate median algorithm that\nsatisfies the strong stability guarantees of differential privacy. Our\ntechniques provide a new approach for analyzing the generalization guarantees\nof differentially private algorithms. \n\n"}
{"id": "1706.05350", "contents": "Title: L2 Regularization versus Batch and Weight Normalization Abstract: Batch Normalization is a commonly used trick to improve the training of deep\nneural networks. These neural networks use L2 regularization, also called\nweight decay, ostensibly to prevent overfitting. However, we show that L2\nregularization has no regularizing effect when combined with normalization.\nInstead, regularization has an influence on the scale of weights, and thereby\non the effective learning rate. We investigate this dependence, both in theory,\nand experimentally. We show that popular optimization methods such as ADAM only\npartially eliminate the influence of normalization on the learning rate. This\nleads to a discussion on other ways to mitigate this issue. \n\n"}
{"id": "1706.05394", "contents": "Title: A Closer Look at Memorization in Deep Networks Abstract: We examine the role of memorization in deep learning, drawing connections to\ncapacity, generalization, and adversarial robustness. While deep networks are\ncapable of memorizing noise data, our results suggest that they tend to\nprioritize learning simple patterns first. In our experiments, we expose\nqualitative differences in gradient-based optimization of deep neural networks\n(DNNs) on noise vs. real data. We also demonstrate that for appropriately tuned\nexplicit regularization (e.g., dropout) we can degrade DNN training performance\non noise datasets without compromising generalization on real data. Our\nanalysis suggests that the notions of effective capacity which are dataset\nindependent are unlikely to explain the generalization performance of deep\nnetworks when trained with gradient based methods because training data itself\nplays an important role in determining the degree of memorization. \n\n"}
{"id": "1706.06182", "contents": "Title: Bernoulli Correlations and Cut Polytopes Abstract: Given $n$ symmetric Bernoulli variables, what can be said about their\ncorrelation matrix viewed as a vector? We show that the set of those vectors\n$R(\\mathcal{B}_n)$ is a polytope and identify its vertices. Those extreme\npoints correspond to correlation vectors associated to the discrete uniform\ndistributions on diagonals of the cube $[0,1]^n$. We also show that the\npolytope is affinely isomorphic to a well-known cut polytope ${\\rm CUT}(n)$\nwhich is defined as a convex hull of the cut vectors in a complete graph with\nvertex set $\\{1,\\ldots,n\\}$. The isomorphism is obtained explicitly as\n$R(\\mathcal{B}_n)= {\\mathbf{1}}-2~{\\rm CUT}(n)$. As a corollary of this work,\nit is straightforward using linear programming to determine if a particular\ncorrelation matrix is realizable or not. Furthermore, a sampling method for\nmultivariate symmetric Bernoullis with given correlation is obtained. In some\ncases the method can also be used for general, not exclusively Bernoulli,\nmarginals. \n\n"}
{"id": "1706.06428", "contents": "Title: An online sequence-to-sequence model for noisy speech recognition Abstract: Generative models have long been the dominant approach for speech\nrecognition. The success of these models however relies on the use of\nsophisticated recipes and complicated machinery that is not easily accessible\nto non-practitioners. Recent innovations in Deep Learning have given rise to an\nalternative - discriminative models called Sequence-to-Sequence models, that\ncan almost match the accuracy of state of the art generative models. While\nthese models are easy to train as they can be trained end-to-end in a single\nstep, they have a practical limitation that they can only be used for offline\nrecognition. This is because the models require that the entirety of the input\nsequence be available at the beginning of inference, an assumption that is not\nvalid for instantaneous speech recognition. To address this problem, online\nsequence-to-sequence models were recently introduced. These models are able to\nstart producing outputs as data arrives, and the model feels confident enough\nto output partial transcripts. These models, like sequence-to-sequence are\ncausal - the output produced by the model until any time, $t$, affects the\nfeatures that are computed subsequently. This makes the model inherently more\npowerful than generative models that are unable to change features that are\ncomputed from the data. This paper highlights two main contributions - an\nimprovement to online sequence-to-sequence model training, and its application\nto noisy settings with mixed speech from two speakers. \n\n"}
{"id": "1706.07510", "contents": "Title: Clustering with Noisy Queries Abstract: In this paper, we initiate a rigorous theoretical study of clustering with\nnoisy queries (or a faulty oracle). Given a set of $n$ elements, our goal is to\nrecover the true clustering by asking minimum number of pairwise queries to an\noracle. Oracle can answer queries of the form : \"do elements $u$ and $v$ belong\nto the same cluster?\" -- the queries can be asked interactively (adaptive\nqueries), or non-adaptively up-front, but its answer can be erroneous with\nprobability $p$. In this paper, we provide the first information theoretic\nlower bound on the number of queries for clustering with noisy oracle in both\nsituations. We design novel algorithms that closely match this query complexity\nlower bound, even when the number of clusters is unknown. Moreover, we design\ncomputationally efficient algorithms both for the adaptive and non-adaptive\nsettings. The problem captures/generalizes multiple application scenarios. It\nis directly motivated by the growing body of work that use crowdsourcing for\n{\\em entity resolution}, a fundamental and challenging data mining task aimed\nto identify all records in a database referring to the same entity. Here crowd\nrepresents the noisy oracle, and the number of queries directly relates to the\ncost of crowdsourcing. Another application comes from the problem of {\\em sign\nedge prediction} in social network, where social interactions can be both\npositive and negative, and one must identify the sign of all pair-wise\ninteractions by querying a few pairs. Furthermore, clustering with noisy oracle\nis intimately connected to correlation clustering, leading to improvement\ntherein. Finally, it introduces a new direction of study in the popular {\\em\nstochastic block model} where one has an incomplete stochastic block model\nmatrix to recover the clusters. \n\n"}
{"id": "1706.07564", "contents": "Title: Least Squares Polynomial Chaos Expansion: A Review of Sampling\n  Strategies Abstract: As non-institutive polynomial chaos expansion (PCE) techniques have gained\ngrowing popularity among researchers, we here provide a comprehensive review of\nmajor sampling strategies for the least squares based PCE. Traditional sampling\nmethods, such as Monte Carlo, Latin hypercube, quasi-Monte Carlo, optimal\ndesign of experiments (ODE), Gaussian quadratures, as well as more recent\ntechniques, such as coherence-optimal and randomized quadratures are discussed.\nWe also propose a hybrid sampling method, dubbed alphabetic-coherence-optimal,\nthat employs the so-called alphabetic optimality criteria used in the context\nof ODE in conjunction with coherence-optimal samples. A comparison between the\nempirical performance of the selected sampling methods applied to three\nnumerical examples, including high-order PCE's, high-dimensional problems, and\nlow oversampling ratios, is presented to provide a road map for practitioners\nseeking the most suitable sampling technique for a problem at hand. We observed\nthat the alphabetic-coherence-optimal technique outperforms other sampling\nmethods, specially when high-order ODE are employed and/or the oversampling\nratio is low. \n\n"}
{"id": "1706.08118", "contents": "Title: Large sets avoiding linear patterns Abstract: We prove that for any dimension function $h$ with $h \\prec x^d$ and for any\ncountable set of linear patterns, there exists a compact set $E$ with\n$\\mathcal{H}^h(E)>0$ avoiding all the given patterns. We also give several\napplications and recover results of Keleti, Maga, and M\\'{a}th\\'{e}. \n\n"}
{"id": "1706.09152", "contents": "Title: Generative Bridging Network in Neural Sequence Prediction Abstract: In order to alleviate data sparsity and overfitting problems in maximum\nlikelihood estimation (MLE) for sequence prediction tasks, we propose the\nGenerative Bridging Network (GBN), in which a novel bridge module is introduced\nto assist the training of the sequence prediction model (the generator\nnetwork). Unlike MLE directly maximizing the conditional likelihood, the bridge\nextends the point-wise ground truth to a bridge distribution conditioned on it,\nand the generator is optimized to minimize their KL-divergence. Three different\nGBNs, namely uniform GBN, language-model GBN and coaching GBN, are proposed to\npenalize confidence, enhance language smoothness and relieve learning burden.\nExperiments conducted on two recognized sequence prediction tasks (machine\ntranslation and abstractive text summarization) show that our proposed GBNs can\nyield significant improvements over strong baselines. Furthermore, by analyzing\nsamples drawn from different bridges, expected influences on the generator are\nverified. \n\n"}
{"id": "1706.09873", "contents": "Title: Importance sampling correction versus standard averages of reversible\n  MCMCs in terms of the asymptotic variance Abstract: We establish an ordering criterion for the asymptotic variances of two\nconsistent Markov chain Monte Carlo (MCMC) estimators: an importance sampling\n(IS) estimator, based on an approximate reversible chain and subsequent IS\nweighting, and a standard MCMC estimator, based on an exact reversible chain.\nEssentially, we relax the criterion of the Peskun type covariance ordering by\nconsidering two different invariant probabilities, and obtain, in place of a\nstrict ordering of asymptotic variances, a bound of the asymptotic variance of\nIS by that of the direct MCMC. Simple examples show that IS can have\narbitrarily better or worse asymptotic variance than Metropolis-Hastings and\ndelayed-acceptance (DA) MCMC. Our ordering implies that IS is guaranteed to be\ncompetitive up to a factor depending on the supremum of the (marginal) IS\nweight. We elaborate upon the criterion in case of unbiased estimators as part\nof an auxiliary variable framework. We show how the criterion implies\nasymptotic variance guarantees for IS in terms of pseudo-marginal (PM) and DA\ncorrections, essentially if the ratio of exact and approximate likelihoods is\nbounded. We also show that convergence of the IS chain can be less affected by\nunbounded high-variance unbiased estimators than PM and DA chains. \n\n"}
{"id": "1706.10029", "contents": "Title: Collaborative-controlled LASSO for Constructing Propensity Score-based\n  Estimators in High-Dimensional Data Abstract: Propensity score (PS) based estimators are increasingly used for causal\ninference in observational studies. However, model selection for PS estimation\nin high-dimensional data has received little attention. In these settings, PS\nmodels have traditionally been selected based on the goodness-of-fit for the\ntreatment mechanism itself, without consideration of the causal parameter of\ninterest. Collaborative minimum loss-based estimation (C-TMLE) is a novel\nmethodology for causal inference that takes into account information on the\ncausal parameter of interest when selecting a PS model. This \"collaborative\nlearning\" considers variable associations with both treatment and outcome when\nselecting a PS model in order to minimize a bias-variance trade off in the\nestimated treatment effect. In this study, we introduce a novel approach for\ncollaborative model selection when using the LASSO estimator for PS estimation\nin high-dimensional covariate settings. To demonstrate the importance of\nselecting the PS model collaboratively, we designed quasi-experiments based on\na real electronic healthcare database, where only the potential outcomes were\nmanually generated, and the treatment and baseline covariates remained\nunchanged. Results showed that the C-TMLE algorithm outperformed other\ncompeting estimators for both point estimation and confidence interval\ncoverage. In addition, the PS model selected by C-TMLE could be applied to\nother PS-based estimators, which also resulted in substantive improvement for\nboth point estimation and confidence interval coverage. We illustrate the\ndiscussed concepts through an empirical example comparing the effects of\nnon-selective nonsteroidal anti-inflammatory drugs with selective COX-2\ninhibitors on gastrointestinal complications in a population of Medicare\nbeneficiaries. \n\n"}
{"id": "1707.00010", "contents": "Title: From Parity to Preference-based Notions of Fairness in Classification Abstract: The adoption of automated, data-driven decision making in an ever expanding\nrange of applications has raised concerns about its potential unfairness\ntowards certain social groups. In this context, a number of recent studies have\nfocused on defining, detecting, and removing unfairness from data-driven\ndecision systems. However, the existing notions of fairness, based on parity\n(equality) in treatment or outcomes for different social groups, tend to be\nquite stringent, limiting the overall decision making accuracy. In this paper,\nwe draw inspiration from the fair-division and envy-freeness literature in\neconomics and game theory and propose preference-based notions of fairness --\ngiven the choice between various sets of decision treatments or outcomes, any\ngroup of users would collectively prefer its treatment or outcomes, regardless\nof the (dis)parity as compared to the other groups. Then, we introduce\ntractable proxies to design margin-based classifiers that satisfy these\npreference-based notions of fairness. Finally, we experiment with a variety of\nsynthetic and real-world datasets and show that preference-based fairness\nallows for greater decision accuracy than parity-based fairness. \n\n"}
{"id": "1707.02320", "contents": "Title: The limit point of the pentagram map Abstract: The pentagram map is a discrete dynamical system defined on the space of\npolygons in the plane. In the first paper on the subject, R. Schwartz proved\nthat the pentagram map produces from each convex polygon a sequence of\nsuccessively smaller polygons that converges exponentially to a point. We\ninvestigate the limit point itself, giving an explicit description of its\nCartesian coordinates as roots of certain degree three polynomials. \n\n"}
{"id": "1707.02333", "contents": "Title: Robust Wald-type tests for non-homogeneous observations based on minimum\n  density power divergence estimator Abstract: This paper considers the problem of robust hypothesis testing under\nnon-identically distributed data. We propose Wald-type tests for both simple\nand composite hypothesis for independent but non-homogeneous observations based\non the robust minimum density power divergence estimator of the common\nunderlying parameter. Asymptotic and theoretical robustness properties of the\nproposed tests have been discussed. Application to the problem of testing the\ngeneral linear hypothesis in a generalized linear model with fixed-design has\nbeen considered in detail with specific illustrations for its special cases\nunder normal and Poisson distributions. \n\n"}
{"id": "1707.03307", "contents": "Title: Fast calibrated additive quantile regression Abstract: We propose a novel framework for fitting additive quantile regression models,\nwhich provides well calibrated inference about the conditional quantiles and\nfast automatic estimation of the smoothing parameters, for model structures as\ndiverse as those usable with distributional GAMs, while maintaining equivalent\nnumerical efficiency and stability. The proposed methods are at once\nstatistically rigorous and computationally efficient, because they are based on\nthe general belief updating framework of Bissiri et al. (2016) to loss based\ninference, but compute by adapting the stable fitting methods of Wood et al.\n(2016). We show how the pinball loss is statistically suboptimal relative to a\nnovel smooth generalisation, which also gives access to fast estimation\nmethods. Further, we provide a novel calibration method for efficiently\nselecting the 'learning rate' balancing the loss with the smoothing priors\nduring inference, thereby obtaining reliable quantile uncertainty estimates.\nOur work was motivated by a probabilistic electricity load forecasting\napplication, used here to demonstrate the proposed approach. The methods\ndescribed here are implemented by the qgam R package, available on the\nComprehensive R Archive Network (CRAN). \n\n"}
{"id": "1707.03321", "contents": "Title: A deep learning architecture for temporal sleep stage classification\n  using multivariate and multimodal time series Abstract: Sleep stage classification constitutes an important preliminary exam in the\ndiagnosis of sleep disorders. It is traditionally performed by a sleep expert\nwho assigns to each 30s of signal a sleep stage, based on the visual inspection\nof signals such as electroencephalograms (EEG), electrooculograms (EOG),\nelectrocardiograms (ECG) and electromyograms (EMG). We introduce here the first\ndeep learning approach for sleep stage classification that learns end-to-end\nwithout computing spectrograms or extracting hand-crafted features, that\nexploits all multivariate and multimodal Polysomnography (PSG) signals (EEG,\nEMG and EOG), and that can exploit the temporal context of each 30s window of\ndata. For each modality the first layer learns linear spatial filters that\nexploit the array of sensors to increase the signal-to-noise ratio, and the\nlast layer feeds the learnt representation to a softmax classifier. Our model\nis compared to alternative automatic approaches based on convolutional networks\nor decisions trees. Results obtained on 61 publicly available PSG records with\nup to 20 EEG channels demonstrate that our network architecture yields\nstate-of-the-art performance. Our study reveals a number of insights on the\nspatio-temporal distribution of the signal of interest: a good trade-off for\noptimal classification performance measured with balanced accuracy is to use 6\nEEG with 2 EOG (left and right) and 3 EMG chin channels. Also exploiting one\nminute of data before and after each data segment offers the strongest\nimprovement when a limited number of channels is available. As sleep experts,\nour system exploits the multivariate and multimodal nature of PSG signals in\norder to deliver state-of-the-art classification performance with a small\ncomputational cost. \n\n"}
{"id": "1707.03384", "contents": "Title: Deep Learning-Based Communication Over the Air Abstract: End-to-end learning of communications systems is a fascinating novel concept\nthat has so far only been validated by simulations for block-based\ntransmissions. It allows learning of transmitter and receiver implementations\nas deep neural networks (NNs) that are optimized for an arbitrary\ndifferentiable end-to-end performance metric, e.g., block error rate (BLER). In\nthis paper, we demonstrate that over-the-air transmissions are possible: We\nbuild, train, and run a complete communications system solely composed of NNs\nusing unsynchronized off-the-shelf software-defined radios (SDRs) and\nopen-source deep learning (DL) software libraries. We extend the existing ideas\ntowards continuous data transmission which eases their current restriction to\nshort block lengths but also entails the issue of receiver synchronization. We\novercome this problem by introducing a frame synchronization module based on\nanother NN. A comparison of the BLER performance of the \"learned\" system with\nthat of a practical baseline shows competitive performance close to 1 dB, even\nwithout extensive hyperparameter tuning. We identify several practical\nchallenges of training such a system over actual channels, in particular the\nmissing channel gradient, and propose a two-step learning procedure based on\nthe idea of transfer learning that circumvents this issue. \n\n"}
{"id": "1707.03663", "contents": "Title: Underdamped Langevin MCMC: A non-asymptotic analysis Abstract: We study the underdamped Langevin diffusion when the log of the target\ndistribution is smooth and strongly concave. We present a MCMC algorithm based\non its discretization and show that it achieves $\\varepsilon$ error (in\n2-Wasserstein distance) in $\\mathcal{O}(\\sqrt{d}/\\varepsilon)$ steps. This is a\nsignificant improvement over the best known rate for overdamped Langevin MCMC,\nwhich is $\\mathcal{O}(d/\\varepsilon^2)$ steps under the same\nsmoothness/concavity assumptions.\n  The underdamped Langevin MCMC scheme can be viewed as a version of\nHamiltonian Monte Carlo (HMC) which has been observed to outperform overdamped\nLangevin MCMC methods in a number of application areas. We provide quantitative\nrates that support this empirical wisdom. \n\n"}
{"id": "1707.08316", "contents": "Title: Learning Sparse Representations in Reinforcement Learning with Sparse\n  Coding Abstract: A variety of representation learning approaches have been investigated for\nreinforcement learning; much less attention, however, has been given to\ninvestigating the utility of sparse coding. Outside of reinforcement learning,\nsparse coding representations have been widely used, with non-convex objectives\nthat result in discriminative representations. In this work, we develop a\nsupervised sparse coding objective for policy evaluation. Despite the\nnon-convexity of this objective, we prove that all local minima are global\nminima, making the approach amenable to simple optimization strategies. We\nempirically show that it is key to use a supervised objective, rather than the\nmore straightforward unsupervised sparse coding approach. We compare the\nlearned representations to a canonical fixed sparse representation, called\ntile-coding, demonstrating that the sparse coding representation outperforms a\nwide variety of tilecoding representations. \n\n"}
{"id": "1707.08438", "contents": "Title: Context-Independent Polyphonic Piano Onset Transcription with an\n  Infinite Training Dataset Abstract: Many of the recent approaches to polyphonic piano note onset transcription\nrequire training a machine learning model on a large piano database. However,\nsuch approaches are limited by dataset availability; additional training data\nis difficult to produce, and proposed systems often perform poorly on novel\nrecording conditions. We propose a method to quickly synthesize arbitrary\nquantities of training data, avoiding the need for curating large datasets.\nVarious aspects of piano note dynamics - including nonlinearity of note\nsignatures with velocity, different articulations, temporal clustering of\nonsets, and nonlinear note partial interference - are modeled to match the\ncharacteristics of real pianos. Our method also avoids the disentanglement\nproblem, a recently noted issue affecting machine-learning based approaches. We\ntrain a feed-forward neural network with two hidden layers on our generated\ntraining data and achieve both good transcription performance on the large MAPS\npiano dataset and excellent generalization qualities. \n\n"}
{"id": "1707.09641", "contents": "Title: Towards Visual Explanations for Convolutional Neural Networks via Input\n  Resampling Abstract: The predictive power of neural networks often costs model interpretability.\nSeveral techniques have been developed for explaining model outputs in terms of\ninput features; however, it is difficult to translate such interpretations into\nactionable insight. Here, we propose a framework to analyze predictions in\nterms of the model's internal features by inspecting information flow through\nthe network. Given a trained network and a test image, we select neurons by two\nmetrics, both measured over a set of images created by perturbations to the\ninput image: (1) magnitude of the correlation between the neuron activation and\nthe network output and (2) precision of the neuron activation. We show that the\nformer metric selects neurons that exert large influence over the network\noutput while the latter metric selects neurons that activate on generalizable\nfeatures. By comparing the sets of neurons selected by these two metrics, our\nframework suggests a way to investigate the internal attention mechanisms of\nconvolutional neural networks. \n\n"}
{"id": "1707.09861", "contents": "Title: Reporting Score Distributions Makes a Difference: Performance Study of\n  LSTM-networks for Sequence Tagging Abstract: In this paper we show that reporting a single performance score is\ninsufficient to compare non-deterministic approaches. We demonstrate for common\nsequence tagging tasks that the seed value for the random number generator can\nresult in statistically significant (p < 10^-4) differences for\nstate-of-the-art systems. For two recent systems for NER, we observe an\nabsolute difference of one percentage point F1-score depending on the selected\nseed value, making these systems perceived either as state-of-the-art or\nmediocre. Instead of publishing and reporting single performance scores, we\npropose to compare score distributions based on multiple executions. Based on\nthe evaluation of 50.000 LSTM-networks for five sequence tagging tasks, we\npresent network architectures that produce both superior performance as well as\nare more stable with respect to the remaining hyperparameters. \n\n"}
{"id": "1708.00075", "contents": "Title: Efficient Regret Minimization in Non-Convex Games Abstract: We consider regret minimization in repeated games with non-convex loss\nfunctions. Minimizing the standard notion of regret is computationally\nintractable. Thus, we define a natural notion of regret which permits efficient\noptimization and generalizes offline guarantees for convergence to an\napproximate local optimum. We give gradient-based methods that achieve optimal\nregret, which in turn guarantee convergence to equilibrium in this framework. \n\n"}
{"id": "1708.00205", "contents": "Title: Dynamic Linear Discriminant Analysis in High Dimensional Space Abstract: High-dimensional data that evolve dynamically feature predominantly in the\nmodern data era. As a partial response to this, recent years have seen\nincreasing emphasis to address the dimensionality challenge. However, the\nnon-static nature of these datasets is largely ignored. This paper addresses\nboth challenges by proposing a novel yet simple dynamic linear programming\ndiscriminant (DLPD) rule for binary classification. Different from the usual\nstatic linear discriminant analysis, the new method is able to capture the\nchanging distributions of the underlying populations by modeling their means\nand covariances as smooth functions of covariates of interest. Under an\napproximate sparse condition, we show that the conditional misclassification\nrate of the DLPD rule converges to the Bayes risk in probability uniformly over\nthe range of the variables used for modeling the dynamics, when the\ndimensionality is allowed to grow exponentially with the sample size. The\nminimax lower bound of the estimation of the Bayes risk is also established,\nimplying that the misclassification rate of our proposed rule is minimax-rate\noptimal. The promising performance of the DLPD rule is illustrated via\nextensive simulation studies and the analysis of a breast cancer dataset. \n\n"}
{"id": "1708.00495", "contents": "Title: \"I can assure you [$\\ldots$] that it's going to be all right\" -- A\n  definition, case for, and survey of algorithmic assurances in human-autonomy\n  trust relationships Abstract: As technology become more advanced, those who design, use and are otherwise\naffected by it want to know that it will perform correctly, and understand why\nit does what it does, and how to use it appropriately. In essence they want to\nbe able to trust the systems that are being designed. In this survey we present\nassurances that are the method by which users can understand how to trust this\ntechnology. Trust between humans and autonomy is reviewed, and the implications\nfor the design of assurances are highlighted. A survey of research that has\nbeen performed with respect to assurances is presented, and several key ideas\nare extracted in order to refine the definition of assurances. Several\ndirections for future research are identified and discussed. \n\n"}
{"id": "1708.00829", "contents": "Title: Complexity Results for MCMC derived from Quantitative Bounds Abstract: This paper considers how to obtain MCMC quantitative convergence bounds which\ncan be translated into tight complexity bounds in high-dimensional {settings}.\nWe propose a modified drift-and-minorization approach, which establishes\ngeneralized drift conditions defined in subsets of the state space. The subsets\nare called the \"large sets\", and are chosen to rule out some \"bad\" states which\nhave poor drift property when the dimension of the state space gets large.\nUsing the \"large sets\" together with a \"fitted family of drift functions\", a\nquantitative bound can be obtained which can be translated into a tight\ncomplexity bound. As a demonstration, we analyze several Gibbs samplers and\nobtain complexity upper bounds for the mixing time. In particular, for one\nexample of Gibbs sampler which is related to the James--Stein estimator, we\nshow that the number of iterations required for the Gibbs sampler to converge\nis constant under certain conditions on the observed data and the initial\nstate. It is our hope that this modified drift-and-minorization approach can be\nemployed in many other specific examples to obtain complexity bounds for\nhigh-dimensional Markov chains. \n\n"}
{"id": "1708.00842", "contents": "Title: Latent Parameter Estimation in Fusion Networks Using Separable\n  Likelihoods Abstract: Multi-sensor state space models underpin fusion applications in networks of\nsensors. Estimation of latent parameters in these models has the potential to\nprovide highly desirable capabilities such as network self-calibration.\nConventional solutions to the problem pose difficulties in scaling with the\nnumber of sensors due to the joint multi-sensor filtering involved when\nevaluating the parameter likelihood. In this article, we propose a separable\npseudo-likelihood which is a more accurate approximation compared to a\npreviously proposed alternative under typical operating conditions. In\naddition, we consider using separable likelihoods in the presence of many\nobjects and ambiguity in associating measurements with objects that originated\nthem. To this end, we use a state space model with a hypothesis based\nparameterisation, and, develop an empirical Bayesian perspective in order to\nevaluate separable likelihoods on this model using local filtering. Bayesian\ninference with this likelihood is carried out using belief propagation on the\nassociated pairwise Markov random field. We specify a particle algorithm for\nlatent parameter estimation in a linear Gaussian state space model and\ndemonstrate its efficacy for network self-calibration using measurements from\nnon-cooperative targets in comparison with alternatives. \n\n"}
{"id": "1708.02317", "contents": "Title: Forbidden subgraphs for graphs of bounded spectral radius, with\n  applications to equiangular lines Abstract: The spectral radius of a graph is the largest eigenvalue of its adjacency\nmatrix. Let $\\mathcal{F}(\\lambda)$ be the family of connected graphs of\nspectral radius $\\le \\lambda$. We show that $\\mathcal{F}(\\lambda)$ can be\ndefined by a finite set of forbidden subgraphs if and only if $\\lambda <\n\\lambda^* := \\sqrt{2+\\sqrt{5}} \\approx 2.058$ and $\\lambda \\not\\in \\{\\alpha_2,\n\\alpha_3, \\dots\\}$, where $\\alpha_m = \\beta_m^{1/2} + \\beta_m^{-1/2}$ and\n$\\beta_m$ is the largest root of $x^{m+1}=1+x+\\dots+x^{m-1}$. The study of\nforbidden subgraphs characterization for $\\mathcal{F}(\\lambda)$ is motivated by\nthe problem of estimating the maximum cardinality of equiangular lines in the\n$n$-dimensional Euclidean space $\\mathbb{R}^n$ --- a family of lines through\nthe origin such that the angle between any pair of them is the same. Denote by\n$N_\\alpha(n)$ the maximum number of equiangular lines in $\\mathbb{R}^n$ with\nangle $\\arccos\\alpha$. We establish the asymptotic formula $N_\\alpha(n) =\nc_\\alpha n + O_\\alpha(1)$ for every $\\alpha \\ge \\frac{1}{1+2\\lambda^*}$. In\nparticular, $N_{1/3}(n) = 2n+O(1)$ and $N_{1/5}(n), N_{1/(1+2\\sqrt{2})}(n) =\n\\frac{3}{2}n+O(1)$. Besides we show that $N_\\alpha(n) \\le 1.49n + O_\\alpha(1)$\nfor every $\\alpha \\neq \\tfrac{1}{3}, \\tfrac{1}{5}, \\tfrac{1}{1+2\\sqrt{2}}$,\nwhich improves a recent result of Balla, Dr\\\"axler, Keevash and Sudakov. \n\n"}
{"id": "1708.03272", "contents": "Title: Fast and accurate Bayesian model criticism and conflict diagnostics\n  using R-INLA Abstract: Bayesian hierarchical models are increasingly popular for realistic modelling\nand analysis of complex data. This trend is accompanied by the need for\nflexible, general, and computationally efficient methods for model criticism\nand conflict detection. Usually, a Bayesian hierarchical model incorporates a\ngrouping of the individual data points, for example individuals in repeated\nmeasurement data. In such cases, the following question arises: Are any of the\ngroups \"outliers\", or in conflict with the remaining groups? Existing general\napproaches aiming to answer such questions tend to be extremely computationally\ndemanding when model fitting is based on MCMC. We show how group-level model\ncriticism and conflict detection can be done quickly and accurately through\nintegrated nested Laplace approximations (INLA). The new method is implemented\nas a part of the open source R-INLA package for Bayesian computing\n(http://r-inla.org). \n\n"}
{"id": "1708.04357", "contents": "Title: Graph Classification via Deep Learning with Virtual Nodes Abstract: Learning representation for graph classification turns a variable-size graph\ninto a fixed-size vector (or matrix). Such a representation works nicely with\nalgebraic manipulations. Here we introduce a simple method to augment an\nattributed graph with a virtual node that is bidirectionally connected to all\nexisting nodes. The virtual node represents the latent aspects of the graph,\nwhich are not immediately available from the attributes and local connectivity\nstructures. The expanded graph is then put through any node representation\nmethod. The representation of the virtual node is then the representation of\nthe entire graph. In this paper, we use the recently introduced Column Network\nfor the expanded graph, resulting in a new end-to-end graph classification\nmodel dubbed Virtual Column Network (VCN). The model is validated on two tasks:\n(i) predicting bio-activity of chemical compounds, and (ii) finding software\nvulnerability from source code. Results demonstrate that VCN is competitive\nagainst well-established rivals. \n\n"}
{"id": "1708.05070", "contents": "Title: Data-driven Advice for Applying Machine Learning to Bioinformatics\n  Problems Abstract: As the bioinformatics field grows, it must keep pace not only with new data\nbut with new algorithms. Here we contribute a thorough analysis of 13\nstate-of-the-art, commonly used machine learning algorithms on a set of 165\npublicly available classification problems in order to provide data-driven\nalgorithm recommendations to current researchers. We present a number of\nstatistical and visual comparisons of algorithm performance and quantify the\neffect of model selection and algorithm tuning for each algorithm and dataset.\nThe analysis culminates in the recommendation of five algorithms with\nhyperparameters that maximize classifier performance across the tested\nproblems, as well as general guidelines for applying machine learning to\nsupervised classification problems. \n\n"}
{"id": "1708.05866", "contents": "Title: A Brief Survey of Deep Reinforcement Learning Abstract: Deep reinforcement learning is poised to revolutionise the field of AI and\nrepresents a step towards building autonomous systems with a higher level\nunderstanding of the visual world. Currently, deep learning is enabling\nreinforcement learning to scale to problems that were previously intractable,\nsuch as learning to play video games directly from pixels. Deep reinforcement\nlearning algorithms are also applied to robotics, allowing control policies for\nrobots to be learned directly from camera inputs in the real world. In this\nsurvey, we begin with an introduction to the general field of reinforcement\nlearning, then progress to the main streams of value-based and policy-based\nmethods. Our survey will cover central algorithms in deep reinforcement\nlearning, including the deep $Q$-network, trust region policy optimisation, and\nasynchronous advantage actor-critic. In parallel, we highlight the unique\nadvantages of deep neural networks, focusing on visual understanding via\nreinforcement learning. To conclude, we describe several current areas of\nresearch within the field. \n\n"}
{"id": "1708.07801", "contents": "Title: Nudging the particle filter Abstract: We investigate a new sampling scheme aimed at improving the performance of\nparticle filters whenever (a) there is a significant mismatch between the\nassumed model dynamics and the actual system, or (b) the posterior probability\ntends to concentrate in relatively small regions of the state space. The\nproposed scheme pushes some particles towards specific regions where the\nlikelihood is expected to be high, an operation known as nudging in the\ngeophysics literature. We re-interpret nudging in a form applicable to any\nparticle filtering scheme, as it does not involve any changes in the rest of\nthe algorithm. Since the particles are modified, but the importance weights do\nnot account for this modification, the use of nudging leads to additional bias\nin the resulting estimators. However, we prove analytically that nudged\nparticle filters can still attain asymptotic convergence with the same error\nrates as conventional particle methods. Simple analysis also yields an\nalternative interpretation of the nudging operation that explains its\nrobustness to model errors. Finally, we show numerical results that illustrate\nthe improvements that can be attained using the proposed scheme. In particular,\nwe present nonlinear tracking examples with synthetic data and a model\ninference example using real-world financial data. \n\n"}
{"id": "1708.08917", "contents": "Title: CirCNN: Accelerating and Compressing Deep Neural Networks Using\n  Block-CirculantWeight Matrices Abstract: Large-scale deep neural networks (DNNs) are both compute and memory\nintensive. As the size of DNNs continues to grow, it is critical to improve the\nenergy efficiency and performance while maintaining accuracy. For DNNs, the\nmodel size is an important factor affecting performance, scalability and energy\nefficiency. Weight pruning achieves good compression ratios but suffers from\nthree drawbacks: 1) the irregular network structure after pruning; 2) the\nincreased training complexity; and 3) the lack of rigorous guarantee of\ncompression ratio and inference accuracy. To overcome these limitations, this\npaper proposes CirCNN, a principled approach to represent weights and process\nneural networks using block-circulant matrices. CirCNN utilizes the Fast\nFourier Transform (FFT)-based fast multiplication, simultaneously reducing the\ncomputational complexity (both in inference and training) from O(n2) to\nO(nlogn) and the storage complexity from O(n2) to O(n), with negligible\naccuracy loss. Compared to other approaches, CirCNN is distinct due to its\nmathematical rigor: it can converge to the same effectiveness as DNNs without\ncompression. The CirCNN architecture, a universal DNN inference engine that can\nbe implemented on various hardware/software platforms with configurable network\narchitecture. To demonstrate the performance and energy efficiency, we test\nCirCNN in FPGA, ASIC and embedded processors. Our results show that CirCNN\narchitecture achieves very high energy efficiency and performance with a small\nhardware footprint. Based on the FPGA implementation and ASIC synthesis\nresults, CirCNN achieves 6-102X energy efficiency improvements compared with\nthe best state-of-the-art results. \n\n"}
{"id": "1709.00106", "contents": "Title: First and Second Order Methods for Online Convolutional Dictionary\n  Learning Abstract: Convolutional sparse representations are a form of sparse representation with\na structured, translation invariant dictionary. Most convolutional dictionary\nlearning algorithms to date operate in batch mode, requiring simultaneous\naccess to all training images during the learning process, which results in\nvery high memory usage and severely limits the training data that can be used.\nVery recently, however, a number of authors have considered the design of\nonline convolutional dictionary learning algorithms that offer far better\nscaling of memory and computational cost with training set size than batch\nmethods. This paper extends our prior work, improving a number of aspects of\nour previous algorithm; proposing an entirely new one, with better performance,\nand that supports the inclusion of a spatial mask for learning from incomplete\ndata; and providing a rigorous theoretical analysis of these methods. \n\n"}
{"id": "1709.00232", "contents": "Title: Estimating functions for jump-diffusions Abstract: Asymptotic theory for approximate martingale estimating functions is\ngeneralised to diffusions with finite-activity jumps, when the sampling\nfrequency and terminal sampling time go to infinity. Rate optimality and\nefficiency are of particular concern. Under mild assumptions, it is shown that\nestimators of drift, diffusion, and jump parameters are consistent and\nasymptotically normal, as well as rate-optimal for the drift and jump\nparameters. Additional conditions are derived, which ensure rate-optimality for\nthe diffusion parameter as well as efficiency for all parameters. The findings\nindicate a potentially fruitful direction for the further development of\nestimation for jump-diffusions. \n\n"}
{"id": "1709.01589", "contents": "Title: An active-learning algorithm that combines sparse polynomial chaos\n  expansions and bootstrap for structural reliability analysis Abstract: Polynomial chaos expansions (PCE) have seen widespread use in the context of\nuncertainty quantification. However, their application to structural\nreliability problems has been hindered by the limited performance of PCE in the\ntails of the model response and due to the lack of local metamodel error\nestimates. We propose a new method to provide local metamodel error estimates\nbased on bootstrap resampling and sparse PCE. An initial experimental design is\niteratively updated based on the current estimation of the limit-state surface\nin an active learning algorithm. The greedy algorithm uses the bootstrap-based\nlocal error estimates for the polynomial chaos predictor to identify the best\ncandidate set of points to enrich the experimental design. We demonstrate the\neffectiveness of this approach on a well-known analytical benchmark\nrepresenting a series system, on a truss structure and on a complex realistic\nframe structure problem. \n\n"}
{"id": "1709.02082", "contents": "Title: A deep generative model for gene expression profiles from single-cell\n  RNA sequencing Abstract: We propose a probabilistic model for interpreting gene expression levels that\nare observed through single-cell RNA sequencing. In the model, each cell has a\nlow-dimensional latent representation. Additional latent variables account for\ntechnical effects that may erroneously set some observations of gene expression\nlevels to zero. Conditional distributions are specified by neural networks,\ngiving the proposed model enough flexibility to fit the data well. We use\nvariational inference and stochastic optimization to approximate the posterior\ndistribution. The inference procedure scales to over one million cells, whereas\ncompeting algorithms do not. Even for smaller datasets, for several tasks, the\nproposed procedure outperforms state-of-the-art methods like ZIFA and\nZINB-WaVE. We also extend our framework to account for batch effects and other\nconfounding factors, and propose a Bayesian hypothesis test for differential\nexpression that outperforms DESeq2. \n\n"}
{"id": "1709.02797", "contents": "Title: On the exact relationship between the denoising function and the data\n  distribution Abstract: We prove an exact relationship between the optimal denoising function and the\ndata distribution in the case of additive Gaussian noise, showing that\ndenoising implicitly models the structure of data allowing it to be exploited\nin the unsupervised learning of representations. This result generalizes a\nknown relationship [2], which is valid only in the limit of small corruption\nnoise. \n\n"}
{"id": "1709.03163", "contents": "Title: Variational inference for the multi-armed contextual bandit Abstract: In many biomedical, science, and engineering problems, one must sequentially\ndecide which action to take next so as to maximize rewards. One general class\nof algorithms for optimizing interactions with the world, while simultaneously\nlearning how the world operates, is the multi-armed bandit setting and, in\nparticular, the contextual bandit case. In this setting, for each executed\naction, one observes rewards that are dependent on a given 'context', available\nat each interaction with the world. The Thompson sampling algorithm has\nrecently been shown to enjoy provable optimality properties for this set of\nproblems, and to perform well in real-world settings. It facilitates generative\nand interpretable modeling of the problem at hand. Nevertheless, the design and\ncomplexity of the model limit its application, since one must both sample from\nthe distributions modeled and calculate their expected rewards. We here show\nhow these limitations can be overcome using variational inference to\napproximate complex models, applying to the reinforcement learning case\nadvances developed for the inference case in the machine learning community\nover the past two decades. We consider contextual multi-armed bandit\napplications where the true reward distribution is unknown and complex, which\nwe approximate with a mixture model whose parameters are inferred via\nvariational inference. We show how the proposed variational Thompson sampling\napproach is accurate in approximating the true distribution, and attains\nreduced regrets even with complex reward distributions. The proposed algorithm\nis valuable for practical scenarios where restrictive modeling assumptions are\nundesirable. \n\n"}
{"id": "1709.04126", "contents": "Title: Advanced Algorithms for Penalized Quantile and Composite Quantile\n  Regression Abstract: In this paper, we discuss a family of robust, high-dimensional regression\nmodels for quantile and composite quantile regression, both with and without an\nadaptive lasso penalty for variable selection. We reformulate these quantile\nregression problems and obtain estimators by applying the alternating direction\nmethod of multipliers (ADMM), majorize-minimization (MM), and coordinate\ndescent (CD) algorithms. Our new approaches address the lack of publicly\navailable methods for (composite) quantile regression, especially for\nhigh-dimensional data, both with and without regularization. Through simulation\nstudies, we demonstrate the need for different algorithms applicable to a\nvariety of data settings, which we implement in the cqrReg package for R. For\ncomparison, we also introduce the widely used interior point (IP) formulation\nand test our methods against the IP algorithms in the existing quantreg\npackage. Our simulation studies show that each of our methods, particularly MM\nand CD, excel in different settings such as with large or high-dimensional data\nsets, respectively, and outperform the methods currently implemented in\nquantreg. The ADMM approach offers specific promise for future developments in\nits amenability to parallelization and scalability. \n\n"}
{"id": "1709.04342", "contents": "Title: Model Selection Confidence Sets by Likelihood Ratio Testing Abstract: The traditional activity of model selection aims at discovering a single\nmodel superior to other candidate models. In the presence of pronounced noise,\nhowever, multiple models are often found to explain the same data equally well.\nTo resolve this model selection ambiguity, we introduce the general approach of\nmodel selection confidence sets (MSCSs) based on likelihood ratio testing. A\nMSCS is defined as a list of models statistically indistinguishable from the\ntrue model at a user-specified level of confidence, which extends the familiar\nnotion of confidence intervals to the model-selection framework. Our approach\nguarantees asymptotically correct coverage probability of the true model when\nboth sample size and model dimension increase. We derive conditions under which\nthe MSCS contains all the relevant information about the true model structure.\nIn addition, we propose natural statistics based on the MSCS to measure\nimportance of variables in a principled way that accounts for the overall model\nuncertainty. When the space of feasible models is large, MSCS is implemented by\nan adaptive stochastic search algorithm which samples MSCS models with high\nprobability. The MSCS methodology is illustrated through numerical experiments\non synthetic data and real data examples. \n\n"}
{"id": "1709.04451", "contents": "Title: Alternating minimization and alternating descent over nonconvex sets Abstract: We analyze the performance of alternating minimization for loss functions\noptimized over two variables, where each variable may be restricted to lie in\nsome potentially nonconvex constraint set. This type of setting arises\nnaturally in high-dimensional statistics and signal processing, where the\nvariables often reflect different structures or components within the signals\nbeing considered. Our analysis relies on the notion of local concavity\ncoefficients, which has been proposed in Barber and Ha to measure and quantify\nthe concavity of a general nonconvex set. Our results further reveal important\ndistinctions between alternating and non-alternating methods. Since computing\nthe alternating minimization steps may not be tractable for some problems, we\nalso consider an inexact version of the algorithm and provide a set of\nsufficient conditions to ensure fast convergence of the inexact algorithms. We\ndemonstrate our framework on several examples, including low rank + sparse\ndecomposition and multitask regression, and provide numerical experiments to\nvalidate our theoretical results. \n\n"}
{"id": "1709.04840", "contents": "Title: Semi-standard partial covariance variable selection when irrepresentable\n  conditions fail Abstract: Traditional variable selection methods could fail to be sign consistent when\nirrepresentable conditions are violated. This is especially critical in\nhigh-dimensional settings when the number of predictors exceeds the sample\nsize. In this paper, we propose a new semi-standard partial covariance (SPAC)\napproach which is capable of reducing correlation effects from other covariates\nwhile fully capturing the magnitude of coefficients. The proposed SPAC is\neffective in choosing covariates which have direct effects on the response\nvariable, while eliminating the predictors which are not directly associated\nwith the response but are highly correlated with the relevant predictors. We\nshow that the proposed SPAC method with the Lasso penalty or the smoothly\nclipped absolute deviation (SCAD) penalty possesses strong sign consistency in\nhigh-dimensional settings. Numerical studies and a post-traumatic stress\ndisorder data application also confirm that the proposed method outperforms the\nexisting Lasso, adaptive Lasso, SCAD, Peter-Clark-simple algorithm, and\nfactor-adjusted regularized model selection methods when the irrepresentable\nconditions fail. \n\n"}
{"id": "1709.05510", "contents": "Title: The Geometric Block Model Abstract: To capture the inherent geometric features of many community detection\nproblems, we propose to use a new random graph model of communities that we\ncall a Geometric Block Model. The geometric block model generalizes the random\ngeometric graphs in the same way that the well-studied stochastic block model\ngeneralizes the Erdos-Renyi random graphs. It is also a natural extension of\nrandom community models inspired by the recent theoretical and practical\nadvancement in community detection. While being a topic of fundamental\ntheoretical interest, our main contribution is to show that many practical\ncommunity structures are better explained by the geometric block model. We also\nshow that a simple triangle-counting algorithm to detect communities in the\ngeometric block model is near-optimal. Indeed, even in the regime where the\naverage degree of the graph grows only logarithmically with the number of\nvertices (sparse-graph), we show that this algorithm performs extremely well,\nboth theoretically and practically. In contrast, the triangle-counting\nalgorithm is far from being optimum for the stochastic block model. We simulate\nour results on both real and synthetic datasets to show superior performance of\nboth the new model as well as our algorithm. \n\n"}
{"id": "1709.05885", "contents": "Title: Variational Gaussian Approximation for Poisson Data Abstract: The Poisson model is frequently employed to describe count data, but in a\nBayesian context it leads to an analytically intractable posterior probability\ndistribution. In this work, we analyze a variational Gaussian approximation to\nthe posterior distribution arising from the Poisson model with a Gaussian\nprior. This is achieved by seeking an optimal Gaussian distribution minimizing\nthe Kullback-Leibler divergence from the posterior distribution to the\napproximation, or equivalently maximizing the lower bound for the model\nevidence. We derive an explicit expression for the lower bound, and show the\nexistence and uniqueness of the optimal Gaussian approximation. The lower bound\nfunctional can be viewed as a variant of classical Tikhonov regularization that\npenalizes also the covariance. Then we develop an efficient alternating\ndirection maximization algorithm for solving the optimization problem, and\nanalyze its convergence. We discuss strategies for reducing the computational\ncomplexity via low rank structure of the forward operator and the sparsity of\nthe covariance. Further, as an application of the lower bound, we discuss\nhierarchical Bayesian modeling for selecting the hyperparameter in the prior\ndistribution, and propose a monotonically convergent algorithm for determining\nthe hyperparameter. We present extensive numerical experiments to illustrate\nthe Gaussian approximation and the algorithms. \n\n"}
{"id": "1709.06181", "contents": "Title: On Nesting Monte Carlo Estimators Abstract: Many problems in machine learning and statistics involve nested expectations\nand thus do not permit conventional Monte Carlo (MC) estimation. For such\nproblems, one must nest estimators, such that terms in an outer estimator\nthemselves involve calculation of a separate, nested, estimation. We\ninvestigate the statistical implications of nesting MC estimators, including\ncases of multiple levels of nesting, and establish the conditions under which\nthey converge. We derive corresponding rates of convergence and provide\nempirical evidence that these rates are observed in practice. We further\nestablish a number of pitfalls that can arise from naive nesting of MC\nestimators, provide guidelines about how these can be avoided, and lay out\nnovel methods for reformulating certain classes of nested expectation problems\ninto single expectations, leading to improved convergence rates. We demonstrate\nthe applicability of our work by using our results to develop a new estimator\nfor discrete Bayesian experimental design problems and derive error bounds for\na class of variational objectives. \n\n"}
{"id": "1709.06489", "contents": "Title: Accurate Genomic Prediction Of Human Height Abstract: We construct genomic predictors for heritable and extremely complex human\nquantitative traits (height, heel bone density, and educational attainment)\nusing modern methods in high dimensional statistics (i.e., machine learning).\nReplication tests show that these predictors capture, respectively, $\\sim$40,\n20, and 9 percent of total variance for the three traits. For example,\npredicted heights correlate $\\sim$0.65 with actual height; actual heights of\nmost individuals in validation samples are within a few cm of the prediction.\nThe variance captured for height is comparable to the estimated SNP\nheritability from GCTA (GREML) analysis, and seems to be close to its\nasymptotic value (i.e., as sample size goes to infinity), suggesting that we\nhave captured most of the heritability for the SNPs used. Thus, our results\nresolve the common SNP portion of the \"missing heritability\" problem -- i.e.,\nthe gap between prediction R-squared and SNP heritability. The $\\sim$20k\nactivated SNPs in our height predictor reveal the genetic architecture of human\nheight, at least for common SNPs. Our primary dataset is the UK Biobank cohort,\ncomprised of almost 500k individual genotypes with multiple phenotypes. We also\nuse other datasets and SNPs found in earlier GWAS for out-of-sample validation\nof our results. \n\n"}
{"id": "1709.06814", "contents": "Title: Improvement on $2$-chains inside thin subsets of Euclidean spaces Abstract: We prove that if the Hausdorff dimension of $E\\subset\\mathbb{R}^d$, $d\\geq 2$\nis greater than $\\frac{d}{2}+\\frac{1}{3}$, the set of gaps of $2$-chains inside\n$E$, $$\\Delta_2(E)=\\{(|x-y|, |y-z|): x, y, z\\in E \\}\\subset\\mathbb{R}^2$$ has\npositive Lebesgue measure. It generalizes Wolff-Erdogan's result on distances\nand improves a result of Bennett, Iosevich and Taylor on finite chains.\n  We also consider the similarity class of $2$-chains,\n$$S_2(E)=\\left\\{\\frac{t_1}{t_2}:(t_1,t_2)\\in\\Delta_2(E)\\right\\}=\\left\\{\\frac{|x-y|}{|y-z|}:\nx, y, z\\in E \\right\\}\\subset\\mathbb{R},$$ and show that $|S_2(E)|>0$ whenever\n$\\dim_{\\mathcal{H}}(E)>\\frac{d}{2}+\\frac{1}{7}$. \n\n"}
{"id": "1709.07359", "contents": "Title: Class-Splitting Generative Adversarial Networks Abstract: Generative Adversarial Networks (GANs) produce systematically better quality\nsamples when class label information is provided., i.e. in the conditional GAN\nsetup. This is still observed for the recently proposed Wasserstein GAN\nformulation which stabilized adversarial training and allows considering high\ncapacity network architectures such as ResNet. In this work we show how to\nboost conditional GAN by augmenting available class labels. The new classes\ncome from clustering in the representation space learned by the same GAN model.\nThe proposed strategy is also feasible when no class information is available,\ni.e. in the unsupervised setup. Our generated samples reach state-of-the-art\nInception scores for CIFAR-10 and STL-10 datasets in both supervised and\nunsupervised setup. \n\n"}
{"id": "1709.07710", "contents": "Title: Barker's algorithm for Bayesian inference with intractable likelihoods Abstract: In this expository paper we abstract and describe a simple MCMC scheme for\nsampling from intractable target densities. The approach has been introduced in\nGon\\c{c}alves et al. (2017a) in the specific context of jump-diffusions, and is\nbased on the Barker's algorithm paired with a simple Bernoulli factory type\nscheme, the so called 2-coin algorithm. In many settings it is an alternative\nto standard Metropolis-Hastings pseudo-marginal method for simulating from\nintractable target densities. Although Barker's is well-known to be slightly\nless efficient than Metropolis-Hastings, the key advantage of our approach is\nthat it allows to implement the \"marginal Barker's\" instead of the extended\nstate space pseudo-marginal Metropolis-Hastings, owing to the special form of\nthe accept/reject probability. We shall illustrate our methodology in the\ncontext of Bayesian inference for discretely observed Wright-Fisher family of\ndiffusions. \n\n"}
{"id": "1710.01013", "contents": "Title: Training Feedforward Neural Networks with Standard Logistic Activations\n  is Feasible Abstract: Training feedforward neural networks with standard logistic activations is\nconsidered difficult because of the intrinsic properties of these sigmoidal\nfunctions. This work aims at showing that these networks can be trained to\nachieve generalization performance comparable to those based on hyperbolic\ntangent activations. The solution consists on applying a set of conditions in\nparameter initialization, which have been derived from the study of the\nproperties of a single neuron from an information-theoretic perspective. The\nproposed initialization is validated through an extensive experimental\nanalysis. \n\n"}
{"id": "1710.01434", "contents": "Title: Bayesian Analysis of fMRI data with Spatially-Varying Autoregressive\n  Orders Abstract: Statistical modeling of fMRI data is challenging as the data are both\nspatially and temporally correlated. Spatially, measurements are taken at\nthousands of contiguous regions, called voxels, and temporally measurements are\ntaken at hundreds of time points at each voxel. Recent advances in Bayesian\nhierarchical modeling have addressed the challenges of spatiotemproal structure\nin fMRI data with models incorporating both spatial and temporal priors for\nsignal and noise. While there has been extensive research on modeling the fMRI\nsignal (i.e., the covolution of the experimental design with the functional\nchoice for the hemodynamic response function) and its spatial variability, less\nattention has been paid to realistic modeling of the temporal dependence that\ntypically exists within the fMRI noise, where a low order autoregressive\nprocess is typically adopted. Furthermore, the AR order is held constant across\nvoxels (e.g. AR(1) at each voxel). Motivated by an event-related fMRI\nexperiment, we propose a novel hierarchical Bayesian model with automatic\nselection of the autoregressive orders of the noise process that vary spatially\nover the brain. With simulation studies we show that our model has improved\naccuracy and apply it to our motivating example. \n\n"}
{"id": "1710.02844", "contents": "Title: Reconstruction of Hidden Representation for Robust Feature Extraction Abstract: This paper aims to develop a new and robust approach to feature\nrepresentation. Motivated by the success of Auto-Encoders, we first theoretical\nsummarize the general properties of all algorithms that are based on\ntraditional Auto-Encoders: 1) The reconstruction error of the input can not be\nlower than a lower bound, which can be viewed as a guiding principle for\nreconstructing the input. Additionally, when the input is corrupted with\nnoises, the reconstruction error of the corrupted input also can not be lower\nthan a lower bound. 2) The reconstruction of a hidden representation achieving\nits ideal situation is the necessary condition for the reconstruction of the\ninput to reach the ideal state. 3) Minimizing the Frobenius norm of the\nJacobian matrix of the hidden representation has a deficiency and may result in\na much worse local optimum value. We believe that minimizing the reconstruction\nerror of the hidden representation is more robust than minimizing the Frobenius\nnorm of the Jacobian matrix of the hidden representation. Based on the above\nanalysis, we propose a new model termed Double Denoising Auto-Encoders (DDAEs),\nwhich uses corruption and reconstruction on both the input and the hidden\nrepresentation. We demonstrate that the proposed model is highly flexible and\nextensible and has a potentially better capability to learn invariant and\nrobust feature representations. We also show that our model is more robust than\nDenoising Auto-Encoders (DAEs) for dealing with noises or inessential features.\nFurthermore, we detail how to train DDAEs with two different pre-training\nmethods by optimizing the objective function in a combined and separate manner,\nrespectively. Comparative experiments illustrate that the proposed model is\nsignificantly better for representation learning than the state-of-the-art\nmodels. \n\n"}
{"id": "1710.03206", "contents": "Title: Replication or exploration? Sequential design for stochastic simulation\n  experiments Abstract: We investigate the merits of replication, and provide methods for optimal\ndesign (including replicates), with the goal of obtaining globally accurate\nemulation of noisy computer simulation experiments. We first show that\nreplication can be beneficial from both design and computational perspectives,\nin the context of Gaussian process surrogate modeling. We then develop a\nlookahead based sequential design scheme that can determine if a new run should\nbe at an existing input location (i.e., replicate) or at a new one (explore).\nWhen paired with a newly developed heteroskedastic Gaussian process model, our\ndynamic design scheme facilitates learning of signal and noise relationships\nwhich can vary throughout the input space. We show that it does so efficiently,\non both computational and statistical grounds. In addition to illustrative\nsynthetic examples, we demonstrate performance on two challenging real-data\nsimulation experiments, from inventory management and epidemiology. \n\n"}
{"id": "1710.03487", "contents": "Title: An Analysis of Dropout for Matrix Factorization Abstract: Dropout is a simple yet effective algorithm for regularizing neural networks\nby randomly dropping out units through Bernoulli multiplicative noise, and for\nsome restricted problem classes, such as linear or logistic regression, several\ntheoretical studies have demonstrated the equivalence between dropout and a\nfully deterministic optimization problem with data-dependent Tikhonov\nregularization. This work presents a theoretical analysis of dropout for matrix\nfactorization, where Bernoulli random variables are used to drop a factor,\nthereby attempting to control the size of the factorization. While recent work\nhas demonstrated the empirical effectiveness of dropout for matrix\nfactorization, a theoretical understanding of the regularization properties of\ndropout in this context remains elusive. This work demonstrates the equivalence\nbetween dropout and a fully deterministic model for matrix factorization in\nwhich the factors are regularized by the sum of the product of the norms of the\ncolumns. While the resulting regularizer is closely related to a variational\nform of the nuclear norm, suggesting that dropout may limit the size of the\nfactorization, we show that it is possible to trivially lower the objective\nvalue by doubling the size of the factorization. We show that this problem is\ncaused by the use of a fixed dropout rate, which motivates the use of a rate\nthat increases with the size of the factorization. Synthetic experiments\nvalidate our theoretical findings. \n\n"}
{"id": "1710.03667", "contents": "Title: High-dimensional dynamics of generalization error in neural networks Abstract: We perform an average case analysis of the generalization dynamics of large\nneural networks trained using gradient descent. We study the\npractically-relevant \"high-dimensional\" regime where the number of free\nparameters in the network is on the order of or even larger than the number of\nexamples in the dataset. Using random matrix theory and exact solutions in\nlinear models, we derive the generalization error and training error dynamics\nof learning and analyze how they depend on the dimensionality of data and\nsignal to noise ratio of the learning problem. We find that the dynamics of\ngradient descent learning naturally protect against overtraining and\noverfitting in large networks. Overtraining is worst at intermediate network\nsizes, when the effective number of free parameters equals the number of\nsamples, and thus can be reduced by making a network smaller or larger.\nAdditionally, in the high-dimensional regime, low generalization error requires\nstarting with small initial weights. We then turn to non-linear neural\nnetworks, and show that making networks very large does not harm their\ngeneralization performance. On the contrary, it can in fact reduce\novertraining, even without early stopping or regularization of any sort. We\nidentify two novel phenomena underlying this behavior in overcomplete models:\nfirst, there is a frozen subspace of the weights in which no learning occurs\nunder gradient descent; and second, the statistical properties of the\nhigh-dimensional regime yield better-conditioned input correlations which\nprotect against overtraining. We demonstrate that naive application of\nworst-case theories such as Rademacher complexity are inaccurate in predicting\nthe generalization performance of deep neural networks, and derive an\nalternative bound which incorporates the frozen subspace and conditioning\neffects and qualitatively matches the behavior observed in simulation. \n\n"}
{"id": "1710.04045", "contents": "Title: Neural-Network Quantum States, String-Bond States, and Chiral\n  Topological States Abstract: Neural-Network Quantum States have been recently introduced as an Ansatz for\ndescribing the wave function of quantum many-body systems. We show that there\nare strong connections between Neural-Network Quantum States in the form of\nRestricted Boltzmann Machines and some classes of Tensor-Network states in\narbitrary dimensions. In particular we demonstrate that short-range Restricted\nBoltzmann Machines are Entangled Plaquette States, while fully connected\nRestricted Boltzmann Machines are String-Bond States with a nonlocal geometry\nand low bond dimension. These results shed light on the underlying architecture\nof Restricted Boltzmann Machines and their efficiency at representing many-body\nquantum states. String-Bond States also provide a generic way of enhancing the\npower of Neural-Network Quantum States and a natural generalization to systems\nwith larger local Hilbert space. We compare the advantages and drawbacks of\nthese different classes of states and present a method to combine them\ntogether. This allows us to benefit from both the entanglement structure of\nTensor Networks and the efficiency of Neural-Network Quantum States into a\nsingle Ansatz capable of targeting the wave function of strongly correlated\nsystems. While it remains a challenge to describe states with chiral\ntopological order using traditional Tensor Networks, we show that\nNeural-Network Quantum States and their String-Bond States extension can\ndescribe a lattice Fractional Quantum Hall state exactly. In addition, we\nprovide numerical evidence that Neural-Network Quantum States can approximate a\nchiral spin liquid with better accuracy than Entangled Plaquette States and\nlocal String-Bond States. Our results demonstrate the efficiency of neural\nnetworks to describe complex quantum wave functions and pave the way towards\nthe use of String-Bond States as a tool in more traditional machine-learning\napplications. \n\n"}
{"id": "1710.04749", "contents": "Title: Explaining Aviation Safety Incidents Using Deep Temporal Multiple\n  Instance Learning Abstract: Although aviation accidents are rare, safety incidents occur more frequently\nand require a careful analysis to detect and mitigate risks in a timely manner.\nAnalyzing safety incidents using operational data and producing event-based\nexplanations is invaluable to airline companies as well as to governing\norganizations such as the Federal Aviation Administration (FAA) in the United\nStates. However, this task is challenging because of the complexity involved in\nmining multi-dimensional heterogeneous time series data, the lack of\ntime-step-wise annotation of events in a flight, and the lack of scalable tools\nto perform analysis over a large number of events. In this work, we propose a\nprecursor mining algorithm that identifies events in the multidimensional time\nseries that are correlated with the safety incident. Precursors are valuable to\nsystems health and safety monitoring and in explaining and forecasting safety\nincidents. Current methods suffer from poor scalability to high dimensional\ntime series data and are inefficient in capturing temporal behavior. We propose\nan approach by combining multiple-instance learning (MIL) and deep recurrent\nneural networks (DRNN) to take advantage of MIL's ability to learn using weakly\nsupervised data and DRNN's ability to model temporal behavior. We describe the\nalgorithm, the data, the intuition behind taking a MIL approach, and a\ncomparative analysis of the proposed algorithm with baseline models. We also\ndiscuss the application to a real-world aviation safety problem using data from\na commercial airline company and discuss the model's abilities and\nshortcomings, with some final remarks about possible deployment directions. \n\n"}
{"id": "1710.04778", "contents": "Title: Retinal Fluid Segmentation and Detection in Optical Coherence Tomography\n  Images using Fully Convolutional Neural Network Abstract: As a non-invasive imaging modality, optical coherence tomography (OCT) can\nprovide micrometer-resolution 3D images of retinal structures. Therefore it is\ncommonly used in the diagnosis of retinal diseases associated with edema in and\nunder the retinal layers. In this paper, a new framework is proposed for the\ntask of fluid segmentation and detection in retinal OCT images. Based on the\nraw images and layers segmented by a graph-cut algorithm, a fully convolutional\nneural network was trained to recognize and label the fluid pixels. Random\nforest classification was performed on the segmented fluid regions to detect\nand reject the falsely labeled fluid regions. The leave-one-out cross\nvalidation experiments on the RETOUCH database show that our method performs\nwell in both segmentation (mean Dice: 0.7317) and detection (mean AUC: 0.985)\ntasks. \n\n"}
{"id": "1710.04948", "contents": "Title: Parsimonious Adaptive Rejection Sampling Abstract: Monte Carlo (MC) methods have become very popular in signal processing during\nthe past decades. The adaptive rejection sampling (ARS) algorithms are\nwell-known MC technique which draw efficiently independent samples from\nunivariate target densities. The ARS schemes yield a sequence of proposal\nfunctions that converge toward the target, so that the probability of accepting\na sample approaches one. However, sampling from the proposal pdf becomes more\ncomputationally demanding each time it is updated. We propose the Parsimonious\nAdaptive Rejection Sampling (PARS) method, where an efficient trade-off between\nacceptance rate and proposal complexity is obtained. Thus, the resulting\nalgorithm is faster than the standard ARS approach. \n\n"}
{"id": "1710.05050", "contents": "Title: Learning Independent Features with Adversarial Nets for Non-linear ICA Abstract: Reliable measures of statistical dependence could be useful tools for\nlearning independent features and performing tasks like source separation using\nIndependent Component Analysis (ICA). Unfortunately, many of such measures,\nlike the mutual information, are hard to estimate and optimize directly. We\npropose to learn independent features with adversarial objectives which\noptimize such measures implicitly. These objectives compare samples from the\njoint distribution and the product of the marginals without the need to compute\nany probability densities. We also propose two methods for obtaining samples\nfrom the product of the marginals using either a simple resampling trick or a\nseparate parametric distribution. Our experiments show that this strategy can\neasily be applied to different types of model architectures and solve both\nlinear and non-linear ICA problems. \n\n"}
{"id": "1710.07110", "contents": "Title: Meta-Learning via Feature-Label Memory Network Abstract: Deep learning typically requires training a very capable architecture using\nlarge datasets. However, many important learning problems demand an ability to\ndraw valid inferences from small size datasets, and such problems pose a\nparticular challenge for deep learning. In this regard, various researches on\n\"meta-learning\" are being actively conducted. Recent work has suggested a\nMemory Augmented Neural Network (MANN) for meta-learning. MANN is an\nimplementation of a Neural Turing Machine (NTM) with the ability to rapidly\nassimilate new data in its memory, and use this data to make accurate\npredictions. In models such as MANN, the input data samples and their\nappropriate labels from previous step are bound together in the same memory\nlocations. This often leads to memory interference when performing a task as\nthese models have to retrieve a feature of an input from a certain memory\nlocation and read only the label information bound to that location. In this\npaper, we tried to address this issue by presenting a more robust MANN. We\nrevisited the idea of meta-learning and proposed a new memory augmented neural\nnetwork by explicitly splitting the external memory into feature and label\nmemories. The feature memory is used to store the features of input data\nsamples and the label memory stores their labels. Hence, when predicting the\nlabel of a given input, our model uses its feature memory unit as a reference\nto extract the stored feature of the input, and based on that feature, it\nretrieves the label information of the input from the label memory unit. In\norder for the network to function in this framework, a new memory-writingmodule\nto encode label information into the label memory in accordance with the\nmeta-learning task structure is designed. Here, we demonstrate that our model\noutperforms MANN by a large margin in supervised one-shot classification tasks\nusing Omniglot and MNIST datasets. \n\n"}
{"id": "1710.07437", "contents": "Title: Distributed Deep Transfer Learning by Basic Probability Assignment Abstract: Transfer learning is a popular practice in deep neural networks, but\nfine-tuning of large number of parameters is a hard task due to the complex\nwiring of neurons between splitting layers and imbalance distributions of data\nin pretrained and transferred domains. The reconstruction of the original\nwiring for the target domain is a heavy burden due to the size of\ninterconnections across neurons. We propose a distributed scheme that tunes the\nconvolutional filters individually while backpropagates them jointly by means\nof basic probability assignment. Some of the most recent advances in evidence\ntheory show that in a vast variety of the imbalanced regimes, optimizing of\nsome proper objective functions derived from contingency matrices prevents\nbiases towards high-prior class distributions. Therefore, the original filters\nget gradually transferred based on individual contributions to overall\nperformance of the target domain. This largely reduces the expected complexity\nof transfer learning whilst highly improves precision. Our experiments on\nstandard benchmarks and scenarios confirm the consistent improvement of our\ndistributed deep transfer learning strategy. \n\n"}
{"id": "1710.07438", "contents": "Title: Unified Backpropagation for Multi-Objective Deep Learning Abstract: A common practice in most of deep convolutional neural architectures is to\nemploy fully-connected layers followed by Softmax activation to minimize\ncross-entropy loss for the sake of classification. Recent studies show that\nsubstitution or addition of the Softmax objective to the cost functions of\nsupport vector machines or linear discriminant analysis is highly beneficial to\nimprove the classification performance in hybrid neural networks. We propose a\nnovel paradigm to link the optimization of several hybrid objectives through\nunified backpropagation. This highly alleviates the burden of extensive\nboosting for independent objective functions or complex formulation of\nmultiobjective gradients. Hybrid loss functions are linked by basic probability\nassignment from evidence theory. We conduct our experiments for a variety of\nscenarios and standard datasets to evaluate the advantage of our proposed\nunification approach to deliver consistent improvements into the classification\nperformance of deep convolutional neural networks. \n\n"}
{"id": "1710.07797", "contents": "Title: Optimal Rates for Learning with Nystr\\\"om Stochastic Gradient Methods Abstract: In the setting of nonparametric regression, we propose and study a\ncombination of stochastic gradient methods with Nystr\\\"om subsampling, allowing\nmultiple passes over the data and mini-batches. Generalization error bounds for\nthe studied algorithm are provided. Particularly, optimal learning rates are\nderived considering different possible choices of the step-size, the mini-batch\nsize, the number of iterations/passes, and the subsampling level. In comparison\nwith state-of-the-art algorithms such as the classic stochastic gradient\nmethods and kernel ridge regression with Nystr\\\"om, the studied algorithm has\nadvantages on the computational complexity, while achieving the same optimal\nlearning rates. Moreover, our results indicate that using mini-batches can\nreduce the total computational cost while achieving the same optimal\nstatistical results. \n\n"}
{"id": "1710.08388", "contents": "Title: A Test for Separability in Covariance Operators of Random Surfaces Abstract: The assumption of separability is a simplifying and very popular assumption\nin the analysis of spatio-temporal or hypersurface data structures. It is often\nmade in situations where the covariance structure cannot be easily estimated,\nfor example because of a small sample size or because of computational storage\nproblems. In this paper we propose a new and very simple test to validate this\nassumption. Our approach is based on a measure of separability which is zero in\nthe case of separability and positive otherwise. The measure can be estimated\nwithout calculating the full non-separable covariance operator. We prove\nasymptotic normality of the corresponding statistic with a limiting variance,\nwhich can easily be estimated from the available data. As a consequence\nquantiles of the standard normal distribution can be used to obtain critical\nvalues and the new test of separability is very easy to implement. In\nparticular, our approach does neither require projections on subspaces\ngenerated by the eigenfunctions of the covariance operator, nor resampling\nprocedures to obtain critical values nor distributional assumptions as used by\nother available methods of constructing tests for separability. We investigate\nthe finite sample performance by means of a simulation study and also provide a\ncomparison with the currently available methodology. Finally, the new procedure\nis illustrated analyzing wind speed and temperature data. \n\n"}
{"id": "1710.09567", "contents": "Title: Big Data Classification Using Augmented Decision Trees Abstract: We present an algorithm for classification tasks on big data. Experiments\nconducted as part of this study indicate that the algorithm can be as accurate\nas ensemble methods such as random forests or gradient boosted trees. Unlike\nensemble methods, the models produced by the algorithm can be easily\ninterpreted. The algorithm is based on a divide and conquer strategy and\nconsists of two steps. The first step consists of using a decision tree to\nsegment the large dataset. By construction, decision trees attempt to create\nhomogeneous class distributions in their leaf nodes. However, non-homogeneous\nleaf nodes are usually produced. The second step of the algorithm consists of\nusing a suitable classifier to determine the class labels for the\nnon-homogeneous leaf nodes. The decision tree segment provides a coarse segment\nprofile while the leaf level classifier can provide information about the\nattributes that affect the label within a segment. \n\n"}
{"id": "1710.10709", "contents": "Title: Distributional Consistency of Lasso by Perturbation Bootstrap Abstract: Least Absolute Shrinkage and Selection Operator or the Lasso, introduced by\nTibshirani (1996), is a popular estimation procedure in multiple linear\nregression when underlying design has a sparse structure, because of its\nproperty that it sets some regression coefficients exactly equal to 0. In this\narticle, we develop a perturbation bootstrap method and establish its validity\nin approximating the distribution of the Lasso in heteroscedastic linear\nregression. We allow the underlying covariates to be either random or\nnon-random. We show that the proposed bootstrap method works irrespective of\nthe nature of the covariates, unlike the resample-based bootstrap of Freedman\n(1981) which must be tailored based on the nature (random vs non-random) of the\ncovariates. Simulation study also justifies our method in finite samples. \n\n"}
{"id": "1710.10768", "contents": "Title: Distance-based classifier by data transformation for high-dimension,\n  strongly spiked eigenvalue models Abstract: We consider classifiers for high-dimensional data under the strongly spiked\neigenvalue (SSE) model. We first show that high-dimensional data often have the\nSSE model. We consider a distance-based classifier using eigenstructures for\nthe SSE model. We apply the noise reduction methodology to estimation of the\neigenvalues and eigenvectors in the SSE model. We create a new distance-based\nclassifier by transforming data from the SSE model to the non-SSE model. We\ngive simulation studies and discuss the performance of the new classifier.\nFinally, we demonstrate the new classifier by using microarray data sets. \n\n"}
{"id": "1710.10784", "contents": "Title: How deep learning works --The geometry of deep learning Abstract: Why and how that deep learning works well on different tasks remains a\nmystery from a theoretical perspective. In this paper we draw a geometric\npicture of the deep learning system by finding its analogies with two existing\ngeometric structures, the geometry of quantum computations and the geometry of\nthe diffeomorphic template matching. In this framework, we give the geometric\nstructures of different deep learning systems including convolutional neural\nnetworks, residual networks, recursive neural networks, recurrent neural\nnetworks and the equilibrium prapagation framework. We can also analysis the\nrelationship between the geometrical structures and their performance of\ndifferent networks in an algorithmic level so that the geometric framework may\nguide the design of the structures and algorithms of deep learning systems. \n\n"}
{"id": "1711.00413", "contents": "Title: Combinatorial cost: a coarse setting Abstract: The main inspiration for this paper is a paper by Elek where he introduces\ncombinatorial cost for graph sequences. We show that having cost equal to 1 and\nhyperfiniteness are coarse invariants. We also show `cost-1' for box spaces\nbehaves multiplicatively when taking subgroups. We show that graph sequences\ncoming from Farber sequences of a group have property A if and only if the\ngroup is amenable. The same is true for hyperfiniteness. This generalises a\ntheorem by Elek. Furthermore we optimise this result when Farber sequences are\nreplaced by sofic approximations. In doing so we introduce a new concept:\nproperty almost-A. \n\n"}
{"id": "1711.00572", "contents": "Title: Consistent estimation of the spectrum of trace class data augmentation\n  algorithms Abstract: Markov chain Monte Carlo is widely used in a variety of scientific\napplications to generate approximate samples from intractable distributions. A\nthorough understanding of the convergence and mixing properties of these Markov\nchains can be obtained by studying the spectrum of the associated Markov\noperator. While several methods to bound/estimate the second largest eigenvalue\nare available in the literature, very few general techniques for consistent\nestimation of the entire spectrum have been proposed. Existing methods for this\npurpose require the Markov transition density to be available in closed form,\nwhich is often not true in practice, especially in modern statistical\napplications. In this paper, we propose a novel method to consistently estimate\nthe entire spectrum of a general class of Markov chains arising from a popular\nand widely used statistical approach known as Data Augmentation. The transition\ndensities of these Markov chains can often only be expressed as intractable\nintegrals. We illustrate the applicability of our method using real and\nsimulated data. \n\n"}
{"id": "1711.00922", "contents": "Title: Binary Bouncy Particle Sampler Abstract: The Bouncy Particle Sampler is a novel rejection-free non-reversible sampler\nfor differentiable probability distributions over continuous variables. We\ngeneralize the algorithm to piecewise differentiable distributions and apply it\nto generic binary distributions using a piecewise differentiable augmentation.\nWe illustrate the new algorithm in a binary Markov Random Field example, and\ncompare it to binary Hamiltonian Monte Carlo. Our results suggest that binary\nBPS samplers are better for easy to mix distributions. \n\n"}
{"id": "1711.02195", "contents": "Title: Optimality of Approximate Inference Algorithms on Stable Instances Abstract: Approximate algorithms for structured prediction problems---such as LP\nrelaxations and the popular alpha-expansion algorithm (Boykov et al.\n2001)---typically far exceed their theoretical performance guarantees on\nreal-world instances. These algorithms often find solutions that are very close\nto optimal. The goal of this paper is to partially explain the performance of\nalpha-expansion and an LP relaxation algorithm on MAP inference in\nFerromagnetic Potts models (FPMs). Our main results give stability conditions\nunder which these two algorithms provably recover the optimal MAP solution.\nThese theoretical results complement numerous empirical observations of good\nperformance. \n\n"}
{"id": "1711.04694", "contents": "Title: ABCpy: A High-Performance Computing Perspective to Approximate Bayesian\n  Computation Abstract: ABCpy is a highly modular scientific library for Approximate Bayesian\nComputation (ABC) written in Python. The main contribution of this paper is to\ndocument a software engineering effort that enables domain scientists to easily\napply ABC to their research without being ABC experts; using ABCpy they can\neasily run large parallel simulations without much knowledge about\nparallelization. Further, ABCpy enables ABC experts to easily develop new\ninference schemes and evaluate them in a standardized environment and to extend\nthe library with new algorithms. These benefits come mainly from the modularity\nof ABCpy. We give an overview of the design of ABCpy and provide a performance\nevaluation concentrating on parallelization. This points us towards the\ninherent imbalance in some of the ABC algorithms. We develop a dynamic\nscheduling MPI implementation to mitigate this issue and evaluate the various\nABC algorithms according to their adaptability towards high-performance\ncomputing. \n\n"}
{"id": "1711.04755", "contents": "Title: ACtuAL: Actor-Critic Under Adversarial Learning Abstract: Generative Adversarial Networks (GANs) are a powerful framework for deep\ngenerative modeling. Posed as a two-player minimax problem, GANs are typically\ntrained end-to-end on real-valued data and can be used to train a generator of\nhigh-dimensional and realistic images. However, a major limitation of GANs is\nthat training relies on passing gradients from the discriminator through the\ngenerator via back-propagation. This makes it fundamentally difficult to train\nGANs with discrete data, as generation in this case typically involves a\nnon-differentiable function. These difficulties extend to the reinforcement\nlearning setting when the action space is composed of discrete decisions. We\naddress these issues by reframing the GAN framework so that the generator is no\nlonger trained using gradients through the discriminator, but is instead\ntrained using a learned critic in the actor-critic framework with a Temporal\nDifference (TD) objective. This is a natural fit for sequence modeling and we\nuse it to achieve improvements on language modeling tasks over the standard\nTeacher-Forcing methods. \n\n"}
{"id": "1711.04817", "contents": "Title: Sparse quadratic classification rules via linear dimension reduction Abstract: We consider the problem of high-dimensional classification between the two\ngroups with unequal covariance matrices. Rather than estimating the full\nquadratic discriminant rule, we propose to perform simultaneous variable\nselection and linear dimension reduction on original data, with the subsequent\napplication of quadratic discriminant analysis on the reduced space. In\ncontrast to quadratic discriminant analysis, the proposed framework doesn't\nrequire estimation of precision matrices and scales linearly with the number of\nmeasurements, making it especially attractive for the use on high-dimensional\ndatasets. We support the methodology with theoretical guarantees on variable\nselection consistency, and empirical comparison with competing approaches. We\napply the method to gene expression data of breast cancer patients, and confirm\nthe crucial importance of ESR1 gene in differentiating estrogen receptor\nstatus. \n\n"}
{"id": "1711.04837", "contents": "Title: Improving Factor-Based Quantitative Investing by Forecasting Company\n  Fundamentals Abstract: On a periodic basis, publicly traded companies are required to report\nfundamentals: financial data such as revenue, operating income, debt, among\nothers. These data points provide some insight into the financial health of a\ncompany. Academic research has identified some factors, i.e. computed features\nof the reported data, that are known through retrospective analysis to\noutperform the market average. Two popular factors are the book value\nnormalized by market capitalization (book-to-market) and the operating income\nnormalized by the enterprise value (EBIT/EV). In this paper: we first show\nthrough simulation that if we could (clairvoyantly) select stocks using factors\ncalculated on future fundamentals (via oracle), then our portfolios would far\noutperform a standard factor approach. Motivated by this analysis, we train\ndeep neural networks to forecast future fundamentals based on a trailing\n5-years window. Quantitative analysis demonstrates a significant improvement in\nMSE over a naive strategy. Moreover, in retrospective analysis using an\nindustry-grade stock portfolio simulator (backtester), we show an improvement\nin compounded annual return to 17.1% (MLP) vs 14.4% for a standard factor\nmodel. \n\n"}
{"id": "1711.06195", "contents": "Title: Neurology-as-a-Service for the Developing World Abstract: Electroencephalography (EEG) is an extensively-used and well-studied\ntechnique in the field of medical diagnostics and treatment for brain\ndisorders, including epilepsy, migraines, and tumors. The analysis and\ninterpretation of EEGs require physicians to have specialized training, which\nis not common even among most doctors in the developed world, let alone the\ndeveloping world where physician shortages plague society. This problem can be\naddressed by teleEEG that uses remote EEG analysis by experts or by local\ncomputer processing of EEGs. However, both of these options are prohibitively\nexpensive and the second option requires abundant computing resources and\ninfrastructure, which is another concern in developing countries where there\nare resource constraints on capital and computing infrastructure. In this work,\nwe present a cloud-based deep neural network approach to provide decision\nsupport for non-specialist physicians in EEG analysis and interpretation. Named\n`neurology-as-a-service,' the approach requires almost no manual intervention\nin feature engineering and in the selection of an optimal architecture and\nhyperparameters of the neural network. In this study, we deploy a pipeline that\nincludes moving EEG data to the cloud and getting optimal models for various\nclassification tasks. Our initial prototype has been tested only in developed\nworld environments to-date, but our intention is to test it in developing world\nenvironments in future work. We demonstrate the performance of our proposed\napproach using the BCI2000 EEG MMI dataset, on which our service attains 63.4%\naccuracy for the task of classifying real vs. imaginary activity performed by\nthe subject, which is significantly higher than what is obtained with a shallow\napproach such as support vector machines. \n\n"}
{"id": "1711.06798", "contents": "Title: MorphNet: Fast & Simple Resource-Constrained Structure Learning of Deep\n  Networks Abstract: We present MorphNet, an approach to automate the design of neural network\nstructures. MorphNet iteratively shrinks and expands a network, shrinking via a\nresource-weighted sparsifying regularizer on activations and expanding via a\nuniform multiplicative factor on all layers. In contrast to previous\napproaches, our method is scalable to large networks, adaptable to specific\nresource constraints (e.g. the number of floating-point operations per\ninference), and capable of increasing the network's performance. When applied\nto standard network architectures on a wide variety of datasets, our approach\ndiscovers novel structures in each domain, obtaining higher performance while\nrespecting the resource constraint. \n\n"}
{"id": "1711.07005", "contents": "Title: Convergence Analysis of the Dynamics of a Special Kind of Two-Layered\n  Neural Networks with $\\ell_1$ and $\\ell_2$ Regularization Abstract: In this paper, we made an extension to the convergence analysis of the\ndynamics of two-layered bias-free networks with one $ReLU$ output. We took into\nconsideration two popular regularization terms: the $\\ell_1$ and $\\ell_2$ norm\nof the parameter vector $w$, and added it to the square loss function with\ncoefficient $\\lambda/2$. We proved that when $\\lambda$ is small, the weight\nvector $w$ converges to the optimal solution $\\hat{w}$ (with respect to the new\nloss function) with probability $\\geq (1-\\varepsilon)(1-A_d)/2$ under random\ninitiations in a sphere centered at the origin, where $\\varepsilon$ is a small\nvalue and $A_d$ is a constant. Numerical experiments including phase diagrams\nand repeated simulations verified our theory. \n\n"}
{"id": "1711.07424", "contents": "Title: Informed proposals for local MCMC in discrete spaces Abstract: There is a lack of methodological results to design efficient Markov chain\nMonte Carlo (MCMC) algorithms for statistical models with discrete-valued\nhigh-dimensional parameters. Motivated by this consideration, we propose a\nsimple framework for the design of informed MCMC proposals (i.e.\nMetropolis-Hastings proposal distributions that appropriately incorporate local\ninformation about the target) which is naturally applicable to both discrete\nand continuous spaces. We explicitly characterize the class of optimal proposal\ndistributions under this framework, which we refer to as locally-balanced\nproposals, and prove their Peskun-optimality in high-dimensional regimes. The\nresulting algorithms are straightforward to implement in discrete spaces and\nprovide orders of magnitude improvements in efficiency compared to alternative\nMCMC schemes, including discrete versions of Hamiltonian Monte Carlo.\nSimulations are performed with both simulated and real datasets, including a\ndetailed application to Bayesian record linkage. A direct connection with\ngradient-based MCMC suggests that locally-balanced proposals may be seen as a\nnatural way to extend the latter to discrete spaces. \n\n"}
{"id": "1711.07582", "contents": "Title: CVXR: An R Package for Disciplined Convex Optimization Abstract: CVXR is an R package that provides an object-oriented modeling language for\nconvex optimization, similar to CVX, CVXPY, YALMIP, and Convex.jl. It allows\nthe user to formulate convex optimization problems in a natural mathematical\nsyntax rather than the restrictive form required by most solvers. The user\nspecifies an objective and set of constraints by combining constants,\nvariables, and parameters using a library of functions with known mathematical\nproperties. CVXR then applies signed disciplined convex programming (DCP) to\nverify the problem's convexity. Once verified, the problem is converted into\nstandard conic form using graph implementations and passed to a cone solver\nsuch as ECOS or SCS. We demonstrate CVXR's modeling framework with several\napplications. \n\n"}
{"id": "1711.08421", "contents": "Title: Relief-Based Feature Selection: Introduction and Review Abstract: Feature selection plays a critical role in biomedical data mining, driven by\nincreasing feature dimensionality in target problems and growing interest in\nadvanced but computationally expensive methodologies able to model complex\nassociations. Specifically, there is a need for feature selection methods that\nare computationally efficient, yet sensitive to complex patterns of\nassociation, e.g. interactions, so that informative features are not mistakenly\neliminated prior to downstream modeling. This paper focuses on Relief-based\nalgorithms (RBAs), a unique family of filter-style feature selection algorithms\nthat have gained appeal by striking an effective balance between these\nobjectives while flexibly adapting to various data characteristics, e.g.\nclassification vs. regression. First, this work broadly examines types of\nfeature selection and defines RBAs within that context. Next, we introduce the\noriginal Relief algorithm and associated concepts, emphasizing the intuition\nbehind how it works, how feature weights generated by the algorithm can be\ninterpreted, and why it is sensitive to feature interactions without evaluating\ncombinations of features. Lastly, we include an expansive review of RBA\nmethodological research beyond Relief and its popular descendant, ReliefF. In\nparticular, we characterize branches of RBA research, and provide comparative\nsummaries of RBA algorithms including contributions, strategies, functionality,\ntime complexity, adaptation to key data characteristics, and software\navailability. \n\n"}
{"id": "1711.09091", "contents": "Title: Demystifying AlphaGo Zero as AlphaGo GAN Abstract: The astonishing success of AlphaGo Zero\\cite{Silver_AlphaGo} invokes a\nworldwide discussion of the future of our human society with a mixed mood of\nhope, anxiousness, excitement and fear. We try to dymystify AlphaGo Zero by a\nqualitative analysis to indicate that AlphaGo Zero can be understood as a\nspecially structured GAN system which is expected to possess an inherent good\nconvergence property. Thus we deduct the success of AlphaGo Zero may not be a\nsign of a new generation of AI. \n\n"}
{"id": "1711.09365", "contents": "Title: Ensemble-marginalized Kalman filter for linear time-dependent PDEs with\n  noisy boundary conditions: Application to heat transfer in building walls Abstract: In this work, we present the ensemble-marginalized Kalman filter (EnMKF), a\nsequential algorithm analogous to our previously proposed approach [1,2], for\nestimating the state and parameters of linear parabolic partial differential\nequations in initial-boundary value problems when the boundary data are noisy.\nWe apply EnMKF to infer the thermal properties of building walls and to\nestimate the corresponding heat flux from real and synthetic data. Compared\nwith a modified Ensemble Kalman Filter (EnKF) that is not marginalized, EnMKF\nreduces the bias error, avoids the collapse of the ensemble without needing to\nadd inflation, and converges to the mean field posterior using $50\\%$ or less\nof the ensemble size required by EnKF. According to our results, the\nmarginalization technique in EnMKF is key to performance improvement with\nsmaller ensembles at any fixed time. \n\n"}
{"id": "1711.09535", "contents": "Title: Learning with Biased Complementary Labels Abstract: In this paper, we study the classification problem in which we have access to\neasily obtainable surrogate for true labels, namely complementary labels, which\nspecify classes that observations do \\textbf{not} belong to. Let $Y$ and\n$\\bar{Y}$ be the true and complementary labels, respectively. We first model\nthe annotation of complementary labels via transition probabilities\n$P(\\bar{Y}=i|Y=j), i\\neq j\\in\\{1,\\cdots,c\\}$, where $c$ is the number of\nclasses. Previous methods implicitly assume that $P(\\bar{Y}=i|Y=j), \\forall\ni\\neq j$, are identical, which is not true in practice because humans are\nbiased toward their own experience. For example, as shown in Figure 1, if an\nannotator is more familiar with monkeys than prairie dogs when providing\ncomplementary labels for meerkats, she is more likely to employ \"monkey\" as a\ncomplementary label. We therefore reason that the transition probabilities will\nbe different. In this paper, we propose a framework that contributes three main\ninnovations to learning with \\textbf{biased} complementary labels: (1) It\nestimates transition probabilities with no bias. (2) It provides a general\nmethod to modify traditional loss functions and extends standard deep neural\nnetwork classifiers to learn with biased complementary labels. (3) It\ntheoretically ensures that the classifier learned with complementary labels\nconverges to the optimal one learned with true labels. Comprehensive\nexperiments on several benchmark datasets validate the superiority of our\nmethod to current state-of-the-art methods. \n\n"}
{"id": "1711.10819", "contents": "Title: Objective Bayesian inference with proper scoring rules Abstract: Standard Bayesian analyses can be difficult to perform when the full\nlikelihood, and consequently the full posterior distribution, is too complex\nand difficult to specify or if robustness with respect to data or to model\nmisspecifications is required. In these situations, we suggest to resort to a\nposterior distribution for the parameter of interest based on proper scoring\nrules. Scoring rules are loss functions designed to measure the quality of a\nprobability distribution for a random variable, given its observed value.\nImportant examples are the Tsallis score and the Hyv\\\"arinen score, which allow\nus to deal with model misspecifications or with complex models. Also the full\nand the composite likelihoods are both special instances of scoring rules.\n  The aim of this paper is twofold. Firstly, we discuss the use of scoring\nrules in the Bayes formula in order to compute a posterior distribution, named\nSR-posterior distribution, and we derive its asymptotic normality. Secondly, we\npropose a procedure for building default priors for the unknown parameter of\ninterest that can be used to update the information provided by the scoring\nrule in the SR-posterior distribution. In particular, a reference prior is\nobtained by maximizing the average $\\alpha-$divergence from the SR-posterior\ndistribution. For $0 \\leq |\\alpha|<1$, the result is a Jeffreys-type prior that\nis proportional to the square root of the determinant of the Godambe\ninformation matrix associated to the scoring rule. Some examples are discussed. \n\n"}
{"id": "1711.11279", "contents": "Title: Interpretability Beyond Feature Attribution: Quantitative Testing with\n  Concept Activation Vectors (TCAV) Abstract: The interpretation of deep learning models is a challenge due to their size,\ncomplexity, and often opaque internal state. In addition, many systems, such as\nimage classifiers, operate on low-level features rather than high-level\nconcepts. To address these challenges, we introduce Concept Activation Vectors\n(CAVs), which provide an interpretation of a neural net's internal state in\nterms of human-friendly concepts. The key idea is to view the high-dimensional\ninternal state of a neural net as an aid, not an obstacle. We show how to use\nCAVs as part of a technique, Testing with CAVs (TCAV), that uses directional\nderivatives to quantify the degree to which a user-defined concept is important\nto a classification result--for example, how sensitive a prediction of \"zebra\"\nis to the presence of stripes. Using the domain of image classification as a\ntesting ground, we describe how CAVs may be used to explore hypotheses and\ngenerate insights for a standard image classification network as well as a\nmedical application. \n\n"}
{"id": "1711.11286", "contents": "Title: Sensitivity analysis for inverse probability weighting estimators via\n  the percentile bootstrap Abstract: To identify the estimand in missing data problems and observational studies,\nit is common to base the statistical estimation on the \"missing at random\" and\n\"no unmeasured confounder\" assumptions. However, these assumptions are\nunverifiable using empirical data and pose serious threats to the validity of\nthe qualitative conclusions of the statistical inference. A sensitivity\nanalysis asks how the conclusions may change if the unverifiable assumptions\nare violated to a certain degree. In this paper we consider a marginal\nsensitivity model which is a natural extension of Rosenbaum's sensitivity model\nthat is widely used for matched observational studies. We aim to construct\nconfidence intervals based on inverse probability weighting estimators, such\nthat asymptotically the intervals have at least nominal coverage of the\nestimand whenever the data generating distribution is in the collection of\nmarginal sensitivity models. We use a percentile bootstrap and a generalized\nminimax/maximin inequality to transform this intractable problem to a linear\nfractional programming problem, which can be solved very efficiently. We\nillustrate our method using a real dataset to estimate the causal effect of\nfish consumption on blood mercury level. \n\n"}
{"id": "1712.00232", "contents": "Title: Optimal Algorithms for Distributed Optimization Abstract: In this paper, we study the optimal convergence rate for distributed convex\noptimization problems in networks. We model the communication restrictions\nimposed by the network as a set of affine constraints and provide optimal\ncomplexity bounds for four different setups, namely: the function $F(\\xb)\n\\triangleq \\sum_{i=1}^{m}f_i(\\xb)$ is strongly convex and smooth, either\nstrongly convex or smooth or just convex. Our results show that Nesterov's\naccelerated gradient descent on the dual problem can be executed in a\ndistributed manner and obtains the same optimal rates as in the centralized\nversion of the problem (up to constant or logarithmic factors) with an\nadditional cost related to the spectral gap of the interaction matrix. Finally,\nwe discuss some extensions to the proposed setup such as proximal friendly\nfunctions, time-varying graphs, improvement of the condition numbers. \n\n"}
{"id": "1712.01521", "contents": "Title: An Online Algorithm for Nonparametric Correlations Abstract: Nonparametric correlations such as Spearman's rank correlation and Kendall's\ntau correlation are widely applied in scientific and engineering fields. This\npaper investigates the problem of computing nonparametric correlations on the\nfly for streaming data. Standard batch algorithms are generally too slow to\nhandle real-world big data applications. They also require too much memory\nbecause all the data need to be stored in the memory before processing. This\npaper proposes a novel online algorithm for computing nonparametric\ncorrelations. The algorithm has O(1) time complexity and O(1) memory cost and\nis quite suitable for edge devices, where only limited memory and processing\npower are available. You can seek a balance between speed and accuracy by\nchanging the number of cutpoints specified in the algorithm. The online\nalgorithm can compute the nonparametric correlations 10 to 1,000 times faster\nthan the corresponding batch algorithm, and it can compute them based either on\nall past observations or on fixed-size sliding windows. \n\n"}
{"id": "1712.04144", "contents": "Title: Information Perspective to Probabilistic Modeling: Boltzmann Machines\n  versus Born Machines Abstract: We compare and contrast the statistical physics and quantum physics inspired\napproaches for unsupervised generative modeling of classical data. The two\napproaches represent probabilities of observed data using energy-based models\nand quantum states respectively.Classical and quantum information patterns of\nthe target datasets therefore provide principled guidelines for structural\ndesign and learning in these two approaches. Taking the restricted Boltzmann\nmachines (RBM) as an example, we analyze the information theoretical bounds of\nthe two approaches. We verify our reasonings by comparing the performance of\nRBMs of various architectures on the standard MNIST datasets. \n\n"}
{"id": "1712.05390", "contents": "Title: Partisan gerrymandering with geographically compact districts Abstract: Bizarrely shaped voting districts are frequently lambasted as likely\ninstances of gerrymandering. In order to systematically identify such\ninstances, researchers have devised several tests for so-called geographic\ncompactness (i.e., shape niceness). We demonstrate that under certain\nconditions, a party can gerrymander a competitive state into geographically\ncompact districts to win an average of over 70% of the districts. Our results\nsuggest that geometric features alone may fail to adequately combat partisan\ngerrymandering. \n\n"}
{"id": "1712.06201", "contents": "Title: Continious-time Importance Sampling: Monte Carlo Methods which Avoid\n  Time-discretisation Error Abstract: In this paper we develop a continuous-time sequential importance sampling\n(CIS) algorithm which eliminates time-discretisation errors and provides online\nunbiased estimation for continuous time Markov processes, in particular for\ndiffusions. Our work removes the strong conditions imposed by the EA and thus\nextends significantly the class of discretisation error-free MC methods for\ndiffusions. The reason that CIS can be applied more generally than EA is that\nit no longer works on the path space of the SDE. Instead it uses proposal\ndistributions for the transition density of the diffusion, and proposal\ndistributions that are absolutely continuous with respect to the true\ntransition density exist for general SDEs. \n\n"}
{"id": "1712.06947", "contents": "Title: Methodological and computational aspects of parallel tempering methods\n  in the infinite swapping limit Abstract: A variant of the parallel tempering method is proposed in terms of a\nstochastic switching process for the coupled dynamics of replica configuration\nand temperature permutation. This formulation is shown to facilitate the\nanalysis of the convergence properties of parallel tempering by large deviation\ntheory, which indicates that the method should be operated in the infinite\nswapping limit to maximize sampling efficiency. The effective equation for the\nreplica alone that arises in this infinite swapping limit simply involves\nreplacing the original potential by a mixture potential. The analysis of the\ngeometric properties of this potential offers a new perspective on the issues\nof how to choose of temperature ladder, and why many temperatures should\ntypically be introduced to boost the sampling efficiency. It is also shown how\nto simulate the effective equation in this many temperature regime using\nmultiscale integrators. Finally, similar ideas are also used to discuss\nextensions of the infinite swapping limits to the technique of simulated\ntempering. \n\n"}
{"id": "1712.08726", "contents": "Title: Denoising of 3D magnetic resonance images with multi-channel residual\n  learning of convolutional neural network Abstract: The denoising of magnetic resonance (MR) images is a task of great importance\nfor improving the acquired image quality. Many methods have been proposed in\nthe literature to retrieve noise free images with good performances. Howerever,\nthe state-of-the-art denoising methods, all needs a time-consuming optimization\nprocesses and their performance strongly depend on the estimated noise level\nparameter. Within this manuscript we propose the idea of denoising MRI Rician\nnoise using a convolutional neural network. The advantage of the proposed\nmethodology is that the learning based model can be directly used in the\ndenosing process without optimization and even without the noise level\nparameter. Specifically, a ten convolutional layers neural network combined\nwith residual learning and multi-channel strategy was proposed. Two training\nways: training on a specific noise level and training on a general level were\nconducted to demonstrate the capability of our methods. Experimental results\nover synthetic and real 3D MR data demonstrate our proposed network can achieve\nsuperior performance compared with other methods in term of both of the peak\nsignal to noise ratio and the global of structure similarity index. Without\nnoise level parameter, our general noise-applicable model is also better than\nthe other compared methods in two datasets. Furthermore, our training model\nshows good general applicability. \n\n"}
{"id": "1712.08837", "contents": "Title: Optimization and Testing in Linear Non-Gaussian Component Analysis Abstract: Independent component analysis (ICA) decomposes multivariate data into\nmutually independent components (ICs). The ICA model is subject to a constraint\nthat at most one of these components is Gaussian, which is required for model\nidentifiability. Linear non-Gaussian component analysis (LNGCA) generalizes the\nICA model to a linear latent factor model with any number of both non-Gaussian\ncomponents (signals) and Gaussian components (noise), where observations are\nlinear combinations of independent components. Although the individual Gaussian\ncomponents are not identifiable, the Gaussian subspace is identifiable. We\nintroduce an estimator along with its optimization approach in which\nnon-Gaussian and Gaussian components are estimated simultaneously, maximizing\nthe discrepancy of each non-Gaussian component from Gaussianity while\nminimizing the discrepancy of each Gaussian component from Gaussianity. When\nthe number of non-Gaussian components is unknown, we develop a statistical test\nto determine it based on resampling and the discrepancy of estimated\ncomponents. Through a variety of simulation studies, we demonstrate the\nimprovements of our estimator over competing estimators, and we illustrate the\neffectiveness of the test to determine the number of non-Gaussian components.\nFurther, we apply our method to real data examples and demonstrate its\npractical value. \n\n"}
{"id": "1712.09117", "contents": "Title: Overcomplete Frame Thresholding for Acoustic Scene Analysis Abstract: In this work, we derive a generic overcomplete frame thresholding scheme\nbased on risk minimization. Overcomplete frames being favored for analysis\ntasks such as classification, regression or anomaly detection, we provide a way\nto leverage those optimal representations in real-world applications through\nthe use of thresholding. We validate the method on a large scale bird activity\ndetection task via the scattering network architecture performed by means of\ncontinuous wavelets, known for being an adequate dictionary in audio\nenvironments. \n\n"}
{"id": "1712.09562", "contents": "Title: Spatial point processes intensity estimation with a diverging number of\n  covariates Abstract: Feature selection procedures for spatial point processes parametric intensity\nestimation have been recently developed since more and more applications\ninvolve a large number of covariates. In this paper, we investigate the setting\nwhere the number of covariates diverges as the domain of observation increases.\nIn particular, we consider estimating equations based on Campbell theorems\nderived from Poisson and logistic regression likelihoods regularized by a\ngeneral penalty function. We prove that, under some conditions, the\nconsistency, the sparsity, and the asymptotic normality are valid for such a\nsetting. We support the theoretical results by numerical ones obtained from\nsimulation experiments and an application to forestry datasets. \n\n"}
{"id": "1712.10062", "contents": "Title: Multi-timescale memory dynamics in a reinforcement learning network with\n  attention-gated memory Abstract: Learning and memory are intertwined in our brain and their relationship is at\nthe core of several recent neural network models. In particular, the\nAttention-Gated MEmory Tagging model (AuGMEnT) is a reinforcement learning\nnetwork with an emphasis on biological plausibility of memory dynamics and\nlearning. We find that the AuGMEnT network does not solve some hierarchical\ntasks, where higher-level stimuli have to be maintained over a long time, while\nlower-level stimuli need to be remembered and forgotten over a shorter\ntimescale. To overcome this limitation, we introduce hybrid AuGMEnT, with leaky\nor short-timescale and non-leaky or long-timescale units in memory, that allow\nto exchange lower-level information while maintaining higher-level one, thus\nsolving both hierarchical and distractor tasks. \n\n"}
{"id": "1712.10131", "contents": "Title: Sparse Polynomial Chaos Expansions via Compressed Sensing and D-optimal\n  Design Abstract: In the field of uncertainty quantification, sparse polynomial chaos (PC)\nexpansions are commonly used by researchers for a variety of purposes, such as\nsurrogate modeling. Ideas from compressed sensing may be employed to exploit\nthis sparsity in order to reduce computational costs. A class of greedy\ncompressed sensing algorithms use least squares minimization to approximate PC\ncoefficients. This least squares problem lends itself to the theory of optimal\ndesign of experiments (ODE). Our work focuses on selecting an experimental\ndesign that improves the accuracy of sparse PC approximations for a fixed\ncomputational budget. We propose DSP, a novel sequential design, greedy\nalgorithm for sparse PC approximation. The algorithm sequentially augments an\nexperimental design according to a set of the basis polynomials deemed\nimportant by the magnitude of their coefficients, at each iteration. Our\nalgorithm incorporates topics from ODE to estimate the PC coefficients. A\nvariety of numerical simulations are performed on three physical models and\nmanufactured sparse PC expansions to provide a comparative study between our\nproposed algorithm and other non-adaptive methods. Further, we examine the\nimportance of sampling by comparing different strategies in terms of their\nability to generate a candidate pool from which an optimal experimental design\nis chosen. It is demonstrated that the most accurate PC coefficient\napproximations, with the least variability, are produced with our\ndesign-adaptive greedy algorithm and the use of a studied importance sampling\nstrategy. We provide theoretical and numerical results which show that using an\noptimal sampling strategy for the candidate pool is key, both in terms of\naccuracy in the approximation, but also in terms of constructing an optimal\ndesign. \n\n"}
{"id": "1801.00723", "contents": "Title: Deep Learning for Identifying Potential Conceptual Shifts for\n  Co-creative Drawing Abstract: We present a system for identifying conceptual shifts between visual\ncategories, which will form the basis for a co-creative drawing system to help\nusers draw more creative sketches. The system recognizes human sketches and\nmatches them to structurally similar sketches from categories to which they do\nnot belong. This would allow a co-creative drawing system to produce an\nambiguous sketch that blends features from both categories. \n\n"}
{"id": "1801.01990", "contents": "Title: Procrustes Metrics on Covariance Operators and Optimal Transportation of\n  Gaussian Processes Abstract: Covariance operators are fundamental in functional data analysis, providing\nthe canonical means to analyse functional variation via the celebrated\nKarhunen--Lo\\`eve expansion. These operators may themselves be subject to\nvariation, for instance in contexts where multiple functional populations are\nto be compared. Statistical techniques to analyse such variation are intimately\nlinked with the choice of metric on covariance operators, and the intrinsic\ninfinite-dimensionality of these operators. In this paper, we describe the\nmanifold geometry of the space of trace-class infinite-dimensional covariance\noperators and associated key statistical properties, under the recently\nproposed infinite-dimensional version of the Procrustes metric. We identify\nthis space with that of centred Gaussian processes equipped with the\nWasserstein metric of optimal transportation. The identification allows us to\nprovide a complete description of those aspects of this manifold geometry that\nare important in terms of statistical inference, and establish key properties\nof the Fr\\'echet mean of a random sample of covariances, as well as generative\nmodels that are canonical for such metrics and link with the problem of\nregistration of functional data. \n\n"}
{"id": "1801.03348", "contents": "Title: Side Disks of a Spherical Great Polygon Abstract: Take a circle and mark $n\\in\\mathbb{N}$ points on it designated as vertices.\nFor any arc segment between two consecutive vertices which does not pass\nthrough any other vertex, there is a disk centered at its midpoint and has its\nend points on the boundary. We analyze intersection behaviour of these disks\nand show that the number of disjoint pairs among them is between\n$\\frac{(n-2)(n-3)}{2}$ and $\\frac{n(n-3)}{2}$ and their intersection graph is a\nsubgraph of a triangulation of a convex $n$-gon. \n\n"}
{"id": "1801.03612", "contents": "Title: Using probabilistic programs as proposals Abstract: Monte Carlo inference has asymptotic guarantees, but can be slow when using\ngeneric proposals. Handcrafted proposals that rely on user knowledge about the\nposterior distribution can be efficient, but are difficult to derive and\nimplement. This paper proposes to let users express their posterior knowledge\nin the form of proposal programs, which are samplers written in probabilistic\nprogramming languages. One strategy for writing good proposal programs is to\ncombine domain-specific heuristic algorithms with neural network models. The\nheuristics identify high probability regions, and the neural networks model the\nposterior uncertainty around the outputs of the algorithm. Proposal programs\ncan be used as proposal distributions in importance sampling and\nMetropolis-Hastings samplers without sacrificing asymptotic consistency, and\ncan be optimized offline using inference compilation. Support for optimizing\nand using proposal programs is easily implemented in a sampling-based\nprobabilistic programming runtime. The paper illustrates the proposed technique\nwith a proposal program that combines RANSAC and neural networks to accelerate\ninference in a Bayesian linear regression with outliers model. \n\n"}
{"id": "1801.03765", "contents": "Title: Non-stationary Douglas-Rachford and alternating direction method of\n  multipliers: adaptive stepsizes and convergence Abstract: We revisit the classical Douglas-Rachford (DR) method for finding a zero of\nthe sum of two maximal monotone operators. Since the practical performance of\nthe DR method crucially depends on the stepsizes, we aim at developing an\nadaptive stepsize rule. To that end, we take a closer look at a linear case of\nthe problem and use our findings to develop a stepsize strategy that eliminates\nthe need for stepsize tuning. We analyze a general non-stationary DR scheme and\nprove its convergence for a convergent sequence of stepsizes with summable\nincrements. This, in turn, proves the convergence of the method with the new\nadaptive stepsize rule. We also derive the related non-stationary alternating\ndirection method of multipliers (ADMM) from such a non-stationary DR method. We\nillustrate the efficiency of the proposed methods on several numerical\nexamples. \n\n"}
{"id": "1801.06768", "contents": "Title: Embedded Model Error Representation for Bayesian Model Calibration Abstract: Model error estimation remains one of the key challenges in uncertainty\nquantification and predictive science. For computational models of complex\nphysical systems, model error, also known as structural error or model\ninadequacy, is often the largest contributor to the overall predictive\nuncertainty. This work builds on a recently developed framework of embedded,\ninternal model correction, in order to represent and quantify structural\nerrors, together with model parameters, within a Bayesian inference context. We\nfocus specifically on a Polynomial Chaos representation with additive\nmodification of existing model parameters, enabling a non-intrusive procedure\nfor efficient approximate likelihood construction, model error estimation, and\ndisambiguation of model and data errors' contributions to predictive\nuncertainty. The framework is demonstrated on several synthetic examples, as\nwell as on a chemical ignition problem. \n\n"}
{"id": "1801.06889", "contents": "Title: Visual Analytics in Deep Learning: An Interrogative Survey for the Next\n  Frontiers Abstract: Deep learning has recently seen rapid development and received significant\nattention due to its state-of-the-art performance on previously-thought hard\nproblems. However, because of the internal complexity and nonlinear structure\nof deep neural networks, the underlying decision making processes for why these\nmodels are achieving such performance are challenging and sometimes mystifying\nto interpret. As deep learning spreads across domains, it is of paramount\nimportance that we equip users of deep learning with tools for understanding\nwhen a model works correctly, when it fails, and ultimately how to improve its\nperformance. Standardized toolkits for building neural networks have helped\ndemocratize deep learning; visual analytics systems have now been developed to\nsupport model explanation, interpretation, debugging, and improvement. We\npresent a survey of the role of visual analytics in deep learning research,\nwhich highlights its short yet impactful history and thoroughly summarizes the\nstate-of-the-art using a human-centered interrogative framework, focusing on\nthe Five W's and How (Why, Who, What, How, When, and Where). We conclude by\nhighlighting research directions and open research problems. This survey helps\nresearchers and practitioners in both visual analytics and deep learning to\nquickly learn key aspects of this young and rapidly growing body of research,\nwhose impact spans a diverse range of domains. \n\n"}
{"id": "1801.09797", "contents": "Title: Discrete Autoencoders for Sequence Models Abstract: Recurrent models for sequences have been recently successful at many tasks,\nespecially for language modeling and machine translation. Nevertheless, it\nremains challenging to extract good representations from these models. For\ninstance, even though language has a clear hierarchical structure going from\ncharacters through words to sentences, it is not apparent in current language\nmodels. We propose to improve the representation in sequence models by\naugmenting current approaches with an autoencoder that is forced to compress\nthe sequence through an intermediate discrete latent space. In order to\npropagate gradients though this discrete representation we introduce an\nimproved semantic hashing technique. We show that this technique performs well\non a newly proposed quantitative efficiency measure. We also analyze latent\ncodes produced by the model showing how they correspond to words and phrases.\nFinally, we present an application of the autoencoder-augmented model to\ngenerating diverse translations. \n\n"}
{"id": "1801.09874", "contents": "Title: Change point analysis in non-stationary processes - a mass excess\n  approach Abstract: This paper considers the problem of testing if a sequence of means\n$(\\mu_t)_{t =1,\\ldots ,n }$ of a non-stationary time series $(X_t)_{t =1,\\ldots\n,n }$ is stable in the sense that the difference of the means $\\mu_1$ and\n$\\mu_t$ between the initial time $t=1$ and any other time is smaller than a\ngiven level, that is $ | \\mu_1 - \\mu_t | \\leq c $ for all $t =1,\\ldots ,n $. A\ntest for hypotheses of this type is developed using a biascorrected monotone\nrearranged local linear estimator and asymptotic normality of the corresponding\ntest statistic is established. As the asymptotic variance depends on the\nlocation and order of the critical roots of the equation $| \\mu_1 - \\mu_t | =\nc$ a new bootstrap procedure is proposed to obtain critical values and its\nconsistency is established. As a consequence we are able to quantitatively\ndescribe relevant deviations of a non-stationary sequence from its initial\nvalue. The results are illustrated by means of a simulation study and by\nanalyzing data examples. \n\n"}
{"id": "1802.03101", "contents": "Title: Convolutional Hashing for Automated Scene Matching Abstract: We present a powerful new loss function and training scheme for learning\nbinary hash functions. In particular, we demonstrate our method by creating for\nthe first time a neural network that outperforms state-of-the-art Haar wavelets\nand color layout descriptors at the task of automated scene matching. By\naccurately relating distance on the manifold of network outputs to distance in\nHamming space, we achieve a 100-fold reduction in nontrivial false positive\nrate and significantly higher true positive rate. We expect our insights to\nprovide large wins for hashing models applied to other information retrieval\nhashing tasks as well. \n\n"}
{"id": "1802.03426", "contents": "Title: UMAP: Uniform Manifold Approximation and Projection for Dimension\n  Reduction Abstract: UMAP (Uniform Manifold Approximation and Projection) is a novel manifold\nlearning technique for dimension reduction. UMAP is constructed from a\ntheoretical framework based in Riemannian geometry and algebraic topology. The\nresult is a practical scalable algorithm that applies to real world data. The\nUMAP algorithm is competitive with t-SNE for visualization quality, and\narguably preserves more of the global structure with superior run time\nperformance. Furthermore, UMAP has no computational restrictions on embedding\ndimension, making it viable as a general purpose dimension reduction technique\nfor machine learning. \n\n"}
{"id": "1802.03497", "contents": "Title: Modeling Global Dynamics from Local Snapshots with Deep Generative\n  Neural Networks Abstract: Complex high dimensional stochastic dynamic systems arise in many\napplications in the natural sciences and especially biology. However, while\nthese systems are difficult to describe analytically, \"snapshot\" measurements\nthat sample the output of the system are often available. In order to model the\ndynamics of such systems given snapshot data, or local transitions, we present\na deep neural network framework we call Dynamics Modeling Network or DyMoN.\nDyMoN is a neural network framework trained as a deep generative Markov model\nwhose next state is a probability distribution based on the current state.\nDyMoN is trained using samples of current and next-state pairs, and thus does\nnot require longitudinal measurements. We show the advantage of DyMoN over\nshallow models such as Kalman filters and hidden Markov models, and other deep\nmodels such as recurrent neural networks in its ability to embody the dynamics\n(which can be studied via perturbation of the neural network) and generate\nlongitudinal hypothetical trajectories. We perform three case studies in which\nwe apply DyMoN to different types of biological systems and extract features of\nthe dynamics in each case by examining the learned model. \n\n"}
{"id": "1802.04240", "contents": "Title: Reinforcement Learning for Solving the Vehicle Routing Problem Abstract: We present an end-to-end framework for solving the Vehicle Routing Problem\n(VRP) using reinforcement learning. In this approach, we train a single model\nthat finds near-optimal solutions for problem instances sampled from a given\ndistribution, only by observing the reward signals and following feasibility\nrules. Our model represents a parameterized stochastic policy, and by applying\na policy gradient algorithm to optimize its parameters, the trained model\nproduces the solution as a sequence of consecutive actions in real time,\nwithout the need to re-train for every new problem instance. On capacitated\nVRP, our approach outperforms classical heuristics and Google's OR-Tools on\nmedium-sized instances in solution quality with comparable computation time\n(after training). We demonstrate how our approach can handle problems with\nsplit delivery and explore the effect of such deliveries on the solution\nquality. Our proposed framework can be applied to other variants of the VRP\nsuch as the stochastic VRP, and has the potential to be applied more generally\nto combinatorial optimization problems. \n\n"}
{"id": "1802.04791", "contents": "Title: Stochastic Variance-Reduced Hamilton Monte Carlo Methods Abstract: We propose a fast stochastic Hamilton Monte Carlo (HMC) method, for sampling\nfrom a smooth and strongly log-concave distribution. At the core of our\nproposed method is a variance reduction technique inspired by the recent\nadvance in stochastic optimization. We show that, to achieve $\\epsilon$\naccuracy in 2-Wasserstein distance, our algorithm achieves $\\tilde\nO(n+\\kappa^{2}d^{1/2}/\\epsilon+\\kappa^{4/3}d^{1/3}n^{2/3}/\\epsilon^{2/3})$\ngradient complexity (i.e., number of component gradient evaluations), which\noutperforms the state-of-the-art HMC and stochastic gradient HMC methods in a\nwide regime. We also extend our algorithm for sampling from smooth and general\nlog-concave distributions, and prove the corresponding gradient complexity as\nwell. Experiments on both synthetic and real data demonstrate the superior\nperformance of our algorithm. \n\n"}
{"id": "1802.04889", "contents": "Title: Understanding Membership Inferences on Well-Generalized Learning Models Abstract: Membership Inference Attack (MIA) determines the presence of a record in a\nmachine learning model's training data by querying the model. Prior work has\nshown that the attack is feasible when the model is overfitted to its training\ndata or when the adversary controls the training algorithm. However, when the\nmodel is not overfitted and the adversary does not control the training\nalgorithm, the threat is not well understood. In this paper, we report a study\nthat discovers overfitting to be a sufficient but not a necessary condition for\nan MIA to succeed. More specifically, we demonstrate that even a\nwell-generalized model contains vulnerable instances subject to a new\ngeneralized MIA (GMIA). In GMIA, we use novel techniques for selecting\nvulnerable instances and detecting their subtle influences ignored by\noverfitting metrics. Specifically, we successfully identify individual records\nwith high precision in real-world datasets by querying black-box machine\nlearning models. Further we show that a vulnerable record can even be\nindirectly attacked by querying other related records and existing\ngeneralization techniques are found to be less effective in protecting the\nvulnerable instances. Our findings sharpen the understanding of the fundamental\ncause of the problem: the unique influences the training instance may have on\nthe model. \n\n"}
{"id": "1802.04908", "contents": "Title: Conditional Density Estimation with Bayesian Normalising Flows Abstract: Modeling complex conditional distributions is critical in a variety of\nsettings. Despite a long tradition of research into conditional density\nestimation, current methods employ either simple parametric forms or are\ndifficult to learn in practice. This paper employs normalising flows as a\nflexible likelihood model and presents an efficient method for fitting them to\ncomplex densities. These estimators must trade-off between modeling\ndistributional complexity, functional complexity and heteroscedasticity without\noverfitting. We recognize these trade-offs as modeling decisions and develop a\nBayesian framework for placing priors over these conditional density estimators\nusing variational Bayesian neural networks. We evaluate this method on several\nsmall benchmark regression datasets, on some of which it obtains state of the\nart performance. Finally, we apply the method to two spatial density modeling\ntasks with over 1 million datapoints using the New York City yellow taxi\ndataset and the Chicago crime dataset. \n\n"}
{"id": "1802.05429", "contents": "Title: Blind Source Separation with Optimal Transport Non-negative Matrix\n  Factorization Abstract: Optimal transport as a loss for machine learning optimization problems has\nrecently gained a lot of attention. Building upon recent advances in\ncomputational optimal transport, we develop an optimal transport non-negative\nmatrix factorization (NMF) algorithm for supervised speech blind source\nseparation (BSS). Optimal transport allows us to design and leverage a cost\nbetween short-time Fourier transform (STFT) spectrogram frequencies, which\ntakes into account how humans perceive sound. We give empirical evidence that\nusing our proposed optimal transport NMF leads to perceptually better results\nthan Euclidean NMF, for both isolated voice reconstruction and BSS tasks.\nFinally, we demonstrate how to use optimal transport for cross domain sound\nprocessing tasks, where frequencies represented in the input spectrograms may\nbe different from one spectrogram to another. \n\n"}
{"id": "1802.05570", "contents": "Title: Optimal Transport: Fast Probabilistic Approximation with Exact Solvers Abstract: We propose a simple subsampling scheme for fast randomized approximate\ncomputation of optimal transport distances. This scheme operates on a random\nsubset of the full data and can use any exact algorithm as a black-box\nback-end, including state-of-the-art solvers and entropically penalized\nversions. It is based on averaging the exact distances between empirical\nmeasures generated from independent samples from the original measures and can\neasily be tuned towards higher accuracy or shorter computation times. To this\nend, we give non-asymptotic deviation bounds for its accuracy in the case of\ndiscrete optimal transport problems. In particular, we show that in many\nimportant instances, including images (2D-histograms), the approximation error\nis independent of the size of the full problem. We present numerical\nexperiments that demonstrate that a very good approximation in typical\napplications can be obtained in a computation time that is several orders of\nmagnitude smaller than what is required for exact computation of the full\nproblem. \n\n"}
{"id": "1802.05584", "contents": "Title: Convolutional Analysis Operator Learning: Acceleration and Convergence Abstract: Convolutional operator learning is gaining attention in many signal\nprocessing and computer vision applications. Learning kernels has mostly relied\non so-called patch-domain approaches that extract and store many overlapping\npatches across training signals. Due to memory demands, patch-domain methods\nhave limitations when learning kernels from large datasets -- particularly with\nmulti-layered structures, e.g., convolutional neural networks -- or when\napplying the learned kernels to high-dimensional signal recovery problems. The\nso-called convolution approach does not store many overlapping patches, and\nthus overcomes the memory problems particularly with careful algorithmic\ndesigns; it has been studied within the \"synthesis\" signal model, e.g.,\nconvolutional dictionary learning. This paper proposes a new convolutional\nanalysis operator learning (CAOL) framework that learns an analysis sparsifying\nregularizer with the convolution perspective, and develops a new convergent\nBlock Proximal Extrapolated Gradient method using a Majorizer (BPEG-M) to solve\nthe corresponding block multi-nonconvex problems. To learn diverse filters\nwithin the CAOL framework, this paper introduces an orthogonality constraint\nthat enforces a tight-frame filter condition, and a regularizer that promotes\ndiversity between filters. Numerical experiments show that, with sharp\nmajorizers, BPEG-M significantly accelerates the CAOL convergence rate compared\nto the state-of-the-art block proximal gradient (BPG) method. Numerical\nexperiments for sparse-view computational tomography show that a convolutional\nsparsifying regularizer learned via CAOL significantly improves reconstruction\nquality compared to a conventional edge-preserving regularizer. Using more and\nwider kernels in a learned regularizer better preserves edges in reconstructed\nimages. \n\n"}
{"id": "1802.06167", "contents": "Title: CapsuleGAN: Generative Adversarial Capsule Network Abstract: We present Generative Adversarial Capsule Network (CapsuleGAN), a framework\nthat uses capsule networks (CapsNets) instead of the standard convolutional\nneural networks (CNNs) as discriminators within the generative adversarial\nnetwork (GAN) setting, while modeling image data. We provide guidelines for\ndesigning CapsNet discriminators and the updated GAN objective function, which\nincorporates the CapsNet margin loss, for training CapsuleGAN models. We show\nthat CapsuleGAN outperforms convolutional-GAN at modeling image data\ndistribution on MNIST and CIFAR-10 datasets, evaluated on the generative\nadversarial metric and at semi-supervised image classification. \n\n"}
{"id": "1802.06501", "contents": "Title: Recommendations with Negative Feedback via Pairwise Deep Reinforcement\n  Learning Abstract: Recommender systems play a crucial role in mitigating the problem of\ninformation overload by suggesting users' personalized items or services. The\nvast majority of traditional recommender systems consider the recommendation\nprocedure as a static process and make recommendations following a fixed\nstrategy. In this paper, we propose a novel recommender system with the\ncapability of continuously improving its strategies during the interactions\nwith users. We model the sequential interactions between users and a\nrecommender system as a Markov Decision Process (MDP) and leverage\nReinforcement Learning (RL) to automatically learn the optimal strategies via\nrecommending trial-and-error items and receiving reinforcements of these items\nfrom users' feedback. Users' feedback can be positive and negative and both\ntypes of feedback have great potentials to boost recommendations. However, the\nnumber of negative feedback is much larger than that of positive one; thus\nincorporating them simultaneously is challenging since positive feedback could\nbe buried by negative one. In this paper, we develop a novel approach to\nincorporate them into the proposed deep recommender system (DEERS) framework.\nThe experimental results based on real-world e-commerce data demonstrate the\neffectiveness of the proposed framework. Further experiments have been\nconducted to understand the importance of both positive and negative feedback\nin recommendations. \n\n"}
{"id": "1802.06677", "contents": "Title: Degeneration in VAE: in the Light of Fisher Information Loss Abstract: While enormous progress has been made to Variational Autoencoder (VAE) in\nrecent years, similar to other deep networks, VAE with deep networks suffers\nfrom the problem of degeneration, which seriously weakens the correlation\nbetween the input and the corresponding latent codes, deviating from the goal\nof the representation learning. To investigate how degeneration affects VAE\nfrom a theoretical perspective, we illustrate the information transmission in\nVAE and analyze the intermediate layers of the encoders/decoders. Specifically,\nwe propose a Fisher Information measure for the layer-wise analysis. With such\nmeasure, we demonstrate that information loss is ineluctable in feed-forward\nnetworks and causes the degeneration in VAE. We show that skip connections in\nVAE enable the preservation of information without changing the model\narchitecture. We call this class of VAE equipped with skip connections as SCVAE\nand perform a range of experiments to show its advantages in information\npreservation and degeneration mitigation. \n\n"}
{"id": "1802.06966", "contents": "Title: Computing the Cumulative Distribution Function and Quantiles of the\n  One-sided Kolmogorov-Smirnov Statistic Abstract: The cumulative distribution and quantile functions for the one-sided one\nsample Kolmogorov-Smirnov probability distributions are used for\ngoodness-of-fit testing. While the Smirnov-Birnbaum-Tingey formula for the CDF\nappears straight forward, its numerical evaluation generates intermediate\nresults spanning many hundreds of orders of magnitude and at times requires\nvery precise accurate representations. Computing the quantile function for any\nspecific probability may require evaluating both the CDF and its derivative,\nboth of which are computationally expensive. To work around avoid these issues,\ndifferent algorithms can be used across different parts of the domain, and\napproximations can be used to reduce the computational requirements. We show\nhere that straight forward implementation incurs accuracy loss for sample sizes\nof well under 1000. Further the approximations in use inside the open source\nSciPy python software often result in increased computation, not just reduced\naccuracy, and at times suffer catastrophic loss of accuracy for any sample\nsize. Then we provide alternate algorithms which restore accuracy and\nefficiency across the whole domain. \n\n"}
{"id": "1802.07148", "contents": "Title: Correlated pseudo-marginal schemes for time-discretised stochastic\n  kinetic models Abstract: The challenging problem of conducting fully Bayesian inference for the\nreaction rate constants governing stochastic kinetic models (SKMs) is\nconsidered. Given the challenges underlying this problem, the Markov jump\nprocess representation is routinely replaced by an approximation based on a\nsuitable time discretisation of the system of interest. Improving the accuracy\nof these schemes amounts to using an ever finer discretisation level, which in\nthe context of the inference problem, requires integrating over the uncertainty\nin the process at a predetermined number of intermediate times between\nobservations. Pseudo-marginal Metropolis-Hastings schemes are increasingly\nused, since for a given discretisation level, the observed data likelihood can\nbe unbiasedly estimated using a particle filter. When observations are\nparticularly informative an auxiliary particle filter can be implemented, by\nemploying an appropriate construct to push the state particles towards the\nobservations in a sensible way. Recent work in state-space settings has shown\nhow the pseudo-marginal approach can be made much more efficient by correlating\nthe underlying pseudo-random numbers used to form the likelihood estimate at\nthe current and proposed values of the unknown parameters. We extend this\napproach to the time-discretised SKM framework by correlating the innovations\nthat drive the auxiliary particle filter. We find that the resulting approach\noffers substantial gains in efficiency over a standard implementation. \n\n"}
{"id": "1802.08334", "contents": "Title: Learning Without Mixing: Towards A Sharp Analysis of Linear System\n  Identification Abstract: We prove that the ordinary least-squares (OLS) estimator attains nearly\nminimax optimal performance for the identification of linear dynamical systems\nfrom a single observed trajectory. Our upper bound relies on a generalization\nof Mendelson's small-ball method to dependent data, eschewing the use of\nstandard mixing-time arguments. Our lower bounds reveal that these upper bounds\nmatch up to logarithmic factors. In particular, we capture the correct\nsignal-to-noise behavior of the problem, showing that more unstable linear\nsystems are easier to estimate. This behavior is qualitatively different from\narguments which rely on mixing-time calculations that suggest that unstable\nsystems are more difficult to estimate. We generalize our technique to provide\nbounds for a more general class of linear response time-series. \n\n"}
{"id": "1802.08622", "contents": "Title: Variable selection via Group LASSO Approach : Application to the Cox\n  Regression and frailty model Abstract: In the analysis of survival outcome supplemented with both clinical\ninformation and high-dimensional gene expression data, use of the traditional\nCox proportional hazards model (1972) fails to meet some emerging needs in\nbiomedical research. First, the number of covariates is generally much larger\nthe sample size. Secondly, predicting an outcome based on individual gene\nexpression is inadequate because multiple biological processes and functional\npathways regulate the expression associated with a gene. Another challenge is\nthat the Cox model assumes that populations are homogenous, implying that all\nindividuals have the same risk of death, which is rarely true due to unmeasured\nrisk factors among populations. In this paper we propose group LASSO with\ngamma-distributed frailty for variable selection in Cox regression by extending\nprevious scholarship to account for heterogeneity among group structures\nrelated to exposure and susceptibility. The consistency property of the\nproposed method is established. This method is appropriate for addressing a\nwide variety of research questions from genetics to air pollution. Simulated\nanalysis shows promising performance by group LASSO compared with other\nmethods, including group SCAD and group MCP. Future directions include\nexpanding the use of frailty with adaptive group LASSO and sparse group LASS. \n\n"}
{"id": "1802.08667", "contents": "Title: De-Biased Machine Learning of Global and Local Parameters Using\n  Regularized Riesz Representers Abstract: We provide adaptive inference methods, based on $\\ell_1$ regularization, for\nregular (semi-parametric) and non-regular (nonparametric) linear functionals of\nthe conditional expectation function. Examples of regular functionals include\naverage treatment effects, policy effects, and derivatives. Examples of\nnon-regular functionals include average treatment effects, policy effects, and\nderivatives conditional on a covariate subvector fixed at a point. We construct\na Neyman orthogonal equation for the target parameter that is approximately\ninvariant to small perturbations of the nuisance parameters. To achieve this\nproperty, we include the Riesz representer for the functional as an additional\nnuisance parameter. Our analysis yields weak ``double sparsity robustness'':\neither the approximation to the regression or the approximation to the\nrepresenter can be ``completely dense'' as long as the other is sufficiently\n``sparse''. Our main results are non-asymptotic and imply asymptotic uniform\nvalidity over large classes of models, translating into honest confidence bands\nfor both global and local parameters. \n\n"}
{"id": "1802.09069", "contents": "Title: Active Learning with Logged Data Abstract: We consider active learning with logged data, where labeled examples are\ndrawn conditioned on a predetermined logging policy, and the goal is to learn a\nclassifier on the entire population, not just conditioned on the logging\npolicy. Prior work addresses this problem either when only logged data is\navailable, or purely in a controlled random experimentation setting where the\nlogged data is ignored. In this work, we combine both approaches to provide an\nalgorithm that uses logged data to bootstrap and inform experimentation, thus\nachieving the best of both worlds. Our work is inspired by a connection between\ncontrolled random experimentation and active learning, and modifies existing\ndisagreement-based active learning algorithms to exploit logged data. \n\n"}
{"id": "1802.09129", "contents": "Title: Multi-Evidence Filtering and Fusion for Multi-Label Classification,\n  Object Detection and Semantic Segmentation Based on Weakly Supervised\n  Learning Abstract: Supervised object detection and semantic segmentation require object or even\npixel level annotations. When there exist image level labels only, it is\nchallenging for weakly supervised algorithms to achieve accurate predictions.\nThe accuracy achieved by top weakly supervised algorithms is still\nsignificantly lower than their fully supervised counterparts. In this paper, we\npropose a novel weakly supervised curriculum learning pipeline for multi-label\nobject recognition, detection and semantic segmentation. In this pipeline, we\nfirst obtain intermediate object localization and pixel labeling results for\nthe training images, and then use such results to train task-specific deep\nnetworks in a fully supervised manner. The entire process consists of four\nstages, including object localization in the training images, filtering and\nfusing object instances, pixel labeling for the training images, and\ntask-specific network training. To obtain clean object instances in the\ntraining images, we propose a novel algorithm for filtering, fusing and\nclassifying object instances collected from multiple solution mechanisms. In\nthis algorithm, we incorporate both metric learning and density-based\nclustering to filter detected object instances. Experiments show that our\nweakly supervised pipeline achieves state-of-the-art results in multi-label\nimage classification as well as weakly supervised object detection and very\ncompetitive results in weakly supervised semantic segmentation on MS-COCO,\nPASCAL VOC 2007 and PASCAL VOC 2012. \n\n"}
{"id": "1802.09405", "contents": "Title: Improving Graph Convolutional Networks with Non-Parametric Activation\n  Functions Abstract: Graph neural networks (GNNs) are a class of neural networks that allow to\nefficiently perform inference on data that is associated to a graph structure,\nsuch as, e.g., citation networks or knowledge graphs. While several variants of\nGNNs have been proposed, they only consider simple nonlinear activation\nfunctions in their layers, such as rectifiers or squashing functions. In this\npaper, we investigate the use of graph convolutional networks (GCNs) when\ncombined with more complex activation functions, able to adapt from the\ntraining data. More specifically, we extend the recently proposed kernel\nactivation function, a non-parametric model which can be implemented easily,\ncan be regularized with standard $\\ell_p$-norms techniques, and is smooth over\nits entire domain. Our experimental evaluation shows that the proposed\narchitecture can significantly improve over its baseline, while similar\nimprovements cannot be obtained by simply increasing the depth or size of the\noriginal GCN. \n\n"}
{"id": "1802.09691", "contents": "Title: Link Prediction Based on Graph Neural Networks Abstract: Link prediction is a key problem for network-structured data. Link prediction\nheuristics use some score functions, such as common neighbors and Katz index,\nto measure the likelihood of links. They have obtained wide practical uses due\nto their simplicity, interpretability, and for some of them, scalability.\nHowever, every heuristic has a strong assumption on when two nodes are likely\nto link, which limits their effectiveness on networks where these assumptions\nfail. In this regard, a more reasonable way should be learning a suitable\nheuristic from a given network instead of using predefined ones. By extracting\na local subgraph around each target link, we aim to learn a function mapping\nthe subgraph patterns to link existence, thus automatically learning a\n`heuristic' that suits the current network. In this paper, we study this\nheuristic learning paradigm for link prediction. First, we develop a novel\n$\\gamma$-decaying heuristic theory. The theory unifies a wide range of\nheuristics in a single framework, and proves that all these heuristics can be\nwell approximated from local subgraphs. Our results show that local subgraphs\nreserve rich information related to link existence. Second, based on the\n$\\gamma$-decaying theory, we propose a new algorithm to learn heuristics from\nlocal subgraphs using a graph neural network (GNN). Its experimental results\nshow unprecedented performance, working consistently well on a wide range of\nproblems. \n\n"}
{"id": "1802.09816", "contents": "Title: Coarse to fine non-rigid registration: a chain of scale-specific neural\n  networks for multimodal image alignment with application to remote sensing Abstract: We tackle here the problem of multimodal image non-rigid registration, which\nis of prime importance in remote sensing and medical imaging. The difficulties\nencountered by classical registration approaches include feature design and\nslow optimization by gradient descent. By analyzing these methods, we note the\nsignificance of the notion of scale. We design easy-to-train,\nfully-convolutional neural networks able to learn scale-specific features. Once\nchained appropriately, they perform global registration in linear time, getting\nrid of gradient descent schemes by predicting directly the deformation.We show\ntheir performance in terms of quality and speed through various tasks of remote\nsensing multimodal image alignment. In particular, we are able to register\ncorrectly cadastral maps of buildings as well as road polylines onto RGB\nimages, and outperform current keypoint matching methods. \n\n"}
{"id": "1802.10529", "contents": "Title: Maximum likelihood estimation of a finite mixture of logistic regression\n  models in a continuous data stream Abstract: In marketing we are often confronted with a continuous stream of responses to\nmarketing messages. Such streaming data provide invaluable information\nregarding message effectiveness and segmentation. However, streaming data are\nhard to analyze using conventional methods: their high volume and the fact that\nthey are continuously augmented means that it takes considerable time to\nanalyze them. We propose a method for estimating a finite mixture of logistic\nregression models which can be used to cluster customers based on a continuous\nstream of responses. This method, which we coin oFMLR, allows segments to be\nidentified in data streams or extremely large static datasets. Contrary to\nblack box algorithms, oFMLR provides model estimates that are directly\ninterpretable. We first introduce oFMLR, explaining in passing general topics\nsuch as online estimation and the EM algorithm, making this paper a high level\noverview of possible methods of dealing with large data streams in marketing\npractice. Next, we discuss model convergence, identifiability, and relations to\nalternative, Bayesian, methods; we also identify more general issues that arise\nfrom dealing with continuously augmented data sets. Finally, we introduce the\noFMLR [R] package and evaluate the method by numerical simulation and by\nanalyzing a large customer clickstream dataset. \n\n"}
{"id": "1803.00472", "contents": "Title: Multimode: An R Package for Mode Assessment Abstract: In several applied fields, multimodality assessment is a crucial task as a\nprevious exploratory tool or for determining the suitability of certain\ndistributions. The goal of this paper is to present the utilities of the R\npackage multimode, which collects different exploratory and testing\nnonparametric approaches for determining the number of modes and their\nestimated location. Specifically, some graphical tools, allowing for the\nidentification of mode patterns, based on the kernel density estimation are\nprovided (SiZer map, mode tree or mode forest). Several formal testing\nprocedures for determining the number of modes are described in this paper and\nimplemented in the multimode package, including methods based on the ideas of\nthe critical bandwidth, the excess mass or using a combination of both. This\npackage also includes a function for estimating the modes locations and\ndifferent classical data examples that have been considered in mode testing\nliterature. \n\n"}
{"id": "1803.01150", "contents": "Title: Confidence intervals for high-dimensional Cox models Abstract: The purpose of this paper is to construct confidence intervals for the\nregression coefficients in high-dimensional Cox proportional hazards regression\nmodels where the number of covariates may be larger than the sample size. Our\ndebiased estimator construction is similar to those in Zhang and Zhang (2014)\nand van de Geer et al. (2014), but the time-dependent covariates and censored\nrisk sets introduce considerable additional challenges. Our theoretical\nresults, which provide conditions under which our confidence intervals are\nasymptotically valid, are supported by extensive numerical experiments. \n\n"}
{"id": "1803.01231", "contents": "Title: Bayesian Projected Calibration of Computer Models Abstract: We develop a Bayesian approach called Bayesian projected calibration to\naddress the problem of calibrating an imperfect computer model using\nobservational data from a complex physical system. The calibration parameter\nand the physical system are parametrized in an identifiable fashion via\n$L_2$-projection. The physical process is assigned a Gaussian process prior,\nwhich naturally induces a prior distribution on the calibration parameter\nthrough the $L_2$-projection constraint. The calibration parameter is estimated\nthrough its posterior distribution, which provides a natural and non-asymptotic\nway for the uncertainty quantification. We provide a rigorous large sample\njustification for the proposed approach by establishing the asymptotic\nnormality of the posterior of the calibration parameter with the efficient\ncovariance matrix. In addition, two efficient computational algorithms based on\nstochastic approximation are designed with theoretical guarantees. Through\nextensive simulation studies and two real-world datasets analyses, we show that\nthe Bayesian projected calibration can accurately estimate the calibration\nparameters, appropriately calibrate the computer models, and compare favorably\nto alternative approaches. \n\n"}
{"id": "1803.01257", "contents": "Title: Nonnegative Matrix Factorization for Signal and Data Analytics:\n  Identifiability, Algorithms, and Applications Abstract: Nonnegative matrix factorization (NMF) has become a workhorse for signal and\ndata analytics, triggered by its model parsimony and interpretability. Perhaps\na bit surprisingly, the understanding to its model identifiability---the major\nreason behind the interpretability in many applications such as topic mining\nand hyperspectral imaging---had been rather limited until recent years.\nBeginning from the 2010s, the identifiability research of NMF has progressed\nconsiderably: Many interesting and important results have been discovered by\nthe signal processing (SP) and machine learning (ML) communities. NMF\nidentifiability has a great impact on many aspects in practice, such as\nill-posed formulation avoidance and performance-guaranteed algorithm design. On\nthe other hand, there is no tutorial paper that introduces NMF from an\nidentifiability viewpoint. In this paper, we aim at filling this gap by\noffering a comprehensive and deep tutorial on model identifiability of NMF as\nwell as the connections to algorithms and applications. This tutorial will help\nresearchers and graduate students grasp the essence and insights of NMF,\nthereby avoiding typical `pitfalls' that are often times due to unidentifiable\nNMF formulations. This paper will also help practitioners pick/design suitable\nfactorization tools for their own problems. \n\n"}
{"id": "1803.01999", "contents": "Title: ABC and Indirect Inference Abstract: This chapter will appear in the forthcoming Handbook of Approximate Bayesian\nComputation (2018).\n  Indirect inference (II) is a classical likelihood-free approach that\npre-dates the main developments of ABC and relies on simulation from a\nparametric model of interest to determine point estimates of the parameters. It\nis not surprising then that some likelihood-free Bayesian approaches have\nharnessed the II literature. This chapter provides an introduction to II and\ndetails the connections between ABC and II. A particular focus is placed on the\nuse of an auxiliary model with a tractable likelihood function, an approach\ncommonly adopted in the II literature, to facilitate likelihood-free Bayesian\ninferences. \n\n"}
{"id": "1803.02043", "contents": "Title: Online Deep Learning: Growing RBM on the fly Abstract: We propose a novel online learning algorithm for Restricted Boltzmann\nMachines (RBM), namely, the Online Generative Discriminative Restricted\nBoltzmann Machine (OGD-RBM), that provides the ability to build and adapt the\nnetwork architecture of RBM according to the statistics of streaming data. The\nOGD-RBM is trained in two phases: (1) an online generative phase for\nunsupervised feature representation at the hidden layer and (2) a\ndiscriminative phase for classification. The online generative training begins\nwith zero neurons in the hidden layer, adds and updates the neurons to adapt to\nstatistics of streaming data in a single pass unsupervised manner, resulting in\na feature representation best suited to the data. The discriminative phase is\nbased on stochastic gradient descent and associates the represented features to\nthe class labels. We demonstrate the OGD-RBM on a set of multi-category and\nbinary classification problems for data sets having varying degrees of\nclass-imbalance. We first apply the OGD-RBM algorithm on the multi-class MNIST\ndataset to characterize the network evolution. We demonstrate that the online\ngenerative phase converges to a stable, concise network architecture, wherein\nindividual neurons are inherently discriminative to the class labels despite\nunsupervised training. We then benchmark OGD-RBM performance to other machine\nlearning, neural network and ClassRBM techniques for credit scoring\napplications using 3 public non-stationary two-class credit datasets with\nvarying degrees of class-imbalance. We report that OGD-RBM improves accuracy by\n2.5-3% over batch learning techniques while requiring at least 24%-70% fewer\nneurons and fewer training samples. This online generative training approach\ncan be extended greedily to multiple layers for training Deep Belief Networks\nin non-stationary data mining applications without the need for a priori fixed\narchitectures. \n\n"}
{"id": "1803.02117", "contents": "Title: Equality case in van der Corput's inequality and collisions in multiple\n  lattice tilings Abstract: Van der Corput's provides the sharp bound vol(C) \\le m 2^d on the volume of a\nd-dimensional origin-symmetric convex body C that has 2m-1 points of the\ninteger lattice in its interior. For m=1, a characterization of the equality\ncase vol(C)= m 2^d is equivalent to the well-known problem of characterizing\ntilings by translations of a convex body. It is rather surprising that so far,\nfor m \\ge 2, no characterization of the equality case has been available,\nthough a hint to the respective characterization problem can be found in the\n1987 monograph of Gruber and Lekkerkerker. We give an explicit characterization\nof the equality case for all m \\ge 2. Our result reveals that, the equality\ncase for m \\ge 2 is more restrictive than for $m=1$. We also present\nconsequences of our characterization in the context of multiple lattice\ntilings. \n\n"}
{"id": "1803.04084", "contents": "Title: Link prediction for egocentrically sampled networks Abstract: Link prediction in networks is typically accomplished by estimating or\nranking the probabilities of edges for all pairs of nodes. In practice,\nespecially for social networks, the data are often collected by egocentric\nsampling, which means selecting a subset of nodes and recording all of their\nedges. This sampling mechanism requires different prediction tools than the\ntypical assumption of links missing at random. We propose a new computationally\nefficient link prediction algorithm for egocentrically sampled networks, which\nestimates the underlying probability matrix by estimating its row space. For\nnetworks created by sampling rows, our method outperforms many popular link\nprediction and graphon estimation techniques. \n\n"}
{"id": "1803.04239", "contents": "Title: FeTa: A DCA Pruning Algorithm with Generalization Error Guarantees Abstract: Recent DNN pruning algorithms have succeeded in reducing the number of\nparameters in fully connected layers, often with little or no drop in\nclassification accuracy. However, most of the existing pruning schemes either\nhave to be applied during training or require a costly retraining procedure\nafter pruning to regain classification accuracy. We start by proposing a cheap\npruning algorithm for fully connected DNN layers based on difference of convex\nfunctions (DC) optimisation, that requires little or no retraining. We then\nprovide a theoretical analysis for the growth in the Generalization Error (GE)\nof a DNN for the case of bounded perturbations to the hidden layers, of which\nweight pruning is a special case. Our pruning method is orders of magnitude\nfaster than competing approaches, while our theoretical analysis sheds light to\npreviously observed problems in DNN pruning. Experiments on commnon feedforward\nneural networks validate our results. \n\n"}
{"id": "1803.04837", "contents": "Title: Learning the Joint Representation of Heterogeneous Temporal Events for\n  Clinical Endpoint Prediction Abstract: The availability of a large amount of electronic health records (EHR)\nprovides huge opportunities to improve health care service by mining these\ndata. One important application is clinical endpoint prediction, which aims to\npredict whether a disease, a symptom or an abnormal lab test will happen in the\nfuture according to patients' history records. This paper develops deep\nlearning techniques for clinical endpoint prediction, which are effective in\nmany practical applications. However, the problem is very challenging since\npatients' history records contain multiple heterogeneous temporal events such\nas lab tests, diagnosis, and drug administrations. The visiting patterns of\ndifferent types of events vary significantly, and there exist complex nonlinear\nrelationships between different events. In this paper, we propose a novel model\nfor learning the joint representation of heterogeneous temporal events. The\nmodel adds a new gate to control the visiting rates of different events which\neffectively models the irregular patterns of different events and their\nnonlinear correlations. Experiment results with real-world clinical data on the\ntasks of predicting death and abnormal lab tests prove the effectiveness of our\nproposed approach over competitive baselines. \n\n"}
{"id": "1803.06531", "contents": "Title: Topology Estimation using Graphical Models in Multi-Phase Power\n  Distribution Grids Abstract: Distribution grid is the medium and low voltage part of a large power system.\nStructurally, the majority of distribution networks operate radially, such that\nenergized lines form a collection of trees, i.e. forest, with a substation\nbeing at the root of any tree. The operational topology/forest may change from\ntime to time, however tracking these changes, even though important for the\ndistribution grid operation and control, is hindered by limited real-time\nmonitoring. This paper develops a learning framework to reconstruct radial\noperational structure of the distribution grid from synchronized voltage\nmeasurements in the grid subject to the exogenous fluctuations in nodal power\nconsumption. To detect operational lines our learning algorithm uses\nconditional independence tests for continuous random variables that is\napplicable to a wide class of probability distributions of the nodal\nconsumption and Gaussian injections in particular. Moreover, our algorithm\napplies to the practical case of unbalanced three-phase power flow. Algorithm\nperformance is validated on AC power flow simulations over IEEE distribution\ngrid test cases. \n\n"}
{"id": "1803.07859", "contents": "Title: Efficient Sampling and Structure Learning of Bayesian Networks Abstract: Bayesian networks are probabilistic graphical models widely employed to\nunderstand dependencies in high dimensional data, and even to facilitate causal\ndiscovery. Learning the underlying network structure, which is encoded as a\ndirected acyclic graph (DAG) is highly challenging mainly due to the vast\nnumber of possible networks in combination with the acyclicity constraint.\nEfforts have focussed on two fronts: constraint-based methods that perform\nconditional independence tests to exclude edges and score and search approaches\nwhich explore the DAG space with greedy or MCMC schemes. Here we synthesise\nthese two fields in a novel hybrid method which reduces the complexity of MCMC\napproaches to that of a constraint-based method. Individual steps in the MCMC\nscheme only require simple table lookups so that very long chains can be\nefficiently obtained. Furthermore, the scheme includes an iterative procedure\nto correct for errors from the conditional independence tests. The algorithm\noffers markedly superior performance to alternatives, particularly because DAGs\ncan also be sampled from the posterior distribution, enabling full Bayesian\nmodel averaging for much larger Bayesian networks. \n\n"}
{"id": "1803.08471", "contents": "Title: Locally Private Bayesian Inference for Count Models Abstract: We present a general method for privacy-preserving Bayesian inference in\nPoisson factorization, a broad class of models that includes some of the most\nwidely used models in the social sciences. Our method satisfies limited\nprecision local privacy, a generalization of local differential privacy, which\nwe introduce to formulate privacy guarantees appropriate for sparse count data.\nWe develop an MCMC algorithm that approximates the locally private posterior\nover model parameters given data that has been locally privatized by the\ngeometric mechanism (Ghosh et al., 2012). Our solution is based on two\ninsights: 1) a novel reinterpretation of the geometric mechanism in terms of\nthe Skellam distribution (Skellam, 1946) and 2) a general theorem that relates\nthe Skellam to the Bessel distribution (Yuan & Kalbfleisch, 2000). We\ndemonstrate our method in two case studies on real-world email data in which we\nshow that our method consistently outperforms the commonly-used naive approach,\nobtaining higher quality topics in text and more accurate link prediction in\nnetworks. On some tasks, our privacy-preserving method even outperforms\nnon-private inference which conditions on the true data. \n\n"}
{"id": "1803.09365", "contents": "Title: Finite Sample Complexity of Sequential Monte Carlo Estimators Abstract: We present bounds for the finite sample error of sequential Monte Carlo\nsamplers on static spaces. Our approach explicitly relates the performance of\nthe algorithm to properties of the chosen sequence of distributions and mixing\nproperties of the associated Markov kernels. This allows us to give the first\nfinite sample comparison to other Monte Carlo schemes. We obtain bounds for the\ncomplexity of sequential Monte Carlo approximations for a variety of target\ndistributions including finite spaces, product measures, and log-concave\ndistributions including Bayesian logistic regression. The bounds obtained are\nwithin a logarithmic factor of similar bounds obtainable for Markov chain Monte\nCarlo. \n\n"}
{"id": "1803.09383", "contents": "Title: Online Second Order Methods for Non-Convex Stochastic Optimizations Abstract: This paper proposes a family of online second order methods for possibly\nnon-convex stochastic optimizations based on the theory of preconditioned\nstochastic gradient descent (PSGD), which can be regarded as an enhance\nstochastic Newton method with the ability to handle gradient noise and\nnon-convexity simultaneously. We have improved the implementations of the\noriginal PSGD in several ways, e.g., new forms of preconditioners, more\naccurate Hessian vector product calculations, and better numerical stability\nwith vanishing or ill-conditioned Hessian, etc.. We also have unrevealed the\nrelationship between feature normalization and PSGD with Kronecker product\npreconditioners, which explains the excellent performance of Kronecker product\npreconditioners in deep neural network learning. A software package\n(https://github.com/lixilinx/psgd_tf) implemented in Tensorflow is provided to\ncompare variations of stochastic gradient descent (SGD) and PSGD with five\ndifferent preconditioners on a wide range of benchmark problems with commonly\nused neural network architectures, e.g., convolutional and recurrent neural\nnetworks. Experimental results clearly demonstrate the advantages of PSGD in\nterms of generalization performance and convergence speed. \n\n"}
{"id": "1803.10746", "contents": "Title: Pseudo-marginal Bayesian inference for supervised Gaussian process\n  latent variable models Abstract: We introduce a Bayesian framework for inference with a supervised version of\nthe Gaussian process latent variable model. The framework overcomes the high\ncorrelations between latent variables and hyperparameters by using an unbiased\npseudo estimate for the marginal likelihood that approximately integrates over\nthe latent variables. This is used to construct a Markov Chain to explore the\nposterior of the hyperparameters. We demonstrate the procedure on simulated and\nreal examples, showing its ability to capture uncertainty and multimodality of\nthe hyperparameters and improved uncertainty quantification in predictions when\ncompared with variational inference. \n\n"}
{"id": "1804.01189", "contents": "Title: Real-Time Prediction of the Duration of Distribution System Outages Abstract: This paper addresses the problem of predicting duration of unplanned power\noutages, using historical outage records to train a series of neural network\npredictors. The initial duration prediction is made based on environmental\nfactors, and it is updated based on incoming field reports using natural\nlanguage processing to automatically analyze the text. Experiments using 15\nyears of outage records show good initial results and improved performance\nleveraging text. Case studies show that the language processing identifies\nphrases that point to outage causes and repair steps. \n\n"}
{"id": "1804.03280", "contents": "Title: A Deep Active Survival Analysis Approach for Precision Treatment\n  Recommendations: Application of Prostate Cancer Abstract: Survival analysis has been developed and applied in the number of areas\nincluding manufacturing, finance, economics and healthcare. In healthcare\ndomain, usually clinical data are high-dimensional, sparse and complex and\nsometimes there exists few amount of time-to-event (labeled) instances.\nTherefore building an accurate survival model from electronic health records is\nchallenging. With this motivation, we address this issue and provide a new\nsurvival analysis framework using deep learning and active learning with a\nnovel sampling strategy. First, our approach provides better representation\nwith lower dimensions from clinical features using labeled (time-to-event) and\nunlabeled (censored) instances and then actively trains the survival model by\nlabeling the censored data using an oracle. As a clinical assistive tool, we\nintroduce a simple effective treatment recommendation approach based on our\nsurvival model. In the experimental study, we apply our approach on\nSEER-Medicare data related to prostate cancer among African-Americans and white\npatients. The results indicate that our approach outperforms significantly than\nbaseline models. \n\n"}
{"id": "1804.03366", "contents": "Title: Testing equality of spectral density operators for functional linear\n  processes Abstract: The problem of testing equality of the entire second order structure of two\nindependent functional linear processes is considered. A fully functional\n$L^2$-type test is developed which evaluates, over all frequencies, the\nHilbert-Schmidt distance between the estimated spectral density operators of\nthe two processes. The asymptotic behavior of the test statistic is\ninvestigated and its limiting distribution under the null hypothesis is\nderived. Furthermore, a novel frequency domain bootstrap method is developed\nwhich approximates more accurately the distribution of the test statistic under\nthe null than the large sample Gaussian approximation obtained. Asymptotic\nvalidity of the bootstrap procedure is established and consistency of the\nbootstrap-based test under the alternative is proved. Numerical simulations\nshow that, even for small samples, the bootstrap-based test has very good size\nand power behavior. An application to meteorological functional time series is\nalso presented. \n\n"}
{"id": "1804.03599", "contents": "Title: Understanding disentangling in $\\beta$-VAE Abstract: We present new intuitions and theoretical assessments of the emergence of\ndisentangled representation in variational autoencoders. Taking a\nrate-distortion theory perspective, we show the circumstances under which\nrepresentations aligned with the underlying generative factors of variation of\ndata emerge when optimising the modified ELBO bound in $\\beta$-VAE, as training\nprogresses. From these insights, we propose a modification to the training\nregime of $\\beta$-VAE, that progressively increases the information capacity of\nthe latent code during training. This modification facilitates the robust\nlearning of disentangled representations in $\\beta$-VAE, without the previous\ntrade-off in reconstruction accuracy. \n\n"}
{"id": "1804.05251", "contents": "Title: An interpretable LSTM neural network for autoregressive exogenous model Abstract: In this paper, we propose an interpretable LSTM recurrent neural network,\ni.e., multi-variable LSTM for time series with exogenous variables. Currently,\nwidely used attention mechanism in recurrent neural networks mostly focuses on\nthe temporal aspect of data and falls short of characterizing variable\nimportance. To this end, our multi-variable LSTM equipped with tensorized\nhidden states is developed to learn variable specific representations, which\ngive rise to both temporal and variable level attention. Preliminary\nexperiments demonstrate comparable prediction performance of multi-variable\nLSTM w.r.t. encoder-decoder based baselines. More interestingly, variable\nimportance in real datasets characterized by the variable attention is highly\nin line with that determined by statistical Granger causality test, which\nexhibits the prospect of multi-variable LSTM as a simple and uniform end-to-end\nframework for both forecasting and knowledge discovery. \n\n"}
{"id": "1804.05589", "contents": "Title: SPSA-FSR: Simultaneous Perturbation Stochastic Approximation for Feature\n  Selection and Ranking Abstract: This manuscript presents the following: (1) an improved version of the Binary\nSimultaneous Perturbation Stochastic Approximation (SPSA) Method for feature\nselection in machine learning (Aksakalli and Malekipirbazari, Pattern\nRecognition Letters, Vol. 75, 2016) based on non-monotone iteration gains\ncomputed via the Barzilai and Borwein (BB) method, (2) its adaptation for\nfeature ranking, and (3) comparison against popular methods on public benchmark\ndatasets. The improved method, which we call SPSA-FSR, dramatically reduces the\nnumber of iterations required for convergence without impacting solution\nquality. SPSA-FSR can be used for feature ranking and feature selection both\nfor classification and regression problems. After a review of the current\nstate-of-the-art, we discuss our improvements in detail and present three sets\nof computational experiments: (1) comparison of SPSA-FS as a (wrapper) feature\nselection method against sequential methods as well as genetic algorithms, (2)\ncomparison of SPSA-FS as a feature ranking method in a classification setting\nagainst random forest importance, chi-squared, and information main methods,\nand (3) comparison of SPSA-FS as a feature ranking method in a regression\nsetting against minimum redundancy maximum relevance (MRMR), RELIEF, and linear\ncorrelation methods. The number of features in the datasets we use range from a\nfew dozens to a few thousands. Our results indicate that SPSA-FS converges to a\ngood feature set in no more than 100 iterations and therefore it is quite fast\nfor a wrapper method. SPSA-FS also outperforms popular feature selection as\nwell as feature ranking methods in majority of test cases, sometimes by a large\nmargin, and it stands as a promising new feature selection and ranking method. \n\n"}
{"id": "1804.06406", "contents": "Title: nestcheck: diagnostic tests for nested sampling calculations Abstract: Nested sampling is an increasingly popular technique for Bayesian\ncomputation, in particular for multimodal, degenerate problems of moderate to\nhigh dimensionality. Without appropriate settings, however, nested sampling\nsoftware may fail to explore such posteriors correctly; for example producing\ncorrelated samples or missing important modes. This paper introduces new\ndiagnostic tests to assess the reliability both of parameter estimation and\nevidence calculations using nested sampling software, and demonstrates them\nempirically. We present two new diagnostic plots for nested sampling, and give\npractical advice for nested sampling software users in astronomy and beyond.\nOur diagnostic tests and diagrams are implemented in nestcheck: a publicly\navailable Python package for analysing nested sampling calculations, which is\ncompatible with output from MultiNest, PolyChord and dyPolyChord. \n\n"}
{"id": "1804.06577", "contents": "Title: Fundamental domains for rhombic lattices with dihedral symmetry of order\n  8 Abstract: We show by construction that every rhombic lattice $\\Gamma$ in\n$\\mathbb{R}^{2}$ has a fundamental domain whose symmetry group contains the\npoint group of $\\Gamma$ as a subgroup of index $2$. This solves the last open\ncase of a question raised in [3] on fundamental domains for planar lattices\nwhose symmetry groups properly contain the point groups of the lattices. \n\n"}
{"id": "1804.06742", "contents": "Title: An efficient open-source implementation to compute the Jacobian matrix\n  for the Newton-Raphson power flow algorithm Abstract: Power flow calculations for systems with a large number of buses, e.g. grids\nwith multiple voltage levels, or time series based calculations result in a\nhigh computational effort. A common power flow solver for the efficient\nanalysis of power systems is the Newton-Raphson algorithm. The main\ncomputational effort of this method results from the linearization of the\nnonlinear power flow problem and solving the resulting linear equation. This\npaper presents an algorithm for the fast linearization of the power flow\nproblem by creating the Jacobian matrix directly in CRS format. The increase in\nspeed is achieved by reducing the number of iterations over the nonzero\nelements of the sparse Jacobian matrix. This allows to efficiently create the\nJacobian matrix without having to approximate the problem. A comparison of the\ncalculation time of three power grids shows that comparable open-source\nimplementations need 3-14x the time to create the Jacobian matrix. \n\n"}
{"id": "1804.07824", "contents": "Title: Autotune: A Derivative-free Optimization Framework for Hyperparameter\n  Tuning Abstract: Machine learning applications often require hyperparameter tuning. The\nhyperparameters usually drive both the efficiency of the model training process\nand the resulting model quality. For hyperparameter tuning, machine learning\nalgorithms are complex black-boxes. This creates a class of challenging\noptimization problems, whose objective functions tend to be nonsmooth,\ndiscontinuous, unpredictably varying in computational expense, and include\ncontinuous, categorical, and/or integer variables. Further, function\nevaluations can fail for a variety of reasons including numerical difficulties\nor hardware failures. Additionally, not all hyperparameter value combinations\nare compatible, which creates so called hidden constraints. Robust and\nefficient optimization algorithms are needed for hyperparameter tuning. In this\npaper we present an automated parallel derivative-free optimization framework\ncalled \\textbf{Autotune}, which combines a number of specialized sampling and\nsearch methods that are very effective in tuning machine learning models\ndespite these challenges. Autotune provides significantly improved models over\nusing default hyperparameter settings with minimal user interaction on\nreal-world applications. Given the inherent expense of training numerous\ncandidate models, we demonstrate the effectiveness of Autotune's search methods\nand the efficient distributed and parallel paradigms for training and tuning\nmodels, and also discuss the resource trade-offs associated with the ability to\nboth distribute the training process and parallelize the tuning process. \n\n"}
{"id": "1804.08222", "contents": "Title: Null-free False Discovery Rate Control Using Decoy Permutations Abstract: The traditional approaches to false discovery rate (FDR) control in multiple\nhypothesis testing are usually based on the null distribution of a test\nstatistic. However, all types of null distributions, including the theoretical,\npermutation-based and empirical ones, have some inherent drawbacks. For\nexample, the theoretical null might fail because of improper assumptions on the\nsample distribution. Here, we propose a null distribution-free approach to FDR\ncontrol for multiple hypothesis testing. This approach, named target-decoy\nprocedure, simply builds on the ordering of tests by some statistic or score,\nthe null distribution of which is not required to be known. Competitive decoy\ntests are constructed from permutations of original samples and are used to\nestimate the false target discoveries. We prove that this approach controls the\nFDR when the statistics are independent between different tests. Simulation\ndemonstrates that it is more stable and powerful than two existing popular\napproaches. Evaluation is also made on a real dataset. \n\n"}
{"id": "1804.08416", "contents": "Title: Learn and Pick Right Nodes to Offload Abstract: Task offloading is a promising technology to exploit the benefits of fog\ncomputing. An effective task offloading strategy is needed to utilize the\ncomputational resources efficiently. In this paper, we endeavor to seek an\nonline task offloading strategy to minimize the long-term latency. In\nparticular, we formulate a stochastic programming problem, where the\nexpectations of the system parameters change abruptly at unknown time instants.\nMeanwhile, we consider the fact that the queried nodes can only feed back the\nprocessing results after finishing the tasks. We then put forward an effective\nalgorithm to solve this challenging stochastic programming under the\nnon-stationary bandit model. We further prove that our proposed algorithm is\nasymptotically optimal in a non-stationary fog-enabled network. Numerical\nsimulations are carried out to corroborate our designs. \n\n"}
{"id": "1804.08501", "contents": "Title: Dropping Networks for Transfer Learning Abstract: Many tasks in natural language understanding require learning relationships\nbetween two sequences for various tasks such as natural language inference,\nparaphrasing and entailment. These aforementioned tasks are similar in nature,\nyet they are often modeled individually. Knowledge transfer can be effective\nfor closely related tasks. However, transferring all knowledge, some of which\nirrelevant for a target task, can lead to sub-optimal results due to\n\\textit{negative} transfer. Hence, this paper focuses on the transferability of\nboth instances and parameters across natural language understanding tasks by\nproposing an ensemble-based transfer learning method. \\newline The primary\ncontribution of this paper is the combination of both \\textit{Dropout} and\n\\textit{Bagging} for improved transferability in neural networks, referred to\nas \\textit{Dropping} herein. We present a straightforward yet novel approach\nfor incorporating source \\textit{Dropping} Networks to a target task for\nfew-shot learning that mitigates \\textit{negative} transfer. This is achieved\nby using a decaying parameter chosen according to the slope changes of a\nsmoothed spline error curve at sub-intervals during training. We compare the\nproposed approach against hard parameter sharing and soft parameter sharing\ntransfer methods in the few-shot learning case. We also compare against models\nthat are fully trained on the target task in the standard supervised learning\nsetup. The aforementioned adjustment leads to improved transfer learning\nperformance and comparable results to the current state of the art only using a\nfraction of the data from the target task. \n\n"}
{"id": "1804.08738", "contents": "Title: Bayesian Updating and Uncertainty Quantification using Sequential\n  Tempered MCMC with the Rank-One Modified Metropolis Algorithm Abstract: Bayesian methods are critical for quantifying the behaviors of systems. They\ncapture our uncertainty about a system's behavior using probability\ndistributions and update this understanding as new information becomes\navailable. Probabilistic predictions that incorporate this uncertainty can then\nbe made to evaluate system performance and make decisions. While Bayesian\nmethods are very useful, they are often computationally intensive. This\nnecessitates the development of more efficient algorithms. Here, we discuss a\ngroup of population Markov Chain Monte Carlo (MCMC) methods for Bayesian\nupdating and system reliability assessment that we call Sequential Tempered\nMCMC (ST-MCMC) algorithms. These algorithms combine 1) a notion of tempering to\ngradually transform a population of samples from the prior to the posterior\nthrough a series of intermediate distributions, 2) importance resampling, and\n3) MCMC. They are a form of Sequential Monte Carlo and include algorithms like\nTransitional Markov Chain Monte Carlo and Subset Simulation. We also introduce\na new sampling algorithm called the Rank-One Modified Metropolis Algorithm\n(ROMMA), which builds upon the Modified Metropolis Algorithm used within Subset\nSimulation to improve performance in high dimensions. Finally, we formulate a\nsingle algorithm to solve combined Bayesian updating and reliability assessment\nproblems to make posterior assessments of system reliability. The algorithms\nare then illustrated by performing prior and posterior reliability assessment\nof a water distribution system with unknown leaks and demands. \n\n"}
{"id": "1804.08750", "contents": "Title: A machine learning model for identifying cyclic alternating patterns in\n  the sleeping brain Abstract: Electroencephalography (EEG) is a method to record the electrical signals in\nthe brain. Recognizing the EEG patterns in the sleeping brain gives insights\ninto the understanding of sleeping disorders. The dataset under consideration\ncontains EEG data points associated with various physiological conditions. This\nstudy attempts to generalize the detection of particular patterns associated\nwith the Non-Rapid Eye Movement (NREM) sleep cycle of the brain using a machine\nlearning model. The proposed model uses additional feature engineering to\nincorporate sequential information for training a classifier to predict the\noccurrence of Cyclic Alternating Pattern (CAP) sequences in the sleep cycle,\nwhich are often associated with sleep disorders. \n\n"}
{"id": "1804.09753", "contents": "Title: The phase transition for the existence of the maximum likelihood\n  estimate in high-dimensional logistic regression Abstract: This paper rigorously establishes that the existence of the maximum\nlikelihood estimate (MLE) in high-dimensional logistic regression models with\nGaussian covariates undergoes a sharp `phase transition'. We introduce an\nexplicit boundary curve $h_{\\text{MLE}}$, parameterized by two scalars\nmeasuring the overall magnitude of the unknown sequence of regression\ncoefficients, with the following property: in the limit of large sample sizes\n$n$ and number of features $p$ proportioned in such a way that $p/n \\rightarrow\n\\kappa$, we show that if the problem is sufficiently high dimensional in the\nsense that $\\kappa > h_{\\text{MLE}}$, then the MLE does not exist with\nprobability one. Conversely, if $\\kappa < h_{\\text{MLE}}$, the MLE\nasymptotically exists with probability one. \n\n"}
{"id": "1804.10109", "contents": "Title: Quantized Compressive K-Means Abstract: The recent framework of compressive statistical learning aims at designing\ntractable learning algorithms that use only a heavily compressed\nrepresentation-or sketch-of massive datasets. Compressive K-Means (CKM) is such\na method: it estimates the centroids of data clusters from pooled, non-linear,\nrandom signatures of the learning examples. While this approach significantly\nreduces computational time on very large datasets, its digital implementation\nwastes acquisition resources because the learning examples are compressed only\nafter the sensing stage. The present work generalizes the sketching procedure\ninitially defined in Compressive K-Means to a large class of periodic\nnonlinearities including hardware-friendly implementations that compressively\nacquire entire datasets. This idea is exemplified in a Quantized Compressive\nK-Means procedure, a variant of CKM that leverages 1-bit universal quantization\n(i.e. retaining the least significant bit of a standard uniform quantizer) as\nthe periodic sketch nonlinearity. Trading for this resource-efficient signature\n(standard in most acquisition schemes) has almost no impact on the clustering\nperformances, as illustrated by numerical experiments. \n\n"}
{"id": "1805.00020", "contents": "Title: A Guide to Constraining Effective Field Theories with Machine Learning Abstract: We develop, discuss, and compare several inference techniques to constrain\ntheory parameters in collider experiments. By harnessing the latent-space\nstructure of particle physics processes, we extract extra information from the\nsimulator. This augmented data can be used to train neural networks that\nprecisely estimate the likelihood ratio. The new methods scale well to many\nobservables and high-dimensional parameter spaces, do not require any\napproximations of the parton shower and detector response, and can be evaluated\nin microseconds. Using weak-boson-fusion Higgs production as an example\nprocess, we compare the performance of several techniques. The best results are\nfound for likelihood ratio estimators trained with extra information about the\nscore, the gradient of the log likelihood function with respect to the theory\nparameters. The score also provides sufficient statistics that contain all the\ninformation needed for inference in the neighborhood of the Standard Model.\nThese methods enable us to put significantly stronger bounds on effective\ndimension-six operators than the traditional approach based on histograms. They\nalso outperform generic machine learning methods that do not make use of the\nparticle physics structure, demonstrating their potential to substantially\nimprove the new physics reach of the LHC legacy results. \n\n"}
{"id": "1805.00784", "contents": "Title: Markov Chain Neural Networks Abstract: In this work we present a modified neural network model which is capable to\nsimulate Markov Chains. We show how to express and train such a network, how to\nensure given statistical properties reflected in the training data and we\ndemonstrate several applications where the network produces non-deterministic\noutcomes. One example is a random walker model, e.g. useful for simulation of\nBrownian motions or a natural Tic-Tac-Toe network which ensures\nnon-deterministic game behavior. \n\n"}
{"id": "1805.00980", "contents": "Title: SaaS: Speed as a Supervisor for Semi-supervised Learning Abstract: We introduce the SaaS Algorithm for semi-supervised learning, which uses\nlearning speed during stochastic gradient descent in a deep neural network to\nmeasure the quality of an iterative estimate of the posterior probability of\nunknown labels. Training speed in supervised learning correlates strongly with\nthe percentage of correct labels, so we use it as an inference criterion for\nthe unknown labels, without attempting to infer the model parameters at first.\nDespite its simplicity, SaaS achieves state-of-the-art results in\nsemi-supervised learning benchmarks. \n\n"}
{"id": "1805.01174", "contents": "Title: Optimization of computational budget for power system risk assessment Abstract: We address the problem of maintaining high voltage power transmission\nnetworks in security at all time, namely anticipating exceeding of thermal\nlimit for eventual single line disconnection (whatever its cause may be) by\nrunning slow, but accurate, physical grid simulators. New conceptual frameworks\nare calling for a probabilistic risk-based security criterion. However, these\napproaches suffer from high requirements in terms of tractability. Here, we\npropose a new method to assess the risk. This method uses both machine learning\ntechniques (artificial neural networks) and more standard simulators based on\nphysical laws. More specifically we train neural networks to estimate the\noverall dangerousness of a grid state. A classical benchmark problem (manpower\n118 buses test case) is used to show the strengths of the proposed method. \n\n"}
{"id": "1805.01870", "contents": "Title: Hedging parameter selection for basis pursuit Abstract: In Compressed Sensing and high dimensional estimation, signal recovery often\nrelies on sparsity assumptions and estimation is performed via\n$\\ell_1$-penalized least-squares optimization, a.k.a. LASSO. The $\\ell_1$\npenalisation is usually controlled by a weight, also called \"relaxation\nparameter\", denoted by $\\lambda$. It is commonly thought that the practical\nefficiency of the LASSO for prediction crucially relies on accurate selection\nof $\\lambda$. In this short note, we propose to consider the hyper-parameter\nselection problem from a new perspective which combines the Hedge online\nlearning method by Freund and Shapire, with the stochastic Frank-Wolfe method\nfor the LASSO. Using the Hedge algorithm, we show that a our simple selection\nrule can achieve prediction results comparable to Cross Validation at a\npotentially much lower computational cost. \n\n"}
{"id": "1805.02608", "contents": "Title: Anticipating contingengies in power grids using fast neural net\n  screening Abstract: We address the problem of maintaining high voltage power transmission\nnetworks in security at all time. This requires that power flowing through all\nlines remain below a certain nominal thermal limit above which lines might\nmelt, break or cause other damages. Current practices include enforcing the\ndeterministic \"N-1\" reliability criterion, namely anticipating exceeding of\nthermal limit for any eventual single line disconnection (whatever its cause\nmay be) by running a slow, but accurate, physical grid simulator. New\nconceptual frameworks are calling for a probabilistic risk based security\ncriterion and are in need of new methods to assess the risk. To tackle this\ndifficult assessment, we address in this paper the problem of rapidly ranking\nhigher order contingencies including all pairs of line disconnections, to\nbetter prioritize simulations. We present a novel method based on neural\nnetworks, which ranks \"N-1\" and \"N-2\" contingencies in decreasing order of\npresumed severity. We demonstrate on a classical benchmark problem that the\nresidual risk of contingencies decreases dramatically compared to considering\nsolely all \"N-1\" cases, at no additional computational cost. We evaluate that\nour method scales up to power grids of the size of the French high voltage\npower grid (over 1000 power lines). \n\n"}
{"id": "1805.02785", "contents": "Title: Fast Online Exact Solutions for Deterministic MDPs with Sparse Rewards Abstract: Markov Decision Processes (MDPs) are a mathematical framework for modeling\nsequential decision making under uncertainty. The classical approaches for\nsolving MDPs are well known and have been widely studied, some of which rely on\napproximation techniques to solve MDPs with large state space and/or action\nspace. However, most of these classical solution approaches and their\napproximation techniques still take much computation time to converge and\nusually must be re-computed if the reward function is changed. This paper\nintroduces a novel alternative approach for exactly and efficiently solving\ndeterministic, continuous MDPs with sparse reward sources. When the environment\nis such that the \"distance\" between states can be determined in constant time,\ne.g. grid world, our algorithm offers $O( |R|^2 \\times |A|^2 \\times |S|)$,\nwhere $|R|$ is the number of reward sources, $|A|$ is the number of actions,\nand $|S|$ is the number of states. Memory complexity for the algorithm is $O(\n|S| + |R| \\times |A|)$. This new approach opens new avenues for boosting\ncomputational performance for certain classes of MDPs and is of tremendous\nvalue for MDP applications such as robotics and unmanned systems. This paper\ndescribes the algorithm and presents numerical experiment results to\ndemonstrate its powerful computational performance. We also provide rigorous\nmathematical description of the approach. \n\n"}
{"id": "1805.03288", "contents": "Title: Fused Density Estimation: Theory and Methods Abstract: In this paper we introduce a method for nonparametric density estimation on\ngeometric networks. We define fused density estimators as solutions to a total\nvariation regularized maximum-likelihood density estimation problem. We provide\ntheoretical support for fused density estimation by proving that the squared\nHellinger rate of convergence for the estimator achieves the minimax bound over\nunivariate densities of log-bounded variation. We reduce the original\nvariational formulation in order to transform it into a tractable,\nfinite-dimensional quadratic program. Because random variables on geometric\nnetworks are simple generalizations of the univariate case, this method also\nprovides a useful tool for univariate density estimation. Lastly, we apply this\nmethod and assess its performance on examples in the univariate and geometric\nnetwork setting. We compare the performance of different optimization\ntechniques to solve the problem, and use these results to inform\nrecommendations for the computation of fused density estimators. \n\n"}
{"id": "1805.03541", "contents": "Title: Stolarsky's invariance principle for projective spaces Abstract: We show that Stolarsky's invariance principle, known for point distributions\non the Euclidean spheres, can be extended to the real, complex, and\nquaternionic projective spaces and the octonionic projective plane. A part of\nthe results (Theorem~1.1 ana Corollary~1.1) was given early in the previous\npaper [22], while the explicit formulas for the constants in the invariance\nprinciples for projective spaces (Theorem~1.2 and Corollary~1.2) are new. \n\n"}
{"id": "1805.03714", "contents": "Title: Foundations of Sequence-to-Sequence Modeling for Time Series Abstract: The availability of large amounts of time series data, paired with the\nperformance of deep-learning algorithms on a broad class of problems, has\nrecently led to significant interest in the use of sequence-to-sequence models\nfor time series forecasting. We provide the first theoretical analysis of this\ntime series forecasting framework. We include a comparison of\nsequence-to-sequence modeling to classical time series models, and as such our\ntheory can serve as a quantitative guide for practitioners choosing between\ndifferent modeling methodologies. \n\n"}
{"id": "1805.05071", "contents": "Title: KL-UCB-switch: optimal regret bounds for stochastic bandits from both a\n  distribution-dependent and a distribution-free viewpoints Abstract: We consider $K$-armed stochastic bandits and consider cumulative regret\nbounds up to time $T$. We are interested in strategies achieving simultaneously\na distribution-free regret bound of optimal order $\\sqrt{KT}$ and a\ndistribution-dependent regret that is asymptotically optimal, that is, matching\nthe $\\kappa\\ln T$ lower bound by Lai and Robbins (1985) and Burnetas and\nKatehakis (1996), where $\\kappa$ is the optimal problem-dependent constant.\nThis constant $\\kappa$ depends on the model $\\mathcal{D}$ considered (the\nfamily of possible distributions over the arms). M\\'enard and Garivier (2017)\nprovided strategies achieving such a bi-optimality in the parametric case of\nmodels given by one-dimensional exponential families, while Lattimore (2016,\n2018) did so for the family of (sub)Gaussian distributions with variance less\nthan $1$. We extend this result to the non-parametric case of all distributions\nover $[0,1]$. We do so by combining the MOSS strategy by Audibert and Bubeck\n(2009), which enjoys a distribution-free regret bound of optimal order\n$\\sqrt{KT}$, and the KL-UCB strategy by Capp\\'e et al. (2013), for which we\nprovide in passing the first analysis of an optimal distribution-dependent\n$\\kappa\\ln T$ regret bound in the model of all distributions over $[0,1]$. We\nwere able to obtain this non-parametric bi-optimality result while working hard\nto streamline the proofs (of previously known regret bounds and thus of the new\nanalyses carried out); a second merit of the present contribution is therefore\nto provide a review of proofs of classical regret bounds for index-based\nstrategies for $K$-armed stochastic bandits. \n\n"}
{"id": "1805.06523", "contents": "Title: End-to-end Learning of a Convolutional Neural Network via Deep Tensor\n  Decomposition Abstract: In this paper we study the problem of learning the weights of a deep\nconvolutional neural network. We consider a network where convolutions are\ncarried out over non-overlapping patches with a single kernel in each layer. We\ndevelop an algorithm for simultaneously learning all the kernels from the\ntraining data. Our approach dubbed Deep Tensor Decomposition (DeepTD) is based\non a rank-1 tensor decomposition. We theoretically investigate DeepTD under a\nrealizable model for the training data where the inputs are chosen i.i.d. from\na Gaussian distribution and the labels are generated according to planted\nconvolutional kernels. We show that DeepTD is data-efficient and provably works\nas soon as the sample size exceeds the total number of convolutional weights in\nthe network. We carry out a variety of numerical experiments to investigate the\neffectiveness of DeepTD and verify our theoretical findings. \n\n"}
{"id": "1805.06639", "contents": "Title: Independent Component Analysis via Energy-based and Kernel-based Mutual\n  Dependence Measures Abstract: We apply both distance-based (Jin and Matteson, 2017) and kernel-based\n(Pfister et al., 2016) mutual dependence measures to independent component\nanalysis (ICA), and generalize dCovICA (Matteson and Tsay, 2017) to MDMICA,\nminimizing empirical dependence measures as an objective function in both\ndeflation and parallel manners. Solving this minimization problem, we introduce\nLatin hypercube sampling (LHS) (McKay et al., 2000), and a global optimization\nmethod, Bayesian optimization (BO) (Mockus, 1994) to improve the initialization\nof the Newton-type local optimization method. The performance of MDMICA is\nevaluated in various simulation studies and an image data example. When the ICA\nmodel is correct, MDMICA achieves competitive results compared to existing\napproaches. When the ICA model is misspecified, the estimated independent\ncomponents are less mutually dependent than the observed components using\nMDMICA, while they are prone to be even more mutually dependent than the\nobserved components using other approaches. \n\n"}
{"id": "1805.07220", "contents": "Title: Memoryless Exact Solutions for Deterministic MDPs with Sparse Rewards Abstract: We propose an algorithm for deterministic continuous Markov Decision\nProcesses with sparse rewards that computes the optimal policy exactly with no\ndependency on the size of the state space. The algorithm has time complexity of\n$O( |R|^3 \\times |A|^2 )$ and memory complexity of $O( |R| \\times |A| )$, where\n$|R|$ is the number of reward sources and $|A|$ is the number of actions.\nFurthermore, we describe a companion algorithm that can follow the optimal\npolicy from any initial state without computing the entire value function,\ninstead computing on-demand the value of states as they are needed. The\nalgorithm to solve the MDP does not depend on the size of the state space for\neither time or memory complexity, and the ability to follow the optimal policy\nis linear in time and space with the path length of following the optimal\npolicy from the initial state. We demonstrate the algorithm operation side by\nside with value iteration on tractable MDPs. \n\n"}
{"id": "1805.07441", "contents": "Title: Overcoming catastrophic forgetting problem by weight consolidation and\n  long-term memory Abstract: Sequential learning of multiple tasks in artificial neural networks using\ngradient descent leads to catastrophic forgetting, whereby previously learned\nknowledge is erased during learning of new, disjoint knowledge. Here, we\npropose a new approach to sequential learning which leverages the recent\ndiscovery of adversarial examples. We use adversarial subspaces from previous\ntasks to enable learning of new tasks with less interference. We apply our\nmethod to sequentially learning to classify digits 0, 1, 2 (task 1), 4, 5, 6,\n(task 2), and 7, 8, 9 (task 3) in MNIST (disjoint MNIST task). We compare and\ncombine our Adversarial Direction (AD) method with the recently proposed\nElastic Weight Consolidation (EWC) method for sequential learning. We train\neach task for 20 epochs, which yields good initial performance (99.24% correct\ntask 1 performance). After training task 2, and then task 3, both plain\ngradient descent (PGD) and EWC largely forget task 1 (task 1 accuracy 32.95%\nfor PGD and 41.02% for EWC), while our combined approach (AD+EWC) still\nachieves 94.53% correct on task 1. We obtain similar results with a much more\ndifficult disjoint CIFAR10 task, which to our knowledge had not been attempted\nbefore (70.10% initial task 1 performance, 67.73% after learning tasks 2 and 3\nfor AD+EWC, while PGD and EWC both fall to chance level). Our results suggest\nthat AD+EWC can provide better sequential learning performance than either PGD\nor EWC. \n\n"}
{"id": "1805.07460", "contents": "Title: Fast Kernel Approximations for Latent Force Models and Convolved\n  Multiple-Output Gaussian processes Abstract: A latent force model is a Gaussian process with a covariance function\ninspired by a differential operator. Such covariance function is obtained by\nperforming convolution integrals between Green's functions associated to the\ndifferential operators, and covariance functions associated to latent\nfunctions. In the classical formulation of latent force models, the covariance\nfunctions are obtained analytically by solving a double integral, leading to\nexpressions that involve numerical solutions of different types of error\nfunctions. In consequence, the covariance matrix calculation is considerably\nexpensive, because it requires the evaluation of one or more of these error\nfunctions. In this paper, we use random Fourier features to approximate the\nsolution of these double integrals obtaining simpler analytical expressions for\nsuch covariance functions. We show experimental results using ordinary\ndifferential operators and provide an extension to build general kernel\nfunctions for convolved multiple output Gaussian processes. \n\n"}
{"id": "1805.07502", "contents": "Title: On Deep Ensemble Learning from a Function Approximation Perspective Abstract: In this paper, we propose to provide a general ensemble learning framework\nbased on deep learning models. Given a group of unit models, the proposed deep\nensemble learning framework will effectively combine their learning results via\na multilayered ensemble model. In the case when the unit model mathematical\nmappings are bounded, sigmoidal and discriminatory, we demonstrate that the\ndeep ensemble learning framework can achieve a universal approximation of any\nfunctions from the input space to the output space. Meanwhile, to achieve such\na performance, the deep ensemble learning framework also impose a strict\nconstraint on the number of involved unit models. According to the theoretic\nproof provided in this paper, given the input feature space of dimension d, the\nrequired unit model number will be 2d, if the ensemble model involves one\nsingle layer. Furthermore, as the ensemble component goes deeper, the number of\nrequired unit model is proved to be lowered down exponentially. \n\n"}
{"id": "1805.07507", "contents": "Title: Reconciled Polynomial Machine: A Unified Representation of Shallow and\n  Deep Learning Models Abstract: In this paper, we aim at introducing a new machine learning model, namely\nreconciled polynomial machine, which can provide a unified representation of\nexisting shallow and deep machine learning models. Reconciled polynomial\nmachine predicts the output by computing the inner product of the feature\nkernel function and variable reconciling function. Analysis of several concrete\nmodels, including Linear Models, FM, MVM, Perceptron, MLP and Deep Neural\nNetworks, will be provided in this paper, which can all be reduced to the\nreconciled polynomial machine representations. Detailed analysis of the\nlearning error by these models will also be illustrated in this paper based on\ntheir reduced representations from the function approximation perspective. \n\n"}
{"id": "1805.07654", "contents": "Title: Sampling-Free Variational Inference of Bayesian Neural Networks by\n  Variance Backpropagation Abstract: We propose a new Bayesian Neural Net formulation that affords variational\ninference for which the evidence lower bound is analytically tractable subject\nto a tight approximation. We achieve this tractability by (i) decomposing ReLU\nnonlinearities into the product of an identity and a Heaviside step function,\n(ii) introducing a separate path that decomposes the neural net expectation\nfrom its variance. We demonstrate formally that introducing separate latent\nbinary variables to the activations allows representing the neural network\nlikelihood as a chain of linear operations. Performing variational inference on\nthis construction enables a sampling-free computation of the evidence lower\nbound which is a more effective approximation than the widely applied Monte\nCarlo sampling and CLT related techniques. We evaluate the model on a range of\nregression and classification tasks against BNN inference alternatives, showing\ncompetitive or improved performance over the current state-of-the-art. \n\n"}
{"id": "1805.07785", "contents": "Title: Conditional Inference in Pre-trained Variational Autoencoders via\n  Cross-coding Abstract: Variational Autoencoders (VAEs) are a popular generative model, but one in\nwhich conditional inference can be challenging. If the decomposition into query\nand evidence variables is fixed, conditional VAEs provide an attractive\nsolution. To support arbitrary queries, one is generally reduced to Markov\nChain Monte Carlo sampling methods that can suffer from long mixing times. In\nthis paper, we propose an idea we term cross-coding to approximate the\ndistribution over the latent variables after conditioning on an evidence\nassignment to some subset of the variables. This allows generating query\nsamples without retraining the full VAE. We experimentally evaluate three\nvariations of cross-coding showing that (i) they can be quickly optimized for\ndifferent decompositions of evidence and query and (ii) they quantitatively and\nqualitatively outperform Hamiltonian Monte Carlo. \n\n"}
{"id": "1805.07848", "contents": "Title: A Universal Music Translation Network Abstract: We present a method for translating music across musical instruments, genres,\nand styles. This method is based on a multi-domain wavenet autoencoder, with a\nshared encoder and a disentangled latent space that is trained end-to-end on\nwaveforms. Employing a diverse training dataset and large net capacity, the\ndomain-independent encoder allows us to translate even from musical domains\nthat were not seen during training. The method is unsupervised and does not\nrely on supervision in the form of matched samples between domains or musical\ntranscriptions. We evaluate our method on NSynth, as well as on a dataset\ncollected from professional musicians, and achieve convincing translations,\neven when translating from whistling, potentially enabling the creation of\ninstrumental music by untrained humans. \n\n"}
{"id": "1805.08321", "contents": "Title: Bandit-Based Monte Carlo Optimization for Nearest Neighbors Abstract: The celebrated Monte Carlo method estimates an expensive-to-compute quantity\nby random sampling. Bandit-based Monte Carlo optimization is a general\ntechnique for computing the minimum of many such expensive-to-compute\nquantities by adaptive random sampling. The technique converts an optimization\nproblem into a statistical estimation problem which is then solved via\nmulti-armed bandits. We apply this technique to solve the problem of\nhigh-dimensional $k$-nearest neighbors, developing an algorithm which we prove\nis able to identify exact nearest neighbors with high probability. We show that\nunder regularity assumptions on a dataset of $n$ points in $d$-dimensional\nspace, the complexity of our algorithm scales logarithmically with the\ndimension of the data as $O\\left((n+d)\\log^2\n\\left(\\frac{nd}{\\delta}\\right)\\right)$ for error probability $\\delta$, rather\nthan linearly as in exact computation requiring $O(nd)$. We corroborate our\ntheoretical results with numerical simulations, showing that our algorithm\noutperforms both exact computation and state-of-the-art algorithms such as\nkGraph, NGT, and LSH on real datasets. \n\n"}
{"id": "1805.08709", "contents": "Title: A Simple Cache Model for Image Recognition Abstract: Training large-scale image recognition models is computationally expensive.\nThis raises the question of whether there might be simple ways to improve the\ntest performance of an already trained model without having to re-train or\nfine-tune it with new data. Here, we show that, surprisingly, this is indeed\npossible. The key observation we make is that the layers of a deep network\nclose to the output layer contain independent, easily extractable\nclass-relevant information that is not contained in the output layer itself. We\npropose to extract this extra class-relevant information using a simple\nkey-value cache memory to improve the classification performance of the model\nat test time. Our cache memory is directly inspired by a similar cache model\npreviously proposed for language modeling (Grave et al., 2017). This cache\ncomponent does not require any training or fine-tuning; it can be applied to\nany pre-trained model and, by properly setting only two hyper-parameters, leads\nto significant improvements in its classification performance. Improvements are\nobserved across several architectures and datasets. In the cache component,\nusing features extracted from layers close to the output (but not from the\noutput layer itself) as keys leads to the largest improvements. Concatenating\nfeatures from multiple layers to form keys can further improve performance over\nusing single-layer features as keys. The cache component also has a\nregularizing effect, a simple consequence of which is that it substantially\nincreases the robustness of models against adversarial attacks. \n\n"}
{"id": "1805.09208", "contents": "Title: Pushing the bounds of dropout Abstract: We show that dropout training is best understood as performing MAP estimation\nconcurrently for a family of conditional models whose objectives are themselves\nlower bounded by the original dropout objective. This discovery allows us to\npick any model from this family after training, which leads to a substantial\nimprovement on regularisation-heavy language modelling. The family includes\nmodels that compute a power mean over the sampled dropout masks, and their less\nstochastic subvariants with tighter and higher lower bounds than the fully\nstochastic dropout objective. We argue that since the deterministic\nsubvariant's bound is equal to its objective, and the highest amongst these\nmodels, the predominant view of it as a good approximation to MC averaging is\nmisleading. Rather, deterministic dropout is the best available approximation\nto the true objective. \n\n"}
{"id": "1805.09370", "contents": "Title: Towards Robust Training of Neural Networks by Regularizing Adversarial\n  Gradients Abstract: In recent years, neural networks have demonstrated outstanding effectiveness\nin a large amount of applications.However, recent works have shown that neural\nnetworks are susceptible to adversarial examples, indicating possible flaws\nintrinsic to the network structures. To address this problem and improve the\nrobustness of neural networks, we investigate the fundamental mechanisms behind\nadversarial examples and propose a novel robust training method via regulating\nadversarial gradients. The regulation effectively squeezes the adversarial\ngradients of neural networks and significantly increases the difficulty of\nadversarial example generation.Without any adversarial example involved, the\nrobust training method could generate naturally robust networks, which are\nnear-immune to various types of adversarial examples. Experiments show the\nnaturally robust networks can achieve optimal accuracy against Fast Gradient\nSign Method (FGSM) and C\\&W attacks on MNIST, Cifar10, and Google Speech\nCommand dataset. Moreover, our proposed method also provides neural networks\nwith consistent robustness against transferable attacks. \n\n"}
{"id": "1805.09781", "contents": "Title: Efficient Inference in Multi-task Cox Process Models Abstract: We generalize the log Gaussian Cox process (LGCP) framework to model multiple\ncorrelated point data jointly. The observations are treated as realizations of\nmultiple LGCPs, whose log intensities are given by linear combinations of\nlatent functions drawn from Gaussian process priors. The combination\ncoefficients are also drawn from Gaussian processes and can incorporate\nadditional dependencies. We derive closed-form expressions for the moments of\nthe intensity functions and develop an efficient variational inference\nalgorithm that is orders of magnitude faster than competing deterministic and\nstochastic approximations of multivariate LGCP, coregionalization models, and\nmulti-task permanental processes. Our approach outperforms these benchmarks in\nmultiple problems, offering the current state of the art in modeling\nmultivariate point processes. \n\n"}
{"id": "1805.10369", "contents": "Title: Stable Recurrent Models Abstract: Stability is a fundamental property of dynamical systems, yet to this date it\nhas had little bearing on the practice of recurrent neural networks. In this\nwork, we conduct a thorough investigation of stable recurrent models.\nTheoretically, we prove stable recurrent neural networks are well approximated\nby feed-forward networks for the purpose of both inference and training by\ngradient descent. Empirically, we demonstrate stable recurrent models often\nperform as well as their unstable counterparts on benchmark sequence tasks.\nTaken together, these findings shed light on the effective power of recurrent\nnetworks and suggest much of sequence learning happens, or can be made to\nhappen, in the stable regime. Moreover, our results help to explain why in many\ncases practitioners succeed in replacing recurrent models by feed-forward\nmodels. \n\n"}
{"id": "1805.10406", "contents": "Title: Robust Nonparametric Regression under Huber's $\\epsilon$-contamination\n  Model Abstract: We consider the non-parametric regression problem under Huber's\n$\\epsilon$-contamination model, in which an $\\epsilon$ fraction of observations\nare subject to arbitrary adversarial noise. We first show that a simple local\nbinning median step can effectively remove the adversary noise and this median\nestimator is minimax optimal up to absolute constants over the H\\\"{o}lder\nfunction class with smoothness parameters smaller than or equal to 1.\nFurthermore, when the underlying function has higher smoothness, we show that\nusing local binning median as pre-preprocessing step to remove the adversarial\nnoise, then we can apply any non-parametric estimator on top of the medians. In\nparticular we show local median binning followed by kernel smoothing and local\npolynomial regression achieve minimaxity over H\\\"{o}lder and Sobolev classes\nwith arbitrary smoothness parameters. Our main proof technique is a decoupled\nanalysis of adversary noise and stochastic noise, which can be potentially\napplied to other robust estimation problems. We also provide numerical results\nto verify the effectiveness of our proposed methods. \n\n"}
{"id": "1805.10833", "contents": "Title: Bayesian Learning with Wasserstein Barycenters Abstract: We introduce and study a novel model-selection strategy for Bayesian\nlearning, based on optimal transport, along with its associated predictive\nposterior law: the Wasserstein population barycenter of the posterior law over\nmodels. We first show how this estimator, termed Bayesian Wasserstein\nbarycenter (BWB), arises naturally in a general, parameter-free Bayesian\nmodel-selection framework, when the considered Bayesian risk is the Wasserstein\ndistance. Examples are given, illustrating how the BWB extends some classic\nparametric and non-parametric selection strategies. Furthermore, we also\nprovide explicit conditions granting the existence and statistical consistency\nof the BWB, and discuss some of its general and specific properties, providing\ninsights into its advantages compared to usual choices, such as the model\naverage estimator. Finally, we illustrate how this estimator can be computed\nusing the stochastic gradient descent (SGD) algorithm in Wasserstein space\nintroduced in a companion paper arXiv:2201.04232v2 [math.OC], and provide a\nnumerical example for experimental validation of the proposed method. \n\n"}
{"id": "1805.11022", "contents": "Title: Online Influence Maximization with Local Observations Abstract: We consider an online influence maximization problem in which a decision\nmaker selects a node among a large number of possibilities and places a piece\nof information at the node. The node transmits the information to some others\nthat are in the same connected component in a random graph. The goal of the\ndecision maker is to reach as many nodes as possible, with the added\ncomplication that feedback is only available about the degree of the selected\nnode. Our main result shows that such local observations can be sufficient for\nmaximizing global influence in two broadly studied families of random graph\nmodels: stochastic block models and Chung--Lu models. With this insight, we\npropose a bandit algorithm that aims at maximizing local (and thus global)\ninfluence, and provide its theoretical analysis in both the subcritical and\nsupercritical regimes of both considered models. Notably, our performance\nguarantees show no explicit dependence on the total number of nodes in the\nnetwork, making our approach well-suited for large-scale applications. \n\n"}
{"id": "1805.11122", "contents": "Title: Differentiable Particle Filters: End-to-End Learning with Algorithmic\n  Priors Abstract: We present differentiable particle filters (DPFs): a differentiable\nimplementation of the particle filter algorithm with learnable motion and\nmeasurement models. Since DPFs are end-to-end differentiable, we can\nefficiently train their models by optimizing end-to-end state estimation\nperformance, rather than proxy objectives such as model accuracy. DPFs encode\nthe structure of recursive state estimation with prediction and measurement\nupdate that operate on a probability distribution over states. This structure\nrepresents an algorithmic prior that improves learning performance in state\nestimation problems while enabling explainability of the learned model. Our\nexperiments on simulated and real data show substantial benefits from end-to-\nend learning with algorithmic priors, e.g. reducing error rates by ~80%. Our\nexperiments also show that, unlike long short-term memory networks, DPFs learn\nlocalization in a policy-agnostic way and thus greatly improve generalization.\nSource code is available at\nhttps://github.com/tu-rbo/differentiable-particle-filters . \n\n"}
{"id": "1805.11504", "contents": "Title: Capturing Variabilities from Computed Tomography Images with Generative\n  Adversarial Networks Abstract: With the advent of Deep Learning (DL) techniques, especially Generative\nAdversarial Networks (GANs), data augmentation and generation are quickly\nevolving domains that have raised much interest recently. However, the DL\ntechniques are data demanding and since, medical data is not easily accessible,\nthey suffer from data insufficiency. To deal with this limitation, different\ndata augmentation techniques are used. Here, we propose a novel unsupervised\ndata-driven approach for data augmentation that can generate 2D Computed\nTomography (CT) images using a simple GAN. The generated CT images have good\nglobal and local features of a real CT image and can be used to augment the\ntraining datasets for effective learning. In this proof-of-concept study, we\nshow that our proposed solution using GANs is able to capture some of the\nglobal and local CT variabilities. Our network is able to generate visually\nrealistic CT images and we aim to further enhance its output by scaling it to a\nhigher resolution and potentially from 2D to 3D. \n\n"}
{"id": "1805.11592", "contents": "Title: Playing hard exploration games by watching YouTube Abstract: Deep reinforcement learning methods traditionally struggle with tasks where\nenvironment rewards are particularly sparse. One successful method of guiding\nexploration in these domains is to imitate trajectories provided by a human\ndemonstrator. However, these demonstrations are typically collected under\nartificial conditions, i.e. with access to the agent's exact environment setup\nand the demonstrator's action and reward trajectories. Here we propose a\ntwo-stage method that overcomes these limitations by relying on noisy,\nunaligned footage without access to such data. First, we learn to map unaligned\nvideos from multiple sources to a common representation using self-supervised\nobjectives constructed over both time and modality (i.e. vision and sound).\nSecond, we embed a single YouTube video in this representation to construct a\nreward function that encourages an agent to imitate human gameplay. This method\nof one-shot imitation allows our agent to convincingly exceed human-level\nperformance on the infamously hard exploration games Montezuma's Revenge,\nPitfall! and Private Eye for the first time, even if the agent is not presented\nwith any environment rewards. \n\n"}
{"id": "1805.11593", "contents": "Title: Observe and Look Further: Achieving Consistent Performance on Atari Abstract: Despite significant advances in the field of deep Reinforcement Learning\n(RL), today's algorithms still fail to learn human-level policies consistently\nover a set of diverse tasks such as Atari 2600 games. We identify three key\nchallenges that any algorithm needs to master in order to perform well on all\ngames: processing diverse reward distributions, reasoning over long time\nhorizons, and exploring efficiently. In this paper, we propose an algorithm\nthat addresses each of these challenges and is able to learn human-level\npolicies on nearly all Atari games. A new transformed Bellman operator allows\nour algorithm to process rewards of varying densities and scales; an auxiliary\ntemporal consistency loss allows us to train stably using a discount factor of\n$\\gamma = 0.999$ (instead of $\\gamma = 0.99$) extending the effective planning\nhorizon by an order of magnitude; and we ease the exploration problem by using\nhuman demonstrations that guide the agent towards rewarding states. When tested\non a set of 42 Atari games, our algorithm exceeds the performance of an average\nhuman on 40 games using a common set of hyper parameters. Furthermore, it is\nthe first deep RL algorithm to solve the first level of Montezuma's Revenge. \n\n"}
{"id": "1805.11614", "contents": "Title: Deep Learning under Privileged Information Using Heteroscedastic Dropout Abstract: Unlike machines, humans learn through rapid, abstract model-building. The\nrole of a teacher is not simply to hammer home right or wrong answers, but\nrather to provide intuitive comments, comparisons, and explanations to a pupil.\nThis is what the Learning Under Privileged Information (LUPI) paradigm\nendeavors to model by utilizing extra knowledge only available during training.\nWe propose a new LUPI algorithm specifically designed for Convolutional Neural\nNetworks (CNNs) and Recurrent Neural Networks (RNNs). We propose to use a\nheteroscedastic dropout (i.e. dropout with a varying variance) and make the\nvariance of the dropout a function of privileged information. Intuitively, this\ncorresponds to using the privileged information to control the uncertainty of\nthe model output. We perform experiments using CNNs and RNNs for the tasks of\nimage classification and machine translation. Our method significantly\nincreases the sample efficiency during learning, resulting in higher accuracy\nwith a large margin when the number of training examples is limited. We also\ntheoretically justify the gains in sample efficiency by providing a\ngeneralization error bound decreasing with $O(\\frac{1}{n})$, where $n$ is the\nnumber of training examples, in an oracle case. \n\n"}
{"id": "1805.11686", "contents": "Title: Variational Inverse Control with Events: A General Framework for\n  Data-Driven Reward Definition Abstract: The design of a reward function often poses a major practical challenge to\nreal-world applications of reinforcement learning. Approaches such as inverse\nreinforcement learning attempt to overcome this challenge, but require expert\ndemonstrations, which can be difficult or expensive to obtain in practice. We\npropose variational inverse control with events (VICE), which generalizes\ninverse reinforcement learning methods to cases where full demonstrations are\nnot needed, such as when only samples of desired goal states are available. Our\nmethod is grounded in an alternative perspective on control and reinforcement\nlearning, where an agent's goal is to maximize the probability that one or more\nevents will happen at some point in the future, rather than maximizing\ncumulative rewards. We demonstrate the effectiveness of our methods on\ncontinuous control tasks, with a focus on high-dimensional observations like\nimages where rewards are hard or even impossible to specify. \n\n"}
{"id": "1805.11797", "contents": "Title: Grow and Prune Compact, Fast, and Accurate LSTMs Abstract: Long short-term memory (LSTM) has been widely used for sequential data\nmodeling. Researchers have increased LSTM depth by stacking LSTM cells to\nimprove performance. This incurs model redundancy, increases run-time delay,\nand makes the LSTMs more prone to overfitting. To address these problems, we\npropose a hidden-layer LSTM (H-LSTM) that adds hidden layers to LSTM's original\none level non-linear control gates. H-LSTM increases accuracy while employing\nfewer external stacked layers, thus reducing the number of parameters and\nrun-time latency significantly. We employ grow-and-prune (GP) training to\niteratively adjust the hidden layers through gradient-based growth and\nmagnitude-based pruning of connections. This learns both the weights and the\ncompact architecture of H-LSTM control gates. We have GP-trained H-LSTMs for\nimage captioning and speech recognition applications. For the NeuralTalk\narchitecture on the MSCOCO dataset, our three models reduce the number of\nparameters by 38.7x [floating-point operations (FLOPs) by 45.5x], run-time\nlatency by 4.5x, and improve the CIDEr score by 2.6. For the DeepSpeech2\narchitecture on the AN4 dataset, our two models reduce the number of parameters\nby 19.4x (FLOPs by 23.5x), run-time latency by 15.7%, and the word error rate\nfrom 12.9% to 8.7%. Thus, GP-trained H-LSTMs can be seen to be compact, fast,\nand accurate. \n\n"}
{"id": "1805.11916", "contents": "Title: On the Spectrum of Random Features Maps of High Dimensional Data Abstract: Random feature maps are ubiquitous in modern statistical machine learning,\nwhere they generalize random projections by means of powerful, yet often\ndifficult to analyze nonlinear operators. In this paper, we leverage the\n\"concentration\" phenomenon induced by random matrix theory to perform a\nspectral analysis on the Gram matrix of these random feature maps, here for\nGaussian mixture models of simultaneously large dimension and size. Our results\nare instrumental to a deeper understanding on the interplay of the nonlinearity\nand the statistics of the data, thereby allowing for a better tuning of random\nfeature-based techniques. \n\n"}
{"id": "1805.12120", "contents": "Title: On Consensus-Optimality Trade-offs in Collaborative Deep Learning Abstract: In distributed machine learning, where agents collaboratively learn from\ndiverse private data sets, there is a fundamental tension between consensus and\noptimality. In this paper, we build on recent algorithmic progresses in\ndistributed deep learning to explore various consensus-optimality trade-offs\nover a fixed communication topology. First, we propose the incremental\nconsensus-based distributed SGD (i-CDSGD) algorithm, which involves multiple\nconsensus steps (where each agent communicates information with its neighbors)\nwithin each SGD iteration. Second, we propose the generalized consensus-based\ndistributed SGD (g-CDSGD) algorithm that enables us to navigate the full\nspectrum from complete consensus (all agents agree) to complete disagreement\n(each agent converges to individual model parameters). We analytically\nestablish convergence of the proposed algorithms for strongly convex and\nnonconvex objective functions; we also analyze the momentum variants of the\nalgorithms for the strongly convex case. We support our algorithms via\nnumerical experiments, and demonstrate significant improvements over existing\nmethods for collaborative deep learning. \n\n"}
{"id": "1806.00582", "contents": "Title: Federated Learning with Non-IID Data Abstract: Federated learning enables resource-constrained edge compute devices, such as\nmobile phones and IoT devices, to learn a shared model for prediction, while\nkeeping the training data local. This decentralized approach to train models\nprovides privacy, security, regulatory and economic benefits. In this work, we\nfocus on the statistical challenge of federated learning when local data is\nnon-IID. We first show that the accuracy of federated learning reduces\nsignificantly, by up to 55% for neural networks trained for highly skewed\nnon-IID data, where each client device trains only on a single class of data.\nWe further show that this accuracy reduction can be explained by the weight\ndivergence, which can be quantified by the earth mover's distance (EMD) between\nthe distribution over classes on each device and the population distribution.\nAs a solution, we propose a strategy to improve training on non-IID data by\ncreating a small subset of data which is globally shared between all the edge\ndevices. Experiments show that accuracy can be increased by 30% for the\nCIFAR-10 dataset with only 5% globally shared data. \n\n"}
{"id": "1806.00989", "contents": "Title: Asymptotic optimality of adaptive importance sampling Abstract: Adaptive importance sampling (AIS) uses past samples to update the\n\\textit{sampling policy} $q_t$ at each stage $t$. Each stage $t$ is formed with\ntwo steps : (i) to explore the space with $n_t$ points according to $q_t$ and\n(ii) to exploit the current amount of information to update the sampling\npolicy. The very fundamental question raised in this paper concerns the\nbehavior of empirical sums based on AIS. Without making any assumption on the\nallocation policy $n_t$, the theory developed involves no restriction on the\nsplit of computational resources between the explore (i) and the exploit (ii)\nstep. It is shown that AIS is asymptotically optimal : the asymptotic behavior\nof AIS is the same as some \"oracle\" strategy that knows the targeted sampling\npolicy from the beginning. From a practical perspective, weighted AIS is\nintroduced, a new method that allows to forget poor samples from early stages. \n\n"}
{"id": "1806.01827", "contents": "Title: Performance Metric Elicitation from Pairwise Classifier Comparisons Abstract: Given a binary prediction problem, which performance metric should the\nclassifier optimize? We address this question by formalizing the problem of\nMetric Elicitation. The goal of metric elicitation is to discover the\nperformance metric of a practitioner, which reflects her innate rewards (costs)\nfor correct (incorrect) classification. In particular, we focus on eliciting\nbinary classification performance metrics from pairwise feedback, where a\npractitioner is queried to provide relative preference between two classifiers.\nBy exploiting key geometric properties of the space of confusion matrices, we\nobtain provably query efficient algorithms for eliciting linear and\nlinear-fractional performance metrics. We further show that our method is\nrobust to feedback and finite sample noise. \n\n"}
{"id": "1806.02512", "contents": "Title: Importance Weighted Generative Networks Abstract: Deep generative networks can simulate from a complex target distribution, by\nminimizing a loss with respect to samples from that distribution. However,\noften we do not have direct access to our target distribution - our data may be\nsubject to sample selection bias, or may be from a different but related\ndistribution. We present methods based on importance weighting that can\nestimate the loss with respect to a target distribution, even if we cannot\naccess that distribution directly, in a variety of settings. These estimators,\nwhich differentially weight the contribution of data to the loss function,\noffer both theoretical guarantees and impressive empirical performance. \n\n"}
{"id": "1806.02794", "contents": "Title: Unbiased Estimation of the Value of an Optimized Policy Abstract: Randomized trials, also known as A/B tests, are used to select between two\npolicies: a control and a treatment. Given a corresponding set of features, we\ncan ideally learn an optimized policy P that maps the A/B test data features to\naction space and optimizes reward. However, although A/B testing provides an\nunbiased estimator for the value of deploying B (i.e., switching from policy A\nto B), direct application of those samples to learn the the optimized policy P\ngenerally does not provide an unbiased estimator of the value of P as the\nsamples were observed when constructing P. In situations where the cost and\nrisks associated of deploying a policy are high, such an unbiased estimator is\nhighly desirable.\n  We present a procedure for learning optimized policies and getting unbiased\nestimates for the value of deploying them. We wrap any policy learning\nprocedure with a bagging process and obtain out-of-bag policy inclusion\ndecisions for each sample. We then prove that inverse-propensity-weighting\neffect estimator is unbiased when applied to the optimized subset. Likewise, we\napply the same idea to obtain out-of-bag unbiased per-sample value estimate of\nthe measurement that is independent of the randomized treatment, and use these\nestimates to build an unbiased doubly-robust effect estimator. Lastly, we\nempirically shown that even when the average treatment effect is negative we\ncan find a positive optimized policy. \n\n"}
{"id": "1806.03286", "contents": "Title: Regression with Comparisons: Escaping the Curse of Dimensionality with\n  Ordinal Information Abstract: In supervised learning, we typically leverage a fully labeled dataset to\ndesign methods for function estimation or prediction. In many practical\nsituations, we are able to obtain alternative feedback, possibly at a low cost.\nA broad goal is to understand the usefulness of, and to design algorithms to\nexploit, this alternative feedback. In this paper, we consider a\nsemi-supervised regression setting, where we obtain additional ordinal (or\ncomparison) information for the unlabeled samples. We consider ordinal feedback\nof varying qualities where we have either a perfect ordering of the samples, a\nnoisy ordering of the samples or noisy pairwise comparisons between the\nsamples. We provide a precise quantification of the usefulness of these types\nof ordinal feedback in both nonparametric and linear regression, showing that\nin many cases it is possible to accurately estimate an underlying function with\na very small labeled set, effectively \\emph{escaping the curse of\ndimensionality}. We also present lower bounds, that establish fundamental\nlimits for the task and show that our algorithms are optimal in a variety of\nsettings. Finally, we present extensive experiments on new datasets that\ndemonstrate the efficacy and practicality of our algorithms and investigate\ntheir robustness to various sources of noise and model misspecification. \n\n"}
{"id": "1806.03316", "contents": "Title: Adversarial Meta-Learning Abstract: Meta-learning enables a model to learn from very limited data to undertake a\nnew task. In this paper, we study the general meta-learning with adversarial\nsamples. We present a meta-learning algorithm, ADML (ADversarial Meta-Learner),\nwhich leverages clean and adversarial samples to optimize the initialization of\na learning model in an adversarial manner. ADML leads to the following\ndesirable properties: 1) it turns out to be very effective even in the cases\nwith only clean samples; 2) it is robust to adversarial samples, i.e., unlike\nother meta-learning algorithms, it only leads to a minor performance\ndegradation when there are adversarial samples; 3) it sheds light on tackling\nthe cases with limited and even contaminated samples. It has been shown by\nextensive experimental results that ADML consistently outperforms three\nrepresentative meta-learning algorithms in the cases involving adversarial\nsamples, on two widely-used image datasets, MiniImageNet and CIFAR100, in terms\nof both accuracy and robustness. \n\n"}
{"id": "1806.03664", "contents": "Title: Conditional Noise-Contrastive Estimation of Unnormalised Models Abstract: Many parametric statistical models are not properly normalised and only\nspecified up to an intractable partition function, which renders parameter\nestimation difficult. Examples of unnormalised models are Gibbs distributions,\nMarkov random fields, and neural network models in unsupervised deep learning.\nIn previous work, the estimation principle called noise-contrastive estimation\n(NCE) was introduced where unnormalised models are estimated by learning to\ndistinguish between data and auxiliary noise. An open question is how to best\nchoose the auxiliary noise distribution. We here propose a new method that\naddresses this issue. The proposed method shares with NCE the idea of\nformulating density estimation as a supervised learning problem but in contrast\nto NCE, the proposed method leverages the observed data when generating noise\nsamples. The noise can thus be generated in a semi-automated manner. We first\npresent the underlying theory of the new method, show that score matching\nemerges as a limiting case, validate the method on continuous and discrete\nvalued synthetic data, and show that we can expect an improved performance\ncompared to NCE when the data lie in a lower-dimensional manifold. Then we\ndemonstrate its applicability in unsupervised deep learning by estimating a\nfour-layer neural image model. \n\n"}
{"id": "1806.04119", "contents": "Title: Valid Post-selection Inference in Assumption-lean Linear Regression Abstract: Construction of valid statistical inference for estimators based on\ndata-driven selection has received a lot of attention in the recent times. Berk\net al. (2013) is possibly the first work to provide valid inference for\nGaussian homoscedastic linear regression with fixed covariates under arbitrary\ncovariate/variable selection. The setting is unrealistic and is extended by\nBachoc et al. (2016) by relaxing the distributional assumptions. A major\ndrawback of the aforementioned works is that the construction of valid\nconfidence regions is computationally intensive. In this paper, we first prove\nthat post-selection inference is equivalent to simultaneous inference and then\nconstruct valid post-selection confidence regions which are computationally\nsimple. Our construction is based on deterministic inequalities and apply to\nindependent as well as dependent random variables without the requirement of\ncorrect distributional assumptions. Finally, we compare the volume of our\nconfidence regions with the existing ones and show that under non-stochastic\ncovariates, our regions are much smaller. \n\n"}
{"id": "1806.05035", "contents": "Title: Development of probabilistic dam breach model using Bayesian inference Abstract: Dam breach models are commonly used to predict outflow hydrographs of\npotentially failing dams and are key ingredients for evaluating flood risk. In\nthis paper a new dam breach modeling framework is introduced that shall improve\nthe reliability of hydrograph predictions of homogeneous earthen embankment\ndams. Striving for a small number of parameters, the simplified physics-based\nmodel describes the processes of failing embankment dams by breach enlargement,\ndriven by progressive surface erosion. Therein the erosion rate of dam material\nis modeled by empirical sediment transport formulations. Embedding the model\ninto a Bayesian multilevel framework allows for quantitative analysis of\ndifferent categories of uncertainties. To this end, data available in\nliterature of observed peak discharge and final breach width of historical dam\nfailures was used to perform model inversion by applying Markov Chain Monte\nCarlo simulation. Prior knowledge is mainly based on non-informative\ndistribution functions. The resulting posterior distribution shows that the\nmain source of uncertainty is a correlated subset of parameters, consisting of\nthe residual error term and the epistemic term quantifying the breach erosion\nrate. The prediction intervals of peak discharge and final breach width are\ncongruent with values known from literature. To finally predict the outflow\nhydrograph for real case applications, an alternative residual model was\nformulated that assumes perfect data and a perfect model. The fully\nprobabilistic fashion of hydrograph prediction has the potential to improve the\nadequate risk management of downstream flooding. \n\n"}
{"id": "1806.05096", "contents": "Title: Introducing user-prescribed constraints in Markov chains for nonlinear\n  dimensionality reduction Abstract: Stochastic kernel based dimensionality reduction approaches have become\npopular in the last decade. The central component of many of these methods is a\nsymmetric kernel that quantifies the vicinity between pairs of data points and\na kernel-induced Markov chain on the data. Typically, the Markov chain is fully\nspecified by the kernel through row normalization. However, in many cases, it\nis desirable to impose user-specified stationary-state and dynamical\nconstraints on the Markov chain. Unfortunately, no systematic framework exists\nto impose such user-defined constraints. Here, we introduce a path entropy\nmaximization based approach to derive the transition probabilities of Markov\nchains using a kernel and additional user-specified constraints. We illustrate\nthe usefulness of these Markov chains with examples. \n\n"}
{"id": "1806.05161", "contents": "Title: Overfitting or perfect fitting? Risk bounds for classification and\n  regression rules that interpolate Abstract: Many modern machine learning models are trained to achieve zero or near-zero\ntraining error in order to obtain near-optimal (but non-zero) test error. This\nphenomenon of strong generalization performance for \"overfitted\" / interpolated\nclassifiers appears to be ubiquitous in high-dimensional data, having been\nobserved in deep networks, kernel machines, boosting and random forests. Their\nperformance is consistently robust even when the data contain large amounts of\nlabel noise.\n  Very little theory is available to explain these observations. The vast\nmajority of theoretical analyses of generalization allows for interpolation\nonly when there is little or no label noise. This paper takes a step toward a\ntheoretical foundation for interpolated classifiers by analyzing local\ninterpolating schemes, including geometric simplicial interpolation algorithm\nand singularly weighted $k$-nearest neighbor schemes. Consistency or\nnear-consistency is proved for these schemes in classification and regression\nproblems. Moreover, the nearest neighbor schemes exhibit optimal rates under\nsome standard statistical assumptions.\n  Finally, this paper suggests a way to explain the phenomenon of adversarial\nexamples, which are seemingly ubiquitous in modern machine learning, and also\ndiscusses some connections to kernel machines and random forests in the\ninterpolated regime. \n\n"}
{"id": "1806.05594", "contents": "Title: There Are Many Consistent Explanations of Unlabeled Data: Why You Should\n  Average Abstract: Presently the most successful approaches to semi-supervised learning are\nbased on consistency regularization, whereby a model is trained to be robust to\nsmall perturbations of its inputs and parameters. To understand consistency\nregularization, we conceptually explore how loss geometry interacts with\ntraining procedures. The consistency loss dramatically improves generalization\nperformance over supervised-only training; however, we show that SGD struggles\nto converge on the consistency loss and continues to make large steps that lead\nto changes in predictions on the test data. Motivated by these observations, we\npropose to train consistency-based methods with Stochastic Weight Averaging\n(SWA), a recent approach which averages weights along the trajectory of SGD\nwith a modified learning rate schedule. We also propose fast-SWA, which further\naccelerates convergence by averaging multiple points within each cycle of a\ncyclical learning rate schedule. With weight averaging, we achieve the best\nknown semi-supervised results on CIFAR-10 and CIFAR-100, over many different\nquantities of labeled training data. For example, we achieve 5.0% error on\nCIFAR-10 with only 4000 labels, compared to the previous best result in the\nliterature of 6.3%. \n\n"}
{"id": "1806.05852", "contents": "Title: Coupled conditional backward sampling particle filter Abstract: The conditional particle filter (CPF) is a promising algorithm for general\nhidden Markov model smoothing. Empirical evidence suggests that the variant of\nCPF with backward sampling (CBPF) performs well even with long time series.\nPrevious theoretical results have not been able to demonstrate the improvement\nbrought by backward sampling, whereas we provide rates showing that CBPF can\nremain effective with a fixed number of particles independent of the time\nhorizon. Our result is based on analysis of a new coupling of two CBPFs, the\ncoupled conditional backward sampling particle filter (CCBPF). We show that\nCCBPF has good stability properties in the sense that with fixed number of\nparticles, the coupling time in terms of iterations increases only linearly\nwith respect to the time horizon under a general (strong mixing) condition. The\nCCBPF is useful not only as a theoretical tool, but also as a practical method\nthat allows for unbiased estimation of smoothing expectations, following the\nrecent developments by Jacob et al. (to appear). Unbiased estimation has many\nadvantages, such as enabling the construction of asymptotically exact\nconfidence intervals and straightforward parallelisation. \n\n"}
{"id": "1806.05982", "contents": "Title: Accelerating delayed-acceptance Markov chain Monte Carlo algorithms Abstract: Delayed-acceptance Markov chain Monte Carlo (DA-MCMC) samples from a\nprobability distribution via a two-stages version of the Metropolis-Hastings\nalgorithm, by combining the target distribution with a \"surrogate\" (i.e. an\napproximate and computationally cheaper version) of said distribution. DA-MCMC\naccelerates MCMC sampling in complex applications, while still targeting the\nexact distribution. We design a computationally faster, albeit approximate,\nDA-MCMC algorithm. We consider parameter inference in a Bayesian setting where\na surrogate likelihood function is introduced in the delayed-acceptance scheme.\nWhen the evaluation of the likelihood function is computationally intensive,\nour scheme produces a 2-4 times speed-up, compared to standard DA-MCMC.\nHowever, the acceleration is highly problem dependent. Inference results for\nthe standard delayed-acceptance algorithm and our approximated version are\nsimilar, indicating that our algorithm can return reliable Bayesian inference.\nAs a computationally intensive case study, we introduce a novel stochastic\ndifferential equation model for protein folding data. \n\n"}
{"id": "1806.06028", "contents": "Title: A new characterization of the Gamma distribution and associated goodness\n  of fit tests Abstract: We propose a class of weighted $L_2$-type tests of fit to the Gamma\ndistribution. Our novel procedure is based on a fixed point property of a new\ntransformation connected to a Steinian characterization of the family of Gamma\ndistributions. We derive the weak limits of the statistic under the null\nhypothesis and under contiguous alternatives. Further, we establish the global\nconsistency of the tests and apply a parametric bootstrap technique in a Monte\nCarlo simulation study to show the competitiveness to existing procedures. \n\n"}
{"id": "1806.06384", "contents": "Title: Multi-variable LSTM neural network for autoregressive exogenous model Abstract: In this paper, we propose multi-variable LSTM capable of accurate forecasting\nand variable importance interpretation for time series with exogenous\nvariables. Current attention mechanism in recurrent neural networks mostly\nfocuses on the temporal aspect of data and falls short of characterizing\nvariable importance. To this end, the multi-variable LSTM equipped with\ntensorized hidden states is developed to learn hidden states for individual\nvariables, which give rise to our mixture temporal and variable attention.\nBased on such attention mechanism, we infer and quantify variable importance.\nExtensive experiments using real datasets with Granger-causality test and the\nsynthetic dataset with ground truth demonstrate the prediction performance and\ninterpretability of multi-variable LSTM in comparison to a variety of\nbaselines. It exhibits the prospect of multi-variable LSTM as an end-to-end\nframework for both forecasting and knowledge discovery. \n\n"}
{"id": "1806.06790", "contents": "Title: Towards Distributed Energy Services: Decentralizing Optimal Power Flow\n  with Machine Learning Abstract: The implementation of optimal power flow (OPF) methods to perform voltage and\npower flow regulation in electric networks is generally believed to require\nextensive communication. We consider distribution systems with multiple\ncontrollable Distributed Energy Resources (DERs) and present a data-driven\napproach to learn control policies for each DER to reconstruct and mimic the\nsolution to a centralized OPF problem from solely locally available\ninformation. Collectively, all local controllers closely match the centralized\nOPF solution, providing near optimal performance and satisfaction of system\nconstraints. A rate distortion framework enables the analysis of how well the\nresulting fully decentralized control policies are able to reconstruct the OPF\nsolution. The methodology provides a natural extension to decide what nodes a\nDER should communicate with to improve the reconstruction of its individual\npolicy. The method is applied on both single- and three-phase test feeder\nnetworks using data from real loads and distributed generators, focusing on\nDERs that do not exhibit inter-temporal dependencies. It provides a framework\nfor Distribution System Operators to efficiently plan and operate the\ncontributions of DERs to achieve Distributed Energy Services in distribution\nnetworks. \n\n"}
{"id": "1806.07247", "contents": "Title: Tensor-Tensor Product Toolbox Abstract: The tensor-tensor product (t-product) [M. E. Kilmer and C. D. Martin, 2011]\nis a natural generalization of matrix multiplication. Based on t-product, many\noperations on matrix can be extended to tensor cases, including tensor SVD,\ntensor spectral norm, tensor nuclear norm [C. Lu, et al., 2018] and many\nothers. The linear algebraic structure of tensors are similar to the matrix\ncases. We develop a Matlab toolbox to implement several basic operations on\ntensors based on t-product. The toolbox is available at\nhttps://github.com/canyilu/tproduct. \n\n"}
{"id": "1806.07409", "contents": "Title: Built-in Vulnerabilities to Imperceptible Adversarial Perturbations Abstract: Designing models that are robust to small adversarial perturbations of their\ninputs has proven remarkably difficult. In this work we show that the reverse\nproblem---making models more vulnerable---is surprisingly easy. After\npresenting some proofs of concept on MNIST, we introduce a generic tilting\nattack that injects vulnerabilities into the linear layers of pre-trained\nnetworks by increasing their sensitivity to components of low variance in the\ntraining data without affecting their performance on test data. We illustrate\nthis attack on a multilayer perceptron trained on SVHN and use it to design a\nstand-alone adversarial module which we call a steganogram decoder. Finally, we\nshow on CIFAR-10 that a poisoning attack with a poisoning rate as low as 0.1%\ncan induce vulnerabilities to chosen imperceptible backdoor signals in\nstate-of-the-art networks. Beyond their practical implications, these different\nresults shed new light on the nature of the adversarial example phenomenon. \n\n"}
{"id": "1806.07537", "contents": "Title: DeepAffinity: Interpretable Deep Learning of Compound-Protein Affinity\n  through Unified Recurrent and Convolutional Neural Networks Abstract: Motivation: Drug discovery demands rapid quantification of compound-protein\ninteraction (CPI). However, there is a lack of methods that can predict\ncompound-protein affinity from sequences alone with high applicability,\naccuracy, and interpretability.\n  Results: We present a seamless integration of domain knowledges and\nlearning-based approaches. Under novel representations of\nstructurally-annotated protein sequences, a semi-supervised deep learning model\nthat unifies recurrent and convolutional neural networks has been proposed to\nexploit both unlabeled and labeled data, for jointly encoding molecular\nrepresentations and predicting affinities. Our representations and models\noutperform conventional options in achieving relative error in IC$_{50}$ within\n5-fold for test cases and 20-fold for protein classes not included for\ntraining. Performances for new protein classes with few labeled data are\nfurther improved by transfer learning. Furthermore, separate and joint\nattention mechanisms are developed and embedded to our model to add to its\ninterpretability, as illustrated in case studies for predicting and explaining\nselective drug-target interactions. Lastly, alternative representations using\nprotein sequences or compound graphs and a unified RNN/GCNN-CNN model using\ngraph CNN (GCNN) are also explored to reveal algorithmic challenges ahead.\n  Availability: Data and source codes are available at\nhttps://github.com/Shen-Lab/DeepAffinity\n  Supplementary Information: Supplementary data are available at\nhttp://shen-lab.github.io/deep-affinity-bioinf18-supp-rev.pdf \n\n"}
{"id": "1806.07819", "contents": "Title: Disentangling Multiple Conditional Inputs in GANs Abstract: In this paper, we propose a method that disentangles the effects of multiple\ninput conditions in Generative Adversarial Networks (GANs). In particular, we\ndemonstrate our method in controlling color, texture, and shape of a generated\ngarment image for computer-aided fashion design. To disentangle the effect of\ninput attributes, we customize conditional GANs with consistency loss\nfunctions. In our experiments, we tune one input at a time and show that we can\nguide our network to generate novel and realistic images of clothing articles.\nIn addition, we present a fashion design process that estimates the input\nattributes of an existing garment and modifies them using our generator. \n\n"}
{"id": "1806.07863", "contents": "Title: Learning ReLU Networks via Alternating Minimization Abstract: We propose and analyze a new family of algorithms for training neural\nnetworks with ReLU activations. Our algorithms are based on the technique of\nalternating minimization: estimating the activation patterns of each ReLU for\nall given samples, interleaved with weight updates via a least-squares step.\nThe main focus of our paper are 1-hidden layer networks with $k$ hidden neurons\nand ReLU activation. We show that under standard distributional assumptions on\nthe $d-$dimensional input data, our algorithm provably recovers the true\n`ground truth' parameters in a linearly convergent fashion. This holds as long\nas the weights are sufficiently well initialized; furthermore, our method\nrequires only $n=\\widetilde{O}(dk^2)$ samples. We also analyze the special case\nof 1-hidden layer networks with skipped connections, commonly used in\nResNet-type architectures, and propose a novel initialization strategy for the\nsame. For ReLU based ResNet type networks, we provide the first linear\nconvergence guarantee with an end-to-end algorithm. We also extend this\nframework to deeper networks and empirically demonstrate its convergence to a\nglobal minimum. \n\n"}
{"id": "1806.08049", "contents": "Title: On the Robustness of Interpretability Methods Abstract: We argue that robustness of explanations---i.e., that similar inputs should\ngive rise to similar explanations---is a key desideratum for interpretability.\nWe introduce metrics to quantify robustness and demonstrate that current\nmethods do not perform well according to these metrics. Finally, we propose\nways that robustness can be enforced on existing interpretability approaches. \n\n"}
{"id": "1806.08656", "contents": "Title: Optimal size of linear matrix inequalities in semidefinite approaches to\n  polynomial optimization Abstract: The abbreviations LMI and SOS stand for `linear matrix inequality' and `sum\nof squares', respectively. The cone $\\Sigma_{n,2d}$ of SOS polynomials in $n$\nvariables of degree at most $2d$ is known to have a semidefinite extended\nformulation with one LMI of size $\\binom{n+d}{n}$. In other words,\n$\\Sigma_{n,2d}$ is a linear image of a set described by one LMI of size\n$\\binom{n+d}{n}$. We show that $\\Sigma_{n,2d}$ has no semidefinite extended\nformulation with finitely many LMIs of size less than $\\binom{n+d}{n}$. Thus,\nthe standard extended formulation of $\\Sigma_{n,2d}$ is optimal in terms of the\nsize of the LMIs. As a direct consequence, it follows that the cone of $k\n\\times k$ symmetric positive semidefinite matrices has no extended formulation\nwith finitely many LMIs of size less than $k$. We also derive analogous results\nfor further cones considered in polynomial optimization such as truncated\nquadratic modules, the cones of copositive and completely positive matrices and\nthe cone of sums of non-negative circuit polynomials. \n\n"}
{"id": "1806.08813", "contents": "Title: An Annealed Sequential Monte Carlo Method for Bayesian Phylogenetics Abstract: We describe an \"embarrassingly parallel\" method for Bayesian phylogenetic\ninference, annealed Sequential Monte Carlo, based on recent advances in the\nSequential Monte Carlo literature such as adaptive determination of annealing\nparameters. The algorithm provides an approximate posterior distribution over\ntrees and evolutionary parameters as well as an unbiased estimator for the\nmarginal likelihood. This unbiasedness property can be used for the purpose of\ntesting the correctness of posterior simulation software. We evaluate the\nperformance of phylogenetic annealed Sequential Monte Carlo by reviewing and\ncomparing with other computational Bayesian phylogenetic methods, in\nparticular, different marginal likelihood estimation methods. Unlike previous\nSequential Monte Carlo methods in phylogenetics, our annealed method can\nutilize standard Markov chain Monte Carlo tree moves and hence benefit from the\nlarge inventory of such moves available in the literature. Consequently, the\nannealed Sequential Monte Carlo method should be relatively easy to incorporate\ninto existing phylogenetic software packages based on Markov chain Monte Carlo\nalgorithms. We illustrate our method using simulation studies and real data\nanalysis. \n\n"}
{"id": "1806.08990", "contents": "Title: Stroke-based Character Reconstruction Abstract: Background elimination for noisy character images or character images from\nreal scene is still a challenging problem, due to the bewildering backgrounds,\nuneven illumination, low resolution and different distortions. We propose a\nstroke-based character reconstruction(SCR) method that use a weighted quadratic\nBezier curve(WQBC) to represent strokes of a character. Only training on our\nsynthetic data, our stroke extractor can achieve excellent reconstruction\neffect in real scenes. Meanwhile. It can also help achieve great ability in\ndefending adversarial attacks of character recognizers. \n\n"}
{"id": "1806.09141", "contents": "Title: Constructing Deep Neural Networks by Bayesian Network Structure Learning Abstract: We introduce a principled approach for unsupervised structure learning of\ndeep neural networks. We propose a new interpretation for depth and inter-layer\nconnectivity where conditional independencies in the input distribution are\nencoded hierarchically in the network structure. Thus, the depth of the network\nis determined inherently. The proposed method casts the problem of neural\nnetwork structure learning as a problem of Bayesian network structure learning.\nThen, instead of directly learning the discriminative structure, it learns a\ngenerative graph, constructs its stochastic inverse, and then constructs a\ndiscriminative graph. We prove that conditional-dependency relations among the\nlatent variables in the generative graph are preserved in the class-conditional\ndiscriminative graph. We demonstrate on image classification benchmarks that\nthe deepest layers (convolutional and dense) of common networks can be replaced\nby significantly smaller learned structures, while maintaining classification\naccuracy---state-of-the-art on tested benchmarks. Our structure learning\nalgorithm requires a small computational cost and runs efficiently on a\nstandard desktop CPU. \n\n"}
{"id": "1806.09277", "contents": "Title: Towards Optimal Transport with Global Invariances Abstract: Many problems in machine learning involve calculating correspondences between\nsets of objects, such as point clouds or images. Discrete optimal transport\nprovides a natural and successful approach to such tasks whenever the two sets\nof objects can be represented in the same space, or at least distances between\nthem can be directly evaluated. Unfortunately neither requirement is likely to\nhold when object representations are learned from data. Indeed, automatically\nderived representations such as word embeddings are typically fixed only up to\nsome global transformations, for example, reflection or rotation. As a result,\npairwise distances across two such instances are ill-defined without specifying\ntheir relative transformation. In this work, we propose a general framework for\noptimal transport in the presence of latent global transformations. We cast the\nproblem as a joint optimization over transport couplings and transformations\nchosen from a flexible class of invariances, propose algorithms to solve it,\nand show promising results in various tasks, including a popular unsupervised\nword translation benchmark. \n\n"}
{"id": "1806.09351", "contents": "Title: Multi-objective Model-based Policy Search for Data-efficient Learning\n  with Sparse Rewards Abstract: The most data-efficient algorithms for reinforcement learning in robotics are\nmodel-based policy search algorithms, which alternate between learning a\ndynamical model of the robot and optimizing a policy to maximize the expected\nreturn given the model and its uncertainties. However, the current algorithms\nlack an effective exploration strategy to deal with sparse or misleading reward\nscenarios: if they do not experience any state with a positive reward during\nthe initial random exploration, it is very unlikely to solve the problem. Here,\nwe propose a novel model-based policy search algorithm, Multi-DEX, that\nleverages a learned dynamical model to efficiently explore the task space and\nsolve tasks with sparse rewards in a few episodes. To achieve this, we frame\nthe policy search problem as a multi-objective, model-based policy optimization\nproblem with three objectives: (1) generate maximally novel state trajectories,\n(2) maximize the expected return and (3) keep the system in state-space regions\nfor which the model is as accurate as possible. We then optimize these\nobjectives using a Pareto-based multi-objective optimization algorithm. The\nexperiments show that Multi-DEX is able to solve sparse reward scenarios (with\na simulated robotic arm) in much lower interaction time than VIME, TRPO,\nGEP-PG, CMA-ES and Black-DROPS. \n\n"}
{"id": "1806.09708", "contents": "Title: Mimic and Classify : A meta-algorithm for Conditional Independence\n  Testing Abstract: Given independent samples generated from the joint distribution\n$p(\\mathbf{x},\\mathbf{y},\\mathbf{z})$, we study the problem of Conditional\nIndependence (CI-Testing), i.e., whether the joint equals the CI distribution\n$p^{CI}(\\mathbf{x},\\mathbf{y},\\mathbf{z})= p(\\mathbf{z})\np(\\mathbf{y}|\\mathbf{z})p(\\mathbf{x}|\\mathbf{z})$ or not. We cast this problem\nunder the purview of the proposed, provable meta-algorithm, \"Mimic and\nClassify\", which is realized in two-steps: (a) Mimic the CI distribution close\nenough to recover the support, and (b) Classify to distinguish the joint and\nthe CI distribution. Thus, as long as we have a good generative model and a\ngood classifier, we potentially have a sound CI Tester. With this modular\nparadigm, CI Testing becomes amiable to be handled by state-of-the-art, both\ngenerative and classification methods from the modern advances in Deep\nLearning, which in general can handle issues related to curse of dimensionality\nand operation in small sample regime. We show intensive numerical experiments\non synthetic and real datasets where new mimic methods such conditional GANs,\nRegression with Neural Nets, outperform the current best CI Testing performance\nin the literature. Our theoretical results provide analysis on the estimation\nof null distribution as well as allow for general measures, i.e., when either\nsome of the random variables are discrete and some are continuous or when one\nor more of them are discrete-continuous mixtures. \n\n"}
{"id": "1806.09710", "contents": "Title: Why Interpretability in Machine Learning? An Answer Using Distributed\n  Detection and Data Fusion Theory Abstract: As artificial intelligence is increasingly affecting all parts of society and\nlife, there is growing recognition that human interpretability of machine\nlearning models is important. It is often argued that accuracy or other similar\ngeneralization performance metrics must be sacrificed in order to gain\ninterpretability. Such arguments, however, fail to acknowledge that the overall\ndecision-making system is composed of two entities: the learned model and a\nhuman who fuses together model outputs with his or her own information. As\nsuch, the relevant performance criteria should be for the entire system, not\njust for the machine learning component. In this work, we characterize the\nperformance of such two-node tandem data fusion systems using the theory of\ndistributed detection. In doing so, we work in the population setting and model\ninterpretable learned models as multi-level quantizers. We prove that under our\nabstraction, the overall system of a human with an interpretable classifier\noutperforms one with a black box classifier. \n\n"}
{"id": "1806.10234", "contents": "Title: Scalable Gaussian Process Inference with Finite-data Mean and Variance\n  Guarantees Abstract: Gaussian processes (GPs) offer a flexible class of priors for nonparametric\nBayesian regression, but popular GP posterior inference methods are typically\nprohibitively slow or lack desirable finite-data guarantees on quality. We\ndevelop an approach to scalable approximate GP regression with finite-data\nguarantees on the accuracy of pointwise posterior mean and variance estimates.\nOur main contribution is a novel objective for approximate inference in the\nnonparametric setting: the preconditioned Fisher (pF) divergence. We show that\nunlike the Kullback--Leibler divergence (used in variational inference), the pF\ndivergence bounds the 2-Wasserstein distance, which in turn provides tight\nbounds the pointwise difference of the mean and variance functions. We\ndemonstrate that, for sparse GP likelihood approximations, we can minimize the\npF divergence efficiently. Our experiments show that optimizing the pF\ndivergence has the same computational requirements as variational sparse GPs\nwhile providing comparable empirical performance--in addition to our novel\nfinite-data quality guarantees. \n\n"}
{"id": "1806.10308", "contents": "Title: Matrix Completion from Non-Uniformly Sampled Entries Abstract: In this paper, we consider matrix completion from non-uniformly sampled\nentries including fully observed and partially observed columns. Specifically,\nwe assume that a small number of columns are randomly selected and fully\nobserved, and each remaining column is partially observed with uniform\nsampling. To recover the unknown matrix, we first recover its column space from\nthe fully observed columns. Then, for each partially observed column, we\nrecover it by finding a vector which lies in the recovered column space and\nconsists of the observed entries. When the unknown $m\\times n$ matrix is\nlow-rank, we show that our algorithm can exactly recover it from merely\n$\\Omega(rn\\ln n)$ entries, where $r$ is the rank of the matrix. Furthermore,\nfor a noisy low-rank matrix, our algorithm computes a low-rank approximation of\nthe unknown matrix and enjoys an additive error bound measured by Frobenius\nnorm. Experimental results on synthetic datasets verify our theoretical claims\nand demonstrate the effectiveness of our proposed algorithm. \n\n"}
{"id": "1806.11244", "contents": "Title: One-Shot Learning of Multi-Step Tasks from Observation via Activity\n  Localization in Auxiliary Video Abstract: Due to burdensome data requirements, learning from demonstration often falls\nshort of its promise to allow users to quickly and naturally program robots.\nDemonstrations are inherently ambiguous and incomplete, making correct\ngeneralization to unseen situations difficult without a large number of\ndemonstrations in varying conditions. By contrast, humans are often able to\nlearn complex tasks from a single demonstration (typically observations without\naction labels) by leveraging context learned over a lifetime. Inspired by this\ncapability, our goal is to enable robots to perform one-shot learning of\nmulti-step tasks from observation by leveraging auxiliary video data as\ncontext. Our primary contribution is a novel system that achieves this goal by:\n(1) using a single user-segmented demonstration to define the primitive actions\nthat comprise a task, (2) localizing additional examples of these actions in\nunsegmented auxiliary videos via a metalearning-based approach, (3) using these\nadditional examples to learn a reward function for each action, and (4)\nperforming reinforcement learning on top of the inferred reward functions to\nlearn action policies that can be combined to accomplish the task. We\nempirically demonstrate that a robot can learn multi-step tasks more\neffectively when provided auxiliary video, and that performance greatly\nimproves when localizing individual actions, compared to learning from\nunsegmented videos. \n\n"}
{"id": "1806.11382", "contents": "Title: Convergence Problems with Generative Adversarial Networks (GANs) Abstract: Generative adversarial networks (GANs) are a novel approach to generative\nmodelling, a task whose goal it is to learn a distribution of real data points.\nThey have often proved difficult to train: GANs are unlike many techniques in\nmachine learning, in that they are best described as a two-player game between\na discriminator and generator. This has yielded both unreliability in the\ntraining process, and a general lack of understanding as to how GANs converge,\nand if so, to what. The purpose of this dissertation is to provide an account\nof the theory of GANs suitable for the mathematician, highlighting both\npositive and negative results. This involves identifying the problems when\ntraining GANs, and how topological and game-theoretic perspectives of GANs have\ncontributed to our understanding and improved our techniques in recent years. \n\n"}
{"id": "1806.11544", "contents": "Title: Nonparametric learning from Bayesian models with randomized objective\n  functions Abstract: Bayesian learning is built on an assumption that the model space contains a\ntrue reflection of the data generating mechanism. This assumption is\nproblematic, particularly in complex data environments. Here we present a\nBayesian nonparametric approach to learning that makes use of statistical\nmodels, but does not assume that the model is true. Our approach has provably\nbetter properties than using a parametric model and admits a Monte Carlo\nsampling scheme that can afford massive scalability on modern computer\narchitectures. The model-based aspect of learning is particularly attractive\nfor regularizing nonparametric inference when the sample size is small, and\nalso for correcting approximate approaches such as variational Bayes (VB). We\ndemonstrate the approach on a number of examples including VB classifiers and\nBayesian random forests. \n\n"}
{"id": "1807.00420", "contents": "Title: A Piecewise Deterministic Markov Process via $(r,\\theta)$ swaps in\n  hyperspherical coordinates Abstract: Recently, a class of stochastic processes known as piecewise deterministic\nMarkov processes has been used to define continuous-time Markov chain Monte\nCarlo algorithms with a number of attractive properties, including\ncompatibility with stochastic gradients like those typically found in\noptimization and variational inference, and high efficiency on certain big data\nproblems. Not many processes in this class that are capable of targeting\narbitrary invariant distributions are currently known, and within one subclass\nall previously known processes utilize linear transition functions. In this\nwork, we derive a process whose transition function is nonlinear through\nsolving its Fokker-Planck equation in hyperspherical coordinates. We explore\nits behavior on Gaussian targets, as well as a Bayesian logistic regression\nmodel with synthetic data. We discuss implications to both the theory of\npiecewise deterministic Markov processes, and to Bayesian statisticians as well\nas physicists seeking to use them for simulation-based computation. \n\n"}
{"id": "1807.00516", "contents": "Title: Balanced Distribution Adaptation for Transfer Learning Abstract: Transfer learning has achieved promising results by leveraging knowledge from\nthe source domain to annotate the target domain which has few or none labels.\nExisting methods often seek to minimize the distribution divergence between\ndomains, such as the marginal distribution, the conditional distribution or\nboth. However, these two distances are often treated equally in existing\nalgorithms, which will result in poor performance in real applications.\nMoreover, existing methods usually assume that the dataset is balanced, which\nalso limits their performances on imbalanced tasks that are quite common in\nreal problems. To tackle the distribution adaptation problem, in this paper, we\npropose a novel transfer learning approach, named as Balanced Distribution\n\\underline{A}daptation~(BDA), which can adaptively leverage the importance of\nthe marginal and conditional distribution discrepancies, and several existing\nmethods can be treated as special cases of BDA. Based on BDA, we also propose a\nnovel Weighted Balanced Distribution Adaptation~(W-BDA) algorithm to tackle the\nclass imbalance issue in transfer learning. W-BDA not only considers the\ndistribution adaptation between domains but also adaptively changes the weight\nof each class. To evaluate the proposed methods, we conduct extensive\nexperiments on several transfer learning tasks, which demonstrate the\neffectiveness of our proposed algorithms over several state-of-the-art methods. \n\n"}
{"id": "1807.00993", "contents": "Title: Improved training of neural trans-dimensional random field language\n  models with dynamic noise-contrastive estimation Abstract: A new whole-sentence language model - neural trans-dimensional random field\nlanguage model (neural TRF LM), where sentences are modeled as a collection of\nrandom fields, and the potential function is defined by a neural network, has\nbeen introduced and successfully trained by noise-contrastive estimation (NCE).\nIn this paper, we extend NCE and propose dynamic noise-contrastive estimation\n(DNCE) to solve the two problems observed in NCE training. First, a dynamic\nnoise distribution is introduced and trained simultaneously to converge to the\ndata distribution. This helps to significantly cut down the noise sample number\nused in NCE and reduce the training cost. Second, DNCE discriminates between\nsentences generated from the noise distribution and sentences generated from\nthe interpolation of the data distribution and the noise distribution. This\nalleviates the overfitting problem caused by the sparseness of the training\nset. With DNCE, we can successfully and efficiently train neural TRF LMs on\nlarge corpus (about 0.8 billion words) with large vocabulary (about 568 K\nwords). Neural TRF LMs perform as good as LSTM LMs with less parameters and\nbeing 5x~114x faster in rescoring sentences. Interpolating neural TRF LMs with\nLSTM LMs and n-gram LMs can further reduce the error rates. \n\n"}
{"id": "1807.01126", "contents": "Title: Weakly Supervised Deep Recurrent Neural Networks for Basic Dance Step\n  Generation Abstract: Synthesizing human's movements such as dancing is a flourishing research\nfield which has several applications in computer graphics. Recent studies have\ndemonstrated the advantages of deep neural networks (DNNs) for achieving\nremarkable performance in motion and music tasks with little effort for feature\npre-processing. However, applying DNNs for generating dance to a piece of music\nis nevertheless challenging, because of 1) DNNs need to generate large\nsequences while mapping the music input, 2) the DNN needs to constraint the\nmotion beat to the music, and 3) DNNs require a considerable amount of\nhand-crafted data. In this study, we propose a weakly supervised deep recurrent\nmethod for real-time basic dance generation with audio power spectrum as input.\nThe proposed model employs convolutional layers and a multilayered Long\nShort-Term memory (LSTM) to process the audio input. Then, another deep LSTM\nlayer decodes the target dance sequence. Notably, this end-to-end approach has\n1) an auto-conditioned decode configuration that reduces accumulation of\nfeedback error of large dance sequence, 2) uses a contrastive cost function to\nregulate the mapping between the music and motion beat, and 3) trains with weak\nlabels generated from the motion beat, reducing the amount of hand-crafted\ndata. We evaluate the proposed network based on i) the similarities between\ngenerated and the baseline dancer motion with a cross entropy measure for large\ndance sequences, and ii) accurate timing between the music and motion beat with\nan F-measure. Experimental results revealed that, after training using a small\ndataset, the model generates basic dance steps with low cross entropy and\nmaintains an F-measure score similar to that of a baseline dancer. \n\n"}
{"id": "1807.01346", "contents": "Title: Finite Sample $L_2$ Bounds for Sequential Monte Carlo and Adaptive Path\n  Selection Abstract: We prove a bound on the finite sample error of sequential Monte Carlo (SMC)\non static spaces using the $L_2$ distance between interpolating distributions\nand the mixing times of Markov kernels. This result is unique in that it is the\nfirst finite sample convergence result for SMC that does not require an upper\nbound on the importance weights. Using this bound we show that careful\nselection of the interpolating distributions can lead to substantial\nimprovements in the computational complexity of the algorithm. This result also\njustifies the adaptive selection of SMC distributions using the relative\neffective sample size commonly used in the literature and we establish\nconditions guaranteeing the approximation accuracy of the adaptive SMC\napproach. We then demonstrate empirically that this procedure provides\nnearly-optimal sequences of distributions in an automatic fashion for realistic\nexamples. \n\n"}
{"id": "1807.02350", "contents": "Title: A Variational Time Series Feature Extractor for Action Prediction Abstract: We propose a Variational Time Series Feature Extractor (VTSFE), inspired by\nthe VAE-DMP model of Chen et al., to be used for action recognition and\nprediction. Our method is based on variational autoencoders. It improves\nVAE-DMP in that it has a better noise inference model, a simpler transition\nmodel constraining the acceleration in the trajectories of the latent space,\nand a tighter lower bound for the variational inference. We apply the method\nfor classification and prediction of whole-body movements on a dataset with 7\ntasks and 10 demonstrations per task, recorded with a wearable motion capture\nsuit. The comparison with VAE and VAE-DMP suggests the better performance of\nour method for feature extraction. An open-source software implementation of\neach method with TensorFlow is also provided. In addition, a more detailed\nversion of this work can be found in the indicated code repository. Although it\nwas meant to, the VTSFE hasn't been tested for action prediction, due to a lack\nof time in the context of Maxime Chaveroche's Master thesis at INRIA. \n\n"}
{"id": "1807.02835", "contents": "Title: Polytope volume by descent in the face lattice and applications in\n  social choice Abstract: We describe the computation of polytope volumes by descent in the face\nlattice, its implementation in Normaliz, and the connection to\nreverse-lexicographic triangulations. The efficiency of the algorithm is\ndemonstrated by several high dimensional polytopes of different\ncharacteristics. Finally, we present an application to voting theory where\npolytope volumes appear as probabilities of certain paradoxa. \n\n"}
{"id": "1807.02857", "contents": "Title: Learning The Sequential Temporal Information with Recurrent Neural\n  Networks Abstract: Recurrent Networks are one of the most powerful and promising artificial\nneural network algorithms to processing the sequential data such as natural\nlanguages, sound, time series data. Unlike traditional feed-forward network,\nRecurrent Network has a inherent feed back loop that allows to store the\ntemporal context information and pass the state of information to the entire\nsequences of the events. This helps to achieve the state of art performance in\nmany important tasks such as language modeling, stock market prediction, image\ncaptioning, speech recognition, machine translation and object tracking etc.,\nHowever, training the fully connected RNN and managing the gradient flow are\nthe complicated process. Many studies are carried out to address the mentioned\nlimitation. This article is intent to provide the brief details about recurrent\nneurons, its variances and trips & tricks to train the fully recurrent neural\nnetwork. This review work is carried out as a part of our IPO studio software\nmodule 'Multiple Object Tracking'. \n\n"}
{"id": "1807.03146", "contents": "Title: Discovery of Latent 3D Keypoints via End-to-end Geometric Reasoning Abstract: This paper presents KeypointNet, an end-to-end geometric reasoning framework\nto learn an optimal set of category-specific 3D keypoints, along with their\ndetectors. Given a single image, KeypointNet extracts 3D keypoints that are\noptimized for a downstream task. We demonstrate this framework on 3D pose\nestimation by proposing a differentiable objective that seeks the optimal set\nof keypoints for recovering the relative pose between two views of an object.\nOur model discovers geometrically and semantically consistent keypoints across\nviewing angles and instances of an object category. Importantly, we find that\nour end-to-end framework using no ground-truth keypoint annotations outperforms\na fully supervised baseline using the same neural network architecture on the\ntask of pose estimation. The discovered 3D keypoints on the car, chair, and\nplane categories of ShapeNet are visualized at http://keypointnet.github.io/. \n\n"}
{"id": "1807.03247", "contents": "Title: An Intriguing Failing of Convolutional Neural Networks and the CoordConv\n  Solution Abstract: Few ideas have enjoyed as large an impact on deep learning as convolution.\nFor any problem involving pixels or spatial representations, common intuition\nholds that convolutional neural networks may be appropriate. In this paper we\nshow a striking counterexample to this intuition via the seemingly trivial\ncoordinate transform problem, which simply requires learning a mapping between\ncoordinates in (x,y) Cartesian space and one-hot pixel space. Although\nconvolutional networks would seem appropriate for this task, we show that they\nfail spectacularly. We demonstrate and carefully analyze the failure first on a\ntoy problem, at which point a simple fix becomes obvious. We call this solution\nCoordConv, which works by giving convolution access to its own input\ncoordinates through the use of extra coordinate channels. Without sacrificing\nthe computational and parametric efficiency of ordinary convolution, CoordConv\nallows networks to learn either complete translation invariance or varying\ndegrees of translation dependence, as required by the end task. CoordConv\nsolves the coordinate transform problem with perfect generalization and 150\ntimes faster with 10--100 times fewer parameters than convolution. This stark\ncontrast raises the question: to what extent has this inability of convolution\npersisted insidiously inside other tasks, subtly hampering performance from\nwithin? A complete answer to this question will require further investigation,\nbut we show preliminary evidence that swapping convolution for CoordConv can\nimprove models on a diverse set of tasks. Using CoordConv in a GAN produced\nless mode collapse as the transform between high-level spatial latents and\npixels becomes easier to learn. A Faster R-CNN detection model trained on MNIST\nshowed 24% better IOU when using CoordConv, and in the RL domain agents playing\nAtari games benefit significantly from the use of CoordConv layers. \n\n"}
{"id": "1807.03521", "contents": "Title: Privacy and Fairness in Recommender Systems via Adversarial Training of\n  User Representations Abstract: Latent factor models for recommender systems represent users and items as low\ndimensional vectors. Privacy risks of such systems have previously been studied\nmostly in the context of recovery of personal information in the form of usage\nrecords from the training data. However, the user representations themselves\nmay be used together with external data to recover private user information\nsuch as gender and age. In this paper we show that user vectors calculated by a\ncommon recommender system can be exploited in this way. We propose the\nprivacy-adversarial framework to eliminate such leakage of private information,\nand study the trade-off between recommender performance and leakage both\ntheoretically and empirically using a benchmark dataset. An advantage of the\nproposed method is that it also helps guarantee fairness of results, since all\nimplicit knowledge of a set of attributes is scrubbed from the representations\nused by the model, and thus can't enter into the decision making. We discuss\nfurther applications of this method towards the generation of deeper and more\ninsightful recommendations. \n\n"}
{"id": "1807.03523", "contents": "Title: DLOPT: Deep Learning Optimization Library Abstract: Deep learning hyper-parameter optimization is a tough task. Finding an\nappropriate network configuration is a key to success, however most of the\ntimes this labor is roughly done. In this work we introduce a novel library to\ntackle this problem, the Deep Learning Optimization Library: DLOPT. We briefly\ndescribe its architecture and present a set of use examples. This is an open\nsource project developed under the GNU GPL v3 license and it is freely\navailable at https://github.com/acamero/dlopt \n\n"}
{"id": "1807.03756", "contents": "Title: Latent Alignment and Variational Attention Abstract: Neural attention has become central to many state-of-the-art models in\nnatural language processing and related domains. Attention networks are an\neasy-to-train and effective method for softly simulating alignment; however,\nthe approach does not marginalize over latent alignments in a probabilistic\nsense. This property makes it difficult to compare attention to other alignment\napproaches, to compose it with probabilistic models, and to perform posterior\ninference conditioned on observed data. A related latent approach, hard\nattention, fixes these issues, but is generally harder to train and less\naccurate. This work considers variational attention networks, alternatives to\nsoft and hard attention for learning latent variable alignment models, with\ntighter approximation bounds based on amortized variational inference. We\nfurther propose methods for reducing the variance of gradients to make these\napproaches computationally feasible. Experiments show that for machine\ntranslation and visual question answering, inefficient exact latent variable\nmodels outperform standard neural attention, but these gains go away when using\nhard attention based training. On the other hand, variational attention retains\nmost of the performance gain but with training speed comparable to neural\nattention. \n\n"}
{"id": "1807.04119", "contents": "Title: Exploiting statistical dependencies of time series with hierarchical\n  correlation reconstruction Abstract: While we are usually focused on forecasting future values of time series, it\nis often valuable to additionally predict their entire probability\ndistributions, e.g. to evaluate risk, Monte Carlo simulations. On example of\ntime series of $\\approx$ 30000 Dow Jones Industrial Averages, there will be\npresented application of hierarchical correlation reconstruction for this\npurpose: MSE estimating polynomial as joint density for (current value,\ncontext), where context is for example a few previous values. Then substituting\nthe currently observed context and normalizing density to 1, we get predicted\nprobability distribution for the current value. In contrast to standard machine\nlearning approaches like neural networks, optimal polynomial coefficients here\nhave inexpensive direct formula, have controllable accuracy, are unique and\nindependently calculated, each has a specific cumulant-like interpretation, and\nsuch approximation can asymptotically approach complete description of any real\njoint distribution - providing universal tool to quantitatively describe and\nexploit statistical dependencies in time series, systematically enhancing\nARMA/ARCH-like approaches, also based on different distributions than Gaussian\nwhich turns out improper for daily log returns. There is also discussed\napplication for non-stationary time series like calculating linear time trend,\nor adapting coefficients to local statistical behavior. \n\n"}
{"id": "1807.04272", "contents": "Title: Towards a Complete Picture of Stationary Covariance Functions on Spheres\n  Cross Time Abstract: With the advent of wide-spread global and continental-scale spatiotemporal\ndatasets, increased attention has been given to covariance functions on spheres\nover time. This paper provides results for stationary covariance functions of\nrandom fields defined over $d$-dimensional spheres cross time. Specifically, we\nprovide a bridge between the characterization in \\cite{berg-porcu} for\ncovariance functions on spheres cross time and Gneiting's lemma\n\\citep{gneiting2002} that deals with planar surfaces.\n  We then prove that there is a valid class of covariance functions similar in\nform to the Gneiting class of space-time covariance functions\n\\citep{gneiting2002} that replaces the squared Euclidean distance with the\ngreat circle distance. Notably, the provided class is shown to be positive\ndefinite on every $d$-dimensional sphere cross time, while the Gneiting class\nis positive definite over $\\R^d \\times \\R$ for fixed $d$ only.\n  In this context, we illustrate the value of our adapted Gneiting class by\ncomparing examples from this class to currently established nonseparable\ncovariance classes using out-of-sample predictive criteria. These comparisons\nare carried out on two climate reanalysis datasets from the National Centers\nfor Environmental Prediction and National Center for Atmospheric Research. For\nthese datasets, we show that examples from our covariance class have better\npredictive performance than competing models. \n\n"}
{"id": "1807.04489", "contents": "Title: Fast yet Simple Natural-Gradient Descent for Variational Inference in\n  Complex Models Abstract: Bayesian inference plays an important role in advancing machine learning, but\nfaces computational challenges when applied to complex models such as deep\nneural networks. Variational inference circumvents these challenges by\nformulating Bayesian inference as an optimization problem and solving it using\ngradient-based optimization. In this paper, we argue in favor of\nnatural-gradient approaches which, unlike their gradient-based counterparts,\ncan improve convergence by exploiting the information geometry of the\nsolutions. We show how to derive fast yet simple natural-gradient updates by\nusing a duality associated with exponential-family distributions. An attractive\nfeature of these methods is that, by using natural-gradients, they are able to\nextract accurate local approximations for individual model components. We\nsummarize recent results for Bayesian deep learning showing the superiority of\nnatural-gradient approaches over their gradient counterparts. \n\n"}
{"id": "1807.06160", "contents": "Title: Layer-wise Relevance Propagation for Explainable Recommendations Abstract: In this paper, we tackle the problem of explanations in a deep-learning based\nmodel for recommendations by leveraging the technique of layer-wise relevance\npropagation. We use a Deep Convolutional Neural Network to extract relevant\nfeatures from the input images before identifying similarity between the images\nin feature space. Relationships between the images are identified by the model\nand layer-wise relevance propagation is used to infer pixel-level details of\nthe images that may have significantly informed the model's choice. We evaluate\nour method on an Amazon products dataset and demonstrate the efficacy of our\napproach. \n\n"}
{"id": "1807.06919", "contents": "Title: Backplay: \"Man muss immer umkehren\" Abstract: Model-free reinforcement learning (RL) requires a large number of trials to\nlearn a good policy, especially in environments with sparse rewards. We explore\na method to improve the sample efficiency when we have access to\ndemonstrations. Our approach, Backplay, uses a single demonstration to\nconstruct a curriculum for a given task. Rather than starting each training\nepisode in the environment's fixed initial state, we start the agent near the\nend of the demonstration and move the starting point backwards during the\ncourse of training until we reach the initial state. Our contributions are that\nwe analytically characterize the types of environments where Backplay can\nimprove training speed, demonstrate the effectiveness of Backplay both in large\ngrid worlds and a complex four player zero-sum game (Pommerman), and show that\nBackplay compares favorably to other competitive methods known to improve\nsample efficiency. This includes reward shaping, behavioral cloning, and\nreverse curriculum generation. \n\n"}
{"id": "1807.07282", "contents": "Title: Anomaly Detection for Water Treatment System based on Neural Network\n  with Automatic Architecture Optimization Abstract: We continue to develop our neural network (NN) based forecasting approach to\nanomaly detection (AD) using the Secure Water Treatment (SWaT) industrial\ncontrol system (ICS) testbed dataset. We propose genetic algorithms (GA) to\nfind the best NN architecture for a given dataset, using the NAB metric to\nassess the quality of different architectures. The drawbacks of the F1-metric\nare analyzed. Several techniques are proposed to improve the quality of AD:\nexponentially weighted smoothing, mean p-powered error measure, individual\nerror weight for each variable, disjoint prediction windows. Based on the\ntechniques used, an approach to anomaly interpretation is introduced. \n\n"}
{"id": "1807.08383", "contents": "Title: PaloBoost: An Overfitting-robust TreeBoost with Out-of-Bag Sample\n  Regularization Techniques Abstract: Stochastic Gradient TreeBoost is often found in many winning solutions in\npublic data science challenges. Unfortunately, the best performance requires\nextensive parameter tuning and can be prone to overfitting. We propose\nPaloBoost, a Stochastic Gradient TreeBoost model that uses novel regularization\ntechniques to guard against overfitting and is robust to parameter settings.\nPaloBoost uses the under-utilized out-of-bag samples to perform gradient-aware\npruning and estimate adaptive learning rates. Unlike other Stochastic Gradient\nTreeBoost models that use the out-of-bag samples to estimate test errors,\nPaloBoost treats the samples as a second batch of training samples to prune the\ntrees and adjust the learning rates. As a result, PaloBoost can dynamically\nadjust tree depths and learning rates to achieve faster learning at the start\nand slower learning as the algorithm converges. We illustrate how these\nregularization techniques can be efficiently implemented and propose a new\nformula for calculating feature importance to reflect the node coverages and\nlearning rates. Extensive experimental results on seven datasets demonstrate\nthat PaloBoost is robust to overfitting, is less sensitivity to the parameters,\nand can also effectively identify meaningful features. \n\n"}
{"id": "1807.08716", "contents": "Title: NullaNet: Training Deep Neural Networks for Reduced-Memory-Access\n  Inference Abstract: Deep neural networks have been successfully deployed in a wide variety of\napplications including computer vision and speech recognition. However,\ncomputational and storage complexity of these models has forced the majority of\ncomputations to be performed on high-end computing platforms or on the cloud.\nTo cope with computational and storage complexity of these models, this paper\npresents a training method that enables a radically different approach for\nrealization of deep neural networks through Boolean logic minimization. The\naforementioned realization completely removes the energy-hungry step of\naccessing memory for obtaining model parameters, consumes about two orders of\nmagnitude fewer computing resources compared to realizations that use\nfloatingpoint operations, and has a substantially lower latency. \n\n"}
{"id": "1807.09657", "contents": "Title: A computational geometry method for the inverse scattering problem Abstract: In this paper we demonstrate a computational method to solve the inverse\nscattering problem for a star-shaped, smooth, penetrable obstacle in 2D. Our\nmethod is based on classical ideas from computational geometry. First, we\napproximate the support of a scatterer by a point cloud. Secondly, we use the\nBayesian paradigm to model the joint conditional probability distribution of\nthe non-convex hull of the point cloud and the constant refractive index of the\nscatterer given near field data. Of note, we use the non-convex hull of the\npoint cloud as spline control points to evaluate, on a finer mesh, the volume\npotential arising in the integral equation formulation of the direct problem.\nFinally, in order to sample the arising posterior distribution, we propose a\nprobability transition kernel that commutes with affine transformations of\nspace. Our findings indicate that our method is reliable to retrieve the\nsupport and constant refractive index of the scatterer simultaneously. Indeed,\nour sampling method is robust to estimate a quantity of interest such as the\narea of the scatterer. We conclude pointing out a series of generalizations of\nour method. \n\n"}
{"id": "1807.09751", "contents": "Title: Multi-Perspective Neural Architecture for Recommendation System Abstract: Currently, there starts a research trend to leverage neural architecture for\nrecommendation systems. Though several deep recommender models are proposed,\nmost methods are too simple to characterize users' complex preference. In this\npaper, for a fine-grain analysis, users' ratings are explained from multiple\nperspectives, based on which, we propose our neural architecture. Specifically,\nour model employs several sequential stages to encode the user and item into\nhidden representations. In one stage, the user and item are represented from\nmultiple perspectives and in each perspective, the representations of user and\nitem put attentions to each other. Last, we metric the output representations\nof final stage to approach the users' rating. Extensive experiments demonstrate\nthat our method achieves substantial improvements against baselines. \n\n"}
{"id": "1807.10259", "contents": "Title: Unbiased inference for discretely observed hidden Markov model\n  diffusions Abstract: We develop a Bayesian inference method for diffusions observed discretely and\nwith noise, which is free of discretisation bias. Unlike existing unbiased\ninference methods, our method does not rely on exact simulation techniques.\nInstead, our method uses standard time-discretised approximations of\ndiffusions, such as the Euler--Maruyama scheme. Our approach is based on\nparticle marginal Metropolis--Hastings, a particle filter, randomised\nmultilevel Monte Carlo, and importance sampling type correction of approximate\nMarkov chain Monte Carlo. The resulting estimator leads to inference without a\nbias from the time-discretisation as the number of Markov chain iterations\nincreases. We give convergence results and recommend allocations for algorithm\ninputs. Our method admits a straightforward parallelisation, and can be\ncomputationally efficient. The user-friendly approach is illustrated on three\nexamples, where the underlying diffusion is an Ornstein--Uhlenbeck process, a\ngeometric Brownian motion, and a 2d non-reversible Langevin equation. \n\n"}
{"id": "1807.10675", "contents": "Title: Resource-Size matters: Improving Neural Named Entity Recognition with\n  Optimized Large Corpora Abstract: This study improves the performance of neural named entity recognition by a\nmargin of up to 11% in F-score on the example of a low-resource language like\nGerman, thereby outperforming existing baselines and establishing a new\nstate-of-the-art on each single open-source dataset. Rather than designing\ndeeper and wider hybrid neural architectures, we gather all available resources\nand perform a detailed optimization and grammar-dependent morphological\nprocessing consisting of lemmatization and part-of-speech tagging prior to\nexposing the raw data to any training process. We test our approach in a\nthreefold monolingual experimental setup of a) single, b) joint, and c)\noptimized training and shed light on the dependency of downstream-tasks on the\nsize of corpora used to compute word embeddings. \n\n"}
{"id": "1808.00783", "contents": "Title: The Quest for the Golden Activation Function Abstract: Deep Neural Networks have been shown to be beneficial for a variety of tasks,\nin particular allowing for end-to-end learning and reducing the requirement for\nmanual design decisions. However, still many parameters have to be chosen in\nadvance, also raising the need to optimize them. One important, but often\nignored system parameter is the selection of a proper activation function.\nThus, in this paper we target to demonstrate the importance of activation\nfunctions in general and show that for different tasks different activation\nfunctions might be meaningful. To avoid the manual design or selection of\nactivation functions, we build on the idea of genetic algorithms to learn the\nbest activation function for a given task. In addition, we introduce two new\nactivation functions, ELiSH and HardELiSH, which can easily be incorporated in\nour framework. In this way, we demonstrate for three different image\nclassification benchmarks that different activation functions are learned, also\nshowing improved results compared to typically used baselines. \n\n"}
{"id": "1808.01876", "contents": "Title: An Efficient Deep Reinforcement Learning Model for Urban Traffic Control Abstract: Urban Traffic Control (UTC) plays an essential role in Intelligent\nTransportation System (ITS) but remains difficult. Since model-based UTC\nmethods may not accurately describe the complex nature of traffic dynamics in\nall situations, model-free data-driven UTC methods, especially reinforcement\nlearning (RL) based UTC methods, received increasing interests in the last\ndecade. However, existing DL approaches did not propose an efficient algorithm\nto solve the complicated multiple intersections control problems whose\nstate-action spaces are vast. To solve this problem, we propose a Deep\nReinforcement Learning (DRL) algorithm that combines several tricks to master\nan appropriate control strategy within an acceptable time. This new algorithm\nrelaxes the fixed traffic demand pattern assumption and reduces human invention\nin parameter tuning. Simulation experiments have shown that our method\noutperforms traditional rule-based approaches and has the potential to handle\nmore complex traffic problems in the real world. \n\n"}
{"id": "1808.01990", "contents": "Title: Hashing with Binary Matrix Pursuit Abstract: We propose theoretical and empirical improvements for two-stage hashing\nmethods. We first provide a theoretical analysis on the quality of the binary\ncodes and show that, under mild assumptions, a residual learning scheme can\nconstruct binary codes that fit any neighborhood structure with arbitrary\naccuracy. Secondly, we show that with high-capacity hash functions such as\nCNNs, binary code inference can be greatly simplified for many standard\nneighborhood definitions, yielding smaller optimization problems and more\nrobust codes. Incorporating our findings, we propose a novel two-stage hashing\nmethod that significantly outperforms previous hashing studies on widely used\nimage retrieval benchmarks. \n\n"}
{"id": "1808.02129", "contents": "Title: Probabilistic Causal Analysis of Social Influence Abstract: Mastering the dynamics of social influence requires separating, in a database\nof information propagation traces, the genuine causal processes from temporal\ncorrelation, i.e., homophily and other spurious causes. However, most studies\nto characterize social influence, and, in general, most data-science analyses\nfocus on correlations, statistical independence, or conditional independence.\nOnly recently, there has been a resurgence of interest in \"causal data\nscience\", e.g., grounded on causality theories. In this paper we adopt a\nprincipled causal approach to the analysis of social influence from\ninformation-propagation data, rooted in the theory of probabilistic causation.\n  Our approach consists of two phases. In the first one, in order to avoid the\npitfalls of misinterpreting causation when the data spans a mixture of several\nsubtypes (\"Simpson's paradox\"), we partition the set of propagation traces into\ngroups, in such a way that each group is as less contradictory as possible in\nterms of the hierarchical structure of information propagation. To achieve this\ngoal, we borrow the notion of \"agony\" and define the Agony-bounded Partitioning\nproblem, which we prove being hard, and for which we develop two efficient\nalgorithms with approximation guarantees. In the second phase, for each group\nfrom the first phase, we apply a constrained MLE approach to ultimately learn a\nminimal causal topology. Experiments on synthetic data show that our method is\nable to retrieve the genuine causal arcs w.r.t. a ground-truth generative\nmodel. Experiments on real data show that, by focusing only on the extracted\ncausal structures instead of the whole social graph, the effectiveness of\npredicting influence spread is significantly improved. \n\n"}
{"id": "1808.03030", "contents": "Title: Policy Optimization as Wasserstein Gradient Flows Abstract: Policy optimization is a core component of reinforcement learning (RL), and\nmost existing RL methods directly optimize parameters of a policy based on\nmaximizing the expected total reward, or its surrogate. Though often achieving\nencouraging empirical success, its underlying mathematical principle on {\\em\npolicy-distribution} optimization is unclear. We place policy optimization into\nthe space of probability measures, and interpret it as Wasserstein gradient\nflows. On the probability-measure space, under specified circumstances, policy\noptimization becomes a convex problem in terms of distribution optimization. To\nmake optimization feasible, we develop efficient algorithms by numerically\nsolving the corresponding discrete gradient flows. Our technique is applicable\nto several RL settings, and is related to many state-of-the-art\npolicy-optimization algorithms. Empirical results verify the effectiveness of\nour framework, often obtaining better performance compared to related\nalgorithms. \n\n"}
{"id": "1808.03215", "contents": "Title: Scalable Gaussian Process Computations Using Hierarchical Matrices Abstract: We present a kernel-independent method that applies hierarchical matrices to\nthe problem of maximum likelihood estimation for Gaussian processes. The\nproposed approximation provides natural and scalable stochastic estimators for\nits gradient and Hessian, as well as the expected Fisher information matrix,\nthat are computable in quasilinear $O(n \\log^2 n)$ complexity for a large range\nof models. To accomplish this, we (i) choose a specific hierarchical\napproximation for covariance matrices that enables the computation of their\nexact derivatives and (ii) use a stabilized form of the Hutchinson stochastic\ntrace estimator. Since both the observed and expected information matrices can\nbe computed in quasilinear complexity, covariance matrices for MLEs can also be\nestimated efficiently. After discussing the associated mathematics, we\ndemonstrate the scalability of the method, discuss details of its\nimplementation, and validate that the resulting MLEs and confidence intervals\nbased on the inverse Fisher information matrix faithfully approach those\nobtained by the exact likelihood. \n\n"}
{"id": "1808.03239", "contents": "Title: Simple Conditions for Metastability of Continuous Markov Chains Abstract: A family $\\{Q_{\\beta}\\}_{\\beta \\geq 0}$ of Markov chains is said to exhibit\n$\\textit{metastable mixing}$ with $\\textit{modes}$\n$S_{\\beta}^{(1)},\\ldots,S_{\\beta}^{(k)}$ if its spectral gap (or some other\nmixing property) is very close to the worst conductance\n$\\min(\\Phi_{\\beta}(S_{\\beta}^{(1)}), \\ldots, \\Phi_{\\beta}(S_{\\beta}^{(k)}))$ of\nits modes. We give simple sufficient conditions for a family of Markov chains\nto exhibit metastability in this sense, and verify that these conditions hold\nfor a prototypical Metropolis-Hastings chain targeting a mixture distribution.\nOur work differs from existing work on metastability in that, for the class of\nexamples we are interested in, it gives an asymptotically exact formula for the\nspectral gap (rather than a bound that can be very far from sharp) while at the\nsame time giving technical conditions that are easier to verify for many\nstatistical examples. Our bounds from this paper are used in a companion paper\nto compare the mixing times of the Hamiltonian Monte Carlo algorithm and a\nrandom walk algorithm for multimodal target distributions. \n\n"}
{"id": "1808.03333", "contents": "Title: Linked Causal Variational Autoencoder for Inferring Paired Spillover\n  Effects Abstract: Modeling spillover effects from observational data is an important problem in\neconomics, business, and other fields of research. % It helps us infer the\ncausality between two seemingly unrelated set of events. For example, if\nconsumer spending in the United States declines, it has spillover effects on\neconomies that depend on the U.S. as their largest export market. In this\npaper, we aim to infer the causation that results in spillover effects between\npairs of entities (or units), we call this effect as \\textit{paired spillover}.\nTo achieve this, we leverage the recent developments in variational inference\nand deep learning techniques to propose a generative model called Linked Causal\nVariational Autoencoder (LCVA). Similar to variational autoencoders (VAE), LCVA\nincorporates an encoder neural network to learn the latent attributes and a\ndecoder network to reconstruct the inputs. However, unlike VAE, LCVA treats the\n\\textit{latent attributes as confounders that are assumed to affect both the\ntreatment and the outcome of units}. Specifically, given a pair of units $u$\nand $\\bar{u}$, their individual treatment and outcomes, the encoder network of\nLCVA samples the confounders by conditioning on the observed covariates of $u$,\nthe treatments of both $u$ and $\\bar{u}$ and the outcome of $u$. Once inferred,\nthe latent attributes (or confounders) of $u$ captures the spillover effect of\n$\\bar{u}$ on $u$. Using a network of users from job training dataset (LaLonde\n(1986)) and co-purchase dataset from Amazon e-commerce domain, we show that\nLCVA is significantly more robust than existing methods in capturing spillover\neffects. \n\n"}
{"id": "1808.03873", "contents": "Title: A Consistent Method for Learning OOMs from Asymptotically Stationary\n  Time Series Data Containing Missing Values Abstract: In the traditional framework of spectral learning of stochastic time series\nmodels, model parameters are estimated based on trajectories of fully recorded\nobservations. However, real-world time series data often contain missing\nvalues, and worse, the distributions of missingness events over time are often\nnot independent of the visible process. Recently, a spectral OOM learning\nalgorithm for time series with missing data was introduced and proved to be\nconsistent, albeit under quite strong conditions. Here we refine the algorithm\nand prove that the original strong conditions can be very much relaxed. We\nvalidate our theoretical findings by numerical experiments, showing that the\nalgorithm can consistently handle missingness patterns whose dynamic interacts\nwith the visible process. \n\n"}
{"id": "1808.04216", "contents": "Title: Effective Unsupervised Author Disambiguation with Relative Frequencies Abstract: This work addresses the problem of author name homonymy in the Web of\nScience. Aiming for an efficient, simple and straightforward solution, we\nintroduce a novel probabilistic similarity measure for author name\ndisambiguation based on feature overlap. Using the researcher-ID available for\na subset of the Web of Science, we evaluate the application of this measure in\nthe context of agglomeratively clustering author mentions. We focus on a\nconcise evaluation that shows clearly for which problem setups and at which\ntime during the clustering process our approach works best. In contrast to most\nother works in this field, we are sceptical towards the performance of author\nname disambiguation methods in general and compare our approach to the trivial\nsingle-cluster baseline. Our results are presented separately for each correct\nclustering size as we can explain that, when treating all cases together, the\ntrivial baseline and more sophisticated approaches are hardly distinguishable\nin terms of evaluation results. Our model shows state-of-the-art performance\nfor all correct clustering sizes without any discriminative training and with\ntuning only one convergence parameter. \n\n"}
{"id": "1808.04299", "contents": "Title: Randomized Hamiltonian Monte Carlo as Scaling Limit of the Bouncy\n  Particle Sampler and Dimension-Free Convergence Rates Abstract: The Bouncy Particle Sampler is a Markov chain Monte Carlo method based on a\nnonreversible piecewise deterministic Markov process. In this scheme, a\nparticle explores the state space of interest by evolving according to a linear\ndynamics which is altered by bouncing on the hyperplane tangent to the gradient\nof the negative log-target density at the arrival times of an inhomogeneous\nPoisson Process (PP) and by randomly perturbing its velocity at the arrival\ntimes of an homogeneous PP. Under regularity conditions, we show here that the\nprocess corresponding to the first component of the particle and its\ncorresponding velocity converges weakly towards a Randomized Hamiltonian Monte\nCarlo (RHMC) process as the dimension of the ambient space goes to infinity.\nRHMC is another piecewise deterministic non-reversible Markov process where a\nHamiltonian dynamics is altered at the arrival times of a homogeneous PP by\nrandomly perturbing the momentum component. We then establish dimension-free\nconvergence rates for RHMC for strongly log-concave targets with bounded\nHessians using coupling ideas and hypocoercivity techniques. \n\n"}
{"id": "1808.04759", "contents": "Title: An Overview and a Benchmark of Active Learning for Outlier Detection\n  with One-Class Classifiers Abstract: Active learning methods increase classification quality by means of user\nfeedback. An important subcategory is active learning for outlier detection\nwith one-class classifiers. While various methods in this category exist,\nselecting one for a given application scenario is difficult. This is because\nexisting methods rely on different assumptions, have different objectives, and\noften are tailored to a specific use case. All this calls for a comprehensive\ncomparison, the topic of this article. This article starts with a\ncategorization of the various methods. We then propose ways to evaluate active\nlearning results. Next, we run extensive experiments to compare existing\nmethods, for a broad variety of scenarios. Based on our results, we formulate\nguidelines on how to select active learning methods for outlier detection with\none-class classifiers. \n\n"}
{"id": "1808.05535", "contents": "Title: Combining time-series and textual data for taxi demand prediction in\n  event areas: a deep learning approach Abstract: Accurate time-series forecasting is vital for numerous areas of application\nsuch as transportation, energy, finance, economics, etc. However, while modern\ntechniques are able to explore large sets of temporal data to build forecasting\nmodels, they typically neglect valuable information that is often available\nunder the form of unstructured text. Although this data is in a radically\ndifferent format, it often contains contextual explanations for many of the\npatterns that are observed in the temporal data. In this paper, we propose two\ndeep learning architectures that leverage word embeddings, convolutional layers\nand attention mechanisms for combining text information with time-series data.\nWe apply these approaches for the problem of taxi demand forecasting in event\nareas. Using publicly available taxi data from New York, we empirically show\nthat by fusing these two complementary cross-modal sources of information, the\nproposed models are able to significantly reduce the error in the forecasts. \n\n"}
{"id": "1808.05884", "contents": "Title: An Empirical Evaluation of the Approximation of Subjective Logic\n  Operators Using Monte Carlo Simulations Abstract: In this paper we analyze the use of subjective logic as a framework for\nperforming approximate transformations over probability distribution functions.\nAs for any approximation, we evaluate subjective logic in terms of\ncomputational efficiency and bias. However, while the computational cost may be\neasily estimated, the bias of subjective logic operators have not yet been\ninvestigated. In order to evaluate this bias, we propose an experimental\nprotocol that exploits Monte Carlo simulations and their properties to assess\nthe distance between the result produced by subjective logic operators and the\ntrue result of the corresponding transformation over probability distributions.\nThis protocol allows a modeler to get an estimate of the degree of\napproximation she must be ready to accept as a trade-off for the computational\nefficiency and the interpretability of the subjective logic framework.\nConcretely, we apply our method to the relevant case study of the subjective\nlogic operator for binomial multiplication and fusion, and we study empirically\ntheir degree of approximation. \n\n"}
{"id": "1808.06581", "contents": "Title: The Deconfounded Recommender: A Causal Inference Approach to\n  Recommendation Abstract: The goal of recommendation is to show users items that they will like. Though\nusually framed as a prediction, the spirit of recommendation is to answer an\ninterventional question---for each user and movie, what would the rating be if\nwe \"forced\" the user to watch the movie? To this end, we develop a causal\napproach to recommendation, one where watching a movie is a \"treatment\" and a\nuser's rating is an \"outcome.\" The problem is there may be unobserved\nconfounders, variables that affect both which movies the users watch and how\nthey rate them; unobserved confounders impede causal predictions with\nobservational data. To solve this problem, we develop the deconfounded\nrecommender, a way to use classical recommendation models for causal\nrecommendation. Following Wang & Blei [23], the deconfounded recommender\ninvolves two probabilistic models. The first models which movies the users\nwatch; it provides a substitute for the unobserved confounders. The second one\nmodels how each user rates each movie; it employs the substitute to help\naccount for confounders. This two-stage approach removes bias due to\nconfounding. It improves recommendation and enjoys stable performance against\ninterventions on test sets. \n\n"}
{"id": "1808.07270", "contents": "Title: Learning to Support: Exploiting Structure Information in Support Sets\n  for One-Shot Learning Abstract: Deep Learning shows very good performance when trained on large labeled data\nsets. The problem of training a deep net on a few or one sample per class\nrequires a different learning approach which can generalize to unseen classes\nusing only a few representatives of these classes. This problem has previously\nbeen approached by meta-learning. Here we propose a novel meta-learner which\nshows state-of-the-art performance on common benchmarks for one/few shot\nclassification. Our model features three novel components: First is a\nfeed-forward embedding that takes random class support samples (after a\ncustomary CNN embedding) and transfers them to a better class representation in\nterms of a classification problem. Second is a novel attention mechanism,\ninspired by competitive learning, which causes class representatives to compete\nwith each other to become a temporary class prototype with respect to the query\npoint. This mechanism allows switching between representatives depending on the\nposition of the query point. Once a prototype is chosen for each class, the\npredicated label is computed using a simple attention mechanism over prototypes\nof all considered classes. The third feature is the ability of our meta-learner\nto incorporate deeper CNN embedding, enabling larger capacity. Finally, to ease\nthe training procedure and reduce overfitting, we averages the top $t$ models\n(evaluated on the validation) over the optimization trajectory. We show that\nthis approach can be viewed as an approximation to an ensemble, which saves the\nfactor of $t$ in training and test times and the factor of of $t$ in the\nstorage of the final model. \n\n"}
{"id": "1808.08414", "contents": "Title: Unsupervised Hypergraph Feature Selection via a Novel Point-Weighting\n  Framework and Low-Rank Representation Abstract: Feature selection methods are widely used in order to solve the 'curse of\ndimensionality' problem. Many proposed feature selection frameworks, treat all\ndata points equally; neglecting their different representation power and\nimportance. In this paper, we propose an unsupervised hypergraph feature\nselection method via a novel point-weighting framework and low-rank\nrepresentation that captures the importance of different data points. We\nintroduce a novel soft hypergraph with low complexity to model data. Then, we\nformulate the feature selection as an optimization problem to preserve local\nrelationships and also global structure of data. Our approach for global\nstructure preservation helps the framework overcome the problem of\nunavailability of data labels in unsupervised learning. The proposed feature\nselection method treats with different data points based on their importance in\ndefining data structure and representation power. Moreover, since the\nrobustness of feature selection methods against noise and outlier is of great\nimportance, we adopt low-rank representation in our model. Also, we provide an\nefficient algorithm to solve the proposed optimization problem. The\ncomputational cost of the proposed algorithm is lower than many\nstate-of-the-art methods which is of high importance in feature selection\ntasks. We conducted comprehensive experiments with various evaluation methods\non different benchmark data sets. These experiments indicate significant\nimprovement, compared with state-of-the-art feature selection methods. \n\n"}
{"id": "1808.08766", "contents": "Title: Learning behavioral context recognition with multi-stream temporal\n  convolutional networks Abstract: Smart devices of everyday use (such as smartphones and wearables) are\nincreasingly integrated with sensors that provide immense amounts of\ninformation about a person's daily life such as behavior and context. The\nautomatic and unobtrusive sensing of behavioral context can help develop\nsolutions for assisted living, fitness tracking, sleep monitoring, and several\nother fields. Towards addressing this issue, we raise the question: can a\nmachine learn to recognize a diverse set of contexts and activities in a\nreal-life through joint learning from raw multi-modal signals (e.g.\naccelerometer, gyroscope and audio etc.)? In this paper, we propose a\nmulti-stream temporal convolutional network to address the problem of\nmulti-label behavioral context recognition. A four-stream network architecture\nhandles learning from each modality with a contextualization module which\nincorporates extracted representations to infer a user's context. Our empirical\nevaluation suggests that a deep convolutional network trained end-to-end\nachieves an optimal recognition rate. Furthermore, the presented architecture\ncan be extended to include similar sensors for performance improvements and\nhandles missing modalities through multi-task learning without any manual\nfeature engineering on highly imbalanced and sparsely labeled dataset. \n\n"}
{"id": "1808.09011", "contents": "Title: Cauchy combination test: a powerful test with analytic p-value\n  calculation under arbitrary dependency structures Abstract: Combining individual p-values to aggregate multiple small effects has a\nlong-standing interest in statistics, dating back to the classic Fisher's\ncombination test. In modern large-scale data analysis, correlation and sparsity\nare common features and efficient computation is a necessary requirement for\ndealing with massive data. To overcome these challenges, we propose a new test\nthat takes advantage of the Cauchy distribution. Our test statistic has a very\nsimple form and is defined as a weighted sum of Cauchy transformation of\nindividual p-values. We prove a non-asymptotic result that the tail of the null\ndistribution of our proposed test statistic can be well approximated by a\nCauchy distribution under arbitrary dependency structures. Based on this\ntheoretical result, the p-value calculation of our proposed test is not only\naccurate, but also as simple as the classic z-test or t-test, making our test\nwell suited for analyzing massive data. We further show that the power of the\nproposed test is asymptotically optimal in a strong sparsity setting. Extensive\nsimulations demonstrate that the proposed test has both strong power against\nsparse alternatives and a good accuracy with respect to p-value calculations,\nespecially for very small p-values. The proposed test has also been applied to\na genome-wide association study of Crohn's disease and compared with several\nexisting tests. \n\n"}
{"id": "1808.09262", "contents": "Title: The Sparse Latent Position Model for nonnegative weighted networks Abstract: This paper introduces a new methodology to analyse bipartite and unipartite\nnetworks with nonnegative edge values. The proposed approach combines and\nadapts a number of ideas from the literature on latent variable network models.\nThe resulting framework is a new type of latent position model which exhibits\ngreat flexibility, and is able to capture important features that are generally\nexhibited by observed networks, such as sparsity and heavy tailed degree\ndistributions. A crucial advantage of the proposed method is that the number of\nlatent dimensions is automatically deduced from the data in one single\nalgorithmic framework. In addition, the model attaches a weight to each of the\nlatent dimensions, hence providing a measure of their relative importance. A\nfast variational Bayesian algorithm is proposed to estimate the parameters of\nthe model. Finally, applications of the proposed methodology are illustrated on\nboth artificial and real datasets, and comparisons with other existing\nprocedures are provided. \n\n"}
{"id": "1808.09340", "contents": "Title: Active set algorithms for estimating shape-constrained density ratios Abstract: In many instances, imposing a constraint on the shape of a density is a\nreasonable and flexible assumption. It offers an alternative to parametric\nmodels which can be too rigid and to other nonparametric methods requiring the\nchoice of tuning parameters. This paper treats the nonparametric estimation of\nlog-concave or log-convex density ratios by means of active set algorithms in a\nunified framework. In the setting of log-concave densities, the new algorithm\nis similar to but substantially faster than previously considered active set\nmethods. Log-convexity is a less common shape constraint which is described by\nsome authors as \"tail inflation\". The active set method proposed here is novel\nin this context. As a by-product, new goodness-of-fit tests of single\nhypotheses are formulated and are shown to be more powerful than higher\ncriticism tests in a simulation study. \n\n"}
{"id": "1808.09856", "contents": "Title: Application of Machine Learning in Rock Facies Classification with\n  Physics-Motivated Feature Augmentation Abstract: With recent progress in algorithms and the availability of massive amounts of\ncomputation power, application of machine learning techniques is becoming a hot\ntopic in the oil and gas industry. One of the most promising aspects to apply\nmachine learning to the upstream field is the rock facies classification in\nreservoir characterization, which is crucial in determining the net pay\nthickness of reservoirs, thus a definitive factor in drilling decision making\nprocess. For complex machine learning tasks like facies classification, feature\nengineering is often critical. This paper shows the inclusion of\nphysics-motivated feature interaction in feature augmentation can further\nimprove the capability of machine learning in rock facies classification. We\ndemonstrate this approach with the SEG 2016 machine learning contest dataset\nand the top winning algorithms. The improvement is roboust and can be $\\sim5\\%$\nbetter than current existing best F-1 score, where F-1 is an evaluation metric\nused to quantify average prediction accuracy. \n\n"}
{"id": "1808.09920", "contents": "Title: Question Answering by Reasoning Across Documents with Graph\n  Convolutional Networks Abstract: Most research in reading comprehension has focused on answering questions\nbased on individual documents or even single paragraphs. We introduce a neural\nmodel which integrates and reasons relying on information spread within\ndocuments and across multiple documents. We frame it as an inference problem on\na graph. Mentions of entities are nodes of this graph while edges encode\nrelations between different mentions (e.g., within- and cross-document\nco-reference). Graph convolutional networks (GCNs) are applied to these graphs\nand trained to perform multi-step reasoning. Our Entity-GCN method is scalable\nand compact, and it achieves state-of-the-art results on a multi-document\nquestion answering dataset, WikiHop (Welbl et al., 2018). \n\n"}
{"id": "1808.10532", "contents": "Title: Uniform Inference in High-Dimensional Gaussian Graphical Models Abstract: Graphical models have become a very popular tool for representing\ndependencies within a large set of variables and are key for representing\ncausal structures. We provide results for uniform inference on high-dimensional\ngraphical models with the number of target parameters $d$ being possible much\nlarger than sample size. This is in particular important when certain features\nor structures of a causal model should be recovered. Our results highlight how\nin high-dimensional settings graphical models can be estimated and recovered\nwith modern machine learning methods in complex data sets. To construct\nsimultaneous confidence regions on many target parameters, sufficiently fast\nestimation rates of the nuisance functions are crucial. In this context, we\nestablish uniform estimation rates and sparsity guarantees of the square-root\nestimator in a random design under approximate sparsity conditions that might\nbe of independent interest for related problems in high-dimensions. We also\ndemonstrate in a comprehensive simulation study that our procedure has good\nsmall sample properties. \n\n"}
{"id": "1808.10650", "contents": "Title: Graph reduction with spectral and cut guarantees Abstract: Can one reduce the size of a graph without significantly altering its basic\nproperties? The graph reduction problem is hereby approached from the\nperspective of restricted spectral approximation, a modification of the\nspectral similarity measure used for graph sparsification. This choice is\nmotivated by the observation that restricted approximation carries strong\nspectral and cut guarantees, and that it implies approximation results for\nunsupervised learning problems relying on spectral embeddings.\n  The paper then focuses on coarsening---the most common type of graph\nreduction. Sufficient conditions are derived for a small graph to approximate a\nlarger one in the sense of restricted similarity. These findings give rise to\nnearly-linear algorithms that, compared to both standard and advanced graph\nreduction methods, find coarse graphs of improved quality, often by a large\nmargin, without sacrificing speed. \n\n"}
{"id": "1809.01185", "contents": "Title: DeepPINK: reproducible feature selection in deep neural networks Abstract: Deep learning has become increasingly popular in both supervised and\nunsupervised machine learning thanks to its outstanding empirical performance.\nHowever, because of their intrinsic complexity, most deep learning methods are\nlargely treated as black box tools with little interpretability. Even though\nrecent attempts have been made to facilitate the interpretability of deep\nneural networks (DNNs), existing methods are susceptible to noise and lack of\nrobustness.\n  Therefore, scientists are justifiably cautious about the reproducibility of\nthe discoveries, which is often related to the interpretability of the\nunderlying statistical models. In this paper, we describe a method to increase\nthe interpretability and reproducibility of DNNs by incorporating the idea of\nfeature selection with controlled error rate. By designing a new DNN\narchitecture and integrating it with the recently proposed knockoffs framework,\nwe perform feature selection with a controlled error rate, while maintaining\nhigh power. This new method, DeepPINK (Deep feature selection using\nPaired-Input Nonlinear Knockoffs), is applied to both simulated and real data\nsets to demonstrate its empirical utility. \n\n"}
{"id": "1809.01369", "contents": "Title: Towards quantitative methods to assess network generative models Abstract: Assessing generative models is not an easy task. Generative models should\nsynthesize graphs which are not replicates of real networks but show\ntopological features similar to real graphs. We introduce an approach for\nassessing graph generative models using graph classifiers. The inability of an\nestablished graph classifier for distinguishing real and synthesized graphs\ncould be considered as a performance measurement for graph generators. \n\n"}
{"id": "1809.01749", "contents": "Title: Geometry of Deep Learning for Magnetic Resonance Fingerprinting Abstract: Current popular methods for Magnetic Resonance Fingerprint (MRF) recovery are\nbottlenecked by the heavy storage and computation requirements of a\ndictionary-matching (DM) step due to the growing size and complexity of the\nfingerprint dictionaries in multi-parametric quantitative MRI applications. In\nthis paper we study a deep learning approach to address these shortcomings.\nCoupled with a dimensionality reduction first layer, the proposed MRF-Net is\nable to reconstruct quantitative maps by saving more than 60 times in memory\nand computations required for a DM baseline. Fine-grid manifold enumeration\ni.e. the MRF dictionary is only used for training the network and not during\nimage reconstruction. We show that the MRF-Net provides a piece-wise affine\napproximation to the Bloch response manifold projection and that rather than\nmemorizing the dictionary, the network efficiently clusters this manifold and\nlearns a set of hierarchical matched-filters for affine regression of the NMR\ncharacteristics in each segment. \n\n"}
{"id": "1809.01819", "contents": "Title: MASA: Motif-Aware State Assignment in Noisy Time Series Data Abstract: Complex systems, such as airplanes, cars, or financial markets, produce\nmultivariate time series data consisting of a large number of system\nmeasurements over a period of time. Such data can be interpreted as a sequence\nof states, where each state represents a prototype of system behavior. An\nimportant problem in this domain is to identify repeated sequences of states,\nknown as motifs. Such motifs correspond to complex behaviors that capture\ncommon sequences of state transitions. For example, in automotive data, a motif\nof \"making a turn\" might manifest as a sequence of states: slowing down,\nturning the wheel, and then speeding back up. However, discovering these motifs\nis challenging, because the individual states and state assignments are\nunknown, have different durations, and need to be jointly learned from the\nnoisy time series. Here we develop motif-aware state assignment (MASA), a\nmethod to discover common motifs in noisy time series data and leverage those\nmotifs to more robustly assign states to measurements. We formulate the problem\nof motif discovery as a large optimization problem, which we solve using an\nexpectation-maximization type approach. MASA performs well in the presence of\nnoise in the input data and is scalable to very large datasets. Experiments on\nsynthetic data show that MASA outperforms state-of-the-art baselines by up to\n38.2%, and two case studies demonstrate how our approach discovers insightful\nmotifs in the presence of noise in real-world time series data. \n\n"}
{"id": "1809.01829", "contents": "Title: Adversarial Reprogramming of Text Classification Neural Networks Abstract: Adversarial Reprogramming has demonstrated success in utilizing pre-trained\nneural network classifiers for alternative classification tasks without\nmodification to the original network. An adversary in such an attack scenario\ntrains an additive contribution to the inputs to repurpose the neural network\nfor the new classification task. While this reprogramming approach works for\nneural networks with a continuous input space such as that of images, it is not\ndirectly applicable to neural networks trained for tasks such as text\nclassification, where the input space is discrete. Repurposing such\nclassification networks would require the attacker to learn an adversarial\nprogram that maps inputs from one discrete space to the other. In this work, we\nintroduce a context-based vocabulary remapping model to reprogram neural\nnetworks trained on a specific sequence classification task, for a new sequence\nclassification task desired by the adversary. We propose training procedures\nfor this adversarial program in both white-box and black-box settings. We\ndemonstrate the application of our model by adversarially repurposing various\ntext-classification models including LSTM, bi-directional LSTM and CNN for\nalternate classification tasks. \n\n"}
{"id": "1809.01843", "contents": "Title: How to Combine Tree-Search Methods in Reinforcement Learning Abstract: Finite-horizon lookahead policies are abundantly used in Reinforcement\nLearning and demonstrate impressive empirical success. Usually, the lookahead\npolicies are implemented with specific planning methods such as Monte Carlo\nTree Search (e.g. in AlphaZero). Referring to the planning problem as tree\nsearch, a reasonable practice in these implementations is to back up the value\nonly at the leaves while the information obtained at the root is not leveraged\nother than for updating the policy. Here, we question the potency of this\napproach. Namely, the latter procedure is non-contractive in general, and its\nconvergence is not guaranteed. Our proposed enhancement is straightforward and\nsimple: use the return from the optimal tree path to back up the values at the\ndescendants of the root. This leads to a $\\gamma^h$-contracting procedure,\nwhere $\\gamma$ is the discount factor and $h$ is the tree depth. To establish\nour results, we first introduce a notion called \\emph{multiple-step greedy\nconsistency}. We then provide convergence rates for two algorithmic\ninstantiations of the above enhancement in the presence of noise injected to\nboth the tree search stage and value estimation stage. \n\n"}
{"id": "1809.03659", "contents": "Title: New models for symbolic data analysis Abstract: Symbolic data analysis (SDA) is an emerging area of statistics concerned with\nunderstanding and modelling data that takes distributional form (i.e. symbols),\nsuch as random lists, intervals and histograms. It was developed under the\npremise that the statistical unit of interest is the symbol, and that inference\nis required at this level. Here we consider a different perspective, which\nopens a new research direction in the field of SDA. We assume that, as with a\nstandard statistical analysis, inference is required at the level of\nindividual-level data. However, the individual-level data are aggregated into\nsymbols - group-based distributional-valued summaries - prior to the analysis.\nIn this way, large and complex datasets can be reduced to a smaller number of\ndistributional summaries, that may be analysed more efficiently than the\noriginal dataset. As such, we develop SDA techniques as a new approach for the\nanalysis of big data. In particular we introduce a new general method for\nconstructing likelihood functions for symbolic data based on a desired\nprobability model for the underlying measurement-level data, while only\nobserving the distributional summaries. This approach opens the door for new\nclasses of symbol design and construction, in addition to developing SDA as a\nviable tool to enable and improve upon classical data analyses, particularly\nfor very large and complex datasets. We illustrate this new direction for SDA\nresearch through several real and simulated data analyses. \n\n"}
{"id": "1809.04541", "contents": "Title: Lugsail lag windows for estimating time-average covariance matrices Abstract: Lag windows are commonly used in time series, econometrics, steady-state\nsimulation, and Markov chain Monte Carlo to estimate time-average covariance\nmatrices. In the presence of positive correlation of the underlying process,\nestimators of this matrix almost always exhibit significant negative bias,\nleading to undesirable finite-sample properties. We propose a new family of lag\nwindows specifically designed to improve finite-sample performance by\noffsetting this negative bias. Any existing lag window can be adapted into a\nlugsail equivalent with no additional assumptions. We use these lag windows\nwithin spectral variance estimators and demonstrate its advantages in a linear\nregression model with autocorrelated and heteroskedastic residuals. We further\nemploy the lugsail lag windows in weighted batch means estimators due to their\ncomputational efficiency on large simulation output. We obtain bias and\nvariance results for these multivariate estimators and significantly weaken the\nmixing condition on the process. Superior finite-sample properties are\nillustrated in a vector autoregressive process and a Bayesian logistic\nregression model. \n\n"}
{"id": "1809.04913", "contents": "Title: Query-Efficient Black-Box Attack by Active Learning Abstract: Deep neural network (DNN) as a popular machine learning model is found to be\nvulnerable to adversarial attack. This attack constructs adversarial examples\nby adding small perturbations to the raw input, while appearing unmodified to\nhuman eyes but will be misclassified by a well-trained classifier. In this\npaper, we focus on the black-box attack setting where attackers have almost no\naccess to the underlying models. To conduct black-box attack, a popular\napproach aims to train a substitute model based on the information queried from\nthe target DNN. The substitute model can then be attacked using existing\nwhite-box attack approaches, and the generated adversarial examples will be\nused to attack the target DNN. Despite its encouraging results, this approach\nsuffers from poor query efficiency, i.e., attackers usually needs to query a\nhuge amount of times to collect enough information for training an accurate\nsubstitute model. To this end, we first utilize state-of-the-art white-box\nattack methods to generate samples for querying, and then introduce an active\nlearning strategy to significantly reduce the number of queries needed.\nBesides, we also propose a diversity criterion to avoid the sampling bias. Our\nextensive experimental results on MNIST and CIFAR-10 show that the proposed\nmethod can reduce more than $90\\%$ of queries while preserve attacking success\nrates and obtain an accurate substitute model which is more than $85\\%$ similar\nwith the target oracle. \n\n"}
{"id": "1809.06594", "contents": "Title: Rare tail approximation using asymptotics and $L^1$ polar coordinates Abstract: In this work, we propose a class of importance sampling (IS) estimators for\nestimating the right tail probability of a sum of continuous random variables\nbased on a change of variables to $L^1$ polar coordinates in which the radial\nand angular components of the IS distribution are considered separately.\n  When the asymptotic behaviour of the sum is known we exploit it for the\nradial change of measure, and the resulting estimator has the appealing form of\nthe (known) asymptotic multiplied by a random multiplicative correction factor.\nGiven we assume knowledge of the asymptotic behaviour of the sum in this\nframework, traditional notions of efficiency that appear in the rare-event\nliterature hold little practical meaning here. Instead, we focus on the\npractical behaviour of the proposed estimator in the pre-asymptotic regime for\nright tail probabilities between roughly $10^{-3}$ and $10^{-7}$.\n  The proposed estimator and procedure are applicable in both the heavy- and\nlight-tailed settings, as well as for independent and dependent summands. In\nthe case of independent summands, we find that our estimator compares\nfavourably with exponential tilting (iid light-tailed summands) and the\nAsmussen--Kroese method (independent subexponential summands).\n  However, for dependent subexponential summands using the same simple angular\ndistribution as for the independent case, the performance of our estimator\nrapidly degenerates with increasing dimension, suggesting an open avenue for\nfurther research. \n\n"}
{"id": "1809.07920", "contents": "Title: The distribution of Weierstrass points on a tropical curve Abstract: We show that on a metric graph of genus $g$, a divisor of degree $n$\ngenerically has $g(n-g+1)$ Weierstrass points. For a sequence of generic\ndivisors on a metric graph whose degrees grow to infinity, we show that the\nassociated Weierstrass points become distributed according to the Zhang\ncanonical measure. In other words, the limiting distribution is determined by\neffective resistances on the metric graph. This distribution result has an\nanalogue for complex algebraic curves, due to Neeman, and for curves over\nnon-Archimedean fields, due to Amini. \n\n"}
{"id": "1809.08097", "contents": "Title: Deep Domain Adaptation under Deep Label Scarcity Abstract: The goal behind Domain Adaptation (DA) is to leverage the labeled examples\nfrom a source domain so as to infer an accurate model in a target domain where\nlabels are not available or in scarce at the best. A state-of-the-art approach\nfor the DA is due to (Ganin et al. 2016), known as DANN, where they attempt to\ninduce a common representation of source and target domains via adversarial\ntraining. This approach requires a large number of labeled examples from the\nsource domain to be able to infer a good model for the target domain. However,\nin many situations obtaining labels in the source domain is expensive which\nresults in deteriorated performance of DANN and limits its applicability in\nsuch scenarios. In this paper, we propose a novel approach to overcome this\nlimitation. In our work, we first establish that DANN reduces the original DA\nproblem into a semi-supervised learning problem over the space of common\nrepresentation. Next, we propose a learning approach, namely TransDANN, that\namalgamates adversarial learning and transductive learning to mitigate the\ndetrimental impact of limited source labels and yields improved performance.\nExperimental results (both on text and images) show a significant boost in the\nperformance of TransDANN over DANN under such scenarios. We also provide\ntheoretical justification for the performance boost. \n\n"}
{"id": "1809.08746", "contents": "Title: Matrix Linear Discriminant Analysis Abstract: We propose a novel linear discriminant analysis approach for the\nclassification of high-dimensional matrix-valued data that commonly arises from\nimaging studies. Motivated by the equivalence of the conventional linear\ndiscriminant analysis and the ordinary least squares, we consider an efficient\nnuclear norm penalized regression that encourages a low-rank structure.\nTheoretical properties including a non-asymptotic risk bound and a rank\nconsistency result are established. Simulation studies and an application to\nelectroencephalography data show the superior performance of the proposed\nmethod over the existing approaches. \n\n"}
{"id": "1809.09219", "contents": "Title: Fast Signal Recovery from Saturated Measurements by Linear Loss and\n  Nonconvex Penalties Abstract: Sign information is the key to overcoming the inevitable saturation error in\ncompressive sensing systems, which causes information loss and results in bias.\nFor sparse signal recovery from saturation, we propose to use a linear loss to\nimprove the effectiveness from existing methods that utilize hard\nconstraints/hinge loss for sign consistency. Due to the use of linear loss, an\nanalytical solution in the update progress is obtained, and some nonconvex\npenalties are applicable, e.g., the minimax concave penalty, the $\\ell_0$ norm,\nand the sorted $\\ell_1$ norm. Theoretical analysis reveals that the estimation\nerror can still be bounded. Generally, with linear loss and nonconvex\npenalties, the recovery performance is significantly improved, and the\ncomputational time is largely saved, which is verified by the numerical\nexperiments. \n\n"}
{"id": "1809.09505", "contents": "Title: Practical bounds on the error of Bayesian posterior approximations: A\n  nonasymptotic approach Abstract: Bayesian inference typically requires the computation of an approximation to\nthe posterior distribution. An important requirement for an approximate\nBayesian inference algorithm is to output high-accuracy posterior mean and\nuncertainty estimates. Classical Monte Carlo methods, particularly Markov Chain\nMonte Carlo, remain the gold standard for approximate Bayesian inference\nbecause they have a robust finite-sample theory and reliable convergence\ndiagnostics. However, alternative methods, which are more scalable or apply to\nproblems where Markov Chain Monte Carlo cannot be used, lack the same\nfinite-data approximation theory and tools for evaluating their accuracy. In\nthis work, we develop a flexible new approach to bounding the error of mean and\nuncertainty estimates of scalable inference algorithms. Our strategy is to\ncontrol the estimation errors in terms of Wasserstein distance, then bound the\nWasserstein distance via a generalized notion of Fisher distance. Unlike\ncomputing the Wasserstein distance, which requires access to the normalized\nposterior distribution, the Fisher distance is tractable to compute because it\nrequires access only to the gradient of the log posterior density. We\ndemonstrate the usefulness of our Fisher distance approach by deriving bounds\non the Wasserstein error of the Laplace approximation and Hilbert coresets. We\nanticipate that our approach will be applicable to many other approximate\ninference methods such as the integrated Laplace approximation, variational\ninference, and approximate Bayesian computation \n\n"}
{"id": "1809.09953", "contents": "Title: Deep Neural Networks for Estimation and Inference Abstract: We study deep neural networks and their use in semiparametric inference. We\nestablish novel rates of convergence for deep feedforward neural nets. Our new\nrates are sufficiently fast (in some cases minimax optimal) to allow us to\nestablish valid second-step inference after first-step estimation with deep\nlearning, a result also new to the literature. Our estimation rates and\nsemiparametric inference results handle the current standard architecture:\nfully connected feedforward neural networks (multi-layer perceptrons), with the\nnow-common rectified linear unit activation function and a depth explicitly\ndiverging with the sample size. We discuss other architectures as well,\nincluding fixed-width, very deep networks. We establish nonasymptotic bounds\nfor these deep nets for a general class of nonparametric regression-type loss\nfunctions, which includes as special cases least squares, logistic regression,\nand other generalized linear models. We then apply our theory to develop\nsemiparametric inference, focusing on causal parameters for concreteness, such\nas treatment effects, expected welfare, and decomposition effects. Inference in\nmany other semiparametric contexts can be readily obtained. We demonstrate the\neffectiveness of deep learning with a Monte Carlo analysis and an empirical\napplication to direct mail marketing. \n\n"}
{"id": "1809.10326", "contents": "Title: Boosting Trust Region Policy Optimization by Normalizing Flows Policy Abstract: We propose to improve trust region policy search with normalizing flows\npolicy. We illustrate that when the trust region is constructed by KL\ndivergence constraints, normalizing flows policy generates samples far from the\n'center' of the previous policy iterate, which potentially enables better\nexploration and helps avoid bad local optima. Through extensive comparisons, we\nshow that the normalizing flows policy significantly improves upon baseline\narchitectures especially on high-dimensional tasks with complex dynamics. \n\n"}
{"id": "1810.00274", "contents": "Title: Bayesian network marker selection via the thresholded graph Laplacian\n  Gaussian prior Abstract: Selecting informative nodes over large-scale networks becomes increasingly\nimportant in many research areas. Most existing methods focus on the local\nnetwork structure and incur heavy computational costs for the large-scale\nproblem. In this work, we propose a novel prior model for Bayesian network\nmarker selection in the generalized linear model (GLM) framework: the\nThresholded Graph Laplacian Gaussian (TGLG) prior, which adopts the graph\nLaplacian matrix to characterize the conditional dependence between neighboring\nmarkers accounting for the global network structure. Under mild conditions, we\nshow the proposed model enjoys the posterior consistency with a diverging\nnumber of edges and nodes in the network. We also develop a Metropolis-adjusted\nLangevin algorithm (MALA) for efficient posterior computation, which is\nscalable to large-scale networks. We illustrate the superiorities of the\nproposed method compared with existing alternatives via extensive simulation\nstudies and an analysis of the breast cancer gene expression dataset in the\nCancer Genome Atlas (TCGA). \n\n"}
{"id": "1810.00424", "contents": "Title: Interpretable Neuron Structuring with Graph Spectral Regularization Abstract: While neural networks are powerful approximators used to classify or embed\ndata into lower dimensional spaces, they are often regarded as black boxes with\nuninterpretable features. Here we propose Graph Spectral Regularization for\nmaking hidden layers more interpretable without significantly impacting\nperformance on the primary task. Taking inspiration from spatial organization\nand localization of neuron activations in biological networks, we use a graph\nLaplacian penalty to structure the activations within a layer. This penalty\nencourages activations to be smooth either on a predetermined graph or on a\nfeature-space graph learned from the data via co-activations of a hidden layer\nof the neural network. We show numerous uses for this additional structure\nincluding cluster indication and visualization in biological and image data\nsets. \n\n"}
{"id": "1810.00609", "contents": "Title: One-Click Annotation with Guided Hierarchical Object Detection Abstract: The increase in data collection has made data annotation an interesting and\nvaluable task in the contemporary world. This paper presents a new methodology\nfor quickly annotating data using click-supervision and hierarchical object\ndetection. The proposed work is semi-automatic in nature where the task of\nannotations is split between the human and a neural network. We show that our\nimproved method of annotation reduces the time, cost and mental stress on a\nhuman annotator. The research also highlights how our method performs better\nthan the current approach in different circumstances such as variation in\nnumber of objects, object size and different datasets. Our approach also\nproposes a new method of using object detectors making it suitable for data\nannotation task. The experiment conducted on PASCAL VOC dataset revealed that\nannotation created from our approach achieves a mAP of 0.995 and a recall of\n0.903. The Our Approach has shown an overall improvement by 8.5%, 18.6% in mean\naverage precision and recall score for KITTI and 69.6%, 36% for CITYSCAPES\ndataset. The proposed framework is 3-4 times faster as compared to the standard\nannotation method. \n\n"}
{"id": "1810.00656", "contents": "Title: Perfect Match: A Simple Method for Learning Representations For\n  Counterfactual Inference With Neural Networks Abstract: Learning representations for counterfactual inference from observational data\nis of high practical relevance for many domains, such as healthcare, public\npolicy and economics. Counterfactual inference enables one to answer \"What\nif...?\" questions, such as \"What would be the outcome if we gave this patient\ntreatment $t_1$?\". However, current methods for training neural networks for\ncounterfactual inference on observational data are either overly complex,\nlimited to settings with only two available treatments, or both. Here, we\npresent Perfect Match (PM), a method for training neural networks for\ncounterfactual inference that is easy to implement, compatible with any\narchitecture, does not add computational complexity or hyperparameters, and\nextends to any number of treatments. PM is based on the idea of augmenting\nsamples within a minibatch with their propensity-matched nearest neighbours.\nOur experiments demonstrate that PM outperforms a number of more complex\nstate-of-the-art methods in inferring counterfactual outcomes across several\nbenchmarks, particularly in settings with many treatments. \n\n"}
{"id": "1810.01861", "contents": "Title: Inhibited Softmax for Uncertainty Estimation in Neural Networks Abstract: We present a new method for uncertainty estimation and out-of-distribution\ndetection in neural networks with softmax output. We extend softmax layer with\nan additional constant input. The corresponding additional output is able to\nrepresent the uncertainty of the network. The proposed method requires neither\nadditional parameters nor multiple forward passes nor input preprocessing nor\nout-of-distribution datasets. We show that our method performs comparably to\nmore computationally expensive methods and outperforms baselines on our\nexperiments from image recognition and sentiment analysis domains. \n\n"}
{"id": "1810.02814", "contents": "Title: Statistical Optimality of Interpolated Nearest Neighbor Algorithms Abstract: In the era of deep learning, understanding over-fitting phenomenon becomes\nincreasingly important. It is observed that carefully designed deep neural\nnetworks achieve small testing error even when the training error is close to\nzero. One possible explanation is that for many modern machine learning\nalgorithms, over-fitting can greatly reduce the estimation bias, while not\nincreasing the estimation variance too much. To illustrate the above idea, we\nprove that the proposed interpolated nearest neighbor algorithm achieves the\nminimax optimal rate in both regression and classification regimes, and observe\nthat they are empirically better than the traditional $k$ nearest neighbor\nmethod in some cases. \n\n"}
{"id": "1810.03048", "contents": "Title: Bayes-CPACE: PAC Optimal Exploration in Continuous Space Bayes-Adaptive\n  Markov Decision Processes Abstract: We present the first PAC optimal algorithm for Bayes-Adaptive Markov Decision\nProcesses (BAMDPs) in continuous state and action spaces, to the best of our\nknowledge. The BAMDP framework elegantly addresses model uncertainty by\nincorporating Bayesian belief updates into long-term expected return. However,\ncomputing an exact optimal Bayesian policy is intractable. Our key insight is\nto compute a near-optimal value function by covering the continuous\nstate-belief-action space with a finite set of representative samples and\nexploiting the Lipschitz continuity of the value function. We prove the\nnear-optimality of our algorithm and analyze a number of schemes that boost the\nalgorithm's efficiency. Finally, we empirically validate our approach on a\nnumber of discrete and continuous BAMDPs and show that the learned policy has\nconsistently competitive performance against baseline approaches. \n\n"}
{"id": "1810.03296", "contents": "Title: Event History Analysis of Dynamic Communication Networks Abstract: Statistical analysis on networks has received growing attention due to demand\nfrom various emerging applications. In dynamic networks, one of the key\ninterests is to model the event history of time-stamped interactions amongst\nnodes. We propose to model dynamic directed communication networks via\nmultivariate counting processes. A pseudo partial likelihood approach is\nexploited to capture the network dependence structure. Asymptotic results of\nthe resulting estimation are established. Numerical results are performed to\ndemonstrate effectiveness of our proposal. \n\n"}
{"id": "1810.04714", "contents": "Title: Training Generative Adversarial Networks with Binary Neurons by\n  End-to-end Backpropagation Abstract: We propose the BinaryGAN, a novel generative adversarial network (GAN) that\nuses binary neurons at the output layer of the generator. We employ the\nsigmoid-adjusted straight-through estimators to estimate the gradients for the\nbinary neurons and train the whole network by end-to-end backpropogation. The\nproposed model is able to directly generate binary-valued predictions at test\ntime. We implement such a model to generate binarized MNIST digits and\nexperimentally compare the performance for different types of binary neurons,\nGAN objectives and network architectures. Although the results are still\npreliminary, we show that it is possible to train a GAN that has binary neurons\nand that the use of gradient estimators can be a promising direction for\nmodeling discrete distributions with GANs. For reproducibility, the source code\nis available at https://github.com/salu133445/binarygan . \n\n"}
{"id": "1810.04754", "contents": "Title: Efficient Tensor Decomposition with Boolean Factors Abstract: Tensor decomposition has been extensively used as a tool for exploratory\nanalysis. Motivated by neuroscience applications, we study tensor decomposition\nwith Boolean factors. The resulting optimization problem is challenging due to\nthe non-convex objective and the combinatorial constraints. We propose Binary\nMatching Pursuit (BMP), a novel generalization of the matching pursuit strategy\nto decompose the tensor efficiently. BMP iteratively searches for atoms in a\ngreedy fashion. The greedy atom search step is solved efficiently via a\nMAXCUT-like boolean quadratic program. We prove that BMP is guaranteed to\nconverge sublinearly to the optimal solution and recover the factors under mild\nidentifiability conditions. Experiments demonstrate the superior performance of\nour method over baselines on synthetic and real datasets. We also showcase the\napplication of BMP in quantifying neural interactions underlying\nhigh-resolution spatiotemporal ECoG recordings. \n\n"}
{"id": "1810.05433", "contents": "Title: Constructions of Primitive Formally Dual Pairs Having Subsets with\n  Unequal Sizes Abstract: The concept of formal duality was proposed by Cohn, Kumar and Sch\\\"urmann,\nwhich reflects a remarkable symmetry among energy-minimizing periodic\nconfigurations. This formal duality was later on translated into a purely\ncombinatorial property by Cohn, Kumar, Reiher and Sch\\\"urmann, where the\ncorresponding combinatorial objects were called formally dual pairs. Almost all\nknown examples of primitive formally dual pairs satisfy that the two subsets\nhave the same size. Indeed, prior to this work, there was only one known\nexample having subsets with unequal sizes in $\\mathbb{Z}_2 \\times\n\\mathbb{Z}_4^2$. Motivated by this example, we propose a lifting construction\nframework and a recursive construction framework, which generate new primitive\nformally dual pairs from known ones. As an application, for $m \\ge 2$, we\nobtain $m+1$ pairwise inequivalent primitive formally dual pairs in\n$\\mathbb{Z}_2 \\times \\mathbb{Z}_4^{2m}$, which have subsets with unequal sizes. \n\n"}
{"id": "1810.06755", "contents": "Title: Discovering Fair Representations in the Data Domain Abstract: Interpretability and fairness are critical in computer vision and machine\nlearning applications, in particular when dealing with human outcomes, e.g.\ninviting or not inviting for a job interview based on application materials\nthat may include photographs. One promising direction to achieve fairness is by\nlearning data representations that remove the semantics of protected\ncharacteristics, and are therefore able to mitigate unfair outcomes. All\navailable models however learn latent embeddings which comes at the cost of\nbeing uninterpretable. We propose to cast this problem as data-to-data\ntranslation, i.e. learning a mapping from an input domain to a fair target\ndomain, where a fairness definition is being enforced. Here the data domain can\nbe images, or any tabular data representation. This task would be\nstraightforward if we had fair target data available, but this is not the case.\nTo overcome this, we learn a highly unconstrained mapping by exploiting\nstatistics of residuals - the difference between input data and its translated\nversion - and the protected characteristics. When applied to the CelebA dataset\nof face images with gender attribute as the protected characteristic, our model\nenforces equality of opportunity by adjusting the eyes and lips regions.\nIntriguingly, on the same dataset we arrive at similar conclusions when using\nsemantic attribute representations of images for translation. On face images of\nthe recent DiF dataset, with the same gender attribute, our method adjusts nose\nregions. In the Adult income dataset, also with protected gender attribute, our\nmodel achieves equality of opportunity by, among others, obfuscating the wife\nand husband relationship. Analyzing those systematic changes will allow us to\nscrutinize the interplay of fairness criterion, chosen protected\ncharacteristics, and prediction performance. \n\n"}
{"id": "1810.06784", "contents": "Title: ProMP: Proximal Meta-Policy Search Abstract: Credit assignment in Meta-reinforcement learning (Meta-RL) is still poorly\nunderstood. Existing methods either neglect credit assignment to pre-adaptation\nbehavior or implement it naively. This leads to poor sample-efficiency during\nmeta-training as well as ineffective task identification strategies. This paper\nprovides a theoretical analysis of credit assignment in gradient-based Meta-RL.\nBuilding on the gained insights we develop a novel meta-learning algorithm that\novercomes both the issue of poor credit assignment and previous difficulties in\nestimating meta-policy gradients. By controlling the statistical distance of\nboth pre-adaptation and adapted policies during meta-policy search, the\nproposed algorithm endows efficient and stable meta-learning. Our approach\nleads to superior pre-adaptation policy behavior and consistently outperforms\nprevious Meta-RL algorithms in sample-efficiency, wall-clock time, and\nasymptotic performance. \n\n"}
{"id": "1810.06838", "contents": "Title: Finite-sample analysis of M-estimators using self-concordance Abstract: The classical asymptotic theory for parametric $M$-estimators guarantees\nthat, in the limit of infinite sample size, the excess risk has a chi-square\ntype distribution, even in the misspecified case. We demonstrate how\nself-concordance of the loss allows to characterize the critical sample size\nsufficient to guarantee a chi-square type in-probability bound for the excess\nrisk. Specifically, we consider two classes of losses: (i) self-concordant\nlosses in the classical sense of Nesterov and Nemirovski, i.e., whose third\nderivative is uniformly bounded with the $3/2$ power of the second derivative;\n(ii) pseudo self-concordant losses, for which the power is removed. These\nclasses contain losses corresponding to several generalized linear models,\nincluding the logistic loss and pseudo-Huber losses. Our basic result under\nminimal assumptions bounds the critical sample size by $O(d \\cdot\nd_{\\text{eff}}),$ where $d$ the parameter dimension and $d_{\\text{eff}}$ the\neffective dimension that accounts for model misspecification. In contrast to\nthe existing results, we only impose local assumptions that concern the\npopulation risk minimizer $\\theta_*$. Namely, we assume that the calibrated\ndesign, i.e., design scaled by the square root of the second derivative of the\nloss, is subgaussian at $\\theta_*$. Besides, for type-ii losses we require\nboundedness of a certain measure of curvature of the population risk at\n$\\theta_*$.Our improved result bounds the critical sample size from above as\n$O(\\max\\{d_{\\text{eff}}, d \\log d\\})$ under slightly stronger assumptions.\nNamely, the local assumptions must hold in the neighborhood of $\\theta_*$ given\nby the Dikin ellipsoid of the population risk. Interestingly, we find that, for\nlogistic regression with Gaussian design, there is no actual restriction of\nconditions: the subgaussian parameter and curvature measure remain\nnear-constant over the Dikin ellipsoid. Finally, we extend some of these\nresults to $\\ell_1$-penalized estimators in high dimensions. \n\n"}
{"id": "1810.06839", "contents": "Title: Sharp Analysis of Learning with Discrete Losses Abstract: The problem of devising learning strategies for discrete losses (e.g.,\nmultilabeling, ranking) is currently addressed with methods and theoretical\nanalyses ad-hoc for each loss. In this paper we study a least-squares framework\nto systematically design learning algorithms for discrete losses, with\nquantitative characterizations in terms of statistical and computational\ncomplexity. In particular we improve existing results by providing explicit\ndependence on the number of labels for a wide class of losses and faster\nlearning rates in conditions of low-noise. Theoretical results are complemented\nwith experiments on real datasets, showing the effectiveness of the proposed\ngeneral approach. \n\n"}
{"id": "1810.06943", "contents": "Title: The Deep Weight Prior Abstract: Bayesian inference is known to provide a general framework for incorporating\nprior knowledge or specific properties into machine learning models via\ncarefully choosing a prior distribution. In this work, we propose a new type of\nprior distributions for convolutional neural networks, deep weight prior (DWP),\nthat exploit generative models to encourage a specific structure of trained\nconvolutional filters e.g., spatial correlations of weights. We define DWP in\nthe form of an implicit distribution and propose a method for variational\ninference with such type of implicit priors. In experiments, we show that DWP\nimproves the performance of Bayesian neural networks when training data are\nlimited, and initialization of weights with samples from DWP accelerates\ntraining of conventional convolutional neural networks. \n\n"}
{"id": "1810.07147", "contents": "Title: Joint Nonparametric Precision Matrix Estimation with Confounding Abstract: We consider the problem of precision matrix estimation where, due to\nextraneous confounding of the underlying precision matrix, the data are\nindependent but not identically distributed. While such confounding occurs in\nmany scientific problems, our approach is inspired by recent neuroscientific\nresearch suggesting that brain function, as measured using functional magnetic\nresonance imagine (fMRI), is susceptible to confounding by physiological noise\nsuch as breathing and subject motion. Following the scientific motivation, we\npropose a graphical model, which in turn motivates a joint nonparametric\nestimator. We provide theoretical guarantees for the consistency and the\nconvergence rate of the proposed estimator. In addition, we demonstrate that\nthe optimization of the proposed estimator can be transformed into a series of\nlinear programming problems, and thus be efficiently solved in parallel.\nEmpirical results are presented using simulated and real brain imaging data,\nwhich suggest that our approach improves precision matrix estimation, as\ncompared to baselines, when confounding is present. \n\n"}
{"id": "1810.07218", "contents": "Title: Incremental Few-Shot Learning with Attention Attractor Networks Abstract: Machine learning classifiers are often trained to recognize a set of\npre-defined classes. However, in many applications, it is often desirable to\nhave the flexibility of learning additional concepts, with limited data and\nwithout re-training on the full training set. This paper addresses this\nproblem, incremental few-shot learning, where a regular classification network\nhas already been trained to recognize a set of base classes, and several extra\nnovel classes are being considered, each with only a few labeled examples.\nAfter learning the novel classes, the model is then evaluated on the overall\nclassification performance on both base and novel classes. To this end, we\npropose a meta-learning model, the Attention Attractor Network, which\nregularizes the learning of novel classes. In each episode, we train a set of\nnew weights to recognize novel classes until they converge, and we show that\nthe technique of recurrent back-propagation can back-propagate through the\noptimization process and facilitate the learning of these parameters. We\ndemonstrate that the learned attractor network can help recognize novel classes\nwhile remembering old classes without the need to review the original training\nset, outperforming various baselines. \n\n"}
{"id": "1810.07291", "contents": "Title: Deep Neural Maps Abstract: We introduce a new unsupervised representation learning and visualization\nusing deep convolutional networks and self organizing maps called Deep Neural\nMaps (DNM). DNM jointly learns an embedding of the input data and a mapping\nfrom the embedding space to a two-dimensional lattice. We compare\nvisualizations of DNM with those of t-SNE and LLE on the MNIST and COIL-20 data\nsets. Our experiments show that the DNM can learn efficient representations of\nthe input data, which reflects characteristics of each class. This is shown via\nback-projecting the neurons of the map on the data space. \n\n"}
{"id": "1810.07785", "contents": "Title: From Deep to Physics-Informed Learning of Turbulence: Diagnostics Abstract: We describe tests validating progress made toward acceleration and automation\nof hydrodynamic codes in the regime of developed turbulence by three Deep\nLearning (DL) Neural Network (NN) schemes trained on Direct Numerical\nSimulations of turbulence. Even the bare DL solutions, which do not take into\naccount any physics of turbulence explicitly, are impressively good overall\nwhen it comes to qualitative description of important features of turbulence.\nHowever, the early tests have also uncovered some caveats of the DL approaches.\nWe observe that the static DL scheme, implementing Convolutional GAN and\ntrained on spatial snapshots of turbulence, fails to reproduce intermittency of\nturbulent fluctuations at small scales and details of the turbulence geometry\nat large scales. We show that the dynamic NN schemes, namely LAT-NET and\nCompressed Convolutional LSTM, trained on a temporal sequence of turbulence\nsnapshots are capable to correct for the caveats of the static NN. We suggest a\npath forward towards improving reproducibility of the large-scale geometry of\nturbulence with NN. \n\n"}
{"id": "1810.07845", "contents": "Title: On Statistical Learning of Simplices: Unmixing Problem Revisited Abstract: We study the sample complexity of learning a high-dimensional simplex from a\nset of points uniformly sampled from its interior. Learning of simplices is a\nlong studied problem in computer science and has applications in computational\nbiology and remote sensing, mostly under the name of `spectral unmixing'. We\ntheoretically show that a sufficient sample complexity for reliable learning of\na $K$-dimensional simplex up to a total-variation error of $\\epsilon$ is\n$O\\left(\\frac{K^2}{\\epsilon}\\log\\frac{K}{\\epsilon}\\right)$, which yields a\nsubstantial improvement over existing bounds. Based on our new theoretical\nframework, we also propose a heuristic approach for the inference of simplices.\nExperimental results on synthetic and real-world datasets demonstrate a\ncomparable performance for our method on noiseless samples, while we outperform\nthe state-of-the-art in noisy cases. \n\n"}
{"id": "1810.07900", "contents": "Title: Policy Gradient in Partially Observable Environments: Approximation and\n  Convergence Abstract: Policy gradient is a generic and flexible reinforcement learning approach\nthat generally enjoys simplicity in analysis, implementation, and deployment.\nIn the last few decades, this approach has been extensively advanced for fully\nobservable environments. In this paper, we generalize a variety of these\nadvances to partially observable settings, and similar to the fully observable\ncase, we keep our focus on the class of Markovian policies. We propose a series\nof technical tools, including a novel notion of advantage function, to develop\npolicy gradient algorithms and study their convergence properties in such\nenvironments. Deploying these tools, we generalize a variety of existing\ntheoretical guarantees, such as policy gradient and convergence theorems, to\npartially observable domains, those which also could be carried to more\nsettings of interest. This study also sheds light on the understanding of\npolicy gradient approaches in real-world applications which tend to be\npartially observable. \n\n"}
{"id": "1810.08010", "contents": "Title: Variational Noise-Contrastive Estimation Abstract: Unnormalised latent variable models are a broad and flexible class of\nstatistical models. However, learning their parameters from data is\nintractable, and few estimation techniques are currently available for such\nmodels. To increase the number of techniques in our arsenal, we propose\nvariational noise-contrastive estimation (VNCE), building on NCE which is a\nmethod that only applies to unnormalised models. The core idea is to use a\nvariational lower bound to the NCE objective function, which can be optimised\nin the same fashion as the evidence lower bound (ELBO) in standard variational\ninference (VI). We prove that VNCE can be used for both parameter estimation of\nunnormalised models and posterior inference of latent variables. The developed\ntheory shows that VNCE has the same level of generality as standard VI, meaning\nthat advances made there can be directly imported to the unnormalised setting.\nWe validate VNCE on toy models and apply it to a realistic problem of\nestimating an undirected graphical model from incomplete data. \n\n"}
{"id": "1810.08127", "contents": "Title: Hausdorff dimension of pinned distance sets and the $L^2$-method Abstract: We prove that for any $E\\subset{\\Bbb R}^2$, $\\dim_{\\mathcal{H}}(E)>1$, there\nexists $x\\in E$ such that the Hausdorff dimension of the pinned distance set\n  $$\\Delta_x(E)=\\{|x-y|: y \\in E\\}$$\n  is no less than $\\min\\left\\{\\frac{4}{3}\\dim_{\\mathcal{H}}(E)-\\frac{2}{3},\n1\\right\\}$. This answers a question recently raised by Guth, Iosevich, Ou and\nWang, as well as improves results of Keleti and Shmerkin.\n  (This version is already published on Proceeding AMS so I would like to leave\nit unchanged. However the statement in the abstract, which is the second part\nof Theorem 1.1, should be weakened a bit to: for any $\\epsilon>0$ there exists\n$x\\in E$ such that the Hausdorff dimension of $\\Delta_x(E)$ is at least\n$\\min\\left\\{\\frac{4}{3}\\dim_{\\mathcal{H}}(E)-\\frac{2}{3}-\\epsilon, 1\\right\\}$,\nand it implies the Hausdorff dimension of the distance set,\n$\\Delta(E)=\\{|x-y|:x,y\\in E\\}$, is at least\n$\\min\\left\\{\\frac{4}{3}\\dim_{\\mathcal{H}}(E)-\\frac{2}{3}, 1\\right\\}$. There is\nno problem in the proof and the first part of Theorem 1.1. I apologize for\nbeing sloppy and would like to thank Yumeng Ou for pointing it out.) \n\n"}
{"id": "1810.08732", "contents": "Title: Named Entity Recognition on Twitter for Turkish using Semi-supervised\n  Learning with Word Embeddings Abstract: Recently, due to the increasing popularity of social media, the necessity for\nextracting information from informal text types, such as microblog texts, has\ngained significant attention. In this study, we focused on the Named Entity\nRecognition (NER) problem on informal text types for Turkish. We utilized a\nsemi-supervised learning approach based on neural networks. We applied a fast\nunsupervised method for learning continuous representations of words in vector\nspace. We made use of these obtained word embeddings, together with language\nindependent features that are engineered to work better on informal text types,\nfor generating a Turkish NER system on microblog texts. We evaluated our\nTurkish NER system on Twitter messages and achieved better F-score performances\nthan the published results of previously proposed NER systems on Turkish\ntweets. Since we did not employ any language dependent features, we believe\nthat our method can be easily adapted to microblog texts in other\nmorphologically rich languages. \n\n"}
{"id": "1810.09230", "contents": "Title: AST-Based Deep Learning for Detecting Malicious PowerShell Abstract: With the celebrated success of deep learning, some attempts to develop\neffective methods for detecting malicious PowerShell programs employ neural\nnets in a traditional natural language processing setup while others employ\nconvolutional neural nets to detect obfuscated malicious commands at a\ncharacter level. While these representations may express salient PowerShell\nproperties, our hypothesis is that tools from static program analysis will be\nmore effective. We propose a hybrid approach combining traditional program\nanalysis (in the form of abstract syntax trees) and deep learning. This poster\npresents preliminary results of a fundamental step in our approach: learning\nembeddings for nodes of PowerShell ASTs. We classify malicious scripts by\nfamily type and explore embedded program vector representations. \n\n"}
{"id": "1810.09311", "contents": "Title: Revisiting Distributional Correspondence Indexing: A Python\n  Reimplementation and New Experiments Abstract: This paper introduces PyDCI, a new implementation of Distributional\nCorrespondence Indexing (DCI) written in Python. DCI is a transfer learning\nmethod for cross-domain and cross-lingual text classification for which we had\nprovided an implementation (here called JaDCI) built on top of JaTeCS, a Java\nframework for text classification. PyDCI is a stand-alone version of DCI that\nexploits scikit-learn and the SciPy stack. We here report on new experiments\nthat we have carried out in order to test PyDCI, and in which we use as\nbaselines new high-performing methods that have appeared after DCI was\noriginally proposed. These experiments show that, thanks to a few subtle ways\nin which we have improved DCI, PyDCI outperforms both JaDCI and the\nabove-mentioned high-performing methods, and delivers the best known results on\nthe two popular benchmarks on which we had tested DCI, i.e.,\nMultiDomainSentiment (a.k.a. MDS -- for cross-domain adaptation) and\nWebis-CLS-10 (for cross-lingual adaptation). PyDCI, together with the code\nallowing to replicate our experiments, is available at\nhttps://github.com/AlexMoreo/pydci . \n\n"}
{"id": "1810.09899", "contents": "Title: Dynamic Likelihood-free Inference via Ratio Estimation (DIRE) Abstract: Parametric statistical models that are implicitly defined in terms of a\nstochastic data generating process are used in a wide range of scientific\ndisciplines because they enable accurate modeling. However, learning the\nparameters from observed data is generally very difficult because their\nlikelihood function is typically intractable. Likelihood-free Bayesian\ninference methods have been proposed which include the frameworks of\napproximate Bayesian computation (ABC), synthetic likelihood, and its recent\ngeneralization that performs likelihood-free inference by ratio estimation\n(LFIRE). A major difficulty in all these methods is choosing summary statistics\nthat reduce the dimensionality of the data to facilitate inference. While\nseveral methods for choosing summary statistics have been proposed for ABC, the\nliterature for synthetic likelihood and LFIRE is very thin to date. We here\naddress this gap in the literature, focusing on the important special case of\ntime-series models. We show that convolutional neural networks trained to\npredict the input parameters from the data provide suitable summary statistics\nfor LFIRE. On a wide range of time-series models, a single neural network\narchitecture produced equally or more accurate posteriors than alternative\nmethods. \n\n"}
{"id": "1810.10482", "contents": "Title: Noisy Blackbox Optimization with Multi-Fidelity Queries: A Tree Search\n  Approach Abstract: We study the problem of black-box optimization of a noisy function in the\npresence of low-cost approximations or fidelities, which is motivated by\nproblems like hyper-parameter tuning. In hyper-parameter tuning evaluating the\nblack-box function at a point involves training a learning algorithm on a large\ndata-set at a particular hyper-parameter and evaluating the validation error.\nEven a single such evaluation can be prohibitively expensive. Therefore, it is\nbeneficial to use low-cost approximations, like training the learning algorithm\non a sub-sampled version of the whole data-set. These low-cost\napproximations/fidelities can however provide a biased and noisy estimate of\nthe function value. In this work, we incorporate the multi-fidelity setup in\nthe powerful framework of noisy black-box optimization through tree-like\nhierarchical partitions. We propose a multi-fidelity bandit based tree-search\nalgorithm for the problem and provide simple regret bounds for our algorithm.\nFinally, we validate the performance of our algorithm on real and synthetic\ndatasets, where it outperforms several benchmarks. \n\n"}
{"id": "1810.10627", "contents": "Title: Streaming Graph Neural Networks Abstract: Graphs are essential representations of many real-world data such as social\nnetworks. Recent years have witnessed the increasing efforts made to extend the\nneural network models to graph-structured data. These methods, which are\nusually known as the graph neural networks, have been applied to advance many\ngraphs related tasks such as reasoning dynamics of the physical system, graph\nclassification, and node classification. Most of the existing graph neural\nnetwork models have been designed for static graphs, while many real-world\ngraphs are inherently dynamic. For example, social networks are naturally\nevolving as new users joining and new relations being created. Current graph\nneural network models cannot utilize the dynamic information in dynamic graphs.\nHowever, the dynamic information has been proven to enhance the performance of\nmany graph analytic tasks such as community detection and link prediction.\nHence, it is necessary to design dedicated graph neural networks for dynamic\ngraphs. In this paper, we propose DGNN, a new {\\bf D}ynamic {\\bf G}raph {\\bf\nN}eural {\\bf N}etwork model, which can model the dynamic information as the\ngraph evolving. In particular, the proposed framework can keep updating node\ninformation by capturing the sequential information of edges (interactions),\nthe time intervals between edges and information propagation coherently.\nExperimental results on various dynamic graphs demonstrate the effectiveness of\nthe proposed framework. \n\n"}
{"id": "1810.11165", "contents": "Title: Efficient learning of neighbor representations for boundary trees and\n  forests Abstract: We introduce a semiparametric approach to neighbor-based classification. We\nbuild off the recently proposed Boundary Trees algorithm by Mathy et al.(2015)\nwhich enables fast neighbor-based classification, regression and retrieval in\nlarge datasets. While boundary trees use an Euclidean measure of similarity,\nthe Differentiable Boundary Tree algorithm by Zoran et al.(2017) was introduced\nto learn low-dimensional representations of complex input data, on which\nsemantic similarity can be calculated to train boundary trees. As is pointed\nout by its authors, the differentiable boundary tree approach contains a few\nlimitations that prevents it from scaling to large datasets. In this paper, we\nintroduce Differentiable Boundary Sets, an algorithm that overcomes the\ncomputational issues of the differentiable boundary tree scheme and also\nimproves its classification accuracy and data representability. Our algorithm\nis efficiently implementable with existing tools and offers a significant\nreduction in training time. We test and compare the algorithms on the well\nknown MNIST handwritten digits dataset and the newer Fashion-MNIST dataset by\nXiao et al.(2017). \n\n"}
{"id": "1810.11726", "contents": "Title: Towards Robust Deep Neural Networks Abstract: We investigate the topics of sensitivity and robustness in feedforward and\nconvolutional neural networks. Combining energy landscape techniques developed\nin computational chemistry with tools drawn from formal methods, we produce\nempirical evidence indicating that networks corresponding to lower-lying minima\nin the optimization landscape of the learning objective tend to be more robust.\nThe robustness estimate used is the inverse of a proposed sensitivity measure,\nwhich we define as the volume of an over-approximation of the reachable set of\nnetwork outputs under all additive $l_{\\infty}$-bounded perturbations on the\ninput data. We present a novel loss function which includes a sensitivity term\nin addition to the traditional task-oriented and regularization terms. In our\nexperiments on standard machine learning and computer vision datasets, we show\nthat the proposed loss function leads to networks which reliably optimize the\nrobustness measure as well as other related metrics of adversarial robustness\nwithout significant degradation in the classification error. Experimental\nresults indicate that the proposed method outperforms state-of-the-art\nsensitivity-based learning approaches with regards to robustness to adversarial\nattacks. We also show that although the introduced framework does not\nexplicitly enforce an adversarial loss, it achieves competitive overall\nperformance relative to methods that do. \n\n"}
{"id": "1810.11750", "contents": "Title: Towards Understanding Learning Representations: To What Extent Do\n  Different Neural Networks Learn the Same Representation Abstract: It is widely believed that learning good representations is one of the main\nreasons for the success of deep neural networks. Although highly intuitive,\nthere is a lack of theory and systematic approach quantitatively characterizing\nwhat representations do deep neural networks learn. In this work, we move a\ntiny step towards a theory and better understanding of the representations.\nSpecifically, we study a simpler problem: How similar are the representations\nlearned by two networks with identical architecture but trained from different\ninitializations. We develop a rigorous theory based on the neuron activation\nsubspace match model. The theory gives a complete characterization of the\nstructure of neuron activation subspace matches, where the core concepts are\nmaximum match and simple match which describe the overall and the finest\nsimilarity between sets of neurons in two networks respectively. We also\npropose efficient algorithms to find the maximum match and simple matches.\nFinally, we conduct extensive experiments using our algorithms. Experimental\nresults suggest that, surprisingly, representations learned by the same\nconvolutional layers of networks trained from different initializations are not\nas similar as prevalently expected, at least in terms of subspace match. \n\n"}
{"id": "1810.11776", "contents": "Title: Learning stable and predictive structures in kinetic systems: Benefits\n  of a causal approach Abstract: Learning kinetic systems from data is one of the core challenges in many\nfields. Identifying stable models is essential for the generalization\ncapabilities of data-driven inference. We introduce a computationally efficient\nframework, called CausalKinetiX, that identifies structure from discrete time,\nnoisy observations, generated from heterogeneous experiments. The algorithm\nassumes the existence of an underlying, invariant kinetic model, a key\ncriterion for reproducible research. Results on both simulated and real-world\nexamples suggest that learning the structure of kinetic systems benefits from a\ncausal perspective. The identified variables and models allow for a concise\ndescription of the dynamics across multiple experimental settings and can be\nused for prediction in unseen experiments. We observe significant improvements\ncompared to well established approaches focusing solely on predictive\nperformance, especially for out-of-sample generalization. \n\n"}
{"id": "1810.11829", "contents": "Title: On preserving non-discrimination when combining expert advice Abstract: We study the interplay between sequential decision making and avoiding\ndiscrimination against protected groups, when examples arrive online and do not\nfollow distributional assumptions. We consider the most basic extension of\nclassical online learning: \"Given a class of predictors that are individually\nnon-discriminatory with respect to a particular metric, how can we combine them\nto perform as well as the best predictor, while preserving non-discrimination?\"\nSurprisingly we show that this task is unachievable for the prevalent notion of\n\"equalized odds\" that requires equal false negative rates and equal false\npositive rates across groups. On the positive side, for another notion of\nnon-discrimination, \"equalized error rates\", we show that running separate\ninstances of the classical multiplicative weights algorithm for each group\nachieves this guarantee. Interestingly, even for this notion, we show that\nalgorithms with stronger performance guarantees than multiplicative weights\ncannot preserve non-discrimination. \n\n"}
{"id": "1810.12273", "contents": "Title: Kalman Gradient Descent: Adaptive Variance Reduction in Stochastic\n  Optimization Abstract: We introduce Kalman Gradient Descent, a stochastic optimization algorithm\nthat uses Kalman filtering to adaptively reduce gradient variance in stochastic\ngradient descent by filtering the gradient estimates. We present both a\ntheoretical analysis of convergence in a non-convex setting and experimental\nresults which demonstrate improved performance on a variety of machine learning\nareas including neural networks and black box variational inference. We also\npresent a distributed version of our algorithm that enables large-dimensional\noptimization, and we extend our algorithm to SGD with momentum and RMSProp. \n\n"}
{"id": "1810.12558", "contents": "Title: Relative Importance Sampling For Off-Policy Actor-Critic in Deep\n  Reinforcement Learning Abstract: Off-policy learning is more unstable compared to on-policy learning in\nreinforcement learning (RL). One reason for the instability of off-policy\nlearning is a discrepancy between the target ($\\pi$) and behavior (b) policy\ndistributions. The discrepancy between $\\pi$ and b distributions can be\nalleviated by employing a smooth variant of the importance sampling (IS), such\nas the relative importance sampling (RIS). RIS has parameter $\\beta\\in[0, 1]$\nwhich controls smoothness. To cope with instability, we present the first\nrelative importance sampling-off-policy actor-critic (RIS-Off-PAC) model-free\nalgorithms in RL. In our method, the network yields a target policy (the\nactor), a value function (the critic) assessing the current policy ($\\pi$)\nusing samples drawn from behavior policy. We use action value generated from\nthe behavior policy in reward function to train our algorithm rather than from\nthe target policy. We also use deep neural networks to train both actor and\ncritic. We evaluated our algorithm on a number of Open AI Gym benchmark\nproblems and demonstrate better or comparable performance to several\nstate-of-the-art RL baselines. \n\n"}
{"id": "1810.13259", "contents": "Title: Non-linear Canonical Correlation Analysis: A Compressed Representation\n  Approach Abstract: Canonical Correlation Analysis (CCA) is a linear representation learning\nmethod that seeks maximally correlated variables in multi-view data. Non-linear\nCCA extends this notion to a broader family of transformations, which are more\npowerful in many real-world applications. Given the joint probability, the\nAlternating Conditional Expectation (ACE) algorithm provides an optimal\nsolution to the non-linear CCA problem. However, it suffers from limited\nperformance and an increasing computational burden when only a finite number of\nsamples is available. In this work we introduce an information-theoretic\ncompressed representation framework for the non-linear CCA problem (CRCCA),\nwhich extends the classical ACE approach. Our suggested framework seeks compact\nrepresentations of the data that allow a maximal level of correlation. This way\nwe control the trade-off between the flexibility and the complexity of the\nmodel. CRCCA provides theoretical bounds and optimality conditions, as we\nestablish fundamental connections to rate-distortion theory, the information\nbottleneck and remote source coding. In addition, it allows a soft\ndimensionality reduction, as the compression level is determined by the mutual\ninformation between the original noisy data and the extracted signals. Finally,\nwe introduce a simple implementation of the CRCCA framework, based on lattice\nquantization. \n\n"}
{"id": "1810.13317", "contents": "Title: Contrastive Multivariate Singular Spectrum Analysis Abstract: We introduce Contrastive Multivariate Singular Spectrum Analysis, a novel\nunsupervised method for dimensionality reduction and signal decomposition of\ntime series data. By utilizing an appropriate background dataset, the method\ntransforms a target time series dataset in a way that evinces the sub-signals\nthat are enhanced in the target dataset, as opposed to only those that account\nfor the greatest variance. This shifts the goal from finding signals that\nexplain the most variance to signals that matter the most to the analyst. We\ndemonstrate our method on an illustrative synthetic example, as well as show\nthe utility of our method in the downstream clustering of electrocardiogram\nsignals from the public MHEALTH dataset. \n\n"}
{"id": "1811.00183", "contents": "Title: Designing an Effective Metric Learning Pipeline for Speaker Diarization Abstract: State-of-the-art speaker diarization systems utilize knowledge from external\ndata, in the form of a pre-trained distance metric, to effectively determine\nrelative speaker identities to unseen data. However, much of recent focus has\nbeen on choosing the appropriate feature extractor, ranging from pre-trained\n$i-$vectors to representations learned via different sequence modeling\narchitectures (e.g. 1D-CNNs, LSTMs, attention models), while adopting\noff-the-shelf metric learning solutions. In this paper, we argue that,\nregardless of the feature extractor, it is crucial to carefully design a metric\nlearning pipeline, namely the loss function, the sampling strategy and the\ndiscrimnative margin parameter, for building robust diarization systems.\nFurthermore, we propose to adopt a fine-grained validation process to obtain a\ncomprehensive evaluation of the generalization power of metric learning\npipelines. To this end, we measure diarization performance across different\nlanguage speakers, and variations in the number of speakers in a recording.\nUsing empirical studies, we provide interesting insights into the effectiveness\nof different design choices and make recommendations. \n\n"}
{"id": "1811.00293", "contents": "Title: Critical initialisation for deep signal propagation in noisy rectifier\n  neural networks Abstract: Stochastic regularisation is an important weapon in the arsenal of a deep\nlearning practitioner. However, despite recent theoretical advances, our\nunderstanding of how noise influences signal propagation in deep neural\nnetworks remains limited. By extending recent work based on mean field theory,\nwe develop a new framework for signal propagation in stochastic regularised\nneural networks. Our noisy signal propagation theory can incorporate several\ncommon noise distributions, including additive and multiplicative Gaussian\nnoise as well as dropout. We use this framework to investigate initialisation\nstrategies for noisy ReLU networks. We show that no critical initialisation\nstrategy exists using additive noise, with signal propagation exploding\nregardless of the selected noise distribution. For multiplicative noise (e.g.\ndropout), we identify alternative critical initialisation strategies that\ndepend on the second moment of the noise distribution. Simulations and\nexperiments on real-world data confirm that our proposed initialisation is able\nto stably propagate signals in deep networks, while using an initialisation\ndisregarding noise fails to do so. Furthermore, we analyse correlation dynamics\nbetween inputs. Stronger noise regularisation is shown to reduce the depth to\nwhich discriminatory information about the inputs to a noisy ReLU network is\nable to propagate, even when initialised at criticality. We support our\ntheoretical predictions for these trainable depths with simulations, as well as\nwith experiments on MNIST and CIFAR-10 \n\n"}
{"id": "1811.00866", "contents": "Title: Efficient Neural Network Robustness Certification with General\n  Activation Functions Abstract: Finding minimum distortion of adversarial examples and thus certifying\nrobustness in neural network classifiers for given data points is known to be a\nchallenging problem. Nevertheless, recently it has been shown to be possible to\ngive a non-trivial certified lower bound of minimum adversarial distortion, and\nsome recent progress has been made towards this direction by exploiting the\npiece-wise linear nature of ReLU activations. However, a generic robustness\ncertification for general activation functions still remains largely\nunexplored. To address this issue, in this paper we introduce CROWN, a general\nframework to certify robustness of neural networks with general activation\nfunctions for given input data points. The novelty in our algorithm consists of\nbounding a given activation function with linear and quadratic functions, hence\nallowing it to tackle general activation functions including but not limited to\nfour popular choices: ReLU, tanh, sigmoid and arctan. In addition, we\nfacilitate the search for a tighter certified lower bound by adaptively\nselecting appropriate surrogates for each neuron activation. Experimental\nresults show that CROWN on ReLU networks can notably improve the certified\nlower bounds compared to the current state-of-the-art algorithm Fast-Lin, while\nhaving comparable computational efficiency. Furthermore, CROWN also\ndemonstrates its effectiveness and flexibility on networks with general\nactivation functions, including tanh, sigmoid and arctan. \n\n"}
{"id": "1811.01057", "contents": "Title: Semidefinite relaxations for certifying robustness to adversarial\n  examples Abstract: Despite their impressive performance on diverse tasks, neural networks fail\ncatastrophically in the presence of adversarial inputs---imperceptibly but\nadversarially perturbed versions of natural inputs. We have witnessed an arms\nrace between defenders who attempt to train robust networks and attackers who\ntry to construct adversarial examples. One promise of ending the arms race is\ndeveloping certified defenses, ones which are provably robust against all\nattackers in some family. These certified defenses are based on convex\nrelaxations which construct an upper bound on the worst case loss over all\nattackers in the family. Previous relaxations are loose on networks that are\nnot trained against the respective relaxation. In this paper, we propose a new\nsemidefinite relaxation for certifying robustness that applies to arbitrary\nReLU networks. We show that our proposed relaxation is tighter than previous\nrelaxations and produces meaningful robustness guarantees on three different\n\"foreign networks\" whose training objectives are agnostic to our proposed\nrelaxation. \n\n"}
{"id": "1811.01520", "contents": "Title: User-Friendly Covariance Estimation for Heavy-Tailed Distributions Abstract: We offer a survey of recent results on covariance estimation for heavy-tailed\ndistributions. By unifying ideas scattered in the literature, we propose\nuser-friendly methods that facilitate practical implementation. Specifically,\nwe introduce element-wise and spectrum-wise truncation operators, as well as\ntheir $M$-estimator counterparts, to robustify the sample covariance matrix.\nDifferent from the classical notion of robustness that is characterized by the\nbreakdown property, we focus on the tail robustness which is evidenced by the\nconnection between nonasymptotic deviation and confidence level. The key\nobservation is that the estimators needs to adapt to the sample size,\ndimensionality of the data and the noise level to achieve optimal tradeoff\nbetween bias and robustness. Furthermore, to facilitate their practical use, we\npropose data-driven procedures that automatically calibrate the tuning\nparameters. We demonstrate their applications to a series of structured models\nin high dimensions, including the bandable and low-rank covariance matrices and\nsparse precision matrices. Numerical studies lend strong support to the\nproposed methods. \n\n"}
{"id": "1811.01848", "contents": "Title: Plan Online, Learn Offline: Efficient Learning and Exploration via\n  Model-Based Control Abstract: We propose a plan online and learn offline (POLO) framework for the setting\nwhere an agent, with an internal model, needs to continually act and learn in\nthe world. Our work builds on the synergistic relationship between local\nmodel-based control, global value function learning, and exploration. We study\nhow local trajectory optimization can cope with approximation errors in the\nvalue function, and can stabilize and accelerate value function learning.\nConversely, we also study how approximate value functions can help reduce the\nplanning horizon and allow for better policies beyond local solutions. Finally,\nwe also demonstrate how trajectory optimization can be used to perform\ntemporally coordinated exploration in conjunction with estimating uncertainty\nin value function approximation. This exploration is critical for fast and\nstable learning of the value function. Combining these components enable\nsolutions to complex simulated control tasks, like humanoid locomotion and\ndexterous in-hand manipulation, in the equivalent of a few minutes of\nexperience in the real world. \n\n"}
{"id": "1811.02628", "contents": "Title: Learning Bone Suppression from Dual Energy Chest X-rays using\n  Adversarial Networks Abstract: Suppressing bones on chest X-rays such as ribs and clavicle is often expected\nto improve pathologies classification. These bones can interfere with a broad\nrange of diagnostic tasks on pulmonary disease except for musculoskeletal\nsystem. Current conventional method for acquisition of bone suppressed X-rays\nis dual energy imaging, which captures two radiographs at a very short interval\nwith different energy levels; however, the patient is exposed to radiation\ntwice and the artifacts arise due to heartbeats between two shots. In this\npaper, we introduce a deep generative model trained to predict bone suppressed\nimages on single energy chest X-rays, analyzing a finite set of previously\nacquired dual energy chest X-rays. Since the relatively small amount of data is\navailable, such approach relies on the methodology maximizing the data\nutilization. Here we integrate the following two approaches. First, we use a\nconditional generative adversarial network that complements the traditional\nregression method minimizing the pairwise image difference. Second, we use Haar\n2D wavelet decomposition to offer a perceptual guideline in frequency details\nto allow the model to converge quickly and efficiently. As a result, we achieve\nstate-of-the-art performance on bone suppression as compared to the existing\napproaches with dual energy chest X-rays. \n\n"}
{"id": "1811.03146", "contents": "Title: Multi-channel discourse as an indicator for Bitcoin price and volume\n  movements Abstract: This research aims to identify how Bitcoin-related news publications and\nonline discourse are expressed in Bitcoin exchange movements of price and\nvolume. Being inherently digital, all Bitcoin-related fundamental data (from\nexchanges, as well as transactional data directly from the blockchain) is\navailable online, something that is not true for traditional businesses or\ncurrencies traded on exchanges. This makes Bitcoin an interesting subject for\nsuch research, as it enables the mapping of sentiment to fundamental events\nthat might otherwise be inaccessible. Furthermore, Bitcoin discussion largely\ntakes place on online forums and chat channels. In stock trading, the value of\nsentiment data in trading decisions has been demonstrated numerous times [1]\n[2] [3], and this research aims to determine whether there is value in such\ndata for Bitcoin trading models. To achieve this, data over the year 2015 has\nbeen collected from Bitcointalk.org, (the biggest Bitcoin forum in post\nvolume), established news sources such as Bloomberg and the Wall Street\nJournal, the complete /r/btc and /r/Bitcoin subreddits, and the bitcoin-otc and\nbitcoin-dev IRC channels. By analyzing this data on sentiment and volume, we\nfind weak to moderate correlations between forum, news, and Reddit sentiment\nand movements in price and volume from 1 to 5 days after the sentiment was\nexpressed. A Granger causality test confirms the predictive causality of the\nsentiment on the daily percentage price and volume movements, and at the same\ntime underscores the predictive causality of market movements on sentiment\nexpressions in online communities \n\n"}
{"id": "1811.03154", "contents": "Title: Poisson Multi-Bernoulli Mapping Using Gibbs Sampling Abstract: This paper addresses the mapping problem. Using a conjugate prior form, we\nderive the exact theoretical batch multi-object posterior density of the map\ngiven a set of measurements. The landmarks in the map are modeled as extended\nobjects, and the measurements are described as a Poisson process, conditioned\non the map. We use a Poisson process prior on the map and prove that the\nposterior distribution is a hybrid Poisson, multi-Bernoulli mixture\ndistribution. We devise a Gibbs sampling algorithm to sample from the batch\nmulti-object posterior. The proposed method can handle uncertainties in the\ndata associations and the cardinality of the set of landmarks, and is\nparallelizable, making it suitable for large-scale problems. The performance of\nthe proposed method is evaluated on synthetic data and is shown to outperform a\nstate-of-the-art method. \n\n"}
{"id": "1811.03407", "contents": "Title: A Factor Graph Approach to Automated Design of Bayesian Signal\n  Processing Algorithms Abstract: The benefits of automating design cycles for Bayesian inference-based\nalgorithms are becoming increasingly recognized by the machine learning\ncommunity. As a result, interest in probabilistic programming frameworks has\nmuch increased over the past few years. This paper explores a specific\nprobabilistic programming paradigm, namely message passing in Forney-style\nfactor graphs (FFGs), in the context of automated design of efficient Bayesian\nsignal processing algorithms. To this end, we developed \"ForneyLab\"\n(https://github.com/biaslab/ForneyLab.jl) as a Julia toolbox for message\npassing-based inference in FFGs. We show by example how ForneyLab enables\nautomatic derivation of Bayesian signal processing algorithms, including\nalgorithms for parameter estimation and model comparison. Crucially, due to the\nmodular makeup of the FFG framework, both the model specification and inference\nmethods are readily extensible in ForneyLab. In order to test this framework,\nwe compared variational message passing as implemented by ForneyLab with\nautomatic differentiation variational inference (ADVI) and Monte Carlo methods\nas implemented by state-of-the-art tools \"Edward\" and \"Stan\". In terms of\nperformance, extensibility and stability issues, ForneyLab appears to enjoy an\nedge relative to its competitors for automated inference in state-space models. \n\n"}
{"id": "1811.06580", "contents": "Title: Subspace Clustering through Sub-Clusters Abstract: The problem of dimension reduction is of increasing importance in modern data\nanalysis. In this paper, we consider modeling the collection of points in a\nhigh dimensional space as a union of low dimensional subspaces. In particular\nwe propose a highly scalable sampling based algorithm that clusters the entire\ndata via first spectral clustering of a small random sample followed by\nclassifying or labeling the remaining out of sample points. The key idea is\nthat this random subset borrows information across the entire data set and that\nthe problem of clustering points can be replaced with the more efficient and\nrobust problem of \"clustering sub-clusters\". We provide theoretical guarantees\nfor our procedure. The numerical results indicate we outperform other\nstate-of-the-art subspace clustering algorithms with respect to accuracy and\nspeed. \n\n"}
{"id": "1811.06601", "contents": "Title: Estimating the Mean and Variance of a High-dimensional Normal\n  Distribution Using a Mixture Prior Abstract: This paper provides a framework for estimating the mean and variance of a\nhigh-dimensional normal density. The main setting considered is a fixed number\nof vector following a high-dimensional normal distribution with unknown mean\nand diagonal covariance matrix. The diagonal covariance matrix can be known or\nunknown. If the covariance matrix is unknown, the sample size can be as small\nas $2$. The proposed estimator is based on the idea that the unobserved pairs\nof mean and variance for each dimension are drawn from an unknown bivariate\ndistribution, which we model as a mixture of normal-inverse gammas. The mixture\nof normal-inverse gamma distributions provides advantages over more traditional\nempirical Bayes methods, which are based on a normal-normal model. When fitting\na mixture model, we are essentially clustering the unobserved mean and variance\npairs for each dimension into different groups, with each group having a\ndifferent normal-inverse gamma distribution. The proposed estimator of each\nmean is the posterior mean of shrinkage estimates, each of which shrinks a\nsample mean towards a different component of the mixture distribution.\nSimilarly, the proposed estimator of variance has an analogous interpretation\nin terms of sample variances and components of the mixture distribution. If\ndiagonal covariance matrix is known, then the sample size can be as small as\n$1$, and we treat the pairs of known variance and unknown mean for each\ndimension as random observations coming from a flexible mixture of\nnormal-inverse gamma distributions. \n\n"}
{"id": "1811.07006", "contents": "Title: Projected BNNs: Avoiding weight-space pathologies by learning latent\n  representations of neural network weights Abstract: As machine learning systems get widely adopted for high-stake decisions,\nquantifying uncertainty over predictions becomes crucial. While modern neural\nnetworks are making remarkable gains in terms of predictive accuracy,\ncharacterizing uncertainty over the parameters of these models is challenging\nbecause of the high dimensionality and complex correlations of the network\nparameter space. This paper introduces a novel variational inference framework\nfor Bayesian neural networks that (1) encodes complex distributions in\nhigh-dimensional parameter space with representations in a low-dimensional\nlatent space, and (2) performs inference efficiently on the low-dimensional\nrepresentations. Across a large array of synthetic and real-world datasets, we\nshow that our method improves uncertainty characterization and model\ngeneralization when compared with methods that work directly in the parameter\nspace. \n\n"}
{"id": "1811.07769", "contents": "Title: Addressing the Invisible: Street Address Generation for Developing\n  Countries with Deep Learning Abstract: More than half of the world's roads lack adequate street addressing systems.\nLack of addresses is even more visible in daily lives of people in developing\ncountries. We would like to object to the assumption that having an address is\na luxury, by proposing a generative address design that maps the world in\naccordance with streets. The addressing scheme is designed considering several\ntraditional street addressing methodologies employed in the urban development\nscenarios around the world. Our algorithm applies deep learning to extract\nroads from satellite images, converts the road pixel confidences into a road\nnetwork, partitions the road network to find neighborhoods, and labels the\nregions, roads, and address units using graph- and proximity-based algorithms.\nWe present our results on a sample US city, and several developing cities,\ncompare travel times of users using current ad hoc and new complete addresses,\nand contrast our addressing solution to current industrial and open geocoding\nalternatives. \n\n"}
{"id": "1811.08413", "contents": "Title: Sampling Can Be Faster Than Optimization Abstract: Optimization algorithms and Monte Carlo sampling algorithms have provided the\ncomputational foundations for the rapid growth in applications of statistical\nmachine learning in recent years. There is, however, limited theoretical\nunderstanding of the relationships between these two kinds of methodology, and\nlimited understanding of relative strengths and weaknesses. Moreover, existing\nresults have been obtained primarily in the setting of convex functions (for\noptimization) and log-concave functions (for sampling). In this setting, where\nlocal properties determine global properties, optimization algorithms are\nunsurprisingly more efficient computationally than sampling algorithms. We\ninstead examine a class of nonconvex objective functions that arise in mixture\nmodeling and multi-stable systems. In this nonconvex setting, we find that the\ncomputational complexity of sampling algorithms scales linearly with the model\ndimension while that of optimization algorithms scales exponentially. \n\n"}
{"id": "1811.08888", "contents": "Title: Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU\n  Networks Abstract: We study the problem of training deep neural networks with Rectified Linear\nUnit (ReLU) activation function using gradient descent and stochastic gradient\ndescent. In particular, we study the binary classification problem and show\nthat for a broad family of loss functions, with proper random weight\ninitialization, both gradient descent and stochastic gradient descent can find\nthe global minima of the training loss for an over-parameterized deep ReLU\nnetwork, under mild assumption on the training data. The key idea of our proof\nis that Gaussian random initialization followed by (stochastic) gradient\ndescent produces a sequence of iterates that stay inside a small perturbation\nregion centering around the initial weights, in which the empirical loss\nfunction of deep ReLU networks enjoys nice local curvature properties that\nensure the global convergence of (stochastic) gradient descent. Our theoretical\nresults shed light on understanding the optimization for deep learning, and\npave the way for studying the optimization dynamics of training modern deep\nneural networks. \n\n"}
{"id": "1811.09026", "contents": "Title: Bandits with Temporal Stochastic Constraints Abstract: We study the effect of impairment on stochastic multi-armed bandits and\ndevelop new ways to mitigate it. Impairment effect is the phenomena where an\nagent only accrues reward for an action if they have played it at least a few\ntimes in the recent past. It is practically motivated by repetition and recency\neffects in domains such as advertising (here consumer behavior may require\nrepeat actions by advertisers) and vocational training (here actions are\ncomplex skills that can only be mastered with repetition to get a payoff).\nImpairment can be naturally modelled as a temporal constraint on the strategy\nspace, and we provide two novel algorithms that achieve sublinear regret, each\nworking with different assumptions on the impairment effect. We introduce a new\nnotion called bucketing in our algorithm design, and show how it can\neffectively address impairment as well as a broader class of temporal\nconstraints. Our regret bounds explicitly capture the cost of impairment and\nshow that it scales (sub-)linearly with the degree of impairment. Our work\ncomplements recent work on modeling delays and corruptions, and we provide\nexperimental evidence supporting our claims. \n\n"}
{"id": "1811.09540", "contents": "Title: High Dimensional Classification through $\\ell_0$-Penalized Empirical\n  Risk Minimization Abstract: We consider a high dimensional binary classification problem and construct a\nclassification procedure by minimizing the empirical misclassification risk\nwith a penalty on the number of selected features. We derive non-asymptotic\nprobability bounds on the estimated sparsity as well as on the excess\nmisclassification risk. In particular, we show that our method yields a sparse\nsolution whose l0-norm can be arbitrarily close to true sparsity with high\nprobability and obtain the rates of convergence for the excess\nmisclassification risk. The proposed procedure is implemented via the method of\nmixed integer linear programming. Its numerical performance is illustrated in\nMonte Carlo experiments. \n\n"}
{"id": "1811.09724", "contents": "Title: 3D Deep Learning with voxelized atomic configurations for modeling\n  atomistic potentials in complex solid-solution alloys Abstract: The need for advanced materials has led to the development of complex,\nmulti-component alloys or solid-solution alloys. These materials have shown\nexceptional properties like strength, toughness, ductility, electrical and\nelectronic properties. Current development of such material systems are\nhindered by expensive experiments and computationally demanding\nfirst-principles simulations. Atomistic simulations can provide reasonable\ninsights on properties in such material systems. However, the issue of\ndesigning robust potentials still exists. In this paper, we explore a deep\nconvolutional neural-network based approach to develop the atomistic potential\nfor such complex alloys to investigate materials for insights into controlling\nproperties. In the present work, we propose a voxel representation of the\natomic configuration of a cell and design a 3D convolutional neural network to\nlearn the interaction of the atoms. Our results highlight the performance of\nthe 3D convolutional neural network and its efficacy in machine-learning the\natomistic potential. We also explore the role of voxel resolution and provide\ninsights into the two bounding box methodologies implemented for voxelization. \n\n"}
{"id": "1811.10192", "contents": "Title: Tweedie Gradient Boosting for Extremely Unbalanced Zero-inflated Data Abstract: Tweedie's compound Poisson model is a popular method to model insurance\nclaims with probability mass at zero and nonnegative, highly right-skewed\ndistribution. In particular, it is not uncommon to have extremely unbalanced\ndata with excessively large proportion of zero claims, and even traditional\nTweedie model may not be satisfactory for fitting the data. In this paper, we\npropose a boosting-assisted zero-inflated Tweedie model, called EMTboost, that\nallows zero probability mass to exceed a traditional model. We makes a\nnonparametric assumption on its Tweedie model component, that unlike a linear\nmodel, is able to capture nonlinearities, discontinuities, and complex higher\norder interactions among predictors. A specialized Expectation-Maximization\nalgorithm is developed that integrates a blockwise coordinate descent strategy\nand a gradient tree-boosting algorithm to estimate key model parameters. We use\nextensive simulation and data analysis on synthetic zero-inflated\nauto-insurance claim data to illustrate our method's prediction performance. \n\n"}
{"id": "1811.10746", "contents": "Title: MATCH-Net: Dynamic Prediction in Survival Analysis using Convolutional\n  Neural Networks Abstract: Accurate prediction of disease trajectories is critical for early\nidentification and timely treatment of patients at risk. Conventional methods\nin survival analysis are often constrained by strong parametric assumptions and\nlimited in their ability to learn from high-dimensional data, while existing\nneural network models are not readily-adapted to the longitudinal setting. This\npaper develops a novel convolutional approach that addresses these drawbacks.\nWe present MATCH-Net: a Missingness-Aware Temporal Convolutional Hitting-time\nNetwork, designed to capture temporal dependencies and heterogeneous\ninteractions in covariate trajectories and patterns of missingness. To the best\nof our knowledge, this is the first investigation of temporal convolutions in\nthe context of dynamic prediction for personalized risk prognosis. Using\nreal-world data from the Alzheimer's Disease Neuroimaging Initiative, we\ndemonstrate state-of-the-art performance without making any assumptions\nregarding underlying longitudinal or time-to-event processes attesting to the\nmodel's potential utility in clinical decision support. \n\n"}
{"id": "1811.10869", "contents": "Title: Efficient non-uniform quantizer for quantized neural network targeting\n  reconfigurable hardware Abstract: Convolutional Neural Networks (CNN) has become more popular choice for\nvarious tasks such as computer vision, speech recognition and natural language\nprocessing. Thanks to their large computational capability and throughput, GPUs\n,which are not power efficient and therefore does not suit low power systems\nsuch as mobile devices, are the most common platform for both training and\ninferencing tasks. Recent studies has shown that FPGAs can provide a good\nalternative to GPUs as a CNN accelerator, due to their re-configurable nature,\nlow power and small latency. In order for FPGA-based accelerators outperform\nGPUs in inference task, both the parameters of the network and the activations\nmust be quantized. While most works use uniform quantizers for both parameters\nand activations, it is not always the optimal one, and a non-uniform quantizer\nneed to be considered. In this work we introduce a custom hardware-friendly\napproach to implement non-uniform quantizers. In addition, we use a single\nscale integer representation of both parameters and activations, for both\ntraining and inference. The combined method yields a hardware efficient\nnon-uniform quantizer, fit for real-time applications. We have tested our\nmethod on CIFAR-10 and CIFAR-100 image classification datasets with ResNet-18\nand VGG-like architectures, and saw little degradation in accuracy. \n\n"}
{"id": "1811.10978", "contents": "Title: Neural Non-Stationary Spectral Kernel Abstract: Standard kernels such as Mat\\'ern or RBF kernels only encode simple monotonic\ndependencies within the input space. Spectral mixture kernels have been\nproposed as general-purpose, flexible kernels for learning and discovering more\ncomplicated patterns in the data. Spectral mixture kernels have recently been\ngeneralized into non-stationary kernels by replacing the mixture weights,\nfrequency means and variances by input-dependent functions. These functions\nhave also been modelled as Gaussian processes on their own. In this paper we\npropose modelling the hyperparameter functions with neural networks, and\nprovide an experimental comparison between the stationary spectral mixture and\nthe two non-stationary spectral mixtures. Scalable Gaussian process inference\nis implemented within the sparse variational framework for all the kernels\nconsidered. We show that the neural variant of the kernel is able to achieve\nthe best performance, among alternatives, on several benchmark datasets. \n\n"}
{"id": "1811.11427", "contents": "Title: Deep Collective Matrix Factorization for Augmented Multi-View Learning Abstract: Learning by integrating multiple heterogeneous data sources is a common\nrequirement in many tasks. Collective Matrix Factorization (CMF) is a technique\nto learn shared latent representations from arbitrary collections of matrices.\nIt can be used to simultaneously complete one or more matrices, for predicting\nthe unknown entries. Classical CMF methods assume linearity in the interaction\nof latent factors which can be restrictive and fails to capture complex\nnon-linear interactions. In this paper, we develop the first deep-learning\nbased method, called dCMF, for unsupervised learning of multiple shared\nrepresentations, that can model such non-linear interactions, from an arbitrary\ncollection of matrices. We address optimization challenges that arise due to\ndependencies between shared representations through Multi-Task Bayesian\nOptimization and design an acquisition function adapted for collective learning\nof hyperparameters. Our experiments show that dCMF significantly outperforms\nprevious CMF algorithms in integrating heterogeneous data for predictive\nmodeling. Further, on two tasks - recommendation and prediction of gene-disease\nassociation - dCMF outperforms state-of-the-art matrix completion algorithms\nthat can utilize auxiliary sources of information. \n\n"}
{"id": "1811.11456", "contents": "Title: GIRNet: Interleaved Multi-Task Recurrent State Sequence Models Abstract: In several natural language tasks, labeled sequences are available in\nseparate domains (say, languages), but the goal is to label sequences with\nmixed domain (such as code-switched text). Or, we may have available models for\nlabeling whole passages (say, with sentiments), which we would like to exploit\ntoward better position-specific label inference (say, target-dependent\nsentiment annotation). A key characteristic shared across such tasks is that\ndifferent positions in a primary instance can benefit from different `experts'\ntrained from auxiliary data, but labeled primary instances are scarce, and\nlabeling the best expert for each position entails unacceptable cognitive\nburden. We propose GITNet, a unified position-sensitive multi-task recurrent\nneural network (RNN) architecture for such applications. Auxiliary and primary\ntasks need not share training instances. Auxiliary RNNs are trained over\nauxiliary instances. A primary instance is also submitted to each auxiliary\nRNN, but their state sequences are gated and merged into a novel composite\nstate sequence tailored to the primary inference task. Our approach is in sharp\ncontrast to recent multi-task networks like the cross-stitch and sluice\nnetwork, which do not control state transfer at such fine granularity. We\ndemonstrate the superiority of GIRNet using three applications: sentiment\nclassification of code-switched passages, part-of-speech tagging of\ncode-switched text, and target position-sensitive annotation of sentiment in\nmonolingual passages. In all cases, we establish new state-of-the-art\nperformance beyond recent competitive baselines. \n\n"}
{"id": "1811.11733", "contents": "Title: orthoDr: Semiparametric Dimension Reduction via Orthogonality\n  Constrained Optimization Abstract: orthoDr is a package in R that solves dimension reduction problems using\northogonality constrained optimization approach. The package serves as a\nunified framework for many regression and survival analysis dimension reduction\nmodels that utilize semiparametric estimating equations. The main computational\nmachinery of orthoDr is a first-order algorithm developed by\n\\cite{wen2013feasible} for optimization within the Stiefel manifold. We\nimplement the algorithm through Rcpp and OpenMP for fast computation. In\naddition, we developed a general-purpose solver for such constrained problems\nwith user-specified objective functions, which works as a drop-in version of\noptim(). The package also serves as a platform for future methodology\ndevelopments along this line of work. \n\n"}
{"id": "1811.12162", "contents": "Title: Effective Resistance-based Germination of Seed Sets for Community\n  Detection Abstract: Community detection is, at its core, an attempt to attach an interpretable\nfunction to an otherwise indecipherable form. The importance of labeling\ncommunities has obvious implications for identifying clusters in social\nnetworks, but it has a number of equally relevant applications in product\nrecommendations, biological systems, and many forms of classification. The\nlocal variety of community detection starts with a small set of labeled seed\nnodes, and aims to estimate the community containing these nodes. One of the\nmost ubiquitous methods - due to its simplicity and efficiency - is\npersonalized PageRank. The most obvious bottleneck for deploying this form of\nPageRank successfully is the quality of the seeds. We introduce a \"germination\"\nstage for these seeds, where an effective resistance-based approach is used to\nincrease the quality and number of seeds from which a community is detected. By\nbreaking seed set expansion into a two-step process, we aim to utilize two\ndistinct random walk-based approaches in the regimes in which they excel. In\nsynthetic and real network data, a simple, greedy algorithm which minimizes the\neffective resistance diameter combined with PageRank achieves clear\nimprovements in precision and recall over a standalone PageRank procedure. \n\n"}
{"id": "1811.12800", "contents": "Title: On the maximal number of real embeddings of minimally rigid graphs in\n  $\\mathbb{R}^2$, $\\mathbb{R}^3$ and $S^2$ Abstract: Rigidity theory studies the properties of graphs that can have rigid\nembeddings in a euclidean space $\\mathbb{R}^d$ or on a sphere and which in\naddition satisfy certain edge length constraints. One of the major open\nproblems in this field is to determine lower and upper bounds on the number of\nrealizations with respect to a given number of vertices. This problem is\nclosely related to the classification of rigid graphs according to their\nmaximal number of real embeddings.\n  In this paper, we are interested in finding edge lengths that can maximize\nthe number of real embeddings of minimally rigid graphs in the plane, space,\nand on the sphere. We use algebraic formulations to provide upper bounds. To\nfind values of the parameters that lead to graphs with a large number of real\nrealizations, possibly attaining the (algebraic) upper bounds, we use some\nstandard heuristics and we also develop a new method inspired by coupler\ncurves. We apply this new method to obtain embeddings in $\\mathbb{R}^3$. One of\nits main novelties is that it allows us to sample efficiently from a larger\nnumber of parameters by selecting only a subset of them at each iteration.\n  Our results include a full classification of the 7-vertex graphs according to\ntheir maximal numbers of real embeddings in the cases of the embeddings in\n$\\mathbb{R}^2$ and $\\mathbb{R}^3$, while in the case of $S^2$ we achieve this\nclassification for all 6-vertex graphs. Additionally, by increasing the number\nof embeddings of selected graphs, we improve the previously known asymptotic\nlower bound on the maximum number of realizations. The methods and the results\nconcerning the spatial embeddings are part of the proceedings of ISSAC 2018\n(Bartzos et al, 2018). \n\n"}
{"id": "1812.00365", "contents": "Title: Quick Best Action Identification in Linear Bandit Problems Abstract: In this paper, we consider a best action identification problem in the\nstochastic linear bandit setup with a fixed confident constraint. In the\nconsidered best action identification problem, instead of minimizing the\naccumulative regret as done in existing works, the learner aims to obtain an\naccurate estimate of the underlying parameter based on his action and reward\nsequences. To improve the estimation efficiency, the learner is allowed to\nselect his action based his historical information; hence the whole procedure\nis designed in a sequential adaptive manner. We first show that the existing\nalgorithms designed to minimize the accumulative regret is not a consistent\nestimator and hence is not a good policy for our problem. We then characterize\na lower bound on the estimation error for any policy. We further design a\nsimple policy and show that the estimation error of the designed policy\nachieves the same scaling order as that of the derived lower bound. \n\n"}
{"id": "1812.00366", "contents": "Title: A Tverberg type theorem for collectively unavoidable complexes Abstract: We prove (Theorem 2.4) that the symmetrized deleted join\n$SymmDelJoin(\\mathcal{K})$ of a \"balanced family\" $\\mathcal{K} = \\langle\nK_i\\rangle_{i=1}^r$ of collectively $r$-unavoidable subcomplexes of $2^{[m]}$\nis $(m-r-1)$-connected. As a consequence we obtain a Tverberg-Van Kampen-Flores\ntype result (Theorem 3.2) which is more conceptual and more general then\npreviously known results. Already the case $r=2$ of Theorem 3.2 seems to be new\nas an extension of the classical Van Kampen-Flores theorem. The main tool used\nin the paper is R. Forman's discrete Morse theory. \n\n"}
{"id": "1812.00456", "contents": "Title: Revisiting the Softmax Bellman Operator: New Benefits and New\n  Perspective Abstract: The impact of softmax on the value function itself in reinforcement learning\n(RL) is often viewed as problematic because it leads to sub-optimal value (or\nQ) functions and interferes with the contraction properties of the Bellman\noperator. Surprisingly, despite these concerns, and independent of its effect\non exploration, the softmax Bellman operator when combined with Deep\nQ-learning, leads to Q-functions with superior policies in practice, even\noutperforming its double Q-learning counterpart. To better understand how and\nwhy this occurs, we revisit theoretical properties of the softmax Bellman\noperator, and prove that $(i)$ it converges to the standard Bellman operator\nexponentially fast in the inverse temperature parameter, and $(ii)$ the\ndistance of its Q function from the optimal one can be bounded. These alone do\nnot explain its superior performance, so we also show that the softmax operator\ncan reduce the overestimation error, which may give some insight into why a\nsub-optimal operator leads to better performance in the presence of value\nfunction approximation. A comparison among different Bellman operators is then\npresented, showing the trade-offs when selecting them. \n\n"}
{"id": "1812.00532", "contents": "Title: Large Spectral Density Matrix Estimation by Thresholding Abstract: Spectral density matrix estimation of multivariate time series is a classical\nproblem in time series and signal processing. In modern neuroscience, spectral\ndensity based metrics are commonly used for analyzing functional connectivity\namong brain regions. In this paper, we develop a non-asymptotic theory for\nregularized estimation of high-dimensional spectral density matrices of\nGaussian and linear processes using thresholded versions of averaged\nperiodograms. Our theoretical analysis ensures that consistent estimation of\nspectral density matrix of a $p$-dimensional time series using $n$ samples is\npossible under high-dimensional regime $\\log p / n \\rightarrow 0$ as long as\nthe true spectral density is approximately sparse. A key technical component of\nour analysis is a new concentration inequality of average periodogram around\nits expectation, which is of independent interest. Our estimation consistency\nresults complement existing results for shrinkage based estimators of\nmultivariate spectral density, which require no assumption on sparsity but only\nensure consistent estimation in a regime $p^2/n \\rightarrow 0$. In addition,\nour proposed thresholding based estimators perform consistent and automatic\nedge selection when learning coherence networks among the components of a\nmultivariate time series. We demonstrate the advantage of our estimators using\nsimulation studies and a real data application on functional connectivity\nanalysis with fMRI data. \n\n"}
{"id": "1812.00812", "contents": "Title: Mapping Informal Settlements in Developing Countries with\n  Multi-resolution, Multi-spectral Data Abstract: Detecting and mapping informal settlements encompasses several of the United\nNations sustainable development goals. This is because informal settlements are\nhome to the most socially and economically vulnerable people on the planet.\nThus, understanding where these settlements are is of paramount importance to\nboth government and non-government organizations (NGOs), such as the United\nNations Children's Fund (UNICEF), who can use this information to deliver\neffective social and economic aid. We propose two effective methods for\ndetecting and mapping the locations of informal settlements. One uses only\nlow-resolution (LR), freely available, Sentinel-2 multispectral satellite\nimagery with noisy annotations, whilst the other is a deep learning approach\nthat uses only costly very-high-resolution (VHR) satellite imagery. To our\nknowledge, we are the first to map informal settlements successfully with\nlow-resolution satellite imagery. We extensively evaluate and compare the\nproposed methods. Please find additional material at\nhttps://frontierdevelopmentlab.github.io/informal-settlements/. \n\n"}
{"id": "1812.00879", "contents": "Title: Image-based model parameter optimization using Model-Assisted Generative\n  Adversarial Networks Abstract: We propose and demonstrate the use of a model-assisted generative adversarial\nnetwork (GAN) to produce fake images that accurately match true images through\nthe variation of the parameters of the model that describes the features of the\nimages. The generator learns the model parameter values that produce fake\nimages that best match the true images. Two case studies show excellent\nagreement between the generated best match parameters and the true parameters.\nThe best match model parameter values can be used to retune the default\nsimulation to minimize any bias when applying image recognition techniques to\nfake and true images. In the case of a real-world experiment, the true images\nare experimental data with unknown true model parameter values, and the fake\nimages are produced by a simulation that takes the model parameters as input.\nThe model-assisted GAN uses a convolutional neural network to emulate the\nsimulation for all parameter values that, when trained, can be used as a\nconditional generator for fast fake-image production. \n\n"}
{"id": "1812.01097", "contents": "Title: LEAF: A Benchmark for Federated Settings Abstract: Modern federated networks, such as those comprised of wearable devices,\nmobile phones, or autonomous vehicles, generate massive amounts of data each\nday. This wealth of data can help to learn models that can improve the user\nexperience on each device. However, the scale and heterogeneity of federated\ndata presents new challenges in research areas such as federated learning,\nmeta-learning, and multi-task learning. As the machine learning community\nbegins to tackle these challenges, we are at a critical time to ensure that\ndevelopments made in these areas are grounded with realistic benchmarks. To\nthis end, we propose LEAF, a modular benchmarking framework for learning in\nfederated settings. LEAF includes a suite of open-source federated datasets, a\nrigorous evaluation framework, and a set of reference implementations, all\ngeared towards capturing the obstacles and intricacies of practical federated\nenvironments. \n\n"}
{"id": "1812.01710", "contents": "Title: GANtruth - an unpaired image-to-image translation method for driving\n  scenarios Abstract: Synthetic image translation has significant potentials in autonomous\ntransportation systems. That is due to the expense of data collection and\nannotation as well as the unmanageable diversity of real-words situations. The\nmain issue with unpaired image-to-image translation is the ill-posed nature of\nthe problem. In this work, we propose a novel method for constraining the\noutput space of unpaired image-to-image translation. We make the assumption\nthat the environment of the source domain is known (e.g. synthetically\ngenerated), and we propose to explicitly enforce preservation of the\nground-truth labels on the translated images.\n  We experiment on preserving ground-truth information such as semantic\nsegmentation, disparity, and instance segmentation. We show significant\nevidence that our method achieves improved performance over the\nstate-of-the-art model of UNIT for translating images from SYNTHIA to\nCityscapes. The generated images are perceived as more realistic in human\nsurveys and outperforms UNIT when used in a domain adaptation scenario for\nsemantic segmentation. \n\n"}
{"id": "1812.01718", "contents": "Title: Deep Learning for Classical Japanese Literature Abstract: Much of machine learning research focuses on producing models which perform\nwell on benchmark tasks, in turn improving our understanding of the challenges\nassociated with those tasks. From the perspective of ML researchers, the\ncontent of the task itself is largely irrelevant, and thus there have\nincreasingly been calls for benchmark tasks to more heavily focus on problems\nwhich are of social or cultural relevance. In this work, we introduce\nKuzushiji-MNIST, a dataset which focuses on Kuzushiji (cursive Japanese), as\nwell as two larger, more challenging datasets, Kuzushiji-49 and\nKuzushiji-Kanji. Through these datasets, we wish to engage the machine learning\ncommunity into the world of classical Japanese literature. Dataset available at\nhttps://github.com/rois-codh/kmnist \n\n"}
{"id": "1812.01815", "contents": "Title: Uncertainty Sampling is Preconditioned Stochastic Gradient Descent on\n  Zero-One Loss Abstract: Uncertainty sampling, a popular active learning algorithm, is used to reduce\nthe amount of data required to learn a classifier, but it has been observed in\npractice to converge to different parameters depending on the initialization\nand sometimes to even better parameters than standard training on all the data.\nIn this work, we give a theoretical explanation of this phenomenon, showing\nthat uncertainty sampling on a convex loss can be interpreted as performing a\npreconditioned stochastic gradient step on a smoothed version of the population\nzero-one loss that converges to the population zero-one loss. Furthermore,\nuncertainty sampling moves in a descent direction and converges to stationary\npoints of the smoothed population zero-one loss. Experiments on synthetic and\nreal datasets support this connection. \n\n"}
{"id": "1812.02609", "contents": "Title: A Framework for Adaptive MCMC Targeting Multimodal Distributions Abstract: We propose a new Monte Carlo method for sampling from multimodal\ndistributions. The idea of this technique is based on splitting the task into\ntwo: finding the modes of a target distribution $\\pi$ and sampling, given the\nknowledge of the locations of the modes. The sampling algorithm relies on steps\nof two types: local ones, preserving the mode; and jumps to regions associated\nwith different modes. Besides, the method learns the optimal parameters of the\nalgorithm while it runs, without requiring user intervention. Our technique\nshould be considered as a flexible framework, in which the design of moves can\nfollow various strategies known from the broad MCMC literature.\n  In order to design an adaptive scheme that facilitates both local and jump\nmoves, we introduce an auxiliary variable representing each mode and we define\na new target distribution $\\tilde{\\pi}$ on an augmented state space\n$\\mathcal{X}~\\times~\\mathcal{I}$, where $\\mathcal{X}$ is the original state\nspace of $\\pi$ and $\\mathcal{I}$ is the set of the modes. As the algorithm runs\nand updates its parameters, the target distribution $\\tilde{\\pi}$ also keeps\nbeing modified. This motivates a new class of algorithms, Auxiliary Variable\nAdaptive MCMC. We prove general ergodic results for the whole class before\nspecialising to the case of our algorithm. \n\n"}
{"id": "1812.02696", "contents": "Title: Differentially Private Fair Learning Abstract: Motivated by settings in which predictive models may be required to be\nnon-discriminatory with respect to certain attributes (such as race), but even\ncollecting the sensitive attribute may be forbidden or restricted, we initiate\nthe study of fair learning under the constraint of differential privacy. We\ndesign two learning algorithms that simultaneously promise differential privacy\nand equalized odds, a 'fairness' condition that corresponds to equalizing false\npositive and negative rates across protected groups. Our first algorithm is a\nprivate implementation of the equalized odds post-processing approach of [Hardt\net al., 2016]. This algorithm is appealingly simple, but must be able to use\nprotected group membership explicitly at test time, which can be viewed as a\nform of 'disparate treatment'. Our second algorithm is a differentially private\nversion of the oracle-efficient in-processing approach of [Agarwal et al.,\n2018] that can be used to find the optimal fair classifier, given access to a\nsubroutine that can solve the original (not necessarily fair) learning problem.\nThis algorithm is more complex but need not have access to protected group\nmembership at test time. We identify new tradeoffs between fairness, accuracy,\nand privacy that emerge only when requiring all three properties, and show that\nthese tradeoffs can be milder if group membership may be used at test time. We\nconclude with a brief experimental evaluation. \n\n"}
{"id": "1812.02783", "contents": "Title: Finite-Sample Analysis For Decentralized Batch Multi-Agent Reinforcement\n  Learning With Networked Agents Abstract: Despite the increasing interest in multi-agent reinforcement learning (MARL)\nin multiple communities, understanding its theoretical foundation has long been\nrecognized as a challenging problem. In this work, we address this problem by\nproviding a finite-sample analysis for decentralized batch MARL with networked\nagents. Specifically, we consider two decentralized MARL settings, where teams\nof agents are connected by time-varying communication networks, and either\ncollaborate or compete in a zero-sum game setting, without any central\ncontroller. These settings cover many conventional MARL settings in the\nliterature. For both settings, we develop batch MARL algorithms that can be\nimplemented in a decentralized fashion, and quantify the finite-sample errors\nof the estimated action-value functions. Our error analysis captures how the\nfunction class, the number of samples within each iteration, and the number of\niterations determine the statistical accuracy of the proposed algorithms. Our\nresults, compared to the finite-sample bounds for single-agent RL, involve\nadditional error terms caused by decentralized computation, which is inherent\nin our decentralized MARL setting. This work appears to be the first\nfinite-sample analysis for batch MARL, a step towards rigorous theoretical\nunderstanding of general MARL algorithms in the finite-sample regime. \n\n"}
{"id": "1812.03596", "contents": "Title: Task-Free Continual Learning Abstract: Methods proposed in the literature towards continual deep learning typically\noperate in a task-based sequential learning setup. A sequence of tasks is\nlearned, one at a time, with all data of current task available but not of\nprevious or future tasks. Task boundaries and identities are known at all\ntimes. This setup, however, is rarely encountered in practical applications.\nTherefore we investigate how to transform continual learning to an online\nsetup. We develop a system that keeps on learning over time in a streaming\nfashion, with data distributions gradually changing and without the notion of\nseparate tasks. To this end, we build on the work on Memory Aware Synapses, and\nshow how this method can be made online by providing a protocol to decide i)\nwhen to update the importance weights, ii) which data to use to update them,\nand iii) how to accumulate the importance weights at each update step.\nExperimental results show the validity of the approach in the context of two\napplications: (self-)supervised learning of a face recognition model by\nwatching soap series and learning a robot to avoid collisions. \n\n"}
{"id": "1812.03599", "contents": "Title: Fast convergence rates of deep neural networks for classification Abstract: We derive the fast convergence rates of a deep neural network (DNN)\nclassifier with the rectified linear unit (ReLU) activation function learned\nusing the hinge loss. We consider three cases for a true model: (1) a smooth\ndecision boundary, (2) smooth conditional class probability, and (3) the margin\ncondition (i.e., the probability of inputs near the decision boundary is\nsmall). We show that the DNN classifier learned using the hinge loss achieves\nfast rate convergences for all three cases provided that the architecture\n(i.e., the number of layers, number of nodes and sparsity). is carefully\nselected. An important implication is that DNN architectures are very flexible\nfor use in various cases without much modification. In addition, we consider a\nDNN classifier learned by minimizing the cross-entropy, and show that the DNN\nclassifier achieves a fast convergence rate under the condition that the\nconditional class probabilities of most data are sufficiently close to either 1\nor zero. This assumption is not unusual for image recognition because human\nbeings are extremely good at recognizing most images. To confirm our\ntheoretical explanation, we present the results of a small numerical study\nconducted to compare the hinge loss and cross-entropy. \n\n"}
{"id": "1812.04801", "contents": "Title: Can I trust you more? Model-Agnostic Hierarchical Explanations Abstract: Interactions such as double negation in sentences and scene interactions in\nimages are common forms of complex dependencies captured by state-of-the-art\nmachine learning models. We propose Mah\\'e, a novel approach to provide\nModel-agnostic hierarchical \\'explanations of how powerful machine learning\nmodels, such as deep neural networks, capture these interactions as either\ndependent on or free of the context of data instances. Specifically, Mah\\'e\nprovides context-dependent explanations by a novel local interpretation\nalgorithm that effectively captures any-order interactions, and obtains\ncontext-free explanations through generalizing context-dependent interactions\nto explain global behaviors. Experimental results show that Mah\\'e obtains\nimproved local interaction interpretations over state-of-the-art methods and\nsuccessfully explains interactions that are context-free. \n\n"}
{"id": "1812.05692", "contents": "Title: Bayesian Sparsification of Gated Recurrent Neural Networks Abstract: Bayesian methods have been successfully applied to sparsify weights of neural\nnetworks and to remove structure units from the networks, e. g. neurons. We\napply and further develop this approach for gated recurrent architectures.\nSpecifically, in addition to sparsification of individual weights and neurons,\nwe propose to sparsify preactivations of gates and information flow in LSTM. It\nmakes some gates and information flow components constant, speeds up forward\npass and improves compression. Moreover, the resulting structure of gate\nsparsity is interpretable and depends on the task. Code is available on github:\nhttps://github.com/tipt0p/SparseBayesianRNN \n\n"}
{"id": "1812.05796", "contents": "Title: AdaFlow: Domain-Adaptive Density Estimator with Application to Anomaly\n  Detection and Unpaired Cross-Domain Translation Abstract: We tackle unsupervised anomaly detection (UAD), a problem of detecting data\nthat significantly differ from normal data. UAD is typically solved by using\ndensity estimation. Recently, deep neural network (DNN)-based density\nestimators, such as Normalizing Flows, have been attracting attention. However,\none of their drawbacks is the difficulty in adapting them to the change in the\nnormal data's distribution. To address this difficulty, we propose AdaFlow, a\nnew DNN-based density estimator that can be easily adapted to the change of the\ndistribution. AdaFlow is a unified model of a Normalizing Flow and Adaptive\nBatch-Normalizations, a module that enables DNNs to adapt to new distributions.\nAdaFlow can be adapted to a new distribution by just conducting forward\npropagation once per sample; hence, it can be used on devices that have limited\ncomputational resources. We have confirmed the effectiveness of the proposed\nmodel through an anomaly detection in a sound task. We also propose a method of\napplying AdaFlow to the unpaired cross-domain translation problem, in which one\nhas to train a cross-domain translation model with only unpaired samples. We\nhave confirmed that our model can be used for the cross-domain translation\nproblem through experiments on image datasets. \n\n"}
{"id": "1812.05981", "contents": "Title: Why ReLU Units Sometimes Die: Analysis of Single-Unit Error\n  Backpropagation in Neural Networks Abstract: Recently, neural networks in machine learning use rectified linear units\n(ReLUs) in early processing layers for better performance. Training these\nstructures sometimes results in \"dying ReLU units\" with near-zero outputs. We\nfirst explore this condition via simulation using the CIFAR-10 dataset and\nvariants of two popular convolutive neural network architectures. Our\nexplorations show that the output activation probability Pr[y>0] is generally\nless than 0.5 at system convergence for layers that do not employ skip\nconnections, and this activation probability tends to decrease as one\nprogresses from input layer to output layer. Employing a simplified model of a\nsingle ReLU unit trained by a variant of error backpropagation, we then perform\na statistical convergence analysis to explore the model's evolutionary\nbehavior. Our analysis describes the potentially-slower convergence speeds of\ndying ReLU units, and this issue can occur regardless of how the weights are\ninitialized. \n\n"}
{"id": "1812.05994", "contents": "Title: Products of Many Large Random Matrices and Gradients in Deep Neural\n  Networks Abstract: We study products of random matrices in the regime where the number of terms\nand the size of the matrices simultaneously tend to infinity. Our main theorem\nis that the logarithm of the $\\ell_2$ norm of such a product applied to any\nfixed vector is asymptotically Gaussian. The fluctuations we find can be\nthought of as a finite temperature correction to the limit in which first the\nsize and then the number of matrices tend to infinity. Depending on the scaling\nlimit considered, the mean and variance of the limiting Gaussian depend only on\neither the first two or the first four moments of the measure from which matrix\nentries are drawn. We also obtain explicit error bounds on the moments of the\nnorm and the Kolmogorov-Smirnov distance to a Gaussian. Finally, we apply our\nresult to obtain precise information about the stability of gradients in\nrandomly initialized deep neural networks with ReLU activations. This provides\na quantitative measure of the extent to which the exploding and vanishing\ngradient problem occurs in a fully connected neural network with ReLU\nactivations and a given architecture. \n\n"}
{"id": "1812.06288", "contents": "Title: A De Bruijn-Erd\\H{o}s theorem in graphs? Abstract: A set of $n$ points in the Euclidean plane determines at least $n$ distinct\nlines unless these $n$ points are collinear. In 2006, Chen and Chv\\'atal asked\nwhether the same statement holds true in general metric spaces, where the line\ndetermined by points $x$ and $y$ is defined as the set consisting of $x$, $y$,\nand all points $z$ such that one of the three points $x,y,z$ lies between the\nother two. The conjecture that it does hold true remains unresolved even in the\nspecial case where the metric space arises from a connected undirected graph\nwith unit lengths assigned to edges. We trace its curriculum vitae and point\nout twenty-nine related open problems plus three additional conjectures. \n\n"}
{"id": "1812.06408", "contents": "Title: Human Pose and Path Estimation from Aerial Video using Dynamic\n  Classifier Selection Abstract: We consider the problem of estimating human pose and trajectory by an aerial\nrobot with a monocular camera in near real time. We present a preliminary\nsolution whose distinguishing feature is a dynamic classifier selection\narchitecture. In our solution, each video frame is corrected for perspective\nusing projective transformation. Then, two alternative feature sets are used:\n(i) Histogram of Oriented Gradients (HOG) of the silhouette, (ii) Convolutional\nNeural Network (CNN) features of the RGB image. The features (HOG or CNN) are\nclassified using a dynamic classifier. A class is defined as a pose-viewpoint\npair, and a total of 64 classes are defined to represent a forward walking and\nturning gait sequence. Our solution provides three main advantages: (i)\nClassification is efficient due to dynamic selection (4-class vs. 64-class\nclassification). (ii) Classification errors are confined to neighbors of the\ntrue view-points. (iii) The robust temporal relationship between poses is used\nto resolve the left-right ambiguities of human silhouettes. Experiments\nconducted on both fronto-parallel videos and aerial videos confirm our solution\ncan achieve accurate pose and trajectory estimation for both scenarios. We\nfound using HOG features provides higher accuracy than using CNN features. For\nexample, applying the HOG-based variant of our scheme to the 'walking on a\nfigure 8-shaped path' dataset (1652 frames) achieved estimation accuracies of\n99.6% for viewpoints and 96.2% for number of poses. \n\n"}
{"id": "1812.06686", "contents": "Title: Sepsis Prediction and Vital Signs Ranking in Intensive Care Unit\n  Patients Abstract: We study multiple rule-based and machine learning (ML) models for sepsis\ndetection. We report the first neural network detection and prediction results\non three categories of sepsis. We have used the retrospective Medical\nInformation Mart for Intensive Care (MIMIC)-III dataset, restricted to\nintensive care unit (ICU) patients. Features for prediction were created from\nonly common vital sign measurements. We show significant improvement of AUC\nscore using neural network based ensemble model compared to single ML and\nrule-based models. For the detection of sepsis, severe sepsis, and septic\nshock, our model achieves an AUC of 0.97, 0.96 and 0.91, respectively. Four\nhours before the positive hours, it predicts the same three categories with an\nAUC of 0.90, 0.91 and 0.90 respectively. Further, we ranked the features and\nfound that using six vital signs consistently provides higher detection and\nprediction AUC for all the models tested. Our novel ensemble model achieves\nhighest AUC in detecting and predicting sepsis, severe sepsis, and septic shock\nin the MIMIC-III ICU patients, and is amenable to deployment in hospital\nsettings. \n\n"}
{"id": "1812.07491", "contents": "Title: S-hypersimplices, pulling triangulations, and monotone paths Abstract: An $S$-hypersimplex for $S \\subseteq \\{0,1, \\dots,d\\}$ is the convex hull of\nall $0/1$-vectors of length $d$ with coordinate sum in $S$. These polytopes\ngeneralize the classical hypersimplices as well as cubes, crosspolytopes, and\nhalfcubes. In this paper we study faces and dissections of $S$-hypersimplices.\nMoreover, we show that monotone path polytopes of $S$-hypersimplices yield all\ntypes of multipermutahedra. In analogy to cubes, we also show that the number\nof simplices in a pulling triangulation of a halfcube is independent of the\npulling order. \n\n"}
{"id": "1812.07544", "contents": "Title: Information-Directed Exploration for Deep Reinforcement Learning Abstract: Efficient exploration remains a major challenge for reinforcement learning.\nOne reason is that the variability of the returns often depends on the current\nstate and action, and is therefore heteroscedastic. Classical exploration\nstrategies such as upper confidence bound algorithms and Thompson sampling fail\nto appropriately account for heteroscedasticity, even in the bandit setting.\nMotivated by recent findings that address this issue in bandits, we propose to\nuse Information-Directed Sampling (IDS) for exploration in reinforcement\nlearning. As our main contribution, we build on recent advances in\ndistributional reinforcement learning and propose a novel, tractable\napproximation of IDS for deep Q-learning. The resulting exploration strategy\nexplicitly accounts for both parametric uncertainty and heteroscedastic\nobservation noise. We evaluate our method on Atari games and demonstrate a\nsignificant improvement over alternative approaches. \n\n"}
{"id": "1812.08352", "contents": "Title: Sequential Attention GAN for Interactive Image Editing Abstract: Most existing text-to-image synthesis tasks are static single-turn\ngeneration, based on pre-defined textual descriptions of images. To explore\nmore practical and interactive real-life applications, we introduce a new task\n- Interactive Image Editing, where users can guide an agent to edit images via\nmulti-turn textual commands on-the-fly. In each session, the agent takes a\nnatural language description from the user as the input and modifies the image\ngenerated in the previous turn to a new design, following the user description.\nThe main challenges in this sequential and interactive image generation task\nare two-fold: 1) contextual consistency between a generated image and the\nprovided textual description; 2) step-by-step region-level modification to\nmaintain visual consistency across the generated image sequence in each\nsession. To address these challenges, we propose a novel Sequential Attention\nGenerative Adversarial Net-work (SeqAttnGAN), which applies a neural state\ntracker to encode the previous image and the textual description in each turn\nof the sequence, and uses a GAN framework to generate a modified version of the\nimage that is consistent with the preceding images and coherent with the\ndescription. To achieve better region-specific refinement, we also introduce a\nsequential attention mechanism into the model. To benchmark on the new task, we\nintroduce two new datasets, Zap-Seq and DeepFashion-Seq, which contain\nmulti-turn sessions with image-description sequences in the fashion domain.\nExperiments on both datasets show that the proposed SeqAttnGANmodel outperforms\nstate-of-the-art approaches on the interactive image editing task across all\nevaluation metrics including visual quality, image sequence coherence, and\ntext-image consistency. \n\n"}
{"id": "1812.09771", "contents": "Title: A determinantal point process for column subset selection Abstract: Dimensionality reduction is a first step of many machine learning pipelines.\nTwo popular approaches are principal component analysis, which projects onto a\nsmall number of well chosen but non-interpretable directions, and feature\nselection, which selects a small number of the original features. Feature\nselection can be abstracted as a numerical linear algebra problem called the\ncolumn subset selection problem (CSSP). CSSP corresponds to selecting the best\nsubset of columns of a matrix $X \\in \\mathbb{R}^{N \\times d}$, where\n\\emph{best} is often meant in the sense of minimizing the approximation error,\ni.e., the norm of the residual after projection of $X$ onto the space spanned\nby the selected columns. Such an optimization over subsets of $\\{1,\\dots,d\\}$\nis usually impractical. One workaround that has been vastly explored is to\nresort to polynomial-cost, random subset selection algorithms that favor small\nvalues of this approximation error. We propose such a randomized algorithm,\nbased on sampling from a projection determinantal point process (DPP), a\nrepulsive distribution over a fixed number $k$ of indices $\\{1,\\dots,d\\}$ that\nfavors diversity among the selected columns. We give bounds on the ratio of the\nexpected approximation error for this DPP over the optimal error of PCA. These\nbounds improve over the state-of-the-art bounds of \\emph{volume sampling} when\nsome realistic structural assumptions are satisfied for $X$. Numerical\nexperiments suggest that our bounds are tight, and that our algorithms have\ncomparable performance with the \\emph{double phase} algorithm, often considered\nto be the practical state-of-the-art. Column subset selection with DPPs thus\ninherits the best of both worlds: good empirical performance and tight error\nbounds. \n\n"}
{"id": "1812.11240", "contents": "Title: Dynamic Planning Networks Abstract: We introduce Dynamic Planning Networks (DPN), a novel architecture for deep\nreinforcement learning, that combines model-based and model-free aspects for\nonline planning. Our architecture learns to dynamically construct plans using a\nlearned state-transition model by selecting and traversing between simulated\nstates and actions to maximize information before acting. In contrast to\nmodel-free methods, model-based planning lets the agent efficiently test action\nhypotheses without performing costly trial-and-error in the environment. DPN\nlearns to efficiently form plans by expanding a single action-conditional state\ntransition at a time instead of exhaustively evaluating each action, reducing\nthe required number of state-transitions during planning by up to 96%. We\nobserve various emergent planning patterns used to solve environments,\nincluding classical search methods such as breadth-first and depth-first\nsearch. DPN shows improved data efficiency, performance, and generalization to\nnew and unseen domains in comparison to several baselines. \n\n"}
{"id": "1812.11433", "contents": "Title: On the Construction of Knockoffs in Case-Control Studies Abstract: Consider a case-control study in which we have a random sample, constructed\nin such a way that the proportion of cases in our sample is different from that\nin the general population---for instance, the sample is constructed to achieve\na fixed ratio of cases to controls. Imagine that we wish to determine which of\nthe potentially many covariates under study truly influence the response by\napplying the new model-X knockoffs approach. This paper demonstrates that it\nsuffices to design knockoff variables using data that may have a different\nratio of cases to controls. For example, the knockoff variables can be\nconstructed using the distribution of the original variables under any of the\nfollowing scenarios: (1) a population of controls only; (2) a population of\ncases only; (3) a population of cases and controls mixed in an arbitrary\nproportion (irrespective of the fraction of cases in the sample at hand). The\nconsequence is that knockoff variables may be constructed using unlabeled data,\nwhich is often available more easily than labeled data, while maintaining\nType-I error guarantees. \n\n"}
{"id": "1812.11780", "contents": "Title: Weakly Supervised Active Learning with Cluster Annotation Abstract: In this work, we introduce a novel framework that employs cluster annotation\nto boost active learning by reducing the number of human interactions required\nto train deep neural networks. Instead of annotating single samples\nindividually, humans can also label clusters, producing a higher number of\nannotated samples with the cost of a small label error. Our experiments show\nthat the proposed framework requires 82% and 87% less human interactions for\nCIFAR-10 and EuroSAT datasets respectively when compared with the\nfully-supervised training while maintaining similar performance on the test\nset. \n\n"}
{"id": "1812.11958", "contents": "Title: Gray-box Adversarial Testing for Control Systems with Machine Learning\n  Component Abstract: Neural Networks (NN) have been proposed in the past as an effective means for\nboth modeling and control of systems with very complex dynamics. However,\ndespite the extensive research, NN-based controllers have not been adopted by\nthe industry for safety critical systems. The primary reason is that systems\nwith learning based controllers are notoriously hard to test and verify. Even\nharder is the analysis of such systems against system-level specifications. In\nthis paper, we provide a gradient based method for searching the input space of\na closed-loop control system in order to find adversarial samples against some\nsystem-level requirements. Our experimental results show that combined with\nrandomized search, our method outperforms Simulated Annealing optimization. \n\n"}
{"id": "1901.01427", "contents": "Title: Poincar\\'e Wasserstein Autoencoder Abstract: This work presents a reformulation of the recently proposed Wasserstein\nautoencoder framework on a non-Euclidean manifold, the Poincar\\'e ball model of\nthe hyperbolic space. By assuming the latent space to be hyperbolic, we can use\nits intrinsic hierarchy to impose structure on the learned latent space\nrepresentations. We demonstrate the model in the visual domain to analyze some\nof its properties and show competitive results on a graph link prediction task. \n\n"}
{"id": "1901.01499", "contents": "Title: Understanding the (un)interpretability of natural image distributions\n  using generative models Abstract: Probability density estimation is a classical and well studied problem, but\nstandard density estimation methods have historically lacked the power to model\ncomplex and high-dimensional image distributions. More recent generative models\nleverage the power of neural networks to implicitly learn and represent\nprobability models over complex images. We describe methods to extract explicit\nprobability density estimates from GANs, and explore the properties of these\nimage density functions. We perform sanity check experiments to provide\nevidence that these probabilities are reasonable. However, we also show that\ndensity functions of natural images are difficult to interpret and thus limited\nin use. We study reasons for this lack of interpretability, and show that we\ncan get interpretability back by doing density estimation on latent\nrepresentations of images. \n\n"}
{"id": "1901.02051", "contents": "Title: DPPNet: Approximating Determinantal Point Processes with Deep Networks Abstract: Determinantal Point Processes (DPPs) provide an elegant and versatile way to\nsample sets of items that balance the point-wise quality with the set-wise\ndiversity of selected items. For this reason, they have gained prominence in\nmany machine learning applications that rely on subset selection. However,\nsampling from a DPP over a ground set of size $N$ is a costly operation,\nrequiring in general an $O(N^3)$ preprocessing cost and an $O(Nk^3)$ sampling\ncost for subsets of size $k$. We approach this problem by introducing DPPNets:\ngenerative deep models that produce DPP-like samples for arbitrary ground sets.\nWe develop an inhibitive attention mechanism based on transformer networks that\ncaptures a notion of dissimilarity between feature vectors. We show\ntheoretically that such an approximation is sensible as it maintains the\nguarantees of inhibition or dissimilarity that makes DPPs so powerful and\nunique. Empirically, we demonstrate that samples from our model receive high\nlikelihood under the more expensive DPP alternative. \n\n"}
{"id": "1901.02182", "contents": "Title: Comments on \"Deep Neural Networks with Random Gaussian Weights: A\n  Universal Classification Strategy?\" Abstract: In a recently published paper [1], it is shown that deep neural networks\n(DNNs) with random Gaussian weights preserve the metric structure of the data,\nwith the property that the distance shrinks more when the angle between the two\ndata points is smaller. We agree that the random projection setup considered in\n[1] preserves distances with a high probability. But as far as we are\nconcerned, the relation between the angle of the data points and the output\ndistances is quite the opposite, i.e., smaller angles result in a weaker\ndistance shrinkage. This leads us to conclude that Theorem 3 and Figure 5 in\n[1] are not accurate. Hence the usage of random Gaussian weights in DNNs cannot\nprovide an ability of universal classification or treating in-class and\nout-of-class data separately. Consequently, the behavior of networks consisting\nof random Gaussian weights only is not useful to explain how DNNs achieve\nstate-of-art results in a large variety of problems. \n\n"}
{"id": "1901.02717", "contents": "Title: Reliable and Explainable Machine Learning Methods for Accelerated\n  Material Discovery Abstract: Material scientists are increasingly adopting the use of machine learning\n(ML) for making potentially important decisions, such as, discovery,\ndevelopment, optimization, synthesis and characterization of materials.\nHowever, despite ML's impressive performance in commercial applications,\nseveral unique challenges exist when applying ML in materials science\napplications. In such a context, the contributions of this work are twofold.\nFirst, we identify common pitfalls of existing ML techniques when learning from\nunderrepresented/imbalanced material data. Specifically, we show that with\nimbalanced data, standard methods for assessing quality of ML models break down\nand lead to misleading conclusions. Furthermore, we found that the model's own\nconfidence score cannot be trusted and model introspection methods (using\nsimpler models) do not help as they result in loss of predictive performance\n(reliability-explainability trade-off). Second, to overcome these challenges,\nwe propose a general-purpose explainable and reliable machine-learning\nframework. Specifically, we propose a novel pipeline that employs an ensemble\nof simpler models to reliably predict material properties. We also propose a\ntransfer learning technique and show that the performance loss due to models'\nsimplicity can be overcome by exploiting correlations among different material\nproperties. A new evaluation metric and a trust score to better quantify the\nconfidence in the predictions are also proposed. To improve the\ninterpretability, we add a rationale generator component to our framework which\nprovides both model-level and decision-level explanations. Finally, we\ndemonstrate the versatility of our technique on two applications: 1) predicting\nproperties of crystalline compounds, and 2) identifying novel potentially\nstable solar cell materials. \n\n"}
{"id": "1901.02878", "contents": "Title: A Constructive Approach for One-Shot Training of Neural Networks Using\n  Hypercube-Based Topological Coverings Abstract: In this paper we presented a novel constructive approach for training deep\nneural networks using geometric approaches. We show that a topological covering\ncan be used to define a class of distributed linear matrix inequalities, which\nin turn directly specify the shape and depth of a neural network architecture.\nThe key insight is a fundamental relationship between linear matrix\ninequalities and their ability to bound the shape of data, and the rectified\nlinear unit (ReLU) activation function employed in modern neural networks. We\nshow that unit cover geometry and cover porosity are two design variables in\ncover-constructive learning that play a critical role in defining the\ncomplexity of the model and generalizability of the resulting neural network\nclassifier. In the context of cover-constructive learning, these findings\nunderscore the age old trade-off between model complexity and overfitting (as\nquantified by the number of elements in the data cover) and generalizability on\ntest data. Finally, we benchmark on algorithm on the Iris, MNIST, and Wine\ndataset and show that the constructive algorithm is able to train a deep neural\nnetwork classifier in one shot, achieving equal or superior levels of training\nand test classification accuracy with reduced training time. \n\n"}
{"id": "1901.04295", "contents": "Title: On-Demand Video Dispatch Networks: A Scalable End-to-End Learning\n  Approach Abstract: We design a dispatch system to improve the peak service quality of video on\ndemand (VOD). Our system predicts the hot videos during the peak hours of the\nnext day based on the historical requests, and dispatches to the content\ndelivery networks (CDNs) at the previous off-peak time. In order to scale to\nbillions of videos, we build the system with two neural networks, one for video\nclustering and the other for dispatch policy developing. The clustering network\nemploys autoencoder layers and reduces the video number to a fixed value. The\npolicy network employs fully connected layers and ranks the clustered videos\nwith dispatch probabilities. The two networks are coupled with weight-sharing\ntemporal layers, which analyze the video request sequences with convolutional\nand recurrent modules. Therefore, the clustering and dispatch tasks are trained\nin an end-to-end mechanism. The real-world results show that our approach\nachieves an average prediction accuracy of 17%, compared with 3% from the\npresent baseline method, for the same amount of dispatches. \n\n"}
{"id": "1901.04321", "contents": "Title: Large-scale Collaborative Filtering with Product Embeddings Abstract: The application of machine learning techniques to large-scale personalized\nrecommendation problems is a challenging task. Such systems must make sense of\nenormous amounts of implicit feedback in order to understand user preferences\nacross numerous product categories. This paper presents a deep learning based\nsolution to this problem within the collaborative filtering with implicit\nfeedback framework. Our approach combines neural attention mechanisms, which\nallow for context dependent weighting of past behavioral signals, with\nrepresentation learning techniques to produce models which obtain extremely\nhigh coverage, can easily incorporate new information as it becomes available,\nand are computationally efficient. Offline experiments demonstrate significant\nperformance improvements when compared to several alternative methods from the\nliterature. Results from an online setting show that the approach compares\nfavorably with current production techniques used to produce personalized\nproduct recommendations. \n\n"}
{"id": "1901.04454", "contents": "Title: Posterior inference unchained with EL_2O Abstract: Statistical inference of analytically non-tractable posteriors is a difficult\nproblem because of marginalization of correlated variables and stochastic\nmethods such as MCMC and VI are commonly used. We argue that stochastic KL\ndivergence minimization used by MCMC and VI is noisy, and we propose instead\nEL_2O, expectation optimization of L_2 distance squared between the approximate\nlog posterior q and the un-normalized log posterior of p. When sampling from q\nthe solutions agree with stochastic KL divergence minimization based VI in the\nlarge sample limit, however EL_2O method is free of sampling noise, has better\noptimization properties, and requires only as many sample evaluations as the\nnumber of parameters we are optimizing if q covers p. As a consequence,\nincreasing the expressivity of q improves both the quality of results and the\nconvergence rate, allowing EL_2O to approach exact inference. Use of automatic\ndifferentiation methods enables us to develop Hessian, gradient and gradient\nfree versions of the method, which can determine M(M+2)/2+1, M+1 and 1\nparameter(s) of q with a single sample, respectively. EL_2O provides a reliable\nestimate of the quality of the approximating posterior, and converges rapidly\non full rank gaussian approximation for q and extensions beyond it, such as\nnonlinear transformations and gaussian mixtures. These can handle general\nposteriors, while still allowing fast analytic marginalizations. We test it on\nseveral examples, including a realistic 13 dimensional galaxy clustering\nanalysis, showing that it is several orders of magnitude faster than MCMC,\nwhile giving smooth and accurate non-gaussian posteriors, often requiring a few\nto a few dozen of iterations only. \n\n"}
{"id": "1901.04807", "contents": "Title: An upper bound on the number of perfect quadratic forms Abstract: In a recent preprint on arXiv Roland Bacher showed that the number $p_d$ of\nnon-similar perfect $d$-dimensional quadratic forms satisfies $e^{\\Omega(d)} <\np_d < e^{O(d^3\\log(d))}$. We improve the upper bound to $e^{O(d^2\\log(d))}$ by\na volumetric argument based on Voronoi's first reduction theory. \n\n"}
{"id": "1901.04866", "contents": "Title: Practical Lossless Compression with Latent Variables using Bits Back\n  Coding Abstract: Deep latent variable models have seen recent success in many data domains.\nLossless compression is an application of these models which, despite having\nthe potential to be highly useful, has yet to be implemented in a practical\nmanner. We present `Bits Back with ANS' (BB-ANS), a scheme to perform lossless\ncompression with latent variable models at a near optimal rate. We demonstrate\nthis scheme by using it to compress the MNIST dataset with a variational\nauto-encoder model (VAE), achieving compression rates superior to standard\nmethods with only a simple VAE. Given that the scheme is highly amenable to\nparallelization, we conclude that with a sufficiently high quality generative\nmodel this scheme could be used to achieve substantial improvements in\ncompression rate with acceptable running time. We make our implementation\navailable open source at https://github.com/bits-back/bits-back . \n\n"}
{"id": "1901.05850", "contents": "Title: Fast Deep Learning for Automatic Modulation Classification Abstract: In this work, we investigate the feasibility and effectiveness of employing\ndeep learning algorithms for automatic recognition of the modulation type of\nreceived wireless communication signals from subsampled data. Recent work\nconsidered a GNU radio-based data set that mimics the imperfections in a real\nwireless channel and uses 10 different modulation types. A Convolutional Neural\nNetwork (CNN) architecture was then developed and shown to achieve performance\nthat exceeds that of expert-based approaches. Here, we continue this line of\nwork and investigate deep neural network architectures that deliver high\nclassification accuracy. We identify three architectures - namely, a\nConvolutional Long Short-term Deep Neural Network (CLDNN), a Long Short-Term\nMemory neural network (LSTM), and a deep Residual Network (ResNet) - that lead\nto typical classification accuracy values around 90% at high SNR. We then study\nalgorithms to reduce the training time by minimizing the size of the training\ndata set, while incurring a minimal loss in classification accuracy. To this\nend, we demonstrate the performance of Principal Component Analysis in\nsignificantly reducing the training time, while maintaining good performance at\nlow SNR. We also investigate subsampling techniques that further reduce the\ntraining time, and pave the way for online classification at high SNR. Finally,\nwe identify representative SNR values for training each of the candidate\narchitectures, and consequently, realize drastic reductions of the training\ntime, with negligible loss in classification accuracy. \n\n"}
{"id": "1901.05947", "contents": "Title: Stochastic Gradient Descent on a Tree: an Adaptive and Robust Approach\n  to Stochastic Convex Optimization Abstract: Online minimization of an unknown convex function over the interval $[0,1]$\nis considered under first-order stochastic bandit feedback, which returns a\nrandom realization of the gradient of the function at each query point. Without\nknowing the distribution of the random gradients, a learning algorithm\nsequentially chooses query points with the objective of minimizing regret\ndefined as the expected cumulative loss of the function values at the query\npoints in excess to the minimum value of the function. An approach based on\ndevising a biased random walk on an infinite-depth binary tree constructed\nthrough successive partitioning of the domain of the function is developed.\nEach move of the random walk is guided by a sequential test based on confidence\nbounds on the empirical mean constructed using the law of the iterated\nlogarithm. With no tuning parameters, this learning algorithm is robust to\nheavy-tailed noise with infinite variance and adaptive to unknown function\ncharacteristics (specifically, convex, strongly convex, and nonsmooth). It\nachieves the corresponding optimal regret orders (up to a $\\sqrt{\\log T}$ or a\n$\\log\\log T$ factor) in each class of functions and offers better or matching\nregret orders than the classical stochastic gradient descent approach which\nrequires the knowledge of the function characteristics for tuning the sequence\nof step-sizes. \n\n"}
{"id": "1901.05954", "contents": "Title: Diverse mini-batch Active Learning Abstract: We study the problem of reducing the amount of labeled training data required\nto train supervised classification models. We approach it by leveraging Active\nLearning, through sequential selection of examples which benefit the model\nmost. Selecting examples one by one is not practical for the amount of training\nexamples required by the modern Deep Learning models. We consider the\nmini-batch Active Learning setting, where several examples are selected at\nonce. We present an approach which takes into account both informativeness of\nthe examples for the model, as well as the diversity of the examples in a\nmini-batch. By using the well studied K-means clustering algorithm, this\napproach scales better than the previously proposed approaches, and achieves\ncomparable or better performance. \n\n"}
{"id": "1901.07186", "contents": "Title: Towards Learning to Imitate from a Single Video Demonstration Abstract: Agents that can learn to imitate given video observation -- \\emph{without\ndirect access to state or action information} are more applicable to learning\nin the natural world. However, formulating a reinforcement learning (RL) agent\nthat facilitates this goal remains a significant challenge. We approach this\nchallenge using contrastive training to learn a reward function comparing an\nagent's behaviour with a single demonstration. We use a Siamese recurrent\nneural network architecture to learn rewards in space and time between motion\nclips while training an RL policy to minimize this distance. Through\nexperimentation, we also find that the inclusion of multi-task data and\nadditional image encoding losses improve the temporal consistency of the\nlearned rewards and, as a result, significantly improves policy learning. We\ndemonstrate our approach on simulated humanoid, dog, and raptor agents in 2D\nand a quadruped and a humanoid in 3D. We show that our method outperforms\ncurrent state-of-the-art techniques in these environments and can learn to\nimitate from a single video demonstration. \n\n"}
{"id": "1901.07666", "contents": "Title: Rapid identification of pathogenic bacteria using Raman spectroscopy and\n  deep learning Abstract: Rapid identification of bacteria is essential to prevent the spread of\ninfectious disease, help combat antimicrobial resistance, and improve patient\noutcomes. Raman optical spectroscopy promises to combine bacterial detection,\nidentification, and antibiotic susceptibility testing in a single step.\nHowever, achieving clinically relevant speeds and accuracies remains\nchallenging due to the weak Raman signal from bacterial cells and the large\nnumber of bacterial species and phenotypes. By amassing the largest known\ndataset of bacterial Raman spectra, we are able to apply state-of-the-art deep\nlearning approaches to identify 30 of the most common bacterial pathogens from\nnoisy Raman spectra, achieving antibiotic treatment identification accuracies\nof 99.0$\\pm$0.1%. This novel approach distinguishes between\nmethicillin-resistant and -susceptible isolates of Staphylococcus aureus (MRSA\nand MSSA) as well as a pair of isogenic MRSA and MSSA that are genetically\nidentical apart from deletion of the mecA resistance gene, indicating the\npotential for culture-free detection of antibiotic resistance. Results from\ninitial clinical validation are promising: using just 10 bacterial spectra from\neach of 25 isolates, we achieve 99.0$\\pm$1.9% species identification accuracy.\nOur combined Raman-deep learning system represents an important\nproof-of-concept for rapid, culture-free identification of bacterial isolates\nand antibiotic resistance and could be readily extended for diagnostics on\nblood, urine, and sputum. \n\n"}
{"id": "1901.08256", "contents": "Title: Large-Batch Training for LSTM and Beyond Abstract: Large-batch training approaches have enabled researchers to utilize\nlarge-scale distributed processing and greatly accelerate deep-neural net (DNN)\ntraining. For example, by scaling the batch size from 256 to 32K, researchers\nhave been able to reduce the training time of ResNet50 on ImageNet from 29\nhours to 2.2 minutes (Ying et al., 2018). In this paper, we propose a new\napproach called linear-epoch gradual-warmup (LEGW) for better large-batch\ntraining. With LEGW, we are able to conduct large-batch training for both CNNs\nand RNNs with the Sqrt Scaling scheme. LEGW enables Sqrt Scaling scheme to be\nuseful in practice and as a result we achieve much better results than the\nLinear Scaling learning rate scheme. For LSTM applications, we are able to\nscale the batch size by a factor of 64 without losing accuracy and without\ntuning the hyper-parameters. For CNN applications, LEGW is able to achieve the\nsame accuracy even as we scale the batch size to 32K. LEGW works better than\nprevious large-batch auto-tuning techniques. LEGW achieves a 5.3X average\nspeedup over the baselines for four LSTM-based applications on the same\nhardware. We also provide some theoretical explanations for LEGW. \n\n"}
{"id": "1901.08394", "contents": "Title: Application of Decision Rules for Handling Class Imbalance in Semantic\n  Segmentation Abstract: As part of autonomous car driving systems, semantic segmentation is an\nessential component to obtain a full understanding of the car's environment.\nOne difficulty, that occurs while training neural networks for this purpose, is\nclass imbalance of training data. Consequently, a neural network trained on\nunbalanced data in combination with maximum a-posteriori classification may\neasily ignore classes that are rare in terms of their frequency in the dataset.\nHowever, these classes are often of highest interest. We approach such\npotential misclassifications by weighting the posterior class probabilities\nwith the prior class probabilities which in our case are the inverse\nfrequencies of the corresponding classes in the training dataset. More\nprecisely, we adopt a localized method by computing the priors pixel-wise such\nthat the impact can be analyzed at pixel level as well. In our experiments, we\ntrain one network from scratch using a proprietary dataset containing 20,000\nannotated frames of video sequences recorded from street scenes. The evaluation\non our test set shows an increase of average recall with regard to instances of\npedestrians and info signs by $25\\%$ and $23.4\\%$, respectively. In addition,\nwe significantly reduce the non-detection rate for instances of the same\nclasses by $61\\%$ and $38\\%$. \n\n"}
{"id": "1901.08544", "contents": "Title: Learning Space Partitions for Nearest Neighbor Search Abstract: Space partitions of $\\mathbb{R}^d$ underlie a vast and important class of\nfast nearest neighbor search (NNS) algorithms. Inspired by recent theoretical\nwork on NNS for general metric spaces [Andoni, Naor, Nikolov, Razenshteyn,\nWaingarten STOC 2018, FOCS 2018], we develop a new framework for building space\npartitions reducing the problem to balanced graph partitioning followed by\nsupervised classification. We instantiate this general approach with the KaHIP\ngraph partitioner [Sanders, Schulz SEA 2013] and neural networks, respectively,\nto obtain a new partitioning procedure called Neural Locality-Sensitive Hashing\n(Neural LSH). On several standard benchmarks for NNS, our experiments show that\nthe partitions obtained by Neural LSH consistently outperform partitions found\nby quantization-based and tree-based methods as well as classic, data-oblivious\nLSH. \n\n"}
{"id": "1901.08556", "contents": "Title: Visualized Insights into the Optimization Landscape of Fully\n  Convolutional Networks Abstract: Many image processing tasks involve image-to-image mapping, which can be\naddressed well by fully convolutional networks (FCN) without any heavy\npreprocessing. Although empirically designing and training FCNs can achieve\nsatisfactory results, reasons for the improvement in performance are slightly\nambiguous. Our study is to make progress in understanding their generalization\nabilities through visualizing the optimization landscapes. The visualization of\nobjective functions is obtained by choosing a solution and projecting its\nvicinity onto a 3D space. We compare three FCN-based networks (two existing\nmodels and a new proposed in this paper for comparison) on multiple datasets.\nIt has been observed in practice that the connections from the pre-pooled\nfeature maps to the post-upsampled can achieve better results. We investigate\nthe cause and provide experiments to shows that the skip-layer connections in\nFCN can promote flat optimization landscape, which is well known to generalize\nbetter. Additionally, we explore the relationship between the models\ngeneralization ability and loss surface under different batch sizes. Results\nshow that large-batch training makes the model converge to sharp minimizers\nwith chaotic vicinities while small-batch method leads the model to flat\nminimizers with smooth and nearly convex regions. Our work may contribute to\ninsights and analysis for designing and training FCNs. \n\n"}
{"id": "1901.09330", "contents": "Title: Reward Shaping via Meta-Learning Abstract: Reward shaping is one of the most effective methods to tackle the crucial yet\nchallenging problem of credit assignment in Reinforcement Learning (RL).\nHowever, designing shaping functions usually requires much expert knowledge and\nhand-engineering, and the difficulties are further exacerbated given multiple\nsimilar tasks to solve. In this paper, we consider reward shaping on a\ndistribution of tasks, and propose a general meta-learning framework to\nautomatically learn the efficient reward shaping on newly sampled tasks,\nassuming only shared state space but not necessarily action space. We first\nderive the theoretically optimal reward shaping in terms of credit assignment\nin model-free RL. We then propose a value-based meta-learning algorithm to\nextract an effective prior over the optimal reward shaping. The prior can be\napplied directly to new tasks, or provably adapted to the task-posterior while\nsolving the task within few gradient updates. We demonstrate the effectiveness\nof our shaping through significantly improved learning efficiency and\ninterpretable visualizations across various settings, including notably a\nsuccessful transfer from DQN to DDPG. \n\n"}
{"id": "1901.09401", "contents": "Title: SGD: General Analysis and Improved Rates Abstract: We propose a general yet simple theorem describing the convergence of SGD\nunder the arbitrary sampling paradigm. Our theorem describes the convergence of\nan infinite array of variants of SGD, each of which is associated with a\nspecific probability law governing the data selection rule used to form\nmini-batches. This is the first time such an analysis is performed, and most of\nour variants of SGD were never explicitly considered in the literature before.\nOur analysis relies on the recently introduced notion of expected smoothness\nand does not rely on a uniform bound on the variance of the stochastic\ngradients. By specializing our theorem to different mini-batching strategies,\nsuch as sampling with replacement and independent sampling, we derive exact\nexpressions for the stepsize as a function of the mini-batch size. With this we\ncan also determine the mini-batch size that optimizes the total complexity, and\nshow explicitly that as the variance of the stochastic gradient evaluated at\nthe minimum grows, so does the optimal mini-batch size. For zero variance, the\noptimal mini-batch size is one. Moreover, we prove insightful\nstepsize-switching rules which describe when one should switch from a constant\nto a decreasing stepsize regime. \n\n"}
{"id": "1901.09455", "contents": "Title: Off-Policy Deep Reinforcement Learning by Bootstrapping the Covariate\n  Shift Abstract: In this paper we revisit the method of off-policy corrections for\nreinforcement learning (COP-TD) pioneered by Hallak et al. (2017). Under this\nmethod, online updates to the value function are reweighted to avoid divergence\nissues typical of off-policy learning. While Hallak et al.'s solution is\nappealing, it cannot easily be transferred to nonlinear function approximation.\nFirst, it requires a projection step onto the probability simplex; second, even\nthough the operator describing the expected behavior of the off-policy learning\nalgorithm is convergent, it is not known to be a contraction mapping, and\nhence, may be more unstable in practice. We address these two issues by\nintroducing a discount factor into COP-TD. We analyze the behavior of\ndiscounted COP-TD and find it better behaved from a theoretical perspective. We\nalso propose an alternative soft normalization penalty that can be minimized\nonline and obviates the need for an explicit projection step. We complement our\nanalysis with an empirical evaluation of the two techniques in an off-policy\nsetting on the game Pong from the Atari domain where we find discounted COP-TD\nto be better behaved in practice than the soft normalization penalty. Finally,\nwe perform a more extensive evaluation of discounted COP-TD in 5 games of the\nAtari domain, where we find performance gains for our approach. \n\n"}
{"id": "1901.09590", "contents": "Title: TuckER: Tensor Factorization for Knowledge Graph Completion Abstract: Knowledge graphs are structured representations of real world facts. However,\nthey typically contain only a small subset of all possible facts. Link\nprediction is a task of inferring missing facts based on existing ones. We\npropose TuckER, a relatively straightforward but powerful linear model based on\nTucker decomposition of the binary tensor representation of knowledge graph\ntriples. TuckER outperforms previous state-of-the-art models across standard\nlink prediction datasets, acting as a strong baseline for more elaborate\nmodels. We show that TuckER is a fully expressive model, derive sufficient\nbounds on its embedding dimensionalities and demonstrate that several\npreviously introduced linear models can be viewed as special cases of TuckER. \n\n"}
{"id": "1901.09720", "contents": "Title: CLIC: Curriculum Learning and Imitation for object Control in\n  non-rewarding environments Abstract: In this paper we study a new reinforcement learning setting where the\nenvironment is non-rewarding, contains several possibly related objects of\nvarious controllability, and where an apt agent Bob acts independently, with\nnon-observable intentions. We argue that this setting defines a realistic\nscenario and we present a generic discrete-state discrete-action model of such\nenvironments. To learn in this environment, we propose an unsupervised\nreinforcement learning agent called CLIC for Curriculum Learning and Imitation\nfor Control. CLIC learns to control individual objects in its environment, and\nimitates Bob's interactions with these objects. It selects objects to focus on\nwhen training and imitating by maximizing its learning progress. We show that\nCLIC is an effective baseline in our new setting. It can effectively observe\nBob to gain control of objects faster, even if Bob is not explicitly teaching.\nIt can also follow Bob when he acts as a mentor and provides ordered\ndemonstrations. Finally, when Bob controls objects that the agent cannot, or in\npresence of a hierarchy between objects in the environment, we show that CLIC\nignores non-reproducible and already mastered interactions with objects,\nresulting in a greater benefit from imitation. \n\n"}
{"id": "1901.09895", "contents": "Title: Modularization of End-to-End Learning: Case Study in Arcade Games Abstract: Complex environments and tasks pose a difficult problem for holistic\nend-to-end learning approaches. Decomposition of an environment into\ninteracting controllable and non-controllable objects allows supervised\nlearning for non-controllable objects and universal value function approximator\nlearning for controllable objects. Such decomposition should lead to a shorter\nlearning time and better generalisation capability. Here, we consider\narcade-game environments as sets of interacting objects (controllable,\nnon-controllable) and propose a set of functional modules that are specialized\non mastering different types of interactions in a broad range of environments.\nThe modules utilize regression, supervised learning, and reinforcement learning\nalgorithms. Results of this case study in different Atari games suggest that\nhuman-level performance can be achieved by a learning agent within a human\namount of game experience (10-15 minutes game time) when a proper decomposition\nof an environment or a task is provided. However, automatization of such\ndecomposition remains a challenging problem. This case study shows how a model\nof a causal structure underlying an environment or a task can benefit learning\ntime and generalization capability of the agent, and argues in favor of\nexploiting modular structure in contrast to using pure end-to-end learning\napproaches. \n\n"}
{"id": "1901.09993", "contents": "Title: Label Efficient Semi-Supervised Learning via Graph Filtering Abstract: Graph-based methods have been demonstrated as one of the most effective\napproaches for semi-supervised learning, as they can exploit the connectivity\npatterns between labeled and unlabeled data samples to improve learning\nperformance. However, existing graph-based methods either are limited in their\nability to jointly model graph structures and data features, such as the\nclassical label propagation methods, or require a considerable amount of\nlabeled data for training and validation due to high model complexity, such as\nthe recent neural-network-based methods. In this paper, we address label\nefficient semi-supervised learning from a graph filtering perspective.\nSpecifically, we propose a graph filtering framework that injects graph\nsimilarity into data features by taking them as signals on the graph and\napplying a low-pass graph filter to extract useful data representations for\nclassification, where label efficiency can be achieved by conveniently\nadjusting the strength of the graph filter. Interestingly, this framework\nunifies two seemingly very different methods -- label propagation and graph\nconvolutional networks. Revisiting them under the graph filtering framework\nleads to new insights that improve their modeling capabilities and reduce model\ncomplexity. Experiments on various semi-supervised classification tasks on four\ncitation networks and one knowledge graph and one semi-supervised regression\ntask for zero-shot image recognition validate our findings and proposals. \n\n"}
{"id": "1901.10076", "contents": "Title: Learning Schatten--von Neumann Operators Abstract: We study the learnability of a class of compact operators known as\nSchatten--von Neumann operators. These operators between infinite-dimensional\nfunction spaces play a central role in a variety of applications in learning\ntheory and inverse problems. We address the question of sample complexity of\nlearning Schatten-von Neumann operators and provide an upper bound on the\nnumber of measurements required for the empirical risk minimizer to generalize\nwith arbitrary precision and probability, as a function of class parameter $p$.\nOur results give generalization guarantees for regression of\ninfinite-dimensional signals from infinite-dimensional data. Next, we adapt the\nrepresenter theorem of Abernethy \\emph{et al.} to show that empirical risk\nminimization over an a priori infinite-dimensional, non-compact set, can be\nconverted to a convex finite dimensional optimization problem over a compact\nset. In summary, the class of $p$-Schatten--von Neumann operators is probably\napproximately correct (PAC)-learnable via a practical convex program for any $p\n< \\infty$. \n\n"}
{"id": "1901.10257", "contents": "Title: A new tidy data structure to support exploration and modeling of\n  temporal data Abstract: Mining temporal data for information is often inhibited by a multitude of\nformats: irregular or multiple time intervals, point events that need\naggregating, multiple observational units or repeated measurements on multiple\nindividuals, and heterogeneous data types. On the other hand, the software\nsupporting time series modeling and forecasting, makes strict assumptions on\nthe data to be provided, typically requiring a matrix of numeric data with\nimplicit time indexes. Going from raw data to model-ready data is painful. This\nwork presents a cohesive and conceptual framework for organizing and\nmanipulating temporal data, which in turn flows into visualization, modeling\nand forecasting routines. Tidy data principles are extended to temporal data\nby: (1) mapping the semantics of a dataset into its physical layout; (2)\nincluding an explicitly declared index variable representing time; (3)\nincorporating a \"key\" comprising single or multiple variables to uniquely\nidentify units over time. This tidy data representation most naturally supports\nthinking of operations on the data as building blocks, forming part of a \"data\npipeline\" in time-based contexts. A sound data pipeline facilitates a fluent\nworkflow for analyzing temporal data. The infrastructure of tidy temporal data\nhas been implemented in the R package \"tsibble\". \n\n"}
{"id": "1901.10275", "contents": "Title: Differentially Private Markov Chain Monte Carlo Abstract: Recent developments in differentially private (DP) machine learning and DP\nBayesian learning have enabled learning under strong privacy guarantees for the\ntraining data subjects. In this paper, we further extend the applicability of\nDP Bayesian learning by presenting the first general DP Markov chain Monte\nCarlo (MCMC) algorithm whose privacy-guarantees are not subject to unrealistic\nassumptions on Markov chain convergence and that is applicable to posterior\ninference in arbitrary models. Our algorithm is based on a decomposition of the\nBarker acceptance test that allows evaluating the R\\'enyi DP privacy cost of\nthe accept-reject choice. We further show how to improve the DP guarantee\nthrough data subsampling and approximate acceptance tests. \n\n"}
{"id": "1901.10568", "contents": "Title: Stochastic Gradient MCMC for Nonlinear State Space Models Abstract: State space models (SSMs) provide a flexible framework for modeling complex\ntime series via a latent stochastic process. Inference for nonlinear,\nnon-Gaussian SSMs is often tackled with particle methods that do not scale well\nto long time series. The challenge is two-fold: not only do computations scale\nlinearly with time, as in the linear case, but particle filters additionally\nsuffer from increasing particle degeneracy with longer series. Stochastic\ngradient MCMC methods have been developed to scale Bayesian inference for\nfinite-state hidden Markov models and linear SSMs using buffered stochastic\ngradient estimates to account for temporal dependencies. We extend these\nstochastic gradient estimators to nonlinear SSMs using particle methods. We\npresent error bounds that account for both buffering error and particle error\nin the case of nonlinear SSMs that are log-concave in the latent process. We\nevaluate our proposed particle buffered stochastic gradient using stochastic\ngradient MCMC for inference on both long sequential synthetic and\nminute-resolution financial returns data, demonstrating the importance of this\nclass of methods. \n\n"}
{"id": "1901.10631", "contents": "Title: Dense graphs have rigid parts Abstract: While the problem of determining whether an embedding of a graph $G$ in\n$\\mathbb{R}^2$ is {\\it infinitesimally rigid} is well understood, specifying\nwhether a given embedding of $G$ is {\\it rigid} or not is still a hard task\nthat usually requires ad hoc arguments. In this paper, we show that {\\it every}\nembedding (not necessarily generic) of a dense enough graph (concretely, a\ngraph with at least $C_0n^{3/2}\\log n$ edges, for some absolute constant\n$C_0>0$), which satisfies some very mild general position requirements (no\nthree vertices of $G$ are embedded to a common line), must have a subframework\nof size at least three which is rigid. For the proof we use a connection,\nestablished in Raz [Ra], between the notion of graph rigidity and\nconfigurations of lines in $\\mathbb{R}^3$. This connection allows us to use\nproperties of line configurations established in Guth and Katz [GK2]. In fact,\nour proof requires an extended version of Guth and Katz result; the extension\nwe need is proved by J\\'anos Koll\\'ar in an Appendix to our paper.\n  We do not know whether our assumption on the number of edges being\n$\\Omega(n^{3/2}\\log n)$ is tight, and we provide a construction that shows that\nrequiring $\\Omega(n\\log n)$ edges is necessary. \n\n"}
{"id": "1901.10946", "contents": "Title: NAOMI: Non-Autoregressive Multiresolution Sequence Imputation Abstract: Missing value imputation is a fundamental problem in spatiotemporal modeling,\nfrom motion tracking to the dynamics of physical systems. Deep autoregressive\nmodels suffer from error propagation which becomes catastrophic for imputing\nlong-range sequences. In this paper, we take a non-autoregressive approach and\npropose a novel deep generative model: Non-AutOregressive Multiresolution\nImputation (NAOMI) to impute long-range sequences given arbitrary missing\npatterns. NAOMI exploits the multiresolution structure of spatiotemporal data\nand decodes recursively from coarse to fine-grained resolutions using a\ndivide-and-conquer strategy. We further enhance our model with adversarial\ntraining. When evaluated extensively on benchmark datasets from systems of both\ndeterministic and stochastic dynamics. NAOMI demonstrates significant\nimprovement in imputation accuracy (reducing average prediction error by 60%\ncompared to autoregressive counterparts) and generalization for long range\nsequences. \n\n"}
{"id": "1901.11459", "contents": "Title: Funnelling: A New Ensemble Method for Heterogeneous Transfer Learning\n  and its Application to Cross-Lingual Text Classification Abstract: Cross-lingual Text Classification (CLC) consists of automatically\nclassifying, according to a common set C of classes, documents each written in\none of a set of languages L, and doing so more accurately than when naively\nclassifying each document via its corresponding language-specific classifier.\nIn order to obtain an increase in the classification accuracy for a given\nlanguage, the system thus needs to also leverage the training examples written\nin the other languages. We tackle multilabel CLC via funnelling, a new ensemble\nlearning method that we propose here. Funnelling consists of generating a\ntwo-tier classification system where all documents, irrespectively of language,\nare classified by the same (2nd-tier) classifier. For this classifier all\ndocuments are represented in a common, language-independent feature space\nconsisting of the posterior probabilities generated by 1st-tier,\nlanguage-dependent classifiers. This allows the classification of all test\ndocuments, of any language, to benefit from the information present in all\ntraining documents, of any language. We present substantial experiments, run on\npublicly available multilingual text collections, in which funnelling is shown\nto significantly outperform a number of state-of-the-art baselines. All code\nand datasets (in vector form) are made publicly available. \n\n"}
{"id": "math/0202103", "contents": "Title: Reconstructing a Simple Polytope from its Graph Abstract: Blind and Mani (1987) proved that the entire combinatorial structure (the\nvertex-facet incidences) of a simple convex polytope is determined by its\nabstract graph. Their proof is not constructive. Kalai (1988) found a short,\nelegant, and algorithmic proof of that result. However, his algorithm has\nalways exponential running time. We show that the problem to reconstruct the\nvertex-facet incidences of a simple polytope P from its graph can be formulated\nas a combinatorial optimization problem that is strongly dual to the problem of\nfinding an abstract objective function on P (i.e., a shelling order of the\nfacets of the dual polytope of P). Thereby, we derive polynomial certificates\nfor both the vertex-facet incidences as well as for the abstract objective\nfunctions in terms of the graph of P. The paper is a variation on joint work\nwith Michael Joswig and Friederike Koerner (2001). \n\n"}
{"id": "math/0202204", "contents": "Title: Some Algorithmic Problems in Polytope Theory Abstract: This is a survey on algorithmic questions about combinatorial and geometric\nproperties of convex polytopes. We give a list of 35 problems; for each the\ncurrent state of knowledege on its theoretical complexity status is reported.\nThe problems are grouped into the sections ``Coordinate Descriptions'',\n``Combinatorial Structure'', ``Isomorphism'', ``Optimization'',\n``Realizability'', and ``Beyond Polytopes''. \n\n"}
{"id": "math/0204231", "contents": "Title: On the number of facets of three-dimensional Dirichlet stereohedra II:\n  Non-cubic Groups Abstract: We prove that Dirichlet stereohedra for non-cubic crystallographic groups in\ndimension 3 cannot have more than 80 facets. The bound depends on the\nparticular crystallographic group considered and is above 50 only on 9 of the\n97 affine conjugacy classes of them.\n  We also construct Dirichlet stereohedra with 32 and 29 facets for a hexagonal\nand a tetragonal group, respectively. \n\n"}
{"id": "math/0206027", "contents": "Title: Expansive Motions and the Polytope of Pointed Pseudo-Triangulations Abstract: We introduce the polytope of pointed pseudo-triangulations of a point set in\nthe plane, defined as the polytope of infinitesimal expansive motions of the\npoints subject to certain constraints on the increase of their distances. Its\n1-skeleton is the graph whose vertices are the pointed pseudo-triangulations of\nthe point set and whose edges are flips of interior pseudo-triangulation edges.\n  For points in convex position we obtain a new realization of the\nassociahedron, i.e., a geometric representation of the set of triangulations of\nan n-gon, or of the set of binary trees on n vertices, or of many other\ncombinatorial objects that are counted by the Catalan numbers. By considering\nthe 1-dimensional version of the polytope of constrained expansive motions we\nobtain a second distinct realization of the associahedron as a perturbation of\nthe positive cell in a Coxeter arrangement.\n  Our methods produce as a by-product a new proof that every simple polygon or\npolygonal arc in the plane has expansive motions, a key step in the proofs of\nthe Carpenter's Rule Theorem by Connelly, Demaine and Rote (2000) and by\nStreinu (2000). \n\n"}
{"id": "math/0302126", "contents": "Title: The polytope of non-crossing graphs on a planar point set Abstract: For any finite set $\\A$ of $n$ points in $\\R^2$, we define a\n$(3n-3)$-dimensional simple polyhedron whose face poset is isomorphic to the\nposet of ``non-crossing marked graphs'' with vertex set $\\A$, where a marked\ngraph is defined as a geometric graph together with a subset of its vertices.\nThe poset of non-crossing graphs on $\\A$ appears as the complement of the star\nof a face in that polyhedron.\n  The polyhedron has a unique maximal bounded face, of dimension $2n_i +n -3$\nwhere $n_i$ is the number of points of $\\A$ in the interior of $\\conv(\\A)$. The\nvertices of this polytope are all the pseudo-triangulations of $\\A$, and the\nedges are flips of two types: the traditional diagonal flips (in\npseudo-triangulations) and the removal or insertion of a single edge.\n  As a by-product of our construction we prove that all pseudo-triangulations\nare infinitesimally rigid graphs. \n\n"}
{"id": "math/0308050", "contents": "Title: Singular 0/1-matrices, and the hyperplanes spanned by random 0/1-vectors Abstract: Let $P(d)$ be the probability that a random 0/1-matrix of size $d \\times d$\nis singular, and let $E(d)$ be the expected number of 0/1-vectors in the linear\nsubspace spanned by d-1 random independent 0/1-vectors. (So $E(d)$ is the\nexpected number of cube vertices on a random affine hyperplane spanned by\nvertices of the cube.)\n  We prove that bounds on $P(d)$ are equivalent to bounds on $E(d)$: \\[ P(d) =\n(2^{-d} E(d) + \\frac{d^2}{2^{d+1}}) (1 + o(1)). \\] We also report about\ncomputational experiments pertaining to these numbers. \n\n"}
{"id": "math/0310142", "contents": "Title: Lower bounds for simplicial covers and triangulations of cubes Abstract: We show that the size of a minimal simplicial cover of a polytope $P$ is a\nlower bound for the size of a minimal triangulation of $P$, including ones with\nextra vertices. We then use this fact to study minimal triangulations of cubes,\nand we improve lower bounds for covers and triangulations in dimensions 4\nthrough at least 12 (and possibly more dimensions as well). Important\ningredients are an analysis of the number of exterior faces that a simplex in\nthe cube can have of a specified dimension and volume, and a characterization\nof corner simplices in terms of their exterior faces. \n\n"}
{"id": "math/0311481", "contents": "Title: The minimal spanning tree and the upper box dimension Abstract: We show that the alpha-weight of an MST over n points in a metric space with\nupper box dimension d has a bound independent of n if alpha is smaller than d\nand does not have one if alpha is larger than d. \n\n"}
{"id": "math/0312068", "contents": "Title: Tropical Halfspaces Abstract: As a new concept tropical halfspaces are introduced to the (linear algebraic)\ngeometry of the tropical semiring (R,min,+). This yields exterior descriptions\nof the tropical polytopes that were recently studied by Develin and Sturmfels\nin a variety of contexts. The key tool to the understanding is a newly defined\nsign of the tropical determinant, which shares remarkably many properties with\nthe ordinary sign of the determinant of a matrix. The methods are used to\nobtain an optimal tropical convex hull algorithm in two dimensions. \n\n"}
{"id": "math/0402047", "contents": "Title: Numerical cubature using error-correcting codes Abstract: We present a construction for improving numerical cubature formulas with\nequal weights and a convolution structure, in particular equal-weight product\nformulas, using linear error-correcting codes. The construction is most\neffective in low degree with extended BCH codes. Using it, we obtain several\nsequences of explicit, positive, interior cubature formulas with good\nasymptotics for each fixed degree $t$ as the dimension $n \\to \\infty$. Using a\nspecial quadrature formula for the interval [arXiv:math.PR/0408360], we obtain\nan equal-weight $t$-cubature formula on the $n$-cube with $O(n^{\\floor{t/2}})$\npoints, which is within a constant of the Stroud lower bound. We also obtain\n$t$-cubature formulas on the $n$-sphere, $n$-ball, and Gaussian $\\R^n$ with\n$O(n^{t-2})$ points when $t$ is odd. When $\\mu$ is spherically symmetric and\n$t=5$, we obtain $O(n^2)$ points. For each $t \\ge 4$, we also obtain explicit,\npositive, interior formulas for the $n$-simplex with $O(n^{t-1})$ points; for\n$t=3$, we obtain O(n) points. These constructions asymptotically improve the\nnon-constructive Tchakaloff bound.\n  Some related results were recently found independently by Victoir, who also\nnoted that the basic construction more directly uses orthogonal arrays. \n\n"}
{"id": "math/0409081", "contents": "Title: The Topological Tverberg Problem and winding numbers Abstract: The Topological Tverberg Theorem claims that any continuous map of a\n(q-1)(d+1)-simplex to \\R^d identifies points from q disjoint faces. (This has\nbeen proved for affine maps, for d=1, and if q is a prime power, but not yet in\ngeneral.) The Topological Tverberg Theorem can be restricted to maps of the\nd-skeleton of the simplex. We further show that it is equivalent to a ``Winding\nNumber Conjecture'' that concerns only maps of the (d-1)-skeleton of a\n(q-1)(d+1)-simplex to \\R^d. ``Many Tverberg partitions'' arise if and only if\nthere are ``many q-winding partitions.'' The d=2 case of the Winding Number\nConjecture is a problem about drawings of the complete graphs K_{3q-2} in the\nplane. We investigate graphs that are minimal with respect to the winding\nnumber condition. \n\n"}
{"id": "math/0507119", "contents": "Title: Lattice Delone simplices with super-exponential volume Abstract: In this short note we give a construction of an infinite series of Delone\nsimplices whose relative volume grows super-exponentially with their dimension.\nThis dramatically improves the previous best lower bound, which was linear. \n\n"}
{"id": "math/0507273", "contents": "Title: Geometric Reasoning with polymake Abstract: The mathematical software system polymake provides a wide range of functions\nfor convex polytopes, simplicial complexes, and other objects. A large part of\nthis paper is dedicated to a tutorial which exemplifies the usage. Later\nsections include a survey of research results obtained with the help of\npolymake so far and a short description of the technical background. \n\n"}
{"id": "math/0512529", "contents": "Title: Dissections, Hom-complexes and the Cayley trick Abstract: We show that certain canonical realizations of the complexes Hom(G,H) and\nHom_+(G,H) of (partial) graph homomorphisms studied by Babson and Kozlov are in\nfact instances of the polyhedral Cayley trick. For G a complete graph, we then\ncharacterize when a canonical projection of these complexes is itself again a\ncomplex, and exhibit several well-known objects that arise as cells or\nsubcomplexes of such projected Hom-complexes: the dissections of a convex\npolygon into k-gons, Postnikov's generalized permutohedra, staircase\ntriangulations, the complex dual to the lower faces of a cyclic polytope, and\nthe graph of weak compositions of an integer into a fixed number of summands. \n\n"}
{"id": "math/0611893", "contents": "Title: A centrally symmetric version of the cyclic polytope Abstract: We define a centrally symmetric analogue of the cyclic polytope and study its\nfacial structure. We conjecture that our polytopes provide asymptotically the\nlargest number of faces in all dimensions among all centrally symmetric\npolytopes with n vertices of a given even dimension d=2k when d is fixed and n\ngrows. For a fixed even dimension d=2k and an integer 0< j <k we prove that the\nmaximum possible number of j-dimensional faces of a centrally symmetric\nd-dimensional polytope with n vertices is at least (c_j(d)+o(1)) {n \\choose\nj+1} for some c_j(d)>0 and at most (1-2^{-d}+o(1)){n \\choose j+1} as n grows.\nWe show that c_1(d) \\geq (d-2)/(d-1). \n\n"}
