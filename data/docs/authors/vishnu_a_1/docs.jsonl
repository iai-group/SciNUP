{"id": "0706.2146", "contents": "Title: Efficient Multidimensional Data Redistribution for Resizable Parallel\n  Computations Abstract: Traditional parallel schedulers running on cluster supercomputers support\nonly static scheduling, where the number of processors allocated to an\napplication remains fixed throughout the execution of the job. This results in\nunder-utilization of idle system resources thereby decreasing overall system\nthroughput. In our research, we have developed a prototype framework called\nReSHAPE, which supports dynamic resizing of parallel MPI applications executing\non distributed memory platforms. The resizing library in ReSHAPE includes\nsupport for releasing and acquiring processors and efficiently redistributing\napplication state to a new set of processors. In this paper, we derive an\nalgorithm for redistributing two-dimensional block-cyclic arrays from $P$ to\n$Q$ processors, organized as 2-D processor grids. The algorithm ensures a\ncontention-free communication schedule for data redistribution if $P_r \\leq\nQ_r$ and $P_c \\leq Q_c$. In other cases, the algorithm implements circular row\nand column shifts on the communication schedule to minimize node contention. \n\n"}
{"id": "0706.3060", "contents": "Title: N-Body Simulations on GPUs Abstract: Commercial graphics processors (GPUs) have high compute capacity at very low\ncost, which makes them attractive for general purpose scientific computing. In\nthis paper we show how graphics processors can be used for N-body simulations\nto obtain improvements in performance over current generation CPUs. We have\ndeveloped a highly optimized algorithm for performing the O(N^2) force\ncalculations that constitute the major part of stellar and molecular dynamics\nsimulations. In some of the calculations, we achieve sustained performance of\nnearly 100 GFlops on an ATI X1900XTX. The performance on GPUs is comparable to\nspecialized processors such as GRAPE-6A and MDGRAPE-3, but at a fraction of the\ncost. Furthermore, the wide availability of GPUs has significant implications\nfor cluster computing and distributed computing efforts like Folding@Home. \n\n"}
{"id": "0707.2630", "contents": "Title: Multi-physics Extension of OpenFMO Framework Abstract: OpenFMO framework, an open-source software (OSS) platform for Fragment\nMolecular Orbital (FMO) method, is extended to multi-physics simulations (MPS).\nAfter reviewing the several FMO implementations on distributed computer\nenvironments, the subsequent development planning corresponding to MPS is\npresented. It is discussed which should be selected as a scientific software,\nlightweight and reconfigurable form or large and self-contained form. \n\n"}
{"id": "0711.2618", "contents": "Title: A System for Distributed Mechanisms: Design, Implementation and\n  Applications Abstract: We describe here a structured system for distributed mechanism design\nappropriate for both Intranet and Internet applications. In our approach the\nplayers dynamically form a network in which they know neither their neighbours\nnor the size of the network and interact to jointly take decisions. The only\nassumption concerning the underlying communication layer is that for each pair\nof processes there is a path of neighbours connecting them. This allows us to\ndeal with arbitrary network topologies.\n  We also discuss the implementation of this system which consists of a\nsequence of layers. The lower layers deal with the operations that implement\nthe basic primitives of distributed computing, namely low level communication\nand distributed termination, while the upper layers use these primitives to\nimplement high level communication among players, including broadcasting and\nmulticasting, and distributed decision making.\n  This yields a highly flexible distributed system whose specific applications\nare realized as instances of its top layer. This design is implemented in Java.\n  The system supports at various levels fault-tolerance and includes a\nprovision for distributed policing the purpose of which is to exclude\n`dishonest' players. Also, it can be used for repeated creation of dynamically\nformed networks of players interested in a joint decision making implemented by\nmeans of a tax-based mechanism. We illustrate its flexibility by discussing a\nnumber of implemented examples. \n\n"}
{"id": "0711.3580", "contents": "Title: An evolutionary model with Turing machines Abstract: The development of a large non-coding fraction in eukaryotic DNA and the\nphenomenon of the code-bloat in the field of evolutionary computations show a\nstriking similarity. This seems to suggest that (in the presence of mechanisms\nof code growth) the evolution of a complex code can't be attained without\nmaintaining a large inactive fraction. To test this hypothesis we performed\ncomputer simulations of an evolutionary toy model for Turing machines, studying\nthe relations among fitness and coding/non-coding ratio while varying mutation\nand code growth rates. The results suggest that, in our model, having a large\nreservoir of non-coding states constitutes a great (long term) evolutionary\nadvantage. \n\n"}
{"id": "0712.4102", "contents": "Title: Digital Ecosystems: Evolving Service-Oriented Architectures Abstract: We view Digital Ecosystems to be the digital counterparts of biological\necosystems, exploiting the self-organising properties of biological ecosystems,\nwhich are considered to be robust, self-organising and scalable architectures\nthat can automatically solve complex, dynamic problems. Digital Ecosystems are\na novel optimisation technique where the optimisation works at two levels: a\nfirst optimisation, migration of agents (representing services) which are\ndistributed in a decentralised peer-to-peer network, operating continuously in\ntime; this process feeds a second optimisation based on evolutionary computing\nthat operates locally on single peers and is aimed at finding solutions to\nsatisfy locally relevant constraints. We created an Ecosystem-Oriented\nArchitecture of Digital Ecosystems by extending Service-Oriented Architectures\nwith distributed evolutionary computing, allowing services to recombine and\nevolve over time, constantly seeking to improve their effectiveness for the\nuser base. Individuals within our Digital Ecosystem will be applications\n(groups of services), created in response to user requests by using\nevolutionary optimisation to aggregate the services. These individuals will\nmigrate through the Digital Ecosystem and adapt to find niches where they are\nuseful in fulfilling other user requests for applications. Simulation results\nimply that the Digital Ecosystem performs better at large scales than a\ncomparable Service-Oriented Architecture, suggesting that incorporating ideas\nfrom theoretical ecology can contribute to useful self-organising properties in\ndigital ecosystems. \n\n"}
{"id": "0808.3693", "contents": "Title: Providing Virtual Execution Environments: A Twofold Illustration Abstract: Platform virtualization helps solving major grid computing challenges: share\nresource with flexible, user-controlled and custom execution environments and\nin the meanwhile, isolate failures and malicious code. Grid resource management\ntools will evolve to embrace support for virtual resource.\n  We present two open source projects that transparently supply virtual\nexecution environments. Tycoon has been developed at HP Labs to optimise\nresource usage in creating an economy where users bid to access virtual\nmachines and compete for CPU cycles. SmartDomains provides a peer-to-peer layer\nthat automates virtual machines deployment using a description language and\ndeployment engine from HP Labs. These projects demonstrate both client-server\nand peer-to-peer approaches to virtual resource management. The first case\nmakes extensive use of virtual machines features for dynamic resource\nallocation. The second translates virtual machines capabilities into a\nsophisticated language where resource management components can be plugged in\nconfigurations and architectures defined at deployment time.\n  We propose to share our experience at CERN openlab developing SmartDomains\nand deploying Tycoon to give an illustrative introduction to emerging research\nin virtual resource management. \n\n"}
{"id": "0812.0736", "contents": "Title: Fully distributed and fault tolerant task management based on diffusions Abstract: The task management is a critical component for the computational grids. The\naim is to assign tasks on nodes according to a global scheduling policy and a\nview of local resources of nodes. A peer-to-peer approach for the task\nmanagement involves a better scalability for the grid and a higher fault\ntolerance. But some mechanisms have to be proposed to avoid the computation of\nreplicated tasks that can reduce the efficiency and increase the load of nodes.\nIn the same way, these mechanisms have to limit the number of exchanged\nmessages to avoid the overload of the network.\n  In a previous paper, we have proposed two methods for the task management\ncalled active and passive. These methods are based on a random walk: they are\nfully distributed and fault tolerant. Each node owns a local tasks states set\nupdated thanks to a random walk and each node is in charge of the local\nassignment. Here, we propose three methods to improve the efficiency of the\nactive method. These new methods are based on a circulating word. The nodes\nlocal tasks states sets are updated thanks to periodical diffusions along trees\nbuilt from the circulating word. Particularly, we show that these methods\nincrease the efficiency of the active method: they produce less replicated\ntasks. These three methods are also fully distributed and fault tolerant. On\nthe other way, the circulating word can be exploited for other applications\nlike the resources management or the nodes synchronization. \n\n"}
{"id": "0902.3631", "contents": "Title: Distributed Agreement in Tile Self-Assembly Abstract: Laboratory investigations have shown that a formal theory of fault-tolerance\nwill be essential to harness nanoscale self-assembly as a medium of\ncomputation. Several researchers have voiced an intuition that self-assembly\nphenomena are related to the field of distributed computing. This paper\nformalizes some of that intuition. We construct tile assembly systems that are\nable to simulate the solution of the wait-free consensus problem in some\ndistributed systems. (For potential future work, this may allow binding errors\nin tile assembly to be analyzed, and managed, with positive results in\ndistributed computing, as a \"blockage\" in our tile assembly model is analogous\nto a crash failure in a distributed computing model.) We also define a\nstrengthening of the \"traditional\" consensus problem, to make explicit an\nexpectation about consensus algorithms that is often implicit in distributed\ncomputing literature. We show that solution of this strengthened consensus\nproblem can be simulated by a two-dimensional tile assembly model only for two\nprocesses, whereas a three-dimensional tile assembly model can simulate its\nsolution in a distributed system with any number of processes. \n\n"}
{"id": "0905.2473", "contents": "Title: On the Workings of Genetic Algorithms: The Genoclique Fixing Hypothesis Abstract: We recently reported that the simple genetic algorithm (SGA) is capable of\nperforming a remarkable form of sublinear computation which has a\nstraightforward connection with the general problem of interacting attributes\nin data-mining. In this paper we explain how the SGA can leverage this\ncomputational proficiency to perform efficient adaptation on a broad class of\nfitness functions. Based on the relative ease with which a practical fitness\nfunction might belong to this broad class, we submit a new hypothesis about the\nworkings of genetic algorithms. We explain why our hypothesis is superior to\nthe building block hypothesis, and, by way of empirical validation, we present\nthe results of an experiment in which the use of a simple mechanism called\nclamping dramatically improved the performance of an SGA with uniform crossover\non large, randomly generated instances of the MAX 3-SAT problem. \n\n"}
{"id": "0906.0065", "contents": "Title: Managing Distributed MARF with SNMP Abstract: The scope of this project's work focuses on the research and prototyping of\nthe extension of the Distributed MARF such that its services can be managed\nthrough the most popular management protocol familiarly, SNMP. The rationale\nbehind SNMP vs. MARF's proprietary management protocols, is that can be\nintegrated with the use of common network service and device management, so the\nadministrators can manage MARF nodes via a already familiar protocol, as well\nas monitor their performance, gather statistics, set desired configuration,\netc. perhaps using the same management tools they've been using for other\nnetwork devices and application servers. \n\n"}
{"id": "0906.2143", "contents": "Title: Dependable Distributed Computing for the International Telecommunication\n  Union Regional Radio Conference RRC06 Abstract: The International Telecommunication Union (ITU) Regional Radio Conference\n(RRC06) established in 2006 a new frequency plan for the introduction of\ndigital broadcasting in European, African, Arab, CIS countries and Iran. The\npreparation of the plan involved complex calculations under short deadline and\nrequired dependable and efficient computing capability. The ITU designed and\ndeployed in-situ a dedicated PC farm, in parallel to the European Organization\nfor Nuclear Research (CERN) which provided and supported a system based on the\nEGEE Grid. The planning cycle at the RRC06 required a periodic execution in the\norder of 200,000 short jobs, using several hundreds of CPU hours, in a period\nof less than 12 hours. The nature of the problem required dynamic\nworkload-balancing and low-latency access to the computing resources. We\npresent the strategy and key technical choices that delivered a reliable\nservice to the RRC06. \n\n"}
{"id": "0908.1797", "contents": "Title: Separation of Circulating Tokens Abstract: Self-stabilizing distributed control is often modeled by token abstractions.\nA system with a single token may implement mutual exclusion; a system with\nmultiple tokens may ensure that immediate neighbors do not simultaneously enjoy\na privilege. For a cyber-physical system, tokens may represent physical objects\nwhose movement is controlled. The problem studied in this paper is to ensure\nthat a synchronous system with m circulating tokens has at least d distance\nbetween tokens. This problem is first considered in a ring where d is given\nwhilst m and the ring size n are unknown. The protocol solving this problem can\nbe uniform, with all processes running the same program, or it can be\nnon-uniform, with some processes acting only as token relays. The protocol for\nthis first problem is simple, and can be expressed with Petri net formalism. A\nsecond problem is to maximize d when m is given, and n is unknown. For the\nsecond problem, the paper presents a non-uniform protocol with a single\ncorrective process. \n\n"}
{"id": "0908.3889", "contents": "Title: Integrating Post-Newtonian Equations on Graphics Processing Units Abstract: We report on early results of a numerical and statistical study of binary\nblack hole inspirals. The two black holes are evolved using post-Newtonian\napproximations starting with initially randomly distributed spin vectors. We\ncharacterize certain aspects of the distribution shortly before merger. In\nparticular we note the uniform distribution of black hole spin vector dot\nproducts shortly before merger and a high correlation between the initial and\nfinal black hole spin vector dot products in the equal-mass, maximally spinning\ncase. These simulations were performed on Graphics Processing Units, and we\ndemonstrate a speed-up of a factor 50 over a more conventional CPU\nimplementation. \n\n"}
{"id": "0909.2859", "contents": "Title: Electric routing and concurrent flow cutting Abstract: We investigate an oblivious routing scheme, amenable to distributed\ncomputation and resilient to graph changes, based on electrical flow. Our main\ntechnical contribution is a new rounding method which we use to obtain a bound\non the L1->L1 operator norm of the inverse graph Laplacian. We show how this\nnorm reflects both latency and congestion of electric routing. \n\n"}
{"id": "0910.2743", "contents": "Title: DILAND: An Algorithm for Distributed Sensor Localization with Noisy\n  Distance Measurements Abstract: In this correspondence, we present an algorithm for distributed sensor\nlocalization with noisy distance measurements (DILAND) that extends and makes\nthe DLRE more robust. DLRE is a distributed sensor localization algorithm in\n$\\mathbb{R}^m$ $(m\\geq1)$ introduced in \\cite{usman_loctsp:08}. DILAND operates\nwhen (i) the communication among the sensors is noisy; (ii) the communication\nlinks in the network may fail with a non-zero probability; and (iii) the\nmeasurements performed to compute distances among the sensors are corrupted\nwith noise. The sensors (which do not know their locations) lie in the convex\nhull of at least $m+1$ anchors (nodes that know their own locations.) Under\nminimal assumptions on the connectivity and triangulation of each sensor in the\nnetwork, this correspondence shows that, under the broad random phenomena\ndescribed above, DILAND converges almost surely (a.s.) to the exact sensor\nlocations. \n\n"}
{"id": "0911.3195", "contents": "Title: Efficient Distributed Random Walks with Applications Abstract: We focus on the problem of performing random walks efficiently in a\ndistributed network. Given bandwidth constraints, the goal is to minimize the\nnumber of rounds required to obtain a random walk sample. We first present a\nfast sublinear time distributed algorithm for performing random walks whose\ntime complexity is sublinear in the length of the walk. Our algorithm performs\na random walk of length $\\ell$ in $\\tilde{O}(\\sqrt{\\ell D})$ rounds (with high\nprobability) on an undirected network, where $D$ is the diameter of the\nnetwork. This improves over the previous best algorithm that ran in\n$\\tilde{O}(\\ell^{2/3}D^{1/3})$ rounds (Das Sarma et al., PODC 2009). We further\nextend our algorithms to efficiently perform $k$ independent random walks in\n$\\tilde{O}(\\sqrt{k\\ell D} + k)$ rounds. We then show that there is a\nfundamental difficulty in improving the dependence on $\\ell$ any further by\nproving a lower bound of $\\Omega(\\sqrt{\\frac{\\ell}{\\log \\ell}} + D)$ under a\ngeneral model of distributed random walk algorithms. Our random walk algorithms\nare useful in speeding up distributed algorithms for a variety of applications\nthat use random walks as a subroutine. We present two main applications. First,\nwe give a fast distributed algorithm for computing a random spanning tree (RST)\nin an arbitrary (undirected) network which runs in $\\tilde{O}(\\sqrt{m}D)$\nrounds (with high probability; here $m$ is the number of edges). Our second\napplication is a fast decentralized algorithm for estimating mixing time and\nrelated parameters of the underlying network. Our algorithm is fully\ndecentralized and can serve as a building block in the design of\ntopologically-aware networks. \n\n"}
{"id": "0912.2572", "contents": "Title: QR Factorization of Tall and Skinny Matrices in a Grid Computing\n  Environment Abstract: Previous studies have reported that common dense linear algebra operations do\nnot achieve speed up by using multiple geographical sites of a computational\ngrid. Because such operations are the building blocks of most scientific\napplications, conventional supercomputers are still strongly predominant in\nhigh-performance computing and the use of grids for speeding up large-scale\nscientific problems is limited to applications exhibiting parallelism at a\nhigher level. We have identified two performance bottlenecks in the distributed\nmemory algorithms implemented in ScaLAPACK, a state-of-the-art dense linear\nalgebra library. First, because ScaLAPACK assumes a homogeneous communication\nnetwork, the implementations of ScaLAPACK algorithms lack locality in their\ncommunication pattern. Second, the number of messages sent in the ScaLAPACK\nalgorithms is significantly greater than other algorithms that trade flops for\ncommunication. In this paper, we present a new approach for computing a QR\nfactorization -- one of the main dense linear algebra kernels -- of tall and\nskinny matrices in a grid computing environment that overcomes these two\nbottlenecks. Our contribution is to articulate a recently proposed algorithm\n(Communication Avoiding QR) with a topology-aware middleware (QCG-OMPI) in\norder to confine intensive communications (ScaLAPACK calls) within the\ndifferent geographical sites. An experimental study conducted on the Grid'5000\nplatform shows that the resulting performance increases linearly with the\nnumber of geographical sites on large-scale problems (and is in particular\nconsistently higher than ScaLAPACK's). \n\n"}
{"id": "0912.3824", "contents": "Title: Highly accelerated simulations of glassy dynamics using GPUs: caveats on\n  limited floating-point precision Abstract: Modern graphics processing units (GPUs) provide impressive computing\nresources, which can be accessed conveniently through the CUDA programming\ninterface. We describe how GPUs can be used to considerably speed up molecular\ndynamics (MD) simulations for system sizes ranging up to about 1 million\nparticles. Particular emphasis is put on the numerical long-time stability in\nterms of energy and momentum conservation, and caveats on limited\nfloating-point precision are issued. Strict energy conservation over 10^8 MD\nsteps is obtained by double-single emulation of the floating-point arithmetic\nin accuracy-critical parts of the algorithm. For the slow dynamics of a\nsupercooled binary Lennard-Jones mixture, we demonstrate that the use of\nsingle-floating point precision may result in quantitatively and even\nphysically wrong results. For simulations of a Lennard-Jones fluid, the\ndescribed implementation shows speedup factors of up to 80 compared to a serial\nimplementation for the CPU, and a single GPU was found to compare with a\nparallelised MD simulation using 64 distributed cores. \n\n"}
{"id": "1001.2569", "contents": "Title: Virtual Private Overlays: Secure Group Commounication in NAT-Constrained\n  Environments Abstract: Structured P2P overlays provide a framework for building distributed\napplications that are self-configuring, scalable, and resilient to node\nfailures. Such systems have been successfully adopted in large-scale Internet\nservices such as content delivery networks and file sharing; however,\nwidespread adoption in small/medium scales has been limited due in part to\nsecurity concerns and difficulty bootstrapping in NAT-constrained environments.\nNonetheless, P2P systems can be designed to provide guaranteed lookup times,\nNAT traversal, point-to-point overlay security, and distributed data stores. In\nthis paper we propose a novel way of creating overlays that are both secure and\nprivate and a method to bootstrap them using a public overlay. Private overlay\nnodes use the public overlay's distributed data store to discover each other,\nand the public overlay's connections to assist with NAT hole punching and as\nrelays providing STUN and TURN NAT traversal techniques. The security framework\nutilizes groups, which are created and managed by users through a web based\nuser interface. Each group acts as a Public Key Infrastructure (PKI) relying on\nthe use of a centrally-managed web site providing an automated Certificate\nAuthority (CA). We present a reference implementation which has been used in a\nP2P VPN (Virtual Private Network). To evaluate our contributions, we apply our\ntechniques to an overlay network modeler, event-driven simulations using\nsimulated time delays, and deployment in the PlanetLab wide-area testbed. \n\n"}
{"id": "1003.3684", "contents": "Title: Parallel Generation of Massive Scale-Free Graphs Abstract: One of the biggest huddles faced by researchers studying algorithms for\nmassive graphs is the lack of large input graphs that are essential for the\ndevelopment and test of the graph algorithms. This paper proposes two efficient\nand highly scalable parallel graph generation algorithms that can produce\nmassive realistic graphs to address this issue. The algorithms, designed to\nachieve high degree of parallelism by minimizing inter-processor\ncommunications, are two of the fastest graph generators which are capable of\ngenerating scale-free graphs with billions of vertices and edges. The synthetic\ngraphs generated by the proposed methods possess the most common properties of\nreal complex networks such as power-law degree distribution, small-worldness,\nand communities-within-communities. Scalability was tested on a large cluster\nat Lawrence Livermore National Laboratory. In the experiment, we were able to\ngenerate a graph with 1 billion vertices and 5 billion edges in less than 13\nseconds. To the best of our knowledge, this is the largest synthetic scale-free\ngraph reported in the literature. \n\n"}
{"id": "1004.1741", "contents": "Title: Efficient multicore-aware parallelization strategies for iterative\n  stencil computations Abstract: Stencil computations consume a major part of runtime in many scientific\nsimulation codes. As prototypes for this class of algorithms we consider the\niterative Jacobi and Gauss-Seidel smoothers and aim at highly efficient\nparallel implementations for cache-based multicore architectures. Temporal\ncache blocking is a known advanced optimization technique, which can reduce the\npressure on the memory bus significantly. We apply and refine this optimization\nfor a recently presented temporal blocking strategy designed to explicitly\nutilize multicore characteristics. Especially for the case of Gauss-Seidel\nsmoothers we show that simultaneous multi-threading (SMT) can yield substantial\nperformance improvements for our optimized algorithm. \n\n"}
{"id": "1006.1923", "contents": "Title: Parallel Approximation Algorithms for Facility-Location Problems Abstract: This paper presents the design and analysis of parallel approximation\nalgorithms for facility-location problems, including $\\NC$ and $\\RNC$\nalgorithms for (metric) facility location, $k$-center, $k$-median, and\n$k$-means. These problems have received considerable attention during the past\ndecades from the approximation algorithms community, concentrating primarily on\nimproving the approximation guarantees. In this paper, we ask, is it possible\nto parallelize some of the beautiful results from the sequential setting?\n  Our starting point is a small, but diverse, subset of results in\napproximation algorithms for facility-location problems, with a primary goal of\ndeveloping techniques for devising their efficient parallel counterparts. We\nfocus on giving algorithms with low depth, near work efficiency (compared to\nthe sequential versions), and low cache complexity. Common in algorithms we\npresent is the idea that instead of picking only the most cost-effective\nelement, we make room for parallelism by allowing a small slack (e.g., a\n$(1+\\vareps)$ factor) in what can be selected---then, we use a clean-up step to\nensure that the behavior does not deviate too much from the sequential steps.\nAll the algorithms we developed are ``cache efficient'' in that the cache\ncomplexity is bounded by $O(w/B)$, where $w$ is the work in the EREW model and\n$B$ is the block size. \n\n"}
{"id": "1006.3148", "contents": "Title: Leveraging shared caches for parallel temporal blocking of stencil codes\n  on multicore processors and clusters Abstract: Bandwidth-starved multicore chips have become ubiquitous. It is well known\nthat the performance of stencil codes can be improved by temporal blocking,\nlessening the pressure on the memory interface. We introduce a new pipelined\napproach that makes explicit use of shared caches in multicore environments and\nminimizes synchronization and boundary overhead. Benchmark results are\npresented for three current x86-based microprocessors, showing clearly that our\noptimization works best on designs with high-speed shared caches and low memory\nbandwidth per core. We furthermore demonstrate that simple bandwidth-based\nperformance models are inaccurate for this kind of algorithm and employ a more\nelaborate, synthetic modeling procedure. Finally we show that temporal blocking\ncan be employed successfully in a hybrid shared/distributed-memory environment,\nalbeit with limited benefit at strong scaling. \n\n"}
{"id": "1008.0064", "contents": "Title: Self-repairing Homomorphic Codes for Distributed Storage Systems Abstract: Erasure codes provide a storage efficient alternative to replication based\nredundancy in (networked) storage systems. They however entail high\ncommunication overhead for maintenance, when some of the encoded fragments are\nlost and need to be replenished. Such overheads arise from the fundamental need\nto recreate (or keep separately) first a copy of the whole object before any\nindividual encoded fragment can be generated and replenished. There has been\nrecently intense interest to explore alternatives, most prominent ones being\nregenerating codes (RGC) and hierarchical codes (HC). We propose as an\nalternative a new family of codes to improve the maintenance process, which we\ncall self-repairing codes (SRC), with the following salient features: (a)\nencoded fragments can be repaired directly from other subsets of encoded\nfragments without having to reconstruct first the original data, ensuring that\n(b) a fragment is repaired from a fixed number of encoded fragments, the number\ndepending only on how many encoded blocks are missing and independent of which\nspecific blocks are missing. These properties allow for not only low\ncommunication overhead to recreate a missing fragment, but also independent\nreconstruction of different missing fragments in parallel, possibly in\ndifferent parts of the network. We analyze the static resilience of SRCs with\nrespect to traditional erasure codes, and observe that SRCs incur marginally\nlarger storage overhead in order to achieve the aforementioned properties. The\nsalient SRC properties naturally translate to low communication overheads for\nreconstruction of lost fragments, and allow reconstruction with lower latency\nby facilitating repairs in parallel. These desirable properties make\nself-repairing codes a good and practical candidate for networked distributed\nstorage systems. \n\n"}
{"id": "1008.0135", "contents": "Title: Interactive Visualization of the Largest Radioastronomy Cubes Abstract: 3D visualization is an important data analysis and knowledge discovery tool,\nhowever, interactive visualization of large 3D astronomical datasets poses a\nchallenge for many existing data visualization packages. We present a solution\nto interactively visualize larger-than-memory 3D astronomical data cubes by\nutilizing a heterogeneous cluster of CPUs and GPUs. The system partitions the\ndata volume into smaller sub-volumes that are distributed over the rendering\nworkstations. A GPU-based ray casting volume rendering is performed to generate\nimages for each sub-volume, which are composited to generate the whole volume\noutput, and returned to the user. Datasets including the HI Parkes All Sky\nSurvey (HIPASS - 12 GB) southern sky and the Galactic All Sky Survey (GASS - 26\nGB) data cubes were used to demonstrate our framework's performance. The\nframework can render the GASS data cube with a maximum render time < 0.3 second\nwith 1024 x 1024 pixels output resolution using 3 rendering workstations and 8\nGPUs. Our framework will scale to visualize larger datasets, even of Terabyte\norder, if proper hardware infrastructure is available. \n\n"}
{"id": "1009.1341", "contents": "Title: Component Specification in the Cactus Framework: The Cactus\n  Configuration Language Abstract: Component frameworks are complex systems that rely on many layers of\nabstraction to function properly. One essential requirement is a consistent\nmeans of describing each individual component and how it relates to both other\ncomponents and the whole framework. As component frameworks are designed to be\nflexible by nature, the description method should be simultaneously powerful,\nlead to efficient code, and be easy to use, so that new users can quickly adapt\ntheir own code to work with the framework. In this paper, we discuss the Cactus\nConfiguration Language (CCL) which is used to describe components (\"thorns'')\nin the Cactus Framework. The CCL provides a description language for the\nvariables, parameters, functions, scheduling and compilation of a component and\nincludes concepts such as interface and implementation which allow thorns\nproviding the same capabilities to be easily interchanged. We include several\napplication examples which illustrate how community toolkits use the CCL and\nCactus and identify needed additions to the language. \n\n"}
{"id": "1009.3291", "contents": "Title: Rebuilding for Array Codes in Distributed Storage Systems Abstract: In distributed storage systems that use coding, the issue of minimizing the\ncommunication required to rebuild a storage node after a failure arises. We\nconsider the problem of repairing an erased node in a distributed storage\nsystem that uses an EVENODD code. EVENODD codes are maximum distance separable\n(MDS) array codes that are used to protect against erasures, and only require\nXOR operations for encoding and decoding. We show that when there are two\nredundancy nodes, to rebuild one erased systematic node, only 3/4 of the\ninformation needs to be transmitted. Interestingly, in many cases, the required\ndisk I/O is also minimized. \n\n"}
{"id": "1010.0485", "contents": "Title: Distributed Storage Codes Meet Multiple-Access Wiretap Channels Abstract: We consider {\\it i)} the overhead minimization of maximum-distance separable\n(MDS) storage codes for the repair of a single failed node and {\\it ii)} the\ntotal secure degrees-of-freedom (S-DoF) maximization in a multiple-access\ncompound wiretap channel. We show that the two problems are connected.\nSpecifically, the overhead minimization for a single node failure of an {\\it\noptimal} MDS code, i.e. one that can achieve the information theoretic overhead\nminimum, is equivalent to maximizing the S-DoF in a multiple-access compound\nwiretap channel. Additionally, we show that maximizing the S-DoF in a\nmultiple-access compound wiretap channel is equivalent to minimizing the\noverhead of an MDS code for the repair of a departed node. An optimal MDS code\nmaps to a full S-DoF channel and a full S-DoF channel maps to an MDS code with\nminimum repair overhead for one failed node. We also state a general framework\nfor code-to-channel and channel-to-code mappings and performance bounds between\nthe two settings. The underlying theme for all connections presented is\ninterference alignment (IA). The connections between the two problems become\napparent when we restate IA as an optimization problem. Specifically, we\nformulate the overhead minimization and the S-DoF maximization as rank\nconstrained, sum-rank and max-rank minimization problems respectively. The\nderived connections allow us to map repair strategies of recently discovered\nrepair codes to beamforming matrices and characterize the maximum S-DoF for the\nsingle antenna multiple-access compound wiretap channel. \n\n"}
{"id": "1010.1595", "contents": "Title: Using parallel computation to improve Independent Metropolis--Hastings\n  based estimation Abstract: In this paper, we consider the implications of the fact that parallel\nraw-power can be exploited by a generic Metropolis--Hastings algorithm if the\nproposed values are independent. In particular, we present improvements to the\nindependent Metropolis--Hastings algorithm that significantly decrease the\nvariance of any estimator derived from the MCMC output, for a null computing\ncost since those improvements are based on a fixed number of target density\nevaluations. Furthermore, the techniques developed in this paper do not\njeopardize the Markovian convergence properties of the algorithm, since they\nare based on the Rao--Blackwell principles of Gelfand and Smith (1990), already\nexploited in Casella and Robert (1996), Atchade and Perron (2005) and Douc and\nRobert (2010). We illustrate those improvements both on a toy normal example\nand on a classical probit regression model, but stress the fact that they are\napplicable in any case where the independent Metropolis-Hastings is applicable. \n\n"}
{"id": "1012.2203", "contents": "Title: A collective of stateless automata in a $n$-dimensional environment as a\n  distributed dynamic automaton-like object: a model and its corollaries Abstract: In this work a collective of interacting stateless automata in a discrete\ngeometric $n$-dimenstional environment is considered as an integral\nautomaton-like computational dynamic object. For such distributed on the\nenvironment object different approaches to definition of the measure of state\ntransition are possible. We propose an approach for defining what a state is.\nThe approach is based on the concept of relativity in Poincar\\'e's\ninterpretation. \n\n"}
{"id": "1103.3737", "contents": "Title: MDS Array Codes with Optimal Rebuilding Abstract: MDS array codes are widely used in storage systems to protect data against\nerasures. We address the \\emph{rebuilding ratio} problem, namely, in the case\nof erasures, what is the the fraction of the remaining information that needs\nto be accessed in order to rebuild \\emph{exactly} the lost information? It is\nclear that when the number of erasures equals the maximum number of erasures\nthat an MDS code can correct then the rebuilding ratio is 1 (access all the\nremaining information). However, the interesting (and more practical) case is\nwhen the number of erasures is smaller than the erasure correcting capability\nof the code. For example, consider an MDS code that can correct two erasures:\nWhat is the smallest amount of information that one needs to access in order to\ncorrect a single erasure? Previous work showed that the rebuilding ratio is\nbounded between 1/2 and 3/4, however, the exact value was left as an open\nproblem. In this paper, we solve this open problem and prove that for the case\nof a single erasure with a 2-erasure correcting code, the rebuilding ratio is\n1/2. In general, we construct a new family of $r$-erasure correcting MDS array\ncodes that has optimal rebuilding ratio of $\\frac{1}{r}$ in the case of a\nsingle erasure. Our array codes have efficient encoding and decoding algorithms\n(for the case $r=2$ they use a finite field of size 3) and an optimal update\nproperty. \n\n"}
{"id": "1105.4780", "contents": "Title: Fault-tolerant Algorithms for Tick-Generation in Asynchronous Logic:\n  Robust Pulse Generation Abstract: Today's hardware technology presents a new challenge in designing robust\nsystems. Deep submicron VLSI technology introduced transient and permanent\nfaults that were never considered in low-level system designs in the past.\nStill, robustness of that part of the system is crucial and needs to be\nguaranteed for any successful product. Distributed systems, on the other hand,\nhave been dealing with similar issues for decades. However, neither the basic\nabstractions nor the complexity of contemporary fault-tolerant distributed\nalgorithms match the peculiarities of hardware implementations. This paper is\nintended to be part of an attempt striving to overcome this gap between theory\nand practice for the clock synchronization problem. Solving this task\nsufficiently well will allow to build a very robust high-precision clocking\nsystem for hardware designs like systems-on-chips in critical applications. As\nour first building block, we describe and prove correct a novel Byzantine\nfault-tolerant self-stabilizing pulse synchronization protocol, which can be\nimplemented using standard asynchronous digital logic. Despite the strict\nlimitations introduced by hardware designs, it offers optimal resilience and\nsmaller complexity than all existing protocols. \n\n"}
{"id": "1106.1652", "contents": "Title: Distributed Storage Codes through Hadamard Designs Abstract: In distributed storage systems that employ erasure coding, the issue of\nminimizing the total {\\it repair bandwidth} required to exactly regenerate a\nstorage node after a failure arises. This repair bandwidth depends on the\nstructure of the storage code and the repair strategies used to restore the\nlost data. Minimizing it requires that undesired data during a repair align in\nthe smallest possible spaces, using the concept of interference alignment (IA).\nHere, a points-on-a-lattice representation of the symbol extension IA of\nCadambe {\\it et al.} provides cues to perfect IA instances which we combine\nwith fundamental properties of Hadamard matrices to construct a new storage\ncode with favorable repair properties. Specifically, we build an explicit\n$(k+2,k)$ storage code over $\\mathbb{GF}(3)$, whose single systematic node\nfailures can be repaired with bandwidth that matches exactly the theoretical\nminimum. Moreover, the repair of single parity node failures generates at most\nthe same repair bandwidth as any systematic node failure. Our code can tolerate\nany single node failure and any pair of failures that involves at most one\nsystematic failure. \n\n"}
{"id": "1106.2275", "contents": "Title: Byzantine Fault Tolerance of Regenerating Codes Abstract: Recent years have witnessed a slew of coding techniques custom designed for\nnetworked storage systems. Network coding inspired regenerating codes are the\nmost prolifically studied among these new age storage centric codes. A lot of\neffort has been invested in understanding the fundamental achievable trade-offs\nof storage and bandwidth usage to maintain redundancy in presence of different\nmodels of failures, showcasing the efficacy of regenerating codes with respect\nto traditional erasure coding techniques. For practical usability in open and\nadversarial environments, as is typical in peer-to-peer systems, we need\nhowever not only resilience against erasures, but also from (adversarial)\nerrors. In this paper, we study the resilience of generalized regenerating\ncodes (supporting multi-repairs, using collaboration among newcomers) in the\npresence of two classes of Byzantine nodes, relatively benign selfish\n(non-cooperating) nodes, as well as under more active, malicious polluting\nnodes. We give upper bounds on the resilience capacity of regenerating codes,\nand show that the advantages of collaborative repair can turn to be detrimental\nin the presence of Byzantine nodes. We further exhibit that system mechanisms\ncan be combined with regenerating codes to mitigate the effect of rogue nodes. \n\n"}
{"id": "1106.3834", "contents": "Title: Dimensionally Constrained Symbolic Regression Abstract: We describe dimensionally constrained symbolic regression which has been\ndeveloped for mass measurement in certain classes of events in high-energy\nphysics (HEP). With symbolic regression, we can derive equations that are well\nknown in HEP. However, in problems with large number of variables, we find that\nby constraining the terms allowed in the symbolic regression, convergence\nbehavior is improved. Dimensionally constrained symbolic regression (DCSR)\nfinds solutions with much better fitness than is normally possible with\nsymbolic regression. In some cases, novel solutions are found. \n\n"}
{"id": "1108.3134", "contents": "Title: Status of the UC-Berkeley SETI Efforts Abstract: We summarize radio and optical SETI programs based at the University of\nCalifornia, Berkeley. The SEVENDIP optical pulse search looks for ns time scale\npulses at visible wavelengths using an automated 30 inch telescope. The ongoing\nSERENDIP V.v sky survey searches for radio signals at the 300 meter Arecibo\nObservatory. The currently installed configuration supports 128 million\nchannels over a 200 MHz bandwidth with ~1.6 Hz spectral resolution. SETI@home\nuses the desktop computers of volunteers to analyze over 160 TB of data at\ntaken at Arecibo looking for two types of continuous wave signals and two types\nof pulsed signals. A version to be released this summer adds autocorrelation\nanalysis to look for complex wave forms that have been repeated (and overlayed)\nafter a short delay. SETI@home will soon be processing data of Kepler exoplanet\nsystems collected at the GBT. The Astropulse project is the first SETI search\nfor $\\mu$s time scale dispersed pulses in the radio spectrum. We recently\nreobserved 114 sky locations where microsecond pulses were detected. This data\nis in process of being transferred to Berkeley for analysis. \n\n"}
{"id": "1108.3433", "contents": "Title: Abstracting Asynchronous Multi-Valued Networks: An Initial Investigation Abstract: Multi-valued networks provide a simple yet expressive qualitative state based\nmodelling approach for biological systems. In this paper we develop an\nabstraction theory for asynchronous multi-valued network models that allows the\nstate space of a model to be reduced while preserving key properties of the\nmodel. The abstraction theory therefore provides a mechanism for coping with\nthe state space explosion problem and supports the analysis and comparison of\nmulti-valued networks. We take as our starting point the abstraction theory for\nsynchronous multi-valued networks which is based on the finite set of traces\nthat represent the behaviour of such a model. The problem with extending this\napproach to the asynchronous case is that we can now have an infinite set of\ntraces associated with a model making a simple trace inclusion test infeasible.\nTo address this we develop a decision procedure for checking asynchronous\nabstractions based on using the finite state graph of an asynchronous\nmulti-valued network to reason about its trace semantics. We illustrate the\nabstraction techniques developed by considering a detailed case study based on\na multi-valued network model of the regulation of tryptophan biosynthesis in\nEscherichia coli. \n\n"}
{"id": "1109.2317", "contents": "Title: An Overview of Codes Tailor-made for Better Repairability in Networked\n  Distributed Storage Systems Abstract: The continuously increasing amount of digital data generated by today's\nsociety asks for better storage solutions. This survey looks at a new\ngeneration of coding techniques designed specifically for the needs of\ndistributed networked storage systems, trying to reach the best compromise\namong storage space efficiency, fault tolerance, and maintenance overheads.\nFour families of codes tailor-made for distributed settings, namely - pyramid,\nhierarchical, regenerating and self-repairing codes - are presented at a high\nlevel, emphasizing the main ideas behind each of these codes, and discussing\ntheir pros and cons, before concluding with a quantitative comparison among\nthem. This survey deliberately excluded technical details for the codes, nor\ndoes it provide an exhaustive summary of the numerous works. Instead, it\nprovides an overview of the major code families in a manner easily accessible\nto a broad audience, by presenting the big picture of advances in coding\ntechniques for distributed storage solutions. \n\n"}
{"id": "1109.4433", "contents": "Title: Asymetric Pavlovian Populations Abstract: Population protocols have been introduced by Angluin et al. as a model of\nnetworks consisting of very limited mobile agents that interact in pairs but\nwith no control over their own movement. A collection of anonymous agents,\nmodeled by finite automata, interact pairwise according to some rules that\nupdate their states. Predicates on the initial configurations that can be\ncomputed by such protocols have been characterized as semi-linear predicates.\nIn an orthogonal way, several distributed systems have been termed in\nliterature as being realizations of games in the sense of game theory. We\ninvestigate under which conditions population protocols, or more generally\npairwise interaction rules, correspond to games. We show that restricting to\nasymetric games is not really a restric- tion: all predicates computable by\nprotocols can actually be computed by protocols corresponding to games, i.e.\nany semi-linear predicate can be computed by a Pavlovian population\nmulti-protocol. \n\n"}
{"id": "1110.0169", "contents": "Title: Robust artificial neural networks and outlier detection. Technical\n  report Abstract: Large outliers break down linear and nonlinear regression models. Robust\nregression methods allow one to filter out the outliers when building a model.\nBy replacing the traditional least squares criterion with the least trimmed\nsquares criterion, in which half of data is treated as potential outliers, one\ncan fit accurate regression models to strongly contaminated data.\nHigh-breakdown methods have become very well established in linear regression,\nbut have started being applied for non-linear regression only recently. In this\nwork, we examine the problem of fitting artificial neural networks to\ncontaminated data using least trimmed squares criterion. We introduce a\npenalized least trimmed squares criterion which prevents unnecessary removal of\nvalid data. Training of ANNs leads to a challenging non-smooth global\noptimization problem. We compare the efficiency of several derivative-free\noptimization methods in solving it, and show that our approach identifies the\noutliers correctly when ANNs are used for nonlinear regression. \n\n"}
{"id": "1111.1750", "contents": "Title: Near Linear-Work Parallel SDD Solvers, Low-Diameter Decomposition, and\n  Low-Stretch Subgraphs Abstract: We present the design and analysis of a near linear-work parallel algorithm\nfor solving symmetric diagonally dominant (SDD) linear systems. On input of a\nSDD $n$-by-$n$ matrix $A$ with $m$ non-zero entries and a vector $b$, our\nalgorithm computes a vector $\\tilde{x}$ such that $\\norm[A]{\\tilde{x} - A^+b}\n\\leq \\vareps \\cdot \\norm[A]{A^+b}$ in $O(m\\log^{O(1)}{n}\\log{\\frac1\\epsilon})$\nwork and $O(m^{1/3+\\theta}\\log \\frac1\\epsilon)$ depth for any fixed $\\theta >\n0$.\n  The algorithm relies on a parallel algorithm for generating low-stretch\nspanning trees or spanning subgraphs. To this end, we first develop a parallel\ndecomposition algorithm that in polylogarithmic depth and $\\otilde(|E|)$ work,\npartitions a graph into components with polylogarithmic diameter such that only\na small fraction of the original edges are between the components. This can be\nused to generate low-stretch spanning trees with average stretch\n$O(n^{\\alpha})$ in $O(n^{1+\\alpha})$ work and $O(n^{\\alpha})$ depth.\nAlternatively, it can be used to generate spanning subgraphs with\npolylogarithmic average stretch in $\\otilde(|E|)$ work and polylogarithmic\ndepth. We apply this subgraph construction to derive a parallel linear system\nsolver. By using this solver in known applications, our results imply improved\nparallel randomized algorithms for several problems, including single-source\nshortest paths, maximum flow, minimum-cost flow, and approximate maximum flow. \n\n"}
{"id": "1111.5596", "contents": "Title: Accelerating QDP++/Chroma on GPUs Abstract: Extensions to the C++ implementation of the QCD Data Parallel Interface are\nprovided enabling acceleration of expression evaluation on NVIDIA GPUs. Single\nexpressions are off-loaded to the device memory and execution domain leveraging\nthe Portable Expression Template Engine and using Just-in-Time compilation\ntechniques. Memory management is automated by a software implementation of a\ncache controlling the GPU's memory. Interoperability with existing Krylov space\nsolvers is demonstrated and special attention is paid on 'Chroma readiness'.\nNon-kernel routines in lattice QCD calculations typically not subject of\nhand-tuned optimisations are accelerated which can reduce the effects otherwise\nsuffered from Amdahl's Law. \n\n"}
{"id": "1112.3880", "contents": "Title: CloudGenius: Automated Decision Support for Migrating Multi-Component\n  Enterprise Applications to Clouds Abstract: One of the key problems in migrating multi-component enterprise applications\nto Clouds is selecting the best mix of VM images and Cloud infrastructure\nservices. A migration process has to ensure that Quality of Service (QoS)\nrequirements are met, while satisfying conflicting selection criteria, e.g.\nthroughput and cost. When selecting Cloud services, application engineers must\nconsider heterogeneous sets of criteria and complex dependencies across\nmultiple layers impossible to resolve manually. To overcome this challenge, we\npresent the generic recommender framework CloudGenius and an implementation\nthat leverage well known multi-criteria decision making technique Analytic\nHierarchy Process to automate the selection process based on a model, factors,\nand QoS requirements related to enterprise applications. In particular, we\nintroduce a structured migration process for multi-component enterprise\napplications, clearly identify the most important criteria relevant to the\nselection problem and present a multi-criteria-based selection algorithm.\nExperiments with the software prototype CumulusGenius show time complexities. \n\n"}
{"id": "1112.6128", "contents": "Title: Holographic Grid Cloud, a futurable high storage technology for the next\n  generation astronomical facilities Abstract: In the immediate future holographic technology will be available to store a\nvery large amount of data in HVD (Holographic Versatile Disk) devices. This\ntechnology make extensive use of the WORM (Write-Once-Read-Many) paradigm: this\nmeans that such devices allow for a simultaneous and parallel reading of\nmillions of volumetric pixels (i.e. voxels). This characteristic will make\naccessible wherever the acquired data from a telescope (or satellite) in a\nquite-simultaneous way.\n  With the support of this new technology the aim of this paper is to identify\nthe guidelines for the implementation of a distributed RAID system, a sort of\n\"storage block\" to distribute astronomical data over different geographical\nsites acting as a single remote device as an effect of a property of\ndistributed computing, the abstraction of resources. The end user will only\nhave to take care on connecting in a opportune and secure mode (using personal\ncertificates) to the remote device and will have access to all (or part) of\nthis potential technology.\n  A Storage-Block+Services engineered on such a platform will allow rapid\nscalability of resources, creating a \"network-distributed cloud\" of services\nfor an instrument or a mission. It is recommended the use of a dedicated\ngrid-infrastructure within each single cloud to enhance some critical tasks and\nto speed-up services working on the redundant, encrypted and compressed\nscientific data. The power, the accessibility, the degree of parallelism and of\nredundancy will only depend on the number of distributed storage-blocks: the\nhigher this amount, the greater will be throughput of the IT-system. A\nstorage-block of this kind is a meeting point between two technologies and two\nantithetical computing paradigms: the Grid-Computing and Cloud-Computing. \n\n"}
{"id": "1202.0457", "contents": "Title: Exact Scalar Minimum Storage Coordinated Regenerating Codes Abstract: We study the exact and optimal repair of multiple failures in codes for\ndistributed storage. More particularly, we examine the use of interference\nalignment to build exact scalar minimum storage coordinated regenerating codes\n(MSCR). We show that it is possible to build codes for the case of k = 2 and d\n> k by aligning interferences independently but that this technique cannot be\napplied as soon as k > 2 and d > k. Our results also apply to adaptive\nregenerating codes. \n\n"}
{"id": "1202.1056", "contents": "Title: Building a Framework for Predictive Science Abstract: Key questions that scientists and engineers typically want to address can be\nformulated in terms of predictive science. Questions such as: \"How well does my\ncomputational model represent reality?\", \"What are the most important\nparameters in the problem?\", and \"What is the best next experiment to perform?\"\nare fundamental in solving scientific problems. Mystic is a framework for\nmassively-parallel optimization and rigorous sensitivity analysis that enables\nthese motivating questions to be addressed quantitatively as global\noptimization problems. Often realistic physics, engineering, and materials\nmodels may have hundreds of input parameters, hundreds of constraints, and may\nrequire execution times of seconds or longer. In more extreme cases, realistic\nmodels may be multi-scale, and require the use of high-performance computing\nclusters for their evaluation. Predictive calculations, formulated as a global\noptimization over a potential surface in design parameter space, may require an\nalready prohibitively large simulation to be performed hundreds, if not\nthousands, of times. The need to prepare, schedule, and monitor thousands of\nmodel evaluations, and dynamically explore and analyze results, is a\nchallenging problem that requires a software infrastructure capable of\ndistributing and managing computations on large-scale heterogeneous resources.\nIn this paper, we present the design behind an optimization framework, and also\na framework for heterogeneous computing, that when utilized together, can make\ncomputationally intractable sensitivity and optimization problems much more\ntractable. \n\n"}
{"id": "1202.2981", "contents": "Title: Theoretical Analysis and Tuning of Decentralized Probabilistic\n  Auto-Scaling Abstract: A major impediment towards the industrial adoption of decentralized\ndistributed systems comes from the difficulty to theoretically prove that these\nsystems exhibit the required behavior. In this paper, we use probability theory\nto analyze a decentralized auto-scaling algorithm in which each node\nprobabilistically decides to scale in or out. We prove that, in the context of\ndynamic workloads, the average load of the system is maintained within a\nvariation interval with a given probability, provided that the number of nodes\nand the variation interval length are higher than certain bounds. The paper\nalso proposes numerical algorithms for approximating these minimum bounds. \n\n"}
{"id": "1202.6094", "contents": "Title: Iterative Approximate Byzantine Consensus in Arbitrary Directed Graphs -\n  Part II: Synchronous and Asynchronous Systems Abstract: This report contains two related sets of results with different assumptions\non synchrony. The first part is about iterative algorithms in synchronous\nsystems. Following our previous work on synchronous iterative approximate\nByzantine consensus (IABC) algorithms, we provide a more intuitive tight\nnecessary and sufficient condition for the existence of such algorithms in\nsynchronous networks1. We believe this condition and the previous results also\nhold in partially asynchronous algorithmic model.\n  In the second part of the report, we explore the problem in asynchronous\nnetworks. While the traditional Byzantine consensus is not solvable in\nasynchronous systems, approximate Byzantine consensus can be solved using\niterative algorithms. \n\n"}
{"id": "1203.1505", "contents": "Title: Performance of a Distributed Stochastic Approximation Algorithm Abstract: In this paper, a distributed stochastic approximation algorithm is studied.\nApplications of such algorithms include decentralized estimation, optimization,\ncontrol or computing. The algorithm consists in two steps: a local step, where\neach node in a network updates a local estimate using a stochastic\napproximation algorithm with decreasing step size, and a gossip step, where a\nnode computes a local weighted average between its estimates and those of its\nneighbors. Convergence of the estimates toward a consensus is established under\nweak assumptions. The approach relies on two main ingredients: the existence of\na Lyapunov function for the mean field in the agreement subspace, and a\ncontraction property of the random matrices of weights in the subspace\northogonal to the agreement subspace. A second order analysis of the algorithm\nis also performed under the form of a Central Limit Theorem. The\nPolyak-averaged version of the algorithm is also considered. \n\n"}
{"id": "1203.1715", "contents": "Title: D-iteration: Evaluation of a Dynamic Partition Strategy Abstract: The aim of this paper is to present a first evaluation of a dynamic partition\nstrategy associated to the recently proposed asynchronous distributed\ncomputation scheme based on the D-iteration approach. The D-iteration is a\nfluid diffusion point of view based iteration method to solve numerically\nlinear equations. Using a simple static partition strategy, it has been shown\nthat, when the computation is distributed over K virtual machines (PIDs), the\nmemory size to be handled by each virtual machine decreases linearly with K and\nthe computation speed increases almost linearly with K with a slope becoming\ncloser to one when the number N of linear equations to be solved increases.\nHere, we want to evaluate how further those results can be improved when a\nsimple dynamic partition strategy is deployed and to show that the dynamic\npartition strategy allows one to control and equalize the computation load\nbetween PIDs without any deep analysis of the matrix or of the underlying graph\nstructure. \n\n"}
{"id": "1203.4938", "contents": "Title: Advanced Programming Platform for efficient use of Data Parallel\n  Hardware Abstract: Graphics processing units (GPU) had evolved from a specialized hardware\ncapable to render high quality graphics in games to a commodity hardware for\neffective processing blocks of data in a parallel schema. This evolution is\nparticularly interesting for scientific groups, which traditionally use mainly\nCPU as a work horse, and now can profit of the arrival of GPU hardware to HPC\nclusters. This new GPU hardware promises a boost in peak performance, but it is\nnot trivial to use. In this article a programming platform designed to promote\na direct use of this specialized hardware is presented. This platform includes\na visual editor of parallel data flows and it is oriented to the execution in\ndistributed clusters with GPUs. Examples of application in two characteristic\nproblems, Fast Fourier Transform and Image Compression, are also shown. \n\n"}
{"id": "1204.1106", "contents": "Title: Message Passing for Dynamic Network Energy Management Abstract: We consider a network of devices, such as generators, fixed loads, deferrable\nloads, and storage devices, each with its own dynamic constraints and\nobjective, connected by lossy capacitated lines. The problem is to minimize the\ntotal network objective subject to the device and line constraints, over a\ngiven time horizon. This is a large optimization problem, with variables for\nconsumption or generation in each time period for each device. In this paper we\ndevelop a decentralized method for solving this problem. The method is\niterative: At each step, each device exchanges simple messages with its\nneighbors in the network and then solves its own optimization problem,\nminimizing its own objective function, augmented by a term determined by the\nmessages it has received. We show that this message passing method converges to\na solution when the device objective and constraints are convex. The method is\ncompletely decentralized, and needs no global coordination other than\nsynchronizing iterations; the problems to be solved by each device can\ntypically be solved extremely efficiently and in parallel. The method is fast\nenough that even a serial implementation can solve substantial problems in\nreasonable time frames. We report results for several numerical experiments,\ndemonstrating the method's speed and scaling, including the solution of a\nproblem instance with over 30 million variables in 52 minutes for a serial\nimplementation; with decentralized computing, the solve time would be less than\none second. \n\n"}
{"id": "1204.1373", "contents": "Title: Spectra: Robust Estimation of Distribution Functions in Networks Abstract: Distributed aggregation allows the derivation of a given global aggregate\nproperty from many individual local values in nodes of an interconnected\nnetwork system. Simple aggregates such as minima/maxima, counts, sums and\naverages have been thoroughly studied in the past and are important tools for\ndistributed algorithms and network coordination. Nonetheless, this kind of\naggregates may not be comprehensive enough to characterize biased data\ndistributions or when in presence of outliers, making the case for richer\nestimates of the values on the network. This work presents Spectra, a\ndistributed algorithm for the estimation of distribution functions over large\nscale networks. The estimate is available at all nodes and the technique\ndepicts important properties, namely: robust when exposed to high levels of\nmessage loss, fast convergence speed and fine precision in the estimate. It can\nalso dynamically cope with changes of the sampled local property, not requiring\nalgorithm restarts, and is highly resilient to node churn. The proposed\napproach is experimentally evaluated and contrasted to a competing state of the\nart distribution aggregation technique. \n\n"}
{"id": "1204.6170", "contents": "Title: A distributed resource allocation algorithm for many processes Abstract: Resource allocation is the problem that a process may enter a critical\nsection CS of its code only when its resource requirements are not in conflict\nwith those of other processes in their critical sections. For each execution of\nCS, these requirements are given anew. In the resource requirements, levels can\nbe distinguished, such as e.g. read access or write access. We allow infinitely\nmany processes that communicate by reliable asynchronous messages and have\nfinite memory. A simple starvation-free solution is presented. Processes only\nwait for one another when they have conflicting resource requirements. The\ncorrectness of the solution is argued with invariants and temporal logic. It\nhas been verified with the proof assistant PVS. \n\n"}
{"id": "1204.6675", "contents": "Title: On the Locality of Some NP-Complete Problems Abstract: We consider the distributed message-passing {LOCAL} model. In this model a\ncommunication network is represented by a graph where vertices host processors,\nand communication is performed over the edges. Computation proceeds in\nsynchronous rounds. The running time of an algorithm is the number of rounds\nfrom the beginning until all vertices terminate. Local computation is free. An\nalgorithm is called {local} if it terminates within a constant number of\nrounds. The question of what problems can be computed locally was raised by\nNaor and Stockmayer \\cite{NS93} in their seminal paper in STOC'93. Since then\nthe quest for problems with local algorithms, and for problems that cannot be\ncomputed locally, has become a central research direction in the field of\ndistributed algorithms \\cite{KMW04,KMW10,LOW08,PR01}.\n  We devise the first local algorithm for an {NP-complete} problem.\nSpecifically, our randomized algorithm computes, with high probability, an\nO(n^{1/2 + epsilon} \\cdot chi)-coloring within O(1) rounds, where epsilon > 0\nis an arbitrarily small constant, and chi is the chromatic number of the input\ngraph. (This problem was shown to be NP-complete in \\cite{Z07}.) On our way to\nthis result we devise a constant-time algorithm for computing (O(1), O(n^{1/2 +\nepsilon}))-network-decompositions. Network-decompositions were introduced by\nAwerbuch et al. \\cite{AGLP89}, and are very useful for solving various\ndistributed problems. The best previously-known algorithm for\nnetwork-decomposition has a polylogarithmic running time (but is applicable for\na wider range of parameters) \\cite{LS93}. We also devise a Delta^{1 +\nepsilon}-coloring algorithm for graphs with sufficiently large maximum degree\nDelta that runs within O(1) rounds. It improves the best previously-known\nresult for this family of graphs, which is O(\\log-star n) \\cite{SW10}. \n\n"}
{"id": "1205.5871", "contents": "Title: Squeezing out the Cloud via Profit-Maximizing Resource Allocation\n  Policies Abstract: We study the problem of maximizing the average hourly profit earned by a\nSoftware-as-a-Service (SaaS) provider who runs a software service on behalf of\na customer using servers rented from an Infrastructure-as-a-Service (IaaS)\nprovider. The SaaS provider earns a fee per successful transaction and incurs\ncosts proportional to the number of server-hours it uses. A number of resource\nallocation policies for this or similar problems have been proposed in previous\nwork. However, to the best of our knowledge, these policies have not been\ncomparatively evaluated in a cloud environment. This paper reports on an\nempirical evaluation of three policies using a replica of Wikipedia deployed on\nthe Amazon EC2 cloud. Experimental results show that a policy based on a\nsolution to an optimization problem derived from the SaaS provider's utility\nfunction outperforms well-known heuristics that have been proposed for similar\nproblems. It is also shown that all three policies outperform a \"reactive\"\nallocation approach based on Amazon's auto-scaling feature. \n\n"}
{"id": "1206.3728", "contents": "Title: Performance Limits for Distributed Estimation Over LMS Adaptive Networks Abstract: In this work we analyze the mean-square performance of different strategies\nfor distributed estimation over least-mean-squares (LMS) adaptive networks. The\nresults highlight some useful properties for distributed adaptation in\ncomparison to fusion-based centralized solutions. The analysis establishes\nthat, by optimizing over the combination weights, diffusion strategies can\ndeliver lower excess-mean-square-error than centralized solutions employing\ntraditional block or incremental LMS strategies. We first study in some detail\nthe situation involving combinations of two adaptive agents and then extend the\nresults to generic N-node ad-hoc networks. In the later case, we establish\nthat, for sufficiently small step-sizes, diffusion strategies can outperform\ncentralized block or incremental LMS strategies by optimizing over\nleft-stochastic combination weighting matrices. The results suggest more\nefficient ways for organizing and processing data at fusion centers, and\npresent useful adaptive strategies that are able to enhance performance when\nimplemented in a distributed manner. \n\n"}
{"id": "1207.1852", "contents": "Title: Optimal Deterministic Routing and Sorting on the Congested Clique Abstract: Consider a clique of n nodes, where in each synchronous round each pair of\nnodes can exchange O(log n) bits. We provide deterministic constant-time\nsolutions for two problems in this model. The first is a routing problem where\neach node is source and destination of n messages of size O(log n). The second\nis a sorting problem where each node i is given n keys of size O(log n) and\nneeds to receive the ith batch of n keys according to the global order of the\nkeys. The latter result also implies deterministic constant-round solutions for\nrelated problems such as selection or determining modes. \n\n"}
{"id": "1207.4442", "contents": "Title: Complex-network analysis of combinatorial spaces: The NK landscape case Abstract: We propose a network characterization of combinatorial fitness landscapes by\nadapting the notion of inherent networks proposed for energy surfaces. We use\nthe well-known family of NK landscapes as an example. In our case the inherent\nnetwork is the graph whose vertices represent the local maxima in the\nlandscape, and the edges account for the transition probabilities between their\ncorresponding basins of attraction. We exhaustively extracted such networks on\nrepresentative NK landscape instances, and performed a statistical\ncharacterization of their properties. We found that most of these network\nproperties are related to the search difficulty on the underlying NK landscapes\nwith varying values of K. \n\n"}
{"id": "1208.5913", "contents": "Title: Logic of Negation-Complete Interactive Proofs (Formal Theory of\n  Epistemic Deciders) Abstract: We produce a decidable classical normal modal logic of internalised\nnegation-complete and thus disjunctive non-monotonic interactive proofs (LDiiP)\nfrom an existing logical counterpart of non-monotonic or instant interactive\nproofs (LiiP). LDiiP internalises agent-centric proof theories that are\nnegation-complete (maximal) and consistent (and hence strictly weaker than, for\nexample, Peano Arithmetic) and enjoy the disjunction property (like\nIntuitionistic Logic). In other words, internalised proof theories are\nultrafilters and all internalised proof goals are definite in the sense of\nbeing either provable or disprovable to an agent by means of disjunctive\ninternalised proofs (thus also called epistemic deciders). Still, LDiiP itself\nis classical (monotonic, non-constructive), negation-incomplete, and does not\nhave the disjunction property. The price to pay for the negation completeness\nof our interactive proofs is their non-monotonicity and non-communality (for\nsingleton agent communities only). As a normal modal logic, LDiiP enjoys a\nstandard Kripke-semantics, which we justify by invoking the Axiom of Choice on\nLiiP's and then construct in terms of a concrete oracle-computable function.\nLDiiP's agent-centric internalised notion of proof can also be viewed as a\nnegation-complete disjunctive explicit refinement of standard KD45-belief, and\nyields a disjunctive but negation-incomplete explicit refinement of\nS4-provability. \n\n"}
{"id": "1208.6051", "contents": "Title: Lower Bounds on Information Dissemination in Dynamic Networks Abstract: We study lower bounds on information dissemination in adversarial dynamic\nnetworks. Initially, k pieces of information (henceforth called tokens) are\ndistributed among n nodes. The tokens need to be broadcast to all nodes through\na synchronous network in which the topology can change arbitrarily from round\nto round provided that some connectivity requirements are satisfied.\n  If the network is guaranteed to be connected in every round and each node can\nbroadcast a single token per round to its neighbors, there is a simple token\ndissemination algorithm that manages to deliver all k tokens to all the nodes\nin O(nk) rounds. Interestingly, in a recent paper, Dutta et al. proved an\nalmost matching Omega(n + nk/log n) lower bound for deterministic\ntoken-forwarding algorithms that are not allowed to combine, split, or change\ntokens in any way. In the present paper, we extend this bound in different\nways.\n  If nodes are allowed to forward b < k tokens instead of only one token in\nevery round, a straight-forward extension of the O(nk) algorithm disseminates\nall k tokens in time O(nk/b). We show that for any randomized token-forwarding\nalgorithm, Omega(n + nk/(b^2 log n log log n)) rounds are necessary. If nodes\ncan only send a single token per round, but we are guaranteed that the network\ngraph is c-vertex connected in every round, we show a lower bound of\nOmega(nk/(c log^{3/2} n)), which almost matches the currently best O(nk/c)\nupper bound. Further, if the network is T-interval connected, a notion that\ncaptures connection stability over time, we prove that Omega(n + nk/(T^2 log\nn)) rounds are needed. The best known upper bound in this case manages to solve\nthe problem in O(n + nk/T) rounds. Finally, we show that even if each node only\nneeds to obtain a delta-fraction of all the tokens for some delta in [0,1],\nOmega(nk delta^3 log n) are still required. \n\n"}
{"id": "1209.0960", "contents": "Title: A Massively Parallel Algebraic Multigrid Preconditioner based on\n  Aggregation for Elliptic Problems with Heterogeneous Coefficients Abstract: This paper describes a massively parallel algebraic multigrid method based on\nnon-smoothed aggregation. It is especially suited for solving heterogeneous\nelliptic problems as it uses a greedy heuristic algorithm for the aggregation\nthat detects changes in the coefficients and prevents aggregation across them.\nUsing decoupled aggregation on each process with data agglomeration onto fewer\nprocesses on the coarse level, it weakly scales well in terms of both total\ntime to solution and time per iteration to nearly 300,000 cores. Because of\nsimple piecewise constant interpolation between the levels, its memory\nconsumption is low and allows solving problems with more than 100,000,000,000\ndegrees of freedom. \n\n"}
{"id": "1209.3487", "contents": "Title: A framework for large-scale distributed AI search across disconnected\n  heterogeneous infrastructures Abstract: We present a framework for a large-scale distributed eScience Artificial\nIntelligence search. Our approach is generic and can be used for many different\nproblems. Unlike many other approaches, we do not require dedicated machines,\nhomogeneous infrastructure or the ability to communicate between nodes. We give\nspecial consideration to the robustness of the framework, minimising the loss\nof effort even after total loss of infrastructure, and allowing easy\nverification of every step of the distribution process. In contrast to most\neScience applications, the input data and specification of the problem is very\nsmall, being easily given in a paragraph of text. The unique challenges our\nframework tackles are related to the combinatorial explosion of the space that\ncontains the possible solutions and the robustness of long-running\ncomputations. Not only is the time required to finish the computations unknown,\nbut also the resource requirements may change during the course of the\ncomputation. We demonstrate the applicability of our framework by using it to\nsolve a challenging and hitherto open problem in computational mathematics. The\nresults demonstrate that our approach easily scales to computations of a size\nthat would have been impossible to tackle in practice just a decade ago. \n\n"}
{"id": "1209.3904", "contents": "Title: A Distributed Algorithm for Gathering Many Fat Mobile Robots in the\n  Plane Abstract: In this work we consider the problem of gathering autonomous robots in the\nplane. In particular, we consider non-transparent unit-disc robots (i.e., fat)\nin an asynchronous setting. Vision is the only mean of coordination. Using a\nstate-machine representation we formulate the gathering problem and develop a\ndistributed algorithm that solves the problem for any number of robots.\n  The main idea behind our algorithm is for the robots to reach a configuration\nin which all the following hold: (a) The robots' centers form a convex hull in\nwhich all robots are on the convex, (b) Each robot can see all other robots,\nand (c) The configuration is connected, that is, every robot touches another\nrobot and all robots together form a connected formation. We show that starting\nfrom any initial configuration, the robots, making only local decisions and\ncoordinate by vision, eventually reach such a configuration and terminate,\nyielding a solution to the gathering problem. \n\n"}
{"id": "1209.4935", "contents": "Title: Adaptive Real Time Imaging Synthesis Telescopes Abstract: The digital revolution is transforming astronomy from a data-starved to a\ndata-submerged science. Instruments such as the Atacama Large Millimeter Array\n(ALMA), the Large Synoptic Survey Telescope (LSST), and the Square Kilometer\nArray (SKA) will measure their accumulated data in petabytes. The capacity to\nproduce enormous volumes of data must be matched with the computing power to\nprocess that data and produce meaningful results. In addition to handling huge\ndata rates, we need adaptive calibration and beamforming to handle atmospheric\nfluctuations and radio frequency interference, and to provide a user\nenvironment which makes the full power of large telescope arrays accessible to\nboth expert and non-expert users. Delayed calibration and analysis limit the\nscience which can be done. To make the best use of both telescope and human\nresources we must reduce the burden of data reduction.\n  Our instrumentation comprises of a flexible correlator, beam former and\nimager with digital signal processing closely coupled with a computing cluster.\nThis instrumentation will be highly accessible to scientists, engineers, and\nstudents for research and development of real-time processing algorithms, and\nwill tap into the pool of talented and innovative students and visiting\nscientists from engineering, computing, and astronomy backgrounds.\n  Adaptive real-time imaging will transform radio astronomy by providing\nreal-time feedback to observers. Calibration of the data is made in close to\nreal time using a model of the sky brightness distribution. The derived\ncalibration parameters are fed back into the imagers and beam formers. The\nregions imaged are used to update and improve the a-priori model, which becomes\nthe final calibrated image by the time the observations are complete. \n\n"}
{"id": "1209.5025", "contents": "Title: Global Majority Consensus by Local Majority Polling on Graphs of a Given\n  Degree Sequence Abstract: Suppose in a graph $G$ vertices can be either red or blue. Let $k$ be odd. At\neach time step, each vertex $v$ in $G$ polls $k$ random neighbours and takes\nthe majority colour. If it doesn't have $k$ neighbours, it simply polls all of\nthem, or all less one if the degree of $v$ is even. We study this protocol on\ngraphs of a given degree sequence, in the following setting: initially each\nvertex of $G$ is red independently with probability $\\alpha < \\frac{1}{2}$, and\nis otherwise blue. We show that if $\\alpha$ is sufficiently biased, then with\nhigh probability consensus is reached on the initial global majority within\n$O(\\log_k \\log_k n)$ steps if $5 \\leq k \\leq d$, and $O(\\log_d \\log_d n)$ steps\nif $k > d$. Here, $d\\geq 5$ is the effective minimum degree, the smallest\ninteger which occurs $\\Theta(n)$ times in the degree sequence. We further show\nthat on such graphs, any local protocol in which a vertex does not change\ncolour if all its neighbours have that same colour, takes time at least\n$\\Omega(\\log_d \\log_d n)$, with high probability. Additionally, we demonstrate\nhow the technique for the above sparse graphs can be applied in a\nstraightforward manner to get bounds for the Erd\\H{o}s-R\\'enyi random graphs in\nthe connected regime. \n\n"}
{"id": "1210.0187", "contents": "Title: External Memory based Distributed Generation of Massive Scale Social\n  Networks on Small Clusters Abstract: Small distributed systems are limited by their main memory to generate\nmassively large graphs. Trivial extension to current graph generators to\nutilize external memory leads to large amount of random I/O hence do not scale\nwith size. In this work we offer a technique to generate massive scale graphs\non small cluster of compute nodes with limited main memory. We develop several\ndistributed and external memory algorithms, primarily, shuffle, relabel,\nredistribute, and, compressed-sparse-row (csr) convert. The algorithms are\nimplemented in MPI/pthread model to help parallelize the operations across\nmulticores within each core. Using our scheme it is feasible to generate a\ngraph of size $2^{38}$ nodes (scale 38) using only 64 compute nodes. This can\nbe compared with the current scheme would require at least 8192 compute node,\nassuming 64GB of main memory.\n  Our work has broader implications for external memory graph libraries such as\nSTXXL and graph processing on SSD-based supercomputers such as Dash and Gordon\n[1][2]. \n\n"}
{"id": "1210.2276", "contents": "Title: A Map-Reduce Parallel Approach to Automatic Synthesis of Control\n  Software Abstract: Many Control Systems are indeed Software Based Control Systems, i.e. control\nsystems whose controller consists of control software running on a\nmicrocontroller device. This motivates investigation on Formal Model Based\nDesign approaches for automatic synthesis of control software.\n  Available algorithms and tools (e.g., QKS) may require weeks or even months\nof computation to synthesize control software for large-size systems. This\nmotivates search for parallel algorithms for control software synthesis.\n  In this paper, we present a Map-Reduce style parallel algorithm for control\nsoftware synthesis when the controlled system (plant) is modeled as discrete\ntime linear hybrid system. Furthermore we present an MPI-based implementation\nPQKS of our algorithm. To the best of our knowledge, this is the first parallel\napproach for control software synthesis.\n  We experimentally show effectiveness of PQKS on two classical control\nsynthesis problems: the inverted pendulum and the multi-input buck DC/DC\nconverter. Experiments show that PQKS efficiency is above 65%. As an example,\nPQKS requires about 16 hours to complete the synthesis of control software for\nthe pendulum on a cluster with 60 processors, instead of the 25 days needed by\nthe sequential algorithm in QKS. \n\n"}
{"id": "1210.3277", "contents": "Title: Shortest, Fastest, and Foremost Broadcast in Dynamic Networks Abstract: Highly dynamic networks rarely offer end-to-end connectivity at a given time.\nYet, connectivity in these networks can be established over time and space,\nbased on temporal analogues of multi-hop paths (also called {\\em journeys}).\nAttempting to optimize the selection of the journeys in these networks\nnaturally leads to the study of three cases: shortest (minimum hop), fastest\n(minimum duration), and foremost (earliest arrival) journeys. Efficient\ncentralized algorithms exists to compute all cases, when the full knowledge of\nthe network evolution is given.\n  In this paper, we study the {\\em distributed} counterparts of these problems,\ni.e. shortest, fastest, and foremost broadcast with termination detection\n(TDB), with minimal knowledge on the topology.\n  We show that the feasibility of each of these problems requires distinct\nfeatures on the evolution, through identifying three classes of dynamic graphs\nwherein the problems become gradually feasible: graphs in which the\nre-appearance of edges is {\\em recurrent} (class R), {\\em bounded-recurrent}\n(B), or {\\em periodic} (P), together with specific knowledge that are\nrespectively $n$ (the number of nodes), $\\Delta$ (a bound on the recurrence\ntime), and $p$ (the period). In these classes it is not required that all pairs\nof nodes get in contact -- only that the overall {\\em footprint} of the graph\nis connected over time.\n  Our results, together with the strict inclusion between $P$, $B$, and $R$,\nimplies a feasibility order among the three variants of the problem, i.e.\nTDB[foremost] requires weaker assumptions on the topology dynamics than\nTDB[shortest], which itself requires less than TDB[fastest]. Reversely, these\ndifferences in feasibility imply that the computational powers of $R_n$,\n$B_\\Delta$, and $P_p$ also form a strict hierarchy. \n\n"}
{"id": "1210.4400", "contents": "Title: Coalesced communication: a design pattern for complex parallel\n  scientific software Abstract: We present a new design pattern for high-performance parallel scientific\nsoftware, named coalesced communication. This pattern allows for a structured\nway to improve the communication performance through coalescence of multiple\ncommunication needs using two communication management components. We apply the\ndesign pattern to several simulations of a lattice-Boltzmann blood flow solver\nwith streaming visualisation which engenders a reduction in the communication\noverhead of approximately 40%. \n\n"}
{"id": "1210.6378", "contents": "Title: Efficient algorithms for tandem queueing system simulation Abstract: Serial and parallel algorithms for simulation of tandem queueing systems with\ninfinite buffers are presented, and their performance is examined. It is shown\nthat the algorithms which are based on a simple computational procedure involve\nlow time and memory requirements. \n\n"}
{"id": "1210.6685", "contents": "Title: Distributed Optimization: Convergence Conditions from a Dynamical System\n  Perspective Abstract: This paper explores the fundamental properties of distributed minimization of\na sum of functions with each function only known to one node, and a\npre-specified level of node knowledge and computational capacity. We define the\noptimization information each node receives from its objective function, the\nneighboring information each node receives from its neighbors, and the\ncomputational capacity each node can take advantage of in controlling its\nstate. It is proven that there exist a neighboring information way and a\ncontrol law that guarantee global optimal consensus if and only if the solution\nsets of the local objective functions admit a nonempty intersection set for\nfixed strongly connected graphs. Then we show that for any tolerated error, we\ncan find a control law that guarantees global optimal consensus within this\nerror for fixed, bidirectional, and connected graphs under mild conditions. For\ntime-varying graphs, we show that optimal consensus can always be achieved as\nlong as the graph is uniformly jointly strongly connected and the nonempty\nintersection condition holds. The results illustrate that nonempty intersection\nfor the local optimal solution sets is a critical condition for successful\ndistributed optimization for a large class of algorithms. \n\n"}
{"id": "1211.2963", "contents": "Title: Flexible composition and execution of high performance, high fidelity\n  multiscale biomedical simulations Abstract: Multiscale simulations are essential in the biomedical domain to accurately\nmodel human physiology. We present a modular approach for designing,\nconstructing and executing multiscale simulations on a wide range of resources,\nfrom desktops to petascale supercomputers, including combinations of these. Our\nwork features two multiscale applications, in-stent restenosis and\ncerebrovascular bloodflow, which combine multiple existing single-scale\napplications to create a multiscale simulation. These applications can be\nefficiently coupled, deployed and executed on computers up to the largest\n(peta) scale, incurring a coupling overhead of 1 to 10% of the total execution\ntime. \n\n"}
{"id": "1212.1941", "contents": "Title: Amortized communication complexity of an equality predicate Abstract: We study the communication complexity of a direct sum of independent copies\nof the equality predicate. We prove that the probabilistic communication\ncomplexity of this problem is equal to O(N); computational complexity of the\nproposed protocol is polynomial in size of inputs. Our protocol improves the\nresult achieved in 1995(Feder, Kushilevitz, Naor, Nisan). Our construction is\nbased on two techniques: Nisan's pseudorandom generator (1992) and Smith's\nstring synchronization algorithm (2007). \n\n"}
{"id": "1212.5880", "contents": "Title: Local Thresholding in General Network Graphs Abstract: Local thresholding algorithms were first presented more than a decade ago and\nhave since been applied to a variety of data mining tasks in peer-to-peer\nsystems, wireless sensor networks, and in grid systems. One critical assumption\nmade by those algorithms has always been cycle-free routing. The existence of\neven one cycle may lead all peers to the wrong outcome. Outside the lab,\nunfortunately, cycle freedom is not easy to achieve.\n  This work is the first to lift the requirement of cycle freedom by presenting\na local thresholding algorithm suitable for general network graphs. The\nalgorithm relies on a new repositioning of the problem in weighted vector\narithmetics, on a new stopping rule, whose proof does not require that the\nnetwork be cycle free, and on new methods for balance correction when the\nstopping rule fails.\n  The new stopping and update rules permit calculation of the very same\nfunctions that were calculable using previous algorithms, which do assume cycle\nfreedom. The algorithm is implemented on a standard peer-to-peer simulator and\nis validated for networks of up to 80,000 peers, organized in three different\ntopologies, which are representative of the topology of major current\ndistributed systems: the Internet, structured peer-to-peer systems, and\nwireless sensor networks. \n\n"}
{"id": "1212.6640", "contents": "Title: Exploring mutexes, the Oracle RDBMS retrial spinlocks Abstract: Spinlocks are widely used in database engines for processes synchronization.\nKGX mutexes is new retrial spinlocks appeared in contemporary Oracle versions\nfor submicrosecond synchronization. The mutex contention is frequently observed\nin highly concurrent OLTP environments.\n  This work explores how Oracle mutexes operate, spin, and sleep. It develops\npredictive mathematical model and discusses parameters and statistics related\nto mutex performance tuning, as well as results of contention experiments. \n\n"}
{"id": "1301.2130", "contents": "Title: Distributed soft thresholding for sparse signal recovery Abstract: In this paper, we address the problem of distributed sparse recovery of\nsignals acquired via compressed measurements in a sensor network. We propose a\nnew class of distributed algorithms to solve Lasso regression problems, when\nthe communication to a fusion center is not possible, e.g., due to\ncommunication cost or privacy reasons. More precisely, we introduce a\ndistributed iterative soft thresholding algorithm (DISTA) that consists of\nthree steps: an averaging step, a gradient step, and a soft thresholding\noperation. We prove the convergence of DISTA in networks represented by regular\ngraphs, and we compare it with existing methods in terms of performance,\nmemory, and complexity. \n\n"}
{"id": "1301.4083", "contents": "Title: Knowledge Matters: Importance of Prior Information for Optimization Abstract: We explore the effect of introducing prior information into the intermediate\nlevel of neural networks for a learning task on which all the state-of-the-art\nmachine learning algorithms tested failed to learn. We motivate our work from\nthe hypothesis that humans learn such intermediate concepts from other\nindividuals via a form of supervision or guidance using a curriculum. The\nexperiments we have conducted provide positive evidence in favor of this\nhypothesis. In our experiments, a two-tiered MLP architecture is trained on a\ndataset with 64x64 binary inputs images, each image with three sprites. The\nfinal task is to decide whether all the sprites are the same or one of them is\ndifferent. Sprites are pentomino tetris shapes and they are placed in an image\nwith different locations using scaling and rotation transformations. The first\npart of the two-tiered MLP is pre-trained with intermediate-level targets being\nthe presence of sprites at each location, while the second part takes the\noutput of the first part as input and predicts the final task's target binary\nevent. The two-tiered MLP architecture, with a few tens of thousand examples,\nwas able to learn the task perfectly, whereas all other algorithms (include\nunsupervised pre-training, but also traditional algorithms like SVMs, decision\ntrees and boosting) all perform no better than chance. We hypothesize that the\noptimization difficulty involved when the intermediate pre-training is not\nperformed is due to the {\\em composition} of two highly non-linear tasks. Our\nfindings are also consistent with hypotheses on cultural learning inspired by\nthe observations of optimization problems with deep learning, presumably\nbecause of effective local minima. \n\n"}
{"id": "1301.5121", "contents": "Title: Partitioning Graph Databases - A Quantitative Evaluation Abstract: Electronic data is growing at increasing rates, in both size and\nconnectivity: the increasing presence of, and interest in, relationships\nbetween data. An example is the Twitter social network graph. Due to this\ngrowth demand is increasing for technologies that can process such data.\nCurrently relational databases are the predominant technology, but they are\npoorly suited to processing connected data as they are optimized for\nindex-intensive operations. Conversely, graph databases are optimized for graph\ncomputation. They link records by direct references, avoiding index lookups,\nand enabling retrieval of adjacent elements in constant time, regardless of\ngraph size. However, as data volume increases these databases outgrow the\nresources of one computer and data partitioning becomes necessary. We evaluate\nthe viability of using graph partitioning algorithms to partition graph\ndatabases. A prototype partitioned database was developed. Three partitioning\nalgorithms explored and one implemented. Three graph datasets were used: two\nreal and one synthetically generated. These were partitioned in various ways\nand the impact on database performance measured. We defined one synthetic\naccess pattern per dataset and executed each on the partitioned datasets.\nEvaluation took place in a simulation environment, ensuring repeatability and\nallowing measurement of metrics like network traffic and load balance. Results\nshow that compared to random partitioning the partitioning algorithm reduced\ntraffic by 40-90%. Executing the algorithm intermittently during usage\nmaintained partition quality, while requiring only 1% the computation of\ninitial partitioning. Strong correlations were found between theoretic quality\nmetrics and generated network traffic under non-uniform access patterns. \n\n"}
{"id": "1302.2129", "contents": "Title: Non-Asymptotic Analysis of an Optimal Algorithm for Network-Constrained\n  Averaging with Noisy Links Abstract: The problem of network-constrained averaging is to compute the average of a\nset of values distributed throughout a graph G using an algorithm that can pass\nmessages only along graph edges. We study this problem in the noisy setting, in\nwhich the communication along each link is modeled by an additive white\nGaussian noise channel. We propose a two-phase decentralized algorithm, and we\nuse stochastic approximation methods in conjunction with the spectral graph\ntheory to provide concrete (non-asymptotic) bounds on the mean-squared error.\nHaving found such bounds, we analyze how the number of iterations T_G(n;\n\\delta) required to achieve mean-squared error \\delta\\ scales as a function of\nthe graph topology and the number of nodes n. Previous work provided guarantees\nwith the number of iterations scaling inversely with the second smallest\neigenvalue of the Laplacian. This paper gives an algorithm that reduces this\ngraph dependence to the graph diameter, which is the best scaling possible. \n\n"}
{"id": "1302.2217", "contents": "Title: Introducing Speculation in Self-Stabilization - An Application to Mutual\n  Exclusion Abstract: Self-stabilization ensures that, after any transient fault, the system\nrecovers in a finite time and eventually exhibits. Speculation consists in\nguaranteeing that the system satisfies its requirements for any execution but\nexhibits significantly better performances for a subset of executions that are\nmore probable. A speculative protocol is in this sense supposed to be both\nrobust and efficient in practice. We introduce the notion of speculative\nstabilization which we illustrate through the mutual exclusion problem. We then\npresent a novel speculatively stabilizing mutual exclusion protocol. Our\nprotocol is self-stabilizing for any asynchronous execution. We prove that its\nstabilization time for synchronous executions is diam(g)/2 steps (where diam(g)\ndenotes the diameter of the system). This complexity result is of independent\ninterest. The celebrated mutual exclusion protocol of Dijkstra stabilizes in n\nsteps (where n is the number of processes) in synchronous executions and the\nquestion whether the stabilization time could be strictly smaller than the\ndiameter has been open since then (almost 40 years). We show that this is\nindeed possible for any underlying topology. We also provide a lower bound\nproof that shows that our new stabilization time of diam(g)/2 steps is optimal\nfor synchronous executions, even if asynchronous stabilization is not required. \n\n"}
{"id": "1302.3860", "contents": "Title: ScalienDB: Designing and Implementing a Distributed Database using Paxos Abstract: ScalienDB is a scalable, replicated database built on top of the Paxos\nalgorithm. It was developed from 2010 to 2012, when the startup backing it\nfailed. This paper discusses the design decisions of the distributed database,\ndescribes interesting parts of the C++ codebase and enumerates lessons learned\nputting ScalienDB into production at a handful of clients. The source code is\navailable on Github under the AGPL license, but it is no longer developed or\nmaintained. \n\n"}
{"id": "1302.4332", "contents": "Title: Streaming Data from HDD to GPUs for Sustained Peak Performance Abstract: In the context of the genome-wide association studies (GWAS), one has to\nsolve long sequences of generalized least-squares problems; such a task has two\nlimiting factors: execution time --often in the range of days or weeks-- and\ndata management --data sets in the order of Terabytes. We present an algorithm\nthat obviates both issues. By pipelining the computation, and thanks to a\nsophisticated transfer strategy, we stream data from hard disk to main memory\nto GPUs and achieve sustained peak performance; with respect to a\nhighly-optimized CPU implementation, our algorithm shows a speedup of 2.6x.\nMoreover, the approach lends itself to multiple GPUs and attains almost perfect\nscalability. When using 4 GPUs, we observe speedups of 9x over the\naforementioned implementation, and 488x over a widespread biology library. \n\n"}
{"id": "1302.6256", "contents": "Title: Parallel Maximum Clique Algorithms with Applications to Network Analysis\n  and Storage Abstract: We propose a fast, parallel maximum clique algorithm for large sparse graphs\nthat is designed to exploit characteristics of social and information networks.\nThe method exhibits a roughly linear runtime scaling over real-world networks\nranging from 1000 to 100 million nodes. In a test on a social network with 1.8\nbillion edges, the algorithm finds the largest clique in about 20 minutes. Our\nmethod employs a branch and bound strategy with novel and aggressive pruning\ntechniques. For instance, we use the core number of a vertex in combination\nwith a good heuristic clique finder to efficiently remove the vast majority of\nthe search space. In addition, we parallelize the exploration of the search\ntree. During the search, processes immediately communicate changes to upper and\nlower bounds on the size of maximum clique, which occasionally results in a\nsuper-linear speedup because vertices with large search spaces can be pruned by\nother processes. We apply the algorithm to two problems: to compute temporal\nstrong components and to compress graphs. \n\n"}
{"id": "1304.1007", "contents": "Title: Linear-in-$\\Delta$ Lower Bounds in the LOCAL Model Abstract: By prior work, there is a distributed algorithm that finds a maximal\nfractional matching (maximal edge packing) in $O(\\Delta)$ rounds, where\n$\\Delta$ is the maximum degree of the graph. We show that this is optimal:\nthere is no distributed algorithm that finds a maximal fractional matching in\n$o(\\Delta)$ rounds.\n  Our work gives the first linear-in-$\\Delta$ lower bound for a natural graph\nproblem in the standard model of distributed computing---prior lower bounds for\na wide range of graph problems have been at best logarithmic in $\\Delta$. \n\n"}
{"id": "1304.2302", "contents": "Title: ClusterCluster: Parallel Markov Chain Monte Carlo for Dirichlet Process\n  Mixtures Abstract: The Dirichlet process (DP) is a fundamental mathematical tool for Bayesian\nnonparametric modeling, and is widely used in tasks such as density estimation,\nnatural language processing, and time series modeling. Although MCMC inference\nmethods for the DP often provide a gold standard in terms asymptotic accuracy,\nthey can be computationally expensive and are not obviously parallelizable. We\npropose a reparameterization of the Dirichlet process that induces conditional\nindependencies between the atoms that form the random measure. This conditional\nindependence enables many of the Markov chain transition operators for DP\ninference to be simulated in parallel across multiple cores. Applied to mixture\nmodeling, our approach enables the Dirichlet process to simultaneously learn\nclusters that describe the data and superclusters that define the granularity\nof parallelization. Unlike previous approaches, our technique does not require\nalteration of the model and leaves the true posterior distribution invariant.\nIt also naturally lends itself to a distributed software implementation in\nterms of Map-Reduce, which we test in cluster configurations of over 50\nmachines and 100 cores. We present experiments exploring the parallel\nefficiency and convergence properties of our approach on both synthetic and\nreal-world data, including runs on 1MM data vectors in 256 dimensions. \n\n"}
{"id": "1304.4453", "contents": "Title: Engineering Parallel Algorithms for Community Detection in Massive\n  Networks Abstract: The amount of graph-structured data has recently experienced an enormous\ngrowth in many applications. To transform such data into useful information,\nfast analytics algorithms and software tools are necessary. One common graph\nanalytics kernel is disjoint community detection (or graph clustering). Despite\nextensive research on heuristic solvers for this task, only few parallel codes\nexist, although parallelism will be necessary to scale to the data volume of\nreal-world applications. We address the deficit in computing capability by a\nflexible and extensible community detection framework with shared-memory\nparallelism. Within this framework we design and implement efficient parallel\ncommunity detection heuristics: A parallel label propagation scheme; the first\nlarge-scale parallelization of the well-known Louvain method, as well as an\nextension of the method adding refinement; and an ensemble scheme combining the\nabove. In extensive experiments driven by the algorithm engineering paradigm,\nwe identify the most successful parameters and combinations of these\nalgorithms. We also compare our implementations with state-of-the-art\ncompetitors. The processing rate of our fastest algorithm often reaches 50M\nedges/second. We recommend the parallel Louvain method and our variant with\nrefinement as both qualitatively strong and fast. Our methods are suitable for\nmassive data sets with billions of edges. \n\n"}
{"id": "1305.1121", "contents": "Title: Storage and Search in Dynamic Peer-to-Peer Networks Abstract: We study robust and efficient distributed algorithms for searching, storing,\nand maintaining data in dynamic Peer-to-Peer (P2P) networks. P2P networks are\nhighly dynamic networks that experience heavy node churn (i.e., nodes join and\nleave the network continuously over time). Our goal is to guarantee, despite\nhigh node churn rate, that a large number of nodes in the network can store,\nretrieve, and maintain a large number of data items. Our main contributions are\nfast randomized distributed algorithms that guarantee the above with high\nprobability (whp) even under high adversarial churn:\n  1. A randomized distributed search algorithm that (whp) guarantees that\nsearches from as many as $n - o(n)$ nodes ($n$ is the stable network size)\nsucceed in ${O}(\\log n)$-rounds despite ${O}(n/\\log^{1+\\delta} n)$ churn, for\nany small constant $\\delta > 0$, per round. We assume that the churn is\ncontrolled by an oblivious adversary (that has complete knowledge and control\nof what nodes join and leave and at what time, but is oblivious to the random\nchoices made by the algorithm).\n  2. A storage and maintenance algorithm that guarantees (whp) data items can\nbe efficiently stored (with only $\\Theta(\\log{n})$ copies of each data item)\nand maintained in a dynamic P2P network with churn rate up to\n${O}(n/\\log^{1+\\delta} n)$ per round. Our search algorithm together with our\nstorage and maintenance algorithm guarantees that as many as $n - o(n)$ nodes\ncan efficiently store, maintain, and search even under ${O}(n/\\log^{1+\\delta}\nn)$ churn per round. Our algorithms require only polylogarithmic in $n$ bits to\nbe processed and sent (per round) by each node.\n  To the best of our knowledge, our algorithms are the first-known,\nfully-distributed storage and search algorithms that provably work under highly\ndynamic settings (i.e., high churn rates per step). \n\n"}
{"id": "1305.4675", "contents": "Title: Algorithms for Self-Healing Networks Abstract: Many modern networks are \\emph{reconfigurable}, in the sense that the\ntopology of the network can be changed by the nodes in the network. For\nexample, peer-to-peer, wireless and ad-hoc networks are reconfigurable. More\ngenerally, many social networks, such as a company's organizational chart;\ninfrastructure networks, such as an airline's transportation network; and\nbiological networks, such as the human brain, are also reconfigurable. Modern\nreconfigurable networks have a complexity unprecedented in the history of\nengineering, resembling more a dynamic and evolving living animal rather than a\nstructure of steel designed from a blueprint. Unfortunately, our mathematical\nand algorithmic tools have not yet developed enough to handle this complexity\nand fully exploit the flexibility of these networks.\n  We believe that it is no longer possible to build networks that are scalable\nand never have node failures. Instead, these networks should be able to admit\nsmall, and maybe, periodic failures and still recover like skin heals from a\ncut. This process, where the network can recover itself by maintaining key\ninvariants in response to attack by a powerful adversary is what we call\n\\emph{self-healing}.\n  Here, we present several fast and provably good distributed algorithms for\nself-healing in reconfigurable dynamic networks. Each of these algorithms have\ndifferent properties, a different set of gaurantees and limitations. We also\ndiscuss future directions and theoretical questions we would like to answer.\n%in the final dissertation that this document is proposed to lead to. \n\n"}
{"id": "1306.6410", "contents": "Title: Monetary Cost Optimizations for Hosting Workflow-as-a-Service in IaaS\n  Clouds Abstract: Recently, we have witnessed workflows from science and other data-intensive\napplications emerging on Infrastructure-asa-Service (IaaS) clouds, and many\nworkflow service providers offering workflow as a service (WaaS). The major\nconcern of WaaS providers is to minimize the monetary cost of executing\nworkflows in the IaaS cloud. While there have been previous studies on this\nconcern, most of them assume static task execution time and static pricing\nscheme, and have the QoS notion of satisfying a deterministic deadline.\nHowever, cloud environment is dynamic, with performance dynamics caused by the\ninterference from concurrent executions and price dynamics like spot prices\noffered by Amazon EC2. Therefore, we argue that WaaS providers should have the\nnotion of offering probabilistic performance guarantees for individual\nworkflows on IaaS clouds. We develop a probabilistic scheduling framework\ncalled Dyna to minimize the monetary cost while offering probabilistic deadline\nguarantees. The framework includes an A*-based instance configuration method\nfor performance dynamics, and a hybrid instance configuration refinement for\nutilizing spot instances. Experimental results with three real-world scientific\nworkflow applications on Amazon EC2 demonstrate (1) the accuracy of our\nframework on satisfying the probabilistic deadline guarantees required by the\nusers; (2) the effectiveness of our framework on reducing monetary cost in\ncomparison with the existing approaches. \n\n"}
{"id": "1307.1051", "contents": "Title: Byzantine Convex Consensus: Preliminary Version Abstract: Much of the past work on asynchronous approximate Byzantine consensus has\nassumed scalar inputs at the nodes [3, 7]. Recent work has yielded approximate\nByzantine consensus algorithms for the case when the input at each node is a\nd-dimensional vector, and the nodes must reach consensus on a vector in the\nconvex hull of the input vectors at the fault-free nodes [8, 12]. The\nd-dimensional vectors can be equivalently viewed as points in the d-dimensional\nEuclidean space. Thus, the algorithms in [8, 12] require the fault-free nodes\nto decide on a point in the d-dimensional space.\n  In this paper, we generalize the problem to allow the decision to be a convex\npolytope in the d-dimensional space, such that the decided polytope is within\nthe convex hull of the input vectors at the fault-free nodes. We name this\nproblem as Byzantine convex consensus (BCC), and present an asynchronous\napproximate BCC algorithm with optimal fault tolerance. Ideally, the goal here\nis to agree on a convex polytope that is as large as possible. While we do not\nclaim that our algorithm satisfies this goal, we show a bound on the output\nconvex polytope chosen by our algorithm. \n\n"}
{"id": "1307.1270", "contents": "Title: A heterogeneous many-core platform for experiments on scalable custom\n  interconnects and management of fault and critical events, applied to\n  many-process applications: Vol. II, 2012 technical report Abstract: This is the second of a planned collection of four yearly volumes describing\nthe deployment of a heterogeneous many-core platform for experiments on\nscalable custom interconnects and management of fault and critical events,\napplied to many-process applications. This volume covers several topics, among\nwhich: 1- a system for awareness of faults and critical events (named LO|FA|MO)\non experimental heterogeneous many-core hardware platforms; 2- the integration\nand test of the experimental hardware heterogeneous many-core platform QUoNG,\nbased on the APEnet+ custom interconnect; 3- the design of a\nSoftware-Programmable Distributed Network Processor architecture (DNP) using\nASIP technology; 4- the initial stages of design of a new DNP generation onto a\n28nm FPGA. These developments were performed in the framework of the EURETILE\nEuropean Project under the Grant Agreement no. 247846. \n\n"}
{"id": "1307.7867", "contents": "Title: A space-time parallel solver for the three-dimensional heat equation Abstract: The paper presents a combination of the time-parallel \"parallel full\napproximation scheme in space and time\" (PFASST) with a parallel multigrid\nmethod (PMG) in space, resulting in a mesh-based solver for the\nthree-dimensional heat equation with a uniquely high degree of efficient\nconcurrency. Parallel scaling tests are reported on the Cray XE6 machine \"Monte\nRosa\" on up to 16,384 cores and on the IBM Blue Gene/Q system \"JUQUEEN\" on up\nto 65,536 cores. The efficacy of the combined spatial- and temporal\nparallelization is shown by demonstrating that using PFASST in addition to PMG\nsignificantly extends the strong-scaling limit. Implications of using spatial\ncoarsening strategies in PFASST's multi-level hierarchy in large-scale parallel\nsimulations are discussed. \n\n"}
{"id": "1308.0843", "contents": "Title: Snowmass Energy Frontier Simulations using the Open Science Grid (A\n  Snowmass 2013 whitepaper) Abstract: Snowmass is a US long-term planning study for the high-energy community by\nthe American Physical Society's Division of Particles and Fields. For its\nsimulation studies, opportunistic resources are harnessed using the Open\nScience Grid infrastructure. Late binding grid technology, GlideinWMS, was used\nfor distributed scheduling of the simulation jobs across many sites mainly in\nthe US. The pilot infrastructure also uses the Parrot mechanism to dynamically\naccess CvmFS in order to ascertain a homogeneous environment across the nodes.\nThis report presents the resource usage and the storage model used for\nsimulating large statistics Standard Model backgrounds needed for Snowmass\nEnergy Frontier studies. \n\n"}
{"id": "1308.0850", "contents": "Title: Generating Sequences With Recurrent Neural Networks Abstract: This paper shows how Long Short-term Memory recurrent neural networks can be\nused to generate complex sequences with long-range structure, simply by\npredicting one data point at a time. The approach is demonstrated for text\n(where the data are discrete) and online handwriting (where the data are\nreal-valued). It is then extended to handwriting synthesis by allowing the\nnetwork to condition its predictions on a text sequence. The resulting system\nis able to generate highly realistic cursive handwriting in a wide variety of\nstyles. \n\n"}
{"id": "1308.2694", "contents": "Title: A Super-Fast Distributed Algorithm for Bipartite Metric Facility\n  Location Abstract: The \\textit{facility location} problem consists of a set of\n\\textit{facilities} $\\mathcal{F}$, a set of \\textit{clients} $\\mathcal{C}$, an\n\\textit{opening cost} $f_i$ associated with each facility $x_i$, and a\n\\textit{connection cost} $D(x_i,y_j)$ between each facility $x_i$ and client\n$y_j$. The goal is to find a subset of facilities to \\textit{open}, and to\nconnect each client to an open facility, so as to minimize the total facility\nopening costs plus connection costs. This paper presents the first\nexpected-sub-logarithmic-round distributed O(1)-approximation algorithm in the\n$\\mathcal{CONGEST}$ model for the \\textit{metric} facility location problem on\nthe complete bipartite network with parts $\\mathcal{F}$ and $\\mathcal{C}$. Our\nalgorithm has an expected running time of $O((\\log \\log n)^3)$ rounds, where $n\n= |\\mathcal{F}| + |\\mathcal{C}|$. This result can be viewed as a continuation\nof our recent work (ICALP 2012) in which we presented the first\nsub-logarithmic-round distributed O(1)-approximation algorithm for metric\nfacility location on a \\textit{clique} network. The bipartite setting presents\nseveral new challenges not present in the problem on a clique network. We\npresent two new techniques to overcome these challenges. (i) In order to deal\nwith the problem of not being able to choose appropriate probabilities (due to\nlack of adequate knowledge), we design an algorithm that performs a random walk\nover a probability space and analyze the progress our algorithm makes as the\nrandom walk proceeds. (ii) In order to deal with a problem of quickly\ndisseminating a collection of messages, possibly containing many duplicates,\nover the bipartite network, we design a probabilistic hashing scheme that\ndelivers all of the messages in expected-$O(\\log \\log n)$ rounds. \n\n"}
{"id": "1308.2930", "contents": "Title: Semistability-Based Convergence Analysis for Paracontracting Multiagent\n  Coordination Optimization Abstract: This sequential technical report extends some of the previous results we\nposted at arXiv:1306.0225. \n\n"}
{"id": "1309.0186", "contents": "Title: A Solution to the Network Challenges of Data Recovery in Erasure-coded\n  Distributed Storage Systems: A Study on the Facebook Warehouse Cluster Abstract: Erasure codes, such as Reed-Solomon (RS) codes, are being increasingly\nemployed in data centers to combat the cost of reliably storing large amounts\nof data. Although these codes provide optimal storage efficiency, they require\nsignificantly high network and disk usage during recovery of missing data. In\nthis paper, we first present a study on the impact of recovery operations of\nerasure-coded data on the data-center network, based on measurements from\nFacebook's warehouse cluster in production. To the best of our knowledge, this\nis the first study of its kind available in the literature. Our study reveals\nthat recovery of RS-coded data results in a significant increase in network\ntraffic, more than a hundred terabytes per day, in a cluster storing multiple\npetabytes of RS-coded data.\n  To address this issue, we present a new storage code using our recently\nproposed \"Piggybacking\" framework, that reduces the network and disk usage\nduring recovery by 30% in theory, while also being storage optimal and\nsupporting arbitrary design parameters. The implementation of the proposed code\nin the Hadoop Distributed File System (HDFS) is underway. We use the\nmeasurements from the warehouse cluster to show that the proposed code would\nlead to a reduction of close to fifty terabytes of cross-rack traffic per day. \n\n"}
{"id": "1309.1272", "contents": "Title: Intrinsic Universality of Causal Graph Dynamics Abstract: Causal graph dynamics are transformations over graphs that capture two\nimportant symmetries of physics, namely causality and homogeneity. They can be\nequivalently defined as continuous and translation invariant transformations or\nfunctions induced by a local rule applied simultaneously on every vertex of the\ngraph. Intrinsic universality is the ability of an instance of a model to\nsimulate every other instance of the model while preserving the structure of\nthe computation at every step of the simulation. In this work we present the\nconstruction of a family of intrinsically universal instances of causal graphs\ndynamics, each instance being able to simulate a subset of instances. \n\n"}
{"id": "1309.3139", "contents": "Title: Exploiting Interference for Efficient Distributed Computation in\n  Cluster-based Wireless Sensor Networks Abstract: This invited paper presents some novel ideas on how to enhance the\nperformance of consensus algorithms in distributed wireless sensor networks,\nwhen communication costs are considered. Of particular interest are consensus\nalgorithms that exploit the broadcast property of the wireless channel to boost\nthe performance in terms of convergence speeds. To this end, we propose a novel\nclustering based consensus algorithm that exploits interference for\ncomputation, while reducing the energy consumption in the network. The\nresulting optimization problem is a semidefinite program, which can be solved\noffline prior to system startup. \n\n"}
{"id": "1309.3830", "contents": "Title: Energy-Aware Aggregation of Dynamic Temporal Workload in Data Centers Abstract: Data center providers seek to minimize their total cost of ownership (TCO),\nwhile power consumption has become a social concern. We present formulations to\nminimize server energy consumption and server cost under three different data\ncenter server setups (homogeneous, heterogeneous, and hybrid hetero-homogeneous\nclusters) with dynamic temporal workload. Our studies show that the homogeneous\nmodel significantly differs from the heterogeneous model in computational time\n(by an order of magnitude). To be able to compute optimal configurations in\nnear real-time for large scale data centers, we propose two modes, aggregation\nby maximum and aggregation by mean. In addition, we propose two aggregation\nmethods, static (periodic) aggregation and dynamic (aperiodic) aggregation. We\nfound that in the aggregation by maximum mode, the dynamic aggregation resulted\nin cost savings of up to approximately 18% over the static aggregation. In the\naggregation by mean mode, the dynamic aggregation by mean could save up to\napproximately 50% workload rearrangement compared to the static aggregation by\nmean mode. Overall, our methodology helps to understand the trade-off in\nenergy-aware aggregation. \n\n"}
{"id": "1309.5478", "contents": "Title: Fast $k$-NNG construction with GPU-based quick multi-select Abstract: In this paper we describe a new brute force algorithm for building the\n$k$-Nearest Neighbor Graph ($k$-NNG). The $k$-NNG algorithm has many\napplications in areas such as machine learning, bio-informatics, and clustering\nanalysis. While there are very efficient algorithms for data of low dimensions,\nfor high dimensional data the brute force search is the best algorithm. There\nare two main parts to the algorithm: the first part is finding the distances\nbetween the input vectors which may be formulated as a matrix multiplication\nproblem. The second is the selection of the $k$-NNs for each of the query\nvectors. For the second part, we describe a novel graphics processing unit\n(GPU) -based multi-select algorithm based on quick sort. Our optimization makes\nclever use of warp voting functions available on the latest GPUs along with\nuse-controlled cache. Benchmarks show significant improvement over\nstate-of-the-art implementations of the $k$-NN search on GPUs. \n\n"}
{"id": "1309.6978", "contents": "Title: Simple and Efficient Local Codes for Distributed Stable Network\n  Construction Abstract: In this work, we study protocols so that populations of distributed processes\ncan construct networks. In order to highlight the basic principles of\ndistributed network construction we keep the model minimal in all respects. In\nparticular, we assume finite-state processes that all begin from the same\ninitial state and all execute the same protocol (i.e. the system is\nhomogeneous). Moreover, we assume pairwise interactions between the processes\nthat are scheduled by an adversary. The only constraint on the adversary\nscheduler is that it must be fair. In order to allow processes to construct\nnetworks, we let them activate and deactivate their pairwise connections. When\ntwo processes interact, the protocol takes as input the states of the processes\nand the state of the their connection and updates all of them. Initially all\nconnections are inactive and the goal is for the processes, after interacting\nand activating/deactivating connections for a while, to end up with a desired\nstable network. We give protocols (optimal in some cases) and lower bounds for\nseveral basic network construction problems such as spanning line, spanning\nring, spanning star, and regular network. We provide proofs of correctness for\nall of our protocols and analyze the expected time to convergence of most of\nthem under a uniform random scheduler that selects the next pair of interacting\nprocesses uniformly at random from all such pairs. Finally, we prove several\nuniversality results by presenting generic protocols that are capable of\nsimulating a Turing Machine (TM) and exploiting it in order to construct a\nlarge class of networks. \n\n"}
{"id": "1309.7697", "contents": "Title: Semi-structured data extraction and modelling: the WIA Project Abstract: Over the last decades, the amount of data of all kinds available\nelectronically has increased dramatically. Data are accessible through a range\nof interfaces including Web browsers, database query languages,\napplication-specific interfaces, built on top of a number of different data\nexchange formats. All these data span from un-structured to highly structured\ndata. Very often, some of them have structure even if the structure is\nimplicit, and not as rigid or regular as that found in standard database\nsystems. Spreadsheet documents are prototypical in this respect. Spreadsheets\nare the lightweight technology able to supply companies with easy to build\nbusiness management and business intelligence applications, and business people\nlargely adopt spreadsheets as smart vehicles for data files generation and\nsharing. Actually, the more spreadsheets grow in complexity (e.g., their use in\nproduct development plans and quoting), the more their arrangement,\nmaintenance, and analysis appear as a knowledge-driven activity. The\nalgorithmic approach to the problem of automatic data structure extraction from\nspreadsheet documents (i.e., grid-structured and free topological-related data)\nemerges from the WIA project: Worksheets Intelligent Analyser. The\nWIA-algorithm shows how to provide a description of spreadsheet contents in\nterms of higher level of abstractions or conceptualisations. In particular, the\nWIA-algorithm target is about the extraction of i) the calculus work-flow\nimplemented in the spreadsheets formulas and ii) the logical role played by the\ndata which take part into the calculus. The aim of the resulting\nconceptualisations is to provide spreadsheets with abstract representations\nuseful for further model refinements and optimizations through evolutionary\nalgorithms computations. \n\n"}
{"id": "1310.3438", "contents": "Title: On Optimal Probabilities in Stochastic Coordinate Descent Methods Abstract: We propose and analyze a new parallel coordinate descent method---`NSync---in\nwhich at each iteration a random subset of coordinates is updated, in parallel,\nallowing for the subsets to be chosen non-uniformly. We derive convergence\nrates under a strong convexity assumption, and comment on how to assign\nprobabilities to the sets to optimize the bound. The complexity and practical\nperformance of the method can outperform its uniform variant by an order of\nmagnitude. Surprisingly, the strategy of updating a single randomly selected\ncoordinate per iteration---with optimal probabilities---may require less\niterations, both in theory and practice, than the strategy of updating all\ncoordinates at every iteration. \n\n"}
{"id": "1310.4664", "contents": "Title: SVD Factorization for Tall-and-Fat Matrices on Parallel Architectures Abstract: We demonstrate an implementation for an approximate rank-k SVD factorization,\ncombining well-known randomized projection techniques with previously known\nparalel solutions in order to compute steps of the random projection based SVD\nprocedure. We structure the problem in a way that it reduces to fast\ncomputation around $k \\times k$ matrices computed on a single machine, greatly\neasing the computability of the problem. The paper is also a tutorial on\nparalel linear algebra methods using a plain architecture without burdensome\nframeworks. \n\n"}
{"id": "1310.4907", "contents": "Title: Message and time efficient multi-broadcast schemes Abstract: We consider message and time efficient broadcasting and multi-broadcasting in\nwireless ad-hoc networks, where a subset of nodes, each with a unique rumor,\nwish to broadcast their rumors to all destinations while minimizing the total\nnumber of transmissions and total time until all rumors arrive to their\ndestination. Under centralized settings, we introduce a novel approximation\nalgorithm that provides almost optimal results with respect to the number of\ntransmissions and total time, separately. Later on, we show how to efficiently\nimplement this algorithm under distributed settings, where the nodes have only\nlocal information about their surroundings. In addition, we show multiple\napproximation techniques based on the network collision detection capabilities\nand explain how to calibrate the algorithms' parameters to produce optimal\nresults for time and messages. \n\n"}
{"id": "1310.6542", "contents": "Title: SensorCloud: Towards the Interdisciplinary Development of a Trustworthy\n  Platform for Globally Interconnected Sensors and Actuators Abstract: Although Cloud Computing promises to lower IT costs and increase users'\nproductivity in everyday life, the unattractive aspect of this new technology\nis that the user no longer owns all the devices which process personal data. To\nlower scepticism, the project SensorCloud investigates techniques to understand\nand compensate these adoption barriers in a scenario consisting of cloud\napplications that utilize sensors and actuators placed in private places. This\nwork provides an interdisciplinary overview of the social and technical core\nresearch challenges for the trustworthy integration of sensor and actuator\ndevices with the Cloud Computing paradigm. Most importantly, these challenges\ninclude i) ease of development, ii) security and privacy, and iii) social\ndimensions of a cloud-based system which integrates into private life. When\nthese challenges are tackled in the development of future cloud systems, the\nattractiveness of new use cases in a sensor-enabled world will considerably be\nincreased for users who currently do not trust the Cloud. \n\n"}
{"id": "1311.1714", "contents": "Title: KaHIP v3.00 -- Karlsruhe High Quality Partitioning -- User Guide Abstract: This paper severs as a user guide to the graph partitioning framework KaHIP\n(Karlsruhe High Quality Partitioning). We give a rough overview of the\ntechniques used within the framework and describe the user interface as well as\nthe file formats used. Moreover, we provide a short description of the current\nlibrary functions provided within the framework. Since version 3.00 we support\nmultilevel partitioning, memetic algorithms, distributed and shared-memory\nparallel algorithms, node separator and ordering algorithms, edge partitioning\nalgorithms as well as ILP solvers. \n\n"}
{"id": "1311.1741", "contents": "Title: Architectural improvements and 28 nm FPGA implementation of the APEnet+\n  3D Torus network for hybrid HPC systems Abstract: Modern Graphics Processing Units (GPUs) are now considered accelerators for\ngeneral purpose computation. A tight interaction between the GPU and the\ninterconnection network is the strategy to express the full potential on\ncapability computing of a multi-GPU system on large HPC clusters; that is the\nreason why an efficient and scalable interconnect is a key technology to\nfinally deliver GPUs for scientific HPC. In this paper we show the latest\narchitectural and performance improvement of the APEnet+ network fabric, a\nFPGA-based PCIe board with 6 fully bidirectional off-board links with 34 Gbps\nof raw bandwidth per direction, and X8 Gen2 bandwidth towards the host PC. The\nboard implements a Remote Direct Memory Access (RDMA) protocol that leverages\nupon peer-to-peer (P2P) capabilities of Fermi- and Kepler-class NVIDIA GPUs to\nobtain real zero-copy, low-latency GPU-to-GPU transfers. Finally, we report on\nthe development activities for 2013 focusing on the adoption of the latest\ngeneration 28 nm FPGAs and the preliminary tests performed on this new\nplatform. \n\n"}
{"id": "1311.4007", "contents": "Title: NaNet: a flexible and configurable low-latency NIC for real-time trigger\n  systems based on GPUs Abstract: NaNet is an FPGA-based PCIe X8 Gen2 NIC supporting 1/10 GbE links and the\ncustom 34 Gbps APElink channel. The design has GPUDirect RDMA capabilities and\nfeatures a network stack protocol offloading module, making it suitable for\nbuilding low-latency, real-time GPU-based computing systems. We provide a\ndetailed description of the NaNet hardware modular architecture. Benchmarks for\nlatency and bandwidth for GbE and APElink channels are presented, followed by a\nperformance analysis on the case study of the GPU-based low level trigger for\nthe RICH detector in the NA62 CERN experiment, using either the NaNet GbE and\nAPElink channels. Finally, we give an outline of project future activities. \n\n"}
{"id": "1312.3020", "contents": "Title: Sparse Allreduce: Efficient Scalable Communication for Power-Law Data Abstract: Many large datasets exhibit power-law statistics: The web graph, social\nnetworks, text data, click through data etc. Their adjacency graphs are termed\nnatural graphs, and are known to be difficult to partition. As a consequence\nmost distributed algorithms on these graphs are communication intensive. Many\nalgorithms on natural graphs involve an Allreduce: a sum or average of\npartitioned data which is then shared back to the cluster nodes. Examples\ninclude PageRank, spectral partitioning, and many machine learning algorithms\nincluding regression, factor (topic) models, and clustering. In this paper we\ndescribe an efficient and scalable Allreduce primitive for power-law data. We\npoint out scaling problems with existing butterfly and round-robin networks for\nSparse Allreduce, and show that a hybrid approach improves on both.\nFurthermore, we show that Sparse Allreduce stages should be nested instead of\ncascaded (as in the dense case). And that the optimum throughput Allreduce\nnetwork should be a butterfly of heterogeneous degree where degree decreases\nwith depth into the network. Finally, a simple replication scheme is introduced\nto deal with node failures. We present experiments showing significant\nimprovements over existing systems such as PowerGraph and Hadoop. \n\n"}
{"id": "1312.4605", "contents": "Title: Parallelizing MCMC via Weierstrass Sampler Abstract: With the rapidly growing scales of statistical problems, subset based\ncommunication-free parallel MCMC methods are a promising future for large scale\nBayesian analysis. In this article, we propose a new Weierstrass sampler for\nparallel MCMC based on independent subsets. The new sampler approximates the\nfull data posterior samples via combining the posterior draws from independent\nsubset MCMC chains, and thus enjoys a higher computational efficiency. We show\nthat the approximation error for the Weierstrass sampler is bounded by some\ntuning parameters and provide suggestions for choice of the values. Simulation\nstudy shows the Weierstrass sampler is very competitive compared to other\nmethods for combining MCMC chains generated for subsets, including averaging\nand kernel smoothing. \n\n"}
{"id": "1312.4853", "contents": "Title: Bid-Centric Cloud Service Provisioning Abstract: Bid-centric service descriptions have the potential to offer a new cloud\nservice provisioning model that promotes portability, diversity of choice and\ndifferentiation between providers. A bid matching model based on requirements\nand capabilities is presented that provides the basis for such an approach. In\norder to facilitate the bidding process, tenders should be specified as\nabstractly as possible so that the solution space is not needlessly restricted.\nTo this end, we describe how partial TOSCA service descriptions allow for a\nrange of diverse solutions to be proposed by multiple providers in response to\ntenders. Rather than adopting a lowest common denominator approach, true\nportability should allow for the relative strengths and differentiating\nfeatures of cloud service providers to be applied to bids. With this in mind,\nwe describe how TOSCA service descriptions could be augmented with additional\ninformation in order to facilitate heterogeneity in proposed solutions, such as\nthe use of coprocessors and provider-specific services. \n\n"}
{"id": "1312.5783", "contents": "Title: Unsupervised Feature Learning by Deep Sparse Coding Abstract: In this paper, we propose a new unsupervised feature learning framework,\nnamely Deep Sparse Coding (DeepSC), that extends sparse coding to a multi-layer\narchitecture for visual object recognition tasks. The main innovation of the\nframework is that it connects the sparse-encoders from different layers by a\nsparse-to-dense module. The sparse-to-dense module is a composition of a local\nspatial pooling step and a low-dimensional embedding process, which takes\nadvantage of the spatial smoothness information in the image. As a result, the\nnew method is able to learn several levels of sparse representation of the\nimage which capture features at a variety of abstraction levels and\nsimultaneously preserve the spatial smoothness between the neighboring image\npatches. Combining the feature representations from multiple layers, DeepSC\nachieves the state-of-the-art performance on multiple object recognition tasks. \n\n"}
{"id": "1312.6203", "contents": "Title: Spectral Networks and Locally Connected Networks on Graphs Abstract: Convolutional Neural Networks are extremely efficient architectures in image\nand audio recognition tasks, thanks to their ability to exploit the local\ntranslational invariance of signal classes over their domain. In this paper we\nconsider possible generalizations of CNNs to signals defined on more general\ndomains without the action of a translation group. In particular, we propose\ntwo constructions, one based upon a hierarchical clustering of the domain, and\nanother based on the spectrum of the graph Laplacian. We show through\nexperiments that for low-dimensional graphs it is possible to learn\nconvolutional layers with a number of parameters independent of the input size,\nresulting in efficient deep architectures. \n\n"}
{"id": "1401.1905", "contents": "Title: A Parameterized Complexity Analysis of Bi-level Optimisation with\n  Evolutionary Algorithms Abstract: Bi-level optimisation problems have gained increasing interest in the field\nof combinatorial optimisation in recent years. With this paper, we start the\nruntime analysis of evolutionary algorithms for bi-level optimisation problems.\nWe examine two NP-hard problems, the generalised minimum spanning tree problem\n(GMST), and the generalised travelling salesman problem (GTSP) in the context\nof parameterised complexity.\n  For the generalised minimum spanning tree problem, we analyse the two\napproaches presented by Hu and Raidl (2012) with respect to the number of\nclusters that distinguish each other by the chosen representation of possible\nsolutions. Our results show that a (1+1) EA working with the spanning nodes\nrepresentation is not a fixed-parameter evolutionary algorithm for the problem,\nwhereas the global structure representation enables to solve the problem in\nfixed-parameter time. We present hard instances for each approach and show that\nthe two approaches are highly complementary by proving that they solve each\nother's hard instances very efficiently.\n  For the generalised travelling salesman problem, we analyse the problem with\nrespect to the number of clusters in the problem instance. Our results show\nthat a (1+1) EA working with the global structure representation is a\nfixed-parameter evolutionary algorithm for the problem. \n\n"}
{"id": "1401.3615", "contents": "Title: Performance Engineering for a Medical Imaging Application on the Intel\n  Xeon Phi Accelerator Abstract: We examine the Xeon Phi, which is based on Intel's Many Integrated Cores\narchitecture, for its suitability to run the FDK algorithm--the most commonly\nused algorithm to perform the 3D image reconstruction in cone-beam computed\ntomography. We study the challenges of efficiently parallelizing the\napplication and means to enable sensible data sharing between threads despite\nthe lack of a shared last level cache. Apart from parallelization, SIMD\nvectorization is critical for good performance on the Xeon Phi; we perform\nvarious micro-benchmarks to investigate the platform's new set of vector\ninstructions and put a special emphasis on the newly introduced vector gather\ncapability. We refine a previous performance model for the application and\nadapt it for the Xeon Phi to validate the performance of our optimized\nhand-written assembly implementation, as well as the performance of several\ndifferent auto-vectorization approaches. \n\n"}
{"id": "1401.4972", "contents": "Title: Compact Deterministic Self-Stabilizing Leader Election: The Exponential\n  Advantage of Being Talkative Abstract: This paper focuses on compact deterministic self-stabilizing solutions for\nthe leader election problem. When the protocol is required to be \\emph{silent}\n(i.e., when communication content remains fixed from some point in time during\nany execution), there exists a lower bound of Omega(\\log n) bits of memory per\nnode participating to the leader election (where n denotes the number of nodes\nin the system). This lower bound holds even in rings. We present a new\ndeterministic (non-silent) self-stabilizing protocol for n-node rings that uses\nonly O(\\log\\log n) memory bits per node, and stabilizes in O(n\\log^2 n) rounds.\nOur protocol has several attractive features that make it suitable for\npractical purposes. First, the communication model fits with the model used by\nexisting compilers for real networks. Second, the size of the ring (or any\nupper bound on this size) needs not to be known by any node. Third, the node\nidentifiers can be of various sizes. Finally, no synchrony assumption, besides\na weakly fair scheduler, is assumed. Therefore, our result shows that, perhaps\nsurprisingly, trading silence for exponential improvement in term of memory\nspace does not come at a high cost regarding stabilization time or minimal\nassumptions. \n\n"}
{"id": "1402.1141", "contents": "Title: Quantum Cybernetics and Complex Quantum Systems Science - A Quantum\n  Connectionist Exploration Abstract: Quantum cybernetics and its connections to complex quantum systems science is\naddressed from the perspective of complex quantum computing systems. In this\nway, the notion of an autonomous quantum computing system is introduced in\nregards to quantum artificial intelligence, and applied to quantum artificial\nneural networks, considered as autonomous quantum computing systems, which\nleads to a quantum connectionist framework within quantum cybernetics for\ncomplex quantum computing systems. Several examples of quantum feedforward\nneural networks are addressed in regards to Boolean functions' computation,\nmultilayer quantum computation dynamics, entanglement and quantum\ncomplementarity. The examples provide a framework for a reflection on the role\nof quantum artificial neural networks as a general framework for addressing\ncomplex quantum systems that perform network-based quantum computation,\npossible consequences are drawn regarding quantum technologies, as well as\nfundamental research in complex quantum systems science and quantum biology. \n\n"}
{"id": "1402.2549", "contents": "Title: Local Approximability of Minimum Dominating Set on Planar Graphs Abstract: We show that there is no deterministic local algorithm (constant-time\ndistributed graph algorithm) that finds a $(7-\\epsilon)$-approximation of a\nminimum dominating set on planar graphs, for any positive constant $\\epsilon$.\nIn prior work, the best lower bound on the approximation ratio has been\n$5-\\epsilon$; there is also an upper bound of $52$. \n\n"}
{"id": "1402.2810", "contents": "Title: Energy Efficient Scheduling of MapReduce Jobs Abstract: MapReduce is emerged as a prominent programming model for data-intensive\ncomputation. In this work, we study power-aware MapReduce scheduling in the\nspeed scaling setting first introduced by Yao et al. [FOCS 1995]. We focus on\nthe minimization of the total weighted completion time of a set of MapReduce\njobs under a given budget of energy. Using a linear programming relaxation of\nour problem, we derive a polynomial time constant-factor approximation\nalgorithm. We also propose a convex programming formulation that we combine\nwith standard list scheduling policies, and we evaluate their performance using\nsimulations. \n\n"}
{"id": "1402.3337", "contents": "Title: Zero-bias autoencoders and the benefits of co-adapting features Abstract: Regularized training of an autoencoder typically results in hidden unit\nbiases that take on large negative values. We show that negative biases are a\nnatural result of using a hidden layer whose responsibility is to both\nrepresent the input data and act as a selection mechanism that ensures sparsity\nof the representation. We then show that negative biases impede the learning of\ndata distributions whose intrinsic dimensionality is high. We also propose a\nnew activation function that decouples the two roles of the hidden layer and\nthat allows us to learn representations on data with very high intrinsic\ndimensionality, where standard autoencoders typically fail. Since the decoupled\nactivation function acts like an implicit regularizer, the model can be trained\nby minimizing the reconstruction error of training data, without requiring any\nadditional regularization. \n\n"}
{"id": "1402.5521", "contents": "Title: Parallel Selective Algorithms for Big Data Optimization Abstract: We propose a decomposition framework for the parallel optimization of the sum\nof a differentiable (possibly nonconvex) function and a (block) separable\nnonsmooth, convex one. The latter term is usually employed to enforce structure\nin the solution, typically sparsity. Our framework is very flexible and\nincludes both fully parallel Jacobi schemes and Gauss- Seidel (i.e.,\nsequential) ones, as well as virtually all possibilities \"in between\" with only\na subset of variables updated at each iteration. Our theoretical convergence\nresults improve on existing ones, and numerical results on LASSO, logistic\nregression, and some nonconvex quadratic problems show that the new method\nconsistently outperforms existing algorithms. \n\n"}
{"id": "1402.7254", "contents": "Title: Generalizations of the distributed Deutsch-Jozsa promise problem Abstract: In the {\\em distributed Deutsch-Jozsa promise problem}, two parties are to\ndetermine whether their respective strings $x,y\\in\\{0,1\\}^n$ are at the {\\em\nHamming distance} $H(x,y)=0$ or $H(x,y)=\\frac{n}{2}$. Buhrman et al. (STOC' 98)\nproved that the exact {\\em quantum communication complexity} of this problem is\n${\\bf O}(\\log {n})$ while the {\\em deterministic communication complexity} is\n${\\bf \\Omega}(n)$. This was the first impressive (exponential) gap between\nquantum and classical communication complexity.\n  In this paper, we generalize the above distributed Deutsch-Jozsa promise\nproblem to determine, for any fixed $\\frac{n}{2}\\leq k\\leq n$, whether\n$H(x,y)=0$ or $H(x,y)= k$, and show that an exponential gap between exact\nquantum and deterministic communication complexity still holds if $k$ is an\neven such that $\\frac{1}{2}n\\leq k<(1-\\lambda) n$, where $0<\n\\lambda<\\frac{1}{2}$ is given. We also deal with a promise version of the\nwell-known {\\em disjointness} problem and show also that for this promise\nproblem there exists an exponential gap between quantum (and also\nprobabilistic) communication complexity and deterministic communication\ncomplexity of the promise version of such a disjointness problem. Finally, some\napplications to quantum, probabilistic and deterministic finite automata of the\nresults obtained are demonstrated. \n\n"}
{"id": "1403.3881", "contents": "Title: Complexity of Equilibrium in Diffusion Games on Social Networks Abstract: In this paper, we consider the competitive diffusion game, and study the\nexistence of its pure-strategy Nash equilibrium when defined over general\nundirected networks. We first determine the set of pure-strategy Nash\nequilibria for two special but well-known classes of networks, namely the\nlattice and the hypercube. Characterizing the utility of the players in terms\nof graphical distances of their initial seed placements to other nodes in the\nnetwork, we show that in general networks the decision process on the existence\nof pure-strategy Nash equilibrium is an NP-hard problem. Following this, we\nprovide some necessary conditions for a given profile to be a Nash equilibrium.\nFurthermore, we study players' utilities in the competitive diffusion game over\nErdos-Renyi random graphs and show that as the size of the network grows, the\nutilities of the players are highly concentrated around their expectation, and\nare bounded below by some threshold based on the parameters of the network.\nFinally, we obtain a lower bound for the maximum social welfare of the game\nwith two players, and study sub-modularity of the players' utilities. \n\n"}
{"id": "1403.5007", "contents": "Title: On Throughput-Delay Optimal Access to Storage Clouds via Load Adaptive\n  Coding and Chunking Abstract: Recent literature including our past work provide analysis and solutions for\nusing (i) erasure coding, (ii) parallelism, or (iii) variable slicing/chunking\n(i.e., dividing an object of a specific size into a variable number of smaller\nchunks) in speeding the I/O performance of storage clouds. However, a\ncomprehensive approach that considers all three dimensions together to achieve\nthe best throughput-delay trade-off curve had been lacking. This paper presents\nthe first set of solutions that can pick the best combination of coding rate\nand object chunking/slicing options as the load dynamically changes. Our\nspecific contributions are as follows: (1) We establish via measurement that\ncombining variable coding rate and chunking is mostly feasible over a popular\npublic cloud. (2) We relate the delay optimal values for chunking level and\ncode rate to the queue backlogs via an approximate queueing analysis. (3) Based\non this analysis, we propose TOFEC that adapts the chunking level and coding\nrate against the queue backlogs. Our trace-driven simulation results show that\nTOFEC's adaptation mechanism converges to an appropriate code that provides the\noptimal throughput-delay trade-off without reducing system capacity. Compared\nto a non-adaptive strategy optimized for throughput, TOFEC delivers $2.5\\times$\nlower latency under light workloads; compared to a non-adaptive strategy\noptimized for latency, TOFEC can scale to support over $3\\times$ as many\nrequests. (4) We propose a simpler greedy solution that performs on a par with\nTOFEC in average delay performance, but exhibits significantly more performance\nvariations. \n\n"}
{"id": "1403.5791", "contents": "Title: Self-stabilizing uncoupled dynamics Abstract: Dynamics in a distributed system are self-stabilizing if they are guaranteed\nto reach a stable state regardless of how the system is initialized. Game\ndynamics are uncoupled if each player's behavior is independent of the other\nplayers' preferences. Recognizing an equilibrium in this setting is a\ndistributed computational task. Self-stabilizing uncoupled dynamics, then, have\nboth resilience to arbitrary initial states and distribution of knowledge. We\nstudy these dynamics by analyzing their behavior in a bounded-recall\nsynchronous environment. We determine, for every \"size\" of game, the minimum\nnumber of periods of play that stochastic (randomized) players must recall in\norder for uncoupled dynamics to be self-stabilizing. We also do this for the\nspecial case when the game is guaranteed to have unique best replies. For\ndeterministic players, we demonstrate two self-stabilizing uncoupled protocols.\nOne applies to all games and uses three steps of recall. The other uses two\nsteps of recall and applies to games where each player has at least four\navailable actions. For uncoupled deterministic players, we prove that a single\nstep of recall is insufficient to achieve self-stabilization, regardless of the\nnumber of available actions. \n\n"}
{"id": "1404.2387", "contents": "Title: Fast Structuring of Radio Networks for Multi-Message Communications Abstract: We introduce collision free layerings as a powerful way to structure radio\nnetworks. These layerings can replace hard-to-compute BFS-trees in many\ncontexts while having an efficient randomized distributed construction. We\ndemonstrate their versatility by using them to provide near optimal distributed\nalgorithms for several multi-message communication primitives.\n  Designing efficient communication primitives for radio networks has a rich\nhistory that began 25 years ago when Bar-Yehuda et al. introduced fast\nrandomized algorithms for broadcasting and for constructing BFS-trees. Their\nBFS-tree construction time was $O(D \\log^2 n)$ rounds, where $D$ is the network\ndiameter and $n$ is the number of nodes. Since then, the complexity of a\nbroadcast has been resolved to be $T_{BC} = \\Theta(D \\log \\frac{n}{D} + \\log^2\nn)$ rounds. On the other hand, BFS-trees have been used as a crucial building\nblock for many communication primitives and their construction time remained a\nbottleneck for these primitives.\n  We introduce collision free layerings that can be used in place of BFS-trees\nand we give a randomized construction of these layerings that runs in nearly\nbroadcast time, that is, w.h.p. in $T_{Lay} = O(D \\log \\frac{n}{D} +\n\\log^{2+\\epsilon} n)$ rounds for any constant $\\epsilon>0$. We then use these\nlayerings to obtain: (1) A randomized algorithm for gathering $k$ messages\nrunning w.h.p. in $O(T_{Lay} + k)$ rounds. (2) A randomized $k$-message\nbroadcast algorithm running w.h.p. in $O(T_{Lay} + k \\log n)$ rounds. These\nalgorithms are optimal up to the small difference in the additive\npoly-logarithmic term between $T_{BC}$ and $T_{Lay}$. Moreover, they imply the\nfirst optimal $O(n \\log n)$ round randomized gossip algorithm. \n\n"}
{"id": "1404.4797", "contents": "Title: Parallel Graph Partitioning for Complex Networks Abstract: Processing large complex networks like social networks or web graphs has\nrecently attracted considerable interest. In order to do this in parallel, we\nneed to partition them into pieces of about equal size. Unfortunately, previous\nparallel graph partitioners originally developed for more regular mesh-like\nnetworks do not work well for these networks. This paper addresses this problem\nby parallelizing and adapting the label propagation technique originally\ndeveloped for graph clustering. By introducing size constraints, label\npropagation becomes applicable for both the coarsening and the refinement phase\nof multilevel graph partitioning. We obtain very high quality by applying a\nhighly parallel evolutionary algorithm to the coarsened graph. The resulting\nsystem is both more scalable and achieves higher quality than state-of-the-art\nsystems like ParMetis or PT-Scotch. For large complex networks the performance\ndifferences are very big. For example, our algorithm can partition a web graph\nwith 3.3 billion edges in less than sixteen seconds using 512 cores of a high\nperformance cluster while producing a high quality partition -- none of the\ncompeting systems can handle this graph on our system. \n\n"}
{"id": "1404.7634", "contents": "Title: Testing Temporal Connectivity in Sparse Dynamic Graphs Abstract: We address the problem of testing whether a given dynamic graph is temporally\nconnected, {\\it i.e} a temporal path (also called a {\\em journey}) exists\nbetween all pairs of vertices. We consider a discrete version of the problem,\nwhere the topology is given as an evolving graph ${\\cal\nG}=\\{G_1,G_2,...,G_{k}\\}$ whose set of vertices is invariant and the set of\n(directed) edges varies over time. Two cases are studied, depending on whether\na single edge or an unlimited number of edges can be crossed in a same $G_i$\n(strict journeys {\\it vs} non-strict journeys).\n  In the case of {\\em strict} journeys, a number of existing algorithms\ndesigned for more general problems can be adapted. We adapt one of them to the\nabove formulation of the problem and characterize its running time complexity.\nThe parameters of interest are the length of the graph sequence $k=|{\\cal G}|$,\nthe maximum {\\em instant} density $\\mu=max(|E_i|)$, and the {\\em cumulated}\ndensity $m=|\\cup E_i|$. Our algorithm has a time complexity of $O(k\\mu n)$,\nwhere $n$ is the number of nodes. This complexity is compared to that of the\nother solutions: one is always more costly (keep in mind that is solves a more\ngeneral problem), the other one is more or less costly depending on the\ninterplay between instant density and cumulated density. The length $k$ of the\nsequence also plays a role. We characterize the key values of $k, \\mu$ and $m$\nfor which either algorithm should be used.\n  In the case of {\\em non-strict} journeys, for which no algorithm is known, we\nshow that some pre-processing of the input graph allows us to re-use the same\nalgorithm than before. By chance, these operations happens to cost again\n$O(k\\mu n)$ time, which implies that the second problem is not more difficult\nthan the first. \n\n"}
{"id": "1405.1380", "contents": "Title: Is Joint Training Better for Deep Auto-Encoders? Abstract: Traditionally, when generative models of data are developed via deep\narchitectures, greedy layer-wise pre-training is employed. In a well-trained\nmodel, the lower layer of the architecture models the data distribution\nconditional upon the hidden variables, while the higher layers model the hidden\ndistribution prior. But due to the greedy scheme of the layerwise training\ntechnique, the parameters of lower layers are fixed when training higher\nlayers. This makes it extremely challenging for the model to learn the hidden\ndistribution prior, which in turn leads to a suboptimal model for the data\ndistribution. We therefore investigate joint training of deep autoencoders,\nwhere the architecture is viewed as one stack of two or more single-layer\nautoencoders. A single global reconstruction objective is jointly optimized,\nsuch that the objective for the single autoencoders at each layer acts as a\nlocal, layer-level regularizer. We empirically evaluate the performance of this\njoint training scheme and observe that it not only learns a better data model,\nbut also learns better higher layer representations, which highlights its\npotential for unsupervised feature learning. In addition, we find that the\nusage of regularizations in the joint training scheme is crucial in achieving\ngood performance. In the supervised setting, joint training also shows superior\nperformance when training deeper models. The joint training framework can thus\nprovide a platform for investigating more efficient usage of different types of\nregularizers, especially in light of the growing volumes of available unlabeled\ndata. \n\n"}
{"id": "1405.2833", "contents": "Title: On the Latency and Energy Efficiency of Erasure-Coded Cloud Storage\n  Systems Abstract: The increase in data storage and power consumption at data-centers has made\nit imperative to design energy efficient Distributed Storage Systems (DSS). The\nenergy efficiency of DSS is strongly influenced not only by the volume of data,\nfrequency of data access and redundancy in data storage, but also by the\nheterogeneity exhibited by the DSS in these dimensions. To this end, we propose\nand analyze the energy efficiency of a heterogeneous distributed storage system\nin which $n$ storage servers (disks) store the data of $R$ distinct classes.\nData of class $i$ is encoded using a $(n,k_{i})$ erasure code and the (random)\ndata retrieval requests can also vary across classes. We show that the energy\nefficiency of such systems is closely related to the average latency and hence\nmotivates us to study the energy efficiency via the lens of average latency.\nThrough this connection, we show that erasure coding serves the dual purpose of\nreducing latency and increasing energy efficiency. We present a queuing\ntheoretic analysis of the proposed model and establish upper and lower bounds\non the average latency for each data class under various scheduling policies.\nThrough extensive simulations, we present qualitative insights which reveal the\nimpact of coding rate, number of servers, service distribution and number of\nredundant requests on the average latency and energy efficiency of the DSS. \n\n"}
{"id": "1405.4256", "contents": "Title: Resource Usage Analysis of Logic Programs via Abstract Interpretation\n  Using Sized Types Abstract: We present a novel general resource analysis for logic programs based on\nsized types. Sized types are representations that incorporate structural\n(shape) information and allow expressing both lower and upper bounds on the\nsize of a set of terms and their subterms at any position and depth. They also\nallow relating the sizes of terms and subterms occurring at different argument\npositions in logic predicates. Using these sized types, the resource analysis\ncan infer both lower and upper bounds on the resources used by all the\nprocedures in a program as functions on input term (and subterm) sizes,\novercoming limitations of existing resource analyses and enhancing their\nprecision. Our new resource analysis has been developed within the abstract\ninterpretation framework, as an extension of the sized types abstract domain,\nand has been integrated into the Ciao preprocessor, CiaoPP. The abstract domain\noperations are integrated with the setting up and solving of recurrence\nequations for inferring both size and resource usage functions. We show that\nthe analysis is an improvement over the previous resource analysis present in\nCiaoPP and compares well in power to state of the art systems. \n\n"}
{"id": "1405.5145", "contents": "Title: Set Consensus: Captured by a Set of Runs with Ramifications Abstract: Are (set)-consensus objects necessary? This paper answer is negative.\n  We show that the availability of consensus objects can be replaced by\nrestricting the set of runs we consider. In particular we concentrate of the\nset of runs of the Immediate-Snapshot-Model (IIS), and given the object we\nidentify this restricted subset of IIS runs.\n  We further show that given an $(m,k)$-set consensus, an object that provides\n$k$-set consensus among $m$ processors, in a system of $n$, $n>m$ processors,\nwe do not need to use the precise power of the objects but rather their\neffective cumulative set consensus power. E.g. when $n=3, m=2,$ and $k=1$ and\nall the 3 processors are active then we only use 2-set consensus among the 3\nprocessors, as if 2-processors consensus is not available. We do this until at\nleast one of the 3 processors obtains an output. We show that this suggests a\nnew direction in the design of algorithms when consensus objects are involved. \n\n"}
{"id": "1405.5326", "contents": "Title: Secure Anonymous Broadcast Abstract: In anonymous broadcast, one or more parties want to anonymously send messages\nto all parties. This problem is increasingly important as a black-box in many\nprivacy-preserving applications such as anonymous communication, distributed\nauctions, and multi-party computation. In this paper, we design decentralized\nprotocols for anonymous broadcast that require each party to send (and compute)\na polylogarithmic number of bits (and operations) per anonymous bit delivered\nwith $O(\\log n)$ rounds of communication. Our protocol is provably secure\nagainst traffic analysis, does not require any trusted party, and is completely\nload-balanced. The protocol tolerates up to $n/6$ statically-scheduled\nByzantine parties that are controlled by a computationally unbounded adversary.\nOur main strategy for achieving scalability is to perform local communications\n(and computations) among a logarithmic number of parties. We provide simulation\nresults to show that our protocol improves significantly over previous work. We\nfinally show that using a common cryptographic tool in our protocol one can\nachieve practical results for anonymous broadcast. \n\n"}
{"id": "1406.1231", "contents": "Title: Multi-task Neural Networks for QSAR Predictions Abstract: Although artificial neural networks have occasionally been used for\nQuantitative Structure-Activity/Property Relationship (QSAR/QSPR) studies in\nthe past, the literature has of late been dominated by other machine learning\ntechniques such as random forests. However, a variety of new neural net\ntechniques along with successful applications in other domains have renewed\ninterest in network approaches. In this work, inspired by the winning team's\nuse of neural networks in a recent QSAR competition, we used an artificial\nneural network to learn a function that predicts activities of compounds for\nmultiple assays at the same time. We conducted experiments leveraging recent\nmethods for dealing with overfitting in neural networks as well as other tricks\nfrom the neural networks literature. We compared our methods to alternative\nmethods reported to perform well on these tasks and found that our neural net\nmethods provided superior performance. \n\n"}
{"id": "1406.1691", "contents": "Title: Towards a Better Understanding of the Local Attractor in Particle Swarm\n  Optimization: Speed and Solution Quality Abstract: Particle Swarm Optimization (PSO) is a popular nature-inspired meta-heuristic\nfor solving continuous optimization problems. Although this technique is widely\nused, the understanding of the mechanisms that make swarms so successful is\nstill limited. We present the first substantial experimental investigation of\nthe influence of the local attractor on the quality of exploration and\nexploitation. We compare in detail classical PSO with the social-only variant\nwhere local attractors are ignored. To measure the exploration capabilities, we\ndetermine how frequently both variants return results in the neighborhood of\nthe global optimum. We measure the quality of exploitation by considering only\nfunction values from runs that reached a search point sufficiently close to the\nglobal optimum and then comparing in how many digits such values still deviate\nfrom the global minimum value. It turns out that the local attractor\nsignificantly improves the exploration, but sometimes reduces the quality of\nthe exploitation. As a compromise, we propose and evaluate a hybrid PSO which\nswitches off its local attractors at a certain point in time. The effects\nmentioned can also be observed by measuring the potential of the swarm. \n\n"}
{"id": "1406.1974", "contents": "Title: Communication Complexity of the Fast Multipole Method and its Algebraic\n  Variants Abstract: A combination of hierarchical tree-like data structures and data access\npatterns from fast multipole methods and hierarchical low-rank approximation of\nlinear operators from H-matrix methods appears to form an algorithmic path\nforward for efficient implementation of many linear algebraic operations of\nscientific computing at the exascale. The combination provides asymptotically\noptimal computational and communication complexity and applicability to large\nclasses of operators that commonly arise in scientific computing applications.\nA convergence of the mathematical theories of the fast multipole and H-matrix\nmethods has been underway for over a decade. We recap this mathematical\nunification and describe implementation aspects of a hybrid of these two\ncompelling hierarchical algorithms on hierarchical distributed-shared memory\narchitectures, which are likely to be the first to reach the exascale. We\npresent a new communication complexity estimate for fast multipole methods on\nsuch architectures. We also show how the data structures and access patterns of\nH-matrices for low-rank operators map onto those of fast multipole, leading to\nan algebraically generalized form of fast multipole that compromises none of\nits architecturally ideal properties. \n\n"}
{"id": "1406.2823", "contents": "Title: A Hitchhiker's Guide to Search-Based Software Engineering for Software\n  Product Lines Abstract: Search Based Software Engineering (SBSE) is an emerging discipline that\nfocuses on the application of search-based optimization techniques to software\nengineering problems. The capacity of SBSE techniques to tackle problems\ninvolving large search spaces make their application attractive for Software\nProduct Lines (SPLs). In recent years, several publications have appeared that\napply SBSE techniques to SPL problems. In this paper, we present the results of\na systematic mapping study of such publications. We identified the stages of\nthe SPL life cycle where SBSE techniques have been used, what case studies have\nbeen employed and how they have been analysed. This mapping study revealed\npotential venues for further research as well as common misunderstanding and\npitfalls when applying SBSE techniques that we address by providing a guideline\nfor researchers and practitioners interested in exploiting these techniques. \n\n"}
{"id": "1406.3482", "contents": "Title: Multiparty Session Actors Abstract: Actor coordination armoured with a suitable protocol description language has\nbeen a pressing problem in the actors community. We study the applicability of\nmultiparty session type (MPST) protocols for verification of actor programs. We\nincorporate sessions to actors by introducing minimum additions to the model\nsuch as the notion of actor roles and protocol mailbox. The framework uses\nScribble, which is a protocol description language based on multiparty session\ntypes. Our programming model supports actor-like syntax and runtime\nverification mechanism guaranteeing type-safety and progress of the\ncommunicating entities. An actor can implement multiple roles in a similar way\nas an object can implement multiple interfaces. Multiple roles allow for\ninter-concurrency in a single actor still preserving its progress property. We\ndemonstrate our framework by designing and implementing a session actor library\nin Python and its runtime verification mechanism. \n\n"}
{"id": "1406.6909", "contents": "Title: Discriminative Unsupervised Feature Learning with Exemplar Convolutional\n  Neural Networks Abstract: Deep convolutional networks have proven to be very successful in learning\ntask specific features that allow for unprecedented performance on various\ncomputer vision tasks. Training of such networks follows mostly the supervised\nlearning paradigm, where sufficiently many input-output pairs are required for\ntraining. Acquisition of large training sets is one of the key challenges, when\napproaching a new task. In this paper, we aim for generic feature learning and\npresent an approach for training a convolutional network using only unlabeled\ndata. To this end, we train the network to discriminate between a set of\nsurrogate classes. Each surrogate class is formed by applying a variety of\ntransformations to a randomly sampled 'seed' image patch. In contrast to\nsupervised network training, the resulting feature representation is not class\nspecific. It rather provides robustness to the transformations that have been\napplied during training. This generic feature representation allows for\nclassification results that outperform the state of the art for unsupervised\nlearning on several popular datasets (STL-10, CIFAR-10, Caltech-101,\nCaltech-256). While such generic features cannot compete with class specific\nfeatures from supervised training on a classification task, we show that they\nare advantageous on geometric matching problems, where they also outperform the\nSIFT descriptor. \n\n"}
{"id": "1407.0386", "contents": "Title: Energy and Performance-Can a Wimpy-Node Cluster Challenge a Brawny\n  Server? Abstract: Traditional DBMS servers are usually over-provisioned for most of their daily\nworkloads and, because they do not show good energy proportionality, waste a\nlot of energy while underutilized. A cluster of small (wimpy) servers, where\nthe number of nodes can dynamically adjust to the current workload, might offer\nbetter energy characteristics for these workloads. Yet, clusters suffer from\n\"friction losses\" and may not be able to quickly adapt to the workload, whereas\na single, brawny server delivers performance instantaneously. In this paper, we\ncompare a small cluster of lightweight nodes to a single server in terms of\nperformance and energy efficiency. We run several benchmarks, consisting of\nOLTP and OLAP queries at variable utilization to test the system's ability to\nadjust to the workloads. To quantify possible energy saving and its conceivable\ndrawback on query runtime, we evaluate our implementation on a cluster as well\nas on a single, brawny server and compare the results w.r.t. performance and\nenergy consumption. Our findings confirm that - based on the workload - energy\ncan be saved without sacrificing too much performance. \n\n"}
{"id": "1407.0898", "contents": "Title: A Coordinate Descent Primal-Dual Algorithm and Application to\n  Distributed Asynchronous Optimization Abstract: Based on the idea of randomized coordinate descent of $\\alpha$-averaged\noperators, a randomized primal-dual optimization algorithm is introduced, where\na random subset of coordinates is updated at each iteration. The algorithm\nbuilds upon a variant of a recent (deterministic) algorithm proposed by V\\~u\nand Condat that includes the well known ADMM as a particular case. The obtained\nalgorithm is used to solve asynchronously a distributed optimization problem. A\nnetwork of agents, each having a separate cost function containing a\ndifferentiable term, seek to find a consensus on the minimum of the aggregate\nobjective. The method yields an algorithm where at each iteration, a random\nsubset of agents wake up, update their local estimates, exchange some data with\ntheir neighbors, and go idle. Numerical results demonstrate the attractive\nperformance of the method. The general approach can be naturally adapted to\nother situations where coordinate descent convex optimization algorithms are\nused with a random choice of the coordinates. \n\n"}
{"id": "1407.3501", "contents": "Title: Robots that can adapt like animals Abstract: As robots leave the controlled environments of factories to autonomously\nfunction in more complex, natural environments, they will have to respond to\nthe inevitable fact that they will become damaged. However, while animals can\nquickly adapt to a wide variety of injuries, current robots cannot \"think\noutside the box\" to find a compensatory behavior when damaged: they are limited\nto their pre-specified self-sensing abilities, can diagnose only anticipated\nfailure modes, and require a pre-programmed contingency plan for every type of\npotential damage, an impracticality for complex robots. Here we introduce an\nintelligent trial and error algorithm that allows robots to adapt to damage in\nless than two minutes, without requiring self-diagnosis or pre-specified\ncontingency plans. Before deployment, a robot exploits a novel algorithm to\ncreate a detailed map of the space of high-performing behaviors: This map\nrepresents the robot's intuitions about what behaviors it can perform and their\nvalue. If the robot is damaged, it uses these intuitions to guide a\ntrial-and-error learning algorithm that conducts intelligent experiments to\nrapidly discover a compensatory behavior that works in spite of the damage.\nExperiments reveal successful adaptations for a legged robot injured in five\ndifferent ways, including damaged, broken, and missing legs, and for a robotic\narm with joints broken in 14 different ways. This new technique will enable\nmore robust, effective, autonomous robots, and suggests principles that animals\nmay use to adapt to injury. \n\n"}
{"id": "1407.4764", "contents": "Title: Efficient On-the-fly Category Retrieval using ConvNets and GPUs Abstract: We investigate the gains in precision and speed, that can be obtained by\nusing Convolutional Networks (ConvNets) for on-the-fly retrieval - where\nclassifiers are learnt at run time for a textual query from downloaded images,\nand used to rank large image or video datasets.\n  We make three contributions: (i) we present an evaluation of state-of-the-art\nimage representations for object category retrieval over standard benchmark\ndatasets containing 1M+ images; (ii) we show that ConvNets can be used to\nobtain features which are incredibly performant, and yet much lower dimensional\nthan previous state-of-the-art image representations, and that their\ndimensionality can be reduced further without loss in performance by\ncompression using product quantization or binarization. Consequently, features\nwith the state-of-the-art performance on large-scale datasets of millions of\nimages can fit in the memory of even a commodity GPU card; (iii) we show that\nan SVM classifier can be learnt within a ConvNet framework on a GPU in parallel\nwith downloading the new training images, allowing for a continuous refinement\nof the model as more images become available, and simultaneous training and\nranking. The outcome is an on-the-fly system that significantly outperforms its\npredecessors in terms of: precision of retrieval, memory requirements, and\nspeed, facilitating accurate on-the-fly learning and ranking in under a second\non a single GPU. \n\n"}
{"id": "1407.5104", "contents": "Title: Pixels to Voxels: Modeling Visual Representation in the Human Brain Abstract: The human brain is adept at solving difficult high-level visual processing\nproblems such as image interpretation and object recognition in natural scenes.\nOver the past few years neuroscientists have made remarkable progress in\nunderstanding how the human brain represents categories of objects and actions\nin natural scenes. However, all current models of high-level human vision\noperate on hand annotated images in which the objects and actions have been\nassigned semantic tags by a human operator. No current models can account for\nhigh-level visual function directly in terms of low-level visual input (i.e.,\npixels). To overcome this fundamental limitation we sought to develop a new\nclass of models that can predict human brain activity directly from low-level\nvisual input (i.e., pixels). We explored two classes of models drawn from\ncomputer vision and machine learning. The first class of models was based on\nFisher Vectors (FV) and the second was based on Convolutional Neural Networks\n(ConvNets). We find that both classes of models accurately predict brain\nactivity in high-level visual areas, directly from pixels and without the need\nfor any semantic tags or hand annotation of images. This is the first time that\nsuch a mapping has been obtained. The fit models provide a new platform for\nexploring the functional principles of human vision, and they show that modern\nmethods of computer vision and machine learning provide important tools for\ncharacterizing brain function. \n\n"}
{"id": "1407.8116", "contents": "Title: Optimizing performance per watt on GPUs in High Performance Computing:\n  temperature, frequency and voltage effects Abstract: The magnitude of the real-time digital signal processing challenge attached\nto large radio astronomical antenna arrays motivates use of high performance\ncomputing (HPC) systems. The need for high power efficiency (performance per\nwatt) at remote observatory sites parallels that in HPC broadly, where\nefficiency is an emerging critical metric. We investigate how the performance\nper watt of graphics processing units (GPUs) is affected by temperature, core\nclock frequency and voltage. Our results highlight how the underlying physical\nprocesses that govern transistor operation affect power efficiency. In\nparticular, we show experimentally that GPU power consumption grows\nnon-linearly with both temperature and supply voltage, as predicted by physical\ntransistor models. We show lowering GPU supply voltage and increasing clock\nfrequency while maintaining a low die temperature increases the power\nefficiency of an NVIDIA K20 GPU by up to 37-48% over default settings when\nrunning xGPU, a compute-bound code used in radio astronomy. We discuss how\ntemperature-aware power models could be used to reduce power consumption for\nfuture HPC installations. Automatic temperature-aware and application-dependent\nvoltage and frequency scaling (T-DVFS and A-DVFS) may provide a mechanism to\nachieve better power efficiency for a wider range of codes running on GPUs \n\n"}
{"id": "1408.0620", "contents": "Title: Approximate Consensus in Highly Dynamic Networks: The Role of Averaging\n  Algorithms Abstract: In this paper, we investigate the approximate consensus problem in highly\ndynamic networks in which topology may change continually and unpredictably. We\nprove that in both synchronous and partially synchronous systems, approximate\nconsensus is solvable if and only if the communication graph in each round has\na rooted spanning tree, i.e., there is a coordinator at each time. The striking\npoint in this result is that the coordinator is not required to be unique and\ncan change arbitrarily from round to round. Interestingly, the class of\naveraging algorithms, which are memoryless and require no process identifiers,\nentirely captures the solvability issue of approximate consensus in that the\nproblem is solvable if and only if it can be solved using any averaging\nalgorithm. Concerning the time complexity of averaging algorithms, we show that\napproximate consensus can be achieved with precision of $\\varepsilon$ in a\ncoordinated network model in $O(n^{n+1} \\log\\frac{1}{\\varepsilon})$ synchronous\nrounds, and in $O(\\Delta n^{n\\Delta+1} \\log\\frac{1}{\\varepsilon})$ rounds when\nthe maximum round delay for a message to be delivered is $\\Delta$. While in\ngeneral, an upper bound on the time complexity of averaging algorithms has to\nbe exponential, we investigate various network models in which this exponential\nbound in the number of nodes reduces to a polynomial bound. We apply our\nresults to networked systems with a fixed topology and classical benign fault\nmodels, and deduce both known and new results for approximate consensus in\nthese systems. In particular, we show that for solving approximate consensus, a\ncomplete network can tolerate up to 2n-3 arbitrarily located link faults at\nevery round, in contrast with the impossibility result established by Santoro\nand Widmayer (STACS '89) showing that exact consensus is not solvable with n-1\nlink faults per round originating from the same node. \n\n"}
{"id": "1408.2858", "contents": "Title: Experimental Evaluation of Multi-Round Matrix Multiplication on\n  MapReduce Abstract: A common approach in the design of MapReduce algorithms is to minimize the\nnumber of rounds. Indeed, there are many examples in the literature of\nmonolithic MapReduce algorithms, which are algorithms requiring just one or two\nrounds. However, we claim that the design of monolithic algorithms may not be\nthe best approach in cloud systems. Indeed, multi-round algorithms may exploit\nsome features of cloud platforms by suitably setting the round number according\nto the execution context. In this paper we carry out an experimental study of\nmulti-round MapReduce algorithms aiming at investigating the performance of the\nmulti-round approach. We use matrix multiplication as a case study. We first\npropose a scalable Hadoop library, named M$_3$, for matrix multiplication in\nthe dense and sparse cases which allows to tradeoff round number with the\namount of data shuffled in each round and the amount of memory required by\nreduce functions. Then, we present an extensive study of this library on an\nin-house cluster and on Amazon Web Services aiming at showing its performance\nand at comparing monolithic and multi-round approaches. The experiments show\nthat, even without a low level optimization, it is possible to design\nmulti-round algorithms with a small running time overhead. \n\n"}
{"id": "1408.3030", "contents": "Title: Distributed Graph Automata and Verification of Distributed Algorithms Abstract: Combining ideas from distributed algorithms and alternating automata, we\nintroduce a new class of finite graph automata that recognize precisely the\nlanguages of finite graphs definable in monadic second-order logic. By\nrestricting transitions to be nondeterministic or deterministic, we also obtain\ntwo strictly weaker variants of our automata for which the emptiness problem is\ndecidable. As an application, we suggest how suitable graph automata might be\nuseful in formal verification of distributed algorithms, using Floyd-Hoare\nlogic. \n\n"}
{"id": "1408.3354", "contents": "Title: Distributed Diffusion-Based LMS for Node-Specific Adaptive Parameter\n  Estimation Abstract: A distributed adaptive algorithm is proposed to solve a node-specific\nparameter estimation problem where nodes are interested in estimating\nparameters of local interest, parameters of common interest to a subset of\nnodes and parameters of global interest to the whole network. To address the\ndifferent node-specific parameter estimation problems, this novel algorithm\nrelies on a diffusion-based implementation of different Least Mean Squares\n(LMS) algorithms, each associated with the estimation of a specific set of\nlocal, common or global parameters. Coupled with the estimation of the\ndifferent sets of parameters, the implementation of each LMS algorithm is only\nundertaken by the nodes of the network interested in a specific set of local,\ncommon or global parameters. The study of convergence in the mean sense reveals\nthat the proposed algorithm is asymptotically unbiased. Moreover, a\nspatial-temporal energy conservation relation is provided to evaluate the\nsteady-state performance at each node in the mean-square sense. Finally, the\ntheoretical results and the effectiveness of the proposed technique are\nvalidated through computer simulations in the context of cooperative spectrum\nsensing in Cognitive Radio networks. \n\n"}
{"id": "1409.0085", "contents": "Title: Designing Path Planning Algorithms for Mobile Anchor towards Range-Free\n  Localization Abstract: Localization is one of the most important factor in wireless sensor networks\nas many applications demand position information of sensors. Recently there is\nan increasing interest on the use of mobile anchors for localizing sensors.\nMost of the works available in the literature either looks into the aspect of\nreducing path length of mobile anchor or tries to increase localization\naccuracy. The challenge is to design a movement strategy for a mobile anchor\nthat reduces path length while meeting the requirements of a good range-free\nlocalization technique. In this paper we propose two cost-effective movement\nstrategies i.e., path planning for a mobile anchor so that localization can be\ndone using the localization scheme \\cite{Lee2009}. In one strategy we use a\nhexagonal movement pattern for the mobile anchor to localize all sensors inside\na bounded rectangular region with lesser movement compared to the existing\nworks in literature. In other strategy we consider a connected network in an\nunbounded region where the mobile anchor moves in the hexagonal pattern to\nlocalize the sensors. In this approach, we guarantee localization of all\nsensors within $r/2$ error-bound where $r$ is the communication range of the\nmobile anchor and sensors. Our simulation results support theoretical results\nalong with localization accuracy. \n\n"}
{"id": "1409.1259", "contents": "Title: On the Properties of Neural Machine Translation: Encoder-Decoder\n  Approaches Abstract: Neural machine translation is a relatively new approach to statistical\nmachine translation based purely on neural networks. The neural machine\ntranslation models often consist of an encoder and a decoder. The encoder\nextracts a fixed-length representation from a variable-length input sentence,\nand the decoder generates a correct translation from this representation. In\nthis paper, we focus on analyzing the properties of the neural machine\ntranslation using two models; RNN Encoder--Decoder and a newly proposed gated\nrecursive convolutional neural network. We show that the neural machine\ntranslation performs relatively well on short sentences without unknown words,\nbut its performance degrades rapidly as the length of the sentence and the\nnumber of unknown words increase. Furthermore, we find that the proposed gated\nrecursive convolutional network learns a grammatical structure of a sentence\nautomatically. \n\n"}
{"id": "1409.1551", "contents": "Title: Synchronizing Edits in Distributed Storage Networks Abstract: We consider the problem of synchronizing data in distributed storage networks\nunder an edit model that includes deletions and insertions. We present two\nmodifications of MDS, regenerating and locally repairable codes that allow\nupdates in the parity-check values to be performed with one round of\ncommunication at low bit rates and using small storage overhead. Our main\ncontributions are novel protocols for synchronizing both hot and semi-static\ndata and protocols for data deduplication applications, based on intermediary\npermutation, Vandermonde and Cauchy matrix coding. \n\n"}
{"id": "1409.2329", "contents": "Title: Recurrent Neural Network Regularization Abstract: We present a simple regularization technique for Recurrent Neural Networks\n(RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most successful\ntechnique for regularizing neural networks, does not work well with RNNs and\nLSTMs. In this paper, we show how to correctly apply dropout to LSTMs, and show\nthat it substantially reduces overfitting on a variety of tasks. These tasks\ninclude language modeling, speech recognition, image caption generation, and\nmachine translation. \n\n"}
{"id": "1409.2383", "contents": "Title: Parallel Algorithms for Constrained Tensor Factorization via the\n  Alternating Direction Method of Multipliers Abstract: Tensor factorization has proven useful in a wide range of applications, from\nsensor array processing to communications, speech and audio signal processing,\nand machine learning. With few recent exceptions, all tensor factorization\nalgorithms were originally developed for centralized, in-memory computation on\na single machine; and the few that break away from this mold do not easily\nincorporate practically important constraints, such as nonnegativity. A new\nconstrained tensor factorization framework is proposed in this paper, building\nupon the Alternating Direction method of Multipliers (ADMoM). It is shown that\nthis simplifies computations, bypassing the need to solve constrained\noptimization problems in each iteration; and it naturally leads to distributed\nalgorithms suitable for parallel implementation on regular high-performance\ncomputing (e.g., mesh) architectures. This opens the door for many emerging big\ndata-enabled applications. The methodology is exemplified using nonnegativity\nas a baseline constraint, but the proposed framework can more-or-less readily\nincorporate many other types of constraints. Numerical experiments are very\nencouraging, indicating that the ADMoM-based nonnegative tensor factorization\n(NTF) has high potential as an alternative to state-of-the-art approaches. \n\n"}
{"id": "1409.5705", "contents": "Title: Distributed Machine Learning via Sufficient Factor Broadcasting Abstract: Matrix-parametrized models, including multiclass logistic regression and\nsparse coding, are used in machine learning (ML) applications ranging from\ncomputer vision to computational biology. When these models are applied to\nlarge-scale ML problems starting at millions of samples and tens of thousands\nof classes, their parameter matrix can grow at an unexpected rate, resulting in\nhigh parameter synchronization costs that greatly slow down distributed\nlearning. To address this issue, we propose a Sufficient Factor Broadcasting\n(SFB) computation model for efficient distributed learning of a large family of\nmatrix-parameterized models, which share the following property: the parameter\nupdate computed on each data sample is a rank-1 matrix, i.e., the outer product\nof two \"sufficient factors\" (SFs). By broadcasting the SFs among worker\nmachines and reconstructing the update matrices locally at each worker, SFB\nimproves communication efficiency --- communication costs are linear in the\nparameter matrix's dimensions, rather than quadratic --- without affecting\ncomputational correctness. We present a theoretical convergence analysis of\nSFB, and empirically corroborate its efficiency on four different\nmatrix-parametrized ML models. \n\n"}
{"id": "1409.7495", "contents": "Title: Unsupervised Domain Adaptation by Backpropagation Abstract: Top-performing deep architectures are trained on massive amounts of labeled\ndata. In the absence of labeled data for a certain task, domain adaptation\noften provides an attractive option given that labeled data of similar nature\nbut from a different domain (e.g. synthetic images) are available. Here, we\npropose a new approach to domain adaptation in deep architectures that can be\ntrained on large amount of labeled data from the source domain and large amount\nof unlabeled data from the target domain (no labeled target-domain data is\nnecessary).\n  As the training progresses, the approach promotes the emergence of \"deep\"\nfeatures that are (i) discriminative for the main learning task on the source\ndomain and (ii) invariant with respect to the shift between the domains. We\nshow that this adaptation behaviour can be achieved in almost any feed-forward\nmodel by augmenting it with few standard layers and a simple new gradient\nreversal layer. The resulting augmented architecture can be trained using\nstandard backpropagation.\n  Overall, the approach can be implemented with little effort using any of the\ndeep-learning packages. The method performs very well in a series of image\nclassification experiments, achieving adaptation effect in the presence of big\ndomain shifts and outperforming previous state-of-the-art on Office datasets. \n\n"}
{"id": "1410.4373", "contents": "Title: Maintaining a Distributed Spanning Forest in Highly Dynamic Networks Abstract: Highly dynamic networks are characterized by frequent changes in the\navailability of communication links. These networks are often partitioned into\nseveral components, which split and merge unpredictably. We present a\ndistributed algorithm that maintains a forest of (as few as possible) spanning\ntrees in such a network, with no restriction on the rate of change. Our\nalgorithm is inspired by high-level graph transformations, which we adapt here\nin a (synchronous) message passing model for dynamic networks. The resulting\nalgorithm has the following properties: First, every decision is purely\nlocal---in each round, a node only considers its role and that of its neighbors\nin the tree, with no further information propagation (in particular, no wave\nmechanisms). Second, whatever the rate and scale of the changes, the algorithm\nguarantees that, by the end of every round, the network is covered by a forest\nof spanning trees in which 1) no cycle occur, 2) every node belongs to exactly\none tree, and 3) every tree contains exactly one root (or token). We primarily\nfocus on the correctness of this algorithm, which is established rigorously.\nWhile performance is not the main focus, we suggest new complexity metrics for\nsuch problems, and report on preliminary experimentation results validating our\nalgorithm in a practical scenario. \n\n"}
{"id": "1410.4984", "contents": "Title: Gaussian Process Models with Parallelization and GPU acceleration Abstract: In this work, we present an extension of Gaussian process (GP) models with\nsophisticated parallelization and GPU acceleration. The parallelization scheme\narises naturally from the modular computational structure w.r.t. datapoints in\nthe sparse Gaussian process formulation. Additionally, the computational\nbottleneck is implemented with GPU acceleration for further speed up. Combining\nboth techniques allows applying Gaussian process models to millions of\ndatapoints. The efficiency of our algorithm is demonstrated with a synthetic\ndataset. Its source code has been integrated into our popular software library\nGPy. \n\n"}
{"id": "1410.7057", "contents": "Title: Sparse Distributed Learning via Heterogeneous Diffusion Adaptive\n  Networks Abstract: In-network distributed estimation of sparse parameter vectors via diffusion\nLMS strategies has been studied and investigated in recent years. In all the\nexisting works, some convex regularization approach has been used at each node\nof the network in order to achieve an overall network performance superior to\nthat of the simple diffusion LMS, albeit at the cost of increased computational\noverhead. In this paper, we provide analytical as well as experimental results\nwhich show that the convex regularization can be selectively applied only to\nsome chosen nodes keeping rest of the nodes sparsity agnostic, while still\nenjoying the same optimum behavior as can be realized by deploying the convex\nregularization at all the nodes. Due to the incorporation of unregularized\nlearning at a subset of nodes, less computational cost is needed in the\nproposed approach. We also provide a guideline for selection of the sparsity\naware nodes and a closed form expression for the optimum regularization\nparameter. \n\n"}
{"id": "1411.0968", "contents": "Title: Convergence Analysis for Regular Wireless Consensus Networks Abstract: Average consensus algorithms can be implemented over wireless sensor networks\n(WSN), where global statistics can be computed using communications among\nsensor nodes locally. Simple execution, robustness to global topology changes\ndue to frequent node failures and underlying distributed philosophy has made\nconsensus algorithms more suitable to WSNs. Since these algorithms are\niterative in nature, their performance is characterized by convergence speed.\nWe study the convergence of the average consensus algorithms for WSNs using\nregular graphs. We obtained the analytical expressions for optimal consensus\nand convergence parameters which decides the convergence time for r-nearest\nneighbor cycle and torus networks. We have also derived the generalized\nexpression for optimal consensus and convergence parameters for m-dimensional\nr-nearest neighbor torus networks. The obtained analytical results agree with\nthe simulation results and shown the effect of network dimension, number of\nnodes and transmission radius on convergence time. This work provides the basic\nanalytical tools for managing and controlling the performance of average\nconsensus algorithm in the finite sized practical networks. \n\n"}
{"id": "1411.2429", "contents": "Title: Patterns in the Chaos - a Study of Performance Variation and\n  Predictability in Public IaaS Clouds Abstract: Benchmarking the performance of public cloud providers is a common research\ntopic. Previous research has already extensively evaluated the performance of\ndifferent cloud platforms for different use cases, and under different\nconstraints and experiment setups. In this paper, we present a principled,\nlarge-scale literature review to collect and codify existing research regarding\nthe predictability of performance in public Infrastructure-as-a-Service (IaaS)\nclouds. We formulate 15 hypotheses relating to the nature of performance\nvariations in IaaS systems, to the factors of influence of performance\nvariations, and how to compare different instance types. In a second step, we\nconduct extensive real-life experimentation on Amazon EC2 and Google Compute\nEngine to empirically validate those hypotheses. At the time of our research,\nperformance in EC2 was substantially less predictable than in GCE. Further, we\nshow that hardware heterogeneity is in practice less prevalent than anticipated\nby earlier research, while multi-tenancy has a dramatic impact on performance\nand predictability. \n\n"}
{"id": "1411.4186", "contents": "Title: Linear Time Average Consensus on Fixed Graphs and Implications for\n  Decentralized Optimization and Multi-Agent Control Abstract: We describe a protocol for the average consensus problem on any fixed\nundirected graph whose convergence time scales linearly in the total number\nnodes $n$. The protocol is completely distributed, with the exception of\nrequiring all nodes to know the same upper bound $U$ on the total number of\nnodes which is correct within a constant multiplicative factor.\n  We next discuss applications of this protocol to problems in multi-agent\ncontrol connected to the consensus problem. In particular, we describe\nprotocols for formation maintenance and leader-following with convergence times\nwhich also scale linearly with the number of nodes.\n  Finally, we develop a distributed protocol for minimizing an average of\n(possibly nondifferentiable) convex functions $ (1/n) \\sum_{i=1}^n\nf_i(\\theta)$, in the setting where only node $i$ in an undirected, connected\ngraph knows the function $f_i(\\theta)$. Under the same assumption about all\nnodes knowing $U$, and additionally assuming that the subgradients of each\n$f_i(\\theta)$ have absolute values upper bounded by some constant $L$ known to\nthe nodes, we show that after $T$ iterations our protocol has error which is\n$O(L \\sqrt{n/T})$. \n\n"}
{"id": "1411.5282", "contents": "Title: Reaching Approximate Byzantine Consensus with Multi-hop Communication Abstract: We address the problem of reaching consensus in the presence of Byzantine\nfaults. In particular, we are interested in investigating the impact of\nmessages relay on the network connectivity for a correct iterative approximate\nByzantine consensus algorithm to exist. The network is modeled by a simple\ndirected graph. We assume a node can send messages to another node that is up\nto $l$ hops away via forwarding by the intermediate nodes on the routes, where\n$l\\in \\mathbb{N}$ is a natural number. We characterize the necessary and\nsufficient topological conditions on the network structure. The tight\nconditions we found are consistent with the tight conditions identified for\n$l=1$, where only local communication is allowed, and are strictly weaker for\n$l>1$. Let $l^*$ denote the length of a longest path in the given network. For\n$l\\ge l^*$ and undirected graphs, our conditions hold if and only if $n\\ge\n3f+1$ and the node-connectivity of the given graph is at least $2f+1$ , where\n$n$ is the total number of nodes and $f$ is the maximal number of Byzantine\nnodes; and for $l\\ge l^*$ and directed graphs, our conditions is equivalent to\nthe tight condition found for exact Byzantine consensus.\n  Our sufficiency is shown by constructing a correct algorithm, wherein the\ntrim function is constructed based on investigating a newly introduced minimal\nmessages cover property. The trim function proposed also works over\nmulti-graphs. \n\n"}
{"id": "1412.1885", "contents": "Title: Decomposition of Big Tensors With Low Multilinear Rank Abstract: Tensor decompositions are promising tools for big data analytics as they\nbring multiple modes and aspects of data to a unified framework, which allows\nus to discover complex internal structures and correlations of data.\nUnfortunately most existing approaches are not designed to meet the major\nchallenges posed by big data analytics. This paper attempts to improve the\nscalability of tensor decompositions and provides two contributions: A flexible\nand fast algorithm for the CP decomposition (FFCP) of tensors based on their\nTucker compression; A distributed randomized Tucker decomposition approach for\narbitrarily big tensors but with relatively low multilinear rank. These two\nalgorithms can deal with huge tensors, even if they are dense. Extensive\nsimulations provide empirical evidence of the validity and efficiency of the\nproposed algorithms. \n\n"}
{"id": "1412.3489", "contents": "Title: Quantum Deep Learning Abstract: In recent years, deep learning has had a profound impact on machine learning\nand artificial intelligence. At the same time, algorithms for quantum computers\nhave been shown to efficiently solve some problems that are intractable on\nconventional, classical computers. We show that quantum computing not only\nreduces the time required to train a deep restricted Boltzmann machine, but\nalso provides a richer and more comprehensive framework for deep learning than\nclassical computing and leads to significant improvements in the optimization\nof the underlying objective function. Our quantum methods also permit efficient\ntraining of full Boltzmann machines and multi-layer, fully connected models and\ndo not have well known classical counterparts. \n\n"}
{"id": "1412.4314", "contents": "Title: Recurrent-Neural-Network for Language Detection on Twitter\n  Code-Switching Corpus Abstract: Mixed language data is one of the difficult yet less explored domains of\nnatural language processing. Most research in fields like machine translation\nor sentiment analysis assume monolingual input. However, people who are capable\nof using more than one language often communicate using multiple languages at\nthe same time. Sociolinguists believe this \"code-switching\" phenomenon to be\nsocially motivated. For example, to express solidarity or to establish\nauthority. Most past work depend on external tools or resources, such as\npart-of-speech tagging, dictionary look-up, or named-entity recognizers to\nextract rich features for training machine learning models. In this paper, we\ntrain recurrent neural networks with only raw features, and use word embedding\nto automatically learn meaningful representations. Using the same\nmixed-language Twitter corpus, our system is able to outperform the best\nSVM-based systems reported in the EMNLP'14 Code-Switching Workshop by 1% in\naccuracy, or by 17% in error rate reduction. \n\n"}
{"id": "1412.4967", "contents": "Title: Sparse, guided feature connections in an Abstract Deep Network Abstract: We present a technique for developing a network of re-used features, where\nthe topology is formed using a coarse learning method, that allows\ngradient-descent fine tuning, known as an Abstract Deep Network (ADN). New\nfeatures are built based on observed co-occurrences, and the network is\nmaintained using a selection process related to evolutionary algorithms. This\nallows coarse ex- ploration of the problem space, effective for irregular\ndomains, while gradient descent allows pre- cise solutions. Accuracy on\nstandard UCI and Protein-Structure Prediction problems is comparable with\nbenchmark SVM and optimized GBML approaches, and shows scalability for\naddressing large problems. The discrete implementation is symbolic, allowing\ninterpretability, while the continuous method using fine-tuning shows improved\naccuracy. The binary multiplexer problem is explored, as an irregular domain\nthat does not support gradient descent learning, showing solution to the bench-\nmark 135-bit problem. A convolutional implementation is demonstrated on image\nclassification, showing an error-rate of 0.79% on the MNIST problem, without a\npre-defined topology. The ADN system provides a method for developing a very\nsparse, deep feature topology, based on observed relationships between\nfeatures, that is able to find solutions in irregular domains, and initialize a\nnetwork prior to gradient descent learning. \n\n"}
{"id": "1412.5068", "contents": "Title: Towards Deep Neural Network Architectures Robust to Adversarial Examples Abstract: Recent work has shown deep neural networks (DNNs) to be highly susceptible to\nwell-designed, small perturbations at the input layer, or so-called adversarial\nexamples. Taking images as an example, such distortions are often\nimperceptible, but can result in 100% mis-classification for a state of the art\nDNN. We study the structure of adversarial examples and explore network\ntopology, pre-processing and training strategies to improve the robustness of\nDNNs. We perform various experiments to assess the removability of adversarial\nexamples by corrupting with additional noise and pre-processing with denoising\nautoencoders (DAEs). We find that DAEs can remove substantial amounts of the\nadversarial noise. How- ever, when stacking the DAE with the original DNN, the\nresulting network can again be attacked by new adversarial examples with even\nsmaller distortion. As a solution, we propose Deep Contractive Network, a model\nwith a new end-to-end training procedure that includes a smoothness penalty\ninspired by the contractive autoencoder (CAE). This increases the network\nrobustness to adversarial examples, without a significant performance penalty. \n\n"}
{"id": "1412.5661", "contents": "Title: DeepID-Net: Deformable Deep Convolutional Neural Networks for Object\n  Detection Abstract: In this paper, we propose deformable deep convolutional neural networks for\ngeneric object detection. This new deep learning object detection framework has\ninnovations in multiple aspects. In the proposed new deep architecture, a new\ndeformation constrained pooling (def-pooling) layer models the deformation of\nobject parts with geometric constraint and penalty. A new pre-training strategy\nis proposed to learn feature representations more suitable for the object\ndetection task and with good generalization capability. By changing the net\nstructures, training strategies, adding and removing some key components in the\ndetection pipeline, a set of models with large diversity are obtained, which\nsignificantly improves the effectiveness of model averaging. The proposed\napproach improves the mean averaged precision obtained by RCNN\n\\cite{girshick2014rich}, which was the state-of-the-art, from 31\\% to 50.3\\% on\nthe ILSVRC2014 detection test set. It also outperforms the winner of\nILSVRC2014, GoogLeNet, by 6.1\\%. Detailed component-wise analysis is also\nprovided through extensive experimental evaluation, which provide a global view\nfor people to understand the deep learning object detection pipeline. \n\n"}
{"id": "1412.6392", "contents": "Title: A Self-adaptive Auto-scaling Method for Scientific Applications on HPC\n  Environments and Clouds Abstract: High intensive computation applications can usually take days to months to\nfinish an execution. During this time, it is common to have variations of the\navailable resources when considering that such hardware is usually shared among\na plurality of researchers/departments within an organization. On the other\nhand, High Performance Clusters can take advantage of Cloud Computing bursting\ntechniques for the execution of applications together with the on-premise\nresources. In order to meet deadlines, high intensive computational\napplications can use the Cloud to boost their performance when they are data\nand task parallel. This article presents an ongoing work towards the use of\nextended resources of an HPC execution platform together with Cloud. We propose\nan unified view of such heterogeneous environments and a method that monitors,\npredicts the application execution time, and dynamically shifts part of the\ndomain -- previously running in local HPC hardware -- to be computed on the\nCloud, meeting then a specific deadline. The method is exemplified along with a\nseismic application that, at runtime, adapts itself to move part of the\nprocessing to the Cloud (in a movement called bursting) and also auto-scales\n(the moved part) over cloud nodes. Our preliminary results show that there is\nan expected overhead for performing this movement and for synchronizing\nresults, but our outcomes demonstrate it is an important feature for meeting\ndeadlines in the case an on-premise cluster is overloaded or cannot provide the\ncapacity needed for a particular project. \n\n"}
{"id": "1412.6583", "contents": "Title: Discovering Hidden Factors of Variation in Deep Networks Abstract: Deep learning has enjoyed a great deal of success because of its ability to\nlearn useful features for tasks such as classification. But there has been less\nexploration in learning the factors of variation apart from the classification\nsignal. By augmenting autoencoders with simple regularization terms during\ntraining, we demonstrate that standard deep architectures can discover and\nexplicitly represent factors of variation beyond those relevant for\ncategorization. We introduce a cross-covariance penalty (XCov) as a method to\ndisentangle factors like handwriting style for digits and subject identity in\nfaces. We demonstrate this on the MNIST handwritten digit database, the Toronto\nFaces Database (TFD) and the Multi-PIE dataset by generating manipulated\ninstances of the data. Furthermore, we demonstrate these deep networks can\nextrapolate `hidden' variation in the supervised signal. \n\n"}
{"id": "1412.6806", "contents": "Title: Striving for Simplicity: The All Convolutional Net Abstract: Most modern convolutional neural networks (CNNs) used for object recognition\nare built using the same principles: Alternating convolution and max-pooling\nlayers followed by a small number of fully connected layers. We re-evaluate the\nstate of the art for object recognition from small images with convolutional\nnetworks, questioning the necessity of different components in the pipeline. We\nfind that max-pooling can simply be replaced by a convolutional layer with\nincreased stride without loss in accuracy on several image recognition\nbenchmarks. Following this finding -- and building on other recent work for\nfinding simple network structures -- we propose a new architecture that\nconsists solely of convolutional layers and yields competitive or state of the\nart performance on several object recognition datasets (CIFAR-10, CIFAR-100,\nImageNet). To analyze the network we introduce a new variant of the\n\"deconvolution approach\" for visualizing features learned by CNNs, which can be\napplied to a broader range of network structures than existing approaches. \n\n"}
{"id": "1412.7024", "contents": "Title: Training deep neural networks with low precision multiplications Abstract: Multipliers are the most space and power-hungry arithmetic operators of the\ndigital implementation of deep neural networks. We train a set of\nstate-of-the-art neural networks (Maxout networks) on three benchmark datasets:\nMNIST, CIFAR-10 and SVHN. They are trained with three distinct formats:\nfloating point, fixed point and dynamic fixed point. For each of those datasets\nand for each of those formats, we assess the impact of the precision of the\nmultiplications on the final error after training. We find that very low\nprecision is sufficient not just for running trained networks but also for\ntraining them. For example, it is possible to train Maxout networks with 10\nbits multiplications. \n\n"}
{"id": "1412.7062", "contents": "Title: Semantic Image Segmentation with Deep Convolutional Nets and Fully\n  Connected CRFs Abstract: Deep Convolutional Neural Networks (DCNNs) have recently shown state of the\nart performance in high level vision tasks, such as image classification and\nobject detection. This work brings together methods from DCNNs and\nprobabilistic graphical models for addressing the task of pixel-level\nclassification (also called \"semantic image segmentation\"). We show that\nresponses at the final layer of DCNNs are not sufficiently localized for\naccurate object segmentation. This is due to the very invariance properties\nthat make DCNNs good for high level tasks. We overcome this poor localization\nproperty of deep networks by combining the responses at the final DCNN layer\nwith a fully connected Conditional Random Field (CRF). Qualitatively, our\n\"DeepLab\" system is able to localize segment boundaries at a level of accuracy\nwhich is beyond previous methods. Quantitatively, our method sets the new\nstate-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching\n71.6% IOU accuracy in the test set. We show how these results can be obtained\nefficiently: Careful network re-purposing and a novel application of the 'hole'\nalgorithm from the wavelet community allow dense computation of neural net\nresponses at 8 frames per second on a modern GPU. \n\n"}
{"id": "1412.7116", "contents": "Title: Online Distributed ADMM on Networks Abstract: This paper examines online distributed Alternating Direction Method of\nMultipliers (ADMM). The goal is to distributively optimize a global objective\nfunction over a network of decision makers under linear constraints. The global\nobjective function is composed of convex cost functions associated with each\nagent. The local cost functions, on the other hand, are assumed to have been\ndecomposed into two distinct convex functions, one of which is revealed to the\ndecision makers over time and one known a priori. In addition, the agents must\nachieve consensus on the global variable that relates to the private local\nvariables via linear constraints. In this work, we extend online ADMM to a\ndistributed setting based on dual-averaging and distributed gradient descent.\nWe then propose a performance metric for such online distributed algorithms and\nexplore the performance of the sequence of decisions generated by the algorithm\nas compared with the best fixed decision in hindsight. This performance metric\nis called the social regret. A sub-linear upper bound on the social regret of\nthe proposed algorithm is then obtained that underscores the role of the\nunderlying network topology and certain condition measures associated with the\nlinear constraints. The online distributed ADMM algorithm is then applied to a\nformation acquisition problem demonstrating the application of the proposed\nsetup in distributed robotics. \n\n"}
{"id": "1412.7419", "contents": "Title: ADASECANT: Robust Adaptive Secant Method for Stochastic Gradient Abstract: Stochastic gradient algorithms have been the main focus of large-scale\nlearning problems and they led to important successes in machine learning. The\nconvergence of SGD depends on the careful choice of learning rate and the\namount of the noise in stochastic estimates of the gradients. In this paper, we\npropose a new adaptive learning rate algorithm, which utilizes curvature\ninformation for automatically tuning the learning rates. The information about\nthe element-wise curvature of the loss function is estimated from the local\nstatistics of the stochastic first order gradients. We further propose a new\nvariance reduction technique to speed up the convergence. In our preliminary\nexperiments with deep neural networks, we obtained better performance compared\nto the popular stochastic gradient algorithms. \n\n"}
{"id": "1412.7489", "contents": "Title: A Unified Perspective on Multi-Domain and Multi-Task Learning Abstract: In this paper, we provide a new neural-network based perspective on\nmulti-task learning (MTL) and multi-domain learning (MDL). By introducing the\nconcept of a semantic descriptor, this framework unifies MDL and MTL as well as\nencompassing various classic and recent MTL/MDL algorithms by interpreting them\nas different ways of constructing semantic descriptors. Our interpretation\nprovides an alternative pipeline for zero-shot learning (ZSL), where a model\nfor a novel class can be constructed without training data. Moreover, it leads\nto a new and practically relevant problem setting of zero-shot domain\nadaptation (ZSDA), which is the analogous to ZSL but for novel domains: A model\nfor an unseen domain can be generated by its semantic descriptor. Experiments\nacross this range of problems demonstrate that our framework outperforms a\nvariety of alternatives. \n\n"}
{"id": "1412.7755", "contents": "Title: Multiple Object Recognition with Visual Attention Abstract: We present an attention-based model for recognizing multiple objects in\nimages. The proposed model is a deep recurrent neural network trained with\nreinforcement learning to attend to the most relevant regions of the input\nimage. We show that the model learns to both localize and recognize multiple\nobjects despite being given only class labels during training. We evaluate the\nmodel on the challenging task of transcribing house number sequences from\nGoogle Street View images and show that it is both more accurate than the\nstate-of-the-art convolutional networks and uses fewer parameters and less\ncomputation. \n\n"}
{"id": "1412.8097", "contents": "Title: The Adversarial Noise Threshold for Distributed Protocols Abstract: We consider the problem of implementing distributed protocols, despite\nadversarial channel errors, on synchronous-messaging networks with arbitrary\ntopology.\n  In our first result we show that any $n$-party $T$-round protocol on an\nundirected communication network $G$ can be compiled into a robust simulation\nprotocol on a sparse ($\\mathcal{O}(n)$ edges) subnetwork so that the simulation\ntolerates an adversarial error rate of $\\Omega\\left(\\frac{1}{n}\\right)$; the\nsimulation has a round complexity of $\\mathcal{O}\\left(\\frac{m \\log n}{n}\nT\\right)$, where $m$ is the number of edges in $G$. (So the simulation is\nwork-preserving up to a $\\log$ factor.) The adversary's error rate is within a\nconstant factor of optimal. Given the error rate, the round complexity blowup\nis within a factor of $\\mathcal{O}(k \\log n)$ of optimal, where $k$ is the edge\nconnectivity of $G$. We also determine that the maximum tolerable error rate on\ndirected communication networks is $\\Theta(1/s)$ where $s$ is the number of\nedges in a minimum equivalent digraph.\n  Next we investigate adversarial per-edge error rates, where the adversary is\ngiven an error budget on each edge of the network. We determine the exact limit\nfor tolerable per-edge error rates on an arbitrary directed graph. However, the\nconstruction that approaches this limit has exponential round complexity, so we\ngive another compiler, which transforms $T$-round protocols into\n$\\mathcal{O}(mT)$-round simulations, and prove that for polynomial-query black\nbox compilers, the per-edge error rate tolerated by this last compiler is\nwithin a constant factor of optimal. \n\n"}
{"id": "1412.8324", "contents": "Title: A Constructive Proof on the Compositionality of Linearizability Abstract: Linearizability is the strongest correctness property for both shared memory\nand message passing systems. One of its useful features is the\ncompositionality: a history (execution) is linearizable if and only if each\nobject (component) subhistory is linearizable. In this paper, we propose a new\nhierarchical system model to address challenges in modular development of cloud\nsystems. Object are defined by induction from the most fundamental atomic\nBoolean registers, and histories are represented as countable well-ordered\nstructures of events to deal with both finite and infinite executions. Then, we\npresent a new constructive proof on the compositionality theorem of\nlinearizability inspired by Multiway Merge. This proof deduces a theoretically\nefficient algorithm which generates linearization in O(N*logP) running time\nwith O(N) space, where P and N are process/event numbers respectively. \n\n"}
{"id": "1501.01678", "contents": "Title: LeoTask: a fast, flexible and reliable framework for computational\n  research Abstract: LeoTask is a Java library for computation-intensive and time-consuming\nresearch tasks. It automatically executes tasks in parallel on multiple CPU\ncores on a computing facility. It uses a configuration file to enable automatic\nexploration of parameter space and flexible aggregation of results, and\ntherefore allows researchers to focus on programming the key logic of a\ncomputing task. It also supports reliable recovery from interruptions, dynamic\nand cloneable networks, and integration with the plotting software Gnuplot. \n\n"}
{"id": "1501.02165", "contents": "Title: Update Consistency for Wait-free Concurrent Objects Abstract: In large scale systems such as the Internet, replicating data is an essential\nfeature in order to provide availability and fault-tolerance. Attiya and Welch\nproved that using strong consistency criteria such as atomicity is costly as\neach operation may need an execution time linear with the latency of the\ncommunication network. Weaker consistency criteria like causal consistency and\nPRAM consistency do not ensure convergence. The different replicas are not\nguaranteed to converge towards a unique state. Eventual consistency guarantees\nthat all replicas eventually converge when the participants stop updating.\nHowever, it fails to fully specify the semantics of the operations on shared\nobjects and requires additional non-intuitive and error-prone distributed\nspecification techniques. This paper introduces and formalizes a new\nconsistency criterion, called update consistency, that requires the state of a\nreplicated object to be consistent with a linearization of all the updates. In\nother words, whereas atomicity imposes a linearization of all of the\noperations, this criterion imposes this only on updates. Consequently some read\noperations may return out-dated values. Update consistency is stronger than\neventual consistency, so we can replace eventually consistent objects with\nupdate consistent ones in any program. Finally, we prove that update\nconsistency is universal, in the sense that any object can be implemented under\nthis criterion in a distributed system where any number of nodes may crash. \n\n"}
{"id": "1501.03975", "contents": "Title: Stochastic Gradient Based Extreme Learning Machines For Online Learning\n  of Advanced Combustion Engines Abstract: In this article, a stochastic gradient based online learning algorithm for\nExtreme Learning Machines (ELM) is developed (SG-ELM). A stability criterion\nbased on Lyapunov approach is used to prove both asymptotic stability of\nestimation error and stability in the estimated parameters suitable for\nidentification of nonlinear dynamic systems. The developed algorithm not only\nguarantees stability, but also reduces the computational demand compared to the\nOS-ELM approach based on recursive least squares. In order to demonstrate the\neffectiveness of the algorithm on a real-world scenario, an advanced combustion\nengine identification problem is considered. The algorithm is applied to two\ncase studies: An online regression learning for system identification of a\nHomogeneous Charge Compression Ignition (HCCI) Engine and an online\nclassification learning (with class imbalance) for identifying the dynamic\noperating envelope of the HCCI Engine. The results indicate that the accuracy\nof the proposed SG-ELM is comparable to that of the state-of-the-art but adds\nstability and a reduction in computational effort. \n\n"}
{"id": "1501.04784", "contents": "Title: Global finite element matrix construction based on a CPU-GPU\n  implementation Abstract: The finite element method (FEM) has several computational steps to\nnumerically solve a particular problem, to which many efforts have been\ndirected to accelerate the solution stage of the linear system of equations.\nHowever, the finite element matrix construction, which is also time-consuming\nfor unstructured meshes, has been less investigated. The generation of the\nglobal finite element matrix is performed in two steps, computing the local\nmatrices by numerical integration and assembling them into a global system,\nwhich has traditionally been done in serial computing. This work presents a\nfast technique to construct the global finite element matrix that arises by\nsolving the Poisson's equation in a three-dimensional domain. The proposed\nmethodology consists in computing the numerical integration, due to its\nintrinsic parallel opportunities, in the graphics processing unit (GPU) and\ncomputing the matrix assembly, due to its intrinsic serial operations, in the\ncentral processing unit (CPU). In the numerical integration, only the lower\ntriangular part of each local stiffness matrix is computed thanks to its\nsymmetry, which saves GPU memory and computing time. As a result of symmetry,\nthe global sparse matrix also contains non-zero elements only in its lower\ntriangular part, which reduces the assembly operations and memory usage. This\nmethodology allows generating the global sparse matrix from any unstructured\nfinite element mesh size on GPUs with little memory capacity, only limited by\nthe CPU memory. \n\n"}
{"id": "1501.05387", "contents": "Title: Gunrock: A High-Performance Graph Processing Library on the GPU Abstract: For large-scale graph analytics on the GPU, the irregularity of data access\nand control flow, and the complexity of programming GPUs have been two\nsignificant challenges for developing a programmable high-performance graph\nlibrary. \"Gunrock\", our graph-processing system designed specifically for the\nGPU, uses a high-level, bulk-synchronous, data-centric abstraction focused on\noperations on a vertex or edge frontier. Gunrock achieves a balance between\nperformance and expressiveness by coupling high performance GPU computing\nprimitives and optimization strategies with a high-level programming model that\nallows programmers to quickly develop new graph primitives with small code size\nand minimal GPU programming knowledge. We evaluate Gunrock on five key graph\nprimitives and show that Gunrock has on average at least an order of magnitude\nspeedup over Boost and PowerGraph, comparable performance to the fastest GPU\nhardwired primitives, and better performance than any other GPU high-level\ngraph library. \n\n"}
{"id": "1501.06238", "contents": "Title: Sky: Opinion Dynamics Based Consensus for P2P Network with Trust\n  Relationships Abstract: Traditional Byzantine consensus does not work in P2P network due to Sybil\nattack while the most prevalent Sybil-proof consensus at present still can't\nresist adversary with dominant compute power. This paper proposed opinion\ndynamics based consensus for P2P network with trust relationships, consisting\nof the sky framework and the sky model. With the sky framework, opinion\ndynamics can be applied in P2P network for consensus which is Sybil-proof\nthrough trust relationships and emerges from local interactions of each node\nwith its direct contacts without topology, global information or even sample of\nthe network involved. The sky model has better performance of convergence than\nexisting models including MR, voter and Sznajd, and its lower bound of fault\ntolerance performance is also analyzed and proved. Simulations show that our\napproach can tolerant failures by at least 13% random nodes or 2% top\ninfluential nodes while over 96% correct nodes still make correct decision\nwithin 70 seconds on the SNAP Wikipedia who-votes-on-whom network for initial\nconfiguration of convergence>0.5 with reasonable latencies. Comparing to\ncompute power based consensus, our approach can resist any faulty or malicious\nnodes by unfollowing them. To the best of our knowledge, it's the first work to\nbring opinion dynamics to P2P network for consensus. \n\n"}
{"id": "1501.06633", "contents": "Title: maxDNN: An Efficient Convolution Kernel for Deep Learning with Maxwell\n  GPUs Abstract: This paper describes maxDNN, a computationally efficient convolution kernel\nfor deep learning with the NVIDIA Maxwell GPU. maxDNN reaches 96.3%\ncomputational efficiency on typical deep learning network architectures. The\ndesign combines ideas from cuda-convnet2 with the Maxas SGEMM assembly code. We\nonly address forward propagation (FPROP) operation of the network, but we\nbelieve that the same techniques used here will be effective for backward\npropagation (BPROP) as well. \n\n"}
{"id": "1501.07800", "contents": "Title: Locality-aware parallel block-sparse matrix-matrix multiplication using\n  the Chunks and Tasks programming model Abstract: We present a method for parallel block-sparse matrix-matrix multiplication on\ndistributed memory clusters. By using a quadtree matrix representation, data\nlocality is exploited without prior information about the matrix sparsity\npattern. A distributed quadtree matrix representation is straightforward to\nimplement due to our recent development of the Chunks and Tasks programming\nmodel [Parallel Comput. 40, 328 (2014)]. The quadtree representation combined\nwith the Chunks and Tasks model leads to favorable weak and strong scaling of\nthe communication cost with the number of processes, as shown both\ntheoretically and in numerical experiments.\n  Matrices are represented by sparse quadtrees of chunk objects. The leaves in\nthe hierarchy are block-sparse submatrices. Sparsity is dynamically detected by\nthe matrix library and may occur at any level in the hierarchy and/or within\nthe submatrix leaves. In case graphics processing units (GPUs) are available,\nboth CPUs and GPUs are used for leaf-level multiplication work, thus making use\nof the full computing capacity of each node.\n  The performance is evaluated for matrices with different sparsity structures,\nincluding examples from electronic structure calculations. Compared to methods\nthat do not exploit data locality, our locality-aware approach reduces\ncommunication significantly, achieving essentially constant communication per\nnode in weak scaling tests. \n\n"}
{"id": "1502.00378", "contents": "Title: Enabling Minimal Dominating Set in Highly Dynamic Distributed Systems Abstract: We address the problem of computing a Minimal Dominating Set in highly\ndynamic distributed systems. We assume weak connectivity, i.e., the network may\nbe disconnected at each time instant and topological changes are unpredictable.\nWe make only weak assumptions on the communication: every process is infinitely\noften able to communicate with other processes (not necessarily directly). Our\ncontribution is threefold. First, we propose a new definition of minimal\ndominating set suitable for the context of time-varying graphs that seems more\nrelevant than existing ones. Next, we provide a necessary and sufficient\ntopological condition for the existence of a deterministic algorithm for\nminimal dominating set construction in our settings. Finally, we propose a new\nmeasure of time complexity in time-varying graph in order to to allow fair\ncomparison between algorithms. Indeed, this measure takes account of\ncommunication delays attributable to dynamicity of the graph and not to the\nalgorithms. \n\n"}
{"id": "1502.02072", "contents": "Title: Massively Multitask Networks for Drug Discovery Abstract: Massively multitask neural architectures provide a learning framework for\ndrug discovery that synthesizes information from many distinct biological\nsources. To train these architectures at scale, we gather large amounts of data\nfrom public sources to create a dataset of nearly 40 million measurements\nacross more than 200 biological targets. We investigate several aspects of the\nmultitask framework by performing a series of empirical studies and obtain some\ninteresting results: (1) massively multitask networks obtain predictive\naccuracies significantly better than single-task methods, (2) the predictive\npower of multitask networks improves as additional tasks and data are added,\n(3) the total amount of data and the total number of tasks both contribute\nsignificantly to multitask improvement, and (4) multitask networks afford\nlimited transferability to tasks not in the training set. Our results\nunderscore the need for greater data sharing and further algorithmic innovation\nto accelerate the drug discovery process. \n\n"}
{"id": "1502.03372", "contents": "Title: A Fast Distributed Stateless Algorithm for $\\alpha$-Fair Packing\n  Problems Abstract: Over the past two decades, fair resource allocation problems have received\nconsiderable attention in a variety of application areas. However, little\nprogress has been made in the design of distributed algorithms with convergence\nguarantees for general and commonly used $\\alpha$-fair allocations. In this\npaper, we study weighted $\\alpha$-fair packing problems, that is, the problems\nof maximizing the objective functions (i) $\\sum_j w_j\nx_j^{1-\\alpha}/(1-\\alpha)$ when $\\alpha > 0$, $\\alpha \\neq 1$ and (ii) $\\sum_j\nw_j \\ln x_j$ when $\\alpha = 1$, over linear constraints $Ax \\leq b$, $x\\geq 0$,\nwhere $w_j$ are positive weights and $A$ and $b$ are non-negative. We consider\nthe distributed computation model that was used for packing linear programs and\nnetwork utility maximization problems. Under this model, we provide a\ndistributed algorithm for general $\\alpha$ that converges to an\n$\\varepsilon-$approximate solution in time (number of distributed iterations)\nthat has an inverse polynomial dependence on the approximation parameter\n$\\varepsilon$ and poly-logarithmic dependence on the problem size. This is the\nfirst distributed algorithm for weighted $\\alpha-$fair packing with\npoly-logarithmic convergence in the input size. The algorithm uses simple local\nupdate rules and is stateless (namely, it allows asynchronous updates, is\nself-stabilizing, and allows incremental and local adjustments). We also obtain\na number of structural results that characterize $\\alpha-$fair allocations as\nthe value of $\\alpha$ is varied. These results deepen our understanding of\nfairness guarantees in $\\alpha-$fair packing allocations, and also provide\ninsight into the behavior of $\\alpha-$fair allocations in the asymptotic cases\n$\\alpha\\rightarrow 0$, $\\alpha \\rightarrow 1$, and $\\alpha \\rightarrow \\infty$. \n\n"}
{"id": "1502.03504", "contents": "Title: Locally-Oriented Programming: A Simple Programming Model for\n  Stencil-Based Computations on Multi-Level Distributed Memory Architectures Abstract: Emerging hybrid accelerator architectures for high performance computing are\noften suited for the use of a data-parallel programming model. Unfortunately,\nprogrammers of these architectures face a steep learning curve that frequently\nrequires learning a new language (e.g., OpenCL). Furthermore, the distributed\n(and frequently multi-level) nature of the memory organization of clusters of\nthese machines provides an additional level of complexity. This paper presents\npreliminary work examining how programming with a local orientation can be\nemployed to provide simpler access to accelerator architectures. A\nlocally-oriented programming model is especially useful for the solution of\nalgorithms requiring the application of a stencil or convolution kernel. In\nthis programming model, a programmer codes the algorithm by modifying only a\nsingle array element (called the local element), but has read-only access to a\nsmall sub-array surrounding the local element. We demonstrate how a\nlocally-oriented programming model can be adopted as a language extension using\nsource-to-source program transformations. \n\n"}
{"id": "1502.03645", "contents": "Title: Numerical simulation of skin transport using Parareal Abstract: In-silico investigation of skin permeation is an important but also\ncomputationally demanding problem. To resolve all scales involved in full\ndetail will not only require exascale computing capacities but also suitable\nparallel algorithms. This article investigates the applicability of the\ntime-parallel Parareal algorithm to a brick and mortar setup, a precursory\nproblem to skin permeation. The C++ library Lib4PrM implementing Parareal is\ncombined with the UG4 simulation framework, which provides the spatial\ndiscretization and parallelization. The combination's performance is studied\nwith respect to convergence and speedup. It is confirmed that anisotropies in\nthe domain and jumps in diffusion coefficients only have a minor impact on\nParareal's convergence. The influence of load imbalances in time due to\ndifferences in number of iterations required by the spatial solver as well as\nspatio-temporal weak scaling is discussed. \n\n"}
{"id": "1502.04511", "contents": "Title: Locally Optimal Load Balancing Abstract: This work studies distributed algorithms for locally optimal load-balancing:\nWe are given a graph of maximum degree $\\Delta$, and each node has up to $L$\nunits of load. The task is to distribute the load more evenly so that the loads\nof adjacent nodes differ by at most $1$.\n  If the graph is a path ($\\Delta = 2$), it is easy to solve the fractional\nversion of the problem in $O(L)$ communication rounds, independently of the\nnumber of nodes. We show that this is tight, and we show that it is possible to\nsolve also the discrete version of the problem in $O(L)$ rounds in paths.\n  For the general case ($\\Delta > 2$), we show that fractional load balancing\ncan be solved in $\\operatorname{poly}(L,\\Delta)$ rounds and discrete load\nbalancing in $f(L,\\Delta)$ rounds for some function $f$, independently of the\nnumber of nodes. \n\n"}
{"id": "1502.04681", "contents": "Title: Unsupervised Learning of Video Representations using LSTMs Abstract: We use multilayer Long Short Term Memory (LSTM) networks to learn\nrepresentations of video sequences. Our model uses an encoder LSTM to map an\ninput sequence into a fixed length representation. This representation is\ndecoded using single or multiple decoder LSTMs to perform different tasks, such\nas reconstructing the input sequence, or predicting the future sequence. We\nexperiment with two kinds of input sequences - patches of image pixels and\nhigh-level representations (\"percepts\") of video frames extracted using a\npretrained convolutional net. We explore different design choices such as\nwhether the decoder LSTMs should condition on the generated output. We analyze\nthe outputs of the model qualitatively to see how well the model can\nextrapolate the learned video representation into the future and into the past.\nWe try to visualize and interpret the learned features. We stress test the\nmodel by running it on longer time scales and on out-of-domain data. We further\nevaluate the representations by finetuning them for a supervised learning\nproblem - human action recognition on the UCF-101 and HMDB-51 datasets. We show\nthat the representations help improve classification accuracy, especially when\nthere are only a few training examples. Even models pretrained on unrelated\ndatasets (300 hours of YouTube videos) can help action recognition performance. \n\n"}
{"id": "1502.05786", "contents": "Title: Randomized Assignment of Jobs to Servers in Heterogeneous Clusters of\n  Shared Servers for Low Delay Abstract: We consider the job assignment problem in a multi-server system consisting of\n$N$ parallel processor sharing servers, categorized into $M$ ($\\ll N$)\ndifferent types according to their processing capacity or speed. Jobs of random\nsizes arrive at the system according to a Poisson process with rate $N\n\\lambda$. Upon each arrival, a small number of servers from each type is\nsampled uniformly at random. The job is then assigned to one of the sampled\nservers based on a selection rule. We propose two schemes, each corresponding\nto a specific selection rule that aims at reducing the mean sojourn time of\njobs in the system.\n  We first show that both methods achieve the maximal stability region. We then\nanalyze the system operating under the proposed schemes as $N \\to \\infty$ which\ncorresponds to the mean field. Our results show that asymptotic independence\namong servers holds even when $M$ is finite and exchangeability holds only\nwithin servers of the same type. We further establish the existence and\nuniqueness of stationary solution of the mean field and show that the tail\ndistribution of server occupancy decays doubly exponentially for each server\ntype. When the estimates of arrival rates are not available, the proposed\nschemes offer simpler alternatives to achieving lower mean sojourn time of\njobs, as shown by our numerical studies. \n\n"}
{"id": "1502.06464", "contents": "Title: Rectified Factor Networks Abstract: We propose rectified factor networks (RFNs) to efficiently construct very\nsparse, non-linear, high-dimensional representations of the input. RFN models\nidentify rare and small events in the input, have a low interference between\ncode units, have a small reconstruction error, and explain the data covariance\nstructure. RFN learning is a generalized alternating minimization algorithm\nderived from the posterior regularization method which enforces non-negative\nand normalized posterior means. We proof convergence and correctness of the RFN\nlearning algorithm. On benchmarks, RFNs are compared to other unsupervised\nmethods like autoencoders, RBMs, factor analysis, ICA, and PCA. In contrast to\nprevious sparse coding methods, RFNs yield sparser codes, capture the data's\ncovariance structure more precisely, and have a significantly smaller\nreconstruction error. We test RFNs as pretraining technique for deep networks\non different vision datasets, where RFNs were superior to RBMs and\nautoencoders. On gene expression data from two pharmaceutical drug discovery\nstudies, RFNs detected small and rare gene modules that revealed highly\nrelevant new biological insights which were so far missed by other unsupervised\nmethods. \n\n"}
{"id": "1503.00321", "contents": "Title: A Sampling Technique of Proving Lower Bounds for Noisy Computations Abstract: We present a technique of proving lower bounds for noisy computations. This\nis achieved by a theorem connecting computations on a kind of randomized\ndecision trees and sampling based algorithms. This approach is surprisingly\npowerful, and applicable to several models of computation previously studied.\n  As a first illustration we show how all the results of Evans and Pippenger\n(SIAM J. Computing, 1999) for noisy decision trees, some of which were derived\nusing Fourier analysis, follow immediately if we consider the sampling-based\nalgorithms that naturally arise from these decision trees.\n  Next, we show a tight lower bound of $\\Omega(N \\log\\log N)$ on the number of\ntransmissions required to compute several functions (including the parity\nfunction and the majority function) in a network of $N$ randomly placed\nsensors, communicating using local transmissions, and operating with power near\nthe connectivity threshold. This result considerably simplifies and strengthens\nan earlier result of Dutta, Kanoria Manjunath and Radhakrishnan (SODA 08) that\nsuch networks cannot compute the parity function reliably with significantly\nfewer than $N\\log \\log N$ transmissions. The lower bound for parity shown\nearlier made use of special properties of the parity function and is\ninapplicable, e.g., to the majority function. In this paper, we use our\napproach to develop an interesting connection between computation of boolean\nfunctions on noisy networks that make few transmissionss, and algorithms that\nwork by sampling only a part of the input. It is straightforward to verify that\nsuch sampling-based algorithms cannot compute the majority function. \n\n"}
{"id": "1503.04422", "contents": "Title: Making Availability as a Service in the Clouds Abstract: Cloud computing has achieved great success in modern IT industry as an\nexcellent computing paradigm due to its flexible management and elastic\nresource sharing. To date, cloud computing takes an irrepalceable position in\nour socioeconomic system and influences almost every aspect of our daily life.\nHowever, it is still in its infancy, many problems still exist.Besides the\nhotly-debated security problem, availability is also an urgent issue.With the\nlimited power of availability mechanisms provided in present cloud platform, we\ncan hardly get detailed availability information of current applications such\nas the root causes of availability problem,mean time to failure, etc. Thus a\nnew mechanism based on deep avaliability analysis is neccessary and\nbenificial.Following the prevalent terminology 'XaaS',this paper proposes a new\nwin-win concept for cloud users and providers in term of 'Availability as a\nService' (abbreviated as 'AaaS').The aim of 'AaaS' is to provide comprehensive\nand aimspecific runtime avaliabilty analysis services for cloud users by\nintegrating plent of data-driven and modeldriven approaches. To illustrate this\nconcept, we realize a prototype named 'EagleEye' with all features of 'AaaS'.\nBy subscribing corresponding services in 'EagleEye', cloud users could get\nspecific availability information of their applications deployed in cloud\nplatform. We envision this new kind of service will be merged into the cloud\nmanagement mechanism in the near future. \n\n"}
{"id": "1503.04596", "contents": "Title: Enhanced Image Classification With a Fast-Learning Shallow Convolutional\n  Neural Network Abstract: We present a neural network architecture and training method designed to\nenable very rapid training and low implementation complexity. Due to its\ntraining speed and very few tunable parameters, the method has strong potential\nfor applications requiring frequent retraining or online training. The approach\nis characterized by (a) convolutional filters based on biologically inspired\nvisual processing filters, (b) randomly-valued classifier-stage input weights,\n(c) use of least squares regression to train the classifier output weights in a\nsingle batch, and (d) linear classifier-stage output units. We demonstrate the\nefficacy of the method by applying it to image classification. Our results\nmatch existing state-of-the-art results on the MNIST (0.37% error) and\nNORB-small (2.2% error) image classification databases, but with very fast\ntraining times compared to standard deep network approaches. The network's\nperformance on the Google Street View House Number (SVHN) (4% error) database\nis also competitive with state-of-the art methods. \n\n"}
{"id": "1503.04963", "contents": "Title: Algebraic Methods in the Congested Clique Abstract: In this work, we use algebraic methods for studying distance computation and\nsubgraph detection tasks in the congested clique model. Specifically, we adapt\nparallel matrix multiplication implementations to the congested clique,\nobtaining an $O(n^{1-2/\\omega})$ round matrix multiplication algorithm, where\n$\\omega < 2.3728639$ is the exponent of matrix multiplication. In conjunction\nwith known techniques from centralised algorithmics, this gives significant\nimprovements over previous best upper bounds in the congested clique model. The\nhighlight results include:\n  -- triangle and 4-cycle counting in $O(n^{0.158})$ rounds, improving upon the\n$O(n^{1/3})$ triangle detection algorithm of Dolev et al. [DISC 2012],\n  -- a $(1 + o(1))$-approximation of all-pairs shortest paths in $O(n^{0.158})$\nrounds, improving upon the $\\tilde{O} (n^{1/2})$-round $(2 +\no(1))$-approximation algorithm of Nanongkai [STOC 2014], and\n  -- computing the girth in $O(n^{0.158})$ rounds, which is the first\nnon-trivial solution in this model.\n  In addition, we present a novel constant-round combinatorial algorithm for\ndetecting 4-cycles. \n\n"}
{"id": "1503.05032", "contents": "Title: CSR5: An Efficient Storage Format for Cross-Platform Sparse\n  Matrix-Vector Multiplication Abstract: Sparse matrix-vector multiplication (SpMV) is a fundamental building block\nfor numerous applications. In this paper, we propose CSR5 (Compressed Sparse\nRow 5), a new storage format, which offers high-throughput SpMV on various\nplatforms including CPUs, GPUs and Xeon Phi. First, the CSR5 format is\ninsensitive to the sparsity structure of the input matrix. Thus the single\nformat can support an SpMV algorithm that is efficient both for regular\nmatrices and for irregular matrices. Furthermore, we show that the overhead of\nthe format conversion from the CSR to the CSR5 can be as low as the cost of a\nfew SpMV operations. We compare the CSR5-based SpMV algorithm with 11\nstate-of-the-art formats and algorithms on four mainstream processors using 14\nregular and 10 irregular matrices as a benchmark suite. For the 14 regular\nmatrices in the suite, we achieve comparable or better performance over the\nprevious work. For the 10 irregular matrices, the CSR5 obtains average\nperformance improvement of 17.6\\%, 28.5\\%, 173.0\\% and 293.3\\% (up to 213.3\\%,\n153.6\\%, 405.1\\% and 943.3\\%) over the best existing work on dual-socket Intel\nCPUs, an nVidia GPU, an AMD GPU and an Intel Xeon Phi, respectively. For\nreal-world applications such as a solver with only tens of iterations, the CSR5\nformat can be more practical because of its low-overhead for format conversion.\nThe source code of this work is downloadable at\nhttps://github.com/bhSPARSE/Benchmark_SpMV_using_CSR5 \n\n"}
{"id": "1503.06424", "contents": "Title: Modeling browser-based distributed evolutionary computation systems Abstract: From the era of big science we are back to the \"do it yourself\", where you do\nnot have any money to buy clusters or subscribe to grids but still have\nalgorithms that crave many computing nodes and need them to measure\nscalability. Fortunately, this coincides with the era of big data, cloud\ncomputing, and browsers that include JavaScript virtual machines. Those are the\nreasons why this paper will focus on two different aspects of volunteer or\nfreeriding computing: first, the pragmatic: where to find those resources,\nwhich ones can be used, what kind of support you have to give them; and then,\nthe theoretical: how evolutionary algorithms can be adapted to an environment\nin which nodes come and go, have different computing capabilities and operate\nin complete asynchrony of each other. We will examine the setup needed to\ncreate a very simple distributed evolutionary algorithm using JavaScript and\nthen find a model of how users react to it by collecting data from several\nexperiments featuring different classical benchmark functions. \n\n"}
{"id": "1503.06974", "contents": "Title: Challenges and Recommendations for Preparing HPC Applications for\n  Exascale Abstract: While the HPC community is working towards the development of the first\nExaflop computer (expected around 2020), after reaching the Petaflop milestone\nin 2008 still only few HPC applications are able to fully exploit the\ncapabilities of Petaflop systems. In this paper we argue that efforts for\npreparing HPC applications for Exascale should start before such systems become\navailable. We identify challenges that need to be addressed and recommend\nsolutions in key areas of interest, including formal modeling, static analysis\nand optimization, runtime analysis and optimization, and autonomic computing.\nFurthermore, we outline a conceptual framework for porting HPC applications to\nfuture Exascale computing systems and propose steps for its implementation. \n\n"}
{"id": "1504.00941", "contents": "Title: A Simple Way to Initialize Recurrent Networks of Rectified Linear Units Abstract: Learning long term dependencies in recurrent networks is difficult due to\nvanishing and exploding gradients. To overcome this difficulty, researchers\nhave developed sophisticated optimization techniques and network architectures.\nIn this paper, we propose a simpler solution that use recurrent neural networks\ncomposed of rectified linear units. Key to our solution is the use of the\nidentity matrix or its scaled version to initialize the recurrent weight\nmatrix. We find that our solution is comparable to LSTM on our four benchmarks:\ntwo toy problems involving long-range temporal structures, a large language\nmodeling problem and a benchmark speech recognition problem. \n\n"}
{"id": "1504.01130", "contents": "Title: Proving the Herman-Protocol Conjecture Abstract: Herman's self-stabilisation algorithm, introduced 25 years ago, is a\nwell-studied synchronous randomised protocol for enabling a ring of $N$\nprocesses collectively holding any odd number of tokens to reach a stable state\nin which a single token remains. Determining the worst-case expected time to\nstabilisation is the central outstanding open problem about this protocol. It\nis known that there is a constant $h$ such that any initial configuration has\nexpected stabilisation time at most $h N^2$. Ten years ago, McIver and Morgan\nestablished a lower bound of $4/27 \\approx 0.148$ for $h$, achieved with three\nequally-spaced tokens, and conjectured this to be the optimal value of $h$. A\nseries of papers over the last decade gradually reduced the upper bound on $h$,\nwith the present record (achieved in 2014) standing at approximately $0.156$.\nIn this paper, we prove McIver and Morgan's conjecture and establish that $h =\n4/27$ is indeed optimal. \n\n"}
{"id": "1504.01352", "contents": "Title: Multi-Broadcasting under the SINR Model Abstract: We study the multi-broadcast problem in multi-hop wireless networks under the\nSINR model deployed in the 2D Euclidean plane. In multi-broadcast, there are\n$k$ initial rumours, potentially belonging to different nodes, that must be\nforwarded to all $n$ nodes of the network. Furthermore, in each round a node\ncan only transmit a small message that could contain at most one initial rumor\nand $O(\\log n)$ control bits. In order to be successfully delivered to a node,\ntransmissions must satisfy the (Signal-to-Inference-and-Noise-Ratio) SINR\ncondition and have sufficiently strong signal at the receiver. We present\ndeterministic algorithms for multi-broadcast for different settings that\nreflect the different types of knowledge about the topology of the network\navailable to the nodes: (i) the whole network topology (ii) their own\ncoordinates and coordinates of their neighbors (iii) only their own\ncoordinates, and (iv) only their own ids and the ids of their neighbors. For\nthe former two settings, we present solutions that are scalable with respect to\nthe diameter of the network and the polylogarithm of the network size, i.e.,\n$\\log^c n$ for some constant $c> 0$, while the solutions for the latter two\nhave round complexity that is superlinear in the number of nodes. The last\nresult is of special significance, as it is the first result for the SINR model\nthat does not require nodes to know their coordinates in the plane (a very\nspecialized type of knowledge), but intricately exploits the understanding that\nnodes are implanted in the 2D Euclidean plane. \n\n"}
{"id": "1504.01575", "contents": "Title: Bidirectional Recurrent Neural Networks as Generative Models -\n  Reconstructing Gaps in Time Series Abstract: Bidirectional recurrent neural networks (RNN) are trained to predict both in\nthe positive and negative time directions simultaneously. They have not been\nused commonly in unsupervised tasks, because a probabilistic interpretation of\nthe model has been difficult. Recently, two different frameworks, GSN and NADE,\nprovide a connection between reconstruction and probabilistic modeling, which\nmakes the interpretation possible. As far as we know, neither GSN or NADE have\nbeen studied in the context of time series before. As an example of an\nunsupervised task, we study the problem of filling in gaps in high-dimensional\ntime series with complex dynamics. Although unidirectional RNNs have recently\nbeen trained successfully to model such time series, inference in the negative\ntime direction is non-trivial. We propose two probabilistic interpretations of\nbidirectional RNNs that can be used to reconstruct missing gaps efficiently.\nOur experiments on text data show that both proposed methods are much more\naccurate than unidirectional reconstructions, although a bit less accurate than\na computationally complex bidirectional Bayesian inference on the\nunidirectional RNN. We also provide results on music data for which the\nBayesian inference is computationally infeasible, demonstrating the scalability\nof the proposed methods. \n\n"}
{"id": "1504.03274", "contents": "Title: Distributed Stochastic Market Clearing with High-Penetration Wind Power Abstract: Integrating renewable energy into the modern power grid requires\nrisk-cognizant dispatch of resources to account for the stochastic availability\nof renewables. Toward this goal, day-ahead stochastic market clearing with\nhigh-penetration wind energy is pursued in this paper based on the DC optimal\npower flow (OPF). The objective is to minimize the social cost which consists\nof conventional generation costs, end-user disutility, as well as a risk\nmeasure of the system re-dispatching cost. Capitalizing on the conditional\nvalue-at-risk (CVaR), the novel model is able to mitigate the potentially high\nrisk of the recourse actions to compensate wind forecast errors. The resulting\nconvex optimization task is tackled via a distribution-free sample average\nbased approximation to bypass the prohibitively complex high-dimensional\nintegration. Furthermore, to cope with possibly large-scale dispatchable loads,\na fast distributed solver is developed with guaranteed convergence using the\nalternating direction method of multipliers (ADMM). Numerical results tested on\na modified benchmark system are reported to corroborate the merits of the novel\nframework and proposed approaches. \n\n"}
{"id": "1504.05046", "contents": "Title: Task-Based Algorithm for Matrix Multiplication: A Step Towards\n  Block-Sparse Tensor Computing Abstract: Distributed-memory matrix multiplication (MM) is a key element of algorithms\nin many domains (machine learning, quantum physics). Conventional algorithms\nfor dense MM rely on regular/uniform data decomposition to ensure load balance.\nThese traits conflict with the irregular structure (block-sparse or rank-sparse\nwithin blocks) that is increasingly relevant for fast methods in quantum\nphysics. To deal with such irregular data we present a new MM algorithm based\non Scalable Universal Matrix Multiplication Algorithm (SUMMA). The novel\nfeatures are: (1) multiple-issue scheduling of SUMMA iterations, and (2)\nfine-grained task-based formulation. The latter eliminates the need for\nexplicit internodal synchronization; with multiple-iteration scheduling this\nallows load imbalance due to nonuniform matrix structure. For square MM with\nuniform and nonuniform block sizes (the latter simulates matrices with general\nirregular structure) we found excellent performance in weak and strong-scaling\nregimes, on commodity and high-end hardware. \n\n"}
{"id": "1504.06963", "contents": "Title: Generalized solution for the Herman Protocol Conjecture Abstract: The Herman Protocol Conjecture states that the expected time\n$\\mathbb{E}(\\mathbf{T})$ of Herman's self-stabilizing algorithm in a system\nconsisting of $N$ identical processes organized in a ring holding several\ntokens is at most $\\frac{4}{27}N^{2}$. We prove the conjecture in its standard\nunbiased and also in a biased form for discrete processes, and extend the\nresult to further variants where the tokens move via certain L\\'evy processes.\nMoreover, we derive a bound on the expected value of\n$\\mathbb{E}(\\alpha^{\\mathbf{T}})$ for all $1\\leq \\alpha\\leq\n(1-\\varepsilon)^{-1}$ with a specific $\\varepsilon>0$. Subject to the\ncorrectness of an optimization result that can be demonstrated empirically, all\nthese estimations attain their maximum on the initial state with three tokens\ndistributed equidistantly on the ring of $N$ processes. Such a relation is the\nsymptom of the fact that both $\\mathbb{E}(\\mathbf{T})$ and\n$\\mathbb{E}(\\alpha^{\\mathbf{T}})$ are weighted sums of the probabilities\n$\\mathbb{P}(\\mathbf{T}\\geq t)$. \n\n"}
{"id": "1504.07395", "contents": "Title: Lexical Translation Model Using a Deep Neural Network Architecture Abstract: In this paper we combine the advantages of a model using global source\nsentence contexts, the Discriminative Word Lexicon, and neural networks. By\nusing deep neural networks instead of the linear maximum entropy model in the\nDiscriminative Word Lexicon models, we are able to leverage dependencies\nbetween different source words due to the non-linearity. Furthermore, the\nmodels for different target words can share parameters and therefore data\nsparsity problems are effectively reduced.\n  By using this approach in a state-of-the-art translation system, we can\nimprove the performance by up to 0.5 BLEU points for three different language\npairs on the TED translation task. \n\n"}
{"id": "1505.00384", "contents": "Title: Making Sense of Hidden Layer Information in Deep Networks by Learning\n  Hierarchical Targets Abstract: This paper proposes an architecture for deep neural networks with hidden\nlayer branches that learn targets of lower hierarchy than final layer targets.\nThe branches provide a channel for enforcing useful information in hidden layer\nwhich helps in attaining better accuracy, both for the final layer and hidden\nlayers. The shared layers modify their weights using the gradients of all cost\nfunctions higher than the branching layer. This model provides a flexible\ninference system with many levels of targets which is modular and can be used\nefficiently in situations requiring different levels of results according to\ncomplexity. This paper applies the idea to a text classification task on 20\nNewsgroups data set with two level of hierarchical targets and a comparison is\nmade with training without the use of hidden layer branches. \n\n"}
{"id": "1505.03703", "contents": "Title: A PCA-Based Convolutional Network Abstract: In this paper, we propose a novel unsupervised deep learning model, called\nPCA-based Convolutional Network (PCN). The architecture of PCN is composed of\nseveral feature extraction stages and a nonlinear output stage. Particularly,\neach feature extraction stage includes two layers: a convolutional layer and a\nfeature pooling layer. In the convolutional layer, the filter banks are simply\nlearned by PCA. In the nonlinear output stage, binary hashing is applied. For\nthe higher convolutional layers, the filter banks are learned from the feature\nmaps that were obtained in the previous stage. To test PCN, we conducted\nextensive experiments on some challenging tasks, including handwritten digits\nrecognition, face recognition and texture classification. The results show that\nPCN performs competitive with or even better than state-of-the-art deep\nlearning models. More importantly, since there is no back propagation for\nsupervised finetuning, PCN is much more efficient than existing deep networks. \n\n"}
{"id": "1505.03851", "contents": "Title: Using Butterfly-Patterned Partial Sums to Optimize GPU Memory Accesses\n  for Drawing from Discrete Distributions Abstract: We describe a technique for drawing values from discrete distributions, such\nas sampling from the random variables of a mixture model, that avoids computing\na complete table of partial sums of the relative probabilities. A table of\nalternate (\"butterfly-patterned\") form is faster to compute, making better use\nof coalesced memory accesses. From this table, complete partial sums are\ncomputed on the fly during a binary search. Measurements using an NVIDIA Titan\nBlack GPU show that for a sufficiently large number of clusters or topics (K >\n200), this technique alone more than doubles the speed of a latent Dirichlet\nallocation (LDA) application already highly tuned for GPU execution. \n\n"}
{"id": "1505.04134", "contents": "Title: An Interrupt-Driven Work-Sharing For-Loop Scheduler Abstract: In this paper we present a parallel for-loop scheduler which is based on\nwork-stealing principles but runs under a completely cooperative scheme. POSIX\nsignals are used by idle threads to interrupt left-behind workers, which in\nturn decide what portion of their workload can be given to the requester. We\ncall this scheme Interrupt-Driven Work-Sharing (IDWS). This article describes\nhow IDWS works, how it can be integrated into any POSIX-compliant OpenMP\nimplementation and how a user can manually replace OpenMP parallel for-loops\nwith IDWS in existing POSIX-compliant C++ applications. Additionally, we\nmeasure its performance using both a synthetic benchmark with varying\ndistributions of workload across the iteration space and a real-life\napplication on Sandy Bridge and Xeon Phi systems. Regardless the workload\ndistribution and the underlying hardware, IDWS is always the best or among the\nbest-performing strategies, providing a good all-around solution to the\nscheduling-choice dilemma. \n\n"}
{"id": "1505.04636", "contents": "Title: Graph Partitioning via Parallel Submodular Approximation to Accelerate\n  Distributed Machine Learning Abstract: Distributed computing excels at processing large scale data, but the\ncommunication cost for synchronizing the shared parameters may slow down the\noverall performance. Fortunately, the interactions between parameter and data\nin many problems are sparse, which admits efficient partition in order to\nreduce the communication overhead.\n  In this paper, we formulate data placement as a graph partitioning problem.\nWe propose a distributed partitioning algorithm. We give both theoretical\nguarantees and a highly efficient implementation. We also provide a highly\nefficient implementation of the algorithm and demonstrate its promising results\non both text datasets and social networks. We show that the proposed algorithm\nleads to 1.6x speedup of a state-of-the-start distributed machine learning\nsystem by eliminating 90\\% of the network communication. \n\n"}
{"id": "1505.04694", "contents": "Title: Thread Parallelism for Highly Irregular Computation in Anisotropic Mesh\n  Adaptation Abstract: Thread-level parallelism in irregular applications with mutable data\ndependencies presents challenges because the underlying data is extensively\nmodified during execution of the algorithm and a high degree of parallelism\nmust be realized while keeping the code race-free. In this article we describe\na methodology for exploiting thread parallelism for a class of graph-mutating\nworklist algorithms, which guarantees safe parallel execution via processing in\nrounds of independent sets and using a deferred update strategy to commit\nchanges in the underlying data structures. Scalability is assisted by atomic\nfetch-and-add operations to create worklists and work-stealing to balance the\nshared-memory workload. This work is motivated by mesh adaptation algorithms,\nfor which we show a parallel efficiency of 60% and 50% on Intel(R) Xeon(R)\nSandy Bridge and AMD Opteron(tm) Magny-Cours systems, respectively, using these\ntechniques. \n\n"}
{"id": "1506.00511", "contents": "Title: Predicting Deep Zero-Shot Convolutional Neural Networks using Textual\n  Descriptions Abstract: One of the main challenges in Zero-Shot Learning of visual categories is\ngathering semantic attributes to accompany images. Recent work has shown that\nlearning from textual descriptions, such as Wikipedia articles, avoids the\nproblem of having to explicitly define these attributes. We present a new model\nthat can classify unseen categories from their textual description.\nSpecifically, we use text features to predict the output weights of both the\nconvolutional and the fully connected layers in a deep convolutional neural\nnetwork (CNN). We take advantage of the architecture of CNNs and learn features\nat different layers, rather than just learning an embedding space for both\nmodalities, as is common with existing approaches. The proposed model also\nallows us to automatically generate a list of pseudo- attributes for each\nvisual category consisting of words from Wikipedia articles. We train our\nmodels end-to-end us- ing the Caltech-UCSD bird and flower datasets and\nevaluate both ROC and Precision-Recall curves. Our empirical results show that\nthe proposed model significantly outperforms previous methods. \n\n"}
{"id": "1506.00828", "contents": "Title: Rumor Spreading with Bounded In-Degree Abstract: In the classic gossip-based model of communication for disseminating\ninformation in a network, in each time unit, every node $u$ is allowed to\ncontact a single random neighbor $v$. If $u$ knows the data (rumor) to be\ndisseminated, it disperses it to $v$ (known as PUSH) and if it does not, it\nrequests it from $v$ (known as PULL). While in the classic gossip model, each\nnode is only allowed to contact a single neighbor in each time unit, each node\ncan possibly be contacted by many neighboring nodes.\n  In the present paper, we consider a restricted model where at each node only\none incoming request can be served. As long as only a single piece of\ninformation needs to be disseminated, this does not make a difference for push\nrequests. It however has a significant effect on pull requests. In the paper,\nwe therefore concentrate on this weaker pull version, which we call 'restricted\npull'.\n  We distinguish two versions of the restricted pull protocol depending on\nwhether the request to be served among a set of pull requests at a given node\nis chosen adversarially or uniformly at random. As a first result, we prove an\nexponential separation between the two variants. We show that there are\ninstances where if an adversary picks the request to be served, the restricted\npull protocol requires a polynomial number of rounds whereas if the winning\nrequest is chosen uniformly at random, the restricted pull protocol only\nrequires a polylogarithmic number of rounds to inform the whole network.\nFurther, as the main technical contribution, we show that if the request to be\nserved is chosen randomly, the slowdown of using restricted pull versus using\nthe classic pull protocol can w.h.p. be upper bounded by $O(\\Delta / \\delta\n\\log n)$, where $\\Delta$ and $\\delta$ are the largest and smallest degree of\nthe network. \n\n"}
{"id": "1506.02617", "contents": "Title: Path-SGD: Path-Normalized Optimization in Deep Neural Networks Abstract: We revisit the choice of SGD for training deep neural networks by\nreconsidering the appropriate geometry in which to optimize the weights. We\nargue for a geometry invariant to rescaling of weights that does not affect the\noutput of the network, and suggest Path-SGD, which is an approximate steepest\ndescent method with respect to a path-wise regularizer related to max-norm\nregularization. Path-SGD is easy and efficient to implement and leads to\nempirical gains over SGD and AdaGrad. \n\n"}
{"id": "1506.03134", "contents": "Title: Pointer Networks Abstract: We introduce a new neural architecture to learn the conditional probability\nof an output sequence with elements that are discrete tokens corresponding to\npositions in an input sequence. Such problems cannot be trivially addressed by\nexistent approaches such as sequence-to-sequence and Neural Turing Machines,\nbecause the number of target classes in each step of the output depends on the\nlength of the input, which is variable. Problems such as sorting variable sized\nsequences, and various combinatorial optimization problems belong to this\nclass. Our model solves the problem of variable size output dictionaries using\na recently proposed mechanism of neural attention. It differs from the previous\nattention attempts in that, instead of using attention to blend hidden units of\nan encoder to a context vector at each decoder step, it uses attention as a\npointer to select a member of the input sequence as the output. We call this\narchitecture a Pointer Net (Ptr-Net). We show Ptr-Nets can be used to learn\napproximate solutions to three challenging geometric problems -- finding planar\nconvex hulls, computing Delaunay triangulations, and the planar Travelling\nSalesman Problem -- using training examples alone. Ptr-Nets not only improve\nover sequence-to-sequence with input attention, but also allow us to generalize\nto variable size output dictionaries. We show that the learnt models generalize\nbeyond the maximum lengths they were trained on. We hope our results on these\ntasks will encourage a broader exploration of neural learning for discrete\nproblems. \n\n"}
{"id": "1506.04449", "contents": "Title: Compressing Convolutional Neural Networks Abstract: Convolutional neural networks (CNN) are increasingly used in many areas of\ncomputer vision. They are particularly attractive because of their ability to\n\"absorb\" great quantities of labeled data through millions of parameters.\nHowever, as model sizes increase, so do the storage and memory requirements of\nthe classifiers. We present a novel network architecture, Frequency-Sensitive\nHashed Nets (FreshNets), which exploits inherent redundancy in both\nconvolutional layers and fully-connected layers of a deep learning model,\nleading to dramatic savings in memory and storage consumption. Based on the key\nobservation that the weights of learned convolutional filters are typically\nsmooth and low-frequency, we first convert filter weights to the frequency\ndomain with a discrete cosine transform (DCT) and use a low-cost hash function\nto randomly group frequency parameters into hash buckets. All parameters\nassigned the same hash bucket share a single value learned with standard\nback-propagation. To further reduce model size we allocate fewer hash buckets\nto high-frequency components, which are generally less important. We evaluate\nFreshNets on eight data sets, and show that it leads to drastically better\ncompressed performance than several relevant baselines. \n\n"}
{"id": "1506.04681", "contents": "Title: Byzantine Multi-Agent Optimization: Part I Abstract: We study Byzantine fault-tolerant distributed optimization of a sum of convex\n(cost) functions with real-valued scalar input/ouput. In particular, the goal\nis to optimize a global cost function $\\frac{1}{|\\mathcal{N}|}\\sum_{i\\in\n\\mathcal{N}} h_i(x)$, where $\\mathcal{N}$ is the set of non-faulty agents, and\n$h_i(x)$ is agent $i$'s local cost function, which is initially known only to\nagent $i$. In general, when some of the agents may be Byzantine faulty, the\nabove goal is unachievable, because the identity of the faulty agents is not\nnecessarily known to the non-faulty agents, and the faulty agents may behave\narbitrarily. Since the above global cost function cannot be optimized exactly\nin presence of Byzantine agents, we define a weaker version of the problem.\n  The goal for the weaker problem is to generate an output that is an optimum\nof a function formed as a convex combination of local cost functions of the\nnon-faulty agents. More precisely, for some choice of weights $\\alpha_i$ for\n$i\\in \\mathcal{N}$ such that $\\alpha_i\\geq 0$ and $\\sum_{i\\in\n\\mathcal{N}}\\alpha_i=1$, the output must be an optimum of the cost function\n$\\sum_{i\\in \\mathcal{N}} \\alpha_ih_i(x)$. Ideally, we would like\n$\\alpha_i=\\frac{1}{|\\mathcal{N}|}$ for all $i\\in \\mathcal{N}$ -- however, this\ncannot be guaranteed due to the presence of faulty agents. In fact, we show\nthat the maximum achievable number of nonzero weights ($\\alpha_i$'s) is\n$|\\mathcal{N}|-f$, where $f$ is the upper bound on the number of Byzantine\nagents. In addition, we present algorithms that ensure that at least\n$|\\mathcal{N}|-f$ agents have weights that are bounded away from 0. We also\npropose a low-complexity suboptimal algorithm, which ensures that at least\n$\\lceil \\frac{n}{2}\\rceil-\\phi$ agents have weights that are bounded away from\n0, where $n$ is the total number of agents, and $\\phi$ ($\\phi\\le f$) is the\nactual number of Byzantine agents. \n\n"}
{"id": "1506.05163", "contents": "Title: Deep Convolutional Networks on Graph-Structured Data Abstract: Deep Learning's recent successes have mostly relied on Convolutional\nNetworks, which exploit fundamental statistical properties of images, sounds\nand video data: the local stationarity and multi-scale compositional structure,\nthat allows expressing long range interactions in terms of shorter, localized\ninteractions. However, there exist other important examples, such as text\ndocuments or bioinformatic data, that may lack some or all of these strong\nstatistical regularities.\n  In this paper we consider the general question of how to construct deep\narchitectures with small learning complexity on general non-Euclidean domains,\nwhich are typically unknown and need to be estimated from the data. In\nparticular, we develop an extension of Spectral Networks which incorporates a\nGraph Estimation procedure, that we test on large-scale classification\nproblems, matching or improving over Dropout Networks with far less parameters\nto estimate. \n\n"}
{"id": "1506.05348", "contents": "Title: The Pandora Software Development Kit for Pattern Recognition Abstract: The development of automated solutions to pattern recognition problems is\nimportant in many areas of scientific research and human endeavour. This paper\ndescribes the implementation of the Pandora Software Development Kit, which\naids the process of designing, implementing and running pattern recognition\nalgorithms. The Pandora Application Programming Interfaces ensure simple\nspecification of the building-blocks defining a pattern recognition problem.\nThe logic required to solve the problem is implemented in algorithms. The\nalgorithms request operations to create or modify data structures and the\noperations are performed by the Pandora framework. This design promotes an\napproach using many decoupled algorithms, each addressing specific topologies.\nDetails of algorithms addressing two pattern recognition problems in High\nEnergy Physics are presented: reconstruction of events at a high-energy e+e-\nlinear collider and reconstruction of cosmic ray or neutrino events in a liquid\nargon time projection chamber. \n\n"}
{"id": "1506.06194", "contents": "Title: Unstructured Overlapping Mesh Distribution in Parallel Abstract: We present a simple mathematical framework and API for parallel mesh and data\ndistribution, load balancing, and overlap generation. It relies on viewing the\nmesh as a Hasse diagram, abstracting away information such as cell shape,\ndimension, and coordinates. The high level of abstraction makes our interface\nboth concise and powerful, as the same algorithm applies to any representable\nmesh, such as hybrid meshes, meshes embedded in higher dimension, and\noverlapped meshes in parallel. We present evidence, both theoretical and\nexperimental, that the algorithms are scalable and efficient. A working\nimplementation can be found in the latest release of the PETSc libraries. \n\n"}
{"id": "1506.07952", "contents": "Title: Tradeoffs Between Cost and Information for Rendezvous and Treasure Hunt Abstract: In rendezvous, two agents traverse network edges in synchronous rounds and\nhave to meet at some node. In treasure hunt, a single agent has to find a\nstationary target situated at an unknown node of the network. We study\ntradeoffs between the amount of information ($\\mathit{advice}$) available\n$\\mathit{a\\ priori}$ to the agents and the cost (number of edge traversals) of\nrendezvous and treasure hunt. Our goal is to find the smallest size of advice\nwhich enables the agents to solve these tasks at some cost $C$ in a network\nwith $e$ edges. This size turns out to depend on the initial distance $D$ and\non the ratio $\\frac{e}{C}$, which is the $\\mathit{relative\\ cost\\ gain}$ due to\nadvice. For arbitrary graphs, we give upper and lower bounds of $O(D\\log(D\\cdot\n\\frac{e}{C}) +\\log\\log e)$ and $\\Omega(D\\log \\frac{e}{C})$, respectively, on\nthe optimal size of advice. For the class of trees, we give nearly tight upper\nand lower bounds of $O(D\\log \\frac{e}{C} + \\log\\log e)$ and $\\Omega (D\\log\n\\frac{e}{C})$, respectively. \n\n"}
{"id": "1506.08258", "contents": "Title: Trigger detection for adaptive scientific workflows using percentile\n  sampling Abstract: Increasing complexity of scientific simulations and HPC architectures are\ndriving the need for adaptive workflows, where the composition and execution of\ncomputational and data manipulation steps dynamically depend on the\nevolutionary state of the simulation itself. Consider for example, the\nfrequency of data storage. Critical phases of the simulation should be captured\nwith high frequency and with high fidelity for post-analysis, however we cannot\nafford to retain the same frequency for the full simulation due to the high\ncost of data movement. We can instead look for triggers, indicators that the\nsimulation will be entering a critical phase and adapt the workflow\naccordingly.\n  We present a method for detecting triggers and demonstrate its use in direct\nnumerical simulations of turbulent combustion using S3D. We show that chemical\nexplosive mode analysis (CEMA) can be used to devise a noise-tolerant indicator\nfor rapid increase in heat release. However, exhaustive computation of CEMA\nvalues dominates the total simulation, thus is prohibitively expensive. To\novercome this bottleneck, we propose a quantile-sampling approach. Our\nalgorithm comes with provable error/confidence bounds, as a function of the\nnumber of samples. Most importantly, the number of samples is independent of\nthe problem size, thus our proposed algorithm offers perfect scalability. Our\nexperiments on homogeneous charge compression ignition (HCCI) and reactivity\ncontrolled compression ignition (RCCI) simulations show that the proposed\nmethod can detect rapid increases in heat release, and its computational\noverhead is negligible. Our results will be used for dynamic workflow decisions\nabout data storage and mesh resolution in future combustion simulations.\nProposed framework is generalizable and we detail how it could be applied to a\nbroad class of scientific simulation workflows. \n\n"}
{"id": "1507.00073", "contents": "Title: Specifying Concurrent Problems: Beyond Linearizability Abstract: Tasks and objects are two predominant ways of specifying distributed\nproblems. A task is specified by an input/output relation, defining for each\nset of processes that may run concurrently, and each assignment of inputs to\nthe processes in the set, the valid outputs of the processes. An object is\nspecified by an automaton describing the outputs the object may produce when it\nis accessed sequentially. Thus, tasks explicitly state what may happen only\nwhen sets of processes run concurrently, while objects only specify what\nhappens when processes access the object sequentially. Each one requires its\nown implementation notion, to tell when an execution satisfies the\nspecification. For objects linearizability is commonly used, a very elegant and\nuseful consistency condition. For tasks implementation notions are less\nexplored.\n  The paper introduces the notion of interval-sequential object. The\ncorresponding implementation notion of interval-linearizability generalizes\nlinearizability, and allows to associate states along the interval of execution\nof an operation. Interval-linearizability allows to specify any task, however,\nthere are sequential one-shot objects that cannot be expressed as tasks, under\nthe simplest interpretation of a task. It also shows that a natural extension\nof the notion of a task is expressive enough to specify any interval-sequential\nobject. \n\n"}
{"id": "1507.00772", "contents": "Title: Optimal and Resilient Pheromone Utilization in Ant Foraging Abstract: Pheromones are a chemical substance produced and released by ants as means of\ncommunication. In this work we present the minimum amount of pheromones\nnecessary and sufficient for a colony of ants (identical mobile agents) to\ndeterministically find a food source (treasure), assuming that each ant has the\ncomputational capabilities of either a Finite State Machine (FSM) or a Turing\nMachine (TM). In addition, we provide pheromone-based foraging algorithms\ncapable of handling fail-stop faults.\n  In more detail, we consider the case where $k$ identical ants, initially\nlocated at the center (nest) of an infinite two-dimensional grid and\ncommunicate only through pheromones, perform a collaborative search for an\nadversarially hidden treasure placed at an unknown distance $D$. We begin by\nproving a tight lower bound of $\\Omega(D)$ on the amount of pheromones required\nby any number of FSM based ants to complete the search, and continue to reduce\nthe lower bound to $\\Omega(k)$ for the stronger ants modeled as TM. We provide\nalgorithms which match the aforementioned lower bounds, and still terminate in\noptimal $\\mathcal{O}(D + D^2 / k)$ time, under both the synchronous and\nasynchronous models. Furthermore, we consider a more realistic setting, where\nan unknown number $f < k$ of ants may fail-stop at any time; we provide\nfault-tolerant FSM algorithms (synchronous and asynchronous), that terminate in\n$\\mathcal{O}(D + D^2/(k-f) + Df)$ rounds and emit no more than the same\nasymptotic minimum number of $\\mathcal{O}(D)$ pheromones overall. \n\n"}
{"id": "1507.01391", "contents": "Title: Sorting and Permuting without Bank Conflicts on GPUs Abstract: In this paper, we look at the complexity of designing algorithms without any\nbank conflicts in the shared memory of Graphical Processing Units (GPUs). Given\ninput of size $n$, $w$ processors and $w$ memory banks, we study three\nfundamental problems: sorting, permuting and $w$-way partitioning (defined as\nsorting an input containing exactly $n/w$ copies of every integer in $[w]$).\n  We solve sorting in optimal $O(\\frac{n}{w} \\log n)$ time. When $n \\ge w^2$,\nwe solve the partitioning problem optimally in $O(n/w)$ time. We also present a\ngeneral solution for the partitioning problem which takes $O(\\frac{n}{w}\n\\log^3_{n/w} w)$ time. Finally, we solve the permutation problem using a\nrandomized algorithm in $O(\\frac{n}{w} \\log\\log\\log_{n/w} n)$ time. Our results\nshow evidence that when working with banked memory architectures, there is a\nseparation between these problems and the permutation and partitioning problems\nare not as easy as simple parallel scanning. \n\n"}
{"id": "1507.03162", "contents": "Title: Continuous Partial Quorums for Consistency-Latency Tuning in Distributed\n  NoSQL Storage Systems Abstract: NoSQL storage systems are used extensively by web applications and provide an\nattractive alternative to conventional databases when the need for scalability\noutweighs the need for transactions. Several of these systems provide\nquorum-based replication and present the application developer with a choice of\nmultiple client-side \"consistency levels\" that determine the number of replicas\naccessed by reads and writes, which in turn affects both latency and the\nconsistency observed by the client application. Since using a fixed combination\nof read and write consistency levels for a given application provides only a\nlimited number of discrete options, we investigate techniques that allow more\nfine-grained tuning of the consistency-latency trade-off, as may be required to\nsupport consistency-based service level agreements (SLAs). We propose a novel\ntechnique called \\emph{continuous partial quorums} (CPQ) that assigns the\nconsistency level on a per-operation basis by choosing randomly between two\noptions, such as eventual and strong consistency, with a tunable probability.\nWe evaluate our technique experimentally using Apache Cassandra and demonstrate\nthat it outperforms an alternative tuning technique that delays operations\nartificially. \n\n"}
{"id": "1507.04296", "contents": "Title: Massively Parallel Methods for Deep Reinforcement Learning Abstract: We present the first massively distributed architecture for deep\nreinforcement learning. This architecture uses four main components: parallel\nactors that generate new behaviour; parallel learners that are trained from\nstored experience; a distributed neural network to represent the value function\nor behaviour policy; and a distributed store of experience. We used our\narchitecture to implement the Deep Q-Network algorithm (DQN). Our distributed\nalgorithm was applied to 49 games from Atari 2600 games from the Arcade\nLearning Environment, using identical hyperparameters. Our performance\nsurpassed non-distributed DQN in 41 of the 49 games and also reduced the\nwall-time required to achieve these results by an order of magnitude on most\ngames. \n\n"}
{"id": "1507.04461", "contents": "Title: Assignment Problems of Different-Sized Inputs in MapReduce Abstract: A MapReduce algorithm can be described by a mapping schema, which assigns\ninputs to a set of reducers, such that for each required output there exists a\nreducer that receives all the inputs that participate in the computation of\nthis output. Reducers have a capacity, which limits the sets of inputs that\nthey can be assigned. However, individual inputs may vary in terms of size. We\nconsider, for the first time, mapping schemas where input sizes are part of the\nconsiderations and restrictions. One of the significant parameters to optimize\nin any MapReduce job is communication cost between the map and reduce phases.\nThe communication cost can be optimized by minimizing the number of copies of\ninputs sent to the reducers. The communication cost is closely related to the\nnumber of reducers of constrained capacity that are used to accommodate\nappropriately the inputs, so that the requirement of how the inputs must meet\nin a reducer is satisfied. In this work, we consider a family of problems where\nit is required that each input meets with each other input in at least one\nreducer. We also consider a slightly different family of problems in which,\neach input of a list, X, is required to meet each input of another list, Y, in\nat least one reducer. We prove that finding an optimal mapping schema for these\nfamilies of problems is NP-hard, and present a bin-packing-based approximation\nalgorithm for finding a near optimal mapping schema. \n\n"}
{"id": "1507.06222", "contents": "Title: STICK: Spike Time Interval Computational Kernel, A Framework for General\n  Purpose Computation using Neurons, Precise Timing, Delays, and Synchrony Abstract: There has been significant research over the past two decades in developing\nnew platforms for spiking neural computation. Current neural computers are\nprimarily developed to mimick biology. They use neural networks which can be\ntrained to perform specific tasks to mainly solve pattern recognition problems.\nThese machines can do more than simulate biology, they allow us to re-think our\ncurrent paradigm of computation. The ultimate goal is to develop brain inspired\ngeneral purpose computation architectures that can breach the current\nbottleneck introduced by the Von Neumann architecture. This work proposes a new\nframework for such a machine. We show that the use of neuron like units with\nprecise timing representation, synaptic diversity, and temporal delays allows\nus to set a complete, scalable compact computation framework. The presented\nframework provides both linear and non linear operations, allowing us to\nrepresent and solve any function. We show usability in solving real use cases\nfrom simple differential equations to sets of non-linear differential equations\nleading to chaotic attractors. \n\n"}
{"id": "1507.08492", "contents": "Title: Cost optimization of data flows based on task re-ordering Abstract: Analyzing big data in a highly dynamic environment becomes more and more\ncritical because of the increasingly need for end-to-end processing of this\ndata. Modern data flows are quite complex and there are not efficient,\ncost-based, fully-automated, scalable optimization solutions that can\nfacilitate flow designers. The state-of-the-art proposals fail to provide near\noptimal solutions even for simple data flows. To tackle this problem, we\nintroduce a set of approximate algorithms for defining the execution order of\nthe constituent tasks, in order to minimize the total execution cost of a data\nflow. We also present the advantages of the parallel execution of data flows.\nWe validated our proposals in both a real tool and synthetic flows and the\nresults show that we can achieve significant speed-ups, moving much closer to\noptimal solutions. \n\n"}
{"id": "1508.00851", "contents": "Title: Fast Consensus under Eventually Stabilizing Message Adversaries Abstract: This paper is devoted to deterministic consensus in synchronous dynamic\nnetworks with unidirectional links, which are under the control of an\nomniscient message adversary. Motivated by unpredictable node/system\ninitialization times and long-lasting periods of massive transient faults, we\nconsider message adversaries that guarantee periods of less erratic message\nloss only eventually: We present a tight bound of $2D+1$ for the termination\ntime of consensus under a message adversary that eventually guarantees a single\nvertex-stable root component with dynamic network diameter $D$, as well as a\nsimple algorithm that matches this bound. It effectively halves the termination\ntime $4D+1$ achieved by an existing consensus algorithm, which also works under\nour message adversary. We also introduce a generalized, considerably stronger\nvariant of our message adversary, and show that our new algorithm, unlike the\nexisting one, still works correctly under it. \n\n"}
{"id": "1508.01084", "contents": "Title: Deep Convolutional Networks are Hierarchical Kernel Machines Abstract: In i-theory a typical layer of a hierarchical architecture consists of HW\nmodules pooling the dot products of the inputs to the layer with the\ntransformations of a few templates under a group. Such layers include as\nspecial cases the convolutional layers of Deep Convolutional Networks (DCNs) as\nwell as the non-convolutional layers (when the group contains only the\nidentity). Rectifying nonlinearities -- which are used by present-day DCNs --\nare one of the several nonlinearities admitted by i-theory for the HW module.\nWe discuss here the equivalence between group averages of linear combinations\nof rectifying nonlinearities and an associated kernel. This property implies\nthat present-day DCNs can be exactly equivalent to a hierarchy of kernel\nmachines with pooling and non-pooling layers. Finally, we describe a conjecture\nfor theoretically understanding hierarchies of such modules. A main consequence\nof the conjecture is that hierarchies of trained HW modules minimize memory\nrequirements while computing a selective and invariant representation. \n\n"}
{"id": "1508.01171", "contents": "Title: Meta-MapReduce: A Technique for Reducing Communication in MapReduce\n  Computations Abstract: MapReduce has proven to be one of the most useful paradigms in the revolution\nof distributed computing, where cloud services and cluster computing become the\nstandard venue for computing. The federation of cloud and big data activities\nis the next challenge where MapReduce should be modified to avoid (big) data\nmigration across remote (cloud) sites. This is exactly our scope of research,\nwhere only the very essential data for obtaining the result is transmitted,\nreducing communication, processing and preserving data privacy as much as\npossible. In this work, we propose an algorithmic technique for MapReduce\nalgorithms, called Meta-MapReduce, that decreases the communication cost by\nallowing us to process and move metadata to clouds and from the map phase to\nreduce phase. In Meta-MapReduce, the reduce phase fetches only the required\ndata at required iterations, which in turn, assists in preserving the data\nprivacy. \n\n"}
{"id": "1508.02788", "contents": "Title: The Effects of Hyperparameters on SGD Training of Neural Networks Abstract: The performance of neural network classifiers is determined by a number of\nhyperparameters, including learning rate, batch size, and depth. A number of\nattempts have been made to explore these parameters in the literature, and at\ntimes, to develop methods for optimizing them. However, exploration of\nparameter spaces has often been limited. In this note, I report the results of\nlarge scale experiments exploring these different parameters and their\ninteractions. \n\n"}
{"id": "1508.03087", "contents": "Title: Providing High and Controllable Performance in Multicore Systems Through\n  Shared Resource Management Abstract: Multiple applications executing concurrently on a multicore system interfere\nwith each other at different shared resources such as main memory and shared\ncaches. Such inter-application interference, if uncontrolled, results in high\nsystem performance degradation and unpredictable application slowdowns. While\nprevious work has proposed application-aware memory scheduling as a solution to\nmitigate inter-application interference and improve system performance,\npreviously proposed memory scheduling techniques incur high hardware complexity\nand unfairly slowdown some applications. Furthermore, previously proposed\nmemory-interference mitigation techniques are not designed to precisely control\napplication performance.\n  This dissertation seeks to achieve high and controllable performance in\nmulticore systems by mitigating and quantifying the impact of shared resource\ninterference. First, towards mitigating memory interference and achieving high\nperformance, we propose the Blacklisting memory scheduler that achieves high\nperformance and fairness at low complexity. Next, towards quantifying the\nimpact of memory interference and achieving controllable performance in the\npresence of memory bandwidth interference, we propose the Memory Interference\ninduced Slowdown Estimation (MISE) model. We propose and demonstrate two use\ncases that can leverage MISE to provide soft performance guarantees and high\noverall performance/fairness. Finally, we seek to quantify the impact of shared\ncache interference on application slowdowns, in addition to memory bandwidth\ninterference. Towards this end, we propose the Application Slowdown Model\n(ASM). We propose and demonstrate several use cases of ASM that leverage it to\nprovide soft performance guarantees and improve performance and fairness. \n\n"}
{"id": "1508.04747", "contents": "Title: Near-Optimal Distributed Maximum Flow Abstract: We present a near-optimal distributed algorithm for $(1+o(1))$-approximation\nof single-commodity maximum flow in undirected weighted networks that runs in\n$(D+ \\sqrt{n})\\cdot n^{o(1)}$ communication rounds in the \\Congest model. Here,\n$n$ and $D$ denote the number of nodes and the network diameter, respectively.\nThis is the first improvement over the trivial bound of $O(n^2)$, and it nearly\nmatches the $\\tilde{\\Omega}(D+ \\sqrt{n})$ round complexity lower bound.\n  The development of the algorithm contains two results of independent\ninterest:\n  (i) A $(D+\\sqrt{n})\\cdot n^{o(1)}$-round distributed construction of a\nspanning tree of average stretch $n^{o(1)}$.\n  (ii) A $(D+\\sqrt{n})\\cdot n^{o(1)}$-round distributed construction of an\n$n^{o(1)}$-congestion approximator consisting of the cuts induced by $O(\\log\nn)$ virtual trees. The distributed representation of the cut approximator\nallows for evaluation in $(D+\\sqrt{n})\\cdot n^{o(1)}$ rounds.\n  All our algorithms make use of randomization and succeed with high\nprobability. \n\n"}
{"id": "1508.06460", "contents": "Title: Deterministic Broadcasting and Gossiping with Beeps Abstract: Broadcasting and gossiping are fundamental communication tasks in networks.\nIn broadcasting,one node of a network has a message that must be learned by all\nother nodes. In gossiping, every node has a (possibly different) message, and\nall messages must be learned by all nodes. We study these well-researched tasks\nin a very weak communication model, called the {\\em beeping model}.\nCommunication proceeds in synchronous rounds. In each round, a node can either\nlisten, i.e., stay silent, or beep, i.e., emit a signal. A node hears a beep in\na round, if it listens in this round and if one or more adjacent nodes beep in\nthis round. All nodes have different labels from the set $\\{0,\\dots , L-1\\}$.\n  Our aim is to provide fast deterministic algorithms for broadcasting and\ngossiping in the beeping model. Let $N$ be an upper bound on the size of the\nnetwork and $D$ its diameter. Let $m$ be the size of the message in\nbroadcasting, and $M$ an upper bound on the size of all input messages in\ngossiping. For the task of broadcasting we give an algorithm working in time\n$O(D+m)$ for arbitrary networks, which is optimal. For the task of gossiping we\ngive an algorithm working in time $O(N(M+D\\log L))$ for arbitrary networks.\n  At the time of writing this paper we were unaware of the paper: A. Czumaj, P.\nDavis, Communicating with Beeps, arxiv:1505.06107 [cs.DC] which contains the\nsame results for broadcasting and a stronger upper bound for gossiping in a\nslightly different model. \n\n"}
{"id": "1508.06615", "contents": "Title: Character-Aware Neural Language Models Abstract: We describe a simple neural language model that relies only on\ncharacter-level inputs. Predictions are still made at the word-level. Our model\nemploys a convolutional neural network (CNN) and a highway network over\ncharacters, whose output is given to a long short-term memory (LSTM) recurrent\nneural network language model (RNN-LM). On the English Penn Treebank the model\nis on par with the existing state-of-the-art despite having 60% fewer\nparameters. On languages with rich morphology (Arabic, Czech, French, German,\nSpanish, Russian), the model outperforms word-level/morpheme-level LSTM\nbaselines, again with fewer parameters. The results suggest that on many\nlanguages, character inputs are sufficient for language modeling. Analysis of\nword representations obtained from the character composition part of the model\nreveals that the model is able to encode, from characters only, both semantic\nand orthographic information. \n\n"}
{"id": "1508.06904", "contents": "Title: Rapid Exact Signal Scanning with Deep Convolutional Neural Networks Abstract: A rigorous formulation of the dynamics of a signal processing scheme aimed at\ndense signal scanning without any loss in accuracy is introduced and analyzed.\nRelated methods proposed in the recent past lack a satisfactory analysis of\nwhether they actually fulfill any exactness constraints. This is improved\nthrough an exact characterization of the requirements for a sound sliding\nwindow approach. The tools developed in this paper are especially beneficial if\nConvolutional Neural Networks are employed, but can also be used as a more\ngeneral framework to validate related approaches to signal scanning. The\nproposed theory helps to eliminate redundant computations and renders special\ncase treatment unnecessary, resulting in a dramatic boost in efficiency\nparticularly on massively parallel processors. This is demonstrated both\ntheoretically in a computational complexity analysis and empirically on modern\nparallel processors. \n\n"}
{"id": "1509.00838", "contents": "Title: What to talk about and how? Selective Generation using LSTMs with\n  Coarse-to-Fine Alignment Abstract: We propose an end-to-end, domain-independent neural encoder-aligner-decoder\nmodel for selective generation, i.e., the joint task of content selection and\nsurface realization. Our model first encodes a full set of over-determined\ndatabase event records via an LSTM-based recurrent neural network, then\nutilizes a novel coarse-to-fine aligner to identify the small subset of salient\nrecords to talk about, and finally employs a decoder to generate free-form\ndescriptions of the aligned, selected records. Our model achieves the best\nselection and generation results reported to-date (with 59% relative\nimprovement in generation) on the benchmark WeatherGov dataset, despite using\nno specialized features or linguistic resources. Using an improved k-nearest\nneighbor beam filter helps further. We also perform a series of ablations and\nvisualizations to elucidate the contributions of our key model components.\nLastly, we evaluate the generalizability of our model on the RoboCup dataset,\nand get results that are competitive with or better than the state-of-the-art,\ndespite being severely data-starved. \n\n"}
{"id": "1509.01149", "contents": "Title: Model Predictive Path Integral Control using Covariance Variable\n  Importance Sampling Abstract: In this paper we develop a Model Predictive Path Integral (MPPI) control\nalgorithm based on a generalized importance sampling scheme and perform\nparallel optimization via sampling using a Graphics Processing Unit (GPU). The\nproposed generalized importance sampling scheme allows for changes in the drift\nand diffusion terms of stochastic diffusion processes and plays a significant\nrole in the performance of the model predictive control algorithm. We compare\nthe proposed algorithm in simulation with a model predictive control version of\ndifferential dynamic programming. \n\n"}
{"id": "1509.01183", "contents": "Title: Parallel Knowledge Embedding with MapReduce on a Multi-core Processor Abstract: This article firstly attempts to explore parallel algorithms of learning\ndistributed representations for both entities and relations in large-scale\nknowledge repositories with {\\it MapReduce} programming model on a multi-core\nprocessor. We accelerate the training progress of a canonical knowledge\nembedding method, i.e. {\\it translating embedding} ({\\bf TransE}) model, by\ndividing a whole knowledge repository into several balanced subsets, and\nfeeding each subset into an individual core where local embeddings can\nconcurrently run updating during the {\\it Map} phase. However, it usually\nsuffers from inconsistent low-dimensional vector representations of the same\nkey, which are collected from different {\\it Map} workers, and further leads to\nconflicts when conducting {\\it Reduce} to merge the various vectors associated\nwith the same key. Therefore, we try several strategies to acquire the merged\nembeddings which may not only retain the performance of {\\it entity inference},\n{\\it relation prediction}, and even {\\it triplet classification} evaluated by\nthe single-thread {\\bf TransE} on several well-known knowledge bases such as\nFreebase and NELL, but also scale up the learning speed along with the number\nof cores within a processor. So far, the empirical studies show that we could\nachieve comparable results as the single-thread {\\bf TransE} performs by the\n{\\it stochastic gradient descend} (SGD) algorithm, as well as increase the\ntraining speed multiple times via adapting the {\\it batch gradient descend}\n(BGD) algorithm for {\\it MapReduce} paradigm. \n\n"}
{"id": "1509.01572", "contents": "Title: Time parallel gravitational collapse simulation Abstract: This article demonstrates the applicability of the parallel-in-time method\nParareal to the numerical solution of the Einstein gravity equations for the\nspherical collapse of a massless scalar field. To account for the shrinking of\nthe spatial domain in time, a tailored load balancing scheme is proposed and\ncompared to load balancing based on number of time steps alone. The performance\nof Parareal is studied for both the sub-critical and black hole case; our\nexperiments show that Parareal generates substantial speedup and, in the\nsuper-critical regime, can reproduce Choptuik's black hole mass scaling law. \n\n"}
{"id": "1509.02058", "contents": "Title: Revisiting Conventional Task Schedulers to Exploit Asymmetry in ARM\n  big.LITTLE Architectures for Dense Linear Algebra Abstract: Dealing with asymmetry in the architecture opens a plethora of questions from\nthe perspective of scheduling task-parallel applications, and there exist early\nattempts to address this problem via ad-hoc strategies embedded into a runtime\nframework. In this paper we take a different path, which consists in addressing\nthe complexity of the problem at the library level, via a few asymmetry-aware\nfundamental kernels, hiding the architecture heterogeneity from the task\nscheduler. For the specific domain of dense linear algebra, we show that this\nis not only possible but delivers much higher performance than a naive approach\nbased on an asymmetry-oblivious scheduler. Furthermore, this solution also\noutperforms an ad-hoc asymmetry-aware scheduler furnished with sophisticated\nscheduling techniques. \n\n"}
{"id": "1509.02140", "contents": "Title: A Faster Counting Protocol for Anonymous Dynamic Networks Abstract: We study the problem of counting the number of nodes in a slotted-time\ncommunication network, under the challenging assumption that nodes do not have\nidentifiers and the network topology changes frequently. That is, for each time\nslot links among nodes can change arbitrarily provided that the network is\nalways connected. Tolerating dynamic topologies is crucial in face of mobility\nand unreliable communication whereas, even if identifiers are available, it\nmight be convenient to ignore them in massive networks with changing topology.\nCounting is a fundamental task in distributed computing since knowing the size\nof the system often facilitates the design of solutions for more complex\nproblems. Currently, the best upper bound proved on the running time to compute\nthe exact network size is double-exponential. However, only linear complexity\nlower bounds are known, leaving open the question of whether efficient Counting\nprotocols for Anonymous Dynamic Networks exist or not. In this paper we make a\nsignificant step towards answering this question by presenting a distributed\nCounting protocol for Anonymous Dynamic Networks which has exponential time\ncomplexity. Our algorithm ensures that eventually every node knows the exact\nsize of the system and stops executing the algorithm. Previous Counting\nprotocols have either double-exponential time complexity, or they are\nexponential but do not terminate, or terminate but do not provide running-time\nguarantees, or guarantee only an exponential upper bound on the network size.\nOther protocols are heuristic and do not guarantee the correct count. \n\n"}
{"id": "1509.02256", "contents": "Title: Matrix Computations and Optimization in Apache Spark Abstract: We describe matrix computations available in the cluster programming\nframework, Apache Spark. Out of the box, Spark provides abstractions and\nimplementations for distributed matrices and optimization routines using these\nmatrices. When translating single-node algorithms to run on a distributed\ncluster, we observe that often a simple idea is enough: separating matrix\noperations from vector operations and shipping the matrix operations to be ran\non the cluster, while keeping vector operations local to the driver. In the\ncase of the Singular Value Decomposition, by taking this idea to an extreme, we\nare able to exploit the computational power of a cluster, while running code\nwritten decades ago for a single core. Another example is our Spark port of the\npopular TFOCS optimization package, originally built for MATLAB, which allows\nfor solving Linear programs as well as a variety of other convex programs. We\nconclude with a comprehensive set of benchmarks for hardware accelerated matrix\ncomputations from the JVM, which is interesting in its own right, as many\ncluster programming frameworks use the JVM. The contributions described in this\npaper are already merged into Apache Spark and available on Spark installations\nby default, and commercially supported by a slew of companies which provide\nfurther services. \n\n"}
{"id": "1509.04048", "contents": "Title: Multiversion Conflict Notion for Transactional Memory Systems Abstract: In recent years, Software Transactional Memory systems (STMs) have garnered\nsignificant interest as an elegant alternative for addressing concurrency\nissues in memory. STM systems take optimistic approach. Multiple transactions\nare allowed to execute concurrently. On completion, each transaction is\nvalidated and if any inconsistency is observed it is aborted. Otherwise it is\nallowed to commit.\n  In databases a class of histories called as conflict-serializability (CSR)\nbased on the notion of conflicts have been identified, whose membership can be\nefficiently verified. As a result, CSR is the commonly used correctness\ncriterion in databases. Similarly, using the notion of conflicts, a correctness\ncriterion, conflict-opacity (co-opacity) which is a sub-class of can be\ndesigned whose membership can be verified in polynomial time. Using the\nverification mechanism, an efficient STM implementation can be designed that is\npermissive w.r.t co-opacity.\n  By storing multiple versions for each transaction object, multi-version STMs\nprovide more concurrency than single-version STMs. But the main drawback of\nco-opacity is that it does not admit histories that are uses multiple versions.\nThis has motivated us to develop a new conflict notions for multi-version STMs.\nIn this paper, we present a new conflict notion multi-version conflict. Using\nthis conflict notion, we identify a new subclass of opacity (a popular\ncorrectness-criterion), mvc-opacity that admits multi-versioned histories and\nwhose membership can be verified in polynomial time. We show that co-opacity is\na proper subset of this class. The proposed conflict notion mv-conflict can be\napplied on non-sequential histories as well unlike traditional conflicts.\nFurther, we believe that this conflict notion can be easily extended to other\ncorrectness-criterion as well. \n\n"}
{"id": "1509.04210", "contents": "Title: Model Accuracy and Runtime Tradeoff in Distributed Deep Learning:A\n  Systematic Study Abstract: This paper presents Rudra, a parameter server based distributed computing\nframework tuned for training large-scale deep neural networks. Using variants\nof the asynchronous stochastic gradient descent algorithm we study the impact\nof synchronization protocol, stale gradient updates, minibatch size, learning\nrates, and number of learners on runtime performance and model accuracy. We\nintroduce a new learning rate modulation strategy to counter the effect of\nstale gradients and propose a new synchronization protocol that can effectively\nbound the staleness in gradients, improve runtime performance and achieve good\nmodel accuracy. Our empirical investigation reveals a principled approach for\ndistributed training of neural networks: the mini-batch size per learner should\nbe reduced as more learners are added to the system to preserve the model\naccuracy. We validate this approach using commonly-used image classification\nbenchmarks: CIFAR10 and ImageNet. \n\n"}
{"id": "1509.08745", "contents": "Title: Compression of Deep Neural Networks on the Fly Abstract: Thanks to their state-of-the-art performance, deep neural networks are\nincreasingly used for object recognition. To achieve these results, they use\nmillions of parameters to be trained. However, when targeting embedded\napplications the size of these models becomes problematic. As a consequence,\ntheir usage on smartphones or other resource limited devices is prohibited. In\nthis paper we introduce a novel compression method for deep neural networks\nthat is performed during the learning phase. It consists in adding an extra\nregularization term to the cost function of fully-connected layers. We combine\nthis method with Product Quantization (PQ) of the trained weights for higher\nsavings in storage consumption. We evaluate our method on two data sets (MNIST\nand CIFAR10), on which we achieve significantly larger compression rates than\nstate-of-the-art methods. \n\n"}
{"id": "1509.08967", "contents": "Title: Very Deep Multilingual Convolutional Neural Networks for LVCSR Abstract: Convolutional neural networks (CNNs) are a standard component of many current\nstate-of-the-art Large Vocabulary Continuous Speech Recognition (LVCSR)\nsystems. However, CNNs in LVCSR have not kept pace with recent advances in\nother domains where deeper neural networks provide superior performance. In\nthis paper we propose a number of architectural advances in CNNs for LVCSR.\nFirst, we introduce a very deep convolutional network architecture with up to\n14 weight layers. There are multiple convolutional layers before each pooling\nlayer, with small 3x3 kernels, inspired by the VGG Imagenet 2014 architecture.\nThen, we introduce multilingual CNNs with multiple untied layers. Finally, we\nintroduce multi-scale input features aimed at exploiting more context at\nnegligible computational cost. We evaluate the improvements first on a Babel\ntask for low resource speech recognition, obtaining an absolute 5.77% WER\nimprovement over the baseline PLP DNN by training our CNN on the combined data\nof six different languages. We then evaluate the very deep CNNs on the Hub5'00\nbenchmark (using the 262 hours of SWB-1 training data) achieving a word error\nrate of 11.8% after cross-entropy training, a 1.4% WER improvement (10.6%\nrelative) over the best published CNN result so far. \n\n"}
{"id": "1510.02855", "contents": "Title: AtomNet: A Deep Convolutional Neural Network for Bioactivity Prediction\n  in Structure-based Drug Discovery Abstract: Deep convolutional neural networks comprise a subclass of deep neural\nnetworks (DNN) with a constrained architecture that leverages the spatial and\ntemporal structure of the domain they model. Convolutional networks achieve the\nbest predictive performance in areas such as speech and image recognition by\nhierarchically composing simple local features into complex models. Although\nDNNs have been used in drug discovery for QSAR and ligand-based bioactivity\npredictions, none of these models have benefited from this powerful\nconvolutional architecture. This paper introduces AtomNet, the first\nstructure-based, deep convolutional neural network designed to predict the\nbioactivity of small molecules for drug discovery applications. We demonstrate\nhow to apply the convolutional concepts of feature locality and hierarchical\ncomposition to the modeling of bioactivity and chemical interactions. In\nfurther contrast to existing DNN techniques, we show that AtomNet's application\nof local convolutional filters to structural target information successfully\npredicts new active molecules for targets with no previously known modulators.\nFinally, we show that AtomNet outperforms previous docking approaches on a\ndiverse set of benchmarks by a large margin, achieving an AUC greater than 0.9\non 57.8% of the targets in the DUDE benchmark. \n\n"}
{"id": "1510.03009", "contents": "Title: Neural Networks with Few Multiplications Abstract: For most deep learning algorithms training is notoriously time consuming.\nSince most of the computation in training neural networks is typically spent on\nfloating point multiplications, we investigate an approach to training that\neliminates the need for most of these. Our method consists of two parts: First\nwe stochastically binarize weights to convert multiplications involved in\ncomputing hidden states to sign changes. Second, while back-propagating error\nderivatives, in addition to binarizing the weights, we quantize the\nrepresentations at each layer to convert the remaining multiplications into\nbinary shifts. Experimental results across 3 popular datasets (MNIST, CIFAR10,\nSVHN) show that this approach not only does not hurt classification performance\nbut can result in even better performance than standard stochastic gradient\ndescent training, paving the way to fast, hardware-friendly training of neural\nnetworks. \n\n"}
{"id": "1510.04233", "contents": "Title: Arabesque: A System for Distributed Graph Mining - Extended version Abstract: Distributed data processing platforms such as MapReduce and Pregel have\nsubstantially simplified the design and deployment of certain classes of\ndistributed graph analytics algorithms. However, these platforms do not\nrepresent a good match for distributed graph mining problems, as for example\nfinding frequent subgraphs in a graph. Given an input graph, these problems\nrequire exploring a very large number of subgraphs and finding patterns that\nmatch some \"interestingness\" criteria desired by the user. These algorithms are\nvery important for areas such as social net- works, semantic web, and\nbioinformatics. In this paper, we present Arabesque, the first distributed data\nprocessing platform for implementing graph mining algorithms. Arabesque\nautomates the process of exploring a very large number of subgraphs. It defines\na high-level filter-process computational model that simplifies the development\nof scalable graph mining algorithms: Arabesque explores subgraphs and passes\nthem to the application, which must simply compute outputs and decide whether\nthe subgraph should be further extended. We use Arabesque's API to produce\ndistributed solutions to three fundamental graph mining problems: frequent\nsubgraph mining, counting motifs, and finding cliques. Our implementations\nrequire a handful of lines of code, scale to trillions of subgraphs, and\nrepresent in some cases the first available distributed solutions. \n\n"}
{"id": "1510.04995", "contents": "Title: Multi-dimensional intra-tile parallelization for memory-starved stencil\n  computations Abstract: Optimizing the performance of stencil algorithms has been the subject of\nintense research over the last two decades. Since many stencil schemes have low\narithmetic intensity, most optimizations focus on increasing the temporal data\naccess locality, thus reducing the data traffic through the main memory\ninterface with the ultimate goal of decoupling from this bottleneck. There are,\nhowever, only few approaches that explicitly leverage the shared cache feature\nof modern multicore chips. If every thread works on its private, separate cache\nblock, the available cache space can become too small, and sufficient temporal\nlocality may not be achieved.\n  We propose a flexible multi-dimensional intra-tile parallelization method for\nstencil algorithms on multicore CPUs with a shared outer-level cache. This\nmethod leads to a significant reduction in the required cache space without\nadverse effects from hardware prefetching or TLB shortage. Our \\emph{Girih}\nframework includes an auto-tuner to select optimal parameter configurations on\nthe target hardware. We conduct performance experiments on two contemporary\nIntel processors and compare with the state-of-the-art stencil frameworks PLUTO\nand Pochoir, using four corner-case stencil schemes and a wide range of problem\nsizes. \\emph{Girih} shows substantial performance advantages and best\narithmetic intensity at almost all problem sizes, especially on low-intensity\nstencils with variable coefficients. We study in detail the performance\nbehavior at varying grid size using phenomenological performance modeling. Our\nanalysis of energy consumption reveals that our method can save energy by\nreduced DRAM bandwidth usage even at marginal performance gain. It is thus well\nsuited for future architectures that will be strongly challenged by the cost of\ndata movement, be it in terms of performance or energy consumption. \n\n"}
{"id": "1510.06689", "contents": "Title: Parallel Tensor Compression for Large-Scale Scientific Data Abstract: As parallel computing trends towards the exascale, scientific data produced\nby high-fidelity simulations are growing increasingly massive. For instance, a\nsimulation on a three-dimensional spatial grid with 512 points per dimension\nthat tracks 64 variables per grid point for 128 time steps yields 8~TB of data,\nassuming double precision. By viewing the data as a dense five-way tensor, we\ncan compute a Tucker decomposition to find inherent low-dimensional multilinear\nstructure, achieving compression ratios of up to 5000 on real-world data sets\nwith negligible loss in accuracy. So that we can operate on such massive data,\nwe present the first-ever distributed-memory parallel implementation for the\nTucker decomposition, whose key computations correspond to parallel linear\nalgebra operations, albeit with nonstandard data layouts. Our approach\nspecifies a data distribution for tensors that avoids any tensor data\nredistribution, either locally or in parallel. We provide accompanying analysis\nof the computation and communication costs of the algorithms. To demonstrate\nthe compression and accuracy of the method, we apply our approach to real-world\ndata sets from combustion science simulations. We also provide detailed\nperformance results, including parallel performance in both weak and strong\nscaling experiments. \n\n"}
{"id": "1510.06706", "contents": "Title: ZNN - A Fast and Scalable Algorithm for Training 3D Convolutional\n  Networks on Multi-Core and Many-Core Shared Memory Machines Abstract: Convolutional networks (ConvNets) have become a popular approach to computer\nvision. It is important to accelerate ConvNet training, which is\ncomputationally costly. We propose a novel parallel algorithm based on\ndecomposition into a set of tasks, most of which are convolutions or FFTs.\nApplying Brent's theorem to the task dependency graph implies that linear\nspeedup with the number of processors is attainable within the PRAM model of\nparallel computation, for wide network architectures. To attain such\nperformance on real shared-memory machines, our algorithm computes convolutions\nconverging on the same node of the network with temporal locality to reduce\ncache misses, and sums the convergent convolution outputs via an almost\nwait-free concurrent method to reduce time spent in critical sections. We\nimplement the algorithm with a publicly available software package called ZNN.\nBenchmarking with multi-core CPUs shows that ZNN can attain speedup roughly\nequal to the number of physical cores. We also show that ZNN can attain over\n90x speedup on a many-core CPU (Xeon Phi Knights Corner). These speedups are\nachieved for network architectures with widths that are in common use. The task\nparallelism of the ZNN algorithm is suited to CPUs, while the SIMD parallelism\nof previous algorithms is compatible with GPUs. Through examples, we show that\nZNN can be either faster or slower than certain GPU implementations depending\non specifics of the network architecture, kernel sizes, and density and size of\nthe output patch. ZNN may be less costly to develop and maintain, due to the\nrelative ease of general-purpose CPU programming. \n\n"}
{"id": "1510.07357", "contents": "Title: Distributed Bare-Bones Communication in Wireless Networks Abstract: We consider wireless networks operating under the SINR model of interference.\nNodes have limited individual knowledge and capabilities: they do not know\ntheir positions in a coordinate system in the plane, further they do not know\ntheir neighborhoods, nor do they know the size of the network $n$, and finally\nthey cannot sense collisions resulting from simultaneous transmissions by at\nleast two neighbors. Each node is equipped with a unique integer name, where\n$N$ as an upper bound on the a range of names. We refer as a backbone to a\nsubnetwork induced by a diameter-preserving dominating set of nodes. Let\n$\\Delta$ denote a maximum number of nodes that can successfully receive a\nmessage transmitted by a node when no other nodes transmit concurrently. We\nstudy distributed algorithms for communication problems in three settings. In\nthe single-node-start case, when one node starts an execution and other nodes\nare awoken by receiving messages from already awoken nodes, we present a\nrandomized broadcast algorithm that wakes up all nodes in $O(n \\log^2 N)$\nrounds with high probability. For the synchronized-start case, when all nodes\nstart an execution simultaneously, we give a randomized algorithm computing a\nbackbone in $O(\\Delta\\log^{7} N)$ rounds with high probability. In the\npartly-coordinated-start case, when a number of nodes start an execution\ntogether and other nodes are awoken by receiving messages from the already\nawoken nodes, we develop an algorithm that creates a backbone in time\n$O(n\\log^2 N +\\Delta\\log^{7} N)$ with high probability. \n\n"}
{"id": "1510.08334", "contents": "Title: Toward fault-tolerant parallel-in-time integration with PFASST Abstract: We introduce and analyze different strategies for the parallel-in-time\nintegration method PFASST to recover from hard faults and subsequent data loss.\nSince PFASST stores solutions at multiple time steps on different processors,\ninformation from adjacent steps can be used to recover after a processor has\nfailed. PFASST's multi-level hierarchy allows to use the coarse level for\ncorrecting the reconstructed solution, which can help to minimize overhead. A\ntheoretical model is devised linking overhead to the number of additional\nPFASST iterations required for convergence after a fault. The potential\nefficiency of different strategies is assessed in terms of required additional\niterations for examples of diffusive and advective type. \n\n"}
{"id": "1510.08545", "contents": "Title: High Energy Physics Forum for Computational Excellence: Working Group\n  Reports (I. Applications Software II. Software Libraries and Tools III.\n  Systems) Abstract: Computing plays an essential role in all aspects of high energy physics. As\ncomputational technology evolves rapidly in new directions, and data throughput\nand volume continue to follow a steep trend-line, it is important for the HEP\ncommunity to develop an effective response to a series of expected challenges.\nIn order to help shape the desired response, the HEP Forum for Computational\nExcellence (HEP-FCE) initiated a roadmap planning activity with two key\noverlapping drivers -- 1) software effectiveness, and 2) infrastructure and\nexpertise advancement. The HEP-FCE formed three working groups, 1) Applications\nSoftware, 2) Software Libraries and Tools, and 3) Systems (including systems\nsoftware), to provide an overview of the current status of HEP computing and to\npresent findings and opportunities for the desired HEP computational roadmap.\nThe final versions of the reports are combined in this document, and are\npresented along with introductory material. \n\n"}
{"id": "1510.08940", "contents": "Title: Combining Peer-to-Peer and Cloud Computing for Large Scale On-line Games Abstract: This thesis investigates the combination of Peer-to-Peer (P2P) and Cloud\nComputing to support Massively Multiplayer On- line Games (MMOGs). MMOGs are\nlarge-scale distributed applications where a large number of users concurrently\nshare a real-time virtual environment. Commercial MMOG infrastructures are\nsized to support peak loads, incurring in high economical cost. Cloud Computing\nrepresents an attractive solution, as it lifts MMOG operators from the burden\nof buying and maintaining hardware, while offering the illusion of infinite\nmachines. However, it requires balancing the tradeoff between resource\nprovisioning and operational costs. P2P- based solutions present several\nadvantages, including the inherent scalability, self-repairing, and natural\nload distribution capabilities. They require additional mechanisms to suit the\nrequirements of a MMOG, such as backup solutions to cope with peer\nunreliability and heterogeneity. We propose mechanisms that integrate P2P and\nCloud Computing combining their advantages. Our techniques allow operators to\nselect the ideal tradeoff between performance and economical costs. Using\nrealistic workloads, we show that hybrid infrastructures can reduce the\neconomical effort of the operator, while offering a level of service comparable\nwith centralized architectures. \n\n"}
{"id": "1511.00212", "contents": "Title: Exploiting Redundant Computation in Communication-Avoiding Algorithms\n  for Algorithm-Based Fault Tolerance Abstract: Communication-avoiding algorithms allow redundant computations to minimize\nthe number of inter-process communications. In this paper, we propose to\nexploit this redundancy for fault-tolerance purpose. We illustrate this idea\nwith QR factorization of tall and skinny matrices, and we evaluate the number\nof failures our algorithm can tolerate under different semantics. \n\n"}
{"id": "1511.00363", "contents": "Title: BinaryConnect: Training Deep Neural Networks with binary weights during\n  propagations Abstract: Deep Neural Networks (DNN) have achieved state-of-the-art results in a wide\nrange of tasks, with the best results obtained with large training sets and\nlarge models. In the past, GPUs enabled these breakthroughs because of their\ngreater computational speed. In the future, faster computation at both training\nand test time is likely to be crucial for further progress and for consumer\napplications on low-power devices. As a result, there is much interest in\nresearch and development of dedicated hardware for Deep Learning (DL). Binary\nweights, i.e., weights which are constrained to only two possible values (e.g.\n-1 or 1), would bring great benefits to specialized DL hardware by replacing\nmany multiply-accumulate operations by simple accumulations, as multipliers are\nthe most space and power-hungry components of the digital implementation of\nneural networks. We introduce BinaryConnect, a method which consists in\ntraining a DNN with binary weights during the forward and backward\npropagations, while retaining precision of the stored weights in which\ngradients are accumulated. Like other dropout schemes, we show that\nBinaryConnect acts as regularizer and we obtain near state-of-the-art results\nwith BinaryConnect on the permutation-invariant MNIST, CIFAR-10 and SVHN. \n\n"}
{"id": "1511.00900", "contents": "Title: A Lower Bound for the Distributed Lov\\'asz Local Lemma Abstract: We show that any randomised Monte Carlo distributed algorithm for the\nLov\\'asz local lemma requires $\\Omega(\\log \\log n)$ communication rounds,\nassuming that it finds a correct assignment with high probability. Our result\nholds even in the special case of $d = O(1)$, where $d$ is the maximum degree\nof the dependency graph. By prior work, there are distributed algorithms for\nthe Lov\\'asz local lemma with a running time of $O(\\log n)$ rounds in\nbounded-degree graphs, and the best lower bound before our work was\n$\\Omega(\\log^* n)$ rounds [Chung et al. 2014]. \n\n"}
{"id": "1511.01088", "contents": "Title: There is no fast lunch: an examination of the running speed of\n  evolutionary algorithms in several languages Abstract: It is quite usual when an evolutionary algorithm tool or library uses a\nlanguage other than C, C++, Java or Matlab that a reviewer or the audience\nquestions its usefulness based on the speed of those other languages,\npurportedly slower than the aforementioned ones. Despite speed being not\neverything needed to design a useful evolutionary algorithm application, in\nthis paper we will measure the speed for several very basic evolutionary\nalgorithm operations in several languages which use different virtual machines\nand approaches, and prove that, in fact, there is no big difference in speed\nbetween interpreted and compiled languages, and that in some cases, interpreted\nlanguages such as JavaScript or Python can be faster than compiled languages\nsuch as Scala, making them worthy of use for evolutionary algorithm\nexperimentation. \n\n"}
{"id": "1511.01158", "contents": "Title: Distributed Deep Learning for Question Answering Abstract: This paper is an empirical study of the distributed deep learning for\nquestion answering subtasks: answer selection and question classification.\nComparison studies of SGD, MSGD, ADADELTA, ADAGRAD, ADAM/ADAMAX, RMSPROP,\nDOWNPOUR and EASGD/EAMSGD algorithms have been presented. Experimental results\nshow that the distributed framework based on the message passing interface can\naccelerate the convergence speed at a sublinear scale. This paper demonstrates\nthe importance of distributed training. For example, with 48 workers, a 24x\nspeedup is achievable for the answer selection task and running time is\ndecreased from 138.2 hours to 5.81 hours, which will increase the productivity\nsignificantly. \n\n"}
{"id": "1511.01865", "contents": "Title: Convolutional Neural Network for Stereotypical Motor Movement Detection\n  in Autism Abstract: Autism Spectrum Disorders (ASDs) are often associated with specific atypical\npostural or motor behaviors, of which Stereotypical Motor Movements (SMMs) have\na specific visibility. While the identification and the quantification of SMM\npatterns remain complex, its automation would provide support to accurate\ntuning of the intervention in the therapy of autism. Therefore, it is essential\nto develop automatic SMM detection systems in a real world setting, taking care\nof strong inter-subject and intra-subject variability. Wireless accelerometer\nsensing technology can provide a valid infrastructure for real-time SMM\ndetection, however such variability remains a problem also for machine learning\nmethods, in particular whenever handcrafted features extracted from\naccelerometer signal are considered. Here, we propose to employ the deep\nlearning paradigm in order to learn discriminating features from multi-sensor\naccelerometer signals. Our results provide preliminary evidence that feature\nlearning and transfer learning embedded in the deep architecture achieve higher\naccurate SMM detectors in longitudinal scenarios. \n\n"}
{"id": "1511.03599", "contents": "Title: A polyphase filter for many-core architectures Abstract: In this article we discuss our implementation of a polyphase filter for\nreal-time data processing in radio astronomy. We describe in detail our\nimplementation of the polyphase filter algorithm and its behaviour on three\ngenerations of NVIDIA GPU cards, on dual Intel Xeon CPUs and the Intel Xeon Phi\n(Knights Corner) platforms. All of our implementations aim to exploit the\npotential for data reuse that the algorithm offers. Our GPU implementations\nexplore two different methods for achieving this, the first makes use of\nL1/Texture cache, the second uses shared memory. We discuss the usability of\neach of our implementations along with their behaviours. We measure performance\nin execution time, which is a critical factor for real-time systems, we also\npresent results in terms of bandwidth (GB/s), compute (GFlop/s) and type\nconversions (GTc/s). We include a presentation of our results in terms of the\nsample rate which can be processed in real-time by a chosen platform, which\nmore intuitively describes the expected performance in a signal processing\nsetting. Our findings show that, for the GPUs considered, the performance of\nour polyphase filter when using lower precision input data is limited by type\nconversions rather than device bandwidth. We compare these results to an\nimplementation on the Xeon Phi. We show that our Xeon Phi implementation has a\nperformance that is 1.47x to 1.95x greater than our CPU implementation, however\nis not insufficient to compete with the performance of GPUs. We conclude with a\ncomparison of our best performing code to two other implementations of the\npolyphase filter, showing that our implementation is faster in nearly all\ncases. This work forms part of the Astro-Accelerate project, a many-core\naccelerated real-time data processing library for digital signal processing of\ntime-domain radio astronomy data. \n\n"}
{"id": "1511.03908", "contents": "Title: Learning Human Identity from Motion Patterns Abstract: We present a large-scale study exploring the capability of temporal deep\nneural networks to interpret natural human kinematics and introduce the first\nmethod for active biometric authentication with mobile inertial sensors. At\nGoogle, we have created a first-of-its-kind dataset of human movements,\npassively collected by 1500 volunteers using their smartphones daily over\nseveral months. We (1) compare several neural architectures for efficient\nlearning of temporal multi-modal data representations, (2) propose an optimized\nshift-invariant dense convolutional mechanism (DCWRNN), and (3) incorporate the\ndiscriminatively-trained dynamic features in a probabilistic generative\nframework taking into account temporal characteristics. Our results demonstrate\nthat human kinematics convey important information about user identity and can\nserve as a valuable component of multi-modal authentication systems. \n\n"}
{"id": "1511.04217", "contents": "Title: A Survey on Reproducibility in Parallel Computing Abstract: We summarize the results of a survey on reproducibility in parallel\ncomputing, which was conducted during the Euro-Par conference in August 2015.\nThe survey form was handed out to all participants of the conference and the\nworkshops. The questionnaire, which specifically targeted the parallel\ncomputing community, contained questions in four different categories: general\nquestions on reproducibility, the current state of reproducibility, the\nreproducibility of the participants' own papers, and questions about the\nparticipants' familiarity with tools, software, or open-source software\nlicenses used for reproducible research. \n\n"}
{"id": "1511.05122", "contents": "Title: Adversarial Manipulation of Deep Representations Abstract: We show that the representation of an image in a deep neural network (DNN)\ncan be manipulated to mimic those of other natural images, with only minor,\nimperceptible perturbations to the original image. Previous methods for\ngenerating adversarial images focused on image perturbations designed to\nproduce erroneous class labels, while we concentrate on the internal layers of\nDNN representations. In this way our new class of adversarial images differs\nqualitatively from others. While the adversary is perceptually similar to one\nimage, its internal representation appears remarkably similar to a different\nimage, one from a different class, bearing little if any apparent similarity to\nthe input; they appear generic and consistent with the space of natural images.\nThis phenomenon raises questions about DNN representations, as well as the\nproperties of natural images themselves. \n\n"}
{"id": "1511.05497", "contents": "Title: Learning Neural Network Architectures using Backpropagation Abstract: Deep neural networks with millions of parameters are at the heart of many\nstate of the art machine learning models today. However, recent works have\nshown that models with much smaller number of parameters can also perform just\nas well. In this work, we introduce the problem of architecture-learning, i.e;\nlearning the architecture of a neural network along with weights. We introduce\na new trainable parameter called tri-state ReLU, which helps in eliminating\nunnecessary neurons. We also propose a smooth regularizer which encourages the\ntotal number of neurons after elimination to be small. The resulting objective\nis differentiable and simple to optimize. We experimentally validate our method\non both small and large networks, and show that it can learn models with a\nconsiderably small number of parameters without affecting prediction accuracy. \n\n"}
{"id": "1511.05946", "contents": "Title: ACDC: A Structured Efficient Linear Layer Abstract: The linear layer is one of the most pervasive modules in deep learning\nrepresentations. However, it requires $O(N^2)$ parameters and $O(N^2)$\noperations. These costs can be prohibitive in mobile applications or prevent\nscaling in many domains. Here, we introduce a deep, differentiable,\nfully-connected neural network module composed of diagonal matrices of\nparameters, $\\mathbf{A}$ and $\\mathbf{D}$, and the discrete cosine transform\n$\\mathbf{C}$. The core module, structured as $\\mathbf{ACDC^{-1}}$, has $O(N)$\nparameters and incurs $O(N log N )$ operations. We present theoretical results\nshowing how deep cascades of ACDC layers approximate linear layers. ACDC is,\nhowever, a stand-alone module and can be used in combination with any other\ntypes of module. In our experiments, we show that it can indeed be successfully\ninterleaved with ReLU modules in convolutional neural networks for image\nrecognition. Our experiments also study critical factors in the training of\nthese structured modules, including initialization and depth. Finally, this\npaper also provides a connection between structured linear transforms used in\ndeep learning and the field of Fourier optics, illustrating how ACDC could in\nprinciple be implemented with lenses and diffractive elements. \n\n"}
{"id": "1511.06348", "contents": "Title: How much data is needed to train a medical image deep learning system to\n  achieve necessary high accuracy? Abstract: The use of Convolutional Neural Networks (CNN) in natural image\nclassification systems has produced very impressive results. Combined with the\ninherent nature of medical images that make them ideal for deep-learning,\nfurther application of such systems to medical image classification holds much\npromise. However, the usefulness and potential impact of such a system can be\ncompletely negated if it does not reach a target accuracy. In this paper, we\npresent a study on determining the optimum size of the training data set\nnecessary to achieve high classification accuracy with low variance in medical\nimage classification systems. The CNN was applied to classify axial Computed\nTomography (CT) images into six anatomical classes. We trained the CNN using\nsix different sizes of training data set (5, 10, 20, 50, 100, and 200) and then\ntested the resulting system with a total of 6000 CT images. All images were\nacquired from the Massachusetts General Hospital (MGH) Picture Archiving and\nCommunication System (PACS). Using this data, we employ the learning curve\napproach to predict classification accuracy at a given training sample size.\nOur research will present a general methodology for determining the training\ndata set size necessary to achieve a certain target classification accuracy\nthat can be easily applied to other problems within such systems. \n\n"}
{"id": "1511.06464", "contents": "Title: Unitary Evolution Recurrent Neural Networks Abstract: Recurrent neural networks (RNNs) are notoriously difficult to train. When the\neigenvalues of the hidden to hidden weight matrix deviate from absolute value\n1, optimization becomes difficult due to the well studied issue of vanishing\nand exploding gradients, especially when trying to learn long-term\ndependencies. To circumvent this problem, we propose a new architecture that\nlearns a unitary weight matrix, with eigenvalues of absolute value exactly 1.\nThe challenge we address is that of parametrizing unitary matrices in a way\nthat does not require expensive computations (such as eigendecomposition) after\neach weight update. We construct an expressive unitary weight matrix by\ncomposing several structured matrices that act as building blocks with\nparameters to be learned. Optimization with this parameterization becomes\nfeasible only when considering hidden states in the complex domain. We\ndemonstrate the potential of this architecture by achieving state of the art\nresults in several hard tasks involving very long-term dependencies. \n\n"}
{"id": "1511.07376", "contents": "Title: CNNdroid: GPU-Accelerated Execution of Trained Deep Convolutional Neural\n  Networks on Android Abstract: Many mobile applications running on smartphones and wearable devices would\npotentially benefit from the accuracy and scalability of deep CNN-based machine\nlearning algorithms. However, performance and energy consumption limitations\nmake the execution of such computationally intensive algorithms on mobile\ndevices prohibitive. We present a GPU-accelerated library, dubbed CNNdroid, for\nexecution of trained deep CNNs on Android-based mobile devices. Empirical\nevaluations show that CNNdroid achieves up to 60X speedup and 130X energy\nsaving on current mobile devices. The CNNdroid open source library is available\nfor download at https://github.com/ENCP/CNNdroid \n\n"}
{"id": "1511.07528", "contents": "Title: The Limitations of Deep Learning in Adversarial Settings Abstract: Deep learning takes advantage of large datasets and computationally efficient\ntraining algorithms to outperform other approaches at various machine learning\ntasks. However, imperfections in the training phase of deep neural networks\nmake them vulnerable to adversarial samples: inputs crafted by adversaries with\nthe intent of causing deep neural networks to misclassify. In this work, we\nformalize the space of adversaries against deep neural networks (DNNs) and\nintroduce a novel class of algorithms to craft adversarial samples based on a\nprecise understanding of the mapping between inputs and outputs of DNNs. In an\napplication to computer vision, we show that our algorithms can reliably\nproduce samples correctly classified by human subjects but misclassified in\nspecific targets by a DNN with a 97% adversarial success rate while only\nmodifying on average 4.02% of the input features per sample. We then evaluate\nthe vulnerability of different sample classes to adversarial perturbations by\ndefining a hardness measure. Finally, we describe preliminary work outlining\ndefenses against adversarial samples by defining a predictive measure of\ndistance between a benign input and a target classification. \n\n"}
{"id": "1511.07693", "contents": "Title: A Distributed System for Storing and Processing Data from\n  Earth-observing Satellites: System Design and Performance Evaluation of the\n  Visualisation Tool Abstract: We present a distributed system for storage, processing, three-dimensional\nvisualisation and basic analysis of data from Earth-observing satellites. The\ndatabase and the server have been designed for high performance and\nscalability, whereas the client is highly portable thanks to having been\ndesigned as a HTML5- and WebGL-based Web application. The system is based on\nthe so-called MEAN stack, a modern replacement for LAMP which has steadily been\ngaining traction among high-performance Web applications. We demonstrate the\nperformance of the system from the perspective of an user operating the client. \n\n"}
{"id": "1512.00567", "contents": "Title: Rethinking the Inception Architecture for Computer Vision Abstract: Convolutional networks are at the core of most state-of-the-art computer\nvision solutions for a wide variety of tasks. Since 2014 very deep\nconvolutional networks started to become mainstream, yielding substantial gains\nin various benchmarks. Although increased model size and computational cost\ntend to translate to immediate quality gains for most tasks (as long as enough\nlabeled data is provided for training), computational efficiency and low\nparameter count are still enabling factors for various use cases such as mobile\nvision and big-data scenarios. Here we explore ways to scale up networks in\nways that aim at utilizing the added computation as efficiently as possible by\nsuitably factorized convolutions and aggressive regularization. We benchmark\nour methods on the ILSVRC 2012 classification challenge validation set\ndemonstrate substantial gains over the state of the art: 21.2% top-1 and 5.6%\ntop-5 error for single frame evaluation using a network with a computational\ncost of 5 billion multiply-adds per inference and with using less than 25\nmillion parameters. With an ensemble of 4 models and multi-crop evaluation, we\nreport 3.5% top-5 error on the validation set (3.6% error on the test set) and\n17.3% top-1 error on the validation set. \n\n"}
{"id": "1512.01362", "contents": "Title: Proposition of a Theoretical Model for Missing Data Imputation using\n  Deep Learning and Evolutionary Algorithms Abstract: In the last couple of decades, there has been major advancements in the\ndomain of missing data imputation. The techniques in the domain include amongst\nothers: Expectation Maximization, Neural Networks with Evolutionary Algorithms\nor optimization techniques and K-Nearest Neighbor approaches to solve the\nproblem. The presence of missing data entries in databases render the tasks of\ndecision-making and data analysis nontrivial. As a result this area has\nattracted a lot of research interest with the aim being to yield accurate and\ntime efficient and sensitive missing data imputation techniques especially when\ntime sensitive applications are concerned like power plants and winding\nprocesses. In this article, considering arbitrary and monotone missing data\npatterns, we hypothesize that the use of deep neural networks built using\nautoencoders and denoising autoencoders in conjunction with genetic algorithms,\nswarm intelligence and maximum likelihood estimator methods as novel data\nimputation techniques will lead to better imputed values than existing\ntechniques. Also considered are the missing at random, missing completely at\nrandom and missing not at random missing data mechanisms. We also intend to use\nfuzzy logic in tandem with deep neural networks to perform the missing data\nimputation tasks, as well as different building blocks for the deep neural\nnetworks like Stacked Restricted Boltzmann Machines and Deep Belief Networks to\ntest our hypothesis. The motivation behind this article is the need for missing\ndata imputation techniques that lead to better imputed values than existing\nmethods with higher accuracies and lower errors. \n\n"}
{"id": "1512.01609", "contents": "Title: Data Center Server Provision: Distributed Asynchronous Control for\n  Coupled Renewal Systems Abstract: This paper considers a cost minimization problem for data centers with N\nservers and randomly arriving service requests. A central router decides which\nserver to use for each new request. Each server has three types of states\n(active, idle, setup) with different costs and time durations. The servers\noperate asynchronously over their own states and can choose one of multiple\nsleep modes when idle. We develop an online distributed control algorithm so\nthat each server makes its own decisions, the request queues are bounded and\nthe overall time average cost is near optimal with probability 1. The algorithm\ndoes not need probability information for the arrival rate or job sizes. Next,\nan improved algorithm that uses a single queue is developed via a\n\"virtualization\" technique which is shown to provide the same (near optimal)\ncosts. Simulation experiments on a real data center traffic trace demonstrate\nthe efficiency of our algorithm compared to other existing algorithms. \n\n"}
{"id": "1512.01708", "contents": "Title: Variance Reduction for Distributed Stochastic Gradient Descent Abstract: Variance reduction (VR) methods boost the performance of stochastic gradient\ndescent (SGD) by enabling the use of larger, constant stepsizes and preserving\nlinear convergence rates. However, current variance reduced SGD methods require\neither high memory usage or an exact gradient computation (using the entire\ndataset) at the end of each epoch. This limits the use of VR methods in\npractical distributed settings. In this paper, we propose a variance reduction\nmethod, called VR-lite, that does not require full gradient computations or\nextra storage. We explore distributed synchronous and asynchronous variants\nthat are scalable and remain stable with low communication frequency. We\nempirically compare both the sequential and distributed algorithms to\nstate-of-the-art stochastic optimization methods, and find that our proposed\nalgorithms perform favorably to other stochastic methods. \n\n"}
{"id": "1512.02497", "contents": "Title: Deep Exemplar 2D-3D Detection by Adapting from Real to Rendered Views Abstract: This paper presents an end-to-end convolutional neural network (CNN) for\n2D-3D exemplar detection. We demonstrate that the ability to adapt the features\nof natural images to better align with those of CAD rendered views is critical\nto the success of our technique. We show that the adaptation can be learned by\ncompositing rendered views of textured object models on natural images. Our\napproach can be naturally incorporated into a CNN detection pipeline and\nextends the accuracy and speed benefits from recent advances in deep learning\nto 2D-3D exemplar detection. We applied our method to two tasks: instance\ndetection, where we evaluated on the IKEA dataset, and object category\ndetection, where we out-perform Aubry et al. for \"chair\" detection on a subset\nof the Pascal VOC dataset. \n\n"}
{"id": "1512.02737", "contents": "Title: Using Symmetry to Schedule Classical Matrix Multiplication Abstract: Presented with a new machine with a specific interconnect topology, algorithm\ndesigners use intuition about the symmetry of the algorithm to design time and\ncommunication-efficient schedules that map the algorithm to the machine. Is\nthere a systematic procedure for designing schedules? We present a new\ntechnique to design schedules for algorithms with no non-trivial dependencies,\nfocusing on the classical matrix multiplication algorithm.\n  We model the symmetry of algorithm with the set of instructions $X$ as the\naction of the group formed by the compositions of bijections from the set $X$\nto itself. We model the machine as the action of the group $N\\times \\Delta$,\nwhere $N$ and $\\Delta$ represent the interconnect topology and time increments\nrespectively, on the set $P\\times T$ of processors iterated over time steps. We\nmodel schedules as symmetry-preserving equivariant maps between the set $X$ and\na subgroup of its symmetry and the set $P\\times T$ with the symmetry\n$N\\times\\Delta$. Such equivariant maps are the solutions of a set of algebraic\nequations involving group homomorphisms. We associate time and communication\ncosts with the solutions to these equations.\n  We solve these equations for the classical matrix multiplication algorithm\nand show that equivariant maps correspond to time- and communication-efficient\nschedules for many topologies. We recover well known variants including the\nCannon's algorithm and the communication-avoiding \"2.5D\" algorithm for toroidal\ninterconnects, systolic computation for planar hexagonal VLSI arrays, recursive\nalgorithms for fat-trees, the cache-oblivious algorithm for the ideal cache\nmodel, and the space-bounded schedule for the parallel memory hierarchy model.\nThis suggests that the design of a schedule for a new class of machines can be\nmotivated by solutions to algebraic equations. \n\n"}
{"id": "1512.02831", "contents": "Title: Bigger Buffer k-d Trees on Multi-Many-Core Systems Abstract: A buffer k-d tree is a k-d tree variant for massively-parallel nearest\nneighbor search. While providing valuable speed-ups on modern many-core devices\nin case both a large number of reference and query points are given, buffer k-d\ntrees are limited by the amount of points that can fit on a single device. In\nthis work, we show how to modify the original data structure and the associated\nworkflow to make the overall approach capable of dealing with massive data\nsets. We further provide a simple yet efficient way of using multiple devices\ngiven in a single workstation. The applicability of the modified framework is\ndemonstrated in the context of astronomy, a field that is faced with huge\namounts of data. \n\n"}
{"id": "1512.02970", "contents": "Title: Efficient Distributed SGD with Variance Reduction Abstract: Stochastic Gradient Descent (SGD) has become one of the most popular\noptimization methods for training machine learning models on massive datasets.\nHowever, SGD suffers from two main drawbacks: (i) The noisy gradient updates\nhave high variance, which slows down convergence as the iterates approach the\noptimum, and (ii) SGD scales poorly in distributed settings, typically\nexperiencing rapidly decreasing marginal benefits as the number of workers\nincreases. In this paper, we propose a highly parallel method, CentralVR, that\nuses error corrections to reduce the variance of SGD gradient updates, and\nscales linearly with the number of worker nodes. CentralVR enjoys low iteration\ncomplexity, provably linear convergence rates, and exhibits linear performance\ngains up to hundreds of cores for massive datasets. We compare CentralVR to\nstate-of-the-art parallel stochastic optimization methods on a variety of\nmodels and datasets, and find that our proposed methods exhibit stronger\nscaling than other SGD variants. \n\n"}
{"id": "1512.02970", "contents": "Title: Efficient Distributed SGD with Variance Reduction Abstract: Stochastic Gradient Descent (SGD) has become one of the most popular\noptimization methods for training machine learning models on massive datasets.\nHowever, SGD suffers from two main drawbacks: (i) The noisy gradient updates\nhave high variance, which slows down convergence as the iterates approach the\noptimum, and (ii) SGD scales poorly in distributed settings, typically\nexperiencing rapidly decreasing marginal benefits as the number of workers\nincreases. In this paper, we propose a highly parallel method, CentralVR, that\nuses error corrections to reduce the variance of SGD gradient updates, and\nscales linearly with the number of worker nodes. CentralVR enjoys low iteration\ncomplexity, provably linear convergence rates, and exhibits linear performance\ngains up to hundreds of cores for massive datasets. We compare CentralVR to\nstate-of-the-art parallel stochastic optimization methods on a variety of\nmodels and datasets, and find that our proposed methods exhibit stronger\nscaling than other SGD variants. \n\n"}
{"id": "1512.03466", "contents": "Title: Computing factorized approximations of Pareto-fronts using\n  mNM-landscapes and Boltzmann distributions Abstract: NM-landscapes have been recently introduced as a class of tunable rugged\nmodels. They are a subset of the general interaction models where all the\ninteractions are of order less or equal $M$. The Boltzmann distribution has\nbeen extensively applied in single-objective evolutionary algorithms to\nimplement selection and study the theoretical properties of model-building\nalgorithms. In this paper we propose the combination of the multi-objective\nNM-landscape model and the Boltzmann distribution to obtain Pareto-front\napproximations. We investigate the joint effect of the parameters of the\nNM-landscapes and the probabilistic factorizations in the shape of the Pareto\nfront approximations. \n\n"}
{"id": "1512.03965", "contents": "Title: The Power of Depth for Feedforward Neural Networks Abstract: We show that there is a simple (approximately radial) function on $\\reals^d$,\nexpressible by a small 3-layer feedforward neural networks, which cannot be\napproximated by any 2-layer network, to more than a certain constant accuracy,\nunless its width is exponential in the dimension. The result holds for\nvirtually all known activation functions, including rectified linear units,\nsigmoids and thresholds, and formally demonstrates that depth -- even if\nincreased by 1 -- can be exponentially more valuable than width for standard\nfeedforward neural networks. Moreover, compared to related results in the\ncontext of Boolean functions, our result requires fewer assumptions, and the\nproof techniques and construction are very different. \n\n"}
{"id": "1512.04280", "contents": "Title: Small-footprint Deep Neural Networks with Highway Connections for Speech\n  Recognition Abstract: For speech recognition, deep neural networks (DNNs) have significantly\nimproved the recognition accuracy in most of benchmark datasets and application\ndomains. However, compared to the conventional Gaussian mixture models,\nDNN-based acoustic models usually have much larger number of model parameters,\nmaking it challenging for their applications in resource constrained platforms,\ne.g., mobile devices. In this paper, we study the application of the recently\nproposed highway network to train small-footprint DNNs, which are {\\it thinner}\nand {\\it deeper}, and have significantly smaller number of model parameters\ncompared to conventional DNNs. We investigated this approach on the AMI meeting\nspeech transcription corpus which has around 70 hours of audio data. The\nhighway neural networks constantly outperformed their plain DNN counterparts,\nand the number of model parameters can be reduced significantly without\nsacrificing the recognition accuracy. \n\n"}
{"id": "1512.04639", "contents": "Title: Linear Models of Computation and Program Learning Abstract: We consider two classes of computations which admit taking linear\ncombinations of execution runs: probabilistic sampling and generalized\nanimation. We argue that the task of program learning should be more tractable\nfor these architectures than for conventional deterministic programs. We look\nat the recent advances in the \"sampling the samplers\" paradigm in higher-order\nprobabilistic programming. We also discuss connections between partial\ninconsistency, non-monotonic inference, and vector semantics. \n\n"}
{"id": "1512.08903", "contents": "Title: Online Keyword Spotting with a Character-Level Recurrent Neural Network Abstract: In this paper, we propose a context-aware keyword spotting model employing a\ncharacter-level recurrent neural network (RNN) for spoken term detection in\ncontinuous speech. The RNN is end-to-end trained with connectionist temporal\nclassification (CTC) to generate the probabilities of character and\nword-boundary labels. There is no need for the phonetic transcription, senone\nmodeling, or system dictionary in training and testing. Also, keywords can\neasily be added and modified by editing the text based keyword list without\nretraining the RNN. Moreover, the unidirectional RNN processes an infinitely\nlong input audio streams without pre-segmentation and keywords are detected\nwith low-latency before the utterance is finished. Experimental results show\nthat the proposed keyword spotter significantly outperforms the deep neural\nnetwork (DNN) and hidden Markov model (HMM) based keyword-filler model even\nwith less computations. \n\n"}
{"id": "1601.00289", "contents": "Title: An Empirical Comparison of Big Graph Frameworks in the Context of\n  Network Analysis Abstract: Complex networks are relational data sets commonly represented as graphs. The\nanalysis of their intricate structure is relevant to many areas of science and\ncommerce, and data sets may reach sizes that require distributed storage and\nprocessing. We describe and compare programming models for distributed\ncomputing with a focus on graph algorithms for large-scale complex network\nanalysis. Four frameworks - GraphLab, Apache Giraph, Giraph++ and Apache Flink\n- are used to implement algorithms for the representative problems Connected\nComponents, Community Detection, PageRank and Clustering Coefficients. The\nimplementations are executed on a computer cluster to evaluate the frameworks'\nsuitability in practice and to compare their performance to that of the\nsingle-machine, shared-memory parallel network analysis package NetworKit. Out\nof the distributed frameworks, GraphLab and Apache Giraph generally show the\nbest performance. In our experiments a cluster of eight computers running\nApache Giraph enables the analysis of a network with about 2 billion edges,\nwhich is too large for a single machine of the same type. However, for networks\nthat fit into memory of one machine, the performance of the shared-memory\nparallel implementation is far better than the distributed ones. The study\nprovides experimental evidence for selecting the appropriate framework\ndepending on the task and data volume. \n\n"}
{"id": "1601.03277", "contents": "Title: Weightless neural network parameters and architecture selection in a\n  quantum computer Abstract: Training artificial neural networks requires a tedious empirical evaluation\nto determine a suitable neural network architecture. To avoid this empirical\nprocess several techniques have been proposed to automatise the architecture\nselection process. In this paper, we propose a method to perform parameter and\narchitecture selection for a quantum weightless neural network (qWNN). The\narchitecture selection is performed through the learning procedure of a qWNN\nwith a learning algorithm that uses the principle of quantum superposition and\na non-linear quantum operator. The main advantage of the proposed method is\nthat it performs a global search in the space of qWNN architecture and\nparameters rather than a local search. \n\n"}
{"id": "1601.05439", "contents": "Title: RepEx: A Flexible Framework for Scalable Replica Exchange Molecular\n  Dynamics Simulations Abstract: Replica Exchange (RE) simulations have emerged as an important algorithmic\ntool for the molecular sciences. RE simulations involve the concurrent\nexecution of independent simulations which infrequently interact and exchange\ninformation. The next set of simulation parameters are based upon the outcome\nof the exchanges.\n  Typically RE functionality is integrated into the molecular simulation\nsoftware package. A primary motivation of the tight integration of RE\nfunctionality with simulation codes has been performance. This is limiting at\nmultiple levels. First, advances in the RE methodology are tied to the\nmolecular simulation code. Consequently these advances remain confined to the\nmolecular simulation code for which they were developed. Second, it is\ndifficult to extend or experiment with novel RE algorithms, since expertise in\nthe molecular simulation code is typically required.\n  In this paper, we propose the RepEx framework which address these\naforementioned shortcomings of existing approaches, while striking the balance\nbetween flexibility (any RE scheme) and scalability (tens of thousands of\nreplicas) over a diverse range of platforms. RepEx is designed to use a\npilot-job based runtime system and support diverse RE Patterns and Execution\nModes. RE Patterns are concerned with synchronization mechanisms in RE\nsimulation, and Execution Modes with spatial and temporal mapping of workload\nto the CPU cores. We discuss how the design and implementation yield the\nfollowing primary contributions of the RepEx framework: (i) its ability to\nsupport different RE schemes independent of molecular simulation codes, (ii)\nprovide the ability to execute different exchange schemes and replica counts\nindependent of the specific availability of resources, (iii) provide a runtime\nsystem that has first-class support for task-level parallelism, and (iv)\nrequired scalability along multiple dimensions. \n\n"}
{"id": "1601.06497", "contents": "Title: Quegel: A General-Purpose Query-Centric Framework for Querying Big\n  Graphs Abstract: Pioneered by Google's Pregel, many distributed systems have been developed\nfor large-scale graph analytics. These systems expose the user-friendly \"think\nlike a vertex\" programming interface to users, and exhibit good horizontal\nscalability. However, these systems are designed for tasks where the majority\nof graph vertices participate in computation, but are not suitable for\nprocessing light-workload graph queries where only a small fraction of vertices\nneed to be accessed. The programming paradigm adopted by these systems can\nseriously under-utilize the resources in a cluster for graph query processing.\nIn this work, we develop a new open-source system, called Quegel, for querying\nbig graphs, which treats queries as first-class citizens in the design of its\ncomputing model. Users only need to specify the Pregel-like algorithm for a\ngeneric query, and Quegel processes light-workload graph queries on demand\nusing a novel superstep-sharing execution model to effectively utilize the\ncluster resources. Quegel further provides a convenient interface for\nconstructing graph indexes, which significantly improve query performance but\nare not supported by existing graph-parallel systems. Our experiments verified\nthat Quegel is highly efficient in answering various types of graph queries and\nis up to orders of magnitude faster than existing systems. \n\n"}
{"id": "1602.00678", "contents": "Title: Ensemble Toolkit: Scalable and Flexible Execution of Ensembles of Tasks Abstract: There are many science applications that require scalable task-level\nparallelism and support for flexible execution and coupling of ensembles of\nsimulations. Most high-performance system software and middleware, however, are\ndesigned to support the execution and optimization of single tasks. Motivated\nby the missing capabilities of these computing systems and the increasing\nimportance of task-level parallelism, we introduce the Ensemble toolkit which\nhas the following application development features: (i) abstractions that\nenable the expression of ensembles as primary entities, and (ii) support for\nensemble-based execution patterns that capture the majority of application\nscenarios. Ensemble toolkit uses a scalable pilot-based runtime system that\ndecouples workload execution and resource management details from the\nexpression of the application, and enables the efficient and dynamic execution\nof ensembles on heterogeneous computing resources. We investigate three\nexecution patterns and characterize the scalability and overhead of Ensemble\ntoolkit for these patterns. We investigate scaling properties for up to O(1000)\nconcurrent ensembles and O(1000) cores and find linear weak and strong scaling\nbehaviour. \n\n"}
{"id": "1602.00991", "contents": "Title: Deep Tracking: Seeing Beyond Seeing Using Recurrent Neural Networks Abstract: This paper presents to the best of our knowledge the first end-to-end object\ntracking approach which directly maps from raw sensor input to object tracks in\nsensor space without requiring any feature engineering or system identification\nin the form of plant or sensor models. Specifically, our system accepts a\nstream of raw sensor data at one end and, in real-time, produces an estimate of\nthe entire environment state at the output including even occluded objects. We\nachieve this by framing the problem as a deep learning task and exploit\nsequence models in the form of recurrent neural networks to learn a mapping\nfrom sensor measurements to object tracks. In particular, we propose a learning\nmethod based on a form of input dropout which allows learning in an\nunsupervised manner, only based on raw, occluded sensor data without access to\nground-truth annotations. We demonstrate our approach using a synthetic dataset\ndesigned to mimic the task of tracking objects in 2D laser data -- as commonly\nencountered in robotics applications -- and show that it learns to track many\ndynamic objects despite occlusions and the presence of sensor noise. \n\n"}
{"id": "1602.02575", "contents": "Title: DECOrrelated feature space partitioning for distributed sparse\n  regression Abstract: Fitting statistical models is computationally challenging when the sample\nsize or the dimension of the dataset is huge. An attractive approach for\ndown-scaling the problem size is to first partition the dataset into subsets\nand then fit using distributed algorithms. The dataset can be partitioned\neither horizontally (in the sample space) or vertically (in the feature space).\nWhile the majority of the literature focuses on sample space partitioning,\nfeature space partitioning is more effective when $p\\gg n$. Existing methods\nfor partitioning features, however, are either vulnerable to high correlations\nor inefficient in reducing the model dimension. In this paper, we solve these\nproblems through a new embarrassingly parallel framework named DECO for\ndistributed variable selection and parameter estimation. In DECO, variables are\nfirst partitioned and allocated to $m$ distributed workers. The decorrelated\nsubset data within each worker are then fitted via any algorithm designed for\nhigh-dimensional problems. We show that by incorporating the decorrelation\nstep, DECO can achieve consistent variable selection and parameter estimation\non each subset with (almost) no assumptions. In addition, the convergence rate\nis nearly minimax optimal for both sparse and weakly sparse models and does NOT\ndepend on the partition number $m$. Extensive numerical experiments are\nprovided to illustrate the performance of the new framework. \n\n"}
{"id": "1602.02644", "contents": "Title: Generating Images with Perceptual Similarity Metrics based on Deep\n  Networks Abstract: Image-generating machine learning models are typically trained with loss\nfunctions based on distance in the image space. This often leads to\nover-smoothed results. We propose a class of loss functions, which we call deep\nperceptual similarity metrics (DeePSiM), that mitigate this problem. Instead of\ncomputing distances in the image space, we compute distances between image\nfeatures extracted by deep neural networks. This metric better reflects\nperceptually similarity of images and thus leads to better results. We show\nthree applications: autoencoder training, a modification of a variational\nautoencoder, and inversion of deep convolutional networks. In all cases, the\ngenerated images look sharp and resemble natural images. \n\n"}
{"id": "1602.02660", "contents": "Title: Exploiting Cyclic Symmetry in Convolutional Neural Networks Abstract: Many classes of images exhibit rotational symmetry. Convolutional neural\nnetworks are sometimes trained using data augmentation to exploit this, but\nthey are still required to learn the rotation equivariance properties from the\ndata. Encoding these properties into the network architecture, as we are\nalready used to doing for translation equivariance by using convolutional\nlayers, could result in a more efficient use of the parameter budget by\nrelieving the model from learning them. We introduce four operations which can\nbe inserted into neural network models as layers, and which can be combined to\nmake these models partially equivariant to rotations. They also enable\nparameter sharing across different orientations. We evaluate the effect of\nthese architectural modifications on three datasets which exhibit rotational\nsymmetry and demonstrate improved performance with smaller models. \n\n"}
{"id": "1602.03072", "contents": "Title: Big Graph Mining: Frameworks and Techniques Abstract: Big graph mining is an important research area and it has attracted\nconsiderable attention. It allows to process, analyze, and extract meaningful\ninformation from large amounts of graph data. Big graph mining has been highly\nmotivated not only by the tremendously increasing size of graphs but also by\nits huge number of applications. Such applications include bioinformatics,\nchemoinformatics and social networks. One of the most challenging tasks in big\ngraph mining is pattern mining in big graphs. This task consists on using data\nmining algorithms to discover interesting, unexpected and useful patterns in\nlarge amounts of graph data. It aims also to provide deeper understanding of\ngraph data. In this context, several graph processing frameworks and scaling\ndata mining/pattern mining techniques have been proposed to deal with very big\ngraphs. This paper gives an overview of existing data mining and graph\nprocessing frameworks that deal with very big graphs. Then it presents a survey\nof current researches in the field of data mining / pattern mining in big\ngraphs and discusses the main research issues related to this field. It also\ngives a categorization of both distributed data mining and machine learning\ntechniques, graph processing frameworks and large scale pattern mining\napproaches. \n\n"}
{"id": "1602.06289", "contents": "Title: Learning to SMILE(S) Abstract: This paper shows how one can directly apply natural language processing (NLP)\nmethods to classification problems in cheminformatics. Connection between these\nseemingly separate fields is shown by considering standard textual\nrepresentation of compound, SMILES. The problem of activity prediction against\na target protein is considered, which is a crucial part of computer aided drug\ndesign process. Conducted experiments show that this way one can not only\noutrank state of the art results of hand crafted representations but also gets\ndirect structural insights into the way decisions are made. \n\n"}
{"id": "1602.07714", "contents": "Title: Learning values across many orders of magnitude Abstract: Most learning algorithms are not invariant to the scale of the function that\nis being approximated. We propose to adaptively normalize the targets used in\nlearning. This is useful in value-based reinforcement learning, where the\nmagnitude of appropriate value approximations can change over time when we\nupdate the policy of behavior. Our main motivation is prior work on learning to\nplay Atari games, where the rewards were all clipped to a predetermined range.\nThis clipping facilitates learning across many different games with a single\nlearning algorithm, but a clipped reward function can result in qualitatively\ndifferent behavior. Using the adaptive normalization we can remove this\ndomain-specific heuristic without diminishing overall performance. \n\n"}
{"id": "1602.07919", "contents": "Title: Experimental Performance Evaluation of Cloud-Based\n  Analytics-as-a-Service Abstract: An increasing number of Analytics-as-a-Service solutions has recently seen\nthe light, in the landscape of cloud-based services. These services allow\nflexible composition of compute and storage components, that create powerful\ndata ingestion and processing pipelines. This work is a first attempt at an\nexperimental evaluation of analytic application performance executed using a\nwide range of storage service configurations. We present an intuitive notion of\ndata locality, that we use as a proxy to rank different service compositions in\nterms of expected performance. Through an empirical analysis, we dissect the\nperformance achieved by analytic workloads and unveil problems due to the\nimpedance mismatch that arise in some configurations. Our work paves the way to\na better understanding of modern cloud-based analytic services and their\nperformance, both for its end-users and their providers. \n\n"}
{"id": "1602.08032", "contents": "Title: Time-Space Trade-offs in Population Protocols Abstract: Population protocols are a popular model of distributed computing, in which\nrandomly-interacting agents with little computational power cooperate to\njointly perform computational tasks. Inspired by developments in molecular\ncomputation, and in particular DNA computing, recent algorithmic work has\nfocused on the complexity of solving simple yet fundamental tasks in the\npopulation model, such as leader election (which requires stabilization to a\nsingle agent in a special \"leader\" state), and majority (in which agents must\nstabilize to a decision as to which of two possible initial states had higher\ninitial count). Known results point towards an inherent trade-off between the\ntime complexity of such algorithms, and the space complexity, i.e. size of the\nmemory available to each agent.\n  In this paper, we explore this trade-off and provide new upper and lower\nbounds for majority and leader election. First, we prove a unified lower bound,\nwhich relates the space available per node with the time complexity achievable\nby a protocol: for instance, our result implies that any protocol solving\neither of these tasks for $n$ agents using $O( \\log \\log n )$ states must take\n$\\Omega( n / \\rm{polylog} n )$ expected time. This is the first result to\ncharacterize time complexity for protocols which employ super-constant number\nof states per node, and proves that fast, poly-logarithmic running times\nrequire protocols to have relatively large space costs.\n  On the positive side, we give algorithms showing that fast, poly-logarithmic\nstabilization time can be achieved using $O( \\log^2 n )$ space per node, in the\ncase of both tasks. Overall, our results highlight a time complexity separation\nbetween $O(\\log \\log n)$ and $\\Theta( \\log^2 n )$ state space size for both\nmajority and leader election in population protocols, and introduce new\ntechniques, which should be applicable more broadly. \n\n"}
{"id": "1603.00223", "contents": "Title: Segmental Recurrent Neural Networks for End-to-end Speech Recognition Abstract: We study the segmental recurrent neural network for end-to-end acoustic\nmodelling. This model connects the segmental conditional random field (CRF)\nwith a recurrent neural network (RNN) used for feature extraction. Compared to\nmost previous CRF-based acoustic models, it does not rely on an external system\nto provide features or segmentation boundaries. Instead, this model\nmarginalises out all the possible segmentations, and features are extracted\nfrom the RNN trained together with the segmental CRF. In essence, this model is\nself-contained and can be trained end-to-end. In this paper, we discuss\npractical training and decoding issues as well as the method to speed up the\ntraining in the context of speech recognition. We performed experiments on the\nTIMIT dataset. We achieved 17.3 phone error rate (PER) from the first-pass\ndecoding --- the best reported result using CRFs, despite the fact that we only\nused a zeroth-order CRF and without using any language model. \n\n"}
{"id": "1603.00307", "contents": "Title: A Graph-Based Semantics Workbench for Concurrent Asynchronous Programs Abstract: A number of novel programming languages and libraries have been proposed that\noffer simpler-to-use models of concurrency than threads. It is challenging,\nhowever, to devise execution models that successfully realise their\nabstractions without forfeiting performance or introducing unintended\nbehaviours. This is exemplified by SCOOP---a concurrent object-oriented\nmessage-passing language---which has seen multiple semantics proposed and\nimplemented over its evolution. We propose a \"semantics workbench\" with fully\nand semi-automatic tools for SCOOP, that can be used to analyse and compare\nprograms with respect to different execution models. We demonstrate its use in\nchecking the consistency of semantics by applying it to a set of representative\nprograms, and highlighting a deadlock-related discrepancy between the principal\nexecution models of the language. Our workbench is based on a modular and\nparameterisable graph transformation semantics implemented in the GROOVE tool.\nWe discuss how graph transformations are leveraged to atomically model\nintricate language abstractions, and how the visual yet algebraic nature of the\nmodel can be used to ascertain soundness. \n\n"}
{"id": "1603.01670", "contents": "Title: Network Morphism Abstract: We present in this paper a systematic study on how to morph a well-trained\nneural network to a new one so that its network function can be completely\npreserved. We define this as \\emph{network morphism} in this research. After\nmorphing a parent network, the child network is expected to inherit the\nknowledge from its parent network and also has the potential to continue\ngrowing into a more powerful one with much shortened training time. The first\nrequirement for this network morphism is its ability to handle diverse morphing\ntypes of networks, including changes of depth, width, kernel size, and even\nsubnet. To meet this requirement, we first introduce the network morphism\nequations, and then develop novel morphing algorithms for all these morphing\ntypes for both classic and convolutional neural networks. The second\nrequirement for this network morphism is its ability to deal with non-linearity\nin a network. We propose a family of parametric-activation functions to\nfacilitate the morphing of any continuous non-linear activation neurons.\nExperimental results on benchmark datasets and typical neural networks\ndemonstrate the effectiveness of the proposed network morphism scheme. \n\n"}
{"id": "1603.02188", "contents": "Title: Self-stabilizing Balls & Bins in Batches Abstract: A fundamental problem in distributed computing is the distribution of\nrequests to a set of uniform servers without a centralized controller.\nClassically, such problems are modeled as static balls into bins processes,\nwhere $m$ balls (tasks) are to be distributed to $n$ bins (servers). In a\nseminal work, Azar et al. proposed the sequential strategy \\greedy{d} for\n$n=m$. When thrown, a ball queries the load of $d$ random bins and is allocated\nto a least loaded of these. Azar et al. showed that $d=2$ yields an exponential\nimprovement compared to $d=1$. Berenbrink et al. extended this to $m\\gg n$,\nshowing that the maximal load difference is independent of $m$ for $d=2$ (in\ncontrast to $d=1$).\n  We propose a new variant of an \\emph{infinite} balls into bins process. Each\nround an expected number of $\\lambda n$ new balls arrive and are distributed\n(in parallel) to the bins. Each non-empty bin deletes one of its balls. This\nsetting models a set of servers processing incoming requests, where clients can\nquery a server's current load but receive no information about parallel\nrequests. We study the \\greedy{d} distribution scheme in this setting and show\na strong self-stabilizing property: For \\emph{any} arrival rate\n$\\lambda=\\lambda(n)<1$, the system load is time-invariant. Moreover, for\n\\emph{any} (even super-exponential) round $t$, the maximum system load is\n(w.h.p.) $O(\\frac{1}{1-\\lambda}\\cdot\\log\\frac{n}{1-\\lambda})$ for $d=1$ and\n$O(\\log\\frac{n}{1-\\lambda})$ for $d=2$. In particular, \\greedy{2} has an\nexponentially smaller system load for high arrival rates. \n\n"}
{"id": "1603.02297", "contents": "Title: TTC: A high-performance Compiler for Tensor Transpositions Abstract: We present TTC, an open-source parallel compiler for multidimensional tensor\ntranspositions. In order to generate high-performance C++ code, TTC explores a\nnumber of optimizations, including software prefetching, blocking,\nloop-reordering, and explicit vectorization. To evaluate the performance of\nmultidimensional transpositions across a range of possible use-cases, we also\nrelease a benchmark covering arbitrary transpositions of up to six dimensions.\nPerformance results show that the routines generated by TTC achieve close to\npeak memory bandwidth on both the Intel Haswell and the AMD Steamroller\narchitectures, and yield significant performance gains over modern compilers.\nBy implementing a set of pruning heuristics, TTC allows users to limit the\nnumber of potential solutions; this option is especially useful when dealing\nwith high-dimensional tensors, as the search space might become prohibitively\nlarge. Experiments indicate that when only 100 potential solutions are\nconsidered, the resulting performance is about 99% of that achieved with\nexhaustive search. \n\n"}
{"id": "1603.02580", "contents": "Title: On the limitations of analysing worst-case dynamic energy of processing Abstract: This paper examines dynamic energy consumption caused by data during software\nexecution on deeply embedded microprocessors, which can be significant on some\ndevices. In worst-case energy consumption analysis, energy models are used to\nfind the most costly execution path. Taking each instruction's worst case\nenergy produces a safe but overly pessimistic upper bound. Algorithms for safe\nand tight bounds would be desirable. We show that finding exact worst-case\nenergy is NP-hard, and that tight bounds cannot be approximated with guaranteed\nsafety. We conclude that any energy model targeting tightness must either\nsacrifice safety or accept overapproximation proportional to data-dependent\nenergy. \n\n"}
{"id": "1603.02655", "contents": "Title: Study and evaluation of an Irregular Graph Algorithm on Multicore and\n  GPU Processor Architectures Abstract: One area of Computing applications which poses significant challenge of\nperformance scalability on Chip Multiprocessors(CMP's) are Irregular\napplications. Such applications have very little computation and unpredictable\nmemory access patterns making them memory-bound in contrast to compute-bound\napplications. Since the gap between processor and memory performance continues\nto exist, difficulty to hide and decrease this gap is one of the important\nfactors which results in poor performance of these applications on CMP's.\n  The goal of this thesis is to overcome many such challenges posed during\nperformance acceleration of an irregular graph algorithm called Triad Census.\nWe accelerated the Triad Census algorithm on two significantly different Chip\nMultiprocessors: Dual-socket Intel Xeon Multicore (8 hardware threads per\nsocket) and 240-processor core NVIDIA Tesla C1060 GPGPU(128 hardware threads\nper core).\n  The experimental results obtained on Intel Multicore Xeon system shows\nperformance speedups (w.r.t baseline sequential) of maximum 56x , average 33x\nand minimum 8.3x for real world graph data sets. On NVIDIA Tesla C1060 GPGPU,\nwe were able to match almost equally the Multicore results - 58.4x maximum,\n32.8x average and 4.2x minimum speedups w.r.t baseline sequential. In terms of\nraw performance, for the graph data set called Patents network, our results on\nIntel Xeon Multicore(16 hw threads) were 1.27x times faster than previous\nresults on Cray XMT(16 hw threads) while results achieved on GPGPU were\ncomparatively slower(0.72x). To the best of our knowledge, this algorithm has\nonly been accelerated on supercomputer class computer named Cray XMT and no\nwork exists that demonstrates performance evaluation and comparison of this\nalgorithm on relatively lower-cost Multicore and GPGPU based platforms. \n\n"}
{"id": "1603.03657", "contents": "Title: Efficient forward propagation of time-sequences in convolutional neural\n  networks using Deep Shifting Abstract: When a Convolutional Neural Network is used for on-the-fly evaluation of\ncontinuously updating time-sequences, many redundant convolution operations are\nperformed. We propose the method of Deep Shifting, which remembers previously\ncalculated results of convolution operations in order to minimize the number of\ncalculations. The reduction in complexity is at least a constant and in the\nbest case quadratic. We demonstrate that this method does indeed save\nsignificant computation time in a practical implementation, especially when the\nnetworks receives a large number of time-frames. \n\n"}
{"id": "1603.04467", "contents": "Title: TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed\n  Systems Abstract: TensorFlow is an interface for expressing machine learning algorithms, and an\nimplementation for executing such algorithms. A computation expressed using\nTensorFlow can be executed with little or no change on a wide variety of\nheterogeneous systems, ranging from mobile devices such as phones and tablets\nup to large-scale distributed systems of hundreds of machines and thousands of\ncomputational devices such as GPU cards. The system is flexible and can be used\nto express a wide variety of algorithms, including training and inference\nalgorithms for deep neural network models, and it has been used for conducting\nresearch and for deploying machine learning systems into production across more\nthan a dozen areas of computer science and other fields, including speech\nrecognition, computer vision, robotics, information retrieval, natural language\nprocessing, geographic information extraction, and computational drug\ndiscovery. This paper describes the TensorFlow interface and an implementation\nof that interface that we have built at Google. The TensorFlow API and a\nreference implementation were released as an open-source package under the\nApache 2.0 license in November, 2015 and are available at www.tensorflow.org. \n\n"}
{"id": "1603.05163", "contents": "Title: Accelerating Data Regeneration for Distributed Storage Systems with\n  Heterogeneous Link Capacities Abstract: Distributed storage systems provide large-scale reliable data storage\nservices by spreading redundancy across a large group of storage nodes. In such\na large system, node failures take place on a regular basis. When a storage\nnode breaks down, a replacement node is expected to regenerate the redundant\ndata as soon as possible in order to maintain the same level of redundancy.\nPrevious results have been mainly focused on the minimization of network\ntraffic in regeneration. However, in practical networks, where link capacities\nvary in a wide range, minimizing network traffic does not always yield the\nminimum regeneration time. In this paper, we investigate two approaches to the\nproblem of minimizing regeneration time in networks with heterogeneous link\ncapacities. The first approach is to download different amounts of repair data\nfrom the helping nodes according to the link capacities. The second approach\ngeneralizes the conventional star-structured regeneration topology to\ntree-structured topologies so that we can utilize the links between helping\nnodes with bypassing low-capacity links. Simulation results show that the\nflexible tree-structured regeneration scheme that combines the advantages of\nboth approaches can achieve a substantial reduction in the regeneration time. \n\n"}
{"id": "1603.05752", "contents": "Title: Optimal Response to Burstable Billing under Demand Uncertainty Abstract: Burstable billing is widely adopted in practice, e.g., by colocation data\ncenter providers, to charge for their users, e.g., data centers, for data\ntransferring. However, there is still a lack of research on what the best way\nis for a user to manage its workload in response to burstable billing. To\novercome this shortcoming, we propose a novel method to optimally respond to\nburstable billing under demand uncertainty. First, we develop a tractable\nmathematical expression to calculate the 95th percentile usage of a user, who\nis charged by provider via burstable billing for bandwidth usage. This model is\nthen used to formulate a new bandwidth allocation problem to maximize the\nuser's surplus, i.e., its net utility minus cost. Additionally, we examine\ndifferent non-convex solution methods for the formulated stochastic\noptimization problem. We also extend our design to the case where a user can\nreceive service from multiple providers, who all employ burstable billing.\nUsing real-world workload traces, we show that our proposed method can reduce\nuser's bandwidth cost by 26% and increase its total surplus by 23%, compared to\nthe current practice of allocating bandwidth on-demand. \n\n"}
{"id": "1603.06127", "contents": "Title: Sentence Pair Scoring: Towards Unified Framework for Text Comprehension Abstract: We review the task of Sentence Pair Scoring, popular in the literature in\nvarious forms - viewed as Answer Sentence Selection, Semantic Text Scoring,\nNext Utterance Ranking, Recognizing Textual Entailment, Paraphrasing or e.g. a\ncomponent of Memory Networks.\n  We argue that all such tasks are similar from the model perspective and\npropose new baselines by comparing the performance of common IR metrics and\npopular convolutional, recurrent and attention-based neural models across many\nSentence Pair Scoring tasks and datasets. We discuss the problem of evaluating\nrandomized models, propose a statistically grounded methodology, and attempt to\nimprove comparisons by releasing new datasets that are much harder than some of\nthe currently used well explored benchmarks. We introduce a unified open source\nsoftware framework with easily pluggable models and tasks, which enables us to\nexperiment with multi-task reusability of trained sentence model. We set a new\nstate-of-art in performance on the Ubuntu Dialogue dataset. \n\n"}
{"id": "1603.06744", "contents": "Title: Latent Predictor Networks for Code Generation Abstract: Many language generation tasks require the production of text conditioned on\nboth structured and unstructured inputs. We present a novel neural network\narchitecture which generates an output sequence conditioned on an arbitrary\nnumber of input functions. Crucially, our approach allows both the choice of\nconditioning context and the granularity of generation, for example characters\nor tokens, to be marginalised, thus permitting scalable and effective training.\nUsing this framework, we address the problem of generating programming code\nfrom a mixed natural language and structured specification. We create two new\ndata sets for this paradigm derived from the collectible trading card games\nMagic the Gathering and Hearthstone. On these, and a third preexisting corpus,\nwe demonstrate that marginalising multiple predictors allows our model to\noutperform strong benchmarks. \n\n"}
{"id": "1603.07064", "contents": "Title: Big Data Spark Solution for Functional Magnetic Resonance Imaging Abstract: Recently, Big Data applications have rapidly expanded into different\nindustries. Healthcare is also one the industries willing to use big data\nplatforms so that some big data analytics tools have been adopted in this field\nto some extent. Medical imaging which is a pillar in diagnostic healthcare\ndeals with high volume of data collection and processing. A huge amount of 3D\nand 4D images are acquired in different forms and resolutions using a variety\nof medical imaging modalities. Preprocessing and analyzing imaging data is\ncurrently a long process and cost and time consuming. However, not many big\ndata platforms have been provided or redesigned for medical imaging purposes\nbecause of some restrictions such as data format. In this paper, we designed,\ndeveloped and successfully tested a new pipeline for medical imaging data\n(especially functional magnetic resonance imaging - fMRI) using Big Data Spark\n/ PySpark platform on a single node which allows us to read and load imaging\ndata, convert them to Resilient Distributed Datasets in order manipulate and\nperform in-memory data processing in parallel and convert final results to\nimaging format while the pipeline provides an option to store the results in\nother formats such as data frame. Using this new solution and pipeline, we\nrepeated our previous works in which we extracted brain networks from fMRI data\nusing template matching and sum of squared differences (SSD) method. The final\nresults revealed our Spark (PySpark) based solution improved the performance\n(in terms of processing time) around 4 times on a single compared to the\nprevious work developed in Python. \n\n"}
{"id": "1603.08029", "contents": "Title: Resnet in Resnet: Generalizing Residual Architectures Abstract: Residual networks (ResNets) have recently achieved state-of-the-art on\nchallenging computer vision tasks. We introduce Resnet in Resnet (RiR): a deep\ndual-stream architecture that generalizes ResNets and standard CNNs and is\neasily implemented with no computational overhead. RiR consistently improves\nperformance over ResNets, outperforms architectures with similar amounts of\naugmentation on CIFAR-10, and establishes a new state-of-the-art on CIFAR-100. \n\n"}
{"id": "1603.08262", "contents": "Title: Towards Machine Intelligence Abstract: There exists a theory of a single general-purpose learning algorithm which\ncould explain the principles of its operation. This theory assumes that the\nbrain has some initial rough architecture, a small library of simple innate\ncircuits which are prewired at birth and proposes that all significant mental\nalgorithms can be learned. Given current understanding and observations, this\npaper reviews and lists the ingredients of such an algorithm from both\narchitectural and functional perspectives. \n\n"}
{"id": "1603.08270", "contents": "Title: Convolutional Networks for Fast, Energy-Efficient Neuromorphic Computing Abstract: Deep networks are now able to achieve human-level performance on a broad\nspectrum of recognition tasks. Independently, neuromorphic computing has now\ndemonstrated unprecedented energy-efficiency through a new chip architecture\nbased on spiking neurons, low precision synapses, and a scalable communication\nnetwork. Here, we demonstrate that neuromorphic computing, despite its novel\narchitectural primitives, can implement deep convolution networks that i)\napproach state-of-the-art classification accuracy across 8 standard datasets,\nencompassing vision and speech, ii) perform inference while preserving the\nhardware's underlying energy-efficiency and high throughput, running on the\naforementioned datasets at between 1200 and 2600 frames per second and using\nbetween 25 and 275 mW (effectively > 6000 frames / sec / W) and iii) can be\nspecified and trained using backpropagation with the same ease-of-use as\ncontemporary deep learning. For the first time, the algorithmic power of deep\nlearning can be merged with the efficiency of neuromorphic processors, bringing\nthe promise of embedded, intelligent, brain-inspired computing one step closer. \n\n"}
{"id": "1603.08393", "contents": "Title: $k$-shot Broadcasting in Ad Hoc Radio Networks Abstract: We study distributed broadcasting protocols with few transmissions (`shots')\nin radio networks where the topology is unknown. In particular, we examine the\ncase in which a bound $k$ is given and a node may transmit at most $k$ times\nduring the broadcasting protocol. Initially, we focus on oblivious algorithms\nfor $k$-shot broadcasting, that is, algorithms where each node decides whether\nto transmit or not with no consideration of the transmission history. Our main\ncontributions are (a) a lower bound of $\\Omega(n^2/k)$ on the broadcasting time\nof any oblivious $k$-shot broadcasting algorithm and (b) an oblivious\nbroadcasting protocol that achieves a matching upper bound, namely $O(n^2/k)$,\nfor every $k \\le \\sqrt{n}$ and an upper bound of $O(n^{3/2})$ for every $k >\n\\sqrt{n}$. We also study the general case of adaptive broadcasting protocols\nwhere nodes decide whether to transmit based on all the available information,\nnamely the transmission history known by each. We prove a lower bound of\n$\\Omega\\left(n^{\\frac{1+k}{k}}\\right)$ on the broadcasting time of any protocol\nby introducing the \\emph{transmission tree} construction which generalizes\nprevious approaches. \n\n"}
{"id": "1603.09382", "contents": "Title: Deep Networks with Stochastic Depth Abstract: Very deep convolutional networks with hundreds of layers have led to\nsignificant reductions in error on competitive benchmarks. Although the\nunmatched expressiveness of the many layers can be highly desirable at test\ntime, training very deep networks comes with its own set of challenges. The\ngradients can vanish, the forward flow often diminishes, and the training time\ncan be painfully slow. To address these problems, we propose stochastic depth,\na training procedure that enables the seemingly contradictory setup to train\nshort networks and use deep networks at test time. We start with very deep\nnetworks but during training, for each mini-batch, randomly drop a subset of\nlayers and bypass them with the identity function. This simple approach\ncomplements the recent success of residual networks. It reduces training time\nsubstantially and improves the test error significantly on almost all data sets\nthat we used for evaluation. With stochastic depth we can increase the depth of\nresidual networks even beyond 1200 layers and still yield meaningful\nimprovements in test error (4.91% on CIFAR-10). \n\n"}
{"id": "1604.00317", "contents": "Title: A Semisupervised Approach for Language Identification based on Ladder\n  Networks Abstract: In this study we address the problem of training a neuralnetwork for language\nidentification using both labeled and unlabeled speech samples in the form of\ni-vectors. We propose a neural network architecture that can also handle\nout-of-set languages. We utilize a modified version of the recently proposed\nLadder Network semisupervised training procedure that optimizes the\nreconstruction costs of a stack of denoising autoencoders. We show that this\napproach can be successfully applied to the case where the training dataset is\ncomposed of both labeled and unlabeled acoustic data. The results show enhanced\nlanguage identification on the NIST 2015 language identification dataset. \n\n"}
{"id": "1604.00562", "contents": "Title: Reasoning About Pragmatics with Neural Listeners and Speakers Abstract: We present a model for pragmatically describing scenes, in which contrastive\nbehavior results from a combination of inference-driven pragmatics and learned\nsemantics. Like previous learned approaches to language generation, our model\nuses a simple feature-driven architecture (here a pair of neural \"listener\" and\n\"speaker\" models) to ground language in the world. Like inference-driven\napproaches to pragmatics, our model actively reasons about listener behavior\nwhen selecting utterances. For training, our approach requires only ordinary\ncaptions, annotated _without_ demonstration of the pragmatic behavior the model\nultimately exhibits. In human evaluations on a referring expression game, our\napproach succeeds 81% of the time, compared to a 69% success rate using\nexisting techniques. \n\n"}
{"id": "1604.01662", "contents": "Title: A Survey on Bayesian Deep Learning Abstract: A comprehensive artificial intelligence system needs to not only perceive the\nenvironment with different `senses' (e.g., seeing and hearing) but also infer\nthe world's conditional (or even causal) relations and corresponding\nuncertainty. The past decade has seen major advances in many perception tasks\nsuch as visual object recognition and speech recognition using deep learning\nmodels. For higher-level inference, however, probabilistic graphical models\nwith their Bayesian nature are still more powerful and flexible. In recent\nyears, Bayesian deep learning has emerged as a unified probabilistic framework\nto tightly integrate deep learning and Bayesian models. In this general\nframework, the perception of text or images using deep learning can boost the\nperformance of higher-level inference and in turn, the feedback from the\ninference process is able to enhance the perception of text or images. This\nsurvey provides a comprehensive introduction to Bayesian deep learning and\nreviews its recent applications on recommender systems, topic models, control,\netc. Besides, we also discuss the relationship and differences between Bayesian\ndeep learning and other related topics such as Bayesian treatment of neural\nnetworks. For a constantly updating project page, please refer to\nhttps://github.com/js05212/BayesianDeepLearning-Survey. \n\n"}
{"id": "1604.01952", "contents": "Title: Deep Online Convex Optimization with Gated Games Abstract: Methods from convex optimization are widely used as building blocks for deep\nlearning algorithms. However, the reasons for their empirical success are\nunclear, since modern convolutional networks (convnets), incorporating\nrectifier units and max-pooling, are neither smooth nor convex. Standard\nguarantees therefore do not apply. This paper provides the first convergence\nrates for gradient descent on rectifier convnets. The proof utilizes the\nparticular structure of rectifier networks which consists in binary\nactive/inactive gates applied on top of an underlying linear network. The\napproach generalizes to max-pooling, dropout and maxout. In other words, to\nprecisely the neural networks that perform best empirically. The key step is to\nintroduce gated games, an extension of convex games with similar convergence\nproperties that capture the gating function of rectifiers. The main result is\nthat rectifier convnets converge to a critical point at a rate controlled by\nthe gated-regret of the units in the network. Corollaries of the main result\ninclude: (i) a game-theoretic description of the representations learned by a\nneural network; (ii) a logarithmic-regret algorithm for training neural nets;\nand (iii) a formal setting for analyzing conditional computation in neural nets\nthat can be applied to recently developed models of attention. \n\n"}
{"id": "1604.02475", "contents": "Title: Performance Limits for Noisy Multi-Measurement Vector Problems Abstract: Compressed sensing (CS) demonstrates that sparse signals can be estimated\nfrom under-determined linear systems. Distributed CS (DCS) further reduces the\nnumber of measurements by considering joint sparsity within signal ensembles.\nDCS with jointly sparse signals has applications in multi-sensor acoustic\nsensing, magnetic resonance imaging with multiple coils, remote sensing, and\narray signal processing. Multi-measurement vector (MMV) problems consider the\nestimation of jointly sparse signals under the DCS framework. Two related MMV\nsettings are studied. In the first setting, each signal vector is measured by a\ndifferent independent and identically distributed (i.i.d.) measurement matrix,\nwhile in the second setting, all signal vectors are measured by the same i.i.d.\nmatrix. Replica analysis is performed for these two MMV settings, and the\nminimum mean squared error (MMSE), which turns out to be identical for both\nsettings, is obtained as a function of the noise variance and number of\nmeasurements. To showcase the application of MMV models, the MMSE's of complex\nCS problems with both real and complex measurement matrices are also analyzed.\nMultiple performance regions for MMV are identified where the MMSE behaves\ndifferently as a function of the noise variance and the number of measurements.\n  Belief propagation (BP) is a CS signal estimation framework that often\nachieves the MMSE asymptotically. A phase transition for BP is identified. This\nphase transition, verified by numerical results, separates the regions where BP\nachieves the MMSE and where it is suboptimal. Numerical results also illustrate\nthat more signal vectors in the jointly sparse signal ensemble lead to a better\nphase transition. \n\n"}
{"id": "1604.02646", "contents": "Title: Visualization Regularizers for Neural Network based Image Recognition Abstract: The success of deep neural networks is mostly due their ability to learn\nmeaningful features from the data. Features learned in the hidden layers of\ndeep neural networks trained in computer vision tasks have been shown to be\nsimilar to mid-level vision features. We leverage this fact in this work and\npropose the visualization regularizer for image tasks. The proposed\nregularization technique enforces smoothness of the features learned by hidden\nnodes and turns out to be a special case of Tikhonov regularization. We achieve\nhigher classification accuracy as compared to existing regularizers such as the\nL2 norm regularizer and dropout, on benchmark datasets without changing the\ntraining computational complexity. \n\n"}
{"id": "1604.02929", "contents": "Title: Solving Optimization Problems by the Public Goods Game Abstract: We introduce a method based on the Public Goods Game for solving optimization\ntasks. In particular, we focus on the Traveling Salesman Problem, i.e. a\nNP-hard problem whose search space exponentially grows increasing the number of\ncities. The proposed method considers a population whose agents are provided\nwith a random solution to the given problem. In doing so, agents interact by\nplaying the Public Goods Game using the fitness of their solution as currency\nof the game. Notably, agents with better solutions provide higher\ncontributions, while those with lower ones tend to imitate the solution of\nricher agents for increasing their fitness. Numerical simulations show that the\nproposed method allows to compute exact solutions, and suboptimal ones, in the\nconsidered search spaces. As result, beyond to propose a new heuristic for\ncombinatorial optimization problems, our work aims to highlight the\npotentiality of evolutionary game theory beyond its current horizons. \n\n"}
{"id": "1604.03687", "contents": "Title: Democratic, Existential, and Consensus-Based Output Conventions in\n  Stable Computation by Chemical Reaction Networks Abstract: We show that some natural output conventions for error-free computation in\nchemical reaction networks (CRN) lead to a common level of computational\nexpressivity. Our main results are that the standard consensus-based output\nconvention have equivalent computational power to (1) existence-based and (2)\ndemocracy-based output conventions. The CRNs using the former output convention\nhave only \"yes\" voters, with the interpretation that the CRN's output is yes if\nany voters are present and no otherwise. The CRNs using the latter output\nconvention define output by majority vote among \"yes\" and \"no\" voters.\n  Both results are proven via a generalized framework that simultaneously\ncaptures several definitions, directly inspired by a Petri net result of\nEsparza, Ganty, Leroux, and Majumder [CONCUR 2015]. These results support the\nthesis that the computational expressivity of error-free CRNs is intrinsic, not\nsensitive to arbitrary definitional choices. \n\n"}
{"id": "1604.04125", "contents": "Title: Filling in the details: Perceiving from low fidelity images Abstract: Humans perceive their surroundings in great detail even though most of our\nvisual field is reduced to low-fidelity color-deprived (e.g. dichromatic) input\nby the retina. In contrast, most deep learning architectures are\ncomputationally wasteful in that they consider every part of the input when\nperforming an image processing task. Yet, the human visual system is able to\nperform visual reasoning despite having only a small fovea of high visual\nacuity. With this in mind, we wish to understand the extent to which\nconnectionist architectures are able to learn from and reason with low acuity,\ndistorted inputs. Specifically, we train autoencoders to generate full-detail\nimages from low-detail \"foveations\" of those images and then measure their\nability to reconstruct the full-detail images from the foveated versions. By\nvarying the type of foveation, we can study how well the architectures can cope\nwith various types of distortion. We find that the autoencoder compensates for\nlower detail by learning increasingly global feature functions. In many cases,\nthe learnt features are suitable for reconstructing the original full-detail\nimage. For example, we find that the networks accurately perceive color in the\nperiphery, even when 75\\% of the input is achromatic. \n\n"}
{"id": "1604.04661", "contents": "Title: Parallelizing Word2Vec in Shared and Distributed Memory Abstract: Word2Vec is a widely used algorithm for extracting low-dimensional vector\nrepresentations of words. It generated considerable excitement in the machine\nlearning and natural language processing (NLP) communities recently due to its\nexceptional performance in many NLP applications such as named entity\nrecognition, sentiment analysis, machine translation and question answering.\nState-of-the-art algorithms including those by Mikolov et al. have been\nparallelized for multi-core CPU architectures but are based on vector-vector\noperations that are memory-bandwidth intensive and do not efficiently use\ncomputational resources. In this paper, we improve reuse of various data\nstructures in the algorithm through the use of minibatching, hence allowing us\nto express the problem using matrix multiply operations. We also explore\ndifferent techniques to distribute word2vec computation across nodes in a\ncompute cluster, and demonstrate good strong scalability up to 32 nodes. In\ncombination, these techniques allow us to scale up the computation near\nlinearly across cores and nodes, and process hundreds of millions of words per\nsecond, which is the fastest word2vec implementation to the best of our\nknowledge. \n\n"}
{"id": "1604.04767", "contents": "Title: Efficient Dictionary Learning with Sparseness-Enforcing Projections Abstract: Learning dictionaries suitable for sparse coding instead of using engineered\nbases has proven effective in a variety of image processing tasks. This paper\nstudies the optimization of dictionaries on image data where the representation\nis enforced to be explicitly sparse with respect to a smooth, normalized\nsparseness measure. This involves the computation of Euclidean projections onto\nlevel sets of the sparseness measure. While previous algorithms for this\noptimization problem had at least quasi-linear time complexity, here the first\nalgorithm with linear time complexity and constant space complexity is\nproposed. The key for this is the mathematically rigorous derivation of a\ncharacterization of the projection's result based on a soft-shrinkage function.\nThis theory is applied in an original algorithm called Easy Dictionary Learning\n(EZDL), which learns dictionaries with a simple and fast-to-compute\nHebbian-like learning rule. The new algorithm is efficient, expressive and\nparticularly simple to implement. It is demonstrated that despite its\nsimplicity, the proposed learning algorithm is able to generate a rich variety\nof dictionaries, in particular a topographic organization of atoms or separable\natoms. Further, the dictionaries are as expressive as those of benchmark\nlearning algorithms in terms of the reproduction quality on entire images, and\nresult in an equivalent denoising performance. EZDL learns approximately 30 %\nfaster than the already very efficient Online Dictionary Learning algorithm,\nand is therefore eligible for rapid data set analysis and problems with vast\nquantities of learning samples. \n\n"}
{"id": "1604.05000", "contents": "Title: LSTM-CF: Unifying Context Modeling and Fusion with LSTMs for RGB-D Scene\n  Labeling Abstract: Semantic labeling of RGB-D scenes is crucial to many intelligent applications\nincluding perceptual robotics. It generates pixelwise and fine-grained label\nmaps from simultaneously sensed photometric (RGB) and depth channels. This\npaper addresses this problem by i) developing a novel Long Short-Term Memorized\nContext Fusion (LSTM-CF) Model that captures and fuses contextual information\nfrom multiple channels of photometric and depth data, and ii) incorporating\nthis model into deep convolutional neural networks (CNNs) for end-to-end\ntraining. Specifically, contexts in photometric and depth channels are,\nrespectively, captured by stacking several convolutional layers and a long\nshort-term memory layer; the memory layer encodes both short-range and\nlong-range spatial dependencies in an image along the vertical direction.\nAnother long short-term memorized fusion layer is set up to integrate the\ncontexts along the vertical direction from different channels, and perform\nbi-directional propagation of the fused vertical contexts along the horizontal\ndirection to obtain true 2D global contexts. At last, the fused contextual\nrepresentation is concatenated with the convolutional features extracted from\nthe photometric channels in order to improve the accuracy of fine-scale\nsemantic labeling. Our proposed model has set a new state of the art, i.e.,\n48.1% and 49.4% average class accuracy over 37 categories (2.2% and 5.4%\nimprovement) on the large-scale SUNRGBD dataset and the NYUDv2dataset,\nrespectively. \n\n"}
{"id": "1604.06853", "contents": "Title: Adaptive Content-based Routing using Subscription Subgrouping in\n  Structured Overlays Abstract: Cyclic or general overlays may provide multiple paths between publishers and\nsubscribers. However, an advertisement tree and a matching subscription\nactivates only one path for notifications routing in publish/subscribe systems.\nThis poses serious challenges in handling network conditions like congestion,\nand link or broker failures. Further, content-based dynamic routing of\nnotifications requires instantaneous updates in routing paths, which is not a\nscalable option. This paper introduces a clustering approach with a bit-vector\ntechnique for inter-cluster dynamic routing of notifications in a structured\ncyclic topology that provides multiple paths between publishers and interested\nsubscribers. The advertisement forwarding process exploits the structured\nnature of the overlay topology to generate advertisement trees of length 1\nwithout generating duplicate messages in the advertisement forwarding process.\nIssued subscriptions are divided into multiple disjoint subgropus, where each\nsubscription is broadcast to a cluster, which is a limited part of the\nstructured cyclic overlay network. We implemented novel static and\nintra-cluster dynamic routing algorithms in the proposed overlay topology for\nour advertisement-based publish/subscribe system, called OctopiA. We also\nperformed a pragmatic comparison of our two algorithms with the\nstate-of-the-art. Experiments on a cluster testbed show that our approach\ngenerates fewer inter-broker messages, and is scalable. \n\n"}
{"id": "1604.07187", "contents": "Title: How Many Cooks Spoil the Soup? Abstract: In this work, we study the following basic question: \"How much parallelism\ndoes a distributed task permit?\" Our definition of parallelism (or symmetry)\nhere is not in terms of speed, but in terms of identical roles that processes\nhave at the same time in the execution. We initiate this study in population\nprotocols, a very simple model that not only allows for a straightforward\ndefinition of what a role is, but also encloses the challenge of isolating the\nproperties that are due to the protocol from those that are due to the\nadversary scheduler, who controls the interactions between the processes. We\n(i) give a partial characterization of the set of predicates on input\nassignments that can be stably computed with maximum symmetry, i.e.,\n$\\Theta(N_{min})$, where $N_{min}$ is the minimum multiplicity of a state in\nthe initial configuration, and (ii) we turn our attention to the remaining\npredicates and prove a strong impossibility result for the parity predicate:\nthe inherent symmetry of any protocol that stably computes it is upper bounded\nby a constant that depends on the size of the protocol. \n\n"}
{"id": "1604.07564", "contents": "Title: A Retraction Theorem for Distributed Synthesis Abstract: We present a general theorem for distributed synthesis problems in\ncoordination games with $\\omega$-regular objectives of the form: If there\nexists a winning strategy for the coalition, then there exists an \"essential\"\nwinning strategy, that is obtained by a retraction of the given one. In\ngeneral, this does not lead to finite-state winning strategies, but when the\nknowledge of agents remains bounded, we can solve the synthesis problem. Our\nstudy is carried out in a setting where objectives are expressed in terms of\nevents that may \\emph{not} be observable. This is natural in games of imperfect\ninformation, rather than the common assumption that objectives are expressed in\nterms of events that are observable to all agents. We characterise decidable\ndistributed synthesis problems in terms of finiteness of knowledge states and\nfinite congruence classes induced by them. \n\n"}
{"id": "1604.08201", "contents": "Title: Interpretable Deep Neural Networks for Single-Trial EEG Classification Abstract: Background: In cognitive neuroscience the potential of Deep Neural Networks\n(DNNs) for solving complex classification tasks is yet to be fully exploited.\nThe most limiting factor is that DNNs as notorious 'black boxes' do not provide\ninsight into neurophysiological phenomena underlying a decision. Layer-wise\nRelevance Propagation (LRP) has been introduced as a novel method to explain\nindividual network decisions. New Method: We propose the application of DNNs\nwith LRP for the first time for EEG data analysis. Through LRP the single-trial\nDNN decisions are transformed into heatmaps indicating each data point's\nrelevance for the outcome of the decision. Results: DNN achieves classification\naccuracies comparable to those of CSP-LDA. In subjects with low performance\nsubject-to-subject transfer of trained DNNs can improve the results. The\nsingle-trial LRP heatmaps reveal neurophysiologically plausible patterns,\nresembling CSP-derived scalp maps. Critically, while CSP patterns represent\nclass-wise aggregated information, LRP heatmaps pinpoint neural patterns to\nsingle time points in single trials. Comparison with Existing Method(s): We\ncompare the classification performance of DNNs to that of linear CSP-LDA on two\ndata sets related to motor-imaginery BCI. Conclusion: We have demonstrated that\nDNN is a powerful non-linear tool for EEG analysis. With LRP a new quality of\nhigh-resolution assessment of neural activity can be reached. LRP is a\npotential remedy for the lack of interpretability of DNNs that has limited\ntheir utility in neuroscientific applications. The extreme specificity of the\nLRP-derived heatmaps opens up new avenues for investigating neural activity\nunderlying complex perception or decision-related processes. \n\n"}
{"id": "1604.08220", "contents": "Title: Diving deeper into mentee networks Abstract: Modern computer vision is all about the possession of powerful image\nrepresentations. Deeper and deeper convolutional neural networks have been\nbuilt using larger and larger datasets and are made publicly available. A large\nswath of computer vision scientists use these pre-trained networks with varying\ndegrees of successes in various tasks. Even though there is tremendous success\nin copying these networks, the representational space is not learnt from the\ntarget dataset in a traditional manner. One of the reasons for opting to use a\npre-trained network over a network learnt from scratch is that small datasets\nprovide less supervision and require meticulous regularization, smaller and\ncareful tweaking of learning rates to even achieve stable learning without\nweight explosion. It is often the case that large deep networks are not\nportable, which necessitates the ability to learn mid-sized networks from\nscratch.\n  In this article, we dive deeper into training these mid-sized networks on\nsmall datasets from scratch by drawing additional supervision from a large\npre-trained network. Such learning also provides better generalization\naccuracies than networks trained with common regularization techniques such as\nl2, l1 and dropouts. We show that features learnt thus, are more general than\nthose learnt independently. We studied various characteristics of such networks\nand found some interesting behaviors. \n\n"}
{"id": "1604.08484", "contents": "Title: Architectural Impact on Performance of In-memory Data Analytics: Apache\n  Spark Case Study Abstract: While cluster computing frameworks are continuously evolving to provide\nreal-time data analysis capabilities, Apache Spark has managed to be at the\nforefront of big data analytics for being a unified framework for both, batch\nand stream data processing. However, recent studies on micro-architectural\ncharacterization of in-memory data analytics are limited to only batch\nprocessing workloads. We compare micro-architectural performance of batch\nprocessing and stream processing workloads in Apache Spark using hardware\nperformance counters on a dual socket server. In our evaluation experiments, we\nhave found that batch processing are stream processing workloads have similar\nmicro-architectural characteristics and are bounded by the latency of frequent\ndata access to DRAM. For data accesses we have found that simultaneous\nmulti-threading is effective in hiding the data latencies. We have also\nobserved that (i) data locality on NUMA nodes can improve the performance by\n10% on average and(ii) disabling next-line L1-D prefetchers can reduce the\nexecution time by up-to 14\\% and (iii) multiple small executors can provide\nup-to 36\\% speedup over single large executor. \n\n"}
{"id": "1604.08618", "contents": "Title: Stringer: Balancing Latency and Resource Usage in Service Function Chain\n  Provisioning Abstract: Network Functions Virtualization, or NFV, enables telecommunications\ninfrastructure providers to replace special-purpose networking equipment with\ncommodity servers running virtualized network functions (VNFs). A service\nprovider utilizing NFV technology faces the SFC provisioning problem of\nassigning VNF instances to nodes in the physical infrastructure (e.g., a\ndatacenter), and routing Service Function Chains (sequences of functions\nrequired by customers, a.k.a. SFCs) in the physical network. In doing so, the\nprovider must balance between various competing goals of performance and\nresource usage. We present an approach for SFC provisioning, consisting of\nthree elements. The first element is a fast, scalable round-robin heuristic.\nThe second element is a Mixed Integer Programming (MIP) based approach. The\nthird element is a queueing-theoretic model to estimate the average latency\nassociated with any SFC provisioning solution. Combined, these elements create\nan approach that generates a set of SFC provisioning solutions, reflecting\ndifferent tradeoffs between resource usage and performance. \n\n"}
{"id": "1605.01361", "contents": "Title: The Optimal Pessimistic Transactional Memory Algorithm Abstract: Transactional Memory (TM) is an approach aiming to simplify concurrent\nprogramming by automating synchronization while maintaining efficiency. TM\nusually employs the optimistic concurrency control approach, which relies on\ntransactions aborting and restarting if conflicts occur. However, an aborted\ntransaction can still leave some effects in the system that cannot be cleaned\nup, if irrevocable operations are present within its code. The pessimistic\napproach eliminates that problem, since it relies on deferring operations in\ncase of conflict rather than aborting, but hitherto pessimistic TMs suffered\nfrom low parallelism due to the need of serializing transactions. In this\npaper, we aim to introduce OptSVA, a pessimistic TM concurrency control\nalgorithm that ensures a high level of parallelism through a battery of\nfar-reaching optimizations including early release, asynchronous execution, and\nthe extensive use of buffering. \n\n"}
{"id": "1605.01369", "contents": "Title: Accelerating Deep Learning with Shrinkage and Recall Abstract: Deep Learning is a very powerful machine learning model. Deep Learning trains\na large number of parameters for multiple layers and is very slow when data is\nin large scale and the architecture size is large. Inspired from the shrinking\ntechnique used in accelerating computation of Support Vector Machines (SVM)\nalgorithm and screening technique used in LASSO, we propose a shrinking Deep\nLearning with recall (sDLr) approach to speed up deep learning computation. We\nexperiment shrinking Deep Learning with recall (sDLr) using Deep Neural Network\n(DNN), Deep Belief Network (DBN) and Convolution Neural Network (CNN) on 4 data\nsets. Results show that the speedup using shrinking Deep Learning with recall\n(sDLr) can reach more than 2.0 while still giving competitive classification\nperformance. \n\n"}
{"id": "1605.01713", "contents": "Title: Not Just a Black Box: Learning Important Features Through Propagating\n  Activation Differences Abstract: Note: This paper describes an older version of DeepLIFT. See\nhttps://arxiv.org/abs/1704.02685 for the newer version. Original abstract\nfollows: The purported \"black box\" nature of neural networks is a barrier to\nadoption in applications where interpretability is essential. Here we present\nDeepLIFT (Learning Important FeaTures), an efficient and effective method for\ncomputing importance scores in a neural network. DeepLIFT compares the\nactivation of each neuron to its 'reference activation' and assigns\ncontribution scores according to the difference. We apply DeepLIFT to models\ntrained on natural images and genomic data, and show significant advantages\nover gradient-based methods. \n\n"}
{"id": "1605.02766", "contents": "Title: LightNet: A Versatile, Standalone Matlab-based Environment for Deep\n  Learning Abstract: LightNet is a lightweight, versatile and purely Matlab-based deep learning\nframework. The idea underlying its design is to provide an easy-to-understand,\neasy-to-use and efficient computational platform for deep learning research.\nThe implemented framework supports major deep learning architectures such as\nMultilayer Perceptron Networks (MLP), Convolutional Neural Networks (CNN) and\nRecurrent Neural Networks (RNN). The framework also supports both CPU and GPU\ncomputation, and the switch between them is straightforward. Different\napplications in computer vision, natural language processing and robotics are\ndemonstrated as experiments. \n\n"}
{"id": "1605.03719", "contents": "Title: Distributed Testing of Excluded Subgraphs Abstract: We study property testing in the context of distributed computing, under the\nclassical CONGEST model. It is known that testing whether a graph is\ntriangle-free can be done in a constant number of rounds, where the constant\ndepends on how far the input graph is from being triangle-free. We show that,\nfor every connected 4-node graph H, testing whether a graph is H-free can be\ndone in a constant number of rounds too. The constant also depends on how far\nthe input graph is from being H-free, and the dependence is identical to the\none in the case of testing triangles. Hence, in particular, testing whether a\ngraph is K_4-free, and testing whether a graph is C_4-free can be done in a\nconstant number of rounds (where K_k denotes the k-node clique, and C_k denotes\nthe k-node cycle). On the other hand, we show that testing K_k-freeness and\nC_k-freeness for k>4 appear to be much harder. Specifically, we investigate two\nnatural types of generic algorithms for testing H-freeness, called DFS tester\nand BFS tester. The latter captures the previously known algorithm to test the\npresence of triangles, while the former captures our generic algorithm to test\nthe presence of a 4-node graph pattern H. We prove that both DFS and BFS\ntesters fail to test K_k-freeness and C_k-freeness in a constant number of\nrounds for k>4. \n\n"}
{"id": "1605.05219", "contents": "Title: Parallel Evaluation of Multi-Semi-Joins Abstract: While services such as Amazon AWS make computing power abundantly available,\nadding more computing nodes can incur high costs in, for instance,\npay-as-you-go plans while not always significantly improving the net running\ntime (aka wall-clock time) of queries. In this work, we provide algorithms for\nparallel evaluation of SGF queries in MapReduce that optimize total time, while\nretaining low net time. Not only can SGF queries specify all semi-join\nreducers, but also more expressive queries involving disjunction and negation.\nSince SGF queries can be seen as Boolean combinations of (potentially nested)\nsemi-joins, we introduce a novel multi-semi-join (MSJ) MapReduce operator that\nenables the evaluation of a set of semi-joins in one job. We use this operator\nto obtain parallel query plans for SGF queries that outvalue sequential plans\nw.r.t. net time and provide additional optimizations aimed at minimizing total\ntime without severely affecting net time. Even though the latter optimizations\nare NP-hard, we present effective greedy algorithms. Our experiments, conducted\nusing our own implementation Gumbo on top of Hadoop, confirm the usefulness of\nparallel query plans, and the effectiveness and scalability of our\noptimizations, all with a significant improvement over Pig and Hive. \n\n"}
{"id": "1605.05368", "contents": "Title: Deep Action Sequence Learning for Causal Shape Transformation Abstract: Deep learning became the method of choice in recent year for solving a wide\nvariety of predictive analytics tasks. For sequence prediction, recurrent\nneural networks (RNN) are often the go-to architecture for exploiting\nsequential information where the output is dependent on previous computation.\nHowever, the dependencies of the computation lie in the latent domain which may\nnot be suitable for certain applications involving the prediction of a\nstep-wise transformation sequence that is dependent on the previous computation\nonly in the visible domain. We propose that a hybrid architecture of\nconvolution neural networks (CNN) and stacked autoencoders (SAE) is sufficient\nto learn a sequence of actions that nonlinearly transforms an input shape or\ndistribution into a target shape or distribution with the same support. While\nsuch a framework can be useful in a variety of problems such as robotic path\nplanning, sequential decision-making in games, and identifying material\nprocessing pathways to achieve desired microstructures, the application of the\nframework is exemplified by the control of fluid deformations in a microfluidic\nchannel by deliberately placing a sequence of pillars. Learning of a multistep\ntopological transform has significant implications for rapid advances in\nmaterial science and biomedical applications. \n\n"}
{"id": "1605.05717", "contents": "Title: RADON: Repairable Atomic Data Object in Networks Abstract: Erasure codes offer an efficient way to decrease storage and communication\ncosts while implementing atomic memory service in asynchronous distributed\nstorage systems. In this paper, we provide erasure-code-based algorithms having\nthe additional ability to perform background repair of crashed nodes. A repair\noperation of a node in the crashed state is triggered externally, and is\ncarried out by the concerned node via message exchanges with other active nodes\nin the system. Upon completion of repair, the node re-enters active state, and\nresumes participation in ongoing and future read, write, and repair operations.\nTo guarantee liveness and atomicity simultaneously, existing works assume\neither the presence of nodes with stable storage, or presence of nodes that\nnever crash during the execution. We demand neither of these; instead we\nconsider a natural, yet practical network stability condition $N1$ that only\nrestricts the number of nodes in the crashed/repair state during broadcast of\nany message.\n  We present an erasure-code based algorithm $RADON_C$ that is always live, and\nguarantees atomicity as long as condition $N1$ holds. In situations when the\nnumber of concurrent writes is limited, $RADON_C$ has significantly improved\nstorage and communication cost over a replication-based algorithm $RADON_R$,\nwhich also works under $N1$. We further show how a slightly stronger network\nstability condition $N2$ can be used to construct algorithms that never violate\natomicity. The guarantee of atomicity comes at the expense of having an\nadditional phase during the read and write operations. \n\n"}
{"id": "1605.06170", "contents": "Title: Evaluation System for a Bayesian Optimization Service Abstract: Bayesian optimization is an elegant solution to the hyperparameter\noptimization problem in machine learning. Building a reliable and robust\nBayesian optimization service requires careful testing methodology and sound\nstatistical analysis. In this talk we will outline our development of an\nevaluation framework to rigorously test and measure the impact of changes to\nthe SigOpt optimization service. We present an overview of our evaluation\nsystem and discuss how this framework empowers our research engineers to\nconfidently and quickly make changes to our core optimization engine \n\n"}
{"id": "1605.06402", "contents": "Title: Ristretto: Hardware-Oriented Approximation of Convolutional Neural\n  Networks Abstract: Convolutional neural networks (CNN) have achieved major breakthroughs in\nrecent years. Their performance in computer vision have matched and in some\nareas even surpassed human capabilities. Deep neural networks can capture\ncomplex non-linear features; however this ability comes at the cost of high\ncomputational and memory requirements. State-of-art networks require billions\nof arithmetic operations and millions of parameters. To enable embedded devices\nsuch as smartphones, Google glasses and monitoring cameras with the astonishing\npower of deep learning, dedicated hardware accelerators can be used to decrease\nboth execution time and power consumption. In applications where fast\nconnection to the cloud is not guaranteed or where privacy is important,\ncomputation needs to be done locally. Many hardware accelerators for deep\nneural networks have been proposed recently. A first important step of\naccelerator design is hardware-oriented approximation of deep networks, which\nenables energy-efficient inference. We present Ristretto, a fast and automated\nframework for CNN approximation. Ristretto simulates the hardware arithmetic of\na custom hardware accelerator. The framework reduces the bit-width of network\nparameters and outputs of resource-intense layers, which reduces the chip area\nfor multiplication units significantly. Alternatively, Ristretto can remove the\nneed for multipliers altogether, resulting in an adder-only arithmetic. The\ntool fine-tunes trimmed networks to achieve high classification accuracy. Since\ntraining of deep neural networks can be time-consuming, Ristretto uses highly\noptimized routines which run on the GPU. This enables fast compression of any\ngiven network. Given a maximum tolerance of 1%, Ristretto can successfully\ncondense CaffeNet and SqueezeNet to 8-bit. The code for Ristretto is available. \n\n"}
{"id": "1605.06486", "contents": "Title: Using Read-$k$ Inequalities to Analyze a Distributed MIS Algorithm Abstract: Until recently, the fastest distributed MIS algorithm, even for simple\ngraphs, e.g., unoriented trees has been the simple randomized algorithm\ndiscovered the 80s. This algorithm (commonly called Luby's algorithm) computes\nan MIS in $O(\\log n)$ rounds (with high probability). This situation changed\nwhen Lenzen and Wattenhofer (PODC 2011) presented a randomized $O(\\sqrt{\\log\nn}\\cdot \\log\\log n)$-round MIS algorithm for unoriented trees. This algorithm\nwas improved by Barenboim et al. (FOCS 2012), resulting in an $O(\\sqrt{\\log n\n\\cdot \\log\\log n})$-round MIS algorithm.\n  The analyses of these tree MIS algorithms depends on \"near independence\" of\nprobabilistic events, a feature of the tree structure of the network. In their\npaper, Lenzen and Wattenhofer hope that their algorithm and analysis could be\nextended to graphs with bounded arboricity. We show how to do this. By using a\nnew tail inequality for read-k families of random variables due to Gavinsky et\nal. (Random Struct Algorithms, 2015), we show how to deal with dependencies\ninduced by the recent tree MIS algorithms when they are executed on bounded\narboricity graphs. Specifically, we analyze a version of the tree MIS algorithm\nof Barenboim et al. and show that it runs in $O(\\mbox{poly}(\\alpha) \\cdot\n\\sqrt{\\log n \\cdot \\log\\log n})$ rounds in the $\\mathcal{CONGEST}$ model for\ngraphs with arboricity $\\alpha$.\n  While the main thrust of this paper is the new probabilistic analysis via\nread-$k$ inequalities, for small values of $\\alpha$, this algorithm is faster\nthan the bounded arboricity MIS algorithm of Barenboim et al. We also note that\nrecently (SODA 2016), Gaffari presented a novel MIS algorithm for general\ngraphs that runs in $O(\\log \\Delta) + 2^{O(\\sqrt{\\log\\log n})}$ rounds; a\ncorollary of this algorithm is an $O(\\log \\alpha + \\sqrt{\\log n})$-round MIS\nalgorithm on arboricity-$\\alpha$ graphs. \n\n"}
{"id": "1605.06560", "contents": "Title: Functional Hashing for Compressing Neural Networks Abstract: As the complexity of deep neural networks (DNNs) trend to grow to absorb the\nincreasing sizes of data, memory and energy consumption has been receiving more\nand more attentions for industrial applications, especially on mobile devices.\nThis paper presents a novel structure based on functional hashing to compress\nDNNs, namely FunHashNN. For each entry in a deep net, FunHashNN uses multiple\nlow-cost hash functions to fetch values in the compression space, and then\nemploys a small reconstruction network to recover that entry. The\nreconstruction network is plugged into the whole network and trained jointly.\nFunHashNN includes the recently proposed HashedNets as a degenerated case, and\nbenefits from larger value capacity and less reconstruction loss. We further\ndiscuss extensions with dual space hashing and multi-hops. On several benchmark\ndatasets, FunHashNN demonstrates high compression ratios with little loss on\nprediction accuracy. \n\n"}
{"id": "1605.07156", "contents": "Title: Genetic Architect: Discovering Genomic Structure with Learned Neural\n  Architectures Abstract: Each human genome is a 3 billion base pair set of encoding instructions.\nDecoding the genome using deep learning fundamentally differs from most tasks,\nas we do not know the full structure of the data and therefore cannot design\narchitectures to suit it. As such, architectures that fit the structure of\ngenomics should be learned not prescribed. Here, we develop a novel search\nalgorithm, applicable across domains, that discovers an optimal architecture\nwhich simultaneously learns general genomic patterns and identifies the most\nimportant sequence motifs in predicting functional genomic outcomes. The\narchitectures we find using this algorithm succeed at using only RNA expression\ndata to predict gene regulatory structure, learn human-interpretable\nvisualizations of key sequence motifs, and surpass state-of-the-art results on\nbenchmark genomics challenges. \n\n"}
{"id": "1605.07262", "contents": "Title: Measuring Neural Net Robustness with Constraints Abstract: Despite having high accuracy, neural nets have been shown to be susceptible\nto adversarial examples, where a small perturbation to an input can cause it to\nbecome mislabeled. We propose metrics for measuring the robustness of a neural\nnet and devise a novel algorithm for approximating these metrics based on an\nencoding of robustness as a linear program. We show how our metrics can be used\nto evaluate the robustness of deep neural nets with experiments on the MNIST\nand CIFAR-10 datasets. Our algorithm generates more informative estimates of\nrobustness metrics compared to estimates based on existing algorithms.\nFurthermore, we show how existing approaches to improving robustness \"overfit\"\nto adversarial examples generated using a specific algorithm. Finally, we show\nthat our techniques can be used to additionally improve neural net robustness\nboth according to the metrics that we propose, but also according to previously\nproposed metrics. \n\n"}
{"id": "1605.08512", "contents": "Title: SNN: Stacked Neural Networks Abstract: It has been proven that transfer learning provides an easy way to achieve\nstate-of-the-art accuracies on several vision tasks by training a simple\nclassifier on top of features obtained from pre-trained neural networks. The\ngoal of this work is to generate better features for transfer learning from\nmultiple publicly available pre-trained neural networks. To this end, we\npropose a novel architecture called Stacked Neural Networks which leverages the\nfast training time of transfer learning while simultaneously being much more\naccurate. We show that using a stacked NN architecture can result in up to 8%\nimprovements in accuracy over state-of-the-art techniques using only one\npre-trained network for transfer learning. A second aim of this work is to make\nnetwork fine- tuning retain the generalizability of the base network to unseen\ntasks. To this end, we propose a new technique called \"joint fine-tuning\" that\nis able to give accuracies comparable to finetuning the same network\nindividually over two datasets. We also show that a jointly finetuned network\ngeneralizes better to unseen tasks when compared to a network finetuned over a\nsingle task. \n\n"}
{"id": "1605.09114", "contents": "Title: ParMAC: distributed optimisation of nested functions, with application\n  to learning binary autoencoders Abstract: Many powerful machine learning models are based on the composition of\nmultiple processing layers, such as deep nets, which gives rise to nonconvex\nobjective functions. A general, recent approach to optimise such \"nested\"\nfunctions is the method of auxiliary coordinates (MAC). MAC introduces an\nauxiliary coordinate for each data point in order to decouple the nested model\ninto independent submodels. This decomposes the optimisation into steps that\nalternate between training single layers and updating the coordinates. It has\nthe advantage that it reuses existing single-layer algorithms, introduces\nparallelism, and does not need to use chain-rule gradients, so it works with\nnondifferentiable layers. With large-scale problems, or when distributing the\ncomputation is necessary for faster training, the dataset may not fit in a\nsingle machine. It is then essential to limit the amount of communication\nbetween machines so it does not obliterate the benefit of parallelism. We\ndescribe a general way to achieve this, ParMAC. ParMAC works on a cluster of\nprocessing machines with a circular topology and alternates two steps until\nconvergence: one step trains the submodels in parallel using stochastic\nupdates, and the other trains the coordinates in parallel. Only submodel\nparameters, no data or coordinates, are ever communicated between machines.\nParMAC exhibits high parallelism, low communication overhead, and facilitates\ndata shuffling, load balancing, fault tolerance and streaming data processing.\nWe study the convergence of ParMAC and propose a theoretical model of its\nruntime and parallel speedup. We develop ParMAC to learn binary autoencoders\nfor fast, approximate image retrieval. We implement it in MPI in a distributed\nsystem and demonstrate nearly perfect speedups in a 128-processor cluster with\na training set of 100 million high-dimensional points. \n\n"}
{"id": "1605.09332", "contents": "Title: Parametric Exponential Linear Unit for Deep Convolutional Neural\n  Networks Abstract: Object recognition is an important task for improving the ability of visual\nsystems to perform complex scene understanding. Recently, the Exponential\nLinear Unit (ELU) has been proposed as a key component for managing bias shift\nin Convolutional Neural Networks (CNNs), but defines a parameter that must be\nset by hand. In this paper, we propose learning a parameterization of ELU in\norder to learn the proper activation shape at each layer in the CNNs. Our\nresults on the MNIST, CIFAR-10/100 and ImageNet datasets using the NiN,\nOverfeat, All-CNN and ResNet networks indicate that our proposed Parametric ELU\n(PELU) has better performances than the non-parametric ELU. We have observed as\nmuch as a 7.28% relative error improvement on ImageNet with the NiN network,\nwith only 0.0003% parameter increase. Our visual examination of the non-linear\nbehaviors adopted by Vgg using PELU shows that the network took advantage of\nthe added flexibility by learning different activations at different layers. \n\n"}
{"id": "1605.09513", "contents": "Title: Evaluating Distributed Execution of Workloads Abstract: Resource selection and task placement for distributed execution poses\nconceptual and implementation difficulties. Although resource selection and\ntask placement are at the core of many tools and workflow systems, the methods\nare ad hoc rather than being based on models. Consequently, partial and\nnon-interoperable implementations proliferate. We address both the conceptual\nand implementation difficulties by experimentally characterizing diverse\nmodalities of resource selection and task placement. We compare the\narchitectures and capabilities of two systems: the AIMES middleware and Swift\nworkflow scripting language and runtime. We integrate these systems to enable\nthe distributed execution of Swift workflows on Pilot-Jobs managed by the AIMES\nmiddleware. Our experiments characterize and compare alternative execution\nstrategies by measuring the time to completion of heterogeneous uncoupled\nworkloads executed at diverse scale and on multiple resources. We measure the\nadverse effects of pilot fragmentation and early binding of tasks to resources\nand the benefits of backfill scheduling across pilots on multiple resources. We\nthen use this insight to execute a multi-stage workflow across five\nproduction-grade resources. We discuss the importance and implications for\nother tools and workflow systems. \n\n"}
{"id": "1605.09721", "contents": "Title: CYCLADES: Conflict-free Asynchronous Machine Learning Abstract: We present CYCLADES, a general framework for parallelizing stochastic\noptimization algorithms in a shared memory setting. CYCLADES is asynchronous\nduring shared model updates, and requires no memory locking mechanisms, similar\nto HOGWILD!-type algorithms. Unlike HOGWILD!, CYCLADES introduces no conflicts\nduring the parallel execution, and offers a black-box analysis for provable\nspeedups across a large family of algorithms. Due to its inherent conflict-free\nnature and cache locality, our multi-core implementation of CYCLADES\nconsistently outperforms HOGWILD!-type algorithms on sufficiently sparse\ndatasets, leading to up to 40% speedup gains compared to the HOGWILD!\nimplementation of SGD, and up to 5x gains over asynchronous implementations of\nvariance reduction algorithms. \n\n"}
{"id": "1606.00350", "contents": "Title: Data Centers as Dispatchable Loads to Harness Stranded Power Abstract: We analyze how both traditional data center integration and dispatchable load\nintegration affect power grid efficiency. We use detailed network models,\nparallel optimization solvers, and thousands of renewable generation scenarios\nto perform our analysis. Our analysis reveals that significant spillage and\nstranded power will be observed in power grids as wind power levels are\nincreased. A counter-intuitive finding is that collocating data centers with\ninflexible loads next to wind farms has limited impacts on renewable portfolio\nstandard (RPS) goals because it provides limited system-level flexibility and\ncan in fact increase stranded power and fossil-fueled generation. In contrast,\noptimally placing data centers that are dispatchable (with flexible loads)\nprovides system-wide flexibility, reduces stranded power, and improves\nefficiency. In short, optimally placed dispatchable computing loads can enable\nbetter scaling to high RPS. We show that these dispatchable computing loads are\npowered to 60~80\\% of their requested capacity, indicating that there are\nsignificant economic incentives provided by stranded power. \n\n"}
{"id": "1606.00931", "contents": "Title: DeepSurv: Personalized Treatment Recommender System Using A Cox\n  Proportional Hazards Deep Neural Network Abstract: Medical practitioners use survival models to explore and understand the\nrelationships between patients' covariates (e.g. clinical and genetic features)\nand the effectiveness of various treatment options. Standard survival models\nlike the linear Cox proportional hazards model require extensive feature\nengineering or prior medical knowledge to model treatment interaction at an\nindividual level. While nonlinear survival methods, such as neural networks and\nsurvival forests, can inherently model these high-level interaction terms, they\nhave yet to be shown as effective treatment recommender systems. We introduce\nDeepSurv, a Cox proportional hazards deep neural network and state-of-the-art\nsurvival method for modeling interactions between a patient's covariates and\ntreatment effectiveness in order to provide personalized treatment\nrecommendations. We perform a number of experiments training DeepSurv on\nsimulated and real survival data. We demonstrate that DeepSurv performs as well\nas or better than other state-of-the-art survival models and validate that\nDeepSurv successfully models increasingly complex relationships between a\npatient's covariates and their risk of failure. We then show how DeepSurv\nmodels the relationship between a patient's features and effectiveness of\ndifferent treatment options to show how DeepSurv can be used to provide\nindividual treatment recommendations. Finally, we train DeepSurv on real\nclinical studies to demonstrate how it's personalized treatment recommendations\nwould increase the survival time of a set of patients. The predictive and\nmodeling capabilities of DeepSurv will enable medical researchers to use deep\nneural networks as a tool in their exploration, understanding, and prediction\nof the effects of a patient's characteristics on their risk of failure. \n\n"}
{"id": "1606.01166", "contents": "Title: Generalizing the Convolution Operator to extend CNNs to Irregular\n  Domains Abstract: Convolutional Neural Networks (CNNs) have become the state-of-the-art in\nsupervised learning vision tasks. Their convolutional filters are of paramount\nimportance for they allow to learn patterns while disregarding their locations\nin input images. When facing highly irregular domains, generalized\nconvolutional operators based on an underlying graph structure have been\nproposed. However, these operators do not exactly match standard ones on grid\ngraphs, and introduce unwanted additional invariance (e.g. with regards to\nrotations). We propose a novel approach to generalize CNNs to irregular domains\nusing weight sharing and graph-based operators. Using experiments, we show that\nthese models resemble CNNs on regular domains and offer better performance than\nmultilayer perceptrons on distorded ones. \n\n"}
{"id": "1606.01404", "contents": "Title: Generating Natural Language Inference Chains Abstract: The ability to reason with natural language is a fundamental prerequisite for\nmany NLP tasks such as information extraction, machine translation and question\nanswering. To quantify this ability, systems are commonly tested whether they\ncan recognize textual entailment, i.e., whether one sentence can be inferred\nfrom another one. However, in most NLP applications only single source\nsentences instead of sentence pairs are available. Hence, we propose a new task\nthat measures how well a model can generate an entailed sentence from a source\nsentence. We take entailment-pairs of the Stanford Natural Language Inference\ncorpus and train an LSTM with attention. On a manually annotated test set we\nfound that 82% of generated sentences are correct, an improvement of 10.3% over\nan LSTM baseline. A qualitative analysis shows that this model is not only\ncapable of shortening input sentences, but also inferring new statements via\nparaphrasing and phrase entailment. We then apply this model recursively to\ninput-output pairs, thereby generating natural language inference chains that\ncan be used to automatically construct an entailment graph from source\nsentences. Finally, by swapping source and target sentences we can also train a\nmodel that given an input sentence invents additional information to generate a\nnew sentence. \n\n"}
{"id": "1606.03498", "contents": "Title: Improved Techniques for Training GANs Abstract: We present a variety of new architectural features and training procedures\nthat we apply to the generative adversarial networks (GANs) framework. We focus\non two applications of GANs: semi-supervised learning, and the generation of\nimages that humans find visually realistic. Unlike most work on generative\nmodels, our primary goal is not to train a model that assigns high likelihood\nto test data, nor do we require the model to be able to learn well without\nusing any labels. Using our new techniques, we achieve state-of-the-art results\nin semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated\nimages are of high quality as confirmed by a visual Turing test: our model\ngenerates MNIST samples that humans cannot distinguish from real data, and\nCIFAR-10 samples that yield a human error rate of 21.3%. We also present\nImageNet samples with unprecedented resolution and show that our methods enable\nthe model to learn recognizable features of ImageNet classes. \n\n"}
{"id": "1606.03742", "contents": "Title: Application-Driven Near-Data Processing for Similarity Search Abstract: Similarity search is a key to a variety of applications including\ncontent-based search for images and video, recommendation systems, data\ndeduplication, natural language processing, computer vision, databases,\ncomputational biology, and computer graphics. At its core, similarity search\nmanifests as k-nearest neighbors (kNN), a computationally simple primitive\nconsisting of highly parallel distance calculations and a global top-k sort.\nHowever, kNN is poorly supported by today's architectures because of its high\nmemory bandwidth requirements.\n  This paper proposes an application-driven near-data processing accelerator\nfor similarity search: the Similarity Search Associative Memory (SSAM). By\ninstantiating compute units close to memory, SSAM benefits from the higher\nmemory bandwidth and density exposed by emerging memory technologies. We\nevaluate the SSAM design down to layout on top of the Micron hybrid memory cube\n(HMC), and show that SSAM can achieve up to two orders of magnitude\narea-normalized throughput and energy efficiency improvement over multicore\nCPUs; we also show SSAM is faster and more energy efficient than competing GPUs\nand FPGAs. Finally, we show that SSAM is also useful for other data intensive\ntasks like kNN index construction, and can be generalized to semantically\nfunction as a high capacity content addressable memory. \n\n"}
{"id": "1606.03966", "contents": "Title: Making Contextual Decisions with Low Technical Debt Abstract: Applications and systems are constantly faced with decisions that require\npicking from a set of actions based on contextual information.\nReinforcement-based learning algorithms such as contextual bandits can be very\neffective in these settings, but applying them in practice is fraught with\ntechnical debt, and no general system exists that supports them completely. We\naddress this and create the first general system for contextual learning,\ncalled the Decision Service.\n  Existing systems often suffer from technical debt that arises from issues\nlike incorrect data collection and weak debuggability, issues we systematically\naddress through our ML methodology and system abstractions. The Decision\nService enables all aspects of contextual bandit learning using four system\nabstractions which connect together in a loop: explore (the decision space),\nlog, learn, and deploy. Notably, our new explore and log abstractions ensure\nthe system produces correct, unbiased data, which our learner uses for online\nlearning and to enable real-time safeguards, all in a fully reproducible\nmanner.\n  The Decision Service has a simple user interface and works with a variety of\napplications: we present two live production deployments for content\nrecommendation that achieved click-through improvements of 25-30%, another with\n18% revenue lift in the landing page, and ongoing applications in tech support\nand machine failure handling. The service makes real-time decisions and learns\ncontinuously and scalably, while significantly lowering technical debt. \n\n"}
{"id": "1606.04074", "contents": "Title: ENTRA: Whole-Systems Energy Transparency Abstract: Promoting energy efficiency to a first class system design goal is an\nimportant research challenge. Although more energy-efficient hardware can be\ndesigned, it is software that controls the hardware; for a given system the\npotential for energy savings is likely to be much greater at the higher levels\nof abstraction in the system stack. Thus the greatest savings are expected from\nenergy-aware software development, which is the vision of the EU ENTRA project.\nThis article presents the concept of energy transparency as a foundation for\nenergy-aware software development. We show how energy modelling of hardware is\ncombined with static analysis to allow the programmer to understand the energy\nconsumption of a program without executing it, thus enabling exploration of the\ndesign space taking energy into consideration. The paper concludes by\nsummarising the current and future challenges identified in the ENTRA project. \n\n"}
{"id": "1606.04487", "contents": "Title: Omnivore: An Optimizer for Multi-device Deep Learning on CPUs and GPUs Abstract: We study the factors affecting training time in multi-device deep learning\nsystems. Given a specification of a convolutional neural network, our goal is\nto minimize the time to train this model on a cluster of commodity CPUs and\nGPUs. We first focus on the single-node setting and show that by using standard\nbatching and data-parallel techniques, throughput can be improved by at least\n5.5x over state-of-the-art systems on CPUs. This ensures an end-to-end training\nspeed directly proportional to the throughput of a device regardless of its\nunderlying hardware, allowing each node in the cluster to be treated as a black\nbox. Our second contribution is a theoretical and empirical study of the\ntradeoffs affecting end-to-end training time in a multiple-device setting. We\nidentify the degree of asynchronous parallelization as a key factor affecting\nboth hardware and statistical efficiency. We see that asynchrony can be viewed\nas introducing a momentum term. Our results imply that tuning momentum is\ncritical in asynchronous parallel configurations, and suggest that published\nresults that have not been fully tuned might report suboptimal performance for\nsome configurations. For our third contribution, we use our novel understanding\nof the interaction between system and optimization dynamics to provide an\nefficient hyperparameter optimizer. Our optimizer involves a predictive model\nfor the total time to convergence and selects an allocation of resources to\nminimize that time. We demonstrate that the most popular distributed deep\nlearning systems fall within our tradeoff space, but do not optimize within the\nspace. By doing this optimization, our prototype runs 1.9x to 12x faster than\nthe fastest state-of-the-art systems. \n\n"}
{"id": "1606.05134", "contents": "Title: Combinatorial Optimization of Work Distribution on Heterogeneous Systems Abstract: We describe an approach that uses combinatorial optimization and machine\nlearning to share the work between the host and device of heterogeneous\ncomputing systems such that the overall application execution time is\nminimized. We propose to use combinatorial optimization to search for the\noptimal system configuration in the given parameter space (such as, the number\nof threads, thread affinity, work distribution for the host and device). For\neach system configuration that is suggested by combinatorial optimization, we\nuse machine learning for evaluation of the system performance. We evaluate our\napproach experimentally using a heterogeneous platform that comprises two\n12-core Intel Xeon E5 CPUs and an Intel Xeon Phi 7120P co-processor with 61\ncores. Using our approach we are able to find a near-optimal system\nconfiguration by performing only about 5% of all possible experiments. \n\n"}
{"id": "1606.05937", "contents": "Title: Formalization of Phase Ordering Abstract: Phasers pose an interesting synchronization mechanism that generalizes many\ncollective synchronization patterns seen in parallel programming languages,\nincluding barriers, clocks, and point-to-point synchronization using latches or\nsemaphores. This work characterizes scheduling constraints on phaser\noperations, by relating the execution state of two tasks that operate on the\nsame phaser. We propose a formalization of Habanero phasers,\nMay-Happen-In-Parallel, and Happens-Before relations for phaser operations, and\nshow that these relations conform with the semantics. Our formalization and\nproofs are fully mechanized using the Coq proof assistant, and are available\nonline. \n\n"}
{"id": "1606.06216", "contents": "Title: Neural networks with differentiable structure Abstract: While gradient descent has proven highly successful in learning connection\nweights for neural networks, the actual structure of these networks is usually\ndetermined by hand, or by other optimization algorithms. Here we describe a\nsimple method to make network structure differentiable, and therefore\naccessible to gradient descent. We test this method on recurrent neural\nnetworks applied to simple sequence prediction problems. Starting with initial\nnetworks containing only one node, the method automatically builds networks\nthat successfully solve the tasks. The number of nodes in the final network\ncorrelates with task difficulty. The method can dynamically increase network\nsize in response to an abrupt complexification in the task; however, reduction\nin network size in response to task simplification is not evident for\nreasonable meta-parameters. The method does not penalize network performance\nfor these test tasks: variable-size networks actually reach better performance\nthan fixed-size networks of higher, lower or identical size. We conclude by\ndiscussing how this method could be applied to more complex networks, such as\nfeedforward layered networks, or multiple-area networks of arbitrary shape. \n\n"}
{"id": "1606.06871", "contents": "Title: A Comprehensive Study of Deep Bidirectional LSTM RNNs for Acoustic\n  Modeling in Speech Recognition Abstract: We present a comprehensive study of deep bidirectional long short-term memory\n(LSTM) recurrent neural network (RNN) based acoustic models for automatic\nspeech recognition (ASR). We study the effect of size and depth and train\nmodels of up to 8 layers. We investigate the training aspect and study\ndifferent variants of optimization methods, batching, truncated\nbackpropagation, different regularization techniques such as dropout and $L_2$\nregularization, and different gradient clipping variants.\n  The major part of the experimental analysis was performed on the Quaero\ncorpus. Additional experiments also were performed on the Switchboard corpus.\nOur best LSTM model has a relative improvement in word error rate of over 14\\%\ncompared to our best feed-forward neural network (FFNN) baseline on the Quaero\ntask. On this task, we get our best result with an 8 layer bidirectional LSTM\nand we show that a pretraining scheme with layer-wise construction helps for\ndeep LSTMs.\n  Finally we compare the training calculation time of many of the presented\nexperiments in relation with recognition performance.\n  All the experiments were done with RETURNN, the RWTH extensible training\nframework for universal recurrent neural networks in combination with RASR, the\nRWTH ASR toolkit. \n\n"}
{"id": "1606.07490", "contents": "Title: Enhancing Accountability and Trust in Distributed Ledgers Abstract: Permisionless decentralized ledgers (\"blockchains\") such as the one\nunderlying the cryptocurrency Bitcoin allow anonymous participants to maintain\nthe ledger, while avoiding control or \"censorship\" by any single entity. In\ncontrast, permissioned decentralized ledgers exploit real-world trust and\naccountability, allowing only explicitly authorized parties to maintain the\nledger. Permissioned ledgers support more flexible governance and a wider\nchoice of consensus mechanisms. Both kinds of decentralized ledgers may be\nsusceptible to manipulation by participants who favor some transactions over\nothers. The real-world accountability underlying permissioned ledgers provides\nan opportunity to impose fairness constraints that can be enforced by\npenalizing violators after-the- fact. To date, however, this opportunity has\nnot been fully exploited, unnecessarily leaving participants latitude to\nmanipulate outcomes undetectably. This paper draws attention to this issue, and\nproposes design principles to make such manipulation more difficult, as well as\nspecific mechanisms to make it easier to detect when violations occur. \n\n"}
{"id": "1606.09402", "contents": "Title: Efficient Randomized Algorithms for the Fixed-Precision Low-Rank Matrix\n  Approximation Abstract: Randomized algorithms for low-rank matrix approximation are investigated,\nwith the emphasis on the fixed-precision problem and computational efficiency\nfor handling large matrices. The algorithms are based on the so-called QB\nfactorization, where Q is an orthonormal matrix. Firstly, a mechanism for\ncalculating the approximation error in Frobenius norm is proposed, which\nenables efficient adaptive rank determination for large and/or sparse matrix.\nIt can be combined with any QB-form factorization algorithm in which B's rows\nare incrementally generated. Based on the blocked randQB algorithm by P.-G.\nMartinsson and S. Voronin, this results in an algorithm called randQB EI. Then,\nwe further revise the algorithm to obtain a pass-efficient algorithm, randQB\nFP, which is mathematically equivalent to the existing randQB algorithms and\nalso suitable for the fixed-precision problem. Especially, randQB FP can serve\nas a single-pass algorithm for calculating leading singular values, under\ncertain condition. With large and/or sparse test matrices, we have empirically\nvalidated the merits of the proposed techniques, which exhibit remarkable\nspeedup and memory saving over the blocked randQB algorithm. We have also\ndemonstrated that the single-pass algorithm derived by randQB FP is much more\naccurate than an existing single-pass algorithm. And with data from a scenic\nimage and an information retrieval application, we have shown the advantages of\nthe proposed algorithms over the adaptive range finder algorithm for solving\nthe fixed-precision problem. \n\n"}
{"id": "1607.00291", "contents": "Title: High-Performance Tensor Contraction without Transposition Abstract: Tensor computations--in particular tensor contraction (TC)--are important\nkernels in many scientific computing applications. Due to the fundamental\nsimilarity of TC to matrix multiplication (MM) and to the availability of\noptimized implementations such as the BLAS, tensor operations have\ntraditionally been implemented in terms of BLAS operations, incurring both a\nperformance and a storage overhead. Instead, we implement TC using the flexible\nBLIS framework, which allows for transposition (reshaping) of the tensor to be\nfused with internal partitioning and packing operations, requiring no explicit\ntransposition operations or additional workspace. This implementation, TBLIS,\nachieves performance approaching that of MM, and in some cases considerably\nhigher than that of traditional TC. Our implementation supports multithreading\nusing an approach identical to that used for MM in BLIS, with similar\nperformance characteristics. The complexity of managing tensor-to-matrix\ntransformations is also handled automatically in our approach, greatly\nsimplifying its use in scientific applications. \n\n"}
{"id": "1607.02480", "contents": "Title: Real-Time Anomaly Detection for Streaming Analytics Abstract: Much of the worlds data is streaming, time-series data, where anomalies give\nsignificant information in critical situations. Yet detecting anomalies in\nstreaming data is a difficult task, requiring detectors to process data in\nreal-time, and learn while simultaneously making predictions. We present a\nnovel anomaly detection technique based on an on-line sequence memory algorithm\ncalled Hierarchical Temporal Memory (HTM). We show results from a live\napplication that detects anomalies in financial metrics in real-time. We also\ntest the algorithm on NAB, a published benchmark for real-time anomaly\ndetection, where our algorithm achieves best-in-class results. \n\n"}
{"id": "1607.02497", "contents": "Title: Is the Multigrid Method Fault Tolerant? The Two-Grid Case Abstract: The predicted reduced resiliency of next-generation high performance\ncomputers means that it will become necessary to take into account the effects\nof randomly occurring faults on numerical methods. Further, in the event of a\nhard fault occurring, a decision has to be made as to what remedial action\nshould be taken in order to resume the execution of the algorithm. The action\nthat is chosen can have a dramatic effect on the performance and\ncharacteristics of the scheme. Ideally, the resulting algorithm should be\nsubjected to the same kind of mathematical analysis that was applied to the\noriginal, deterministic variant.\n  The purpose of this work is to provide an analysis of the behaviour of the\nmultigrid algorithm in the presence of faults. Multigrid is arguably the method\nof choice for the solution of large-scale linear algebra problems arising from\ndiscretization of partial differential equations and it is of considerable\nimportance to anticipate its behaviour on an exascale machine. The analysis of\nresilience of algorithms is in its infancy and the current work is perhaps the\nfirst to provide a mathematical model for faults and analyse the behaviour of a\nstate-of-the-art algorithm under the model. It is shown that the Two Grid\nMethod fails to be resilient to faults. Attention is then turned to identifying\nthe minimal necessary remedial action required to restore the rate of\nconvergence to that enjoyed by the ideal fault-free method. \n\n"}
{"id": "1607.04793", "contents": "Title: Learning to Decode Linear Codes Using Deep Learning Abstract: A novel deep learning method for improving the belief propagation algorithm\nis proposed. The method generalizes the standard belief propagation algorithm\nby assigning weights to the edges of the Tanner graph. These edges are then\ntrained using deep learning techniques. A well-known property of the belief\npropagation algorithm is the independence of the performance on the transmitted\ncodeword. A crucial property of our new method is that our decoder preserved\nthis property. Furthermore, this property allows us to learn only a single\ncodeword instead of exponential number of code-words. Improvements over the\nbelief propagation algorithm are demonstrated for various high density parity\ncheck codes. \n\n"}
{"id": "1607.06139", "contents": "Title: A Complexity-Based Hierarchy for Multiprocessor Synchronization Abstract: For many years, Herlihy's elegant computability based Consensus Hierarchy has\nbeen our best explanation of the relative power of various types of\nmultiprocessor synchronization objects when used in deterministic algorithms.\nHowever, key to this hierarchy is treating synchronization instructions as\ndistinct objects, an approach that is far from the real-world, where\nmultiprocessor programs apply synchronization instructions to collections of\narbitrary memory locations. We were surprised to realize that, when considering\ninstructions applied to memory locations, the computability based hierarchy\ncollapses. This leaves open the question of how to better capture the power of\nvarious synchronization instructions.\n  In this paper, we provide an approach to answering this question. We present\na hierarchy of synchronization instructions, classified by their space\ncomplexity in solving obstruction-free consensus. Our hierarchy provides a\nclassification of combinations of known instructions that seems to fit with our\nintuition of how useful some are in practice, while questioning the\neffectiveness of others. We prove an essentially tight characterization of the\npower of buffered read and write instructions.Interestingly, we show a similar\nresult for multi-location atomic assignments. \n\n"}
{"id": "1607.07348", "contents": "Title: Spark Parameter Tuning via Trial-and-Error Abstract: Spark has been established as an attractive platform for big data analysis,\nsince it manages to hide most of the complexities related to parallelism, fault\ntolerance and cluster setting from developers. However, this comes at the\nexpense of having over 150 configurable parameters, the impact of which cannot\nbe exhaustively examined due to the exponential amount of their combinations.\nThe default values allow developers to quickly deploy their applications but\nleave the question as to whether performance can be improved open. In this\nwork, we investigate the impact of the most important of the tunable Spark\nparameters on the application performance and guide developers on how to\nproceed to changes to the default values. We conduct a series of experiments\nwith known benchmarks on the MareNostrum petascale supercomputer to test the\nperformance sensitivity. More importantly, we offer a trial-and-error\nmethodology for tuning parameters in arbitrary applications based on evidence\nfrom a very small number of experimental runs. We test our methodology in three\ncase studies, where we manage to achieve speedups of more than 10 times. \n\n"}
{"id": "1608.00039", "contents": "Title: Distributed Learning for Stochastic Generalized Nash Equilibrium\n  Problems Abstract: This work examines a stochastic formulation of the generalized Nash\nequilibrium problem (GNEP) where agents are subject to randomness in the\nenvironment of unknown statistical distribution. We focus on fully-distributed\nonline learning by agents and employ penalized individual cost functions to\ndeal with coupled constraints. Three stochastic gradient strategies are\ndeveloped with constant step-sizes. We allow the agents to use heterogeneous\nstep-sizes and show that the penalty solution is able to approach the Nash\nequilibrium in a stable manner within $O(\\mu_\\text{max})$, for small step-size\nvalue $\\mu_\\text{max}$ and sufficiently large penalty parameters. The operation\nof the algorithm is illustrated by considering the network Cournot competition\nproblem. \n\n"}
{"id": "1608.00218", "contents": "Title: Hyperparameter Transfer Learning through Surrogate Alignment for\n  Efficient Deep Neural Network Training Abstract: Recently, several optimization methods have been successfully applied to the\nhyperparameter optimization of deep neural networks (DNNs). The methods work by\nmodeling the joint distribution of hyperparameter values and corresponding\nerror. Those methods become less practical when applied to modern DNNs whose\ntraining may take a few days and thus one cannot collect sufficient\nobservations to accurately model the distribution. To address this challenging\nissue, we propose a method that learns to transfer optimal hyperparameter\nvalues for a small source dataset to hyperparameter values with comparable\nperformance on a dataset of interest. As opposed to existing transfer learning\nmethods, our proposed method does not use hand-designed features. Instead, it\nuses surrogates to model the hyperparameter-error distributions of the two\ndatasets and trains a neural network to learn the transfer function. Extensive\nexperiments on three CV benchmark datasets clearly demonstrate the efficiency\nof our method. \n\n"}
{"id": "1608.00695", "contents": "Title: The blockchain: a new framework for robotic swarm systems Abstract: Swarms of robots will revolutionize many industrial applications, from\ntargeted material delivery to precision farming. However, several of the\nheterogeneous characteristics that make them ideal for certain future\napplications --- robot autonomy, decentralized control, collective emergent\nbehavior, etc. --- hinder the evolution of the technology from academic\ninstitutions to real-world problems. Blockchain, an emerging technology\noriginated in the Bitcoin field, demonstrates that by combining peer-to-peer\nnetworks with cryptographic algorithms a group of agents can reach an agreement\non a particular state of affairs and record that agreement without the need for\na controlling authority. The combination of blockchain with other distributed\nsystems, such as robotic swarm systems, can provide the necessary capabilities\nto make robotic swarm operations more secure, autonomous, flexible and even\nprofitable. This work explains how blockchain technology can provide innovative\nsolutions to four emergent issues in the swarm robotics research field. New\nsecurity, decision making, behavior differentiation and business models for\nswarm robotic systems are described by providing case scenarios and examples.\nFinally, limitations and possible future problems that arise from the\ncombination of these two technologies are described. \n\n"}
{"id": "1608.00726", "contents": "Title: Infinite Unlimited Churn Abstract: We study unlimited infinite churn in peer-to-peer overlay networks. Under\nthis churn, arbitrary many peers may concurrently request to join or leave the\noverlay network; moreover these requests may never stop coming. We prove that\nunlimited adversarial churn, where processes may just exit the overlay network,\nis unsolvable. We focus on cooperative churn where exiting processes\nparticipate in the churn handling algorithm. We define the problem of unlimited\ninfinite churn in this setting. We distinguish the fair version of the problem,\nwhere each request is eventually satisfied, from the unfair version that just\nguarantees progress. We focus on local solutions to the problem, and prove that\na local solution to the Fair Infinite Unlimited Churn is impossible. We then\npresent and prove correct an algorithm UIUC that solves the Unfair Infinite\nUnlimited Churn Problem for a linearized peer-to-peer overlay network. We\nextend this solution to skip lists and skip graphs. \n\n"}
{"id": "1608.01019", "contents": "Title: The Entity Registry System. From concept to deployment Abstract: The entity registry system (ERS) is a decentralized entity registry that can\nbe used to replace the Web as a platform for publishing linked data when the\nlatter is not available. In developing countries, where off-line is the default\nmode of operation, centralized linked data solutions fail to address the needs\nof the communities. Although the features are mostly completed, the system is\nnot yet ready for deployment. This project aims to provide extensive tests and\nscalability investigations that would make it ready for a real scenario. \n\n"}
{"id": "1608.01689", "contents": "Title: Derandomizing Local Distributed Algorithms under Bandwidth Restrictions Abstract: This paper addresses the cornerstone family of \\emph{local problems} in\ndistributed computing, and investigates the curious gap between randomized and\ndeterministic solutions under bandwidth restrictions.\n  Our main contribution is in providing tools for derandomizing solutions to\nlocal problems, when the $n$ nodes can only send $O(\\log n)$-bit messages in\neach round of communication. We combine bounded independence, which we show to\nbe sufficient for some algorithms, with the method of conditional expectations\nand with additional machinery, to obtain the following results.\n  Our techniques give a deterministic maximal independent set (MIS) algorithm\nin the CONGEST model, where the communication graph is identical to the input\ngraph, in $O(D\\log^2 n)$ rounds, where $D$ is the diameter of the graph. The\nbest known running time in terms of $n$ alone is $2^{O(\\sqrt{\\log n})}$, which\nis super-polylogarithmic, and requires large messages. For the CONGEST model,\nthe only known previous solution is a coloring-based $O(\\Delta + \\log^*\nn)$-round algorithm, where $\\Delta$ is the maximal degree in the graph.\n  On the way to obtaining the above, we show that in the \\emph{Congested\nClique} model, which allows all-to-all communication, there is a deterministic\nMIS algorithm that runs in $O(\\log \\Delta \\log n)$ rounds.%, where $\\Delta$ is\nthe maximum degree. When $\\Delta=O(n^{1/3})$, the bound improves to $O(\\log\n\\Delta)$ and holds also for $(\\Delta+1)$-coloring.\n  In addition, we deterministically construct a $(2k-1)$-spanner with\n$O(kn^{1+1/k}\\log n)$ edges in $O(k \\log n)$ rounds. For comparison, in the\nmore stringent CONGEST model, the best deterministic algorithm for constructing\na $(2k-1)$-spanner with $O(kn^{1+1/k})$ edges runs in $O(n^{1-1/k})$ rounds. \n\n"}
{"id": "1608.03321", "contents": "Title: An Erlang Implementation of Multiparty Session Actors Abstract: By requiring co-ordination to take place using explicit message passing\ninstead of relying on shared memory, actor-based programming languages have\nbeen shown to be effective tools for building reliable and fault-tolerant\ndistributed systems. Although naturally communication-centric, communication\npatterns in actor-based applications remain informally specified, meaning that\nerrors in communication are detected late, if at all.\n  Multiparty session types are a formalism to describe, at a global level, the\ninteractions between multiple communicating entities. This article describes\nthe implementation of a prototype framework for monitoring Erlang/OTP\ngen_server applications against multiparty session types, showing how previous\nwork on multiparty session actors can be adapted to a purely actor-based\nlanguage, and how monitor violations and termination of session participants\ncan be reported in line with the Erlang mantra of \"let it fail\". Finally, the\nframework is used to implement two case studies: an adaptation of a\nfreely-available DNS server, and a chat server. \n\n"}
{"id": "1608.05138", "contents": "Title: Hybrid CPU-GPU Framework for Network Motifs Abstract: Massively parallel architectures such as the GPU are becoming increasingly\nimportant due to the recent proliferation of data. In this paper, we propose a\nkey class of hybrid parallel graphlet algorithms that leverages multiple CPUs\nand GPUs simultaneously for computing k-vertex induced subgraph statistics\n(called graphlets). In addition to the hybrid multi-core CPU-GPU framework, we\nalso investigate single GPU methods (using multiple cores) and multi-GPU\nmethods that leverage all available GPUs simultaneously for computing induced\nsubgraph statistics. Both methods leverage GPU devices only, whereas the hybrid\nmulti-core CPU-GPU framework leverages all available multi-core CPUs and\nmultiple GPUs for computing graphlets in large networks. Compared to recent\napproaches, our methods are orders of magnitude faster, while also more cost\neffective enjoying superior performance per capita and per watt. In particular,\nthe methods are up to 300 times faster than the recent state-of-the-art method.\nTo the best of our knowledge, this is the first work to leverage multiple CPUs\nand GPUs simultaneously for computing induced subgraph statistics. \n\n"}
{"id": "1608.05265", "contents": "Title: Generation of the Single Precision BLAS library for the Parallella\n  platform, with Epiphany co-processor acceleration, using the BLIS framework Abstract: The Parallella is a hybrid computing platform that came into existence as the\nresult of a Kickstarter project by Adapteva. It is composed of the high\nperformance, energy-efficient, manycore architecture, Epiphany chip (used as\nco-processor) and one Zynq-7000 series chip, which normally runs a regular\nLinux OS version, serves as the main processor, and implements \"glue logic\" in\nits internal FPGA to communicate with the many interfaces in the Parallella. In\nthis paper an Epiphany-accelerated BLAS library for the Parallella platform was\ncreated (which could be suitable, also, for similar hybrid platforms that\ninclude the Epiphany chip as a coprocessor). For the actual instantiation of\nthe BLAS, the BLIS framework was used. There have been previous implementations\nof Matrix-Matrix multiplication, on this platform, that achieved very good\nperformances inside the Epiphany chip (up to 85% of peak), but not so good ones\nfor the complete Parallella platform (due to inter-chip data transfer bandwidth\nlimitations). The main purpose of this work was to get closer to practical\nLinear Algebra aplications for the entire Parallella platform, with scientific\ncomputing in view. \n\n"}
{"id": "1608.05288", "contents": "Title: Accelerating Exact and Approximate Inference for (Distributed) Discrete\n  Optimization with GPUs Abstract: Discrete optimization is a central problem in artificial intelligence. The\noptimization of the aggregated cost of a network of cost functions arises in a\nvariety of problems including (W)CSP, DCOP, as well as optimization in\nstochastic variants such as the tasks of finding the most probable explanation\n(MPE) in belief networks. Inference-based algorithms are powerful techniques\nfor solving discrete optimization problems, which can be used independently or\nin combination with other techniques. However, their applicability is often\nlimited by their compute intensive nature and their space requirements. This\npaper proposes the design and implementation of a novel inference-based\ntechnique, which exploits modern massively parallel architectures, such as\nthose found in Graphical Processing Units (GPUs), to speed up the resolution of\nexact and approximated inference-based algorithms for discrete optimization.\nThe paper studies the proposed algorithm in both centralized and distributed\noptimization contexts. The paper demonstrates that the use of GPUs provides\nsignificant advantages in terms of runtime and scalability, achieving up to two\norders of magnitude in speedups and showing a considerable reduction in\nexecution time (up to 345 times faster) with respect to a sequential version. \n\n"}
{"id": "1608.05401", "contents": "Title: Distributed Optimization of Convex Sum of Non-Convex Functions Abstract: We present a distributed solution to optimizing a convex function composed of\nseveral non-convex functions. Each non-convex function is privately stored with\nan agent while the agents communicate with neighbors to form a network. We show\nthat coupled consensus and projected gradient descent algorithm proposed in [1]\ncan optimize convex sum of non-convex functions under an additional assumption\non gradient Lipschitzness. We further discuss the applications of this analysis\nin improving privacy in distributed optimization. \n\n"}
{"id": "1608.07200", "contents": "Title: Bulk-synchronous pseudo-streaming algorithms for many-core accelerators Abstract: The bulk-synchronous parallel (BSP) model provides a framework for writing\nparallel programs with predictable performance. In this paper we extend the BSP\nmodel to support what we will call pseudo-streaming algorithms for\naccelerators. We also generalize the BSP cost function to these algorithms, so\nthat it is possible to predict the running time for programs targeting\nmany-core accelerators and to identify possible bottlenecks. Several examples\nof algorithms within this new framework will be explored. We extend the BSPlib\nstandard by proposing a small number of new BSP primitives to create and use\nstreams in a portable way. We will introduce a software library called Epiphany\nBSP that implements these ideas for the Parallella development board. Finally\nwe will give experimental results for pseudo-streaming algorithms on the\nParallella platform. \n\n"}
{"id": "1609.01360", "contents": "Title: Evolutionary Synthesis of Deep Neural Networks via Synaptic\n  Cluster-driven Genetic Encoding Abstract: There has been significant recent interest towards achieving highly efficient\ndeep neural network architectures. A promising paradigm for achieving this is\nthe concept of evolutionary deep intelligence, which attempts to mimic\nbiological evolution processes to synthesize highly-efficient deep neural\nnetworks over successive generations. An important aspect of evolutionary deep\nintelligence is the genetic encoding scheme used to mimic heredity, which can\nhave a significant impact on the quality of offspring deep neural networks.\nMotivated by the neurobiological phenomenon of synaptic clustering, we\nintroduce a new genetic encoding scheme where synaptic probability is driven\ntowards the formation of a highly sparse set of synaptic clusters. Experimental\nresults for the task of image classification demonstrated that the synthesized\noffspring networks using this synaptic cluster-driven genetic encoding scheme\ncan achieve state-of-the-art performance while having network architectures\nthat are not only significantly more efficient (with a ~125-fold decrease in\nsynapses for MNIST) compared to the original ancestor network, but also\ntailored for GPU-accelerated machine learning applications. \n\n"}
{"id": "1609.01490", "contents": "Title: A Non-linear GPU Thread Map for Triangular Domains Abstract: There is a stage in the GPU computing pipeline where a grid of thread-blocks,\nin \\textit{parallel space}, is mapped onto the problem domain, in \\textit{data\nspace}. Since the parallel space is restricted to a box type geometry, the\nmapping approach is typically a $k$-dimensional bounding box (BB) that covers a\n$p$-dimensional data space. Threads that fall inside the domain perform\ncomputations while threads that fall outside are discarded at runtime. In this\nwork we study the case of mapping threads efficiently onto triangular domain\nproblems and propose a block-space linear map $\\lambda(\\omega)$, based on the\nproperties of the lower triangular matrix, that reduces the number of\nunnnecessary threads from $\\mathcal{O}(n^2)$ to $\\mathcal{O}(n)$. Performance\nresults for global memory accesses show an improvement of up to $18\\%$ with\nrespect to the \\textit{bounding-box} approach, placing $\\lambda(\\omega)$ on\nsecond place below the \\textit{rectangular-box} approach and above the\n\\textit{recursive-partition} and \\textit{upper-triangular} approaches. For\nshared memory scenarios $\\lambda(\\omega)$ was the fastest approach achieving\n$7\\%$ of performance improvement while preserving thread locality. The results\nobtained in this work make $\\lambda(\\omega)$ an interesting map for efficient\nGPU computing on parallel problems that define a triangular domain with or\nwithout neighborhood interactions. The extension to tetrahedral domains is\nanalyzed, with applications to triplet-interaction n-body applications. \n\n"}
{"id": "1609.01690", "contents": "Title: A Unified Coding Framework for Distributed Computing with Straggling\n  Servers Abstract: We propose a unified coded framework for distributed computing with\nstraggling servers, by introducing a tradeoff between \"latency of computation\"\nand \"load of communication\" for some linear computation tasks. We show that the\ncoded scheme of [1]-[3] that repeats the intermediate computations to create\ncoded multicasting opportunities to reduce communication load, and the coded\nscheme of [4], [5] that generates redundant intermediate computations to combat\nagainst straggling servers can be viewed as special instances of the proposed\nframework, by considering two extremes of this tradeoff: minimizing either the\nload of communication or the latency of computation individually. Furthermore,\nthe latency-load tradeoff achieved by the proposed coded framework allows to\nsystematically operate at any point on that tradeoff to perform distributed\ncomputing tasks. We also prove an information-theoretic lower bound on the\nlatency-load tradeoff, which is shown to be within a constant multiplicative\ngap from the achieved tradeoff at the two end points. \n\n"}
{"id": "1609.02104", "contents": "Title: A Consumer-Centric Market for Database Computation in the Cloud Abstract: The availability of public computing resources in the cloud has\nrevolutionized data analysis, but requesting cloud resources often involves\ncomplex decisions for consumers. Under the current pricing mechanisms, cloud\nservice providers offer several service options and charge consumers based on\nthe resources they use. Before they can decide which cloud resources to\nrequest, consumers have to estimate the completion time and cost of their\ncomputational tasks for different service options and possibly for different\nservice providers. This estimation is challenging even for expert cloud users.\nWe propose a new market-based framework for pricing computational tasks in the\ncloud. Our framework introduces an agent between consumers and cloud providers.\nThe agent takes data and computational tasks from users, estimates time and\ncost for evaluating the tasks, and returns to consumers contracts that specify\nthe price and completion time. Our framework can be applied directly to\nexisting cloud markets without altering the way cloud providers offer and price\nservices. In addition, it simplifies cloud use for consumers by allowing them\nto compare contracts, rather than choose resources directly. We present design,\nanalytical, and algorithmic contributions focusing on pricing computation\ncontracts, analyzing their properties, and optimizing them in complex\nworkflows. We conduct an experimental evaluation of our market framework over a\nreal-world cloud service and demonstrate empirically that our market ensures\nthree key properties: competitiveness, fairness, and resilience. Finally, we\npresent a fine-grained pricing mechanism for complex workflows and show that it\ncan increase agent profits by more than an order of magnitude in some cases. \n\n"}
{"id": "1609.02305", "contents": "Title: Survey of Consistent Software-Defined Network Updates Abstract: Computer networks have become a critical infrastructure. In fact, networks\nshould not only meet strict requirements in terms of correctness, availability,\nand performance, but they should also be very flexible and support fast\nupdates, e.g., due to policy changes, increasing traffic, or failures. This\npaper presents a structured survey of mechanism and protocols to update\ncomputer networks in a fast and consistent manner. In particular, we identify\nand discuss the different desirable consistency properties that should be\nprovided throughout a network update, the algorithmic techniques which are\nneeded to meet these consistency properties, and the implications on the speed\nand costs at which updates can be performed. We also explain the relationship\nbetween consistent network update problems and classic algorithmic optimization\nones. While our survey is mainly motivated by the advent of Software-Defined\nNetworks (SDNs) and their primary need for correct and efficient update\ntechniques, the fundamental underlying problems are not new, and we provide a\nhistorical perspective of the subject as well. \n\n"}
{"id": "1609.04243", "contents": "Title: Convolutional Recurrent Neural Networks for Music Classification Abstract: We introduce a convolutional recurrent neural network (CRNN) for music\ntagging. CRNNs take advantage of convolutional neural networks (CNNs) for local\nfeature extraction and recurrent neural networks for temporal summarisation of\nthe extracted features. We compare CRNN with three CNN structures that have\nbeen used for music tagging while controlling the number of parameters with\nrespect to their performance and training time per sample. Overall, we found\nthat CRNNs show a strong performance with respect to the number of parameter\nand training time, indicating the effectiveness of its hybrid structure in\nmusic feature extraction and feature summarisation. \n\n"}
{"id": "1609.05181", "contents": "Title: Information Theoretic Limits of Data Shuffling for Distributed Learning Abstract: Data shuffling is one of the fundamental building blocks for distributed\nlearning algorithms, that increases the statistical gain for each step of the\nlearning process. In each iteration, different shuffled data points are\nassigned by a central node to a distributed set of workers to perform local\ncomputations, which leads to communication bottlenecks. The focus of this paper\nis on formalizing and understanding the fundamental information-theoretic\ntrade-off between storage (per worker) and the worst-case communication\noverhead for the data shuffling problem. We completely characterize the\ninformation theoretic trade-off for $K=2$, and $K=3$ workers, for any value of\nstorage capacity, and show that increasing the storage across workers can\nreduce the communication overhead by leveraging coding. We propose a novel and\nsystematic data delivery and storage update strategy for each data shuffle\niteration, which preserves the structural properties of the storage across the\nworkers, and aids in minimizing the communication overhead in subsequent data\nshuffling iterations. \n\n"}
{"id": "1609.05767", "contents": "Title: Minimizing Total Busy Time with Application to Energy-efficient\n  Scheduling of Virtual Machines in IaaS clouds Abstract: Infrastructure-as-a-Service (IaaS) clouds have become more popular enabling\nusers to run applications under virtual machines. Energy efficiency for IaaS\nclouds is still challenge. This paper investigates the energy-efficient\nscheduling problems of virtual machines (VMs) onto physical machines (PMs) in\nIaaS clouds along characteristics: multiple resources, fixed intervals and\nnon-preemption of virtual machines. The scheduling problems are NP-hard. Most\nof existing works on VM placement reduce the total energy consumption by using\nthe minimum number of active physical machines. There, however, are cases using\nthe minimum number of physical machines results in longer the total busy time\nof the physical machines. For the scheduling problems, minimizing the total\nenergy consumption of all physical machines is equivalent to minimizing total\nbusy time of all physical machines. In this paper, we propose an scheduling\nalgorithm, denoted as EMinTRE-LFT, for minimizing the total energy consumption\nof physical machines in the scheduling problems. Our extensive simulations\nusing parallel workload models in Parallel Workload Archive show that the\nproposed algorithm has the least total energy consumption compared to the\nstate-of-the art algorithms. \n\n"}
{"id": "1609.07093", "contents": "Title: Neural Photo Editing with Introspective Adversarial Networks Abstract: The increasingly photorealistic sample quality of generative image models\nsuggests their feasibility in applications beyond image generation. We present\nthe Neural Photo Editor, an interface that leverages the power of generative\nneural networks to make large, semantically coherent changes to existing\nimages. To tackle the challenge of achieving accurate reconstructions without\nloss of feature quality, we introduce the Introspective Adversarial Network, a\nnovel hybridization of the VAE and GAN. Our model efficiently captures\nlong-range dependencies through use of a computational block based on\nweight-shared dilated convolutions, and improves generalization performance\nwith Orthogonal Regularization, a novel weight regularization method. We\nvalidate our contributions on CelebA, SVHN, and CIFAR-100, and produce samples\nand reconstructions with high visual fidelity. \n\n"}
{"id": "1609.07750", "contents": "Title: Accurate and Efficient Hyperbolic Tangent Activation Function on FPGA\n  using the DCT Interpolation Filter Abstract: Implementing an accurate and fast activation function with low cost is a\ncrucial aspect to the implementation of Deep Neural Networks (DNNs) on FPGAs.\nWe propose a high-accuracy approximation approach for the hyperbolic tangent\nactivation function of artificial neurons in DNNs. It is based on the Discrete\nCosine Transform Interpolation Filter (DCTIF). The proposed architecture\ncombines simple arithmetic operations on stored samples of the hyperbolic\ntangent function and on input data. The proposed DCTIF implementation achieves\ntwo orders of magnitude greater precision than previous work while using the\nsame or fewer computational resources. Various combinations of DCTIF parameters\ncan be chosen to tradeoff the accuracy and complexity of the hyperbolic tangent\nfunction. In one case, the proposed architecture approximates the hyperbolic\ntangent activation function with 10E-5 maximum error while requiring only 1.52\nKbits memory and 57 LUTs of a Virtex-7 FPGA. We also discuss how the activation\nfunction accuracy affects the performance of DNNs in terms of their training\nand testing accuracies. We show that a high accuracy approximation can be\nnecessary in order to maintain the same DNN training and testing performances\nrealized by the exact function. \n\n"}
{"id": "1609.08144", "contents": "Title: Google's Neural Machine Translation System: Bridging the Gap between\n  Human and Machine Translation Abstract: Neural Machine Translation (NMT) is an end-to-end learning approach for\nautomated translation, with the potential to overcome many of the weaknesses of\nconventional phrase-based translation systems. Unfortunately, NMT systems are\nknown to be computationally expensive both in training and in translation\ninference. Also, most NMT systems have difficulty with rare words. These issues\nhave hindered NMT's use in practical deployments and services, where both\naccuracy and speed are essential. In this work, we present GNMT, Google's\nNeural Machine Translation system, which attempts to address many of these\nissues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder\nlayers using attention and residual connections. To improve parallelism and\ntherefore decrease training time, our attention mechanism connects the bottom\nlayer of the decoder to the top layer of the encoder. To accelerate the final\ntranslation speed, we employ low-precision arithmetic during inference\ncomputations. To improve handling of rare words, we divide words into a limited\nset of common sub-word units (\"wordpieces\") for both input and output. This\nmethod provides a good balance between the flexibility of \"character\"-delimited\nmodels and the efficiency of \"word\"-delimited models, naturally handles\ntranslation of rare words, and ultimately improves the overall accuracy of the\nsystem. Our beam search technique employs a length-normalization procedure and\nuses a coverage penalty, which encourages generation of an output sentence that\nis most likely to cover all the words in the source sentence. On the WMT'14\nEnglish-to-French and English-to-German benchmarks, GNMT achieves competitive\nresults to state-of-the-art. Using a human side-by-side evaluation on a set of\nisolated simple sentences, it reduces translation errors by an average of 60%\ncompared to Google's phrase-based production system. \n\n"}
{"id": "1609.08194", "contents": "Title: Online Segment to Segment Neural Transduction Abstract: We introduce an online neural sequence to sequence model that learns to\nalternate between encoding and decoding segments of the input as it is read. By\nindependently tracking the encoding and decoding representations our algorithm\npermits exact polynomial marginalization of the latent segmentation during\ntraining, and during decoding beam search is employed to find the best\nalignment path together with the predicted output sequence. Our model tackles\nthe bottleneck of vanilla encoder-decoders that have to read and memorize the\nentire input sequence in their fixed-length hidden states before producing any\noutput. It is different from previous attentive models in that, instead of\ntreating the attention weights as output of a deterministic function, our model\nassigns attention weights to a sequential latent variable which can be\nmarginalized out and permits online generation. Experiments on abstractive\nsentence summarization and morphological inflection show significant\nperformance gains over the baseline encoder-decoders. \n\n"}
{"id": "1609.08326", "contents": "Title: Asynchronous Stochastic Gradient Descent with Delay Compensation Abstract: With the fast development of deep learning, it has become common to learn big\nneural networks using massive training data. Asynchronous Stochastic Gradient\nDescent (ASGD) is widely adopted to fulfill this task for its efficiency, which\nis, however, known to suffer from the problem of delayed gradients. That is,\nwhen a local worker adds its gradient to the global model, the global model may\nhave been updated by other workers and this gradient becomes \"delayed\". We\npropose a novel technology to compensate this delay, so as to make the\noptimization behavior of ASGD closer to that of sequential SGD. This is\nachieved by leveraging Taylor expansion of the gradient function and efficient\napproximation to the Hessian matrix of the loss function. We call the new\nalgorithm Delay Compensated ASGD (DC-ASGD). We evaluated the proposed algorithm\non CIFAR-10 and ImageNet datasets, and the experimental results demonstrate\nthat DC-ASGD outperforms both synchronous SGD and asynchronous SGD, and nearly\napproaches the performance of sequential SGD. \n\n"}
{"id": "1609.08574", "contents": "Title: Asynchronous progress design for a MPI-based PGAS one-sided\n  communication system Abstract: Remote-memory-access models, also known as one-sided communication models,\nare becoming an interesting alternative to traditional two-sided communication\nmodels in the field of High Performance Computing. In this paper we extend\nprevious work on an MPI-based, locality-aware remote-memory-access model with a\nasynchronous progress-engine for non-blocking communication operations. Most\nprevious related work suggests to drive progression on communication through an\nadditional thread within the application process. In contrast, our scheme uses\nan arbitrary number of dedicated processes to drive asynchronous progression.\nFurther, we describe a prototypical library implementation of our concepts,\nnamely DART, which is used to quantitatively evaluate our design against a\nMPI-3 baseline reference. The evaluation consists of micro-benchmark to measure\noverlap of communication and computation and a scientific application kernel to\nassess total performance impact on realistic use-cases. Our benchmarks shows,\nthat our asynchronous progression scheme can overlap computation and\ncommunication efficiently and lead to substantially shorter communication cost\nin real applications. \n\n"}
{"id": "1609.08703", "contents": "Title: Optimizing Neural Network Hyperparameters with Gaussian Processes for\n  Dialog Act Classification Abstract: Systems based on artificial neural networks (ANNs) have achieved\nstate-of-the-art results in many natural language processing tasks. Although\nANNs do not require manually engineered features, ANNs have many\nhyperparameters to be optimized. The choice of hyperparameters significantly\nimpacts models' performances. However, the ANN hyperparameters are typically\nchosen by manual, grid, or random search, which either requires expert\nexperiences or is computationally expensive. Recent approaches based on\nBayesian optimization using Gaussian processes (GPs) is a more systematic way\nto automatically pinpoint optimal or near-optimal machine learning\nhyperparameters. Using a previously published ANN model yielding\nstate-of-the-art results for dialog act classification, we demonstrate that\noptimizing hyperparameters using GP further improves the results, and reduces\nthe computational time by a factor of 4 compared to a random search. Therefore\nit is a useful technique for tuning ANN models to yield the best performances\nfor natural language processing tasks. \n\n"}
{"id": "1609.09224", "contents": "Title: Auto-scaling Web Applications in Clouds: A Taxonomy and Survey Abstract: Web application providers have been migrating their applications to cloud\ndata centers, attracted by the emerging cloud computing paradigm. One of the\nappealing features of the cloud is elasticity. It allows cloud users to acquire\nor release computing resources on-demand, which enables web application\nproviders to automatically scale the resources provisioned to their\napplications without human intervention under a dynamic workload to minimize\nresource cost while satisfying Quality of Service (QoS) requirements. In this\npaper, we comprehensively analyze the challenges that remain in auto-scaling\nweb applications in clouds and review the developments in this field. We\npresent a taxonomy of auto-scalers according to the identified challenges and\nkey properties. We analyze the surveyed works and map them to the taxonomy to\nidentify the weaknesses in this field. Moreover, based on the analysis, we\npropose new future directions that can be explored in this area. \n\n"}
{"id": "1609.09563", "contents": "Title: Asynchronous Multi-Task Learning Abstract: Many real-world machine learning applications involve several learning tasks\nwhich are inter-related. For example, in healthcare domain, we need to learn a\npredictive model of a certain disease for many hospitals. The models for each\nhospital may be different because of the inherent differences in the\ndistributions of the patient populations. However, the models are also closely\nrelated because of the nature of the learning tasks modeling the same disease.\nBy simultaneously learning all the tasks, multi-task learning (MTL) paradigm\nperforms inductive knowledge transfer among tasks to improve the generalization\nperformance. When datasets for the learning tasks are stored at different\nlocations, it may not always be feasible to transfer the data to provide a\ndata-centralized computing environment due to various practical issues such as\nhigh data volume and privacy. In this paper, we propose a principled MTL\nframework for distributed and asynchronous optimization to address the\naforementioned challenges. In our framework, gradient update does not wait for\ncollecting the gradient information from all the tasks. Therefore, the proposed\nmethod is very efficient when the communication delay is too high for some task\nnodes. We show that many regularized MTL formulations can benefit from this\nframework, including the low-rank MTL for shared subspace learning. Empirical\nstudies on both synthetic and real-world datasets demonstrate the efficiency\nand effectiveness of the proposed framework. \n\n"}
{"id": "1609.09823", "contents": "Title: On the Worst-case Communication Overhead for Distributed Data Shuffling Abstract: Distributed learning platforms for processing large scale data-sets are\nbecoming increasingly prevalent. In typical distributed implementations, a\ncentralized master node breaks the data-set into smaller batches for parallel\nprocessing across distributed workers to achieve speed-up and efficiency.\nSeveral computational tasks are of sequential nature, and involve multiple\npasses over the data. At each iteration over the data, it is common practice to\nrandomly re-shuffle the data at the master node, assigning different batches\nfor each worker to process. This random re-shuffling operation comes at the\ncost of extra communication overhead, since at each shuffle, new data points\nneed to be delivered to the distributed workers.\n  In this paper, we focus on characterizing the information theoretically\noptimal communication overhead for the distributed data shuffling problem. We\npropose a novel coded data delivery scheme for the case of no excess storage,\nwhere every worker can only store the assigned data batches under processing.\nOur scheme exploits a new type of coding opportunity and is applicable to any\narbitrary shuffle, and for any number of workers. We also present an\ninformation theoretic lower bound on the minimum communication overhead for\ndata shuffling, and show that the proposed scheme matches this lower bound for\nthe worst-case communication overhead. \n\n"}
{"id": "1610.01140", "contents": "Title: Reasoning about identifier spaces: How to make Chord correct Abstract: The Chord distributed hash table (DHT) is well-known and often used to\nimplement peer-to-peer systems. Chord peers find other peers, and access their\ndata, through a ring-shaped pointer structure in a large identifier space.\nDespite claims of proven correctness, i.e., eventual reachability, previous\nwork has shown that the Chord ring-maintenance protocol is not correct under\nits original operating assumptions. Previous work has not, however, discovered\nwhether Chord could be made correct under the same assumptions. The\ncontribution of this paper is to provide the first specification of correct\noperations and initialization for Chord, an inductive invariant that is\nnecessary and sufficient to support a proof of correctness, and two independent\nproofs of correctness. One proof is informal and intuitive, and applies to\nnetworks of any size. The other proof is based on a formal model in Alloy, and\nuses fully automated analysis to prove the assertions for networks of bounded\nsize. The two proofs complement each other in several important ways. \n\n"}
{"id": "1610.01439", "contents": "Title: Nonlinear Systems Identification Using Deep Dynamic Neural Networks Abstract: Neural networks are known to be effective function approximators. Recently,\ndeep neural networks have proven to be very effective in pattern recognition,\nclassification tasks and human-level control to model highly nonlinear\nrealworld systems. This paper investigates the effectiveness of deep neural\nnetworks in the modeling of dynamical systems with complex behavior. Three deep\nneural network structures are trained on sequential data, and we investigate\nthe effectiveness of these networks in modeling associated characteristics of\nthe underlying dynamical systems. We carry out similar evaluations on select\npublicly available system identification datasets. We demonstrate that deep\nneural networks are effective model estimators from input-output data \n\n"}
{"id": "1610.01757", "contents": "Title: Ischemic Stroke Identification Based on EEG and EOG using 1D\n  Convolutional Neural Network and Batch Normalization Abstract: In 2015, stroke was the number one cause of death in Indonesia. The majority\ntype of stroke is ischemic. The standard tool for diagnosing stroke is CT-Scan.\nFor developing countries like Indonesia, the availability of CT-Scan is very\nlimited and still relatively expensive. Because of the availability, another\ndevice that potential to diagnose stroke in Indonesia is EEG. Ischemic stroke\noccurs because of obstruction that can make the cerebral blood flow (CBF) on a\nperson with stroke has become lower than CBF on a normal person (control) so\nthat the EEG signal have a deceleration. On this study, we perform the ability\nof 1D Convolutional Neural Network (1DCNN) to construct classification model\nthat can distinguish the EEG and EOG stroke data from EEG and EOG control data.\nTo accelerate training process our model we use Batch Normalization. Involving\n62 person data object and from leave one out the scenario with five times\nrepetition of measurement we obtain the average of accuracy 0.86 (F-Score\n0.861) only at 200 epoch. This result is better than all over shallow and\npopular classifiers as the comparator (the best result of accuracy 0.69 and\nF-Score 0.72 ). The feature used in our study were only 24 handcrafted feature\nwith simple feature extraction process. \n\n"}
{"id": "1610.01989", "contents": "Title: Regularized Dynamic Boltzmann Machine with Delay Pruning for\n  Unsupervised Learning of Temporal Sequences Abstract: We introduce Delay Pruning, a simple yet powerful technique to regularize\ndynamic Boltzmann machines (DyBM). The recently introduced DyBM provides a\nparticularly structured Boltzmann machine, as a generative model of a\nmulti-dimensional time-series. This Boltzmann machine can have infinitely many\nlayers of units but allows exact inference and learning based on its\nbiologically motivated structure. DyBM uses the idea of conduction delays in\nthe form of fixed length first-in first-out (FIFO) queues, with a neuron\nconnected to another via this FIFO queue, and spikes from a pre-synaptic neuron\ntravel along the queue to the post-synaptic neuron with a constant period of\ndelay. Here, we present Delay Pruning as a mechanism to prune the lengths of\nthe FIFO queues (making them zero) by setting some delay lengths to one with a\nfixed probability, and finally selecting the best performing model with fixed\ndelays. The uniqueness of structure and a non-sampling based learning rule in\nDyBM, make the application of previously proposed regularization techniques\nlike Dropout or DropConnect difficult, leading to poor generalization. First,\nwe evaluate the performance of Delay Pruning to let DyBM learn a\nmultidimensional temporal sequence generated by a Markov chain. Finally, we\nshow the effectiveness of delay pruning in learning high dimensional sequences\nusing the moving MNIST dataset, and compare it with Dropout and DropConnect\nmethods. \n\n"}
{"id": "1610.02348", "contents": "Title: Adaptive Convolutional ELM For Concept Drift Handling in Online Stream\n  Data Abstract: In big data era, the data continuously generated and its distribution may\nkeep changes overtime. These challenges in online stream of data are known as\nconcept drift. In this paper, we proposed the Adaptive Convolutional ELM method\n(ACNNELM) as enhancement of Convolutional Neural Network (CNN) with a hybrid\nExtreme Learning Machine (ELM) model plus adaptive capability. This method is\naimed for concept drift handling. We enhanced the CNN as convolutional\nhiererchical features representation learner combined with Elastic ELM\n(E$^2$LM) as a parallel supervised classifier. We propose an Adaptive OS-ELM\n(AOS-ELM) for concept drift adaptability in classifier level (named ACNNELM-1)\nand matrices concatenation ensembles for concept drift adaptability in ensemble\nlevel (named ACNNELM-2). Our proposed Adaptive CNNELM is flexible that works\nwell in classifier level and ensemble level while most current methods only\nproposed to work on either one of the levels.\n  We verified our method in extended MNIST data set and not MNIST data set. We\nset the experiment to simulate virtual drift, real drift, and hybrid drift\nevent and we demonstrated how our CNNELM adaptability works. Our proposed\nmethod works well and gives better accuracy, computation scalability, and\nconcept drifts adaptability compared to the regular ELM and CNN. Further\nresearches are still required to study the optimum parameters and to use more\nvaried image data set. \n\n"}
{"id": "1610.04714", "contents": "Title: A New Perspective on Randomized Gossip Algorithms Abstract: In this short note we propose a new approach for the design and analysis of\nrandomized gossip algorithms which can be used to solve the average consensus\nproblem. We show how that Randomized Block Kaczmarz (RBK) method - a method for\nsolving linear systems - works as gossip algorithm when applied to a special\nsystem encoding the underlying network. The famous pairwise gossip algorithm\narises as a special case. Subsequently, we reveal a hidden duality of\nrandomized gossip algorithms, with the dual iterative process maintaining a set\nof numbers attached to the edges as opposed to nodes of the network. We prove\nthat RBK obtains a superlinear speedup in the size of the block, and\ndemonstrate this effect through experiments. \n\n"}
{"id": "1610.05507", "contents": "Title: Analysis and Implementation of an Asynchronous Optimization Algorithm\n  for the Parameter Server Abstract: This paper presents an asynchronous incremental aggregated gradient algorithm\nand its implementation in a parameter server framework for solving regularized\noptimization problems. The algorithm can handle both general convex (possibly\nnon-smooth) regularizers and general convex constraints. When the empirical\ndata loss is strongly convex, we establish linear convergence rate, give\nexplicit expressions for step-size choices that guarantee convergence to the\noptimum, and bound the associated convergence factors. The expressions have an\nexplicit dependence on the degree of asynchrony and recover classical results\nunder synchronous operation. Simulations and implementations on commercial\ncompute clouds validate our findings. \n\n"}
{"id": "1610.06258", "contents": "Title: Using Fast Weights to Attend to the Recent Past Abstract: Until recently, research on artificial neural networks was largely restricted\nto systems with only two types of variable: Neural activities that represent\nthe current or recent input and weights that learn to capture regularities\namong inputs, outputs and payoffs. There is no good reason for this\nrestriction. Synapses have dynamics at many different time-scales and this\nsuggests that artificial neural networks might benefit from variables that\nchange slower than activities but much faster than the standard weights. These\n\"fast weights\" can be used to store temporary memories of the recent past and\nthey provide a neurally plausible way of implementing the type of attention to\nthe past that has recently proved very helpful in sequence-to-sequence models.\nBy using fast weights we can avoid the need to store copies of neural activity\npatterns. \n\n"}
{"id": "1610.06759", "contents": "Title: Deterministic Distributed (Delta + o(\\Delta))-Edge-Coloring, and\n  Vertex-Coloring of Graphs with Bounded Diversity Abstract: We consider coloring problems in the distributed message-passing setting. The\npreviously-known deterministic algorithms for edge-coloring employed at least\n(2Delta - 1) colors, even though any graph admits an edge-coloring with Delta +\n1 colors [V64]. Moreover, the previously-known deterministic algorithms that\nemployed at most O(Delta) colors required superlogarithmic time\n[B15,BE10,BE11,FHK15]. In the current paper we devise deterministic\nedge-coloring algorithms that employ only Delta + o(Delta) colors, for a very\nwide family of graphs. Specifically, as long as the arboricity is a =\nO(Delta^{1 - \\epsilon}), for a constant epsilon > 0, our algorithm computes\nsuch a coloring within {polylogarithmic} deterministic time. We also devise\nsignificantly improved deterministic edge-coloring algorithms for {general\ngraphs} for a very wide range of parameters. Specifically, for any value $\\chi$\nin the range [4Delta, 2^{o(log Delta)} \\cdot Delta], our \\chi-edge-coloring\nalgorithm has smaller running time than the best previously-known\n\\chi-edge-coloring algorithms. Our algorithms are actually much more general,\nsince edge-coloring is equivalent to {vertex-coloring of line graphs.} Our\nmethod is applicable to vertex-coloring of the family of graphs with {bounded\ndiversity} that contains line graphs, line graphs of hypergraphs, and many\nother graphs.\n  Our results are obtained using a novel technique that connects vertices or\nedges in a certain way that reduces clique size. The resulting structures,\nwhich we call {connectors}, can be colored more efficiently than the original\ngraph. Moreover, the color classes constitute simpler subgraphs that can be\ncolored even more efficiently using appropriate connectors. Hence, we recurse\nuntil we obtain sufficiently simple structures that are colored directly. We\nintroduce several types of connectors that are useful for various scenarios. \n\n"}
{"id": "1610.07236", "contents": "Title: Hybrid Static/Dynamic Schedules for Tiled Polyhedral Programs Abstract: Polyhedral compilers perform optimizations such as tiling and\nparallelization; when doing both, they usually generate code that executes\n\"barrier-synchronized wavefronts\" of tiles. We present a system to express and\ngenerate code for hybrid schedules, where some constraints are automatically\nsatisfied through the structure of the code, and the remainder are dynamically\nenforced at run-time with data flow mechanisms. We prove bounds on the added\noverheads that are better, by at least one polynomial degree, than those of\nprevious techniques.\n  We propose a generic mechanism to implement the needed synchronization, and\nshow it can be easily realized for a variety of targets: OpenMP, Pthreads, GPU\n(CUDA or OpenCL) code, languages like X10, Habanero, Cilk, as well as data flow\nplatforms like DAGuE, and OpenStream and MPI. We also provide a simple concrete\nimplementation that works without the need of any sophisticated run-time\nmechanism.\n  Our experiments show our simple implementation to be competitive or better\nthan the wavefront-synchronized code generated by other systems. We also show\nhow the proposed mechanism can achieve 24% to 70% reduction in energy. \n\n"}
{"id": "1610.08833", "contents": "Title: A Survey of High Level Frameworks in Block-Structured Adaptive Mesh\n  Refinement Packages Abstract: Over the last decade block-structured adaptive mesh refinement (SAMR) has\nfound increasing use in large, publicly available codes and frameworks. SAMR\nframeworks have evolved along different paths. Some have stayed focused on\nspecific domain areas, others have pursued a more general functionality,\nproviding the building blocks for a larger variety of applications. In this\nsurvey paper we examine a representative set of SAMR packages and SAMR-based\ncodes that have been in existence for half a decade or more, have a reasonably\nsized and active user base outside of their home institutions, and are publicly\navailable. The set consists of a mix of SAMR packages and application codes\nthat cover a broad range of scientific domains. We look at their high-level\nframeworks, and their approach to dealing with the advent of radical changes in\nhardware architecture. The codes included in this survey are BoxLib, Cactus,\nChombo, Enzo, FLASH, and Uintah. \n\n"}
{"id": "1610.09146", "contents": "Title: Performance evaluation of explicit finite difference algorithms with\n  varying amounts of computational and memory intensity Abstract: Future architectures designed to deliver exascale performance motivate the\nneed for novel algorithmic changes in order to fully exploit their\ncapabilities. In this paper, the performance of several numerical algorithms,\ncharacterised by varying degrees of memory and computational intensity, are\nevaluated in the context of finite difference methods for fluid dynamics\nproblems. It is shown that, by storing some of the evaluated derivatives as\nsingle thread- or process-local variables in memory, or recomputing the\nderivatives on-the-fly, a speed-up of ~2 can be obtained compared to\ntraditional algorithms that store all derivatives in global arrays. \n\n"}
{"id": "1611.00035", "contents": "Title: Full-Capacity Unitary Recurrent Neural Networks Abstract: Recurrent neural networks are powerful models for processing sequential data,\nbut they are generally plagued by vanishing and exploding gradient problems.\nUnitary recurrent neural networks (uRNNs), which use unitary recurrence\nmatrices, have recently been proposed as a means to avoid these issues.\nHowever, in previous experiments, the recurrence matrices were restricted to be\na product of parameterized unitary matrices, and an open question remains: when\ndoes such a parameterization fail to represent all unitary matrices, and how\ndoes this restricted representational capacity limit what can be learned? To\naddress this question, we propose full-capacity uRNNs that optimize their\nrecurrence matrix over all unitary matrices, leading to significantly improved\nperformance over uRNNs that use a restricted-capacity recurrence matrix. Our\ncontribution consists of two main components. First, we provide a theoretical\nargument to determine if a unitary parameterization has restricted capacity.\nUsing this argument, we show that a recently proposed unitary parameterization\nhas restricted capacity for hidden state dimension greater than 7. Second, we\nshow how a complete, full-capacity unitary recurrence matrix can be optimized\nover the differentiable manifold of unitary matrices. The resulting\nmultiplicative gradient step is very simple and does not require gradient\nclipping or learning rate adaptation. We confirm the utility of our claims by\nempirically evaluating our new full-capacity uRNNs on both synthetic and\nnatural data, achieving superior performance compared to both LSTMs and the\noriginal restricted-capacity uRNNs. \n\n"}
{"id": "1611.00404", "contents": "Title: Per-Server Dominant-Share Fairness (PS-DSF): A Multi-Resource Fair\n  Allocation Mechanism for Heterogeneous Servers Abstract: Users of cloud computing platforms pose different types of demands for\nmultiple resources on servers (physical or virtual machines). Besides\ndifferences in their resource capacities, servers may be additionally\nheterogeneous in their ability to service users - certain users' tasks may only\nbe serviced by a subset of the servers. We identify important shortcomings in\nexisting multi-resource fair allocation mechanisms - Dominant Resource Fairness\n(DRF) and its follow up work - when used in such environments. We develop a new\nfair allocation mechanism called Per-Server Dominant-Share Fairness (PS-DSF)\nwhich we show offers all desirable sharing properties that DRF is able to offer\nin the case of a single \"resource pool\" (i.e., if the resources of all servers\nwere pooled together into one hypothetical server). We evaluate the performance\nof PS-DSF through simulations. Our evaluation shows the enhanced efficiency of\nPS-DSF compared to the existing allocation mechanisms. We argue how our\nproposed allocation mechanism is applicable in cloud computing networks and\nespecially large scale data-centers. \n\n"}
{"id": "1611.00847", "contents": "Title: Deep Convolutional Neural Network Design Patterns Abstract: Recent research in the deep learning field has produced a plethora of new\narchitectures. At the same time, a growing number of groups are applying deep\nlearning to new applications. Some of these groups are likely to be composed of\ninexperienced deep learning practitioners who are baffled by the dizzying array\nof architecture choices and therefore opt to use an older architecture (i.e.,\nAlexnet). Here we attempt to bridge this gap by mining the collective knowledge\ncontained in recent deep learning research to discover underlying principles\nfor designing neural network architectures. In addition, we describe several\narchitectural innovations, including Fractal of FractalNet network, Stagewise\nBoosting Networks, and Taylor Series Networks (our Caffe code and prototxt\nfiles is available at https://github.com/iPhysicist/CNNDesignPatterns). We hope\nothers are inspired to build on our preliminary work. \n\n"}
{"id": "1611.00864", "contents": "Title: Spatio-temporal Dynamics of Intrinsic Networks in Functional Magnetic\n  Imaging Data Using Recurrent Neural Networks Abstract: We introduce a novel recurrent neural network (RNN) approach to account for\ntemporal dynamics and dependencies in brain networks observed via functional\nmagnetic resonance imaging (fMRI). Our approach directly parameterizes temporal\ndynamics through recurrent connections, which can be used to formulate blind\nsource separation with a conditional (rather than marginal) independence\nassumption, which we call RNN-ICA. This formulation enables us to visualize the\ntemporal dynamics of both first order (activity) and second order (directed\nconnectivity) information in brain networks that are widely studied in a static\nsense, but not well-characterized dynamically. RNN-ICA predicts dynamics\ndirectly from the recurrent states of the RNN in both task and resting state\nfMRI. Our results show both task-related and group-differentiating directed\nconnectivity. \n\n"}
{"id": "1611.01578", "contents": "Title: Neural Architecture Search with Reinforcement Learning Abstract: Neural networks are powerful and flexible models that work well for many\ndifficult learning tasks in image, speech and natural language understanding.\nDespite their success, neural networks are still hard to design. In this paper,\nwe use a recurrent network to generate the model descriptions of neural\nnetworks and train this RNN with reinforcement learning to maximize the\nexpected accuracy of the generated architectures on a validation set. On the\nCIFAR-10 dataset, our method, starting from scratch, can design a novel network\narchitecture that rivals the best human-invented architecture in terms of test\nset accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is\n0.09 percent better and 1.05x faster than the previous state-of-the-art model\nthat used a similar architectural scheme. On the Penn Treebank dataset, our\nmodel can compose a novel recurrent cell that outperforms the widely-used LSTM\ncell, and other state-of-the-art baselines. Our cell achieves a test set\nperplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than\nthe previous state-of-the-art model. The cell can also be transferred to the\ncharacter language modeling task on PTB and achieves a state-of-the-art\nperplexity of 1.214. \n\n"}
{"id": "1611.01639", "contents": "Title: Robustly representing uncertainty in deep neural networks through\n  sampling Abstract: As deep neural networks (DNNs) are applied to increasingly challenging\nproblems, they will need to be able to represent their own uncertainty.\nModeling uncertainty is one of the key features of Bayesian methods. Using\nBernoulli dropout with sampling at prediction time has recently been proposed\nas an efficient and well performing variational inference method for DNNs.\nHowever, sampling from other multiplicative noise based variational\ndistributions has not been investigated in depth. We evaluated Bayesian DNNs\ntrained with Bernoulli or Gaussian multiplicative masking of either the units\n(dropout) or the weights (dropconnect). We tested the calibration of the\nprobabilistic predictions of Bayesian convolutional neural networks (CNNs) on\nMNIST and CIFAR-10. Sampling at prediction time increased the calibration of\nthe DNNs' probabalistic predictions. Sampling weights, whether Gaussian or\nBernoulli, led to more robust representation of uncertainty compared to\nsampling of units. However, using either Gaussian or Bernoulli dropout led to\nincreased test set classification accuracy. Based on these findings we used\nboth Bernoulli dropout and Gaussian dropconnect concurrently, which we show\napproximates the use of a spike-and-slab variational distribution without\nincreasing the number of learned parameters. We found that spike-and-slab\nsampling had higher test set performance than Gaussian dropconnect and more\nrobustly represented its uncertainty compared to Bernoulli dropout. \n\n"}
{"id": "1611.01734", "contents": "Title: Deep Biaffine Attention for Neural Dependency Parsing Abstract: This paper builds off recent work from Kiperwasser & Goldberg (2016) using\nneural attention in a simple graph-based dependency parser. We use a larger but\nmore thoroughly regularized parser than other recent BiLSTM-based approaches,\nwith biaffine classifiers to predict arcs and labels. Our parser gets state of\nthe art or near state of the art performance on standard treebanks for six\ndifferent languages, achieving 95.7% UAS and 94.1% LAS on the most popular\nEnglish PTB dataset. This makes it the highest-performing graph-based parser on\nthis benchmark---outperforming Kiperwasser Goldberg (2016) by 1.8% and\n2.2%---and comparable to the highest performing transition-based parser\n(Kuncoro et al., 2016), which achieves 95.8% UAS and 94.6% LAS. We also show\nwhich hyperparameter choices had a significant effect on parsing accuracy,\nallowing us to achieve large gains over other graph-based approaches. \n\n"}
{"id": "1611.01967", "contents": "Title: Regularizing CNNs with Locally Constrained Decorrelations Abstract: Regularization is key for deep learning since it allows training more complex\nmodels while keeping lower levels of overfitting. However, the most prevalent\nregularizations do not leverage all the capacity of the models since they rely\non reducing the effective number of parameters. Feature decorrelation is an\nalternative for using the full capacity of the models but the overfitting\nreduction margins are too narrow given the overhead it introduces. In this\npaper, we show that regularizing negatively correlated features is an obstacle\nfor effective decorrelation and present OrthoReg, a novel regularization\ntechnique that locally enforces feature orthogonality. As a result, imposing\nlocality constraints in feature decorrelation removes interferences between\nnegatively correlated feature weights, allowing the regularizer to reach higher\ndecorrelation bounds, and reducing the overfitting more effectively. In\nparticular, we show that the models regularized with OrthoReg have higher\naccuracy bounds even when batch normalization and dropout are present.\nMoreover, since our regularization is directly performed on the weights, it is\nespecially suitable for fully convolutional neural networks, where the weight\nspace is constant compared to the feature map space. As a result, we are able\nto reduce the overfitting of state-of-the-art CNNs on CIFAR-10, CIFAR-100, and\nSVHN. \n\n"}
{"id": "1611.02261", "contents": "Title: Memory-augmented Attention Modelling for Videos Abstract: We present a method to improve video description generation by modeling\nhigher-order interactions between video frames and described concepts. By\nstoring past visual attention in the video associated to previously generated\nwords, the system is able to decide what to look at and describe in light of\nwhat it has already looked at and described. This enables not only more\neffective local attention, but tractable consideration of the video sequence\nwhile generating each word. Evaluation on the challenging and popular MSVD and\nCharades datasets demonstrates that the proposed architecture outperforms\nprevious video description approaches without requiring external temporal video\nfeatures. \n\n"}
{"id": "1611.02589", "contents": "Title: An Optimal Ancestry Labeling Scheme with Applications to XML Trees and\n  Universal Posets Abstract: In this paper we solve the ancestry-labeling scheme problem which aims at\nassigning the shortest possible labels (bit strings) to nodes of rooted trees,\nso that ancestry queries between any two nodes can be answered by inspecting\ntheir assigned labels only. This problem was introduced more than twenty years\nago by Kannan et al. [STOC '88], and is among the most well-studied problems in\nthe field of informative labeling schemes. We construct an ancestry-labeling\nscheme for $n$-node trees with label size $\\log_2 n + O(\\log \\log n)$ bits,\nthus matching the $\\log_2 n + O(\\log \\log n)$ bits lower bound given by Alstrup\net al. [SODA '03]. Our scheme is based on a simplified ancestry scheme that\noperates extremely well on a restricted set of trees. In particular, for the\nset of n-node trees with depth at most d, the simplified ancestry scheme enjoys\nlabel size of $\\log_2 n + 2 \\log_2 d + O(1)$ bits. Since the depth of most XML\ntrees is at most some small constant, such an ancestry scheme may be of\npractical use. In addition, we also obtain an adjacency-labeling scheme that\nlabels n-node trees of depth d with labels of size $\\log_2 n + 3 \\log_2 d +\nO(1)$ bits. All our schemes assign the labels in linear time, and guarantee\nthat any query can be answered in constant time. Finally, our ancestry scheme\nfinds applications to the construction of small universal partially ordered\nsets (posets). Specifically, for any fixed integer k, it enables the\nconstruction of a universal poset of size $\\tilde O(n^k)$ for the family of\n$n$-element posets with tree-dimension at most $k$. Up to lower order terms,\nthis bound is tight thanks to a lower bound of $n^{k-o(1)}$ due to Alon and\nScheinerman [Order '88]. \n\n"}
{"id": "1611.03215", "contents": "Title: CMS software and computing for LHC Run 2 Abstract: The CMS offline software and computing system has successfully met the\nchallenge of LHC Run 2. In this presentation, we will discuss how the entire\nsystem was improved in anticipation of increased trigger output rate, increased\nrate of pileup interactions and the evolution of computing technology. The\nprimary goals behind these changes was to increase the flexibility of computing\nfacilities where ever possible, as to increase our operational efficiency, and\nto decrease the computing resources needed to accomplish the primary offline\ncomputing workflows. These changes have resulted in a new approach to\ndistributed computing in CMS for Run 2 and for the future as the LHC luminosity\nshould continue to increase. We will discuss changes and plans to our data\nfederation, which was one of the key changes towards a more flexible computing\nmodel for Run 2. Our software framework and algorithms also underwent\nsignificant changes. We will summarize the our experience with a new\nmulti-threaded framework as deployed on our prompt reconstruction farm for 2015\nand across the CMS WLCG Tier-1 facilities. We will discuss our experience with\na analysis data format which is ten times smaller than our primary Run 1\nformat. This \"miniAOD\" format has proven to be easier to analyze while be\nextremely flexible for analysts. Finally, we describe improvements to our\nworkflow management system that have resulted in increased automation and\nreliability for all facets of CMS production and user analysis operations. \n\n"}
{"id": "1611.03607", "contents": "Title: Deep Recurrent Neural Network for Mobile Human Activity Recognition with\n  High Throughput Abstract: In this paper, we propose a method of human activity recognition with high\nthroughput from raw accelerometer data applying a deep recurrent neural network\n(DRNN), and investigate various architectures and its combination to find the\nbest parameter values. The \"high throughput\" refers to short time at a time of\nrecognition. We investigated various parameters and architectures of the DRNN\nby using the training dataset of 432 trials with 6 activity classes from 7\npeople. The maximum recognition rate was 95.42% and 83.43% against the test\ndata of 108 segmented trials each of which has single activity class and 18\nmultiple sequential trials, respectively. Here, the maximum recognition rates\nby traditional methods were 71.65% and 54.97% for each. In addition, the\nefficiency of the found parameters was evaluated by using additional dataset.\nFurther, as for throughput of the recognition per unit time, the constructed\nDRNN was requiring only 1.347 [ms], while the best traditional method required\n11.031 [ms] which includes 11.027 [ms] for feature calculation. These\nadvantages are caused by the compact and small architecture of the constructed\nreal time oriented DRNN. \n\n"}
{"id": "1611.04581", "contents": "Title: How to scale distributed deep learning? Abstract: Training time on large datasets for deep neural networks is the principal\nworkflow bottleneck in a number of important applications of deep learning,\nsuch as object classification and detection in automatic driver assistance\nsystems (ADAS). To minimize training time, the training of a deep neural\nnetwork must be scaled beyond a single machine to as many machines as possible\nby distributing the optimization method used for training. While a number of\napproaches have been proposed for distributed stochastic gradient descent\n(SGD), at the current time synchronous approaches to distributed SGD appear to\nbe showing the greatest performance at large scale. Synchronous scaling of SGD\nsuffers from the need to synchronize all processors on each gradient step and\nis not resilient in the face of failing or lagging processors. In asynchronous\napproaches using parameter servers, training is slowed by contention to the\nparameter server. In this paper we compare the convergence of synchronous and\nasynchronous SGD for training a modern ResNet network architecture on the\nImageNet classification problem. We also propose an asynchronous method,\ngossiping SGD, that aims to retain the positive features of both systems by\nreplacing the all-reduce collective operation of synchronous training with a\ngossip aggregation algorithm. We find, perhaps counterintuitively, that\nasynchronous SGD, including both elastic averaging and gossiping, converges\nfaster at fewer nodes (up to about 32 nodes), whereas synchronous SGD scales\nbetter to more nodes (up to about 100 nodes). \n\n"}
{"id": "1611.05827", "contents": "Title: Towards a Mathematical Understanding of the Difficulty in Learning with\n  Feedforward Neural Networks Abstract: Training deep neural networks for solving machine learning problems is one\ngreat challenge in the field, mainly due to its associated optimisation problem\nbeing highly non-convex. Recent developments have suggested that many training\nalgorithms do not suffer from undesired local minima under certain scenario,\nand consequently led to great efforts in pursuing mathematical explanations for\nsuch observations. This work provides an alternative mathematical understanding\nof the challenge from a smooth optimisation perspective. By assuming exact\nlearning of finite samples, sufficient conditions are identified via a critical\npoint analysis to ensure any local minimum to be globally minimal as well.\nFurthermore, a state of the art algorithm, known as the Generalised\nGauss-Newton (GGN) algorithm, is rigorously revisited as an approximate\nNewton's algorithm, which shares the property of being locally quadratically\nconvergent to a global minimum under the condition of exact learning. \n\n"}
{"id": "1611.06310", "contents": "Title: Local minima in training of neural networks Abstract: There has been a lot of recent interest in trying to characterize the error\nsurface of deep models. This stems from a long standing question. Given that\ndeep networks are highly nonlinear systems optimized by local gradient methods,\nwhy do they not seem to be affected by bad local minima? It is widely believed\nthat training of deep models using gradient methods works so well because the\nerror surface either has no local minima, or if they exist they need to be\nclose in value to the global minimum. It is known that such results hold under\nvery strong assumptions which are not satisfied by real models. In this paper\nwe present examples showing that for such theorem to be true additional\nassumptions on the data, initialization schemes and/or the model classes have\nto be made. We look at the particular case of finite size datasets. We\ndemonstrate that in this scenario one can construct counter-examples (datasets\nor initialization schemes) when the network does become susceptible to bad\nlocal minima over the weight space. \n\n"}
{"id": "1611.06565", "contents": "Title: Deep Tensor Convolution on Multicores Abstract: Deep convolutional neural networks (ConvNets) of 3-dimensional kernels allow\njoint modeling of spatiotemporal features. These networks have improved\nperformance of video and volumetric image analysis, but have been limited in\nsize due to the low memory ceiling of GPU hardware. Existing CPU\nimplementations overcome this constraint but are impractically slow. Here we\nextend and optimize the faster Winograd-class of convolutional algorithms to\nthe $N$-dimensional case and specifically for CPU hardware. First, we remove\nthe need to manually hand-craft algorithms by exploiting the relaxed\nconstraints and cheap sparse access of CPU memory. Second, we maximize CPU\nutilization and multicore scalability by transforming data matrices to be\ncache-aware, integer multiples of AVX vector widths. Treating 2-dimensional\nConvNets as a special (and the least beneficial) case of our approach, we\ndemonstrate a 5 to 25-fold improvement in throughput compared to previous\nstate-of-the-art. \n\n"}
{"id": "1611.06576", "contents": "Title: Distributed Nonconvex Optimization for Sparse Representation Abstract: We consider a non-convex constrained Lagrangian formulation of a fundamental\nbi-criteria optimization problem for variable selection in statistical\nlearning; the two criteria are a smooth (possibly) nonconvex loss function,\nmeasuring the fitness of the model to data, and the latter function is a\ndifference-of-convex (DC) regularization, employed to promote some extra\nstructure on the solution, like sparsity. This general class of nonconvex\nproblems arises in many big-data applications, from statistical machine\nlearning to physical sciences and engineering. We develop the first unified\ndistributed algorithmic framework for these problems and establish its\nasymptotic convergence to d-stationary solutions. Two key features of the\nmethod are: i) it can be implemented on arbitrary networks (digraphs) with\n(possibly) time-varying connectivity; and ii) it does not require the\nrestrictive assumption that the (sub)gradient of the objective function is\nbounded, which enlarges significantly the class of statistical learning\nproblems that can be solved with convergence guarantees. \n\n"}
{"id": "1611.06864", "contents": "Title: Population Protocols with Faulty Interactions: the Impact of a Leader Abstract: We consider the problem of simulating traditional population protocols under\nweaker models of communication, which include one-way interactions (as opposed\nto two-way interactions) and omission faults (i.e., failure by an agent to read\nits partner's state during an interaction), which in turn may be detectable or\nundetectable. We focus on the impact of a leader, and we give a complete\ncharacterization of the models in which the presence of a unique leader in the\nsystem allows the construction of simulators: when simulations are possible, we\ngive explicit protocols; when they are not, we give proofs of impossibility.\nSpecifically, if each agent has only a finite amount of memory, the simulation\nis possible only if there are no omission faults. If agents have an unbounded\namount of memory, the simulation is possible as long as omissions are\ndetectable. If an upper bound on the number of omissions involving the leader\nis known, the simulation is always possible, except in the one-way model in\nwhich one side is unable to detect the interaction. \n\n"}
{"id": "1611.07238", "contents": "Title: Time and Space Optimal Counting in Population Protocols Abstract: This work concerns the general issue of combined optimality in terms of time\nand space complexity. In this context, we study the problem of (exact) counting\nresource-limited and passively mobile nodes in the model of population\nprotocols, in which the space complexity is crucial. The counted nodes are\nmemory-limited anonymous devices (called agents) communicating asynchronously\nin pairs (according to a fairness condition). Moreover, we assume that these\nagents are prone to failures so that they cannot be correctly initialized. This\nstudy considers two classical fairness conditions, and for each we investigate\nthe issue of time optimality of counting given the optimal space per agent. In\nthe case of randomly interacting agents (probabilistic fairness), as usual, the\nconvergence time is measured in terms of parallel time (or parallel\ninteractions), which is defined as the number of pairwise interactions until\nconvergence, divided by n (the number of agents). In case of weak fairness,\nwhere it is only required that every pair of agents interacts infinitely often,\nthe convergence time is defined in terms of non-null transitions, i.e, the\ntransitions that affect the states of the interacting agents.First, assuming\nprobabilistic fairness, we present a \"non-guessing\" time optimal protocol of\nO(n log n) expected time given an optimal space of only one bit, and we prove\nthe time optimality of this protocol. Then, for weak fairness, we show that a\nspace optimal (semi-uniform) solution cannot converge faster than in\n$\\Omega$(2^n) time (non-null transitions). This result, together with the time\ncomplexity analysis of an already known space optimal protocol, shows that it\nis also optimal in time (given the optimal space constrains). \n\n"}
{"id": "1611.08083", "contents": "Title: Survey of Expressivity in Deep Neural Networks Abstract: We survey results on neural network expressivity described in \"On the\nExpressive Power of Deep Neural Networks\". The paper motivates and develops\nthree natural measures of expressiveness, which all display an exponential\ndependence on the depth of the network. In fact, all of these measures are\nrelated to a fourth quantity, trajectory length. This quantity grows\nexponentially in the depth of the network, and is responsible for the depth\nsensitivity observed. These results translate to consequences for networks\nduring and after training. They suggest that parameters earlier in a network\nhave greater influence on its expressive power -- in particular, given a layer,\nits influence on expressivity is determined by the remaining depth of the\nnetwork after that layer. This is verified with experiments on MNIST and\nCIFAR-10. We also explore the effect of training on the input-output map, and\nfind that it trades off between the stability and expressivity. \n\n"}
{"id": "1611.08554", "contents": "Title: Asynchronous Distributed Automata: A Characterization of the Modal\n  Mu-Fragment Abstract: We establish the equivalence between a class of asynchronous distributed\nautomata and a small fragment of least fixpoint logic, when restricted to\nfinite directed graphs. More specifically, the logic we consider is (a variant\nof) the fragment of the modal $\\mu$-calculus that allows least fixpoints but\nforbids greatest fixpoints. The corresponding automaton model uses a network of\nidentical finite-state machines that communicate in an asynchronous manner and\nwhose state diagram must be acyclic except for self-loops. Exploiting the\nconnection with logic, we also prove that the expressive power of those\nmachines is independent of whether or not messages can be lost. \n\n"}
{"id": "1611.09168", "contents": "Title: A duality-based approach for distributed min-max optimization Abstract: In this paper we consider a distributed optimization scenario in which a set\nof processors aims at cooperatively solving a class of min-max optimization\nproblems. This set-up is motivated by peak-demand minimization problems in\nsmart grids. Here, the goal is to minimize the peak value over a finite horizon\nwith: (i) the demand at each time instant being the sum of contributions from\ndifferent devices, and (ii) the device states at different time instants being\ncoupled through local constraints (e.g., the dynamics). The min-max structure\nand the double coupling (through the devices and over the time horizon) makes\nthis problem challenging in a distributed set-up (e.g., existing distributed\ndual decomposition approaches cannot be applied). We propose a distributed\nalgorithm based on the combination of duality methods and properties from\nmin-max optimization. Specifically, we repeatedly apply duality theory and\nproperly introduce ad-hoc slack variables in order to derive a series of\nequivalent problems. On the resulting problem we apply a dual subgradient\nmethod, which turns out to be a distributed algorithm consisting of a\nminimization on the original primal variables and a suitable dual update. We\nprove the convergence of the proposed algorithm in objective value. Moreover,\nwe show that every limit point of the primal sequence is an optimal (feasible)\nsolution. Finally, we provide numerical computations for a peak-demand\noptimization problem in a network of thermostatically controlled loads. \n\n"}
{"id": "1611.09913", "contents": "Title: Capacity and Trainability in Recurrent Neural Networks Abstract: Two potential bottlenecks on the expressiveness of recurrent neural networks\n(RNNs) are their ability to store information about the task in their\nparameters, and to store information about the input history in their units. We\nshow experimentally that all common RNN architectures achieve nearly the same\nper-task and per-unit capacity bounds with careful training, for a variety of\ntasks and stacking depths. They can store an amount of task information which\nis linear in the number of parameters, and is approximately 5 bits per\nparameter. They can additionally store approximately one real number from their\ninput history per hidden unit. We further find that for several tasks it is the\nper-task parameter capacity bound that determines performance. These results\nsuggest that many previous results comparing RNN architectures are driven\nprimarily by differences in training effectiveness, rather than differences in\ncapacity. Supporting this observation, we compare training difficulty for\nseveral architectures, and show that vanilla RNNs are far more difficult to\ntrain, yet have slightly higher capacity. Finally, we propose two novel RNN\narchitectures, one of which is easier to train than the LSTM or GRU for deeply\nstacked architectures. \n\n"}
{"id": "1611.10068", "contents": "Title: Stateless Computation Abstract: We present and explore a model of stateless and self-stabilizing distributed\ncomputation, inspired by real-world applications such as routing on today's\nInternet. Processors in our model do not have an internal state, but rather\ninteract by repeatedly mapping incoming messages (\"labels\") to outgoing\nmessages and output values. While seemingly too restrictive to be of interest,\nstateless computation encompasses both classical game-theoretic notions of\nstrategic interaction and a broad range of practical applications (e.g.,\nInternet protocols, circuits, diffusion of technologies in social networks). We\nembark on a holistic exploration of stateless computation. We tackle two\nimportant questions: (1) Under what conditions is self-stabilization, i.e.,\nguaranteed \"convergence\" to a \"legitimate\" global configuration, achievable for\nstateless computation? and (2) What is the computational power of stateless\ncomputation? Our results for self-stabilization include a general necessary\ncondition for self-stabilization and hardness results for verifying that a\nstateless protocol is self-stabilizing. Our main results for the power of\nstateless computation show that labels of logarithmic length in the number of\nprocessors yield substantial computational power even on ring topologies. We\npresent a separation between unidirectional and bidirectional rings (L/poly vs.\nP/poly), reflecting the sequential nature of computation on a unidirectional\nring, as opposed to the parallelism afforded by the bidirectional ring. We\nleave the reader with many exciting directions for future research. \n\n"}
{"id": "1612.00712", "contents": "Title: Probabilistic Neural Programs Abstract: We present probabilistic neural programs, a framework for program induction\nthat permits flexible specification of both a computational model and inference\nalgorithm while simultaneously enabling the use of deep neural networks.\nProbabilistic neural programs combine a computation graph for specifying a\nneural network with an operator for weighted nondeterministic choice. Thus, a\nprogram describes both a collection of decisions as well as the neural network\narchitecture used to make each one. We evaluate our approach on a challenging\ndiagram question answering task where probabilistic neural programs correctly\nexecute nearly twice as many programs as a baseline model. \n\n"}
{"id": "1612.00817", "contents": "Title: Summary - TerpreT: A Probabilistic Programming Language for Program\n  Induction Abstract: We study machine learning formulations of inductive program synthesis; that\nis, given input-output examples, synthesize source code that maps inputs to\ncorresponding outputs. Our key contribution is TerpreT, a domain-specific\nlanguage for expressing program synthesis problems. A TerpreT model is composed\nof a specification of a program representation and an interpreter that\ndescribes how programs map inputs to outputs. The inference task is to observe\na set of input-output examples and infer the underlying program. From a TerpreT\nmodel we automatically perform inference using four different back-ends:\ngradient descent (thus each TerpreT model can be seen as defining a\ndifferentiable interpreter), linear program (LP) relaxations for graphical\nmodels, discrete satisfiability solving, and the Sketch program synthesis\nsystem. TerpreT has two main benefits. First, it enables rapid exploration of a\nrange of domains, program representations, and interpreter models. Second, it\nseparates the model specification from the inference algorithm, allowing proper\ncomparisons between different approaches to inference.\n  We illustrate the value of TerpreT by developing several interpreter models\nand performing an extensive empirical comparison between alternative inference\nalgorithms on a variety of program models. To our knowledge, this is the first\nwork to compare gradient-based search over program space to traditional\nsearch-based alternatives. Our key empirical finding is that constraint solvers\ndominate the gradient descent and LP-based formulations.\n  This is a workshop summary of a longer report at arXiv:1608.04428 \n\n"}
{"id": "1612.01294", "contents": "Title: Message Passing Multi-Agent GANs Abstract: Communicating and sharing intelligence among agents is an important facet of\nachieving Artificial General Intelligence. As a first step towards this\nchallenge, we introduce a novel framework for image generation: Message Passing\nMulti-Agent Generative Adversarial Networks (MPM GANs). While GANs have\nrecently been shown to be very effective for image generation and other tasks,\nthese networks have been limited to mostly single generator-discriminator\nnetworks. We show that we can obtain multi-agent GANs that communicate through\nmessage passing to achieve better image generation. The objectives of the\nindividual agents in this framework are two fold: a co-operation objective and\na competing objective. The co-operation objective ensures that the message\nsharing mechanism guides the other generator to generate better than itself\nwhile the competing objective encourages each generator to generate better than\nits counterpart. We analyze and visualize the messages that these GANs share\namong themselves in various scenarios. We quantitatively show that the message\nsharing formulation serves as a regularizer for the adversarial training.\nQualitatively, we show that the different generators capture different traits\nof the underlying data distribution. \n\n"}
{"id": "1612.01458", "contents": "Title: Support vector regression model for BigData systems Abstract: Nowadays Big Data are becoming more and more important. Many sectors of our\neconomy are now guided by data-driven decision processes. Big Data and business\nintelligence applications are facilitated by the MapReduce programming model\nwhile, at infrastructural layer, cloud computing provides flexible and cost\neffective solutions for allocating on demand large clusters. In such systems,\ncapacity allocation, which is the ability to optimally size minimal resources\nfor achieve a certain level of performance, is a key challenge to enhance\nperformance for MapReduce jobs and minimize cloud resource costs. In order to\ndo so, one of the biggest challenge is to build an accurate performance model\nto estimate job execution time of MapReduce systems. Previous works applied\nsimulation based models for modeling such systems. Although this approach can\naccurately describe the behavior of Big Data clusters, it is too\ncomputationally expensive and does not scale to large system. We try to\novercome these issues by applying machine learning techniques. More precisely\nwe focus on Support Vector Regression (SVR) which is intrinsically more robust\nw.r.t other techniques, like, e.g., neural networks, and less sensitive to\noutliers in the training set. To better investigate these benefits, we compare\nSVR to linear regression. \n\n"}
{"id": "1612.03079", "contents": "Title: Clipper: A Low-Latency Online Prediction Serving System Abstract: Machine learning is being deployed in a growing number of applications which\ndemand real-time, accurate, and robust predictions under heavy query load.\nHowever, most machine learning frameworks and systems only address model\ntraining and not deployment.\n  In this paper, we introduce Clipper, a general-purpose low-latency prediction\nserving system. Interposing between end-user applications and a wide range of\nmachine learning frameworks, Clipper introduces a modular architecture to\nsimplify model deployment across frameworks and applications. Furthermore, by\nintroducing caching, batching, and adaptive model selection techniques, Clipper\nreduces prediction latency and improves prediction throughput, accuracy, and\nrobustness without modifying the underlying machine learning frameworks. We\nevaluate Clipper on four common machine learning benchmark datasets and\ndemonstrate its ability to meet the latency, accuracy, and throughput demands\nof online serving applications. Finally, we compare Clipper to the TensorFlow\nServing system and demonstrate that we are able to achieve comparable\nthroughput and latency while enabling model composition and online learning to\nimprove accuracy and render more robust predictions. \n\n"}
{"id": "1612.05317", "contents": "Title: A Fast Exact Quantum Algorithm for Solitude Verification Abstract: Solitude verification is arguably one of the simplest fundamental problems in\ndistributed computing, where the goal is to verify that there is a unique\ncontender in a network. This paper devises a quantum algorithm that exactly\nsolves the problem on an anonymous network, which is known as a network model\nwith minimal assumptions [Angluin, STOC'80]. The algorithm runs in $O(N)$\nrounds if every party initially has the common knowledge of an upper bound $N$\non the number of parties. This implies that all solvable problems can be solved\nin $O(N)$ rounds on average without error (i.e., with zero-sided error) on the\nnetwork. As a generalization, a quantum algorithm that works in $O(N\\log_2\n(\\max\\{k,2\\}))$ rounds is obtained for the problem of exactly computing any\nsymmetric Boolean function, over $n$ distributed input bits, which is constant\nover all the $n$ bits whose sum is larger than $k$ for $k\\in \\{0,1,\\dots,\nN-1\\}$. All these algorithms work with the bit complexities bounded by a\npolynomial in $N$. \n\n"}
{"id": "1612.05695", "contents": "Title: Reinforcement Learning Using Quantum Boltzmann Machines Abstract: We investigate whether quantum annealers with select chip layouts can\noutperform classical computers in reinforcement learning tasks. We associate a\ntransverse field Ising spin Hamiltonian with a layout of qubits similar to that\nof a deep Boltzmann machine (DBM) and use simulated quantum annealing (SQA) to\nnumerically simulate quantum sampling from this system. We design a\nreinforcement learning algorithm in which the set of visible nodes representing\nthe states and actions of an optimal policy are the first and last layers of\nthe deep network. In absence of a transverse field, our simulations show that\nDBMs are trained more effectively than restricted Boltzmann machines (RBM) with\nthe same number of nodes. We then develop a framework for training the network\nas a quantum Boltzmann machine (QBM) in the presence of a significant\ntransverse field for reinforcement learning. This method also outperforms the\nreinforcement learning method that uses RBMs. \n\n"}
{"id": "1612.07828", "contents": "Title: Learning from Simulated and Unsupervised Images through Adversarial\n  Training Abstract: With recent progress in graphics, it has become more tractable to train\nmodels on synthetic images, potentially avoiding the need for expensive\nannotations. However, learning from synthetic images may not achieve the\ndesired performance due to a gap between synthetic and real image\ndistributions. To reduce this gap, we propose Simulated+Unsupervised (S+U)\nlearning, where the task is to learn a model to improve the realism of a\nsimulator's output using unlabeled real data, while preserving the annotation\ninformation from the simulator. We develop a method for S+U learning that uses\nan adversarial network similar to Generative Adversarial Networks (GANs), but\nwith synthetic images as inputs instead of random vectors. We make several key\nmodifications to the standard GAN algorithm to preserve annotations, avoid\nartifacts, and stabilize training: (i) a 'self-regularization' term, (ii) a\nlocal adversarial loss, and (iii) updating the discriminator using a history of\nrefined images. We show that this enables generation of highly realistic\nimages, which we demonstrate both qualitatively and with a user study. We\nquantitatively evaluate the generated images by training models for gaze\nestimation and hand pose estimation. We show a significant improvement over\nusing synthetic images, and achieve state-of-the-art results on the MPIIGaze\ndataset without any labeled real data. \n\n"}
{"id": "1612.08463", "contents": "Title: Request-Based Gossiping without Deadlocks Abstract: By the distributed averaging problem is meant the problem of computing the\naverage value of a set of numbers possessed by the agents in a distributed\nnetwork using only communication between neighboring agents. Gossiping is a\nwell-known approach to the problem which seeks to iteratively arrive at a\nsolution by allowing each agent to interchange information with at most one\nneighbor at each iterative step. Crafting a gossiping protocol which\naccomplishes this is challenging because gossiping is an inherently\ncollaborative process which can lead to deadlocks unless careful precautions\nare taken to ensure that it does not. Many gossiping protocols are\nrequest-based which means simply that a gossip between two agents will occur\nwhenever one of the two agents accepts a request to gossip placed by the other.\nIn this paper, we present three deterministic request-based protocols. We show\nby example that the first can deadlock. The second is guaranteed to avoid\ndeadlocks and requires fewer transmissions per iteration than standard\nbroadcast-based distributed averaging protocols by exploiting the idea of local\nordering together with the notion of an agent's neighbor queue; the protocol\nrequires the simplest queue updates, which provides an in-depth understanding\nof how local ordering and queue updates avoid deadlocks. It is shown that a\nthird protocol which uses a slightly more complicated queue update rule can\nlead to significantly faster convergence; a worst case bound on convergence\nrate is provided. \n\n"}
{"id": "1612.09029", "contents": "Title: Distributed Convex Optimization with Inequality Constraints over\n  Time-varying Unbalanced Digraphs Abstract: This paper considers a distributed convex optimization problem with\ninequality constraints over time-varying unbalanced digraphs, where the cost\nfunction is a sum of local objectives, and each node of the graph only knows\nits local objective and inequality constraints. Although there is a vast\nliterature on distributed optimization, most of them require the graph to be\nbalanced, which is quite restrictive and not necessary. Very recently, the\nunbalanced problem has been resolved only for either time-invariant graphs or\nunconstrained optimization. This work addresses the unbalancedness by focusing\non an epigraph form of the constrained optimization. A striking feature is that\nthis novel idea can be easily used to study time-varying unbalanced digraphs.\nUnder local communications, a simple iterative algorithm is then designed for\neach node. We prove that if the graph is uniformly jointly strongly connected,\neach node asymptotically converges to some common optimal solution. \n\n"}
{"id": "1612.09426", "contents": "Title: The Balance Attack Against Proof-Of-Work Blockchains: The R3 Testbed as\n  an Example Abstract: In this paper, we identify a new form of attack, called the Balance attack,\nagainst proof-of-work blockchain systems. The novelty of this attack consists\nof delaying network communications between multiple subgroups of nodes with\nbalanced mining power. Our theoretical analysis captures the precise tradeoff\nbetween the network delay and the mining power of the attacker needed to double\nspend in Ethereum with high probability.\n  We quantify our probabilistic analysis with statistics taken from the R3\nconsortium, and show that a single machine needs 20 minutes to attack the\nconsortium. Finally, we run an Ethereum private chain in a distributed system\nwith similar settings as R3 to demonstrate the feasibility of the approach, and\ndiscuss the application of the Balance attack to Bitcoin. Our results clearly\nconfirm that main proof-of-work blockchain protocols can be badly suited for\nconsortium blockchains. \n\n"}
{"id": "1701.00997", "contents": "Title: Distributed Co-Simulation of Maritime Systems and Operations Abstract: Here, we present the concept of an open virtual prototyping framework for\nmaritime systems and operations that enables its users to develop re-usable\ncomponent or subsystem models, and combine them in full-system simulations for\nprototyping, verification, training, and performance studies. This framework\nconsists of a set of guidelines for model coupling, high-level and low-level\ncoupling interfaces to guarantee interoperability, a full-system simulation\nsoftware, and example models and demonstrators. We discuss the requirements for\nsuch a framework, address the challenges and the possibilities in fulfilling\nthem, and aim to give a list of best practices for modular and efficient\nvirtual prototyping and full-system simulation. The context of our work is\nwithin maritime systems and operations, but the issues and solutions we present\nhere are general enough to be of interest to a much broader audience, both\nindustrial and scientific. \n\n"}
{"id": "1701.01189", "contents": "Title: GPU Multisplit: an extended study of a parallel algorithm Abstract: Multisplit is a broadly useful parallel primitive that permutes its input\ndata into contiguous buckets or bins, where the function that categorizes an\nelement into a bucket is provided by the programmer. Due to the lack of an\nefficient multisplit on GPUs, programmers often choose to implement multisplit\nwith a sort. One way is to first generate an auxiliary array of bucket IDs and\nthen sort input data based on it. In case smaller indexed buckets possess\nsmaller valued keys, another way for multisplit is to directly sort input data.\nBoth methods are inefficient and require more work than necessary: the former\nrequires more expensive data movements while the latter spends unnecessary\neffort in sorting elements within each bucket. In this work, we provide a\nparallel model and multiple implementations for the multisplit problem. Our\nprincipal focus is multisplit for a small (up to 256) number of buckets. We use\nwarp-synchronous programming models and emphasize warp-wide communications to\navoid branch divergence and reduce memory usage. We also hierarchically reorder\ninput elements to achieve better coalescing of global memory accesses. On a\nGeForce GTX 1080 GPU, we can reach a peak throughput of 18.93 Gkeys/s (or 11.68\nGpairs/s) for a key-only (or key-value) multisplit. Finally, we demonstrate how\nmultisplit can be used as a building block for radix sort. In our\nmultisplit-based sort implementation, we achieve comparable performance to the\nfastest GPU sort routines, sorting 32-bit keys (and key-value pairs) with a\nthroughput of 3.0 G keys/s (and 2.1 Gpair/s). \n\n"}
{"id": "1701.01539", "contents": "Title: Algorithms for Optimal Replica Placement Under Correlated Failure in\n  Hierarchical Failure Domains Abstract: In data centers, data replication is the primary method used to ensure\navailability of customer data. To avoid correlated failure, cloud storage\ninfrastructure providers model hierarchical failure domains using a tree, and\navoid placing a large number of data replicas within the same failure domain\n(i.e. on the same branch of the tree). Typical best practices ensure that\nreplicas are distributed across failure domains, but relatively little is known\nconcerning optimization algorithms for distributing data replicas. Using a\nhierarchical model, we answer how to distribute replicas across failure domains\noptimally. We formulate a novel optimization problem for replica placement in\ndata centers. As part of our problem, we formalize and explain a new criterion\nfor optimizing a replica placement. Our overall goal is to choose placements in\nwhich correlated failures disable as few replicas as possible. We provide two\noptimization algorithms for dependency models represented by trees. We first\npresent an $O(n + \\rho \\log \\rho)$ time dynamic programming algorithm for\nplacing $\\rho$ replicas of a single file on the leaves (representing servers)\nof a tree with $n$ vertices. We next consider the problem of placing replicas\nof $m$ blocks of data, where each block may have different replication factors.\nFor this problem, we give an exact algorithm which runs in polynomial time when\nthe skew, the difference in the number of replicas between the largest and\nsmallest blocks of data, is constant. \n\n"}
{"id": "1701.01811", "contents": "Title: Structural Attention Neural Networks for improved sentiment analysis Abstract: We introduce a tree-structured attention neural network for sentences and\nsmall phrases and apply it to the problem of sentiment classification. Our\nmodel expands the current recursive models by incorporating structural\ninformation around a node of a syntactic tree using both bottom-up and top-down\ninformation propagation. Also, the model utilizes structural attention to\nidentify the most salient representations during the construction of the\nsyntactic tree. To our knowledge, the proposed models achieve state of the art\nperformance on the Stanford Sentiment Treebank dataset. \n\n"}
{"id": "1701.02324", "contents": "Title: An $N \\log N$ Parallel Fast Direct Solver for Kernel Matrices Abstract: Kernel matrices appear in machine learning and non-parametric statistics.\nGiven $N$ points in $d$ dimensions and a kernel function that requires\n$\\mathcal{O}(d)$ work to evaluate, we present an $\\mathcal{O}(dN\\log N)$-work\nalgorithm for the approximate factorization of a regularized kernel matrix, a\ncommon computational bottleneck in the training phase of a learning task. With\nthis factorization, solving a linear system with a kernel matrix can be done\nwith $\\mathcal{O}(N\\log N)$ work. Our algorithm only requires kernel\nevaluations and does not require that the kernel matrix admits an efficient\nglobal low rank approximation. Instead our factorization only assumes low-rank\nproperties for the off-diagonal blocks under an appropriate row and column\nordering. We also present a hybrid method that, when the factorization is\nprohibitively expensive, combines a partial factorization with iterative\nmethods. As a highlight, we are able to approximately factorize a dense\n$11M\\times11M$ kernel matrix in 2 minutes on 3,072 x86 \"Haswell\" cores and a\n$4.5M\\times4.5M$ matrix in 1 minute using 4,352 \"Knights Landing\" cores. \n\n"}
{"id": "1701.02628", "contents": "Title: Greed is Good: Optimistic Algorithms for Bipartite-Graph Partial\n  Coloring on Multicore Architectures Abstract: In parallel computing, a valid graph coloring yields a lock-free processing\nof the colored tasks, data points, etc., without expensive synchronization\nmechanisms. However, coloring is not free and the overhead can be significant.\nIn particular, for the bipartite-graph partial coloring (BGPC) and distance-2\ngraph coloring (D2GC) problems, which have various use-cases within the\nscientific computing and numerical optimization domains, the coloring overhead\ncan be in the order of minutes with a single thread for many real-life graphs.\n  In this work, we propose parallel algorithms for bipartite-graph partial\ncoloring on shared-memory architectures. Compared to the existing shared-memory\nBGPC algorithms, the proposed ones employ greedier and more optimistic\ntechniques that yield a better parallel coloring performance. In particular, on\n16 cores, the proposed algorithms perform more than 4x faster than their\ncounterparts in the ColPack library which is, to the best of our knowledge, the\nonly publicly-available coloring library for multicore architectures. In\naddition to BGPC, the proposed techniques are employed to devise parallel\ndistance-2 graph coloring algorithms and similar performance improvements have\nbeen observed. Finally, we propose two costless balancing heuristics for BGPC\nthat can reduce the skewness and imbalance on the cardinality of color sets\n(almost) for free. The heuristics can also be used for the D2GC problem and in\ngeneral, they will probably yield a better color-based parallelization\nperformance especially on many-core architectures. \n\n"}
{"id": "1701.03038", "contents": "Title: Decoding with Finite-State Transducers on GPUs Abstract: Weighted finite automata and transducers (including hidden Markov models and\nconditional random fields) are widely used in natural language processing (NLP)\nto perform tasks such as morphological analysis, part-of-speech tagging,\nchunking, named entity recognition, speech recognition, and others.\nParallelizing finite state algorithms on graphics processing units (GPUs) would\nbenefit many areas of NLP. Although researchers have implemented GPU versions\nof basic graph algorithms, limited previous work, to our knowledge, has been\ndone on GPU algorithms for weighted finite automata. We introduce a GPU\nimplementation of the Viterbi and forward-backward algorithm, achieving\ndecoding speedups of up to 5.2x over our serial implementation running on\ndifferent computer architectures and 6093x over OpenFST. \n\n"}
{"id": "1701.03682", "contents": "Title: LIDE: Language Identification from Text Documents Abstract: The increase in the use of microblogging came along with the rapid growth on\nshort linguistic data. On the other hand deep learning is considered to be the\nnew frontier to extract meaningful information out of large amount of raw data\nin an automated manner. In this study, we engaged these two emerging fields to\ncome up with a robust language identifier on demand, namely Language\nIdentification Engine (LIDE). As a result, we achieved 95.12% accuracy in\nDiscriminating between Similar Languages (DSL) Shared Task 2015 dataset, which\nis comparable to the maximum reported accuracy of 95.54% achieved so far. \n\n"}
{"id": "1701.05451", "contents": "Title: Feasibility of Fog Computing Abstract: As billions of devices get connected to the Internet, it will not be\nsustainable to use the cloud as a centralised server. The way forward is to\ndecentralise computations away from the cloud towards the edge of the network\ncloser to the user. This reduces the latency of communication between a user\ndevice and the cloud, and is the premise of 'fog computing' defined in this\npaper. The aim of this paper is to highlight the feasibility and the benefits\nin improving the Quality-of-Service and Experience by using fog computing. For\nan online game use-case, we found that the average response time for a user is\nimproved by 20% when using the edge of the network in comparison to using a\ncloud-only model. It was also observed that the volume of traffic between the\nedge and the cloud server is reduced by over 90% for the use-case. The\npreliminary results highlight the potential of fog computing in achieving a\nsustainable computing model and highlights the benefits of integrating the edge\nof the network into the computing ecosystem. \n\n"}
{"id": "1701.05935", "contents": "Title: Integration of Preferences in Decomposition Multi-Objective Optimization Abstract: Most existing studies on evolutionary multi-objective optimization focus on\napproximating the whole Pareto-optimal front. Nevertheless, rather than the\nwhole front, which demands for too many points (especially in a\nhigh-dimensional space), the decision maker might only interest in a partial\nregion, called the region of interest. In this case, solutions outside this\nregion can be noisy to the decision making procedure. Even worse, there is no\nguarantee that we can find the preferred solutions when tackling problems with\ncomplicated properties or a large number of objectives. In this paper, we\ndevelop a systematic way to incorporate the decision maker's preference\ninformation into the decomposition-based evolutionary multi-objective\noptimization methods. Generally speaking, our basic idea is a non-uniform\nmapping scheme by which the originally uniformly distributed reference points\non a canonical simplex can be mapped to the new positions close to the\naspiration level vector specified by the decision maker. By these means, we are\nable to steer the search process towards the region of interest either directly\nor in an interactive manner and also handle a large number of objectives. In\nthe meanwhile, the boundary solutions can be approximated given the decision\nmaker's requirements. Furthermore, the extent of the region of the interest is\nintuitively understandable and controllable in a closed form. Extensive\nexperiments, both proof-of-principle and on a variety of problems with 3 to 10\nobjectives, fully demonstrate the effectiveness of our proposed method for\napproximating the preferred solutions in the region of interest. \n\n"}
{"id": "1701.06548", "contents": "Title: Regularizing Neural Networks by Penalizing Confident Output\n  Distributions Abstract: We systematically explore regularizing neural networks by penalizing low\nentropy output distributions. We show that penalizing low entropy output\ndistributions, which has been shown to improve exploration in reinforcement\nlearning, acts as a strong regularizer in supervised learning. Furthermore, we\nconnect a maximum entropy based confidence penalty to label smoothing through\nthe direction of the KL divergence. We exhaustively evaluate the proposed\nconfidence penalty and label smoothing on 6 common benchmarks: image\nclassification (MNIST and Cifar-10), language modeling (Penn Treebank), machine\ntranslation (WMT'14 English-to-German), and speech recognition (TIMIT and WSJ).\nWe find that both label smoothing and the confidence penalty improve\nstate-of-the-art models across benchmarks without modifying existing\nhyperparameters, suggesting the wide applicability of these regularizers. \n\n"}
{"id": "1701.07154", "contents": "Title: Fog-Assisted Operational Cost Reduction for Cloud Data Centers Abstract: In this paper, we intend to reduce the operational cost of cloud data centers\nwith the help of fog devices, which can avoid the revenue loss due to wide-area\nnetwork propagation delay and save network bandwidth cost by serving nearby\ncloud users. Since fog devices may not be owned by a cloud service provider,\nthey should be compensated for serving the requests of cloud users. When taking\neconomical compensation into consideration, the optimal number of requests\nprocessed locally by each fog device should be decided. As a result, existing\nload balancing schemes developed for cloud data centers can not be applied\ndirectly and it is very necessary to redesign a cost-ware load balancing\nalgorithm for the fog-cloud system. To achieve the above aim, we first\nformulate a fog-assisted operational cost minimization problem for the cloud\nservice provider. Then, we design a parallel and distributed load balancing\nalgorithm with low computational complexity based on Proximal Jacobian\nAlternating Direction Method of Multipliers (PJ-ADMM). Finally, extensive\nsimulation results show the effectiveness of the proposed algorithm. \n\n"}
{"id": "1701.08474", "contents": "Title: IFCIoT: Integrated Fog Cloud IoT Architectural Paradigm for Future\n  Internet of Things Abstract: We propose a novel integrated fog cloud IoT (IFCIoT) architectural paradigm\nthat promises increased performance, energy efficiency, reduced latency,\nquicker response time, scalability, and better localized accuracy for future\nIoT applications. The fog nodes (e.g., edge servers, smart routers, base\nstations) receive computation offloading requests and sensed data from various\nIoT devices. To enhance performance, energy efficiency, and real-time\nresponsiveness of applications, we propose a reconfigurable and layered fog\nnode (edge server) architecture that analyzes the applications' characteristics\nand reconfigure the architectural resources to better meet the peak workload\ndemands. The layers of the proposed fog node architecture include application\nlayer, analytics layer, virtualization layer, reconfiguration layer, and\nhardware layer. The layered architecture facilitates abstraction and\nimplementation for fog computing paradigm that is distributed in nature and\nwhere multiple vendors (e.g., applications, services, data and content\nproviders) are involved. We also elaborate the potential applications of IFCIoT\narchitecture, such as smart cities, intelligent transportation systems,\nlocalized weather maps and environmental monitoring, and real-time agricultural\ndata analytics and control. \n\n"}
{"id": "1702.02455", "contents": "Title: Deterministic Protocols in the SINR Model without Knowledge of\n  Coordinates Abstract: Much work has been developed for studying the classical broadcasting problem\nin the SINR (Signal-to-Interference-plus-Noise-Ratio) model for wireless device\ntransmission. The setting typically studied is when all radio nodes transmit a\nsignal of the same strength. This work studies the challenging problem of\ndevising a distributed algorithm for multi-broadcasting, assuming a subset of\nnodes are initially awake, for the SINR model when each device only has access\nto knowledge about the total number of nodes in the network $n$, the range from\nwhich each node's label is taken $\\lbrace 1,\\dots,N \\rbrace$, and the label of\nthe device itself. Specifically, we assume no knowledge of the physical\ncoordinates of devices and also no knowledge of the neighborhood of each node.\n  We present a deterministic protocol for this problem in $O(n \\lg N \\lg n)$\nrounds. There is no known polynomial time deterministic algorithm in literature\nfor this setting, and it remains the principle open problem in this domain. A\nlower bound of $\\Omega(n \\lg N)$ rounds is known for deterministic broadcasting\nwithout local knowledge.\n  In addition to the above result, we present algorithms to achieve\nmulti-broadcast in $O(n \\lg N)$ rounds and create a backbone in $O(n \\lg N)$\nrounds, assuming that all nodes are initially awake. For a given backbone,\nmessages can be exchanged between every pair of connected nodes in the backbone\nin $O(\\lg N)$ rounds and between any node and its designated contact node in\nthe backbone in $O(\\Delta \\lg N)$ rounds. \n\n"}
{"id": "1702.02540", "contents": "Title: Automatic Rule Extraction from Long Short Term Memory Networks Abstract: Although deep learning models have proven effective at solving problems in\nnatural language processing, the mechanism by which they come to their\nconclusions is often unclear. As a result, these models are generally treated\nas black boxes, yielding no insight of the underlying learned patterns. In this\npaper we consider Long Short Term Memory networks (LSTMs) and demonstrate a new\napproach for tracking the importance of a given input to the LSTM for a given\noutput. By identifying consistently important patterns of words, we are able to\ndistill state of the art LSTMs on sentiment analysis and question answering\ninto a set of representative phrases. This representation is then\nquantitatively validated by using the extracted phrases to construct a simple,\nrule-based classifier which approximates the output of the LSTM. \n\n"}
{"id": "1702.03192", "contents": "Title: Supervised Learning Based Algorithm Selection for Deep Neural Networks Abstract: Many recent deep learning platforms rely on third-party libraries (such as\ncuBLAS) to utilize the computing power of modern hardware accelerators (such as\nGPUs). However, we observe that they may achieve suboptimal performance because\nthe library functions are not used appropriately. In this paper, we target at\noptimizing the operations of multiplying a matrix with the transpose of another\nmatrix (referred to as NT operation hereafter), which contribute about half of\nthe training time of fully connected deep neural networks. Rather than directly\ncalling the library function, we propose a supervised learning based algorithm\nselection approach named MTNN, which uses a gradient boosted decision tree to\nselect one from two alternative NT implementations intelligently: (1) calling\nthe cuBLAS library function; (2) calling our proposed algorithm TNN that uses\nan efficient out-of-place matrix transpose. We evaluate the performance of MTNN\non two modern GPUs: NVIDIA GTX 1080 and NVIDIA Titan X Pascal. MTNN can achieve\n96\\% of prediction accuracy with very low computational overhead, which results\nin an average of 54\\% performance improvement on a range of NT operations. To\nfurther evaluate the impact of MTNN on the training process of deep neural\nnetworks, we have integrated MTNN into a popular deep learning platform Caffe.\nOur experimental results show that the revised Caffe can outperform the\noriginal one by an average of 28\\%. Both MTNN and the revised Caffe are\nopen-source. \n\n"}
{"id": "1702.04164", "contents": "Title: Better Process Mapping and Sparse Quadratic Assignment Abstract: Communication and topology aware process mapping is a powerful approach to\nreduce communication time in parallel applications with known communication\npatterns on large, distributed memory systems. We address the problem as a\nquadratic assignment problem (QAP), and present algorithms to construct initial\nmappings of processes to processors, and fast local search algorithms to\nfurther improve the mappings. By exploiting assumptions that typically hold for\napplications and modern supercomputer systems such as sparse communication\npatterns and hierarchically organized communication systems, we obtain\nsignificantly more powerful algorithms for these special QAPs. Our multilevel\nconstruction algorithms employ perfectly balanced graph partitioning techniques\nand exploit the given communication system hierarchy in significant ways. We\npresent improvements to a local search algorithm of Brandfass et al. (2013),\nand further decrease the running time by reducing the time needed to perform\nswaps in the assignment as well as by carefully constraining local search\nneighborhoods. We also investigate different algorithms to create the\ncommunication graph that is mapped onto the processor network. Experiments\nindicate that our algorithms not only dramatically speed up local search, but\ndue to the multilevel approach also find much better solutions in practice. \n\n"}
{"id": "1702.04263", "contents": "Title: Okapi: Causally Consistent Geo-Replication Made Faster, Cheaper and More\n  Available Abstract: Okapi is a new causally consistent geo-replicated key- value store. Okapi\nleverages two key design choices to achieve high performance. First, it relies\non hybrid logical/physical clocks to achieve low latency even in the presence\nof clock skew. Second, Okapi achieves higher resource efficiency and better\navailability, at the expense of a slight increase in update visibility latency.\nTo this end, Okapi implements a new stabilization protocol that uses a\ncombination of vector and scalar clocks and makes a remote update visible when\nits delivery has been acknowledged by every data center. We evaluate Okapi with\ndifferent workloads on Amazon AWS, using three geographically distributed\nregions and 96 nodes. We compare Okapi with two recent approaches to causal\nconsistency, Cure and GentleRain. We show that Okapi delivers up to two orders\nof magnitude better performance than GentleRain and that Okapi achieves up to\n3.5x lower latency and a 60% reduction of the meta-data overhead with respect\nto Cure. \n\n"}
{"id": "1702.04921", "contents": "Title: Ignore or Comply? On Breaking Symmetry in Consensus Abstract: We study consensus processes on the complete graph of $n$ nodes. Initially,\neach node supports one from up to n opinions. Nodes randomly and in parallel\nsample the opinions of constant many nodes. Based on these samples, they use an\nupdate rule to change their own opinion.\n  The goal is to reach consensus, a configuration where all nodes support the\nsame opinion. We compare two well-known update rules: 2-Choices and 3-Majority.\nIn the former, each node samples two nodes and adopts their opinion if they\nagree. In the latter, each node samples three nodes: If an opinion is supported\nby at least two samples the node adopts it, otherwise it randomly adopts one of\nthe sampled opinions. Known results for these update rules focus on initial\nconfigurations with a limited number of colors (say $n^{1/3}$ ), or typically\nassume a bias, where one opinion has a much larger support than any other. For\nsuch biased configurations, the time to reach consensus is roughly the same for\n2-Choices and 3-Majority.\n  Interestingly, we prove that this is no longer true for configurations with a\nlarge number of initial colors. In particular, we show that 3-Majority reaches\nconsensus with high probability in $O(n^{3/4}\\log^{7/8}n)$ rounds, while\n2-Choices can need $\\Omega(n/\\log n)$ rounds. We thus get the first\nunconditional sublinear bound for 3-Majority and the first result separating\nthe consensus time of these processes. Along the way, we develop a framework\nthat allows a fine-grained comparison between consensus processes from a\nspecific class. We believe that this framework might help to classify the\nperformance of more consensus processes. \n\n"}
{"id": "1702.05456", "contents": "Title: LCL problems on grids Abstract: LCLs or locally checkable labelling problems (e.g. maximal independent set,\nmaximal matching, and vertex colouring) in the LOCAL model of computation are\nvery well-understood in cycles (toroidal 1-dimensional grids): every problem\nhas a complexity of $O(1)$, $\\Theta(\\log^* n)$, or $\\Theta(n)$, and the design\nof optimal algorithms can be fully automated.\n  This work develops the complexity theory of LCL problems for toroidal\n2-dimensional grids. The complexity classes are the same as in the\n1-dimensional case: $O(1)$, $\\Theta(\\log^* n)$, and $\\Theta(n)$. However, given\nan LCL problem it is undecidable whether its complexity is $\\Theta(\\log^* n)$\nor $\\Theta(n)$ in 2-dimensional grids.\n  Nevertheless, if we correctly guess that the complexity of a problem is\n$\\Theta(\\log^* n)$, we can completely automate the design of optimal\nalgorithms. For any problem we can find an algorithm that is of a normal form\n$A' \\circ S_k$, where $A'$ is a finite function, $S_k$ is an algorithm for\nfinding a maximal independent set in $k$th power of the grid, and $k$ is a\nconstant.\n  Finally, partially with the help of automated design tools, we classify the\ncomplexity of several concrete LCL problems related to colourings and\norientations. \n\n"}
{"id": "1702.05510", "contents": "Title: Java Code Analysis and Transformation into AWS Lambda Functions Abstract: Software developers are faced with the issue of either adapting their\nprogramming model to the execution model (e.g. cloud platforms) or finding\nappropriate tools to adapt the model and code automatically. A recent execution\nmodel which would benefit from automated enablement is Function-as-a-Service.\nAutomating this process requires a pipeline which includes steps for code\nanalysis, transformation and deployment. In this paper, we outline the design\nand runtime characteristics of Podilizer, a tool which implements the pipeline\nspecifically for Java source code as input and AWS Lambda as output. We\ncontribute technical and economic metrics about this concrete 'FaaSification'\nprocess by observing the behaviour of Podilizer with two representative Java\nsoftware projects. \n\n"}
{"id": "1702.07005", "contents": "Title: Large-Scale Stochastic Learning using GPUs Abstract: In this work we propose an accelerated stochastic learning system for very\nlarge-scale applications. Acceleration is achieved by mapping the training\nalgorithm onto massively parallel processors: we demonstrate a parallel,\nasynchronous GPU implementation of the widely used stochastic coordinate\ndescent/ascent algorithm that can provide up to 35x speed-up over a sequential\nCPU implementation. In order to train on very large datasets that do not fit\ninside the memory of a single GPU, we then consider techniques for distributed\nstochastic learning. We propose a novel method for optimally aggregating model\nupdates from worker nodes when the training data is distributed either by\nexample or by feature. Using this technique, we demonstrate that one can scale\nout stochastic learning across up to 8 worker nodes without any significant\nloss of training time. Finally, we combine GPU acceleration with the optimized\ndistributed method to train on a dataset consisting of 200 million training\nexamples and 75 million features. We show by scaling out across 4 GPUs, one can\nattain a high degree of training accuracy in around 4 seconds: a 20x speed-up\nin training time compared to a multi-threaded, distributed implementation\nacross 4 CPUs. \n\n"}
{"id": "1702.07605", "contents": "Title: Compact Self-Stabilizing Leader Election for Arbitrary Networks Abstract: We present a self-stabilizing leader election algorithm for arbitrary\nnetworks, with space-complexity $O(\\max\\{\\log \\Delta, \\log \\log n\\})$ bits per\nnode in $n$-node networks with maximum degree~$\\Delta$. This space complexity\nis sub-logarithmic in $n$ as long as $\\Delta = n^{o(1)}$. The best\nspace-complexity known so far for arbitrary networks was $O(\\log n)$ bits per\nnode, and algorithms with sub-logarithmic space-complexities were known for the\nring only. To our knowledge, our algorithm is the first algorithm for\nself-stabilizing leader election to break the $\\Omega(\\log n)$ bound for silent\nalgorithms in arbitrary networks. Breaking this bound was obtained via the\ndesign of a (non-silent) self-stabilizing algorithm using sophisticated tools\nsuch as solving the distance-2 coloring problem in a silent self-stabilizing\nmanner, with space-complexity $O(\\max\\{\\log \\Delta, \\log \\log n\\})$ bits per\nnode. Solving this latter coloring problem allows us to implement a\nsub-logarithmic encoding of spanning trees --- storing the IDs of the neighbors\nrequires $\\Omega(\\log n)$ bits per node, while we encode spanning trees using\n$O(\\max\\{\\log \\Delta, \\log \\log n\\})$ bits per node. Moreover, we show how to\nconstruct such compactly encoded spanning trees without relying on variables\nencoding distances or number of nodes, as these two types of variables would\nalso require $\\Omega(\\log n)$ bits per node. \n\n"}
{"id": "1702.07617", "contents": "Title: DALiuGE: A Graph Execution Framework for Harnessing the Astronomical\n  Data Deluge Abstract: The Data Activated Liu Graph Engine - DALiuGE - is an execution framework for\nprocessing large astronomical datasets at a scale required by the Square\nKilometre Array Phase 1 (SKA1). It includes an interface for expressing complex\ndata reduction pipelines consisting of both data sets and algorithmic\ncomponents and an implementation run-time to execute such pipelines on\ndistributed resources. By mapping the logical view of a pipeline to its\nphysical realisation, DALiuGE separates the concerns of multiple stakeholders,\nallowing them to collectively optimise large-scale data processing solutions in\na coherent manner. The execution in DALiuGE is data-activated, where each\nindividual data item autonomously triggers the processing on itself. Such\ndecentralisation also makes the execution framework very scalable and flexible,\nsupporting pipeline sizes ranging from less than ten tasks running on a laptop\nto tens of millions of concurrent tasks on the second fastest supercomputer in\nthe world. DALiuGE has been used in production for reducing interferometry data\nsets from the Karl E. Jansky Very Large Array and the Mingantu Ultrawide\nSpectral Radioheliograph; and is being developed as the execution framework\nprototype for the Science Data Processor (SDP) consortium of the Square\nKilometre Array (SKA) telescope. This paper presents a technical overview of\nDALiuGE and discusses case studies from the CHILES and MUSER projects that use\nDALiuGE to execute production pipelines. In a companion paper, we provide\nin-depth analysis of DALiuGE's scalability to very large numbers of tasks on\ntwo supercomputing facilities. \n\n"}
{"id": "1702.07805", "contents": "Title: Analyzing and Exploiting NARX Recurrent Neural Networks for Long-Term\n  Dependencies Abstract: Recurrent neural networks (RNNs) have achieved state-of-the-art performance\non many diverse tasks, from machine translation to surgical activity\nrecognition, yet training RNNs to capture long-term dependencies remains\ndifficult. To date, the vast majority of successful RNN architectures alleviate\nthis problem using nearly-additive connections between states, as introduced by\nlong short-term memory (LSTM). We take an orthogonal approach and introduce\nMIST RNNs, a NARX RNN architecture that allows direct connections from the very\ndistant past. We show that MIST RNNs 1) exhibit superior vanishing-gradient\nproperties in comparison to LSTM and previously-proposed NARX RNNs; 2) are far\nmore efficient than previously-proposed NARX RNN architectures, requiring even\nfewer computations than LSTM; and 3) improve performance substantially over\nLSTM and Clockwork RNNs on tasks requiring very long-term dependencies. \n\n"}
{"id": "1702.07811", "contents": "Title: Adaptive Neural Networks for Efficient Inference Abstract: We present an approach to adaptively utilize deep neural networks in order to\nreduce the evaluation time on new examples without loss of accuracy. Rather\nthan attempting to redesign or approximate existing networks, we propose two\nschemes that adaptively utilize networks. We first pose an adaptive network\nevaluation scheme, where we learn a system to adaptively choose the components\nof a deep network to be evaluated for each example. By allowing examples\ncorrectly classified using early layers of the system to exit, we avoid the\ncomputational time associated with full evaluation of the network. We extend\nthis to learn a network selection system that adaptively selects the network to\nbe evaluated for each example. We show that computational time can be\ndramatically reduced by exploiting the fact that many examples can be correctly\nclassified using relatively efficient networks and that complex,\ncomputationally costly networks are only necessary for a small fraction of\nexamples. We pose a global objective for learning an adaptive early exit or\nnetwork selection policy and solve it by reducing the policy learning problem\nto a layer-by-layer weighted binary classification problem. Empirically, these\napproaches yield dramatic reductions in computational cost, with up to a 2.8x\nspeedup on state-of-the-art networks from the ImageNet image recognition\nchallenge with minimal (<1%) loss of top5 accuracy. \n\n"}
{"id": "1703.00522", "contents": "Title: Understanding Synthetic Gradients and Decoupled Neural Interfaces Abstract: When training neural networks, the use of Synthetic Gradients (SG) allows\nlayers or modules to be trained without update locking - without waiting for a\ntrue error gradient to be backpropagated - resulting in Decoupled Neural\nInterfaces (DNIs). This unlocked ability of being able to update parts of a\nneural network asynchronously and with only local information was demonstrated\nto work empirically in Jaderberg et al (2016). However, there has been very\nlittle demonstration of what changes DNIs and SGs impose from a functional,\nrepresentational, and learning dynamics point of view. In this paper, we study\nDNIs through the use of synthetic gradients on feed-forward networks to better\nunderstand their behaviour and elucidate their effect on optimisation. We show\nthat the incorporation of SGs does not affect the representational strength of\nthe learning system for a neural network, and prove the convergence of the\nlearning system for linear and deep linear models. On practical problems we\ninvestigate the mechanism by which synthetic gradient estimators approximate\nthe true loss, and, surprisingly, how that leads to drastically different\nlayer-wise representations. Finally, we also expose the relationship of using\nsynthetic gradients to other error approximation techniques and find a unifying\nlanguage for discussion and comparison. \n\n"}
{"id": "1703.00564", "contents": "Title: MoleculeNet: A Benchmark for Molecular Machine Learning Abstract: Molecular machine learning has been maturing rapidly over the last few years.\nImproved methods and the presence of larger datasets have enabled machine\nlearning algorithms to make increasingly accurate predictions about molecular\nproperties. However, algorithmic progress has been limited due to the lack of a\nstandard benchmark to compare the efficacy of proposed methods; most new\nalgorithms are benchmarked on different datasets making it challenging to gauge\nthe quality of proposed methods. This work introduces MoleculeNet, a large\nscale benchmark for molecular machine learning. MoleculeNet curates multiple\npublic datasets, establishes metrics for evaluation, and offers high quality\nopen-source implementations of multiple previously proposed molecular\nfeaturization and learning algorithms (released as part of the DeepChem open\nsource library). MoleculeNet benchmarks demonstrate that learnable\nrepresentations are powerful tools for molecular machine learning and broadly\noffer the best performance. However, this result comes with caveats. Learnable\nrepresentations still struggle to deal with complex tasks under data scarcity\nand highly imbalanced classification. For quantum mechanical and biophysical\ndatasets, the use of physics-aware featurizations can be more important than\nchoice of particular learning algorithm. \n\n"}
{"id": "1703.00687", "contents": "Title: Even faster sorting of (not only) integers Abstract: In this paper we introduce RADULS2, the fastest parallel sorter based on\nradix algorithm. It is optimized to process huge amounts of data making use of\nmodern multicore CPUs. The main novelties include: extremely optimized\nalgorithm for handling tiny arrays (up to about a hundred of records) that\ncould appear even billions times as subproblems to handle and improved\nprocessing of larger subarrays with better use of non-temporal memory stores. \n\n"}
{"id": "1703.01673", "contents": "Title: Learn-and-Adapt Stochastic Dual Gradients for Network Resource\n  Allocation Abstract: Network resource allocation shows revived popularity in the era of data\ndeluge and information explosion. Existing stochastic optimization approaches\nfall short in attaining a desirable cost-delay tradeoff. Recognizing the\ncentral role of Lagrange multipliers in network resource allocation, a novel\nlearn-and-adapt stochastic dual gradient (LA-SDG) method is developed in this\npaper to learn the sample-optimal Lagrange multiplier from historical data, and\naccordingly adapt the upcoming resource allocation strategy. Remarkably, LA-SDG\nonly requires just an extra sample (gradient) evaluation relative to the\ncelebrated stochastic dual gradient (SDG) method. LA-SDG can be interpreted as\na foresighted learning scheme with an eye on the future, or, a modified\nheavy-ball iteration from an optimization viewpoint. It is established - both\ntheoretically and empirically - that LA-SDG markedly improves the cost-delay\ntradeoff over state-of-the-art allocation schemes. \n\n"}
{"id": "1703.01704", "contents": "Title: Ad-hoc Affectance-selective Families for Layer Dissemination Abstract: Information dissemination protocols for ad-hoc wireless networks frequently\nuse a minimal subset of the available communication links, defining a rooted\n\"broadcast\" tree. In this work, we focus on the core challenge of disseminating\nfrom one layer to the next one of such tree. We call this problem Layer\nDissemination. We study Layer Dissemination under a generalized model of\ninterference, called affectance. The affectance model subsumes previous models,\nsuch as Radio Network and Signal to Inteference-plus-Noise Ratio. We present\nrandomized and deterministic protocols for Layer Dissemination. These protocols\nare based on a combinatorial object that we call Affectance-selective Families.\nOur approach combines an engineering solution with theoretical guarantees. That\nis, we provide a method to characterize the network with a global measure of\naffectance based on measurements of interference in the specific deployment\narea. Then, our protocols distributedly produce an ad-hoc transmissions\nschedule for dissemination. In the randomized protocol only the network\ncharacterization is needed, whereas the deterministic protocol requires full\nknowledge of affectance. Our theoretical analysis provides guarantees on\nschedule length. We also present simulations of a real network-deployment area\ncontrasting the performance of our randomized protocol, which takes into\naccount affectance, against previous work for interference models that ignore\nsome physical constraints. The striking improvement in performance shown by our\nsimulations show the importance of utilizing a more physically-accurate model\nof interference that takes into account other effects beyond distance to\ntransmitters. \n\n"}
{"id": "1703.03864", "contents": "Title: Evolution Strategies as a Scalable Alternative to Reinforcement Learning Abstract: We explore the use of Evolution Strategies (ES), a class of black box\noptimization algorithms, as an alternative to popular MDP-based RL techniques\nsuch as Q-learning and Policy Gradients. Experiments on MuJoCo and Atari show\nthat ES is a viable solution strategy that scales extremely well with the\nnumber of CPUs available: By using a novel communication strategy based on\ncommon random numbers, our ES implementation only needs to communicate scalars,\nmaking it possible to scale to over a thousand parallel workers. This allows us\nto solve 3D humanoid walking in 10 minutes and obtain competitive results on\nmost Atari games after one hour of training. In addition, we highlight several\nadvantages of ES as a black box optimization technique: it is invariant to\naction frequency and delayed rewards, tolerant of extremely long horizons, and\ndoes not need temporal discounting or value function approximation. \n\n"}
{"id": "1703.03924", "contents": "Title: Real-Time Machine Learning: The Missing Pieces Abstract: Machine learning applications are increasingly deployed not only to serve\npredictions using static models, but also as tightly-integrated components of\nfeedback loops involving dynamic, real-time decision making. These applications\npose a new set of requirements, none of which are difficult to achieve in\nisolation, but the combination of which creates a challenge for existing\ndistributed execution frameworks: computation with millisecond latency at high\nthroughput, adaptive construction of arbitrary task graphs, and execution of\nheterogeneous kernels over diverse sets of resources. We assert that a new\ndistributed execution framework is needed for such ML applications and propose\na candidate approach with a proof-of-concept architecture that achieves a 63x\nperformance improvement over a state-of-the-art execution framework for a\nrepresentative application. \n\n"}
{"id": "1703.04221", "contents": "Title: A Hierarchical Framework of Cloud Resource Allocation and Power\n  Management Using Deep Reinforcement Learning Abstract: Automatic decision-making approaches, such as reinforcement learning (RL),\nhave been applied to (partially) solve the resource allocation problem\nadaptively in the cloud computing system. However, a complete cloud resource\nallocation framework exhibits high dimensions in state and action spaces, which\nprohibit the usefulness of traditional RL techniques. In addition, high power\nconsumption has become one of the critical concerns in design and control of\ncloud computing systems, which degrades system reliability and increases\ncooling cost. An effective dynamic power management (DPM) policy should\nminimize power consumption while maintaining performance degradation within an\nacceptable level. Thus, a joint virtual machine (VM) resource allocation and\npower management framework is critical to the overall cloud computing system.\nMoreover, novel solution framework is necessary to address the even higher\ndimensions in state and action spaces. In this paper, we propose a novel\nhierarchical framework for solving the overall resource allocation and power\nmanagement problem in cloud computing systems. The proposed hierarchical\nframework comprises a global tier for VM resource allocation to the servers and\na local tier for distributed power management of local servers. The emerging\ndeep reinforcement learning (DRL) technique, which can deal with complicated\ncontrol problems with large state space, is adopted to solve the global tier\nproblem. Furthermore, an autoencoder and a novel weight sharing structure are\nadopted to handle the high-dimensional state space and accelerate the\nconvergence speed. On the other hand, the local tier of distributed server\npower managements comprises an LSTM based workload predictor and a model-free\nRL based power manager, operating in a distributed manner. \n\n"}
{"id": "1703.04706", "contents": "Title: Tree Memory Networks for Modelling Long-term Temporal Dependencies Abstract: In the domain of sequence modelling, Recurrent Neural Networks (RNN) have\nbeen capable of achieving impressive results in a variety of application areas\nincluding visual question answering, part-of-speech tagging and machine\ntranslation. However this success in modelling short term dependencies has not\nsuccessfully transitioned to application areas such as trajectory prediction,\nwhich require capturing both short term and long term relationships. In this\npaper, we propose a Tree Memory Network (TMN) for modelling long term and short\nterm relationships in sequence-to-sequence mapping problems. The proposed\nnetwork architecture is composed of an input module, controller and a memory\nmodule. In contrast to related literature, which models the memory as a\nsequence of historical states, we model the memory as a recursive tree\nstructure. This structure more effectively captures temporal dependencies\nacross both short term and long term sequences using its hierarchical\nstructure. We demonstrate the effectiveness and flexibility of the proposed TMN\nin two practical problems, aircraft trajectory modelling and pedestrian\ntrajectory modelling in a surveillance setting, and in both cases we outperform\nthe current state-of-the-art. Furthermore, we perform an in depth analysis on\nthe evolution of the memory module content over time and provide visual\nevidence on how the proposed TMN is able to map both long term and short term\nrelationships efficiently via a hierarchical structure. \n\n"}
{"id": "1703.05045", "contents": "Title: Average whenever you meet: Opportunistic protocols for community\n  detection Abstract: Consider the following asynchronous, opportunistic communication model over a\ngraph $G$: in each round, one edge is activated uniformly and independently at\nrandom and (only) its two endpoints can exchange messages and perform local\ncomputations. Under this model, we study the following random process: The\nfirst time a vertex is an endpoint of an active edge, it chooses a random\nnumber, say $\\pm 1$ with probability $1/2$; then, in each round, the two\nendpoints of the currently active edge update their values to their average. We\nshow that, if $G$ exhibits a two-community structure (for example, two\nexpanders connected by a sparse cut), the values held by the nodes will\ncollectively reflect the underlying community structure over a suitable phase\nof the above process, allowing efficient and effective recovery in important\ncases.\n  In more detail, we first provide a first-moment analysis showing that, for a\nlarge class of almost-regular clustered graphs that includes the stochastic\nblock model, the expected values held by all but a negligible fraction of the\nnodes eventually reflect the underlying cut signal. We prove this property\nemerges after a mixing period of length $\\mathcal O(n\\log n)$. We further\nprovide a second-moment analysis for a more restricted class of regular\nclustered graphs that includes the regular stochastic block model. For this\ncase, we are able to show that most nodes can efficiently and locally identify\ntheir community of reference over a suitable time window. This results in the\nfirst opportunistic protocols that approximately recover community structure\nusing only polylogarithmic work per node. Even for the above class of regular\ngraphs, our second moment analysis requires new concentration bounds on the\nproduct of certain random matrices that are technically challenging and\npossibly of independent interest. \n\n"}
{"id": "1703.05364", "contents": "Title: A Study of Complex Deep Learning Networks on High Performance,\n  Neuromorphic, and Quantum Computers Abstract: Current Deep Learning approaches have been very successful using\nconvolutional neural networks (CNN) trained on large graphical processing units\n(GPU)-based computers. Three limitations of this approach are: 1) they are\nbased on a simple layered network topology, i.e., highly connected layers,\nwithout intra-layer connections; 2) the networks are manually configured to\nachieve optimal results, and 3) the implementation of neuron model is expensive\nin both cost and power. In this paper, we evaluate deep learning models using\nthree different computing architectures to address these problems: quantum\ncomputing to train complex topologies, high performance computing (HPC) to\nautomatically determine network topology, and neuromorphic computing for a\nlow-power hardware implementation. We use the MNIST dataset for our experiment,\ndue to input size limitations of current quantum computers. Our results show\nthe feasibility of using the three architectures in tandem to address the above\ndeep learning limitations. We show a quantum computer can find high quality\nvalues of intra-layer connections weights, in a tractable time as the\ncomplexity of the network increases; a high performance computer can find\noptimal layer-based topologies; and a neuromorphic computer can represent the\ncomplex topology and weights derived from the other architectures in low power\nmemristive hardware. \n\n"}
{"id": "1703.06290", "contents": "Title: A wake-sleep algorithm for recurrent, spiking neural networks Abstract: We investigate a recently proposed model for cortical computation which\nperforms relational inference. It consists of several interconnected,\nstructurally equivalent populations of leaky integrate-and-fire (LIF) neurons,\nwhich are trained in a self-organized fashion with spike-timing dependent\nplasticity (STDP). Despite its robust learning dynamics, the model is\nsusceptible to a problem typical for recurrent networks which use a correlation\nbased (Hebbian) learning rule: if trained with high learning rates, the\nrecurrent connections can cause strong feedback loops in the network dynamics,\nwhich lead to the emergence of attractor states. This causes a strong reduction\nin the number of representable patterns and a decay in the inference ability of\nthe network. As a solution, we introduce a conceptually very simple\n\"wake-sleep\" algorithm: during the wake phase, training is executed normally,\nwhile during the sleep phase, the network \"dreams\" samples from its generative\nmodel, which are induced by random input. This process allows us to activate\nthe attractor states in the network, which can then be unlearned effectively by\nan anti-Hebbian mechanism. The algorithm allows us to increase learning rates\nup to a factor of ten while avoiding clustering, which allows the network to\nlearn several times faster. Also for low learning rates, where clustering is\nnot an issue, it improves convergence speed and reduces the final inference\nerror. \n\n"}
{"id": "1703.07076", "contents": "Title: SMILES Enumeration as Data Augmentation for Neural Network Modeling of\n  Molecules Abstract: Simplified Molecular Input Line Entry System (SMILES) is a single line text\nrepresentation of a unique molecule. One molecule can however have multiple\nSMILES strings, which is a reason that canonical SMILES have been defined,\nwhich ensures a one to one correspondence between SMILES string and molecule.\nHere the fact that multiple SMILES represent the same molecule is explored as a\ntechnique for data augmentation of a molecular QSAR dataset modeled by a long\nshort term memory (LSTM) cell based neural network. The augmented dataset was\n130 times bigger than the original. The network trained with the augmented\ndataset shows better performance on a test set when compared to a model built\nwith only one canonical SMILES string per molecule. The correlation coefficient\nR2 on the test set was improved from 0.56 to 0.66 when using SMILES\nenumeration, and the root mean square error (RMS) likewise fell from 0.62 to\n0.55. The technique also works in the prediction phase. By taking the average\nper molecule of the predictions for the enumerated SMILES a further improvement\nto a correlation coefficient of 0.68 and a RMS of 0.52 was found. \n\n"}
{"id": "1703.07841", "contents": "Title: Classification-based RNN machine translation using GRUs Abstract: We report the results of our classification-based machine translation model,\nbuilt upon the framework of a recurrent neural network using gated recurrent\nunits. Unlike other RNN models that attempt to maximize the overall conditional\nlog probability of sentences against sentences, our model focuses a\nclassification approach of estimating the conditional probability of the next\nword given the input sequence. This simpler approach using GRUs was hoped to be\ncomparable with more complicated RNN models, but achievements in this\nimplementation were modest and there remains a lot of room for improving this\nclassification approach. \n\n"}
{"id": "1703.08280", "contents": "Title: LRC: Dependency-Aware Cache Management for Data Analytics Clusters Abstract: Memory caches are being aggressively used in today's data-parallel systems\nsuch as Spark, Tez, and Piccolo. However, prevalent systems employ rather\nsimple cache management policies--notably the Least Recently Used (LRU)\npolicy--that are oblivious to the application semantics of data dependency,\nexpressed as a directed acyclic graph (DAG). Without this knowledge, memory\ncaching can at best be performed by \"guessing\" the future data access patterns\nbased on historical information (e.g., the access recency and/or frequency),\nwhich frequently results in inefficient, erroneous caching with low hit ratio\nand a long response time. In this paper, we propose a novel cache replacement\npolicy, Least Reference Count (LRC), which exploits the application-specific\nDAG information to optimize the cache management. LRC evicts the cached data\nblocks whose reference count is the smallest. The reference count is defined,\nfor each data block, as the number of dependent child blocks that have not been\ncomputed yet. We demonstrate the efficacy of LRC through both empirical\nanalysis and cluster deployments against popular benchmarking workloads. Our\nSpark implementation shows that, compared with LRU, LRC speeds up typical\napplications by 60%. \n\n"}
{"id": "1703.08370", "contents": "Title: A randomized primal distributed algorithm for partitioned and big-data\n  non-convex optimization Abstract: In this paper we consider a distributed optimization scenario in which the\naggregate objective function to minimize is partitioned, big-data and possibly\nnon-convex. Specifically, we focus on a set-up in which the dimension of the\ndecision variable depends on the network size as well as the number of local\nfunctions, but each local function handled by a node depends only on a (small)\nportion of the entire optimization variable. This problem set-up has been shown\nto appear in many interesting network application scenarios. As main paper\ncontribution, we develop a simple, primal distributed algorithm to solve the\noptimization problem, based on a randomized descent approach, which works under\nasynchronous gossip communication. We prove that the proposed asynchronous\nalgorithm is a proper, ad-hoc version of a coordinate descent method and thus\nconverges to a stationary point. To show the effectiveness of the proposed\nalgorithm, we also present numerical simulations on a non-convex quadratic\nprogram, which confirm the theoretical results. \n\n"}
{"id": "1703.09137", "contents": "Title: Where to put the Image in an Image Caption Generator Abstract: When a recurrent neural network language model is used for caption\ngeneration, the image information can be fed to the neural network either by\ndirectly incorporating it in the RNN -- conditioning the language model by\n`injecting' image features -- or in a layer following the RNN -- conditioning\nthe language model by `merging' image features. While both options are attested\nin the literature, there is as yet no systematic comparison between the two. In\nthis paper we empirically show that it is not especially detrimental to\nperformance whether one architecture is used or another. The merge architecture\ndoes have practical advantages, as conditioning by merging allows the RNN's\nhidden state vector to shrink in size by up to four times. Our results suggest\nthat the visual and linguistic modalities for caption generation need not be\njointly encoded by the RNN as that yields large, memory-intensive models with\nfew tangible advantages in performance; rather, the multimodal integration\nshould be delayed to a subsequent stage. \n\n"}
{"id": "1703.09707", "contents": "Title: Accelerating gravitational microlensing simulations using the Xeon Phi\n  coprocessor Abstract: Recently Graphics Processing Units (GPUs) have been used to speed up very\nCPU-intensive gravitational microlensing simulations. In this work, we use the\nXeon Phi coprocessor to accelerate such simulations and compare its performance\non a microlensing code with that of NVIDIA's GPUs. For the selected set of\nparameters evaluated in our experiment, we find that the speedup by Intel's\nKnights Corner coprocessor is comparable to that by NVIDIA's Fermi family of\nGPUs with compute capability 2.0, but less significant than GPUs with higher\ncompute capabilities such as the Kepler. However, the very recently released\nsecond generation Xeon Phi, Knights Landing, is about 5.8 times faster than the\nKnights Corner, and about 2.9 times faster than the Kepler GPU used in our\nsimulations. We conclude that the Xeon Phi is a very promising alternative to\nGPUs for modern high performance microlensing simulations. \n\n"}
{"id": "1703.09833", "contents": "Title: Theory II: Landscape of the Empirical Risk in Deep Learning Abstract: Previous theoretical work on deep learning and neural network optimization\ntend to focus on avoiding saddle points and local minima. However, the\npractical observation is that, at least in the case of the most successful Deep\nConvolutional Neural Networks (DCNNs), practitioners can always increase the\nnetwork size to fit the training data (an extreme example would be [1]). The\nmost successful DCNNs such as VGG and ResNets are best used with a degree of\n\"overparametrization\". In this work, we characterize with a mix of theory and\nexperiments, the landscape of the empirical risk of overparametrized DCNNs. We\nfirst prove in the regression framework the existence of a large number of\ndegenerate global minimizers with zero empirical error (modulo inconsistent\nequations). The argument that relies on the use of Bezout theorem is rigorous\nwhen the RELUs are replaced by a polynomial nonlinearity (which empirically\nworks as well). As described in our Theory III [2] paper, the same minimizers\nare degenerate and thus very likely to be found by SGD that will furthermore\nselect with higher probability the most robust zero-minimizer. We further\nexperimentally explored and visualized the landscape of empirical risk of a\nDCNN on CIFAR-10 during the entire training process and especially the global\nminima. Finally, based on our theoretical and experimental results, we propose\nan intuitive model of the landscape of DCNN's empirical loss surface, which\nmight not be as complicated as people commonly believe. \n\n"}
{"id": "1703.10371", "contents": "Title: Born to Learn: the Inspiration, Progress, and Future of Evolved Plastic\n  Artificial Neural Networks Abstract: Biological plastic neural networks are systems of extraordinary computational\ncapabilities shaped by evolution, development, and lifetime learning. The\ninterplay of these elements leads to the emergence of adaptive behavior and\nintelligence. Inspired by such intricate natural phenomena, Evolved Plastic\nArtificial Neural Networks (EPANNs) use simulated evolution in-silico to breed\nplastic neural networks with a large variety of dynamics, architectures, and\nplasticity rules: these artificial systems are composed of inputs, outputs, and\nplastic components that change in response to experiences in an environment.\nThese systems may autonomously discover novel adaptive algorithms, and lead to\nhypotheses on the emergence of biological adaptation. EPANNs have seen\nconsiderable progress over the last two decades. Current scientific and\ntechnological advances in artificial neural networks are now setting the\nconditions for radically new approaches and results. In particular, the\nlimitations of hand-designed networks could be overcome by more flexible and\ninnovative solutions. This paper brings together a variety of inspiring ideas\nthat define the field of EPANNs. The main methods and results are reviewed.\nFinally, new opportunities and developments are presented. \n\n"}
{"id": "1703.11005", "contents": "Title: A simplicial complex model of dynamic epistemic logic for fault-tolerant\n  distributed computing Abstract: The usual epistemic S5 model for multi-agent systems is a Kripke graph, whose\nedges are labeled with the agents that do not distinguish between two states.\nWe propose to uncover the higher dimensional information implicit in the Kripke\ngraph, by using as a model its dual, a chromatic simplicial complex. For each\nstate of the Kripke model there is a facet in the complex, with one vertex per\nagent. If an edge (u,v) is labeled with a set of agents S, the facets\ncorresponding to u and v intersect in a simplex consisting of one vertex for\neach agent of S. Then we use dynamic epistemic logic to study how the\nsimplicial complex epistemic model changes after the agents communicate with\neach other. We show that there are topological invariants preserved from the\ninitial epistemic complex to the epistemic complex after an action model is\napplied, that depend on how reliable the communication is. In turn these\ntopological properties determine the knowledge that the agents may gain after\nthe communication happens. \n\n"}
{"id": "1704.00616", "contents": "Title: Chained Multi-stream Networks Exploiting Pose, Motion, and Appearance\n  for Action Classification and Detection Abstract: General human action recognition requires understanding of various visual\ncues. In this paper, we propose a network architecture that computes and\nintegrates the most important visual cues for action recognition: pose, motion,\nand the raw images. For the integration, we introduce a Markov chain model\nwhich adds cues successively. The resulting approach is efficient and\napplicable to action classification as well as to spatial and temporal action\nlocalization. The two contributions clearly improve the performance over\nrespective baselines. The overall approach achieves state-of-the-art action\nclassification performance on HMDB51, J-HMDB and NTU RGB+D datasets. Moreover,\nit yields state-of-the-art spatio-temporal action localization results on\nUCF101 and J-HMDB. \n\n"}
{"id": "1704.01523", "contents": "Title: MIT at SemEval-2017 Task 10: Relation Extraction with Convolutional\n  Neural Networks Abstract: Over 50 million scholarly articles have been published: they constitute a\nunique repository of knowledge. In particular, one may infer from them\nrelations between scientific concepts, such as synonyms and hyponyms.\nArtificial neural networks have been recently explored for relation extraction.\nIn this work, we continue this line of work and present a system based on a\nconvolutional neural network to extract relations. Our model ranked first in\nthe SemEval-2017 task 10 (ScienceIE) for relation extraction in scientific\narticles (subtask C). \n\n"}
{"id": "1704.02658", "contents": "Title: Distributed Statistical Estimation and Rates of Convergence in Normal\n  Approximation Abstract: This paper presents a class of new algorithms for distributed statistical\nestimation that exploit divide-and-conquer approach. We show that one of the\nkey benefits of the divide-and-conquer strategy is robustness, an important\ncharacteristic for large distributed systems. We establish connections between\nperformance of these distributed algorithms and the rates of convergence in\nnormal approximation, and prove non-asymptotic deviations guarantees, as well\nas limit theorems, for the resulting estimators. Our techniques are illustrated\nthrough several examples: in particular, we obtain new results for the\nmedian-of-means estimator, as well as provide performance guarantees for\ndistributed maximum likelihood estimation. \n\n"}
{"id": "1704.04119", "contents": "Title: A Search for Improved Performance in Regular Expressions Abstract: The primary aim of automated performance improvement is to reduce the running\ntime of programs while maintaining (or improving on) functionality. In this\npaper, Genetic Programming is used to find performance improvements in regular\nexpressions for an array of target programs, representing the first application\nof automated software improvement for run-time performance in the Regular\nExpression language. This particular problem is interesting as there may be\nmany possible alternative regular expressions which perform the same task while\nexhibiting subtle differences in performance. A benchmark suite of candidate\nregular expressions is proposed for improvement. We show that the application\nof Genetic Programming techniques can result in performance improvements in all\ncases.\n  As we start evolution from a known good regular expression, diversity is\ncritical in escaping the local optima of the seed expression. In order to\nunderstand diversity during evolution we compare an initial population\nconsisting of only seed programs with a population initialised using a\ncombination of a single seed individual with individuals generated using PI\nGrow and Ramped-half-and-half initialisation mechanisms. \n\n"}
{"id": "1704.04374", "contents": "Title: HPTT: A High-Performance Tensor Transposition C++ Library Abstract: Recently we presented TTC, a domain-specific compiler for tensor\ntranspositions. Despite the fact that the performance of the generated code is\nnearly optimal, due to its offline nature, TTC cannot be utilized in all the\napplication codes in which the tensor sizes and the necessary tensor\npermutations are determined at runtime. To overcome this limitation, we\nintroduce the open-source C++ library High-Performance Tensor Transposition\n(HPTT). Similar to TTC, HPTT incorporates optimizations such as blocking,\nmulti-threading, and explicit vectorization; furthermore it decomposes any\ntransposition into multiple loops around a so called micro-kernel. This modular\ndesign---inspired by BLIS---makes HPTT easy to port to different architectures,\nby only replacing the hand-vectorized micro-kernel (e.g., a 4x4 transpose).\nHPTT also offers an optional autotuning framework---guided by a performance\nmodel---that explores a vast search space of implementations at runtime\n(similar to FFTW). Across a wide range of different tensor transpositions and\narchitectures (e.g., Intel Ivy Bridge, Intel Knights Landing, ARMv7, IBM\nPower7), HPTT attains a bandwidth comparable to that of SAXPY, and yields\nremarkable speedups over Eigen's tensor transposition implementation. Most\nimportantly, the integration of HPTT into the Cyclops Tensor Framework (CTF)\nimproves the overall performance of tensor contractions by up to 3.1x. \n\n"}
{"id": "1704.05021", "contents": "Title: Sparse Communication for Distributed Gradient Descent Abstract: We make distributed stochastic gradient descent faster by exchanging sparse\nupdates instead of dense updates. Gradient updates are positively skewed as\nmost updates are near zero, so we map the 99% smallest updates (by absolute\nvalue) to zero then exchange sparse matrices. This method can be combined with\nquantization to further improve the compression. We explore different\nconfigurations and apply them to neural machine translation and MNIST image\nclassification tasks. Most configurations work on MNIST, whereas different\nconfigurations reduce convergence rate on the more complex translation task.\nOur experiments show that we can achieve up to 49% speed up on MNIST and 22% on\nNMT without damaging the final accuracy or BLEU. \n\n"}
{"id": "1704.05143", "contents": "Title: The Emergence of Canalization and Evolvability in an Open-Ended,\n  Interactive Evolutionary System Abstract: Natural evolution has produced a tremendous diversity of functional\norganisms. Many believe an essential component of this process was the\nevolution of evolvability, whereby evolution speeds up its ability to innovate\nby generating a more adaptive pool of offspring. One hypothesized mechanism for\nevolvability is developmental canalization, wherein certain dimensions of\nvariation become more likely to be traversed and others are prevented from\nbeing explored (e.g. offspring tend to have similarly sized legs, and mutations\naffect the length of both legs, not each leg individually). While ubiquitous in\nnature, canalization almost never evolves in computational simulations of\nevolution. Not only does that deprive us of in silico models in which to study\nthe evolution of evolvability, but it also raises the question of which\nconditions give rise to this form of evolvability. Answering this question\nwould shed light on why such evolvability emerged naturally and could\naccelerate engineering efforts to harness evolution to solve important\nengineering challenges. In this paper we reveal a unique system in which\ncanalization did emerge in computational evolution. We document that genomes\nentrench certain dimensions of variation that were frequently explored during\ntheir evolutionary history. The genetic representation of these organisms also\nevolved to be highly modular and hierarchical, and we show that these\norganizational properties correlate with increased fitness. Interestingly, the\ntype of computational evolutionary experiment that produced this evolvability\nwas very different from traditional digital evolution in that there was no\nobjective, suggesting that open-ended, divergent evolutionary processes may be\nnecessary for the evolution of evolvability. \n\n"}
{"id": "1704.05592", "contents": "Title: Testing Docker Performance for HPC Applications Abstract: The main goal for this article is to compare performance penalties when using\nKVM virtualization and Docker containers for creating isolated environments for\nHPC applications. The article provides both data obtained using commonly\naccepted synthetic tests (High Performance Linpack) and real life applications\n(OpenFOAM). The article highlights the influence on resulting application\nperformance of major infrastructure configuration options: CPU type presented\nto VM, networking connection type used. \n\n"}
{"id": "1704.06191", "contents": "Title: Softmax GAN Abstract: Softmax GAN is a novel variant of Generative Adversarial Network (GAN). The\nkey idea of Softmax GAN is to replace the classification loss in the original\nGAN with a softmax cross-entropy loss in the sample space of one single batch.\nIn the adversarial learning of $N$ real training samples and $M$ generated\nsamples, the target of discriminator training is to distribute all the\nprobability mass to the real samples, each with probability $\\frac{1}{M}$, and\ndistribute zero probability to generated data. In the generator training phase,\nthe target is to assign equal probability to all data points in the batch, each\nwith probability $\\frac{1}{M+N}$. While the original GAN is closely related to\nNoise Contrastive Estimation (NCE), we show that Softmax GAN is the Importance\nSampling version of GAN. We futher demonstrate with experiments that this\nsimple change stabilizes GAN training. \n\n"}
{"id": "1704.07911", "contents": "Title: Explaining How a Deep Neural Network Trained with End-to-End Learning\n  Steers a Car Abstract: As part of a complete software stack for autonomous driving, NVIDIA has\ncreated a neural-network-based system, known as PilotNet, which outputs\nsteering angles given images of the road ahead. PilotNet is trained using road\nimages paired with the steering angles generated by a human driving a\ndata-collection car. It derives the necessary domain knowledge by observing\nhuman drivers. This eliminates the need for human engineers to anticipate what\nis important in an image and foresee all the necessary rules for safe driving.\nRoad tests demonstrated that PilotNet can successfully perform lane keeping in\na wide variety of driving conditions, regardless of whether lane markings are\npresent or not.\n  The goal of the work described here is to explain what PilotNet learns and\nhow it makes its decisions. To this end we developed a method for determining\nwhich elements in the road image most influence PilotNet's steering decision.\nResults show that PilotNet indeed learns to recognize relevant objects on the\nroad.\n  In addition to learning the obvious features such as lane markings, edges of\nroads, and other cars, PilotNet learns more subtle features that would be hard\nto anticipate and program by engineers, for example, bushes lining the edge of\nthe road and atypical vehicle classes. \n\n"}
{"id": "1704.08713", "contents": "Title: Finding the Size and the Diameter of a Radio Network Using Short Labels Abstract: The number of nodes of a network, called its size, and the largest distance\nbetween nodes of a network, called its diameter, are among the most important\nnetwork parameters. Knowing the size and/or diameter is a prerequisite of many\ndistributed network algorithms. A radio network is a collection of nodes, with\nwireless transmission and receiving capabilities. It is modeled as a simple\nundirected graph whose nodes communicate in synchronous rounds. In each round,\na node can either transmit a message to all its neighbors, or stay silent and\nlisten. At the receiving end, a node $v$ hears a message from a neighbor $w$ in\na round $i$, if $v$ listens in round $i$, and if $w$ is its only neighbor that\ntransmits in round $i$. If $v$ listens in a round, and multiple neighbors of\n$v$ transmit in this round, a collision occurs at $v$. If $v$ transmits in a\nround, it does not hear anything. If listening nodes can distinguish collision\nfrom silence, we say that the network has collision detection capability,\notherwise there is no collision detection. We consider the tasks of size\ndiscovery and diameter discovery: finding the size (resp. the diameter) of an\nunknown radio network with collision detection. All nodes have to output the\nsize (resp. the diameter) of the network, using a deterministic algorithm.\nNodes have labels which are binary strings. The length of a labeling scheme is\nthe largest length of a label. We concentrate on the following problems:\n  1. What is the shortest labeling scheme that permits size discovery in all\nradio networks of maximum degree $\\Delta$? 2. What is the shortest labeling\nscheme that permits diameter discovery in all radio networks?\n  We show that the minimum length of a labeling scheme that permits size\ndiscovery is $\\Theta(\\log\\log \\Delta)$. By contrast, we show that diameter\ndiscovery can be done using a labeling scheme of constant length. \n\n"}
{"id": "1705.01146", "contents": "Title: Population protocols for leader election and exact majority with O(log^2\n  n) states and O(log^2 n) convergence time Abstract: We consider the model of population protocols, which can be viewed as a\nsequence of random pairwise interactions of $n$ agents (nodes). We show\npopulation protocols for two problems: the leader election and the exact\nmajority voting. The leader election starts with all agents in the same initial\nstate and the goal is to converge to the (global) state when exactly one agent\nis in a distinct state $L$. The exact majority voting starts with each agent in\none of the two distinct states $A$ or $B$ and the goal is to make all nodes\nknow which of these two states was the initial majority state, even if that\nmajority was just by a single vote.\n  Alistarh and Gelashvili [ICALP 2015] showed a leader-election protocol which\nconverges in $O(\\log^3 n)$ time w.h.p. and in expectation and needs\n$\\Theta(\\log^3 n)$ states per agent. We present a protocol which elects the\nleader in $O(\\log^2 n)$ time w.h.p. and in expectation and uses $\\Theta(\\log^2\nn)$ states per agent. For the exact majority voting, we show a population\nprotocol with the same asymptotic performance: $O(\\log^2 n)$ time and\n$\\Theta(\\log^2 n)$ states per agent. The exact-majority protocol proposed by\nAlistarh et al. [PODC 2015] achieves expected $O(\\log^2 n)$ time, but requires\na relatively high initial imbalance between $A$'s and $B$'s or a large number\nof states per agent. More recently, Alistarh et al. [SODA 2017] showed\n$O(\\log^2 n)$-state protocols for both problems, with the exact majority\nprotocol converging in time $O(\\log^3 n)$, and the leader election protocol\nconverging in time $O(\\log^{6.3} n)$ w.h.p. and $O(\\log^{5.3} n)$ in\nexpectation.\n  Our leader election and exact majority protocols are based on the idea of\nagents counting their local interactions and rely on the probabilistic fact\nthat the uniform random selection would limit the divergence of the individual\ncounts. \n\n"}
{"id": "1705.01176", "contents": "Title: How does Docker affect energy consumption? Evaluating workloads in and\n  out of Docker containers Abstract: Context: Virtual machines provide isolation of services at the cost of\nhypervisors and more resource usage. This spurred the growth of systems like\nDocker that enable single hosts to isolate several applications, similar to\nVMs, within a low-overhead abstraction called containers.\n  Motivation: Although containers tout low overhead performance, do they still\nhave low energy consumption?\n  Methodology: This work statistically compares ($t$-test, Wilcoxon) the energy\nconsumption of three application workloads in Docker and on bare-metal Linux.\n  Results: In all cases, there was a statistically significant ($t$-test and\nWilcoxon $p < 0.05$) increase in energy consumption when running tests in\nDocker, mostly due to the performance of I/O system calls. \n\n"}
{"id": "1705.01462", "contents": "Title: Ternary Neural Networks with Fine-Grained Quantization Abstract: We propose a novel fine-grained quantization (FGQ) method to ternarize\npre-trained full precision models, while also constraining activations to 8 and\n4-bits. Using this method, we demonstrate a minimal loss in classification\naccuracy on state-of-the-art topologies without additional training. We provide\nan improved theoretical formulation that forms the basis for a higher quality\nsolution using FGQ. Our method involves ternarizing the original weight tensor\nin groups of $N$ weights. Using $N=4$, we achieve Top-1 accuracy within $3.7\\%$\nand $4.2\\%$ of the baseline full precision result for Resnet-101 and Resnet-50\nrespectively, while eliminating $75\\%$ of all multiplications. These results\nenable a full 8/4-bit inference pipeline, with best-reported accuracy using\nternary weights on ImageNet dataset, with a potential of $9\\times$ improvement\nin performance. Also, for smaller networks like AlexNet, FGQ achieves\nstate-of-the-art results. We further study the impact of group size on both\nperformance and accuracy. With a group size of $N=64$, we eliminate\n$\\approx99\\%$ of the multiplications; however, this introduces a noticeable\ndrop in accuracy, which necessitates fine tuning the parameters at lower\nprecision. We address this by fine-tuning Resnet-50 with 8-bit activations and\nternary weights at $N=64$, improving the Top-1 accuracy to within $4\\%$ of the\nfull precision result with $<30\\%$ additional training overhead. Our final\nquantized model can run on a full 8-bit compute pipeline using 2-bit weights\nand has the potential of up to $15\\times$ improvement in performance compared\nto baseline full-precision models. \n\n"}
{"id": "1705.02609", "contents": "Title: Emptiness Problems for Distributed Automata Abstract: We investigate the decidability of the emptiness problem for three classes of\ndistributed automata. These devices operate on finite directed graphs, acting\nas networks of identical finite-state machines that communicate in an infinite\nsequence of synchronous rounds. The problem is shown to be decidable in\nLogSpace for a class of forgetful automata, where the nodes see the messages\nreceived from their neighbors but cannot remember their own state. When\nrestricted to the appropriate families of graphs, these forgetful automata are\nequivalent to classical finite word automata, but strictly more expressive than\nfinite tree automata. On the other hand, we also show that the emptiness\nproblem is undecidable in general. This already holds for two heavily\nrestricted classes of distributed automata: those that reject immediately if\nthey receive more than one message per round, and those whose state diagram\nmust be acyclic except for self-loops. \n\n"}
{"id": "1705.02851", "contents": "Title: Flat Parallelization Abstract: There are two intertwined factors that affect performance of concurrent data\nstructures: the ability of processes to access the data in parallel and the\ncost of synchronization. It has been observed that for a large class of\n\"concurrency-unfriendly\" data structures, fine-grained parallelization does not\npay off: an implementation based on a single global lock outperforms\nfine-grained solutions. The flat combining paradigm exploits this by ensuring\nthat a thread holding the global lock sequentially combines requests and then\nexecutes the combined requests on behalf of concurrent threads.\n  In this paper, we propose a synchronization technique that unites flat\ncombining and parallel bulk updates borrowed from parallel algorithms designed\nfor the PRAM model. The idea is that the combiner thread assigns waiting\nthreads to perform concurrent requests in parallel.\n  We foresee the technique to help in implementing efficient\n\"concurrency-ambivalent\" data structures, which can benefit from both\nparallelism and serialization, depending on the operational context. To\nvalidate the idea, we considered heap-based implementations of a priority\nqueue. These data structures exhibit two important features: concurrent remove\noperations are likely to conflict and thus may benefit from combining, while\nconcurrent insert operations can often be at least partly applied in parallel\nthus may benefit from parallel batching. We show that the resulting flat\nparallelization algorithm performs well compared to state-of-the-art priority\nqueue implementations. \n\n"}
{"id": "1705.02898", "contents": "Title: Tight Bounds for Asymptotic and Approximate Consensus Abstract: We study the performance of asymptotic and approximate consensus algorithms\nunder harsh environmental conditions. The asymptotic consensus problem requires\na set of agents to repeatedly set their outputs such that the outputs converge\nto a common value within the convex hull of initial values. This problem, and\nthe related approximate consensus problem, are fundamental building blocks in\ndistributed systems where exact consensus among agents is not required or\npossible, e.g., man-made distributed control systems, and have applications in\nthe analysis of natural distributed systems, such as flocking and opinion\ndynamics. We prove tight lower bounds on the contraction rates of asymptotic\nconsensus algorithms in dynamic networks, from which we deduce bounds on the\ntime complexity of approximate consensus algorithms. In particular, the\nobtained bounds show optimality of asymptotic and approximate consensus\nalgorithms presented in [Charron-Bost et al., ICALP'16] for certain dynamic\nnetworks, including the weakest dynamic network model in which asymptotic and\napproximate consensus are solvable. As a corollary we also obtain\nasymptotically tight bounds for asymptotic consensus in the classical\nasynchronous model with crashes.\n  Central to our lower bound proofs is an extended notion of valency, the set\nof reachable limits of an asymptotic consensus algorithm starting from a given\nconfiguration. We further relate topological properties of valencies to the\nsolvability of exact consensus, shedding some light on the relation of these\nthree fundamental problems in dynamic networks. \n\n"}
{"id": "1705.02970", "contents": "Title: TaskUniVerse: A Task-Based Unified Interface for Versatile Parallel\n  Execution Abstract: Task based parallel programming has shown competitive outcomes in many\naspects of parallel programming such as efficiency, performance, productivity\nand scalability. Different approaches are used by different software\ndevelopment frameworks to provide these outcomes to the programmer, while\nmaking the underlying hardware architecture transparent to her. However, since\nprograms are not portable between these frameworks, using one framework or the\nother is still a vital decision by the programmer whose concerns are\nexpandability, adaptivity, maintainability and interoperability of the\nprograms. In this work, we propose a unified programming interface that a\nprogrammer can use for working with different task based parallel frameworks\ntransparently. In this approach we abstract the common concepts of task based\nparallel programming and provide them to the programmer in a single programming\ninterface uniformly for all frameworks. We have tested the interface by running\nprograms which implement matrix operations within frameworks that are optimized\nfor shared and distributed memory architectures and accelerators, while the\ncooperation between frameworks is configured externally with no need to modify\nthe programs. Further possible extensions of the interface and future potential\nresearch are also described. \n\n"}
{"id": "1705.02995", "contents": "Title: Developing All-Skyrmion Spiking Neural Network Abstract: In this work, we have proposed a revolutionary neuromorphic computing\nmethodology to implement All-Skyrmion Spiking Neural Network (AS-SNN). Such\nproposed methodology is based on our finding that skyrmion is a topological\nstable spin texture and its spatiotemporal motion along the magnetic nano-track\nintuitively interprets the pulse signal transmission between two interconnected\nneurons. In such design, spike train in SNN could be encoded as particle-like\nskyrmion train and further processed by the proposed skyrmion-synapse and\nskyrmion-neuron within the same magnetic nano-track to generate output skyrmion\nas post-spike. Then, both pre-neuron spikes and post-neuron spikes are encoded\nas particle-like skyrmions without conversion between charge and spin signals,\nwhich fundamentally differentiates our proposed design from other hybrid\nSpin-CMOS designs. The system level simulation shows 87.1% inference accuracy\nfor handwritten digit recognition task, while the energy dissipation is ~1\nfJ/per spike which is 3 orders smaller in comparison with CMOS based IBM\nTrueNorth system. \n\n"}
{"id": "1705.03125", "contents": "Title: Affinity Scheduling and the Applications on Data Center Scheduling with\n  Data Locality Abstract: MapReduce framework is the de facto standard in Hadoop. Considering the data\nlocality in data centers, the load balancing problem of map tasks is a special\ncase of affinity scheduling problem. There is a huge body of work on affinity\nscheduling, proposing heuristic algorithms which try to increase data locality\nin data centers like Delay Scheduling and Quincy. However, not enough attention\nhas been put on theoretical guarantees on throughput and delay optimality of\nsuch algorithms. In this work, we present and compare different algorithms and\ndiscuss their shortcoming and strengths. To the best of our knowledge, most\ndata centers are using static load balancing algorithms which are not efficient\nin any ways and results in wasting the resources and causing unnecessary delays\nfor users. \n\n"}
{"id": "1705.03152", "contents": "Title: Phone-aware Neural Language Identification Abstract: Pure acoustic neural models, particularly the LSTM-RNN model, have shown\ngreat potential in language identification (LID). However, the phonetic\ninformation has been largely overlooked by most of existing neural LID models,\nalthough this information has been used in the conventional phonetic LID\nsystems with a great success. We present a phone-aware neural LID architecture,\nwhich is a deep LSTM-RNN LID system but accepts output from an RNN-based ASR\nsystem. By utilizing the phonetic knowledge, the LID performance can be\nsignificantly improved. Interestingly, even if the test language is not\ninvolved in the ASR training, the phonetic knowledge still presents a large\ncontribution. Our experiments conducted on four languages within the Babel\ncorpus demonstrated that the phone-aware approach is highly effective. \n\n"}
{"id": "1705.03414", "contents": "Title: A Distributed Learning Dynamics in Social Groups Abstract: We study a distributed learning process observed in human groups and other\nsocial animals. This learning process appears in settings in which each\nindividual in a group is trying to decide over time, in a distributed manner,\nwhich option to select among a shared set of options. Specifically, we consider\na stochastic dynamics in a group in which every individual selects an option in\nthe following two-step process: (1) select a random individual and observe the\noption that individual chose in the previous time step, and (2) adopt that\noption if its stochastic quality was good at that time step. Various\ninstantiations of such distributed learning appear in nature, and have also\nbeen studied in the social science literature. From the perspective of an\nindividual, an attractive feature of this learning process is that it is a\nsimple heuristic that requires extremely limited computational capacities. But\nwhat does it mean for the group -- could such a simple, distributed and\nessentially memoryless process lead the group as a whole to perform optimally?\nWe show that the answer to this question is yes -- this distributed learning is\nhighly effective at identifying the best option and is close to optimal for the\ngroup overall. Our analysis also gives quantitative bounds that show fast\nconvergence of these stochastic dynamics. Prior to our work the only\ntheoretical work related to such learning dynamics has been either in\ndeterministic special cases or in the asymptotic setting. Finally, we observe\nthat our infinite population dynamics is a stochastic variant of the classic\nmultiplicative weights update (MWU) method. Consequently, we arrive at the\nfollowing interesting converse: the learning dynamics on a finite population\nconsidered here can be viewed as a novel distributed and low-memory\nimplementation of the classic MWU method. \n\n"}
{"id": "1705.03427", "contents": "Title: Rapid Mixing of Local Graph Dynamics Abstract: Graph dynamics arise naturally in many contexts. For instance in peer-to-peer\nnetworks, a participating peer may replace an existing connection with one\nneighbour by a new connection with a neighbour's neighbour. Several such local\nrewiring rules have been proposed to ensure that peer-to-peer networks achieve\ngood connectivity properties (e.g. high expansion) in equilibrium. However it\nhas remained an open question whether there existed such rules that also led to\nfast convergence to equilibrium. In this work we provide an affirmative answer:\nWe exhibit a local rewiring rule that converges to equilibrium after each\nparticipating node has undergone only a number of rewirings that is\npoly-logarithmic in the system size. The proof involves consideration of the\nwhole isoperimetric profile of the graph, and may be of independent interest. \n\n"}
{"id": "1705.03572", "contents": "Title: Discovery Radiomics via Evolutionary Deep Radiomic Sequencer Discovery\n  for Pathologically-Proven Lung Cancer Detection Abstract: While lung cancer is the second most diagnosed form of cancer in men and\nwomen, a sufficiently early diagnosis can be pivotal in patient survival rates.\nImaging-based, or radiomics-driven, detection methods have been developed to\naid diagnosticians, but largely rely on hand-crafted features which may not\nfully encapsulate the differences between cancerous and healthy tissue.\nRecently, the concept of discovery radiomics was introduced, where custom\nabstract features are discovered from readily available imaging data. We\npropose a novel evolutionary deep radiomic sequencer discovery approach based\non evolutionary deep intelligence. Motivated by patient privacy concerns and\nthe idea of operational artificial intelligence, the evolutionary deep radiomic\nsequencer discovery approach organically evolves increasingly more efficient\ndeep radiomic sequencers that produce significantly more compact yet similarly\ndescriptive radiomic sequences over multiple generations. As a result, this\nframework improves operational efficiency and enables diagnosis to be run\nlocally at the radiologist's computer while maintaining detection accuracy. We\nevaluated the evolved deep radiomic sequencer (EDRS) discovered via the\nproposed evolutionary deep radiomic sequencer discovery framework against\nstate-of-the-art radiomics-driven and discovery radiomics methods using\nclinical lung CT data with pathologically-proven diagnostic data from the\nLIDC-IDRI dataset. The evolved deep radiomic sequencer shows improved\nsensitivity (93.42%), specificity (82.39%), and diagnostic accuracy (88.78%)\nrelative to previous radiomics approaches. \n\n"}
{"id": "1705.04144", "contents": "Title: Error-Sensitive Proof-Labeling Schemes Abstract: Proof-labeling schemes are known mechanisms providing nodes of networks with\ncertificates that can be verified locally by distributed algorithms. Given a\nboolean predicate on network states, such schemes enable to check whether the\npredicate is satisfied by the actual state of the network, by having nodes\ninteracting with their neighbors only. Proof-labeling schemes are typically\ndesigned for enforcing fault-tolerance, by making sure that if the current\nstate of the network is illegal with respect to some given predicate, then at\nleast one node will detect it. Such a node can raise an alarm, or launch a\nrecovery procedure enabling the system to return to a legal state. In this\npaper, we introduce error-sensitive proof-labeling schemes. These are\nproof-labeling schemes which guarantee that the number of nodes detecting\nillegal states is linearly proportional to the edit-distance between the\ncurrent state and the set of legal states. By using error-sensitive\nproof-labeling schemes, states which are far from satisfying the predicate will\nbe detected by many nodes, enabling fast return to legality. We provide a\nstructural characterization of the set of boolean predicates on network states\nfor which there exist error-sensitive proof-labeling schemes. This\ncharacterization allows us to show that classical predicates such as, e.g.,\nacyclicity, and leader admit error-sensitive proof-labeling schemes, while\nothers like regular subgraphs don't. We also focus on compact error-sensitive\nproof-labeling schemes. In particular, we show that the known proof-labeling\nschemes for spanning tree and minimum spanning tree, using certificates on\n$O(\\log n)$ bits, and on $O\\left(\\log^2n\\right)$ bits, respectively, are\nerror-sensitive, as long as the trees are locally represented by adjacency\nlists, and not just by parent pointers. \n\n"}
{"id": "1705.04374", "contents": "Title: Optimal fidelity multi-level Monte Carlo for quantification of\n  uncertainty in simulations of cloud cavitation collapse Abstract: We quantify uncertainties in the location and magnitude of extreme pressure\nspots revealed from large scale multi-phase flow simulations of cloud\ncavitation collapse. We examine clouds containing 500 cavities and quantify\nuncertainties related to their initial spatial arrangement. The resulting\n2000-dimensional space is sampled using a non-intrusive and computationally\nefficient Multi-Level Monte Carlo (MLMC) methodology. We introduce novel\noptimal control variate coefficients to enhance the variance reduction in MLMC.\nThe proposed optimal fidelity MLMC leads to more than two orders of magnitude\nspeedup when compared to standard Monte Carlo methods. We identify large\nuncertainties in the location and magnitude of the peak pressure pulse and\npresent its statistical correlations and joint probability density functions\nwith the geometrical characteristics of the cloud. Characteristic properties of\nspatial cloud structure are identified as potential causes of significant\nuncertainties in exerted collapse pressures. \n\n"}
{"id": "1705.04378", "contents": "Title: An overview and comparative analysis of Recurrent Neural Networks for\n  Short Term Load Forecasting Abstract: The key component in forecasting demand and consumption of resources in a\nsupply network is an accurate prediction of real-valued time series. Indeed,\nboth service interruptions and resource waste can be reduced with the\nimplementation of an effective forecasting system. Significant research has\nthus been devoted to the design and development of methodologies for short term\nload forecasting over the past decades. A class of mathematical models, called\nRecurrent Neural Networks, are nowadays gaining renewed interest among\nresearchers and they are replacing many practical implementation of the\nforecasting systems, previously based on static methods. Despite the undeniable\nexpressive power of these architectures, their recurrent nature complicates\ntheir understanding and poses challenges in the training procedures. Recently,\nnew important families of recurrent architectures have emerged and their\napplicability in the context of load forecasting has not been investigated\ncompletely yet. In this paper we perform a comparative study on the problem of\nShort-Term Load Forecast, by using different classes of state-of-the-art\nRecurrent Neural Networks. We test the reviewed models first on controlled\nsynthetic tasks and then on different real datasets, covering important\npractical cases of study. We provide a general overview of the most important\narchitectures and we define guidelines for configuring the recurrent networks\nto predict real-valued time series. \n\n"}
{"id": "1705.04835", "contents": "Title: Which Broadcast Abstraction Captures $k$-Set Agreement? Abstract: It is well-known that consensus (one-set agreement) and total order broadcast\nare equivalent in asynchronous systems prone to process crash failures.\nConsidering wait-free systems, this article addresses and answers the following\nquestion: which is the communication abstraction that \"captures\" $k$-set\nagreement? To this end, it introduces a new broadcast communication\nabstraction, called $k$-BO-Broadcast, which restricts the disagreement on the\nlocal deliveries of the messages that have been broadcast ($1$-BO-Broadcast\nboils down to total order broadcast). Hence, in this context, $k=1$ is not a\nspecial number, but only the first integer in an increasing integer sequence.\n  This establishes a new \"correspondence\" between distributed agreement\nproblems and communication abstractions, which enriches our understanding of\nthe relations linking fundamental issues of fault-tolerant distributed\ncomputing. \n\n"}
{"id": "1705.05231", "contents": "Title: Autonomous and Connected Intersection Crossing Traffic Management using\n  Discrete-Time Occupancies Trajectory Abstract: In this paper, we address a problem of safe and efficient intersection\ncrossing traffic management of autonomous and connected ground traffic. Toward\nthis objective, we propose an algorithm that is called the Discrete-time\noccupancies trajectory based Intersection traffic Coordination Algorithm\n(DICA). We first prove that the basic DICA is deadlock free and also starvation\nfree. Then, we show that the basic DICA has a computational complexity of\n$\\mathcal{O}(n^2 L_m^3)$ where $n$ is the number of vehicles granted to cross\nan intersection and $L_m$ is the maximum length of intersection crossing\nroutes.\n  To improve the overall computational efficiency of the algorithm, the basic\nDICA is enhanced by several computational approaches that are proposed in this\npaper. The enhanced algorithm has the computational complexity of\n$\\mathcal{O}(n^2 L_m \\log_2 L_m)$. The improved computational efficiency of the\nenhanced algorithm is validated through simulation using an open source traffic\nsimulator, called the Simulation of Urban MObility (SUMO). The overall\nthroughput as well as the computational efficiency of the enhanced algorithm\nare also compared with those of an optimized traffic light control. \n\n"}
{"id": "1705.06266", "contents": "Title: A Parallel Solver for Graph Laplacians Abstract: Problems from graph drawing, spectral clustering, network flow and graph\npartitioning can all be expressed in terms of graph Laplacian matrices. There\nare a variety of practical approaches to solving these problems in serial.\nHowever, as problem sizes increase and single core speeds stagnate, parallelism\nis essential to solve such problems quickly. We present an unsmoothed\naggregation multigrid method for solving graph Laplacians in a distributed\nmemory setting. We introduce new parallel aggregation and low degree\nelimination algorithms targeted specifically at irregular degree graphs. These\nalgorithms are expressed in terms of sparse matrix-vector products using\ngeneralized sum and product operations. This formulation is amenable to linear\nalgebra using arbitrary distributions and allows us to operate on a 2D sparse\nmatrix distribution, which is necessary for parallel scalability. Our solver\noutperforms the natural parallel extension of the current state of the art in\nan algorithmic comparison. We demonstrate scalability to 576 processes and\ngraphs with up to 1.7 billion edges. \n\n"}
{"id": "1705.06693", "contents": "Title: Limited-Memory Matrix Adaptation for Large Scale Black-box Optimization Abstract: The Covariance Matrix Adaptation Evolution Strategy (CMA-ES) is a popular\nmethod to deal with nonconvex and/or stochastic optimization problems when the\ngradient information is not available. Being based on the CMA-ES, the recently\nproposed Matrix Adaptation Evolution Strategy (MA-ES) provides a rather\nsurprising result that the covariance matrix and all associated operations\n(e.g., potentially unstable eigendecomposition) can be replaced in the CMA-ES\nby a updated transformation matrix without any loss of performance. In order to\nfurther simplify MA-ES and reduce its $\\mathcal{O}\\big(n^2\\big)$ time and\nstorage complexity to $\\mathcal{O}\\big(n\\log(n)\\big)$, we present the\nLimited-Memory Matrix Adaptation Evolution Strategy (LM-MA-ES) for efficient\nzeroth order large-scale optimization. The algorithm demonstrates\nstate-of-the-art performance on a set of established large-scale benchmarks. We\nexplore the algorithm on the problem of generating adversarial inputs for a\n(non-smooth) random forest classifier, demonstrating a surprising vulnerability\nof the classifier. \n\n"}
{"id": "1705.06820", "contents": "Title: Pixel Deconvolutional Networks Abstract: Deconvolutional layers have been widely used in a variety of deep models for\nup-sampling, including encoder-decoder networks for semantic segmentation and\ndeep generative models for unsupervised learning. One of the key limitations of\ndeconvolutional operations is that they result in the so-called checkerboard\nproblem. This is caused by the fact that no direct relationship exists among\nadjacent pixels on the output feature map. To address this problem, we propose\nthe pixel deconvolutional layer (PixelDCL) to establish direct relationships\namong adjacent pixels on the up-sampled feature map. Our method is based on a\nfresh interpretation of the regular deconvolution operation. The resulting\nPixelDCL can be used to replace any deconvolutional layer in a plug-and-play\nmanner without compromising the fully trainable capabilities of original\nmodels. The proposed PixelDCL may result in slight decrease in efficiency, but\nthis can be overcome by an implementation trick. Experimental results on\nsemantic segmentation demonstrate that PixelDCL can consider spatial features\nsuch as edges and shapes and yields more accurate segmentation outputs than\ndeconvolutional layers. When used in image generation tasks, our PixelDCL can\nlargely overcome the checkerboard problem suffered by regular deconvolution\noperations. \n\n"}
{"id": "1705.06821", "contents": "Title: Spatial Variational Auto-Encoding via Matrix-Variate Normal\n  Distributions Abstract: The key idea of variational auto-encoders (VAEs) resembles that of\ntraditional auto-encoder models in which spatial information is supposed to be\nexplicitly encoded in the latent space. However, the latent variables in VAEs\nare vectors, which can be interpreted as multiple feature maps of size 1x1.\nSuch representations can only convey spatial information implicitly when\ncoupled with powerful decoders. In this work, we propose spatial VAEs that use\nfeature maps of larger size as latent variables to explicitly capture spatial\ninformation. This is achieved by allowing the latent variables to be sampled\nfrom matrix-variate normal (MVN) distributions whose parameters are computed\nfrom the encoder network. To increase dependencies among locations on latent\nfeature maps and reduce the number of parameters, we further propose spatial\nVAEs via low-rank MVN distributions. Experimental results show that the\nproposed spatial VAEs outperform original VAEs in capturing rich structural and\nspatial information. \n\n"}
{"id": "1705.07212", "contents": "Title: Space Complexity of Fault Tolerant Register Emulations Abstract: Driven by the rising popularity of cloud storage, the costs associated with\nimplementing reliable storage services from a collection of fault-prone servers\nhave recently become an actively studied question. The well-known ABD result\nshows that an f-tolerant register can be emulated using a collection of 2f + 1\nfault-prone servers each storing a single read-modify-write object type, which\nis known to be optimal. In this paper we generalize this bound: we investigate\nthe inherent space complexity of emulating reliable multi-writer registers as a\nfucntion of the type of the base objects exposed by the underlying servers, the\nnumber of writers to the emulated register, the number of available servers,\nand the failure threshold. We establish a sharp separation between registers,\nand both max-registers (the base object types assumed by ABD) and CAS in terms\nof the resources (i.e., the number of base objects of the respective types)\nrequired to support the emulation; we show that no such separation exists\nbetween max-registers and CAS. Our main technical contribution is lower and\nupper bounds on the resources required in case the underlying base objects are\nfault-prone read/write registers. We show that the number of required registers\nis directly proportional to the number of writers and inversely proportional to\nthe number of servers. \n\n"}
{"id": "1705.07565", "contents": "Title: Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain\n  Surgeon Abstract: How to develop slim and accurate deep neural networks has become crucial for\nreal- world applications, especially for those employed in embedded systems.\nThough previous work along this research line has shown some promising results,\nmost existing methods either fail to significantly compress a well-trained deep\nnetwork or require a heavy retraining process for the pruned deep network to\nre-boost its prediction performance. In this paper, we propose a new layer-wise\npruning method for deep neural networks. In our proposed method, parameters of\neach individual layer are pruned independently based on second order\nderivatives of a layer-wise error function with respect to the corresponding\nparameters. We prove that the final prediction performance drop after pruning\nis bounded by a linear combination of the reconstructed errors caused at each\nlayer. Therefore, there is a guarantee that one only needs to perform a light\nretraining process on the pruned network to resume its original prediction\nperformance. We conduct extensive experiments on benchmark datasets to\ndemonstrate the effectiveness of our pruning method compared with several\nstate-of-the-art baseline methods. \n\n"}
{"id": "1705.07861", "contents": "Title: Symmetry Breaking in the Congest Model: Time- and Message-Efficient\n  Algorithms for Ruling Sets Abstract: We study local symmetry breaking problems in the CONGEST model, focusing on\nruling set problems, which generalize the fundamental Maximal Independent Set\n(MIS) problem. A $\\beta$-ruling set is an independent set such that every node\nin the graph is at most $\\beta$ hops from a node in the independent set. Our\nwork is motivated by the following central question: can we break the\n$\\Theta(\\log n)$ time complexity barrier and the $\\Theta(m)$ message complexity\nbarrier in the CONGEST model for MIS or closely-related symmetry breaking\nproblems? We present the following results:\n  - Time Complexity: We show that we can break the $O(\\log n)$ \"barrier\" for 2-\nand 3-ruling sets. We compute 3-ruling sets in $O\\left(\\frac{\\log n}{\\log \\log\nn}\\right)$ rounds with high probability (whp). More generally we show that\n2-ruling sets can be computed in $O\\left(\\log \\Delta \\cdot (\\log n)^{1/2 +\n\\varepsilon} + \\frac{\\log n}{\\log\\log n}\\right)$ rounds for any $\\varepsilon >\n0$, which is $o(\\log n)$ for a wide range of $\\Delta$ values (e.g., $\\Delta =\n2^{(\\log n)^{1/2-\\varepsilon}}$). These are the first 2- and 3-ruling set\nalgorithms to improve over the $O(\\log n)$-round complexity of Luby's algorithm\nin the CONGEST model.\n  - Message Complexity: We show an $\\Omega(n^2)$ lower bound on the message\ncomplexity of computing an MIS (i.e., 1-ruling set) which holds also for\nrandomized algorithms and present a contrast to this by showing a randomized\nalgorithm for 2-ruling sets that, whp, uses only $O(n \\log^2 n)$ messages and\nruns in $O(\\Delta \\log n)$ rounds. This is the first message-efficient\nalgorithm known for ruling sets, which has message complexity nearly linear in\n$n$ (which is optimal up to a polylogarithmic factor). \n\n"}
{"id": "1705.09056", "contents": "Title: Can Decentralized Algorithms Outperform Centralized Algorithms? A Case\n  Study for Decentralized Parallel Stochastic Gradient Descent Abstract: Most distributed machine learning systems nowadays, including TensorFlow and\nCNTK, are built in a centralized fashion. One bottleneck of centralized\nalgorithms lies on high communication cost on the central node. Motivated by\nthis, we ask, can decentralized algorithms be faster than its centralized\ncounterpart?\n  Although decentralized PSGD (D-PSGD) algorithms have been studied by the\ncontrol community, existing analysis and theory do not show any advantage over\ncentralized PSGD (C-PSGD) algorithms, simply assuming the application scenario\nwhere only the decentralized network is available. In this paper, we study a\nD-PSGD algorithm and provide the first theoretical analysis that indicates a\nregime in which decentralized algorithms might outperform centralized\nalgorithms for distributed stochastic gradient descent. This is because D-PSGD\nhas comparable total computational complexities to C-PSGD but requires much\nless communication cost on the busiest node. We further conduct an empirical\nstudy to validate our theoretical analysis across multiple frameworks (CNTK and\nTorch), different network configurations, and computation platforms up to 112\nGPUs. On network configurations with low bandwidth or high latency, D-PSGD can\nbe up to one order of magnitude faster than its well-optimized centralized\ncounterparts. \n\n"}
{"id": "1705.09786", "contents": "Title: AMPNet: Asynchronous Model-Parallel Training for Dynamic Neural Networks Abstract: New types of machine learning hardware in development and entering the market\nhold the promise of revolutionizing deep learning in a manner as profound as\nGPUs. However, existing software frameworks and training algorithms for deep\nlearning have yet to evolve to fully leverage the capability of the new wave of\nsilicon. We already see the limitations of existing algorithms for models that\nexploit structured input via complex and instance-dependent control flow, which\nprohibits minibatching. We present an asynchronous model-parallel (AMP)\ntraining algorithm that is specifically motivated by training on networks of\ninterconnected devices. Through an implementation on multi-core CPUs, we show\nthat AMP training converges to the same accuracy as conventional synchronous\ntraining algorithms in a similar number of epochs, but utilizes the available\nhardware more efficiently even for small minibatch sizes, resulting in\nsignificantly shorter overall training times. Our framework opens the door for\nscaling up a new class of deep learning models that cannot be efficiently\ntrained today. \n\n"}
{"id": "1705.09792", "contents": "Title: Deep Complex Networks Abstract: At present, the vast majority of building blocks, techniques, and\narchitectures for deep learning are based on real-valued operations and\nrepresentations. However, recent work on recurrent neural networks and older\nfundamental theoretical analysis suggests that complex numbers could have a\nricher representational capacity and could also facilitate noise-robust memory\nretrieval mechanisms. Despite their attractive properties and potential for\nopening up entirely new neural architectures, complex-valued deep neural\nnetworks have been marginalized due to the absence of the building blocks\nrequired to design such models. In this work, we provide the key atomic\ncomponents for complex-valued deep neural networks and apply them to\nconvolutional feed-forward networks and convolutional LSTMs. More precisely, we\nrely on complex convolutions and present algorithms for complex\nbatch-normalization, complex weight initialization strategies for\ncomplex-valued neural nets and we use them in experiments with end-to-end\ntraining schemes. We demonstrate that such complex-valued models are\ncompetitive with their real-valued counterparts. We test deep complex models on\nseveral computer vision tasks, on music transcription using the MusicNet\ndataset and on Speech Spectrum Prediction using the TIMIT dataset. We achieve\nstate-of-the-art performance on these audio-related tasks. \n\n"}
{"id": "1705.09864", "contents": "Title: BMXNet: An Open-Source Binary Neural Network Implementation Based on\n  MXNet Abstract: Binary Neural Networks (BNNs) can drastically reduce memory size and accesses\nby applying bit-wise operations instead of standard arithmetic operations.\nTherefore it could significantly improve the efficiency and lower the energy\nconsumption at runtime, which enables the application of state-of-the-art deep\nlearning models on low power devices. BMXNet is an open-source BNN library\nbased on MXNet, which supports both XNOR-Networks and Quantized Neural\nNetworks. The developed BNN layers can be seamlessly applied with other\nstandard library components and work in both GPU and CPU mode. BMXNet is\nmaintained and developed by the multimedia research group at Hasso Plattner\nInstitute and released under Apache license. Extensive experiments validate the\nefficiency and effectiveness of our implementation. The BMXNet library, several\nsample projects, and a collection of pre-trained binary deep models are\navailable for download at https://github.com/hpi-xnor \n\n"}
{"id": "1705.10591", "contents": "Title: Optimizing Memory Efficiency for Convolution Kernels on Kepler GPUs Abstract: Convolution is a fundamental operation in many applications, such as computer\nvision, natural language processing, image processing, etc. Recent successes of\nconvolutional neural networks in various deep learning applications put even\nhigher demand on fast convolution. The high computation throughput and memory\nbandwidth of graphics processing units (GPUs) make GPUs a natural choice for\naccelerating convolution operations. However, maximally exploiting the\navailable memory bandwidth of GPUs for convolution is a challenging task. This\npaper introduces a general model to address the mismatch between the memory\nbank width of GPUs and computation data width of threads. Based on this model,\nwe develop two convolution kernels, one for the general case and the other for\na special case with one input channel. By carefully optimizing memory access\npatterns and computation patterns, we design a communication-optimized kernel\nfor the special case and a communication-reduced kernel for the general case.\nExperimental data based on implementations on Kepler GPUs show that our kernels\nachieve 5.16X and 35.5% average performance improvement over the latest cuDNN\nlibrary, for the special case and the general case, respectively. \n\n"}
{"id": "1705.10823", "contents": "Title: Accelerating Neural Architecture Search using Performance Prediction Abstract: Methods for neural network hyperparameter optimization and meta-modeling are\ncomputationally expensive due to the need to train a large number of model\nconfigurations. In this paper, we show that standard frequentist regression\nmodels can predict the final performance of partially trained model\nconfigurations using features based on network architectures, hyperparameters,\nand time-series validation performance data. We empirically show that our\nperformance prediction models are much more effective than prominent Bayesian\ncounterparts, are simpler to implement, and are faster to train. Our models can\npredict final performance in both visual classification and language modeling\ndomains, are effective for predicting performance of drastically varying model\narchitectures, and can even generalize between model classes. Using these\nprediction models, we also propose an early stopping method for hyperparameter\noptimization and meta-modeling, which obtains a speedup of a factor up to 6x in\nboth hyperparameter optimization and meta-modeling. Finally, we empirically\nshow that our early stopping method can be seamlessly incorporated into both\nreinforcement learning-based architecture selection algorithms and bandit based\nsearch methods. Through extensive experimentation, we empirically show our\nperformance prediction models and early stopping algorithm are state-of-the-art\nin terms of prediction accuracy and speedup achieved while still identifying\nthe optimal model configurations. \n\n"}
{"id": "1705.11046", "contents": "Title: Implicit Consensus: Blockchain with Unbounded Throughput Abstract: Recently, the blockchain technique was put in the spotlight as it introduced\na systematic approach for multiple parties to reach consensus without needing\ntrust. However, the application of this technique in practice is severely\nrestricted due to its limitations in throughput. In this paper, we propose a\nnovel consensus model, namely the implicit consensus, with a distinctive\nblockchain-based distributed ledger in which each node holds its individual\nblockchain. In our system, the consensus is not on the transactions, but on a\nspecial type of blocks called Check Points that are used to validate individual\ntransactions. Our system exploits the ideas of self-interest and spontaneous\nsharding and achieves unbounded throughput with the transaction reliability\nthat equivalent to traditional Byzantine fault tolerance schemes. \n\n"}
{"id": "1706.00095", "contents": "Title: Using GPI-2 for Distributed Memory Paralleliziation of the Caffe Toolbox\n  to Speed up Deep Neural Network Training Abstract: Deep Neural Network (DNN) are currently of great inter- est in research and\napplication. The training of these net- works is a compute intensive and time\nconsuming task. To reduce training times to a bearable amount at reasonable\ncost we extend the popular Caffe toolbox for DNN with an efficient distributed\nmemory communication pattern. To achieve good scalability we emphasize the\noverlap of computation and communication and prefer fine granu- lar\nsynchronization patterns over global barriers. To im- plement these\ncommunication patterns we rely on the the Global address space Programming\nInterface version 2 (GPI-2) communication library. This interface provides a\nlight-weight set of asynchronous one-sided communica- tion primitives\nsupplemented by non-blocking fine gran- ular data synchronization mechanisms.\nTherefore, Caf- feGPI is the name of our parallel version of Caffe. First\nbenchmarks demonstrate better scaling behavior com- pared with other\nextensions, e.g., the Intel TM Caffe. Even within a single symmetric\nmultiprocessing machine with four graphics processing units, the CaffeGPI\nscales bet- ter than the standard Caffe toolbox. These first results\ndemonstrate that the use of standard High Performance Computing (HPC) hardware\nis a valid cost saving ap- proach to train large DDNs. I/O is an other\nbottleneck to work with DDNs in a standard parallel HPC setting, which we will\nconsider in more detail in a forthcoming paper. \n\n"}
{"id": "1706.00648", "contents": "Title: Dataflow Matrix Machines as a Model of Computations with Linear Streams Abstract: We overview dataflow matrix machines as a Turing complete generalization of\nrecurrent neural networks and as a programming platform. We describe vector\nspace of finite prefix trees with numerical leaves which allows us to combine\nexpressive power of dataflow matrix machines with simplicity of traditional\nrecurrent neural networks. \n\n"}
{"id": "1706.02677", "contents": "Title: Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour Abstract: Deep learning thrives with large neural networks and large datasets. However,\nlarger networks and larger datasets result in longer training times that impede\nresearch and development progress. Distributed synchronous SGD offers a\npotential solution to this problem by dividing SGD minibatches over a pool of\nparallel workers. Yet to make this scheme efficient, the per-worker workload\nmust be large, which implies nontrivial growth in the SGD minibatch size. In\nthis paper, we empirically show that on the ImageNet dataset large minibatches\ncause optimization difficulties, but when these are addressed the trained\nnetworks exhibit good generalization. Specifically, we show no loss of accuracy\nwhen training with large minibatch sizes up to 8192 images. To achieve this\nresult, we adopt a hyper-parameter-free linear scaling rule for adjusting\nlearning rates as a function of minibatch size and develop a new warmup scheme\nthat overcomes optimization challenges early in training. With these simple\ntechniques, our Caffe2-based system trains ResNet-50 with a minibatch size of\n8192 on 256 GPUs in one hour, while matching small minibatch accuracy. Using\ncommodity hardware, our implementation achieves ~90% scaling efficiency when\nmoving from 8 to 256 GPUs. Our findings enable training visual recognition\nmodels on internet-scale data with high efficiency. \n\n"}
{"id": "1706.02684", "contents": "Title: Learning Local Receptive Fields and their Weight Sharing Scheme on\n  Graphs Abstract: We propose a simple and generic layer formulation that extends the properties\nof convolutional layers to any domain that can be described by a graph. Namely,\nwe use the support of its adjacency matrix to design learnable weight sharing\nfilters able to exploit the underlying structure of signals in the same fashion\nas for images. The proposed formulation makes it possible to learn the weights\nof the filter as well as a scheme that controls how they are shared across the\ngraph. We perform validation experiments with image datasets and show that\nthese filters offer performances comparable with convolutional ones. \n\n"}
{"id": "1706.02850", "contents": "Title: Weakly supervised training of deep convolutional neural networks for\n  overhead pedestrian localization in depth fields Abstract: Overhead depth map measurements capture sufficient amount of information to\nenable human experts to track pedestrians accurately. However, fully automating\nthis process using image analysis algorithms can be challenging. Even though\nhand-crafted image analysis algorithms are successful in many common cases,\nthey fail frequently when there are complex interactions of multiple objects in\nthe image. Many of the assumptions underpinning the hand-crafted solutions do\nnot hold in these cases and the multitude of exceptions are hard to model\nprecisely. Deep Learning (DL) algorithms, on the other hand, do not require\nhand crafted solutions and are the current state-of-the-art in object\nlocalization in images. However, they require exceeding amount of annotations\nto produce successful models. In the case of object localization these\nannotations are difficult and time consuming to produce. In this work we\npresent an approach for developing pedestrian localization models using DL\nalgorithms with efficient weak supervision from an expert. We circumvent the\nneed for annotation of large corpus of data by annotating only small amount of\npatches and relying on synthetic data augmentation as a vehicle for injecting\nexpert knowledge in the model training. This approach of weak supervision\nthrough expert selection of representative patches, suitable transformations\nand synthetic data augmentations enables us to successfully develop DL models\nfor pedestrian localization efficiently. \n\n"}
{"id": "1706.03254", "contents": "Title: On Hash-Based Work Distribution Methods for Parallel Best-First Search Abstract: Parallel best-first search algorithms such as Hash Distributed A* (HDA*)\ndistribute work among the processes using a global hash function. We analyze\nthe search and communication overheads of state-of-the-art hash-based parallel\nbest-first search algorithms, and show that although Zobrist hashing, the\nstandard hash function used by HDA*, achieves good load balance for many\ndomains, it incurs significant communication overhead since almost all\ngenerated nodes are transferred to a different processor than their parents. We\npropose Abstract Zobrist hashing, a new work distribution method for parallel\nsearch which, instead of computing a hash value based on the raw features of a\nstate, uses a feature projection function to generate a set of abstract\nfeatures which results in a higher locality, resulting in reduced\ncommunications overhead. We show that Abstract Zobrist hashing outperforms\nprevious methods on search domains using hand-coded, domain specific feature\nprojection functions. We then propose GRAZHDA*, a graph-partitioning based\napproach to automatically generating feature projection functions. GRAZHDA*\nseeks to approximate the partitioning of the actual search space graph by\npartitioning the domain transition graph, an abstraction of the state space\ngraph. We show that GRAZHDA* outperforms previous methods on domain-independent\nplanning. \n\n"}
{"id": "1706.03968", "contents": "Title: Asynchronous Graph Pattern Matching on Multiprocessor Systems Abstract: Pattern matching on large graphs is the foundation for a variety of\napplication domains. Strict latency requirements and continuously increasing\ngraph sizes demand the usage of highly parallel in-memory graph processing\nengines that need to consider non-uniform memory access (NUMA) and concurrency\nissues to scale up on modern multiprocessor systems. To tackle these aspects,\ngraph partitioning becomes increasingly important. Hence, we present a\ntechnique to process graph pattern matching on NUMA systems in this paper. As a\nscalable pattern matching processing infrastructure, we leverage a\ndata-oriented architecture that preserves data locality and minimizes\nconcurrency-related bottlenecks on NUMA systems. We show in detail, how graph\npattern matching can be asynchronously processed on a multiprocessor system. \n\n"}
{"id": "1706.05297", "contents": "Title: Hypergraphical Clustering Games of Mis-Coordination Abstract: We introduce and motivate the study of hypergraphical clustering games of\nmis-coordination. For two specific variants we prove the existence of a pure\nNash equilibrium and provide bounds on the price of anarchy as a function of\nthe cardinality of the action set and the size of the hyperedges. \n\n"}
{"id": "1707.01428", "contents": "Title: SHADHO: Massively Scalable Hardware-Aware Distributed Hyperparameter\n  Optimization Abstract: Computer vision is experiencing an AI renaissance, in which machine learning\nmodels are expediting important breakthroughs in academic research and\ncommercial applications. Effectively training these models, however, is not\ntrivial due in part to hyperparameters: user-configured values that control a\nmodel's ability to learn from data. Existing hyperparameter optimization\nmethods are highly parallel but make no effort to balance the search across\nheterogeneous hardware or to prioritize searching high-impact spaces. In this\npaper, we introduce a framework for massively Scalable Hardware-Aware\nDistributed Hyperparameter Optimization (SHADHO). Our framework calculates the\nrelative complexity of each search space and monitors performance on the\nlearning task over all trials. These metrics are then used as heuristics to\nassign hyperparameters to distributed workers based on their hardware. We first\ndemonstrate that our framework achieves double the throughput of a standard\ndistributed hyperparameter optimization framework by optimizing SVM for MNIST\nusing 150 distributed workers. We then conduct model search with SHADHO over\nthe course of one week using 74 GPUs across two compute clusters to optimize\nU-Net for a cell segmentation task, discovering 515 models that achieve a lower\nvalidation loss than standard U-Net. \n\n"}
{"id": "1707.02589", "contents": "Title: Exploiting the Tradeoff between Program Accuracy and Soft-error\n  Resiliency Overhead for Machine Learning Workloads Abstract: To protect multicores from soft-error perturbations, resiliency schemes have\nbeen developed with high coverage but high power and performance overheads.\nEmerging safety-critical machine learning applications are increasingly being\ndeployed on these platforms. Moreover, these systems are exposed to harsh\nenvironments, such as unmanned aerial vehicles (UAVs) and self-driving cars.\nDue to the unique structure and computational behavior of such applications,\nresearch has been done on relaxing their accuracy for performance benefits. We\nobserve that not all transient errors affect program correctness, some errors\nonly affect program accuracy, i.e., the program completes with certain\nacceptable deviations from error free outcome. This paper illustrates the idea\nof cross-layer soft-error resilience using machine learning workloads, where\nprogram accuracy is introduced as a tradeoff to deliver resilient yet efficient\nexecution on futuristic large-scale multicores. \n\n"}
{"id": "1707.02647", "contents": "Title: Cappuccino: Efficient Inference Software Synthesis for Mobile\n  System-on-Chips Abstract: Convolutional Neural Networks (CNNs) exhibit remarkable performance in\nvarious machine learning tasks. As sensor-equipped Internet of Things (IoT)\ndevices permeate into every aspect of modern life, the ability to execute CNN\ninference, a computationally intensive application, on resource constrained\ndevices has become increasingly important. In this context, we present\nCappuccino, a framework for synthesis of efficient inference software targeting\nmobile System-on-Chips (SoCs). We propose techniques for efficient\nparallelization of CNN inference targeting mobile SoCs, and explore the\nunderlying tradeoffs. Experiments with different CNNs on three mobile devices\ndemonstrate the effectiveness of our approach. \n\n"}
{"id": "1707.03093", "contents": "Title: Gray-box optimization and factorized distribution algorithms: where two\n  worlds collide Abstract: The concept of gray-box optimization, in juxtaposition to black-box\noptimization, revolves about the idea of exploiting the problem structure to\nimplement more efficient evolutionary algorithms (EAs). Work on factorized\ndistribution algorithms (FDAs), whose factorizations are directly derived from\nthe problem structure, has also contributed to show how exploiting the problem\nstructure produces important gains in the efficiency of EAs. In this paper we\nanalyze the general question of using problem structure in EAs focusing on\nconfronting work done in gray-box optimization with related research\naccomplished in FDAs. This contrasted analysis helps us to identify, in current\nstudies on the use problem structure in EAs, two distinct analytical\ncharacterizations of how these algorithms work. Moreover, we claim that these\ntwo characterizations collide and compete at the time of providing a coherent\nframework to investigate this type of algorithms. To illustrate this claim, we\npresent a contrasted analysis of formalisms, questions, and results produced in\nFDAs and gray-box optimization. Common underlying principles in the two\napproaches, which are usually overlooked, are identified and discussed.\nBesides, an extensive review of previous research related to different uses of\nthe problem structure in EAs is presented. The paper also elaborates on some of\nthe questions that arise when extending the use of problem structure in EAs,\nsuch as the question of evolvability, high cardinality of the variables and\nlarge definition sets, constrained and multi-objective problems, etc. Finally,\nemergent approaches that exploit neural models to capture the problem structure\nare covered. \n\n"}
{"id": "1707.03198", "contents": "Title: The swiss army knife of job submission tools: grid-control Abstract: Grid-control is a lightweight and highly portable open source submission tool\nthat supports virtually all workflows in high energy physics (HEP). Since 2007\nit has been used by a sizeable number of HEP analyses to process tasks that\nsometimes consist of up 100k jobs. grid-control is built around a powerful\nplugin and configuration system, that allows users to easily specify all\naspects of the desired workflow. Job submission to a wide range of local or\nremote batch systems or grid middleware is supported. Tasks can be conveniently\nspecified through the parameter space that will be processed, which can consist\nof any number of variables and data sources with complex dependencies on each\nother. Dataset information is processed through a configurable pipeline of\ndataset filters, partition plugins and partition filters. The partition plugins\ncan take the number of files, size of the work units, metadata or combinations\nthereof into account. All changes to the input datasets or variables are\npropagated through the processing pipeline and can transparently trigger\nadjustments to the parameter space and the job submission. While the core\nfunctionality is completely experiment independent, integration with the CMS\ncomputing environment is provided by a small set of plugins. \n\n"}
{"id": "1707.04035", "contents": "Title: Kafnets: kernel-based non-parametric activation functions for neural\n  networks Abstract: Neural networks are generally built by interleaving (adaptable) linear layers\nwith (fixed) nonlinear activation functions. To increase their flexibility,\nseveral authors have proposed methods for adapting the activation functions\nthemselves, endowing them with varying degrees of flexibility. None of these\napproaches, however, have gained wide acceptance in practice, and research in\nthis topic remains open. In this paper, we introduce a novel family of flexible\nactivation functions that are based on an inexpensive kernel expansion at every\nneuron. Leveraging over several properties of kernel-based models, we propose\nmultiple variations for designing and initializing these kernel activation\nfunctions (KAFs), including a multidimensional scheme allowing to nonlinearly\ncombine information from different paths in the network. The resulting KAFs can\napproximate any mapping defined over a subset of the real line, either convex\nor nonconvex. Furthermore, they are smooth over their entire domain, linear in\ntheir parameters, and they can be regularized using any known scheme, including\nthe use of $\\ell_1$ penalties to enforce sparseness. To the best of our\nknowledge, no other known model satisfies all these properties simultaneously.\nIn addition, we provide a relatively complete overview on alternative\ntechniques for adapting the activation functions, which is currently lacking in\nthe literature. A large set of experiments validates our proposal. \n\n"}
{"id": "1707.05005", "contents": "Title: graph2vec: Learning Distributed Representations of Graphs Abstract: Recent works on representation learning for graph structured data\npredominantly focus on learning distributed representations of graph\nsubstructures such as nodes and subgraphs. However, many graph analytics tasks\nsuch as graph classification and clustering require representing entire graphs\nas fixed length feature vectors. While the aforementioned approaches are\nnaturally unequipped to learn such representations, graph kernels remain as the\nmost effective way of obtaining them. However, these graph kernels use\nhandcrafted features (e.g., shortest paths, graphlets, etc.) and hence are\nhampered by problems such as poor generalization. To address this limitation,\nin this work, we propose a neural embedding framework named graph2vec to learn\ndata-driven distributed representations of arbitrary sized graphs. graph2vec's\nembeddings are learnt in an unsupervised manner and are task agnostic. Hence,\nthey could be used for any downstream task such as graph classification,\nclustering and even seeding supervised representation learning approaches. Our\nexperiments on several benchmark and large real-world datasets show that\ngraph2vec achieves significant improvements in classification and clustering\naccuracies over substructure representation learning approaches and are\ncompetitive with state-of-the-art graph kernels. \n\n"}
{"id": "1707.05867", "contents": "Title: Reconciling Graphs and Sets of Sets Abstract: We explore a generalization of set reconciliation, where the goal is to\nreconcile sets of sets. Alice and Bob each have a parent set consisting of $s$\nchild sets, each containing at most $h$ elements from a universe of size $u$.\nThey want to reconcile their sets of sets in a scenario where the total number\nof differences between all of their child sets (under the minimum difference\nmatching between their child sets) is $d$. We give several algorithms for this\nproblem, and discuss applications to reconciliation problems on graphs,\ndatabases, and collections of documents. We specifically focus on graph\nreconciliation, providing protocols based on set of sets reconciliation for\nrandom graphs from $G(n,p)$ and for forests of rooted trees. \n\n"}
{"id": "1707.07659", "contents": "Title: An Improved Approximate Consensus Algorithm in the Presence of Mobile\n  Faults Abstract: This paper explores the problem of reaching approximate consensus in\nsynchronous point-to-point networks, where each pair of nodes is able to\ncommunicate with each other directly and reliably. We consider the mobile\nByzantine fault model proposed by Garay '94 -- in the model, an omniscient\nadversary can corrupt up to $f$ nodes in each round, and at the beginning of\neach round, faults may \"move\" in the system (i.e., different sets of nodes may\nbecome faulty in different rounds). Recent work by Bonomi et al. '16 proposed a\nsimple iterative approximate consensus algorithm which requires at least $4f+1$\nnodes. This paper proposes a novel technique of using \"confession\" (a mechanism\nto allow others to ignore past behavior) and a variant of reliable broadcast to\nimprove the fault-tolerance level. In particular, we present an approximate\nconsensus algorithm that requires only $\\lceil 7f/2\\rceil + 1$ nodes, an\n$\\lfloor f/2 \\rfloor$ improvement over the state-of-the-art algorithms.\nMoreover, we also show that the proposed algorithm is optimal within a family\nof round-based algorithms. \n\n"}
{"id": "1707.08496", "contents": "Title: Fast Distributed Approximation for Max-Cut Abstract: Finding a maximum cut is a fundamental task in many computational settings.\nSurprisingly, it has been insufficiently studied in the classic distributed\nsettings, where vertices communicate by synchronously sending messages to their\nneighbors according to the underlying graph, known as the $\\mathcal{LOCAL}$ or\n$\\mathcal{CONGEST}$ models. We amend this by obtaining almost optimal\nalgorithms for Max-Cut on a wide class of graphs in these models. In\nparticular, for any $\\epsilon > 0$, we develop randomized approximation\nalgorithms achieving a ratio of $(1-\\epsilon)$ to the optimum for Max-Cut on\nbipartite graphs in the $\\mathcal{CONGEST}$ model, and on general graphs in the\n$\\mathcal{LOCAL}$ model.\n  We further present efficient deterministic algorithms, including a\n$1/3$-approximation for Max-Dicut in our models, thus improving the best known\n(randomized) ratio of $1/4$. Our algorithms make non-trivial use of the greedy\napproach of Buchbinder et al. (SIAM Journal on Computing, 2015) for maximizing\nan unconstrained (non-monotone) submodular function, which may be of\nindependent interest. \n\n"}
{"id": "1707.09068", "contents": "Title: Tartan: Accelerating Fully-Connected and Convolutional Layers in Deep\n  Learning Networks by Exploiting Numerical Precision Variability Abstract: Tartan (TRT), a hardware accelerator for inference with Deep Neural Networks\n(DNNs), is presented and evaluated on Convolutional Neural Networks. TRT\nexploits the variable per layer precision requirements of DNNs to deliver\nexecution time that is proportional to the precision p in bits used per layer\nfor convolutional and fully-connected layers. Prior art has demonstrated an\naccelerator with the same execution performance only for convolutional layers.\nExperiments on image classification CNNs show that on average across all\nnetworks studied, TRT outperforms a state-of-the-art bit-parallel accelerator\nby 1:90x without any loss in accuracy while it is 1:17x more energy efficient.\nTRT requires no network retraining while it enables trading off accuracy for\nadditional improvements in execution performance and energy efficiency. For\nexample, if a 1% relative loss in accuracy is acceptable, TRT is on average\n2:04x faster and 1:25x more energy efficient than a conventional bit-parallel\naccelerator. A Tartan configuration that processes 2-bits at time, requires\nless area than the 1-bit configuration, improves efficiency to 1:24x over the\nbit-parallel baseline while being 73% faster for convolutional layers and 60%\nfaster for fully-connected layers is also presented. \n\n"}
{"id": "1707.09317", "contents": "Title: A Minimum-Cost Flow Model for Workload Optimization on Cloud\n  Infrastructure Abstract: Recent technology advancements in the areas of compute, storage and\nnetworking, along with the increased demand for organizations to cut costs\nwhile remaining responsive to increasing service demands have led to the growth\nin the adoption of cloud computing services. Cloud services provide the promise\nof improved agility, resiliency, scalability and a lowered Total Cost of\nOwnership (TCO). This research introduces a framework for minimizing cost and\nmaximizing resource utilization by using an Integer Linear Programming (ILP)\napproach to optimize the assignment of workloads to servers on Amazon Web\nServices (AWS) cloud infrastructure. The model is based on the classical\nminimum-cost flow model, known as the assignment model. \n\n"}
{"id": "1707.09668", "contents": "Title: Handling Nested Parallelism and Extreme Load Imbalance in an Orbital\n  Analysis Code Abstract: Nested parallelism exists in scientific codes that are searching\nmulti-dimensional spaces. However, implementations of nested parallelism often\nhave overhead and load balance issues. The Orbital Analysis code we present\nexhibits a sparse search space, significant load imbalances, and stopping when\nthe first solution is reached. All these aspects of the algorithm exacerbate\nthe problem of using nested parallelism effectively. In this paper, we present\nan inspector/executor strategy for chunking such computations into parallel\nwavefronts. The presented shared memory parallelization is no longer nested and\nexhibits significantly less load imbalance. We evaluate this approach on an\nOrbital analysis code, and we improve the execution time from the original\nimplementation by an order of magnitude. As part of a Graduate Computer Science\ncourse in Parallel Programming models, we show how the approach can be\nimplemented in parallel Perl, Python, Chapel, Pthreads, and OpenMP. Future work\nincludes investigating how to automate and generalize the parallelization\napproach. \n\n"}
{"id": "1708.00111", "contents": "Title: A Continuous Relaxation of Beam Search for End-to-end Training of Neural\n  Sequence Models Abstract: Beam search is a desirable choice of test-time decoding algorithm for neural\nsequence models because it potentially avoids search errors made by simpler\ngreedy methods. However, typical cross entropy training procedures for these\nmodels do not directly consider the behaviour of the final decoding method. As\na result, for cross-entropy trained models, beam decoding can sometimes yield\nreduced test performance when compared with greedy decoding. In order to train\nmodels that can more effectively make use of beam search, we propose a new\ntraining procedure that focuses on the final loss metric (e.g. Hamming loss)\nevaluated on the output of beam search. While well-defined, this \"direct loss\"\nobjective is itself discontinuous and thus difficult to optimize. Hence, in our\napproach, we form a sub-differentiable surrogate objective by introducing a\nnovel continuous approximation of the beam search decoding procedure. In\nexperiments, we show that optimizing this new training objective yields\nsubstantially better results on two sequence tasks (Named Entity Recognition\nand CCG Supertagging) when compared with both cross entropy trained greedy\ndecoding and cross entropy trained beam decoding baselines. \n\n"}
{"id": "1708.00117", "contents": "Title: Compiling Deep Learning Models for Custom Hardware Accelerators Abstract: Convolutional neural networks (CNNs) are the core of most state-of-the-art\ndeep learning algorithms specialized for object detection and classification.\nCNNs are both computationally complex and embarrassingly parallel. Two\nproperties that leave room for potential software and hardware optimizations\nfor embedded systems. Given a programmable hardware accelerator with a CNN\noriented custom instructions set, the compiler's task is to exploit the\nhardware's full potential, while abiding with the hardware constraints and\nmaintaining generality to run different CNN models with varying workload\nproperties. Snowflake is an efficient and scalable hardware accelerator\nimplemented on programmable logic devices. It implements a control pipeline for\na custom instruction set. The goal of this paper is to present Snowflake's\ncompiler that generates machine level instructions from Torch7 model\ndescription files. The main software design points explored in this work are:\nmodel structure parsing, CNN workload breakdown, loop rearrangement for memory\nbandwidth optimizations and memory access balancing. The performance achieved\nby compiler generated instructions matches against hand optimized code for\nconvolution layers. Generated instructions also efficiently execute AlexNet and\nResNet18 inference on Snowflake. Snowflake with $256$ processing units was\nsynthesized on Xilinx's Zynq XC7Z045 FPGA. At $250$ MHz, AlexNet achieved in\n$93.6$ frames/s and $1.2$ GB/s of off-chip memory bandwidth, and $21.4$\nframes/s and $2.2$ GB/s for ResNet18. Total on-chip power is $5$ W. \n\n"}
{"id": "1708.01012", "contents": "Title: On the convergence properties of a $K$-step averaging stochastic\n  gradient descent algorithm for nonconvex optimization Abstract: Despite their popularity, the practical performance of asynchronous\nstochastic gradient descent methods (ASGD) for solving large scale machine\nlearning problems are not as good as theoretical results indicate. We adopt and\nanalyze a synchronous K-step averaging stochastic gradient descent algorithm\nwhich we call K-AVG. We establish the convergence results of K-AVG for\nnonconvex objectives and explain why the K-step delay is necessary and leads to\nbetter performance than traditional parallel stochastic gradient descent which\nis a special case of K-AVG with $K=1$. We also show that K-AVG scales better\nthan ASGD. Another advantage of K-AVG over ASGD is that it allows larger\nstepsizes. On a cluster of $128$ GPUs, K-AVG is faster than ASGD\nimplementations and achieves better accuracies and faster convergence for\n\\cifar dataset. \n\n"}
{"id": "1708.01135", "contents": "Title: Long range forces in a performance portable Molecular Dynamics framework Abstract: Molecular Dynamics (MD) codes predict the fundamental properties of matter by\nfollowing the trajectories of a collection of interacting model particles. To\nexploit diverse modern manycore hardware, efficient codes must use all\navailable parallelism. At the same time they need to be portable and easily\nextendible by the domain specialist (physicist/chemist) without detailed\nknowledge of this hardware. To address this challenge, we recently described a\nnew Domain Specific Language (DSL) for the development of performance portable\nMD codes based on a \"Separation of Concerns\": a Python framework automatically\ngenerates efficient parallel code for a range of target architectures.\n  Electrostatic interactions between charged particles are important in many\nphysical systems and often dominate the runtime. Here we discuss the inclusion\nof long-range interaction algorithms in our code generation framework. These\nalgorithms require global communications and careful consideration has to be\ngiven to any impact on parallel scalability. We implemented an Ewald summation\nalgorithm for electrostatic forces, present scaling comparisons for different\nsystem sizes and compare to the performance of existing codes. We also report\non further performance optimisations delivered with OpenMP shared memory\nparallelism. \n\n"}
{"id": "1708.01285", "contents": "Title: Proof of Work Without All the Work: Computationally Efficient\n  Attack-Resistant Systems Abstract: Proof-of-work (PoW) is an algorithmic tool used to secure networks by\nimposing a computational cost on participating devices. Unfortunately,\ntraditional PoW schemes require that correct devices perform computational work\nperpetually, even when the system is not under attack.\n  We address this issue by designing a general PoW protocol that ensures two\nproperties. First, the network stays secure. In particular, the fraction of\nidentities in the system that are controlled by an attacker is always less than\n1/2. Second, our protocol's computational cost is commensurate with the cost of\nan attacker. In particular, the total computational cost of correct devices is\na linear function of the attacker's computational cost plus the number of\ncorrect devices that have joined the system. Consequently, if the network is\nattacked, we ensure security with cost that grows linearly with the attacker's\ncost; and, in the absence of attack, our computational cost remains small. We\nprove similar guarantees for bandwidth cost.\n  Our results hold in a dynamic, decentralized system where participants join\nand depart over time, and where the total computational power of the attacker\nis up to a constant fraction of the total computational power of correct\ndevices. We demonstrate how to leverage our results to address important\nsecurity problems in distributed computing including: Sybil attacks, Byzantine\nconsensus, and Committee election. \n\n"}
{"id": "1708.02645", "contents": "Title: Embracing a new era of highly efficient and productive quantum Monte\n  Carlo simulations Abstract: QMCPACK has enabled cutting-edge materials research on supercomputers for\nover a decade. It scales nearly ideally but has low single-node efficiency due\nto the physics-based abstractions using array-of-structures objects, causing\ninefficient vectorization. We present a systematic approach to transform\nQMCPACK to better exploit the new hardware features of modern CPUs in portable\nand maintainable ways. We develop miniapps for fast prototyping and\noptimizations. We implement new containers in structure-of-arrays data layout\nto facilitate vectorizations by the compilers. Further speedup and smaller\nmemory-footprints are obtained by computing data on the fly with the vectorized\nroutines and expanding single-precision use. All these are seamlessly\nincorporated in production QMCPACK. We demonstrate upto 4.5x speedups on recent\nIntel processors and IBM Blue Gene/Q for representative workloads. Energy\nconsumption is reduced significantly commensurate to the speedup factor.\nMemory-footprints are reduced by up-to 3.8x, opening the possibility to solve\nmuch larger problems of future. \n\n"}
{"id": "1708.02735", "contents": "Title: Gaussian Prototypical Networks for Few-Shot Learning on Omniglot Abstract: We propose a novel architecture for $k$-shot classification on the Omniglot\ndataset. Building on prototypical networks, we extend their architecture to\nwhat we call Gaussian prototypical networks. Prototypical networks learn a map\nbetween images and embedding vectors, and use their clustering for\nclassification. In our model, a part of the encoder output is interpreted as a\nconfidence region estimate about the embedding point, and expressed as a\nGaussian covariance matrix. Our network then constructs a direction and class\ndependent distance metric on the embedding space, using uncertainties of\nindividual data points as weights. We show that Gaussian prototypical networks\nare a preferred architecture over vanilla prototypical networks with an\nequivalent number of parameters. We report state-of-the-art performance in\n1-shot and 5-shot classification both in 5-way and 20-way regime (for 5-shot\n5-way, we are comparable to previous state-of-the-art) on the Omniglot dataset.\nWe explore artificially down-sampling a fraction of images in the training set,\nwhich improves our performance even further. We therefore hypothesize that\nGaussian prototypical networks might perform better in less homogeneous,\nnoisier datasets, which are commonplace in real world applications. \n\n"}
{"id": "1708.02825", "contents": "Title: Mutual Visibility by Robots with Persistent Memory Abstract: This paper addresses the mutual visibility problem for a set of\nsemi-synchronous, opaque robots occupying distinct positions in the Euclidean\nplane. Since robots are opaque, if three robots lie on a line, the middle robot\nobstructs the visions of the two other robots. The mutual visibility problem\nasks the robots to coordinate their movements to form a configuration, within\nfinite time and without collision, in which no three robots are collinear.\nRobots are endowed with a constant bits of persistent memory. In this work, we\nconsider the FSTATE computational model in which the persistent memory is used\nby the robots only to remember their previous internal states. Except from this\npersistent memory, robots are oblivious i.e., they do not carry forward any\nother information from their previous computational cycles. The paper presents\na distributed algorithm to solve the mutual visibility problem for a set of\nsemi-synchronous robots using only 1 bit of persistent memory. The proposed\nalgorithm does not impose any other restriction on the capability of the robots\nand guarantees collision-free movements for the robots. \n\n"}
{"id": "1708.03264", "contents": "Title: Contextuality from missing and versioned data Abstract: Traditionally categorical data analysis (e.g. generalized linear models)\nworks with simple, flat datasets akin to a single table in a database with no\nnotion of missing data or conflicting versions. In contrast, modern data\nanalysis must deal with distributed databases with many partial local tables\nthat need not always agree. The computational agents tabulating these tables\nare spatially separated, with binding speed-of-light constraints and data\narriving too rapidly for these distributed views ever to be fully informed and\nglobally consistent. Contextuality is a mathematical property which describes a\nkind of inconsistency arising in quantum mechanics (e.g. in Bell's theorem). In\nthis paper we show how contextuality can arise in common data collection\nscenarios, including missing data and versioning (as in low-latency distributed\ndatabases employing snapshot isolation). In the companion paper, we develop\nstatistical models adapted to this regime. \n\n"}
{"id": "1708.03792", "contents": "Title: Evacuating Two Robots from Two Unknown Exits on the Perimeter of a Disk Abstract: Distributed evacuation of mobile robots is a recent development. We consider\nthe evacuation problem of two robots which are initially located at the center\nof a unit disk. Both the robots have to evacuate the disk through the exits\nsituated on the perimeter of the disk at an unknown location. The distance\nbetween two exits along the perimeter $d$ is given. We consider two different\ncommunication models. First, in the wireless model, the robots can send a\nmessage to each other over a long distance. Second, in face-to-face\ncommunication model, the robots can exchange information with each other only\nwhen they touch each other. The objective of the evacuation problem is to\ndesign an algorithm which minimizes the evacuation time of both the robots. For\nthe wireless communication model, we propose a generic algorithm for two robots\nmoving to two points on the perimeter with an initial separation of $\\zeta \\leq\nd$. We also investigate evacuation problem for both unlabeled and labeled exits\nin the wireless communication model. For the face-to-face communication model,\nwe propose two different algorithms for $\\zeta =0$ and $\\zeta =d$ for unlabeled\nexits. We also propose a generic algorithm for $\\zeta \\leq d$ for labeled\nexits. We provide lower bounds corresponding to different $d$ values in the\nface-to-face communication model. We evaluate the performance our algorithms\nwith simulation for both of the communication models. \n\n"}
{"id": "1708.03888", "contents": "Title: Large Batch Training of Convolutional Networks Abstract: A common way to speed up training of large convolutional networks is to add\ncomputational units. Training is then performed using data-parallel synchronous\nStochastic Gradient Descent (SGD) with mini-batch divided between computational\nunits. With an increase in the number of nodes, the batch size grows. But\ntraining with large batch size often results in the lower model accuracy. We\nargue that the current recipe for large batch training (linear learning rate\nscaling with warm-up) is not general enough and training may diverge. To\novercome this optimization difficulties we propose a new training algorithm\nbased on Layer-wise Adaptive Rate Scaling (LARS). Using LARS, we scaled Alexnet\nup to a batch size of 8K, and Resnet-50 to a batch size of 32K without loss in\naccuracy. \n\n"}
{"id": "1708.06127", "contents": "Title: Practical Minimum Cut Algorithms Abstract: The minimum cut problem for an undirected edge-weighted graph asks us to\ndivide its set of nodes into two blocks while minimizing the weight sum of the\ncut edges. Here, we introduce a linear-time algorithm to compute near-minimum\ncuts. Our algorithm is based on cluster contraction using label propagation and\nPadberg and Rinaldi's contraction heuristics [SIAM Review, 1991]. We give both\nsequential and shared-memory parallel implementations of our algorithm.\nExtensive experiments on both real-world and generated instances show that our\nalgorithm finds the optimal cut on nearly all instances significantly faster\nthan other state-of-the-art algorithms while our error rate is lower than that\nof other heuristic algorithms. In addition, our parallel algorithm shows good\nscalability. \n\n"}
{"id": "1708.07120", "contents": "Title: Super-Convergence: Very Fast Training of Neural Networks Using Large\n  Learning Rates Abstract: In this paper, we describe a phenomenon, which we named \"super-convergence\",\nwhere neural networks can be trained an order of magnitude faster than with\nstandard training methods. The existence of super-convergence is relevant to\nunderstanding why deep networks generalize well. One of the key elements of\nsuper-convergence is training with one learning rate cycle and a large maximum\nlearning rate. A primary insight that allows super-convergence training is that\nlarge learning rates regularize the training, hence requiring a reduction of\nall other forms of regularization in order to preserve an optimal\nregularization balance. We also derive a simplification of the Hessian Free\noptimization method to compute an estimate of the optimal learning rate.\nExperiments demonstrate super-convergence for Cifar-10/100, MNIST and Imagenet\ndatasets, and resnet, wide-resnet, densenet, and inception architectures. In\naddition, we show that super-convergence provides a greater boost in\nperformance relative to standard training when the amount of labeled training\ndata is limited. The architectures and code to replicate the figures in this\npaper are available at github.com/lnsmith54/super-convergence. See\nhttp://www.fast.ai/2018/04/30/dawnbench-fastai/ for an application of\nsuper-convergence to win the DAWNBench challenge (see\nhttps://dawn.cs.stanford.edu/benchmark/). \n\n"}
{"id": "1708.07233", "contents": "Title: Reliability and Fault-Tolerance by Choreographic Design Abstract: Distributed programs are hard to get right because they are required to be\nopen, scalable, long-running, and tolerant to faults. In particular, the recent\napproaches to distributed software based on (micro-)services where different\nservices are developed independently by disparate teams exacerbate the problem.\nIn fact, services are meant to be composed together and run in open context\nwhere unpredictable behaviours can emerge. This makes it necessary to adopt\nsuitable strategies for monitoring the execution and incorporate recovery and\nadaptation mechanisms so to make distributed programs more flexible and robust.\nThe typical approach that is currently adopted is to embed such mechanisms in\nthe program logic, which makes it hard to extract, compare and debug. We\npropose an approach that employs formal abstractions for specifying failure\nrecovery and adaptation strategies. Although implementation agnostic, these\nabstractions would be amenable to algorithmic synthesis of code, monitoring and\ntests. We consider message-passing programs (a la Erlang, Go, or MPI) that are\ngaining momentum both in academia and industry. Our research agenda consists of\n(1) the definition of formal behavioural models encompassing failures, (2) the\nspecification of the relevant properties of adaptation and recovery strategy,\n(3) the automatic generation of monitoring, recovery, and adaptation logic in\ntarget languages of interest. \n\n"}
{"id": "1708.08296", "contents": "Title: Explainable Artificial Intelligence: Understanding, Visualizing and\n  Interpreting Deep Learning Models Abstract: With the availability of large databases and recent improvements in deep\nlearning methodology, the performance of AI systems is reaching or even\nexceeding the human level on an increasing number of complex tasks. Impressive\nexamples of this development can be found in domains such as image\nclassification, sentiment analysis, speech understanding or strategic game\nplaying. However, because of their nested non-linear structure, these highly\nsuccessful machine learning and artificial intelligence models are usually\napplied in a black box manner, i.e., no information is provided about what\nexactly makes them arrive at their predictions. Since this lack of transparency\ncan be a major drawback, e.g., in medical applications, the development of\nmethods for visualizing, explaining and interpreting deep learning models has\nrecently attracted increasing attention. This paper summarizes recent\ndevelopments in this field and makes a plea for more interpretability in\nartificial intelligence. Furthermore, it presents two approaches to explaining\npredictions of deep learning models, one method which computes the sensitivity\nof the prediction with respect to changes in the input and one approach which\nmeaningfully decomposes the decision in terms of the input variables. These\nmethods are evaluated on three classification tasks. \n\n"}
{"id": "1708.09419", "contents": "Title: Proposal for a fully decentralized blockchain and proof-of-work\n  algorithm for solving NP-complete problems Abstract: We propose a proof-of-work algorithm that rewards blockchain miners for using\ncomputational resources to solve NP-complete puzzles. The resulting blockchain\nwill publicly store and improve solutions to problems with real world\napplications while maintaining a secure and fully functional transaction\nledger. \n\n"}
{"id": "1709.01440", "contents": "Title: Locality-Aware Hybrid Coded MapReduce for Server-Rack Architecture Abstract: MapReduce is a widely used framework for distributed computing. Data\nshuffling between the Map phase and Reduce phase of a job involves a large\namount of data transfer across servers, which in turn accounts for increase in\njob completion time. Recently, Coded MapReduce has been proposed to offer\nsavings with respect to the communication cost incurred in data shuffling. This\nis achieved by creating coded multicast opportunities for shuffling through\nrepeating Map tasks at multiple servers. We consider a server-rack architecture\nfor MapReduce and in this architecture, propose to divide the total\ncommunication cost into two: intra-rack communication cost and cross-rack\ncommunication cost. Having noted that cross-rack data transfer operates at\nlower speed as compared to intra-rack data transfer, we present a scheme termed\nas Hybrid Coded MapReduce which results in lower cross-rack communication than\nCoded MapReduce at the cost of increase in intra-rack communication. In\naddition, we pose the problem of assigning Map tasks to servers to maximize\ndata locality in the framework of Hybrid Coded MapReduce as a constrained\ninteger optimization problem. We show through simulations that data locality\ncan be improved considerably by using the solution of optimization to assign\nMap tasks to servers. \n\n"}
{"id": "1709.02091", "contents": "Title: Asynchronous COMID: the theoretic basis for transmitted data\n  sparsification tricks on Parameter Server Abstract: Asynchronous FTRL-proximal and L2 norm done at server are two widely used\ntricks in Parameters Server which is an implement of delayed SGD. Their\ncommonness is leaving parts of updating computation on server which reduces the\nburden of network via making transmitted data sparse. But above tricks'\nconvergences are not well-proved. In this paper, based on above commonness, we\npropose a more general algorithm named as asynchronous COMID and prove its\nconvergence. We prove that asynchronous FTRL-proximal and L2 norm done at\nserver are applications of asynchronous COMID, which demonstrates the\nconvergences of above two tricks. Then, we conduct experiments to verify\ntheoretical results. Experimental results show that compared with delayed SGD\non Parameters Server, asynchronous COMID reduces the burden of the network\nwithout any harm on the mathematical convergence speed and final output. \n\n"}
{"id": "1709.02125", "contents": "Title: Beyond 16GB: Out-of-Core Stencil Computations Abstract: Stencil computations are a key class of applications, widely used in the\nscientific computing community, and a class that has particularly benefited\nfrom performance improvements on architectures with high memory bandwidth.\nUnfortunately, such architectures come with a limited amount of fast memory,\nwhich is limiting the size of the problems that can be efficiently solved. In\nthis paper, we address this challenge by applying the well-known cache-blocking\ntiling technique to large scale stencil codes implemented using the OPS domain\nspecific language, such as CloverLeaf 2D, CloverLeaf 3D, and OpenSBLI. We\nintroduce a number of techniques and optimisations to help manage data resident\nin fast memory, and minimise data movement. Evaluating our work on Intel's\nKnights Landing Platform as well as NVIDIA P100 GPUs, we demonstrate that it is\npossible to solve 3 times larger problems than the on-chip memory size with at\nmost 15\\% loss in efficiency \n\n"}
{"id": "1709.04482", "contents": "Title: Analyzing Hidden Representations in End-to-End Automatic Speech\n  Recognition Systems Abstract: Neural models have become ubiquitous in automatic speech recognition systems.\nWhile neural networks are typically used as acoustic models in more complex\nsystems, recent studies have explored end-to-end speech recognition systems\nbased on neural networks, which can be trained to directly predict text from\ninput acoustic features. Although such systems are conceptually elegant and\nsimpler than traditional systems, it is less obvious how to interpret the\ntrained models. In this work, we analyze the speech representations learned by\na deep end-to-end model that is based on convolutional and recurrent layers,\nand trained with a connectionist temporal classification (CTC) loss. We use a\npre-trained model to generate frame-level features which are given to a\nclassifier that is trained on frame classification into phones. We evaluate\nrepresentations from different layers of the deep model and compare their\nquality for predicting phone labels. Our experiments shed light on important\naspects of the end-to-end model such as layer depth, model complexity, and\nother design choices. \n\n"}
{"id": "1709.04599", "contents": "Title: Simple Round Compression for Parallel Vertex Cover Abstract: Recently, Czumaj et.al. (arXiv 2017) presented a parallel (almost)\n$2$-approximation algorithm for the maximum matching problem in only\n$O({(\\log\\log{n})^2})$ rounds of the massive parallel computation (MPC)\nframework, when the memory per machine is $O(n)$. The main approach in their\nwork is a way of compressing $O(\\log{n})$ rounds of a distributed algorithm for\nmaximum matching into only $O({(\\log\\log{n})^2})$ MPC rounds.\n  In this note, we present a similar algorithm for the closely related problem\nof approximating the minimum vertex cover in the MPC framework. We show that\none can achieve an $O(\\log{n})$ approximation to minimum vertex cover in only\n$O(\\log\\log{n})$ MPC rounds when the memory per machine is $O(n)$. Our\nalgorithm for vertex cover is similar to the maximum matching algorithm of\nCzumaj et.al. but avoids many of the intricacies in their approach and as a\nresult admits a considerably simpler analysis (at a cost of a worse\napproximation guarantee). We obtain this result by modifying a previous\nparallel algorithm by Khanna and the author (SPAA 2017) for vertex cover that\nallowed for compressing $O(\\log{n})$ rounds of a distributed algorithm into\nconstant MPC rounds when the memory allowed per machine is $O(n\\sqrt{n})$. \n\n"}
{"id": "1709.05011", "contents": "Title: ImageNet Training in Minutes Abstract: Finishing 90-epoch ImageNet-1k training with ResNet-50 on a NVIDIA M40 GPU\ntakes 14 days. This training requires 10^18 single precision operations in\ntotal. On the other hand, the world's current fastest supercomputer can finish\n2 * 10^17 single precision operations per second (Dongarra et al 2017,\nhttps://www.top500.org/lists/2017/06/). If we can make full use of the\nsupercomputer for DNN training, we should be able to finish the 90-epoch\nResNet-50 training in one minute. However, the current bottleneck for fast DNN\ntraining is in the algorithm level. Specifically, the current batch size (e.g.\n512) is too small to make efficient use of many processors. For large-scale DNN\ntraining, we focus on using large-batch data-parallelism synchronous SGD\nwithout losing accuracy in the fixed epochs. The LARS algorithm (You, Gitman,\nGinsburg, 2017, arXiv:1708.03888) enables us to scale the batch size to\nextremely large case (e.g. 32K). We finish the 100-epoch ImageNet training with\nAlexNet in 11 minutes on 1024 CPUs. About three times faster than Facebook's\nresult (Goyal et al 2017, arXiv:1706.02677), we finish the 90-epoch ImageNet\ntraining with ResNet-50 in 20 minutes on 2048 KNLs without losing accuracy.\nState-of-the-art ImageNet training speed with ResNet-50 is 74.9% top-1 test\naccuracy in 15 minutes. We got 74.9% top-1 test accuracy in 64 epochs, which\nonly needs 14 minutes. Furthermore, when we increase the batch size to above\n16K, our accuracy is much higher than Facebook's on corresponding batch sizes.\nOur source code is available upon request. \n\n"}
{"id": "1709.05703", "contents": "Title: AI Programmer: Autonomously Creating Software Programs Using Genetic\n  Algorithms Abstract: In this paper, we present the first-of-its-kind machine learning (ML) system,\ncalled AI Programmer, that can automatically generate full software programs\nrequiring only minimal human guidance. At its core, AI Programmer uses genetic\nalgorithms (GA) coupled with a tightly constrained programming language that\nminimizes the overhead of its ML search space. Part of AI Programmer's novelty\nstems from (i) its unique system design, including an embedded, hand-crafted\ninterpreter for efficiency and security and (ii) its augmentation of GAs to\ninclude instruction-gene randomization bindings and programming\nlanguage-specific genome construction and elimination techniques. We provide a\ndetailed examination of AI Programmer's system design, several examples\ndetailing how the system works, and experimental data demonstrating its\nsoftware generation capabilities and performance using only mainstream CPUs. \n\n"}
{"id": "1709.05943", "contents": "Title: Fast YOLO: A Fast You Only Look Once System for Real-time Embedded\n  Object Detection in Video Abstract: Object detection is considered one of the most challenging problems in this\nfield of computer vision, as it involves the combination of object\nclassification and object localization within a scene. Recently, deep neural\nnetworks (DNNs) have been demonstrated to achieve superior object detection\nperformance compared to other approaches, with YOLOv2 (an improved You Only\nLook Once model) being one of the state-of-the-art in DNN-based object\ndetection methods in terms of both speed and accuracy. Although YOLOv2 can\nachieve real-time performance on a powerful GPU, it still remains very\nchallenging for leveraging this approach for real-time object detection in\nvideo on embedded computing devices with limited computational power and\nlimited memory. In this paper, we propose a new framework called Fast YOLO, a\nfast You Only Look Once framework which accelerates YOLOv2 to be able to\nperform object detection in video on embedded devices in a real-time manner.\nFirst, we leverage the evolutionary deep intelligence framework to evolve the\nYOLOv2 network architecture and produce an optimized architecture (referred to\nas O-YOLOv2 here) that has 2.8X fewer parameters with just a ~2% IOU drop. To\nfurther reduce power consumption on embedded devices while maintaining\nperformance, a motion-adaptive inference method is introduced into the proposed\nFast YOLO framework to reduce the frequency of deep inference with O-YOLOv2\nbased on temporal motion characteristics. Experimental results show that the\nproposed Fast YOLO framework can reduce the number of deep inferences by an\naverage of 38.13%, and an average speedup of ~3.3X for objection detection in\nvideo compared to the original YOLOv2, leading Fast YOLO to run an average of\n~18FPS on a Nvidia Jetson TX1 embedded system. \n\n"}
{"id": "1709.06127", "contents": "Title: Diluting the Scalability Boundaries: Exploring the Use of Disaggregated\n  Architectures for High-Level Network Data Analysis Abstract: Traditional data centers are designed with a rigid architecture of\nfit-for-purpose servers that provision resources beyond the average workload in\norder to deal with occasional peaks of data. Heterogeneous data centers are\npushing towards more cost-efficient architectures with better resource\nprovisioning. In this paper we study the feasibility of using disaggregated\narchitectures for intensive data applications, in contrast to the monolithic\napproach of server-oriented architectures. Particularly, we have tested a\nproactive network analysis system in which the workload demands are highly\nvariable. In the context of the dReDBox disaggregated architecture, the results\nshow that the overhead caused by using remote memory resources is significant,\nbetween 66\\% and 80\\%, but we have also observed that the memory usage is one\norder of magnitude higher for the stress case with respect to average\nworkloads. Therefore, dimensioning memory for the worst case in conventional\nsystems will result in a notable waste of resources. Finally, we found that,\nfor the selected use case, parallelism is limited by memory. Therefore, using a\ndisaggregated architecture will allow for increased parallelism, which, at the\nsame time, will mitigate the overhead caused by remote memory. \n\n"}
{"id": "1709.06160", "contents": "Title: On Dynamic Precision Scaling Abstract: Based on the observation that application phases exhibit varying degrees of\nsensitivity to noise (i.e., accuracy loss) in computation during execution,\nthis paper explores how Dynamic Precision Scaling (DPS) can maximize power\nefficiency by tailoring the precision of computation adaptively to temporal\nchanges in algorithmic noise tolerance. DPS can decrease the arithmetic\nprecision of noise-tolerant phases to result in power savings at the same\noperating speed (or faster execution within the same power budget), while\nkeeping the overall loss in accuracy due to precision reduction bounded. \n\n"}
{"id": "1709.06175", "contents": "Title: Blocking Versus Non-Blocking Halo Exchange Abstract: This report describes the design, implementation and analysis of a\nnon-blocking halo exchange routine as an alternative to the blocking halo\nexchange routine in the lattice Boltzmann code Ludwig. The alternative,\nnon-blocking, routine is implemented in such a way to allow work-communication\noverlap. Detailed benchmarks in this report show that the non-blocking version\nis a good alternative even without any work-communication overlap.\nWork-Communication overlap can be used to improve the performance of the\nnon-blocking routine. Development and benchmarking were conducted on the UK\nnational supercomputer, ARCHER. \n\n"}
{"id": "1709.07563", "contents": "Title: EmuFog: Extensible and Scalable Emulation of Large-Scale Fog Computing\n  Infrastructures Abstract: The diversity of Fog Computing deployment models and the lack of publicly\navailable Fog infrastructure makes the design of an efficient application or\nresource management policy a challenging task. Such research often requires a\ntest framework that facilitates the experimental evaluation of an application\nor protocol design in a repeatable and controllable manner. In this paper, we\npresent EmuFog---an extensible emulation framework tailored for Fog Computing\nscenarios---that enables the from-scratch design of Fog Computing\ninfrastructures and the emulation of real applications and workloads. EmuFog\nenables researchers to design the network topology according to the use-case,\nembed Fog Computing nodes in the topology and run Docker-based applications on\nthose nodes connected by an emulated network. Each of the sub-modules of EmuFog\nare easily extensible, although EmuFog provides a default implementation for\neach of them. The scalability and efficacy of EmuFog are evaluated both on\nsynthetic and real-world network topologies. \n\n"}
{"id": "1709.07822", "contents": "Title: Planar Graph Perfect Matching is in NC Abstract: Is perfect matching in NC? That is, is there a deterministic fast parallel\nalgorithm for it? This has been an outstanding open question in theoretical\ncomputer science for over three decades, ever since the discovery of RNC\nmatching algorithms. Within this question, the case of planar graphs has\nremained an enigma: On the one hand, counting the number of perfect matchings\nis far harder than finding one (the former is #P-complete and the latter is in\nP), and on the other, for planar graphs, counting has long been known to be in\nNC whereas finding one has resisted a solution.\n  In this paper, we give an NC algorithm for finding a perfect matching in a\nplanar graph. Our algorithm uses the above-stated fact about counting matchings\nin a crucial way. Our main new idea is an NC algorithm for finding a face of\nthe perfect matching polytope at which $\\Omega(n)$ new conditions, involving\nconstraints of the polytope, are simultaneously satisfied. Several other ideas\nare also needed, such as finding a point in the interior of the minimum weight\nface of this polytope and finding a balanced tight odd set in NC. \n\n"}
{"id": "1709.08524", "contents": "Title: Generative learning for deep networks Abstract: Learning, taking into account full distribution of the data, referred to as\ngenerative, is not feasible with deep neural networks (DNNs) because they model\nonly the conditional distribution of the outputs given the inputs. Current\nsolutions are either based on joint probability models facing difficult\nestimation problems or learn two separate networks, mapping inputs to outputs\n(recognition) and vice-versa (generation). We propose an intermediate approach.\nFirst, we show that forward computation in DNNs with logistic sigmoid\nactivations corresponds to a simplified approximate Bayesian inference in a\ndirected probabilistic multi-layer model. This connection allows to interpret\nDNN as a probabilistic model of the output and all hidden units given the\ninput. Second, we propose that in order for the recognition and generation\nnetworks to be more consistent with the joint model of the data, weights of the\nrecognition and generator network should be related by transposition. We\ndemonstrate in a tentative experiment that such a coupled pair can be learned\ngeneratively, modelling the full distribution of the data, and has enough\ncapacity to perform well in both recognition and generation. \n\n"}
{"id": "1709.08853", "contents": "Title: Object-oriented Neural Programming (OONP) for Document Understanding Abstract: We propose Object-oriented Neural Programming (OONP), a framework for\nsemantically parsing documents in specific domains. Basically, OONP reads a\ndocument and parses it into a predesigned object-oriented data structure\n(referred to as ontology in this paper) that reflects the domain-specific\nsemantics of the document. An OONP parser models semantic parsing as a decision\nprocess: a neural net-based Reader sequentially goes through the document, and\nduring the process it builds and updates an intermediate ontology to summarize\nits partial understanding of the text it covers. OONP supports a rich family of\noperations (both symbolic and differentiable) for composing the ontology, and a\nbig variety of forms (both symbolic and differentiable) for representing the\nstate and the document. An OONP parser can be trained with supervision of\ndifferent forms and strength, including supervised learning (SL) ,\nreinforcement learning (RL) and hybrid of the two. Our experiments on both\nsynthetic and real-world document parsing tasks have shown that OONP can learn\nto handle fairly complicated ontology with training data of modest sizes. \n\n"}
{"id": "1709.09491", "contents": "Title: Flexible Support for Fast Parallel Commutative Updates Abstract: Privatizing data is a useful strategy for increasing parallelism in a shared\nmemory multithreaded program. Independent cores can compute independently on\nduplicates of shared data, combining their results at the end of their\ncomputations. Conventional approaches to privatization, however, rely on\nexplicit static or dynamic memory allocation for duplicated state, increasing\nmemory footprint and contention for cache resources, especially in shared\ncaches. In this work, we describe CCache, a system for on-demand privatization\nof data manipulated by commutative operations. CCache garners the benefits of\nprivatization, without the increase in memory footprint or cache occupancy.\nEach core in CCache dynamically privatizes commutatively manipulated data,\noperating on a copy. Periodically or at the end of its computation, the core\nmerges its value with the value resident in memory, and when all cores have\nmerged, the in-memory copy contains the up-to-date value. We describe a\nlow-complexity architectural implementation of CCache that extends a\nconventional multicore to support on-demand privatization without using\nadditional memory for private copies. We evaluate CCache on several high-value\napplications, including random access key-value store, clustering, breadth\nfirst search and graph ranking, showing speedups upto 3.2X. \n\n"}
{"id": "1709.10140", "contents": "Title: Performance Evaluation of Container-based Virtualization for High\n  Performance Computing Environments Abstract: Virtualization technologies have evolved along with the development of\ncomputational environments since virtualization offered needed features at that\ntime such as isolation, accountability, resource allocation, resource fair\nsharing and so on. Novel processor technologies bring to commodity computers\nthe possibility to emulate diverse environments where a wide range of\ncomputational scenarios can be run. Along with processors evolution, system\ndevelopers have created different virtualization mechanisms where each new\ndevelopment enhanced the performance of previous virtualized environments.\nRecently, operating system-based virtualization technologies captured the\nattention of communities abroad (from industry to academy and research) because\ntheir important improvements on performance area.\n  In this paper, the features of three container-based operating systems\nvirtualization tools (LXC, Docker and Singularity) are presented. LXC, Docker,\nSingularity and bare metal are put under test through a customized single node\nHPL-Benchmark and a MPI-based application for the multi node testbed. Also the\ndisk I/O performance, Memory (RAM) performance, Network bandwidth and GPU\nperformance are tested for the COS technologies vs bare metal. Preliminary\nresults and conclusions around them are presented and discussed. \n\n"}
{"id": "1710.00778", "contents": "Title: Proactive Doppler Shift Compensation in Vehicular Cyber-Physical Systems Abstract: In vehicular cyber-physical systems (CPS), safety information, including\nvehicular speed and location information, is shared among vehicles via wireless\nwaves at specific frequency. This helps control vehicle to alleviate traffic\ncongestion and road accidents. However, Doppler shift existing between vehicles\nwith high relative speed causes an apparent frequency shift for the received\nwireless wave, which consequently decreases the reliability of the recovered\nsafety information and jeopardizes the safety of vehicular CPS. Passive\nconfrontation of Doppler shift at the receiver side is not applicable due to\nmultiple Doppler shifts at each receiver. In this paper, we provide a proactive\nDoppler shift compensation algorithm based on the probabilistic graphical\nmodel. Each vehicle pre-compensates its carrier frequency individually so that\nthere is no frequency shift from the desired carrier frequency between each\npair of transceiver. The pre-compensated offset for each vehicle is computed in\na distributed fashion in order to be adaptive to the distributed and dynamic\ntopology of vehicular CPS. Besides, the updating procedure is designed in a\nbroadcasting fashion to reduce communication burden. It is rigorously proved\nthat the proposed algorithm is convergence guaranteed even for systems with\npacket drops and random communication delays. Simulations based on real map and\ntransportation data verify the accuracy and convergence property of the\nproposed algorithm. It is shown that this method achieves almost the optimal\nfrequency compensation accuracy with an error approaching the Cram\\'{e}r-Rao\nlower bound. \n\n"}
{"id": "1710.02238", "contents": "Title: How Much Chemistry Does a Deep Neural Network Need to Know to Make\n  Accurate Predictions? Abstract: The meteoric rise of deep learning models in computer vision research, having\nachieved human-level accuracy in image recognition tasks is firm evidence of\nthe impact of representation learning of deep neural networks. In the chemistry\ndomain, recent advances have also led to the development of similar CNN models,\nsuch as Chemception, that is trained to predict chemical properties using\nimages of molecular drawings. In this work, we investigate the effects of\nsystematically removing and adding localized domain-specific information to the\nimage channels of the training data. By augmenting images with only 3\nadditional basic information, and without introducing any architectural\nchanges, we demonstrate that an augmented Chemception (AugChemception)\noutperforms the original model in the prediction of toxicity, activity, and\nsolvation free energy. Then, by altering the information content in the images,\nand examining the resulting model's performance, we also identify two distinct\nlearning patterns in predicting toxicity/activity as compared to solvation free\nenergy. These patterns suggest that Chemception is learning about its tasks in\nthe manner that is consistent with established knowledge. Thus, our work\ndemonstrates that advanced chemical knowledge is not a pre-requisite for deep\nlearning models to accurately predict complex chemical properties. \n\n"}
{"id": "1710.02254", "contents": "Title: Lattice Recurrent Unit: Improving Convergence and Statistical Efficiency\n  for Sequence Modeling Abstract: Recurrent neural networks have shown remarkable success in modeling\nsequences. However low resource situations still adversely affect the\ngeneralizability of these models. We introduce a new family of models, called\nLattice Recurrent Units (LRU), to address the challenge of learning deep\nmulti-layer recurrent models with limited resources. LRU models achieve this\ngoal by creating distinct (but coupled) flow of information inside the units: a\nfirst flow along time dimension and a second flow along depth dimension. It\nalso offers a symmetry in how information can flow horizontally and vertically.\nWe analyze the effects of decoupling three different components of our LRU\nmodel: Reset Gate, Update Gate and Projected State. We evaluate this family on\nnew LRU models on computational convergence rates and statistical efficiency.\nOur experiments are performed on four publicly-available datasets, comparing\nwith Grid-LSTM and Recurrent Highway networks. Our results show that LRU has\nbetter empirical computational convergence rates and statistical efficiency\nvalues, along with learning more accurate language models. \n\n"}
{"id": "1710.03070", "contents": "Title: full-FORCE: A Target-Based Method for Training Recurrent Networks Abstract: Trained recurrent networks are powerful tools for modeling dynamic neural\ncomputations. We present a target-based method for modifying the full\nconnectivity matrix of a recurrent network to train it to perform tasks\ninvolving temporally complex input/output transformations. The method\nintroduces a second network during training to provide suitable \"target\"\ndynamics useful for performing the task. Because it exploits the full recurrent\nconnectivity, the method produces networks that perform tasks with fewer\nneurons and greater noise robustness than traditional least-squares (FORCE)\napproaches. In addition, we show how introducing additional input signals into\nthe target-generating network, which act as task hints, greatly extends the\nrange of tasks that can be learned and provides control over the complexity and\nnature of the dynamics of the trained, task-performing network. \n\n"}
{"id": "1710.03178", "contents": "Title: Constant-Length Labeling Schemes for Deterministic Radio Broadcast Abstract: Broadcast is one of the fundamental network communication primitives. One\nnode of a network, called the $\\mathit{source}$, has a message that has to be\nlearned by all other nodes. We consider the feasibility of deterministic\nbroadcast in radio networks. If nodes of the network do not have any labels,\ndeterministic broadcast is impossible even in the four-cycle. On the other\nhand, if all nodes have distinct labels, then broadcast can be carried out,\ne.g., in a round-robin fashion, and hence $O(\\log n)$-bit labels are sufficient\nfor this task in $n$-node networks. In fact, $O(\\log \\Delta)$-bit labels, where\n$\\Delta$ is the maximum degree, are enough to broadcast successfully. Hence, it\nis natural to ask if very short labels are sufficient for broadcast. Our main\nresult is a positive answer to this question. We show that every radio network\ncan be labeled using 2 bits in such a way that broadcast can be accomplished by\nsome universal deterministic algorithm that does not know the network topology\nnor any bound on its size. Moreover, at the expense of an extra bit in the\nlabels, we get the additional strong property that there exists a common round\nin which all nodes know that broadcast has been completed. Finally, we show\nthat 3-bit labels are also sufficient to solve both versions of broadcast in\nthe case where the labeling scheme does not know which node is the source. \n\n"}
{"id": "1710.04162", "contents": "Title: Synkhronos: a Multi-GPU Theano Extension for Data Parallelism Abstract: We present Synkhronos, an extension to Theano for multi-GPU computations\nleveraging data parallelism. Our framework provides automated execution and\nsynchronization across devices, allowing users to continue to write serial\nprograms without risk of race conditions. The NVIDIA Collective Communication\nLibrary is used for high-bandwidth inter-GPU communication. Further\nenhancements to the Theano function interface include input slicing (with\naggregation) and input indexing, which perform common data-parallel computation\npatterns efficiently. One example use case is synchronous SGD, which has\nrecently been shown to scale well for a growing set of deep learning problems.\nWhen training ResNet-50, we achieve a near-linear speedup of 7.5x on an NVIDIA\nDGX-1 using 8 GPUs, relative to Theano-only code running a single GPU in\nisolation. Yet Synkhronos remains general to any data-parallel computation\nprogrammable in Theano. By implementing parallelism at the level of individual\nTheano functions, our framework uniquely addresses a niche between manual\nmulti-device programming and prescribed multi-GPU training routines. \n\n"}
{"id": "1710.09025", "contents": "Title: Optimization of population annealing Monte Carlo for large-scale\n  spin-glass simulations Abstract: Population annealing Monte Carlo is an efficient sequential algorithm for\nsimulating k-local Boolean Hamiltonians. Because of its structure, the\nalgorithm is inherently parallel and therefore well suited for large-scale\nsimulations of computationally hard problems. Here we present various ways of\noptimizing population annealing Monte Carlo using 2-local spin-glass\nHamiltonians as a case study. We demonstrate how the algorithm can be optimized\nfrom an implementation, algorithmic accelerator, as well as scalable\nparallelization points of view. This makes population annealing Monte Carlo\nperfectly suited to study other frustrated problems such as pyrochlore\nlattices, constraint-satisfaction problems, as well as higher-order\nHamiltonians commonly found in, e.g., topological color codes. \n\n"}
{"id": "1710.10196", "contents": "Title: Progressive Growing of GANs for Improved Quality, Stability, and\n  Variation Abstract: We describe a new training methodology for generative adversarial networks.\nThe key idea is to grow both the generator and discriminator progressively:\nstarting from a low resolution, we add new layers that model increasingly fine\ndetails as training progresses. This both speeds the training up and greatly\nstabilizes it, allowing us to produce images of unprecedented quality, e.g.,\nCelebA images at 1024^2. We also propose a simple way to increase the variation\nin generated images, and achieve a record inception score of 8.80 in\nunsupervised CIFAR10. Additionally, we describe several implementation details\nthat are important for discouraging unhealthy competition between the generator\nand discriminator. Finally, we suggest a new metric for evaluating GAN results,\nboth in terms of image quality and variation. As an additional contribution, we\nconstruct a higher-quality version of the CelebA dataset. \n\n"}
{"id": "1710.10304", "contents": "Title: Few-shot Autoregressive Density Estimation: Towards Learning to Learn\n  Distributions Abstract: Deep autoregressive models have shown state-of-the-art performance in density\nestimation for natural images on large-scale datasets such as ImageNet.\nHowever, such models require many thousands of gradient-based weight updates\nand unique image examples for training. Ideally, the models would rapidly learn\nvisual concepts from only a handful of examples, similar to the manner in which\nhumans learns across many vision tasks. In this paper, we show how 1) neural\nattention and 2) meta learning techniques can be used in combination with\nautoregressive models to enable effective few-shot density estimation. Our\nproposed modifications to PixelCNN result in state-of-the art few-shot density\nestimation on the Omniglot dataset. Furthermore, we visualize the learned\nattention policy and find that it learns intuitive algorithms for simple tasks\nsuch as image mirroring on ImageNet and handwriting on Omniglot without\nsupervision. Finally, we extend the model to natural images and demonstrate\nfew-shot image generation on the Stanford Online Products dataset. \n\n"}
{"id": "1710.11351", "contents": "Title: ChainerMN: Scalable Distributed Deep Learning Framework Abstract: One of the keys for deep learning to have made a breakthrough in various\nfields was to utilize high computing powers centering around GPUs. Enabling the\nuse of further computing abilities by distributed processing is essential not\nonly to make the deep learning bigger and faster but also to tackle unsolved\nchallenges. We present the design, implementation, and evaluation of ChainerMN,\nthe distributed deep learning framework we have developed. We demonstrate that\nChainerMN can scale the learning process of the ResNet-50 model to the ImageNet\ndataset up to 128 GPUs with the parallel efficiency of 90%. \n\n"}
{"id": "1710.11573", "contents": "Title: Deep Learning as a Mixed Convex-Combinatorial Optimization Problem Abstract: As neural networks grow deeper and wider, learning networks with\nhard-threshold activations is becoming increasingly important, both for network\nquantization, which can drastically reduce time and energy requirements, and\nfor creating large integrated systems of deep networks, which may have\nnon-differentiable components and must avoid vanishing and exploding gradients\nfor effective learning. However, since gradient descent is not applicable to\nhard-threshold functions, it is not clear how to learn networks of them in a\nprincipled way. We address this problem by observing that setting targets for\nhard-threshold hidden units in order to minimize loss is a discrete\noptimization problem, and can be solved as such. The discrete optimization goal\nis to find a set of targets such that each unit, including the output, has a\nlinearly separable problem to solve. Given these targets, the network\ndecomposes into individual perceptrons, which can then be learned with standard\nconvex approaches. Based on this, we develop a recursive mini-batch algorithm\nfor learning deep hard-threshold networks that includes the popular but poorly\njustified straight-through estimator as a special case. Empirically, we show\nthat our algorithm improves classification accuracy in a number of settings,\nincluding for AlexNet and ResNet-18 on ImageNet, when compared to the\nstraight-through estimator. \n\n"}
{"id": "1711.00215", "contents": "Title: Minimum Energy Quantized Neural Networks Abstract: This work targets the automated minimum-energy optimization of Quantized\nNeural Networks (QNNs) - networks using low precision weights and activations.\nThese networks are trained from scratch at an arbitrary fixed point precision.\nAt iso-accuracy, QNNs using fewer bits require deeper and wider network\narchitectures than networks using higher precision operators, while they\nrequire less complex arithmetic and less bits per weights. This fundamental\ntrade-off is analyzed and quantified to find the minimum energy QNN for any\nbenchmark and hence optimize energy-efficiency. To this end, the energy\nconsumption of inference is modeled for a generic hardware platform. This\nallows drawing several conclusions across different benchmarks. First, energy\nconsumption varies orders of magnitude at iso-accuracy depending on the number\nof bits used in the QNN. Second, in a typical system, BinaryNets or int4\nimplementations lead to the minimum energy solution, outperforming int8\nnetworks up to 2-10x at iso-accuracy. All code used for QNN training is\navailable from https://github.com/BertMoons. \n\n"}
{"id": "1711.00436", "contents": "Title: Hierarchical Representations for Efficient Architecture Search Abstract: We explore efficient neural architecture search methods and show that a\nsimple yet powerful evolutionary algorithm can discover new architectures with\nexcellent performance. Our approach combines a novel hierarchical genetic\nrepresentation scheme that imitates the modularized design pattern commonly\nadopted by human experts, and an expressive search space that supports complex\ntopologies. Our algorithm efficiently discovers architectures that outperform a\nlarge number of manually designed models for image classification, obtaining\ntop-1 error of 3.6% on CIFAR-10 and 20.3% when transferred to ImageNet, which\nis competitive with the best existing neural architecture search approaches. We\nalso present results using random search, achieving 0.3% less top-1 accuracy on\nCIFAR-10 and 0.1% less on ImageNet whilst reducing the search time from 36\nhours down to 1 hour. \n\n"}
{"id": "1711.00549", "contents": "Title: Just ASK: Building an Architecture for Extensible Self-Service Spoken\n  Language Understanding Abstract: This paper presents the design of the machine learning architecture that\nunderlies the Alexa Skills Kit (ASK) a large scale Spoken Language\nUnderstanding (SLU) Software Development Kit (SDK) that enables developers to\nextend the capabilities of Amazon's virtual assistant, Alexa. At Amazon, the\ninfrastructure powers over 25,000 skills deployed through the ASK, as well as\nAWS's Amazon Lex SLU Service. The ASK emphasizes flexibility, predictability\nand a rapid iteration cycle for third party developers. It imposes inductive\nbiases that allow it to learn robust SLU models from extremely small and sparse\ndatasets and, in doing so, removes significant barriers to entry for software\ndevelopers and dialogue systems researchers. \n\n"}
{"id": "1711.00970", "contents": "Title: A Classification-Based Study of Covariate Shift in GAN Distributions Abstract: A basic, and still largely unanswered, question in the context of Generative\nAdversarial Networks (GANs) is whether they are truly able to capture all the\nfundamental characteristics of the distributions they are trained on. In\nparticular, evaluating the diversity of GAN distributions is challenging and\nexisting methods provide only a partial understanding of this issue. In this\npaper, we develop quantitative and scalable tools for assessing the diversity\nof GAN distributions. Specifically, we take a classification-based perspective\nand view loss of diversity as a form of covariate shift introduced by GANs. We\nexamine two specific forms of such shift: mode collapse and boundary\ndistortion. In contrast to prior work, our methods need only minimal human\nsupervision and can be readily applied to state-of-the-art GANs on large,\ncanonical datasets. Examining popular GANs using our tools indicates that these\nGANs have significant problems in reproducing the more distributional\nproperties of their training dataset. \n\n"}
{"id": "1711.01110", "contents": "Title: A Rudimentary Model for Low-Latency Anonymous Communication Systems Abstract: In this paper we present a rudimentary model for low-latency anonymous\ncommunication systems. Specifically, we study distributed OR algorithm as an\nabstract of the system. Based on our model, we give several satisfactory lower\nbounds of anonymity leakage of a deterministic OR algorithm. Some of them\nreveal a trade-off between anonymity and communication complexity. For the\nrandomized OR algorithm, we only give a relatively trivial but possibly tight\nlower bound when leaving out communication complexity. And we find the\nrelationship between our model and some open case in the study of secret\nsharing scheme, if considering communication complexity. \n\n"}
{"id": "1711.01239", "contents": "Title: Routing Networks: Adaptive Selection of Non-linear Functions for\n  Multi-Task Learning Abstract: Multi-task learning (MTL) with neural networks leverages commonalities in\ntasks to improve performance, but often suffers from task interference which\nreduces the benefits of transfer. To address this issue we introduce the\nrouting network paradigm, a novel neural network and training algorithm. A\nrouting network is a kind of self-organizing neural network consisting of two\ncomponents: a router and a set of one or more function blocks. A function block\nmay be any neural network - for example a fully-connected or a convolutional\nlayer. Given an input the router makes a routing decision, choosing a function\nblock to apply and passing the output back to the router recursively,\nterminating when a fixed recursion depth is reached. In this way the routing\nnetwork dynamically composes different function blocks for each input. We\nemploy a collaborative multi-agent reinforcement learning (MARL) approach to\njointly train the router and function blocks. We evaluate our model against\ncross-stitch networks and shared-layer baselines on multi-task settings of the\nMNIST, mini-imagenet, and CIFAR-100 datasets. Our experiments demonstrate a\nsignificant improvement in accuracy, with sharper convergence. In addition,\nrouting networks have nearly constant per-task training cost while cross-stitch\nnetworks scale linearly with the number of tasks. On CIFAR-100 (20 tasks) we\nobtain cross-stitch performance levels with an 85% reduction in training time. \n\n"}
{"id": "1711.01243", "contents": "Title: ReBNet: Residual Binarized Neural Network Abstract: This paper proposes ReBNet, an end-to-end framework for training\nreconfigurable binary neural networks on software and developing efficient\naccelerators for execution on FPGA. Binary neural networks offer an intriguing\nopportunity for deploying large-scale deep learning models on\nresource-constrained devices. Binarization reduces the memory footprint and\nreplaces the power-hungry matrix-multiplication with light-weight XnorPopcount\noperations. However, binary networks suffer from a degraded accuracy compared\nto their fixed-point counterparts. We show that the state-of-the-art methods\nfor optimizing binary networks accuracy, significantly increase the\nimplementation cost and complexity. To compensate for the degraded accuracy\nwhile adhering to the simplicity of binary networks, we devise the first\nreconfigurable scheme that can adjust the classification accuracy based on the\napplication. Our proposition improves the classification accuracy by\nrepresenting features with multiple levels of residual binarization. Unlike\nprevious methods, our approach does not exacerbate the area cost of the\nhardware accelerator. Instead, it provides a tradeoff between throughput and\naccuracy while the area overhead of multi-level binarization is negligible. \n\n"}
{"id": "1711.01559", "contents": "Title: Machine Learning Approach to RF Transmitter Identification Abstract: With the development and widespread use of wireless devices in recent years\n(mobile phones, Internet of Things, Wi-Fi), the electromagnetic spectrum has\nbecome extremely crowded. In order to counter security threats posed by rogue\nor unknown transmitters, it is important to identify RF transmitters not by the\ndata content of the transmissions but based on the intrinsic physical\ncharacteristics of the transmitters. RF waveforms represent a particular\nchallenge because of the extremely high data rates involved and the potentially\nlarge number of transmitters present in a given location. These factors outline\nthe need for rapid fingerprinting and identification methods that go beyond the\ntraditional hand-engineered approaches. In this study, we investigate the use\nof machine learning (ML) strategies to the classification and identification\nproblems, and the use of wavelets to reduce the amount of data required. Four\ndifferent ML strategies are evaluated: deep neural nets (DNN), convolutional\nneural nets (CNN), support vector machines (SVM), and multi-stage training\n(MST) using accelerated Levenberg-Marquardt (A-LM) updates. The A-LM MST method\npreconditioned by wavelets was by far the most accurate, achieving 100%\nclassification accuracy of transmitters, as tested using data originating from\n12 different transmitters. We discuss strategies for extension of MST to a much\nlarger number of transmitters. \n\n"}
{"id": "1711.01897", "contents": "Title: Simple and efficient GPU parallelization of existing H-Matrix\n  accelerated BEM code Abstract: In this paper, we demonstrate how GPU-accelerated BEM routines can be used in\na simple black-box fashion to accelerate fast boundary element formulations\nbased on Hierarchical Matrices (H-Matrices) with ACA (Adaptive Cross\nApproximation). In particular, we focus on the expensive evaluation of the\ndiscrete weak form of boundary operators associated with the Laplace and the\nHelmholtz equation in three space dimensions. The method is based on offloading\nthe CPU assembly of elements during the ACA assembly onto a GPU device and to\nuse threading strategies across ACA blocks to create sufficient workload for\nthe GPU. The proposed GPU strategy is designed such that it can be implemented\nin existing code with minimal changes to the surrounding application structure.\nThis is in particular interesting for existing legacy code that is not from the\nground-up designed with GPU computing in mind. Our benchmark study gives\nrealistic impressions of the benefits of GPU-accelerated BEM simulations by\nusing state-of-the-art multi-threaded computations on modern high-performance\nCPUs as a reference, rather than drawing synthetic comparisons with\nsingle-threaded codes. Speed-up plots illustrate that performance gains up to a\nfactor of 5.5 could be realized with GPU computing under these conditions. This\nrefers to a boundary element model with about 4 million unknowns, whose\nH-Matrix weak form associated with a real-valued (Laplace) boundary operator is\nset up in only 100 minutes harnessing the two GPUs instead of 9 hours when\nusing the 20 CPU cores at disposal only. The benchmark study is followed by a\nparticularly demanding real-life application, where we compute the scattered\nhigh-frequency sound field of a submarine to demonstrate the increase in\noverall application performance from moving to a GPU-based ACA assembly. \n\n"}
{"id": "1711.01919", "contents": "Title: Fast Integral Histogram Computations on GPU for Real-Time Video\n  Analytics Abstract: In many Multimedia content analytics frameworks feature likelihood maps\nrepresented as histograms play a critical role in the overall algorithm.\nIntegral histograms provide an efficient computational framework for extracting\nmulti-scale histogram-based regional descriptors in constant time which are\nconsidered as the principle building blocks of many video content analytics\nframeworks. We evaluate four different mappings of the integral histogram\ncomputation onto Graphics Processing Units (GPUs) using different kernel\noptimization strategies. Our kernels perform cumulative sums on row and column\nhistograms in a cross-weave or wavefront scan order, use different data\norganization and scheduling methods that is shown to critically affect\nutilization of GPU resources (cores and shared memory). Tiling the 3-D array\ninto smaller regular data blocks significantly speeds up the efficiency of the\ncomputation compared to a strip-based organization. The tiled integral\nhistogram using a diagonal wavefront scan has the best performance of about\n300.4 frames/sec for 640 x 480 images and 32 bins with a speedup factor of\nabout 120 using GTX Titan X graphics card compared to a single threaded\nsequential CPU implementation. Double-buffering has been exploited to overlap\ncomputation and communication across sequence of images. Mapping integral\nhistogram bins computations onto multiple GPUs enables us to process 32 giga\nbytes integral histogram data (of 64MB Image and 128 bins) with a frame rate of\n0.73 Hz and speedup factor of 153X over single-threaded CPU implementation and\nthe speedup of 45X over 16-threaded CPU implementation. \n\n"}
{"id": "1711.02282", "contents": "Title: Variational Walkback: Learning a Transition Operator as a Stochastic\n  Recurrent Net Abstract: We propose a novel method to directly learn a stochastic transition operator\nwhose repeated application provides generated samples. Traditional undirected\ngraphical models approach this problem indirectly by learning a Markov chain\nmodel whose stationary distribution obeys detailed balance with respect to a\nparameterized energy function. The energy function is then modified so the\nmodel and data distributions match, with no guarantee on the number of steps\nrequired for the Markov chain to converge. Moreover, the detailed balance\ncondition is highly restrictive: energy based models corresponding to neural\nnetworks must have symmetric weights, unlike biological neural circuits. In\ncontrast, we develop a method for directly learning arbitrarily parameterized\ntransition operators capable of expressing non-equilibrium stationary\ndistributions that violate detailed balance, thereby enabling us to learn more\nbiologically plausible asymmetric neural networks and more general non-energy\nbased dynamical systems. The proposed training objective, which we derive via\nprincipled variational methods, encourages the transition operator to \"walk\nback\" in multi-step trajectories that start at data-points, as quickly as\npossible back to the original data points. We present a series of experimental\nresults illustrating the soundness of the proposed approach, Variational\nWalkback (VW), on the MNIST, CIFAR-10, SVHN and CelebA datasets, demonstrating\nsuperior samples compared to earlier attempts to learn a transition operator.\nWe also show that although each rapid training trajectory is limited to a\nfinite but variable number of steps, our transition operator continues to\ngenerate good samples well past the length of such trajectories, thereby\ndemonstrating the match of its non-equilibrium stationary distribution to the\ndata distribution. Source Code: http://github.com/anirudh9119/walkback_nips17 \n\n"}
{"id": "1711.02659", "contents": "Title: Optimizing ROOT IO For Analysis Abstract: The ROOT I/O (RIO) subsystem is foundational to most HEP experiments - it\nprovides a file format, a set of APIs/semantics, and a reference implementation\nin C++. It is often found at the base of an experiment's framework and is used\nto serialize the experiment's data; in the case of an LHC experiment, this may\nbe hundreds of petabytes of files! Individual physicists will further use RIO\nto perform their end-stage analysis, reading from intermediate files they\ngenerate from experiment data.\n  RIO is thus incredibly flexible: it must serve as a file format for archival\n(optimized for space) and for working data (optimized for read speed). To date,\nmost of the technical work has focused on improving the former use case. We\npresent work designed to help improve RIO for analysis. We analyze the\nreal-world impact of LZ4 to decrease decompression times (and the corresponding\ncost in disk space). We introduce new APIs that read RIO data in bulk, removing\nthe per-event overhead of a C++ function call. We compare the performance with\nthe existing RIO APIs for simple structure data and show how this can be\ncomplimentary with efforts to improve the parallelism of the RIO stack. \n\n"}
{"id": "1711.02976", "contents": "Title: RPYFMM: Parallel Adaptive Fast Multipole Method for\n  Rotne-Prager-Yamakawa Tensor in Biomolecular Hydrodynamics Simulations Abstract: RPYFMM is a software package for the efficient evaluation of the potential\nfield governed by the Rotne-Prager-Yamakawa (RPY) tensor interactions in\nbiomolecular hydrodynamics simulations. In our algorithm, the RPY tensor is\ndecomposed as a linear combination of four Laplace interactions, each of which\nis evaluated using the adaptive fast multipole method (FMM) [1] where the\nexponential expansions are applied to diagonalize the multipole-to-local\ntranslation operators. RPYFMM offers a unified execution on both shared and\ndistributed memory computers by leveraging the DASHMM library [2, 3].\nPreliminary numerical results show that the interactions for a molecular system\nof 15 million particles (beads) can be computed within one second on a Cray\nXC30 cluster using 12, 288 cores, while achieving approximately 54%\nstrong-scaling efficiency. \n\n"}
{"id": "1711.03180", "contents": "Title: Deep D-bar: Real time Electrical Impedance Tomography Imaging with Deep\n  Neural Networks Abstract: The mathematical problem for Electrical Impedance Tomography (EIT) is a\nhighly nonlinear ill-posed inverse problem requiring carefully designed\nreconstruction procedures to ensure reliable image generation. D-bar methods\nare based on a rigorous mathematical analysis and provide robust direct\nreconstructions by using a low-pass filtering of the associated nonlinear\nFourier data. Similarly to low-pass filtering of linear Fourier data, only\nusing low frequencies in the image recovery process results in blurred images\nlacking sharp features such as clear organ boundaries. Convolutional Neural\nNetworks provide a powerful framework for post-processing such convolved direct\nreconstructions. In this study, we demonstrate that these CNN techniques lead\nto sharp and reliable reconstructions even for the highly nonlinear inverse\nproblem of EIT. The network is trained on data sets of simulated examples and\nthen applied to experimental data without the need to perform an additional\ntransfer training. Results for absolute EIT images are presented using\nexperimental EIT data from the ACT4 and KIT4 EIT systems. \n\n"}
{"id": "1711.03229", "contents": "Title: A Dwarf-based Scalable Big Data Benchmarking Methodology Abstract: Different from the traditional benchmarking methodology that creates a new\nbenchmark or proxy for every possible workload, this paper presents a scalable\nbig data benchmarking methodology. Among a wide variety of big data analytics\nworkloads, we identify eight big data dwarfs, each of which captures the common\nrequirements of each class of unit of computation while being reasonably\ndivorced from individual implementations. We implement the eight dwarfs on\ndifferent software stacks, e.g., OpenMP, MPI, Hadoop as the dwarf components.\nFor the purpose of architecture simulation, we construct and tune big data\nproxy benchmarks using the directed acyclic graph (DAG)-like combinations of\nthe dwarf components with different weights to mimic the benchmarks in\nBigDataBench. Our proxy benchmarks preserve the micro-architecture, memory, and\nI/O characteristics, and they shorten the simulation time by 100s times while\nmaintain the average micro-architectural data accuracy above 90 percentage on\nboth X86 64 and ARMv8 processors. We will open-source the big data dwarf\ncomponents and proxy benchmarks soon. \n\n"}
{"id": "1711.03906", "contents": "Title: D-SLATS: Distributed Simultaneous Localization and Time Synchronization Abstract: Through the last decade, we have witnessed a surge of Internet of Things\n(IoT) devices, and with that a greater need to choreograph their actions across\nboth time and space. Although these two problems, namely time synchronization\nand localization, share many aspects in common, they are traditionally treated\nseparately or combined on centralized approaches that results in an ineffcient\nuse of resources, or in solutions that are not scalable in terms of the number\nof IoT devices. Therefore, we propose D-SLATS, a framework comprised of three\ndifferent and independent algorithms to jointly solve time synchronization and\nlocalization problems in a distributed fashion. The First two algorithms are\nbased mainly on the distributed Extended Kalman Filter (EKF) whereas the third\none uses optimization techniques. No fusion center is required, and the devices\nonly communicate with their neighbors. The proposed methods are evaluated on\ncustom Ultra-Wideband communication Testbed and a quadrotor, representing a\nnetwork of both static and mobile nodes. Our algorithms achieve up to three\nmicroseconds time synchronization accuracy and 30 cm localization error. \n\n"}
{"id": "1711.04325", "contents": "Title: Extremely Large Minibatch SGD: Training ResNet-50 on ImageNet in 15\n  Minutes Abstract: We demonstrate that training ResNet-50 on ImageNet for 90 epochs can be\nachieved in 15 minutes with 1024 Tesla P100 GPUs. This was made possible by\nusing a large minibatch size of 32k. To maintain accuracy with this large\nminibatch size, we employed several techniques such as RMSprop warm-up, batch\nnormalization without moving averages, and a slow-start learning rate schedule.\nThis paper also describes the details of the hardware and software of the\nsystem used to achieve the above performance. \n\n"}
{"id": "1711.04325", "contents": "Title: Extremely Large Minibatch SGD: Training ResNet-50 on ImageNet in 15\n  Minutes Abstract: We demonstrate that training ResNet-50 on ImageNet for 90 epochs can be\nachieved in 15 minutes with 1024 Tesla P100 GPUs. This was made possible by\nusing a large minibatch size of 32k. To maintain accuracy with this large\nminibatch size, we employed several techniques such as RMSprop warm-up, batch\nnormalization without moving averages, and a slow-start learning rate schedule.\nThis paper also describes the details of the hardware and software of the\nsystem used to achieve the above performance. \n\n"}
{"id": "1711.05795", "contents": "Title: Finer Grained Entity Typing with TypeNet Abstract: We consider the challenging problem of entity typing over an extremely fine\ngrained set of types, wherein a single mention or entity can have many\nsimultaneous and often hierarchically-structured types. Despite the importance\nof the problem, there is a relative lack of resources in the form of\nfine-grained, deep type hierarchies aligned to existing knowledge bases. In\nresponse, we introduce TypeNet, a dataset of entity types consisting of over\n1941 types organized in a hierarchy, obtained by manually annotating a mapping\nfrom 1081 Freebase types to WordNet. We also experiment with several models\ncomparable to state-of-the-art systems and explore techniques to incorporate a\nstructure loss on the hierarchy with the standard mention typing loss, as a\nfirst step towards future research on this dataset. \n\n"}
{"id": "1711.05852", "contents": "Title: Apprentice: Using Knowledge Distillation Techniques To Improve\n  Low-Precision Network Accuracy Abstract: Deep learning networks have achieved state-of-the-art accuracies on computer\nvision workloads like image classification and object detection. The performant\nsystems, however, typically involve big models with numerous parameters. Once\ntrained, a challenging aspect for such top performing models is deployment on\nresource constrained inference systems - the models (often deep networks or\nwide networks or both) are compute and memory intensive. Low-precision numerics\nand model compression using knowledge distillation are popular techniques to\nlower both the compute requirements and memory footprint of these deployed\nmodels. In this paper, we study the combination of these two techniques and\nshow that the performance of low-precision networks can be significantly\nimproved by using knowledge distillation techniques. Our approach, Apprentice,\nachieves state-of-the-art accuracies using ternary precision and 4-bit\nprecision for variants of ResNet architecture on ImageNet dataset. We present\nthree schemes using which one can apply knowledge distillation techniques to\nvarious stages of the train-and-deploy pipeline. \n\n"}
{"id": "1711.05932", "contents": "Title: A Design-Time/Run-Time Application Mapping Methodology for Predictable\n  Execution Time in MPSoCs Abstract: Executing multiple applications on a single MPSoC brings the major challenge\nof satisfying multiple quality requirements regarding real-time, energy, etc.\nHybrid application mapping denotes the combination of design-time analysis with\nrun-time application mapping. In this article, we present such a methodology,\nwhich comprises a design space exploration coupled with a formal performance\nanalysis. This results in several resource reservation configurations,\noptimized for multiple objectives, with verified real-time guarantees for each\nindividual application. The Pareto-optimal configurations are handed over to\nrun-time management which searches for a suitable mapping according to this\ninformation. To provide any real-time guarantees, the performance analysis\nneeds to be composable and the influence of the applications on each other has\nto be bounded. We achieve this either by spatial or a novel temporal isolation\nfor tasks and by exploiting composable NoCs. With the proposed temporal\nisolation, tasks of different applications can be mapped to the same resource\nwhile with spatial isolation, one computing resource can be exclusively used by\nonly one application. The experiments reveal that the success rate in finding\nfeasible application mappings can be increased by the proposed temporal\nisolation by up to 30% and energy consumption can be reduced compared to\nspatial isolation. \n\n"}
{"id": "1711.06315", "contents": "Title: SparCE: Sparsity aware General Purpose Core Extensions to Accelerate\n  Deep Neural Networks Abstract: Deep Neural Networks (DNNs) have emerged as the method of choice for solving\na wide range of machine learning tasks. The enormous computational demands\nposed by DNNs have most commonly been addressed through the design of custom\naccelerators. However, these accelerators are prohibitive in many design\nscenarios (e.g., wearable devices and IoT sensors), due to stringent area/cost\nconstraints. Accelerating DNNs on these low-power systems, comprising of mainly\nthe general-purpose processor (GPP) cores, requires new approaches. We improve\nthe performance of DNNs on GPPs by exploiting a key attribute of DNNs, i.e.,\nsparsity. We propose Sparsity aware Core Extensions (SparCE)- a set of\nmicro-architectural and ISA extensions that leverage sparsity and are minimally\nintrusive and low-overhead. We dynamically detect zero operands and skip a set\nof future instructions that use it. Our design ensures that the instructions to\nbe skipped are prevented from even being fetched, as squashing instructions\ncomes with a penalty. SparCE consists of 2 key micro-architectural\nenhancements- a Sparsity Register File (SpRF) that tracks zero registers and a\nSparsity aware Skip Address (SASA) table that indicates instructions to be\nskipped. When an instruction is fetched, SparCE dynamically pre-identifies\nwhether the following instruction(s) can be skipped and appropriately modifies\nthe program counter, thereby skipping the redundant instructions and improving\nperformance. We model SparCE using the gem5 architectural simulator, and\nevaluate our approach on 6 image-recognition DNNs in the context of both\ntraining and inference using the Caffe framework. On a scalar microprocessor,\nSparCE achieves 19%-31% reduction in application-level. We also evaluate SparCE\non a 4-way SIMD ARMv8 processor using the OpenBLAS library, and demonstrate\nthat SparCE achieves 8%-15% reduction in the application-level execution time. \n\n"}
{"id": "1711.06673", "contents": "Title: Neon2: Finding Local Minima via First-Order Oracles Abstract: We propose a reduction for non-convex optimization that can (1) turn an\nstationary-point finding algorithm into an local-minimum finding one, and (2)\nreplace the Hessian-vector product computations with only gradient\ncomputations. It works both in the stochastic and the deterministic settings,\nwithout hurting the algorithm's performance.\n  As applications, our reduction turns Natasha2 into a first-order method\nwithout hurting its performance. It also converts SGD, GD, SCSG, and SVRG into\nalgorithms finding approximate local minima, outperforming some best known\nresults. \n\n"}
{"id": "1711.07227", "contents": "Title: Linear-Complexity Relaxed Word Mover's Distance with GPU Acceleration Abstract: The amount of unstructured text-based data is growing every day. Querying,\nclustering, and classifying this big data requires similarity computations\nacross large sets of documents. Whereas low-complexity similarity metrics are\navailable, attention has been shifting towards more complex methods that\nachieve a higher accuracy. In particular, the Word Mover's Distance (WMD)\nmethod proposed by Kusner et al. is a promising new approach, but its time\ncomplexity grows cubically with the number of unique words in the documents.\nThe Relaxed Word Mover's Distance (RWMD) method, again proposed by Kusner et\nal., reduces the time complexity from qubic to quadratic and results in a\nlimited loss in accuracy compared with WMD. Our work contributes a\nlow-complexity implementation of the RWMD that reduces the average time\ncomplexity to linear when operating on large sets of documents. Our\nlinear-complexity RWMD implementation, henceforth referred to as LC-RWMD, maps\nwell onto GPUs and can be efficiently distributed across a cluster of GPUs. Our\nexperiments on real-life datasets demonstrate 1) a performance improvement of\ntwo orders of magnitude with respect to our GPU-based distributed\nimplementation of the quadratic RWMD, and 2) a performance improvement of three\nto four orders of magnitude with respect to our distributed WMD implementation\nthat uses GPU-based RWMD for pruning. \n\n"}
{"id": "1711.07423", "contents": "Title: Majority Model on Random Regular Graphs Abstract: Consider a graph $G=(V,E)$ and an initial random coloring where each vertex\n$v \\in V$ is blue with probability $P_b$ and red otherwise, independently from\nall other vertices. In each round, all vertices simultaneously switch their\ncolor to the most frequent color in their neighborhood and in case of a tie, a\nvertex keeps its current color. The main goal of the present paper is to\nanalyze the behavior of this basic and natural process on the random\n$d$-regular graph $\\mathbb{G}_{n,d}$. It is shown that for all $\\epsilon>0$,\n$P_b \\le 1/2-\\epsilon$ results in final complete occupancy by red in\n$\\mathcal{O}(\\log_d\\log n)$ rounds with high probability, provided that $d\\geq\nc/\\epsilon^2$ for a suitable constant $c$. Furthermore, we show that with high\nprobability, $\\mathbb{G}_{n,d}$ is immune; i.e., the smallest dynamic monopoly\nis of linear size. A dynamic monopoly is a subset of vertices that can take\nover in the sense that a commonly chosen initial color eventually spreads\nthroughout the whole graph, irrespective of the colors of other vertices. This\nanswers an open question of Peleg. \n\n"}
{"id": "1711.09123", "contents": "Title: A Manifesto for Future Generation Cloud Computing: Research Directions\n  for the Next Decade Abstract: The Cloud computing paradigm has revolutionised the computer science horizon\nduring the past decade and has enabled the emergence of computing as the fifth\nutility. It has captured significant attention of academia, industries, and\ngovernment bodies. Now, it has emerged as the backbone of modern economy by\noffering subscription-based services anytime, anywhere following a\npay-as-you-go model. This has instigated (1) shorter establishment times for\nstart-ups, (2) creation of scalable global enterprise applications, (3) better\ncost-to-value associativity for scientific and high performance computing\napplications, and (4) different invocation/execution models for pervasive and\nubiquitous applications. The recent technological developments and paradigms\nsuch as serverless computing, software-defined networking, Internet of Things,\nand processing at network edge are creating new opportunities for Cloud\ncomputing. However, they are also posing several new challenges and creating\nthe need for new approaches and research strategies, as well as the\nre-evaluation of the models that were developed to address issues such as\nscalability, elasticity, reliability, security, sustainability, and application\nmodels. The proposed manifesto addresses them by identifying the major open\nchallenges in Cloud computing, emerging trends, and impact areas. It then\noffers research directions for the next decade, thus helping in the realisation\nof Future Generation Cloud Computing. \n\n"}
{"id": "1711.09745", "contents": "Title: An edge-fog-cloud platform for anticipatory learning process designed\n  for Internet of Mobile Things Abstract: This paper presents a novel architecture for data analytics targeting an\nanticipatory learning process in the context of the Internet of Mobile Things.\nThe architecture is geo-distributed and composed by edge, fog, and cloud\nresources that operate collectively to support such an anticipatory learning\nprocess. We designed the architecture to manage large volumes of data streams\ncoming from the IoMT devices, analyze in successive phases climbing up in the\nhierarchy of resources from edge, fog and cloud. We discuss the characteristics\nof the analytical tasks at each layer. We notice that the amount of data being\ntransported in the network decreases going from the edge, to the fog and\nfinally to the cloud, while the complexity of the computation increases. Such\ndesign allows to support different kind of analytical needs, from real-time to\nhistorical according to the type of resource being utilized. We have\nimplemented the proposed architecture as a proof-of-concept using the transit\ndata feeds from the area of Greater Moncton, Canada. \n\n"}
{"id": "1711.09964", "contents": "Title: On the Approximability of Related Machine Scheduling under Arbitrary\n  Precedence Abstract: Distributed computing systems often need to consider the scheduling problem\ninvolving a collection of highly dependent data-processing tasks that must work\nin concert to achieve mission-critical objectives. This paper considers the\nunrelated machine scheduling problem for minimizing weighted sum completion\ntime under arbitrary precedence constraints and on heterogeneous machines with\ndifferent processing speeds. The problem is known to be strongly NP-hard even\nin the single machine setting. By making use of Queyranne's constraint set and\nconstructing a novel Linear Programming relaxation for the scheduling problem\nunder arbitrary precedence constraints, our results in this paper advance the\nstate of the art. We develop a $2(1+(m-1)/D)$-approximation algorithm (and\n$2(1+(m-1)/D)+1$-approximation) for the scheduling problem with zero release\ntime (and arbitrary release time), where $m$ is the number of servers and $D$\nis the task-skewness product. The algorithm can be efficiently computed in\npolynomial time using the Ellipsoid method and achieves nearly optimal\nperformance in practice as $D>O(m)$ when the number of tasks per job to\nschedule is sufficiently larger than the number of machines available. Our\nimplementation and evaluation using a heterogeneous testbed and real-world\nbenchmarks confirms significant improvement in weighted sum completion time for\ndependent computing tasks. \n\n"}
{"id": "1711.10102", "contents": "Title: A Game-theoretic Framework for Revenue Sharing in Edge-Cloud Computing\n  System Abstract: We introduce a game-theoretic framework to ex- plore revenue sharing in an\nEdge-Cloud computing system, in which computing service providers at the edge\nof the Internet (edge providers) and computing service providers at the cloud\n(cloud providers) co-exist and collectively provide computing resources to\nclients (e.g., end users or applications) at the edge. Different from\ntraditional cloud computing, the providers in an Edge-Cloud system are\nindependent and self-interested. To achieve high system-level efficiency, the\nmanager of the system adopts a task distribution mechanism to maximize the\ntotal revenue received from clients and also adopts a revenue sharing mechanism\nto split the received revenue among computing servers (and hence service\nproviders). Under those system-level mechanisms, service providers attempt to\ngame with the system in order to maximize their own utilities, by strategically\nallocating their resources (e.g., computing servers).\n  Our framework models the competition among the providers in an Edge-Cloud\nsystem as a non-cooperative game. Our simulations and experiments on an\nemulation system have shown the existence of Nash equilibrium in such a game.\nWe find that revenue sharing mechanisms have a significant impact on the\nsystem-level efficiency at Nash equilibria, and surprisingly the revenue\nsharing mechanism based directly on actual contributions can result in\nsignificantly worse system efficiency than Shapley value sharing mechanism and\nOrtmann proportional sharing mechanism. Our framework provides an effective\neconomics approach to understanding and designing efficient Edge-Cloud\ncomputing systems. \n\n"}
{"id": "1711.10204", "contents": "Title: Block Neural Network Avoids Catastrophic Forgetting When Learning\n  Multiple Task Abstract: In the present work we propose a Deep Feed Forward network architecture which\ncan be trained according to a sequential learning paradigm, where tasks of\nincreasing difficulty are learned sequentially, yet avoiding catastrophic\nforgetting. The proposed architecture can re-use the features learned on\nprevious tasks in a new task when the old tasks and the new one are related.\nThe architecture needs fewer computational resources (neurons and connections)\nand less data for learning the new task than a network trained from scratch \n\n"}
{"id": "1711.10400", "contents": "Title: Adversarial Networks for Prostate Cancer Detection Abstract: The large number of trainable parameters of deep neural networks renders them\ninherently data hungry. This characteristic heavily challenges the medical\nimaging community and to make things even worse, many imaging modalities are\nambiguous in nature leading to rater-dependant annotations that current loss\nformulations fail to capture. We propose employing adversarial training for\nsegmentation networks in order to alleviate aforementioned problems. We learn\nto segment aggressive prostate cancer utilizing challenging MRI images of 152\npatients and show that the proposed scheme is superior over the de facto\nstandard in terms of the detection sensitivity and the dice-score for\naggressive prostate cancer. The achieved relative gains are shown to be\nparticularly pronounced in the small dataset limit. \n\n"}
{"id": "1711.10783", "contents": "Title: Partial Consensus and Conservative Fusion of Gaussian Mixtures for\n  Distributed PHD Fusion Abstract: We propose a novel consensus notion, called \"partial consensus\", for\ndistributed GM-PHD (Gaussian mixture probability hypothesis density) fusion\nbased on a peer-to-peer (P2P) sensor network, in which only highly-weighted\nposterior Gaussian components (GCs) are disseminated in the P2P communication\nfor fusion while the insignificant GCs are not involved. The partial consensus\ndoes not only enjoy high efficiency in both network communication and local\nfusion computation, but also significantly reduces the affect of potential\nfalse data (clutter) to the filter, leading to increased signal-to-noise ratio\nat local sensors. Two \"conservative\" mixture reduction schemes are advocated\nfor fusing the shared GCs in a fully distributed manner. One is given by\npairwise averaging GCs between sensors based on Hungarian assignment and the\nother is merging close GCs based a new GM merging scheme. The proposed\napproaches have a close connection to the conservative fusion approaches known\nas covariance union and arithmetic mean density. In parallel, average consensus\nis sought on the cardinality distribution (namely the GM weight sum) among\nsensors. Simulations for tracking either a single target or multiple targets\nthat simultaneously appear are presented based on a sensor network where each\nsensor operates a GM-PHD filter, in order to compare our approaches with the\nbenchmark generalized covariance intersection approach. The results demonstrate\nthat the partial, arithmetic average, consensus outperforms the complete,\ngeometric average, consensus. \n\n"}
{"id": "1712.00605", "contents": "Title: Toward Reliable and Rapid Elasticity for Streaming Dataflows on Clouds Abstract: The pervasive availability of streaming data is driving interest in\ndistributed Fast Data platforms for streaming applications. Such\nlatency-sensitive applications need to respond to dynamism in the input rates\nand task behavior using scale-in and -out on elastic Cloud resources. Platforms\nlike Apache Storm do not provide robust capabilities for responding to such\ndynamism and for rapid task migration across VMs. We propose several dataflow\ncheckpoint and migration approaches that allow a running streaming dataflow to\nmigrate, without any loss of in-flight messages or their internal tasks states,\nwhile reducing the time to recover and stabilize. We implement and evaluate\nthese migration strategies on Apache Storm using micro and application\ndataflows for scaling in and out on up to 2-21 Azure VMs. Our results show that\nwe can migrate dataflows of large sizes within 50 sec, in comparison to Storm's\ndefault approach that takes over $100~sec$. We also find that our approaches\nstabilize the application much earlier and there is no failure and\nre-processing of messages. \n\n"}
{"id": "1712.02034", "contents": "Title: SMILES2Vec: An Interpretable General-Purpose Deep Neural Network for\n  Predicting Chemical Properties Abstract: Chemical databases store information in text representations, and the SMILES\nformat is a universal standard used in many cheminformatics software. Encoded\nin each SMILES string is structural information that can be used to predict\ncomplex chemical properties. In this work, we develop SMILES2vec, a deep RNN\nthat automatically learns features from SMILES to predict chemical properties,\nwithout the need for additional explicit feature engineering. Using Bayesian\noptimization methods to tune the network architecture, we show that an\noptimized SMILES2vec model can serve as a general-purpose neural network for\npredicting distinct chemical properties including toxicity, activity,\nsolubility and solvation energy, while also outperforming contemporary MLP\nneural networks that uses engineered features. Furthermore, we demonstrate\nproof-of-concept of interpretability by developing an explanation mask that\nlocalizes on the most important characters used in making a prediction. When\ntested on the solubility dataset, it identified specific parts of a chemical\nthat is consistent with established first-principles knowledge with an accuracy\nof 88%. Our work demonstrates that neural networks can learn technically\naccurate chemical concept and provide state-of-the-art accuracy, making\ninterpretable deep neural networks a useful tool of relevance to the chemical\nindustry. \n\n"}
{"id": "1712.02501", "contents": "Title: CNNs are Globally Optimal Given Multi-Layer Support Abstract: Stochastic Gradient Descent (SGD) is the central workhorse for training\nmodern CNNs. Although giving impressive empirical performance it can be slow to\nconverge. In this paper we explore a novel strategy for training a CNN using an\nalternation strategy that offers substantial speedups during training. We make\nthe following contributions: (i) replace the ReLU non-linearity within a CNN\nwith positive hard-thresholding, (ii) reinterpret this non-linearity as a\nbinary state vector making the entire CNN linear if the multi-layer support is\nknown, and (iii) demonstrate that under certain conditions a global optima to\nthe CNN can be found through local descent. We then employ a novel alternation\nstrategy (between weights and support) for CNN training that leads to\nsubstantially faster convergence rates, nice theoretical properties, and\nachieving state of the art results across large scale datasets (e.g. ImageNet)\nas well as other standard benchmarks. \n\n"}
{"id": "1712.02734", "contents": "Title: Using Rule-Based Labels for Weak Supervised Learning: A ChemNet for\n  Transferable Chemical Property Prediction Abstract: With access to large datasets, deep neural networks (DNN) have achieved\nhuman-level accuracy in image and speech recognition tasks. However, in\nchemistry, data is inherently small and fragmented. In this work, we develop an\napproach of using rule-based knowledge for training ChemNet, a transferable and\ngeneralizable deep neural network for chemical property prediction that learns\nin a weak-supervised manner from large unlabeled chemical databases. When\ncoupled with transfer learning approaches to predict other smaller datasets for\nchemical properties that it was not originally trained on, we show that\nChemNet's accuracy outperforms contemporary DNN models that were trained using\nconventional supervised learning. Furthermore, we demonstrate that the ChemNet\npre-training approach is equally effective on both CNN (Chemception) and RNN\n(SMILES2vec) models, indicating that this approach is network architecture\nagnostic and is effective across multiple data modalities. Our results indicate\na pre-trained ChemNet that incorporates chemistry domain knowledge, enables the\ndevelopment of generalizable neural networks for more accurate prediction of\nnovel chemical properties. \n\n"}
{"id": "1712.02779", "contents": "Title: Exploring the Landscape of Spatial Robustness Abstract: The study of adversarial robustness has so far largely focused on\nperturbations bound in p-norms. However, state-of-the-art models turn out to be\nalso vulnerable to other, more natural classes of perturbations such as\ntranslations and rotations. In this work, we thoroughly investigate the\nvulnerability of neural network--based classifiers to rotations and\ntranslations. While data augmentation offers relatively small robustness, we\nuse ideas from robust optimization and test-time input aggregation to\nsignificantly improve robustness. Finally we find that, in contrast to the\np-norm case, first-order methods cannot reliably find worst-case perturbations.\nThis highlights spatial robustness as a fundamentally different setting\nrequiring additional study. Code available at\nhttps://github.com/MadryLab/adversarial_spatial and\nhttps://github.com/MadryLab/spatial-pytorch. \n\n"}
{"id": "1712.03112", "contents": "Title: Effective Extensible Programming: Unleashing Julia on GPUs Abstract: GPUs and other accelerators are popular devices for accelerating\ncompute-intensive, parallelizable applications. However, programming these\ndevices is a difficult task. Writing efficient device code is challenging, and\nis typically done in a low-level programming language. High-level languages are\nrarely supported, or do not integrate with the rest of the high-level language\necosystem. To overcome this, we propose compiler infrastructure to efficiently\nadd support for new hardware or environments to an existing programming\nlanguage.\n  We evaluate our approach by adding support for NVIDIA GPUs to the Julia\nprogramming language. By integrating with the existing compiler, we\nsignificantly lower the cost to implement and maintain the new compiler, and\nfacilitate reuse of existing application code. Moreover, use of the high-level\nJulia programming language enables new and dynamic approaches for GPU\nprogramming. This greatly improves programmer productivity, while maintaining\napplication performance similar to that of the official NVIDIA CUDA toolkit. \n\n"}
{"id": "1712.03351", "contents": "Title: Peephole: Predicting Network Performance Before Training Abstract: The quest for performant networks has been a significant force that drives\nthe advancements of deep learning in recent years. While rewarding, improving\nnetwork design has never been an easy journey. The large design space combined\nwith the tremendous cost required for network training poses a major obstacle\nto this endeavor. In this work, we propose a new approach to this problem,\nnamely, predicting the performance of a network before training, based on its\narchitecture. Specifically, we develop a unified way to encode individual\nlayers into vectors and bring them together to form an integrated description\nvia LSTM. Taking advantage of the recurrent network's strong expressive power,\nthis method can reliably predict the performances of various network\narchitectures. Our empirical studies showed that it not only achieved accurate\npredictions but also produced consistent rankings across datasets -- a key\ndesideratum in performance prediction. \n\n"}
{"id": "1712.04170", "contents": "Title: Interpretable Policies for Reinforcement Learning by Genetic Programming Abstract: The search for interpretable reinforcement learning policies is of high\nacademic and industrial interest. Especially for industrial systems, domain\nexperts are more likely to deploy autonomously learned controllers if they are\nunderstandable and convenient to evaluate. Basic algebraic equations are\nsupposed to meet these requirements, as long as they are restricted to an\nadequate complexity. Here we introduce the genetic programming for\nreinforcement learning (GPRL) approach based on model-based batch reinforcement\nlearning and genetic programming, which autonomously learns policy equations\nfrom pre-existing default state-action trajectory samples. GPRL is compared to\na straight-forward method which utilizes genetic programming for symbolic\nregression, yielding policies imitating an existing well-performing, but\nnon-interpretable policy. Experiments on three reinforcement learning\nbenchmarks, i.e., mountain car, cart-pole balancing, and industrial benchmark,\ndemonstrate the superiority of our GPRL approach compared to the symbolic\nregression method. GPRL is capable of producing well-performing interpretable\nreinforcement learning policies from pre-existing default trajectory data. \n\n"}
{"id": "1712.05043", "contents": "Title: Evolving Unsupervised Deep Neural Networks for Learning Meaningful\n  Representations Abstract: Deep Learning (DL) aims at learning the \\emph{meaningful representations}. A\nmeaningful representation refers to the one that gives rise to significant\nperformance improvement of associated Machine Learning (ML) tasks by replacing\nthe raw data as the input. However, optimal architecture design and model\nparameter estimation in DL algorithms are widely considered to be intractable.\nEvolutionary algorithms are much preferable for complex and non-convex problems\ndue to its inherent characteristics of gradient-free and insensitivity to local\noptimum. In this paper, we propose a computationally economical algorithm for\nevolving \\emph{unsupervised deep neural networks} to efficiently learn\n\\emph{meaningful representations}, which is very suitable in the current Big\nData era where sufficient labeled data for training is often expensive to\nacquire. In the proposed algorithm, finding an appropriate architecture and the\ninitialized parameter values for a ML task at hand is modeled by one\ncomputational efficient gene encoding approach, which is employed to\neffectively model the task with a large number of parameters. In addition, a\nlocal search strategy is incorporated to facilitate the exploitation search for\nfurther improving the performance. Furthermore, a small proportion labeled data\nis utilized during evolution search to guarantee the learnt representations to\nbe meaningful. The performance of the proposed algorithm has been thoroughly\ninvestigated over classification tasks. Specifically, error classification rate\non MNIST with $1.15\\%$ is reached by the proposed algorithm consistently, which\nis a very promising result against state-of-the-art unsupervised DL algorithms. \n\n"}
{"id": "1712.05695", "contents": "Title: Lightweight Neural Networks Abstract: Most of the weights in a Lightweight Neural Network have a value of zero,\nwhile the remaining ones are either +1 or -1. These universal approximators\nrequire approximately 1.1 bits/weight of storage, posses a quick forward pass\nand achieve classification accuracies similar to conventional continuous-weight\nnetworks. Their training regimen focuses on error reduction initially, but\nlater emphasizes discretization of weights. They ignore insignificant inputs,\nremove unnecessary weights, and drop unneeded hidden neurons. We have\nsuccessfully tested them on the MNIST, credit card fraud, and credit card\ndefaults data sets using networks having 2 to 16 hidden layers and up to 4.4\nmillion weights. \n\n"}
{"id": "1712.06128", "contents": "Title: A Distributed Particle-PHD Filter with Arithmetic-Average PHD Fusion Abstract: We propose a particle-based distributed PHD filter for tracking an unknown,\ntime-varying number of targets. To reduce communication, the local PHD filters\nat neighboring sensors communicate Gaussian mixture (GM) parameters. In\ncontrast to most existing distributed PHD filters, our filter employs an\n`arithmetic average' fusion. For particles--GM conversion, we use a method that\navoids particle clustering and enables a significance-based pruning of the GM\ncomponents. For GM--particles conversion, we develop an importance sampling\nbased method that enables a parallelization of filtering and\ndissemination/fusion operations. The proposed distributed particle-PHD filter\nis able to integrate GM-based local PHD filters. Simulations demonstrate the\nexcellent performance and small communication and computation requirements of\nour filter. \n\n"}
{"id": "1712.08644", "contents": "Title: DeepPicar: A Low-cost Deep Neural Network-based Autonomous Car Abstract: We present DeepPicar, a low-cost deep neural network based autonomous car\nplatform. DeepPicar is a small scale replication of a real self-driving car\ncalled DAVE-2 by NVIDIA. DAVE-2 uses a deep convolutional neural network (CNN),\nwhich takes images from a front-facing camera as input and produces car\nsteering angles as output. DeepPicar uses the same network architecture---9\nlayers, 27 million connections and 250K parameters---and can drive itself in\nreal-time using a web camera and a Raspberry Pi 3 quad-core platform. Using\nDeepPicar, we analyze the Pi 3's computing capabilities to support end-to-end\ndeep learning based real-time control of autonomous vehicles. We also\nsystematically compare other contemporary embedded computing platforms using\nthe DeepPicar's CNN-based real-time control workload. We find that all tested\nplatforms, including the Pi 3, are capable of supporting the CNN-based\nreal-time control, from 20 Hz up to 100 Hz, depending on hardware platform.\nHowever, we find that shared resource contention remains an important issue\nthat must be considered in applying CNN models on shared memory based embedded\ncomputing platforms; we observe up to 11.6X execution time increase in the CNN\nbased control loop due to shared resource contention. To protect the CNN\nworkload, we also evaluate state-of-the-art cache partitioning and memory\nbandwidth throttling techniques on the Pi 3. We find that cache partitioning is\nineffective, while memory bandwidth throttling is an effective solution. \n\n"}
{"id": "1712.09381", "contents": "Title: RLlib: Abstractions for Distributed Reinforcement Learning Abstract: Reinforcement learning (RL) algorithms involve the deep nesting of highly\nirregular computation patterns, each of which typically exhibits opportunities\nfor distributed computation. We argue for distributing RL components in a\ncomposable way by adapting algorithms for top-down hierarchical control,\nthereby encapsulating parallelism and resource requirements within\nshort-running compute tasks. We demonstrate the benefits of this principle\nthrough RLlib: a library that provides scalable software primitives for RL.\nThese primitives enable a broad range of algorithms to be implemented with high\nperformance, scalability, and substantial code reuse. RLlib is available at\nhttps://rllib.io/. \n\n"}
{"id": "1712.09552", "contents": "Title: Big Data and Fog Computing Abstract: Fog computing serves as a computing layer that sits between the edge devices\nand the cloud in the network topology. They have more compute capacity than the\nedge but much less so than cloud data centers. They typically have high uptime\nand always-on Internet connectivity. Applications that make use of the fog can\navoid the network performance limitation of cloud computing while being less\nresource constrained than edge computing. As a result, they offer a useful\nbalance of the current paradigms. This article explores various aspects of fog\ncomputing in the context of big data. \n\n"}
{"id": "1712.09709", "contents": "Title: Report: Dynamic Eye Movement Matching and Visualization Tool in Neuro\n  Gesture Abstract: In the research of the impact of gestures using by a lecturer, one\nchallenging task is to infer the attention of a group of audiences. Two\nimportant measurements that can help infer the level of attention are eye\nmovement data and Electroencephalography (EEG) data. Under the fundamental\nassumption that a group of people would look at the same place if they all pay\nattention at the same time, we apply a method, \"Time Warp Edit Distance\", to\ncalculate the similarity of their eye movement trajectories. Moreover, we also\ncluster eye movement pattern of audiences based on these pair-wised similarity\nmetrics. Besides, since we don't have a direct metric for the \"attention\"\nground truth, a visual assessment would be beneficial to evaluate the\ngesture-attention relationship. Thus we also implement a visualization tool. \n\n"}
{"id": "1801.00062", "contents": "Title: Dendritic error backpropagation in deep cortical microcircuits Abstract: Animal behaviour depends on learning to associate sensory stimuli with the\ndesired motor command. Understanding how the brain orchestrates the necessary\nsynaptic modifications across different brain areas has remained a longstanding\npuzzle. Here, we introduce a multi-area neuronal network model in which\nsynaptic plasticity continuously adapts the network towards a global desired\noutput. In this model synaptic learning is driven by a local dendritic\nprediction error that arises from a failure to predict the top-down input given\nthe bottom-up activities. Such errors occur at apical dendrites of pyramidal\nneurons where both long-range excitatory feedback and local inhibitory\npredictions are integrated. When local inhibition fails to match excitatory\nfeedback an error occurs which triggers plasticity at bottom-up synapses at\nbasal dendrites of the same pyramidal neurons. We demonstrate the learning\ncapabilities of the model in a number of tasks and show that it approximates\nthe classical error backpropagation algorithm. Finally, complementing this\ncortical circuit with a disinhibitory mechanism enables attention-like stimulus\ndenoising and generation. Our framework makes several experimental predictions\non the function of dendritic integration and cortical microcircuits, is\nconsistent with recent observations of cross-area learning, and suggests a\nbiological implementation of deep learning. \n\n"}
{"id": "1801.00246", "contents": "Title: A GPU Accelerated Discontinuous Galerkin Incompressible Flow Solver Abstract: We present a GPU-accelerated version of a high-order discontinuous Galerkin\ndiscretization of the unsteady incompressible Navier-Stokes equations. The\nequations are discretized in time using a semi-implicit scheme with explicit\ntreatment of the nonlinear term and implicit treatment of the split Stokes\noperators. The pressure system is solved with a conjugate gradient method\ntogether with a fully GPU-accelerated multigrid preconditioner which is\ndesigned to minimize memory requirements and to increase overall performance. A\nsemi-Lagrangian subcycling advection algorithm is used to shift the\ncomputational load per timestep away from the pressure Poisson solve by\nallowing larger timestep sizes in exchange for an increased number of advection\nsteps. Numerical results confirm we achieve the design order accuracy in time\nand space. We optimize the performance of the most time-consuming kernels by\ntuning the fine-grain parallelism, memory utilization, and maximizing\nbandwidth. To assess overall performance we present an empirically calibrated\nroofline performance model for a target GPU to explain the achieved efficiency.\nWe demonstrate that, in the most cases, the kernels used in the solver are\nclose to their empirically predicted roofline performance. \n\n"}
{"id": "1801.01574", "contents": "Title: Testing Optimality of Sequential Decision-Making Abstract: This paper provides a statistical method to test whether a system that\nperforms a binary sequential hypothesis test is optimal in the sense of\nminimizing the average decision times while taking decisions with given\nreliabilities. The proposed method requires samples of the decision times, the\ndecision outcomes, and the true hypotheses, but does not require knowledge on\nthe statistics of the observations or the properties of the decision-making\nsystem. The method is based on fluctuation relations for decision time\ndistributions which are proved for sequential probability ratio tests. These\nrelations follow from the martingale property of probability ratios and hold\nunder fairly general conditions. We illustrate these tests with numerical\nexperiments and discuss potential applications. \n\n"}
{"id": "1801.01843", "contents": "Title: Design and Performance Characterization of RADICAL-Pilot on Titan Abstract: Many extreme scale scientific applications have workloads comprised of a\nlarge number of individual high-performance tasks. The Pilot abstraction\ndecouples workload specification, resource management, and task execution via\njob placeholders and late-binding. As such, suitable implementations of the\nPilot abstraction can support the collective execution of large number of tasks\non supercomputers. We introduce RADICAL-Pilot (RP) as a portable, modular and\nextensible Python-based Pilot system. We describe RP's design, architecture and\nimplementation. We characterize its performance and show its ability to\nscalably execute workloads comprised of thousands of MPI tasks on Titan--a DOE\nleadership class facility. Specifically, we investigate RP's weak (strong)\nscaling properties up to 131K (65K) cores and 4096 (16384) 32 core tasks.\nRADICAL-Pilot can be used stand-alone, as well as integrated with other tools\nas a runtime system. \n\n"}
{"id": "1801.02362", "contents": "Title: Acceleration of Mean Square Distance Calculations with Floating Close\n  Structure in Metadynamics Simulations Abstract: Molecular dynamics simulates the~movements of atoms. Due to its high cost,\nmany methods have been developed to \"push the~simulation forward\". One of them,\nmetadynamics, can hasten the~molecular dynamics with the~help of variables\ndescribing the~simulated process. However, the~evaluation of these variables\ncan include numerous mean square distance calculations that introduce\nsubstantial computational demands, thus jeopardize the~benefit of the~approach.\nRecently, we proposed an~approximative method that significantly reduces\nthe~number of these distance calculations. Here we evaluate the~performance and\nthe~scalability on two molecular systems. We assess the~maximal theoretical\nspeed-up based on the reduction of distance computations and Ahmdal's law and\ncompare it to the~practical speed-up achieved with our implementation. \n\n"}
{"id": "1801.02531", "contents": "Title: A Scale-out Blockchain for Value Transfer with Spontaneous Sharding Abstract: Bitcoin, as well as many of its successors, require the whole transaction\nrecord to be reliably acquired by all nodes to prevent double-spending.\nRecently, many blockchains have been proposed to achieve scale-out throughput\nby letting nodes only acquire a fraction of the whole transaction set. However,\nthese schemes, e.g., sharding and off-chain techniques, suffer from a\ndegradation in decentralization or the capacity of fault tolerance.\n  In this paper, we show that the complete set of transactions is not a\nnecessity for the prevention of double-spending if the properties of value\ntransfers is fully explored. In other words, we show that a value-transfer\nledger like Bitcoin has the potential to scale-out by its nature without\nsacrificing security or decentralization. Firstly, we give a formal definition\nfor the value-transfer ledger and its distinct features from a generic\ndatabase. Then, we introduce an off-chain based scheme with a shared main chain\nfor consensus and an individual chain for each node for recording transactions.\nA locally executable validation scheme is proposed with uncompromising validity\nand consistency. A beneficial consequence of our design is that nodes will\nspontaneously try to reduce their transmission cost by only providing the\ntransactions needed to show that their transactions are double-spending-proof.\nAs a result, the network is sharded as each node only acquires part of the\ntransaction record and a scale-out throughput could be achieved, which we call\n\"spontaneous sharding\". \n\n"}
{"id": "1801.02726", "contents": "Title: Near Maximum Likelihood Decoding with Deep Learning Abstract: A novel and efficient neural decoder algorithm is proposed. The proposed\ndecoder is based on the neural Belief Propagation algorithm and the\nAutomorphism Group. By combining neural belief propagation with permutations\nfrom the Automorphism Group we achieve near maximum likelihood performance for\nHigh Density Parity Check codes. Moreover, the proposed decoder significantly\nimproves the decoding complexity, compared to our earlier work on the topic. We\nalso investigate the training process and show how it can be accelerated.\nSimulations of the hessian and the condition number show why the learning\nprocess is accelerated. We demonstrate the decoding algorithm for various\nlinear block codes of length up to 63 bits. \n\n"}
{"id": "1801.03065", "contents": "Title: Multi-threaded Sparse Matrix-Matrix Multiplication for Many-Core and GPU\n  Architectures Abstract: Sparse Matrix-Matrix multiplication is a key kernel that has applications in\nseveral domains such as scientific computing and graph analysis. Several\nalgorithms have been studied in the past for this foundational kernel. In this\npaper, we develop parallel algorithms for sparse matrix-matrix multiplication\nwith a focus on performance portability across different high performance\ncomputing architectures. The performance of these algorithms depend on the data\nstructures used in them. We compare different types of accumulators in these\nalgorithms and demonstrate the performance difference between these data\nstructures. Furthermore, we develop a meta-algorithm, kkSpGEMM, to choose the\nright algorithm and data structure based on the characteristics of the problem.\nWe show performance comparisons on three architectures and demonstrate the need\nfor the community to develop two phase sparse matrix-matrix multiplication\nimplementations for efficient reuse of the data structures involved. \n\n"}
{"id": "1801.03855", "contents": "Title: MXNET-MPI: Embedding MPI parallelism in Parameter Server Task Model for\n  scaling Deep Learning Abstract: Existing Deep Learning frameworks exclusively use either Parameter Server(PS)\napproach or MPI parallelism. In this paper, we discuss the drawbacks of such\napproaches and propose a generic framework supporting both PS and MPI\nprogramming paradigms, co-existing at the same time. The key advantage of the\nnew model is to embed the scaling benefits of MPI parallelism into the loosely\ncoupled PS task model. Apart from providing a practical usage model of MPI in\ncloud, such framework allows for novel communication avoiding algorithms that\ndo parameter averaging in Stochastic Gradient Descent(SGD) approaches. We show\nhow MPI and PS models can synergestically apply algorithms such as Elastic SGD\nto improve the rate of convergence against existing approaches. These new\nalgorithms directly help scaling SGD clusterwide. Further, we also optimize the\ncritical component of the framework, namely global aggregation or allreduce\nusing a novel concept of tensor collectives. These treat a group of vectors on\na node as a single object allowing for the existing single vector algorithms to\nbe directly applicable. We back our claims with sufficient emperical evidence\nusing large scale ImageNet 1K data. Our framework is built upon MXNET but the\ndesign is generic and can be adapted to other popular DL infrastructures. \n\n"}
{"id": "1801.04179", "contents": "Title: Arhuaco: Deep Learning and Isolation Based Security for Distributed\n  High-Throughput Computing Abstract: Grid computing systems require innovative methods and tools to identify\ncybersecurity incidents and perform autonomous actions i.e. without\nadministrator intervention. They also require methods to isolate and trace job\npayload activity in order to protect users and find evidence of malicious\nbehavior. We introduce an integrated approach of security monitoring via\nSecurity by Isolation with Linux Containers and Deep Learning methods for the\nanalysis of real time data in Grid jobs running inside virtualized\nHigh-Throughput Computing infrastructure in order to detect and prevent\nintrusions. A dataset for malware detection in Grid computing is described. We\nshow in addition the utilization of generative methods with Recurrent Neural\nNetworks to improve the collected dataset. We present Arhuaco, a prototype\nimplementation of the proposed methods. We empirically study the performance of\nour technique. The results show that Arhuaco outperforms other methods used in\nIntrusion Detection Systems for Grid Computing. The study is carried out in the\nALICE Collaboration Grid, part of the Worldwide LHC Computing Grid. \n\n"}
{"id": "1801.04487", "contents": "Title: Better Runtime Guarantees Via Stochastic Domination Abstract: Apart from few exceptions, the mathematical runtime analysis of evolutionary\nalgorithms is mostly concerned with expected runtimes. In this work, we argue\nthat stochastic domination is a notion that should be used more frequently in\nthis area. Stochastic domination allows to formulate much more informative\nperformance guarantees, it allows to decouple the algorithm analysis into the\ntrue algorithmic part of detecting a domination statement and the\nprobability-theoretical part of deriving the desired probabilistic guarantees\nfrom this statement, and it helps finding simpler and more natural proofs. As\nparticular results, we prove a fitness level theorem which shows that the\nruntime is dominated by a sum of independent geometric random variables, we\nprove the first tail bounds for several classic runtime problems, and we give a\nshort and natural proof for Witt's result that the runtime of any $(\\mu,p)$\nmutation-based algorithm on any function with unique optimum is subdominated by\nthe runtime of a variant of the \\oea on the \\onemax function. As side-products,\nwe determine the fastest unbiased (1+1) algorithm for the \\leadingones\nbenchmark problem, both in the general case and when restricted to static\nmutation operators, and we prove a Chernoff-type tail bound for sums of\nindependent coupon collector distributions. \n\n"}
{"id": "1801.04686", "contents": "Title: Hierarchical Coding for Distributed Computing Abstract: Coding for distributed computing supports low-latency computation by\nrelieving the burden of straggling workers. While most existing works assume a\nsimple master-worker model, we consider a hierarchical computational structure\nconsisting of groups of workers, motivated by the need to reflect the\narchitectures of real-world distributed computing systems. In this work, we\npropose a hierarchical coding scheme for this model, as well as analyze its\ndecoding cost and expected computation time. Specifically, we first provide\nupper and lower bounds on the expected computing time of the proposed scheme.\nWe also show that our scheme enables efficient parallel decoding, thus reducing\ndecoding costs by orders of magnitude over non-hierarchical schemes. When\nconsidering both decoding cost and computing time, the proposed hierarchical\ncoding is shown to outperform existing schemes in many practical scenarios. \n\n"}
{"id": "1801.05522", "contents": "Title: Coded Computing for Distributed Graph Analytics Abstract: Performance of distributed graph processing systems significantly suffers\nfrom 'communication bottleneck' as a large number of messages are exchanged\namong servers at each step of the computation. Motivated by graph based\nMapReduce, we propose a coded computing framework that leverages computation\nredundancy to alleviate the communication bottleneck in distributed graph\nprocessing. We develop a novel 'coding' scheme that systematically injects\nstructured redundancy in computation phase to enable 'coded' multicasting\nopportunities during message exchange between servers, reducing communication\nload substantially in large-scale graph processing. For theoretical analysis,\nwe consider random graph models, and prove that our proposed scheme enables an\n(asymptotically) inverse-linear trade-off between 'computation load' and\n'average communication load' for two popular random graph models -- Erdos-Renyi\nmodel, and power law model. Particularly, for a given computation load r, (i.e.\nwhen each graph vertex is carefully stored at r servers), the proposed scheme\nslashes the average communication load by (nearly) a multiplicative factor of\nr. For the Erdos-Renyi model, our proposed scheme is optimal asymptotically as\nthe graph size increases by providing an information-theoretic converse. To\nillustrate the benefits of our scheme in practice, we implement PageRank over\nAmazon EC2, using artificial as well as real-world datasets, demonstrating\nsignificant gains over conventional PageRank. We also specialize our scheme and\nextend our theoretical results to two other random graph models -- random\nbi-partite model, and stochastic block model. They asymptotically enable\ninverse-linear trade-offs between computation and communication loads in\ndistributed graph processing for these popular random graph models as well. We\ncomplement the achievability results with converse bounds for both of these\nmodels. \n\n"}
{"id": "1801.05868", "contents": "Title: Joint Service Caching and Task Offloading for Mobile Edge Computing in\n  Dense Networks Abstract: Mobile Edge Computing (MEC) pushes computing functionalities away from the\ncentralized cloud to the network edge, thereby meeting the latency requirements\nof many emerging mobile applications and saving backhaul network bandwidth.\nAlthough many existing works have studied computation offloading policies,\nservice caching is an equally, if not more important, design topic of MEC, yet\nreceives much less attention. Service caching refers to caching application\nservices and their related databases/libraries in the edge server (e.g.\nMEC-enabled BS), thereby enabling corresponding computation tasks to be\nexecuted. Because only a small number of application services can be cached in\nresource-limited edge server at the same time, which services to cache has to\nbe judiciously decided to maximize the edge computing performance. In this\npaper, we investigate the extremely compelling but much less studied problem of\ndynamic service caching in MEC-enabled dense cellular networks. We propose an\nefficient online algorithm, called OREO, which jointly optimizes dynamic\nservice caching and task offloading to address a number of key challenges in\nMEC systems, including service heterogeneity, unknown system dynamics, spatial\ndemand coupling and decentralized coordination. Our algorithm is developed\nbased on Lyapunov optimization and Gibbs sampling, works online without\nrequiring future information, and achieves provable close-to-optimal\nperformance. Simulation results show that our algorithm can effectively reduce\ncomputation latency for end users while keeping energy consumption low. \n\n"}
{"id": "1801.06733", "contents": "Title: Probabilistic Tools for the Analysis of Randomized Optimization\n  Heuristics Abstract: This chapter collects several probabilistic tools that proved to be useful in\nthe analysis of randomized search heuristics. This includes classic material\nlike Markov, Chebyshev and Chernoff inequalities, but also lesser known topics\nlike stochastic domination and coupling or Chernoff bounds for geometrically\ndistributed random variables and for negatively correlated random variables.\nMost of the results presented here have appeared previously, some, however,\nonly in recent conference publications. While the focus is on collecting tools\nfor the analysis of randomized search heuristics, many of these may be useful\nas well in the analysis of classic randomized algorithms or discrete random\nstructures. \n\n"}
{"id": "1801.07353", "contents": "Title: Flexible Deep Neural Network Processing Abstract: The recent success of Deep Neural Networks (DNNs) has drastically improved\nthe state of the art for many application domains. While achieving high\naccuracy performance, deploying state-of-the-art DNNs is a challenge since they\ntypically require billions of expensive arithmetic computations. In addition,\nDNNs are typically deployed in ensemble to boost accuracy performance, which\nfurther exacerbates the system requirements. This computational overhead is an\nissue for many platforms, e.g. data centers and embedded systems, with tight\nlatency and energy budgets. In this article, we introduce flexible DNNs\nensemble processing technique, which achieves large reduction in average\ninference latency while incurring small to negligible accuracy drop. Our\ntechnique is flexible in that it allows for dynamic adaptation between quality\nof results (QoR) and execution runtime. We demonstrate the effectiveness of the\ntechnique on AlexNet and ResNet-50 using the ImageNet dataset. This technique\ncan also easily handle other types of networks. \n\n"}
{"id": "1801.08030", "contents": "Title: On Scale-out Deep Learning Training for Cloud and HPC Abstract: The exponential growth in use of large deep neural networks has accelerated\nthe need for training these deep neural networks in hours or even minutes. This\ncan only be achieved through scalable and efficient distributed training, since\na single node/card cannot satisfy the compute, memory, and I/O requirements of\ntoday's state-of-the-art deep neural networks. However, scaling synchronous\nStochastic Gradient Descent (SGD) is still a challenging problem and requires\ncontinued research/development. This entails innovations spanning algorithms,\nframeworks, communication libraries, and system design. In this paper, we\ndescribe the philosophy, design, and implementation of Intel Machine Learning\nScalability Library (MLSL) and present proof-points demonstrating scaling DL\ntraining on 100s to 1000s of nodes across Cloud and HPC systems. \n\n"}
{"id": "1801.08873", "contents": "Title: Mirrored and Hybrid Disk Arrays: Organization, Scheduling, Reliability,\n  and Performance Abstract: Basic mirroring (BM) classified as RAID level 1 replicates data on two disks,\nthus doubling disk access bandwidth for read requests. RAID1/0 is an array of\nBM pairs with balanced loads due to striping. When a disk fails the read load\non its pair is doubled, which results in halving the maximum attainable\nbandwidth. We review RAID1 organizations which attain a balanced load upon disk\nfailure, but as shown by reliability analysis tend to be less reliable than\nRAID1/0. Hybrid disk arrays which store XORed instead of replicated data tend\nto have a higher reliability than mirrored disks, but incur a higher overhead\nin updating data. Read request response time can be improved by processing them\nat a higher priority than writes, since they have a direct effect on\napplication response time. Shortest seek distance and affinity based routing\nboth shorten seek time. Anticipatory arm placement places arms optimally to\nminimize the seek distance. The analysis of RAID1 in normal, degraded, and\nrebuild mode is provided to quantify RAID1/0 performance. We compare the\nreliability of mirrored disk organizations against each other and hybrid disks\nand erasure coded disk arrays. \n\n"}
{"id": "1801.09335", "contents": "Title: Stochastic Downsampling for Cost-Adjustable Inference and Improved\n  Regularization in Convolutional Networks Abstract: It is desirable to train convolutional networks (CNNs) to run more\nefficiently during inference. In many cases however, the computational budget\nthat the system has for inference cannot be known beforehand during training,\nor the inference budget is dependent on the changing real-time resource\navailability. Thus, it is inadequate to train just inference-efficient CNNs,\nwhose inference costs are not adjustable and cannot adapt to varied inference\nbudgets. We propose a novel approach for cost-adjustable inference in CNNs -\nStochastic Downsampling Point (SDPoint). During training, SDPoint applies\nfeature map downsampling to a random point in the layer hierarchy, with a\nrandom downsampling ratio. The different stochastic downsampling configurations\nknown as SDPoint instances (of the same model) have computational costs\ndifferent from each other, while being trained to minimize the same prediction\nloss. Sharing network parameters across different instances provides\nsignificant regularization boost. During inference, one may handpick a SDPoint\ninstance that best fits the inference budget. The effectiveness of SDPoint, as\nboth a cost-adjustable inference approach and a regularizer, is validated\nthrough extensive experiments on image classification. \n\n"}
{"id": "1801.09802", "contents": "Title: Automatically Leveraging MapReduce Frameworks for Data-Intensive\n  Applications Abstract: MapReduce is a popular programming paradigm for developing large-scale,\ndata-intensive computation. Many frameworks that implement this paradigm have\nrecently been developed. To leverage these frameworks, however, developers must\nbecome familiar with their APIs and rewrite existing code. Casper is a new tool\nthat automatically translates sequential Java programs into the MapReduce\nparadigm. Casper identifies potential code fragments to rewrite and translates\nthem in two steps: (1) Casper uses program synthesis to search for a program\nsummary (i.e., a functional specification) of each code fragment. The summary\nis expressed using a high-level intermediate language resembling the MapReduce\nparadigm and verified to be semantically equivalent to the original using a\ntheorem prover. (2) Casper generates executable code from the summary, using\neither the Hadoop, Spark, or Flink API. We evaluated Casper by automatically\nconverting real-world, sequential Java benchmarks to MapReduce. The resulting\nbenchmarks perform up to 48.2x faster compared to the original. \n\n"}
{"id": "1801.10292", "contents": "Title: On the Optimal Recovery Threshold of Coded Matrix Multiplication Abstract: We provide novel coded computation strategies for distributed matrix-matrix\nproducts that outperform the recent \"Polynomial code\" constructions in recovery\nthreshold, i.e., the required number of successful workers. When $m$-th\nfraction of each matrix can be stored in each worker node, Polynomial codes\nrequire $m^2$ successful workers, while our MatDot codes only require $2m-1$\nsuccessful workers, albeit at a higher communication cost from each worker to\nthe fusion node. We also provide a systematic construction of MatDot codes.\nFurther, we propose \"PolyDot\" coding that interpolates between Polynomial codes\nand MatDot codes to trade off communication cost and recovery threshold.\nFinally, we demonstrate a coding technique for multiplying $n$ matrices ($n\n\\geq 3$) by applying MatDot and PolyDot coding ideas. \n\n"}
{"id": "1801.10556", "contents": "Title: D2.3 Power models, energy models and libraries for energy-efficient\n  concurrent data structures and algorithms Abstract: This deliverable reports the results of the power models, energy models and\nlibraries for energy-efficient concurrent data structures and algorithms as\navailable by project month 30 of Work Package 2 (WP2). It reports i) the latest\nresults of Task 2.2-2.4 on providing programming abstractions and libraries for\ndeveloping energy-efficient data structures and algorithms and ii) the improved\nresults of Task 2.1 on investigating and modeling the trade-off between energy\nand performance of concurrent data structures and algorithms. The work has been\nconducted on two main EXCESS platforms: Intel platforms with recent Intel\nmulticore CPUs and Movidius Myriad platforms. \n\n"}
{"id": "1802.03160", "contents": "Title: Distributed Spanner Approximation Abstract: We address the fundamental network design problem of constructing approximate\nminimum spanners. Our contributions are for the distributed setting, providing\nboth algorithmic and hardness results.\n  Our main hardness result shows that an $\\alpha$-approximation for the minimum\ndirected $k$-spanner problem for $k \\geq 5$ requires $\\Omega(n\n/\\sqrt{\\alpha}\\log{n})$ rounds using deterministic algorithms or\n$\\Omega(\\sqrt{n }/\\sqrt{\\alpha}\\log{n})$ rounds using randomized ones, in the\nCONGEST model of distributed computing. Combined with the constant-round\n$O(n^{\\epsilon})$-approximation algorithm in the LOCAL model of [Barenboim,\nElkin and Gavoille, 2016], as well as a polylog-round\n$(1+\\epsilon)$-approximation algorithm in the LOCAL model that we show here,\nour lower bounds for the CONGEST model imply a strict separation between the\nLOCAL and CONGEST models. Notably, to the best of our knowledge, this is the\nfirst separation between these models for a local approximation problem.\n  Similarly, a separation between the directed and undirected cases is implied.\nWe also prove a nearly-linear lower bound for the minimum weighted $k$-spanner\nproblem for $k \\geq 4$, and we show lower bounds for the weighted 2-spanner\nproblem.\n  On the algorithmic side, apart from the aforementioned\n$(1+\\epsilon)$-approximation algorithm for minimum $k$-spanners, our main\ncontribution is a new distributed construction of minimum 2-spanners that uses\nonly polynomial local computations. Our algorithm has a guaranteed\napproximation ratio of $O(\\log(m/n))$ for a graph with $n$ vertices and $m$\nedges, which matches the best known ratio for polynomial time sequential\nalgorithms [Kortsarz and Peleg, 1994], and is tight if we restrict ourselves to\npolynomial local computations. Our approach allows us to extend our algorithm\nto work also for the directed, weighted, and client-server variants of the\nproblem. \n\n"}
{"id": "1802.03209", "contents": "Title: Drift Theory in Continuous Search Spaces: Expected Hitting Time of the\n  (1+1)-ES with 1/5 Success Rule Abstract: This paper explores the use of the standard approach for proving runtime\nbounds in discrete domains---often referred to as drift analysis---in the\ncontext of optimization on a continuous domain. Using this framework we analyze\nthe (1+1) Evolution Strategy with one-fifth success rule on the sphere\nfunction. To deal with potential functions that are not lower-bounded, we\nformulate novel drift theorems. We then use the theorems to prove bounds on the\nexpected hitting time to reach a certain target fitness in finite dimension\n$d$. The bounds are akin to linear convergence. We then study the dependency of\nthe different terms on $d$ proving a convergence rate dependency of\n$\\Theta(1/d)$. Our results constitute the first non-asymptotic analysis for the\nalgorithm considered as well as the first explicit application of drift\nanalysis to a randomized search heuristic with continuous domain. \n\n"}
{"id": "1802.03318", "contents": "Title: Nature vs. Nurture: The Role of Environmental Resources in Evolutionary\n  Deep Intelligence Abstract: Evolutionary deep intelligence synthesizes highly efficient deep neural\nnetworks architectures over successive generations. Inspired by the nature\nversus nurture debate, we propose a study to examine the role of external\nfactors on the network synthesis process by varying the availability of\nsimulated environmental resources. Experimental results were obtained for\nnetworks synthesized via asexual evolutionary synthesis (1-parent) and sexual\nevolutionary synthesis (2-parent, 3-parent, and 5-parent) using a 10% subset of\nthe MNIST dataset. Results show that a lower environmental factor model\nresulted in a more gradual loss in performance accuracy and decrease in storage\nsize. This potentially allows significantly reduced storage size with minimal\nto no drop in performance accuracy, and the best networks were synthesized\nusing the lowest environmental factor models. \n\n"}
{"id": "1802.03480", "contents": "Title: GraphVAE: Towards Generation of Small Graphs Using Variational\n  Autoencoders Abstract: Deep learning on graphs has become a popular research topic with many\napplications. However, past work has concentrated on learning graph embedding\ntasks, which is in contrast with advances in generative models for images and\ntext. Is it possible to transfer this progress to the domain of graphs? We\npropose to sidestep hurdles associated with linearization of such discrete\nstructures by having a decoder output a probabilistic fully-connected graph of\na predefined maximum size directly at once. Our method is formulated as a\nvariational autoencoder. We evaluate on the challenging task of molecule\ngeneration. \n\n"}
{"id": "1802.03505", "contents": "Title: Coulomb Autoencoders Abstract: Learning the true density in high-dimensional feature spaces is a well-known\nproblem in machine learning. In this work, we consider generative autoencoders\nbased on maximum-mean discrepancy (MMD) and provide theoretical insights. In\nparticular, (i) we prove that MMD coupled with Coulomb kernels has optimal\nconvergence properties, which are similar to convex functionals, thus improving\nthe training of autoencoders, and (ii) we provide a probabilistic bound on the\ngeneralization performance, highlighting some fundamental conditions to achieve\nbetter generalization. We validate the theory on synthetic examples and on the\npopular dataset of celebrities' faces, showing that our model, called Coulomb\nautoencoders, outperform the state-of-the-art. \n\n"}
{"id": "1802.04789", "contents": "Title: Sparse Matrix Multiplication and Triangle Listing in the Congested\n  Clique Model Abstract: We multiply two $n \\times n$ matrices $S,T$ over semirings in the Congested\nClique model, where $n$ fully connected nodes communicate synchronously using\n$O(\\log n)$-bit messages, within $O(nz(S)^{1/3} nz(T)^{1/3}/n + 1)$ rounds of\ncommunication, where $nz(A)$ denotes the number of non-zero elements in a\nmatrix $A$. By leveraging the sparsity of the input matrices, our algorithm\ngreatly reduces communication compared with general algorithms [Censor-Hillel\net al., PODC 2015], improving upon the state-of-the-art for matrices with\n$o(n^2)$ non-zero elements. Our algorithm exhibits the additional strength of\nsurpassing previous solutions also when only one matrix is sparse. This allows\nefficiently raising a sparse matrix to a power greater than 2. As applications,\nwe speed up 4-cycle counting and APSP in sparse graphs.\n  Our algorithmic contribution is a new \\emph{deterministic} method of\nrestructuring the input matrices in a sparsity-aware manner, which assigns each\nnode with element-wise multiplication tasks that are not necessarily\nconsecutive but are balanced, yielding communication-efficient multiplication.\n  Moreover, this new deterministic method for restructuring matrices may be\nused to restructure the adjacency matrix of input graphs, enabling faster\nsolutions for graph related problems. As an example, we present a new\ndeterministic algorithm which solves the triangle listing problem in\n$O(m/n^{5/3} + 1)$ rounds, a complexity that was previously obtained by a\n\\emph{randomized} algorithm [Pandurangan et al., SPAA 2018] and matches the\nlower bound of $\\tilde{\\Omega}(n^{1/3})$ when $m=n^2$ of [Izumi and Le Gall,\nPODC 2017, Pandurangan et al., SPAA 2018].\n  Our triangle listing algorithm implies triangle counting with the same\ncomplexity of $O(m/n^{5/3} + 1)$ rounds, which is a \\emph{cubic} improvement\nover the previous $O(m^2/n^3)$-round algorithm [Dolev et al., DISC 2012]. \n\n"}
{"id": "1802.04819", "contents": "Title: SLAQ: Quality-Driven Scheduling for Distributed Machine Learning Abstract: Training machine learning (ML) models with large datasets can incur\nsignificant resource contention on shared clusters. This training typically\ninvolves many iterations that continually improve the quality of the model. Yet\nin exploratory settings, better models can be obtained faster by directing\nresources to jobs with the most potential for improvement. We describe SLAQ, a\ncluster scheduling system for approximate ML training jobs that aims to\nmaximize the overall job quality. When allocating cluster resources, SLAQ\nexplores the quality-runtime trade-offs across multiple jobs to maximize\nsystem-wide quality improvement. To do so, SLAQ leverages the iterative nature\nof ML training algorithms, by collecting quality and resource usage information\nfrom concurrent jobs, and then generating highly-tailored quality-improvement\npredictions for future iterations. Experiments show that SLAQ achieves an\naverage quality improvement of up to 73% and an average delay reduction of up\nto 44% on a large set of ML training jobs, compared to resource fairness\nschedulers. \n\n"}
{"id": "1802.04924", "contents": "Title: Exploring Hidden Dimensions in Parallelizing Convolutional Neural\n  Networks Abstract: The past few years have witnessed growth in the computational requirements\nfor training deep convolutional neural networks. Current approaches parallelize\ntraining onto multiple devices by applying a single parallelization strategy\n(e.g., data or model parallelism) to all layers in a network. Although easy to\nreason about, these approaches result in suboptimal runtime performance in\nlarge-scale distributed training, since different layers in a network may\nprefer different parallelization strategies. In this paper, we propose\nlayer-wise parallelism that allows each layer in a network to use an individual\nparallelization strategy. We jointly optimize how each layer is parallelized by\nsolving a graph search problem. Our evaluation shows that layer-wise\nparallelism outperforms state-of-the-art approaches by increasing training\nthroughput, reducing communication costs, achieving better scalability to\nmultiple GPUs, while maintaining original network accuracy. \n\n"}
{"id": "1802.05324", "contents": "Title: Advancing System Performance with Redundancy: From Biological to\n  Artificial Designs Abstract: Redundancy is a fundamental characteristic of many biological processes such\nas those in the genetic, visual, muscular and nervous system; yet its function\nhas not been fully understood. The conventional interpretation of redundancy is\nthat it serves as a fault-tolerance mechanism, which leads to redundancy's de\nfacto application in man-made systems for reliability enhancement. On the\ncontrary, our previous works have demonstrated an example where redundancy can\nbe engineered solely for enhancing other aspects of the system, namely accuracy\nand precision. This design was inspired by the binocular structure of the human\nvision which we believe may share a similar operation. In this paper, we\npresent a unified theory describing how such utilization of redundancy is\nfeasible through two complementary mechanisms: representational redundancy\n(RPR) and entangled redundancy (ETR). Besides the previous works, we point out\ntwo additional examples where our new understanding of redundancy can be\napplied to justify a system's superior performance. One is the human\nmusculoskeletal system (HMS) - a biological instance, and one is the deep\nresidual neural network (ResNet) - an artificial counterpart. We envision that\nour theory would provide a framework for the future development of bio-inspired\nredundant artificial systems as well as assist the studies of the fundamental\nmechanisms governing various biological processes. \n\n"}
{"id": "1802.06488", "contents": "Title: Tiny SSD: A Tiny Single-shot Detection Deep Convolutional Neural Network\n  for Real-time Embedded Object Detection Abstract: Object detection is a major challenge in computer vision, involving both\nobject classification and object localization within a scene. While deep neural\nnetworks have been shown in recent years to yield very powerful techniques for\ntackling the challenge of object detection, one of the biggest challenges with\nenabling such object detection networks for widespread deployment on embedded\ndevices is high computational and memory requirements. Recently, there has been\nan increasing focus in exploring small deep neural network architectures for\nobject detection that are more suitable for embedded devices, such as Tiny YOLO\nand SqueezeDet. Inspired by the efficiency of the Fire microarchitecture\nintroduced in SqueezeNet and the object detection performance of the\nsingle-shot detection macroarchitecture introduced in SSD, this paper\nintroduces Tiny SSD, a single-shot detection deep convolutional neural network\nfor real-time embedded object detection that is composed of a highly optimized,\nnon-uniform Fire sub-network stack and a non-uniform sub-network stack of\nhighly optimized SSD-based auxiliary convolutional feature layers designed\nspecifically to minimize model size while maintaining object detection\nperformance. The resulting Tiny SSD possess a model size of 2.3MB (~26X smaller\nthan Tiny YOLO) while still achieving an mAP of 61.3% on VOC 2007 (~4.2% higher\nthan Tiny YOLO). These experimental results show that very small deep neural\nnetwork architectures can be designed for real-time object detection that are\nwell-suited for embedded scenarios. \n\n"}
{"id": "1802.06686", "contents": "Title: On Local Distributed Sampling and Counting Abstract: In classic distributed graph problems, each instance on a graph specifies a\nspace of feasible solutions (e.g. all proper ($\\Delta+1$)-list-colorings of the\ngraph), and the task of distributed algorithm is to construct a feasible\nsolution using local information.\n  We study distributed sampling and counting problems, in which each instance\nspecifies a joint distribution of feasible solutions. The task of distributed\nalgorithm is to sample from this joint distribution, or to locally measure the\nvolume of the probability space via the marginal probabilities. The latter task\nis also known as inference, which is a local counterpart of counting.\n  For self-reducible classes of instances, the following equivalences are\nestablished in the LOCAL model up to polylogarithmic factors:\n  $\\bullet$ For all joint distributions, approximate inference and approximate\nsampling are computationally equivalent.\n  $\\bullet$ For all joint distributions defined by local constraints, exact\nsampling is reducible to either one of the above tasks.\n  $\\bullet$ If further, sequentially constructing a feasible solution is\ntrivial locally, then all above tasks are easy if and only if the joint\ndistribution exhibits strong spatial mixing.\n  Combining with the state of the arts of strong spatial mixing, we obtain\nefficient sampling algorithms in the LOCAL model for various important sampling\nproblems, including: an $O(\\sqrt{\\Delta}\\log^3n)$-round algorithm for exact\nsampling matchings in graphs with maximum degree $\\Delta$, and an\n$O(\\log^3n)$-round algorithm for sampling according to the hardcore model\n(weighted independent sets) in the uniqueness regime, which along with the\n$\\Omega(\\mathrm{diam})$ lower bound in arXiv:1702.00142 for sampling according\nto the hardcore model in the non-uniqueness regime, gives the first\ncomputational phase transition for distributed sampling. \n\n"}
{"id": "1802.06949", "contents": "Title: Efficient Embedding of MPI Collectives in MXNET DAGs for scaling Deep\n  Learning Abstract: Availability of high performance computing infrastructures such as clusters\nof GPUs and CPUs have fueled the growth of distributed learning systems. Deep\nLearning frameworks express neural nets as DAGs and execute these DAGs on\ncomputation resources such as GPUs. In this paper, we propose efficient designs\nof embedding MPI collective operations into data parallel DAGs. Incorrect\ndesigns can easily lead to deadlocks or program crashes. In particular, we\ndemonstrate three designs: Funneled, Concurrent communication and Dependency\nchaining of using MPI collectives with DAGs. These designs automatically enable\noverlap of computation with communication by allowing for concurrent execution\nwith the other tasks. We directly implement these designs into the KVStore API\nof the MXNET. This allows us to directly leverage the rest of the\ninfrastructure. Using ImageNet and CIFAR data sets, we show the potential of\nour designs. In particular, our designs scale to 256 GPUs with as low as 50\nseconds of epoch times for ImageNet 1K datasets. \n\n"}
{"id": "1802.07008", "contents": "Title: Segmentation hi\\'erarchique faiblement supervis\\'ee Abstract: Image segmentation is the process of partitioning an image into a set of\nmeaningful regions according to some criteria. Hierarchical segmentation has\nemerged as a major trend in this regard as it favors the emergence of important\nregions at different scales. On the other hand, many methods allow us to have\nprior information on the position of structures of interest in the images. In\nthis paper, we present a versatile hierarchical segmentation method that takes\ninto account any prior spatial information and outputs a hierarchical\nsegmentation that emphasizes the contours or regions of interest while\npreserving the important structures in the image. An application of this method\nto the weakly-supervised segmentation problem is presented. \n\n"}
{"id": "1802.07034", "contents": "Title: Memetic Graph Clustering Abstract: It is common knowledge that there is no single best strategy for graph\nclustering, which justifies a plethora of existing approaches. In this paper,\nwe present a general memetic algorithm, VieClus, to tackle the graph clustering\nproblem. This algorithm can be adapted to optimize different objective\nfunctions. A key component of our contribution are natural recombine operators\nthat employ ensemble clusterings as well as multi-level techniques. Lastly, we\ncombine these techniques with a scalable communication protocol, producing a\nsystem that is able to compute high-quality solutions in a short amount of\ntime. We instantiate our scheme with local search for modularity and show that\nour algorithm successfully improves or reproduces all entries of the 10th\nDIMACS implementation~challenge under consideration using a small amount of\ntime. \n\n"}
{"id": "1802.07089", "contents": "Title: Attentive Tensor Product Learning Abstract: This paper proposes a new architecture - Attentive Tensor Product Learning\n(ATPL) - to represent grammatical structures in deep learning models. ATPL is a\nnew architecture to bridge this gap by exploiting Tensor Product\nRepresentations (TPR), a structured neural-symbolic model developed in\ncognitive science, aiming to integrate deep learning with explicit language\nstructures and rules. The key ideas of ATPL are: 1) unsupervised learning of\nrole-unbinding vectors of words via TPR-based deep neural network; 2) employing\nattention modules to compute TPR; and 3) integration of TPR with typical deep\nlearning architectures including Long Short-Term Memory (LSTM) and Feedforward\nNeural Network (FFNN). The novelty of our approach lies in its ability to\nextract the grammatical structure of a sentence by using role-unbinding\nvectors, which are obtained in an unsupervised manner. This ATPL approach is\napplied to 1) image captioning, 2) part of speech (POS) tagging, and 3)\nconstituency parsing of a sentence. Experimental results demonstrate the\neffectiveness of the proposed approach. \n\n"}
{"id": "1802.07133", "contents": "Title: Towards Deep Representation Learning with Genetic Programming Abstract: Genetic Programming (GP) is an evolutionary algorithm commonly used for\nmachine learning tasks. In this paper we present a method that allows GP to\ntransform the representation of a large-scale machine learning dataset into a\nmore compact representation, by means of processing features from the original\nrepresentation at individual level. We develop as a proof of concept of this\nmethod an autoencoder. We tested a preliminary version of our approach in a\nvariety of well-known machine learning image datasets. We speculate that this\nmethod, used in an iterative manner, can produce results competitive with\nstate-of-art deep neural networks. \n\n"}
{"id": "1802.08375", "contents": "Title: Reusing Weights in Subword-aware Neural Language Models Abstract: We propose several ways of reusing subword embeddings and other weights in\nsubword-aware neural language models. The proposed techniques do not benefit a\ncompetitive character-aware model, but some of them improve the performance of\nsyllable- and morpheme-aware models while showing significant reductions in\nmodel sizes. We discover a simple hands-on principle: in a multi-layer input\nembedding model, layers should be tied consecutively bottom-up if reused at\noutput. Our best morpheme-aware model with properly reused weights beats the\ncompetitive word-level model by a large margin across multiple languages and\nhas 20%-87% fewer parameters. \n\n"}
{"id": "1802.08530", "contents": "Title: Training wide residual networks for deployment using a single bit for\n  each weight Abstract: For fast and energy-efficient deployment of trained deep neural networks on\nresource-constrained embedded hardware, each learned weight parameter should\nideally be represented and stored using a single bit. Error-rates usually\nincrease when this requirement is imposed. Here, we report large improvements\nin error rates on multiple datasets, for deep convolutional neural networks\ndeployed with 1-bit-per-weight. Using wide residual networks as our main\nbaseline, our approach simplifies existing methods that binarize weights by\napplying the sign function in training; we apply scaling factors for each layer\nwith constant unlearned values equal to the layer-specific standard deviations\nused for initialization. For CIFAR-10, CIFAR-100 and ImageNet, and models with\n1-bit-per-weight requiring less than 10 MB of parameter memory, we achieve\nerror rates of 3.9%, 18.5% and 26.0% / 8.5% (Top-1 / Top-5) respectively. We\nalso considered MNIST, SVHN and ImageNet32, achieving 1-bit-per-weight test\nresults of 0.27%, 1.9%, and 41.3% / 19.1% respectively. For CIFAR, our error\nrates halve previously reported values, and are within about 1% of our\nerror-rates for the same network with full-precision weights. For networks that\noverfit, we also show significant improvements in error rate by not learning\nbatch normalization scale and offset parameters. This applies to both full\nprecision and 1-bit-per-weight networks. Using a warm-restart learning-rate\nschedule, we found that training for 1-bit-per-weight is just as fast as\nfull-precision networks, with better accuracy than standard schedules, and\nachieved about 98%-99% of peak performance in just 62 training epochs for\nCIFAR-10/100. For full training code and trained models in MATLAB, Keras and\nPyTorch see https://github.com/McDonnell-Lab/1-bit-per-weight/ . \n\n"}
{"id": "1803.01358", "contents": "Title: Applied Erasure Coding in Networks and Distributed Storage Abstract: The amount of digital data is rapidly growing. There is an increasing use of\na wide range of computer systems, from mobile devices to large-scale data\ncenters, and important for reliable operation of all computer systems is\nmitigating the occurrence and the impact of errors in digital data. The demand\nfor new ultra-fast and highly reliable coding techniques for data at rest and\nfor data in transit is a major research challenge. Reliability is one of the\nmost important design requirements. The simplest way of providing a degree of\nreliability is by using data replication techniques. However, replication is\nhighly inefficient in terms of capacity utilization. Erasure coding has\ntherefore become a viable alternative to replication since it provides the same\nlevel of reliability as replication with significantly less storage overhead.\nThe present thesis investigates efficient constructions of erasure codes for\ndifferent applications. Methods from both coding and information theory have\nbeen applied to network coding, Optical Packet Switching (OPS) networks and\ndistributed storage systems. The following four issues are addressed: -\nConstruction of binary and non-binary erasure codes; - Reduction of the header\noverhead due to the encoding coefficients in network coding; - Construction and\nimplementation of new erasure codes for large-scale distributed storage systems\nthat provide savings in the storage and network resources compared to\nstate-of-the-art codes; and - Provision of a unified view on Quality of Service\n(QoS) in OPS networks when erasure codes are used, with the focus on Packet\nLoss Rate (PLR), survivability and secrecy. \n\n"}
{"id": "1803.02811", "contents": "Title: Accelerated Methods for Deep Reinforcement Learning Abstract: Deep reinforcement learning (RL) has achieved many recent successes, yet\nexperiment turn-around time remains a key bottleneck in research and in\npractice. We investigate how to optimize existing deep RL algorithms for modern\ncomputers, specifically for a combination of CPUs and GPUs. We confirm that\nboth policy gradient and Q-value learning algorithms can be adapted to learn\nusing many parallel simulator instances. We further find it possible to train\nusing batch sizes considerably larger than are standard, without negatively\naffecting sample complexity or final performance. We leverage these facts to\nbuild a unified framework for parallelization that dramatically hastens\nexperiments in both classes of algorithm. All neural network computations use\nGPUs, accelerating both data collection and training. Our results include using\nan entire DGX-1 to learn successful strategies in Atari games in mere minutes,\nusing both synchronous and asynchronous algorithms. \n\n"}
{"id": "1803.03031", "contents": "Title: Redundancy in Distributed Proofs Abstract: Distributed proofs are mechanisms enabling the nodes of a network to\ncollectivity and efficiently check the correctness of Boolean predicates on the\nstructure of the network, or on data-structures distributed over the nodes\n(e.g., spanning trees or routing tables). We consider mechanisms consisting of\ntwo components: a \\emph{prover} assigning a \\emph{certificate} to each node,\nand a distributed algorithm called \\emph{verifier} that is in charge of\nverifying the distributed proof formed by the collection of all certificates.\n  In this paper, we show that many network predicates have distributed proofs\noffering a high level of redundancy, explicitly or implicitly. We use this\nremarkable property of distributed proofs for establishing perfect tradeoffs\nbetween the \\emph{size of the certificate} stored at every node, and the\n\\emph{number of rounds} of the verification protocol. If we allow every node to\ncommunicate to distance at most $t$, one might expect that the certificate\nsizes can be reduced by a multiplicative factor of at least~$t$. In trees,\ncycles and grids, we show that such tradeoffs can be established for \\emph{all}\nnetwork predicates, i.e., it is always possible to linearly decrease the\ncertificate size. In arbitrary graphs, we show that any part of the\ncertificates common to all nodes can be evenly redistributed among these nodes,\nachieving even a better tradeoff: this common part of the certificate can be\nreduced by the size of a smallest ball of radius $t$ in the network.\n  In addition to these general results, we establish several upper and lower\nbounds on the certificate sizes used for distributed proofs for spanning trees,\nminimum-weight spanning trees, diameter, additive and multiplicative spanners,\nand more, improving and generalizing previous results from the literature. \n\n"}
{"id": "1803.04970", "contents": "Title: Scalable Algorithms for Parallel Tree-based Adaptive Mesh Refinement\n  with General Element Types Abstract: In this thesis, we develop, discuss and implement algorithms for scalable\nparallel tree-based adaptive mesh refinement (AMR) using space-filling curves\n(SFCs). We create an AMR software that works independently of the used element\ntype, such as for example lines, triangles, tetrahedra, quadrilaterals,\nhexahedra, and prisms. Along with a detailed mathematical discussion, this\nrequires the implementation as a numerical software and its validation, as well\nas scalability tests on current supercomputers. For triangular and tetrahedral\nelements (simplices) with red-refinement (1:4 in 2D, 1:8 in 3D), we develop a\nnew SFC index, the tetrahedral Morton index (TM-index). Its construction is\nsimilar to the Morton index for quadrilaterals/hexahedra, as it is also based\non bitwise interleaving the coordinates of a certain vertex of the simplex, the\nanchor node. We develop and demonstrate a new simplicial SFC and create a fast\nand scalable tree-based AMR software that offers a flexibility and generality\nthat was previously not available. \n\n"}
{"id": "1803.05320", "contents": "Title: Efficient Realization of Givens Rotation through Algorithm-Architecture\n  Co-design for Acceleration of QR Factorization Abstract: We present efficient realization of Generalized Givens Rotation (GGR) based\nQR factorization that achieves 3-100x better performance in terms of\nGflops/watt over state-of-the-art realizations on multicore, and General\nPurpose Graphics Processing Units (GPGPUs). GGR is an improvement over\nclassical Givens Rotation (GR) operation that can annihilate multiple elements\nof rows and columns of an input matrix simultaneously. GGR takes 33% lesser\nmultiplications compared to GR. For custom implementation of GGR, we identify\nmacro operations in GGR and realize them on a Reconfigurable Data-path (RDP)\ntightly coupled to pipeline of a Processing Element (PE). In PE, GGR attains\nspeed-up of 1.1x over Modified Householder Transform (MHT) presented in the\nliterature. For parallel realization of GGR, we use REDEFINE, a scalable\nmassively parallel Coarse-grained Reconfigurable Architecture, and show that\nthe speed-up attained is commensurate with the hardware resources in REDEFINE.\nGGR also outperforms General Matrix Multiplication (gemm) by 10% in-terms of\nGflops/watt which is counter-intuitive. \n\n"}
{"id": "1803.05575", "contents": "Title: Global Stabilization for Causally Consistent Partial Replication Abstract: Causally consistent distributed storage systems have received significant\nattention recently due to the potential for providing high throughput and\ncausality guarantees. {\\em Global stabilization} is a technique established for\nachieving causal consistency in distributed multi-version key-value store\nsystems, adopted by the previous work such as GentleRain\n\\cite{Du2014GentleRainCA} and Cure \\cite{akkoorath2016cure}. Intuitively, this\napproach serializes all updates by their physical time and computes the\n``Global Stable Time'' which is a time point $t$ such that versions with\ntimestamp $\\leq t$ can be returned to the client without violating causality.\nHowever, all previous designs with global stabilization assume {\\em full\nreplication}, where each data center stores a full copy of data, and each\nclient is restricted to access servers within one data center. In this paper,\nwe propose a theoretical framework to support {\\em general partial replication}\nwith causal consistency via global stabilization, where each server can store\nan arbitrary subset of the data, and each client is allowed to communicate with\nany subset of the servers and migrate among them without extra delays. We\npropose an algorithm that implements causal consistency for distributed\nmulti-version key-value stores with general partially replication. We prove the\noptimality of the Global Stable Time computation in our algorithm regarding the\nremote update visibility latency, i.e. how fast update from a remote server is\nvisible to the client, under general partial replication. We also provide\ntrade-offs to further optimize the remote update visibility by introducing\nextra delays during client's migration. Simulation results on the performance\nof our algorithm compared to the previous work are also provided. \n\n"}
{"id": "1803.05859", "contents": "Title: Neural Network Quine Abstract: Self-replication is a key aspect of biological life that has been largely\noverlooked in Artificial Intelligence systems. Here we describe how to build\nand train self-replicating neural networks. The network replicates itself by\nlearning to output its own weights. The network is designed using a loss\nfunction that can be optimized with either gradient-based or non-gradient-based\nmethods. We also describe a method we call regeneration to train the network\nwithout explicit optimization, by injecting the network with predictions of its\nown parameters. The best solution for a self-replicating network was found by\nalternating between regeneration and optimization steps. Finally, we describe a\ndesign for a self-replicating neural network that can solve an auxiliary task\nsuch as MNIST image classification. We observe that there is a trade-off\nbetween the network's ability to classify images and its ability to replicate,\nbut training is biased towards increasing its specialization at image\nclassification at the expense of replication. This is analogous to the\ntrade-off between reproduction and other tasks observed in nature. We suggest\nthat a self-replication mechanism for artificial intelligence is useful because\nit introduces the possibility of continual improvement through natural\nselection. \n\n"}
{"id": "1803.06341", "contents": "Title: Distributed Transactions: Dissecting the Nightmare Abstract: Many distributed storage systems are transactional and a lot of work has been\ndevoted to optimizing their performance, especially the performance of\nread-only transactions that are considered the most frequent in practice. Yet,\nthe results obtained so far are rather disappointing, and some of the design\ndecisions seem contrived. This paper contributes to explaining this state of\naffairs by proving intrinsic limitations of transactional storage systems, even\nthose that need not ensure strong consistency but only causality.\n  We first consider general storage systems where some transactions are\nread-only and some also involve write operations. We show that even read-only\ntransactions cannot be \"fast\": their operations cannot be executed within one\nround-trip message exchange between a client seeking an object and the server\nstoring it. We then consider systems (as sometimes implemented today) where all\ntransactions are read-only, i.e., updates are performed as individual\noperations outside transactions. In this case, read-only transactions can\nindeed be \"fast\", but we prove that they need to be \"visible\". They induce\ninherent updates on the servers, which in turn impact their overall\nperformance. \n\n"}
{"id": "1803.06443", "contents": "Title: Communication Compression for Decentralized Training Abstract: Optimizing distributed learning systems is an art of balancing between\ncomputation and communication. There have been two lines of research that try\nto deal with slower networks: {\\em communication compression} for low bandwidth\nnetworks, and {\\em decentralization} for high latency networks. In this paper,\nWe explore a natural question: {\\em can the combination of both techniques lead\nto a system that is robust to both bandwidth and latency?}\n  Although the system implication of such combination is trivial, the\nunderlying theoretical principle and algorithm design is challenging: unlike\ncentralized algorithms, simply compressing exchanged information, even in an\nunbiased stochastic way, within the decentralized network would accumulate the\nerror and fail to converge. In this paper, we develop a framework of\ncompressed, decentralized training and propose two different strategies, which\nwe call {\\em extrapolation compression} and {\\em difference compression}. We\nanalyze both algorithms and prove both converge at the rate of $O(1/\\sqrt{nT})$\nwhere $n$ is the number of workers and $T$ is the number of iterations,\nmatching the convergence rate for full precision, centralized training. We\nvalidate our algorithms and find that our proposed algorithm outperforms the\nbest of merely decentralized and merely quantized algorithm significantly for\nnetworks with {\\em both} high latency and low bandwidth. \n\n"}
{"id": "1803.06561", "contents": "Title: AutoML from Service Provider's Perspective: Multi-device, Multi-tenant\n  Model Selection with GP-EI Abstract: AutoML has become a popular service that is provided by most leading cloud\nservice providers today. In this paper, we focus on the AutoML problem from the\n\\emph{service provider's perspective}, motivated by the following practical\nconsideration: When an AutoML service needs to serve {\\em multiple users} with\n{\\em multiple devices} at the same time, how can we allocate these devices to\nusers in an efficient way? We focus on GP-EI, one of the most popular\nalgorithms for automatic model selection and hyperparameter tuning, used by\nsystems such as Google Vizer. The technical contribution of this paper is the\nfirst multi-device, multi-tenant algorithm for GP-EI that is aware of\n\\emph{multiple} computation devices and multiple users sharing the same set of\ncomputation devices. Theoretically, given $N$ users and $M$ devices, we obtain\na regret bound of $O((\\text{\\bf {MIU}}(T,K) + M)\\frac{N^2}{M})$, where\n$\\text{\\bf {MIU}}(T,K)$ refers to the maximal incremental uncertainty up to\ntime $T$ for the covariance matrix $K$. Empirically, we evaluate our algorithm\non two applications of automatic model selection, and show that our algorithm\nsignificantly outperforms the strategy of serving users independently.\nMoreover, when multiple computation devices are available, we achieve\nnear-linear speedup when the number of users is much larger than the number of\ndevices. \n\n"}
{"id": "1803.06744", "contents": "Title: Fast Neural Architecture Construction using EnvelopeNets Abstract: Fast Neural Architecture Construction (NAC) is a method to construct deep\nnetwork architectures by pruning and expansion of a base network. In recent\nyears, several automated search methods for neural network architectures have\nbeen proposed using methods such as evolutionary algorithms and reinforcement\nlearning. These methods use a single scalar objective function (usually\naccuracy) that is evaluated after a full training and evaluation cycle. In\ncontrast NAC directly compares the utility of different filters using\nstatistics derived from filter featuremaps reach a state where the utility of\ndifferent filters within a network can be compared and hence can be used to\nconstruct networks. The training epochs needed for filters within a network to\nreach this state is much less than the training epochs needed for the accuracy\nof a network to stabilize. NAC exploits this finding to construct convolutional\nneural nets (CNNs) with close to state of the art accuracy, in < 1 GPU day,\nfaster than most of the current neural architecture search methods. The\nconstructed networks show close to state of the art performance on the image\nclassification problem on well known datasets (CIFAR-10, ImageNet) and\nconsistently show better performance than hand constructed and randomly\ngenerated networks of the same depth, operators and approximately the same\nnumber of parameters. \n\n"}
{"id": "1803.07068", "contents": "Title: D$^2$: Decentralized Training over Decentralized Data Abstract: While training a machine learning model using multiple workers, each of which\ncollects data from their own data sources, it would be most useful when the\ndata collected from different workers can be {\\em unique} and {\\em different}.\nIronically, recent analysis of decentralized parallel stochastic gradient\ndescent (D-PSGD) relies on the assumption that the data hosted on different\nworkers are {\\em not too different}. In this paper, we ask the question: {\\em\nCan we design a decentralized parallel stochastic gradient descent algorithm\nthat is less sensitive to the data variance across workers?} In this paper, we\npresent D$^2$, a novel decentralized parallel stochastic gradient descent\nalgorithm designed for large data variance \\xr{among workers} (imprecisely,\n\"decentralized\" data). The core of D$^2$ is a variance blackuction extension of\nthe standard D-PSGD algorithm, which improves the convergence rate from\n$O\\left({\\sigma \\over \\sqrt{nT}} + {(n\\zeta^2)^{\\frac{1}{3}} \\over\nT^{2/3}}\\right)$ to $O\\left({\\sigma \\over \\sqrt{nT}}\\right)$ where $\\zeta^{2}$\ndenotes the variance among data on different workers. As a result, D$^2$ is\nrobust to data variance among workers. We empirically evaluated D$^2$ on image\nclassification tasks where each worker has access to only the data of a limited\nset of labels, and find that D$^2$ significantly outperforms D-PSGD. \n\n"}
{"id": "1803.07870", "contents": "Title: Reservoir computing approaches for representation and classification of\n  multivariate time series Abstract: Classification of multivariate time series (MTS) has been tackled with a\nlarge variety of methodologies and applied to a wide range of scenarios.\nReservoir Computing (RC) provides efficient tools to generate a vectorial,\nfixed-size representation of the MTS that can be further processed by standard\nclassifiers. Despite their unrivaled training speed, MTS classifiers based on a\nstandard RC architecture fail to achieve the same accuracy of fully trainable\nneural networks. In this paper we introduce the reservoir model space, an\nunsupervised approach based on RC to learn vectorial representations of MTS.\nEach MTS is encoded within the parameters of a linear model trained to predict\na low-dimensional embedding of the reservoir dynamics. Compared to other RC\nmethods, our model space yields better representations and attains comparable\ncomputational performance, thanks to an intermediate dimensionality reduction\nprocedure. As a second contribution we propose a modular RC framework for MTS\nclassification, with an associated open-source Python library. The framework\nprovides different modules to seamlessly implement advanced RC architectures.\nThe architectures are compared to other MTS classifiers, including deep\nlearning models and time series kernels. Results obtained on benchmark and\nreal-world MTS datasets show that RC classifiers are dramatically faster and,\nwhen implemented using our proposed representation, also achieve superior\nclassification accuracy. \n\n"}
{"id": "1803.07877", "contents": "Title: A Case Study for Grain Quality Assurance Tracking based on a Blockchain\n  Business Network Abstract: One of the key processes in Agriculture is quality measurement throughout the\ntransportation of grains along its complex supply chain. This procedure is\nsuitable for failures, such as delays to final destinations, poor monitoring,\nand frauds. To address the grain quality measurement challenge through the\ntransportation chain, novel technologies, such as Distributed Ledger and\nBlockchain, can bring more efficiency and resilience to the process.\nParticularly, Blockchain is a new type of distributed database in which\ntransactions are securely appended using cryptography and hashed pointers.\nThose transactions can be generated and ruled by special network-embedded\nsoftware -- known as smart contracts -- that may be public to all nodes of the\nnetwork or may be private to a specific set of peer nodes. This paper analyses\nthe implementation of Blockchain technology targeting grain quality assurance\ntracking in a real scenario. Preliminary results support a potential demand for\na Blockchain-based certification that would lead to an added valuation of\naround 15% for GM-free soy in the scope of a Grain Exporter Business Network in\nBrazil. \n\n"}
{"id": "1803.08203", "contents": "Title: Residual Networks: Lyapunov Stability and Convex Decomposition Abstract: While training error of most deep neural networks degrades as the depth of\nthe network increases, residual networks appear to be an exception. We show\nthat the main reason for this is the Lyapunov stability of the gradient descent\nalgorithm: for an arbitrarily chosen step size, the equilibria of the gradient\ndescent are most likely to remain stable for the parametrization of residual\nnetworks. We then present an architecture with a pair of residual networks to\napproximate a large class of functions by decomposing them into a convex and a\nconcave part. Some parameters of this model are shown to change little during\ntraining, and this imperfect optimization prevents overfitting the data and\nleads to solutions with small Lipschitz constants, while providing clues about\nthe generalization of other deep networks. \n\n"}
{"id": "1803.08660", "contents": "Title: Lifting Layers: Analysis and Applications Abstract: The great advances of learning-based approaches in image processing and\ncomputer vision are largely based on deeply nested networks that compose linear\ntransfer functions with suitable non-linearities. Interestingly, the most\nfrequently used non-linearities in imaging applications (variants of the\nrectified linear unit) are uncommon in low dimensional approximation problems.\nIn this paper we propose a novel non-linear transfer function, called lifting,\nwhich is motivated from a related technique in convex optimization. A lifting\nlayer increases the dimensionality of the input, naturally yields a linear\nspline when combined with a fully connected layer, and therefore closes the gap\nbetween low and high dimensional approximation problems. Moreover, applying the\nlifting operation to the loss layer of the network allows us to handle\nnon-convex and flat (zero-gradient) cost functions. We analyze the proposed\nlifting theoretically, exemplify interesting properties in synthetic\nexperiments and demonstrate its effectiveness in deep learning approaches to\nimage classification and denoising. \n\n"}
{"id": "1803.09004", "contents": "Title: Face Recognition with Hybrid Efficient Convolution Algorithms on FPGAs Abstract: Deep Convolutional Neural Networks have become a Swiss knife in solving\ncritical artificial intelligence tasks. However, deploying deep CNN models for\nlatency-critical tasks remains to be challenging because of the complex nature\nof CNNs. Recently, FPGA has become a favorable device to accelerate deep CNNs\nthanks to its high parallel processing capability and energy efficiency. In\nthis work, we explore different fast convolution algorithms including Winograd\nand Fast Fourier Transform (FFT), and find an optimal strategy to apply them\ntogether on different types of convolutions. We also propose an optimization\nscheme to exploit parallelism on novel CNN architectures such as Inception\nmodules in GoogLeNet. We implement a configurable IP-based face recognition\nacceleration system based on FaceNet using High-Level Synthesis. Our\nimplementation on a Xilinx Ultrascale device achieves 3.75x latency speedup\ncompared to a high-end NVIDIA GPU and surpasses previous FPGA results\nsignificantly. \n\n"}
{"id": "1803.09254", "contents": "Title: A theory of the phenomenology of multipopulation genetic algorithm with\n  an application to the Ising model Abstract: Genetic algorithm (GA) is a stochastic metaheuristic process consisting on\nthe evolution of a population of candidate solutions for a given optimization\nproblem. By extension, multipopulation genetic algorithm (MPGA) aims for\nefficiency by evolving many populations, or islands, in parallel and performing\nmigrations between them periodically. The connectivity between islands\nconstrains the directions of migration and characterizes MPGA as a dynamic\nprocess over a network. As such, predicting the evolution of the quality of the\nsolutions is a difficult challenge, implying in the waste of computer resources\nand energy when the parameters are inadequate. By using models derived from\nstatistical mechanics, this work aims to estimate equations for the study of\ndynamics in relation to the connectivity in MPGA. To illustrate the importance\nof understanding MPGA, we show its application as an efficient alternative to\nthe thermalization phase of Metropolis-Hastings algorithm applied to the Ising\nmodel. \n\n"}
{"id": "1803.09807", "contents": "Title: Deep learning as a tool for neural data analysis: speech classification\n  and cross-frequency coupling in human sensorimotor cortex Abstract: A fundamental challenge in neuroscience is to understand what structure in\nthe world is represented in spatially distributed patterns of neural activity\nfrom multiple single-trial measurements. This is often accomplished by learning\na simple, linear transformations between neural features and features of the\nsensory stimuli or motor task. While successful in some early sensory\nprocessing areas, linear mappings are unlikely to be ideal tools for\nelucidating nonlinear, hierarchical representations of higher-order brain areas\nduring complex tasks, such as the production of speech by humans. Here, we\napply deep networks to predict produced speech syllables from cortical surface\nelectric potentials recorded from human sensorimotor cortex. We found that deep\nnetworks had higher decoding prediction accuracy compared to baseline models,\nand also exhibited greater improvements in accuracy with increasing dataset\nsize. We further demonstrate that deep network's confusions revealed\nhierarchical latent structure in the neural data, which recapitulated the\nunderlying articulatory nature of speech motor control. Finally, we used deep\nnetworks to compare task-relevant information in different neural frequency\nbands, and found that the high-gamma band contains the vast majority of\ninformation relevant for the speech prediction task, with little-to-no\nadditional contribution from lower-frequencies. Together, these results\ndemonstrate the utility of deep networks as a data analysis tool for\nneuroscience. \n\n"}
{"id": "1803.09820", "contents": "Title: A disciplined approach to neural network hyper-parameters: Part 1 --\n  learning rate, batch size, momentum, and weight decay Abstract: Although deep learning has produced dazzling successes for applications of\nimage, speech, and video processing in the past few years, most trainings are\nwith suboptimal hyper-parameters, requiring unnecessarily long training times.\nSetting the hyper-parameters remains a black art that requires years of\nexperience to acquire. This report proposes several efficient ways to set the\nhyper-parameters that significantly reduce training time and improves\nperformance. Specifically, this report shows how to examine the training\nvalidation/test loss function for subtle clues of underfitting and overfitting\nand suggests guidelines for moving toward the optimal balance point. Then it\ndiscusses how to increase/decrease the learning rate/momentum to speed up\ntraining. Our experiments show that it is crucial to balance every manner of\nregularization for each dataset and architecture. Weight decay is used as a\nsample regularizer to show how its optimal value is tightly coupled with the\nlearning rates and momentums. Files to help replicate the results reported here\nare available. \n\n"}
{"id": "1803.10560", "contents": "Title: Normalization of Neural Networks using Analytic Variance Propagation Abstract: We address the problem of estimating statistics of hidden units in a neural\nnetwork using a method of analytic moment propagation. These statistics are\nuseful for approximate whitening of the inputs in front of saturating\nnon-linearities such as a sigmoid function. This is important for\ninitialization of training and for reducing the accumulated scale and bias\ndependencies (compensating covariate shift), which presumably eases the\nlearning. In batch normalization, which is currently a very widely applied\ntechnique, sample estimates of statistics of hidden units over a batch are\nused. The proposed estimation uses an analytic propagation of mean and variance\nof the training set through the network. The result depends on the network\nstructure and its current weights but not on the specific batch input. The\nestimates are suitable for initialization and normalization, efficient to\ncompute and independent of the batch size. The experimental verification well\nsupports these claims. However, the method does not share the generalization\nproperties of BN, to which our experiments give some additional insight. \n\n"}
{"id": "1803.10901", "contents": "Title: Statistical Validity and Consistency of Big Data Analytics: A General\n  Framework Abstract: Informatics and technological advancements have triggered generation of huge\nvolume of data with varied complexity in its management and analysis. Big Data\nanalytics is the practice of revealing hidden aspects of such data and making\ninferences from it. Although storage, retrieval and management of Big Data seem\npossible through efficient algorithm and system development, concern about\nstatistical consistency remains to be addressed in view of its specific\ncharacteristics. Since Big Data does not conform to standard analytics, we need\nproper modification of the existing statistical theory and tools. Here we\npropose, with illustrations, a general statistical framework and an algorithmic\nprinciple for Big Data analytics that ensure statistical accuracy of the\nconclusions. The proposed framework has the potential to push forward\nadvancement of Big Data analytics in the right direction. The\npartition-repetition approach proposed here is broad enough to encompass all\npractical data analytic problems. \n\n"}
{"id": "1803.11389", "contents": "Title: Single Stream Parallelization of Recurrent Neural Networks for Low Power\n  and Fast Inference Abstract: As neural network algorithms show high performance in many applications,\ntheir efficient inference on mobile and embedded systems are of great\ninterests. When a single stream recurrent neural network (RNN) is executed for\na personal user in embedded systems, it demands a large amount of DRAM accesses\nbecause the network size is usually much bigger than the cache size and the\nweights of an RNN are used only once at each time step. We overcome this\nproblem by parallelizing the algorithm and executing it multiple time steps at\na time. This approach also reduces the power consumption by lowering the number\nof DRAM accesses. QRNN (Quasi Recurrent Neural Networks) and SRU (Simple\nRecurrent Unit) based recurrent neural networks are used for implementation.\nThe experiments for SRU showed about 300% and 930% of speed-up when the numbers\nof multi time steps are 4 and 16, respectively, in an ARM CPU based system. \n\n"}
{"id": "1803.11410", "contents": "Title: The Resistance to Label Noise in K-NN and DNN Depends on its\n  Concentration Abstract: We investigate the classification performance of K-nearest neighbors (K-NN)\nand deep neural networks (DNNs) in the presence of label noise. We first show\nempirically that a DNN's prediction for a given test example depends on the\nlabels of the training examples in its local neighborhood. This motivates us to\nderive a realizable analytic expression that approximates the multi-class K-NN\nclassification error in the presence of label noise, which is of independent\nimportance. We then suggest that the expression for K-NN may serve as a\nfirst-order approximation for the DNN error. Finally, we demonstrate\nempirically the proximity of the developed expression to the observed\nperformance of K-NN and DNN classifiers. Our result may explain the already\nobserved surprising resistance of DNN to some types of label noise. It also\ncharacterizes an important factor of it showing that the more concentrated the\nnoise the greater is the degradation in performance. \n\n"}
{"id": "1804.00358", "contents": "Title: Evolution and Limiting Configuration of a Long-Range Schelling-Type Spin\n  System Abstract: We consider a long-range interacting particle system in which binary\nparticles -- whose initial states are chosen uniformly at random -- are located\nat the nodes of a flat torus $(\\mathbb{Z}/h\\mathbb{Z})^2$. Each node of the\ntorus is connected to all the nodes located in an $l_\\infty$-ball of radius $w$\nin the toroidal space centered at itself and we assume that $h$ is\nexponentially larger than $w^2$. Based on the states of the neighboring\nparticles and on the value of a common intolerance threshold $\\tau$, every\nparticle is labeled \"stable,\" or \"unstable.\" Every unstable particle that can\nbecome stable by flipping its state is labeled \"p-stable.\" Finally, unstable\nparticles that remained p-stable for a random, independent and identically\ndistributed waiting time, flip their state and become stable. When the waiting\ntimes have an exponential distribution and $\\tau \\le 1/2$, this model is\nequivalent to a Schelling model of self-organized segregation in an open\nsystem, a zero-temperature Ising model with Glauber dynamics, or an\nAsynchronous Cellular Automaton (ACA) with extended Moore neighborhoods. We\nfirst prove a shape theorem for the spreading of the \"affected\" nodes of a\ngiven state -- namely nodes on which a particle of a given state would be\np-stable. As $w \\rightarrow \\infty$, this spreading starts with high\nprobability (w.h.p.) from any $l_\\infty$-ball in the torus having radius $w/2$\nand containing only affected nodes, and continues for a time that is at least\nexponential in the cardinalilty of the neighborhood of interaction $N =\n(2w+1)^2$. Second, we show that when the process reaches a limiting\nconfiguration and no more state changes occur, for all ${\\tau \\in\n(\\tau^*,1-\\tau^*) \\setminus \\{1/2\\}}$ where ${\\tau^* \\approx 0.488}$, w.h.p.\nany particle is contained in a large \"monochromatic ball\" of cardinality\nexponential in $N$. \n\n"}
{"id": "1804.00497", "contents": "Title: MicronNet: A Highly Compact Deep Convolutional Neural Network\n  Architecture for Real-time Embedded Traffic Sign Classification Abstract: Traffic sign recognition is a very important computer vision task for a\nnumber of real-world applications such as intelligent transportation\nsurveillance and analysis. While deep neural networks have been demonstrated in\nrecent years to provide state-of-the-art performance traffic sign recognition,\na key challenge for enabling the widespread deployment of deep neural networks\nfor embedded traffic sign recognition is the high computational and memory\nrequirements of such networks. As a consequence, there are significant benefits\nin investigating compact deep neural network architectures for traffic sign\nrecognition that are better suited for embedded devices. In this paper, we\nintroduce MicronNet, a highly compact deep convolutional neural network for\nreal-time embedded traffic sign recognition designed based on macroarchitecture\ndesign principles (e.g., spectral macroarchitecture augmentation, parameter\nprecision optimization, etc.) as well as numerical microarchitecture\noptimization strategies. The resulting overall architecture of MicronNet is\nthus designed with as few parameters and computations as possible while\nmaintaining recognition performance, leading to optimized information density\nof the proposed network. The resulting MicronNet possesses a model size of just\n~1MB and ~510,000 parameters (~27x fewer parameters than state-of-the-art)\nwhile still achieving a human performance level top-1 accuracy of 98.9% on the\nGerman traffic sign recognition benchmark. Furthermore, MicronNet requires just\n~10 million multiply-accumulate operations to perform inference, and has a\ntime-to-compute of just 32.19 ms on a Cortex-A53 high efficiency processor.\nThese experimental results show that highly compact, optimized deep neural\nnetwork architectures can be designed for real-time traffic sign recognition\nthat are well-suited for embedded scenarios. \n\n"}
{"id": "1804.00695", "contents": "Title: Sparse Matrix-Matrix Multiplication on Multilevel Memory Architectures :\n  Algorithms and Experiments Abstract: Architectures with multiple classes of memory media are becoming a common\npart of mainstream supercomputer deployments. So called multi-level memories\noffer differing characteristics for each memory component including variation\nin bandwidth, latency and capacity. This paper investigates the performance of\nsparse matrix multiplication kernels on two leading high-performance computing\narchitectures -- Intel's Knights Landing processor and NVIDIA's Pascal GPU. We\ndescribe a data placement method and a chunking-based algorithm for our kernels\nthat exploits the existence of the multiple memory spaces in each hardware\nplatform. We evaluate the performance of these methods w.r.t. standard\nalgorithms using the auto-caching mechanisms. Our results show that standard\nalgorithms that exploit cache reuse performed as well as multi-memory-aware\nalgorithms for architectures such as KNLs where the memory subsystems have\nsimilar latencies. However, for architectures such as GPUs where memory\nsubsystems differ significantly in both bandwidth and latency,\nmulti-memory-aware methods are crucial for good performance. In addition, our\nnew approaches permit the user to run problems that require larger capacities\nthan the fastest memory of each compute node without depending on the\nsoftware-managed cache mechanisms. \n\n"}
{"id": "1804.00703", "contents": "Title: A Complete Model for Modular Simulation of Data Centre Power Load Abstract: Data centres are very fast growing structures with significant contribution\nto the world's energy consumption. Reducing the energy consumption of data\ncentres is easier when the components that comprise a data centre and their\nrespective energy consumption are known. A complete model for a modular design\nof a data centre and a technique for simulating each module's energy\nconsumption are presented. Detailed power consumption modelling for each\ncomponent as well as their interactions are the merits of this model. Unlike\nexisting research, the present modular simulation model can take different\ndesign structures of data centres into account and provide us with hourly power\nconsumption profiles for each component. The impacts of environmental\nparameters such as temperature and humidity are also investigated and\nincorporated into the model. The flexibility, scalability, comprehensiveness\nand modularity of this model provides researchers and designers with a powerful\ntool for energy analysis, management and planning of data centres with\ndifferent designs and locations. \n\n"}
{"id": "1804.01626", "contents": "Title: SBFT: a Scalable and Decentralized Trust Infrastructure Abstract: SBFT is a state of the art Byzantine fault tolerant permissioned blockchain\nsystem that addresses the challenges of scalability, decentralization and\nworld-scale geo-replication. SBFTis optimized for decentralization and can\neasily handle more than 200 active replicas in a real world-scale deployment.\nWe evaluate \\sysname in a world-scale geo-replicated deployment with 209\nreplicas withstanding f=64 Byzantine failures. We provide experiments that show\nhow the different algorithmic ingredients of \\sysname increase its performance\nand scalability. The results show that SBFT simultaneously provides almost 2x\nbetter throughput and about 1.5x better latency relative to a highly optimized\nsystem that implements the PBFT protocol. To achieve this performance\nimprovement, SBFT uses a combination of four ingredients: using collectors and\nthreshold signatures to reduce communication to linear, using an optimistic\nfast path, reducing client communication and utilizing redundant servers for\nthe fast path. \n\n"}
{"id": "1804.01712", "contents": "Title: Variational Rejection Sampling Abstract: Learning latent variable models with stochastic variational inference is\nchallenging when the approximate posterior is far from the true posterior, due\nto high variance in the gradient estimates. We propose a novel rejection\nsampling step that discards samples from the variational posterior which are\nassigned low likelihoods by the model. Our approach provides an arbitrarily\naccurate approximation of the true posterior at the expense of extra\ncomputation. Using a new gradient estimator for the resulting unnormalized\nproposal distribution, we achieve average improvements of 3.71 nats and 0.21\nnats over state-of-the-art single-sample and multi-sample alternatives\nrespectively for estimating marginal log-likelihoods using sigmoid belief\nnetworks on the MNIST dataset. \n\n"}
{"id": "1804.04177", "contents": "Title: Detecting Malicious PowerShell Commands using Deep Neural Networks Abstract: Microsoft's PowerShell is a command-line shell and scripting language that is\ninstalled by default on Windows machines. While PowerShell can be configured by\nadministrators for restricting access and reducing vulnerabilities, these\nrestrictions can be bypassed. Moreover, PowerShell commands can be easily\ngenerated dynamically, executed from memory, encoded and obfuscated, thus\nmaking the logging and forensic analysis of code executed by PowerShell\nchallenging.For all these reasons, PowerShell is increasingly used by\ncybercriminals as part of their attacks' tool chain, mainly for downloading\nmalicious contents and for lateral movement. Indeed, a recent comprehensive\ntechnical report by Symantec dedicated to PowerShell's abuse by cybercrimials\nreported on a sharp increase in the number of malicious PowerShell samples they\nreceived and in the number of penetration tools and frameworks that use\nPowerShell. This highlights the urgent need of developing effective methods for\ndetecting malicious PowerShell commands.In this work, we address this challenge\nby implementing several novel detectors of malicious PowerShell commands and\nevaluating their performance. We implemented both \"traditional\" natural\nlanguage processing (NLP) based detectors and detectors based on\ncharacter-level convolutional neural networks (CNNs). Detectors' performance\nwas evaluated using a large real-world dataset.Our evaluation results show\nthat, although our detectors individually yield high performance, an ensemble\ndetector that combines an NLP-based classifier with a CNN-based classifier\nprovides the best performance, since the latter classifier is able to detect\nmalicious commands that succeed in evading the former. Our analysis of these\nevasive commands reveals that some obfuscation patterns automatically detected\nby the CNN classifier are intrinsically difficult to detect using the NLP\ntechniques we applied. \n\n"}
{"id": "1804.05374", "contents": "Title: Twin Regularization for online speech recognition Abstract: Online speech recognition is crucial for developing natural human-machine\ninterfaces. This modality, however, is significantly more challenging than\noff-line ASR, since real-time/low-latency constraints inevitably hinder the use\nof future information, that is known to be very helpful to perform robust\npredictions. A popular solution to mitigate this issue consists of feeding\nneural acoustic models with context windows that gather some future frames.\nThis introduces a latency which depends on the number of employed look-ahead\nfeatures. This paper explores a different approach, based on estimating the\nfuture rather than waiting for it. Our technique encourages the hidden\nrepresentations of a unidirectional recurrent network to embed some useful\ninformation about the future. Inspired by a recently proposed technique called\nTwin Networks, we add a regularization term that forces forward hidden states\nto be as close as possible to cotemporal backward ones, computed by a \"twin\"\nneural network running backwards in time. The experiments, conducted on a\nnumber of datasets, recurrent architectures, input features, and acoustic\nconditions, have shown the effectiveness of this approach. One important\nadvantage is that our method does not introduce any additional computation at\ntest time if compared to standard unidirectional recurrent networks. \n\n"}
{"id": "1804.05714", "contents": "Title: Developing Synthesis Flows Without Human Knowledge Abstract: Design flows are the explicit combinations of design transformations,\nprimarily involved in synthesis, placement and routing processes, to accomplish\nthe design of Integrated Circuits (ICs) and System-on-Chip (SoC). Mostly, the\nflows are developed based on the knowledge of the experts. However, due to the\nlarge search space of design flows and the increasing design complexity,\ndeveloping Intellectual Property (IP)-specific synthesis flows providing high\nQuality of Result (QoR) is extremely challenging. This work presents a fully\nautonomous framework that artificially produces design-specific synthesis flows\nwithout human guidance and baseline flows, using Convolutional Neural Network\n(CNN). The demonstrations are made by successfully designing logic synthesis\nflows of three large scaled designs. \n\n"}
{"id": "1804.06173", "contents": "Title: Memetic Algorithms Beat Evolutionary Algorithms on the Class of Hurdle\n  Problems Abstract: Memetic algorithms are popular hybrid search heuristics that integrate local\nsearch into the search process of an evolutionary algorithm in order to combine\nthe advantages of rapid exploitation and global optimisation. However, these\nalgorithms are not well understood and the field is lacking a solid theoretical\nfoundation that explains when and why memetic algorithms are effective.\n  We provide a rigorous runtime analysis of a simple memetic algorithm, the\n$(1+1)$ MA, on the Hurdle problem class, a landscape class of tuneable\ndifficulty that shows a \"big valley structure\", a characteristic feature of\nmany hard problems from combinatorial optimisation. The only parameter of this\nclass is the hurdle width w, which describes the length of fitness valleys that\nhave to be overcome. We show that the $(1+1)$ EA requires $\\Theta(n^w)$\nexpected function evaluations to find the optimum, whereas the $(1+1)$ MA with\nbest-improvement and first-improvement local search can find the optimum in\n$\\Theta(n^2+n^3/w^2)$ and $\\Theta(n^3/w^2)$ function evaluations, respectively.\nSurprisingly, while increasing the hurdle width makes the problem harder for\nevolutionary algorithms, the problem becomes easier for memetic algorithms. We\ndiscuss how these findings can explain and illustrate the success of memetic\nalgorithms for problems with big valley structures. \n\n"}
{"id": "1804.06231", "contents": "Title: An Efficient SIMD Implementation of Pseudo-Verlet Lists for Neighbour\n  Interactions in Particle-Based Codes Abstract: In particle-based simulations, neighbour finding (i.e finding pairs of\nparticles to interact within a given range) is the most time consuming part of\nthe computation. One of the best such algorithms, which can be used for both\nMolecular Dynamics (MD) and Smoothed Particle Hydrodynamics (SPH) simulations,\nis the pseudo-Verlet list algorithm. This algorithm, however, does not\nvectorise trivially, and hence makes it difficult to exploit SIMD-parallel\narchitectures. In this paper, we present several novel modifications as well as\na vectorisation strategy for the algorithm which lead to overall speed-ups over\nthe scalar version of the algorithm of 2.24x for the AVX instruction set (SIMD\nwidth of 8), 2.43x for AVX2, and 4.07x for AVX-512 (SIMD width of 16). \n\n"}
{"id": "1804.06462", "contents": "Title: Mage: Online Interference-Aware Scheduling in Multi-Scale Heterogeneous\n  Systems Abstract: Heterogeneity has grown in popularity both at the core and server level as a\nway to improve both performance and energy efficiency. However, despite these\nbenefits, scheduling applications in heterogeneous machines remains\nchallenging. Additionally, when these heterogeneous resources accommodate\nmultiple applications to increase utilization, resources are prone to\ncontention, destructive interference, and unpredictable performance. Existing\nsolutions examine heterogeneity either across or within a server, leading to\nmissed performance and efficiency opportunities. We present Mage, a practical\ninterference-aware runtime that optimizes performance and efficiency in systems\nwith intra- and inter-server heterogeneity. Mage leverages fast and online data\nmining to quickly explore the space of application placements, and determine\nthe one that minimizes destructive interference between co-resident\napplications. Mage continuously monitors the performance of active\napplications, and, upon detecting QoS violations, it determines whether\nalternative placements would prove more beneficial, taking into account any\noverheads from migration. Across 350 application mixes on a heterogeneous CMP,\nMage improves performance by 38% and up to 2x compared to a greedy scheduler.\nAcross 160 mixes on a heterogeneous cluster, Mage improves performance by 30%\non average and up to 52% over the greedy scheduler, and by 11% over the\ncombination of Paragon [15] for inter- and intra-server heterogeneity. \n\n"}
{"id": "1804.06568", "contents": "Title: Walkman: A Communication-Efficient Random-Walk Algorithm for\n  Decentralized Optimization Abstract: This paper addresses consensus optimization problems in a multi-agent\nnetwork, where all agents collaboratively find a minimizer for the sum of their\nprivate functions. We develop a new decentralized algorithm in which each agent\ncommunicates only with its neighbors.\n  State-of-the-art decentralized algorithms use communications between either\nall pairs of adjacent agents or a random subset of them at each iteration.\nAnother class of algorithms uses a random walk incremental strategy, which\nsequentially activates a succession of nodes; these incremental algorithms\nrequire diminishing step sizes to converge to the solution, so their\nconvergence is relatively slow.\n  In this work, we propose a random walk algorithm that uses a fixed step size\nand converges faster than the existing random walk incremental algorithms. Our\nalgorithm is also communication efficient. Each iteration uses only one link to\ncommunicate the latest information for an agent to another. Since this\ncommunication rule mimics a man walking around the network, we call our new\nalgorithm Walkman. We establish convergence for convex and nonconvex\nobjectives. For decentralized least squares, we derive a linear rate of\nconvergence and obtain a better communication complexity than those of other\ndecentralized algorithms. Numerical experiments verify our analysis results. \n\n"}
{"id": "1804.07017", "contents": "Title: Programming Parallel Dense Matrix Factorizations with Look-Ahead and\n  OpenMP Abstract: We investigate a parallelization strategy for dense matrix factorization\n(DMF) algorithms, using OpenMP, that departs from the legacy (or conventional)\nsolution, which simply extracts concurrency from a multithreaded version of\nBLAS. This approach is also different from the more sophisticated\nruntime-assisted implementations, which decompose the operation into tasks and\nidentify dependencies via directives and runtime support. Instead, our strategy\nattains high performance by explicitly embedding a static look-ahead technique\ninto the DMF code, in order to overcome the performance bottleneck of the panel\nfactorization, and realizing the trailing update via a cache-aware\nmulti-threaded implementation of the BLAS. Although the parallel algorithms are\nspecified with a highlevel of abstraction, the actual implementation can be\neasily derived from them, paving the road to deriving a high performance\nimplementation of a considerable fraction of LAPACK functionality on any\nmulticore platform with an OpenMP-like runtime. \n\n"}
{"id": "1804.07747", "contents": "Title: Cut to Fit: Tailoring the Partitioning to the Computation Abstract: Social Graph Analytics applications are very often built using off-the-shelf\nanalytics frameworks. These, however, are profiled and optimized for the\ngeneral case and have to perform for all kinds of graphs. This paper\ninvestigates how knowledge of the application and the dataset can help optimize\nperformance with minimal effort. We concentrate on the impact of partitioning\nstrategies on the performance of computations on social graphs. We evaluate six\ngraph partitioning algorithms on a set of six social graphs, using four\nstandard graph algorithms by measuring a set of five partitioning metrics.\n  We analyze the performance of each partitioning strategy with respect to (i)\nthe properties of the graph dataset, (ii) each analytics computation,of\npartitions. We discover that communication cost is the best predictor of\nperformance for most -but not all- analytics computations. We also find that\nthe best partitioning strategy for a particular kind of algorithm may not be\nthe best for another, and that optimizing for the general case of all\nalgorithms may not select the optimal partitioning strategy for a given graph\nalgorithm. We conclude with insights on selecting the right data partitioning\nstrategy, which has significant impact on the performance of large graph\nanalytics computations; certainly enough to warrant optimization of the\npartitioning strategy to the computation and to the dataset. \n\n"}
{"id": "1804.07824", "contents": "Title: Autotune: A Derivative-free Optimization Framework for Hyperparameter\n  Tuning Abstract: Machine learning applications often require hyperparameter tuning. The\nhyperparameters usually drive both the efficiency of the model training process\nand the resulting model quality. For hyperparameter tuning, machine learning\nalgorithms are complex black-boxes. This creates a class of challenging\noptimization problems, whose objective functions tend to be nonsmooth,\ndiscontinuous, unpredictably varying in computational expense, and include\ncontinuous, categorical, and/or integer variables. Further, function\nevaluations can fail for a variety of reasons including numerical difficulties\nor hardware failures. Additionally, not all hyperparameter value combinations\nare compatible, which creates so called hidden constraints. Robust and\nefficient optimization algorithms are needed for hyperparameter tuning. In this\npaper we present an automated parallel derivative-free optimization framework\ncalled \\textbf{Autotune}, which combines a number of specialized sampling and\nsearch methods that are very effective in tuning machine learning models\ndespite these challenges. Autotune provides significantly improved models over\nusing default hyperparameter settings with minimal user interaction on\nreal-world applications. Given the inherent expense of training numerous\ncandidate models, we demonstrate the effectiveness of Autotune's search methods\nand the efficient distributed and parallel paradigms for training and tuning\nmodels, and also discuss the resource trade-offs associated with the ability to\nboth distribute the training process and parallelize the tuning process. \n\n"}
{"id": "1804.08378", "contents": "Title: BrainSlug: Transparent Acceleration of Deep Learning Through Depth-First\n  Parallelism Abstract: Neural network frameworks such as PyTorch and TensorFlow are the workhorses\nof numerous machine learning applications ranging from object recognition to\nmachine translation. While these frameworks are versatile and straightforward\nto use, the training of and inference in deep neural networks is resource\n(energy, compute, and memory) intensive. In contrast to recent works focusing\non algorithmic enhancements, we introduce BrainSlug, a framework that\ntransparently accelerates neural network workloads by changing the default\nlayer-by-layer processing to a depth-first approach, reducing the amount of\ndata required by the computations and thus improving the performance of the\navailable hardware caches. BrainSlug achieves performance improvements of up to\n41.1% on CPUs and 35.7% on GPUs. These optimizations come at zero cost to the\nuser as they do not require hardware changes and only need tiny adjustments to\nthe software. \n\n"}
{"id": "1804.08940", "contents": "Title: How swarm size during evolution impacts the behavior, generalizability,\n  and brain complexity of animats performing a spatial navigation task Abstract: While it is relatively easy to imitate and evolve natural swarm behavior in\nsimulations, less is known about the social characteristics of simulated,\nevolved swarms, such as the optimal (evolutionary) group size, why individuals\nin a swarm perform certain actions, and how behavior would change in swarms of\ndifferent sizes. To address these questions, we used a genetic algorithm to\nevolve animats equipped with Markov Brains in a spatial navigation task that\nfacilitates swarm behavior. The animats' goal was to frequently cross between\ntwo rooms without colliding with other animats. Animats were evolved in swarms\nof various sizes. We then evaluated the task performance and social behavior of\nthe final generation from each evolution when placed with swarms of different\nsizes in order to evaluate their generalizability across conditions. According\nto our experiments, we find that swarm size during evolution matters: animats\nevolved in a balanced swarm developed more flexible behavior, higher fitness\nacross conditions, and, in addition, higher brain complexity. \n\n"}
{"id": "1804.09494", "contents": "Title: On Optimizing Distributed Tucker Decomposition for Sparse Tensors Abstract: The Tucker decomposition generalizes the notion of Singular Value\nDecomposition (SVD) to tensors, the higher dimensional analogues of matrices.\nWe study the problem of constructing the Tucker decomposition of sparse tensors\non distributed memory systems via the HOOI procedure, a popular iterative\nmethod. The scheme used for distributing the input tensor among the processors\n(MPI ranks) critically influences the HOOI execution time. Prior work has\nproposed different distribution schemes: an offline scheme based on\nsophisticated hypergraph partitioning method and simple, lightweight\nalternatives that can be used real-time. While the hypergraph based scheme\ntypically results in faster HOOI execution time, being complex, the time taken\nfor determining the distribution is an order of magnitude higher than the\nexecution time of a single HOOI iteration. Our main contribution is a\nlightweight distribution scheme, which achieves the best of both worlds. We\nshow that the scheme is near-optimal on certain fundamental metrics associated\nwith the HOOI procedure and as a result, near-optimal on the computational load\n(FLOPs). Though the scheme may incur higher communication volume, the\ncomputation time is the dominant factor and as the result, the scheme achieves\nbetter performance on the overall HOOI execution time. Our experimental\nevaluation on large real-life tensors (having up to 4 billion elements) shows\nthat the scheme outperforms the prior schemes on the HOOI execution time by a\nfactor of up to 3x. On the other hand, its distribution time is comparable to\nthe prior lightweight schemes and is typically lesser than the execution time\nof a single HOOI iteration. \n\n"}
{"id": "1804.09859", "contents": "Title: Competitive Learning Enriches Learning Representation and Accelerates\n  the Fine-tuning of CNNs Abstract: In this study, we propose the integration of competitive learning into\nconvolutional neural networks (CNNs) to improve the representation learning and\nefficiency of fine-tuning. Conventional CNNs use back propagation learning, and\nit enables powerful representation learning by a discrimination task. However,\nit requires huge amount of labeled data, and acquisition of labeled data is\nmuch harder than that of unlabeled data. Thus, efficient use of unlabeled data\nis getting crucial for DNNs. To address the problem, we introduce unsupervised\ncompetitive learning into the convolutional layer, and utilize unlabeled data\nfor effective representation learning. The results of validation experiments\nusing a toy model demonstrated that strong representation learning effectively\nextracted bases of images into convolutional filters using unlabeled data, and\naccelerated the speed of the fine-tuning of subsequent supervised back\npropagation learning. The leverage was more apparent when the number of filters\nwas sufficiently large, and, in such a case, the error rate steeply decreased\nin the initial phase of fine-tuning. Thus, the proposed method enlarged the\nnumber of filters in CNNs, and enabled a more detailed and generalized\nrepresentation. It could provide a possibility of not only deep but broad\nneural networks. \n\n"}
{"id": "1804.10001", "contents": "Title: Profile-guided memory optimization for deep neural networks Abstract: Recent years have seen deep neural networks (DNNs) becoming wider and deeper\nto achieve better performance in many applications of AI. Such DNNs however\nrequire huge amounts of memory to store weights and intermediate results (e.g.,\nactivations, feature maps, etc.) in propagation. This requirement makes it\ndifficult to run the DNNs on devices with limited, hard-to-extend memory,\ndegrades the running time performance, and restricts the design of network\nmodels. We address this challenge by developing a novel profile-guided memory\noptimization to efficiently and quickly allocate memory blocks during the\npropagation in DNNs. The optimization utilizes a simple and fast heuristic\nalgorithm based on the two-dimensional rectangle packing problem. Experimenting\nwith well-known neural network models, we confirm that our method not only\nreduces the memory consumption by up to $49.5\\%$ but also accelerates training\nand inference by up to a factor of four thanks to the rapidity of the memory\nallocation and the ability to use larger mini-batch sizes. \n\n"}
{"id": "1804.10123", "contents": "Title: IamNN: Iterative and Adaptive Mobile Neural Network for Efficient Image\n  Classification Abstract: Deep residual networks (ResNets) made a recent breakthrough in deep learning.\nThe core idea of ResNets is to have shortcut connections between layers that\nallow the network to be much deeper while still being easy to optimize avoiding\nvanishing gradients. These shortcut connections have interesting side-effects\nthat make ResNets behave differently from other typical network architectures.\nIn this work we use these properties to design a network based on a ResNet but\nwith parameter sharing and with adaptive computation time. The resulting\nnetwork is much smaller than the original network and can adapt the\ncomputational cost to the complexity of the input image. \n\n"}
{"id": "1804.10331", "contents": "Title: Rateless Codes for Near-Perfect Load Balancing in Distributed\n  Matrix-Vector Multiplication Abstract: Large-scale machine learning and data mining applications require computer\nsystems to perform massive matrix-vector and matrix-matrix multiplication\noperations that need to be parallelized across multiple nodes. The presence of\nstraggling nodes -- computing nodes that unpredictably slowdown or fail -- is a\nmajor bottleneck in such distributed computations. Ideal load balancing\nstrategies that dynamically allocate more tasks to faster nodes require\nknowledge or monitoring of node speeds as well as the ability to quickly move\ndata. Recently proposed fixed-rate erasure coding strategies can handle\nunpredictable node slowdown, but they ignore partial work done by straggling\nnodes thus resulting in a lot of redundant computation. We propose a\n\\emph{rateless fountain coding} strategy that achieves the best of both worlds\n-- we prove that its latency is asymptotically equal to ideal load balancing,\nand it performs asymptotically zero redundant computations. Our idea is to\ncreate linear combinations of the $m$ rows of the matrix and assign these\nencoded rows to different worker nodes. The original matrix-vector product can\nbe decoded as soon as slightly more than $m$ row-vector products are\ncollectively finished by the nodes. We conduct experiments in three computing\nenvironments: local parallel computing, Amazon EC2, and Amazon Lambda, which\nshow that rateless coding gives as much as $3\\times$ speed-up over uncoded\nschemes. \n\n"}
{"id": "1804.10642", "contents": "Title: Co-Design of Deep Neural Nets and Neural Net Accelerators for Embedded\n  Vision Applications Abstract: Deep Learning is arguably the most rapidly evolving research area in recent\nyears. As a result it is not surprising that the design of state-of-the-art\ndeep neural net models proceeds without much consideration of the latest\nhardware targets, and the design of neural net accelerators proceeds without\nmuch consideration of the characteristics of the latest deep neural net models.\nNevertheless, in this paper we show that there are significant improvements\navailable if deep neural net models and neural net accelerators are\nco-designed. \n\n"}
{"id": "1804.10727", "contents": "Title: Low-memory convolutional neural networks through incremental depth-first\n  processing Abstract: We introduce an incremental processing scheme for convolutional neural\nnetwork (CNN) inference, targeted at embedded applications with limited memory\nbudgets. Instead of processing layers one by one, individual input pixels are\npropagated through all parts of the network they can influence under the given\nstructural constraints. This depth-first updating scheme comes with hard bounds\non the memory footprint: the memory required is constant in the case of 1D\ninput and proportional to the square root of the input dimension in the case of\n2D input. \n\n"}
{"id": "1804.10919", "contents": "Title: Randomization and quantization for average consensus Abstract: A variety of problems in distributed control involve a networked system of\nautonomous agents cooperating to carry out some complex task in a decentralized\nfashion, e.g., orienting a flock of drones, or aggregating data from a network\nof sensors. Many of these complex tasks reduce to the computation of a global\nfunction of values privately held by the agents, such as the maximum or the\naverage. Distributed algorithms implementing these functions should rely on\nlimited assumptions on the topology of the network or the information available\nto the agents, reflecting the decentralized nature of the problem.\n  We present a randomized algorithm for computing the average in networks with\ndirected, time-varying communication topologies. With high probability, the\nsystem converges to an estimate of the average in linear time in the number of\nagents, provided that the communication topology remains strongly connected\nover time. This algorithm leverages properties of exponential random variables,\nwhich allows for approximating sums by computing minima. It is completely\ndecentralized, in the sense that it does not rely on agent identifiers, or\nglobal information of any kind. Besides, the agents do not need to know their\nout-degree; hence, our algorithm demonstrates how randomization can be used to\ncircumvent the impossibility result established in [1].\n  Using a logarithmic rounding rule, we show that this algorithm can be used\nunder the additional constraints of finite memory and channel capacity. We\nfurthermore extend the algorithm with a termination test, by which the agents\ncan decide irrevocably in finite time - rather than simply converge - on an\nestimate of the average. This terminating variant works under asynchronous\nstarts and yields linear decision times while still using quantized - albeit\nlarger - values. \n\n"}
{"id": "1804.11256", "contents": "Title: On the Feasibility of Real-Time 3D Hand Tracking using Edge GPGPU\n  Acceleration Abstract: This paper presents the case study of a non-intrusive porting of a monolithic\nC++ library for real-time 3D hand tracking, to the domain of edge-based\ncomputation. Towards a proof of concept, the case study considers a pair of\nworkstations, a computationally powerful and a computationally weak one. By\nwrapping the C++ library in Java container and by capitalizing on a Java-based\noffloading infrastructure that supports both CPU and GPGPU computations, we are\nable to establish automatically the required server-client workflow that best\naddresses the resource allocation problem in the effort to execute from the\nweak workstation. As a result, the weak workstation can perform well at the\ntask, despite lacking the sufficient hardware to do the required computations\nlocally. This is achieved by offloading computations which rely on GPGPU, to\nthe powerful workstation, across the network that connects them. We show the\nedge-based computation challenges associated with the information flow of the\nported algorithm, demonstrate how we cope with them, and identify what needs to\nbe improved for achieving even better performance. \n\n"}
{"id": "1805.00342", "contents": "Title: A Feedback Neural Network for Small Target Motion Detection in Cluttered\n  Backgrounds Abstract: Small target motion detection is critical for insects to search for and track\nmates or prey which always appear as small dim speckles in the visual field. A\nclass of specific neurons, called small target motion detectors (STMDs), has\nbeen characterized by exquisite sensitivity for small target motion.\nUnderstanding and analyzing visual pathway of STMD neurons are beneficial to\ndesign artificial visual systems for small target motion detection. Feedback\nloops have been widely identified in visual neural circuits and play an\nimportant role in target detection. However, if there exists a feedback loop in\nthe STMD visual pathway or if a feedback loop could significantly improve the\ndetection performance of STMD neurons, is unclear. In this paper, we propose a\nfeedback neural network for small target motion detection against naturally\ncluttered backgrounds. In order to form a feedback loop, model output is\ntemporally delayed and relayed to previous neural layer as feedback signal.\nExtensive experiments showed that the significant improvement of the proposed\nfeedback neural network over the existing STMD-based models for small target\nmotion detection. \n\n"}
{"id": "1805.00658", "contents": "Title: Distributed Big-Data Optimization via Block-Iterative Convexification\n  and Averaging Abstract: In this paper, we study distributed big-data nonconvex optimization in\nmulti-agent networks. We consider the (constrained) minimization of the sum of\na smooth (possibly) nonconvex function, i.e., the agents' sum-utility, plus a\nconvex (possibly) nonsmooth regularizer. Our interest is in big-data problems\nwherein there is a large number of variables to optimize. If treated by means\nof standard distributed optimization algorithms, these large-scale problems may\nbe intractable, due to the prohibitive local computation and communication\nburden at each node. We propose a novel distributed solution method whereby at\neach iteration agents optimize and then communicate (in an uncoordinated\nfashion) only a subset of their decision variables. To deal with non-convexity\nof the cost function, the novel scheme hinges on Successive Convex\nApproximation (SCA) techniques coupled with i) a tracking mechanism\ninstrumental to locally estimate gradient averages; and ii) a novel block-wise\nconsensus-based protocol to perform local block-averaging operations and\ngradient tacking. Asymptotic convergence to stationary solutions of the\nnonconvex problem is established. Finally, numerical results show the\neffectiveness of the proposed algorithm and highlight how the block dimension\nimpacts on the communication overhead and practical convergence speed. \n\n"}
{"id": "1805.00774", "contents": "Title: Breaking the $\\tilde\\Omega(\\sqrt{n})$ Barrier: Fast Consensus under a\n  Late Adversary Abstract: We study the consensus problem in a synchronous distributed system of $n$\nnodes under an adaptive adversary that has a slightly outdated view of the\nsystem and can block all incoming and outgoing communication of a constant\nfraction of the nodes in each round. Motivated by a result of Ben-Or and\nBar-Joseph (1998), showing that any consensus algorithm that is resilient\nagainst a linear number of crash faults requires $\\tilde \\Omega(\\sqrt{n})$\nrounds in an $n$-node network against an adaptive adversary, we consider a late\nadaptive adversary, who has full knowledge of the network state at the\nbeginning of the previous round and unlimited computational power, but is\noblivious to the current state of the nodes.\n  Our main contributions are randomized distributed algorithms that achieve\nalmost-everywhere consensus w.h.p. against a late adaptive adversary who can\nblock up to $\\epsilon n$ nodes in each round, for a small constant $\\epsilon\n>0$. Our first protocol achieves binary and plurality consensus, and the second\none achieves multi-value consensus. Both of our algorithms succeed in $O(\\log\nn)$ rounds with high probability, thus showing an exponential gap to the\naforementioned lower bound for strongly adaptive crash-failure adversaries,\nwhich can be strengthened to $\\Omega(n)$ when allowing the adversary to block\nnodes instead of permanently crashing them. In our algorithms each node\ncontacts only an (amortized) constant number of peers in each communication\nround. We show that our algorithms are optimal up to constant (resp.\nsub-logarithmic) factors by proving that every almost-everywhere consensus\nprotocol takes $\\Omega(\\log_d n)$ rounds in the worst case, where $d$ is an\nupper bound on the number of communication requests initiated per node in each\nround. We complement our theoretical results with an experimental evaluation of\nthe first protocol revealing a short convergence time. \n\n"}
{"id": "1805.00857", "contents": "Title: A new analysis of Work Stealing with latency Abstract: We study in this paper the impact of communication latency on the classical\nWork Stealing load balancing algorithm. Our paper extends the reference model\nin which we introduce a latency parameter. By using a theoretical analysis and\nsimulation, we study the overall impact of this latency on the Makespan\n(maximum completion time). We derive a new expression of the expected running\ntime of a bag of tasks scheduled by Work Stealing. This expression enables us\nto predict under which conditions a given run will yield acceptable\nperformance. For instance, we can easily calibrate the maximal number of\nprocessors to use for a given work/platform combination. All our results are\nvalidated through simulation on a wide range of parameters. \n\n"}
{"id": "1805.01516", "contents": "Title: How deep should be the depth of convolutional neural networks: a\n  backyard dog case study Abstract: The work concerns the problem of reducing a pre-trained deep neuronal network\nto a smaller network, with just few layers, whilst retaining the network's\nfunctionality on a given task\n  The proposed approach is motivated by the observation that the aim to deliver\nthe highest accuracy possible in the broadest range of operational conditions,\nwhich many deep neural networks models strive to achieve, may not necessarily\nbe always needed, desired, or even achievable due to the lack of data or\ntechnical constraints. In relation to the face recognition problem, we\nformulated an example of such a usecase, the `backyard dog' problem. The\n`backyard dog', implemented by a lean network, should correctly identify\nmembers from a limited group of individuals, a `family', and should distinguish\nbetween them. At the same time, the network must produce an alarm to an image\nof an individual who is not in a member of the family. To produce such a\nnetwork, we propose a shallowing algorithm. The algorithm takes an existing\ndeep learning model on its input and outputs a shallowed version of it. The\nalgorithm is non-iterative and is based on the Advanced Supervised Principal\nComponent Analysis. Performance of the algorithm is assessed in exhaustive\nnumerical experiments. In the above usecase, the `backyard dog' problem, the\nmethod is capable of drastically reducing the depth of deep learning neural\nnetworks, albeit at the cost of mild performance deterioration.\n  We developed a simple non-iterative method for shallowing down pre-trained\ndeep networks. The method is generic in the sense that it applies to a broad\nclass of feed-forward networks, and is based on the Advanced Supervise\nPrincipal Component Analysis. The method enables generation of families of\nsmaller-size shallower specialized networks tuned for specific operational\nconditions and tasks from a single larger and more universal legacy network. \n\n"}
{"id": "1805.01768", "contents": "Title: Work Stealing with latency Abstract: We study in this paper the impact of communication latency on the classical\nWork Stealing load balancing algorithm. Our approach considers existing\nperformance models and the underlying algorithms. We introduce a latency\nparameter in the model and study its overall impact by careful observations of\nsimulation results. Using this method we are able to derive a new expression of\nthe expected running time of divisible load applications. This expression\nenables us to predict under which conditions a given run will yield acceptable\nperformance. For instance, we can easily calibrate the maximal number of\nprocessors one should use for a given work platform combination. We also\nconsider the impact of several algorithmic variants like simultaneous transfers\nof work or thresholds for avoiding useless transfers. All our results are\nvalidated through simulation on a wide range of parameters. \n\n"}
{"id": "1805.02010", "contents": "Title: Generalised Dining Philosophers as Feedback Control Abstract: We revisit the Generalised Dining Philosophers problem through the\nperspective of feedback control. The result is a modular development of the\nsolution using the notions of system and system composition (the latter due to\nTabuada) in a formal setting that employs simple equational reasoning. The\nmodular approach separates the solution architecture from the algorithmic\nminutiae and has the benefit of simplifying the design and correctness proofs.\n  Three variants of the problem are considered: N=1, and N>1 with centralised\nand distributed topology. The base case (N=1) reveals important insights into\nthe problem specification and the architecture of the solution. In each case,\nsolving the Generalised Dining Philosophers reduces to designing an appropriate\nfeedback controller. \n\n"}
{"id": "1805.03472", "contents": "Title: Skeap & Seap: Scalable Distributed Priority Queues for Constant and\n  Arbitrary Priorities Abstract: We propose two protocols for distributed priority queues (for simplicity\ndenoted 'heap') called SKEAP and SEAP. SKEAP realizes a distributed heap for a\nconstant amount of priorities and SEAP one for an arbitrary amount. Both\nprotocols build on an overlay, which induces an aggregation tree on top of\nwhich heap operations are aggregated in batches, ensuring that our protocols\nscale even for a high rate of incoming requests. As part of SEAP we provide a\nnovel distributed protocol for the $k$-selection problem that runs in $O(\\log\nn)$ rounds w.h.p. SKEAP guarantees sequential consistency for its heap\noperations, while SEAP guarantees serializability. SKEAP and SEAP provide\nlogarithmic runtimes w.h.p. on all their operations with SEAP having to use\nonly $O(\\log n)$ bit messages. \n\n"}
{"id": "1805.03473", "contents": "Title: Learning representations for multivariate time series with missing data\n  using Temporal Kernelized Autoencoders Abstract: Learning compressed representations of multivariate time series (MTS)\nfacilitates data analysis in the presence of noise and redundant information,\nand for a large number of variates and time steps. However, classical\ndimensionality reduction approaches are designed for vectorial data and cannot\ndeal explicitly with missing values. In this work, we propose a novel\nautoencoder architecture based on recurrent neural networks to generate\ncompressed representations of MTS. The proposed model can process inputs\ncharacterized by variable lengths and it is specifically designed to handle\nmissing data. Our autoencoder learns fixed-length vectorial representations,\nwhose pairwise similarities are aligned to a kernel function that operates in\ninput space and that handles missing values. This allows to learn good\nrepresentations, even in the presence of a significant amount of missing data.\nTo show the effectiveness of the proposed approach, we evaluate the quality of\nthe learned representations in several classification tasks, including those\ninvolving medical data, and we compare to other methods for dimensionality\nreduction. Successively, we design two frameworks based on the proposed\narchitecture: one for imputing missing data and another for one-class\nclassification. Finally, we analyze under what circumstances an autoencoder\nwith recurrent layers can learn better compressed representations of MTS than\nfeed-forward architectures. \n\n"}
{"id": "1805.03812", "contents": "Title: A DAG Model of Synchronous Stochastic Gradient Descent in Distributed\n  Deep Learning Abstract: With huge amounts of training data, deep learning has made great\nbreakthroughs in many artificial intelligence (AI) applications. However, such\nlarge-scale data sets present computational challenges, requiring training to\nbe distributed on a cluster equipped with accelerators like GPUs. With the fast\nincrease of GPU computing power, the data communications among GPUs have become\na potential bottleneck on the overall training performance. In this paper, we\nfirst propose a general directed acyclic graph (DAG) model to describe the\ndistributed synchronous stochastic gradient descent (S-SGD) algorithm, which\nhas been widely used in distributed deep learning frameworks. To understand the\npractical impact of data communications on training performance, we conduct\nextensive empirical studies on four state-of-the-art distributed deep learning\nframeworks (i.e., Caffe-MPI, CNTK, MXNet and TensorFlow) over multi-GPU and\nmulti-node environments with different data communication techniques, including\nPCIe, NVLink, 10GbE, and InfiniBand. Through both analytical and experimental\nstudies, we identify the potential bottlenecks and overheads that could be\nfurther optimized. At last, we make the data set of our experimental traces\npublicly available, which could be used to support simulation-based studies. \n\n"}
{"id": "1805.03870", "contents": "Title: Scaling Nakamoto Consensus to Thousands of Transactions per Second Abstract: This paper presents Conflux, a fast, scalable and decentralized blockchain\nsystem that optimistically process concurrent blocks without discarding any as\nforks. The Conflux consensus protocol represents relationships between blocks\nas a direct acyclic graph and achieves consensus on a total order of the\nblocks. Conflux then, from the block order, deterministically derives a\ntransaction total order as the blockchain ledger. We evaluated Conflux on\nAmazon EC2 clusters with up to 20k full nodes. Conflux achieves a transaction\nthroughput of 5.76GB/h while confirming transactions in 4.5-7.4 minutes. The\nthroughput is equivalent to 6400 transactions per second for typical Bitcoin\ntransactions. Our results also indicate that when running Conflux, the\nconsensus protocol is no longer the throughput bottleneck. The bottleneck is\ninstead at the processing capability of individual nodes. \n\n"}
{"id": "1805.03887", "contents": "Title: Scaling associative classification for very large datasets Abstract: Supervised learning algorithms are nowadays successfully scaling up to\ndatasets that are very large in volume, leveraging the potential of in-memory\ncluster-computing Big Data frameworks. Still, massive datasets with a number of\nlarge-domain categorical features are a difficult challenge for any classifier.\nMost off-the-shelf solutions cannot cope with this problem. In this work we\nintroduce DAC, a Distributed Associative Classifier. DAC exploits ensemble\nlearning to distribute the training of an associative classifier among parallel\nworkers and improve the final quality of the model. Furthermore, it adopts\nseveral novel techniques to reach high scalability without sacrificing quality,\namong which a preventive pruning of classification rules in the extraction\nphase based on Gini impurity. We ran experiments on Apache Spark, on a real\nlarge-scale dataset with more than 4 billion records and 800 million distinct\ncategories. The results showed that DAC improves on a state-of-the-art solution\nin both prediction quality and execution time. Since the generated model is\nhuman-readable, it can not only classify new records, but also allow\nunderstanding both the logic behind the prediction and the properties of the\nmodel, becoming a useful aid for decision makers. \n\n"}
{"id": "1805.03992", "contents": "Title: Order out of Chaos: Proving Linearizability Using Local Views Abstract: Proving the linearizability of highly concurrent data structures, such as\nthose using optimistic concurrency control, is a challenging task. The main\ndifficulty is in reasoning about the view of the memory obtained by the\nthreads, because as they execute, threads observe different fragments of memory\nfrom different points in time. Until today, every linearizability proof has\ntackled this challenge from scratch.\n  We present a unifying proof argument for the correctness of unsynchronized\ntraversals, and apply it to prove the linearizability of several highly\nconcurrent search data structures, including an optimistic self-balancing\nbinary search tree, the Lazy List and a lock-free skip list. Our framework\nharnesses {\\em sequential reasoning} about the view of a thread, considering\nthe thread as if it traverses the data structure without interference from\nother operations. Our key contribution is showing that properties of\nreachability along search paths can be deduced for concurrent traversals from\nsuch interference-free traversals, when certain intuitive conditions are met.\nBasing the correctness of traversals on such \\emph{local view arguments}\ngreatly simplifies linearizability proofs.\n  To apply our framework, the user proves that the data structure satisfies two\nconditions: (1) acyclicity of the order on memory, even when it is considered\nacross intermediate memory states, and (2) preservation of search paths to\nlocations modified by interfering writes. Establishing the conditions, as well\nas the full linearizability proof utilizing our proof argument, reduces to\nsimple concurrent reasoning. The result is a clear and comprehensible\ncorrectness proof, and elucidates common patterns underlying several existing\ndata structures. \n\n"}
{"id": "1805.04071", "contents": "Title: The Energy Complexity of Diameter and Minimum Cut Computation in\n  Bounded-Genus Networks Abstract: This paper investigates the energy complexity of distributed graph problems\nin multi-hop radio networks, where the energy cost of an algorithm is measured\nby the maximum number of awake rounds of a vertex. Recent works revealed that\nsome problems, such as broadcast, breadth-first search, and maximal matching,\ncan be solved with energy-efficient algorithms that consume only $\\text{poly}\n\\log n$ energy. However, there exist some problems, such as computing the\ndiameter of the graph, that require $\\Omega(n)$ energy to solve. To improve\nenergy efficiency for these problems, we focus on a special graph class:\nbounded-genus graphs. We present algorithms for computing the exact diameter,\nthe exact global minimum cut size, and a $(1 \\pm\\epsilon)$-approximate $s$-$t$\nminimum cut size with $\\tilde{O}(\\sqrt{n})$ energy for bounded-genus graphs.\nOur approach is based on a generic framework that divides the vertex set into\nhigh-degree and low-degree parts and leverages the structural properties of\nbounded-genus graphs to control the number of certain connected components in\nthe subgraph induced by the low-degree part. \n\n"}
{"id": "1805.04263", "contents": "Title: OpSets: Sequential Specifications for Replicated Datatypes (Extended\n  Version) Abstract: We introduce OpSets, an executable framework for specifying and reasoning\nabout the semantics of replicated datatypes that provide eventual consistency\nin a distributed system, and for mechanically verifying algorithms that\nimplement these datatypes. Our approach is simple but expressive, allowing us\nto succinctly specify a variety of abstract datatypes, including maps, sets,\nlists, text, graphs, trees, and registers. Our datatypes are also composable,\nenabling the construction of complex data structures. To demonstrate the\nutility of OpSets for analysing replication algorithms, we highlight an\nimportant correctness property for collaborative text editing that has\ntraditionally been overlooked; algorithms that do not satisfy this property can\nexhibit awkward interleaving of text. We use OpSets to specify this correctness\nproperty and prove that although one existing replication algorithm satisfies\nthis property, several other published algorithms do not. We also show how\nOpSets can be used to develop new replicated datatypes: we provide a simple\nspecification of an atomic move operation for trees, an operation that had\npreviously been thought to be impossible to implement without locking. We use\nthe Isabelle/HOL proof assistant to formalise the OpSets approach and produce\nmechanised proofs of correctness of the main claims in this paper, thereby\neliminating the ambiguity of previous informal approaches, and ruling out\nreasoning errors that could occur in handwritten proofs. \n\n"}
{"id": "1805.04779", "contents": "Title: Persistent Non-Blocking Binary Search Trees Supporting Wait-Free Range\n  Queries Abstract: This paper presents the first implementation of a search tree data structure\nin an asynchronous shared-memory system that provides a wait-free algorithm for\nexecuting range queries on the tree, in addition to non-blocking algorithms for\nInsert, Delete and Find, using single-word Compare-and-Swap (CAS). The\nimplementation is linearizable and tolerates any number of crash failures.\nInsert and Delete operations that operate on different parts of the tree run\nfully in parallel (without any interference with one another). We employ a\nlightweight helping mechanism, where each Insert, Delete and Find operation\nhelps only update operations that affect the local neighbourhood of the leaf it\narrives at. Similarly, a Scan helps only those updates taking place on nodes of\nthe part of the tree it traverses, and therefore Scans operating on different\nparts of the tree do not interfere with one another. Our implementation works\nin a dynamic system where the number of processes may change over time.\n  The implementation builds upon the non-blocking binary search tree\nimplementation presented by Ellen et al. (in PODC 2010) by applying a simple\nmechanism to make the tree persistent. \n\n"}
{"id": "1805.05197", "contents": "Title: Fork and Join Queueing Networks with Heavy Tails: Scaling Dimension and\n  Throughput Limit Abstract: Parallel and distributed computing systems are foundational to the success of\ncloud computing and big data analytics. These systems process computational\nworkflows in a way that can be mathematically modeled by a fork-and-join\nqueueing network with blocking (FJQN/B). While engineering solutions have long\nbeen made to build and scale such systems, it is challenging to rigorously\ncharacterize their throughput performance at scale theoretically. What further\ncomplicates the study is the presence of heavy-tailed delays that have been\nwidely documented therein. To this end, we introduce two fundamental concepts\nfor networks of arbitrary topology (scaling dimension and extended metric\ndimension) and utilize an infinite sequence of growing FJQN/Bs to study the\nthroughput limit. The throughput is said to be scalable if the throughput limit\ninfimum of the sequence is strictly positive as the network size grows to\ninfinity. We investigate throughput scalability by focusing on heavy-tailed\nservice times that are regularly varying (with index $\\alpha>1$) and featuring\nthe network topology described by the two aforementioned dimensions. In\nparticular, we show that an infinite sequence of FJQN/Bs is throughput scalable\nif the extended metric dimension $<\\alpha-1$ and only if the scaling dimension\n$\\le\\alpha-1$. These theoretical results provide new insights on the\nscalability of a rich class of FJQN/Bs with various structures, including\ntandem, lattice, hexagon, pyramid, tree, and fractals. \n\n"}
{"id": "1805.05988", "contents": "Title: Executions in (Semi-)Integer Petri Nets are Compact Closed Categories Abstract: In this work, we analyse Petri nets where places are allowed to have a\nnegative number of tokens. For each net we build its correspondent category of\nexecutions, which is compact closed, and prove that this procedure is\nfunctorial. We moreover exhibit a procedure to recover the original net from\nits category of executions, show that it is again functorial, and that this\ngives rise to an adjoint pair. Finally, we use compact closeness to infer that\nallowing negative tokens in a Petri net makes the causal relations between\ntransition firings non-trivial, and we use this to model interesting phenomena\nin economics and computer science. \n\n"}
{"id": "1805.06149", "contents": "Title: Convex Hull Formation for Programmable Matter Abstract: We envision programmable matter as a system of nano-scale agents (called\nparticles) with very limited computational capabilities that move and compute\ncollectively to achieve a desired goal. We use the geometric amoebot model as\nour computational framework, which assumes particles move on the triangular\nlattice. Motivated by the problem of sealing an object using minimal resources,\nwe show how a particle system can self-organize to form an object's convex\nhull. We give a distributed, local algorithm for convex hull formation and\nprove that it runs in $\\mathcal{O}(B)$ asynchronous rounds, where $B$ is the\nlength of the object's boundary. Within the same asymptotic runtime, this\nalgorithm can be extended to also form the object's (weak) $\\mathcal{O}$-hull,\nwhich uses the same number of particles but minimizes the area enclosed by the\nhull. Our algorithms are the first to compute convex hulls with distributed\nentities that have strictly local sensing, constant-size memory, and no shared\nsense of orientation or coordinates. Ours is also the first distributed\napproach to computing restricted-orientation convex hulls. This approach\ninvolves coordinating particles as distributed memory; thus, as a supporting\nbut independent result, we present and analyze an algorithm for organizing\nparticles with constant-size memory as distributed binary counters that\nefficiently support increments, decrements, and zero-tests --- even as the\nparticles move. \n\n"}
{"id": "1805.06238", "contents": "Title: Distributed Automata and Logic Abstract: Distributed automata are finite-state machines that operate on finite\ndirected graphs. Acting as synchronous distributed algorithms, they use their\ninput graph as a network in which identical processors communicate for a\npossibly infinite number of synchronous rounds. For the local variant of those\nautomata, where the number of rounds is bounded by a constant, Hella et al.\n(2012, 2015) have established a logical characterization in terms of basic\nmodal logic. In this thesis, we provide similar logical characterizations for\ntwo more expressive classes of distributed automata.\n  The first class extends local automata with a global acceptance condition and\nthe ability to alternate between nondeterministic and parallel computations. We\nshow that it is equivalent to monadic second-order logic on graphs. By\nrestricting transitions to be nondeterministic or deterministic, we also obtain\ntwo strictly weaker variants for which the emptiness problem is decidable.\n  Our second class transfers the standard notion of asynchronous algorithm to\nthe setting of nonlocal distributed automata. The resulting machines are shown\nto be equivalent to a small fragment of least fixpoint logic, and more\nspecifically, to a restricted variant of the modal {\\mu}-calculus that allows\nleast fixpoints but forbids greatest fixpoints. Exploiting the connection with\nlogic, we additionally prove that the expressive power of those asynchronous\nautomata is independent of whether or not messages can be lost. \n\n"}
{"id": "1805.07047", "contents": "Title: Blockchain Cohomology Abstract: We follow existing distributed systems frameworks employing methods from\nalgebraic topology to formally define primitives of blockchain technology. We\ndefine the notion of cross chain liquidity, sharding and probability spaces\nbetween and within blockchain protocols. We incorporate recent advancements in\nsynthetic homology to show that this topological framework can be implemented\nwithin a type system. We use recursion schemes to define kernels admitting\nsmooth manifolds across protocol complexes, leading to the formal definition of\na Poincare protocol. \n\n"}
{"id": "1805.07294", "contents": "Title: Distributed Computation in Node-Capacitated Networks Abstract: In this paper, we study distributed graph algorithms in networks in which the\nnodes have a limited communication capacity. Many distributed systems are built\non top of an underlying networking infrastructure, for example by using a\nvirtual communication topology known as an overlay network. Although this\nunderlying network might allow each node to directly communicate with a large\nnumber of other nodes, the amount of communication that a node can perform in a\nfixed amount of time is typically much more limited. We introduce the\nNode-Capacitated Clique model as an abstract communication model, which allows\nus to study the effect of nodes having limited communication capacity on the\ncomplexity of distributed graph computations. In this model, the $n$ nodes of a\nnetwork are connected as a clique and communicate in synchronous rounds. In\neach round, every node can exchange messages of $O(\\log n)$ bits with at most\n$O(\\log n)$ other nodes. When solving a graph problem, the input graph $G$ is\ndefined on the same set of $n$ nodes, where each node knows which other nodes\nare its neighbors in $G$. To initiate research on the Node-Capacitated Clique\nmodel, we present distributed algorithms for the Minimum Spanning Tree (MST),\nBFS Tree, Maximal Independent Set, Maximal Matching, and Vertex Coloring\nproblems. We show that even with only $O(\\log n)$ concurrent interactions per\nnode, the MST problem can still be solved in polylogarithmic time. In all other\ncases, the runtime of our algorithms depends linearly on the arboricity of $G$,\nwhich is a constant for many important graph families such as planar graphs. \n\n"}
{"id": "1805.07339", "contents": "Title: Scanner: Efficient Video Analysis at Scale Abstract: A growing number of visual computing applications depend on the analysis of\nlarge video collections. The challenge is that scaling applications to operate\non these datasets requires efficient systems for pixel data access and parallel\nprocessing across large numbers of machines. Few programmers have the\ncapability to operate efficiently at these scales, limiting the field's ability\nto explore new applications that leverage big video data. In response, we have\ncreated Scanner, a system for productive and efficient video analysis at scale.\nScanner organizes video collections as tables in a data store optimized for\nsampling frames from compressed video, and executes pixel processing\ncomputations, expressed as dataflow graphs, on these frames. Scanner schedules\nvideo analysis applications expressed using these abstractions onto\nheterogeneous throughput computing hardware, such as multi-core CPUs, GPUs, and\nmedia processing ASICs, for high-throughput pixel processing. We demonstrate\nthe productivity of Scanner by authoring a variety of video processing\napplications including the synthesis of stereo VR video streams from\nmulti-camera rigs, markerless 3D human pose reconstruction from video, and\ndata-mining big video datasets such as hundreds of feature-length films or over\n70,000 hours of TV news. These applications achieve near-expert performance on\na single machine and scale efficiently to hundreds of machines, enabling\nformerly long-running big video data analysis tasks to be carried out in\nminutes to hours. \n\n"}
{"id": "1805.07433", "contents": "Title: DeepLogic: Towards End-to-End Differentiable Logical Reasoning Abstract: Combining machine learning with logic-based expert systems in order to get\nthe best of both worlds are becoming increasingly popular. However, to what\nextent machine learning can already learn to reason over rule-based knowledge\nis still an open problem. In this paper, we explore how symbolic logic, defined\nas logic programs at a character level, is learned to be represented in a\nhigh-dimensional vector space using RNN-based iterative neural networks to\nperform reasoning. We create a new dataset that defines 12 classes of logic\nprograms exemplifying increased level of complexity of logical reasoning and\ntrain the networks in an end-to-end fashion to learn whether a logic program\nentails a given query. We analyse how learning the inference algorithm gives\nrise to representations of atoms, literals and rules within logic programs and\nevaluate against increasing lengths of predicate and constant symbols as well\nas increasing steps of multi-hop reasoning. \n\n"}
{"id": "1805.07491", "contents": "Title: A Compositional Approach to Network Algorithms Abstract: We present elements of a typing theory for flow networks, where \"types\",\n\"typings\", and \"type inference\" are formulated in terms of familiar notions\nfrom polyhedral analysis and convex optimization. Based on this typing theory,\nwe develop an alternative approach to the design and analysis of network\nalgorithms, which we illustrate by applying it to the max-flow problem in\nmultiple-source, multiple-sink, capacited directed planar graphs. \n\n"}
{"id": "1805.07954", "contents": "Title: Silence Abstract: The cost of communication is a substantial factor affecting the scalability\nof many distributed applications. Every message sent can incur a cost in\nstorage, computation, energy and bandwidth. Consequently, reducing the\ncommunication costs of distributed applications is highly desirable. The best\nway to reduce message costs is by communicating without sending any messages\nwhatsoever. This paper initiates a rigorous investigation into the use of\nsilence in synchronous settings, in which processes can fail. We formalize\nsufficient conditions for information transfer using silence, as well as\nnecessary conditions for particular cases of interest. This allows us to\nidentify message patterns that enable communication through silence. In\nparticular, a pattern called a {\\em silent choir} is identified, and shown to\nbe central to information transfer via silence in failure-prone systems. The\npower of the new framework is demonstrated on the {\\em atomic commitment}\nproblem (AC). A complete characterization of the tradeoff between message\ncomplexity and round complexity in the synchronous model with crash failures is\nprovided, in terms of lower bounds and matching protocols. In particular, a new\nmessage-optimal AC protocol is designed using silence, in which processes\ndecide in~3 rounds in the common case. This significantly improves on the best\npreviously known message-optimal AC protocol, in which decisions were performed\nin $\\Theta(n)$ rounds. \n\n"}
{"id": "1805.08340", "contents": "Title: Reducing Parameter Space for Neural Network Training Abstract: For neural networks (NNs) with rectified linear unit (ReLU) or binary\nactivation functions, we show that their training can be accomplished in a\nreduced parameter space. Specifically, the weights in each neuron can be\ntrained on the unit sphere, as opposed to the entire space, and the threshold\ncan be trained in a bounded interval, as opposed to the real line. We show that\nthe NNs in the reduced parameter space are mathematically equivalent to the\nstandard NNs with parameters in the whole space. The reduced parameter space\nshall facilitate the optimization procedure for the network training, as the\nsearch space becomes (much) smaller. We demonstrate the improved training\nperformance using numerical examples. \n\n"}
{"id": "1805.08541", "contents": "Title: Blockchain and Trusted Computing: Problems, Pitfalls, and a Solution for\n  Hyperledger Fabric Abstract: A smart contract on a blockchain cannot keep a secret because its data is\nreplicated on all nodes in a network. To remedy this problem, it has been\nsuggested to combine blockchains with trusted execution environments (TEEs),\nsuch as Intel SGX, for executing applications that demand privacy. Untrusted\nblockchain nodes cannot get access to the data and computations inside the TEE.\n  This paper first explores some pitfalls that arise from the combination of\nTEEs with blockchains. Since TEEs are, in principle, stateless they are\nsusceptible to rollback attacks, which should be prevented to maintain privacy\nfor the application. However, in blockchains with non-final consensus\nprotocols, such as the proof-of-work in Ethereum and others, the contract\nexecution must handle rollbacks by design. This implies that TEEs for securing\nblockchain execution cannot be directly used for such blockchains; this\napproach works only when the consensus decisions are final.\n  Second, this work introduces an architecture and a prototype for\nsmart-contract execution within Intel SGX technology for Hyperledger Fabric, a\nprominent platform for enterprise blockchain applications. Our system resolves\ndifficulties posed by the execute-order-validate architecture of Fabric and\nprevents rollback attacks on TEE-based execution as far as possible. For\nincreasing security, our design encapsulates each application on the blockchain\nwithin its own enclave that shields it from the host system. An evaluation\nshows that the overhead moving execution into SGX is within 10%-20% for a\nsealed-bid auction application. \n\n"}
{"id": "1805.08893", "contents": "Title: On-the-fly Vertex Reuse for Massively-Parallel Software Geometry\n  Processing Abstract: Compute-mode rendering is becoming more and more attractive for non-standard\nrendering applications, due to the high flexibility of compute-mode execution.\nThese newly designed pipelines often include streaming vertex and geometry\nprocessing stages. In typical triangle meshes, the same transformed vertex is\non average required six times during rendering. To avoid redundant computation,\na post-transform cache is traditionally suggested to enable reuse of vertex\nprocessing results. However, traditional caching neither scales well as the\nhardware becomes more parallel, nor can be efficiently implemented in a\nsoftware design. We investigate alternative strategies to reusing vertex\nshading results on-the-fly for massively parallel software geometry processing.\nForming static and dynamic batching on the data input stream, we analyze the\neffectiveness of identifying potential local reuse based on sorting, hashing,\nand efficient intra-thread-group communication. Altogether, we present four\nvertex reuse strategies, tailored to modern parallel architectures. Our\nsimulations showcase that our batch-based strategies significantly outperform\nparallel caches in terms of reuse. On actual GPU hardware, our evaluation shows\nthat our strategies not only lead to good reuse of processing results, but also\nboost performance by $2-3\\times$ compared to na\\\"ively ignoring reuse in a\nvariety of practical applications. \n\n"}
{"id": "1805.09355", "contents": "Title: Scoring Lexical Entailment with a Supervised Directional Similarity\n  Network Abstract: We present the Supervised Directional Similarity Network (SDSN), a novel\nneural architecture for learning task-specific transformation functions on top\nof general-purpose word embeddings. Relying on only a limited amount of\nsupervision from task-specific scores on a subset of the vocabulary, our\narchitecture is able to generalise and transform a general-purpose\ndistributional vector space to model the relation of lexical entailment.\nExperiments show excellent performance on scoring graded lexical entailment,\nraising the state-of-the-art on the HyperLex dataset by approximately 25%. \n\n"}
{"id": "1805.09470", "contents": "Title: Taming Convergence for Asynchronous Stochastic Gradient Descent with\n  Unbounded Delay in Non-Convex Learning Abstract: Understanding the convergence performance of asynchronous stochastic gradient\ndescent method (Async-SGD) has received increasing attention in recent years\ndue to their foundational role in machine learning. To date, however, most of\nthe existing works are restricted to either bounded gradient delays or convex\nsettings. In this paper, we focus on Async-SGD and its variant Async-SGDI\n(which uses increasing batch size) for non-convex optimization problems with\nunbounded gradient delays. We prove $o(1/\\sqrt{k})$ convergence rate for\nAsync-SGD and $o(1/k)$ for Async-SGDI. Also, a unifying sufficient condition\nfor Async-SGD's convergence is established, which includes two major gradient\ndelay models in the literature as special cases and yields a new delay model\nnot considered thus far. \n\n"}
{"id": "1805.09682", "contents": "Title: Phocas: dimensional Byzantine-resilient stochastic gradient descent Abstract: We propose a novel robust aggregation rule for distributed synchronous\nStochastic Gradient Descent~(SGD) under a general Byzantine failure model. The\nattackers can arbitrarily manipulate the data transferred between the servers\nand the workers in the parameter server~(PS) architecture. We prove the\nByzantine resilience of the proposed aggregation rules. Empirical analysis\nshows that the proposed techniques outperform current approaches for realistic\nuse cases and Byzantine attack scenarios. \n\n"}
{"id": "1805.09891", "contents": "Title: Coded FFT and Its Communication Overhead Abstract: We propose a coded computing strategy and examine communication costs of\ncoded computing algorithms to make distributed Fast Fourier Transform (FFT)\nresilient to errors during the computation. We apply maximum distance separable\n(MDS) codes to a widely used \"Transpose\" algorithm for parallel FFT. In the\nuncoded distributed FFT algorithm, the most expensive step is a single\n\"all-to-all\" communication. We compare this with communication overhead of\ncoding in our coded FFT algorithm. We show that by using a systematic MDS code,\nthe communication overhead of coding is negligible in comparison with the\ncommunication costs inherent in an uncoded FFT implementation if the number of\nparity nodes is at most $o(\\log K)$, where $K$ is the number of systematic\nnodes. \n\n"}
{"id": "1805.09965", "contents": "Title: LAG: Lazily Aggregated Gradient for Communication-Efficient Distributed\n  Learning Abstract: This paper presents a new class of gradient methods for distributed machine\nlearning that adaptively skip the gradient calculations to learn with reduced\ncommunication and computation. Simple rules are designed to detect\nslowly-varying gradients and, therefore, trigger the reuse of outdated\ngradients. The resultant gradient-based algorithms are termed Lazily Aggregated\nGradient --- justifying our acronym LAG used henceforth. Theoretically, the\nmerits of this contribution are: i) the convergence rate is the same as batch\ngradient descent in strongly-convex, convex, and nonconvex smooth cases; and,\nii) if the distributed datasets are heterogeneous (quantified by certain\nmeasurable constants), the communication rounds needed to achieve a targeted\naccuracy are reduced thanks to the adaptive reuse of lagged gradients.\nNumerical experiments on both synthetic and real data corroborate a significant\ncommunication reduction compared to alternatives. \n\n"}
{"id": "1805.10002", "contents": "Title: Learning to Propagate Labels: Transductive Propagation Network for\n  Few-shot Learning Abstract: The goal of few-shot learning is to learn a classifier that generalizes well\neven when trained with a limited number of training instances per class. The\nrecently introduced meta-learning approaches tackle this problem by learning a\ngeneric classifier across a large number of multiclass classification tasks and\ngeneralizing the model to a new task. Yet, even with such meta-learning, the\nlow-data problem in the novel classification task still remains. In this paper,\nwe propose Transductive Propagation Network (TPN), a novel meta-learning\nframework for transductive inference that classifies the entire test set at\nonce to alleviate the low-data problem. Specifically, we propose to learn to\npropagate labels from labeled instances to unlabeled test instances, by\nlearning a graph construction module that exploits the manifold structure in\nthe data. TPN jointly learns both the parameters of feature embedding and the\ngraph construction in an end-to-end manner. We validate TPN on multiple\nbenchmark datasets, on which it largely outperforms existing few-shot learning\napproaches and achieves the state-of-the-art results. \n\n"}
{"id": "1805.10338", "contents": "Title: Zero-Shot Dual Machine Translation Abstract: Neural Machine Translation (NMT) systems rely on large amounts of parallel\ndata. This is a major challenge for low-resource languages. Building on recent\nwork on unsupervised and semi-supervised methods, we present an approach that\ncombines zero-shot and dual learning. The latter relies on reinforcement\nlearning, to exploit the duality of the machine translation task, and requires\nonly monolingual data for the target language pair. Experiments show that a\nzero-shot dual system, trained on English-French and English-Spanish,\noutperforms by large margins a standard NMT system in zero-shot translation\nperformance on Spanish-French (both directions). The zero-shot dual method\napproaches the performance, within 2.2 BLEU points, of a comparable supervised\nsetting. Our method can obtain improvements also on the setting where a small\namount of parallel data for the zero-shot language pair is available. Adding\nRussian, to extend our experiments to jointly modeling 6 zero-shot translation\ndirections, all directions improve between 4 and 15 BLEU points, again,\nreaching performance near that of the supervised setting. \n\n"}
{"id": "1805.11454", "contents": "Title: Distributed Stochastic Gradient Tracking Methods Abstract: In this paper, we study the problem of distributed multi-agent optimization\nover a network, where each agent possesses a local cost function that is smooth\nand strongly convex. The global objective is to find a common solution that\nminimizes the average of all cost functions. Assuming agents only have access\nto unbiased estimates of the gradients of their local cost functions, we\nconsider a distributed stochastic gradient tracking method (DSGT) and a\ngossip-like stochastic gradient tracking method (GSGT). We show that, in\nexpectation, the iterates generated by each agent are attracted to a\nneighborhood of the optimal solution, where they accumulate exponentially fast\n(under a constant stepsize choice). Under DSGT, the limiting (expected) error\nbounds on the distance of the iterates from the optimal solution decrease with\nthe network size $n$, which is a comparable performance to a centralized\nstochastic gradient algorithm. Moreover, we show that when the network is\nwell-connected, GSGT incurs lower communication cost than DSGT while\nmaintaining a similar computational cost. Numerical example further\ndemonstrates the effectiveness of the proposed methods. \n\n"}
{"id": "1805.11797", "contents": "Title: Grow and Prune Compact, Fast, and Accurate LSTMs Abstract: Long short-term memory (LSTM) has been widely used for sequential data\nmodeling. Researchers have increased LSTM depth by stacking LSTM cells to\nimprove performance. This incurs model redundancy, increases run-time delay,\nand makes the LSTMs more prone to overfitting. To address these problems, we\npropose a hidden-layer LSTM (H-LSTM) that adds hidden layers to LSTM's original\none level non-linear control gates. H-LSTM increases accuracy while employing\nfewer external stacked layers, thus reducing the number of parameters and\nrun-time latency significantly. We employ grow-and-prune (GP) training to\niteratively adjust the hidden layers through gradient-based growth and\nmagnitude-based pruning of connections. This learns both the weights and the\ncompact architecture of H-LSTM control gates. We have GP-trained H-LSTMs for\nimage captioning and speech recognition applications. For the NeuralTalk\narchitecture on the MSCOCO dataset, our three models reduce the number of\nparameters by 38.7x [floating-point operations (FLOPs) by 45.5x], run-time\nlatency by 4.5x, and improve the CIDEr score by 2.6. For the DeepSpeech2\narchitecture on the AN4 dataset, our two models reduce the number of parameters\nby 19.4x (FLOPs by 23.5x), run-time latency by 15.7%, and the word error rate\nfrom 12.9% to 8.7%. Thus, GP-trained H-LSTMs can be seen to be compact, fast,\nand accurate. \n\n"}
{"id": "1805.12024", "contents": "Title: Privacy Aware Offloading of Deep Neural Networks Abstract: Deep neural networks require large amounts of resources which makes them hard\nto use on resource constrained devices such as Internet-of-things devices.\nOffloading the computations to the cloud can circumvent these constraints but\nintroduces a privacy risk since the operator of the cloud is not necessarily\ntrustworthy. We propose a technique that obfuscates the data before sending it\nto the remote computation node. The obfuscated data is unintelligible for a\nhuman eavesdropper but can still be classified with a high accuracy by a neural\nnetwork trained on unobfuscated images. \n\n"}
{"id": "1805.12388", "contents": "Title: Sample Reuse via Importance Sampling in Information Geometric\n  Optimization Abstract: In this paper we propose a technique to reduce the number of function\nevaluations, which is often the bottleneck of the black-box optimization, in\nthe information geometric optimization (IGO) that is a generic framework of the\nprobability model-based black-box optimization algorithms and generalizes\nseveral well-known evolutionary algorithms, such as the population-based\nincremental learning (PBIL) and the pure rank-$\\mu$ update covariance matrix\nadaptation evolution strategy (CMA-ES). In each iteration, the IGO algorithms\nupdate the parameters of the probability distribution to the natural gradient\ndirection estimated by Monte-Carlo with the samples drawn from the current\ndistribution. Our strategy is to reuse previously generated and evaluated\nsamples based on the importance sampling. It is a technique to reduce the\nestimation variance without introducing a bias in Monte-Carlo estimation. We\napply the sample reuse technique to the PBIL and the pure rank-$\\mu$ update\nCMA-ES and empirically investigate its effect. The experimental results show\nthat the sample reuse helps to reduce the number of function evaluations on\nmany benchmark functions for both the PBIL and the pure rank-$\\mu$ update\nCMA-ES. Moreover, we demonstrate how to combine the importance sampling\ntechnique with a variant of the CMA-ES involving an algorithmic component that\nis not derived in the IGO framework. \n\n"}
{"id": "1806.00039", "contents": "Title: Blip: JIT and Footloose On The Edge Abstract: Edge environments offer a number of advantages for software developers\nincluding the ability to create services which can offer lower latency, better\nprivacy, and reduced operational costs than traditional cloud hosted services.\nHowever large technical challenges exist, which prevent developers from\nutilising the Edge; complexities related to the heterogeneous nature of the\nEdge environment, issues with orchestration and application management and\nlastly, the inherent issues in creating decentralised distributed applications\nwhich operate at a large geographic scale. In this conceptual and architectural\npaper we envision a solution, Blip, which offers an easy to use programming and\noperational environment which addresses the these issues. It aims to remove the\ntechnical barriers which will inhibit the wider adoption Edge application\ndevelopment. This paper validates the Blip concept by demonstrating how it will\ndeliver on the advantages of the Edge for a familiar scenario. \n\n"}
{"id": "1806.01105", "contents": "Title: Performance tuning for deep learning on a many-core processor (master\n  thesis) Abstract: Convolutional neural networks (CNNs) are becoming very successful and popular\nfor a variety of applications. The Loki many-core processor architecture is\nvery promising for achieving specialised hardware performance and efficiency\nwhile being a general purpose solution. Loki combines many simple cores with\nincreased control for the programmer. This freedom can be exploited to produce\nmuch more efficient code than in conventional multiprocessors but it also\ncreates a very big design space for possible optimisations. In this project, I\nexplore possible optimisations for a CNN application, their portability on\ndifferent Loki-specific configurations, convolution parameters and inputs.\nFinally, I investigate the potential for adaptive algorithms for further\nperformance increase. \n\n"}
{"id": "1806.01108", "contents": "Title: Hardware Transactional Persistent Memory Abstract: Emerging Persistent Memory technologies (also PM, Non-Volatile DIMMs, Storage\nClass Memory or SCM) hold tremendous promise for accelerating popular\ndata-management applications like in-memory databases. However, programmers now\nneed to deal with ensuring the atomicity of transactions on Persistent Memory\nresident data and maintaining consistency between the order in which processors\nperform stores and that in which the updated values become durable.\n  The problem is specially challenging when high-performance isolation\nmechanisms like Hardware Transactional Memory (HTM) are used for concurrency\ncontrol. This work shows how HTM transactions can be ordered correctly and\natomically into PM by the use of a novel software protocol combined with a\nPersistent Memory Controller, without requiring changes to processor cache\nhardware or HTM protocols. In contrast, previous approaches require significant\nchanges to existing processor microarchitectures. Our approach, evaluated using\nboth micro-benchmarks and the STAMP suite compares well with standard\n(volatile) HTM transactions. It also yields significant gains in throughput and\nlatency in comparison with persistent transactional locking. \n\n"}
{"id": "1806.01110", "contents": "Title: Spark-MPI: Approaching the Fifth Paradigm of Cognitive Applications Abstract: Over the past decade, the fourth paradigm of data-intensive science rapidly\nbecame a major driving concept of multiple application domains encompassing and\ngenerating large-scale devices such as light sources and cutting edge\ntelescopes. The success of data-intensive projects subsequently triggered the\nnext generation of machine learning approaches. These new artificial\nintelligent systems clearly represent a paradigm shift from data processing\npipelines towards the fifth paradigm of composite cognitive applications\nrequiring the integration of Big Data processing platforms and HPC\ntechnologies. The paper addresses the existing impedance mismatch between\ndata-intensive and compute-intensive ecosystems by presenting the Spark-MPI\napproach based on the MPI Exascale Process Management Interface (PMIx). The\napproach is demonstrated within the context of hybrid MPI/GPU ptychographic\nimage reconstruction pipelines and distributed deep learning applications. \n\n"}
{"id": "1806.01331", "contents": "Title: Precise Runtime Analysis for Plateau Functions Abstract: To gain a better theoretical understanding of how evolutionary algorithms\n(EAs) cope with plateaus of constant fitness, we propose the $n$-dimensional\nPlateau$_k$ function as natural benchmark and analyze how different variants of\nthe $(1 + 1)$ EA optimize it. The Plateau$_k$ function has a plateau of\nsecond-best fitness in a ball of radius $k$ around the optimum. As evolutionary\nalgorithm, we regard the $(1 + 1)$ EA using an arbitrary unbiased mutation\noperator. Denoting by $\\alpha$ the random number of bits flipped in an\napplication of this operator and assuming that $\\Pr[\\alpha = 1]$ has at least\nsome small sub-constant value, we show the surprising result that for all\nconstant $k \\ge 2$, the runtime $T$ follows a distribution close to the\ngeometric one with success probability equal to the probability to flip between\n$1$ and $k$ bits divided by the size of the plateau. Consequently, the expected\nruntime is the inverse of this number, and thus only depends on the probability\nto flip between $1$ and $k$ bits, but not on other characteristics of the\nmutation operator. Our result also implies that the optimal mutation rate for\nstandard bit mutation here is approximately $k/(en)$. Our main analysis tool is\na combined analysis of the Markov chains on the search point space and on the\nHamming level space, an approach that promises to be useful also for other\nplateau problems. \n\n"}
{"id": "1806.01423", "contents": "Title: BindsNET: A machine learning-oriented spiking neural networks library in\n  Python Abstract: The development of spiking neural network simulation software is a critical\ncomponent enabling the modeling of neural systems and the development of\nbiologically inspired algorithms. Existing software frameworks support a wide\nrange of neural functionality, software abstraction levels, and hardware\ndevices, yet are typically not suitable for rapid prototyping or application to\nproblems in the domain of machine learning. In this paper, we describe a new\nPython package for the simulation of spiking neural networks, specifically\ngeared towards machine learning and reinforcement learning. Our software,\ncalled BindsNET, enables rapid building and simulation of spiking networks and\nfeatures user-friendly, concise syntax. BindsNET is built on top of the PyTorch\ndeep neural networks library, enabling fast CPU and GPU computation for large\nspiking networks. The BindsNET framework can be adjusted to meet the needs of\nother existing computing and hardware environments, e.g., TensorFlow. We also\nprovide an interface into the OpenAI gym library, allowing for training and\nevaluation of spiking networks on reinforcement learning problems. We argue\nthat this package facilitates the use of spiking networks for large-scale\nmachine learning experimentation, and show some simple examples of how we\nenvision BindsNET can be used in practice. BindsNET code is available at\nhttps://github.com/Hananel-Hazan/bindsnet \n\n"}
{"id": "1806.02003", "contents": "Title: Deep Algorithms: designs for networks Abstract: A new design methodology for neural networks that is guided by traditional\nalgorithm design is presented. To prove our point, we present two heuristics\nand demonstrate an algorithmic technique for incorporating additional weights\nin their signal-flow graphs. We show that with training the performance of\nthese networks can not only exceed the performance of the initial network, but\ncan match the performance of more-traditional neural network architectures. A\nkey feature of our approach is that these networks are initialized with\nparameters that provide a known performance threshold for the architecture on a\ngiven task. \n\n"}
{"id": "1806.02284", "contents": "Title: Corpus Conversion Service: A Machine Learning Platform to Ingest\n  Documents at Scale Abstract: Over the past few decades, the amount of scientific articles and technical\nliterature has increased exponentially in size. Consequently, there is a great\nneed for systems that can ingest these documents at scale and make the\ncontained knowledge discoverable. Unfortunately, both the format of these\ndocuments (e.g. the PDF format or bitmap images) as well as the presentation of\nthe data (e.g. complex tables) make the extraction of qualitative and\nquantitive data extremely challenging. In this paper, we present a modular,\ncloud-based platform to ingest documents at scale. This platform, called the\nCorpus Conversion Service (CCS), implements a pipeline which allows users to\nparse and annotate documents (i.e. collect ground-truth), train\nmachine-learning classification algorithms and ultimately convert any type of\nPDF or bitmap-documents to a structured content representation format. We will\nshow that each of the modules is scalable due to an asynchronous microservice\narchitecture and can therefore handle massive amounts of documents.\nFurthermore, we will show that our capability to gather ground-truth is\naccelerated by machine-learning algorithms by at least one order of magnitude.\nThis allows us to both gather large amounts of ground-truth in very little time\nand obtain very good precision/recall metrics in the range of 99\\% with regard\nto content conversion to structured output. The CCS platform is currently\ndeployed on IBM internal infrastructure and serving more than 250 active users\nfor knowledge-engineering project engagements. \n\n"}
{"id": "1806.02508", "contents": "Title: Semi-Dynamic Load Balancing: Efficient Distributed Learning in\n  Non-Dedicated Environments Abstract: Machine learning (ML) models are increasingly trained in clusters with\nnon-dedicated workers possessing heterogeneous resources. In such scenarios,\nmodel training efficiency can be negatively affected by stragglers -- workers\nthat run much slower than others. Efficient model training requires eliminating\nsuch stragglers, yet for modern ML workloads, existing load balancing\nstrategies are inefficient and even infeasible. In this paper, we propose a\nnovel strategy called semi-dynamic load balancing to eliminate stragglers of\ndistributed ML workloads. The key insight is that ML workers shall be\nload-balanced at iteration boundaries, being non-intrusive to intra-iteration\nexecution. We develop LB-BSP based on such an insight, which is an integrated\nworker coordination mechanism that adapts workers' load to their instantaneous\nprocessing capabilities by right-sizing the sample batches at the\nsynchronization barriers. We have custom-designed the batch sizing algorithm\nrespectively for CPU and GPU clusters based on their own characteristics.\nLB-BSP has been implemented as a Python module for ML frameworks like\nTensorFlow and PyTorch. Our EC2 deployment confirms that LB-BSP is practical,\neffective and light-weight, and is able to accelerating distributed training by\nup to $54\\%$. \n\n"}
{"id": "1806.02679", "contents": "Title: Semi-Supervised Learning via Compact Latent Space Clustering Abstract: We present a novel cost function for semi-supervised learning of neural\nnetworks that encourages compact clustering of the latent space to facilitate\nseparation. The key idea is to dynamically create a graph over embeddings of\nlabeled and unlabeled samples of a training batch to capture underlying\nstructure in feature space, and use label propagation to estimate its high and\nlow density regions. We then devise a cost function based on Markov chains on\nthe graph that regularizes the latent space to form a single compact cluster\nper class, while avoiding to disturb existing clusters during optimization. We\nevaluate our approach on three benchmarks and compare to state-of-the art with\npromising results. Our approach combines the benefits of graph-based\nregularization with efficient, inductive inference, does not require\nmodifications to a network architecture, and can thus be easily applied to\nexisting networks to enable an effective use of unlabeled data. \n\n"}
{"id": "1806.03103", "contents": "Title: An Explicit Construction of Systematic MDS Codes with Small\n  Sub-packetization for All-Node Repair Abstract: An explicit construction of systematic MDS codes, called HashTag+ codes, with\narbitrary sub-packetization level for all-node repair is proposed. It is shown\nthat even for small sub-packetization levels, HashTag+ codes achieve the\noptimal MSR point for repair of any parity node, while the repair bandwidth for\na single systematic node depends on the sub-packetization level. Compared to\nother codes in the literature, HashTag+ codes provide from 20% to 40% savings\nin the average amount of data accessed and transferred during repair. \n\n"}
{"id": "1806.03377", "contents": "Title: PipeDream: Fast and Efficient Pipeline Parallel DNN Training Abstract: PipeDream is a Deep Neural Network(DNN) training system for GPUs that\nparallelizes computation by pipelining execution across multiple machines. Its\npipeline parallel computing model avoids the slowdowns faced by data-parallel\ntraining when large models and/or limited network bandwidth induce high\ncommunication-to-computation ratios. PipeDream reduces communication by up to\n95% for large DNNs relative to data-parallel training, and allows perfect\noverlap of communication and computation. PipeDream keeps all available GPUs\nproductive by systematically partitioning DNN layers among them to balance work\nand minimize communication, versions model parameters for backward pass\ncorrectness, and schedules the forward and backward passes of different inputs\nin round-robin fashion to optimize \"time to target accuracy\". Experiments with\nfive different DNNs on two different clusters show that PipeDream is up to 5x\nfaster in time-to-accuracy compared to data-parallel training. \n\n"}
{"id": "1806.04328", "contents": "Title: Broadcast and minimum spanning tree with $o(m)$ messages in the\n  asynchronous CONGEST model Abstract: We provide the first asynchronous distributed algorithms to compute broadcast\nand minimum spanning tree with $o(m)$ bits of communication, in a graph with\n$n$ nodes and $m$ edges. For decades, it was believed that $\\Omega(m)$ bits of\ncommunication are required for any algorithm that constructs a broadcast tree.\nIn 2015, King, Kutten and Thorup showed that in the KT1 model where nodes have\ninitial knowledge of their neighbors' identities it is possible to construct\nMST in $\\tilde{O}(n)$ messages in the synchronous CONGEST model. In the CONGEST\nmodel messages are of size $O(\\log n)$. However, no algorithm with $o(m)$\nmessages were known for the asynchronous case. Here, we provide an algorithm\nthat uses $O(n^{3/2} \\log^{3/2} n)$ messages to find MST in the asynchronous\nCONGEST model. Our algorithm is randomized Monte Carlo and outputs MST with\nhigh probability. We will provide an algorithm for computing a spanning tree\nwith $O(n^{3/2} \\log^{3/2} n)$ messages. Given a spanning tree, we can compute\nMST with $\\tilde{O}(n)$ messages. \n\n"}
{"id": "1806.04646", "contents": "Title: Adversarial Attacks on Variational Autoencoders Abstract: Adversarial attacks are malicious inputs that derail machine-learning models.\nWe propose a scheme to attack autoencoders, as well as a quantitative\nevaluation framework that correlates well with the qualitative assessment of\nthe attacks. We assess --- with statistically validated experiments --- the\nresistance to attacks of three variational autoencoders (simple, convolutional,\nand DRAW) in three datasets (MNIST, SVHN, CelebA), showing that both DRAW's\nrecurrence and attention mechanism lead to better resistance. As autoencoders\nare proposed for compressing data --- a scenario in which their safety is\nparamount --- we expect more attention will be given to adversarial attacks on\nthem. \n\n"}
{"id": "1806.05358", "contents": "Title: Defending Against Saddle Point Attack in Byzantine-Robust Distributed\n  Learning Abstract: We study robust distributed learning that involves minimizing a non-convex\nloss function with saddle points. We consider the Byzantine setting where some\nworker machines have abnormal or even arbitrary and adversarial behavior. In\nthis setting, the Byzantine machines may create fake local minima near a saddle\npoint that is far away from any true local minimum, even when robust gradient\nestimators are used. We develop ByzantinePGD, a robust first-order algorithm\nthat can provably escape saddle points and fake local minima, and converge to\nan approximate true local minimizer with low iteration complexity. As a\nby-product, we give a simpler algorithm and analysis for escaping saddle points\nin the usual non-Byzantine setting. We further discuss three robust gradient\nestimators that can be used in ByzantinePGD, including median, trimmed mean,\nand iterative filtering. We characterize their performance in concrete\nstatistical settings, and argue for their near-optimality in low and high\ndimensional regimes. \n\n"}
{"id": "1806.05865", "contents": "Title: Data-Efficient Design Exploration through Surrogate-Assisted\n  Illumination Abstract: Design optimization techniques are often used at the beginning of the design\nprocess to explore the space of possible designs. In these domains illumination\nalgorithms, such as MAP-Elites, are promising alternatives to classic\noptimization algorithms because they produce diverse, high-quality solutions in\na single run, instead of only a single near-optimal solution. Unfortunately,\nthese algorithms currently require a large number of function evaluations,\nlimiting their applicability. In this article we introduce a new illumination\nalgorithm, Surrogate-Assisted Illumination (SAIL), that leverages surrogate\nmodeling techniques to create a map of the design space according to\nuser-defined features while minimizing the number of fitness evaluations. On a\n2-dimensional airfoil optimization problem SAIL produces hundreds of diverse\nbut high-performing designs with several orders of magnitude fewer evaluations\nthan MAP-Elites or CMA-ES. We demonstrate that SAIL is also capable of\nproducing maps of high-performing designs in realistic 3-dimensional\naerodynamic tasks with an accurate flow simulation. Data-efficient design\nexploration with SAIL can help designers understand what is possible, beyond\nwhat is optimal, by considering more than pure objective-based optimization. \n\n"}
{"id": "1806.05978", "contents": "Title: Uncertainty Estimations by Softplus normalization in Bayesian\n  Convolutional Neural Networks with Variational Inference Abstract: We introduce a novel uncertainty estimation for classification tasks for\nBayesian convolutional neural networks with variational inference. By\nnormalizing the output of a Softplus function in the final layer, we estimate\naleatoric and epistemic uncertainty in a coherent manner. The intractable\nposterior probability distributions over weights are inferred by Bayes by\nBackprop. Firstly, we demonstrate how this reliable variational inference\nmethod can serve as a fundamental construct for various network architectures.\nOn multiple datasets in supervised learning settings (MNIST, CIFAR-10,\nCIFAR-100), this variational inference method achieves performances equivalent\nto frequentist inference in identical architectures, while the two desiderata,\na measure for uncertainty and regularization are incorporated naturally.\nSecondly, we examine how our proposed measure for aleatoric and epistemic\nuncertainties is derived and validate it on the aforementioned datasets. \n\n"}
{"id": "1806.06296", "contents": "Title: Right for the Right Reason: Training Agnostic Networks Abstract: We consider the problem of a neural network being requested to classify\nimages (or other inputs) without making implicit use of a \"protected concept\",\nthat is a concept that should not play any role in the decision of the network.\nTypically these concepts include information such as gender or race, or other\ncontextual information such as image backgrounds that might be implicitly\nreflected in unknown correlations with other variables, making it insufficient\nto simply remove them from the input features. In other words, making accurate\npredictions is not good enough if those predictions rely on information that\nshould not be used: predictive performance is not the only important metric for\nlearning systems. We apply a method developed in the context of domain\nadaptation to address this problem of \"being right for the right reason\", where\nwe request a classifier to make a decision in a way that is entirely 'agnostic'\nto a given protected concept (e.g. gender, race, background etc.), even if this\ncould be implicitly reflected in other attributes via unknown correlations.\nAfter defining the concept of an 'agnostic model', we demonstrate how the\nDomain-Adversarial Neural Network can remove unwanted information from a model\nusing a gradient reversal layer. \n\n"}
{"id": "1806.06576", "contents": "Title: VEBO: A Vertex- and Edge-Balanced Ordering Heuristic to Load Balance\n  Parallel Graph Processing Abstract: Graph partitioning drives graph processing in distributed, disk-based and\nNUMA-aware systems. A commonly used partitioning goal is to balance the number\nof edges per partition in conjunction with minimizing the edge or vertex cut.\nWhile this type of partitioning is computationally expensive, we observe that\nsuch topology-driven partitioning nonetheless results in computational load\nimbalance. We propose Vertex- and Edge-Balanced Ordering (VEBO): balance the\nnumber of edges and the number of unique destinations of those edges. VEBO\noptimally balances edges and vertices for graphs with a power-law degree\ndistribution. Experimental evaluation on three shared-memory graph processing\nsystems (Ligra, Polymer and GraphGrind) shows that VEBO achieves excellent load\nbalance and improves performance by 1.09x over Ligra, 1.41x over Polymer and\n1.65x over GraphGrind, compared to their respective partitioning algorithms,\naveraged across 8 algorithms and 7 graphs. \n\n"}
{"id": "1806.06926", "contents": "Title: Understanding Patch-Based Learning by Explaining Predictions Abstract: Deep networks are able to learn highly predictive models of video data. Due\nto video length, a common strategy is to train them on small video snippets. We\napply the deep Taylor / LRP technique to understand the deep network's\nclassification decisions, and identify a \"border effect\": a tendency of the\nclassifier to look mainly at the bordering frames of the input. This effect\nrelates to the step size used to build the video snippet, which we can then\ntune in order to improve the classifier's accuracy without retraining the\nmodel. To our knowledge, this is the the first work to apply the deep Taylor /\nLRP technique on any video analyzing neural network. \n\n"}
{"id": "1806.07081", "contents": "Title: Distributed Optimization over Directed Graphs with Row Stochasticity and\n  Constraint Regularity Abstract: This paper deals with an optimization problem over a network of agents, where\nthe cost function is the sum of the individual objectives of the agents and the\nconstraint set is the intersection of local constraints. Most existing methods\nemploying subgradient and consensus steps for solving this problem require the\nweight matrix associated with the network to be column stochastic or even\ndoubly stochastic, conditions that can be hard to arrange in directed networks.\nMoreover, known convergence analyses for distributed subgradient methods vary\ndepending on whether the problem is unconstrained or constrained, and whether\nthe local constraint sets are identical or nonidentical and compact. The main\ngoals of this paper are: (i) removing the common column stochasticity\nrequirement; (ii) relaxing the compactness assumption, and (iii) providing a\nunified convergence analysis. Specifically, assuming the communication graph to\nbe fixed and strongly connected and the weight matrix to (only) be row\nstochastic, a distributed projected subgradient algorithm and its variation are\npresented to solve the problem for cost functions that are convex and Lipschitz\ncontinuous. Based on a regularity assumption on the local constraint sets, a\nunified convergence analysis is given that can be applied to both unconstrained\nand constrained problems and without assuming compactness of the constraint\nsets or an interior point in their intersection. Further, we also establish an\nupper bound on the absolute objective error evaluated at each agent's available\nlocal estimate under a nonincreasing step size sequence. This bound allows us\nto analyze the convergence rate of both algorithms. \n\n"}
{"id": "1806.08085", "contents": "Title: Inference of Quantized Neural Networks on Heterogeneous All-Programmable\n  Devices Abstract: Neural networks have established as a generic and powerful means to approach\nchallenging problems such as image classification, object detection or decision\nmaking. Their successful employment foots on an enormous demand of compute. The\nquantization of network parameters and the processed data has proven a valuable\nmeasure to reduce the challenges of network inference so effectively that the\nfeasible scope of applications is expanded even into the embedded domain. This\npaper describes the making of a real-time object detection in a live video\nstream processed on an embedded all-programmable device. The presented case\nillustrates how the required processing is tamed and parallelized across both\nthe CPU cores and the programmable logic and how the most suitable resources\nand powerful extensions, such as NEON vectorization, are leveraged for the\nindividual processing steps. The crafted result is an extended Darknet\nframework implementing a fully integrated, end-to-end solution from video\ncapture over object annotation to video output applying neural network\ninference at different quantization levels running at 16~frames per second on\nan embedded Zynq UltraScale+ (XCZU3EG) platform. \n\n"}
{"id": "1806.10113", "contents": "Title: Improving tasks throughput on accelerators using OpenCL command\n  concurrency Abstract: A heterogeneous architecture composed by a host and an accelerator must\nfrequently deal with situations where several independent tasks are available\nto be offloaded onto the accelerator. These tasks can be generated by\nconcurrent applications executing in the host or, in case the host is a node of\na computer cluster, by applications running on other cluster nodes that are\nwilling to offload tasks in the accelerator connected to the host. In this work\nwe show that a runtime scheduler that selects the best execution order of a\ngroup of tasks on the accelerator can significantly reduce the total execution\ntime of the tasks and, consequently, increase the accelerator use. Our solution\nis based on a temporal execution model that is able to predict with high\naccuracy the execution time of a set of concurrent tasks launched on the\naccelerator. The execution model has been validated in AMD, NVIDIA, and Xeon\nPhi devices using synthetic benchmarks. Moreover, employing the temporal\nexecution model, a heuristic is proposed which is able to establish a\nnear-optimal tasks execution ordering that significantly reduces the total\nexecution time, including data transfers.The heuristic has been evaluated with\nfive different benchmarks composed of dominant kernel and dominant transfer\nreal tasks. Experiments indicate the heuristic is able to find always an\nordering with a better execution time than the average of every possible\nexecution order and, most times, it achieves a near-optimal ordering (very\nclose to the execution time of the best execution order) with a negligible\noverhead. Concretely, our heuristic obtains, on average for all the devices,\nbetween 84\\% and 96\\% of the improvement achieved by the best execution order. \n\n"}
{"id": "1806.10181", "contents": "Title: Unsupervised Learning by Competing Hidden Units Abstract: It is widely believed that the backpropagation algorithm is essential for\nlearning good feature detectors in early layers of artificial neural networks,\nso that these detectors are useful for the task performed by the higher layers\nof that neural network. At the same time, the traditional form of\nbackpropagation is biologically implausible. In the present paper we propose an\nunusual learning rule, which has a degree of biological plausibility, and which\nis motivated by Hebb's idea that change of the synapse strength should be local\n- i.e. should depend only on the activities of the pre and post synaptic\nneurons. We design a learning algorithm that utilizes global inhibition in the\nhidden layer, and is capable of learning early feature detectors in a\ncompletely unsupervised way. These learned lower layer feature detectors can be\nused to train higher layer weights in a usual supervised way so that the\nperformance of the full network is comparable to the performance of standard\nfeedforward networks trained end-to-end with a backpropagation algorithm. \n\n"}
{"id": "1806.10929", "contents": "Title: When Can a Distributed Ledger Replace a Trusted Third Party? Abstract: The functionality that distributed ledger technology provides, i.e., an\nimmutable and fraud-resistant registry with validation and verification\nmechanisms, has traditionally been implemented with a trusted third party. Due\nto the distributed nature of ledger technology, there is a strong recent trend\ntowards using ledgers to implement novel decentralized applications for a wide\nrange of use cases, e.g., in the financial sector and sharing economy. While\nthere can be several arguments for the use of a ledger, the key question is\nwhether it can fully replace any single trusted party in the system as\notherwise a (potentially simpler) solution can be built around the trusted\nparty. In this paper, we introduce an abstract view on ledger use cases and\npresent two fundamental criteria that must be met for any use case to be\nimplemented using a ledger-based approach without having to rely on any\nparticular party in the system. Moreover, we evaluate several ledger use cases\nthat have recently received considerable attention according to these criteria,\nrevealing that often participants need to trust each other despite using a\ndistributed ledger. Consequently, the potential of using a ledger as a\nreplacement for a trusted party is limited for these use cases. \n\n"}
{"id": "1806.11420", "contents": "Title: Discourse-Wizard: Discovering Deep Discourse Structure in your\n  Conversation with RNNs Abstract: Spoken language understanding is one of the key factors in a dialogue system,\nand a context in a conversation plays an important role to understand the\ncurrent utterance. In this work, we demonstrate the importance of context\nwithin the dialogue for neural network models through an online web interface\nlive demo. We developed two different neural models: a model that does not use\ncontext and a context-based model. The no-context model classifies dialogue\nacts at an utterance-level whereas the context-based model takes some preceding\nutterances into account. We make these trained neural models available as a\nlive demo called Discourse-Wizard using a modular server architecture. The live\ndemo provides an easy to use interface for conversational analysis and for\ndiscovering deep discourse structures in a conversation. \n\n"}
{"id": "1807.00220", "contents": "Title: Storage-Repair Bandwidth Trade-off for Wireless Caching with Partial\n  Failure and Broadcast Repair Abstract: Repair of multiple partially failed cache nodes is studied in a distributed\nwireless content caching system, where $r$ out of a total of $n$ cache nodes\nlose part of their cached data. Broadcast repair of failed cache contents at\nthe network edge is studied; that is, the surviving cache nodes transmit\nbroadcast messages to the failed ones, which are then used, together with the\nsurviving data in their local cache memories, to recover the lost content. The\ntrade-off between the storage capacity and the repair bandwidth is derived. It\nis shown that utilizing the broadcast nature of the wireless medium and the\nsurviving cache contents at partially failed nodes significantly reduces the\nrequired repair bandwidth per node. \n\n"}
{"id": "1807.00324", "contents": "Title: Joint Failure Recovery, Fault Prevention, and Energy-efficient Resource\n  Management for Real-time SFC in Fog-supported SDN Abstract: In this paper, we focus on the problems of traffic engineering, failure\nrecovery, fault prevention, and Service Function Chain (SFC) with reliability\nand energy consumption constraints in Software Defined Networks (SDN). These\ntypes of deployments use Fog computing as an emerging paradigm to manage the\ndistributed small-size traffic flows passing through the SDN-enabled switches\n(possibly Fog Nodes). The main aim of this integration is to support service\ndelivery in real-time, failure recovery, and fault-awareness in an SFC context.\nFirstly, we present an architecture for Failure Recovery and Fault Prevention\ncalled FRFP; this is a multi-tier structure in which the real-time traffic\nflows pass through SDN-enabled switches to jointly decrease the network\nside-effects of flow rerouting and energy consumption of the Fog Nodes. We then\nmathematically formulate an optimization problem called the Optimal\nFog-Supported Energy-Aware SFC rerouting algorithm (OFES) and propose a\nnear-optimal heuristic called Heuristic OFES (HFES) to solve the corresponding\nproblem in polynomial time. In this way, the energy consumption and the\nreliability of the selected paths are optimized, while the Quality of Service\nconstraints are met and the network congestion is minimized. In a reliability\ncontext, the focus of this work is on fault prevention; however, since we use a\nreallocation technique, the proposed scheme can be used as a failure recovery\nscheme. We compare the performance of HFES and OFES in terms of power\nconsumption, average path length, fault probability, network side-effects, link\nutilization, and Fog Node utilization. Additionally, we analyze the\ncomputational complexity of HFES. We use a real-world network topology to\nevaluate our algorithm. The simulation results show that the heuristic\nalgorithm is applicable to large-scale networks. \n\n"}
{"id": "1807.00930", "contents": "Title: Neural Random Projections for Language Modelling Abstract: Neural network-based language models deal with data sparsity problems by\nmapping the large discrete space of words into a smaller continuous space of\nreal-valued vectors. By learning distributed vector representations for words,\neach training sample informs the neural network model about a combinatorial\nnumber of other patterns. In this paper, we exploit the sparsity in natural\nlanguage even further by encoding each unique input word using a fixed sparse\nrandom representation. These sparse codes are then projected onto a smaller\nembedding space which allows for the encoding of word occurrences from a\npossibly unknown vocabulary, along with the creation of more compact language\nmodels using a reduced number of parameters. We investigate the properties of\nour encoding mechanism empirically, by evaluating its performance on the widely\nused Penn Treebank corpus. We show that guaranteeing approximately equidistant\n(nearly orthogonal) vector representations for unique discrete inputs is enough\nto provide the neural network model with enough information to learn --and make\nuse-- of distributed representations for these inputs. \n\n"}
{"id": "1807.01341", "contents": "Title: SWIFT: Maintaining weak-scalability with a dynamic range of $10^4$ in\n  time-step size to harness extreme adaptivity Abstract: Cosmological simulations require the use of a multiple time-stepping scheme.\nWithout such a scheme, cosmological simulations would be impossible due to\ntheir high level of dynamic range; over eleven orders of magnitude in density.\nSuch a large dynamic range leads to a range of over four orders of magnitude in\ntime-step, which presents a significant load-balancing challenge. In this work,\nthe extreme adaptivity that cosmological simulations present is tackled in\nthree main ways through the use of the code SWIFT. First, an adaptive mesh is\nused to ensure that only the relevant particles are interacted in a given\ntime-step. Second, task-based parallelism is used to ensure efficient\nload-balancing within a single node, using pthreads and SIMD vectorisation.\nFinally, a domain decomposition strategy is presented, using the graph domain\ndecomposition library METIS, that bisects the work that must be performed by\nthe simulation between nodes using MPI. These three strategies are shown to\ngive SWIFT near-perfect weak-scaling characteristics, only losing 25%\nperformance when scaling from 1 to 4096 cores on a representative problem,\nwhilst being more than 30x faster than the de-facto standard Gadget-2 code. \n\n"}
{"id": "1807.01430", "contents": "Title: SGAD: Soft-Guided Adaptively-Dropped Neural Network Abstract: Deep neural networks (DNNs) have been proven to have many redundancies.\nHence, many efforts have been made to compress DNNs. However, the existing\nmodel compression methods treat all the input samples equally while ignoring\nthe fact that the difficulties of various input samples being correctly\nclassified are different. To address this problem, DNNs with adaptive dropping\nmechanism are well explored in this work. To inform the DNNs how difficult the\ninput samples can be classified, a guideline that contains the information of\ninput samples is introduced to improve the performance. Based on the developed\nguideline and adaptive dropping mechanism, an innovative soft-guided\nadaptively-dropped (SGAD) neural network is proposed in this paper. Compared\nwith the 32 layers residual neural networks, the presented SGAD can reduce the\nFLOPs by 77% with less than 1% drop in accuracy on CIFAR-10. \n\n"}
{"id": "1807.02562", "contents": "Title: Exploring Scientific Application Performance Using Large Scale Object\n  Storage Abstract: One of the major performance and scalability bottlenecks in large scientific\napplications is parallel reading and writing to supercomputer I/O systems. The\nusage of parallel file systems and consistency requirements of POSIX, that all\nthe traditional HPC parallel I/O interfaces adhere to, pose limitations to the\nscalability of scientific applications. Object storage is a widely used storage\ntechnology in cloud computing and is more frequently proposed for HPC workload\nto address and improve the current scalability and performance of I/O in\nscientific applications. While object storage is a promising technology, it is\nstill unclear how scientific applications will use object storage and what the\nmain performance benefits will be. This work addresses these questions, by\nemulating an object storage used by a traditional scientific application and\nevaluating potential performance benefits. We show that scientific applications\ncan benefit from the usage of object storage on large scales. \n\n"}
{"id": "1807.03010", "contents": "Title: XNOR Neural Engine: a Hardware Accelerator IP for 21.6 fJ/op Binary\n  Neural Network Inference Abstract: Binary Neural Networks (BNNs) are promising to deliver accuracy comparable to\nconventional deep neural networks at a fraction of the cost in terms of memory\nand energy. In this paper, we introduce the XNOR Neural Engine (XNE), a fully\ndigital configurable hardware accelerator IP for BNNs, integrated within a\nmicrocontroller unit (MCU) equipped with an autonomous I/O subsystem and hybrid\nSRAM / standard cell memory. The XNE is able to fully compute convolutional and\ndense layers in autonomy or in cooperation with the core in the MCU to realize\nmore complex behaviors. We show post-synthesis results in 65nm and 22nm\ntechnology for the XNE IP and post-layout results in 22nm for the full MCU\nindicating that this system can drop the energy cost per binary operation to\n21.6fJ per operation at 0.4V, and at the same time is flexible and performant\nenough to execute state-of-the-art BNN topologies such as ResNet-34 in less\nthan 2.2mJ per frame at 8.9 fps. \n\n"}
{"id": "1807.03165", "contents": "Title: Sparse Deep Neural Network Exact Solutions Abstract: Deep neural networks (DNNs) have emerged as key enablers of machine learning.\nApplying larger DNNs to more diverse applications is an important challenge.\nThe computations performed during DNN training and inference are dominated by\noperations on the weight matrices describing the DNN. As DNNs incorporate more\nlayers and more neurons per layers, these weight matrices may be required to be\nsparse because of memory limitations. Sparse DNNs are one possible approach,\nbut the underlying theory is in the early stages of development and presents a\nnumber of challenges, including determining the accuracy of inference and\nselecting nonzero weights for training. Associative array algebra has been\ndeveloped by the big data community to combine and extend database, matrix, and\ngraph/network concepts for use in large, sparse data problems. Applying this\nmathematics to DNNs simplifies the formulation of DNN mathematics and reveals\nthat DNNs are linear over oscillating semirings. This work uses associative\narray DNNs to construct exact solutions and corresponding perturbation models\nto the rectified linear unit (ReLU) DNN equations that can be used to construct\ntest vectors for sparse DNN implementations over various precisions. These\nsolutions can be used for DNN verification, theoretical explorations of DNN\nproperties, and a starting point for the challenge of sparse training. \n\n"}
{"id": "1807.03210", "contents": "Title: Efficient Decentralized Deep Learning by Dynamic Model Averaging Abstract: We propose an efficient protocol for decentralized training of deep neural\nnetworks from distributed data sources. The proposed protocol allows to handle\ndifferent phases of model training equally well and to quickly adapt to concept\ndrifts. This leads to a reduction of communication by an order of magnitude\ncompared to periodically communicating state-of-the-art approaches. Moreover,\nwe derive a communication bound that scales well with the hardness of the\nserialized learning problem. The reduction in communication comes at almost no\ncost, as the predictive performance remains virtually unchanged. Indeed, the\nproposed protocol retains loss bounds of periodically averaging schemes. An\nextensive empirical evaluation validates major improvement of the trade-off\nbetween model performance and communication which could be beneficial for\nnumerous decentralized learning applications, such as autonomous driving, or\nvoice recognition and image classification on mobile phones. \n\n"}
{"id": "1807.03632", "contents": "Title: The SAGE Project: a Storage Centric Approach for Exascale Computing Abstract: SAGE (Percipient StorAGe for Exascale Data Centric Computing) is a European\nCommission funded project towards the era of Exascale computing. Its goal is to\ndesign and implement a Big Data/Extreme Computing (BDEC) capable infrastructure\nwith associated software stack. The SAGE system follows a \"storage centric\"\napproach as it is capable of storing and processing large data volumes at the\nExascale regime.\n  SAGE addresses the convergence of Big Data Analysis and HPC in an era of\nnext-generation data centric computing. This convergence is driven by the\nproliferation of massive data sources, such as large, dispersed scientific\ninstruments and sensors where data needs to be processed, analyzed and\nintegrated into simulations to derive scientific and innovative insights. A\nfirst prototype of the SAGE system has been been implemented and installed at\nthe Julich Supercomputing Center. The SAGE storage system consists of multiple\ntypes of storage device technologies in a multi-tier I/O hierarchy, including\nflash, disk, and non-volatile memory technologies. The main SAGE software\ncomponent is the Seagate Mero Object Storage that is accessible via the Clovis\nAPI and higher level interfaces. The SAGE project also includes scientific\napplications for the validation of the SAGE concepts.\n  The objective of this paper is to present the SAGE project concepts, the\nprototype of the SAGE platform and discuss the software architecture of the\nSAGE system. \n\n"}
{"id": "1807.05076", "contents": "Title: Metalearning with Hebbian Fast Weights Abstract: We unify recent neural approaches to one-shot learning with older ideas of\nassociative memory in a model for metalearning. Our model learns jointly to\nrepresent data and to bind class labels to representations in a single shot. It\nbuilds representations via slow weights, learned across tasks through SGD,\nwhile fast weights constructed by a Hebbian learning rule implement one-shot\nbinding for each new task. On the Omniglot, Mini-ImageNet, and Penn Treebank\none-shot learning benchmarks, our model achieves state-of-the-art results. \n\n"}
{"id": "1807.05358", "contents": "Title: Beyond Data and Model Parallelism for Deep Neural Networks Abstract: The computational requirements for training deep neural networks (DNNs) have\ngrown to the point that it is now standard practice to parallelize training.\nExisting deep learning systems commonly use data or model parallelism, but\nunfortunately, these strategies often result in suboptimal parallelization\nperformance.\n  In this paper, we define a more comprehensive search space of parallelization\nstrategies for DNNs called SOAP, which includes strategies to parallelize a DNN\nin the Sample, Operation, Attribute, and Parameter dimensions. We also propose\nFlexFlow, a deep learning framework that uses guided randomized search of the\nSOAP space to find a fast parallelization strategy for a specific parallel\nmachine. To accelerate this search, FlexFlow introduces a novel execution\nsimulator that can accurately predict a parallelization strategy's performance\nand is three orders of magnitude faster than prior approaches that have to\nexecute each strategy. We evaluate FlexFlow with six real-world DNN benchmarks\non two GPU clusters and show that FlexFlow can increase training throughput by\nup to 3.8x over state-of-the-art approaches, even when including its search\ntime, and also improves scalability. \n\n"}
{"id": "1807.06624", "contents": "Title: Distributed Triangle Detection via Expander Decomposition Abstract: We present improved distributed algorithms for triangle detection and its\nvariants in the CONGEST model. We show that Triangle Detection, Counting, and\nEnumeration can be solved in $\\tilde{O}(n^{1/2})$ rounds. In contrast, the\nprevious state-of-the-art bounds for Triangle Detection and Enumeration were\n$\\tilde{O}(n^{2/3})$ and $\\tilde{O}(n^{3/4})$, respectively, due to Izumi and\nLeGall (PODC 2017).\n  The main technical novelty in this work is a distributed graph partitioning\nalgorithm. We show that in $\\tilde{O}(n^{1-\\delta})$ rounds we can partition\nthe edge set of the network $G=(V,E)$ into three parts $E=E_m\\cup E_s\\cup E_r$\nsuch that\n  (a) Each connected component induced by $E_m$ has minimum degree\n$\\Omega(n^\\delta)$ and conductance $\\Omega(1/\\text{poly} \\log(n))$. As a\nconsequence the mixing time of a random walk within the component is\n$O(\\text{poly} \\log(n))$.\n  (b) The subgraph induced by $E_s$ has arboricity at most $n^{\\delta}$.\n  (c) $|E_r| \\leq |E|/6$.\n  All of our algorithms are based on the following generic framework, which we\nbelieve is of interest beyond this work. Roughly, we deal with the set $E_s$ by\nan algorithm that is efficient for low-arboricity graphs, and deal with the set\n$E_r$ using recursive calls. For each connected component induced by $E_m$, we\nare able to simulate congested clique algorithms with small overhead by\napplying a routing algorithm due to Ghaffari, Kuhn, and Su (PODC 2017) for high\nconductance graphs. \n\n"}
{"id": "1807.07422", "contents": "Title: Delay and Communication Tradeoffs for Blockchain Systems with\n  Lightweight IoT Clients Abstract: The emerging blockchain protocols provide a decentralized architecture that\nis suitable of supporting Internet of Things (IoT) interactions. However,\nkeeping a local copy of the blockchain ledger is infeasible for low-power and\nmemory-constrained devices. For this reason, they are equipped with lightweight\nsoftware implementations that only download the useful data structures, e.g.\nstate of accounts, from the blockchain network, when they are updated. In this\npaper, we consider and analyze a novel scheme, implemented by the nodes of the\nblockchain network, which aggregates the blockchain data in periodic updates\nand further reduces the communication cost of the connected IoT devices. We\nshow that the aggregation period should be selected based on the channel\nquality, the offered rate, and the statistics of updates of the useful data\nstructures. The results, obtained for the Ethereum protocol, illustrate the\nbenefits of the aggregation scheme in terms of a reduced duty cycle of the\ndevice, particularly for low signal-to-noise ratios, and the overall reduction\nof the amount of information transmitted in downlink (e.g., from the wireless\nbase station to the IoT device). A potential application of the proposed scheme\nis to let the IoT device request more information than actually needed, hence\nincreasing its privacy, while keeping the communication cost constant. In\nconclusion, our work is the first to provide rigorous guidelines for the design\nof lightweight blockchain protocols with wireless connectivity. \n\n"}
{"id": "1807.07928", "contents": "Title: Eyeriss v2: A Flexible Accelerator for Emerging Deep Neural Networks on\n  Mobile Devices Abstract: A recent trend in DNN development is to extend the reach of deep learning\napplications to platforms that are more resource and energy constrained, e.g.,\nmobile devices. These endeavors aim to reduce the DNN model size and improve\nthe hardware processing efficiency, and have resulted in DNNs that are much\nmore compact in their structures and/or have high data sparsity. These compact\nor sparse models are different from the traditional large ones in that there is\nmuch more variation in their layer shapes and sizes, and often require\nspecialized hardware to exploit sparsity for performance improvement. Thus,\nmany DNN accelerators designed for large DNNs do not perform well on these\nmodels. In this work, we present Eyeriss v2, a DNN accelerator architecture\ndesigned for running compact and sparse DNNs. To deal with the widely varying\nlayer shapes and sizes, it introduces a highly flexible on-chip network, called\nhierarchical mesh, that can adapt to the different amounts of data reuse and\nbandwidth requirements of different data types, which improves the utilization\nof the computation resources. Furthermore, Eyeriss v2 can process sparse data\ndirectly in the compressed domain for both weights and activations, and\ntherefore is able to improve both processing speed and energy efficiency with\nsparse models. Overall, with sparse MobileNet, Eyeriss v2 in a 65nm CMOS\nprocess achieves a throughput of 1470.6 inferences/sec and 2560.3 inferences/J\nat a batch size of 1, which is 12.6x faster and 2.5x more energy efficient than\nthe original Eyeriss running MobileNet. We also present an analysis methodology\ncalled Eyexam that provides a systematic way of understanding the performance\nlimits for DNN processors as a function of specific characteristics of the DNN\nmodel and accelerator design; it applies these characteristics as sequential\nsteps to increasingly tighten the bound on the performance limits. \n\n"}
{"id": "1807.09844", "contents": "Title: Modular Mechanistic Networks: On Bridging Mechanistic and\n  Phenomenological Models with Deep Neural Networks in Natural Language\n  Processing Abstract: Natural language processing (NLP) can be done using either top-down (theory\ndriven) and bottom-up (data driven) approaches, which we call mechanistic and\nphenomenological respectively. The approaches are frequently considered to\nstand in opposition to each other. Examining some recent approaches in deep\nlearning we argue that deep neural networks incorporate both perspectives and,\nfurthermore, that leveraging this aspect of deep learning may help in solving\ncomplex problems within language technology, such as modelling language and\nperception in the domain of spatial cognition. \n\n"}
{"id": "1807.09946", "contents": "Title: Computationally Efficient Measures of Internal Neuron Importance Abstract: The challenge of assigning importance to individual neurons in a network is\nof interest when interpreting deep learning models. In recent work, Dhamdhere\net al. proposed Total Conductance, a \"natural refinement of Integrated\nGradients\" for attributing importance to internal neurons. Unfortunately, the\nauthors found that calculating conductance in tensorflow required the addition\nof several custom gradient operators and did not scale well. In this work, we\nshow that the formula for Total Conductance is mathematically equivalent to\nPath Integrated Gradients computed on a hidden layer in the network. We provide\na scalable implementation of Total Conductance using standard tensorflow\ngradient operators that we call Neuron Integrated Gradients. We compare Neuron\nIntegrated Gradients to DeepLIFT, a pre-existing computationally efficient\napproach that is applicable to calculating internal neuron importance. We find\nthat DeepLIFT produces strong empirical results and is faster to compute, but\nbecause it lacks the theoretical properties of Neuron Integrated Gradients, it\nmay not always be preferred in practice. Colab notebook reproducing results:\nhttp://bit.ly/neuronintegratedgradients \n\n"}
{"id": "1807.10749", "contents": "Title: Quantum Supremacy Is Both Closer and Farther than It Appears Abstract: As quantum computers improve in the number of qubits and fidelity, the\nquestion of when they surpass state-of-the-art classical computation for a\nwell-defined computational task is attracting much attention. The leading\ncandidate task for this milestone entails sampling from the output distribution\ndefined by a random quantum circuit. We develop a massively-parallel simulation\ntool Rollright that does not require inter-process communication (IPC) or\nproprietary hardware. We also develop two ways to trade circuit fidelity for\ncomputational speedups, so as to match the fidelity of a given quantum computer\n--- a task previously thought impossible. We report massive speedups for the\nsampling task over prior software from Microsoft, IBM, Alibaba and Google, as\nwell as supercomputer and GPU-based simulations. By using publicly available\nGoogle Cloud Computing, we price such simulations and enable comparisons by\ntotal cost across hardware platforms. We simulate approximate sampling from the\noutput of a circuit with 7x8 qubits and depth 1+40+1 by producing one million\nbitstring probabilities with fidelity 0.5%, at an estimated cost of $35184. The\nsimulation costs scale linearly with fidelity, and using this scaling we\nestimate that extending circuit depth to 1+48+1 increases costs to one million\ndollars. Scaling the simulation to 10M bitstring probabilities needed for\nsampling 1M bitstrings helps comparing simulation to quantum computers. We\ndescribe refinements in benchmarks that slow down leading simulators, halving\nthe circuit depth that can be simulated within the same time. \n\n"}
{"id": "1808.00193", "contents": "Title: Reinforced Evolutionary Neural Architecture Search Abstract: Neural Architecture Search (NAS) is an important yet challenging task in\nnetwork design due to its high computational consumption. To address this\nissue, we propose the Reinforced Evolutionary Neural Architecture Search (RE-\nNAS), which is an evolutionary method with the reinforced mutation for NAS. Our\nmethod integrates reinforced mutation into an evolution algorithm for neural\narchitecture exploration, in which a mutation controller is introduced to learn\nthe effects of slight modifications and make mutation actions. The reinforced\nmutation controller guides the model population to evolve efficiently.\nFurthermore, as child models can inherit parameters from their parents during\nevolution, our method requires very limited computational resources. In\nexperiments, we conduct the proposed search method on CIFAR-10 and obtain a\npowerful network architecture, RENASNet. This architecture achieves a\ncompetitive result on CIFAR-10. The explored network architecture is\ntransferable to ImageNet and achieves a new state-of-the-art accuracy, i.e.,\n75.7% top-1 accuracy with 5.36M parameters on mobile ImageNet. We further test\nits performance on semantic segmentation with DeepLabv3 on the PASCAL VOC.\nRENASNet outperforms MobileNet-v1, MobileNet-v2 and NASNet. It achieves 75.83%\nmIOU without being pre-trained on COCO. \n\n"}
{"id": "1808.00252", "contents": "Title: A Deep Neural Model Of Emotion Appraisal Abstract: Emotional concepts play a huge role in our daily life since they take part\ninto many cognitive processes: from the perception of the environment around us\nto different learning processes and natural communication. Social robots need\nto communicate with humans, which increased also the popularity of affective\nembodied models that adopt different emotional concepts in many everyday tasks.\nHowever, there is still a gap between the development of these solutions and\nthe integration and development of a complex emotion appraisal system, which is\nmuch necessary for true social robots. In this paper, we propose a deep neural\nmodel which is designed in the light of different aspects of developmental\nlearning of emotional concepts to provide an integrated solution for internal\nand external emotion appraisal. We evaluate the performance of the proposed\nmodel with different challenging corpora and compare it with state-of-the-art\nmodels for external emotion appraisal. To extend the evaluation of the proposed\nmodel, we designed and collected a novel dataset based on a Human-Robot\nInteraction (HRI) scenario. We deployed the model in an iCub robot and\nevaluated the capability of the robot to learn and describe the affective\nbehavior of different persons based on observation. The performed experiments\ndemonstrate that the proposed model is competitive with the state of the art in\ndescribing emotion behavior in general. In addition, it is able to generate\ninternal emotional concepts that evolve through time: it continuously forms and\nupdates the formed emotional concepts, which is a step towards creating an\nemotional appraisal model grounded in the robot experiences. \n\n"}
{"id": "1808.00481", "contents": "Title: Space Complexity of Implementing Large Shared Registers Abstract: We prove two new space lower bounds for the problem of implementing a large\nshared register using smaller physical shared registers. We focus on the case\nwhere both the implemented and physical registers are single-writer, which\nmeans they can be accessed concurrently by multiple readers but only by a\nsingle writer. To strengthen our lower bounds, we let the physical registers be\natomic and we only require the implemented register to be regular. Furthermore,\nthe lower bounds hold for obstruction-free implementations, which means they\nalso hold for lock-free and wait-free implementations.\n  If $m$ is the number values representable by the large register and $b$ is\nthe number of values representable by each physical register, our first lower\nbound says that any obstruction-free implementation that has an invisible\nreader requires at least $\\lceil \\frac{m-1}{b-1} \\rceil$ physical registers. A\nreader is considered invisible if it never writes to shared registers. This\nlower bound is tight for the invisible reader case. We also prove a $\\lceil\n\\min(\\frac{m-1}{b-1}, r+\\frac{\\log{m}}{\\log{b}}) \\rceil$ space lower bound for\nthe general case, which covers both visible and invisible readers. In this\nbound, $r$ represents the number of readers. \n\n"}
{"id": "1808.00556", "contents": "Title: Container solutions for HPC Systems: A Case Study of Using Shifter on\n  Blue Waters Abstract: Software container solutions have revolutionized application development\napproaches by enabling lightweight platform abstractions within the so-called\n\"containers.\" Several solutions are being actively developed in attempts to\nbring the benefits of containers to high-performance computing systems with\ntheir stringent security demands on the one hand and fundamental resource\nsharing requirements on the other.\n  In this paper, we discuss the benefits and short-comings of such solutions\nwhen deployed on real HPC systems and applied to production scientific\napplications.We highlight use cases that are either enabled by or significantly\nbenefit from such solutions. We discuss the efforts by HPC system\nadministrators and support staff to support users of these type of workloads on\nHPC systems not initially designed with these workloads in mind focusing on\nNCSA's Blue Waters system. \n\n"}
{"id": "1808.00829", "contents": "Title: A Systematic Comparison of Dynamic Load Balancing Algorithms for\n  Massively Parallel Rigid Particle Dynamics Abstract: As compute power increases with time, more involved and larger simulations\nbecome possible. However, it gets increasingly difficult to efficiently use the\nprovided computational resources. Especially in particle-based simulations with\na spatial domain partitioning large load imbalances can occur due to the\nsimulation being dynamic. Then a static domain partitioning may not be\nsuitable. This can deteriorate the overall runtime of the simulation\nsignificantly. Sophisticated load balancing strategies must be designed to\nalleviate this problem. In this paper we conduct a systematic evaluation of the\nperformance of six different load balancing algorithms. Our tests cover a wide\nrange of simulation sizes, and employ one of the largest supercomputers\navailable. In particular we study the runtime and memory complexity of all\ncomponents of the simulation carefully. When progressing to extreme scale\nsimulations it is essential to identify bottlenecks and to predict the scaling\nbehaviour. Scaling experiments are shown for up to over one million processes.\nThe performance of each algorithm is analyzed with respect to the quality of\nthe load balancing and its runtime costs. For all tests, the waLBerla\nmultiphysics framework is employed. \n\n"}
{"id": "1808.02134", "contents": "Title: Kerman: A Hybrid Lightweight Tracking Algorithm to Enable Smart\n  Surveillance as an Edge Service Abstract: Edge computing pushes the cloud computing boundaries beyond uncertain network\nresource by leveraging computational processes close to the source and target\nof data. Time-sensitive and data-intensive video surveillance applications\nbenefit from on-site or near-site data mining. In recent years, many smart\nvideo surveillance approaches are proposed for object detection and tracking by\nusing Artificial Intelligence (AI) and Machine Learning (ML) algorithms.\nHowever, it is still hard to migrate those computing and data-intensive tasks\nfrom Cloud to Edge due to the high computational requirement. In this paper, we\nenvision to achieve intelligent surveillance as an edge service by proposing a\nhybrid lightweight tracking algorithm named Kerman (Kernelized Kalman filter).\nKerman is a decision tree based hybrid Kernelized Correlation Filter (KCF)\nalgorithm proposed for human object tracking, which is coupled with a\nlightweight Convolutional Neural Network (L-CNN) for high performance. The\nproposed Kerman algorithm has been implemented on a couple of single board\ncomputers (SBC) as edge devices and validated using real-world surveillance\nvideo streams. The experimental results are promising that the Kerman algorithm\nis able to track the object of interest with a decent accuracy at a resource\nconsumption affordable by edge devices. \n\n"}
{"id": "1808.02546", "contents": "Title: Parallel and Streaming Algorithms for K-Core Decomposition Abstract: The $k$-core decomposition is a fundamental primitive in many machine\nlearning and data mining applications. We present the first distributed and the\nfirst streaming algorithms to compute and maintain an approximate $k$-core\ndecomposition with provable guarantees. Our algorithms achieve rigorous bounds\non space complexity while bounding the number of passes or number of rounds of\ncomputation. We do so by presenting a new powerful sketching technique for\n$k$-core decomposition, and then by showing it can be computed efficiently in\nboth streaming and MapReduce models. Finally, we confirm the effectiveness of\nour sketching technique empirically on a number of publicly available graphs. \n\n"}
{"id": "1808.03703", "contents": "Title: LemmaTag: Jointly Tagging and Lemmatizing for Morphologically-Rich\n  Languages with BRNNs Abstract: We present LemmaTag, a featureless neural network architecture that jointly\ngenerates part-of-speech tags and lemmas for sentences by using bidirectional\nRNNs with character-level and word-level embeddings. We demonstrate that both\ntasks benefit from sharing the encoding part of the network, predicting tag\nsubcategories, and using the tagger output as an input to the lemmatizer. We\nevaluate our model across several languages with complex morphology, which\nsurpasses state-of-the-art accuracy in both part-of-speech tagging and\nlemmatization in Czech, German, and Arabic. \n\n"}
{"id": "1808.04118", "contents": "Title: AsySPA: An Exact Asynchronous Algorithm for Convex Optimization Over\n  Digraphs Abstract: This paper proposes a novel exact distributed asynchronous subgradient-push\nalgorithm (AsySPA) to solve an additive cost optimization problem over directed\ngraphs where each node only has access to a local convex function and updates\nasynchronously with an arbitrary rate. Specifically, each node of a strongly\nconnected digraph does not wait for updates from other nodes but simply starts\na new update within any bounded time interval by using local information\navailable from its in-neighbors. \"Exact\" means that every node of the AsySPA\ncan asymptotically converge to the same optimal solution, even under different\nupdate rates among nodes and bounded communication delays. To address uneven\nupdate rates, we design a simple mechanism to adaptively adjust stepsizes per\nupdate in each node, which is substantially different from the existing works.\nThen, we construct a delay-free augmented system to address asynchrony and\ndelays, and study its convergence by proposing a generalized subgradient\nalgorithm, which clearly has its own significance and helps us to explicitly\nevaluate the convergence rate of the AsySPA. Finally, we demonstrate advantages\nof the AsySPA in both theory and simulation. \n\n"}
{"id": "1808.04143", "contents": "Title: A Preliminary Study On Emerging Cloud Computing Security Challenges Abstract: Cloud computing is the internet based provisioning of the computing\nresources, software, and information on demand. Cloud Computing is referred to\nas one of most recent emerging paradigms of computing utilities. Since Cloud\ncomputing is the dominant infrastructure of the shared services over the\ninternet, it is important to be aware of the security risk and the challenges\nassociated with this emerging computing paradigm. This survey provides a brief\nintroduction to the cloud computing, its major characteristics, and service\nmodels. It also explores cloud security threats, lists a few security solutions\n, and proposes a promsing research direction to deal with the evolving security\nchallenges in Cloud computing. \n\n"}
{"id": "1808.07576", "contents": "Title: Cooperative SGD: A unified Framework for the Design and Analysis of\n  Communication-Efficient SGD Algorithms Abstract: Communication-efficient SGD algorithms, which allow nodes to perform local\nupdates and periodically synchronize local models, are highly effective in\nimproving the speed and scalability of distributed SGD. However, a rigorous\nconvergence analysis and comparative study of different communication-reduction\nstrategies remains a largely open problem. This paper presents a unified\nframework called Cooperative SGD that subsumes existing communication-efficient\nSGD algorithms such as periodic-averaging, elastic-averaging and decentralized\nSGD. By analyzing Cooperative SGD, we provide novel convergence guarantees for\nexisting algorithms. Moreover, this framework enables us to design new\ncommunication-efficient SGD algorithms that strike the best balance between\nreducing communication overhead and achieving fast error convergence with low\nerror floor. \n\n"}
{"id": "1808.10011", "contents": "Title: Fast and accessible first-principles calculations of vibrational\n  properties of materials Abstract: We present example applications of an approach to first-principles\ncalculations of vibrational properties of materials implemented within the\nExabyte.io platform. We deploy models based on the Density Functional\nPerturbation Theory to extract the phonon dispersion relations and densities of\nstates for an example set of 35 samples and find the results to be in agreement\nwith prior similar calculations. We construct modeling workflows that are both\naccessible, accurate, and efficient with respect to the human time involved.\nThis is achieved through efficient parallelization of the tasks for the\nindividual vibrational modes. We report achieved speedups in the 10-100 range,\napproximately, and maximum attainable speedups in the 30-300 range,\ncorrespondingly. We analyze the execution times on the current up-to-date\ncomputational infrastructure centrally available from a public cloud provider.\nResults and all associated data, including the materials and simulation\nworkflows, are made available online in an accessible, repeatable and\nextensible setting. \n\n"}
{"id": "1809.01275", "contents": "Title: Solving Non-smooth Constrained Programs with Lower Complexity than\n  $\\mathcal{O}(1/\\varepsilon)$: A Primal-Dual Homotopy Smoothing Approach Abstract: We propose a new primal-dual homotopy smoothing algorithm for a linearly\nconstrained convex program, where neither the primal nor the dual function has\nto be smooth or strongly convex. The best known iteration complexity solving\nsuch a non-smooth problem is $\\mathcal{O}(\\varepsilon^{-1})$. In this paper, we\nshow that by leveraging a local error bound condition on the dual function, the\nproposed algorithm can achieve a better primal convergence time of\n$\\mathcal{O}\\left(\\varepsilon^{-2/(2+\\beta)}\\log_2(\\varepsilon^{-1})\\right)$,\nwhere $\\beta\\in(0,1]$ is a local error bound parameter. As an example\napplication of the general algorithm, we show that the distributed geometric\nmedian problem, which can be formulated as a constrained convex program, has\nits dual function non-smooth but satisfying the aforementioned local error\nbound condition with $\\beta=1/2$, therefore enjoying a convergence time of\n$\\mathcal{O}\\left(\\varepsilon^{-4/5}\\log_2(\\varepsilon^{-1})\\right)$. This\nresult improves upon the $\\mathcal{O}(\\varepsilon^{-1})$ convergence time bound\nachieved by existing distributed optimization algorithms. Simulation\nexperiments also demonstrate the performance of our proposed algorithm. \n\n"}
{"id": "1809.01516", "contents": "Title: Parallel numerical method for nonlocal-in-time Schr\\\"odinger equation Abstract: We present a new parallel numerical method for solving the non-stationary\nSchr\\\"odinger equation with linear nonlocal condition and time-dependent\npotential which does not commute with the stationary part of the Hamiltonian.\nThe given problem is discretized in-time using a polynomial-based collocation\nscheme. We establish the conditions on the existence of solution to the\ndiscretized problem, estimate the accuracy of the discretized solution and\npropose the method how this solution can be approximately found in an efficient\nparallel manner. \n\n"}
{"id": "1809.02032", "contents": "Title: Latent Molecular Optimization for Targeted Therapeutic Design Abstract: We devise an approach for targeted molecular design, a problem of interest in\ncomputational drug discovery: given a target protein site, we wish to generate\na chemical with both high binding affinity to the target and satisfactory\npharmacological properties. This problem is made difficult by the enormity and\ndiscreteness of the space of potential therapeutics, as well as the\ngraph-structured nature of biomolecular surface sites. Using a dataset of\nprotein-ligand complexes, we surmount these issues by extracting a signature of\nthe target site with a graph convolutional network and by encoding the discrete\nchemical into a continuous latent vector space. The latter embedding permits\ngradient-based optimization in molecular space, which we perform using learned\ndifferentiable models of binding affinity and other pharmacological properties.\nWe show that our approach is able to efficiently optimize these multiple\nobjectives and discover new molecules with potentially useful binding\nproperties, validated via docking methods. \n\n"}
{"id": "1809.02627", "contents": "Title: Unity: A General Platform for Intelligent Agents Abstract: Recent advances in artificial intelligence have been driven by the presence\nof increasingly realistic and complex simulated environments. However, many of\nthe existing environments provide either unrealistic visuals, inaccurate\nphysics, low task complexity, restricted agent perspective, or a limited\ncapacity for interaction among artificial agents. Furthermore, many platforms\nlack the ability to flexibly configure the simulation, making the simulated\nenvironment a black-box from the perspective of the learning system. In this\nwork, we propose a novel taxonomy of existing simulation platforms and discuss\nthe highest level class of general platforms which enable the development of\nlearning environments that are rich in visual, physical, task, and social\ncomplexity. We argue that modern game engines are uniquely suited to act as\ngeneral platforms and as a case study examine the Unity engine and open source\nUnity ML-Agents Toolkit. We then survey the research enabled by Unity and the\nUnity ML-Agents Toolkit, discussing the kinds of research a flexible,\ninteractive and easily configurable general platform can facilitate. \n\n"}
{"id": "1809.02942", "contents": "Title: Cellular automata as convolutional neural networks Abstract: Deep learning techniques have recently demonstrated broad success in\npredicting complex dynamical systems ranging from turbulence to human speech,\nmotivating broader questions about how neural networks encode and represent\ndynamical rules. We explore this problem in the context of cellular automata\n(CA), simple dynamical systems that are intrinsically discrete and thus\ndifficult to analyze using standard tools from dynamical systems theory. We\nshow that any CA may readily be represented using a convolutional neural\nnetwork with a network-in-network architecture. This motivates our development\nof a general convolutional multilayer perceptron architecture, which we find\ncan learn the dynamical rules for arbitrary CA when given videos of the CA as\ntraining data. In the limit of large network widths, we find that training\ndynamics are nearly identical across replicates, and that common patterns\nemerge in the structure of networks trained on different CA rulesets. We train\nensembles of networks on randomly-sampled CA, and we probe how the trained\nnetworks internally represent the CA rules using an information-theoretic\ntechnique based on distributions of layer activation patterns. We find that CA\nwith simpler rule tables produce trained networks with hierarchical structure\nand layer specialization, while more complex CA produce shallower\nrepresentations---illustrating how the underlying complexity of the CA's rules\ninfluences the specificity of these internal representations. Our results\nsuggest how the entropy of a physical process can affect its representation\nwhen learned by neural networks. \n\n"}
{"id": "1809.03721", "contents": "Title: Deep Asymmetric Networks with a Set of Node-wise Variant Activation\n  Functions Abstract: This work presents deep asymmetric networks with a set of node-wise variant\nactivation functions. The nodes' sensitivities are affected by activation\nfunction selections such that the nodes with smaller indices become\nincreasingly more sensitive. As a result, features learned by the nodes are\nsorted by the node indices in the order of their importance. Asymmetric\nnetworks not only learn input features but also the importance of those\nfeatures. Nodes of lesser importance in asymmetric networks can be pruned to\nreduce the complexity of the networks, and the pruned networks can be retrained\nwithout incurring performance losses. We validate the feature-sorting property\nusing both shallow and deep asymmetric networks as well as deep asymmetric\nnetworks transferred from famous networks. \n\n"}
{"id": "1809.04497", "contents": "Title: Hyperprior Induced Unsupervised Disentanglement of Latent\n  Representations Abstract: We address the problem of unsupervised disentanglement of latent\nrepresentations learnt via deep generative models. In contrast to current\napproaches that operate on the evidence lower bound (ELBO), we argue that\nstatistical independence in the latent space of VAEs can be enforced in a\nprincipled hierarchical Bayesian manner. To this effect, we augment the\nstandard VAE with an inverse-Wishart (IW) prior on the covariance matrix of the\nlatent code. By tuning the IW parameters, we are able to encourage (or\ndiscourage) independence in the learnt latent dimensions. Extensive\nexperimental results on a range of datasets (2DShapes, 3DChairs, 3DFaces and\nCelebA) show our approach to outperform the $\\beta$-VAE and is competitive with\nthe state-of-the-art FactorVAE. Our approach achieves significantly better\ndisentanglement and reconstruction on a new dataset (CorrelatedEllipses) which\nintroduces correlations between the factors of variation. \n\n"}
{"id": "1809.04561", "contents": "Title: Constant Amortized RMR Complexity Deterministic Abortable Mutual\n  Exclusion Algorithm for CC and DSM Models Abstract: The abortable mutual exclusion problem was introduced by Scott and Scherer to\nmeet a need that arises in database and real time systems, where processes\nsometimes have to abandon their attempt to acquire a mutual exclusion lock to\ninitiate recovery from a potential deadlock or to avoid overshooting a\ndeadline. Algorithms of O(1) RMR complexity have been known for the standard\nmutual exclusion problem for both the Cache-Coherent (CC) and Distributed\nShared Memory (DSM) models of multiprocessors, but whether O(1) RMR complexity\nis also achievable for abortable mutual exclusion has remained open for the 18\nyears that this problem has been investigated.\n  Jayanti gives a Theta(log n) worst case RMR complexity solution for both\nmodels, where n is the maximum number of processes that execute the algorithm\nconcurrently. Giakouppis and Woelfel's algorithm, presented at PODC last year,\nis an O(1) amortized complexity algorithm, but it works only for the CC model,\nuses randomization, does not satisfy Starvation Freedom, and the O(1) amortized\nbound holds only in expectation and is proven for the a weak (oblivious)\nadversary model.\n  We design an algorithm that is free of these limitations: our algorithm is\ndeterministic, supports fast aborts (a process completes an abort in O(1)\nsteps), has a small space complexity of O(n), requires hardware support for\nonly the Fetch&Store instruction, satisfies a novely defined First Come First\nServed for abortable locks, and most importantly, has O(1) amortized RMR\ncomplexity for both the CC and DSM models. Our algorithm is short and practical\nwith fewer than a dozen lines of code, and is accompanied by a rigorous proof\nof mutual exclusion through invariants and of starvation-freedom and complexity\nanalysis through distance and potential functions. Thus, modulo amortization,\nour result answers affirmatively the long standing open question described\nabove. \n\n"}
{"id": "1809.05793", "contents": "Title: Direct Training for Spiking Neural Networks: Faster, Larger, Better Abstract: Spiking neural networks (SNNs) that enables energy efficient implementation\non emerging neuromorphic hardware are gaining more attention. Yet now, SNNs\nhave not shown competitive performance compared with artificial neural networks\n(ANNs), due to the lack of effective learning algorithms and efficient\nprogramming frameworks. We address this issue from two aspects: (1) We propose\na neuron normalization technique to adjust the neural selectivity and develop a\ndirect learning algorithm for deep SNNs. (2) Via narrowing the rate coding\nwindow and converting the leaky integrate-and-fire (LIF) model into an\nexplicitly iterative version, we present a Pytorch-based implementation method\ntowards the training of large-scale SNNs. In this way, we are able to train\ndeep SNNs with tens of times speedup. As a result, we achieve significantly\nbetter accuracy than the reported works on neuromorphic datasets (N-MNIST and\nDVS-CIFAR10), and comparable accuracy as existing ANNs and pre-trained SNNs on\nnon-spiking datasets (CIFAR10). {To our best knowledge, this is the first work\nthat demonstrates direct training of deep SNNs with high performance on\nCIFAR10, and the efficient implementation provides a new way to explore the\npotential of SNNs. \n\n"}
{"id": "1809.07687", "contents": "Title: Next Stop \"NoOps\": Enabling Cross-System Diagnostics Through Graph-based\n  Composition of Logs and Metrics Abstract: Performing diagnostics in IT systems is an increasingly complicated task, and\nit is not doable in satisfactory time by even the most skillful operators.\nSystems and their architecture change very rapidly in response to business and\nuser demand. Many organizations see value in the maintenance and management\nmodel of NoOps that stands for No Operations. One of the implementations of\nthis model is a system that is maintained automatically without any human\nintervention. The path to NoOps involves not only precise and fast diagnostics\nbut also reusing as much knowledge as possible after the system is reconfigured\nor changed. The biggest challenge is to leverage knowledge on one IT system and\nreuse this knowledge for diagnostics of another, different system. We propose a\nframework of weighted graphs which can transfer knowledge, and perform\nhigh-quality diagnostics of IT systems. We encode all possible data in a graph\nrepresentation of a system state and automatically calculate weights of these\ngraphs. Then, thanks to the evaluation of similarity between graphs, we\ntransfer knowledge about failures from one system to another and use it for\ndiagnostics. We successfully evaluate the proposed approach on Spark, Hadoop,\nKafka and Cassandra systems. \n\n"}
{"id": "1809.09096", "contents": "Title: Text Summarization as Tree Transduction by Top-Down TreeLSTM Abstract: Extractive compression is a challenging natural language processing problem.\nThis work contributes by formulating neural extractive compression as a parse\ntree transduction problem, rather than a sequence transduction task. Motivated\nby this, we introduce a deep neural model for learning\nstructure-to-substructure tree transductions by extending the standard Long\nShort-Term Memory, considering the parent-child relationships in the structural\nrecursion. The proposed model can achieve state of the art performance on\nsentence compression benchmarks, both in terms of accuracy and compression\nrate. \n\n"}
{"id": "1809.09858", "contents": "Title: Dissecting Tendermint Abstract: In this paper we analyze Tendermint proposed in [7], one of the most popular\nblockchains based on PBFT Consensus. The current paper dissects Tendermint\nunder various system communication models and Byzantine adversaries. Our\nmethodology consists in identifying the algorithmic principles of Tendermint\nnecessary for a specific combination of communication model-adversary. This\nmethodology allowed to identify bugs [3] in preliminary versions of the\nprotocol ([19], [7]) and to prove its correctness under the most adversarial\nconditions: an eventually synchronous communication model and asymmetric\nByzantine faults. \n\n"}
{"id": "1809.09930", "contents": "Title: GPU Accelerated Similarity Self-Join for Multi-Dimensional Data Abstract: The self-join finds all objects in a dataset that are within a search\ndistance, epsilon, of each other; therefore, the self-join is a building block\nof many algorithms. We advance a GPU-accelerated self-join algorithm targeted\ntowards high dimensional data. The massive parallelism afforded by the GPU and\nhigh aggregate memory bandwidth makes the architecture well-suited for\ndata-intensive workloads. We leverage a grid-based, GPU-tailored index to\nperform range queries. We propose the following optimizations: (i) a trade-off\nbetween candidate set filtering and index search overhead by exploiting\nproperties of the index; (ii) reordering the data based on variance in each\ndimension to improve the filtering power of the index; and (iii) a pruning\nmethod for reducing the number of expensive distance calculations. Across most\nscenarios on real-world and synthetic datasets, our algorithm outperforms the\nparallel state-of-the-art approach. Exascale systems are converging on\nheterogeneous distributed-memory architectures. We show that an entity\npartitioning method can be utilized to achieve a balanced workload, and thus\ngood scalability for multi-GPU or distributed-memory self-joins. \n\n"}
{"id": "1809.10799", "contents": "Title: FanStore: Enabling Efficient and Scalable I/O for Distributed Deep\n  Learning Abstract: Emerging Deep Learning (DL) applications introduce heavy I/O workloads on\ncomputer clusters. The inherent long lasting, repeated, and random file access\npattern can easily saturate the metadata and data service and negatively impact\nother users. In this paper, we present FanStore, a transient runtime file\nsystem that optimizes DL I/O on existing hardware/software stacks. FanStore\ndistributes datasets to the local storage of compute nodes, and maintains a\nglobal namespace. With the techniques of system call interception, distributed\nmetadata management, and generic data compression, FanStore provides a\nPOSIX-compliant interface with native hardware throughput in an efficient and\nscalable manner. Users do not have to make intrusive code changes to use\nFanStore and take advantage of the optimized I/O. Our experiments with\nbenchmarks and real applications show that FanStore can scale DL training to\n512 compute nodes with over 90\\% scaling efficiency. \n\n"}
{"id": "1810.00104", "contents": "Title: Temporal Cliques Admit Sparse Spanners Abstract: Let $G=(V,E)$ be an undirected graph on $n$ vertices and $\\lambda:E\\to\n2^{\\mathbb{N}}$ a mapping that assigns to every edge a non-empty set of integer\nlabels (times). Such a graph is {\\em temporally connected} if a path exists\nwith non-decreasing times from every vertex to every other vertex. In a seminal\npaper, Kempe, Kleinberg, and Kumar \\cite{KKK02} asked whether, given such a\ntemporal graph, a {\\em sparse} subset of edges always exists whose labels\nsuffice to preserve temporal connectivity -- a {\\em temporal spanner}. Axiotis\nand Fotakis \\cite{AF16} answered negatively by exhibiting a family of\n$\\Theta(n^2)$-dense temporal graphs which admit no temporal spanner of density\n$o(n^2)$. In this paper, we give the first positive answer as to the existence\nof $o(n^2)$-sparse spanners in a dense class of temporal graphs, by showing\n(constructively) that if $G$ is a complete graph, then one can always find a\ntemporal spanner of density $O(n \\log n)$. \n\n"}
{"id": "1810.00305", "contents": "Title: Resource Management in Fog/Edge Computing: A Survey Abstract: Contrary to using distant and centralized cloud data center resources,\nemploying decentralized resources at the edge of a network for processing data\ncloser to user devices, such as smartphones and tablets, is an upcoming\ncomputing paradigm, referred to as fog/edge computing. Fog/edge resources are\ntypically resource-constrained, heterogeneous, and dynamic compared to the\ncloud, thereby making resource management an important challenge that needs to\nbe addressed. This article reviews publications as early as 1991, with 85% of\nthe publications between 2013-2018, to identify and classify the architectures,\ninfrastructure, and underlying algorithms for managing resources in fog/edge\ncomputing. \n\n"}
{"id": "1810.01993", "contents": "Title: Exascale Deep Learning for Climate Analytics Abstract: We extract pixel-level masks of extreme weather patterns using variants of\nTiramisu and DeepLabv3+ neural networks. We describe improvements to the\nsoftware frameworks, input pipeline, and the network training algorithms\nnecessary to efficiently scale deep learning on the Piz Daint and Summit\nsystems. The Tiramisu network scales to 5300 P100 GPUs with a sustained\nthroughput of 21.0 PF/s and parallel efficiency of 79.0%. DeepLabv3+ scales up\nto 27360 V100 GPUs with a sustained throughput of 325.8 PF/s and a parallel\nefficiency of 90.7% in single precision. By taking advantage of the FP16 Tensor\nCores, a half-precision version of the DeepLabv3+ network achieves a peak and\nsustained throughput of 1.13 EF/s and 999.0 PF/s respectively. \n\n"}
{"id": "1810.02186", "contents": "Title: OPERA: Reasoning about continuous common knowledge in asynchronous\n  distributed systems Abstract: This paper introduces a new family of consensus protocols, namely\n\\emph{Lachesis-class} denoted by $\\mathcal{L}$, for distributed networks with\nguaranteed Byzantine fault tolerance. Each Lachesis protocol $L$ in\n$\\mathcal{L}$ has complete asynchrony, is leaderless, has no round robin, no\nproof-of-work, and has eventual consensus.\n  The core concept of our technology is the \\emph{OPERA chain}, generated by\nthe Lachesis protocol. In the most general form, each node in Lachesis has a\nset of $k$ neighbours of most preference. When receiving transactions a node\ncreates and shares an event block with all neighbours. Each event block is\nsigned by the hashes of the creating node and its $k$ peers. The OPERA chain of\nthe event blocks is a Directed Acyclic Graph (DAG); it guarantees practical\nByzantine fault tolerance (pBFT). Our framework is then presented using Lamport\ntimestamps and concurrent common knowledge.\n  Further, we present an example of Lachesis consensus protocol $L_0$ of our\nframework. Our $L_0$ protocol can reach consensus upon 2/3 of all participants'\nagreement to an event block without any additional communication overhead.\n$L_0$ protocol relies on a cost function to identify $k$ peers and to generate\nthe DAG-based OPERA chain. By creating a binary flag table that stores\nconnection information and share information between blocks, Lachesis achieves\nconsensus in fewer steps than pBFT protocol for consensus. \n\n"}
{"id": "1810.02470", "contents": "Title: Prototyping Formal System Models with Active Objects Abstract: We propose active object languages as a development tool for formal system\nmodels of distributed systems. Additionally to a formalization based on a term\nrewriting system, we use established Software Engineering concepts, including\nsoftware product lines and object orientation that come with extensive tool\nsupport. We illustrate our modeling approach by prototyping a weak memory\nmodel. The resulting executable model is modular and has clear interfaces\nbetween communicating participants through object-oriented modeling.\nRelaxations of the basic memory model are expressed as self-contained variants\nof a software product line. As a modeling language we use the formal active\nobject language ABS which comes with an extensive tool set. This permits rapid\nformalization of core ideas, early validity checks in terms of formal invariant\nproofs, and debugging support by executing test runs. Hence, our approach\nsupports the prototyping of formal system models with early feedback. \n\n"}
{"id": "1810.03035", "contents": "Title: Characterizing Deep-Learning I/O Workloads in TensorFlow Abstract: The performance of Deep-Learning (DL) computing frameworks rely on the\nperformance of data ingestion and checkpointing. In fact, during the training,\na considerable high number of relatively small files are first loaded and\npre-processed on CPUs and then moved to accelerator for computation. In\naddition, checkpointing and restart operations are carried out to allow DL\ncomputing frameworks to restart quickly from a checkpoint. Because of this, I/O\naffects the performance of DL applications. In this work, we characterize the\nI/O performance and scaling of TensorFlow, an open-source programming framework\ndeveloped by Google and specifically designed for solving DL problems. To\nmeasure TensorFlow I/O performance, we first design a micro-benchmark to\nmeasure TensorFlow reads, and then use a TensorFlow mini-application based on\nAlexNet to measure the performance cost of I/O and checkpointing in TensorFlow.\nTo improve the checkpointing performance, we design and implement a burst\nbuffer. We find that increasing the number of threads increases TensorFlow\nbandwidth by a maximum of 2.3x and 7.8x on our benchmark environments. The use\nof the tensorFlow prefetcher results in a complete overlap of computation on\naccelerator and input pipeline on CPU eliminating the effective cost of I/O on\nthe overall performance. The use of a burst buffer to checkpoint to a fast\nsmall capacity storage and copy asynchronously the checkpoints to a slower\nlarge capacity storage resulted in a performance improvement of 2.6x with\nrespect to checkpointing directly to slower storage on our benchmark\nenvironment. \n\n"}
{"id": "1810.04254", "contents": "Title: Extreme Classification in Log Memory Abstract: We present Merged-Averaged Classifiers via Hashing (MACH) for\nK-classification with ultra-large values of K. Compared to traditional\none-vs-all classifiers that require O(Kd) memory and inference cost, MACH only\nneed O(d log K) (d is dimensionality )memory while only requiring O(K log K + d\nlog K) operation for inference. MACH is a generic K-classification algorithm,\nwith provably theoretical guarantees, which requires O(log K) memory without\nany assumption on the relationship between classes. MACH uses universal hashing\nto reduce classification with a large number of classes to few independent\nclassification tasks with small (constant) number of classes. We provide\ntheoretical quantification of discriminability-memory tradeoff. With MACH we\ncan train ODP dataset with 100,000 classes and 400,000 features on a single\nTitan X GPU, with the classification accuracy of 19.28%, which is the\nbest-reported accuracy on this dataset. Before this work, the best performing\nbaseline is a one-vs-all classifier that requires 40 billion parameters (160 GB\nmodel size) and achieves 9% accuracy. In contrast, MACH can achieve 9% accuracy\nwith 480x reduction in the model size (of mere 0.3GB). With MACH, we also\ndemonstrate complete training of fine-grained imagenet dataset (compressed size\n104GB), with 21,000 classes, on a single GPU. To the best of our knowledge,\nthis is the first work to demonstrate complete training of these extreme-class\ndatasets on a single Titan X. \n\n"}
{"id": "1810.05148", "contents": "Title: Bayesian Deep Convolutional Networks with Many Channels are Gaussian\n  Processes Abstract: There is a previously identified equivalence between wide fully connected\nneural networks (FCNs) and Gaussian processes (GPs). This equivalence enables,\nfor instance, test set predictions that would have resulted from a fully\nBayesian, infinitely wide trained FCN to be computed without ever instantiating\nthe FCN, but by instead evaluating the corresponding GP. In this work, we\nderive an analogous equivalence for multi-layer convolutional neural networks\n(CNNs) both with and without pooling layers, and achieve state of the art\nresults on CIFAR10 for GPs without trainable kernels. We also introduce a Monte\nCarlo method to estimate the GP corresponding to a given neural network\narchitecture, even in cases where the analytic form has too many terms to be\ncomputationally feasible.\n  Surprisingly, in the absence of pooling layers, the GPs corresponding to CNNs\nwith and without weight sharing are identical. As a consequence, translation\nequivariance, beneficial in finite channel CNNs trained with stochastic\ngradient descent (SGD), is guaranteed to play no role in the Bayesian treatment\nof the infinite channel limit - a qualitative difference between the two\nregimes that is not present in the FCN case. We confirm experimentally, that\nwhile in some scenarios the performance of SGD-trained finite CNNs approaches\nthat of the corresponding GPs as the channel count increases, with careful\ntuning SGD-trained CNNs can significantly outperform their corresponding GPs,\nsuggesting advantages from SGD training compared to fully Bayesian parameter\nestimation. \n\n"}
{"id": "1810.05486", "contents": "Title: Training Deep Neural Network in Limited Precision Abstract: Energy and resource efficient training of DNNs will greatly extend the\napplications of deep learning. However, there are three major obstacles which\nmandate accurate calculation in high precision. In this paper, we tackle two of\nthem related to the loss of gradients during parameter update and\nbackpropagation through a softmax nonlinearity layer in low precision training.\nWe implemented SGD with Kahan summation by employing an additional parameter to\nvirtually extend the bit-width of the parameters for a reliable parameter\nupdate. We also proposed a simple guideline to help select the appropriate\nbit-width for the last FC layer followed by a softmax nonlinearity layer. It\ndetermines the lower bound of the required bit-width based on the class size of\nthe dataset. Extensive experiments on various network architectures and\nbenchmarks verifies the effectiveness of the proposed technique for low\nprecision training. \n\n"}
{"id": "1810.05526", "contents": "Title: Automatic Configuration of Deep Neural Networks with EGO Abstract: Designing the architecture for an artificial neural network is a cumbersome\ntask because of the numerous parameters to configure, including activation\nfunctions, layer types, and hyper-parameters. With the large number of\nparameters for most networks nowadays, it is intractable to find a good\nconfiguration for a given task by hand. In this paper an Efficient Global\nOptimization (EGO) algorithm is adapted to automatically optimize and configure\nconvolutional neural network architectures. A configurable neural network\narchitecture based solely on convolutional layers is proposed for the\noptimization. Without using any knowledge on the target problem and not using\nany data augmentation techniques, it is shown that on several image\nclassification tasks this approach is able to find competitive network\narchitectures in terms of prediction accuracy, compared to the best\nhand-crafted ones in literature. In addition, a very small training budget (200\nevaluations and 10 epochs in training) is spent on each optimized architectures\nin contrast to the usual long training time of hand-crafted networks. Moreover,\ninstead of the standard sequential evaluation in EGO, several candidate\narchitectures are proposed and evaluated in parallel, which saves the execution\noverheads significantly and leads to an efficient automation for deep neural\nnetwork design. \n\n"}
{"id": "1810.06659", "contents": "Title: CAVBench: A Benchmark Suite for Connected and Autonomous Vehicles Abstract: Connected and autonomous vehicles (CAVs) have recently attracted a\nsignificant amount of attention both from researchers and industry. Numerous\nstudies targeting algorithms, software frameworks, and applications on the CAVs\nscenario have emerged. Meanwhile, several pioneer efforts have focused on the\nedge computing system and architecture design for the CAVs scenario and\nprovided various heterogeneous platform prototypes for CAVs. However, a\nstandard and comprehensive application benchmark for CAVs is missing, hindering\nthe study of these emerging computing systems. To address this challenging\nproblem, we present CAVBench, the first benchmark suite for the edge computing\nsystem in the CAVs scenario. CAVBench is comprised of six typical applications\ncovering four dominate CAVs scenarios and takes four datasets as standard\ninput. CAVBench provides quantitative evaluation results via application and\nsystem perspective output metrics. We perform a series of experiments and\nacquire three systemic characteristics of the applications in CAVBench. First,\nthe operation intensity of the applications is polarized, which explains why\nheterogeneous hardware is important for a CAVs computing system. Second, all\napplications in CAVBench consume high memory bandwidth, so the system should be\nequipped with high bandwidth memory or leverage good memory bandwidth\nmanagement to avoid the performance degradation caused by memory bandwidth\ncompetition. Third, some applications have worse data/instruction locality\nbased on the cache miss observation, so the computing system targeting these\napplications should optimize the cache architecture. Last, we use the CAVBench\nto evaluate a typical edge computing platform and present the quantitative and\nqualitative analysis of the benchmarking results. \n\n"}
{"id": "1810.07378", "contents": "Title: Progressive Weight Pruning of Deep Neural Networks using ADMM Abstract: Deep neural networks (DNNs) although achieving human-level performance in\nmany domains, have very large model size that hinders their broader\napplications on edge computing devices. Extensive research work have been\nconducted on DNN model compression or pruning. However, most of the previous\nwork took heuristic approaches. This work proposes a progressive weight pruning\napproach based on ADMM (Alternating Direction Method of Multipliers), a\npowerful technique to deal with non-convex optimization problems with\npotentially combinatorial constraints. Motivated by dynamic programming, the\nproposed method reaches extremely high pruning rate by using partial prunings\nwith moderate pruning rates. Therefore, it resolves the accuracy degradation\nand long convergence time problems when pursuing extremely high pruning ratios.\nIt achieves up to 34 times pruning rate for ImageNet dataset and 167 times\npruning rate for MNIST dataset, significantly higher than those reached by the\nliterature work. Under the same number of epochs, the proposed method also\nachieves faster convergence and higher compression rates. The codes and pruned\nDNN models are released in the link bit.ly/2zxdlss \n\n"}
{"id": "1810.07751", "contents": "Title: Intelligence Beyond the Edge: Inference on Intermittent Embedded Systems Abstract: Energy-harvesting technology provides a promising platform for future IoT\napplications. However, since communication is very expensive in these devices,\napplications will require inference \"beyond the edge\" to avoid wasting precious\nenergy on pointless communication. We show that application performance is\nhighly sensitive to inference accuracy. Unfortunately, accurate inference\nrequires large amounts of computation and memory, and energy-harvesting systems\nare severely resource-constrained. Moreover, energy-harvesting systems operate\nintermittently, suffering frequent power failures that corrupt results and\nimpede forward progress.\n  This paper overcomes these challenges to present the first full-scale\ndemonstration of DNN inference on an energy-harvesting system. We design and\nimplement SONIC, an intermittence-aware software system with specialized\nsupport for DNN inference. SONIC introduces loop continuation, a new technique\nthat dramatically reduces the cost of guaranteeing correct intermittent\nexecution for loop-heavy code like DNN inference. To build a complete system,\nwe further present GENESIS, a tool that automatically compresses networks to\noptimally balance inference accuracy and energy, and TAILS, which exploits SIMD\nhardware available in some microcontrollers to improve energy efficiency. Both\nSONIC & TAILS guarantee correct intermittent execution without any hand-tuning\nor performance loss across different power systems. Across three neural\nnetworks on a commercially available microcontroller, SONIC & TAILS reduce\ninference energy by 6.9x and 12.2x, respectively, over the state-of-the-art. \n\n"}
{"id": "1810.07791", "contents": "Title: MaaSim: A Liveability Simulation for Improving the Quality of Life in\n  Cities Abstract: Urbanism is no longer planned on paper thanks to powerful models and 3D\nsimulation platforms. However, current work is not open to the public and lacks\nan optimisation agent that could help in decision making. This paper describes\nthe creation of an open-source simulation based on an existing Dutch\nliveability score with a built-in AI module. Features are selected using\nfeature engineering and Random Forests. Then, a modified scoring function is\nbuilt based on the former liveability classes. The score is predicted using\nRandom Forest for regression and achieved a recall of 0.83 with 10-fold\ncross-validation. Afterwards, Exploratory Factor Analysis is applied to select\nthe actions present in the model. The resulting indicators are divided into 5\ngroups, and 12 actions are generated. The performance of four optimisation\nalgorithms is compared, namely NSGA-II, PAES, SPEA2 and eps-MOEA, on three\nestablished criteria of quality: cardinality, the spread of the solutions,\nspacing, and the resulting score and number of turns. Although all four\nalgorithms show different strengths, eps-MOEA is selected to be the most\nsuitable for this problem. Ultimately, the simulation incorporates the model\nand the selected AI module in a GUI written in the Kivy framework for Python.\nTests performed on users show positive responses and encourage further\ninitiatives towards joining technology and public applications. \n\n"}
{"id": "1810.08038", "contents": "Title: Toward a Uniform Approach to the Unfolding of Nets Abstract: In this paper we introduce the notion of spread net. Spread nets are (safe)\nPetri nets equipped with vector clocks on places and with ticking functions on\ntransitions, and are such that vector clocks are consistent with the ticking of\ntransitions. Such nets generalize previous families of nets like unfoldings,\nmerged processes and trellis processes, and can thus be used to represent runs\nof a net in a true concurrency semantics through an operation called the\nspreading of a net. By contrast with previous constructions, which may identify\nconflicts, spread nets allow loops in time \n\n"}
{"id": "1810.08092", "contents": "Title: Deconstructing the Blockchain to Approach Physical Limits Abstract: Transaction throughput, confirmation latency and confirmation reliability are\nfundamental performance measures of any blockchain system in addition to its\nsecurity. In a decentralized setting, these measures are limited by two\nunderlying physical network attributes: communication capacity and\nspeed-of-light propagation delay. Existing systems operate far away from these\nphysical limits. In this work we introduce Prism, a new proof-of-work\nblockchain protocol, which can achieve 1) security against up to 50%\nadversarial hashing power; 2) optimal throughput up to the capacity C of the\nnetwork; 3) confirmation latency for honest transactions proportional to the\npropagation delay D, with confirmation error probability exponentially small in\nCD ; 4) eventual total ordering of all transactions. Our approach to the design\nof this protocol is based on deconstructing the blockchain into its basic\nfunctionalities and systematically scaling up these functionalities to approach\ntheir physical limits. \n\n"}
{"id": "1810.08313", "contents": "Title: Adaptive Communication Strategies to Achieve the Best Error-Runtime\n  Trade-off in Local-Update SGD Abstract: Large-scale machine learning training, in particular distributed stochastic\ngradient descent, needs to be robust to inherent system variability such as\nnode straggling and random communication delays. This work considers a\ndistributed training framework where each worker node is allowed to perform\nlocal model updates and the resulting models are averaged periodically. We\nanalyze the true speed of error convergence with respect to wall-clock time\n(instead of the number of iterations), and analyze how it is affected by the\nfrequency of averaging. The main contribution is the design of AdaComm, an\nadaptive communication strategy that starts with infrequent averaging to save\ncommunication delay and improve convergence speed, and then increases the\ncommunication frequency in order to achieve a low error floor. Rigorous\nexperiments on training deep neural networks show that AdaComm can take $3\n\\times$ less time than fully synchronous SGD, and still reach the same final\ntraining loss. \n\n"}
{"id": "1810.08578", "contents": "Title: Leveraging Product as an Activation Function in Deep Networks Abstract: Product unit neural networks (PUNNs) are powerful representational models\nwith a strong theoretical basis, but have proven to be difficult to train with\ngradient-based optimizers. We present windowed product unit neural networks\n(WPUNNs), a simple method of leveraging product as a nonlinearity in a neural\nnetwork. Windowing the product tames the complex gradient surface and enables\nWPUNNs to learn effectively, solving the problems faced by PUNNs. WPUNNs use\nproduct layers between traditional sum layers, capturing the representational\npower of product units and using the product itself as a nonlinearity. We find\nthe result that this method works as well as traditional nonlinearities like\nReLU on the MNIST dataset. We demonstrate that WPUNNs can also generalize gated\nunits in recurrent neural networks, yielding results comparable to LSTM\nnetworks. \n\n"}
{"id": "1810.09027", "contents": "Title: Massively Parallel Approximate Distance Sketches Abstract: Data structures that allow efficient distance estimation (distance oracles,\ndistance sketches, etc.) have been extensively studied, and are particularly\nwell studied in centralized models and classical distributed models such as\nCONGEST. We initiate their study in newer (and arguably more realistic) models\nof distributed computation: the Congested Clique model and the Massively\nParallel Computation (MPC) model. We provide efficient constructions in both of\nthese models, but our core results are for MPC. In MPC we give two main\nresults: an algorithm that constructs stretch/space optimal distance sketches\nbut takes a (small) polynomial number of rounds, and an algorithm that\nconstructs distance sketches with worse stretch but that only takes\npolylogarithmic rounds.\n  Along the way, we show that other useful combinatorial structures can also be\ncomputed in MPC. In particular, one key component we use to construct distance\nsketches are an MPC construction of the hopsets of Elkin and Neiman (2016).\nThis result has additional applications such as the first polylogarithmic time\nalgorithm for constant approximate single-source shortest paths for weighted\ngraphs in the low memory MPC setting. \n\n"}
{"id": "1810.09376", "contents": "Title: Data Motif-based Proxy Benchmarks for Big Data and AI Workloads Abstract: For the architecture community, reasonable simulation time is a strong\nrequirement in addition to performance data accuracy. However, emerging big\ndata and AI workloads are too huge at binary size level and prohibitively\nexpensive to run on cycle-accurate simulators. The concept of data motif, which\nis identified as a class of units of computation performed on initial or\nintermediate data, is the first step towards building proxy benchmark to mimic\nthe real-world big data and AI workloads. However, there is no practical way to\nconstruct a proxy benchmark based on the data motifs to help simulation-based\nresearch. In this paper, we embark on a study to bridge the gap between data\nmotif and a practical proxy benchmark. We propose a data motif-based proxy\nbenchmark generating methodology by means of machine learning method, which\ncombine data motifs with different weights to mimic the big data and AI\nworkloads. Furthermore, we implement various data motifs using light-weight\nstacks and apply the methodology to five real-world workloads to construct a\nsuite of proxy benchmarks, considering the data types, patterns, and\ndistributions. The evaluation results show that our proxy benchmarks shorten\nthe execution time by 100s times on real systems while maintaining the average\nsystem and micro-architecture performance data accuracy above 90%, even\nchanging the input data sets or cluster configurations. Moreover, the generated\nproxy benchmarks reflect consistent performance trends across different\narchitectures. To facilitate the community, we will release the proxy\nbenchmarks on the project homepage http://prof.ict.ac.cn/BigDataBench. \n\n"}
{"id": "1810.09945", "contents": "Title: Analyzing Neuroimaging Data Through Recurrent Deep Learning Models Abstract: The application of deep learning (DL) models to neuroimaging data poses\nseveral challenges, due to the high dimensionality, low sample size and complex\ntemporo-spatial dependency structure of these datasets. Even further, DL models\nact as as black-box models, impeding insight into the association of cognitive\nstate and brain activity. To approach these challenges, we introduce the\nDeepLight framework, which utilizes long short-term memory (LSTM) based DL\nmodels to analyze whole-brain functional Magnetic Resonance Imaging (fMRI)\ndata. To decode a cognitive state (e.g., seeing the image of a house),\nDeepLight separates the fMRI volume into a sequence of axial brain slices,\nwhich is then sequentially processed by an LSTM. To maintain interpretability,\nDeepLight adapts the layer-wise relevance propagation (LRP) technique. Thereby,\ndecomposing its decoding decision into the contributions of the single input\nvoxels to this decision. Importantly, the decomposition is performed on the\nlevel of single fMRI volumes, enabling DeepLight to study the associations\nbetween cognitive state and brain activity on several levels of data\ngranularity, from the level of the group down to the level of single time\npoints. To demonstrate the versatility of DeepLight, we apply it to a large\nfMRI dataset of the Human Connectome Project. We show that DeepLight\noutperforms conventional approaches of uni- and multivariate fMRI analysis in\ndecoding the cognitive states and in identifying the physiologically\nappropriate brain regions associated with these states. We further demonstrate\nDeepLight's ability to study the fine-grained temporo-spatial variability of\nbrain activity over sequences of single fMRI samples. \n\n"}
{"id": "1810.10180", "contents": "Title: Understanding and correcting pathologies in the training of learned\n  optimizers Abstract: Deep learning has shown that learned functions can dramatically outperform\nhand-designed functions on perceptual tasks. Analogously, this suggests that\nlearned optimizers may similarly outperform current hand-designed optimizers,\nespecially for specific problems. However, learned optimizers are notoriously\ndifficult to train and have yet to demonstrate wall-clock speedups over\nhand-designed optimizers, and thus are rarely used in practice. Typically,\nlearned optimizers are trained by truncated backpropagation through an unrolled\noptimization process resulting in gradients that are either strongly biased\n(for short truncations) or have exploding norm (for long truncations). In this\nwork we propose a training scheme which overcomes both of these difficulties,\nby dynamically weighting two unbiased gradient estimators for a variational\nloss on optimizer performance, allowing us to train neural networks to perform\noptimization of a specific task faster than tuned first-order methods. We\ndemonstrate these results on problems where our learned optimizer trains\nconvolutional networks faster in wall-clock time compared to tuned first-order\nmethods and with an improvement in test loss. \n\n"}
{"id": "1810.10687", "contents": "Title: Structure Learning of Deep Networks via DNA Computing Algorithm Abstract: Convolutional Neural Network (CNN) has gained state-of-the-art results in\nmany pattern recognition and computer vision tasks. However, most of the CNN\nstructures are manually designed by experienced researchers. Therefore, auto-\nmatically building high performance networks becomes an important problem. In\nthis paper, we introduce the idea of using DNA computing algorithm to\nautomatically learn high-performance architectures. In DNA computing algorithm,\nwe use short DNA strands to represent layers and long DNA strands to represent\noverall networks. We found that most of the learned models perform similarly,\nand only those performing worse during the first runs of training will perform\nworse finally than others. The indicates that: 1) Using DNA computing algorithm\nto learn deep architectures is feasible; 2) Local minima should not be a\nproblem of deep networks; 3) We can use early stop to kill the models with the\nbad performance just after several runs of training. In our experiments, an\naccuracy 99.73% was obtained on the MNIST data set and an accuracy 95.10% was\nobtained on the CIFAR-10 data set. \n\n"}
{"id": "1810.11441", "contents": "Title: Energy Efficient Adversarial Routing in Shared Channels Abstract: We investigate routing on networks modeled as multiple access channels, when\npackets are injected continually. There is an energy cap understood as a bound\non the number of stations that can be switched on simultaneously. Each packet\nis injected into some station and needs to be delivered to its destination\nstation via the channel. A station has to be switched on in order to receive a\npacket when it is heard on the channel. Each station manages when it is\nswitched on and off by way of a programmable wakeup mechanism, which is\nscheduled by a routing algorithm. Packet injection is governed by adversarial\nmodels that determine upper bounds on injection rates and burstiness. We\ndevelop deterministic distributed routing algorithms and assess their\nperformance in the worst-case sense. One of the algorithms maintains bounded\nqueues for the maximum injection rate 1 subject only to the energy cap 3. This\nenergy cap is provably optimal, in that obtaining the same throughput with the\nenergy cap 2 is impossible. We give algorithms subject to the minimum energy\ncap 2 that have latency polynomial in the total number of stations n for each\nfixed adversary of injection rate less than 1. An algorithm is\nk-energy-oblivious if at most k stations are switched on in a round and for\neach station the rounds when it will be switched on are determined in advance.\nWe give a k-energy-oblivious algorithm that has packet delay O(n) for\nadversaries of injection rates less than (k-1)/(n-1), and show that there is no\nk-energy-oblivious stable algorithm against adversaries with injection rates\ngreater than k/n. We give a k-energy-oblivious algorithm routing directly that\nhas latency O(n^2/k) for adversaries of sufficiently small injection rates that\nare O(k^2/n^2). We show that no k-energy-oblivious algorithm routing directly\ncan be stable against adversaries with injection rates greater than\nk(k-1)/n(n-1). \n\n"}
{"id": "1810.11491", "contents": "Title: Empirical Evaluation of Contextual Policy Search with a Comparison-based\n  Surrogate Model and Active Covariance Matrix Adaptation Abstract: Contextual policy search (CPS) is a class of multi-task reinforcement\nlearning algorithms that is particularly useful for robotic applications. A\nrecent state-of-the-art method is Contextual Covariance Matrix Adaptation\nEvolution Strategies (C-CMA-ES). It is based on the standard black-box\noptimization algorithm CMA-ES. There are two useful extensions of CMA-ES that\nwe will transfer to C-CMA-ES and evaluate empirically: ACM-ES, which uses a\ncomparison-based surrogate model, and aCMA-ES, which uses an active update of\nthe covariance matrix. We will show that improvements with these methods can be\nimpressive in terms of sample-efficiency, although this is not relevant any\nmore for the robotic domain. \n\n"}
{"id": "1810.11714", "contents": "Title: The CoSTAR Block Stacking Dataset: Learning with Workspace Constraints Abstract: A robot can now grasp an object more effectively than ever before, but once\nit has the object what happens next? We show that a mild relaxation of the task\nand workspace constraints implicit in existing object grasping datasets can\ncause neural network based grasping algorithms to fail on even a simple block\nstacking task when executed under more realistic circumstances.\n  To address this, we introduce the JHU CoSTAR Block Stacking Dataset (BSD),\nwhere a robot interacts with 5.1 cm colored blocks to complete an\norder-fulfillment style block stacking task. It contains dynamic scenes and\nreal time-series data in a less constrained environment than comparable\ndatasets. There are nearly 12,000 stacking attempts and over 2 million frames\nof real data. We discuss the ways in which this dataset provides a valuable\nresource for a broad range of other topics of investigation.\n  We find that hand-designed neural networks that work on prior datasets do not\ngeneralize to this task. Thus, to establish a baseline for this dataset, we\ndemonstrate an automated search of neural network based models using a novel\nmultiple-input HyperTree MetaModel, and find a final model which makes\nreasonable 3D pose predictions for grasping and stacking on our dataset.\n  The CoSTAR BSD, code, and instructions are available at\nhttps://sites.google.com/site/costardataset. \n\n"}
{"id": "1810.11787", "contents": "Title: A Hitchhiker's Guide On Distributed Training of Deep Neural Networks Abstract: Deep learning has led to tremendous advancements in the field of Artificial\nIntelligence. One caveat however is the substantial amount of compute needed to\ntrain these deep learning models. Training a benchmark dataset like ImageNet on\na single machine with a modern GPU can take upto a week, distributing training\non multiple machines has been observed to drastically bring this time down.\nRecent work has brought down ImageNet training time to a time as low as 4\nminutes by using a cluster of 2048 GPUs. This paper surveys the various\nalgorithms and techniques used to distribute training and presents the current\nstate of the art for a modern distributed training framework. More\nspecifically, we explore the synchronous and asynchronous variants of\ndistributed Stochastic Gradient Descent, various All Reduce gradient\naggregation strategies and best practices for obtaining higher throughout and\nlower latency over a cluster such as mixed precision training, large batch\ntraining and gradient compression. \n\n"}
{"id": "1810.12297", "contents": "Title: Optimizing Data-Intensive Computations in Existing Libraries with Split\n  Annotations Abstract: Data movement between main memory and the CPU is a major bottleneck in\nparallel data-intensive applications. In response, researchers have proposed\nusing compilers and intermediate representations (IRs) that apply optimizations\nsuch as loop fusion under existing high-level APIs such as NumPy and\nTensorFlow. Even though these techniques generally do not require changes to\nuser applications, they require intrusive changes to the library itself: often,\nlibrary developers must rewrite each function using a new IR. In this paper, we\npropose a new technique called split annotations (SAs) that enables key data\nmovement optimizations over unmodified library functions. SAs only require\ndevelopers to annotate functions and implement an API that specifies how to\npartition data in the library. The annotation and API describe how to enable\ncross-function data pipelining and parallelization, while respecting each\nfunction's correctness constraints. We implement a parallel runtime for SAs in\na system called Mozart. We show that Mozart can accelerate workloads in\nlibraries such as Intel MKL and Pandas by up to 15x, with no library\nmodifications. Mozart also provides performance gains competitive with\nsolutions that require rewriting libraries, and can sometimes outperform these\nsystems by up to 2x by leveraging existing hand-optimized code. \n\n"}
{"id": "1810.12754", "contents": "Title: Recurrent Attention Unit Abstract: Recurrent Neural Network (RNN) has been successfully applied in many sequence\nlearning problems. Such as handwriting recognition, image description, natural\nlanguage processing and video motion analysis. After years of development,\nresearchers have improved the internal structure of the RNN and introduced many\nvariants. Among others, Gated Recurrent Unit (GRU) is one of the most widely\nused RNN model. However, GRU lacks the capability of adaptively paying\nattention to certain regions or locations, so that it may cause information\nredundancy or loss during leaning. In this paper, we propose a RNN model,\ncalled Recurrent Attention Unit (RAU), which seamlessly integrates the\nattention mechanism into the interior of GRU by adding an attention gate. The\nattention gate can enhance GRU's ability to remember long-term memory and help\nmemory cells quickly discard unimportant content. RAU is capable of extracting\ninformation from the sequential data by adaptively selecting a sequence of\nregions or locations and pay more attention to the selected regions during\nlearning. Extensive experiments on image classification, sentiment\nclassification and language modeling show that RAU consistently outperforms GRU\nand other baseline methods. \n\n"}
{"id": "1811.00143", "contents": "Title: Democratizing Production-Scale Distributed Deep Learning Abstract: The interest and demand for training deep neural networks have been\nexperiencing rapid growth, spanning a wide range of applications in both\nacademia and industry. However, training them distributed and at scale remains\ndifficult due to the complex ecosystem of tools and hardware involved. One\nconsequence is that the responsibility of orchestrating these complex\ncomponents is often left to one-off scripts and glue code customized for\nspecific problems. To address these restrictions, we introduce \\emph{Alchemist}\n- an internal service built at Apple from the ground up for \\emph{easy},\n\\emph{fast}, and \\emph{scalable} distributed training. We discuss its design,\nimplementation, and examples of running different flavors of distributed\ntraining. We also present case studies of its internal adoption in the\ndevelopment of autonomous systems, where training times have been reduced by\n10x to keep up with the ever-growing data collection. \n\n"}
{"id": "1811.01235", "contents": "Title: Hardness of computing and approximating predicates and functions with\n  leaderless population protocols Abstract: Population protocols are a distributed computing model appropriate for\ndescribing massive numbers of agents with limited computational power. A\npopulation protocol \"has an initial leader\" if every valid initial\nconfiguration contains a single agent in a special \"leader\" state that helps to\ncoordinate the computation. Although the class of predicates and functions\ncomputable with probability 1 is the same whether or not there is an initial\nleader (semilinear functions and predicates), it is not known whether a leader\nis necessary for fast computation. Efficient population protocols are generally\ndefined as those computing in polylogarithmic in $n$ (parallel) time. We\nconsider leaderless population protocols, regarding the computation finished\nwhen a configuration is reached from which a different output is no longer\nreachable.\n  In this setting we show that a wide class of functions and predicates\ncomputable by population protocols are not efficiently computable (they require\nat least linear time to stabilize on a correct answer), nor are some linear\nfunctions even efficiently approximable. For example, the widely studied\nparity, majority, and equality predicates cannot be computed in sublinear time.\nMoreover, it requires at least linear time for a population protocol even to\napproximate any linear function with a coefficient outside of $\\mathbb{N}$: for\nsufficiently small $\\gamma > 0$, the output of a sublinear time protocol can\nstabilize outside the interval $f(m) (1 \\pm \\gamma)$ on infinitely many inputs\n$m$. We also show that it requires linear time to exactly compute a wide range\nof semilinear functions (e.g., $f(m)=m$ if $m$ is even and $2m$ if $m$ is odd).\n  Finally, we show that with a sufficiently large value of $\\gamma$, a\npopulation protocol can approximate any linear $f$ with nonnegative rational\ncoefficients, within approximation factor $\\gamma$, in $O(\\log n)$ time. \n\n"}
{"id": "1811.01332", "contents": "Title: Validated Asynchronous Byzantine Agreement with Optimal Resilience and\n  Asymptotically Optimal Time and Word Communication Abstract: We provide a new protocol for Validated Asynchronous Byzantine Agreement.\nValidated (multi-valued) Asynchronous Byzantine Agreement is a key building\nblock in constructing Atomic Broadcast and fault-tolerant state machine\nreplication in the asynchronous setting. Our protocol can withstand the optimal\nnumber $f<n/3$ of Byzantine failures and reaches agreement in the\nasymptotically optimal expected $O(1)$ running time. Honest parties in our\nprotocol send only an expected $O(n^2)$ messages where each message contains a\nvalue and a constant number of signatures. Hence our total expected\ncommunication is $O(n^2)$ words. The best previous result of Cachin et al. from\n2001 solves Validated Byzantine Agreement with optimal resilience and $O(1)$\nexpected time but with $O(n^3)$ expected word communication. Our work addresses\nan open question of Cachin et al. from 2001 and improves the expected word\ncommunication from $O(n^3)$ to the asymptotically optimal $O(n^2)$. \n\n"}
{"id": "1811.02144", "contents": "Title: Erasure coding for distributed matrix multiplication for matrices with\n  bounded entries Abstract: Distributed matrix multiplication is widely used in several scientific\ndomains. It is well recognized that computation times on distributed clusters\nare often dominated by the slowest workers (called stragglers). Recent work has\ndemonstrated that straggler mitigation can be viewed as a problem of designing\nerasure codes. For matrices $\\mathbf A$ and $\\mathbf B$, the technique\nessentially maps the computation of $\\mathbf A^T \\mathbf B$ into the\nmultiplication of smaller (coded) submatrices. The stragglers are treated as\nerasures in this process. The computation can be completed as long as a certain\nnumber of workers (called the recovery threshold) complete their assigned\ntasks.\n  We present a novel coding strategy for this problem when the absolute values\nof the matrix entries are sufficiently small. We demonstrate a tradeoff between\nthe assumed absolute value bounds on the matrix entries and the recovery\nthreshold. At one extreme, we are optimal with respect to the recovery\nthreshold and on the other extreme, we match the threshold of prior work.\nExperimental results on cloud-based clusters validate the benefits of our\nmethod. \n\n"}
{"id": "1811.02883", "contents": "Title: SCALE-Sim: Systolic CNN Accelerator Simulator Abstract: Systolic Arrays are one of the most popular compute substrates within Deep\nLearning accelerators today, as they provide extremely high efficiency for\nrunning dense matrix multiplications. However, the research community lacks\ntools to insights on both the design trade-offs and efficient mapping\nstrategies for systolic-array based accelerators. We introduce Systolic CNN\nAccelerator Simulator (SCALE-Sim), which is a configurable systolic array based\ncycle accurate DNN accelerator simulator. SCALE-Sim exposes various\nmicro-architectural features as well as system integration parameters to the\ndesigner to enable comprehensive design space exploration. This is the first\nsystolic-array simulator tuned for running DNNs to the best of our knowledge.\nUsing SCALE-Sim, we conduct a suite of case studies and demonstrate the effect\nof bandwidth, data flow and aspect ratio on the overall runtime and energy of\nDeep Learning kernels across vision, speech, text, and games. We believe that\nthese insights will be highly beneficial to architects and ML practitioners. \n\n"}
{"id": "1811.02884", "contents": "Title: MGSim + MGMark: A Framework for Multi-GPU System Research Abstract: The rapidly growing popularity and scale of data-parallel workloads demand a\ncorresponding increase in raw computational power of GPUs (Graphics Processing\nUnits). As single-GPU systems struggle to satisfy the performance demands,\nmulti-GPU systems have begun to dominate the high-performance computing world.\nThe advent of such systems raises a number of design challenges, including the\nGPU microarchitecture, multi-GPU interconnect fabrics, runtime libraries and\nassociated programming models. The research community currently lacks a\npublically available and comprehensive multi-GPU simulation framework and\nbenchmark suite to evaluate multi-GPU system design solutions.\n  In this work, we present MGSim, a cycle-accurate, extensively validated,\nmulti-GPU simulator, based on AMD's Graphics Core Next 3 (GCN3) instruction set\narchitecture. We complement MGSim with MGMark, a suite of multi-GPU workloads\nthat explores multi-GPU collaborative execution patterns. Our simulator is\nscalable and comes with in-built support for multi-threaded execution to enable\nfast and efficient simulations. In terms of performance accuracy, MGSim differs\n$5.5\\%$ on average when compared against actual GPU hardware. We also achieve a\n$3.5\\times$ and a $2.5\\times$ average speedup in function emulation and\narchitectural simulation with 4 CPU cores, while delivering the same accuracy\nas the serial simulation.\n  We illustrate the novel simulation capabilities provided by our simulator\nthrough a case study exploring programming models based on a unified multi-GPU\nsystem (U-MGPU) and a discrete multi-GPU system (D-MGPU) that both utilize\nunified memory space and cross-GPU memory access. We evaluate the design\nimplications from our case study, suggesting that D-MGPU is an attractive\nprogramming model for future multi-GPU systems. \n\n"}
{"id": "1811.03403", "contents": "Title: ExGate: Externally Controlled Gating for Feature-based Attention in\n  Artificial Neural Networks Abstract: Perceptual capabilities of artificial systems have come a long way since the\nadvent of deep learning. These methods have proven to be effective, however\nthey are not as efficient as their biological counterparts. Visual attention is\na set of mechanisms that are employed in biological visual systems to ease\ncomputational load by only processing pertinent parts of the stimuli. This\npaper addresses the implementation of top-down, feature-based attention in an\nartificial neural network by use of externally controlled neuron gating. Our\nresults showed a 5% increase in classification accuracy on the CIFAR-10 dataset\nversus a non-gated version, while adding very few parameters. Our gated model\nalso produces more reasonable errors in predictions by drastically reducing\nprediction of classes that belong to a different category to the true class. \n\n"}
{"id": "1811.03619", "contents": "Title: Pipe-SGD: A Decentralized Pipelined SGD Framework for Distributed Deep\n  Net Training Abstract: Distributed training of deep nets is an important technique to address some\nof the present day computing challenges like memory consumption and\ncomputational demands. Classical distributed approaches, synchronous or\nasynchronous, are based on the parameter server architecture, i.e., worker\nnodes compute gradients which are communicated to the parameter server while\nupdated parameters are returned. Recently, distributed training with AllReduce\noperations gained popularity as well. While many of those operations seem\nappealing, little is reported about wall-clock training time improvements. In\nthis paper, we carefully analyze the AllReduce based setup, propose timing\nmodels which include network latency, bandwidth, cluster size and compute time,\nand demonstrate that a pipelined training with a width of two combines the best\nof both synchronous and asynchronous training. Specifically, for a setup\nconsisting of a four-node GPU cluster we show wall-clock time training\nimprovements of up to 5.4x compared to conventional approaches. \n\n"}
{"id": "1811.03962", "contents": "Title: A Convergence Theory for Deep Learning via Over-Parameterization Abstract: Deep neural networks (DNNs) have demonstrated dominating performance in many\nfields; since AlexNet, networks used in practice are going wider and deeper. On\nthe theoretical side, a long line of works has been focusing on training neural\nnetworks with one hidden layer. The theory of multi-layer networks remains\nlargely unsettled.\n  In this work, we prove why stochastic gradient descent (SGD) can find\n$\\textit{global minima}$ on the training objective of DNNs in\n$\\textit{polynomial time}$. We only make two assumptions: the inputs are\nnon-degenerate and the network is over-parameterized. The latter means the\nnetwork width is sufficiently large: $\\textit{polynomial}$ in $L$, the number\nof layers and in $n$, the number of samples.\n  Our key technique is to derive that, in a sufficiently large neighborhood of\nthe random initialization, the optimization landscape is almost-convex and\nsemi-smooth even with ReLU activations. This implies an equivalence between\nover-parameterized neural networks and neural tangent kernel (NTK) in the\nfinite (and polynomial) width setting.\n  As concrete examples, starting from randomly initialized weights, we prove\nthat SGD can attain 100% training accuracy in classification tasks, or minimize\nregression loss in linear convergence speed, with running time polynomial in\n$n,L$. Our theory applies to the widely-used but non-smooth ReLU activation,\nand to any smooth and possibly non-convex loss functions. In terms of network\narchitectures, our theory at least applies to fully-connected neural networks,\nconvolutional neural networks (CNN), and residual neural networks (ResNet). \n\n"}
{"id": "1811.03968", "contents": "Title: Collaboratively Learning the Best Option on Graphs, Using Bounded Local\n  Memory Abstract: We consider multi-armed bandit problems in social groups wherein each\nindividual has bounded memory and shares the common goal of learning the best\narm/option. We say an individual learns the best option if eventually (as $t\\to\n\\infty$) it pulls only the arm with the highest expected reward. While this\ngoal is provably impossible for an isolated individual due to bounded memory,\nwe show that, in social groups, this goal can be achieved easily with the aid\nof social persuasion (i.e., communication) as long as the communication\nnetworks/graphs satisfy some mild conditions. To deal with the interplay\nbetween the randomness in the rewards and in the social interaction, we employ\nthe {\\em mean-field approximation} method. Considering the possibility that the\nindividuals in the networks may not be exchangeable when the communication\nnetworks are not cliques, we go beyond the classic mean-field techniques and\napply a refined version of mean-field approximation:\n  (1) Using coupling we show that, if the communication graph is connected and\nis either regular or has doubly-stochastic degree-weighted adjacency matrix,\nwith probability $\\to 1$ as the social group size $N \\to \\infty$, every\nindividual in the social group learns the best option.\n  (2) If the minimum degree of the graph diverges as $N \\to \\infty$, over an\narbitrary but given finite time horizon, the sample paths describing the\nopinion evolutions of the individuals are asymptotically independent. In\naddition, the proportions of the population with different opinions converge to\nthe unique solution of a system of ODEs. In the solution of the obtained ODEs,\nthe proportion of the population holding the correct opinion converges to $1$\nexponentially fast in time.\n  Notably, our results hold even if the communication graphs are highly sparse. \n\n"}
{"id": "1811.04624", "contents": "Title: Importance Weighted Evolution Strategies Abstract: Evolution Strategies (ES) emerged as a scalable alternative to popular\nReinforcement Learning (RL) techniques, providing an almost perfect speedup\nwhen distributed across hundreds of CPU cores thanks to a reduced communication\noverhead. Despite providing large improvements in wall-clock time, ES is data\ninefficient when compared to competing RL methods. One of the main causes of\nsuch inefficiency is the collection of large batches of experience, which are\ndiscarded after each policy update. In this work, we study how to perform more\nthan one update per batch of experience by means of Importance Sampling while\npreserving the scalability of the original method. The proposed method,\nImportance Weighted Evolution Strategies (IW-ES), shows promising results and\nis a first step towards designing efficient ES algorithms. \n\n"}
{"id": "1811.05592", "contents": "Title: Controllability, Multiplexing, and Transfer Learning in Networks using\n  Evolutionary Learning Abstract: Networks are fundamental building blocks for representing data, and\ncomputations. Remarkable progress in learning in structurally defined (shallow\nor deep) networks has recently been achieved. Here we introduce evolutionary\nexploratory search and learning method of topologically flexible networks under\nthe constraint of producing elementary computational steady-state input-output\noperations.\n  Our results include; (1) the identification of networks, over four orders of\nmagnitude, implementing computation of steady-state input-output functions,\nsuch as a band-pass filter, a threshold function, and an inverse band-pass\nfunction. Next, (2) the learned networks are technically controllable as only a\nsmall number of driver nodes are required to move the system to a new state.\nFurthermore, we find that the fraction of required driver nodes is constant\nduring evolutionary learning, suggesting a stable system design. (3), our\nframework allows multiplexing of different computations using the same network.\nFor example, using a binary representation of the inputs, the network can\nreadily compute three different input-output functions. Finally, (4) the\nproposed evolutionary learning demonstrates transfer learning. If the system\nlearns one function A, then learning B requires on average less number of steps\nas compared to learning B from tabula rasa.\n  We conclude that the constrained evolutionary learning produces large robust\ncontrollable circuits, capable of multiplexing and transfer learning. Our study\nsuggests that network-based computations of steady-state functions,\nrepresenting either cellular modules of cell-to-cell communication networks or\ninternal molecular circuits communicating within a cell, could be a powerful\nmodel for biologically inspired computing. This complements conceptualizations\nsuch as attractor based models, or reservoir computing. \n\n"}
{"id": "1811.06751", "contents": "Title: All roads lead to Rome: Many ways to double spend your cryptocurrency Abstract: In 2008, Satoshi Nakamoto proposed an electronic cash system (bitcoin) that\nis completely realized by peer-to-peer technology. The core value of this\nscheme is that it proposes a solution based on Proof-of Work, so that the cash\nsystem can run in a peer-to-peer environment and be able to prevent\ndouble-spend attacks. Bitcoin has been developed for ten years, and since then\ncountless digital currencies have been created. But the discussion of\ndouble-spend attacks seems to still concentrate on 51% Attacks. In fact, our\nresearch has found that there are many other way to achieve double-spend\nattacks. In this paper, by introducing a number of double-spend attack\nvulnerabilities that we have found in EOS, NEO and other large blockchain\nplatforms, we summarized various reasons for causing double-spend attacks, and\npropose an efficient mitigation measure against them. \n\n"}
{"id": "1811.08834", "contents": "Title: A Survey on Spark Ecosystem for Big Data Processing Abstract: With the explosive increase of big data in industry and academic fields, it\nis necessary to apply large-scale data processing systems to analysis Big Data.\nArguably, Spark is state of the art in large-scale data computing systems\nnowadays, due to its good properties including generality, fault tolerance,\nhigh performance of in-memory data processing, and scalability. Spark adopts a\nflexible Resident Distributed Dataset (RDD) programming model with a set of\nprovided transformation and action operators whose operating functions can be\ncustomized by users according to their applications. It is originally\npositioned as a fast and general data processing system. A large body of\nresearch efforts have been made to make it more efficient (faster) and general\nby considering various circumstances since its introduction. In this survey, we\naim to have a thorough review of various kinds of optimization techniques on\nthe generality and performance improvement of Spark. We introduce Spark\nprogramming model and computing system, discuss the pros and cons of Spark, and\nhave an investigation and classification of various solving techniques in the\nliterature. Moreover, we also introduce various data management and processing\nsystems, machine learning algorithms and applications supported by Spark.\nFinally, we make a discussion on the open issues and challenges for large-scale\nin-memory data processing with Spark. \n\n"}
{"id": "1811.09047", "contents": "Title: Fog Computing Architecture: Survey and Challenges Abstract: Emerging technologies that generate a huge amount of data such as the\nInternet of Things (IoT) services need latency aware computing platforms to\nsupport time-critical applications. Due to the on-demand services and\nscalability features of cloud computing, Big Data application processing is\ndone in the cloud infrastructure. Managing Big Data applications exclusively in\nthe cloud is not an efficient solution for latency-sensitive applications\nrelated to smart transportation systems, healthcare solutions, emergency\nresponse systems and content delivery applications. Thus, the Fog computing\nparadigm that allows applications to perform computing operations in-between\nthe cloud and the end devices has emerged. In Fog architecture, IoT devices and\nsensors are connected to the Fog devices which are located in close proximity\nto the users and it is also responsible for intermediate computation and\nstorage. Most computations will be done on the edge by eliminating full\ndependencies on the cloud resources. In this chapter, we investigate and survey\nFog computing architectures which have been proposed over the past few years.\nMoreover, we study the requirements of IoT applications and platforms, and the\nlimitations faced by cloud systems when executing IoT applications. Finally, we\nreview current research works that particularly focus on Big Data application\nexecution on Fog and address several open challenges as well as future research\ndirections. \n\n"}
{"id": "1811.09143", "contents": "Title: Verifying C11 Programs Operationally Abstract: This paper develops an operational semantics for a release-acquire fragment\nof the C11 memory model with relaxed accesses. We show that the semantics is\nboth sound and complete with respect to the axiomatic model. The semantics\nrelies on a per-thread notion of observability, which allows one to reason\nabout a weak memory C11 program in program order. On top of this, we develop a\nproof calculus for invariant-based reasoning, which we use to verify the\nrelease-acquire version of Peterson's mutual exclusion algorithm. \n\n"}
{"id": "1811.09725", "contents": "Title: Interpretable Convolutional Filters with SincNet Abstract: Deep learning is currently playing a crucial role toward higher levels of\nartificial intelligence. This paradigm allows neural networks to learn complex\nand abstract representations, that are progressively obtained by combining\nsimpler ones. Nevertheless, the internal \"black-box\" representations\nautomatically discovered by current neural architectures often suffer from a\nlack of interpretability, making of primary interest the study of explainable\nmachine learning techniques. This paper summarizes our recent efforts to\ndevelop a more interpretable neural model for directly processing speech from\nthe raw waveform. In particular, we propose SincNet, a novel Convolutional\nNeural Network (CNN) that encourages the first layer to discover more\nmeaningful filters by exploiting parametrized sinc functions. In contrast to\nstandard CNNs, which learn all the elements of each filter, only low and high\ncutoff frequencies of band-pass filters are directly learned from data. This\ninductive bias offers a very compact way to derive a customized filter-bank\nfront-end, that only depends on some parameters with a clear physical meaning.\nOur experiments, conducted on both speaker and speech recognition, show that\nthe proposed architecture converges faster, performs better, and is more\ninterpretable than standard CNNs. \n\n"}
{"id": "1811.09786", "contents": "Title: Recurrently Controlled Recurrent Networks Abstract: Recurrent neural networks (RNNs) such as long short-term memory and gated\nrecurrent units are pivotal building blocks across a broad spectrum of sequence\nmodeling problems. This paper proposes a recurrently controlled recurrent\nnetwork (RCRN) for expressive and powerful sequence encoding. More concretely,\nthe key idea behind our approach is to learn the recurrent gating functions\nusing recurrent networks. Our architecture is split into two components - a\ncontroller cell and a listener cell whereby the recurrent controller actively\ninfluences the compositionality of the listener cell. We conduct extensive\nexperiments on a myriad of tasks in the NLP domain such as sentiment analysis\n(SST, IMDb, Amazon reviews, etc.), question classification (TREC), entailment\nclassification (SNLI, SciTail), answer selection (WikiQA, TrecQA) and reading\ncomprehension (NarrativeQA). Across all 26 datasets, our results demonstrate\nthat RCRN not only consistently outperforms BiLSTMs but also stacked BiLSTMs,\nsuggesting that our controller architecture might be a suitable replacement for\nthe widely adopted stacked architecture. \n\n"}
{"id": "1811.11307", "contents": "Title: Improved Speech Enhancement with the Wave-U-Net Abstract: We study the use of the Wave-U-Net architecture for speech enhancement, a\nmodel introduced by Stoller et al for the separation of music vocals and\naccompaniment. This end-to-end learning method for audio source separation\noperates directly in the time domain, permitting the integrated modelling of\nphase information and being able to take large temporal contexts into account.\nOur experiments show that the proposed method improves several metrics, namely\nPESQ, CSIG, CBAK, COVL and SSNR, over the state-of-the-art with respect to the\nspeech enhancement task on the Voice Bank corpus (VCTK) dataset. We find that a\nreduced number of hidden layers is sufficient for speech enhancement in\ncomparison to the original system designed for singing voice separation in\nmusic. We see this initial result as an encouraging signal to further explore\nspeech enhancement in the time-domain, both as an end in itself and as a\npre-processing step to speech recognition systems. \n\n"}
{"id": "1811.12628", "contents": "Title: OHIE: Blockchain Scaling Made Simple Abstract: Many blockchain consensus protocols have been proposed recently to scale the\nthroughput of a blockchain with available bandwidth. However, these protocols\nare becoming increasingly complex, making it more and more difficult to produce\nproofs of their security guarantees. We propose a novel permissionless\nblockchain protocol OHIE which explicitly aims for simplicity. OHIE composes as\nmany parallel instances of Bitcoin's original (and simple) backbone protocol as\nneeded to achieve excellent throughput. We formally prove the safety and\nliveness properties of OHIE. We demonstrate its performance with a prototype\nimplementation and large-scale experiments with up to 50,000 nodes. In our\nexperiments, OHIE achieves linear scaling with available bandwidth, providing\nabout 4-10 Mbps transaction throughput (under 8-20 Mbps per-node available\nbandwidth configurations) and at least about 20x better decentralization over\nprior works. \n\n"}
{"id": "1812.00182", "contents": "Title: NTX: An Energy-efficient Streaming Accelerator for Floating-point\n  Generalized Reduction Workloads in 22nm FD-SOI Abstract: Specialized coprocessors for Multiply-Accumulate (MAC) intensive workloads\nsuch as Deep Learning are becoming widespread in SoC platforms, from GPUs to\nmobile SoCs. In this paper we revisit NTX (an efficient accelerator developed\nfor training Deep Neural Networks at scale) as a generalized MAC and reduction\nstreaming engine. The architecture consists of a set of 32 bit floating-point\nstreaming co-processors that are loosely coupled to a RISC-V core in charge of\norchestrating data movement and computation. Post-layout results of a recent\nsilicon implementation in 22 nm FD-SOI technology show the accelerator's\ncapability to deliver up to 20 Gflop/s at 1.25 GHz and 168 mW. Based on these\nresults we show that a version of NTX scaled down to 14 nm can achieve a 3x\nenergy efficiency improvement over contemporary GPUs at 10.4x less silicon\narea, and a compute performance of 1.4 Tflop/s for training large\nstate-of-the-art networks with full floating-point precision. An extended\nevaluation of MAC-intensive kernels shows that NTX can consistently achieve up\nto 87% of its peak performance across general reduction workloads beyond\nmachine learning. Its modular architecture enables deployment at different\nscales ranging from high-performance GPU-class to low-power embedded scenarios. \n\n"}
{"id": "1812.00493", "contents": "Title: Towards a More Practice-Aware Runtime Analysis of Evolutionary\n  Algorithms Abstract: Theory of evolutionary computation (EC) aims at providing mathematically\nfounded statements about the performance of evolutionary algorithms (EAs). The\npredominant topic in this research domain is runtime analysis, which studies\nthe time it takes a given EA to solve a given optimization problem. Runtime\nanalysis has witnessed significant advances in the last couple of years,\nallowing us to compute precise runtime estimates for several EAs and several\nproblems. Runtime analysis is, however (and unfortunately!), often judged by\npractitioners to be of little relevance for real applications of EAs. Several\nreasons for this claim exist. We address two of them in this present work:\n  (1) EA implementations often differ from their vanilla pseudocode\ndescription, which, in turn, typically form the basis for runtime analysis. To\nclose the resulting gap between empirically observed and theoretically derived\nperformance estimates, we therefore suggest to take this discrepancy into\naccount in the mathematical analysis and to adjust, for example, the cost\nassigned to the evaluation of search points that equal one of their direct\nparents (provided that this is easy to verify as is the case in almost all\nstandard EAs).\n  (2) Most runtime analysis results make statements about the expected time to\nreach an optimal solution (and possibly the distribution of this optimization\ntime) only, thus explicitly or implicitly neglecting the importance of\nunderstanding how the function values evolve over time. We suggest to extend\nruntime statements to runtime profiles, covering the expected time needed to\nreach points of intermediate fitness values.\n  As a direct consequence, we obtain a result showing that the greedy (2+1) GA\nof Sudholt [GECCO 2012] outperforms any unary unbiased black-box algorithm on\nOneMax. \n\n"}
{"id": "1812.01665", "contents": "Title: Auto-tuning TensorFlow Threading Model for CPU Backend Abstract: TensorFlow is a popular deep learning framework used by data scientists to\nsolve a wide-range of machine learning and deep learning problems such as image\nclassification and speech recognition. It also operates at a large scale and in\nheterogeneous environments --- it allows users to train neural network models\nor deploy them for inference using GPUs, CPUs and deep learning specific\ncustom-designed hardware such as TPUs. Even though TensorFlow supports a\nvariety of optimized backends, realizing the best performance using a backend\nmay require additional efforts. For instance, getting the best performance from\na CPU backend requires careful tuning of its threading model. Unfortunately,\nthe best tuning approach used today is manual, tedious, time-consuming, and,\nmore importantly, may not guarantee the best performance.\n  In this paper, we develop an automatic approach, called TensorTuner, to\nsearch for optimal parameter settings of TensorFlow's threading model for CPU\nbackends. We evaluate TensorTuner on both Eigen and Intel's MKL CPU backends\nusing a set of neural networks from TensorFlow's benchmarking suite. Our\nevaluation results demonstrate that the parameter settings found by TensorTuner\nproduce 2% to 123% performance improvement for the Eigen CPU backend and 1.5%\nto 28% performance improvement for the MKL CPU backend over the performance\nobtained using their best-known parameter settings. This highlights the fact\nthat the default parameter settings in Eigen CPU backend are not the ideal\nsettings; and even for a carefully hand-tuned MKL backend, the settings may be\nsub-optimal. Our evaluations also revealed that TensorTuner is efficient at\nfinding the optimal settings --- it is able to converge to the optimal settings\nquickly by pruning more than 90% of the parameter search space. \n\n"}
{"id": "1812.01895", "contents": "Title: Computational Graph Approach for Detection of Composite Human Activities Abstract: Existing work in human activity detection classifies physical activities\nusing a single fixed-length subset of a sensor signal. However, temporally\nconsecutive subsets of a sensor signal are not utilized. This is not optimal\nfor classifying physical activities (composite activities) that are composed of\na temporal series of simpler activities (atomic activities). A sport consists\nof physical activities combined in a fashion unique to that sport. The\nconstituent physical activities and the sport are not fundamentally different.\nWe propose a computational graph architecture for human activity detection\nbased on the readings of a triaxial accelerometer. The resulting model learns\n1) a representation of the atomic activities of a sport and 2) to classify\nphysical activities as compositions of the atomic activities. The proposed\nmodel, alongside with a set of baseline models, was tested for a simultaneous\nclassification of eight physical activities (walking, nordic walking, running,\nsoccer, rowing, bicycling, exercise bicycling and lying down). The proposed\nmodel obtained an overall mean accuracy of 77.91% (population) and 95.28%\n(personalized). The corresponding accuracies of the best baseline model were\n73.52% and 90.03%. However, without combining consecutive atomic activities,\nthe corresponding accuracies of the proposed model were 71.52% and 91.22%. The\nresults show that our proposed model is accurate, outperforms the baseline\nmodels and learns to combine simple activities into complex activities.\nComposite activities can be classified as combinations of atomic activities.\nOur proposed architecture is a basis for accurate models in human activity\ndetection. \n\n"}
{"id": "1812.02407", "contents": "Title: Elastic Gossip: Distributing Neural Network Training Using Gossip-like\n  Protocols Abstract: Distributing Neural Network training is of particular interest for several\nreasons including scaling using computing clusters, training at data sources\nsuch as IOT devices and edge servers, utilizing underutilized resources across\nheterogeneous environments, and so on. Most contemporary approaches primarily\naddress scaling using computing clusters and require high network bandwidth and\nfrequent communication. This thesis presents an overview of standard approaches\nto distribute training and proposes a novel technique involving\npairwise-communication using Gossip-like protocols, called Elastic Gossip. This\napproach builds upon an existing technique known as Elastic Averaging SGD\n(EASGD), and is similar to another technique called Gossiping SGD which also\nuses Gossip-like protocols. Elastic Gossip is empirically evaluated against\nGossiping SGD using the MNIST digit recognition and CIFAR-10 classification\ntasks, using commonly used Neural Network architectures spanning Multi-Layer\nPerceptrons (MLPs) and Convolutional Neural Networks (CNNs). It is found that\nElastic Gossip, Gossiping SGD, and All-reduce SGD perform quite comparably,\neven though the latter entails a substantially higher communication cost. While\nElastic Gossip performs better than Gossiping SGD in these experiments, it is\npossible that a more thorough search over hyper-parameter space, specific to a\ngiven application, may yield configurations of Gossiping SGD that work better\nthan Elastic Gossip. \n\n"}
{"id": "1812.03183", "contents": "Title: A hybrid machine-learning algorithm for designing quantum experiments Abstract: We introduce a hybrid machine-learning algorithm for designing quantum optics\nexperiments that produce specific quantum states. Our algorithm successfully\nfound experimental schemes to produce all 5 states we asked it to, including\nSchr\\\"odinger cat states and cubic phase states, all to a fidelity of over\n$96\\%$. Here we specifically focus on designing realistic experiments, and\nhence all of the algorithm's designs only contain experimental elements that\nare available with current technology. The core of our algorithm is a genetic\nalgorithm that searches for optimal arrangements of the experimental elements,\nbut to speed up the initial search we incorporate a neural network that\nclassifies quantum states. The latter is of independent interest, as it quickly\nlearned to accurately classify quantum states given their photon-number\ndistributions. \n\n"}
{"id": "1812.03770", "contents": "Title: Functional Design of Computation Graph Abstract: Representing the control flow of a computer program as a computation graph\ncan bring many benefits in a broad variety of domains where performance is\ncritical. This technique is a core component of most major numerical libraries\n(TensorFlow, PyTorch, Theano, MXNet,...) and is successfully used to speed up\nand optimise many computationally-intensive tasks. However, different design\nchoices in each of these libraries lead to noticeable differences in efficiency\nand in the way an end user writes efficient code. In this report, we detail the\nimplementation and features of the computation graph support in OCaml's\nnumerical library Owl, a recent entry in the world of scientific computing. \n\n"}
{"id": "1812.04070", "contents": "Title: SIMD-X: Programming and Processing of Graph Algorithms on GPUs Abstract: With high computation power and memory bandwidth, graphics processing units\n(GPUs) lend themselves to accelerate data-intensive analytics, especially when\nsuch applications fit the single instruction multiple data (SIMD) model.\nHowever, graph algorithms such as breadth-first search and k-core, often fail\nto take full advantage of GPUs, due to irregularity in memory access and\ncontrol flow. To address this challenge, we have developed SIMD-X, for\nprogramming and processing of single instruction multiple, complex, data on\nGPUs. Specifically, the new Active-Compute-Combine (ACC) model not only\nprovides ease of programming to programmers, but more importantly creates\nopportunities for system-level optimizations. To this end, SIMD-X utilizes\njust-in-time task management which filters out inactive vertices at runtime and\nintelligently maps various tasks to different amount of GPU cores in pursuit of\nworkload balancing. In addition, SIMD-X leverages push-pull based kernel fusion\nthat, with the help of a new deadlock-free global barrier, reduces a large\nnumber of computation kernels to very few. Using SIMD-X, a user can program a\ngraph algorithm in tens of lines of code, while achieving 3?, 6?, 24?, 3?\nspeedup over Gunrock, Galois, CuSha, and Ligra, respectively. \n\n"}
{"id": "1812.04380", "contents": "Title: DRONE: a Distributed Subgraph-Centric Framework for Processing Large\n  Scale Power-law Graphs Abstract: Nowadays, in the big data era, social networks, graph databases, knowledge\ngraphs, electronic commerce etc. demand efficient and scalable capability to\nprocess an ever increasing volume of graph-structured data. To meet the\nchallenge, two mainstream distributed programming models, vertex-centric (VC)\nand subgraph-centric (SC) were proposed. Compared to the VC model, the SC model\nconverges faster with less communication overhead on well-partitioned graphs,\nand is easy to program due to the \"think like a graph\" philosophy. The edge-cut\nmethod is considered as a natural choice of subgraph-centric model for graph\npartitioning, and has been adopted by Giraph++, Blogel and GRAPE. However, the\nedge-cut method causes significant performance bottleneck for processing large\nscale power-law graphs. Thus, the SC model is less competitive in practice. In\nthis paper, we present an innovative distributed graph computing framework,\nDRONE (Distributed gRaph cOmputiNg Engine). It combines the subgraph-centric\nmodel and the vertex-cut graph partitioning strategy. Experiments show that\nDRONE outperforms the state-of-art distributed graph computing engines on\nreal-world graphs and synthetic power-law graphs. DRONE is capable of scaling\nup to process one-trillion-edge synthetic power-law graphs, which is orders of\nmagnitude larger than previously reported by existing SC-based frameworks. \n\n"}
{"id": "1812.04431", "contents": "Title: Distributed Weight Balancing in Directed Topologies Abstract: This doctoral thesis concerns novel distributed algorithms for weight\nbalancing over directed (communication) topologies. A directed topology\n(digraph) with nonnegative (or positive) weights assigned on each edge is\nweight-balanced if, for each node, the sum of the weights of in-coming edges\nequals the sum of the weights of out-going edges. The novel algorithms\nintroduced in this thesis can facilitate the development of strategies for\ngenerating weight balanced digraphs, in a distributed manner, and find numerous\napplications in coordination and control of multi-component systems. In the\nfirst part of this thesis, we introduce a novel distributed algorithm that\noperates over a static topology and solves the weight balancing problem when\nthe weights are restricted to be nonnegative integers. In the second part of\nthe thesis, we present a novel distributed algorithm which solves the integer\nweight balancing problem in the presence of arbitrary (time-varying and\ninhomogeneous) delays that might affect the transmission at a particular link\nat a particular time. In the third part of this thesis, we present a novel\ndistributed algorithm for obtaining admissible and balanced integer weights for\nthe case when there are lower and upper weight constraints on the communication\nlinks. In the fourth part of this thesis we present a novel distributed\nalgorithm which solves the integer weight balancing problem under lower and\nupper weight constraints over the communication links for the case where\narbitrary (time-varying and inhomogeneous) time delays and possible packet\ndrops affect the transmission at a particular link at a particular time. \n\n"}
{"id": "1812.05974", "contents": "Title: Distributed Submodular Minimization over Networks: a Greedy Column\n  Generation Approach Abstract: Submodular optimization is a special class of combinatorial optimization\narising in several machine learning problems, but also in cooperative control\nof complex systems. In this paper, we consider agents in an asynchronous,\nunreliable and time-varying directed network that aim at cooperatively solving\nsubmodular minimization problems in a fully distributed way. The challenge is\nthat the (submodular) objective set-function is only partially known by agents,\nthat is, each one is able to evaluate the function only for subsets including\nitself. We propose a distributed algorithm based on a proper linear programming\nreformulation of the combinatorial problem. Our algorithm builds on a column\ngeneration approach in which each agent maintains a local candidate basis and\nlocally generates columns with a suitable greedy inner routine. A key\ninteresting feature of the proposed algorithm is that the pricing problem,\nwhich involves an exponential number of constraints, is solved by the agents\nthrough a polynomial time greedy algorithm. We prove that the proposed\ndistributed algorithm converges in finite time to an optimal solution of the\nsubmodular minimization problem and we corroborate the theoretical results by\nperforming numerical computations on instances of the $s$--$t$ minimum graph\ncut problem. \n\n"}
{"id": "1812.06156", "contents": "Title: Trollslayer: Crowdsourcing and Characterization of Abusive Birds in\n  Twitter Abstract: As of today, abuse is a pressing issue to participants and administrators of\nOnline Social Networks (OSN). Abuse in Twitter can spawn from arguments\ngenerated for influencing outcomes of a political election, the use of bots to\nautomatically spread misinformation, and generally speaking, activities that\ndeny, disrupt, degrade or deceive other participants and, or the network. Given\nthe difficulty in finding and accessing a large enough sample of abuse ground\ntruth from the Twitter platform, we built and deployed a custom crawler that we\nuse to judiciously collect a new dataset from the Twitter platform with the aim\nof characterizing the nature of abusive users, a.k.a abusive birds, in the\nwild. We provide a comprehensive set of features based on users' attributes, as\nwell as social-graph metadata. The former includes metadata about the account\nitself, while the latter is computed from the social graph among the sender and\nthe receiver of each message. Attribute-based features are useful to\ncharacterize user's accounts in OSN, while graph-based features can reveal the\ndynamics of information dissemination across the network. In particular, we\nderive the Jaccard index as a key feature to reveal the benign or malicious\nnature of directed messages in Twitter. To the best of our knowledge, we are\nthe first to propose such a similarity metric to characterize abuse in Twitter. \n\n"}
{"id": "1812.06492", "contents": "Title: Performance Evaluation of Big Data Processing Strategies for\n  Neuroimaging Abstract: Neuroimaging datasets are rapidly growing in size as a result of advancements\nin image acquisition methods, open-science and data sharing. However, the\nadoption of Big Data processing strategies by neuroimaging processing engines\nremains limited. Here, we evaluate three Big Data processing strategies\n(in-memory computing, data locality and lazy evaluation) on typical\nneuroimaging use cases, represented by the BigBrain dataset. We contrast these\nvarious strategies using Apache Spark and Nipype as our representative Big Data\nand neuroimaging processing engines, on Dell EMC's Top-500 cluster. Big Data\nthresholds were modelled by comparing the data-write rate of the application to\nthe filesystem bandwidth and number of concurrent processes. This model\nacknowledges the fact that page caching provided by the Linux kernel is\ncritical to the performance of Big Data applications. Results show that\nin-memory computing alone speeds-up executions by a factor of up to 1.6,\nwhereas when combined with data locality, this factor reaches 5.3. Lazy\nevaluation strategies were found to increase the likelihood of cache hits,\nfurther improving processing time. Such important speed-up values are likely to\nbe observed on typical image processing operations performed on images of size\nlarger than 75GB. A ballpark speculation from our model showed that in-memory\ncomputing alone will not speed-up current functional MRI analyses unless\ncoupled with data locality and processing around 280 subjects concurrently.\nFurthermore, we observe that emulating in-memory computing using in-memory file\nsystems (tmpfs) does not reach the performance of an in-memory engine,\npresumably due to swapping to disk and the lack of data cleanup. We conclude\nthat Big Data processing strategies are worth developing for neuroimaging\napplications. \n\n"}
{"id": "1812.07636", "contents": "Title: Distributed Algorithms for Internet-of-Things-enabled Prosumer Markets:\n  A Control Theoretic Perspective Abstract: Internet-of-Things (IoT) enables the development of sharing economy\napplications. In many sharing economy scenarios, agents both produce as well as\nconsume a resource; we call them prosumers. A community of prosumers agrees to\nsell excess resource to another community in a prosumer market. In this\nchapter, we propose a control theoretic approach to regulate the number of\nprosumers in a prosumer community, where each prosumer has a cost function that\nis coupled through its time-averaged production and consumption of the\nresource. Furthermore, each prosumer runs its distributed algorithm and takes\nonly binary decisions in a probabilistic way, whether to produce one unit of\nthe resource or not and to consume one unit of the resource or not. In the\nproposed approach, prosumers do not explicitly exchange information with each\nother due to privacy reasons, but little exchange of information is required\nfor feedback signals, broadcast by a central agency. In the proposed approach,\nprosumers achieve the optimal values asymptotically. Furthermore, the proposed\napproach is suitable to implement in an IoT context with minimal demands on\ninfrastructure. We describe two use cases; community-based car sharing and\ncollaborative energy storage for prosumer markets. We also present simulation\nresults to check the efficacy of the algorithms. \n\n"}
{"id": "1812.08446", "contents": "Title: Atomic Appends: Selling Cars and Coordinating Armies with Multiple\n  Distributed Ledgers Abstract: The various applications using Distributed Ledger Technologies (DLT) or\nblockchains, have led to the introduction of a new `marketplace' where multiple\ntypes of digital assets may be exchanged. As each blockchain is designed to\nsupport specific types of assets and transactions, and no blockchain will\nprevail, the need to perform interblockchain transactions is already pressing.\n  In this work we examine the fundamental problem of interoperable and\ninterconnected blockchains. In particular, we begin by introducing the\nMulti-Distributed Ledger Objects (MDLO), which is the result of aggregating\nmultiple Distributed Ledger Objects -- DLO (a DLO is a formalization of the\nblockchain) and that supports append and get operations of records (e.g.,\ntransactions) in them from multiple clients concurrently. Next, we define the\nAtomicAppends problem, which emerges when the exchange of digital assets\nbetween multiple clients may involve appending records in more than one DLO.\nSpecifically, AtomicAppend requires that either all records will be appended on\nthe involved DLOs or none. We examine the solvability of this problem assuming\nrational and risk-averse clients that may fail by crashing, and under different\nclient utility and append models, timing models, and client failure scenarios.\nWe show that for some cases the existence of an intermediary is necessary for\nthe problem solution. We propose the implementation of such intermediary over a\nspecialized blockchain, we term Smart DLO (SDLO), and we show how this can be\nused to solve the AtomicAppends problem even in an asynchronous, client\ncompetitive environment, where all the clients may crash. \n\n"}
{"id": "1812.09113", "contents": "Title: Introducing Neuromodulation in Deep Neural Networks to Learn Adaptive\n  Behaviours Abstract: Animals excel at adapting their intentions, attention, and actions to the\nenvironment, making them remarkably efficient at interacting with a rich,\nunpredictable and ever-changing external world, a property that intelligent\nmachines currently lack. Such an adaptation property relies heavily on cellular\nneuromodulation, the biological mechanism that dynamically controls intrinsic\nproperties of neurons and their response to external stimuli in a\ncontext-dependent manner. In this paper, we take inspiration from cellular\nneuromodulation to construct a new deep neural network architecture that is\nspecifically designed to learn adaptive behaviours. The network adaptation\ncapabilities are tested on navigation benchmarks in a meta-reinforcement\nlearning context and compared with state-of-the-art approaches. Results show\nthat neuromodulation is capable of adapting an agent to different tasks and\nthat neuromodulation-based approaches provide a promising way of improving\nadaptation of artificial systems. \n\n"}
{"id": "1812.09141", "contents": "Title: Speeding-up the Verification Phase of Set Similarity Joins in the GPGPU\n  paradigm Abstract: We investigate the problem of exact set similarity joins using a co-process\nCPU-GPU scheme. The state-of-the-art CPU solutions split the wok in two main\nphases. First, filtering and index building takes place to reduce the candidate\nsets to be compared as much as possible; then the pairs are compared to verify\nwhether they should become part of the result. We investigate in-depth\nsolutions for transferring the second, so-called verification phase, to the GPU\naddressing several challenges regarding the data serialization and layout, the\nthread management and the techniques to compare sets of tokens. Using real\ndatasets, we provide concrete experimental proofs that our solutions have\nreached their maximum potential, since they totally overlap verification with\nCPU tasks, and manage to yield significant speed-ups, up to 2.6X in our cases. \n\n"}
{"id": "1812.09537", "contents": "Title: Bioinformatics Computational Cluster Batch Task Profiling with Machine\n  Learning for Failure Prediction Abstract: Motivation: Traditional computational cluster schedulers are based on user\ninputs and run time needs request for memory and CPU, not IO. Heavily IO bound\ntask run times, like ones seen in many big data and bioinformatics problems,\nare dependent on the IO subsystems scheduling and are problematic for cluster\nresource scheduling. The problematic rescheduling of IO intensive and errant\ntasks is a lost resource. Understanding the conditions in both successful and\nfailed tasks and differentiating them could provide knowledge to enhancing\ncluster scheduling and intelligent resource optimization.\n  Results: We analyze a production computational cluster contributing 6.7\nthousand CPU hours to research over two years. Through this analysis we develop\na machine learning task profiling agent for clusters that attempts to predict\nfailures between identically provision requested tasks. \n\n"}
{"id": "1812.10240", "contents": "Title: Studying the Plasticity in Deep Convolutional Neural Networks using\n  Random Pruning Abstract: Recently there has been a lot of work on pruning filters from deep\nconvolutional neural networks (CNNs) with the intention of reducing\ncomputations.The key idea is to rank the filters based on a certain criterion\n(say, l1-norm) and retain only the top ranked filters. Once the low scoring\nfilters are pruned away the remainder of the network is fine tuned and is shown\nto give performance comparable to the original unpruned network. In this work,\nwe report experiments which suggest that the comparable performance of the\npruned network is not due to the specific criterion chosen but due to the\ninherent plasticity of deep neural networks which allows them to recover from\nthe loss of pruned filters once the rest of the filters are fine-tuned.\nSpecifically we show counter-intuitive results wherein by randomly pruning\n25-50% filters from deep CNNs we are able to obtain the same performance as\nobtained by using state-of-the-art pruning methods. We empirically validate our\nclaims by doing an exhaustive evaluation with VGG-16 and ResNet-50. We also\nevaluate a real world scenario where a CNN trained on all 1000 ImageNet classes\nneeds to be tested on only a small set of classes at test time (say, only\nanimals). We create a new benchmark dataset from ImageNet to evaluate such\nclass specific pruning and show that even here a random pruning strategy gives\nclose to state-of-the-art performance. Unlike existing approaches which mainly\nfocus on the task of image classification, in this work we also report results\non object detection and image segmentation. We show that using a simple random\npruning strategy we can achieve significant speed up in object detection (74%\nimprovement in fps) while retaining the same accuracy as that of the original\nFaster RCNN model. Similarly we show that the performance of a pruned\nSegmentation Network (SegNet) is actually very similar to that of the original\nunpruned SegNet. \n\n"}
{"id": "1812.10624", "contents": "Title: Stanza: Layer Separation for Distributed Training in Deep Learning Abstract: The parameter server architecture is prevalently used for distributed deep\nlearning. Each worker machine in a parameter server system trains the complete\nmodel, which leads to a hefty amount of network data transfer between workers\nand servers. We empirically observe that the data transfer has a non-negligible\nimpact on training time.\n  To tackle the problem, we design a new distributed training system called\nStanza. Stanza exploits the fact that in many models such as convolution neural\nnetworks, most data exchange is attributed to the fully connected layers, while\nmost computation is carried out in convolutional layers. Thus, we propose layer\nseparation in distributed training: the majority of the nodes just train the\nconvolutional layers, and the rest train the fully connected layers only.\nGradients and parameters of the fully connected layers no longer need to be\nexchanged across the cluster, thereby substantially reducing the data transfer\nvolume. We implement Stanza on PyTorch and evaluate its performance on Azure\nand EC2. Results show that Stanza accelerates training significantly over\ncurrent parameter server systems: on EC2 instances with Tesla V100 GPU and 10Gb\nbandwidth for example, Stanza is 1.34x--13.9x faster for common deep learning\nmodels. \n\n"}
{"id": "1812.10668", "contents": "Title: An efficient cloud scheduler design supporting preemptible instances Abstract: Maximizing resource utilization by performing an efficient resource\nprovisioning is a key factor for any cloud provider: commercial actors can\nmaximize their revenues, whereas scientific and non-commercial providers can\nmaximize their infrastructure utilization. Traditionally, batch systems have\nallowed data centers to fill their resources as much as possible by using\nbackfilling and similar techniques. However, in an IaaS cloud, where virtual\nmachines are supposed to live indefinitely, or at least as long as the user is\nable to pay for them, these policies are not easily implementable. In this work\nwe present a new scheduling algorithm for IaaS providers that is able to\nsupport preemptible instances, that can be stopped by higher priority requests\nwithout introducing large modifications in the current cloud schedulers. This\nscheduler enables the implementation of new cloud usage and payment models that\nallow more efficient usage of the resources and potential new revenue sources\nfor commercial providers. We also study the correctness and the performace\noverhead of the proposed scheduler agains existing solutions. \n\n"}
{"id": "1812.10844", "contents": "Title: AT2: Asynchronous Trustworthy Transfers Abstract: Many blockchain-based protocols, such as Bitcoin, implement a decentralized\nasset transfer (or exchange) system. As clearly stated in the original paper by\nNakamoto, the crux of this problem lies in prohibiting any participant from\nengaging in double-spending. There seems to be a common belief that consensus\nis necessary for solving the double-spending problem. Indeed, whether it is for\na permissionless or a permissioned environment, the typical solution uses\nconsensus to build a totally ordered ledger of submitted transfers. In this\npaper we show that this common belief is false: consensus is not needed to\nimplement of a decentralized asset transfer system. We do so by introducing AT2\n(Asynchronous Trustworthy Transfers), a class of consensusless algorithms. To\nshow formally that consensus is unnecessary for asset transfers, we consider\nthis problem first in the shared-memory context. We introduce AT2$_{SM}$, a\nwait-free algorithm that asynchronously implements asset transfer in the\nread-write shared-memory model. In other words, we show that the consensus\nnumber of an asset-transfer object is one. In the message passing model with\nByzantine faults, we introduce a generic asynchronous algorithm called\nAT2$_{MP}$ and discuss two instantiations of this solution. First, AT2$_{D}$\nensures deterministic guarantees and consequently targets a small scale\ndeployment (tens to hundreds of nodes), typically for a permissioned\nenvironment. Second, AT2$_{P}$ provides probabilistic guarantees and scales\nwell to a very large system size (tens of thousands of nodes), ensuring\nlogarithmic latency and communication complexity. Instead of consensus, we\nconstruct AT2$_{D}$ and AT2$_{P}$ on top of a broadcast primitive with causal\nordering guarantees offering deterministic and probabilistic properties,\nrespectively. \n\n"}
{"id": "1812.11337", "contents": "Title: Quantized Guided Pruning for Efficient Hardware Implementations of\n  Convolutional Neural Networks Abstract: Convolutional Neural Networks (CNNs) are state-of-the-art in numerous\ncomputer vision tasks such as object classification and detection. However, the\nlarge amount of parameters they contain leads to a high computational\ncomplexity and strongly limits their usability in budget-constrained devices\nsuch as embedded devices. In this paper, we propose a combination of a new\npruning technique and a quantization scheme that effectively reduce the\ncomplexity and memory usage of convolutional layers of CNNs, and replace the\ncomplex convolutional operation by a low-cost multiplexer. We perform\nexperiments on the CIFAR10, CIFAR100 and SVHN and show that the proposed method\nachieves almost state-of-the-art accuracy, while drastically reducing the\ncomputational and memory footprints. We also propose an efficient hardware\narchitecture to accelerate CNN operations. The proposed hardware architecture\nis a pipeline and accommodates multiple layers working at the same time to\nspeed up the inference process. \n\n"}
{"id": "1812.11391", "contents": "Title: SLIM LSTMs Abstract: Long Short-Term Memory (LSTM) Recurrent Neural networks (RNNs) rely on gating\nsignals, each driven by a function of a weighted sum of at least 3 components:\n(i) one of an adaptive weight matrix multiplied by the incoming external input\nvector sequence, (ii) one adaptive weight matrix multiplied by the previous\nmemory/state vector, and (iii) one adaptive bias vector. In effect, they\naugment the simple Recurrent Neural Networks (sRNNs) structure with the\naddition of a \"memory cell\" and the incorporation of at most 3 gating signals.\n  The standard LSTM structure and components encompass redundancy and overly\nincreased parameterization. In this paper, we systemically introduce variants\nof the LSTM RNNs, referred to as SLIM LSTMs. These variants express\naggressively reduced parameterizations to achieve computational saving and/or\nspeedup in (training) performance---while necessarily retaining (validation\naccuracy) performance comparable to the standard LSTM RNN. \n\n"}
{"id": "1812.11937", "contents": "Title: Two \"correlation games\" for a nonlinear network with Hebbian excitatory\n  neurons and anti-Hebbian inhibitory neurons Abstract: A companion paper introduces a nonlinear network with Hebbian excitatory (E)\nneurons that are reciprocally coupled with anti-Hebbian inhibitory (I) neurons\nand also receive Hebbian feedforward excitation from sensory (S) afferents. The\npresent paper derives the network from two normative principles that are\nmathematically equivalent but conceptually different. The first principle\nformulates unsupervised learning as a constrained optimization problem:\nmaximization of S-E correlations subject to a copositivity constraint on E-E\ncorrelations. A combination of Legendre and Lagrangian duality yields a\nzero-sum continuous game between excitatory and inhibitory connections that is\nsolved by the neural network. The second principle defines a zero-sum game\nbetween E and I cells. E cells want to maximize S-E correlations and minimize\nE-I correlations, while I cells want to maximize I-E correlations and minimize\npower. The conflict between I and E objectives effectively forces the E cells\nto decorrelate from each other, although only incompletely. Legendre duality\nyields the neural network. \n\n"}
{"id": "1901.00041", "contents": "Title: Dynamic Space-Time Scheduling for GPU Inference Abstract: Serving deep neural networks in latency critical interactive settings often\nrequires GPU acceleration. However, the small batch sizes typical in online\ninference results in poor GPU utilization, a potential performance gap which\nGPU resource sharing can address. In this paper, we explore several techniques\nto leverage both temporal and spatial multiplexing to improve GPU utilization\nfor deep learning inference workloads. We evaluate the performance trade-offs\nof each approach with respect to resource-efficiency, latency predictability,\nand isolation when compared with conventional batched inference. Our\nexperimental analysis suggests up to a 5x potential for improved utilization\nthrough the exploration of more advanced spatial and temporal multiplexing\nstrategies. Our preliminary prototype of a dynamic space-time scheduler\ndemonstrates a 3.23x floating-point throughput increase over space-only\nmultiplexing and a 7.73x increase over time-only multiplexing for convolutions,\nwhile also providing better isolation and latency predictability. \n\n"}
{"id": "1901.00109", "contents": "Title: Morphological Network: How Far Can We Go with Morphological Neurons? Abstract: Morphological neurons, that is morphological operators such as dilation and\nerosion with learnable structuring elements, have intrigued researchers for\nquite some time because of the power these operators bring to the table despite\ntheir simplicity. These operators are known to be powerful nonlinear tools, but\nfor a given problem coming up with a sequence of operations and their\nstructuring element is a non-trivial task. So, the existing works have mainly\nfocused on this part of the problem without delving deep into their\napplicability as generic operators. A few works have tried to utilize\nmorphological neurons as a part of classification (and regression) networks\nwhen the input is a feature vector. However, these methods mainly focus on a\nspecific problem, without going into generic theoretical analysis. In this\nwork, we have theoretically analyzed morphological neurons and have shown that\nthese are far more powerful than previously anticipated. Our proposed\nmorphological block, containing dilation and erosion followed by their linear\ncombination, represents a sum of hinge functions. Existing works show that\nhinge functions perform quite well in classification and regression problems.\nTwo morphological blocks can even approximate any continuous function. However,\nto facilitate the theoretical analysis that we have done in this paper, we have\nrestricted ourselves to the 1D version of the operators, where the structuring\nelement operates on the whole input. Experimental evaluations also indicate the\neffectiveness of networks built with morphological neurons, over similarly\nstructured neural networks. \n\n"}
{"id": "1901.00121", "contents": "Title: FPGA-based Accelerators of Deep Learning Networks for Learning and\n  Classification: A Review Abstract: Due to recent advances in digital technologies, and availability of credible\ndata, an area of artificial intelligence, deep learning, has emerged, and has\ndemonstrated its ability and effectiveness in solving complex learning problems\nnot possible before. In particular, convolution neural networks (CNNs) have\ndemonstrated their effectiveness in image detection and recognition\napplications. However, they require intensive CPU operations and memory\nbandwidth that make general CPUs fail to achieve desired performance levels.\nConsequently, hardware accelerators that use application specific integrated\ncircuits (ASICs), field programmable gate arrays (FPGAs), and graphic\nprocessing units (GPUs) have been employed to improve the throughput of CNNs.\nMore precisely, FPGAs have been recently adopted for accelerating the\nimplementation of deep learning networks due to their ability to maximize\nparallelism as well as due to their energy efficiency. In this paper, we review\nrecent existing techniques for accelerating deep learning networks on FPGAs. We\nhighlight the key features employed by the various techniques for improving the\nacceleration performance. In addition, we provide recommendations for enhancing\nthe utilization of FPGAs for CNNs acceleration. The techniques investigated in\nthis paper represent the recent trends in FPGA-based accelerators of deep\nlearning networks. Thus, this review is expected to direct the future advances\non efficient hardware accelerators and to be useful for deep learning\nresearchers. \n\n"}
{"id": "1901.00279", "contents": "Title: Elimination of All Bad Local Minima in Deep Learning Abstract: In this paper, we theoretically prove that adding one special neuron per\noutput unit eliminates all suboptimal local minima of any deep neural network,\nfor multi-class classification, binary classification, and regression with an\narbitrary loss function, under practical assumptions. At every local minimum of\nany deep neural network with these added neurons, the set of parameters of the\noriginal neural network (without added neurons) is guaranteed to be a global\nminimum of the original neural network. The effects of the added neurons are\nproven to automatically vanish at every local minimum. Moreover, we provide a\nnovel theoretical characterization of a failure mode of eliminating suboptimal\nlocal minima via an additional theorem and several examples. This paper also\nintroduces a novel proof technique based on the perturbable gradient basis\n(PGB) necessary condition of local minima, which provides new insight into the\nelimination of local minima and is applicable to analyze various models and\ntransformations of objective functions beyond the elimination of local minima. \n\n"}
{"id": "1901.00844", "contents": "Title: Machine Learning at the Wireless Edge: Distributed Stochastic Gradient\n  Descent Over-the-Air Abstract: We study federated machine learning (ML) at the wireless edge, where power-\nand bandwidth-limited wireless devices with local datasets carry out\ndistributed stochastic gradient descent (DSGD) with the help of a remote\nparameter server (PS). Standard approaches assume separate computation and\ncommunication, where local gradient estimates are compressed and transmitted to\nthe PS over orthogonal links. Following this digital approach, we introduce\nD-DSGD, in which the wireless devices employ gradient quantization and error\naccumulation, and transmit their gradient estimates to the PS over a multiple\naccess channel (MAC). We then introduce a novel analog scheme, called A-DSGD,\nwhich exploits the additive nature of the wireless MAC for over-the-air\ngradient computation, and provide convergence analysis for this approach. In\nA-DSGD, the devices first sparsify their gradient estimates, and then project\nthem to a lower dimensional space imposed by the available channel bandwidth.\nThese projections are sent directly over the MAC without employing any digital\ncode. Numerical results show that A-DSGD converges faster than D-DSGD thanks to\nits more efficient use of the limited bandwidth and the natural alignment of\nthe gradient estimates over the channel. The improvement is particularly\ncompelling at low power and low bandwidth regimes. We also illustrate for a\nclassification problem that, A-DSGD is more robust to bias in data distribution\nacross devices, while D-DSGD significantly outperforms other digital schemes in\nthe literature. We also observe that both D-DSGD and A-DSGD perform better by\nincreasing the number of devices (while keeping the total dataset size\nconstant), showing their ability in harnessing the computation power of edge\ndevices. \n\n"}
{"id": "1901.00943", "contents": "Title: Self-supervised Learning of Image Embedding for Continuous Control Abstract: Operating directly from raw high dimensional sensory inputs like images is\nstill a challenge for robotic control. Recently, Reinforcement Learning methods\nhave been proposed to solve specific tasks end-to-end, from pixels to torques.\nHowever, these approaches assume the access to a specified reward which may\nrequire specialized instrumentation of the environment. Furthermore, the\nobtained policy and representations tend to be task specific and may not\ntransfer well. In this work we investigate completely self-supervised learning\nof a general image embedding and control primitives, based on finding the\nshortest time to reach any state. We also introduce a new structure for the\nstate-action value function that builds a connection between model-free and\nmodel-based methods, and improves the performance of the learning algorithm. We\nexperimentally demonstrate these findings in three simulated robotic tasks. \n\n"}
{"id": "1901.01347", "contents": "Title: Learning to Remember More with Less Memorization Abstract: Memory-augmented neural networks consisting of a neural controller and an\nexternal memory have shown potentials in long-term sequential learning. Current\nRAM-like memory models maintain memory accessing every timesteps, thus they do\nnot effectively leverage the short-term memory held in the controller. We\nhypothesize that this scheme of writing is suboptimal in memory utilization and\nintroduces redundant computation. To validate our hypothesis, we derive a\ntheoretical bound on the amount of information stored in a RAM-like system and\nformulate an optimization problem that maximizes the bound. The proposed\nsolution dubbed Uniform Writing is proved to be optimal under the assumption of\nequal timestep contributions. To relax this assumption, we introduce\nmodifications to the original solution, resulting in a solution termed Cached\nUniform Writing. This method aims to balance between maximizing memorization\nand forgetting via overwriting mechanisms. Through an extensive set of\nexperiments, we empirically demonstrate the advantages of our solutions over\nother recurrent architectures, claiming the state-of-the-arts in various\nsequential modeling tasks. \n\n"}
{"id": "1901.01376", "contents": "Title: An Empirical Study of Speculative Concurrency in Ethereum Smart\n  Contracts Abstract: We use historical data to estimate the potential benefit of speculative\ntechniques for executing Ethereum smart contracts in parallel. We replay\ntransaction traces of sampled blocks from the Ethereum blockchain over time,\nusing a simple speculative execution engine. In this engine, miners attempt to\nexecute all transactions in a block in parallel, rolling back those that cause\ndata conflicts. Aborted transactions are then executed sequentially. Validators\nexecute the same schedule as miners.\n  We find that our speculative technique yields estimated speed-ups starting at\nabout 8-fold in 2016, declining to about 2-fold at the end of 2017, where\nspeed-up is measured using either gas costs or instruction counts. We also\nobserve that a small set of contracts are responsible for many data conflicts\nresulting from speculative concurrent execution. \n\n"}
{"id": "1901.01462", "contents": "Title: Rethinking the Artificial Neural Networks: A Mesh of Subnets with a\n  Central Mechanism for Storing and Predicting the Data Abstract: The Artificial Neural Networks (ANNs) have been originally designed to\nfunction like a biological neural network, but does an ANN really work in the\nsame way as a biological neural network? As we know, the human brain holds\ninformation in its memory cells, so if the ANNs use the same model as our\nbrains, they should store datasets in a similar manner. The most popular type\nof ANN architecture is based on a layered structure of neurons, whereas a human\nbrain has trillions of complex interconnections of neurons continuously\nestablishing new connections, updating existing ones, and removing the\nirrelevant connections across different parts of the brain. In this paper, we\npropose a novel approach to building ANNs which are truly inspired by the\nbiological network containing a mesh of subnets controlled by a central\nmechanism. A subnet is a network of neurons that hold the dataset values. We\nattempt to address the following fundamental questions: (1) What is the\narchitecture of the ANN model? Whether the layered architecture is the most\nappropriate choice? (2) Whether a neuron is a process or a memory cell? (3)\nWhat is the best way of interconnecting neurons and what weight-assignment\nmechanism should be used? (4) How to incorporate prior knowledge, bias, and\ngeneralizations for features extraction and prediction? Our proposed ANN\narchitecture leverages the accuracy on textual data and our experimental\nfindings confirm the effectiveness of our model. We also collaborate with the\nconstruction of the ANN model for storing and processing the images. \n\n"}
{"id": "1901.01908", "contents": "Title: Koji: Automating pipelines with mixed-semantics data sources Abstract: We propose a new result-oriented semantic for defining data processing\nworkflows that manipulate data in different semantic forms (files or services)\nin a unified manner. This approach enables users to define workflows for a vast\nvariety of reproducible data-processing tasks in a simple declarative manner\nwhich focuses on application-level results, while automating all control-plane\nconsiderations (like failure recovery without loss of progress and computation\nreuse) behind the scenes.\n  The uniform treatment of files and services as data enables easy integration\nwith existing data sources (e.g. data acquisition APIs) and sinks of data (e.g.\ndatabase services). Whereas the focus on containers as transformations enables\nreuse of existing data-processing systems.\n  We describe a declarative configuration mechanism, which can be viewed as an\nintermediate representation (IR) of reproducible data processing pipelines in\nthe same spirit as, for instance, TensorFlow\\cite{tensorflow} and\nONNX\\cite{onnx} utilize IRs for defining tensor-processing pipelines. \n\n"}
{"id": "1901.02755", "contents": "Title: Consensus Mechanism Design based on Structured Directed Acyclic Graphs Abstract: We introduce a structure for the directed acyclic graph (DAG) and a mechanism\ndesign based on that structure so that peers can reach consensus at large scale\nbased on proof of work (PoW). We also design a mempool transaction assignment\nmethod based on the DAG structure to render negligible the probability that a\ntransaction being processed by more than one miners. The result is a\nsignificant scale-up of the capacity without sacrificing security and\ndecentralization. \n\n"}
{"id": "1901.03000", "contents": "Title: Capacity of Distributed Storage Systems with Clusters and Separate Nodes Abstract: In distributed storage systems (DSSs), the optimal tradeoff between node\nstorage and repair bandwidth is an important issue for designing distributed\ncoding strategies to ensure large scale data reliability. The capacity of DSSs\nis obtained as a function of node storage and repair bandwidth parameters,\ncharacterizing the tradeoff. There are lots of works on DSSs with clusters\n(racks) where the repair bandwidths from intra-cluster and cross-cluster are\ndifferentiated. However, separate nodes are also prevalent in the realistic\nDSSs, but the works on DSSs with clusters and separate nodes (CSN-DSSs) are\ninsufficient. In this paper, we formulate the capacity of CSN-DSSs with one\nseparate node for the first time where the bandwidth to repair a separate node\nis of cross-cluster. Consequently, the optimal tradeoff between node storage\nand repair bandwidth are derived and compared with cluster DSSs. A regenerating\ncode instance is constructed based on the tradeoff. Furthermore, the influence\nof adding a separate node is analyzed and formulated theoretically. We prove\nthat when each cluster contains R nodes and any k nodes suffice to recover the\noriginal file (MDS property), adding an extra separate node will keep the\ncapacity if R|k, and reduce the capacity otherwise. \n\n"}
{"id": "1901.03086", "contents": "Title: Adaptive Event Dispatching in Serverless Computing Infrastructures Abstract: Serverless computing is an emerging Cloud service model. It is currently\ngaining momentum as the next step in the evolution of hosted computing from\ncapacitated machine virtualisation and microservices towards utility computing.\nThe term \"serverless\" has become a synonym for the entirely\nresource-transparent deployment model of cloud-based event-driven distributed\napplications. This work investigates how adaptive event dispatching can improve\nserverless platform resource efficiency and contributes a novel approach that\nallows for better scaling and fitting of the platform's resource consumption to\nactual demand. \n\n"}
{"id": "1901.03279", "contents": "Title: FireLedger: A High Throughput Blockchain Consensus Protocol Abstract: Blockchains are distributed secure ledgers to which transactions are issued\ncontinuously and each block of transactions is tightly coupled to its\npredecessors. Permissioned blockchains place special emphasis on transactions\nthroughput. In this paper we present FireLedger, which leverages the iterative\nnature of blockchains in order to improve their throughput in optimistic\nexecution scenarios. FireLedger trades latency for throughput in the sense that\nin FireLedger the last f + 1 blocks of each node's blockchain are considered\ntentative, i.e., they may be rescinded in case one of the last f + 1 blocks\nproposers was Byzantine. Yet, when optimistic assumptions are met, a new block\nis decided in each communication step, which consists of a proposer that sends\nonly its proposal and all other participants are sending a single bit each. Our\nperformance study demonstrates that in a single Amazon data-center, FireLedger\nrunning on 10 mid-range Amazon nodes obtains a throughput of up to 160K\ntransactions per second for (typical Bitcoin size) 512 bytes transactions. In a\n10 nodes Amazon geo-distributed setting with 512 bytes transactions, FireLedger\nobtains a throughput of 30K tps. Moreover, on higher end Amazon machines,\nFireLedger obtains $20%-600%$ better throughput than state of the art protocols\nlike HotStuff and BFT-SMaRt, depending on the exact configuration. \n\n"}
{"id": "1901.04169", "contents": "Title: Towards Testing of Deep Learning Systems with Training Set Reduction Abstract: Testing the implementation of deep learning systems and their training\nroutines is crucial to maintain a reliable code base. Modern software\ndevelopment employs processes, such as Continuous Integration, in which changes\nto the software are frequently integrated and tested. However, testing the\ntraining routines requires running them and fully training a deep learning\nmodel can be resource-intensive, when using the full data set. Using only a\nsubset of the training data can improve test run time, but can also reduce its\neffectiveness. We evaluate different ways for training set reduction and their\nability to mimic the characteristics of model training with the original full\ndata set. Our results underline the usefulness of training set reduction,\nespecially in resource-constrained environments. \n\n"}
{"id": "1901.04709", "contents": "Title: Self-Stabilization Through the Lens of Game Theory Abstract: In 1974 E.W. Dijkstra introduced the seminal concept of self-stabilization\nthat turned out to be one of the main approaches to fault-tolerant computing.\nWe show here how his three solutions can be formalized and reasoned about using\nthe concepts of game theory. We also determine the precise number of steps\nneeded to reach self-stabilization in his first solution. \n\n"}
{"id": "1901.04988", "contents": "Title: A Survey of FPGA Based Deep Learning Accelerators: Challenges and\n  Opportunities Abstract: With the rapid development of in-depth learning, neural network and deep\nlearning algorithms have been widely used in various fields, e.g., image, video\nand voice processing. However, the neural network model is getting larger and\nlarger, which is expressed in the calculation of model parameters. Although a\nwealth of existing efforts on GPU platforms currently used by researchers for\nimproving computing performance, dedicated hardware solutions are essential and\nemerging to provide advantages over pure software solutions. In this paper, we\nsystematically investigate the neural network accelerator based on FPGA.\nSpecifically, we respectively review the accelerators designed for specific\nproblems, specific algorithms, algorithm features, and general templates. We\nalso compared the design and implementation of the accelerator based on FPGA\nunder different devices and network models and compared it with the versions of\nCPU and GPU. Finally, we present to discuss the advantages and disadvantages of\naccelerators on FPGA platforms and to further explore the opportunities for\nfuture research. \n\n"}
{"id": "1901.05049", "contents": "Title: Bonseyes AI Pipeline -- bringing AI to you. End-to-end integration of\n  data, algorithms and deployment tools Abstract: Next generation of embedded Information and Communication Technology (ICT)\nsystems are collaborative systems able to perform autonomous tasks. The\nremarkable expansion of the embedded ICT market, together with the rise and\nbreakthroughs of Artificial Intelligence (AI), have put the focus on the Edge\nas it stands as one of the keys for the next technological revolution: the\nseamless integration of AI in our daily life. However, training and deployment\nof custom AI solutions on embedded devices require a fine-grained integration\nof data, algorithms, and tools to achieve high accuracy. Such integration\nrequires a high level of expertise that becomes a real bottleneck for small and\nmedium enterprises wanting to deploy AI solutions on the Edge which,\nultimately, slows down the adoption of AI on daily-life applications. In this\nwork, we present a modular AI pipeline as an integrating framework to bring\ndata, algorithms, and deployment tools together. By removing the integration\nbarriers and lowering the required expertise, we can interconnect the different\nstages of tools and provide a modular end-to-end development of AI products for\nembedded devices. Our AI pipeline consists of four modular main steps: i) data\ningestion, ii) model training, iii) deployment optimization and, iv) the IoT\nhub integration. To show the effectiveness of our pipeline, we provide examples\nof different AI applications during each of the steps. Besides, we integrate\nour deployment framework, LPDNN, into the AI pipeline and present its\nlightweight architecture and deployment capabilities for embedded devices.\nFinally, we demonstrate the results of the AI pipeline by showing the\ndeployment of several AI applications such as keyword spotting, image\nclassification and object detection on a set of well-known embedded platforms,\nwhere LPDNN consistently outperforms all other popular deployment frameworks. \n\n"}
{"id": "1901.06401", "contents": "Title: Slim LSTM networks: LSTM_6 and LSTM_C6 Abstract: We have shown previously that our parameter-reduced variants of Long\nShort-Term Memory (LSTM) Recurrent Neural Networks (RNN) are comparable in\nperformance to the standard LSTM RNN on the MNIST dataset. In this study, we\nshow that this is also the case for two diverse benchmark datasets, namely, the\nreview sentiment IMDB and the 20 Newsgroup datasets. Specifically, we focus on\ntwo of the simplest variants, namely LSTM_6 (i.e., standard LSTM with three\nconstant fixed gates) and LSTM_C6 (i.e., LSTM_6 with further reduced cell body\ninput block). We demonstrate that these two aggressively reduced-parameter\nvariants are competitive with the standard LSTM when hyper-parameters, e.g.,\nlearning parameter, number of hidden units and gate constants are set properly.\nThese architectures enable speeding up training computations and hence, these\nnetworks would be more suitable for online training and inference onto portable\ndevices with relatively limited computational resources. \n\n"}
{"id": "1901.06834", "contents": "Title: Perception-in-the-Loop Adversarial Examples Abstract: We present a scalable, black box, perception-in-the-loop technique to find\nadversarial examples for deep neural network classifiers. Black box means that\nour procedure only has input-output access to the classifier, and not to the\ninternal structure, parameters, or intermediate confidence values.\nPerception-in-the-loop means that the notion of proximity between inputs can be\ndirectly queried from human participants rather than an arbitrarily chosen\nmetric. Our technique is based on covariance matrix adaptation evolution\nstrategy (CMA-ES), a black box optimization approach. CMA-ES explores the\nsearch space iteratively in a black box manner, by generating populations of\ncandidates according to a distribution, choosing the best candidates according\nto a cost function, and updating the posterior distribution to favor the best\ncandidates. We run CMA-ES using human participants to provide the fitness\nfunction, using the insight that the choice of best candidates in CMA-ES can be\nnaturally modeled as a perception task: pick the top $k$ inputs perceptually\nclosest to a fixed input. We empirically demonstrate that finding adversarial\nexamples is feasible using small populations and few iterations. We compare the\nperformance of CMA-ES on the MNIST benchmark with other black-box approaches\nusing $L_p$ norms as a cost function, and show that it performs favorably both\nin terms of success in finding adversarial examples and in minimizing the\ndistance between the original and the adversarial input. In experiments on the\nMNIST, CIFAR10, and GTSRB benchmarks, we demonstrate that CMA-ES can find\nperceptually similar adversarial inputs with a small number of iterations and\nsmall population sizes when using perception-in-the-loop. Finally, we show that\nnetworks trained specifically to be robust against $L_\\infty$ norm can still be\nsusceptible to perceptually similar adversarial examples. \n\n"}
{"id": "1901.07294", "contents": "Title: SVE-enabling Lattice QCD Codes Abstract: Optimization of applications for supercomputers of the highest performance\nclass requires parallelization at multiple levels using different techniques.\nIn this contribution we focus on parallelization of particle physics\nsimulations through vector instructions. With the advent of the Scalable Vector\nExtension (SVE) ISA, future ARM-based processors are expected to provide a\nsignificant level of parallelism at this level. \n\n"}
{"id": "1901.07302", "contents": "Title: IOTA-based Directed Acyclic Graphs without Orphans Abstract: Directed Acylic Graphs (DAGs) are emerging as an attractive alternative to\ntraditional blockchain architectures for distributed ledger technology (DLT).\nIn particular DAG ledgers with stochastic attachment mechanisms potentially\noffer many advantages over blockchain, including scalability and faster\ntransaction speeds. However, the random nature of the attachment mechanism\ncoupled with the requirement of protection against double-spend transactions\nleaves open the possibility that not all transactions will be eventually\nvalidated. Such transactions are said to be orphaned, and will never be\nvalidated. Our principal contribution is to propose a simple modification to\nthe attachment mechanism for the Tangle (the IOTA DAG architecture). This\nmodification ensures that all transactions are validated in finite time, and\npreserves essential features of the popular Monte-Carlo selection algorithm. In\norder to demonstrate these results we derive a fluid approximation for the\nTangle (in the limit of infinite arrival rate) and prove that this fluid model\nexhibits the desired behavior. We also present simulations which validate the\nresults for finite arrival rates. \n\n"}
{"id": "1901.08296", "contents": "Title: Deep Learning on Attributed Graphs: A Journey from Graphs to Their\n  Embeddings and Back Abstract: A graph is a powerful concept for representation of relations between pairs\nof entities. Data with underlying graph structure can be found across many\ndisciplines and there is a natural desire for understanding such data better.\nDeep learning (DL) has achieved significant breakthroughs in a variety of\nmachine learning tasks in recent years, especially where data is structured on\na grid, such as in text, speech, or image understanding. However, surprisingly\nlittle has been done to explore the applicability of DL on arbitrary\ngraph-structured data directly.\n  The goal of this thesis is to investigate architectures for DL on graphs and\nstudy how to transfer, adapt or generalize concepts that work well on\nsequential and image data to this domain. We concentrate on two important\nprimitives: embedding graphs or their nodes into a continuous vector space\nrepresentation (encoding) and, conversely, generating graphs from such vectors\nback (decoding). To that end, we make the following contributions.\n  First, we introduce Edge-Conditioned Convolutions (ECC), a convolution-like\noperation on graphs performed in the spatial domain where filters are\ndynamically generated based on edge attributes. The method is used to encode\ngraphs with arbitrary and varying structure.\n  Second, we propose SuperPoint Graph, an intermediate point cloud\nrepresentation with rich edge attributes encoding the contextual relationship\nbetween object parts. Based on this representation, ECC is employed to segment\nlarge-scale point clouds without major sacrifice in fine details.\n  Third, we present GraphVAE, a graph generator allowing us to decode graphs\nwith variable but upper-bounded number of nodes making use of approximate graph\nmatching for aligning the predictions of an autoencoder with its inputs. The\nmethod is applied to the task of molecule generation. \n\n"}
{"id": "1901.08326", "contents": "Title: A stack-vector routing protocol for automatic tunneling Abstract: In a network, a tunnel is a part of a path where a protocol is encapsulated\nin another one. A tunnel starts with an encapsulation and ends with the\ncorresponding decapsulation. Several tunnels can be nested at some stage,\nforming a protocol stack. Tunneling is very important nowadays and it is\ninvolved in several tasks: IPv4/IPv6 transition, VPNs, security (IPsec, onion\nrouting), etc. However, tunnel establishment is mainly performed manually or by\nscript, which present obvious scalability issues. Some works attempt to\nautomate a part of the process (e.g., TSP, ISATAP, etc.). However, the\ndetermination of the tunnel(s) endpoints is not fully automated, especially in\nthe case of an arbitrary number of nested tunnels. The lack of routing\nprotocols performing automatic tunneling is due to the unavailability of path\ncomputation algorithms taking into account encapsulations and decapsulations.\nThere is a polynomial centralized algorithm to perform the task. However, to\nthe best of our knowledge, no fully distributed path computation algorithm is\nknown. Here, we propose the first fully distributed algorithm for path\ncomputation with automatic tunneling, i.e., taking into account encapsulation,\ndecapsulation and conversion of protocols. Our algorithm is a generalization of\nthe distributed Bellman-Ford algorithm, where the distance vector is replaced\nby a protocol stack vector. This allows to know how to route a packet with some\nprotocol stack. We prove that the messages size of our algorithm is polynomial,\neven if the shortest path can be of exponential length. We also prove that the\nalgorithm converges after a polynomial number of steps in a synchronized\nsetting. We adapt our algorithm into a proto-protocol for routing with\nautomatic tunneling and we show its efficiency through simulations. \n\n"}
{"id": "1901.08460", "contents": "Title: Fully Decentralized Joint Learning of Personalized Models and\n  Collaboration Graphs Abstract: We consider the fully decentralized machine learning scenario where many\nusers with personal datasets collaborate to learn models through local\npeer-to-peer exchanges, without a central coordinator. We propose to train\npersonalized models that leverage a collaboration graph describing the\nrelationships between user personal tasks, which we learn jointly with the\nmodels. Our fully decentralized optimization procedure alternates between\ntraining nonlinear models given the graph in a greedy boosting manner, and\nupdating the collaboration graph (with controlled sparsity) given the models.\nThroughout the process, users exchange messages only with a small number of\npeers (their direct neighbors when updating the models, and a few random users\nwhen updating the graph), ensuring that the procedure naturally scales with the\nnumber of users. Overall, our approach is communication-efficient and avoids\nexchanging personal data. We provide an extensive analysis of the convergence\nrate, memory and communication complexity of our approach, and demonstrate its\nbenefits compared to competing techniques on synthetic and real datasets. \n\n"}
{"id": "1901.08705", "contents": "Title: Ambitious Data Science Can Be Painless Abstract: Modern data science research can involve massive computational\nexperimentation; an ambitious PhD in computational fields may do experiments\nconsuming several million CPU hours. Traditional computing practices, in which\nresearchers use laptops or shared campus-resident resources, are inadequate for\nexperiments at the massive scale and varied scope that we now see in data\nscience. On the other hand, modern cloud computing promises seemingly unlimited\ncomputational resources that can be custom configured, and seems to offer a\npowerful new venue for ambitious data-driven science. Exploiting the cloud\nfully, the amount of work that could be completed in a fixed amount of time can\nexpand by several orders of magnitude.\n  As potentially powerful as cloud-based experimentation may be in the\nabstract, it has not yet become a standard option for researchers in many\nacademic disciplines. The prospect of actually conducting massive computational\nexperiments in today's cloud systems confronts the potential user with daunting\nchallenges. Leading considerations include: (i) the seeming complexity of\ntoday's cloud computing interface, (ii) the difficulty of executing an\noverwhelmingly large number of jobs, and (iii) the difficulty of monitoring and\ncombining a massive collection of separate results. Starting a massive\nexperiment `bare-handed' seems therefore highly problematic and prone to rapid\n`researcher burn out'.\n  New software stacks are emerging that render massive cloud experiments\nrelatively painless. Such stacks simplify experimentation by systematizing\nexperiment definition, automating distribution and management of tasks, and\nallowing easy harvesting of results and documentation. In this article, we\ndiscuss several painless computing stacks that abstract away the difficulties\nof massive experimentation, thereby allowing a proliferation of ambitious\nexperiments for scientific discovery. \n\n"}
{"id": "1901.09002", "contents": "Title: A Neurally-Inspired Hierarchical Prediction Network for Spatiotemporal\n  Sequence Learning and Prediction Abstract: In this paper we developed a hierarchical network model, called Hierarchical\nPrediction Network (HPNet), to understand how spatiotemporal memories might be\nlearned and encoded in the recurrent circuits in the visual cortical hierarchy\nfor predicting future video frames. This neurally inspired model operates in\nthe analysis-by-synthesis framework. It contains a feed-forward path that\ncomputes and encodes spatiotemporal features of successive complexity and a\nfeedback path for the successive levels to project their interpretations to the\nlevel below. Within each level, the feed-forward path and the feedback path\nintersect in a recurrent gated circuit, instantiated in a LSTM module, to\ngenerate a prediction or explanation of the incoming signals. The network\nlearns its internal model of the world by minimizing the errors of its\nprediction of the incoming signals at each level of the hierarchy. We found\nthat hierarchical interaction in the network increases semantic clustering of\nglobal movement patterns in the population codes of the units along the\nhierarchy, even in the earliest module. This facilitates the learning of\nrelationships among movement patterns, yielding state-of-the-art performance in\nlong range video sequence predictions in the benchmark datasets. The network\nmodel automatically reproduces a variety of prediction suppression and\nfamiliarity suppression neurophysiological phenomena observed in the visual\ncortex, suggesting that hierarchical prediction might indeed be an important\nprinciple for representational learning in the visual cortex. \n\n"}
{"id": "1901.09049", "contents": "Title: Biologically inspired alternatives to backpropagation through time for\n  learning in recurrent neural nets Abstract: The way how recurrently connected networks of spiking neurons in the brain\nacquire powerful information processing capabilities through learning has\nremained a mystery. This lack of understanding is linked to a lack of learning\nalgorithms for recurrent networks of spiking neurons (RSNNs) that are both\nfunctionally powerful and can be implemented by known biological mechanisms.\nSince RSNNs are simultaneously a primary target for implementations of\nbrain-inspired circuits in neuromorphic hardware, this lack of algorithmic\ninsight also hinders technological progress in that area. The gold standard for\nlearning in recurrent neural networks in machine learning is back-propagation\nthrough time (BPTT), which implements stochastic gradient descent with regard\nto a given loss function. But BPTT is unrealistic from a biological\nperspective, since it requires a transmission of error signals backwards in\ntime and in space, i.e., from post- to presynaptic neurons. We show that an\nonline merging of locally available information during a computation with\nsuitable top-down learning signals in real-time provides highly capable\napproximations to BPTT. For tasks where information on errors arises only late\nduring a network computation, we enrich locally available information through\nfeedforward eligibility traces of synapses that can easily be computed in an\nonline manner. The resulting new generation of learning algorithms for\nrecurrent neural networks provides a new understanding of network learning in\nthe brain that can be tested experimentally. In addition, these algorithms\nprovide efficient methods for on-chip training of RSNNs in neuromorphic\nhardware. \n\n"}
{"id": "1901.09339", "contents": "Title: Heterogeneity-aware Gradient Coding for Straggler Tolerance Abstract: Gradient descent algorithms are widely used in machine learning. In order to\ndeal with huge volume of data, we consider the implementation of gradient\ndescent algorithms in a distributed computing setting where multiple workers\ncompute the gradient over some partial data and the master node aggregates\ntheir results to obtain the gradient over the whole data. However, its\nperformance can be severely affected by straggler workers. Recently, some\ncoding-based approaches are introduced to mitigate the straggler problem, but\nthey are efficient only when the workers are homogeneous, i.e., having the same\ncomputation capabilities. In this paper, we consider that the workers are\nheterogeneous which are common in modern distributed systems. We propose a\nnovel heterogeneity-aware gradient coding scheme which can not only tolerate a\npredetermined number of stragglers but also fully utilize the computation\ncapabilities of heterogeneous workers. We show that this scheme is optimal when\nthe computation capabilities of workers are estimated accurately. A variant of\nthis scheme is further proposed to improve the performance when the estimations\nof the computation capabilities are not so accurate. We conduct our schemes for\ngradient descent based image classification on QingCloud clusters. Evaluation\nresults show that our schemes can reduce the whole computation time by up to\n$3\\times$ compared with a state-of-the-art coding scheme. \n\n"}
{"id": "1901.09948", "contents": "Title: Surrogate Gradient Learning in Spiking Neural Networks Abstract: Spiking neural networks are nature's versatile solution to fault-tolerant and\nenergy efficient signal processing. To translate these benefits into hardware,\na growing number of neuromorphic spiking neural network processors attempt to\nemulate biological neural networks. These developments have created an imminent\nneed for methods and tools to enable such systems to solve real-world signal\nprocessing problems. Like conventional neural networks, spiking neural networks\ncan be trained on real, domain specific data. However, their training requires\novercoming a number of challenges linked to their binary and dynamical nature.\nThis article elucidates step-by-step the problems typically encountered when\ntraining spiking neural networks, and guides the reader through the key\nconcepts of synaptic plasticity and data-driven learning in the spiking\nsetting. To that end, it gives an overview of existing approaches and provides\nan introduction to surrogate gradient methods, specifically, as a particularly\nflexible and efficient method to overcome the aforementioned challenges. \n\n"}
{"id": "1901.10008", "contents": "Title: The OoO VLIW JIT Compiler for GPU Inference Abstract: Current trends in Machine Learning~(ML) inference on hardware accelerated\ndevices (e.g., GPUs, TPUs) point to alarmingly low utilization. As ML inference\nis increasingly time-bounded by tight latency SLOs, increasing data parallelism\nis not an option. The need for better efficiency motivates GPU multiplexing.\nFurthermore, existing GPU programming abstractions force programmers to\nmicro-manage GPU resources in an early-binding, context-free fashion. We\npropose a VLIW-inspired Out-of-Order (OoO) Just-in-Time (JIT) compiler that\ncoalesces and reorders execution kernels at runtime for throughput-optimal\ndevice utilization while satisfying latency SLOs. We quantify the\ninefficiencies of space-only and time-only multiplexing alternatives and\ndemonstrate an achievable 7.7x opportunity gap through spatial coalescing. \n\n"}
{"id": "1901.10023", "contents": "Title: Managing Fog Networks using Reinforcement Learning Based Load Balancing\n  Algorithm Abstract: The powerful paradigm of Fog computing is currently receiving major interest,\nas it provides the possibility to integrate virtualized servers into networks\nand brings cloud service closer to end devices. To support this distributed\nintelligent platform, Software-Defined Network (SDN) has emerged as a viable\nnetwork technology in the Fog computing environment. However, uncertainties\nrelated to task demands and the different computing capacities of Fog nodes,\ninquire an effective load balancing algorithm. In this paper, the load\nbalancing problem has been addressed under the constraint of achieving the\nminimum latency in Fog networks. To handle this problem, a reinforcement\nlearning based decision-making process has been proposed to find the optimal\noffloading decision with unknown reward and transition functions. The proposed\nprocess allows Fog nodes to offload an optimal number of tasks among incoming\ntasks by selecting an available neighboring Fog node under their respective\nresource capabilities with the aim to minimize the processing time and the\noverall overloading probability. Compared with the traditional approaches, the\nproposed scheme not only simplifies the algorithmic framework without imposing\nany specific assumption on the network model but also guarantees convergence in\npolynomial time. The results show that, during average delays, the proposed\nreinforcement learning-based offloading method achieves significant performance\nimprovements over the variation of service rate and traffic arrival rate. The\nproposed algorithm achieves 1.17%, 1.02%, and 3.21% lower overload probability\nrelative to random, least-queue and nearest offloading selection schemes,\nrespectively. \n\n"}
{"id": "1901.10277", "contents": "Title: High-Quality Self-Supervised Deep Image Denoising Abstract: We describe a novel method for training high-quality image denoising models\nbased on unorganized collections of corrupted images. The training does not\nneed access to clean reference images, or explicit pairs of corrupted images,\nand can thus be applied in situations where such data is unacceptably expensive\nor impossible to acquire. We build on a recent technique that removes the need\nfor reference data by employing networks with a \"blind spot\" in the receptive\nfield, and significantly improve two key aspects: image quality and training\nefficiency. Our result quality is on par with state-of-the-art neural network\ndenoisers in the case of i.i.d. additive Gaussian noise, and not far behind\nwith Poisson and impulse noise. We also successfully handle cases where\nparameters of the noise model are variable and/or unknown in both training and\nevaluation data. \n\n"}
{"id": "astro-ph/0703485", "contents": "Title: Towards Distributed Petascale Computing Abstract: In this chapter we will argue that studying such multi-scale multi-science\nsystems gives rise to inherently hybrid models containing many different\nalgorithms best serviced by different types of computing environments (ranging\nfrom massively parallel computers, via large-scale special purpose machines to\nclusters of PC's) whose total integrated computing capacity can easily reach\nthe PFlop/s scale. Such hybrid models, in combination with the by now\ninherently distributed nature of the data on which the models `feed' suggest a\ndistributed computing model, where parts of the multi-scale multi-science model\nare executed on the most suitable computing environment, and/or where the\ncomputations are carried out close to the required data (i.e. bring the\ncomputations to the data instead of the other way around). We presents an\nestimate for the compute requirements to simulate the Galaxy as a typical\nexample of a multi-scale multi-physics application, requiring distributed\nPetaflop/s computational power. \n\n"}
{"id": "cond-mat/9909114", "contents": "Title: From Massively Parallel Algorithms and Fluctuating Time Horizons to\n  Non-equilibrium Surface Growth Abstract: We study the asymptotic scaling properties of a massively parallel algorithm\nfor discrete-event simulations where the discrete events are Poisson arrivals.\nThe evolution of the simulated time horizon is analogous to a non-equilibrium\nsurface. Monte Carlo simulations and a coarse-grained approximation indicate\nthat the macroscopic landscape in the steady state is governed by the\nEdwards-Wilkinson Hamiltonian. Since the efficiency of the algorithm\ncorresponds to the density of local minima in the associated surface, our\nresults imply that the algorithm is asymptotically scalable. \n\n"}
{"id": "cs/0306043", "contents": "Title: Skip Graphs Abstract: Skip graphs are a novel distributed data structure, based on skip lists, that\nprovide the full functionality of a balanced tree in a distributed system where\nresources are stored in separate nodes that may fail at any time. They are\ndesigned for use in searching peer-to-peer systems, and by providing the\nability to perform queries based on key ordering, they improve on existing\nsearch tools that provide only hash table functionality. Unlike skip lists or\nother tree data structures, skip graphs are highly resilient, tolerating a\nlarge fraction of failed nodes without losing connectivity. In addition,\nconstructing, inserting new nodes into, searching a skip graph, and detecting\nand repairing errors in the data structure introduced by node failures can be\ndone using simple and straightforward algorithms. \n\n"}
{"id": "cs/0307036", "contents": "Title: Small-World File-Sharing Communities Abstract: Web caches, content distribution networks, peer-to-peer file sharing\nnetworks, distributed file systems, and data grids all have in common that they\ninvolve a community of users who generate requests for shared data. In each\ncase, overall system performance can be improved significantly if we can first\nidentify and then exploit interesting structure within a community's access\npatterns. To this end, we propose a novel perspective on file sharing based on\nthe study of the relationships that form among users based on the files in\nwhich they are interested.\n  We propose a new structure that captures common user interests in data--the\ndata-sharing graph-- and justify its utility with studies on three\ndata-distribution systems: a high-energy physics collaboration, the Web, and\nthe Kazaa peer-to-peer network. We find small-world patterns in the\ndata-sharing graphs of all three communities. We analyze these graphs and\npropose some probable causes for these emergent small-world patterns. The\nsignificance of small-world patterns is twofold: it provides a rigorous support\nto intuition and, perhaps most importantly, it suggests ways to design\nmechanisms that exploit these naturally emerging patterns. \n\n"}
{"id": "cs/0310030", "contents": "Title: A Particular Bug Trap: Execution Replay Using Virtual Machines Abstract: Execution-replay (ER) is well known in the literature but has been restricted\nto special system architectures for many years. Improved hardware resources and\nthe maturity of virtual machine technology promise to make ER useful for a\nbroader range of development projects.\n  This paper describes an approach to create a practical, generic ER\ninfrastructure for desktop PC systems using virtual machine technology. In the\ncreated VM environment arbitrary application programs will run and be replayed\nunmodified, neither instrumentation nor recompilation are required. \n\n"}
{"id": "cs/0407058", "contents": "Title: Communication-Aware Processor Allocation for Supercomputers Abstract: This paper gives processor-allocation algorithms for minimizing the average\nnumber of communication hops between the assigned processors for grid\narchitectures, in the presence of occupied cells. The simpler problem of\nassigning processors on a free grid has been studied by Karp, McKellar, and\nWong who show that the solutions have nontrivial structure; they left open the\ncomplexity of the problem.\n  The associated clustering problem is as follows: Given n points in Re^d, find\nk points that minimize their average pairwise L1 distance. We present a natural\napproximation algorithm and show that it is a 7/4-approximation for 2D grids.\nFor d-dimensional space, the approximation guarantee is 2-(1/2d), which is\ntight. We also give a polynomial-time approximation scheme (PTAS) for constant\ndimension d, and report on experimental results. \n\n"}
{"id": "cs/0411075", "contents": "Title: A Self-Reconfigurable Computing Platform Hardware Architecture Abstract: Field Programmable Gate Arrays (FPGAs) have recently been increasingly used\nfor highly-parallel processing of compute intensive tasks. This paper\nintroduces an FPGA hardware platform architecture that is PC-based, allows for\nfast reconfiguration over the PCI bus, and retains a simple physical hardware\ndesign. The design considerations are first discussed, then the resulting\nsystem architecture designed is illustrated. Finally, experimental results on\nthe FPGA resources utilized for this design are presented. \n\n"}
{"id": "cs/0506034", "contents": "Title: A Taxonomy of Data Grids for Distributed Data Sharing, Management and\n  Processing Abstract: Data Grids have been adopted as the platform for scientific communities that\nneed to share, access, transport, process and manage large data collections\ndistributed worldwide. They combine high-end computing technologies with\nhigh-performance networking and wide-area storage management techniques. In\nthis paper, we discuss the key concepts behind Data Grids and compare them with\nother data sharing and distribution paradigms such as content delivery\nnetworks, peer-to-peer networks and distributed databases. We then provide\ncomprehensive taxonomies that cover various aspects of architecture, data\ntransportation, data replication and resource allocation and scheduling.\nFinally, we map the proposed taxonomy to various Data Grid systems not only to\nvalidate the taxonomy but also to identify areas for future exploration.\nThrough this taxonomy, we aim to categorise existing systems to better\nunderstand their goals and their methodology. This would help evaluate their\napplicability for solving similar problems. This taxonomy also provides a \"gap\nanalysis\" of this area through which researchers can potentially identify new\nissues for investigation. Finally, we hope that the proposed taxonomy and\nmapping also helps to provide an easy way for new practitioners to understand\nthis complex area of research. \n\n"}
{"id": "cs/0507072", "contents": "Title: Reliable Data Storage in Distributed Hash Tables Abstract: Distributed Hash Tables offer a resilient lookup service for unstable\ndistributed environments. Resilient data storage, however, requires additional\ndata replication and maintenance algorithms. These algorithms can have an\nimpact on both the performance and the scalability of the system. In this\npaper, we describe the goals and design space of these replication algorithms.\n  We examine an existing replication algorithm, and present a new analysis of\nits reliability. We then present a new dynamic replication algorithm which can\noperate in unstable environments. We give several possible replica placement\nstrategies for this algorithm, and show how they impact reliability and\nperformance.\n  Finally we compare all replication algorithms through simulation, showing\nquantitatively the difference between their bandwidth use, fault tolerance and\nperformance. \n\n"}
{"id": "cs/0602055", "contents": "Title: Revisiting Evolutionary Algorithms with On-the-Fly Population Size\n  Adjustment Abstract: In an evolutionary algorithm, the population has a very important role as its\nsize has direct implications regarding solution quality, speed, and\nreliability. Theoretical studies have been done in the past to investigate the\nrole of population sizing in evolutionary algorithms. In addition to those\nstudies, several self-adjusting population sizing mechanisms have been proposed\nin the literature. This paper revisits the latter topic and pays special\nattention to the genetic algorithm with adaptive population size (APGA), for\nwhich several researchers have claimed to be very effective at autonomously\n(re)sizing the population.\n  As opposed to those previous claims, this paper suggests a complete opposite\nview. Specifically, it shows that APGA is not capable of adapting the\npopulation size at all. This claim is supported on theoretical grounds and\nconfirmed by computer simulations. \n\n"}
{"id": "cs/0602068", "contents": "Title: Parallel Symbolic Computation of Curvature Invariants in General\n  Relativity Abstract: We present a practical application of parallel symbolic computation in\nGeneral Relativity: the calculation of curvature invariants for large\ndimension. We discuss the structure of the calculations, an implementation of\nthe technique and scaling of the computation with spacetime dimension for\nvarious invariants. \n\n"}
{"id": "cs/0701075", "contents": "Title: Open-architecture Implementation of Fragment Molecular Orbital Method\n  for Peta-scale Computing Abstract: We present our perspective and goals on highperformance computing for\nnanoscience in accordance with the global trend toward \"peta-scale computing.\"\nAfter reviewing our results obtained through the grid-enabled version of the\nfragment molecular orbital method (FMO) on the grid testbed by the Japanese\nGrid Project, National Research Grid Initiative (NAREGI), we show that FMO is\none of the best candidates for peta-scale applications by predicting its\neffective performance in peta-scale computers. Finally, we introduce our new\nproject constructing a peta-scale application in an open-architecture\nimplementation of FMO in order to realize both goals of highperformance in\npeta-scale computers and extendibility to multiphysics simulations. \n\n"}
{"id": "cs/9908013", "contents": "Title: Collective Intelligence for Control of Distributed Dynamical Systems Abstract: We consider the El Farol bar problem, also known as the minority game (W. B.\nArthur, ``The American Economic Review'', 84(2): 406--411 (1994), D. Challet\nand Y.C. Zhang, ``Physica A'', 256:514 (1998)). We view it as an instance of\nthe general problem of how to configure the nodal elements of a distributed\ndynamical system so that they do not ``work at cross purposes'', in that their\ncollective dynamics avoids frustration and thereby achieves a provided global\ngoal. We summarize a mathematical theory for such configuration applicable when\n(as in the bar problem) the global goal can be expressed as minimizing a global\nenergy function and the nodes can be expressed as minimizers of local free\nenergy functions. We show that a system designed with that theory performs\nnearly optimally for the bar problem. \n\n"}
{"id": "cs/9912012", "contents": "Title: Avoiding Braess' Paradox through Collective Intelligence Abstract: In an Ideal Shortest Path Algorithm (ISPA), at each moment each router in a\nnetwork sends all of its traffic down the path that will incur the lowest cost\nto that traffic. In the limit of an infinitesimally small amount of traffic for\na particular router, its routing that traffic via an ISPA is optimal, as far as\ncost incurred by that traffic is concerned. We demonstrate though that in many\ncases, due to the side-effects of one router's actions on another routers\nperformance, having routers use ISPA's is suboptimal as far as global aggregate\ncost is concerned, even when only used to route infinitesimally small amounts\nof traffic. As a particular example of this we present an instance of Braess'\nparadox for ISPA's, in which adding new links to a network decreases overall\nthroughput. We also demonstrate that load-balancing, in which the routing\ndecisions are made to optimize the global cost incurred by all traffic\ncurrently being routed, is suboptimal as far as global cost averaged across\ntime is concerned. This is also due to \"side-effects\", in this case of current\nrouting decision on future traffic.\n  The theory of COllective INtelligence (COIN) is concerned precisely with the\nissue of avoiding such deleterious side-effects. We present key concepts from\nthat theory and use them to derive an idealized algorithm whose performance is\nbetter than that of the ISPA, even in the infinitesimal limit. We present\nexperiments verifying this, and also showing that a machine-learning-based\nversion of this COIN algorithm in which costs are only imprecisely estimated (a\nversion potentially applicable in the real world) also outperforms the ISPA,\ndespite having access to less information than does the ISPA. In particular,\nthis COIN algorithm avoids Braess' paradox. \n\n"}
{"id": "hep-lat/0308005", "contents": "Title: Parallel implementation of a lattice-gauge-theory code: studying quark\n  confinement on PC clusters Abstract: We consider the implementation of a parallel Monte Carlo code for\nhigh-performance simulations on PC clusters with MPI. We carry out tests of\nspeedup and efficiency. The code is used for numerical simulations of pure\nSU(2) lattice gauge theory at very large lattice volumes, in order to study the\ninfrared behavior of gluon and ghost propagators. This problem is directly\nrelated to the confinement of quarks and gluons in the physics of strong\ninteractions. \n\n"}
{"id": "math/0508533", "contents": "Title: On the cascade rollback synchronization Abstract: We consider a cascade model of $N$ different processors performing a\ndistributed parallel simulation. The main goal of the study is to show that the\nlong-time dynamics of the system has a cluster behavior. To attack this problem\nwe combine two methods: stochastic comparison and Foster-Lyapunov functions. \n\n"}
{"id": "nlin/0506061", "contents": "Title: Transmitting a signal by amplitude modulation in a chaotic network Abstract: We discuss the ability of a network with non linear relays and chaotic\ndynamics to transmit signals, on the basis of a linear response theory\ndeveloped by Ruelle \\cite{Ruelle} for dissipative systems. We show in\nparticular how the dynamics interfere with the graph topology to produce an\neffective transmission network, whose topology depends on the signal, and\ncannot be directly read on the ``wired'' network. This leads one to reconsider\nnotions such as ``hubs''. Then, we show examples where, with a suitable choice\nof the carrier frequency (resonance), one can transmit a signal from a node to\nanother one by amplitude modulation, \\textit{in spite of chaos}. Also, we give\nan example where a signal, transmitted to any node via different paths, can\nonly be recovered by a couple of \\textit{specific} nodes. This opens the\npossibility for encoding data in a way such that the recovery of the signal\nrequires the knowledge of the carrier frequency \\textit{and} can be performed\nonly at some specific node. \n\n"}
{"id": "nlin/0609038", "contents": "Title: From Neuron to Neural Networks dynamics Abstract: This paper presents an overview of some techniques and concepts coming from\ndynamical system theory and used for the analysis of dynamical neural networks\nmodels. In a first section, we describe the dynamics of the neuron, starting\nfrom the Hodgkin-Huxley description, which is somehow the canonical description\nfor the ``biological neuron''. We discuss some models reducing the\nHodgkin-Huxley model to a two dimensional dynamical system, keeping one of the\nmain feature of the neuron: its excitability. We present then examples of phase\ndiagram and bifurcation analysis for the Hodgin-Huxley equations. Finally, we\nend this section by a dynamical system analysis for the nervous flux\npropagation along the axon. We then consider neuron couplings, with a brief\ndescription of synapses, synaptic plasticiy and learning, in a second section.\nWe also briefly discuss the delicate issue of causal action from one neuron to\nanother when complex feedback effects and non linear dynamics are involved. The\nthird section presents the limit of weak coupling and the use of normal forms\ntechnics to handle this situation. We consider then several examples of\nrecurrent models with different type of synaptic interactions (symmetric,\ncooperative, random). We introduce various techniques coming from statistical\nphysics and dynamical systems theory. A last section is devoted to a detailed\nexample of recurrent model where we go in deep in the analysis of the dynamics\nand discuss the effect of learning on the neuron dynamics. We also present\nrecent methods allowing the analysis of the non linear effects of the neural\ndynamics on signal propagation and causal action. An appendix, presenting the\nmain notions of dynamical systems theory useful for the comprehension of the\nchapter, has been added for the convenience of the reader. \n\n"}
{"id": "physics/0601118", "contents": "Title: Learning about knowledge: A complex network approach Abstract: This article describes an approach to modeling knowledge acquisition in terms\nof walks along complex networks. Each subset of knowledge is represented as a\nnode, and relations between such knowledge are expressed as edges. Two types of\nedges are considered, corresponding to free and conditional transitions. The\nlatter case implies that a node can only be reached after visiting previously a\nset of nodes (the required conditions). The process of knowledge acquisition\ncan then be simulated by considering the number of nodes visited as a single\nagent moves along the network, starting from its lowest layer. It is shown that\nhierarchical networks, i.e. networks composed of successive interconnected\nlayers, arise naturally as a consequence of compositions of the prerequisite\nrelationships between the nodes. In order to avoid deadlocks, i.e. unreachable\nnodes, the subnetwork in each layer is assumed to be a connected component.\nSeveral configurations of such hierarchical knowledge networks are simulated\nand the performance of the moving agent quantified in terms of the percentage\nof visited nodes after each movement. The Barab\\'asi-Albert and random models\nare considered for the layer and interconnecting subnetworks. Although all\nsubnetworks in each realization have the same number of nodes, several\ninterconnectivities, defined by the average node degree of the interconnection\nnetworks, have been considered. Two visiting strategies are investigated:\nrandom choice among the existing edges and preferential choice to so far\nuntracked edges. A series of interesting results are obtained, including the\nidentification of a series of plateaux of knowledge stagnation in the case of\nthe preferential movements strategy in presence of conditional edges. \n\n"}
