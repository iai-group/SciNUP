{"id": "0708.3699", "contents": "Title: Convolutional Entanglement Distillation Abstract: We develop a theory of entanglement distillation that exploits a\nconvolutional coding structure. We provide a method for converting an arbitrary\nclassical binary or quaternary convolutional code into a convolutional\nentanglement distillation protocol. The imported classical convolutional code\ndoes not have to be dual-containing or self-orthogonal. The yield and\nerror-correcting properties of such a protocol depend respectively on the rate\nand error-correcting properties of the imported classical convolutional code. A\nconvolutional entanglement distillation protocol has several other benefits.\nTwo parties sharing noisy ebits can distill noiseless ebits ``online'' as they\nacquire more noisy ebits. Distillation yield is high and decoding complexity is\nsimple for a convolutional entanglement distillation protocol. Our theory of\nconvolutional entanglement distillation reduces the problem of finding a good\nconvolutional entanglement distillation protocol to the well-established\nproblem of finding a good classical convolutional code. \n\n"}
{"id": "0709.2833", "contents": "Title: Distributed Space Time Codes with Low Decoding Complexity for\n  Asynchronous Relay Networks Abstract: Recently Li and Xia have proposed a transmission scheme for wireless relay\nnetworks based on the Alamouti space time code and orthogonal frequency\ndivision multiplexing to combat the effect of timing errors at the relay nodes.\nThis transmission scheme is amazingly simple and achieves a diversity order of\ntwo for any number of relays. Motivated by its simplicity, this scheme is\nextended to a more general transmission scheme that can achieve full\ncooperative diversity for any number of relays. The conditions on the\ndistributed space time code (DSTC) structure that admit its application in the\nproposed transmission scheme are identified and it is pointed out that the\nrecently proposed full diversity four group decodable DSTCs from precoded\nco-ordinate interleaved orthogonal designs and extended Clifford algebras\nsatisfy these conditions. It is then shown how differential encoding at the\nsource can be combined with the proposed transmission scheme to arrive at a new\ntransmission scheme that can achieve full cooperative diversity in asynchronous\nwireless relay networks with no channel information and also no timing error\nknowledge at the destination node. Finally, four group decodable distributed\ndifferential space time codes applicable in this new transmission scheme for\npower of two number of relays are also provided. \n\n"}
{"id": "0802.3250", "contents": "Title: Valuation of Mortality Risk via the Instantaneous Sharpe Ratio:\n  Applications to Life Annuities Abstract: We develop a theory for valuing non-diversifiable mortality risk in an\nincomplete market. We do this by assuming that the company issuing a\nmortality-contingent claim requires compensation for this risk in the form of a\npre-specified instantaneous Sharpe ratio. We apply our method to value life\nannuities. One result of our paper is that the value of the life annuity is\n{\\it identical} to the upper good deal bound of Cochrane and Sa\\'{a}-Requejo\n(2000) and of Bj\\\"{o}rk and Slinko (2006) applied to our setting. A second\nresult of our paper is that the value per contract solves a {\\it linear}\npartial differential equation as the number of contracts approaches infinity.\nOne can represent the limiting value as an expectation with respect to an\nequivalent martingale measure (as in Blanchet-Scalliet, El Karoui, and\nMartellini (2005)), and from this representation, one can interpret the\ninstantaneous Sharpe ratio as an annuity market's price of mortality risk. \n\n"}
{"id": "0804.0980", "contents": "Title: Large MIMO Detection: A Low-Complexity Detector at High Spectral\n  Efficiencies Abstract: We consider large MIMO systems, where by `{\\em large}' we mean number of\ntransmit and receive antennas of the order of tens to hundreds. Such large MIMO\nsystems will be of immense interest because of the very high spectral\nefficiencies possible in such systems. We present a low-complexity detector\nwhich achieves uncoded near-exponential diversity performance for hundreds of\nantennas (i.e., achieves near SISO AWGN performance in a large MIMO fading\nenvironment) with an average per-bit complexity of just $O(N_tN_r)$, where\n$N_t$ and $N_r$ denote the number of transmit and receive antennas,\nrespectively. With an outer turbo code, the proposed detector achieves good\ncoded bit error performance as well. For example, in a 600 transmit and 600\nreceive antennas V-BLAST system with a high spectral efficiency of 200 bps/Hz\n(using BPSK and rate-1/3 turbo code), our simulation results show that the\nproposed detector performs close to within about 4.6 dB from theoretical\ncapacity. We also adopt the proposed detector for the low-complexity decoding\nof high-rate non-orthogonal space-time block codes (STBC) from division\nalgebras (DA). For example, we have decoded the $16\\times 16$ full-rate\nnon-orthogonal STBC from DA using the proposed detector and show that it\nperforms close to within about 5.5 dB of the capacity using 4-QAM and rate-3/4\nturbo code at a spectral efficiency of 24 bps/Hz. The practical feasibility of\nthe proposed high-performance low-complexity detector could potentially trigger\nwide interest in the implementation of large MIMO systems. In large MC-CDMA\nsystems with hundreds of users, the proposed detector is shown to achieve near\nsingle-user performance at an average per-bit complexity linear in number of\nusers, which is quite appealing for its use in practical CDMA systems. \n\n"}
{"id": "0805.0910", "contents": "Title: Lyapunov control of a quantum particle in a decaying potential Abstract: A Lyapunov-based approach for the trajectory generation of an $N$-dimensional\nSchr{\\\"o}dinger equation in whole $\\RR^N$ is proposed. For the case of a\nquantum particle in an $N$-dimensional decaying potential the convergence is\nprecisely analyzed. The free system admitting a mixed spectrum, the dispersion\nthrough the absolutely continuous part is the main obstacle to ensure such a\nstabilization result. Whenever, the system is completely initialized in the\ndiscrete part of the spectrum, a Lyapunov strategy encoding both the distance\nwith respect to the target state and the penalization of the passage through\nthe continuous part of the spectrum, ensures the approximate stabilization. \n\n"}
{"id": "0806.4200", "contents": "Title: The Secrecy Rate Region of the Broadcast Channel Abstract: In this paper, we consider a scenario where a source node wishes to broadcast\ntwo confidential messages for two respective receivers, while a wire-tapper\nalso receives the transmitted signal. This model is motivated by wireless\ncommunications, where individual secure messages are broadcast over open media\nand can be received by any illegitimate receiver. The secrecy level is measured\nby equivocation rate at the eavesdropper. We first study the general\n(non-degraded) broadcast channel with confidential messages. We present an\ninner bound on the secrecy capacity region for this model. The inner bound\ncoding scheme is based on a combination of random binning and the\nGelfand-Pinsker bining. This scheme matches the Marton's inner bound on the\nbroadcast channel without confidentiality constraint. We further study the\nsituation where the channels are degraded. For the degraded broadcast channel\nwith confidential messages, we present the secrecy capacity region. Our\nachievable coding scheme is based on Cover's superposition scheme and random\nbinning. We refer to this scheme as Secret Superposition Scheme. In this\nscheme, we show that randomization in the first layer increases the secrecy\nrate of the second layer. This capacity region matches the capacity region of\nthe degraded broadcast channel without security constraint. It also matches the\nsecrecy capacity for the conventional wire-tap channel. Our converse proof is\nbased on a combination of the converse proof of the conventional degraded\nbroadcast channel and Csiszar lemma. Finally, we assume that the channels are\nAdditive White Gaussian Noise (AWGN) and show that secret superposition scheme\nwith Gaussian codebook is optimal. The converse proof is based on the\ngeneralized entropy power inequality. \n\n"}
{"id": "0809.3540", "contents": "Title: A Note on the Equivalence of Gibbs Free Energy and Information Theoretic\n  Capacity Abstract: The minimization of Gibbs free energy is based on the changes in work and\nfree energy that occur in a physical or chemical system. The maximization of\nmutual information, the capacity, of a noisy channel is determined based on the\nmarginal probabilities and conditional entropies associated with a\ncommunications system. As different as the procedures might first appear,\nthrough the exploration of a simple, \"dual use\" Ising model, it is seen that\nthe two concepts are in fact the same. In particular, the case of a binary\nsymmetric channel is calculated in detail. \n\n"}
{"id": "0810.4658", "contents": "Title: Indexability of Restless Bandit Problems and Optimality of Whittle's\n  Index for Dynamic Multichannel Access Abstract: We consider a class of restless multi-armed bandit problems (RMBP) that\narises in dynamic multichannel access, user/server scheduling, and optimal\nactivation in multi-agent systems. For this class of RMBP, we establish the\nindexability and obtain Whittle's index in closed-form for both discounted and\naverage reward criteria. These results lead to a direct implementation of\nWhittle's index policy with remarkably low complexity. When these Markov chains\nare stochastically identical, we show that Whittle's index policy is optimal\nunder certain conditions. Furthermore, it has a semi-universal structure that\nobviates the need to know the Markov transition probabilities. The optimality\nand the semi-universal structure result from the equivalency between Whittle's\nindex policy and the myopic policy established in this work. For non-identical\nchannels, we develop efficient algorithms for computing a performance upper\nbound given by Lagrangian relaxation. The tightness of the upper bound and the\nnear-optimal performance of Whittle's index policy are illustrated with\nsimulation examples. \n\n"}
{"id": "0811.0196", "contents": "Title: Reduced-Complexity Reed--Solomon Decoders Based on Cyclotomic FFTs Abstract: In this paper, we reduce the computational complexities of partial and dual\npartial cyclotomic FFTs (CFFTs), which are discrete Fourier transforms where\nspectral and temporal components are constrained, based on their properties as\nwell as a common subexpression elimination algorithm. Our partial CFFTs achieve\nsmaller computational complexities than previously proposed partial CFFTs.\nUtilizing our CFFTs in both transform- and time-domain Reed--Solomon decoders,\nwe achieve significant complexity reductions. \n\n"}
{"id": "0901.1869", "contents": "Title: Low-Complexity Near-ML Decoding of Large Non-Orthogonal STBCs Using PDA Abstract: Non-orthogonal space-time block codes (STBC) from cyclic division algebras\n(CDA) having large dimensions are attractive because they can simultaneously\nachieve both high spectral efficiencies (same spectral efficiency as in V-BLAST\nfor a given number of transmit antennas) {\\em as well as} full transmit\ndiversity. Decoding of non-orthogonal STBCs with hundreds of dimensions has\nbeen a challenge. In this paper, we present a probabilistic data association\n(PDA) based algorithm for decoding non-orthogonal STBCs with large dimensions.\nOur simulation results show that the proposed PDA-based algorithm achieves near\nSISO AWGN uncoded BER as well as near-capacity coded BER (within about 5 dB of\nthe theoretical capacity) for large non-orthogonal STBCs from CDA. We study the\neffect of spatial correlation on the BER, and show that the performance loss\ndue to spatial correlation can be alleviated by providing more receive spatial\ndimensions. We report good BER performance when a training-based iterative\ndecoding/channel estimation is used (instead of assuming perfect channel\nknowledge) in channels with large coherence times. A comparison of the\nperformances of the PDA algorithm and the likelihood ascent search (LAS)\nalgorithm (reported in our recent work) is also presented. \n\n"}
{"id": "0905.3360", "contents": "Title: A Generalized Statistical Complexity Measure: Applications to Quantum\n  Systems Abstract: A two-parameter family of complexity measures $\\tilde{C}^{(\\alpha,\\beta)}$\nbased on the R\\'enyi entropies is introduced and characterized by a detailed\nstudy of its mathematical properties. This family is the generalization of a\ncontinuous version of the LMC complexity, which is recovered for $\\alpha=1$ and\n$\\beta=2$. These complexity measures are obtained by multiplying two quantities\nbringing global information on the probability distribution defining the\nsystem. When one of the parameters, $\\alpha$ or $\\beta$, goes to infinity, one\nof the global factors becomes a local factor. For this special case, the\ncomplexity is calculated on different quantum systems: H-atom, harmonic\noscillator and square well. \n\n"}
{"id": "0908.0856", "contents": "Title: Outage Capacity of Incremental Relaying at Low Signal-to-Noise Ratios Abstract: We present the \\epsilon-outage capacity of incremental relaying at low\nsignal-to-noise ratios (SNR) in a wireless cooperative network with slow\nRayleigh fading channels. The relay performs decode-and-forward and repetition\ncoding is employed in the network, which is optimal in the low SNR regime. We\nderive an expression on the optimal relay location that maximizes the\n\\epsilon-outage capacity. It is shown that this location is independent of the\noutage probability and SNR but only depends on the channel conditions\nrepresented by a path-loss factor. We compare our results to the\n\\epsilon-outage capacity of the cut-set bound and demonstrate that the ratio\nbetween the \\epsilon-outage capacity of incremental relaying and the cut-set\nbound lies within 1/\\sqrt{2} and 1. Furthermore, we derive lower bounds on the\n\\epsilon-outage capacity for the case of K relays. \n\n"}
{"id": "0909.1115", "contents": "Title: Capacity Region of Layered Erasure One-sided Interference Channels\n  without CSIT Abstract: This paper studies a layered erasure interference channel model, which is a\nsimplification of the Gaussian interference channel with fading using the\ndeterministic model approach. In particular, the capacity region of the layered\nerasure one-sided interference channel is completely determined, assuming that\nthe channel state information (CSI) is known to the receivers, but there is no\nCSI at transmitters (CSIT). The result holds for arbitrary fading statistics.\nPrevious results of Aggarwal, Sankar, Calderbank and Poor on the capacity\nregion or sum capacity under several interference configurations are shown to\nbe special cases of the capacity region shown in this paper. \n\n"}
{"id": "0909.4876", "contents": "Title: A Program in Dialectical Rough Set Theory Abstract: A dialectical rough set theory focussed on the relation between roughly\nequivalent objects and classical objects was introduced in \\cite{AM699} by the\npresent author. The focus of our investigation is on elucidating the minimal\nconditions on the nature of granularity, underlying semantic domain and nature\nof the general rough set theories (RST) involved for possible extension of the\nsemantics to more general RST on a paradigm. On this basis we also formulate a\nprogram in dialectical rough set theory. The dialectical approach provides\nbetter semantics in many difficult cases and helps in formalising a wide\nvariety of concepts and notions that remain untamed at meta levels in the usual\napproaches. This is a brief version of a more detailed forthcoming paper by the\npresent author. \n\n"}
{"id": "0910.0575", "contents": "Title: A Note on Functional Averages over Gaussian Ensembles Abstract: In this work we find a new formula for matrix averages over the Gaussian\nensemble. Let ${\\bf H}$ be an $n\\times n$ Gaussian random matrix with complex,\nindependent, and identically distributed entries of zero mean and unit\nvariance. Given an $n\\times n$ positive definite matrix ${\\bf A}$, and a\ncontinuous function $f:\\R^{+}\\to\\R$ such that $\\int_{0}^{\\infty}{e^{-\\alpha\nt}|f(t)|^2\\,dt}<\\infty$ for every $\\alpha>0$, we find a new formula for the\nexpectation $\\E[\\mathrm{Tr}(f({\\bf HAH^{*}}))]$. Taking $f(x)=\\log(1+x)$ gives\nanother formula for the capacity of the MIMO communication channel, and taking\n$f(x)=(1+x)^{-1}$ gives the MMSE achieved by a linear receiver. \n\n"}
{"id": "0910.1300", "contents": "Title: D-MG Tradeoff of DF and AF Relaying Protocols over Asynchronous PAM\n  Cooperative Networks Abstract: The diversity multiplexing tradeoff of a general two-hop asynchronous\ncooperative network is examined for various relaying protocols such as\nnon-orthogonal selection decode-and-forward (NSDF), orthogonal selection\ndecode-and-forward (OSDF), non-orthogonal amplify-and-forward (NAF), and\northogonal amplify-and-forward (OAF). The transmitter nodes are assumed to send\npulse amplitude modulation (PAM) signals asynchronously, in which information\nsymbols are linearly modulated by a shaping waveform to be sent to the\ndestination. We consider two different cases with respect to the length of the\nshaping waveforms in the time domain. In the theoretical case where the shaping\nwaveforms with infinite time support are used, it is shown that asynchronism\ndoes not affect the DMT performance of the system and the same DMT as that of\nthe corresponding synchronous network is obtained for all the aforementioned\nprotocols. In the practical case where finite length shaping waveforms are\nused, it is shown that better diversity gains can be achieved at the expense of\nbandwidth expansion. In the decode-and-forward (DF) type protocols, the\nasynchronous network provides better diversity gains than those of the\ncorresponding synchronous network throughout the range of the multiplexing\ngain. In the amplify-and-forward (AF) type protocols, the asynchronous network\nprovides the same DMT as that of the corresponding synchronous counterpart\nunder the OAF protocol; however, a better diversity gain is achieved under the\nNAF protocol throughout the range of the multiplexing gain. In particular, in\nthe single relay asynchronous network, the NAF protocol provides the same DMT\nas that of the 2 {\\times} 1 multiple-input single-output (MISO) channel. \n\n"}
{"id": "0911.4167", "contents": "Title: Wyner-Ziv Coding over Broadcast Channels: Digital Schemes Abstract: This paper addresses lossy transmission of a common source over a broadcast\nchannel when there is correlated side information at the receivers, with\nemphasis on the quadratic Gaussian and binary Hamming cases. A digital scheme\nthat combines ideas from the lossless version of the problem, i.e.,\nSlepian-Wolf coding over broadcast channels, and dirty paper coding, is\npresented and analyzed. This scheme uses layered coding where the common layer\ninformation is intended for both receivers and the refinement information is\ndestined only for one receiver. For the quadratic Gaussian case, a quantity\ncharacterizing the overall quality of each receiver is identified in terms of\nchannel and side information parameters. It is shown that it is more\nadvantageous to send the refinement information to the receiver with \"better\"\noverall quality. In the case where all receivers have the same overall quality,\nthe presented scheme becomes optimal. Unlike its lossless counterpart, however,\nthe problem eludes a complete characterization. \n\n"}
{"id": "0911.5106", "contents": "Title: A conversion between utility and information Abstract: Rewards typically express desirabilities or preferences over a set of\nalternatives. Here we propose that rewards can be defined for any probability\ndistribution based on three desiderata, namely that rewards should be\nreal-valued, additive and order-preserving, where the latter implies that more\nprobable events should also be more desirable. Our main result states that\nrewards are then uniquely determined by the negative information content. To\nanalyze stochastic processes, we define the utility of a realization as its\nreward rate. Under this interpretation, we show that the expected utility of a\nstochastic process is its negative entropy rate. Furthermore, we apply our\nresults to analyze agent-environment interactions. We show that the expected\nutility that will actually be achieved by the agent is given by the negative\ncross-entropy from the input-output (I/O) distribution of the coupled\ninteraction system and the agent's I/O distribution. Thus, our results allow\nfor an information-theoretic interpretation of the notion of utility and the\ncharacterization of agent-environment interactions in terms of entropy\ndynamics. \n\n"}
{"id": "0912.4995", "contents": "Title: 1-State Error-Trellis Decoding of LDPC Convolutional Codes Based on\n  Circulant Matrices Abstract: We consider the decoding of convolutional codes using an error trellis\nconstructed based on a submatrix of a given check matrix. In the proposed\nmethod, the syndrome-subsequence computed using the remaining submatrix is\nutilized as auxiliary information for decoding. Then the ML error path is\ncorrectly decoded using the degenerate error trellis. We also show that the\ndecoding complexity of the proposed method is basically identical with that of\nthe conventional one based on the original error trellis. Next, we apply the\nmethod to check matrices with monomial entries proposed by Tanner et al. By\nchoosing any row of the check matrix as the submatrix for error-trellis\nconstruction, a 1-state error trellis is obtained. Noting the fact that a\nlikelihood-concentration on the all-zero state and the states with many 0's\noccurs in the error trellis, we present a simplified decoding method based on a\n1-state error trellis, from which decoding-complexity reduction is realized. \n\n"}
{"id": "1003.5819", "contents": "Title: A unified controllability/observability theory for some stochastic and\n  deterministic partial differential equations Abstract: The purpose of this paper is to present a universal approach to the study of\ncontrollability/observability problems for infinite dimensional systems\ngoverned by some stochastic/deterministic partial differential equations. The\ncrucial analytic tool is a class of fundamental weighted identities for\nstochastic/deterministic partial differential operators, via which one can\nderive the desired global Carleman estimates. This method can also give a\nunified treatment of the stabilization, global unique continuation, and inverse\nproblems for some stochastic/deterministic partial differential equations. \n\n"}
{"id": "1004.5070", "contents": "Title: Multichannel Sampling of Pulse Streams at the Rate of Innovation Abstract: We consider minimal-rate sampling schemes for infinite streams of delayed and\nweighted versions of a known pulse shape. The minimal sampling rate for these\nparametric signals is referred to as the rate of innovation and is equal to the\nnumber of degrees of freedom per unit time. Although sampling of infinite pulse\nstreams was treated in previous works, either the rate of innovation was not\nachieved, or the pulse shape was limited to Diracs. In this paper we propose a\nmultichannel architecture for sampling pulse streams with arbitrary shape,\noperating at the rate of innovation. Our approach is based on modulating the\ninput signal with a set of properly chosen waveforms, followed by a bank of\nintegrators. This architecture is motivated by recent work on sub-Nyquist\nsampling of multiband signals. We show that the pulse stream can be recovered\nfrom the proposed minimal-rate samples using standard tools taken from spectral\nestimation in a stable way even at high rates of innovation. In addition, we\naddress practical implementation issues, such as reduction of hardware\ncomplexity and immunity to failure in the sampling channels. The resulting\nscheme is flexible and exhibits better noise robustness than previous\napproaches. \n\n"}
{"id": "1005.5591", "contents": "Title: On the minimum weight problem of permutation codes under Chebyshev\n  distance Abstract: Permutation codes of length $n$ and distance $d$ is a set of permutations on\n$n$ symbols, where the distance between any two elements in the set is at least\n$d$. Subgroup permutation codes are permutation codes with the property that\nthe elements are closed under the operation of composition. In this paper,\nunder the distance metric $\\ell_{\\infty}$-norm, we prove that finding the\nminimum weight codeword for subgroup permutation code is NP-complete. Moreover,\nwe show that it is NP-hard to approximate the minimum weight within the factor\n$7/6-\\epsilon$ for any $\\epsilon>0$. \n\n"}
{"id": "1006.4895", "contents": "Title: On the complexity of nonlinear mixed-integer optimization Abstract: This is a survey on the computational complexity of nonlinear mixed-integer\noptimization. It highlights a selection of important topics, ranging from\nincomputability results that arise from number theory and logic, to recently\nobtained fully polynomial time approximation schemes in fixed dimension, and to\nstrongly polynomial-time algorithms for special cases. \n\n"}
{"id": "1007.0743", "contents": "Title: Fractional variational calculus in terms of a combined Caputo derivative Abstract: We generalize the fractional Caputo derivative to the fractional derivative\n${^CD^{\\alpha,\\beta}_{\\gamma}}$, which is a convex combination of the left\nCaputo fractional derivative of order $\\alpha$ and the right Caputo fractional\nderivative of order $\\beta$. The fractional variational problems under our\nconsideration are formulated in terms of ${^CD^{\\alpha,\\beta}_{\\gamma}}$. The\nEuler-Lagrange equations for the basic and isoperimetric problems, as well as\ntransversality conditions, are proved. \n\n"}
{"id": "1008.3705", "contents": "Title: Techniques for Enhanced Physical-Layer Security Abstract: Information-theoretic security--widely accepted as the strictest notion of\nsecurity--relies on channel coding techniques that exploit the inherent\nrandomness of propagation channels to strengthen the security of communications\nsystems. Within this paradigm, we explore strategies to improve secure\nconnectivity in a wireless network. We first consider the intrinsically secure\ncommunications graph (iS-graph), a convenient representation of the links that\ncan be established with information-theoretic security on a large-scale\nnetwork. We then propose and characterize two techniques--sectorized\ntransmission and eavesdropper neutralization--which are shown to dramatically\nenhance the connectivity of the iS-graph. \n\n"}
{"id": "1012.1908", "contents": "Title: NP-hardness of Deciding Convexity of Quartic Polynomials and Related\n  Problems Abstract: We show that unless P=NP, there exists no polynomial time (or even\npseudo-polynomial time) algorithm that can decide whether a multivariate\npolynomial of degree four (or higher even degree) is globally convex. This\nsolves a problem that has been open since 1992 when N. Z. Shor asked for the\ncomplexity of deciding convexity for quartic polynomials. We also prove that\ndeciding strict convexity, strong convexity, quasiconvexity, and\npseudoconvexity of polynomials of even degree four or higher is strongly\nNP-hard. By contrast, we show that quasiconvexity and pseudoconvexity of odd\ndegree polynomials can be decided in polynomial time. \n\n"}
{"id": "1103.5258", "contents": "Title: Controllability of rolling without twisting or slipping in higher\n  dimensions Abstract: We describe how the dynamical system of rolling two $n$-dimensional\nconnected, oriented Riemannian manifolds $M$ and $\\hat M$ without twisting or\nslipping, can be lifted to a nonholonomic system of elements in the product of\nthe oriented orthonormal frame bundles belonging to the manifolds. By\nconsidering the lifted problem and using properties of the elements in the\nrespective principal Ehresmann connections, we obtain sufficient conditions for\nthe local controllability of the system in terms of the curvature tensors and\nthe sectional curvatures of the manifolds involved. We also give some results\nfor the particular cases when $M$ and $\\hat M$ are locally symmetric or\ncomplete. \n\n"}
{"id": "1106.5413", "contents": "Title: Accelerated Linearized Bregman Method Abstract: In this paper, we propose and analyze an accelerated linearized Bregman (ALB)\nmethod for solving the basis pursuit and related sparse optimization problems.\nThis accelerated algorithm is based on the fact that the linearized Bregman\n(LB) algorithm is equivalent to a gradient descent method applied to a certain\ndual formulation. We show that the LB method requires $O(1/\\epsilon)$\niterations to obtain an $\\epsilon$-optimal solution and the ALB algorithm\nreduces this iteration complexity to $O(1/\\sqrt{\\epsilon})$ while requiring\nalmost the same computational effort on each iteration. Numerical results on\ncompressed sensing and matrix completion problems are presented that\ndemonstrate that the ALB method can be significantly faster than the LB method. \n\n"}
{"id": "1107.2353", "contents": "Title: Blending Bayesian and frequentist methods according to the precision of\n  prior information with an application to hypothesis testing Abstract: The following zero-sum game between nature and a statistician blends Bayesian\nmethods with frequentist methods such as p-values and confidence intervals.\nNature chooses a posterior distribution consistent with a set of possible\npriors. At the same time, the statistician selects a parameter distribution for\ninference with the goal of maximizing the minimum Kullback-Leibler information\ngained over a confidence distribution or other benchmark distribution. An\napplication to testing a simple null hypothesis leads the statistician to\nreport a posterior probability of the hypothesis that is informed by both\nBayesian and frequentist methodology, each weighted according how well the\nprior is known.\n  Since neither the Bayesian approach nor the frequentist approach is entirely\nsatisfactory in situations involving partial knowledge of the prior\ndistribution, the proposed procedure reduces to a Bayesian method given\ncomplete knowledge of the prior, to a frequentist method given complete\nignorance about the prior, and to a blend between the two methods given partial\nknowledge of the prior. The blended approach resembles the Bayesian method\nrather than the frequentist method to the precise extent that the prior is\nknown.\n  The problem of testing a point null hypothesis illustrates the proposed\nframework. The blended probability that the null hypothesis is true is equal to\nthe p-value or a lower bound of an unknown Bayesian posterior probability,\nwhichever is greater. Thus, given total ignorance represented by a lower bound\nof 0, the p-value is used instead of any Bayesian posterior probability. At the\nopposite extreme of a known prior, the p-value is ignored. In the intermediate\ncase, the possible Bayesian posterior probability that is closest to the\np-value is used for inference. Thus, both the Bayesian method and the\nfrequentist method influence the inferences made. \n\n"}
{"id": "1107.4652", "contents": "Title: On the Achievability of Interference Alignment for Three-Cell Constant\n  Cellular Interfering Networks Abstract: For a three-cell constant cellular interfering network, a new property of\nalignment is identified, i.e., interference alignment (IA) solution obtained in\nan user-cooperation scenario can also be applied in a non-cooperation\nenvironment. By using this property, an algorithm is proposed by jointly\ndesigning transmit and receive beamforming matrices. Analysis and numerical\nresults show that more degree of freedom (DoF) can be achieved compared with\nconventional schemes in most cases. \n\n"}
{"id": "1109.2088", "contents": "Title: Online Learning Algorithms for Stochastic Water-Filling Abstract: Water-filling is the term for the classic solution to the problem of\nallocating constrained power to a set of parallel channels to maximize the\ntotal data-rate. It is used widely in practice, for example, for power\nallocation to sub-carriers in multi-user OFDM systems such as WiMax. The\nclassic water-filling algorithm is deterministic and requires perfect knowledge\nof the channel gain to noise ratios. In this paper we consider how to do power\nallocation over stochastically time-varying (i.i.d.) channels with unknown gain\nto noise ratio distributions. We adopt an online learning framework based on\nstochastic multi-armed bandits. We consider two variations of the problem, one\nin which the goal is to find a power allocation to maximize $\\sum\\limits_i\n\\mathbb{E}[\\log(1 + SNR_i)]$, and another in which the goal is to find a power\nallocation to maximize $\\sum\\limits_i \\log(1 + \\mathbb{E}[SNR_i])$. For the\nfirst problem, we propose a \\emph{cognitive water-filling} algorithm that we\ncall CWF1. We show that CWF1 obtains a regret (defined as the cumulative gap\nover time between the sum-rate obtained by a distribution-aware genie and this\npolicy) that grows polynomially in the number of channels and logarithmically\nin time, implying that it asymptotically achieves the optimal time-averaged\nrate that can be obtained when the gain distributions are known. For the second\nproblem, we present an algorithm called CWF2, which is, to our knowledge, the\nfirst algorithm in the literature on stochastic multi-armed bandits to exploit\nnon-linear dependencies between the arms. We prove that the number of times\nCWF2 picks the incorrect power allocation is bounded by a function that is\npolynomial in the number of channels and logarithmic in time, implying that its\nfrequency of incorrect allocation tends to zero. \n\n"}
{"id": "1109.4909", "contents": "Title: Sparse Online Low-Rank Projection and Outlier Rejection (SOLO) for 3-D\n  Rigid-Body Motion Registration Abstract: Motivated by an emerging theory of robust low-rank matrix representation, in\nthis paper, we introduce a novel solution for online rigid-body motion\nregistration. The goal is to develop algorithmic techniques that enable a\nrobust, real-time motion registration solution suitable for low-cost, portable\n3-D camera devices. Assuming 3-D image features are tracked via a standard\ntracker, the algorithm first utilizes Robust PCA to initialize a low-rank shape\nrepresentation of the rigid body. Robust PCA finds the global optimal solution\nof the initialization, while its complexity is comparable to singular value\ndecomposition. In the online update stage, we propose a more efficient\nalgorithm for sparse subspace projection to sequentially project new feature\nobservations onto the shape subspace. The lightweight update stage guarantees\nthe real-time performance of the solution while maintaining good registration\neven when the image sequence is contaminated by noise, gross data corruption,\noutlying features, and missing data. The state-of-the-art accuracy of the\nsolution is validated through extensive simulation and a real-world experiment,\nwhile the system enjoys one to two orders of magnitude speed-up compared to\nwell-established RANSAC solutions. The new algorithm will be released online to\naid peer evaluation. \n\n"}
{"id": "1109.5346", "contents": "Title: Polar codes for degradable quantum channels Abstract: Channel polarization is a phenomenon in which a particular recursive encoding\ninduces a set of synthesized channels from many instances of a memoryless\nchannel, such that a fraction of the synthesized channels becomes near perfect\nfor data transmission and the other fraction becomes near useless for this\ntask. Mahdavifar and Vardy have recently exploited this phenomenon to construct\ncodes that achieve the symmetric private capacity for private data transmission\nover a degraded wiretap channel. In the current paper, we build on their work\nand demonstrate how to construct quantum wiretap polar codes that achieve the\nsymmetric private capacity of a degraded quantum wiretap channel with a\nclassical eavesdropper. Due to the Schumacher-Westmoreland correspondence\nbetween quantum privacy and quantum coherence, we can construct quantum polar\ncodes by operating these quantum wiretap polar codes in superposition, much\nlike Devetak's technique for demonstrating the achievability of the coherent\ninformation rate for quantum data transmission. Our scheme achieves the\nsymmetric coherent information rate for quantum channels that are degradable\nwith a classical environment. This condition on the environment may seem\nrestrictive, but we show that many quantum channels satisfy this criterion,\nincluding amplitude damping channels, photon-detected jump channels, dephasing\nchannels, erasure channels, and cloning channels. Our quantum polar coding\nscheme has the desirable properties of being channel-adapted and symmetric\ncapacity-achieving along with having an efficient encoder, but we have not\ndemonstrated that the decoding is efficient. Also, the scheme may require\nentanglement assistance, but we show that the rate of entanglement consumption\nvanishes in the limit of large blocklength if the channel is degradable with\nclassical environment. \n\n"}
{"id": "1110.6487", "contents": "Title: On the Feedback Capacity of the Fully Connected $K$-User Interference\n  Channel Abstract: The symmetric K user interference channel with fully connected topology is\nconsidered, in which (a) each receiver suffers interference from all other\n(K-1) transmitters, and (b) each transmitter has causal and noiseless feedback\nfrom its respective receiver. The number of generalized degrees of freedom\n(GDoF) is characterized in terms of \\alpha, where the interference-to-noise\nratio (INR) is given by INR=SNR^\\alpha. It is shown that the per-user GDoF of\nthis network is the same as that of the 2-user interference channel with\nfeedback, except for \\alpha=1, for which existence of feedback does not help in\nterms of GDoF. The coding scheme proposed for this network, termed cooperative\ninterference alignment, is based on two key ingredients, namely, interference\nalignment and interference decoding. Moreover, an approximate characterization\nis provided for the symmetric feedback capacity of the network, when the SNR\nand INR are far apart from each other. \n\n"}
{"id": "1110.6916", "contents": "Title: Multi-Terminal Source Coding With Action Dependent Side Information Abstract: We consider multi-terminal source coding with a single encoder and multiple\ndecoders where either the encoder or the decoders can take cost constrained\nactions which affect the quality of the side information present at the\ndecoders. For the scenario where decoders take actions, we characterize the\nrate-cost trade-off region for lossless source coding, and give an\nachievability scheme for lossy source coding for two decoders which is optimum\nfor a variety of special cases of interest. For the case where the encoder\ntakes actions, we characterize the rate-cost trade-off for a class of lossless\nsource coding scenarios with multiple decoders. Finally, we also consider\nextensions to other multi-terminal source coding settings with actions, and\ncharacterize the rate -distortion-cost tradeoff for a case of successive\nrefinement with actions. \n\n"}
{"id": "1111.1555", "contents": "Title: A scheme to protect against multiple quantum erasures Abstract: We present a scheme able to protect k >= 3 qubits of information against the\noccurrence of multiple erasures, based on the code proposed by Yang et al.\n(2004 JETP Letters 79 236). In this scheme redundant blocks are used and we\nrestrict to the case that each erasure must occur in distinct blocks. We\nexplicitly characterize the encoding operation and the restoring operation\nrequired to implement this scheme. The operators used in these operations can\nbe adjusted to construct different quantum erasure-correcting codes. A special\nfeature of this scheme is that no measurement is required. To illustrate our\nscheme, we present an example in which five-qubits of information are protected\nagainst the occurrence of two erasures. \n\n"}
{"id": "1112.2603", "contents": "Title: Probabilistic analysis of the Grassmann condition number Abstract: We analyze the probability that a random m-dimensional linear subspace of R^n\nboth intersects a regular closed convex cone C\\subseteq R^n and lies within\ndistance \\alpha of an m-dimensional subspace not intersecting C (except at the\norigin). The result is expressed in terms of the spherical intrinsic volumes of\nthe cone C. This allows us to perform an average analysis of the Grassmann\ncondition number \\C(A) for the homogeneous convex feasibility problem \\exists\nx\\in C\\setminus 0 : Ax=0. The Grassmann condition number is a geometric version\nof Renegar's condition number, that we have introduced recently in [SIOPT\n22(3):1029-1041, 2012]. We thus give the first average analysis of convex\nprogramming that is not restricted to linear programming. In particular, we\nprove that if the entries of A\\in R^{m\\times n} are chosen i.i.d. standard\nnormal, then for any regular cone C, we have E[ln\\C(A)]<1.5 ln(n)+1.5. The\nproofs rely on various techniques from Riemannian geometry applied to Grassmann\nmanifolds. \n\n"}
{"id": "1112.3208", "contents": "Title: Practical Methods for Wireless Network Coding with Multiple Unicast\n  Transmissions Abstract: We propose a simple yet effective wireless network coding and decoding\ntechnique for a multiple unicast network. It utilizes spatial diversity through\ncooperation between nodes which carry out distributed encoding operations\ndictated by generator matrices of linear block codes. In order to exemplify the\ntechnique, we make use of greedy codes over the binary field and show that the\narbitrary diversity orders can be flexibly assigned to nodes. Furthermore, we\npresent the optimal detection rule for the given model that accounts for\nintermediate node errors and suggest a low-complexity network decoder using the\nsum-product (SP) algorithm. The proposed SP detector exhibits near optimal\nperformance. We also show asymptotic superiority of network coding over a\nmethod that utilizes the wireless channel in a repetitive manner without\nnetwork coding (NC) and give related rate-diversity trade-off curves. Finally,\nwe extend the given encoding method through selective encoding in order to\nobtain extra coding gains. \n\n"}
{"id": "1201.0737", "contents": "Title: Spectrum Sensing in the Presence of Multiple Primary Users Abstract: We consider multi-antenna cooperative spectrum sensing in cognitive radio\nnetworks, when there may be multiple primary users. A detector based on the\nspherical test is analyzed in such a scenario. Based on the moments of the\ndistributions involved, simple and accurate analytical formulae for the key\nperformance metrics of the detector are derived. The false alarm and the\ndetection probabilities, as well as the detection threshold and Receiver\nOperation Characteristics are available in closed form. Simulations are\nprovided to verify the accuracy of the derived results, and to compare with\nother detectors in realistic sensing scenarios. \n\n"}
{"id": "1201.2523", "contents": "Title: At Low SNR Asymmetric Quantizers Are Better Abstract: We study the capacity of the discrete-time Gaussian channel when its output\nis quantized with a one-bit quantizer. We focus on the low signal-to-noise\nratio (SNR) regime, where communication at very low spectral efficiencies takes\nplace. In this regime a symmetric threshold quantizer is known to reduce\nchannel capacity by a factor of 2/pi, i.e., to cause an asymptotic power loss\nof approximately two decibels. Here it is shown that this power loss can be\navoided by using asymmetric threshold quantizers and asymmetric signaling\nconstellations. To avoid this power loss, flash-signaling input distributions\nare essential. Consequently, one-bit output quantization of the Gaussian\nchannel reduces spectral efficiency. Threshold quantizers are not only\nasymptotically optimal: at every fixed SNR a threshold quantizer maximizes\ncapacity among all one-bit output quantizers. The picture changes on the\nRayleigh-fading channel. In the noncoherent case a one-bit output quantizer\ncauses an unavoidable low-SNR asymptotic power loss. In the coherent case,\nhowever, this power loss is avoidable provided that we allow the quantizer to\ndepend on the fading level. \n\n"}
{"id": "1202.0854", "contents": "Title: Reverse Compute and Forward: A Low-Complexity Architecture for Downlink\n  Distributed Antenna Systems Abstract: We consider a distributed antenna system where $L$ antenna terminals (ATs)\nare connected to a Central Processor (CP) via digital error-free links of\nfinite capacity $R_0$, and serve $L$ user terminals (UTs). This system model\nhas been widely investigated both for the uplink and the downlink, which are\ninstances of the general multiple-access relay and broadcast relay networks. In\nthis work we focus on the downlink, and propose a novel downlink precoding\nscheme nicknamed \"Reverse Quantized Compute and Forward\" (RQCoF). For this\nscheme we obtain achievable rates and compare with the state of the art\navailable in the literature. We also provide simulation results for a realistic\nnetwork with fading and pathloss with $K > L$ UTs, and show that channel-based\nuser selection produces large benefits and essentially removes the problem of\nrank deficiency in the system matrix. \n\n"}
{"id": "1202.2113", "contents": "Title: Decentralized Delay Optimal Control for Interference Networks with\n  Limited Renewable Energy Storage Abstract: In this paper, we consider delay minimization for interference networks with\nrenewable energy source, where the transmission power of a node comes from both\nthe conventional utility power (AC power) and the renewable energy source. We\nassume the transmission power of each node is a function of the local channel\nstate, local data queue state and local energy queue state only. In turn, we\nconsider two delay optimization formulations, namely the decentralized\npartially observable Markov decision process (DEC-POMDP) and Non-cooperative\npartially observable stochastic game (POSG). In DEC-POMDP formulation, we\nderive a decentralized online learning algorithm to determine the control\nactions and Lagrangian multipliers (LMs) simultaneously, based on the policy\ngradient approach. Under some mild technical conditions, the proposed\ndecentralized policy gradient algorithm converges almost surely to a local\noptimal solution. On the other hand, in the non-cooperative POSG formulation,\nthe transmitter nodes are non-cooperative. We extend the decentralized policy\ngradient solution and establish the technical proof for almost-sure convergence\nof the learning algorithms. In both cases, the solutions are very robust to\nmodel variations. Finally, the delay performance of the proposed solutions are\ncompared with conventional baseline schemes for interference networks and it is\nillustrated that substantial delay performance gain and energy savings can be\nachieved. \n\n"}
{"id": "1202.3531", "contents": "Title: Recovering Jointly Sparse Signals via Joint Basis Pursuit Abstract: This work considers recovery of signals that are sparse over two bases. For\ninstance, a signal might be sparse in both time and frequency, or a matrix can\nbe low rank and sparse simultaneously. To facilitate recovery, we consider\nminimizing the sum of the $\\ell_1$-norms that correspond to each basis, which\nis a tractable convex approach. We find novel optimality conditions which\nindicates a gain over traditional approaches where $\\ell_1$ minimization is\ndone over only one basis. Next, we analyze these optimality conditions for the\nparticular case of time-frequency bases. Denoting sparsity in the first and\nsecond bases by $k_1,k_2$ respectively, we show that, for a general class of\nsignals, using this approach, one requires as small as\n$O(\\max\\{k_1,k_2\\}\\log\\log n)$ measurements for successful recovery hence\novercoming the classical requirement of\n$\\Theta(\\min\\{k_1,k_2\\}\\log(\\frac{n}{\\min\\{k_1,k_2\\}}))$ for $\\ell_1$\nminimization when $k_1\\approx k_2$. Extensive simulations show that, our\nanalysis is approximately tight. \n\n"}
{"id": "1202.4098", "contents": "Title: Energy-Efficient Sensing and Communication of Parallel Gaussian Sources Abstract: Energy efficiency is a key requirement in the design of wireless sensor\nnetworks. While most theoretical studies only account for the energy\nrequirements of communication, the sensing process, which includes measurements\nand compression, can also consume comparable energy. In this paper, the problem\nof sensing and communicating parallel sources is studied by accounting for the\ncost of both communication and sensing. In the first formulation of the\nproblem, the sensor has a separate energy budget for sensing and a rate budget\nfor communication, while, in the second, it has a single energy budget for both\ntasks. Assuming that sources with larger variances have lower sensing costs,\nthe optimal allocation of sensing energy and rate that minimizes the overall\ndistortion is derived for the first problem. Moreover, structural results on\nthe solution of the second problem are derived under the assumption that the\nsources with larger variances are transmitted on channels with lower noise.\nClosed-form solutions are also obtained for the case where the energy budget is\nsufficiently large. For an arbitrary order on the variances and costs, the\noptimal solution to the first problem is also obtained numerically and compared\nwith several suboptimal strategies. \n\n"}
{"id": "1203.0781", "contents": "Title: Posterior Mean Super-Resolution with a Compound Gaussian Markov Random\n  Field Prior Abstract: This manuscript proposes a posterior mean (PM) super-resolution (SR) method\nwith a compound Gaussian Markov random field (MRF) prior. SR is a technique to\nestimate a spatially high-resolution image from observed multiple\nlow-resolution images. A compound Gaussian MRF model provides a preferable\nprior for natural images that preserves edges. PM is the optimal estimator for\nthe objective function of peak signal-to-noise ratio (PSNR). This estimator is\nnumerically determined by using variational Bayes (VB). We then solve the\nconjugate prior problem on VB and the exponential-order calculation cost\nproblem of a compound Gaussian MRF prior with simple Taylor approximations. In\nexperiments, the proposed method roughly overcomes existing methods. \n\n"}
{"id": "1204.0820", "contents": "Title: Tail-Constraining Stochastic Linear-Quadratic Control: Large Deviation\n  and Statistical Physics Approach Abstract: Standard definition of the stochastic Risk-Sensitive Linear-Quadratic (RS-LQ)\ncontrol depends on the risk parameter, which is normally left to be set\nexogenously. We reconsider the classical approach and suggest two alternatives\nresolving the spurious freedom naturally. One approach consists in seeking for\nthe minimum of the tail of the Probability Distribution Function (PDF) of the\ncost functional at some large fixed value. Another option suggests to minimize\nthe expectation value of the cost functional under constraint on the value of\nthe PDF tail. Under assumption of the resulting control stability, both\nproblems are reduced to static optimizations over stationary control matrix.\nThe solutions are illustrated on the examples of scalar and 1d chain (string)\nsystems. Large Deviation self-similar asymptotic of the cost functional PDF is\nanalyzed. \n\n"}
{"id": "1205.4168", "contents": "Title: Approximate Feedback Capacity of the Gaussian Multicast Channel Abstract: We characterize the capacity region to within log{2(M-1)} bits/s/Hz for the\nM-transmitter K-receiver Gaussian multicast channel with feedback where each\nreceiver wishes to decode every message from the M transmitters. Extending\nCover-Leung's achievable scheme intended for (M,K)=(2,1), we show that this\ngeneralized scheme achieves the cutset-based outer bound within log{2(M-1)}\nbits per transmitter for all channel parameters. In contrast to the capacity in\nthe non-feedback case, the feedback capacity improves upon the naive\nintersection of the feedback capacities of K individual multiple access\nchannels. We find that feedback provides unbounded multiplicative gain at high\nsignal-to-noise ratios as was shown in the Gaussian interference channel. To\ncomplement the results, we establish the exact feedback capacity of the\nAvestimehr-Diggavi-Tse (ADT) deterministic model, from which we make the\nobservation that feedback can also be beneficial for function computation. \n\n"}
{"id": "1206.0823", "contents": "Title: Orthogonal Matching Pursuit with Noisy and Missing Data: Low and High\n  Dimensional Results Abstract: Many models for sparse regression typically assume that the covariates are\nknown completely, and without noise. Particularly in high-dimensional\napplications, this is often not the case. This paper develops efficient\nOMP-like algorithms to deal with precisely this setting. Our algorithms are as\nefficient as OMP, and improve on the best-known results for missing and noisy\ndata in regression, both in the high-dimensional setting where we seek to\nrecover a sparse vector from only a few measurements, and in the classical\nlow-dimensional setting where we recover an unstructured regressor. In the\nhigh-dimensional setting, our support-recovery algorithm requires no knowledge\nof even the statistics of the noise. Along the way, we also obtain improved\nperformance guarantees for OMP for the standard sparse regression problem with\nGaussian noise. \n\n"}
{"id": "1206.4229", "contents": "Title: Information field dynamics for simulation scheme construction Abstract: Information field dynamics (IFD) is introduced here as a framework to derive\nnumerical schemes for the simulation of physical and other fields without\nassuming a particular sub-grid structure as many schemes do. IFD constructs an\nensemble of non-parametric sub-grid field configurations from the combination\nof the data in computer memory, representing constraints on possible field\nconfigurations, and prior assumptions on the sub-grid field statistics. Each of\nthese field configurations can formally be evolved to a later moment since any\ndifferential operator of the dynamics can act on fields living in continuous\nspace. However, these virtually evolved fields need again a representation by\ndata in computer memory. The maximum entropy principle of information theory\nguides the construction of updated datasets via entropic matching, optimally\nrepresenting these field configurations at the later time. The field dynamics\nthereby become represented by a finite set of evolution equations for the data\nthat can be solved numerically. The sub-grid dynamics is treated within an\nauxiliary analytic consideration and the resulting scheme acts solely on the\ndata space. It should provide a more accurate description of the physical field\ndynamics than simulation schemes constructed ad-hoc, due to the more rigorous\naccounting of sub-grid physics and the space discretization process.\nAssimilation of measurement data into an IFD simulation is conceptually\nstraightforward since measurement and simulation data can just be merged. The\nIFD approach is illustrated using the example of a coarsely discretized\nrepresentation of a thermally excited classical Klein-Gordon field. This should\npave the way towards the construction of schemes for more complex systems like\nturbulent hydrodynamics. \n\n"}
{"id": "1206.5919", "contents": "Title: Performance Improvement of Iterative Multiuser Detection for Large\n  Sparsely-Spread CDMA Systems by Spatial Coupling Abstract: Kudekar et al. proved that the belief-propagation (BP) performance for\nlow-density parity check (LDPC) codes can be boosted up to the\nmaximum-a-posteriori (MAP) performance by spatial coupling. In this paper,\nspatial coupling is applied to sparsely-spread code-division multiple-access\n(CDMA) systems to improve the performance of iterative multiuser detection\nbased on BP. Two iterative receivers based on BP are considered: One receiver\nis based on exact BP and the other on an approximate BP with Gaussian\napproximation. The performance of the two BP receivers is evaluated via density\nevolution (DE) in the dense limit after taking the large-system limit, in which\nthe number of users and the spreading factor tend to infinity while their ratio\nis kept constant. The two BP receivers are shown to achieve the same\nperformance as each other in these limits. Furthermore, taking a continuum\nlimit for the obtained DE equations implies that the performance of the two BP\nreceivers can be improved up to the performance achieved by the symbol-wise MAP\ndetection, called individually-optimal detection, via spatial coupling.\nNumerical simulations show that spatial coupling can provide a significant\nimprovement in bit error rate for finite-sized systems especially in the region\nof high system loads. \n\n"}
{"id": "1207.1149", "contents": "Title: Graver basis and proximity techniques for block-structured separable\n  convex integer minimization problems Abstract: We consider N-fold 4-block decomposable integer programs, which\nsimultaneously generalize N-fold integer programs and two-stage stochastic\ninteger programs with N scenarios. In previous work [R. Hemmecke, M. Koeppe, R.\nWeismantel, A polynomial-time algorithm for optimizing over N-fold 4-block\ndecomposable integer programs, Proc. IPCO 2010, Lecture Notes in Computer\nScience, vol. 6080, Springer, 2010, pp. 219--229], it was proved that for fixed\nblocks but variable N, these integer programs are polynomial-time solvable for\nany linear objective. We extend this result to the minimization of separable\nconvex objective functions. Our algorithm combines Graver basis techniques with\na proximity result [D.S. Hochbaum and J.G. Shanthikumar, Convex separable\noptimization is not much harder than linear optimization, J. ACM 37 (1990),\n843--862], which allows us to use convex continuous optimization as a\nsubroutine. \n\n"}
{"id": "1207.5680", "contents": "Title: Ergodic BSDEs driven by Markov Chains Abstract: We consider ergodic backward stochastic differential equations, in a setting\nwhere noise is generated by a countable state uniformly ergodic Markov chain.\nWe show that for Lipschitz drivers such that a comparison theorem holds, these\nequations admit unique solutions. To obtain this result, we show by coupling\nand splitting techniques that uniform ergodicity estimates of Markov chains are\nrobust to perturbations of the rate matrix, and that these perturbations\ncorrespond in a natural way to EBSDEs. We then consider applications of this\ntheory to Markov decision problems with a risk-averse average reward criterion. \n\n"}
{"id": "1208.2346", "contents": "Title: On existence of Budaghyan-Carlet APN hexanomials Abstract: Budaghyan and Carlet constructed a family of almost perfect nonlinear (APN)\nhexanomials over a field with r^2 elements, and with terms of degrees r+1, s+1,\nrs+1, rs+r, rs+s, and r+s, where r = 2^m and s = 2^n with GCD(m,n)=1. The\nconstruction requires a technical condition, which was verified empirically in\na finite number of examples. Bracken, Tan, and Tan (arXiv:1110.3177 [cs.it])\nproved the condition holds when m = 2 or 4 (mod 6). In this article, we prove\nthat the construction of Budaghyan and Carlet produces APN polynomials for all\nm and n.\n  In the case where GCD(m,n) = k >= 1, Budaghyan and Carlet showed that the\nnonzero derivatives of the hexanomials are 2^k-to-one maps from F_{r^2} to\nF_{r^2}, provided the same technical condition holds. We prove their\nconstruction produces hexanomials with this differential property for all m and\nn. \n\n"}
{"id": "1208.4391", "contents": "Title: Shape Tracking With Occlusions via Coarse-To-Fine Region-Based Sobolev\n  Descent Abstract: We present a method to track the precise shape of an object in video based on\nnew modeling and optimization on a new Riemannian manifold of parameterized\nregions.\n  Joint dynamic shape and appearance models, in which a template of the object\nis propagated to match the object shape and radiance in the next frame, are\nadvantageous over methods employing global image statistics in cases of complex\nobject radiance and cluttered background. In cases of 3D object motion and\nviewpoint change, self-occlusions and dis-occlusions of the object are\nprominent, and current methods employing joint shape and appearance models are\nunable to adapt to new shape and appearance information, leading to inaccurate\nshape detection. In this work, we model self-occlusions and dis-occlusions in a\njoint shape and appearance tracking framework.\n  Self-occlusions and the warp to propagate the template are coupled, thus a\njoint problem is formulated. We derive a coarse-to-fine optimization scheme,\nadvantageous in object tracking, that initially perturbs the template by coarse\nperturbations before transitioning to finer-scale perturbations, traversing all\nscales, seamlessly and automatically. The scheme is a gradient descent on a\nnovel infinite-dimensional Riemannian manifold that we introduce. The manifold\nconsists of planar parameterized regions, and the metric that we introduce is a\nnovel Sobolev-type metric defined on infinitesimal vector fields on regions.\nThe metric has the property of resulting in a gradient descent that\nautomatically favors coarse-scale deformations (when they reduce the energy)\nbefore moving to finer-scale deformations.\n  Experiments on video exhibiting occlusion/dis-occlusion, complex radiance and\nbackground show that occlusion/dis-occlusion modeling leads to superior shape\naccuracy compared to recent methods employing joint shape/appearance models or\nemploying global statistics. \n\n"}
{"id": "1208.5365", "contents": "Title: A Missing and Found Recognition System for Hajj and Umrah Abstract: This note describes an integrated recognition system for identifying missing\nand found objects as well as missing, dead, and found people during Hajj and\nUmrah seasons in the two Holy cities of Makkah and Madina in the Kingdom of\nSaudi Arabia. It is assumed that the total estimated number of pilgrims will\nreach 20 millions during the next decade. The ultimate goal of this system is\nto integrate facial recognition and object identification solutions into the\nHajj and Umrah rituals. The missing and found computerized system is part of\nthe CrowdSensing system for Hajj and Umrah crowd estimation, management and\nsafety. \n\n"}
{"id": "1209.3318", "contents": "Title: Hessian Schatten-Norm Regularization for Linear Inverse Problems Abstract: We introduce a novel family of invariant, convex, and non-quadratic\nfunctionals that we employ to derive regularized solutions of ill-posed linear\ninverse imaging problems. The proposed regularizers involve the Schatten norms\nof the Hessian matrix, computed at every pixel of the image. They can be viewed\nas second-order extensions of the popular total-variation (TV) semi-norm since\nthey satisfy the same invariance properties. Meanwhile, by taking advantage of\nsecond-order derivatives, they avoid the staircase effect, a common artifact of\nTV-based reconstructions, and perform well for a wide range of applications. To\nsolve the corresponding optimization problems, we propose an algorithm that is\nbased on a primal-dual formulation. A fundamental ingredient of this algorithm\nis the projection of matrices onto Schatten norm balls of arbitrary radius.\nThis operation is performed efficiently based on a direct link we provide\nbetween vector projections onto $\\ell_q$ norm balls and matrix projections onto\nSchatten norm balls. Finally, we demonstrate the effectiveness of the proposed\nmethods through experimental results on several inverse imaging problems with\nreal and simulated data. \n\n"}
{"id": "1209.6605", "contents": "Title: Two Person Zero-sum Game in Weak Formulation and Path Dependent\n  Bellman-Isaacs Equation Abstract: In this paper we study a two person zero sum stochastic differential game in\nweak formulation. Unlike standard literature which uses strategy type of\ncontrols, the weak formulation allows us to consider the game with control\nagainst control. We shall prove the existence of game value under natural\nconditions. Another main feature of the paper is that we allow for\nnon-Markovian structure, and thus the game value is a random process. We\ncharacterize the value process as the unique viscosity solution of the\ncorresponding path dependent Bellman-Isaacs equation, a notion recently\nintroduced by Ekren, Keller, Touzi and Zhang (arXiv:1109.5971) and Ekren, Touzi\nand Zhang. \n\n"}
{"id": "1210.5058", "contents": "Title: Properties of Persistent Mutual Information and Emergence Abstract: The persistent mutual information (PMI) is a complexity measure for\nstochastic processes. It is related to well-known complexity measures like\nexcess entropy or statistical complexity. Essentially it is a variation of the\nexcess entropy so that it can be interpreted as a specific measure of system\ninternal memory. The PMI was first introduced in 2010 by Ball, Diakonova and\nMacKay as a measure for (strong) emergence. In this paper we define the PMI\nmathematically and investigate the relation to excess entropy and statistical\ncomplexity. In particular we prove that the excess entropy is an upper bound of\nthe PMI. Furthermore we show some properties of the PMI and calculate it\nexplicitly for some example processes. We also discuss to what extend it is a\nmeasure for emergence and compare it with alternative approaches used to\nformalize emergence. \n\n"}
{"id": "1210.5940", "contents": "Title: Properties of perfect transitive binary codes of length 15 and extended\n  perfect transitive binary codes of length 16 Abstract: Some properties of perfect transitive binary codes of length 15 and extended\nperfect transitive binary codes of length 16 are presented for reference\npurposes. \n\n"}
{"id": "1211.3189", "contents": "Title: A characterization of two-weight projective cyclic codes Abstract: We give necessary conditions for a two-weight projective cyclic code to be\nthe direct sum of two one-weight irreducible cyclic subcodes of the same\ndimension, following the work of Wolfmann and Vega. This confirms Vega's\nconjecture that all the two-weight cyclic codes of this type are the known ones\nin the projective case. \n\n"}
{"id": "1211.6643", "contents": "Title: A Graph-Theoretical Approach for the Analysis and Model Reduction of\n  Complex-Balanced Chemical Reaction Networks Abstract: In this paper we derive a compact mathematical formulation describing the\ndynamics of chemical reaction networks that are complex-balanced and are\ngoverned by mass action kinetics. The formulation is based on the graph of\n(substrate and product) complexes and the stoichiometric information of these\ncomplexes, and crucially uses a balanced weighted Laplacian matrix. It is shown\nthat this formulation leads to elegant methods for characterizing the space of\nall equilibria for complex-balanced networks and for deriving stability\nproperties of such networks. We propose a method for model reduction of\ncomplex-balanced networks, which is similar to the Kron reduction method for\nelectrical networks and involves the computation of Schur complements of the\nbalanced weighted Laplacian matrix. \n\n"}
{"id": "1212.1269", "contents": "Title: Approximate Dynamic Programming via Sum of Squares Programming Abstract: We describe an approximate dynamic programming method for stochastic control\nproblems on infinite state and input spaces. The optimal value function is\napproximated by a linear combination of basis functions with coefficients as\ndecision variables. By relaxing the Bellman equation to an inequality, one\nobtains a linear program in the basis coefficients with an infinite set of\nconstraints. We show that a recently introduced method, which obtains convex\nquadratic value function approximations, can be extended to higher order\npolynomial approximations via sum of squares programming techniques. An\napproximate value function can then be computed offline by solving a\nsemidefinite program, without having to sample the infinite constraint. The\npolicy is evaluated online by solving a polynomial optimization problem, which\nalso turns out to be convex in some cases. We experimentally validate the\nmethod on an autonomous helicopter testbed using a 10-dimensional helicopter\nmodel. \n\n"}
{"id": "1212.2191", "contents": "Title: On the dynamic programming principle for controlled diffusion processes\n  in a cylindrical region Abstract: We prove the dynamic programming principle for a class of diffusion processes\ncontrolled up to the time of exit from a cylindrical region $[0,T)\\times G$. It\nis assumed that the functional to be maximized is in the Lagrange form with\nnonnegative integrand. Besides this we only adopt the standard assumptions,\nensuring the existence of a unique strong solution of a stochastic differential\nequation for the state process. \n\n"}
{"id": "1212.2398", "contents": "Title: An Information Theoretic Algorithm for Finding Periodicities in Stellar\n  Light Curves Abstract: We propose a new information theoretic metric for finding periodicities in\nstellar light curves. Light curves are astronomical time series of brightness\nover time, and are characterized as being noisy and unevenly sampled. The\nproposed metric combines correntropy (generalized correlation) with a periodic\nkernel to measure similarity among samples separated by a given period. The new\nmetric provides a periodogram, called Correntropy Kernelized Periodogram (CKP),\nwhose peaks are associated with the fundamental frequencies present in the\ndata. The CKP does not require any resampling, slotting or folding scheme as it\nis computed directly from the available samples. CKP is the main part of a\nfully-automated pipeline for periodic light curve discrimination to be used in\nastronomical survey databases. We show that the CKP method outperformed the\nslotted correntropy, and conventional methods used in astronomy for periodicity\ndiscrimination and period estimation tasks, using a set of light curves drawn\nfrom the MACHO survey. The proposed metric achieved 97.2% of true positives\nwith 0% of false positives at the confidence level of 99% for the periodicity\ndiscrimination task; and 88% of hits with 11.6% of multiples and 0.4% of misses\nin the period estimation task. \n\n"}
{"id": "1212.3530", "contents": "Title: A Multi-Orientation Analysis Approach to Retinal Vessel Tracking Abstract: This paper presents a method for retinal vasculature extraction based on\nbiologically inspired multi-orientation analysis. We apply multi-orientation\nanalysis via so-called invertible orientation scores, modeling the cortical\ncolumns in the visual system of higher mammals. This allows us to generically\ndeal with many hitherto complex problems inherent to vessel tracking, such as\ncrossings, bifurcations, parallel vessels, vessels of varying widths and\nvessels with high curvature. Our approach applies tracking in invertible\norientation scores via a novel geometrical principle for curve optimization in\nthe Euclidean motion group SE(2). The method runs fully automatically and\nprovides a detailed model of the retinal vasculature, which is crucial as a\nsound basis for further quantitative analysis of the retina, especially in\nscreening applications. \n\n"}
{"id": "1212.4527", "contents": "Title: GMM-Based Hidden Markov Random Field for Color Image and 3D Volume\n  Segmentation Abstract: In this project, we first study the Gaussian-based hidden Markov random field\n(HMRF) model and its expectation-maximization (EM) algorithm. Then we\ngeneralize it to Gaussian mixture model-based hidden Markov random field. The\nalgorithm is implemented in MATLAB. We also apply this algorithm to color image\nsegmentation problems and 3D volume segmentation problems. \n\n"}
{"id": "1212.5309", "contents": "Title: On evaluation of the mean service cycle time in tandem queueing systems Abstract: The problem of exact evaluation of the mean service cycle time in tandem\nsystems of single-server queues with both infinite and finite buffers is\nconsidered. It is assumed that the interarrival and service times of customers\nform sequences of independent and identically distributed random variables with\nknown mean values. We start with tandem queues with infinite buffers, and show\nthat under the above assumptions, the mean cycle time exists. Furthermore, if\nthe random variables which represent interarrival and service times have finite\nvariance, the mean cycle time can be calculated as the maximum out from the\nmean values of these variables. Finally, obtained results are extended to\nevaluation of the mean cycle time in particular tandem systems with finite\nbuffers and blocking. \n\n"}
{"id": "1301.5033", "contents": "Title: On the Distribution of MIMO Mutual Information: An In-Depth Painlev\\'{e}\n  Based Characterization Abstract: This paper builds upon our recent work which computed the moment generating\nfunction of the MIMO mutual information exactly in terms of a Painlev\\'{e} V\ndifferential equation. By exploiting this key analytical tool, we provide an\nin-depth characterization of the mutual information distribution for\nsufficiently large (but finite) antenna numbers. In particular, we derive\nsystematic closed-form expansions for the high order cumulants. These results\nyield considerable new insight, such as providing a technical explanation as to\nwhy the well known Gaussian approximation is quite robust to large SNR for the\ncase of unequal antenna arrays, whilst it deviates strongly for equal antenna\narrays. In addition, by drawing upon our high order cumulant expansions, we\nemploy the Edgeworth expansion technique to propose a refined Gaussian\napproximation which is shown to give a very accurate closed-form\ncharacterization of the mutual information distribution, both around the mean\nand for moderate deviations into the tails (where the Gaussian approximation\nfails remarkably). For stronger deviations where the Edgeworth expansion\nbecomes unwieldy, we employ the saddle point method and asymptotic integration\ntools to establish new analytical characterizations which are shown to be very\nsimple and accurate. Based on these results we also recover key well\nestablished properties of the tail distribution, including the\ndiversity-multiplexing-tradeoff. \n\n"}
{"id": "1302.0870", "contents": "Title: Centrality-constrained graph embedding Abstract: Visual rendering of graphs is a key task in the mapping of complex network\ndata. Although most graph drawing algorithms emphasize aesthetic appeal,\ncertain applications such as travel-time maps place more importance on\nvisualization of structural network properties. The present paper advocates a\ngraph embedding approach with centrality considerations to comply with node\nhierarchy. The problem is formulated as one of constrained multi-dimensional\nscaling (MDS), and it is solved via block coordinate descent iterations with\nsuccessive approximations and guaranteed convergence to a KKT point. In\naddition, a regularization term enforcing graph smoothness is incorporated with\nthe goal of reducing edge crossings. Experimental results demonstrate that the\nalgorithm converges, and can be used to efficiently embed large graphs on the\norder of thousands of nodes. \n\n"}
{"id": "1303.0126", "contents": "Title: Linear PDEs and eigenvalue problems corresponding to ergodic stochastic\n  optimization problems on compact manifolds Abstract: We consider long term average or `ergodic' optimal control poblems with a\nspecial structure: Control is exerted in all directions and the control costs\nare proportional to the square of the norm of the control field with respect to\nthe metric induced by the noise. The long term stochastic dynamics on the\nmanifold will be completely characterized by the long term density $\\rho$ and\nthe long term current density $J$. As such, control problems may be\nreformulated as variational problems over $\\rho$ and $J$. We discuss several\noptimization problems: the problem in which both $\\rho$ and $J$ are varied\nfreely, the problem in which $\\rho$ is fixed and the one in which $J$ is fixed.\nThese problems lead to different kinds of operator problems: linear PDEs in the\nfirst two cases and a nonlinear PDE in the latter case. These results are\nobtained through through variational principle using infinite dimensional\nLagrange multipliers. In the case where the initial dynamics are reversible we\nobtain the result that the optimally controlled diffusion is also\nsymmetrizable. The particular case of constraining the dynamics to be\nreversible of the optimally controlled process leads to a linear eigenvalue\nproblem for the square root of the density process. \n\n"}
{"id": "1303.7289", "contents": "Title: Upper-bounding $\\ell_1$-optimization weak thresholds Abstract: In our recent work \\cite{StojnicCSetam09} we considered solving\nunder-determined systems of linear equations with sparse solutions. In a large\ndimensional and statistical context we proved that if the number of equations\nin the system is proportional to the length of the unknown vector then there is\na sparsity (number of non-zero elements of the unknown vector) also\nproportional to the length of the unknown vector such that a polynomial\n$\\ell_1$-optimization technique succeeds in solving the system. We provided\nlower bounds on the proportionality constants that are in a solid numerical\nagreement with what one can observe through numerical experiments. Here we\ncreate a mechanism that can be used to derive the upper bounds on the\nproportionality constants. Moreover, the upper bounds obtained through such a\nmechanism match the lower bounds from \\cite{StojnicCSetam09} and ultimately\nmake the latter ones optimal. \n\n"}
{"id": "1304.3646", "contents": "Title: Network connectivity through small openings Abstract: Network connectivity is usually addressed for convex domains where a direct\nline of sight exists between any two transmitting/receiving nodes. Here, we\ndevelop a general theory for the network connectivity properties across a small\nopening, rendering the domain essentially non-convex. Our analytic approach can\ngo only so far as we encounter what is referred to in statistical physics as\nquenched disorder making the problem non-trivial. We confirm our theory through\ncomputer simulations, obtain leading order approximations and discuss possible\nextensions and applications. \n\n"}
{"id": "1304.6962", "contents": "Title: Variable projection methods for approximate (greatest) common divisor\n  computations Abstract: We consider the problem of finding for a given $N$-tuple of polynomials (real\nor complex) the closest $N$-tuple that has a common divisor of degree at least\n$d$. Extended weighted Euclidean seminorm of the coefficients is used as a\nmeasure of closeness. Two equivalent representations of the problem are\nconsidered: (i) direct parameterization over the common divisors and quotients\n(image representation), and (ii) Sylvester low-rank approximation (kernel\nrepresentation). We use the duality between least-squares and least-norm\nproblems to show that (i) and (ii) are closely related to mosaic Hankel\nlow-rank approximation. This allows us to apply to the approximate common\ndivisor problem recent results on complexity and accuracy of computations for\nmosaic Hankel low-rank approximation. We develop optimization methods based on\nthe variable projection principle both for image and kernel representation.\nThese methods have linear complexity in the degrees of the polynomials for\nsmall and large $d$. We provide a software implementation of the developed\nmethods, which is based on a software package for structured low-rank\napproximation. \n\n"}
{"id": "1305.2524", "contents": "Title: Corrupted Sensing: Novel Guarantees for Separating Structured Signals Abstract: We study the problem of corrupted sensing, a generalization of compressed\nsensing in which one aims to recover a signal from a collection of corrupted or\nunreliable measurements. While an arbitrary signal cannot be recovered in the\nface of arbitrary corruption, tractable recovery is possible when both signal\nand corruption are suitably structured. We quantify the relationship between\nsignal recovery and two geometric measures of structure, the Gaussian\ncomplexity of a tangent cone and the Gaussian distance to a subdifferential. We\ntake a convex programming approach to disentangling signal and corruption,\nanalyzing both penalized programs that trade off between signal and corruption\ncomplexity, and constrained programs that bound the complexity of signal or\ncorruption when prior information is available. In each case, we provide\nconditions for exact signal recovery from structured corruption and stable\nsignal recovery from structured corruption with added unstructured noise. Our\nsimulations demonstrate close agreement between our theoretical recovery bounds\nand the sharp phase transitions observed in practice. In addition, we provide\nnew interpretable bounds for the Gaussian complexity of sparse vectors,\nblock-sparse vectors, and low-rank matrices, which lead to sharper guarantees\nof recovery when combined with our results and those in the literature. \n\n"}
{"id": "1305.3498", "contents": "Title: An Improved Sub-Packetization Bound for Minimum Storage Regenerating\n  Codes Abstract: Distributed storage systems employ codes to provide resilience to failure of\nmultiple storage disks. Specifically, an $(n, k)$ MDS code stores $k$ symbols\nin $n$ disks such that the overall system is tolerant to a failure of up to\n$n-k$ disks. However, access to at least $k$ disks is still required to repair\na single erasure. To reduce repair bandwidth, array codes are used where the\nstored symbols or packets are vectors of length $\\ell$. MDS array codes have\nthe potential to repair a single erasure using a fraction $1/(n-k)$ of data\nstored in the remaining disks. We introduce new methods of analysis which\ncapitalize on the translation of the storage system problem into a geometric\nproblem on a set of operators and subspaces. In particular, we ask the\nfollowing question: for a given $(n, k)$, what is the minimum vector-length or\nsub-packetization factor $\\ell$ required to achieve this optimal fraction? For\n\\emph{exact recovery} of systematic disks in an MDS code of low redundancy,\ni.e. $k/n > 1/2$, the best known explicit codes \\cite{WTB12} have a\nsub-packetization factor $\\ell$ which is exponential in $k$. It has been\nconjectured \\cite{TWB12} that for a fixed number of parity nodes, it is in fact\nnecessary for $\\ell$ to be exponential in $k$. In this paper, we provide a new\nlog-squared converse bound on $k$ for a given $\\ell$, and prove that $k \\le\n2\\log_2\\ell\\left(\\log_{\\delta}\\ell+1\\right)$, for an arbitrary number of parity\nnodes $r = n-k$, where $\\delta = r/(r-1)$. \n\n"}
{"id": "1305.7477", "contents": "Title: On model selection consistency of regularized M-estimators Abstract: Regularized M-estimators are used in diverse areas of science and engineering\nto fit high-dimensional models with some low-dimensional structure. Usually the\nlow-dimensional structure is encoded by the presence of the (unknown)\nparameters in some low-dimensional model subspace. In such settings, it is\ndesirable for estimates of the model parameters to be \\emph{model selection\nconsistent}: the estimates also fall in the model subspace. We develop a\ngeneral framework for establishing consistency and model selection consistency\nof regularized M-estimators and show how it applies to some special cases of\ninterest in statistical learning. Our analysis identifies two key properties of\nregularized M-estimators, referred to as geometric decomposability and\nirrepresentability, that ensure the estimators are consistent and model\nselection consistent. \n\n"}
{"id": "1306.3609", "contents": "Title: Volume Ratio, Sparsity, and Minimaxity under Unitarily Invariant Norms Abstract: The current paper presents a novel machinery for studying non-asymptotic\nminimax estimation of high-dimensional matrices, which yields tight minimax\nrates for a large collection of loss functions in a variety of problems.\n  Based on the convex geometry of finite-dimensional Banach spaces, we first\ndevelop a volume ratio approach for determining minimax estimation rates of\nunconstrained normal mean matrices under all squared unitarily invariant norm\nlosses. In addition, we establish the minimax rates for estimating mean\nmatrices with submatrix sparsity, where the sparsity constraint introduces an\nadditional term in the rate whose dependence on the norm differs completely\nfrom the rate of the unconstrained problem. Moreover, the approach is\napplicable to the matrix completion problem under the low-rank constraint.\n  The new method also extends beyond the normal mean model. In particular, it\nyields tight rates in covariance matrix estimation and Poisson rate matrix\nestimation problems for all unitarily invariant norms. \n\n"}
{"id": "1306.3770", "contents": "Title: Lifting $\\ell_1$-optimization strong and sectional thresholds Abstract: In this paper we revisit under-determined linear systems of equations with\nsparse solutions. As is well known, these systems are among core mathematical\nproblems of a very popular compressed sensing field. The popularity of the\nfield as well as a substantial academic interest in linear systems with sparse\nsolutions are in a significant part due to seminal results\n\\cite{CRT,DonohoPol}. Namely, working in a statistical scenario,\n\\cite{CRT,DonohoPol} provided substantial mathematical progress in\ncharacterizing relation between the dimensions of the systems and the sparsity\nof unknown vectors recoverable through a particular polynomial technique called\n$\\ell_1$-minimization. In our own series of work\n\\cite{StojnicCSetam09,StojnicUpper10,StojnicEquiv10} we also provided a\ncollection of mathematical results related to these problems. While, Donoho's\nwork \\cite{DonohoPol,DonohoUnsigned} established (and our own work\n\\cite{StojnicCSetam09,StojnicUpper10,StojnicEquiv10} reaffirmed) the typical or\nthe so-called \\emph{weak threshold} behavior of $\\ell_1$-minimization many\nimportant questions remain unanswered. Among the most important ones are those\nthat relate to non-typical or the so-called \\emph{strong threshold} behavior.\nThese questions are usually combinatorial in nature and known techniques come\nup short of providing the exact answers. In this paper we provide a powerful\nmechanism that that can be used to attack the \"tough\" scenario, i.e. the\n\\emph{strong threshold} (and its a similar form called \\emph{sectional\nthreshold}) of $\\ell_1$-minimization. \n\n"}
{"id": "1307.0991", "contents": "Title: Mixed Noisy Network Coding and Cooperative Unicasting in Wireless\n  Networks Abstract: The problem of communicating a single message to a destination in presence of\nmultiple relay nodes, referred to as cooperative unicast network, is\nconsidered. First, we introduce \"Mixed Noisy Network Coding\" (MNNC) scheme\nwhich generalizes \"Noisy Network Coding\" (NNC) where relays are allowed to\ndecode-and-forward (DF) messages while all of them (without exception) transmit\nnoisy descriptions of their observations. These descriptions are exploited at\nthe destination and the DF relays aim to decode the transmitted messages while\ncreating full cooperation among the nodes. Moreover, the destination and the DF\nrelays can independently select the set of descriptions to be decoded or\ntreated as interference. This concept is further extended to multi-hopping\nscenarios, referred to as \"Layered MNNC\" (LMNNC), where DF relays are organized\ninto disjoint groups representing one hop in the network. For cooperative\nunicast additive white Gaussian noise (AWGN) networks we show that -provided DF\nrelays are properly chosen- MNNC improves over all previously established\nconstant gaps to the cut-set bound. Secondly, we consider the composite\ncooperative unicast network where the channel parameters are randomly drawn\nbefore communication starts and remain fixed during the transmission. Each draw\nis assumed to be unknown at the source and fully known at the destination but\nonly partly known at the relays. We introduce through MNNC scheme the concept\nof \"Selective Coding Strategy\" (SCS) that enables relays to decide dynamically\nwhether, in addition to communicate noisy descriptions, is possible to decode\nand forward messages. It is demonstrated through slow-fading AWGN relay\nnetworks that SCS clearly outperforms conventional coding schemes. \n\n"}
{"id": "1307.2342", "contents": "Title: Model Selection with Low Complexity Priors Abstract: Regularization plays a pivotal role when facing the challenge of solving\nill-posed inverse problems, where the number of observations is smaller than\nthe ambient dimension of the object to be estimated. A line of recent work has\nstudied regularization models with various types of low-dimensional structures.\nIn such settings, the general approach is to solve a regularized optimization\nproblem, which combines a data fidelity term and some regularization penalty\nthat promotes the assumed low-dimensional/simple structure. This paper provides\na general framework to capture this low-dimensional structure through what we\ncoin partly smooth functions relative to a linear manifold. These are convex,\nnon-negative, closed and finite-valued functions that will promote objects\nliving on low-dimensional subspaces. This class of regularizers encompasses\nmany popular examples such as the L1 norm, L1-L2 norm (group sparsity), as well\nas several others including the Linfty norm. We also show that the set of\npartly smooth functions relative to a linear manifold is closed under addition\nand pre-composition by a linear operator, which allows to cover mixed\nregularization, and the so-called analysis-type priors (e.g. total variation,\nfused Lasso, finite-valued polyhedral gauges). Our main result presents a\nunified sharp analysis of exact and robust recovery of the low-dimensional\nsubspace model associated to the object to recover from partial measurements.\nThis analysis is illustrated on a number of special and previously studied\ncases, and on an analysis of the performance of Linfty regularization in a\ncompressed sensing scenario. \n\n"}
{"id": "1307.5348", "contents": "Title: Tensor-based formulation and nuclear norm regularization for\n  multi-energy computed tomography Abstract: The development of energy selective, photon counting X-ray detectors allows\nfor a wide range of new possibilities in the area of computed tomographic image\nformation. Under the assumption of perfect energy resolution, here we propose a\ntensor-based iterative algorithm that simultaneously reconstructs the X-ray\nattenuation distribution for each energy. We use a multi-linear image model\nrather than a more standard \"stacked vector\" representation in order to develop\nnovel tensor-based regularizers. Specifically, we model the multi-spectral\nunknown as a 3-way tensor where the first two dimensions are space and the\nthird dimension is energy. This approach allows for the design of tensor\nnuclear norm regularizers, which like its two dimensional counterpart, is a\nconvex function of the multi-spectral unknown. The solution to the resulting\nconvex optimization problem is obtained using an alternating direction method\nof multipliers (ADMM) approach. Simulation results shows that the generalized\ntensor nuclear norm can be used as a stand alone regularization technique for\nthe energy selective (spectral) computed tomography (CT) problem and when\ncombined with total variation regularization it enhances the regularization\ncapabilities especially at low energy images where the effects of noise are\nmost prominent. \n\n"}
{"id": "1309.0448", "contents": "Title: Distributed Sensing and Transmission of Sporadic Random Samples in a\n  Multiple-Access Channel Abstract: This work considers distributed sensing and transmission of sporadic random\nsamples. Lower bounds are derived for the reconstruction error of a single\nnormally or uniformly-distributed finite-dimensional vector imperfectly\nmeasured by a network of sensors and transmitted with finite energy to a common\nreceiver via an additive white Gaussian noise asynchronous multiple-access\nchannel. Transmission makes use of a perfect causal feedback link to the\nencoder connected to each sensor. A retransmission protocol inspired by the\nclassical scheme in [1] applied to the transmission of single and bi-variate\nanalog samples analyzed in [2] and [3] is extended to the more general network\nscenario, for which asymptotic upper-bounds on the reconstruction error are\nprovided. Both the upper and lower-bounds show that collaboration can be\nachieved through energy accumulation under certain circumstances. In order to\ninvestigate the practical performance of the proposed retransmission protocol\nwe provide a numerical evaluation of the upper-bounds in the non-asymptotic\nenergy regime using low-order quantization in the sensors. The latter includes\na minor modification of the protocol to improve reconstruction fidelity.\nNumerical results show that an increase in the size of the network brings\nbenefit in terms of performance, but that the gain in terms of energy\nefficiency diminishes quickly at finite energies due to a non-coherent\ncombining loss. \n\n"}
{"id": "1309.0474", "contents": "Title: Smooth solutions to portfolio liquidation problems under price-sensitive\n  market impact Abstract: We consider the stochastic control problem of a financial trader that needs\nto unwind a large asset portfolio within a short period of time. The trader can\nsimultaneously submit active orders to a primary market and passive orders to a\ndark pool. Our framework is flexible enough to allow for price-dependent impact\nfunctions describing the trading costs in the primary market and\nprice-dependent adverse selection costs associated with dark pool trading. We\nprove that the value function can be characterized in terms of the unique\nsmooth solution to a PDE with singular terminal value, establish its explicit\nasymptotic behavior at the terminal time, and give the optimal trading strategy\nin feedback form. \n\n"}
{"id": "1309.3256", "contents": "Title: Recovery guarantees for exemplar-based clustering Abstract: For a certain class of distributions, we prove that the linear programming\nrelaxation of $k$-medoids clustering---a variant of $k$-means clustering where\nmeans are replaced by exemplars from within the dataset---distinguishes points\ndrawn from nonoverlapping balls with high probability once the number of points\ndrawn and the separation distance between any two balls are sufficiently large.\nOur results hold in the nontrivial regime where the separation distance is\nsmall enough that points drawn from different balls may be closer to each other\nthan points drawn from the same ball; in this case, clustering by thresholding\npairwise distances between points can fail. We also exhibit numerical evidence\nof high-probability recovery in a substantially more permissive regime. \n\n"}
{"id": "1310.5479", "contents": "Title: Applications of Large Random Matrices in Communications Engineering Abstract: This work gives an overview of analytic tools for the design, analysis, and\nmodelling of communication systems which can be described by linear vector\nchannels such as y = Hx+z where the number of components in each vector is\nlarge. Tools from probability theory, operator algebra, and statistical physics\nare reviewed. The survey of analytical tools is complemented by examples of\napplications in communications engineering. Asymptotic eigenvalue distributions\nof many classes of random matrices are given. The treatment includes the\nproblem of moments and the introduction of the Stieltjes transform. Free\nprobability theory, which evolved from non-commutative operator algebras, is\nexplained from a probabilistic point of view in order to better fit the\nengineering community. For that purpose freeness is defined without reference\nto non-commutative algebras. The treatment includes additive and multiplicative\nfree convolution, the R-transform, the S-transform, and the free central limit\ntheorem. The replica method developed in statistical physics for the purpose of\nanalyzing spin glasses is reviewed from the viewpoint of its applications in\ncommunications engineering. Correspondences between free energy and mutual\ninformation as well as energy functions and detector metrics are established.\nThese analytic tools are applied to the design and the analysis of linear\nmultiuser detectors, the modelling of scattering in communication channels with\ndual antennas arrays, and the analysis of optimal detection for communication\nvia code-division multiple-access and/or dual antenna array channels. \n\n"}
{"id": "1310.5715", "contents": "Title: Stochastic Gradient Descent, Weighted Sampling, and the Randomized\n  Kaczmarz algorithm Abstract: We obtain an improved finite-sample guarantee on the linear convergence of\nstochastic gradient descent for smooth and strongly convex objectives,\nimproving from a quadratic dependence on the conditioning $(L/\\mu)^2$ (where\n$L$ is a bound on the smoothness and $\\mu$ on the strong convexity) to a linear\ndependence on $L/\\mu$. Furthermore, we show how reweighting the sampling\ndistribution (i.e. importance sampling) is necessary in order to further\nimprove convergence, and obtain a linear dependence in the average smoothness,\ndominating previous results. We also discuss importance sampling for SGD more\nbroadly and show how it can improve convergence also in other scenarios. Our\nresults are based on a connection we make between SGD and the randomized\nKaczmarz algorithm, which allows us to transfer ideas between the separate\nbodies of literature studying each of the two methods. In particular, we recast\nthe randomized Kaczmarz algorithm as an instance of SGD, and apply our results\nto prove its exponential convergence, but to the solution of a weighted least\nsquares problem rather than the original least squares problem. We then present\na modified Kaczmarz algorithm with partially biased sampling which does\nconverge to the original least squares solution with the same exponential\nconvergence rate. \n\n"}
{"id": "1311.2335", "contents": "Title: A First-Order Algorithm for the A-Optimal Experimental Design Problem: A\n  Mathematical Programming Approach Abstract: We develop and analyse a first-order algorithm for the A-optimal experimental\ndesign problem. The problem is first presented as a special case of a\nparametric family of optimal design problems for which duality results and\noptimality conditions are given. Then, two first-order (Frank-Wolfe type)\nalgorithms are presented, accompanied by a detailed time-complexity analysis of\nthe algorithms and computational results on various sized problems. \n\n"}
{"id": "1311.2745", "contents": "Title: Sparse Phase Retrieval: Uniqueness Guarantees and Recovery Algorithms Abstract: The problem of signal recovery from its Fourier transform magnitude is of\nparamount importance in various fields of engineering and has been around for\nover 100 years. Due to the absence of phase information, some form of\nadditional information is required in order to be able to uniquely identify the\nsignal of interest. In this work, we focus our attention on discrete-time\nsparse signals (of length $n$). We first show that, if the DFT dimension is\ngreater than or equal to $2n$, almost all signals with {\\em aperiodic} support\ncan be uniquely identified by their Fourier transform magnitude (up to\ntime-shift, conjugate-flip and global phase).\n  Then, we develop an efficient Two-stage Sparse Phase Retrieval algorithm\n(TSPR), which involves: (i) identifying the support, i.e., the locations of the\nnon-zero components, of the signal using a combinatorial algorithm (ii)\nidentifying the signal values in the support using a convex algorithm. We show\nthat TSPR can {\\em provably} recover most $O(n^{1/2-\\eps})$-sparse signals (up\nto a time-shift, conjugate-flip and global phase). We also show that, for most\n$O(n^{1/4-\\eps})$-sparse signals, the recovery is {\\em robust} in the presence\nof measurement noise. Numerical experiments complement our theoretical analysis\nand verify the effectiveness of TSPR. \n\n"}
{"id": "1312.0232", "contents": "Title: Stochastic continuum armed bandit problem of few linear parameters in\n  high dimensions Abstract: We consider a stochastic continuum armed bandit problem where the arms are\nindexed by the $\\ell_2$ ball $B_{d}(1+\\nu)$ of radius $1+\\nu$ in\n$\\mathbb{R}^d$. The reward functions $r :B_{d}(1+\\nu) \\rightarrow \\mathbb{R}$\nare considered to intrinsically depend on $k \\ll d$ unknown linear parameters\nso that $r(\\mathbf{x}) = g(\\mathbf{A} \\mathbf{x})$ where $\\mathbf{A}$ is a full\nrank $k \\times d$ matrix. Assuming the mean reward function to be smooth we\nmake use of results from low-rank matrix recovery literature and derive an\nefficient randomized algorithm which achieves a regret bound of $O(C(k,d)\nn^{\\frac{1+k}{2+k}} (\\log n)^{\\frac{1}{2+k}})$ with high probability. Here\n$C(k,d)$ is at most polynomial in $d$ and $k$ and $n$ is the number of rounds\nor the sampling budget which is assumed to be known beforehand. \n\n"}
{"id": "1312.0914", "contents": "Title: Characterizing the Rate Region of the (4,3,3) Exact-Repair Regenerating\n  Codes Abstract: Exact-repair regenerating codes are considered for the case (n,k,d)=(4,3,3),\nfor which a complete characterization of the rate region is provided. This\ncharacterization answers in the affirmative the open question whether there\nexists a non-vanishing gap between the optimal bandwidth-storage tradeoff of\nthe functional-repair regenerating codes (i.e., the cut-set bound) and that of\nthe exact-repair regenerating codes. To obtain an explicit information\ntheoretic converse, a computer-aided proof (CAP) approach based on primal and\ndual relation is developed. This CAP approach extends Yeung's linear\nprogramming (LP) method, which was previously only used on information\ntheoretic problems with a few random variables due to the exponential growth of\nthe number of variables in the corresponding LP problem. The symmetry in the\nexact-repair regenerating code problem allows an effective reduction of the\nnumber of variables, and together with several other problem-specific\nreductions, the LP problem is reduced to a manageable scale. For the\nachievability, only one non-trivial corner point of the rate region needs to be\naddressed in this case, for which an explicit binary code construction is\ngiven. \n\n"}
{"id": "1312.1918", "contents": "Title: Cut-Set Bounds for Networks with Zero-Delay Nodes Abstract: In a network, a node is said to incur a delay if its encoding of each\ntransmitted symbol involves only its received symbols obtained before the time\nslot in which the transmitted symbol is sent (hence the transmitted symbol sent\nin a time slot cannot depend on the received symbol obtained in the same time\nslot). A node is said to incur no delay if its received symbol obtained in a\ntime slot is available for encoding its transmitted symbol sent in the same\ntime slot. Under the classical model, every node in a discrete memoryless\nnetwork (DMN) incurs a unit delay, and the capacity region of the DMN satisfies\nthe well-known cut-set outer bound. In this paper, we propose a generalized\nmodel for the DMN where some nodes may incur no delay. Under our generalized\nmodel, we obtain a new cut-set outer bound, which is proved to be tight for\nsome two-node DMN and is shown to subsume an existing cut-set bound for the\ncausal relay network. In addition, we establish under the generalized model\nanother cut-set outer bound on the positive-delay region -- the set of\nachievable rate tuples under the constraint that every node incurs a delay. We\nuse the cut-set bound on the positive-delay region to show that for some\ntwo-node DMN under the generalized model, the positive-delay region is strictly\nsmaller than the capacity region. \n\n"}
{"id": "1312.5276", "contents": "Title: Integration by parts and representation of information functionals Abstract: We introduce a new formalism for computing expectations of functionals of\narbitrary random vectors, by using generalised integration by parts formulae.\nIn doing so we extend recent representation formulae for the score function\nintroduced in Nourdin, Peccati and Swan (JFA, to appear) and also provide a new\nproof of a central identity first discovered in Guo, Shamai, and Verd{\\'u}\n(IEEE Trans. Information Theory, 2005). We derive a representation for the\nstandardized Fisher information of sums of i.i.d. random vectors which use our\nidentities to provide rates of convergence in information theoretic central\nlimit theorems (both in Fisher information distance and in relative entropy). \n\n"}
{"id": "1312.5765", "contents": "Title: Multi-Branch Matching Pursuit with applications to MIMO radar Abstract: We present an algorithm, dubbed Multi-Branch Matching Pursuit (MBMP), to\nsolve the sparse recovery problem over redundant dictionaries. MBMP combines\nthree different paradigms: being a greedy method, it performs iterative signal\nsupport estimation; as a rank-aware method, it is able to exploit signal\nsubspace information when multiple snapshots are available; and, as its name\nforetells, it leverages a multi-branch (i.e., tree-search) strategy that allows\nus to trade-off hardware complexity (e.g. measurements) for computational\ncomplexity. We derive a sufficient condition under which MBMP can recover a\nsparse signal from noiseless measurements. This condition, named MB-coherence,\nis met when the dictionary is sufficiently incoherent. It incorporates the\nnumber of branches of MBMP and it requires fewer measurements than other\nconditions (e.g. the Neuman ERC or the cumulative coherence). As such,\nsuccessful recovery with MBMP is guaranteed for dictionaries that do not\nsatisfy previously known conditions. \n\n"}
{"id": "1312.6204", "contents": "Title: One-Shot Adaptation of Supervised Deep Convolutional Models Abstract: Dataset bias remains a significant barrier towards solving real world\ncomputer vision tasks. Though deep convolutional networks have proven to be a\ncompetitive approach for image classification, a question remains: have these\nmodels have solved the dataset bias problem? In general, training or\nfine-tuning a state-of-the-art deep model on a new domain requires a\nsignificant amount of data, which for many applications is simply not\navailable. Transfer of models directly to new domains without adaptation has\nhistorically led to poor recognition performance. In this paper, we pose the\nfollowing question: is a single image dataset, much larger than previously\nexplored for adaptation, comprehensive enough to learn general deep models that\nmay be effectively applied to new image domains? In other words, are deep CNNs\ntrained on large amounts of labeled data as susceptible to dataset bias as\nprevious methods have been shown to be? We show that a generic supervised deep\nCNN model trained on a large dataset reduces, but does not remove, dataset\nbias. Furthermore, we propose several methods for adaptation with deep models\nthat are able to operate with little (one example per category) or no labeled\ndomain specific data. Our experiments show that adaptation of deep models on\nbenchmark visual domain adaptation datasets can provide a significant\nperformance boost. \n\n"}
{"id": "1312.7793", "contents": "Title: Direction of Arrival Estimation Using Co-prime Arrays: A Super\n  Resolution Viewpoint Abstract: We consider the problem of direction of arrival (DOA) estimation using a\nnewly proposed structure of non-uniform linear arrays, referred to as co-prime\narrays, in this paper. By exploiting the second order statistical information\nof the received signals, co-prime arrays exhibit O(MN) degrees of freedom with\nonly M + N sensors. A sparsity based recovery method is proposed to fully\nutilize these degrees of freedom. Unlike traditional sparse recovery methods,\nthe proposed method is based on the developing theory of super resolution,\nwhich considers a continuous range of possible sources instead of discretizing\nthis range into a discrete grid. With this approach, off-grid effects inherited\nin traditional sparse recovery can be neglected, thus improving the accuracy of\nDOA estimation. In this paper we show that in the noiseless case one can\ntheoretically detect up to M N sources with only 2M + N sensors. The noise 2\nstatistics of co-prime arrays are also analyzed to demonstrate the robustness\nof the proposed optimization scheme. A source number detection method is\npresented based on the spectrum reconstructed from the sparse method. By\nextensive numerical examples, we show the superiority of the proposed method in\nterms of DOA estimation accuracy, degrees of freedom, and resolution ability\ncompared with previous methods, such as MUSIC with spatial smoothing and the\ndiscrete sparse recovery method. \n\n"}
{"id": "1401.0978", "contents": "Title: A Principled Infotheoretic \\phi-like Measure Abstract: Integrated information theory is a mathematical, quantifiable theory of\nconscious experience. The linchpin of this theory, the $\\phi$ measure,\nquantifies a system's irreducibility to disjoint parts. Purely as a measure of\nirreducibility, we pinpoint three concerns about $\\phi$ and propose a revised\nmeasure, $\\psi$, which addresses them. Our measure $\\psi$ is rigorously\ngrounded in Partial Information Decomposition and is faster to compute than\n$\\phi$. \n\n"}
{"id": "1401.1255", "contents": "Title: Interiors of completely positive cones Abstract: A symmetric matrix $A$ is completely positive (CP) if there exists an\nentrywise nonnegative matrix $B$ such that $A = BB^T$. We characterize the\ninterior of the CP cone. A semidefinite algorithm is proposed for checking\ninteriors of the CP cone, and its properties are studied. A CP-decomposition of\na matrix in Dickinson's form can be obtained if it is an interior of the CP\ncone. Some computational experiments are also presented. \n\n"}
{"id": "1401.1671", "contents": "Title: Distributed Energy Efficient Channel Allocation Abstract: Design of energy efficient protocols for modern wireless systems has become\nan important area of research. In this paper, we propose a distributed\noptimization algorithm for the channel assignment problem for multiple\ninterfering transceiver pairs that cannot communicate with each other. We first\nmodify the auction algorithm for maximal energy efficiency and show that the\nproblem can be solved without explicit message passing using the carrier sense\nmultiple access (CSMA) protocols. We then develop a novel scheme by converting\nthe channel assignment problem into perfect matchings on bipartite graphs. The\nproposed scheme improves the energy efficiency and does not require any\nexplicit message passing or a shared memory between the users. We derive bounds\non the convergence rate and show that the proposed algorithm converges faster\nthan the distributed auction algorithm and achieves near-optimal performance\nunder Rayleigh fading channels. We also present an asymptotic performance\nanalysis of the fast matching algorithm for energy efficient resource\nallocation and prove the optimality for large enough number of users and number\nof channels. Finally, we provide numerical assessments that confirm the energy\nefficiency gains compared to the state of the art. \n\n"}
{"id": "1401.3809", "contents": "Title: An Information-Spectrum Approach to Weak Variable-Length Source Coding\n  with Side-Information Abstract: This paper studies variable-length (VL) source coding of general sources with\nside-information. Novel one-shot coding theorems for coding with common\nside-information available at the encoder and the decoder and Slepian- Wolf\n(SW) coding (i.e., with side-information only at the decoder) are given, and\nthen, are applied to asymptotic analyses of these coding problems. Especially,\na general formula for the infimum of the coding rate asymptotically achievable\nby weak VL-SW coding (i.e., VL-SW coding with vanishing error probability) is\nderived. Further, the general formula is applied to investigating weak VL-SW\ncoding of mixed sources. Our results derive and extend several known results on\nSW coding and weak VL coding, e.g., the optimal achievable rate of VL-SW coding\nfor mixture of i.i.d. sources is given for countably infinite alphabet case\nwith mild condition. In addition, the usefulness of the encoder\nside-information is investigated. Our result shows that if the encoder\nside-information is useless in weak VL coding then it is also useless even in\nthe case where the error probability may be positive asymptotically. \n\n"}
{"id": "1402.0453", "contents": "Title: Fine-Grained Visual Categorization via Multi-stage Metric Learning Abstract: Fine-grained visual categorization (FGVC) is to categorize objects into\nsubordinate classes instead of basic classes. One major challenge in FGVC is\nthe co-occurrence of two issues: 1) many subordinate classes are highly\ncorrelated and are difficult to distinguish, and 2) there exists the large\nintra-class variation (e.g., due to object pose). This paper proposes to\nexplicitly address the above two issues via distance metric learning (DML). DML\naddresses the first issue by learning an embedding so that data points from the\nsame class will be pulled together while those from different classes should be\npushed apart from each other; and it addresses the second issue by allowing the\nflexibility that only a portion of the neighbors (not all data points) from the\nsame class need to be pulled together. However, feature representation of an\nimage is often high dimensional, and DML is known to have difficulty in dealing\nwith high dimensional feature vectors since it would require $\\mathcal{O}(d^2)$\nfor storage and $\\mathcal{O}(d^3)$ for optimization. To this end, we proposed a\nmulti-stage metric learning framework that divides the large-scale high\ndimensional learning problem to a series of simple subproblems, achieving\n$\\mathcal{O}(d)$ computational complexity. The empirical study with FVGC\nbenchmark datasets verifies that our method is both effective and efficient\ncompared to the state-of-the-art FGVC approaches. \n\n"}
{"id": "1402.1473", "contents": "Title: Near-Optimal Joint Object Matching via Convex Relaxation Abstract: Joint matching over a collection of objects aims at aggregating information\nfrom a large collection of similar instances (e.g. images, graphs, shapes) to\nimprove maps between pairs of them. Given multiple matches computed between a\nfew object pairs in isolation, the goal is to recover an entire collection of\nmaps that are (1) globally consistent, and (2) close to the provided maps ---\nand under certain conditions provably the ground-truth maps. Despite recent\nadvances on this problem, the best-known recovery guarantees are limited to a\nsmall constant barrier --- none of the existing methods find theoretical\nsupport when more than $50\\%$ of input correspondences are corrupted. Moreover,\nprior approaches focus mostly on fully similar objects, while it is practically\nmore demanding to match instances that are only partially similar to each\nother.\n  In this paper, we develop an algorithm to jointly match multiple objects that\nexhibit only partial similarities, given a few pairwise matches that are\ndensely corrupted. Specifically, we propose to recover the ground-truth maps\nvia a parameter-free convex program called MatchLift, following a spectral\nmethod that pre-estimates the total number of distinct elements to be matched.\nEncouragingly, MatchLift exhibits near-optimal error-correction ability, i.e.\nin the asymptotic regime it is guaranteed to work even when a dominant fraction\n$1-\\Theta\\left(\\frac{\\log^{2}n}{\\sqrt{n}}\\right)$ of the input maps behave like\nrandom outliers. Furthermore, MatchLift succeeds with minimal input complexity,\nnamely, perfect matching can be achieved as soon as the provided maps form a\nconnected map graph. We evaluate the proposed algorithm on various benchmark\ndata sets including synthetic examples and real-world examples, all of which\nconfirm the practical applicability of MatchLift. \n\n"}
{"id": "1402.4881", "contents": "Title: Fixed Error Asymptotics For Erasure and List Decoding Abstract: We derive the optimum second-order coding rates, known as second-order\ncapacities, for erasure and list decoding. For erasure decoding for discrete\nmemoryless channels, we show that second-order capacity is\n$\\sqrt{V}\\Phi^{-1}(\\epsilon_t)$ where $V$ is the channel dispersion and\n$\\epsilon_t$ is the total error probability, i.e., the sum of the erasure and\nundetected errors. We show numerically that the expected rate at finite\nblocklength for erasures decoding can exceed the finite blocklength channel\ncoding rate. We also show that the analogous result also holds for lossless\nsource coding with decoder side information, i.e., Slepian-Wolf coding. For\nlist decoding, we consider list codes of deterministic size that scales as\n$\\exp(\\sqrt{n}l)$ and show that the second-order capacity is\n$l+\\sqrt{V}\\Phi^{-1}(\\epsilon)$ where $\\epsilon$ is the permissible error\nprobability. We also consider lists of polynomial size $n^\\alpha$ and derive\nbounds on the third-order coding rate in terms of the order of the polynomial\n$\\alpha$. These bounds are tight for symmetric and singular channels. The\ndirect parts of the coding theorems leverage on the simple threshold decoder\nand converses are proved using variants of the hypothesis testing converse. \n\n"}
{"id": "1402.5684", "contents": "Title: Discriminative Functional Connectivity Measures for Brain Decoding Abstract: We propose a statistical learning model for classifying cognitive processes\nbased on distributed patterns of neural activation in the brain, acquired via\nfunctional magnetic resonance imaging (fMRI). In the proposed learning method,\nlocal meshes are formed around each voxel. The distance between voxels in the\nmesh is determined by using a functional neighbourhood concept. In order to\ndefine the functional neighbourhood, the similarities between the time series\nrecorded for voxels are measured and functional connectivity matrices are\nconstructed. Then, the local mesh for each voxel is formed by including the\nfunctionally closest neighbouring voxels in the mesh. The relationship between\nthe voxels within a mesh is estimated by using a linear regression model. These\nrelationship vectors, called Functional Connectivity aware Local Relational\nFeatures (FC-LRF) are then used to train a statistical learning machine. The\nproposed method was tested on a recognition memory experiment, including data\npertaining to encoding and retrieval of words belonging to ten different\nsemantic categories. Two popular classifiers, namely k-nearest neighbour (k-nn)\nand Support Vector Machine (SVM), are trained in order to predict the semantic\ncategory of the item being retrieved, based on activation patterns during\nencoding. The classification performance of the Functional Mesh Learning model,\nwhich range in 62%-71% is superior to the classical multi-voxel pattern\nanalysis (MVPA) methods, which range in 40%-48%, for ten semantic categories. \n\n"}
{"id": "1403.1494", "contents": "Title: Fundamental Limits and Tradeoffs on Disturbance Propagation in\n  Large-Scale Dynamical Networks Abstract: We consider performance deterioration of interconnected linear dynamical\nnetworks subject to exogenous stochastic disturbances. The focus of this paper\nis on first-order and second-order linear consensus networks. We employ the\nexpected value of the steady state dispersion of the state of the entire\nnetwork as a performance measure and develop a graph-theoretic methodology to\nrelate structural specifications of the underlying graphs of the network to the\nperformance measure. We explicitly quantify several inherent fundamental limits\non the best achievable levels of performance in linear consensus networks and\nshow that these limits of performance are merely imposed by the specific\nstructure of the underlying graphs. Furthermore, we discover new connections\nbetween notions of sparsity and the performance measure. Particularly, we\ncharacterize several fundamental tradeoffs that reveal interplay between the\nperformance measure and various sparsity measures of a linear consensus\nnetwork. At the end, we apply our results to two real-world dynamical networks\nand provide energy interpretations for the proposed performance measures. It is\nshown that the total power loss in synchronous power networks and total kinetic\nenergy of a network of autonomous vehicles in a formation are viable\nperformance measure for these networks and fundamental limits on these measures\nquantify the best achievable levels of energy-efficiency in these dynamical\nnetworks. \n\n"}
{"id": "1403.4134", "contents": "Title: Probabilistic and Distributed Control of a Large-Scale Swarm of\n  Autonomous Agents Abstract: We present a novel method for guiding a large-scale swarm of autonomous\nagents into a desired formation shape in a distributed and scalable manner. Our\nProbabilistic Swarm Guidance using Inhomogeneous Markov Chains (PSG-IMC)\nalgorithm adopts an Eulerian framework, where the physical space is partitioned\ninto bins and the swarm's density distribution over each bin is controlled.\nEach agent determines its bin transition probabilities using a\ntime-inhomogeneous Markov chain. These time-varying Markov matrices are\nconstructed by each agent in real-time using the feedback from the current\nswarm distribution, which is estimated in a distributed manner. The PSG-IMC\nalgorithm minimizes the expected cost of the transitions per time instant,\nrequired to achieve and maintain the desired formation shape, even when agents\nare added to or removed from the swarm. The algorithm scales well with a large\nnumber of agents and complex formation shapes, and can also be adapted for area\nexploration applications. We demonstrate the effectiveness of this proposed\nswarm guidance algorithm by using results of numerical simulations and hardware\nexperiments with multiple quadrotors. \n\n"}
{"id": "1403.6887", "contents": "Title: Distributional Analysis for Model Predictive Deferrable Load Control Abstract: Deferrable load control is essential for handling the uncertainties\nassociated with the increasing penetration of renewable generation. Model\npredictive control has emerged as an effective approach for deferrable load\ncontrol, and has received considerable attention. In particular, previous work\nhas analyzed the average-case performance of model predictive deferrable load\ncontrol. However, to this point, distributional analysis of model predictive\ndeferrable load control has been elusive. In this paper, we prove strong\nconcentration results on the distribution of the load variance obtained by\nmodel predictive deferrable load control. These concentration results highlight\nthat the typical performance of model predictive deferrable load control is\ntightly concentrated around the average-case performance. \n\n"}
{"id": "1403.6974", "contents": "Title: Design and Analysis of a Greedy Pursuit for Distributed Compressed\n  Sensing Abstract: We consider a distributed compressed sensing scenario where many sensors\nmeasure correlated sparse signals and the sensors are connected through a\nnetwork. Correlation between sparse signals is modeled by a partial common\nsupport-set. For such a scenario, the main objective of this paper is to\ndevelop a greedy pursuit algorithm. We develop a distributed parallel pursuit\n(DIPP) algorithm based on exchange of information about estimated support-sets\nat sensors. The exchange of information helps to improve estimation of the\npartial common support-set, that in turn helps to gradually improve estimation\nof support-sets in all sensors, leading to a better quality reconstruction\nperformance. We provide restricted isometry property (RIP) based theoretical\nanalysis on the algorithm's convergence and reconstruction performance. Under\ncertain theoretical requirements on the quality of information exchange over\nnetwork and RIP parameters of sensor nodes, we show that the DIPP algorithm\nconverges to a performance level that depends on a scaled additive measurement\nnoise power (convergence in theory) where the scaling coefficient is a function\nof RIP parameters and information processing quality parameters. Using\nsimulations, we show practical reconstruction performance of DIPP vis-a-vis\namount of undersampling, signal-to-measurement-noise ratios and\nnetwork-connectivity conditions. \n\n"}
{"id": "1404.0700", "contents": "Title: Distributed Optimal Power Flow Algorithm for Balanced Radial\n  Distribution Networks Abstract: The optimal power flow (OPF) problem is fundamental in power system\noperations and planning. Large-scale renewable penetration in distribution\nnetworks calls for real-time feedback control, and hence the need for fast and\ndistributed solutions for OPF. This is difficult because OPF is nonconvex and\nKirchhoff's laws are global. In this paper we propose a solution for balanced\nradial distribution networks. It exploits recent results that suggest solving\nfor a globally optimal solution of OPF over a radial network through the\nsecond-order cone program (SOCP) relaxation. Our distributed algorithm is based\non alternating direction method of multiplier (ADMM), but unlike standard ADMM\nalgorithms that often require iteratively solving optimization subproblems in\neach ADMM iteration, our decomposition allows us to derive closed form\nsolutions for these subproblems, greatly speeding up each ADMM iteration. We\npresent simulations on a real-world 2,065-bus distribution network to\nillustrate the scalability and optimality of the proposed algorithm. \n\n"}
{"id": "1404.2741", "contents": "Title: Nonlinearity of Boolean functions: an algorithmic approach based on\n  multivariate polynomials Abstract: We compute the nonlinearity of Boolean functions with Groebner basis\ntechniques, providing two algorithms: one over the binary field and the other\nover the rationals. We also estimate their complexity. Then we show how to\nimprove our rational algorithm, arriving at a worst-case complexity of\n$O(n2^n)$ operations over the integers, that is, sums and doublings. This way,\nwith a different approach, we reach the same complexity of established\nalgorithms, such as those based on the fast Walsh transform. \n\n"}
{"id": "1404.2993", "contents": "Title: On More Bent Functions From Dillon Exponents Abstract: In this paper, we obtain a new class of $p$-ary binomial bent functions which\nare determined by Kloosterman sums. The bentness of another three classes of\nfunctions is characterized by some exponential sums and some results in\n\\cite{Linian2013} are generalized. Furthermore we obtain, in some special\ncases, some bent functions are determined by Kloosterman sums. \n\n"}
{"id": "1405.1354", "contents": "Title: Remarks on existence and uniqueness of Cournot-Nash equilibria in the\n  non-potential case Abstract: This article is devoted to various methods (optimal transport, fixed-point,\nordinary differential equations) to obtain existence and/or uniqueness of\nCournot-Nash equilibria for games with a continuum of players with both\nattractive and repulsive effects. We mainly address separable situations but\nfor which the game does not have a potential. We also present several numerical\nsimulations which illustrate the applicability of our approach to compute\nCournot-Nash equilibria. \n\n"}
{"id": "1405.4520", "contents": "Title: Dynamical Analysis of a Networked Control System Abstract: A new network data transmission strategy was proposed in Zhang \\& Chen [2005]\n(arXiv:1405.2404), where the resulting nonlinear system was analyzed and the\neffectiveness of the transmission strategy was demonstrated via simulations. In\nthis paper, we further generalize the results of Zhang \\& Chen [2005] in the\nfollowing ways: 1) Construct first-return maps of the nonlinear systems\nformulated in Zhang \\& Chen [2005] and derive several existence conditions of\nperiodic orbits and study their properties. 2) Formulate the new system as a\nhybrid system, which will ease the succeeding analysis. 3) Prove that this type\nof hybrid systems is not structurally stable based on phase transition which\ncan be applied to higher-dimensional cases effortlessly. 4) Simulate a\nhigher-dimensional model with emphasis on their rich dynamics. 5) Study a class\nof continuous-time hybrid systems as the counterparts of the discrete-time\nsystems discussed above. 6) Propose new controller design methods based on this\nnetwork data transmission strategy to improve the performance of each\nindividual system and the whole network. We hope that this research and the\nproblems posed here will rouse interests of researchers in such fields as\ncontrol, dynamical systems and numerical analysis. \n\n"}
{"id": "1405.5892", "contents": "Title: Nonlinear POMDPs for Active State Tracking with Sensing Costs Abstract: Active state tracking is needed in object classification, target tracking,\nmedical diagnosis and estimation of sparse signals among other various\napplications. Herein, active state tracking of a discrete-time, finite-state\nMarkov chain is considered. Noisy Gaussian observations are dynamically\ncollected by exerting appropriate control over their information content, while\nincurring a related sensing cost. The objective is to devise sensing strategies\nto optimize the trade-off between tracking performance and sensing cost. A\nrecently proposed Kalman-like estimator \\cite{ZoisTSP14} is employed for state\ntracking. The associated mean-squared error and a generic sensing cost metric\nare then used in a partially observable Markov decision process formulation,\nand the optimal sensing strategy is derived via a dynamic programming\nrecursion. The resulting recursion proves to be non-linear, challenging control\npolicy design. Properties of the related cost functions are derived and\nsufficient conditions are provided regarding the structure of the optimal\ncontrol policy enabling characterization of when passive state tracking is\noptimal. To overcome the associated computational burden of the optimal sensing\nstrategy, two lower complexity strategies are proposed, which exploit the\naforementioned properties. The performance of the proposed strategies is\nillustrated in a wireless body sensing application, where cost savings as high\nas $60\\%$ are demonstrated for a $4\\%$ detection error with respect to a static\nequal allocation sensing strategy. \n\n"}
{"id": "1405.7147", "contents": "Title: New extremal binary self-dual codes from F_4 + uF_4-lifts of quadratic\n  double circulant codes over F_4 Abstract: In this work, quadratic double and quadratic bordered double circulant\nconstructions are applied to F_4 + uF_4 as well as F_4, as a result of which\nextremal binary self-dual codes of length 56 and 64 are obtained. The binary\nextension theorems as well as the ring extension version are used to obtain 7\nextremal self-dual binary codes of length 58, 24 extremal self-dual binary\ncodes of length 66 and 29 extremal self-dual binary codes of length 68, all\nwith new weight enumerators, updating the list of all the known extremal\nself-dual codes in the literature. \n\n"}
{"id": "1406.2080", "contents": "Title: Training Convolutional Networks with Noisy Labels Abstract: The availability of large labeled datasets has allowed Convolutional Network\nmodels to achieve impressive recognition results. However, in many settings\nmanual annotation of the data is impractical; instead our data has noisy\nlabels, i.e. there is some freely available label for each image which may or\nmay not be accurate. In this paper, we explore the performance of\ndiscriminatively-trained Convnets when trained on such noisy data. We introduce\nan extra noise layer into the network which adapts the network outputs to match\nthe noisy label distribution. The parameters of this noise layer can be\nestimated as part of the training process and involve simple modifications to\ncurrent training infrastructures for deep networks. We demonstrate the\napproaches on several datasets, including large scale experiments on the\nImageNet classification benchmark. \n\n"}
{"id": "1406.2255", "contents": "Title: Energy-Efficient Cooperative Cognitive Relaying Schemes for Cognitive\n  Radio Networks Abstract: We investigate a cognitive radio network in which a primary user (PU) may\ncooperate with a cognitive radio user (i.e., a secondary user (SU)) for\ntransmissions of its data packets. The PU is assumed to be a buffered node\noperating in a time-slotted fashion where the time is partitioned into\nequal-length slots. We develop two schemes which involve cooperation between\nprimary and secondary users. To satisfy certain quality of service (QoS)\nrequirements, users share time slot duration and channel frequency bandwidth.\nMoreover, the SU may leverage the primary feedback message to further increase\nboth its data rate and satisfy the PU QoS requirements. The proposed\ncooperative schemes are designed such that the SU data rate is maximized under\nthe constraint that the PU average queueing delay is maintained less than the\naverage queueing delay in case of non-cooperative PU. In addition, the proposed\nschemes guarantee the stability of the PU queue and maintain the average energy\nemitted by the SU below a certain value. The proposed schemes also provide more\nrobust and potentially continuous service for SUs compared to the conventional\npractice in cognitive networks where SUs transmit in the spectrum holes and\nsilence sessions of the PUs. We include primary source burstiness, sensing\nerrors, and feedback decoding errors to the analysis of our proposed\ncooperative schemes. The optimization problems are solved offline and require a\nsimple 2-dimensional grid-based search over the optimization variables.\nNumerical results show the beneficial gains of the cooperative schemes in terms\nof SU data rate and PU throughput, average PU queueing delay, and average PU\nenergy savings. \n\n"}
{"id": "1406.4928", "contents": "Title: Diversity Multiplexing Tradeoff of the Half-duplex Slow Fading Multiple\n  Access Channel based on Generalized Quantize-and-Forward Scheme Abstract: This paper investigates the Diversity Multiplexing Tradeoff (DMT) of the\ngeneralized quantize-and-forward (GQF) relaying scheme over the slow fading\nhalf-duplex multiple-access relay channel (HD-MARC). The compress-and-forward\n(CF) scheme has been shown to achieve the optimal DMT when the channel state\ninformation (CSI) of the relay-destination link is available at the relay.\nHowever, having the CSI of relay-destination link at relay is not always\npossible due to the practical considerations of the wireless system. In\ncontrast, in this work, the DMT of the GQF scheme is derived without\nrelay-destination link CSI at the relay. It is shown that even without\nknowledge of relay-destination CSI, the GQF scheme achieves the same DMT,\nachievable by CF scheme with full knowledge of CSI. \n\n"}
{"id": "1406.5582", "contents": "Title: Optimal Offline Packet Scheduling in Energy Harvesting 2-user Multiple\n  Access Channel with Common Data Abstract: The lifetime and the sustainability of the wireless sensor networks (WSNs)\ncan be increased with energy harvesting transmitters utilizing optimum packet\nscheduling. On the other hand, WSNs are observed to collect spatially or\ntemporally correlated data which should be taken into account for the optimum\npacket scheduling in an energy harvesting system. However, the solutions\navailable for 2-user multiple-access channel (MAC) systems with energy\nharvesting transmitters do not consider the common data or the correlation\namong the data. In this paper, optimal packet scheduling for energy harvesting\n2-user Gaussian MAC with common data is achieved by assuming deterministic\nknowledge of the data and energy packets, i.e., offline solution. The optimum\ndeparture region is found by using Karush- Kuhn-Tucker (KKT) conditions\ngeneralizing the solutions obtained for the MAC without common data. An\nefficient iterative backward water-filling algorithm is defined. The optimum\nsolution is numerically compared with the case of no scheduling, uniform power\nscheduling and the previous solutions defined for the MAC without common data\nby showing the improvement obtained with the optimization. \n\n"}
{"id": "1406.6669", "contents": "Title: A note on the causality of singular linear discrete time systems Abstract: In this article we study the causality of non-homogeneous linear singular\ndiscrete time systems whose coefficients are square constant matrices. By\nassuming that the input vector changes only at equally space sampling instants\nwe provide properties for causality between state and inputs and causality\nbetween output and inputs. \n\n"}
{"id": "1407.1424", "contents": "Title: Cross Layer Provision of Future Cellular Networks Abstract: To cope with the growing demand for wireless data and to extend service\ncoverage, future 5G networks will increasingly rely on the use of low powered\nnodes to support massive connectivity in diverse set of applications and\nservices [1]. To this end, virtualized and mass-scale cloud architectures are\nproposed as promising technologies for 5G in which all the nodes are connected\nvia a backhaul network and managed centrally by such cloud centers. The\nsignificant computing power made available by the cloud technologies has\nenabled the implementation of sophisticated signal processing algorithms,\nespecially by way of parallel processing, for both interference management and\nnetwork provision. The latter two are among the major signal processing tasks\nfor 5G due to increased level of frequency sharing, node density, interference\nand network congestion. This article outlines several theoretical and practical\naspects of joint interference management and network provisioning for future 5G\nnetworks. A cross-layer optimization framework is proposed for joint user\nadmission, user-base station association, power control, user grouping,\ntransceiver design as well as routing and flow control. We show that many of\nthese cross-layer tasks can be treated in a unified way and implemented in a\nparallel manner using an efficient algorithmic framework called WMMSE (Weighted\nMMSE). Some recent developments in this area are highlighted and future\nresearch directions are identified. \n\n"}
{"id": "1407.6723", "contents": "Title: Douglas-Rachford Splitting: Complexity Estimates and Accelerated\n  Variants Abstract: We propose a new approach for analyzing convergence of the Douglas-Rachford\nsplitting method for solving convex composite optimization problems. The\napproach is based on a continuously differentiable function, the\nDouglas-Rachford Envelope (DRE), whose stationary points correspond to the\nsolutions of the original (possibly nonsmooth) problem. By proving the\nequivalence between the Douglas-Rachford splitting method and a scaled gradient\nmethod applied to the DRE, results from smooth unconstrained optimization are\nemployed to analyze convergence properties of DRS, to tune the method and to\nderive an accelerated version of it. \n\n"}
{"id": "1408.0377", "contents": "Title: Layered, Exact-Repair Regenerating Codes Via Embedded Error Correction\n  and Block Designs Abstract: A new class of exact-repair regenerating codes is constructed by stitching\ntogether shorter erasure correction codes, where the stitching pattern can be\nviewed as block designs. The proposed codes have the \"help-by-transfer\"\nproperty where the helper nodes simply transfer part of the stored data\ndirectly, without performing any computation. This embedded error correction\nstructure makes the decoding process straightforward, and in some cases the\ncomplexity is very low. We show that this construction is able to achieve\nperformance better than space-sharing between the minimum storage regenerating\ncodes and the minimum repair-bandwidth regenerating codes, and it is the first\nclass of codes to achieve this performance. In fact, it is shown that the\nproposed construction can achieve a non-trivial point on the optimal\nfunctional-repair tradeoff, and it is asymptotically optimal at high rate,\ni.e., it asymptotically approaches the minimum storage and the minimum\nrepair-bandwidth simultaneously. \n\n"}
{"id": "1408.0847", "contents": "Title: Optimality of doubly reflected Levy processes in singular control Abstract: We consider a class of two-sided singular control problems. A controller\neither increases or decreases a given spectrally negative Levy process so as to\nminimize the total costs comprising of the running and control costs where the\nlatter is proportional to the size of control. We provide a sufficient\ncondition for the optimality of a double barrier strategy, and in particular\nshow that it holds when the running cost function is convex. Using the\nfluctuation theory of doubly reflected Levy processes, we express concisely the\noptimal strategy as well as the value function using the scale function.\nNumerical examples are provided to confirm the analytical results. \n\n"}
{"id": "1408.3693", "contents": "Title: Stability and Performance Limits of Adaptive Primal-Dual Networks Abstract: This work studies distributed primal-dual strategies for adaptation and\nlearning over networks from streaming data. Two first-order methods are\nconsidered based on the Arrow-Hurwicz (AH) and augmented Lagrangian (AL)\ntechniques. Several revealing results are discovered in relation to the\nperformance and stability of these strategies when employed over adaptive\nnetworks. The conclusions establish that the advantages that these methods have\nfor deterministic optimization problems do not necessarily carry over to\nstochastic optimization problems. It is found that they have narrower stability\nranges and worse steady-state mean-square-error performance than primal methods\nof the consensus and diffusion type. It is also found that the AH technique can\nbecome unstable under a partial observation model, while the other techniques\nare able to recover the unknown under this scenario. A method to enhance the\nperformance of AL strategies is proposed by tying the selection of the\nstep-size to their regularization parameter. It is shown that this method\nallows the AL algorithm to approach the performance of consensus and diffusion\nstrategies but that it remains less stable than these other strategies. \n\n"}
{"id": "1408.4859", "contents": "Title: Optimal Switching Synthesis for Jump Linear Systems with Gaussian\n  initial state uncertainty Abstract: This paper provides a method to design an optimal switching sequence for jump\nlinear systems with given Gaussian initial state uncertainty. In the practical\nperspective, the initial state contains some uncertainties that come from\nmeasurement errors or sensor inaccuracies and we assume that the type of this\nuncertainty has the form of Gaussian distribution. In order to cope with\nGaussian initial state uncertainty and to measure the system performance,\nWasserstein metric that defines the distance between probability density\nfunctions is used. Combining with the receding horizon framework, an optimal\nswitching sequence for jump linear systems can be obtained by minimizing the\nobjective function that is expressed in terms of Wasserstein distance. The\nproposed optimal switching synthesis also guarantees the mean square stability\nfor jump linear systems. The validations of the proposed methods are verified\nby examples. \n\n"}
{"id": "1408.5468", "contents": "Title: A Sytematic Piggybacking Design for Minimum Storage Regenerating Codes Abstract: Piggybacking is an efficient method to decrease the repair bandwidth of\nMaximum Distance Separable (MDS) codes or Minimum Storage Regenerating (MSR)\ncodes. In this paper, for minimizing the repair bandwidth of parity nodes of\nthe known MSR codes with high rate, which is usually the whole size of the\noriginal data, i.e., the maximal, a new systematic piggybacking design is\nproposed through an in-depth analysis of the design of piggybacking. As a\nresult, new MSR codes are obtained with almost optimal repair bandwidth of\nparity nodes while retaining the optimal repair bandwidth of systematic nodes.\nFurthermore, MSR codes with balanced download during node repair process are\npresented based on the new piggybacking design. \n\n"}
{"id": "1408.6799", "contents": "Title: Stochastic Perron for stochastic target games Abstract: We extend the stochastic Perron method to analyze the framework of stochastic\ntarget games, in which one player tries to find a strategy such that the state\nprocess almost surely reaches a given target no matter which action is chosen\nby the other player. Within this framework, our method produces a viscosity\nsub-solution (super-solution) of a Hamilton-Jacobi-Bellman (HJB) equation. We\nthen characterize the value function as a viscosity solution to the HJB\nequation using a comparison result and a byproduct to obtain the dynamic\nprogramming principle. \n\n"}
{"id": "1409.1606", "contents": "Title: Power Optimal Non-contiguous Spectrum Access in Multi Front End Radio\n  Enabled Point-to-Point Link Abstract: Non-contiguous spectrum chunks allow wireless links to flexibly access a wide\namount of bandwidth. Multi- Channel Multi-Radio (MC-MR) and Non-Contiguous\nOrthogonal Frequency Division Multiplexing (NC-OFDM) are the two commercially\nviable strategies to access non-contiguous spectrum chunks. MC-MR accesses\nmultiple non-contiguous chunks by activating multiple front ends which, in\nturn, increases the circuit power consumption of each of the activated front\nends. NC-OFDM accesses non-contiguous spectrum chunks with a single front end\nby nulling remaining subchannels but increases spectrum span which, in turn,\nincreases the power consumption of ADC and DAC. This work focuses on a\npoint-to-point link where transmitter and receiver have multiple front ends and\ncan employ NC-OFDM technology. We investigate optimal spectrum fragmentation in\neach front end from a system power (summation of transmit power and circuit\npower) perspective. We formulate a mixed integer non-linear program (MINLP) to\nperform power control and scheduling, and minimize system power by providing a\ngreedy algorithm (O(M^3 I)) where M and I denote the number of channels and\nradio front ends respectively. \n\n"}
{"id": "1409.2177", "contents": "Title: The Large Margin Mechanism for Differentially Private Maximization Abstract: A basic problem in the design of privacy-preserving algorithms is the private\nmaximization problem: the goal is to pick an item from a universe that\n(approximately) maximizes a data-dependent function, all under the constraint\nof differential privacy. This problem has been used as a sub-routine in many\nprivacy-preserving algorithms for statistics and machine-learning.\n  Previous algorithms for this problem are either range-dependent---i.e., their\nutility diminishes with the size of the universe---or only apply to very\nrestricted function classes. This work provides the first general-purpose,\nrange-independent algorithm for private maximization that guarantees\napproximate differential privacy. Its applicability is demonstrated on two\nfundamental tasks in data mining and machine learning. \n\n"}
{"id": "1409.3836", "contents": "Title: Hardness of parameter estimation in graphical models Abstract: We consider the problem of learning the canonical parameters specifying an\nundirected graphical model (Markov random field) from the mean parameters. For\ngraphical models representing a minimal exponential family, the canonical\nparameters are uniquely determined by the mean parameters, so the problem is\nfeasible in principle. The goal of this paper is to investigate the\ncomputational feasibility of this statistical task. Our main result shows that\nparameter estimation is in general intractable: no algorithm can learn the\ncanonical parameters of a generic pair-wise binary graphical model from the\nmean parameters in time bounded by a polynomial in the number of variables\n(unless RP = NP). Indeed, such a result has been believed to be true (see the\nmonograph by Wainwright and Jordan (2008)) but no proof was known.\n  Our proof gives a polynomial time reduction from approximating the partition\nfunction of the hard-core model, known to be hard, to learning approximate\nparameters. Our reduction entails showing that the marginal polytope boundary\nhas an inherent repulsive property, which validates an optimization procedure\nover the polytope that does not use any knowledge of its structure (as required\nby the ellipsoid method and others). \n\n"}
{"id": "1409.6911", "contents": "Title: Do More Dropouts in Pool5 Feature Maps for Better Object Detection Abstract: Deep Convolutional Neural Networks (CNNs) have gained great success in image\nclassification and object detection. In these fields, the outputs of all layers\nof CNNs are usually considered as a high dimensional feature vector extracted\nfrom an input image and the correspondence between finer level feature vectors\nand concepts that the input image contains is all-important. However, fewer\nstudies focus on this deserving issue. On considering the correspondence, we\npropose a novel approach which generates an edited version for each original\nCNN feature vector by applying the maximum entropy principle to abandon\nparticular vectors. These selected vectors correspond to the unfriendly\nconcepts in each image category. The classifier trained from merged feature\nsets can significantly improve model generalization of individual categories\nwhen training data is limited. The experimental results for\nclassification-based object detection on canonical datasets including VOC 2007\n(60.1%), 2010 (56.4%) and 2012 (56.3%) show obvious improvement in mean average\nprecision (mAP) with simple linear support vector machines. \n\n"}
{"id": "1410.1830", "contents": "Title: Controllability and Fraction of Leaders in Infinite Network Abstract: In this paper, we study controllability of a network of linear\nsingle-integrator agents when the network size goes to infinity. We first\ninvestigate the effect of increasing size by injecting an input at every node\nand requiring that network controllability Gramian remain well-conditioned with\nthe increasing dimension. We provide theoretical justification to the intuition\nthat high degree nodes pose a challenge to network controllability. In\nparticular, the controllability Gramian for the networks with bounded maximum\ndegrees is shown to remain well-conditioned even as the network size goes to\ninfinity. In the canonical cases of star, chain and ring networks, we also\nprovide closed-form expressions which bound the condition number of the\ncontrollability Gramian in terms of the network size. We next consider the\neffect of the choice and number of leader nodes by actuating only a subset of\nnodes and considering the least eigenvalue of the Gramian as the network size\nincreases. Accordingly, while a directed star topology can never be made\ncontrollable for all sizes by injecting an input just at a fraction $f<1$ of\nnodes; for path or cycle networks, the designer can actuate a non-zero fraction\nof nodes and spread them throughout the network in such way that the least\neigenvalue of the Gramians remain bounded away from zero with the increasing\nsize. The results offer interesting insights on the challenges of control in\nlarge networks and with high-degree nodes. \n\n"}
{"id": "1410.2861", "contents": "Title: Multiuser Joint Energy-Bandwidth Allocation with Energy Harvesting -\n  Part I: Optimum Algorithm & Multiple Point-to-Point Channels Abstract: In this paper, we develop optimal energy-bandwidth allocation algorithms in\nfading channels for multiple energy harvesting transmitters, each may\ncommunicate with multiple receivers via orthogonal channels. We first assume\nthat the side information of both the channel states and the energy harvesting\nstates is known for $K$ time slots {\\em a priori}, and the battery capacity and\nthe maximum transmission power in each time slot are bounded. The objective is\nto maximize the weighted sum-rate of all transmitters over the $K$ time slots\nby assigning the transmission power and bandwidth for each transmitter in each\nslot. The problem is formulated as a convex optimization problem with ${\\cal\nO}(MK)$ constraints, where $M$ is the number of the receivers, making it hard\nto solve with a generic convex solver. An iterative algorithm is proposed that\nalternatively solves two subproblems in each iteration. The convergence and the\noptimality of this algorithm are also shown. We then consider the special case\nthat each transmitter only communicates with one receiver and the objective is\nto maximize the total throughput. We develop efficient algorithms for solving\nthe two subproblems and the optimal energy-bandwidth allocation can be obtained\nwith an overall complexity of ${\\cal O}(MK^2)$. Moreover, a heuristic algorithm\nis also proposed for energy-bandwidth allocation based on causal information of\nchannel and energy harvesting states. \n\n"}
{"id": "1410.3538", "contents": "Title: Dynamic Programming Principle for Stochastic Recursive Optimal Control\n  Problem under G-framework Abstract: In this paper, we study a stochastic recursive optimal control problem in\nwhich the cost functional is described by the solution of a backward stochastic\ndifferential equation driven by G-Brownian motion. Under standard assumptions,\nwe establish the dynamic programming principle and the related fully nonlinear\nHJB equation in the framework of G-expectation. Finally, we show that the value\nfunction is the viscosity solution of the obtained HJB equation. \n\n"}
{"id": "1410.6313", "contents": "Title: Canonical Polyadic Decomposition with Auxiliary Information for Brain\n  Computer Interface Abstract: Physiological signals are often organized in the form of multiple dimensions\n(e.g., channel, time, task, and 3D voxel), so it is better to preserve original\norganization structure when processing. Unlike vector-based methods that\ndestroy data structure, Canonical Polyadic Decomposition (CPD) aims to process\nphysiological signals in the form of multi-way array, which considers\nrelationships between dimensions and preserves structure information contained\nby the physiological signal. Nowadays, CPD is utilized as an unsupervised\nmethod for feature extraction in a classification problem. After that, a\nclassifier, such as support vector machine, is required to classify those\nfeatures. In this manner, classification task is achieved in two isolated\nsteps. We proposed supervised Canonical Polyadic Decomposition by directly\nincorporating auxiliary label information during decomposition, by which a\nclassification task can be achieved without an extra step of classifier\ntraining. The proposed method merges the decomposition and classifier learning\ntogether, so it reduces procedure of classification task compared with that of\nrespective decomposition and classification. In order to evaluate the\nperformance of the proposed method, three different kinds of signals, synthetic\nsignal, EEG signal, and MEG signal, were used. The results based on evaluations\nof synthetic and real signals demonstrated that the proposed method is\neffective and efficient. \n\n"}
{"id": "1410.6447", "contents": "Title: Density-Based Region Search with Arbitrary Shape for Object Localization Abstract: Region search is widely used for object localization. Typically, the region\nsearch methods project the score of a classifier into an image plane, and then\nsearch the region with the maximal score. The recently proposed region search\nmethods, such as efficient subwindow search and efficient region search, %which\nlocalize objects from the score distribution on an image are much more\nefficient than sliding window search. However, for some classifiers and tasks,\nthe projected scores are nearly all positive, and hence maximizing the score of\na region results in localizing nearly the entire images as objects, which is\nmeaningless.\n  In this paper, we observe that the large scores are mainly concentrated on or\naround objects. Based on this observation, we propose a method, named level set\nmaximum-weight connected subgraph (LS-MWCS), which localizes objects with\narbitrary shapes by searching regions with the densest score rather than the\nmaximal score. The region density can be controlled by a parameter flexibly.\nAnd we prove an important property of the proposed LS-MWCS, which guarantees\nthat the region with the densest score can be searched. Moreover, the LS-MWCS\ncan be efficiently optimized by belief propagation. The method is evaluated on\nthe problem of weakly-supervised object localization, and the quantitative\nresults demonstrate the superiorities of our LS-MWCS compared to other\nstate-of-the-art methods. \n\n"}
{"id": "1410.6913", "contents": "Title: Low rank matrix recovery from rank one measurements Abstract: We study the recovery of Hermitian low rank matrices $X \\in \\mathbb{C}^{n\n\\times n}$ from undersampled measurements via nuclear norm minimization. We\nconsider the particular scenario where the measurements are Frobenius inner\nproducts with random rank-one matrices of the form $a_j a_j^*$ for some\nmeasurement vectors $a_1,...,a_m$, i.e., the measurements are given by $y_j =\n\\mathrm{tr}(X a_j a_j^*)$. The case where the matrix $X=x x^*$ to be recovered\nis of rank one reduces to the problem of phaseless estimation (from\nmeasurements, $y_j = |\\langle x,a_j\\rangle|^2$ via the PhaseLift approach,\nwhich has been introduced recently. We derive bounds for the number $m$ of\nmeasurements that guarantee successful uniform recovery of Hermitian rank $r$\nmatrices, either for the vectors $a_j$, $j=1,...,m$, being chosen independently\nat random according to a standard Gaussian distribution, or $a_j$ being sampled\nindependently from an (approximate) complex projective $t$-design with $t=4$.\nIn the Gaussian case, we require $m \\geq C r n$ measurements, while in the case\nof $4$-designs we need $m \\geq Cr n \\log(n)$. Our results are uniform in the\nsense that one random choice of the measurement vectors $a_j$ guarantees\nrecovery of all rank $r$-matrices simultaneously with high probability.\nMoreover, we prove robustness of recovery under perturbation of the\nmeasurements by noise. The result for approximate $4$-designs generalizes and\nimproves a recent bound on phase retrieval due to Gross, Kueng and Krahmer. In\naddition, it has applications in quantum state tomography. Our proofs employ\nthe so-called bowling scheme which is based on recent ideas by Mendelson and\nKoltchinskii. \n\n"}
{"id": "1410.8419", "contents": "Title: Optimal Opinion Control: The Campaign Problem Abstract: Opinion dynamics is nowadays a very common field of research. In this article\nwe formulate and then study a novel, namely strategic perspective on such\ndynamics: There are the usual normal agents that update their opinions, for\ninstance according the well-known bounded confidence mechanism. But,\nadditionally, there is at least one strategic agent. That agent uses opinions\nas freely selectable strategies to get control on the dynamics: The strategic\nagent of our benchmark problem tries, during a campaign of a certain length, to\ninfluence the ongoing dynamics among normal agents with strategically placed\nopinions (one per period) in such a way, that, by the end of the campaign, as\nmuch as possible normals end up with opinions in a certain interval of the\nopinion space. Structurally, such a problem is an optimal control problem. That\ntype of problem is ubiquitous. Resorting to advanced and partly non-standard\nmethods for computing optimal controls, we solve some instances of the campaign\nproblem. But even for a very small number of normal agents, just one strategic\nagent, and a ten-period campaign length, the problem turns out to be extremely\ndifficult. Explicitly we discuss moral and political concerns that immediately\narise, if someone starts to analyze the possibilities of an optimal opinion\ncontrol. \n\n"}
{"id": "1411.0114", "contents": "Title: On the Transmit Beamforming for MIMO Wiretap Channels: Large-System\n  Analysis Abstract: With the growth of wireless networks, security has become a fundamental issue\nin wireless communications due to the broadcast nature of these networks. In\nthis work, we consider MIMO wiretap channels in a fast fading environment, for\nwhich the overall performance is characterized by the ergodic MIMO secrecy\nrate. Unfortunately, the direct solution to finding ergodic secrecy rates is\nprohibitive due to the expectations in the rates expressions in this setting.\nTo overcome this difficulty, we invoke the large-system assumption, which\nallows a deterministic approximation to the ergodic mutual information.\nLeveraging results from random matrix theory, we are able to characterize the\nachievable ergodic secrecy rates. Based on this characterization, we address\nthe problem of covariance optimization at the transmitter. Our numerical\nresults demonstrate a good match between the large-system approximation and the\nactual simulated secrecy rates, as well as some interesting features of the\nprecoder optimization. \n\n"}
{"id": "1411.1045", "contents": "Title: Deep Gaze I: Boosting Saliency Prediction with Feature Maps Trained on\n  ImageNet Abstract: Recent results suggest that state-of-the-art saliency models perform far from\noptimal in predicting fixations. This lack in performance has been attributed\nto an inability to model the influence of high-level image features such as\nobjects. Recent seminal advances in applying deep neural networks to tasks like\nobject recognition suggests that they are able to capture this kind of\nstructure. However, the enormous amount of training data necessary to train\nthese networks makes them difficult to apply directly to saliency prediction.\nWe present a novel way of reusing existing neural networks that have been\npretrained on the task of object recognition in models of fixation prediction.\nUsing the well-known network of Krizhevsky et al. (2012), we come up with a new\nsaliency model that significantly outperforms all state-of-the-art models on\nthe MIT Saliency Benchmark. We show that the structure of this network allows\nnew insights in the psychophysics of fixation selection and potentially their\nneural implementation. To train our network, we build on recent work on the\nmodeling of saliency as point processes. \n\n"}
{"id": "1411.2876", "contents": "Title: Stochastic Intermediate Gradient Method for Convex Problems with Inexact\n  Stochastic Oracle Abstract: In this paper we introduce new methods for convex optimization problems with\ninexact stochastic oracle. First method is an extension of the intermediate\ngradient method proposed by Devolder, Glineur and Nesterov for problems with\ninexact oracle. Our new method can be applied to the problems with composite\nstructure, stochastic inexact oracle and allows using non-Euclidean setup. We\nprove estimates for mean rate of convergence and probabilities of large\ndeviations from this rate. Also we introduce two modifications of this method\nfor strongly convex problems. For the first modification we prove mean rate of\nconvergence estimates and for the second we prove estimates for large\ndeviations from the mean rate of convergence. All the rates give the complexity\nestimates for proposed methods which up to multiplicative constant coincide\nwith lower complexity bound for the considered class of convex composite\noptimization problems with stochastic inexact oracle. \n\n"}
{"id": "1411.4033", "contents": "Title: Sparse And Low Rank Decomposition Based Batch Image Alignment for\n  Speckle Reduction of retinal OCT Images Abstract: Optical Coherence Tomography (OCT) is an emerging technique in the field of\nbiomedical imaging, with applications in ophthalmology, dermatology, coronary\nimaging etc. Due to the underlying physics, OCT images usually suffer from a\ngranular pattern, called speckle noise, which restricts the process of\ninterpretation. Here, a sparse and low rank decomposition based method is used\nfor speckle reduction in retinal OCT images. This technique works on input data\nthat consists of several B-scans of the same location. The next step is the\nbatch alignment of the images using a sparse and low-rank decomposition based\ntechnique. Finally the denoised image is created by median filtering of the\nlow-rank component of the processed data. Simultaneous decomposition and\nalignment of the images result in better performance in comparison to simple\nregistration-based methods that are used in the literature for noise reduction\nof OCT images. \n\n"}
{"id": "1411.4253", "contents": "Title: Energy-efficient Decoders for Compressive Sensing: Fundamental Limits\n  and Implementations Abstract: The fundamental problem considered in this paper is \"What is the\n\\textit{energy} consumed for the implementation of a \\emph{compressive sensing}\ndecoding algorithm on a circuit?\". Using the \"information-friction\" framework,\nwe examine the smallest amount of \\textit{bit-meters} as a measure for the\nenergy consumed by a circuit. We derive a fundamental lower bound for the\nimplementation of compressive sensing decoding algorithms on a circuit. In the\nsetting where the number of measurements scales linearly with the sparsity and\nthe sparsity is sub-linear with the length of the signal, we show that the\n\\textit{bit-meters} consumption for these algorithms is order-tight, i.e., it\nmatches the lower bound asymptotically up to a constant factor. Our\nimplementations yield interesting insights into design of energy-efficient\ncircuits that are not captured by the notion of computational efficiency alone. \n\n"}
{"id": "1411.5057", "contents": "Title: Fast Iteratively Reweighted Least Squares Algorithms for Analysis-Based\n  Sparsity Reconstruction Abstract: In this paper, we propose a novel algorithm for analysis-based sparsity\nreconstruction. It can solve the generalized problem by structured sparsity\nregularization with an orthogonal basis and total variation regularization. The\nproposed algorithm is based on the iterative reweighted least squares (IRLS)\nmodel, which is further accelerated by the preconditioned conjugate gradient\nmethod. The convergence rate of the proposed algorithm is almost the same as\nthat of the traditional IRLS algorithms, that is, exponentially fast. Moreover,\nwith the specifically devised preconditioner, the computational cost for each\niteration is significantly less than that of traditional IRLS algorithms, which\nenables our approach to handle large scale problems. In addition to the fast\nconvergence, it is straightforward to apply our method to standard sparsity,\ngroup sparsity, overlapping group sparsity and TV based problems. Experiments\nare conducted on a practical application: compressive sensing magnetic\nresonance imaging. Extensive results demonstrate that the proposed algorithm\nachieves superior performance over 14 state-of-the-art algorithms in terms of\nboth accuracy and computational cost. \n\n"}
{"id": "1411.5328", "contents": "Title: ConceptLearner: Discovering Visual Concepts from Weakly Labeled Image\n  Collections Abstract: Discovering visual knowledge from weakly labeled data is crucial to scale up\ncomputer vision recognition system, since it is expensive to obtain fully\nlabeled data for a large number of concept categories. In this paper, we\npropose ConceptLearner, which is a scalable approach to discover visual\nconcepts from weakly labeled image collections. Thousands of visual concept\ndetectors are learned automatically, without human in the loop for additional\nannotation. We show that these learned detectors could be applied to recognize\nconcepts at image-level and to detect concepts at image region-level\naccurately. Under domain-specific supervision, we further evaluate the learned\nconcepts for scene recognition on SUN database and for object detection on\nPascal VOC 2007. ConceptLearner shows promising performance compared to fully\nsupervised and weakly supervised methods. \n\n"}
{"id": "1411.5731", "contents": "Title: Visual Sentiment Prediction with Deep Convolutional Neural Networks Abstract: Images have become one of the most popular types of media through which users\nconvey their emotions within online social networks. Although vast amount of\nresearch is devoted to sentiment analysis of textual data, there has been very\nlimited work that focuses on analyzing sentiment of image data. In this work,\nwe propose a novel visual sentiment prediction framework that performs image\nunderstanding with Deep Convolutional Neural Networks (CNN). Specifically, the\nproposed sentiment prediction framework performs transfer learning from a CNN\nwith millions of parameters, which is pre-trained on large-scale data for\nobject recognition. Experiments conducted on two real-world datasets from\nTwitter and Tumblr demonstrate the effectiveness of the proposed visual\nsentiment analysis framework. \n\n"}
{"id": "1411.6282", "contents": "Title: Multi-input control-affine systems static feedback equivalent to a\n  triangular form and their flatness Abstract: In this paper, we give a complete geometric characterization of control\nsystems, with m+1 inputs, locally static feedback equivalent to a triangular\nform compatible with the chained form, for m=1, respectively with the m-chained\nform, for m>1. They are x-flat systems. We provide a system of first order\nPDE's to be solved in order to find all x-flat outputs, for m=1, respectively\nall minimal x-flat outputs, for m>1. We illustrate our results by examples, in\nparticular by an application to a mechanical system: the coin rolling without\nslipping on a moving table. \n\n"}
{"id": "1412.2669", "contents": "Title: Two step recovery of jointly sparse and low-rank matrices: theoretical\n  guarantees Abstract: We introduce a two step algorithm with theoretical guarantees to recover a\njointly sparse and low-rank matrix from undersampled measurements of its\ncolumns. The algorithm first estimates the row subspace of the matrix using a\nset of common measurements of the columns. In the second step, the subspace\naware recovery of the matrix is solved using a simple least square algorithm.\nThe results are verified in the context of recovering CINE data from\nundersampled measurements; we obtain good recovery when the sampling conditions\nare satisfied. \n\n"}
{"id": "1412.4237", "contents": "Title: First order algorithms in variational image processing Abstract: Variational methods in imaging are nowadays developing towards a quite\nuniversal and flexible tool, allowing for highly successful approaches on tasks\nlike denoising, deblurring, inpainting, segmentation, super-resolution,\ndisparity, and optical flow estimation. The overall structure of such\napproaches is of the form ${\\cal D}(Ku) + \\alpha {\\cal R} (u) \\rightarrow\n\\min_u$ ; where the functional ${\\cal D}$ is a data fidelity term also\ndepending on some input data $f$ and measuring the deviation of $Ku$ from such\nand ${\\cal R}$ is a regularization functional. Moreover $K$ is a (often linear)\nforward operator modeling the dependence of data on an underlying image, and\n$\\alpha$ is a positive regularization parameter. While ${\\cal D}$ is often\nsmooth and (strictly) convex, the current practice almost exclusively uses\nnonsmooth regularization functionals. The majority of successful techniques is\nusing nonsmooth and convex functionals like the total variation and\ngeneralizations thereof or $\\ell_1$-norms of coefficients arising from scalar\nproducts with some frame system. The efficient solution of such variational\nproblems in imaging demands for appropriate algorithms. Taking into account the\nspecific structure as a sum of two very different terms to be minimized,\nsplitting algorithms are a quite canonical choice. Consequently this field has\nrevived the interest in techniques like operator splittings or augmented\nLagrangians. Here we shall provide an overview of methods currently developed\nand recent results as well as some computational studies providing a comparison\nof different methods and also illustrating their success in applications. \n\n"}
{"id": "1412.4659", "contents": "Title: Finding a sparse vector in a subspace: Linear sparsity using alternating\n  directions Abstract: Is it possible to find the sparsest vector (direction) in a generic subspace\n$\\mathcal{S} \\subseteq \\mathbb{R}^p$ with $\\mathrm{dim}(\\mathcal{S})= n < p$?\nThis problem can be considered a homogeneous variant of the sparse recovery\nproblem, and finds connections to sparse dictionary learning, sparse PCA, and\nmany other problems in signal processing and machine learning. In this paper,\nwe focus on a **planted sparse model** for the subspace: the target sparse\nvector is embedded in an otherwise random subspace. Simple convex heuristics\nfor this planted recovery problem provably break down when the fraction of\nnonzero entries in the target sparse vector substantially exceeds\n$O(1/\\sqrt{n})$. In contrast, we exhibit a relatively simple nonconvex approach\nbased on alternating directions, which provably succeeds even when the fraction\nof nonzero entries is $\\Omega(1)$. To the best of our knowledge, this is the\nfirst practical algorithm to achieve linear scaling under the planted sparse\nmodel. Empirically, our proposed algorithm also succeeds in more challenging\ndata models, e.g., sparse dictionary learning. \n\n"}
{"id": "1412.5551", "contents": "Title: Statistical Modeling and Performance Characterization of an Ultrafast\n  Digital Lightwave Communication System Using a Power-Cubic Optical Nonlinear\n  Preprocessor (Extended Version) Abstract: In this paper, we present an analytical approach in obtaining the probability\ndensity function (pdf) of the random decision variable Y, formed at the output\nof power-cubic all-optical nonlinear preprocessor followed by the\nphotodetector. Our approach can be used to accurately evaluate the performance\nof ultrafast pulse detection in the presence of Gaussian noise. Through\nrigorous Monte-Carlo simulation, the accuracy of widely used Gaussian\napproximation of decision variable Y is refuted. However, in this paper we show\nthat the so called Log-Pearson type-3 probability density function (LP3 pdf) is\nan excellent representation for the decision variable Y . Three distinguishable\nparameters of the LP3 pdf are obtained through analytical derivation of three\nmoments of the decision variable Y . Furthermore, toward a more realistic\nmodel, in addition to ASE Gaussian noise, the effects of shot and thermal\nnoises are also included. Finally, using the presented analytical approach, it\nis shown that power-cubic preprocessor outperforms its quadratic counterparts,\ni.e., Second Harmonic Generation (SHG) and Two Photon Absorption (TPA) devices,\nin high power regime where shot and thermal noises can be neglected. \n\n"}
{"id": "1412.6279", "contents": "Title: Non-parametric PSF estimation from celestial transit solar images using\n  blind deconvolution Abstract: Context: Characterization of instrumental effects in astronomical imaging is\nimportant in order to extract accurate physical information from the\nobservations. The measured image in a real optical instrument is usually\nrepresented by the convolution of an ideal image with a Point Spread Function\n(PSF). Additionally, the image acquisition process is also contaminated by\nother sources of noise (read-out, photon-counting). The problem of estimating\nboth the PSF and a denoised image is called blind deconvolution and is\nill-posed.\n  Aims: We propose a blind deconvolution scheme that relies on image\nregularization. Contrarily to most methods presented in the literature, our\nmethod does not assume a parametric model of the PSF and can thus be applied to\nany telescope.\n  Methods: Our scheme uses a wavelet analysis prior model on the image and weak\nassumptions on the PSF. We use observations from a celestial transit, where the\nocculting body can be assumed to be a black disk. These constraints allow us to\nretain meaningful solutions for the filter and the image, eliminating trivial,\ntranslated and interchanged solutions. Under an additive Gaussian noise\nassumption, they also enforce noise canceling and avoid reconstruction\nartifacts by promoting the whiteness of the residual between the blurred\nobservations and the cleaned data.\n  Results: Our method is applied to synthetic and experimental data. The PSF is\nestimated for the SECCHI/EUVI instrument using the 2007 Lunar transit, and for\nSDO/AIA using the 2012 Venus transit. Results show that the proposed\nnon-parametric blind deconvolution method is able to estimate the core of the\nPSF with a similar quality to parametric methods proposed in the literature. We\nalso show that, if these parametric estimations are incorporated in the\nacquisition model, the resulting PSF outperforms both the parametric and\nnon-parametric methods. \n\n"}
{"id": "1412.8419", "contents": "Title: Simple Image Description Generator via a Linear Phrase-Based Approach Abstract: Generating a novel textual description of an image is an interesting problem\nthat connects computer vision and natural language processing. In this paper,\nwe present a simple model that is able to generate descriptive sentences given\na sample image. This model has a strong focus on the syntax of the\ndescriptions. We train a purely bilinear model that learns a metric between an\nimage representation (generated from a previously trained Convolutional Neural\nNetwork) and phrases that are used to described them. The system is then able\nto infer phrases from a given image sample. Based on caption syntax statistics,\nwe propose a simple language model that can produce relevant descriptions for a\ngiven test image using the phrases inferred. Our approach, which is\nconsiderably simpler than state-of-the-art models, achieves comparable results\non the recently release Microsoft COCO dataset. \n\n"}
{"id": "1501.00630", "contents": "Title: Non-iterative rigid 2D/3D point-set registration using semidefinite\n  programming Abstract: We describe a convex programming framework for pose estimation in 2D/3D\npoint-set registration with unknown point correspondences. We give two\nmixed-integer nonlinear program (MINP) formulations of the 2D/3D registration\nproblem when there are multiple 2D images, and propose convex relaxations for\nboth of the MINPs to semidefinite programs (SDP) that can be solved efficiently\nby interior point methods. Our approach to the 2D/3D registration problem is\nnon-iterative in nature as we jointly solve for pose and correspondence.\nFurthermore, these convex programs can readily incorporate feature descriptors\nof points to enhance registration results. We prove that the convex programs\nexactly recover the solution to the original nonconvex 2D/3D registration\nproblem under noiseless condition. We apply these formulations to the\nregistration of 3D models of coronary vessels to their 2D projections obtained\nfrom multiple intra-operative fluoroscopic images. For this application, we\nexperimentally corroborate the exact recovery property in the absence of noise\nand further demonstrate robustness of the convex programs in the presence of\nnoise. \n\n"}
{"id": "1501.00680", "contents": "Title: A New Method for Signal and Image Analysis: The Square Wave Method Abstract: A brief review is provided of the use of the Square Wave Method (SWM) in the\nfield of signal and image analysis and it is specified how results thus\nobtained are expressed using the Square Wave Transform (SWT), in the frequency\ndomain. To illustrate the new approach introduced in this field, the results of\ntwo cases are analyzed: a) a sequence of samples (that is, measured values) of\nan electromyographic recording; and b) the classic image of Lenna. \n\n"}
{"id": "1501.02046", "contents": "Title: Multiuser MIMO Wireless Energy Transfer With Coexisting Opportunistic\n  Communication Abstract: This letter considers spectrum sharing between a primary multiuser\nmultiple-input multiple-output (MIMO) wireless energy transfer (WET) system and\na coexisting secondary point-to-point MIMO wireless information transmission\n(WIT) system, where WET generates interference to WIT and degrades its\nthroughput performance. We show that due to the interference, the WIT system\nsuffers from a loss of the degrees of freedom (DoF) proportional to the number\nof energy beams sent by the energy transmitter (ET), which, in general, needs\nto be larger than one in order to optimize the multiuser WET with user fairness\nconsideration. To minimize the DoF loss in WIT, we further propose a new\nsingle-beam energy transmission scheme based on the principle of time sharing,\nwhere the ET transmits one of the optimal energy beams at each time. This new\nscheme achieves the same optimal performance for the WET system, and minimizes\nthe impact of its interference to the WIT system. \n\n"}
{"id": "1501.03407", "contents": "Title: User Association in Massive MIMO HetNets Abstract: Massive MIMO and small cell are both recognized as the key technologies for\nthe future 5G wireless systems. In this paper, we investigate the problem of\nuser association in a heterogeneous network (HetNet) with massive MIMO and\nsmall cells, where the macro base station (BS) is equipped with a massive MIMO\nand the picocell BS's are equipped with regular MIMOs. We first develop\ncentralized user association algorithms with proven optimality, considering\nvarious objectives such as rate maximization, proportional fairness, and joint\nuser association and resource allocation. We then model the massive MIMO HetNet\nas a repeated game, which leads to distributed user association algorithms with\nproven convergence to the Nash Equilibrium (NE). We demonstrate the efficacy of\nthese optimal schemes by comparison with several greedy algorithms through\nsimulations. \n\n"}
{"id": "1501.04191", "contents": "Title: Regularity of collections of sets and convergence of inexact alternating\n  projections Abstract: We study the usage of regularity properties of collections of sets in\nconvergence analysis of alternating projection methods for solving feasibility\nproblems. Several equivalent characterizations of these properties are\nprovided. Two settings of inexact alternating projections are considered and\nthe corresponding convergence estimates are established and discussed. \n\n"}
{"id": "1501.07591", "contents": "Title: Direct solution to constrained tropical optimization problems with\n  application to project scheduling Abstract: We examine a new optimization problem formulated in the tropical mathematics\nsetting as a further extension of certain known problems. The problem is to\nminimize a nonlinear objective function, which is defined on vectors over an\nidempotent semifield by using multiplicative conjugate transposition, subject\nto inequality constraints. As compared to the known problems, the new one has a\nmore general objective function and additional constraints. We provide a\ncomplete solution in an explicit form to the problem by using an approach that\nintroduces an auxiliary variable to represent the values of the objective\nfunction, and then reduces the initial problem to a parametrized vector\ninequality. The minimum of the objective function is evaluated by applying the\nexistence conditions for the solution of this inequality. A complete solution\nto the problem is given by solving the parametrized inequality, provided the\nparameter is set to the minimum value. As a consequence, we obtain solutions to\nnew special cases of the general problem. To illustrate the application of the\nresults, we solve a real-world problem drawn from time-constrained project\nscheduling, and offer a representative numerical example. \n\n"}
{"id": "1501.07873", "contents": "Title: Sketch-a-Net that Beats Humans Abstract: We propose a multi-scale multi-channel deep neural network framework that,\nfor the first time, yields sketch recognition performance surpassing that of\nhumans. Our superior performance is a result of explicitly embedding the unique\ncharacteristics of sketches in our model: (i) a network architecture designed\nfor sketch rather than natural photo statistics, (ii) a multi-channel\ngeneralisation that encodes sequential ordering in the sketching process, and\n(iii) a multi-scale network ensemble with joint Bayesian fusion that accounts\nfor the different levels of abstraction exhibited in free-hand sketches. We\nshow that state-of-the-art deep networks specifically engineered for photos of\nnatural objects fail to perform well on sketch recognition, regardless whether\nthey are trained using photo or sketch. Our network on the other hand not only\ndelivers the best performance on the largest human sketch dataset to date, but\nalso is small in size making efficient training possible using just CPUs. \n\n"}
{"id": "1502.00362", "contents": "Title: Designing Networks: A Mixed-Integer Linear Optimization Approach Abstract: Designing networks with specified collective properties is useful in a\nvariety of application areas, enabling the study of how given properties affect\nthe behavior of network models, the downscaling of empirical networks to\nworkable sizes, and the analysis of network evolution. Despite the importance\nof the task, there currently exists a gap in our ability to systematically\ngenerate networks that adhere to theoretical guarantees for the given property\nspecifications. In this paper, we propose the use of Mixed-Integer Linear\nOptimization modeling and solution methodologies to address this Network\nGeneration Problem. We present a number of useful modeling techniques and apply\nthem to mathematically express and constrain network properties in the context\nof an optimization formulation. We then develop complete formulations for the\ngeneration of networks that attain specified levels of connectivity, spread,\nassortativity and robustness, and we illustrate these via a number of\ncomputational case studies. \n\n"}
{"id": "1502.02925", "contents": "Title: On the Finite Length Scaling of Ternary Polar Codes Abstract: The polarization process of polar codes over a ternary alphabet is studied.\nRecently it has been shown that the scaling of the blocklength of polar codes\nwith prime alphabet size scales polynomially with respect to the inverse of the\ngap between code rate and channel capacity. However, except for the binary\ncase, the degree of the polynomial in the bound is extremely large. In this\nwork, it is shown that a much lower degree polynomial can be computed\nnumerically for the ternary case. Similar results are conjectured for the\ngeneral case of prime alphabet size. \n\n"}
{"id": "1502.04652", "contents": "Title: Inferring 3D Object Pose in RGB-D Images Abstract: The goal of this work is to replace objects in an RGB-D scene with\ncorresponding 3D models from a library. We approach this problem by first\ndetecting and segmenting object instances in the scene using the approach from\nGupta et al. [13]. We use a convolutional neural network (CNN) to predict the\npose of the object. This CNN is trained using pixel normals in images\ncontaining rendered synthetic objects. When tested on real data, it outperforms\nalternative algorithms trained on real data. We then use this coarse pose\nestimate along with the inferred pixel support to align a small number of\nprototypical models to the data, and place the model that fits the best into\nthe scene. We observe a 48% relative improvement in performance at the task of\n3D detection over the current state-of-the-art [33], while being an order of\nmagnitude faster at the same time. \n\n"}
{"id": "1502.05422", "contents": "Title: Non-Markovian optimal stopping problems and constrained BSDEs with jump Abstract: We consider a non-Markovian optimal stopping problem on finite horizon. We\nprove that the value process can be represented by means of a backward\nstochastic differential equation (BSDE), defined on an enlarged probability\nspace, containing a stochastic integral having a one-jump point process as\nintegrator and an (unknown) process with a sign constraint as integrand. This\nprovides an alternative representation with respect to the classical one given\nby a reflected BSDE. The connection between the two BSDEs is also clarified.\nFinally, we prove that the value of the optimal stopping problem is the same as\nthe value of an auxiliary optimization problem where the intensity of the point\nprocess is controlled. \n\n"}
{"id": "1502.05689", "contents": "Title: Unsupervised Network Pretraining via Encoding Human Design Abstract: Over the years, computer vision researchers have spent an immense amount of\neffort on designing image features for the visual object recognition task. We\npropose to incorporate this valuable experience to guide the task of training\ndeep neural networks. Our idea is to pretrain the network through the task of\nreplicating the process of hand-designed feature extraction. By learning to\nreplicate the process, the neural network integrates previous research\nknowledge and learns to model visual objects in a way similar to the\nhand-designed features. In the succeeding finetuning step, it further learns\nobject-specific representations from labeled data and this boosts its\nclassification power. We pretrain two convolutional neural networks where one\nreplicates the process of histogram of oriented gradients feature extraction,\nand the other replicates the process of region covariance feature extraction.\nAfter finetuning, we achieve substantially better performance than the baseline\nmethods. \n\n"}
{"id": "1502.05789", "contents": "Title: Location Identification of Power Line Outages Using PMU Measurements\n  with Bad Data Abstract: The use of phasor angle measurements provided by phasor measurement units\n(PMUs) in fault detection is regarded as a promising method in identifying\nlocations of power line outages. However, communication errors or system\nmalfunctions may introduce errors to the measurements and thus yield bad data.\nMost of the existing methods on line outage identification fail to consider\nsuch error. This paper develops a framework for identifying multiple power line\noutages based on the PMUs' measurements in the presence of bad data. In\nparticular, we design an algorithm to identify locations of line outage and\nrecover the faulty measurements simultaneously. The proposed algorithm does not\nrequire any prior information on the number of line outages and the noise\nvariance. Case studies carried out on test systems of different sizes validate\nthe effectiveness and efficiency of the proposed approach. \n\n"}
{"id": "1502.06108", "contents": "Title: Don't Just Listen, Use Your Imagination: Leveraging Visual Common Sense\n  for Non-Visual Tasks Abstract: Artificial agents today can answer factual questions. But they fall short on\nquestions that require common sense reasoning. Perhaps this is because most\nexisting common sense databases rely on text to learn and represent knowledge.\nBut much of common sense knowledge is unwritten - partly because it tends not\nto be interesting enough to talk about, and partly because some common sense is\nunnatural to articulate in text. While unwritten, it is not unseen. In this\npaper we leverage semantic common sense knowledge learned from images - i.e.\nvisual common sense - in two textual tasks: fill-in-the-blank and visual\nparaphrasing. We propose to \"imagine\" the scene behind the text, and leverage\nvisual cues from the \"imagined\" scenes in addition to textual cues while\nanswering these questions. We imagine the scenes as a visual abstraction. Our\napproach outperforms a strong text-only baseline on these tasks. Our proposed\ntasks can serve as benchmarks to quantitatively evaluate progress in solving\ntasks that go \"beyond recognition\". Our code and datasets are publicly\navailable. \n\n"}
{"id": "1503.00064", "contents": "Title: Generating Multi-Sentence Lingual Descriptions of Indoor Scenes Abstract: This paper proposes a novel framework for generating lingual descriptions of\nindoor scenes. Whereas substantial efforts have been made to tackle this\nproblem, previous approaches focusing primarily on generating a single sentence\nfor each image, which is not sufficient for describing complex scenes. We\nattempt to go beyond this, by generating coherent descriptions with multiple\nsentences. Our approach is distinguished from conventional ones in several\naspects: (1) a 3D visual parsing system that jointly infers objects,\nattributes, and relations; (2) a generative grammar learned automatically from\ntraining text; and (3) a text generation algorithm that takes into account the\ncoherence among sentences. Experiments on the augmented NYU-v2 dataset show\nthat our framework can generate natural descriptions with substantially higher\nROGUE scores compared to those produced by the baseline. \n\n"}
{"id": "1503.01313", "contents": "Title: A Novel Performance Evaluation Methodology for Single-Target Trackers Abstract: This paper addresses the problem of single-target tracker performance\nevaluation. We consider the performance measures, the dataset and the\nevaluation system to be the most important components of tracker evaluation and\npropose requirements for each of them. The requirements are the basis of a new\nevaluation methodology that aims at a simple and easily interpretable tracker\ncomparison. The ranking-based methodology addresses tracker equivalence in\nterms of statistical significance and practical differences. A fully-annotated\ndataset with per-frame annotations with several visual attributes is\nintroduced. The diversity of its visual properties is maximized in a novel way\nby clustering a large number of videos according to their visual attributes.\nThis makes it the most sophistically constructed and annotated dataset to date.\nA multi-platform evaluation system allowing easy integration of third-party\ntrackers is presented as well. The proposed evaluation methodology was tested\non the VOT2014 challenge on the new dataset and 38 trackers, making it the\nlargest benchmark to date. Most of the tested trackers are indeed\nstate-of-the-art since they outperform the standard baselines, resulting in a\nhighly-challenging benchmark. An exhaustive analysis of the dataset from the\nperspective of tracking difficulty is carried out. To facilitate tracker\ncomparison a new performance visualization technique is proposed. \n\n"}
{"id": "1503.02216", "contents": "Title: Higher order Matching Pursuit for Low Rank Tensor Learning Abstract: Low rank tensor learning, such as tensor completion and multilinear multitask\nlearning, has received much attention in recent years. In this paper, we\npropose higher order matching pursuit for low rank tensor learning problems\nwith a convex or a nonconvex cost function, which is a generalization of the\nmatching pursuit type methods. At each iteration, the main cost of the proposed\nmethods is only to compute a rank-one tensor, which can be done efficiently,\nmaking the proposed methods scalable to large scale problems. Moreover, storing\nthe resulting rank-one tensors is of low storage requirement, which can help to\nbreak the curse of dimensionality. The linear convergence rate of the proposed\nmethods is established in various circumstances. Along with the main methods,\nwe also provide a method of low computational complexity for approximately\ncomputing the rank-one tensors, with provable approximation ratio, which helps\nto improve the efficiency of the main methods and to analyze the convergence\nrate. Experimental results on synthetic as well as real datasets verify the\nefficiency and effectiveness of the proposed methods. \n\n"}
{"id": "1503.02339", "contents": "Title: Multiple and single snapshot compressive beamforming Abstract: For a sound field observed on a sensor array, compressive sensing (CS)\nreconstructs the direction-of-arrival (DOA) of multiple sources using a\nsparsity constraint. The DOA estimation is posed as an underdetermined problem\nby expressing the acoustic pressure at each sensor as a phase-lagged\nsuperposition of source amplitudes at all hypothetical DOAs. Regularizing with\nan $\\ell_1$-norm constraint renders the problem solvable with convex\noptimization, and promoting sparsity gives high-resolution DOA maps. Here, the\nsparse source distribution is derived using maximum a posteriori (MAP)\nestimates for both single and multiple snapshots. CS does not require inversion\nof the data covariance matrix and thus works well even for a single snapshot\nwhere it gives higher resolution than conventional beamforming. For multiple\nsnapshots, CS outperforms conventional high-resolution methods, even with\ncoherent arrivals and at low signal-to-noise ratio. The superior resolution of\nCS is demonstrated with vertical array data from the SWellEx96 experiment for\ncoherent multi-paths. \n\n"}
{"id": "1503.02351", "contents": "Title: Fully Connected Deep Structured Networks Abstract: Convolutional neural networks with many layers have recently been shown to\nachieve excellent results on many high-level tasks such as image\nclassification, object detection and more recently also semantic segmentation.\nParticularly for semantic segmentation, a two-stage procedure is often\nemployed. Hereby, convolutional networks are trained to provide good local\npixel-wise features for the second step being traditionally a more global\ngraphical model. In this work we unify this two-stage process into a single\njoint training algorithm. We demonstrate our method on the semantic image\nsegmentation task and show encouraging results on the challenging PASCAL VOC\n2012 dataset. \n\n"}
{"id": "1503.05113", "contents": "Title: Quantifying Morphological Computation based on an Information\n  Decomposition of the Sensorimotor Loop Abstract: The question how an agent is affected by its embodiment has attracted growing\nattention in recent years. A new field of artificial intelligence has emerged,\nwhich is based on the idea that intelligence cannot be understood without\ntaking into account embodiment. We believe that a formal approach to\nquantifying the embodiment's effect on the agent's behaviour is beneficial to\nthe fields of artificial life and artificial intelligence. The contribution of\nan agent's body and environment to its behaviour is also known as morphological\ncomputation. Therefore, in this work, we propose a quantification of\nmorphological computation, which is based on an information decomposition of\nthe sensorimotor loop into shared, unique and synergistic information. In\nnumerical simulation based on a formal representation of the sensorimotor loop,\nwe show that the unique information of the body and environment is a good\nmeasure for morphological computation. The results are compared to our\npreviously derived quantification of morphological computation. \n\n"}
{"id": "1503.07685", "contents": "Title: The matching problem between functional shapes via a BV penalty term: a\n  $\\Gamma$-convergence result Abstract: This paper proves a $\\Gamma$-convergence result for the discrete energy (to\nthe continuous one) of the matching problem for signals defined on surfaces. In\nparticular, we highlight some geometric properties that must be guaranteed in\nthe discretization process to ensure the convergence of minimizers. The proof\nis given in the framework of functional shapes introduced in \\cite{ABN}. In\nparticular, we consider a varifold-type attachment term, and a $BV$ penalty\nterm is used instead of the original $L^2$ norm. \n\n"}
{"id": "1503.08663", "contents": "Title: Visual Saliency Based on Multiscale Deep Features Abstract: Visual saliency is a fundamental problem in both cognitive and computational\nsciences, including computer vision. In this CVPR 2015 paper, we discover that\na high-quality visual saliency model can be trained with multiscale features\nextracted using a popular deep learning architecture, convolutional neural\nnetworks (CNNs), which have had many successes in visual recognition tasks. For\nlearning such saliency models, we introduce a neural network architecture,\nwhich has fully connected layers on top of CNNs responsible for extracting\nfeatures at three different scales. We then propose a refinement method to\nenhance the spatial coherence of our saliency results. Finally, aggregating\nmultiple saliency maps computed for different levels of image segmentation can\nfurther boost the performance, yielding saliency maps better than those\ngenerated from a single segmentation. To promote further research and\nevaluation of visual saliency models, we also construct a new large database of\n4447 challenging images and their pixelwise saliency annotation. Experimental\nresults demonstrate that our proposed method is capable of achieving\nstate-of-the-art performance on all public benchmarks, improving the F-Measure\nby 5.0% and 13.2% respectively on the MSRA-B dataset and our new dataset\n(HKU-IS), and lowering the mean absolute error by 5.7% and 35.1% respectively\non these two datasets. \n\n"}
{"id": "1504.00874", "contents": "Title: Steering state statistics with output feedback Abstract: Consider a linear stochastic system whose initial state is a random vector\nwith a specified Gaussian distribution. Such a distribution may represent a\ncollection of particles abiding by the specified system dynamics. In recent\npublications, we have shown that, provided the system is controllable, it is\nalways possible to steer the state covariance to any specified terminal\nGaussian distribution using state feedback. The purpose of the present work is\nto show that, in the case where only partial state observation is available, a\nnecessary and sufficient condition for being able to steer the system to a\nspecified terminal Gaussian distribution for the state vector is that the\nterminal state covariance be greater (in the positive-definite sense) than the\nerror covariance of a corresponding Kalman filter. \n\n"}
{"id": "1504.01070", "contents": "Title: Sync-Rank: Robust Ranking, Constrained Ranking and Rank Aggregation via\n  Eigenvector and Semidefinite Programming Synchronization Abstract: We consider the classic problem of establishing a statistical ranking of a\nset of n items given a set of inconsistent and incomplete pairwise comparisons\nbetween such items. Instantiations of this problem occur in numerous\napplications in data analysis (e.g., ranking teams in sports data), computer\nvision, and machine learning. We formulate the above problem of ranking with\nincomplete noisy information as an instance of the group synchronization\nproblem over the group SO(2) of planar rotations, whose usefulness has been\ndemonstrated in numerous applications in recent years. Its least squares\nsolution can be approximated by either a spectral or a semidefinite programming\n(SDP) relaxation, followed by a rounding procedure. We perform extensive\nnumerical simulations on both synthetic and real-world data sets, showing that\nour proposed method compares favorably to other algorithms from the recent\nliterature. Existing theoretical guarantees on the group synchronization\nproblem imply lower bounds on the largest amount of noise permissible in the\nranking data while still achieving exact recovery. We propose a similar\nsynchronization-based algorithm for the rank-aggregation problem, which\nintegrates in a globally consistent ranking pairwise comparisons given by\ndifferent rating systems on the same set of items. We also discuss the problem\nof semi-supervised ranking when there is available information on the ground\ntruth rank of a subset of players, and propose an algorithm based on SDP which\nrecovers the ranks of the remaining players. Finally, synchronization-based\nranking, combined with a spectral technique for the densest subgraph problem,\nallows one to extract locally-consistent partial rankings, in other words, to\nidentify the rank of a small subset of players whose pairwise comparisons are\nless noisy than the rest of the data, which other methods are not able to\nidentify. \n\n"}
{"id": "1504.01903", "contents": "Title: Non-convex dynamic programming and optimal investment Abstract: We establish the existence of minimizers in a rather general setting of\ndynamic stochastic optimization without assuming either convexity or coercivity\nof the objective function. We apply this to prove the existence of optimal\nportfolios for non-concave utility maximization problems in financial market\nmodels with frictions (such as illiquidity), a first result of its kind. The\nproofs are based on the dynamic programming principle whose validity is\nestablished under quite general assumptions. \n\n"}
{"id": "1504.03048", "contents": "Title: The weight distributions of two classes of p ary cyclic codes with few\n  weights Abstract: Cyclic codes have attracted a lot of research interest for decades as they\nhave efficient encoding and decoding algorithms.\n  In this paper, for an odd prime $p$, the weight distributions of two classes\nof $p$-ary cyclic codes are completely determined. We show that both codes have\nat most five nonzero weights. \n\n"}
{"id": "1504.04419", "contents": "Title: Wasserstein continuity of entropy and outer bounds for interference\n  channels Abstract: It is shown that under suitable regularity conditions, differential entropy\nis a Lipschitz functional on the space of distributions on $n$-dimensional\nEuclidean space with respect to the quadratic Wasserstein distance. Under\nsimilar conditions, (discrete) Shannon entropy is shown to be Lipschitz\ncontinuous in distributions over the product space with respect to Ornstein's\n$\\bar d$-distance (Wasserstein distance corresponding to the Hamming distance).\nThese results together with Talagrand's and Marton's transportation-information\ninequalities allow one to replace the unknown multi-user interference with its\ni.i.d. approximations. As an application, a new outer bound for the two-user\nGaussian interference channel is proved, which, in particular, settles the\n\"missing corner point\" problem of Costa (1985). \n\n"}
{"id": "1504.06011", "contents": "Title: A Robust Approach to Chance Constrained Optimal Power Flow with\n  Renewable Generation Abstract: Optimal Power Flow (OPF) dispatches controllable generation at minimum cost\nsubject to operational constraints on generation and transmission assets. The\nuncertainty and variability of intermittent renewable generation is challenging\ncurrent deterministic OPF approaches. Recent formulations of OPF use chance\nconstraints to limit the risk from renewable generation uncertainty, however,\nthese new approaches typically assume the probability distributions which\ncharacterize the uncertainty and variability are known exactly. We formulate a\nRobust Chance Constrained (RCC) OPF that accounts for uncertainty in the\nparameters of these probability distributions by allowing them to be within an\nuncertainty set. The RCC OPF is solved using a cutting-plane algorithm that\nscales to large power systems. We demonstrate the RRC OPF on a modified model\nof the Bonneville Power Administration network, which includes 2209 buses and\n176 controllable generators. Deterministic, chance constrained (CC), and RCC\nOPF formulations are compared using several metrics including cost of\ngeneration, area control error, ramping of controllable generators, and\noccurrence of transmission line overloads as well as the respective\ncomputational performance. \n\n"}
{"id": "1504.06787", "contents": "Title: Max-margin Deep Generative Models Abstract: Deep generative models (DGMs) are effective on learning multilayered\nrepresentations of complex data and performing inference of input data by\nexploring the generative ability. However, little work has been done on\nexamining or empowering the discriminative ability of DGMs on making accurate\npredictions. This paper presents max-margin deep generative models (mmDGMs),\nwhich explore the strongly discriminative principle of max-margin learning to\nimprove the discriminative power of DGMs, while retaining the generative\ncapability. We develop an efficient doubly stochastic subgradient algorithm for\nthe piecewise linear objective. Empirical results on MNIST and SVHN datasets\ndemonstrate that (1) max-margin learning can significantly improve the\nprediction performance of DGMs and meanwhile retain the generative ability; and\n(2) mmDGMs are competitive to the state-of-the-art fully discriminative\nnetworks by employing deep convolutional neural networks (CNNs) as both\nrecognition and generative models. \n\n"}
{"id": "1504.06897", "contents": "Title: Linear Spatial Pyramid Matching Using Non-convex and non-negative Sparse\n  Coding for Image Classification Abstract: Recently sparse coding have been highly successful in image classification\nmainly due to its capability of incorporating the sparsity of image\nrepresentation. In this paper, we propose an improved sparse coding model based\non linear spatial pyramid matching(SPM) and Scale Invariant Feature Transform\n(SIFT ) descriptors. The novelty is the simultaneous non-convex and\nnon-negative characters added to the sparse coding model. Our numerical\nexperiments show that the improved approach using non-convex and non-negative\nsparse coding is superior than the original ScSPM[1] on several typical\ndatabases. \n\n"}
{"id": "1504.07590", "contents": "Title: A Robust Lane Detection and Departure Warning System Abstract: In this work, we have developed a robust lane detection and departure warning\ntechnique. Our system is based on single camera sensor. For lane detection a\nmodified Inverse Perspective Mapping using only a few extrinsic camera\nparameters and illuminant Invariant techniques is used. Lane markings are\nrepresented using a combination of 2nd and 4th order steerable filters, robust\nto shadowing. Effect of shadowing and extra sun light are removed using Lab\ncolor space, and illuminant invariant representation. Lanes are assumed to be\ncubic curves and fitted using robust RANSAC. This method can reliably detect\nlanes of the road and its boundary. This method has been experimented in Indian\nroad conditions under different challenging situations and the result obtained\nwere very good. For lane departure angle an optical flow based method were\nused. \n\n"}
{"id": "1504.07889", "contents": "Title: Bilinear CNNs for Fine-grained Visual Recognition Abstract: We present a simple and effective architecture for fine-grained visual\nrecognition called Bilinear Convolutional Neural Networks (B-CNNs). These\nnetworks represent an image as a pooled outer product of features derived from\ntwo CNNs and capture localized feature interactions in a translationally\ninvariant manner. B-CNNs belong to the class of orderless texture\nrepresentations but unlike prior work they can be trained in an end-to-end\nmanner. Our most accurate model obtains 84.1%, 79.4%, 86.9% and 91.3% per-image\naccuracy on the Caltech-UCSD birds [67], NABirds [64], FGVC aircraft [42], and\nStanford cars [33] dataset respectively and runs at 30 frames-per-second on a\nNVIDIA Titan X GPU. We then present a systematic analysis of these networks and\nshow that (1) the bilinear features are highly redundant and can be reduced by\nan order of magnitude in size without significant loss in accuracy, (2) are\nalso effective for other image classification tasks such as texture and scene\nrecognition, and (3) can be trained from scratch on the ImageNet dataset\noffering consistent improvements over the baseline architecture. Finally, we\npresent visualizations of these models on various datasets using top\nactivations of neural units and gradient-based inversion techniques. The source\ncode for the complete system is available at http://vis-www.cs.umass.edu/bcnn. \n\n"}
{"id": "1505.00670", "contents": "Title: Interleaved Text/Image Deep Mining on a Large-Scale Radiology Database\n  for Automated Image Interpretation Abstract: Despite tremendous progress in computer vision, there has not been an attempt\nfor machine learning on very large-scale medical image databases. We present an\ninterleaved text/image deep learning system to extract and mine the semantic\ninteractions of radiology images and reports from a national research\nhospital's Picture Archiving and Communication System. With natural language\nprocessing, we mine a collection of representative ~216K two-dimensional key\nimages selected by clinicians for diagnostic reference, and match the images\nwith their descriptions in an automated manner. Our system interleaves between\nunsupervised learning and supervised learning on document- and sentence-level\ntext collections, to generate semantic labels and to predict them given an\nimage. Given an image of a patient scan, semantic topics in radiology levels\nare predicted, and associated key-words are generated. Also, a number of\nfrequent disease types are detected as present or absent, to provide more\nspecific interpretation of a patient scan. This shows the potential of\nlarge-scale learning and prediction in electronic patient records available in\nmost modern clinical institutions. \n\n"}
{"id": "1505.02074", "contents": "Title: Exploring Models and Data for Image Question Answering Abstract: This work aims to address the problem of image-based question-answering (QA)\nwith new models and datasets. In our work, we propose to use neural networks\nand visual semantic embeddings, without intermediate stages such as object\ndetection and image segmentation, to predict answers to simple questions about\nimages. Our model performs 1.8 times better than the only published results on\nan existing image QA dataset. We also present a question generation algorithm\nthat converts image descriptions, which are widely available, into QA form. We\nused this algorithm to produce an order-of-magnitude larger dataset, with more\nevenly distributed answers. A suite of baseline results on this new dataset are\nalso presented. \n\n"}
{"id": "1505.05643", "contents": "Title: Object Modelling with a Handheld RGB-D Camera Abstract: This work presents a flexible system to reconstruct 3D models of objects\ncaptured with an RGB-D sensor. A major advantage of the method is that our\nreconstruction pipeline allows the user to acquire a full 3D model of the\nobject. This is achieved by acquiring several partial 3D models in different\nsessions that are automatically merged together to reconstruct a full model. In\naddition, the 3D models acquired by our system can be directly used by\nstate-of-the-art object instance recognition and object tracking modules,\nproviding object-perception capabilities for different applications, such as\nhuman-object interaction analysis or robot grasping. The system does not impose\nconstraints in the appearance of objects (textured, untextured) nor in the\nmodelling setup (moving camera with static object or a turn-table setup). The\nproposed reconstruction system has been used to model a large number of objects\nresulting in metrically accurate and visually appealing 3D models. \n\n"}
{"id": "1506.00740", "contents": "Title: Asymmetric Lee Distance Codes for DNA-Based Storage Abstract: We consider a new family of codes, termed asymmetric Lee distance codes, that\narise in the design and implementation of DNA-based storage systems and systems\nwith parallel string transmission protocols. The codewords are defined over a\nquaternary alphabet, although the results carry over to other alphabet sizes;\nfurthermore, symbol confusability is dictated by their underlying binary\nrepresentation. Our contributions are two-fold. First, we demonstrate that the\nnew distance represents a linear combination of the Lee and Hamming distance\nand derive upper bounds on the size of the codes under this metric based on\nlinear programming techniques. Second, we propose a number of code\nconstructions which imply lower bounds. \n\n"}
{"id": "1506.01665", "contents": "Title: Sliding mode control for a nonlinear phase-field system Abstract: In the present contribution the sliding mode control (SMC) problem for a\nphase-field model of Caginalp type is considered. First we prove the\nwell-posedness and some regularity results for the phase-field type state\nsystems modified by the state-feedback control laws. Then, we show that the\nchosen SMC laws force the system to reach within finite time the sliding\nmanifold (that we chose in order that one of the physical variables or a\ncombination of them remains constant in time). We study three different types\nof feedback control laws: the first one appears in the internal energy balance\nand forces a linear combination of the temperature and the phase to reach a\ngiven (space dependent) value, while the second and third ones are added in the\nphase relation and lead the phase onto a prescribed target. While the control\nlaw is non-local in space for the first two problems, it is local in the third\none, i.e., its value at any point and any time just depends on the value of the\nstate. \n\n"}
{"id": "1506.04439", "contents": "Title: Optimal stopping under probability distortions and law invariant\n  coherent risk measures Abstract: In this paper we study optimal stopping problems with respect to distorted\nexpectations of the form \\begin{eqnarray*}\n\\mathcal{E}(X)=\\int_{-\\infty}^{\\infty} x\\,dG(F_X(x)), \\end{eqnarray*} where\n$F_X$ is the distribution function of $X$ and $G$ is a convex distribution\nfunction on $[0,1].$ As a matter of fact, except for $G$ being the identity on\n$[0,1],$ dynamic versions of $\\mathcal{E}(X)$ do not have the so-called\ntime-consistency property necessary for the dynamic programming approach. So\nthe standard approaches are not applicable to optimal stopping under\n$\\mathcal{E}(X).$ In this paper, we prove a novel representation, which relates\nthe solution of an optimal stopping problem under distorted expectation to the\nsequence of standard optimal stopping problems and hence makes the application\nof the standard dynamic programming-based approaches possible. Furthermore, by\nmeans of the well known Kusuoka representation, we extend our results to\noptimal stopping under general law invariant coherent risk measures. Finally,\nbased on our novel representations, we develop several Monte Carlo\napproximation algorithms and illustrate their power for optimal stopping under\nAverage Value at Risk and the absolute semideviation risk measures. \n\n"}
{"id": "1506.05011", "contents": "Title: Bayesian representation learning with oracle constraints Abstract: Representation learning systems typically rely on massive amounts of labeled\ndata in order to be trained to high accuracy. Recently, high-dimensional\nparametric models like neural networks have succeeded in building rich\nrepresentations using either compressive, reconstructive or supervised\ncriteria. However, the semantic structure inherent in observations is\noftentimes lost in the process. Human perception excels at understanding\nsemantics but cannot always be expressed in terms of labels. Thus,\n\\emph{oracles} or \\emph{human-in-the-loop systems}, for example crowdsourcing,\nare often employed to generate similarity constraints using an implicit\nsimilarity function encoded in human perception. In this work we propose to\ncombine \\emph{generative unsupervised feature learning} with a\n\\emph{probabilistic treatment of oracle information like triplets} in order to\ntransfer implicit privileged oracle knowledge into explicit nonlinear Bayesian\nlatent factor models of the observations. We use a fast variational algorithm\nto learn the joint model and demonstrate applicability to a well-known image\ndataset. We show how implicit triplet information can provide rich information\nto learn representations that outperform previous metric learning approaches as\nwell as generative models without this side-information in a variety of\npredictive tasks. In addition, we illustrate that the proposed approach\ncompartmentalizes the latent spaces semantically which allows interpretation of\nthe latent variables. \n\n"}
{"id": "1506.05532", "contents": "Title: A Spatial Layout and Scale Invariant Feature Representation for Indoor\n  Scene Classification Abstract: Unlike standard object classification, where the image to be classified\ncontains one or multiple instances of the same object, indoor scene\nclassification is quite different since the image consists of multiple distinct\nobjects. Further, these objects can be of varying sizes and are present across\nnumerous spatial locations in different layouts. For automatic indoor scene\ncategorization, large scale spatial layout deformations and scale variations\nare therefore two major challenges and the design of rich feature descriptors\nwhich are robust to these challenges is still an open problem. This paper\nintroduces a new learnable feature descriptor called \"spatial layout and scale\ninvariant convolutional activations\" to deal with these challenges. For this\npurpose, a new Convolutional Neural Network architecture is designed which\nincorporates a novel 'Spatially Unstructured' layer to introduce robustness\nagainst spatial layout deformations. To achieve scale invariance, we present a\npyramidal image representation. For feasible training of the proposed network\nfor images of indoor scenes, the paper proposes a new methodology which\nefficiently adapts a trained network model (on a large scale data) for our task\nwith only a limited amount of available training data. Compared with existing\nstate of the art, the proposed approach achieves a relative performance\nimprovement of 3.2%, 3.8%, 7.0%, 11.9% and 2.1% on MIT-67, Scene-15, Sports-8,\nGraz-02 and NYU datasets respectively. \n\n"}
{"id": "1506.06981", "contents": "Title: R-CNN minus R Abstract: Deep convolutional neural networks (CNNs) have had a major impact in most\nareas of image understanding, including object category detection. In object\ndetection, methods such as R-CNN have obtained excellent results by integrating\nCNNs with region proposal generation algorithms such as selective search. In\nthis paper, we investigate the role of proposal generation in CNN-based\ndetectors in order to determine whether it is a necessary modelling component,\ncarrying essential geometric information not contained in the CNN, or whether\nit is merely a way of accelerating detection. We do so by designing and\nevaluating a detector that uses a trivial region generation scheme, constant\nfor each image. Combined with SPP, this results in an excellent and fast\ndetector that does not require to process an image with algorithms other than\nthe CNN itself. We also streamline and simplify the training of CNN-based\ndetectors by integrating several learning steps in a single algorithm, as well\nas by proposing a number of improvements that accelerate detection. \n\n"}
{"id": "1506.08019", "contents": "Title: Multiobjective approach to optimal control for a dengue transmission\n  model Abstract: During the last decades, the global prevalence of dengue progressed\ndramatically. It is a disease which is now endemic in more than one hundred\ncountries of Africa, America, Asia and the Western Pacific. This study\naddresses a mathematical model for the dengue disease transmission and finding\nthe most effective ways of controlling the disease. The model is described by a\nsystem of ordinary differential equations representing human and vector\ndynamics. Multiobjective optimization is applied to find the optimal control\nstrategies, considering the simultaneous minimization of infected humans and\ncosts due to insecticide application. The obtained results show that\nmultiobjective optimization is an effective tool for finding the optimal\ncontrol. The set of trade-off solutions encompasses a whole range of optimal\nscenarios, providing valuable information about the dynamics of infection\ntransmissions. The results are discussed for different values of model\nparameters. \n\n"}
{"id": "1507.01476", "contents": "Title: Semi-proximal Mirror-Prox for Nonsmooth Composite Minimization Abstract: We propose a new first-order optimisation algorithm to solve high-dimensional\nnon-smooth composite minimisation problems. Typical examples of such problems\nhave an objective that decomposes into a non-smooth empirical risk part and a\nnon-smooth regularisation penalty. The proposed algorithm, called Semi-Proximal\nMirror-Prox, leverages the Fenchel-type representation of one part of the\nobjective while handling the other part of the objective via linear\nminimization over the domain. The algorithm stands in contrast with more\nclassical proximal gradient algorithms with smoothing, which require the\ncomputation of proximal operators at each iteration and can therefore be\nimpractical for high-dimensional problems. We establish the theoretical\nconvergence rate of Semi-Proximal Mirror-Prox, which exhibits the optimal\ncomplexity bounds, i.e. $O(1/\\epsilon^2)$, for the number of calls to linear\nminimization oracle. We present promising experimental results showing the\ninterest of the approach in comparison to competing methods. \n\n"}
{"id": "1507.01581", "contents": "Title: Joint Calibration for Semantic Segmentation Abstract: Semantic segmentation is the task of assigning a class-label to each pixel in\nan image. We propose a region-based semantic segmentation framework which\nhandles both full and weak supervision, and addresses three common problems:\n(1) Objects occur at multiple scales and therefore we should use regions at\nmultiple scales. However, these regions are overlapping which creates\nconflicting class predictions at the pixel-level. (2) Class frequencies are\nhighly imbalanced in realistic datasets. (3) Each pixel can only be assigned to\na single class, which creates competition between classes. We address all three\nproblems with a joint calibration method which optimizes a multi-class loss\ndefined over the final pixel-level output labeling, as opposed to simply region\nclassification. Our method outperforms the state-of-the-art on the popular SIFT\nFlow [18] dataset in both the fully and weakly supervised setting by a\nconsiderably margin (+6% and +10%, respectively). \n\n"}
{"id": "1507.02444", "contents": "Title: Non-Asymptotic Achievable Rates for Energy-Harvesting Channels using\n  Save-and-Transmit Abstract: This paper investigates the information-theoretic limits of energy-harvesting\n(EH) channels in the finite blocklength regime. The EH process is characterized\nby a sequence of i.i.d. random variables with finite variances. We use the\nsave-and-transmit strategy proposed by Ozel and Ulukus (2012) together with\nShannon's non-asymptotic achievability bound to obtain lower bounds on the\nachievable rates for both additive white Gaussian noise channels and discrete\nmemoryless channels under EH constraints. The first-order terms of the lower\nbounds of the achievable rates are equal to $C$ and the second-order (backoff\nfrom capacity) terms are proportional to $-\\sqrt{ \\frac{\\log n}{n}}$, where $n$\ndenotes the blocklength and $C$ denotes the capacity of the EH channel, which\nis the same as the capacity without the EH constraints. The constant of\nproportionality of the backoff term is found and qualitative interpretations\nare provided. \n\n"}
{"id": "1507.02772", "contents": "Title: Riemannian Dictionary Learning and Sparse Coding for Positive Definite\n  Matrices Abstract: Data encoded as symmetric positive definite (SPD) matrices frequently arise\nin many areas of computer vision and machine learning. While these matrices\nform an open subset of the Euclidean space of symmetric matrices, viewing them\nthrough the lens of non-Euclidean Riemannian geometry often turns out to be\nbetter suited in capturing several desirable data properties. However,\nformulating classical machine learning algorithms within such a geometry is\noften non-trivial and computationally expensive. Inspired by the great success\nof dictionary learning and sparse coding for vector-valued data, our goal in\nthis paper is to represent data in the form of SPD matrices as sparse conic\ncombinations of SPD atoms from a learned dictionary via a Riemannian geometric\napproach. To that end, we formulate a novel Riemannian optimization objective\nfor dictionary learning and sparse coding in which the representation loss is\ncharacterized via the affine invariant Riemannian metric. We also present a\ncomputationally simple algorithm for optimizing our model. Experiments on\nseveral computer vision datasets demonstrate superior classification and\nretrieval performance using our approach when compared to sparse coding via\nalternative non-Riemannian formulations. \n\n"}
{"id": "1507.07144", "contents": "Title: Strongly convex functions, Moreau envelopes and the generic nature of\n  convex functions with strong minimizers Abstract: In this work, using Moreau envelopes, we define a complete metric for the set\nof proper lower semicontinuous convex functions. Under this metric, the\nconvergence of each sequence of convex functions is epi-convergence. We show\nthat the set of strongly convex functions is dense but it is only of the first\ncategory. On the other hand, it is shown that the set of convex functions with\nstrong minima is of the second category. \n\n"}
{"id": "1508.01108", "contents": "Title: Evaluating color texture descriptors under large variations of\n  controlled lighting conditions Abstract: The recognition of color texture under varying lighting conditions is still\nan open issue. Several features have been proposed for this purpose, ranging\nfrom traditional statistical descriptors to features extracted with neural\nnetworks. Still, it is not completely clear under what circumstances a feature\nperforms better than the others. In this paper we report an extensive\ncomparison of old and new texture features, with and without a color\nnormalization step, with a particular focus on how they are affected by small\nand large variation in the lighting conditions. The evaluation is performed on\na new texture database including 68 samples of raw food acquired under 46\nconditions that present single and combined variations of light color,\ndirection and intensity. The database allows to systematically investigate the\nrobustness of texture descriptors across a large range of variations of imaging\nconditions. \n\n"}
{"id": "1508.01161", "contents": "Title: Pushing towards the Limit of Sampling Rate: Adaptive Chasing Sampling Abstract: Measurement samples are often taken in various monitoring applications. To\nreduce the sensing cost, it is desirable to achieve better sensing quality\nwhile using fewer samples. Compressive Sensing (CS) technique finds its role\nwhen the signal to be sampled meets certain sparsity requirements. In this\npaper we investigate the possibility and basic techniques that could further\nreduce the number of samples involved in conventional CS theory by exploiting\nlearning-based non-uniform adaptive sampling.\n  Based on a typical signal sensing application, we illustrate and evaluate the\nperformance of two of our algorithms, Individual Chasing and Centroid Chasing,\nfor signals of different distribution features. Our proposed learning-based\nadaptive sampling schemes complement existing efforts in CS fields and do not\ndepend on any specific signal reconstruction technique. Compared to\nconventional sparse sampling methods, the simulation results demonstrate that\nour algorithms allow $46\\%$ less number of samples for accurate signal\nreconstruction and achieve up to $57\\%$ smaller signal reconstruction error\nunder the same noise condition. \n\n"}
{"id": "1508.03899", "contents": "Title: Convergence Analysis of Algorithms for DC Programming Abstract: We consider the minimization problems of the form $P(\\varphi, g, h)$:\n$\\min\\{f(x) = \\varphi(x) + g(x) - h(x): x \\in \\Bbb R^n\\}$, where $\\varphi$ is a\ndifferentiable function and $g$, $h$ are convex functions, and introduce\niterative methods to finding a critical point of $f$ when $f$ is\ndifferentiable. We show that the point computed by proximal point algorithm at\neach iteration can be used to determine a descent direction for the objective\nfunction at this point. This algorithm can be considered as a combination of\nproximal point algorithm together with a linesearch step that uses this descent\ndirection. We also study convergence results of these algorithms and the\ninertial proximal methods proposed by P.E. Maing$\\acute{e}$ {\\it et.al.}\n\\cite{MM} under the main assumption that the objective function satisfies the\nKurdika-{\\L}ojasiewicz property. \n\n"}
{"id": "1508.05306", "contents": "Title: Exemplar Based Deep Discriminative and Shareable Feature Learning for\n  Scene Image Classification Abstract: In order to encode the class correlation and class specific information in\nimage representation, we propose a new local feature learning approach named\nDeep Discriminative and Shareable Feature Learning (DDSFL). DDSFL aims to\nhierarchically learn feature transformation filter banks to transform raw pixel\nimage patches to features. The learned filter banks are expected to: (1) encode\ncommon visual patterns of a flexible number of categories; (2) encode\ndiscriminative information; and (3) hierarchically extract patterns at\ndifferent visual levels. Particularly, in each single layer of DDSFL, shareable\nfilters are jointly learned for classes which share the similar patterns.\nDiscriminative power of the filters is achieved by enforcing the features from\nthe same category to be close, while features from different categories to be\nfar away from each other. Furthermore, we also propose two exemplar selection\nmethods to iteratively select training data for more efficient and effective\nlearning. Based on the experimental results, DDSFL can achieve very promising\nperformance, and it also shows great complementary effect to the\nstate-of-the-art Caffe features. \n\n"}
{"id": "1508.06904", "contents": "Title: Rapid Exact Signal Scanning with Deep Convolutional Neural Networks Abstract: A rigorous formulation of the dynamics of a signal processing scheme aimed at\ndense signal scanning without any loss in accuracy is introduced and analyzed.\nRelated methods proposed in the recent past lack a satisfactory analysis of\nwhether they actually fulfill any exactness constraints. This is improved\nthrough an exact characterization of the requirements for a sound sliding\nwindow approach. The tools developed in this paper are especially beneficial if\nConvolutional Neural Networks are employed, but can also be used as a more\ngeneral framework to validate related approaches to signal scanning. The\nproposed theory helps to eliminate redundant computations and renders special\ncase treatment unnecessary, resulting in a dramatic boost in efficiency\nparticularly on massively parallel processors. This is demonstrated both\ntheoretically in a computational complexity analysis and empirically on modern\nparallel processors. \n\n"}
{"id": "1509.01788", "contents": "Title: Joint Color-Spatial-Directional clustering and Region Merging (JCSD-RM)\n  for unsupervised RGB-D image segmentation Abstract: Recent advances in depth imaging sensors provide easy access to the\nsynchronized depth with color, called RGB-D image. In this paper, we propose an\nunsupervised method for indoor RGB-D image segmentation and analysis. We\nconsider a statistical image generation model based on the color and geometry\nof the scene. Our method consists of a joint color-spatial-directional\nclustering method followed by a statistical planar region merging method. We\nevaluate our method on the NYU depth database and compare it with existing\nunsupervised RGB-D segmentation methods. Results show that, it is comparable\nwith the state of the art methods and it needs less computation time. Moreover,\nit opens interesting perspectives to fuse color and geometry in an unsupervised\nmanner. \n\n"}
{"id": "1509.01978", "contents": "Title: An Approach to the Analysis of the South Slavic Medieval Labels Using\n  Image Texture Abstract: The paper presents a new script classification method for the discrimination\nof the South Slavic medieval labels. It consists in the textural analysis of\nthe script types. In the first step, each letter is coded by the equivalent\nscript type, which is defined by its typographical features. Obtained coded\ntext is subjected to the run-length statistical analysis and to the adjacent\nlocal binary pattern analysis in order to extract the features. The result\nshows a diversity between the extracted features of the scripts, which makes\nthe feature classification more effective. It is the basis for the\nclassification process of the script identification by using an extension of a\nstate-of-the-art approach for document clustering. The proposed method is\nevaluated on an example of hand-engraved in stone and hand-printed in paper\nlabels in old Cyrillic, angular and round Glagolitic. Experiments demonstrate\nvery positive results, which prove the effectiveness of the proposed method. \n\n"}
{"id": "1509.04491", "contents": "Title: Sparse Multinomial Logistic Regression via Approximate Message Passing Abstract: For the problem of multi-class linear classification and feature selection,\nwe propose approximate message passing approaches to sparse multinomial\nlogistic regression (MLR). First, we propose two algorithms based on the Hybrid\nGeneralized Approximate Message Passing (HyGAMP) framework: one finds the\nmaximum a posteriori (MAP) linear classifier and the other finds an\napproximation of the test-error-rate minimizing linear classifier. Then we\ndesign computationally simplified variants of these two algorithms. Next, we\ndetail methods to tune the hyperparameters of their assumed statistical models\nusing Stein's unbiased risk estimate (SURE) and expectation-maximization (EM),\nrespectively. Finally, using both synthetic and real-world datasets, we\ndemonstrate improved error-rate and runtime performance relative to existing\nstate-of-the-art approaches to sparse MLR. \n\n"}
{"id": "1509.05267", "contents": "Title: Deep Multi-task Learning for Railway Track Inspection Abstract: Railroad tracks need to be periodically inspected and monitored to ensure\nsafe transportation. Automated track inspection using computer vision and\npattern recognition methods have recently shown the potential to improve safety\nby allowing for more frequent inspections while reducing human errors.\nAchieving full automation is still very challenging due to the number of\ndifferent possible failure modes as well as the broad range of image variations\nthat can potentially trigger false alarms. Also, the number of defective\ncomponents is very small, so not many training examples are available for the\nmachine to learn a robust anomaly detector. In this paper, we show that\ndetection performance can be improved by combining multiple detectors within a\nmulti-task learning framework. We show that this approach results in better\naccuracy in detecting defects on railway ties and fasteners. \n\n"}
{"id": "1509.06729", "contents": "Title: Algebraic Clustering of Affine Subspaces Abstract: Subspace clustering is an important problem in machine learning with many\napplications in computer vision and pattern recognition. Prior work has studied\nthis problem using algebraic, iterative, statistical, low-rank and sparse\nrepresentation techniques. While these methods have been applied to both linear\nand affine subspaces, theoretical results have only been established in the\ncase of linear subspaces. For example, algebraic subspace clustering (ASC) is\nguaranteed to provide the correct clustering when the data points are in\ngeneral position and the union of subspaces is transversal. In this paper we\nstudy in a rigorous fashion the properties of ASC in the case of affine\nsubspaces. Using notions from algebraic geometry, we prove that the\nhomogenization trick, which embeds points in a union of affine subspaces into\npoints in a union of linear subspaces, preserves the general position of the\npoints and the transversality of the union of subspaces in the embedded space,\nthus establishing the correctness of ASC for affine subpaces. \n\n"}
{"id": "1509.08075", "contents": "Title: Segment-Phrase Table for Semantic Segmentation, Visual Entailment and\n  Paraphrasing Abstract: We introduce Segment-Phrase Table (SPT), a large collection of bijective\nassociations between textual phrases and their corresponding segmentations.\nLeveraging recent progress in object recognition and natural language\nsemantics, we show how we can successfully build a high-quality segment-phrase\ntable using minimal human supervision. More importantly, we demonstrate the\nunique value unleashed by this rich bimodal resource, for both vision as well\nas natural language understanding. First, we show that fine-grained textual\nlabels facilitate contextual reasoning that helps in satisfying semantic\nconstraints across image segments. This feature enables us to achieve\nstate-of-the-art segmentation results on benchmark datasets. Next, we show that\nthe association of high-quality segmentations to textual phrases aids in richer\nsemantic understanding and reasoning of these textual phrases. Leveraging this\nfeature, we motivate the problem of visual entailment and visual paraphrasing,\nand demonstrate its utility on a large dataset. \n\n"}
{"id": "1510.00252", "contents": "Title: RF Lens-Embedded Massive MIMO Systems: Fabrication Issues and Codebook\n  Design Abstract: In this paper, we investigate a radio frequency (RF) lens-embedded massive\nmultiple-input multiple-output (MIMO) system and evaluate the system\nperformance of limited feedback by utilizing a technique for generating a\nsuitable codebook for the system. We fabricate an RF lens that operates on a 77\nGHz (mmWave) band. Experimental results show a proper value of amplitude gain\nand an appropriate focusing property. In addition, using a simple numerical\ntechnique--beam propagation method (BPM)--we estimate the power profile of the\nRF lens and verify its accordance with experimental results. We also design a\ncodebook--multi-variance codebook quantization (MVCQ)--for limited feedback by\nconsidering the characteristics of the RF lens antenna for massive MIMO\nsystems. Numerical results confirm that the proposed system shows significant\nperformance enhancement over a conventional massive MIMO system without an RF\nlens. \n\n"}
{"id": "1510.01722", "contents": "Title: Structured Transforms for Small-Footprint Deep Learning Abstract: We consider the task of building compact deep learning pipelines suitable for\ndeployment on storage and power constrained mobile devices. We propose a\nunified framework to learn a broad family of structured parameter matrices that\nare characterized by the notion of low displacement rank. Our structured\ntransforms admit fast function and gradient evaluation, and span a rich range\nof parameter sharing configurations whose statistical modeling capacity can be\nexplicitly tuned along a continuum from structured to unstructured.\nExperimental results show that these transforms can significantly accelerate\ninference and forward/backward passes during training, and offer superior\naccuracy-compactness-speed tradeoffs in comparison to a number of existing\ntechniques. In keyword spotting applications in mobile speech recognition, our\nmethods are much more effective than standard linear low-rank bottleneck layers\nand nearly retain the performance of state of the art models, while providing\nmore than 3.5-fold compression. \n\n"}
{"id": "1510.02927", "contents": "Title: DeepFix: A Fully Convolutional Neural Network for predicting Human Eye\n  Fixations Abstract: Understanding and predicting the human visual attentional mechanism is an\nactive area of research in the fields of neuroscience and computer vision. In\nthis work, we propose DeepFix, a first-of-its-kind fully convolutional neural\nnetwork for accurate saliency prediction. Unlike classical works which\ncharacterize the saliency map using various hand-crafted features, our model\nautomatically learns features in a hierarchical fashion and predicts saliency\nmap in an end-to-end manner. DeepFix is designed to capture semantics at\nmultiple scales while taking global context into account using network layers\nwith very large receptive fields. Generally, fully convolutional nets are\nspatially invariant which prevents them from modeling location dependent\npatterns (e.g. centre-bias). Our network overcomes this limitation by\nincorporating a novel Location Biased Convolutional layer. We evaluate our\nmodel on two challenging eye fixation datasets -- MIT300, CAT2000 and show that\nit outperforms other recent approaches by a significant margin. \n\n"}
{"id": "1510.03510", "contents": "Title: Repeat-Accumulate Codes for Reconciliation in Continuous Variable\n  Quantum Key Distribution Abstract: This paper investigates the design of low-complexity error correction codes\nfor the verification step in continuous variable quantum key distribution\n(CVQKD) systems. We design new coding schemes based on quasi-cyclic\nrepeat-accumulate codes which demonstrate good performances for CVQKD\nreconciliation. \n\n"}
{"id": "1510.06701", "contents": "Title: On the Take-off of Airborne Wind Energy Systems Based on Rigid Wings Abstract: The problem of launching a tethered aircraft to be used for airborne wind\nenergy generation is investigated. Exploiting well-assessed physical\nprinciples, an analysis of three different take-off approaches is carried out.\nThe approaches are then compared on the basis of quantitative and qualitative\ncriteria introduced to assess their technical and economic viability. Finally,\na deeper study of the concept that is deemed the most viable one, i.e. a linear\ntake-off maneuver combined with on-board propellers, is performed by means of\nnumerical simulations. The latter are used to refine the initial analysis in\nterms of power required for take-off, and further confirm the viability of the\napproach. \n\n"}
{"id": "1511.00513", "contents": "Title: Pixel-wise Segmentation of Street with Neural Networks Abstract: Pixel-wise street segmentation of photographs taken from a drivers\nperspective is important for self-driving cars and can also support other\nobject recognition tasks. A framework called SST was developed to examine the\naccuracy and execution time of different neural networks. The best neural\nnetwork achieved an $F_1$-score of 89.5% with a simple feedforward neural\nnetwork which trained to solve a regression task. \n\n"}
{"id": "1511.02093", "contents": "Title: Evaluation of the Hamming weights of a class of linear codes based on\n  Gauss sums Abstract: Linear codes with a few weights have been widely investigated in recent\nyears. In this paper, we mainly use Gauss sums to represent the Hamming weights\nof a class of $q$-ary linear codes under some certain conditions, where $q$ is\na power of a prime. The lower bound of its minimum Hamming distance is\nobtained. In some special cases, we evaluate the weight distributions of the\nlinear codes by semi-primitive Gauss sums and obtain some one-weight,\ntwo-weight linear codes. It is quite interesting that we find new optimal codes\nachieving some bounds on linear codes. The linear codes in this paper can be\nused in secret sharing schemes, authentication codes and data storage systems. \n\n"}
{"id": "1511.03206", "contents": "Title: The Radon cumulative distribution transform and its application to image\n  classification Abstract: Invertible image representation methods (transforms) are routinely employed\nas low-level image processing operations based on which feature extraction and\nrecognition algorithms are developed. Most transforms in current use (e.g.\nFourier, Wavelet, etc.) are linear transforms, and, by themselves, are unable\nto substantially simplify the representation of image classes for\nclassification. Here we describe a nonlinear, invertible, low-level image\nprocessing transform based on combining the well known Radon transform for\nimage data, and the 1D Cumulative Distribution Transform proposed earlier. We\ndescribe a few of the properties of this new transform, and with both\ntheoretical and experimental results show that it can often render certain\nproblems linearly separable in transform space. \n\n"}
{"id": "1511.03339", "contents": "Title: Attention to Scale: Scale-aware Semantic Image Segmentation Abstract: Incorporating multi-scale features in fully convolutional neural networks\n(FCNs) has been a key element to achieving state-of-the-art performance on\nsemantic image segmentation. One common way to extract multi-scale features is\nto feed multiple resized input images to a shared deep network and then merge\nthe resulting features for pixelwise classification. In this work, we propose\nan attention mechanism that learns to softly weight the multi-scale features at\neach pixel location. We adapt a state-of-the-art semantic image segmentation\nmodel, which we jointly train with multi-scale input images and the attention\nmodel. The proposed attention model not only outperforms average- and\nmax-pooling, but allows us to diagnostically visualize the importance of\nfeatures at different positions and scales. Moreover, we show that adding extra\nsupervision to the output at each scale is essential to achieving excellent\nperformance when merging multi-scale features. We demonstrate the effectiveness\nof our model with extensive experiments on three challenging datasets,\nincluding PASCAL-Person-Part, PASCAL VOC 2012 and a subset of MS-COCO 2014. \n\n"}
{"id": "1511.03629", "contents": "Title: A Continuous Max-Flow Approach to Cyclic Field Reconstruction Abstract: Reconstruction of an image from noisy data using Markov Random Field theory\nhas been explored by both the graph-cuts and continuous max-flow community in\nthe form of the Potts and Ishikawa models. However, neither model takes into\naccount the particular cyclic topology of specific intensity types such as the\nhue in natural colour images, or the phase in complex valued MRI. This paper\npresents \\textit{cyclic continuous max-flow} image reconstruction which models\nthe intensity being reconstructed as having a fundamentally cyclic topology.\nThis model complements the Ishikawa model in that it is designed with image\nreconstruction in mind, having the topology of the intensity space inherent in\nthe model while being readily extendable to an arbitrary intensity resolution. \n\n"}
{"id": "1511.04777", "contents": "Title: Complete Dictionary Recovery over the Sphere II: Recovery by Riemannian\n  Trust-region Method Abstract: We consider the problem of recovering a complete (i.e., square and\ninvertible) matrix $\\mathbf A_0$, from $\\mathbf Y \\in \\mathbb{R}^{n \\times p}$\nwith $\\mathbf Y = \\mathbf A_0 \\mathbf X_0$, provided $\\mathbf X_0$ is\nsufficiently sparse. This recovery problem is central to theoretical\nunderstanding of dictionary learning, which seeks a sparse representation for a\ncollection of input signals and finds numerous applications in modern signal\nprocessing and machine learning. We give the first efficient algorithm that\nprovably recovers $\\mathbf A_0$ when $\\mathbf X_0$ has $O(n)$ nonzeros per\ncolumn, under suitable probability model for $\\mathbf X_0$.\n  Our algorithmic pipeline centers around solving a certain nonconvex\noptimization problem with a spherical constraint, and hence is naturally\nphrased in the language of manifold optimization. In a companion paper\n(arXiv:1511.03607), we have showed that with high probability our nonconvex\nformulation has no \"spurious\" local minimizers and around any saddle point the\nobjective function has a negative directional curvature. In this paper, we take\nadvantage of the particular geometric structure, and describe a Riemannian\ntrust region algorithm that provably converges to a local minimizer with from\narbitrary initializations. Such minimizers give excellent approximations to\nrows of $\\mathbf X_0$. The rows are then recovered by linear programming\nrounding and deflation. \n\n"}
{"id": "1511.05512", "contents": "Title: Moral Lineage Tracing Abstract: Lineage tracing, the tracking of living cells as they move and divide, is a\ncentral problem in biological image analysis. Solutions, called lineage\nforests, are key to understanding how the structure of multicellular organisms\nemerges. We propose an integer linear program (ILP) whose feasible solutions\ndefine a decomposition of each image in a sequence into cells (segmentation),\nand a lineage forest of cells across images (tracing). Unlike previous\nformulations, we do not constrain the set of decompositions, except by\ncontracting pixels to superpixels. The main challenge, as we show, is to\nenforce the morality of lineages, i.e., the constraint that cells do not merge.\nTo enforce morality, we introduce path-cut inequalities. To find feasible\nsolutions of the NP-hard ILP, with certified bounds to the global optimum, we\ndefine efficient separation procedures and apply these as part of a\nbranch-and-cut algorithm. We show the effectiveness of this approach by\nanalyzing feasible solutions for real microscopy data in terms of bounds and\nrun-time, and by their weighted edit distance to ground truth lineage forests\ntraced by humans. \n\n"}
{"id": "1511.05666", "contents": "Title: Super-Resolution with Deep Convolutional Sufficient Statistics Abstract: Inverse problems in image and audio, and super-resolution in particular, can\nbe seen as high-dimensional structured prediction problems, where the goal is\nto characterize the conditional distribution of a high-resolution output given\nits low-resolution corrupted observation. When the scaling ratio is small,\npoint estimates achieve impressive performance, but soon they suffer from the\nregression-to-the-mean problem, result of their inability to capture the\nmulti-modality of this conditional distribution. Modeling high-dimensional\nimage and audio distributions is a hard task, requiring both the ability to\nmodel complex geometrical structures and textured regions. In this paper, we\npropose to use as conditional model a Gibbs distribution, where its sufficient\nstatistics are given by deep convolutional neural networks. The features\ncomputed by the network are stable to local deformation, and have reduced\nvariance when the input is a stationary texture. These properties imply that\nthe resulting sufficient statistics minimize the uncertainty of the target\nsignals given the degraded observations, while being highly informative. The\nfilters of the CNN are initialized by multiscale complex wavelets, and then we\npropose an algorithm to fine-tune them by estimating the gradient of the\nconditional log-likelihood, which bears some similarities with Generative\nAdversarial Networks. We evaluate experimentally the proposed approach in the\nimage super-resolution task, but the approach is general and could be used in\nother challenging ill-posed problems such as audio bandwidth extension. \n\n"}
{"id": "1511.06062", "contents": "Title: Compact Bilinear Pooling Abstract: Bilinear models has been shown to achieve impressive performance on a wide\nrange of visual tasks, such as semantic segmentation, fine grained recognition\nand face recognition. However, bilinear features are high dimensional,\ntypically on the order of hundreds of thousands to a few million, which makes\nthem impractical for subsequent analysis. We propose two compact bilinear\nrepresentations with the same discriminative power as the full bilinear\nrepresentation but with only a few thousand dimensions. Our compact\nrepresentations allow back-propagation of classification errors enabling an\nend-to-end optimization of the visual recognition system. The compact bilinear\nrepresentations are derived through a novel kernelized analysis of bilinear\npooling which provide insights into the discriminative power of bilinear\npooling, and a platform for further research in compact pooling methods.\nExperimentation illustrate the utility of the proposed representations for\nimage classification and few-shot learning across several datasets. \n\n"}
{"id": "1511.06233", "contents": "Title: Towards Open Set Deep Networks Abstract: Deep networks have produced significant gains for various visual recognition\nproblems, leading to high impact academic and commercial applications. Recent\nwork in deep networks highlighted that it is easy to generate images that\nhumans would never classify as a particular object class, yet networks classify\nsuch images high confidence as that given class - deep network are easily\nfooled with images humans do not consider meaningful. The closed set nature of\ndeep networks forces them to choose from one of the known classes leading to\nsuch artifacts. Recognition in the real world is open set, i.e. the recognition\nsystem should reject unknown/unseen classes at test time. We present a\nmethodology to adapt deep networks for open set recognition, by introducing a\nnew model layer, OpenMax, which estimates the probability of an input being\nfrom an unknown class. A key element of estimating the unknown probability is\nadapting Meta-Recognition concepts to the activation patterns in the\npenultimate layer of the network. OpenMax allows rejection of \"fooling\" and\nunrelated open set images presented to the system; OpenMax greatly reduces the\nnumber of obvious errors made by a deep network. We prove that the OpenMax\nconcept provides bounded open space risk, thereby formally providing an open\nset recognition solution. We evaluate the resulting open set deep networks\nusing pre-trained networks from the Caffe Model-zoo on ImageNet 2012 validation\ndata, and thousands of fooling and open set images. The proposed OpenMax model\nsignificantly outperforms open set recognition accuracy of basic deep networks\nas well as deep networks with thresholding of SoftMax probabilities. \n\n"}
{"id": "1511.06518", "contents": "Title: Enhanced Transmit Antenna Selection Scheme for Secure Throughput\n  Maximization Without CSI at the Transmitter and its Applications on Smart\n  Grids Abstract: This paper addresses the establishment of secure communication links between\nsmart-meters (Alice) and an aggregator (Bob) in the presence of an eavesdropper\n(Eve). The proposed scenario assumes: (i) MIMOME wiretap channel; (ii) transmit\nantenna selection at the Alice; (iii) no channel state information at the\ntransmitter; (iv) fixed Wyner codes; and (v) guarantee of secure throughput by\nboth quality of service and secrecy outage constraints. We propose a simple\nprotocol to enhance security via transmit antenna selection, and then assess\nits performance in closed-form by means of secrecy outage and successful\ntransmission probabilities. We assume these probabilities are our constraints\nand then maximize the secure throughput, establishing a security-reliability\ntrade-off for the proposed scenario. Our numerical results illustrate the\neffect of this trade-off on the secure throughput as well as on the number of\nantennas at Alice, Bob and Eve. Interestingly, a small sacrifice in reliability\nallows secrecy enhancement in terms of secure bps/Hz. We apply this idea in our\nsmart grid application to exemplify that, although Eve may acquire some samples\nof the average power demand of a household, it is not enough to properly\nreconstruct such curve. \n\n"}
{"id": "1511.06522", "contents": "Title: Integrating Deep Features for Material Recognition Abstract: We propose a method for integration of features extracted using deep\nrepresentations of Convolutional Neural Networks (CNNs) each of which is\nlearned using a different image dataset of objects and materials for material\nrecognition. Given a set of representations of multiple pre-trained CNNs, we\nfirst compute activations of features using the representations on the images\nto select a set of samples which are best represented by the features. Then, we\nmeasure the uncertainty of the features by computing the entropy of class\ndistributions for each sample set. Finally, we compute the contribution of each\nfeature to representation of classes for feature selection and integration. We\nexamine the proposed method on three benchmark datasets for material\nrecognition. Experimental results show that the proposed method achieves\nstate-of-the-art performance by integrating deep features. Additionally, we\nintroduce a new material dataset called EFMD by extending Flickr Material\nDatabase (FMD). By the employment of the EFMD with transfer learning for\nupdating the learned CNN models, we achieve 84.0%+/-1.8% accuracy on the FMD\ndataset which is close to human performance that is 84.9%. \n\n"}
{"id": "1511.06566", "contents": "Title: Acceleration of the PDHGM on strongly convex subspaces Abstract: We propose several variants of the primal-dual method due to Chambolle and\nPock. Without requiring full strong convexity of the objective functions, our\nmethods are accelerated on subspaces with strong convexity. This yields mixed\nrates, $O(1/N^2)$ with respect to initialisation and $O(1/N)$ with respect to\nthe dual sequence, and the residual part of the primal sequence. We demonstrate\nthe efficacy of the proposed methods on image processing problems lacking\nstrong convexity, such as total generalised variation denoising and total\nvariation deblurring. \n\n"}
{"id": "1511.06860", "contents": "Title: Convex Sparse Spectral Clustering: Single-view to Multi-view Abstract: Spectral Clustering (SC) is one of the most widely used methods for data\nclustering. It first finds a low-dimensonal embedding $U$ of data by computing\nthe eigenvectors of the normalized Laplacian matrix, and then performs k-means\non $U^\\top$ to get the final clustering result. In this work, we observe that,\nin the ideal case, $UU^\\top$ should be block diagonal and thus sparse.\nTherefore we propose the Sparse Spectral Clustering (SSC) method which extends\nSC with sparse regularization on $UU^\\top$. To address the computational issue\nof the nonconvex SSC model, we propose a novel convex relaxation of SSC based\non the convex hull of the fixed rank projection matrices. Then the convex SSC\nmodel can be efficiently solved by the Alternating Direction Method of\n\\canyi{Multipliers} (ADMM). Furthermore, we propose the Pairwise Sparse\nSpectral Clustering (PSSC) which extends SSC to boost the clustering\nperformance by using the multi-view information of data. Experimental\ncomparisons with several baselines on real-world datasets testify to the\nefficacy of our proposed methods. \n\n"}
{"id": "1511.07533", "contents": "Title: Distributed Energy Beamforming with One-Bit Feedback Abstract: Energy beamforming (EB) is a key technique for achieving efficient\nradio-frequency (RF) transmission enabled wireless energy transfer (WET). By\noptimally designing the waveforms from multiple energy transmitters (ETs) over\nthe wireless channels, they are constructively combined at the energy receiver\n(ER) to achieve an EB gain that scales with the number of ETs. However, the\noptimal design of transmit waveforms requires accurate channel state\ninformation (CSI) at the ETs, which is challenging to obtain in practical WET\nsystems. In this paper, we propose a new channel training scheme to achieve\noptimal EB gain in a distributed WET system, where multiple separated ETs\nadjust their transmit phases to collaboratively send power to a single ER in an\niterative manner, based on one-bit feedback from the ER per training interval\nwhich indicates the increase/decrease of the received power level from one\nparticular ET over two preassigned transmit phases. The proposed EB algorithm\ncan be efficiently implemented in practical WET systems even with a large\nnumber of distributed ETs, and is analytically shown to converge quickly to the\noptimal EB design as the number of feedback intervals per ET increases.\nNumerical results are provided to evaluate the performance of the proposed\nalgorithm as compared to other distributed EB designs. \n\n"}
{"id": "1512.00156", "contents": "Title: Covariance-domain Dictionary Learning for Overcomplete EEG Source\n  Identification Abstract: We propose an algorithm targeting the identification of more sources than\nchannels for electroencephalography (EEG). Our overcomplete source\nidentification algorithm, Cov-DL, leverages dictionary learning methods applied\nin the covariance-domain. Assuming that EEG sources are uncorrelated within\nmoving time-windows and the scalp mixing is linear, the forward problem can be\ntransferred to the covariance domain which has higher dimensionality than the\noriginal EEG channel domain. This allows for learning the overcomplete mixing\nmatrix that generates the scalp EEG even when there may be more sources than\nsensors active at any time segment, i.e. when there are non-sparse sources.\nThis is contrary to straight-forward dictionary learning methods that are based\non the assumption of sparsity, which is not a satisfied condition in the case\nof low-density EEG systems. We present two different learning strategies for\nCov-DL, determined by the size of the target mixing matrix. We demonstrate that\nCov-DL outperforms existing overcomplete ICA algorithms under various scenarios\nof EEG simulations and real EEG experiments. \n\n"}
{"id": "1512.00818", "contents": "Title: Zero-Shot Event Detection by Multimodal Distributional Semantic\n  Embedding of Videos Abstract: We propose a new zero-shot Event Detection method by Multi-modal\nDistributional Semantic embedding of videos. Our model embeds object and action\nconcepts as well as other available modalities from videos into a\ndistributional semantic space. To our knowledge, this is the first Zero-Shot\nevent detection model that is built on top of distributional semantics and\nextends it in the following directions: (a) semantic embedding of multimodal\ninformation in videos (with focus on the visual modalities), (b) automatically\ndetermining relevance of concepts/attributes to a free text query, which could\nbe useful for other applications, and (c) retrieving videos by free text event\nquery (e.g., \"changing a vehicle tire\") based on their content. We embed videos\ninto a distributional semantic space and then measure the similarity between\nvideos and the event query in a free text form. We validated our method on the\nlarge TRECVID MED (Multimedia Event Detection) challenge. Using only the event\ntitle as a query, our method outperformed the state-of-the-art that uses big\ndescriptions from 12.6% to 13.5% with MAP metric and 0.73 to 0.83 with ROC-AUC\nmetric. It is also an order of magnitude faster. \n\n"}
{"id": "1512.02326", "contents": "Title: Learning to Point and Count Abstract: This paper proposes the problem of point-and-count as a test case to break\nthe what-and-where deadlock. Different from the traditional detection problem,\nthe goal is to discover key salient points as a way to localize and count the\nnumber of objects simultaneously. We propose two alternatives, one that counts\nfirst and then point, and another that works the other way around.\nFundamentally, they pivot around whether we solve \"what\" or \"where\" first. We\nevaluate their performance on dataset that contains multiple instances of the\nsame class, demonstrating the potentials and their synergies. The experiences\nderive a few important insights that explains why this is a much harder problem\nthan classification, including strong data bias and the inability to deal with\nobject scales robustly in state-of-art convolutional neural networks. \n\n"}
{"id": "1512.02413", "contents": "Title: Tracking Objects with Higher Order Interactions using Delayed Column\n  Generation Abstract: We study the problem of multi-target tracking and data association in video.\nWe formulate this in terms of selecting a subset of high-quality tracks subject\nto the constraint that no pair of selected tracks is associated with a common\ndetection (of an object). This objective is equivalent to the classic NP-hard\nproblem of finding a maximum-weight set packing (MWSP) where tracks correspond\nto sets and is made further difficult since the number of candidate tracks\ngrows exponentially in the number of detections. We present a relaxation of\nthis combinatorial problem that uses a column generation formulation where the\npricing problem is solved via dynamic programming to efficiently explore the\nspace of tracks. We employ row generation to tighten the bound in such a way as\nto preserve efficient inference in the pricing problem. We show the practical\nutility of this algorithm for tracking problems in natural and biological video\ndatasets. \n\n"}
{"id": "1512.03460", "contents": "Title: Neural Self Talk: Image Understanding via Continuous Questioning and\n  Answering Abstract: In this paper we consider the problem of continuously discovering image\ncontents by actively asking image based questions and subsequently answering\nthe questions being asked. The key components include a Visual Question\nGeneration (VQG) module and a Visual Question Answering module, in which\nRecurrent Neural Networks (RNN) and Convolutional Neural Network (CNN) are\nused. Given a dataset that contains images, questions and their answers, both\nmodules are trained at the same time, with the difference being VQG uses the\nimages as input and the corresponding questions as output, while VQA uses\nimages and questions as input and the corresponding answers as output. We\nevaluate the self talk process subjectively using Amazon Mechanical Turk, which\nshow effectiveness of the proposed method. \n\n"}
{"id": "1512.04205", "contents": "Title: Compressed Dynamic Mode Decomposition for Background Modeling Abstract: We introduce the method of compressed dynamic mode decomposition (cDMD) for\nbackground modeling. The dynamic mode decomposition (DMD) is a regression\ntechnique that integrates two of the leading data analysis methods in use\ntoday: Fourier transforms and singular value decomposition. Borrowing ideas\nfrom compressed sensing and matrix sketching, cDMD eases the computational\nworkload of high resolution video processing. The key principal of cDMD is to\nobtain the decomposition on a (small) compressed matrix representation of the\nvideo feed. Hence, the cDMD algorithm scales with the intrinsic rank of the\nmatrix, rather then the size of the actual video (data) matrix. Selection of\nthe optimal modes characterizing the background is formulated as a\nsparsity-constrained sparse coding problem. Our results show, that the quality\nof the resulting background model is competitive, quantified by the F-measure,\nRecall and Precision. A GPU (graphics processing unit) accelerated\nimplementation is also presented which further boosts the computational\nefficiency of the algorithm. \n\n"}
{"id": "1601.02088", "contents": "Title: Multicuts and Perturb & MAP for Probabilistic Graph Clustering Abstract: We present a probabilistic graphical model formulation for the graph\nclustering problem. This enables to locally represent uncertainty of image\npartitions by approximate marginal distributions in a mathematically\nsubstantiated way, and to rectify local data term cues so as to close contours\nand to obtain valid partitions.\n  We exploit recent progress on globally optimal MAP inference by integer\nprogramming and on perturbation-based approximations of the log-partition\nfunction, in order to sample clusterings and to estimate marginal distributions\nof node-pairs both more accurately and more efficiently than state-of-the-art\nmethods. Our approach works for any graphically represented problem instance.\nThis is demonstrated for image segmentation and social network cluster\nanalysis. Our mathematical ansatz should be relevant also for other\ncombinatorial problems. \n\n"}
{"id": "1601.06280", "contents": "Title: Sub-Quadratic Decoding of Gabidulin Codes Abstract: This paper shows how to decode errors and erasures with Gabidulin codes in\nsub-quadratic time in the code length, improving previous algorithms which had\nat least quadratic complexity. The complexity reduction is achieved by\naccelerating operations on linearized polynomials. In particular, we present\nfast algorithms for division, multi-point evaluation and interpolation of\nlinearized polynomials and show how to efficiently compute minimal subspace\npolynomials. \n\n"}
{"id": "1601.06611", "contents": "Title: \"Pretty strong\" converse for the private capacity of degraded quantum\n  wiretap channels Abstract: In the vein of the recent \"pretty strong\" converse for the quantum and\nprivate capacity of degradable quantum channels [Morgan/Winter, IEEE Trans.\nInf. Theory 60(1):317-333, 2014], we use the same techniques, in particular the\ncalculus of min-entropies, to show a pretty strong converse for the private\ncapacity of degraded classical-quantum-quantum (cqq-)wiretap channels, which\ngeneralize Wyner's model of the degraded classical wiretap channel.\n  While the result is not completely tight, leaving some gap between the region\nof error and privacy parameters for which the converse bound holds, and a\nlarger no-go region, it represents a further step towards an understanding of\nstrong converses of wiretap channels [cf. Hayashi/Tyagi/Watanabe,\narXiv:1410.0443 for the classical case]. \n\n"}
{"id": "1601.06759", "contents": "Title: Pixel Recurrent Neural Networks Abstract: Modeling the distribution of natural images is a landmark problem in\nunsupervised learning. This task requires an image model that is at once\nexpressive, tractable and scalable. We present a deep neural network that\nsequentially predicts the pixels in an image along the two spatial dimensions.\nOur method models the discrete probability of the raw pixel values and encodes\nthe complete set of dependencies in the image. Architectural novelties include\nfast two-dimensional recurrent layers and an effective use of residual\nconnections in deep recurrent networks. We achieve log-likelihood scores on\nnatural images that are considerably better than the previous state of the art.\nOur main results also provide benchmarks on the diverse ImageNet dataset.\nSamples generated from the model appear crisp, varied and globally coherent. \n\n"}
{"id": "1602.01464", "contents": "Title: Latent-Class Hough Forests for 6 DoF Object Pose Estimation Abstract: In this paper we present Latent-Class Hough Forests, a method for object\ndetection and 6 DoF pose estimation in heavily cluttered and occluded\nscenarios. We adapt a state of the art template matching feature into a\nscale-invariant patch descriptor and integrate it into a regression forest\nusing a novel template-based split function. We train with positive samples\nonly and we treat class distributions at the leaf nodes as latent variables.\nDuring testing we infer by iteratively updating these distributions, providing\naccurate estimation of background clutter and foreground occlusions and, thus,\nbetter detection rate. Furthermore, as a by-product, our Latent-Class Hough\nForests can provide accurate occlusion aware segmentation masks, even in the\nmulti-instance scenario. In addition to an existing public dataset, which\ncontains only single-instance sequences with large amounts of clutter, we have\ncollected two, more challenging, datasets for multiple-instance detection\ncontaining heavy 2D and 3D clutter as well as foreground occlusions. We provide\nextensive experiments on the various parameters of the framework such as patch\nsize, number of trees and number of iterations to infer class distributions at\ntest time. We also evaluate the Latent-Class Hough Forests on all datasets\nwhere we outperform state of the art methods. \n\n"}
{"id": "1602.03536", "contents": "Title: Duality between erasures and defects Abstract: We investigate the duality of the binary erasure channel (BEC) and the binary\ndefect channel (BDC). This duality holds for channel capacities, capacity\nachieving schemes, minimum distances, and upper bounds on the probability of\nfailure to retrieve the original message. In addition, the relations between\nBEC, BDC, binary erasure quantization (BEQ), and write-once memory (WOM) are\ndescribed. From these relations we claim that the capacity of the BDC can be\nachieved by Reed-Muller (RM) codes under maximum a posterior (MAP) decoding.\nAlso, polar codes with a successive cancellation encoder achieve the capacity\nof the BDC.\n  Inspired by the duality between the BEC and the BDC, we introduce locally\nrewritable codes (LWC) for resistive memories, which are the counterparts of\nlocally repairable codes (LRC) for distributed storage systems. The proposed\nLWC can improve endurance limit and power efficiency of resistive memories. \n\n"}
{"id": "1602.04105", "contents": "Title: Convolutional Radio Modulation Recognition Networks Abstract: We study the adaptation of convolutional neural networks to the complex\ntemporal radio signal domain. We compare the efficacy of radio modulation\nclassification using naively learned features against using expert features\nwhich are widely used in the field today and we show significant performance\nimprovements. We show that blind temporal learning on large and densely encoded\ntime series using deep convolutional neural networks is viable and a strong\ncandidate approach for this task especially at low signal to noise ratio. \n\n"}
{"id": "1602.04193", "contents": "Title: Distributed Average Consensus with Bounded Quantizer and Unbounded Input Abstract: This paper considers distributed average consensus using finite-bit bounded\nquantizer with possibly unbounded data. Under the framework of the alternating\ndirection method of multipliers (ADMM), we develop distributed averaging\nalgorithms where each node iteratively updates using only the local information\nand finitely quantized outputs from its neighbors. It is shown that all the\nagent variables either converge to the same quantization level or cycle around\nthe data average after finite iterations. An error bound for the consensus\nvalue is established, which turns out to be the same as that of using the\nunbounded rounding quantizer provided that an algorithm parameter (i.e., ADMM\nstep size) is small enough. We also analyze the effect of the algorithm\nparameter and propose an adaptive parameter selection strategy that only\nrequires knowledge of the number of agents in order to accelerate the algorithm\nwith certain consensus accuracy guarantee. Finally, simulations are performed\nto illustrate the effectiveness of the proposed algorithms. \n\n"}
{"id": "1602.08575", "contents": "Title: Superresolution of Noisy Remotely Sensed Images Through Directional\n  Representations Abstract: We develop an algorithm for single-image superresolution of remotely sensed\ndata, based on the discrete shearlet transform. The shearlet transform extracts\ndirectional features of signals, and is known to provide near-optimally sparse\nrepresentations for a broad class of images. This often leads to superior\nperformance in edge detection and image representation when compared to\nisotropic frames. We justify the use of shearlets mathematically, before\npresenting a denoising single-image superresolution algorithm that combines the\nshearlet transform with sparse mixing estimators (SME). Our algorithm is\ncompared with a variety of single-image superresolution methods, including\nwavelet SME superresolution. Our numerical results demonstrate competitive\nperformance in terms of PSNR and SSIM. \n\n"}
{"id": "1603.00124", "contents": "Title: Learning Multilayer Channel Features for Pedestrian Detection Abstract: Pedestrian detection based on the combination of Convolutional Neural Network\n(i.e., CNN) and traditional handcrafted features (i.e., HOG+LUV) has achieved\ngreat success. Generally, HOG+LUV are used to generate the candidate proposals\nand then CNN classifies these proposals. Despite its success, there is still\nroom for improvement. For example, CNN classifies these proposals by the\nfull-connected layer features while proposal scores and the features in the\ninner-layers of CNN are ignored. In this paper, we propose a unifying framework\ncalled Multilayer Channel Features (MCF) to overcome the drawback. It firstly\nintegrates HOG+LUV with each layer of CNN into a multi-layer image channels.\nBased on the multi-layer image channels, a multi-stage cascade AdaBoost is then\nlearned. The weak classifiers in each stage of the multi-stage cascade is\nlearned from the image channels of corresponding layer. With more abundant\nfeatures, MCF achieves the state-of-the-art on Caltech pedestrian dataset\n(i.e., 10.40% miss rate). Using new and accurate annotations, MCF achieves\n7.98% miss rate. As many non-pedestrian detection windows can be quickly\nrejected by the first few stages, it accelerates detection speed by 1.43 times.\nBy eliminating the highly overlapped detection windows with lower scores after\nthe first stage, it's 4.07 times faster with negligible performance loss. \n\n"}
{"id": "1603.01250", "contents": "Title: Decision Forests, Convolutional Networks and the Models in-Between Abstract: This paper investigates the connections between two state of the art\nclassifiers: decision forests (DFs, including decision jungles) and\nconvolutional neural networks (CNNs). Decision forests are computationally\nefficient thanks to their conditional computation property (computation is\nconfined to only a small region of the tree, the nodes along a single branch).\nCNNs achieve state of the art accuracy, thanks to their representation learning\ncapabilities. We present a systematic analysis of how to fuse conditional\ncomputation with representation learning and achieve a continuum of hybrid\nmodels with different ratios of accuracy vs. efficiency. We call this new\nfamily of hybrid models conditional networks. Conditional networks can be\nthought of as: i) decision trees augmented with data transformation operators,\nor ii) CNNs, with block-diagonal sparse weight matrices, and explicit data\nrouting functions. Experimental validation is performed on the common task of\nimage classification on both the CIFAR and Imagenet datasets. Compared to state\nof the art CNNs, our hybrid models yield the same accuracy with a fraction of\nthe compute cost and much smaller number of parameters. \n\n"}
{"id": "1603.01417", "contents": "Title: Dynamic Memory Networks for Visual and Textual Question Answering Abstract: Neural network architectures with memory and attention mechanisms exhibit\ncertain reasoning capabilities required for question answering. One such\narchitecture, the dynamic memory network (DMN), obtained high accuracy on a\nvariety of language tasks. However, it was not shown whether the architecture\nachieves strong results for question answering when supporting facts are not\nmarked during training or whether it could be applied to other modalities such\nas images. Based on an analysis of the DMN, we propose several improvements to\nits memory and input modules. Together with these changes we introduce a novel\ninput module for images in order to be able to answer visual questions. Our new\nDMN+ model improves the state of the art on both the Visual Question Answering\ndataset and the \\babi-10k text question-answering dataset without supporting\nfact supervision. \n\n"}
{"id": "1603.01670", "contents": "Title: Network Morphism Abstract: We present in this paper a systematic study on how to morph a well-trained\nneural network to a new one so that its network function can be completely\npreserved. We define this as \\emph{network morphism} in this research. After\nmorphing a parent network, the child network is expected to inherit the\nknowledge from its parent network and also has the potential to continue\ngrowing into a more powerful one with much shortened training time. The first\nrequirement for this network morphism is its ability to handle diverse morphing\ntypes of networks, including changes of depth, width, kernel size, and even\nsubnet. To meet this requirement, we first introduce the network morphism\nequations, and then develop novel morphing algorithms for all these morphing\ntypes for both classic and convolutional neural networks. The second\nrequirement for this network morphism is its ability to deal with non-linearity\nin a network. We propose a family of parametric-activation functions to\nfacilitate the morphing of any continuous non-linear activation neurons.\nExperimental results on benchmark datasets and typical neural networks\ndemonstrate the effectiveness of the proposed network morphism scheme. \n\n"}
{"id": "1603.03657", "contents": "Title: Efficient forward propagation of time-sequences in convolutional neural\n  networks using Deep Shifting Abstract: When a Convolutional Neural Network is used for on-the-fly evaluation of\ncontinuously updating time-sequences, many redundant convolution operations are\nperformed. We propose the method of Deep Shifting, which remembers previously\ncalculated results of convolution operations in order to minimize the number of\ncalculations. The reduction in complexity is at least a constant and in the\nbest case quadratic. We demonstrate that this method does indeed save\nsignificant computation time in a practical implementation, especially when the\nnetworks receives a large number of time-frames. \n\n"}
{"id": "1603.04150", "contents": "Title: Regression-based Hypergraph Learning for Image Clustering and\n  Classification Abstract: Inspired by the recently remarkable successes of Sparse Representation (SR),\nCollaborative Representation (CR) and sparse graph, we present a novel\nhypergraph model named Regression-based Hypergraph (RH) which utilizes the\nregression models to construct the high quality hypergraphs. Moreover, we plug\nRH into two conventional hypergraph learning frameworks, namely hypergraph\nspectral clustering and hypergraph transduction, to present Regression-based\nHypergraph Spectral Clustering (RHSC) and Regression-based Hypergraph\nTransduction (RHT) models for addressing the image clustering and\nclassification issues. Sparse Representation and Collaborative Representation\nare employed to instantiate two RH instances and their RHSC and RHT algorithms.\nThe experimental results on six popular image databases demonstrate that the\nproposed RH learning algorithms achieve promising image clustering and\nclassification performances, and also validate that RH can inherit the\ndesirable properties from both hypergraph models and regression models. \n\n"}
{"id": "1603.04186", "contents": "Title: Visual Concept Recognition and Localization via Iterative Introspection Abstract: Convolutional neural networks have been shown to develop internal\nrepresentations, which correspond closely to semantically meaningful objects\nand parts, although trained solely on class labels. Class Activation Mapping\n(CAM) is a recent method that makes it possible to easily highlight the image\nregions contributing to a network's classification decision. We build upon\nthese two developments to enable a network to re-examine informative image\nregions, which we term introspection. We propose a weakly-supervised iterative\nscheme, which shifts its center of attention to increasingly discriminative\nregions as it progresses, by alternating stages of classification and\nintrospection. We evaluate our method and show its effectiveness over a range\nof several datasets, where we obtain competitive or state-of-the-art results:\non Stanford-40 Actions, we set a new state-of the art of 81.74%. On\nFGVC-Aircraft and the Stanford Dogs dataset, we show consistent improvements\nover baselines, some of which include significantly more supervision. \n\n"}
{"id": "1603.04245", "contents": "Title: A Variational Perspective on Accelerated Methods in Optimization Abstract: Accelerated gradient methods play a central role in optimization, achieving\noptimal rates in many settings. While many generalizations and extensions of\nNesterov's original acceleration method have been proposed, it is not yet clear\nwhat is the natural scope of the acceleration concept. In this paper, we study\naccelerated methods from a continuous-time perspective. We show that there is a\nLagrangian functional that we call the \\emph{Bregman Lagrangian} which\ngenerates a large class of accelerated methods in continuous time, including\n(but not limited to) accelerated gradient descent, its non-Euclidean extension,\nand accelerated higher-order gradient methods. We show that the continuous-time\nlimit of all of these methods correspond to traveling the same curve in\nspacetime at different speeds. From this perspective, Nesterov's technique and\nmany of its generalizations can be viewed as a systematic way to go from the\ncontinuous-time curves generated by the Bregman Lagrangian to a family of\ndiscrete-time accelerated algorithms. \n\n"}
{"id": "1603.06036", "contents": "Title: Fractal Dimension Invariant Filtering and Its CNN-based Implementation Abstract: Fractal analysis has been widely used in computer vision, especially in\ntexture image processing and texture analysis. The key concept of fractal-based\nimage model is the fractal dimension, which is invariant to bi-Lipschitz\ntransformation of image, and thus capable of representing intrinsic structural\ninformation of image robustly. However, the invariance of fractal dimension\ngenerally does not hold after filtering, which limits the application of\nfractal-based image model. In this paper, we propose a novel fractal dimension\ninvariant filtering (FDIF) method, extending the invariance of fractal\ndimension to filtering operations. Utilizing the notion of local\nself-similarity, we first develop a local fractal model for images. By adding a\nnonlinear post-processing step behind anisotropic filter banks, we demonstrate\nthat the proposed filtering method is capable of preserving the local\ninvariance of the fractal dimension of image. Meanwhile, we show that the FDIF\nmethod can be re-instantiated approximately via a CNN-based architecture, where\nthe convolution layer extracts anisotropic structure of image and the nonlinear\nlayer enhances the structure via preserving local fractal dimension of image.\nThe proposed filtering method provides us with a novel geometric interpretation\nof CNN-based image model. Focusing on a challenging image processing task ---\ndetecting complicated curves from the texture-like images, the proposed method\nobtains superior results to the state-of-art approaches. \n\n"}
{"id": "1603.06531", "contents": "Title: Deep video gesture recognition using illumination invariants Abstract: In this paper we present architectures based on deep neural nets for gesture\nrecognition in videos, which are invariant to local scaling. We amalgamate\nautoencoder and predictor architectures using an adaptive weighting scheme\ncoping with a reduced size labeled dataset, while enriching our models from\nenormous unlabeled sets. We further improve robustness to lighting conditions\nby introducing a new adaptive filer based on temporal local scale\nnormalization. We provide superior results over known methods, including recent\nreported approaches based on neural nets. \n\n"}
{"id": "1603.07697", "contents": "Title: Joint Projection and Dictionary Learning using Low-rank Regularization\n  and Graph Constraints Abstract: In this paper, we aim at learning simultaneously a discriminative dictionary\nand a robust projection matrix from noisy data. The joint learning, makes the\nlearned projection and dictionary a better fit for each other, so a more\naccurate classification can be obtained. However, current prevailing joint\ndimensionality reduction and dictionary learning methods, would fail when the\ntraining samples are noisy or heavily corrupted. To address this issue, we\npropose a joint projection and dictionary learning using low-rank\nregularization and graph constraints (JPDL-LR). Specifically, the\ndiscrimination of the dictionary is achieved by imposing Fisher criterion on\nthe coding coefficients. In addition, our method explicitly encodes the local\nstructure of data by incorporating a graph regularization term, that further\nimproves the discriminative ability of the projection matrix. Inspired by\nrecent advances of low-rank representation for removing outliers and noise, we\nenforce a low-rank constraint on sub-dictionaries of all classes to make them\nmore compact and robust to noise. Experimental results on several benchmark\ndatasets verify the effectiveness and robustness of our method for both\ndimensionality reduction and image classification, especially when the data\ncontains considerable noise or variations. \n\n"}
{"id": "1603.07758", "contents": "Title: A universal tradeoff between power, precision and speed in physical\n  communication Abstract: Maximizing the speed and precision of communication while minimizing power\ndissipation is a fundamental engineering design goal. Also, biological systems\nachieve remarkable speed, precision and power efficiency using poorly\nunderstood physical design principles. Powerful theories like information\ntheory and thermodynamics do not provide general limits on power, precision and\nspeed. Here we go beyond these classical theories to prove that the product of\nprecision and speed is universally bounded by power dissipation in any physical\ncommunication channel whose dynamics is faster than that of the signal.\nMoreover, our derivation involves a novel connection between friction and\ninformation geometry. These results may yield insight into both the engineering\ndesign of communication devices and the structure and function of biological\nsignaling systems. \n\n"}
{"id": "1603.09056", "contents": "Title: Image Restoration Using Very Deep Convolutional Encoder-Decoder Networks\n  with Symmetric Skip Connections Abstract: In this paper, we propose a very deep fully convolutional encoding-decoding\nframework for image restoration such as denoising and super-resolution. The\nnetwork is composed of multiple layers of convolution and de-convolution\noperators, learning end-to-end mappings from corrupted images to the original\nones. The convolutional layers act as the feature extractor, which capture the\nabstraction of image contents while eliminating noises/corruptions.\nDe-convolutional layers are then used to recover the image details. We propose\nto symmetrically link convolutional and de-convolutional layers with skip-layer\nconnections, with which the training converges much faster and attains a\nhigher-quality local optimum. First, The skip connections allow the signal to\nbe back-propagated to bottom layers directly, and thus tackles the problem of\ngradient vanishing, making training deep networks easier and achieving\nrestoration performance gains consequently. Second, these skip connections pass\nimage details from convolutional layers to de-convolutional layers, which is\nbeneficial in recovering the original image. Significantly, with the large\ncapacity, we can handle different levels of noises using a single model.\nExperimental results show that our network achieves better performance than all\npreviously reported state-of-the-art methods. \n\n"}
{"id": "1603.09446", "contents": "Title: Object Skeleton Extraction in Natural Images by Fusing Scale-associated\n  Deep Side Outputs Abstract: Object skeleton is a useful cue for object detection, complementary to the\nobject contour, as it provides a structural representation to describe the\nrelationship among object parts. While object skeleton extraction in natural\nimages is a very challenging problem, as it requires the extractor to be able\nto capture both local and global image context to determine the intrinsic scale\nof each skeleton pixel. Existing methods rely on per-pixel based multi-scale\nfeature computation, which results in difficult modeling and high time\nconsumption. In this paper, we present a fully convolutional network with\nmultiple scale-associated side outputs to address this problem. By observing\nthe relationship between the receptive field sizes of the sequential stages in\nthe network and the skeleton scales they can capture, we introduce a\nscale-associated side output to each stage. We impose supervision to different\nstages by guiding the scale-associated side outputs toward groundtruth\nskeletons of different scales. The responses of the multiple scale-associated\nside outputs are then fused in a scale-specific way to localize skeleton pixels\nwith multiple scales effectively. Our method achieves promising results on two\nskeleton extraction datasets, and significantly outperforms other competitors. \n\n"}
{"id": "1604.00691", "contents": "Title: Event excitation for event-driven control and optimization of\n  multi-agent systems Abstract: We consider event-driven methods in a general framework for the control and\noptimization of multi-agent systems, viewing them as stochastic hybrid systems.\nSuch systems often have feasible realizations in which the events needed to\nexcite an on-line event-driven controller cannot occur, rendering the use of\nsuch controllers ineffective. We show that this commonly happens in\nenvironments which contain discrete points of interest which the agents must\nvisit. To address this problem in event-driven gradient-based optimization\nproblems, we propose a new metric for the objective function which creates a\npotential field guaranteeing that gradient values are non-zero when no events\nare present and which results in eventual event excitation. We apply this\napproach to the class of cooperative multi-agent data collection problems using\nthe event-driven Infinitesimal Perturbation Analysis (IPA) methodology and\ninclude numerical examples illustrating its effectiveness. \n\n"}
{"id": "1604.01252", "contents": "Title: Comparative Deep Learning of Hybrid Representations for Image\n  Recommendations Abstract: In many image-related tasks, learning expressive and discriminative\nrepresentations of images is essential, and deep learning has been studied for\nautomating the learning of such representations. Some user-centric tasks, such\nas image recommendations, call for effective representations of not only images\nbut also preferences and intents of users over images. Such representations are\ntermed \\emph{hybrid} and addressed via a deep learning approach in this paper.\nWe design a dual-net deep network, in which the two sub-networks map input\nimages and preferences of users into a same latent semantic space, and then the\ndistances between images and users in the latent space are calculated to make\ndecisions. We further propose a comparative deep learning (CDL) method to train\nthe deep network, using a pair of images compared against one user to learn the\npattern of their relative distances. The CDL embraces much more training data\nthan naive deep learning, and thus achieves superior performance than the\nlatter, with no cost of increasing network complexity. Experimental results\nwith real-world data sets for image recommendations have shown the proposed\ndual-net network and CDL greatly outperform other state-of-the-art image\nrecommendation solutions. \n\n"}
{"id": "1604.01655", "contents": "Title: Correlated and Individual Multi-Modal Deep Learning for RGB-D Object\n  Recognition Abstract: In this paper, we propose a new correlated and individual multi-modal deep\nlearning (CIMDL) method for RGB-D object recognition. Unlike most conventional\nRGB-D object recognition methods which extract features from the RGB and depth\nchannels individually, our CIMDL jointly learns feature representations from\nraw RGB-D data with a pair of deep neural networks, so that the sharable and\nmodal-specific information can be simultaneously exploited. Specifically, we\nconstruct a pair of deep convolutional neural networks (CNNs) for the RGB and\ndepth data, and concatenate them at the top layer of the network with a loss\nfunction which learns a new feature space where both correlated part and the\nindividual part of the RGB-D information are well modelled. The parameters of\nthe whole networks are updated by using the back-propagation criterion.\nExperimental results on two widely used RGB-D object image benchmark datasets\nclearly show that our method outperforms state-of-the-arts. \n\n"}
{"id": "1604.01729", "contents": "Title: Improving LSTM-based Video Description with Linguistic Knowledge Mined\n  from Text Abstract: This paper investigates how linguistic knowledge mined from large text\ncorpora can aid the generation of natural language descriptions of videos.\nSpecifically, we integrate both a neural language model and distributional\nsemantics trained on large text corpora into a recent LSTM-based architecture\nfor video description. We evaluate our approach on a collection of Youtube\nvideos as well as two large movie description datasets showing significant\nimprovements in grammaticality while modestly improving descriptive quality. \n\n"}
{"id": "1604.01931", "contents": "Title: Geometric Scene Parsing with Hierarchical LSTM Abstract: This paper addresses the problem of geometric scene parsing, i.e.\nsimultaneously labeling geometric surfaces (e.g. sky, ground and vertical\nplane) and determining the interaction relations (e.g. layering, supporting,\nsiding and affinity) between main regions. This problem is more challenging\nthan the traditional semantic scene labeling, as recovering geometric\nstructures necessarily requires the rich and diverse contextual information. To\nachieve these goals, we propose a novel recurrent neural network model, named\nHierarchical Long Short-Term Memory (H-LSTM). It contains two coupled\nsub-networks: the Pixel LSTM (P-LSTM) and the Multi-scale Super-pixel LSTM\n(MS-LSTM) for handling the surface labeling and relation prediction,\nrespectively. The two sub-networks provide complementary information to each\nother to exploit hierarchical scene contexts, and they are jointly optimized\nfor boosting the performance. Our extensive experiments show that our model is\ncapable of parsing scene geometric structures and outperforming several\nstate-of-the-art methods by large margins. In addition, we show promising 3D\nreconstruction results from the still images based on the geometric parsing. \n\n"}
{"id": "1604.03913", "contents": "Title: Dynamic Approaches for Some Time Inconsistent Problems Abstract: In this paper we investigate possible approaches to study general\ntime-inconsistent optimization problems without assuming the existence of\noptimal strategy. This leads immediately to the need to refine the concept of\ntime-consistency as well as any method that is based on Pontryagin's Maximum\nPrinciple. The fundamental obstacle is the dilemma of having to invoke the {\\it\nDynamic Programming Principle} (DPP) in a time-inconsistent setting, which is\ncontradictory in nature. The main contribution of this work is the introduction\nof the idea of the \"dynamic utility\" under which the original time inconsistent\nproblem (under the fixed utility) becomes a time consistent one. As a benchmark\nmodel, we shall consider a stochastic controlled problem with multidimensional\nbackward SDE dynamics, which covers many existing time-inconsistent problems in\nthe literature as special cases, and we argue that the time inconsistency is\nessentially equivalent to the lack of {\\it comparison principle}. We shall\npropose three approaches aiming at reviving the DPP in this setting: the\nduality approach, the dynamic utility approach, and the master equation\napproach. Unlike the game approach in many existing works in continuous time\nmodels, all our approaches produce the same value as the original static\nproblem. \n\n"}
{"id": "1604.05495", "contents": "Title: Deep Saliency with Encoded Low level Distance Map and High Level\n  Features Abstract: Recent advances in saliency detection have utilized deep learning to obtain\nhigh level features to detect salient regions in a scene. These advances have\ndemonstrated superior results over previous works that utilize hand-crafted low\nlevel features for saliency detection. In this paper, we demonstrate that\nhand-crafted features can provide complementary information to enhance\nperformance of saliency detection that utilizes only high level features. Our\nmethod utilizes both high level and low level features for saliency detection\nunder a unified deep learning framework. The high level features are extracted\nusing the VGG-net, and the low level features are compared with other parts of\nan image to form a low level distance map. The low level distance map is then\nencoded using a convolutional neural network(CNN) with multiple 1X1\nconvolutional and ReLU layers. We concatenate the encoded low level distance\nmap and the high level features, and connect them to a fully connected neural\nnetwork classifier to evaluate the saliency of a query region. Our experiments\nshow that our method can further improve the performance of state-of-the-art\ndeep learning-based saliency detection methods. \n\n"}
{"id": "1604.06057", "contents": "Title: Hierarchical Deep Reinforcement Learning: Integrating Temporal\n  Abstraction and Intrinsic Motivation Abstract: Learning goal-directed behavior in environments with sparse feedback is a\nmajor challenge for reinforcement learning algorithms. The primary difficulty\narises due to insufficient exploration, resulting in an agent being unable to\nlearn robust value functions. Intrinsically motivated agents can explore new\nbehavior for its own sake rather than to directly solve problems. Such\nintrinsic behaviors could eventually help the agent solve tasks posed by the\nenvironment. We present hierarchical-DQN (h-DQN), a framework to integrate\nhierarchical value functions, operating at different temporal scales, with\nintrinsically motivated deep reinforcement learning. A top-level value function\nlearns a policy over intrinsic goals, and a lower-level function learns a\npolicy over atomic actions to satisfy the given goals. h-DQN allows for\nflexible goal specifications, such as functions over entities and relations.\nThis provides an efficient space for exploration in complicated environments.\nWe demonstrate the strength of our approach on two problems with very sparse,\ndelayed feedback: (1) a complex discrete stochastic decision process, and (2)\nthe classic ATARI game `Montezuma's Revenge'. \n\n"}
{"id": "1604.06194", "contents": "Title: Dynamic matrix factorization with social influence Abstract: Matrix factorization is a key component of collaborative filtering-based\nrecommendation systems because it allows us to complete sparse user-by-item\nratings matrices under a low-rank assumption that encodes the belief that\nsimilar users give similar ratings and that similar items garner similar\nratings. This paradigm has had immeasurable practical success, but it is not\nthe complete story for understanding and inferring the preferences of people.\nFirst, peoples' preferences and their observable manifestations as ratings\nevolve over time along general patterns of trajectories. Second, an individual\nperson's preferences evolve over time through influence of their social\nconnections. In this paper, we develop a unified process model for both types\nof dynamics within a state space approach, together with an efficient\noptimization scheme for estimation within that model. The model combines\nelements from recent developments in dynamic matrix factorization, opinion\ndynamics and social learning, and trust-based recommendation. The estimation\nbuilds upon recent advances in numerical nonlinear optimization. Empirical\nresults on a large-scale data set from the Epinions website demonstrate\nconsistent reduction in root mean squared error by consideration of the two\ntypes of dynamics. \n\n"}
{"id": "1604.06832", "contents": "Title: Refining Architectures of Deep Convolutional Neural Networks Abstract: Deep Convolutional Neural Networks (CNNs) have recently evinced immense\nsuccess for various image recognition tasks. However, a question of paramount\nimportance is somewhat unanswered in deep learning research - is the selected\nCNN optimal for the dataset in terms of accuracy and model size? In this paper,\nwe intend to answer this question and introduce a novel strategy that alters\nthe architecture of a given CNN for a specified dataset, to potentially enhance\nthe original accuracy while possibly reducing the model size. We use two\noperations for architecture refinement, viz. stretching and symmetrical\nsplitting. Our procedure starts with a pre-trained CNN for a given dataset, and\noptimally decides the stretch and split factors across the network to refine\nthe architecture. We empirically demonstrate the necessity of the two\noperations. We evaluate our approach on two natural scenes attributes datasets,\nSUN Attributes and CAMIT-NSAD, with architectures of GoogleNet and VGG-11, that\nare quite contrasting in their construction. We justify our choice of datasets,\nand show that they are interestingly distinct from each other, and together\npose a challenge to our architectural refinement algorithm. Our results\nsubstantiate the usefulness of the proposed method. \n\n"}
{"id": "1604.07060", "contents": "Title: Binary Codes for Tagging X-Ray Images via Deep De-Noising Autoencoders Abstract: A Content-Based Image Retrieval (CBIR) system which identifies similar\nmedical images based on a query image can assist clinicians for more accurate\ndiagnosis. The recent CBIR research trend favors the construction and use of\nbinary codes to represent images. Deep architectures could learn the non-linear\nrelationship among image pixels adaptively, allowing the automatic learning of\nhigh-level features from raw pixels. However, most of them require class\nlabels, which are expensive to obtain, particularly for medical images. The\nmethods which do not need class labels utilize a deep autoencoder for binary\nhashing, but the code construction involves a specific training algorithm and\nan ad-hoc regularization technique. In this study, we explored using a deep\nde-noising autoencoder (DDA), with a new unsupervised training scheme using\nonly backpropagation and dropout, to hash images into binary codes. We\nconducted experiments on more than 14,000 x-ray images. By using class labels\nonly for evaluating the retrieval results, we constructed a 16-bit DDA and a\n512-bit DDA independently. Comparing to other unsupervised methods, we\nsucceeded to obtain the lowest total error by using the 512-bit codes for\nretrieval via exhaustive search, and speed up 9.27 times with the use of the\n16-bit codes while keeping a comparable total error. We found that our new\ntraining scheme could reduce the total retrieval error significantly by 21.9%.\nTo further boost the image retrieval performance, we developed Radon\nAutoencoder Barcode (RABC) which are learned from the Radon projections of\nimages using a de-noising autoencoder. Experimental results demonstrated its\nsuperior performance in retrieval when it was combined with DDA binary codes. \n\n"}
{"id": "1604.07669", "contents": "Title: Real-time Action Recognition with Enhanced Motion Vector CNNs Abstract: The deep two-stream architecture exhibited excellent performance on video\nbased action recognition. The most computationally expensive step in this\napproach comes from the calculation of optical flow which prevents it to be\nreal-time. This paper accelerates this architecture by replacing optical flow\nwith motion vector which can be obtained directly from compressed videos\nwithout extra calculation. However, motion vector lacks fine structures, and\ncontains noisy and inaccurate motion patterns, leading to the evident\ndegradation of recognition performance. Our key insight for relieving this\nproblem is that optical flow and motion vector are inherent correlated.\nTransferring the knowledge learned with optical flow CNN to motion vector CNN\ncan significantly boost the performance of the latter. Specifically, we\nintroduce three strategies for this, initialization transfer, supervision\ntransfer and their combination. Experimental results show that our method\nachieves comparable recognition performance to the state-of-the-art, while our\nmethod can process 390.7 frames per second, which is 27 times faster than the\noriginal two-stream method. \n\n"}
{"id": "1605.00164", "contents": "Title: Look-ahead before you leap: end-to-end active recognition by forecasting\n  the effect of motion Abstract: Visual recognition systems mounted on autonomous moving agents face the\nchallenge of unconstrained data, but simultaneously have the opportunity to\nimprove their performance by moving to acquire new views of test data. In this\nwork, we first show how a recurrent neural network-based system may be trained\nto perform end-to-end learning of motion policies suited for this \"active\nrecognition\" setting. Further, we hypothesize that active vision requires an\nagent to have the capacity to reason about the effects of its motions on its\nview of the world. To verify this hypothesis, we attempt to induce this\ncapacity in our active recognition pipeline, by simultaneously learning to\nforecast the effects of the agent's motions on its internal representation of\nthe environment conditional on all past views. Results across two challenging\ndatasets confirm both that our end-to-end system successfully learns meaningful\npolicies for active category recognition, and that \"learning to look ahead\"\nfurther boosts recognition performance. \n\n"}
{"id": "1605.04770", "contents": "Title: Automatic Image Annotation via Label Transfer in the Semantic Space Abstract: Automatic image annotation is among the fundamental problems in computer\nvision and pattern recognition, and it is becoming increasingly important in\norder to develop algorithms that are able to search and browse large-scale\nimage collections. In this paper, we propose a label propagation framework\nbased on Kernel Canonical Correlation Analysis (KCCA), which builds a latent\nsemantic space where correlation of visual and textual features are well\npreserved into a semantic embedding. The proposed approach is robust and can\nwork either when the training set is well annotated by experts, as well as when\nit is noisy such as in the case of user-generated tags in social media. We\nreport extensive results on four popular datasets. Our results show that our\nKCCA-based framework can be applied to several state-of-the-art label transfer\nmethods to obtain significant improvements. Our approach works even with the\nnoisy tags of social users, provided that appropriate denoising is performed.\nExperiments on a large scale setting show that our method can provide some\nbenefits even when the semantic space is estimated on a subset of training\nimages. \n\n"}
{"id": "1605.04988", "contents": "Title: Going Deeper into Action Recognition: A Survey Abstract: Understanding human actions in visual data is tied to advances in\ncomplementary research areas including object recognition, human dynamics,\ndomain adaptation and semantic segmentation. Over the last decade, human action\nanalysis evolved from earlier schemes that are often limited to controlled\nenvironments to nowadays advanced solutions that can learn from millions of\nvideos and apply to almost all daily activities. Given the broad range of\napplications from video surveillance to human-computer interaction, scientific\nmilestones in action recognition are achieved more rapidly, eventually leading\nto the demise of what used to be good in a short time. This motivated us to\nprovide a comprehensive review of the notable steps taken towards recognizing\nhuman actions. To this end, we start our discussion with the pioneering methods\nthat use handcrafted representations, and then, navigate into the realm of deep\nlearning based approaches. We aim to remain objective throughout this survey,\ntouching upon encouraging improvements as well as inevitable fallbacks, in the\nhope of raising fresh questions and motivating new research directions for the\nreader. \n\n"}
{"id": "1605.06049", "contents": "Title: A Multi-Batch L-BFGS Method for Machine Learning Abstract: The question of how to parallelize the stochastic gradient descent (SGD)\nmethod has received much attention in the literature. In this paper, we focus\ninstead on batch methods that use a sizeable fraction of the training set at\neach iteration to facilitate parallelism, and that employ second-order\ninformation. In order to improve the learning process, we follow a multi-batch\napproach in which the batch changes at each iteration. This can cause\ndifficulties because L-BFGS employs gradient differences to update the Hessian\napproximations, and when these gradients are computed using different data\npoints the process can be unstable. This paper shows how to perform stable\nquasi-Newton updating in the multi-batch setting, illustrates the behavior of\nthe algorithm in a distributed computing platform, and studies its convergence\nproperties for both the convex and nonconvex cases. \n\n"}
{"id": "1605.06294", "contents": "Title: Regularity of Minimizers of Shape Optimization Problems involving\n  Perimeter Abstract: We prove existence and regularity of optimal shapes for the\nproblem$$\\min\\Big\\{P(\\Omega)+\\mathcal{G}(\\Omega):\\ \\Omega\\subset D,\\\n|\\Omega|=m\\Big\\},$$where $P$ denotes the perimeter, $|\\cdot|$ is the volume,\nand the functional $\\mathcal{G}$ is either one of the\nfollowing:\\textless{}ul\\textgreater{}\\textless{}li\\textgreater{} the Dirichlet\nenergy $E\\_f$, with respect to a (possibly sign-changing) function $f\\in\nL^p$;\\textless{}/li\\textgreater{}\\textless{}li\\textgreater{}a spectral\nfunctional of the form $F(\\lambda\\_{1},\\dots,\\lambda\\_{k})$, where $\\lambda\\_k$\nis the $k$th eigenvalue of the Dirichlet Laplacian and\n$F:\\mathbb{R}^k\\to\\mathbb{R}$ is Lipschitz continuous and increasing in each\nvariable.\\textless{}/li\\textgreater{}\\textless{}/ul\\textgreater{}The domain $D$\nis the whole space $\\mathbb{R}^d$ or a bounded domain. We also give general\nassumptions on the functional $\\mathcal{G}$ so that the result remains valid. \n\n"}
{"id": "1605.06457", "contents": "Title: Virtual Worlds as Proxy for Multi-Object Tracking Analysis Abstract: Modern computer vision algorithms typically require expensive data\nacquisition and accurate manual labeling. In this work, we instead leverage the\nrecent progress in computer graphics to generate fully labeled, dynamic, and\nphoto-realistic proxy virtual worlds. We propose an efficient real-to-virtual\nworld cloning method, and validate our approach by building and publicly\nreleasing a new video dataset, called Virtual KITTI (see\nhttp://www.xrce.xerox.com/Research-Development/Computer-Vision/Proxy-Virtual-Worlds),\nautomatically labeled with accurate ground truth for object detection,\ntracking, scene and instance segmentation, depth, and optical flow. We provide\nquantitative experimental evidence suggesting that (i) modern deep learning\nalgorithms pre-trained on real data behave similarly in real and virtual\nworlds, and (ii) pre-training on virtual data improves performance. As the gap\nbetween real and virtual worlds is small, virtual worlds enable measuring the\nimpact of various weather and imaging conditions on recognition performance,\nall other things being equal. We show these factors may affect drastically\notherwise high-performing deep models for tracking. \n\n"}
{"id": "1605.07686", "contents": "Title: Local Perturb-and-MAP for Structured Prediction Abstract: Conditional random fields (CRFs) provide a powerful tool for structured\nprediction, but cast significant challenges in both the learning and inference\nsteps. Approximation techniques are widely used in both steps, which should be\nconsidered jointly to guarantee good performance (a.k.a. \"inferning\").\nPerturb-and-MAP models provide a promising alternative to CRFs, but require\nglobal combinatorial optimization and hence they are usable only on specific\nmodels. In this work, we present a new Local Perturb-and-MAP (locPMAP)\nframework that replaces the global optimization with a local optimization by\nexploiting our observed connection between locPMAP and the pseudolikelihood of\nthe original CRF model. We test our approach on three different vision tasks\nand show that our method achieves consistently improved performance over other\napproximate inference techniques optimized to a pseudolikelihood objective.\nAdditionally, we demonstrate that we can integrate our method in the fully\nconvolutional network framework to increase our model's complexity. Finally,\nour observed connection between locPMAP and the pseudolikelihood leads to a\nnovel perspective for understanding and using pseudolikelihood. \n\n"}
{"id": "1606.00625", "contents": "Title: Storytelling of Photo Stream with Bidirectional Multi-thread Recurrent\n  Neural Network Abstract: Visual storytelling aims to generate human-level narrative language (i.e., a\nnatural paragraph with multiple sentences) from a photo streams. A typical\nphoto story consists of a global timeline with multi-thread local storylines,\nwhere each storyline occurs in one different scene. Such complex structure\nleads to large content gaps at scene transitions between consecutive photos.\nMost existing image/video captioning methods can only achieve limited\nperformance, because the units in traditional recurrent neural networks (RNN)\ntend to \"forget\" the previous state when the visual sequence is inconsistent.\nIn this paper, we propose a novel visual storytelling approach with\nBidirectional Multi-thread Recurrent Neural Network (BMRNN). First, based on\nthe mined local storylines, a skip gated recurrent unit (sGRU) with delay\ncontrol is proposed to maintain longer range visual information. Second, by\nusing sGRU as basic units, the BMRNN is trained to align the local storylines\ninto the global sequential timeline. Third, a new training scheme with a\nstoryline-constrained objective function is proposed by jointly considering\nboth global and local matches. Experiments on three standard storytelling\ndatasets show that the BMRNN model outperforms the state-of-the-art methods. \n\n"}
{"id": "1606.00930", "contents": "Title: Comparison of 14 different families of classification algorithms on 115\n  binary datasets Abstract: We tested 14 very different classification algorithms (random forest,\ngradient boosting machines, SVM - linear, polynomial, and RBF - 1-hidden-layer\nneural nets, extreme learning machines, k-nearest neighbors and a bagging of\nknn, naive Bayes, learning vector quantization, elastic net logistic\nregression, sparse linear discriminant analysis, and a boosting of linear\nclassifiers) on 115 real life binary datasets. We followed the Demsar analysis\nand found that the three best classifiers (random forest, gbm and RBF SVM) are\nnot significantly different from each other. We also discuss that a change of\nless then 0.0112 in the error rate should be considered as an irrelevant\nchange, and used a Bayesian ANOVA analysis to conclude that with high\nprobability the differences between these three classifiers is not of practical\nconsequence. We also verified the execution time of \"standard implementations\"\nof these algorithms and concluded that RBF SVM is the fastest (significantly\nso) both in training time and in training plus testing time. \n\n"}
{"id": "1606.00963", "contents": "Title: Optimal quantization for a probability measure on a nonuniform stretched\n  Sierpi\\'{n}ski triangle Abstract: Quantization for a Borel probability measure refers to the idea of estimating\na given probability by a discrete probability with support containing a finite\nnumber of elements. In this paper, we have considered a Borel probability\nmeasure $P$ on $\\mathbb R^2$, which has support a nonuniform stretched\nSierpi\\'{n}ski triangle generated by a set of three contractive similarity\nmappings on $\\mathbb R^2$. For this probability measure, we investigate the\noptimal sets of $n$-means and the $n$th quantization errors for all positive\nintegers $n$. \n\n"}
{"id": "1606.02382", "contents": "Title: Deep Learning Convolutional Networks for Multiphoton Microscopy\n  Vasculature Segmentation Abstract: Recently there has been an increasing trend to use deep learning frameworks\nfor both 2D consumer images and for 3D medical images. However, there has been\nlittle effort to use deep frameworks for volumetric vascular segmentation. We\nwanted to address this by providing a freely available dataset of 12 annotated\ntwo-photon vasculature microscopy stacks. We demonstrated the use of deep\nlearning framework consisting both 2D and 3D convolutional filters (ConvNet).\nOur hybrid 2D-3D architecture produced promising segmentation result. We\nderived the architectures from Lee et al. who used the ZNN framework initially\ndesigned for electron microscope image segmentation. We hope that by sharing\nour volumetric vasculature datasets, we will inspire other researchers to\nexperiment with vasculature dataset and improve the used network architectures. \n\n"}
{"id": "1606.03073", "contents": "Title: Convolutional Sketch Inversion Abstract: In this paper, we use deep neural networks for inverting face sketches to\nsynthesize photorealistic face images. We first construct a semi-simulated\ndataset containing a very large number of computer-generated face sketches with\ndifferent styles and corresponding face images by expanding existing\nunconstrained face data sets. We then train models achieving state-of-the-art\nresults on both computer-generated sketches and hand-drawn sketches by\nleveraging recent advances in deep learning such as batch normalization, deep\nresidual learning, perceptual losses and stochastic optimization in combination\nwith our new dataset. We finally demonstrate potential applications of our\nmodels in fine arts and forensic arts. In contrast to existing patch-based\napproaches, our deep-neural-network-based approach can be used for synthesizing\nphotorealistic face images by inverting face sketches in the wild. \n\n"}
{"id": "1606.03878", "contents": "Title: Device-independent dimension tests in the prepare-and-measure scenario Abstract: Analyzing the dimension of an unknown quantum system in a device-independent\nmanner, i.e., using only the measurement statistics, is a fundamental task in\nquantum physics and quantum information theory. In this paper, we consider this\nproblem in the prepare-and-measure scenario. Specifically, we provide a lower\nbound on the dimension of the prepared quantum systems which is a function that\nonly depends on the measurement statistics. Furthermore, we show that our bound\nperforms well on several examples. {In particular}, we show that our bound\nprovides new insights into the notion of dimension witness, and we also use it\nto show that the sets of restricted-dimensional prepare-and-measure\ncorrelations are not always convex. \n\n"}
{"id": "1606.04062", "contents": "Title: Causal transport in discrete time and applications Abstract: Loosely speaking, causal transport plans are a relaxation of adapted\nprocesses in the same sense as Kantorovich transport plans extend Monge-type\ntransport maps. The corresponding causal version of the transport problem has\nrecently been introduced by Lassalle. Working in a discrete time setup, we\nestablish a dynamic programming principle that links the causal transport\nproblem to the transport problem for general costs recently considered by\nGozlan et al. Based on this recursive principle, we give conditions under which\nthe celebrated Knothe-Rosenblatt rearrangement can be viewed as a causal\nanalogue to the Brenier's map. Moreover, these considerations provide\ntransport-information inequalities for the nested distance between stochastic\nprocesses pioneered by Pflug and Pichler, and so serve to gauge the discrepancy\nbetween stochastic programs driven by different noise distributions. \n\n"}
{"id": "1606.04232", "contents": "Title: DCNNs on a Diet: Sampling Strategies for Reducing the Training Set Size Abstract: Large-scale supervised classification algorithms, especially those based on\ndeep convolutional neural networks (DCNNs), require vast amounts of training\ndata to achieve state-of-the-art performance. Decreasing this data requirement\nwould significantly speed up the training process and possibly improve\ngeneralization. Motivated by this objective, we consider the task of adaptively\nfinding concise training subsets which will be iteratively presented to the\nlearner. We use convex optimization methods, based on an objective criterion\nand feedback from the current performance of the classifier, to efficiently\nidentify informative samples to train on. We propose an algorithm to decompose\nthe optimization problem into smaller per-class problems, which can be solved\nin parallel. We test our approach on standard classification tasks and\ndemonstrate its effectiveness in decreasing the training set size without\ncompromising performance. We also show that our approach can make the\nclassifier more robust in the presence of label noise and class imbalance. \n\n"}
{"id": "1606.04760", "contents": "Title: Adapting to unknown noise level in sparse deconvolution Abstract: In this paper, we study sparse spike deconvolution over the space of\ncomplex-valued measures when the input measure is a finite sum of Dirac masses.\nWe introduce a modified version of the Beurling Lasso (BLasso), a semi-definite\nprogram that we refer to as the Concomitant Beurling Lasso (CBLasso). This new\nprocedure estimates the target measure and the unknown noise level\nsimultaneously. Contrary to previous estimators in the literature, theory holds\nfor a tuning parameter that depends only on the sample size, so that it can be\nused for unknown noise level problems. Consistent noise level estimation is\nstandardly proved. As for Radon measure estimation, theoretical guarantees\nmatch the previous state-of-the-art results in Super-Resolution regarding\nminimax prediction and localization. The proofs are based on a bound on the\nnoise level given by a new tail estimate of the supremum of a stationary\nnon-Gaussian process through the Rice method. \n\n"}
{"id": "1606.04992", "contents": "Title: A Hierarchical Pose-Based Approach to Complex Action Understanding Using\n  Dictionaries of Actionlets and Motion Poselets Abstract: In this paper, we introduce a new hierarchical model for human action\nrecognition using body joint locations. Our model can categorize complex\nactions in videos, and perform spatio-temporal annotations of the atomic\nactions that compose the complex action being performed.That is, for each\natomic action, the model generates temporal action annotations by estimating\nits starting and ending times, as well as, spatial annotations by inferring the\nhuman body parts that are involved in executing the action. our model includes\nthree key novel properties: (i) it can be trained with no spatial supervision,\nas it can automatically discover active body parts from temporal action\nannotations only; (ii) it jointly learns flexible representations for motion\nposelets and actionlets that encode the visual variability of body parts and\natomic actions; (iii) a mechanism to discard idle or non-informative body parts\nwhich increases its robustness to common pose estimation errors. We evaluate\nthe performance of our method using multiple action recognition benchmarks. Our\nmodel consistently outperforms baselines and state-of-the-art action\nrecognition methods. \n\n"}
{"id": "1606.08101", "contents": "Title: Real-time Decentralized and Robust Voltage Control in Distribution\n  Networks Abstract: Voltage control plays an important role in the operation of electricity\ndistribution networks, especially when there is a large penetration of\nrenewable energy resources. In this paper, we focus on voltage control through\nreactive power compensation and study how different information structures\naffect the control performance. In particular, we first show that using only\nvoltage measurements to determine reactive power compensation is insufficient\nto maintain voltage in the acceptable range. Then we propose two fully\ndecentralized and robust algorithms by adding additional information, which can\nstabilize the voltage in the acceptable range. The one with higher complexity\ncan further minimize a cost of reactive power compensation in a particular\nform. Both of the two algorithms use only local measurements and local\nvariables and require no communication. In addition, the two algorithms are\nrobust against heterogeneous update rates and delays. \n\n"}
{"id": "1607.00548", "contents": "Title: Active Object Localization in Visual Situations Abstract: We describe a method for performing active localization of objects in\ninstances of visual situations. A visual situation is an abstract\nconcept---e.g., \"a boxing match\", \"a birthday party\", \"walking the dog\",\n\"waiting for a bus\"---whose image instantiations are linked more by their\ncommon spatial and semantic structure than by low-level visual similarity. Our\nsystem combines given and learned knowledge of the structure of a particular\nsituation, and adapts that knowledge to a new situation instance as it actively\nsearches for objects. More specifically, the system learns a set of probability\ndistributions describing spatial and other relationships among relevant\nobjects. The system uses those distributions to iteratively sample object\nproposals on a test image, but also continually uses information from those\nobject proposals to adaptively modify the distributions based on what the\nsystem has detected. We test our approach's ability to efficiently localize\nobjects, using a situation-specific image dataset created by our group. We\ncompare the results with several baselines and variations on our method, and\ndemonstrate the strong benefit of using situation knowledge and active\ncontext-driven localization. Finally, we contrast our method with several other\napproaches that use context as well as active search for object localization in\nimages. \n\n"}
{"id": "1607.02385", "contents": "Title: Finite Length Performance of Random Slotted ALOHA Strategies Abstract: Multiple connected devices sharing common wireless resources might create\ninterference if they access the channel simultaneously. Medium access control\n(MAC) protocols gener- ally regulate the access of the devices to the shared\nchannel to limit signal interference. In particular, irregular repetition\nslotted ALOHA (IRSA) techniques can achieve high-throughput performance when\ninterference cancellation methods are adopted to recover from collisions. In\nthis work, we study the finite length performance for IRSA schemes by building\non the analogy between successive interference cancellation and iterative\nbelief- propagation on erasure channels. We use a novel combinatorial\nderivation based on the matrix-occupancy theory to compute the error\nprobability and we validate our method with simulation results. \n\n"}
{"id": "1607.02748", "contents": "Title: Adversarial Training For Sketch Retrieval Abstract: Generative Adversarial Networks (GAN) are able to learn excellent\nrepresentations for unlabelled data which can be applied to image generation\nand scene classification. Representations learned by GANs have not yet been\napplied to retrieval. In this paper, we show that the representations learned\nby GANs can indeed be used for retrieval. We consider heritage documents that\ncontain unlabelled Merchant Marks, sketch-like symbols that are similar to\nhieroglyphs. We introduce a novel GAN architecture with design features that\nmake it suitable for sketch retrieval. The performance of this sketch-GAN is\ncompared to a modified version of the original GAN architecture with respect to\nsimple invariance properties. Experiments suggest that sketch-GANs learn\nrepresentations that are suitable for retrieval and which also have increased\nstability to rotation, scale and translation compared to the standard GAN\narchitecture. \n\n"}
{"id": "1607.03426", "contents": "Title: On modeling and global solutions for d.c. optimization problems by\n  canonical duality theory Abstract: This paper presents a canonical d.c. (difference of canonical and convex\nfunctions) programming problem, which can be used to model general global\noptimization problems in complex systems. It shows that by using the canonical\nduality theory, a large class of nonconvex minimization problems can be\nequivalently converted to a unified concave maximization problem over a convex\ndomain, which can be solved easily under certain conditions. Additionally, a\ndetailed proof for triality theory is provided, which can be used to identify\nlocal extremal solutions. Applications are illustrated and open problems are\npresented. \n\n"}
{"id": "1607.03566", "contents": "Title: Polyhedral approximation in mixed-integer convex optimization Abstract: Generalizing both mixed-integer linear optimization and convex optimization,\nmixed-integer convex optimization possesses broad modeling power but has seen\nrelatively few advances in general-purpose solvers in recent years. In this\npaper, we intend to provide a broadly accessible introduction to our recent\nwork in developing algorithms and software for this problem class. Our approach\nis based on constructing polyhedral outer approximations of the convex\nconstraints, resulting in a global solution by solving a finite number of\nmixed-integer linear and continuous convex subproblems. The key advance we\npresent is to strengthen the polyhedral approximations by constructing them in\na higher-dimensional space. In order to automate this extended formulation we\nrely on the algebraic modeling technique of disciplined convex programming\n(DCP), and for generality and ease of implementation we use conic\nrepresentations of the convex constraints. Although our framework requires a\nmanual translation of existing models into DCP form, after performing this\ntransformation on the MINLPLIB2 benchmark library we were able to solve a\nnumber of unsolved instances and on many other instances achieve superior\nperformance compared with state-of-the-art solvers like Bonmin, SCIP, and\nArtelys Knitro. \n\n"}
{"id": "1607.04433", "contents": "Title: End-to-End Learning for Image Burst Deblurring Abstract: We present a neural network model approach for multi-frame blind\ndeconvolution. The discriminative approach adopts and combines two recent\ntechniques for image deblurring into a single neural network architecture. Our\nproposed hybrid-architecture combines the explicit prediction of a\ndeconvolution filter and non-trivial averaging of Fourier coefficients in the\nfrequency domain. In order to make full use of the information contained in all\nimages in one burst, the proposed network embeds smaller networks, which\nexplicitly allow the model to transfer information between images in early\nlayers. Our system is trained end-to-end using standard backpropagation on a\nset of artificially generated training examples, enabling competitive\nperformance in multi-frame blind deconvolution, both with respect to quality\nand runtime. \n\n"}
{"id": "1607.05074", "contents": "Title: Deep Active Contours Abstract: We propose a method for interactive boundary extraction which combines a\ndeep, patch-based representation with an active contour framework. We train a\nclass-specific convolutional neural network which predicts a vector pointing\nfrom the respective point on the evolving contour towards the closest point on\nthe boundary of the object of interest. These predictions form a vector field\nwhich is then used for evolving the contour by the Sobolev active contour\nframework proposed by Sundaramoorthi et al. The resulting interactive\nsegmentation method is very efficient in terms of required computational\nresources and can even be trained on comparatively small graphics cards. We\nevaluate the potential of the proposed method on both medical and non-medical\nchallenge data sets, such as the STACOM data set and the PASCAL VOC 2012 data\nset. \n\n"}
{"id": "1607.05836", "contents": "Title: Improved Deep Learning of Object Category using Pose Information Abstract: Despite significant recent progress, the best available computer vision\nalgorithms still lag far behind human capabilities, even for recognizing\nindividual discrete objects under various poses, illuminations, and\nbackgrounds. Here we present a new approach to using object pose information to\nimprove deep network learning. While existing large-scale datasets, e.g.\nImageNet, do not have pose information, we leverage the newly published\nturntable dataset, iLab-20M, which has ~22M images of 704 object instances shot\nunder different lightings, camera viewpoints and turntable rotations, to do\nmore controlled object recognition experiments. We introduce a new\nconvolutional neural network architecture, what/where CNN (2W-CNN), built on a\nlinear-chain feedforward CNN (e.g., AlexNet), augmented by hierarchical layers\nregularized by object poses. Pose information is only used as feedback signal\nduring training, in addition to category information; during test, the\nfeedforward network only predicts category. To validate the approach, we train\nboth 2W-CNN and AlexNet using a fraction of the dataset, and 2W-CNN achieves 6%\nperformance improvement in category prediction. We show mathematically that\n2W-CNN has inherent advantages over AlexNet under the stochastic gradient\ndescent (SGD) optimization procedure. Further more, we fine-tune object\nrecognition on ImageNet by using the pretrained 2W-CNN and AlexNet features on\niLab-20M, results show that significant improvements have been achieved,\ncompared with training AlexNet from scratch. Moreover, fine-tuning 2W-CNN\nfeatures performs even better than fine-tuning the pretrained AlexNet features.\nThese results show pretrained features on iLab- 20M generalizes well to natural\nimage datasets, and 2WCNN learns even better features for object recognition\nthan AlexNet. \n\n"}
{"id": "1607.06514", "contents": "Title: Geometric Neural Phrase Pooling: Modeling the Spatial Co-occurrence of\n  Neurons Abstract: Deep Convolutional Neural Networks (CNNs) are playing important roles in\nstate-of-the-art visual recognition. This paper focuses on modeling the spatial\nco-occurrence of neuron responses, which is less studied in the previous work.\nFor this, we consider the neurons in the hidden layer as neural words, and\nconstruct a set of geometric neural phrases on top of them. The idea that\ngrouping neural words into neural phrases is borrowed from the\nBag-of-Visual-Words (BoVW) model. Next, the Geometric Neural Phrase Pooling\n(GNPP) algorithm is proposed to efficiently encode these neural phrases. GNPP\nacts as a new type of hidden layer, which punishes the isolated neuron\nresponses after convolution, and can be inserted into a CNN model with little\nextra computational overhead. Experimental results show that GNPP produces\nsignificant and consistent accuracy gain in image classification. \n\n"}
{"id": "1607.07429", "contents": "Title: Much Ado About Time: Exhaustive Annotation of Temporal Data Abstract: Large-scale annotated datasets allow AI systems to learn from and build upon\nthe knowledge of the crowd. Many crowdsourcing techniques have been developed\nfor collecting image annotations. These techniques often implicitly rely on the\nfact that a new input image takes a negligible amount of time to perceive. In\ncontrast, we investigate and determine the most cost-effective way of obtaining\nhigh-quality multi-label annotations for temporal data such as videos. Watching\neven a short 30-second video clip requires a significant time investment from a\ncrowd worker; thus, requesting multiple annotations following a single viewing\nis an important cost-saving strategy. But how many questions should we ask per\nvideo? We conclude that the optimal strategy is to ask as many questions as\npossible in a HIT (up to 52 binary questions after watching a 30-second video\nclip in our experiments). We demonstrate that while workers may not correctly\nanswer all questions, the cost-benefit analysis nevertheless favors consensus\nfrom multiple such cheap-yet-imperfect iterations over more complex\nalternatives. When compared with a one-question-per-video baseline, our method\nis able to achieve a 10% improvement in recall 76.7% ours versus 66.7%\nbaseline) at comparable precision (83.8% ours versus 83.0% baseline) in about\nhalf the annotation time (3.8 minutes ours compared to 7.1 minutes baseline).\nWe demonstrate the effectiveness of our method by collecting multi-label\nannotations of 157 human activities on 1,815 videos. \n\n"}
{"id": "1607.08366", "contents": "Title: 25 years of CNNs: Can we compare to human abstraction capabilities? Abstract: We try to determine the progress made by convolutional neural networks over\nthe past 25 years in classifying images into abstractc lasses. For this purpose\nwe compare the performance of LeNet to that of GoogLeNet at classifying\nrandomly generated images which are differentiated by an abstract property\n(e.g., one class contains two objects of the same size, the other class two\nobjects of different sizes). Our results show that there is still work to do in\norder to solve vision problems humans are able to solve without much\ndifficulty. \n\n"}
{"id": "1607.08659", "contents": "Title: General Automatic Human Shape and Motion Capture Using Volumetric\n  Contour Cues Abstract: Markerless motion capture algorithms require a 3D body with properly\npersonalized skeleton dimension and/or body shape and appearance to\nsuccessfully track a person. Unfortunately, many tracking methods consider\nmodel personalization a different problem and use manual or semi-automatic\nmodel initialization, which greatly reduces applicability. In this paper, we\npropose a fully automatic algorithm that jointly creates a rigged actor model\ncommonly used for animation - skeleton, volumetric shape, appearance, and\noptionally a body surface - and estimates the actor's motion from multi-view\nvideo input only. The approach is rigorously designed to work on footage of\ngeneral outdoor scenes recorded with very few cameras and without background\nsubtraction. Our method uses a new image formation model with analytic\nvisibility and analytically differentiable alignment energy. For\nreconstruction, 3D body shape is approximated as Gaussian density field. For\npose and shape estimation, we minimize a new edge-based alignment energy\ninspired by volume raycasting in an absorbing medium. We further propose a new\nstatistical human body model that represents the body surface, volumetric\nGaussian density, as well as variability in skeleton shape. Given any\nmulti-view sequence, our method jointly optimizes the pose and shape parameters\nof this model fully automatically in a spatiotemporal way. \n\n"}
{"id": "1608.00911", "contents": "Title: Modeling Spatial and Temporal Cues for Multi-label Facial Action Unit\n  Detection Abstract: Facial action units (AUs) are essential to decode human facial expressions.\nResearchers have focused on training AU detectors with a variety of features\nand classifiers. However, several issues remain. These are spatial\nrepresentation, temporal modeling, and AU correlation. Unlike most studies that\ntackle these issues separately, we propose a hybrid network architecture to\njointly address them. Specifically, spatial representations are extracted by a\nConvolutional Neural Network (CNN), which, as analyzed in this paper, is able\nto reduce person-specific biases caused by hand-crafted features (eg, SIFT and\nGabor). To model temporal dependencies, Long Short-Term Memory (LSTMs) are\nstacked on top of these representations, regardless of the lengths of input\nvideos. The outputs of CNNs and LSTMs are further aggregated into a fusion\nnetwork to produce per-frame predictions of 12 AUs. Our network naturally\naddresses the three issues, and leads to superior performance compared to\nexisting methods that consider these issues independently. Extensive\nexperiments were conducted on two large spontaneous datasets, GFT and BP4D,\ncontaining more than 400,000 frames coded with 12 AUs. On both datasets, we\nreport significant improvement over a standard multi-label CNN and\nfeature-based state-of-the-art. Finally, we provide visualization of the\nlearned AU models, which, to our best knowledge, reveal how machines see facial\nAUs for the first time. \n\n"}
{"id": "1608.01578", "contents": "Title: Basis for the linear space of matrices under equivalence Abstract: The semi-tensor product (STP) of matrices which was proposed by Daizhan Cheng\nin 2001 [2], is a natural generalization of the standard matrix product and\nwell defined at every two finite-dimensional matrices. In 2016, Cheng proposed\na new concept of semi-tensor addition (STA) which is a natural generalization\nof the standard matrix addition and well defined at every two\nfinite-dimensional matrices with the same ratio between the numbers of rows and\ncolumns [1]. In addition, an identify equivalence relation between matrices was\ndefined in [1], STP and STA were proved valid for the corresponding identify\nequivalence classes, and the corresponding quotient space was endowed with an\nalgebraic structure and a manifold structure. In this follow-up paper, we give\na new concise basis for the quotient space, which also shows that the Lie\nalgebra corresponding to the quotient space is of countably infinite dimension. \n\n"}
{"id": "1608.02165", "contents": "Title: ShapeFit and ShapeKick for Robust, Scalable Structure from Motion Abstract: We introduce a new method for location recovery from pair-wise directions\nthat leverages an efficient convex program that comes with exact recovery\nguarantees, even in the presence of adversarial outliers. When pairwise\ndirections represent scaled relative positions between pairs of views\n(estimated for instance with epipolar geometry) our method can be used for\nlocation recovery, that is the determination of relative pose up to a single\nunknown scale. For this task, our method yields performance comparable to the\nstate-of-the-art with an order of magnitude speed-up. Our proposed numerical\nframework is flexible in that it accommodates other approaches to location\nrecovery and can be used to speed up other methods. These properties are\ndemonstrated by extensively testing against state-of-the-art methods for\nlocation recovery on 13 large, irregular collections of images of real scenes\nin addition to simulated data with ground truth. \n\n"}
{"id": "1608.03710", "contents": "Title: Joint 3D Positioning and Network Synchronization in 5G Ultra-Dense\n  Networks Using UKF and EKF Abstract: It is commonly expected that future fifth generation (5G) networks will be\ndeployed with a high spatial density of access nodes (ANs) in order to meet the\nenvisioned capacity requirements of the upcoming wireless networks.\nDensification is beneficial not only for communications but it also creates a\nconvenient infrastructure for highly accurate user node (UN) positioning.\nDespite the fact that positioning will play an important role in future\nnetworks, thus enabling a huge amount of location-based applications and\nservices, this great opportunity has not been widely explored in the existing\nliterature. Therefore, this paper proposes an unscented Kalman filter\n(UKF)-based method for estimating directions of arrival (DoAs) and times of\narrival (ToA) at ANs as well as performing joint 3D positioning and network\nsynchronization in a network-centric manner. In addition to the proposed\nUKF-based solution, the existing 2D extended Kalman filter (EKF)-based solution\nis extended to cover also realistic 3D positioning scenarios. Building on the\npremises of 5G ultra-dense networks (UDNs), the performance of both methods is\nevaluated and analysed in terms of DoA and ToA estimation as well as\npositioning and clock offset estimation accuracy, using the METIS map-based\nray-tracing channel model and 3D trajectories for vehicles and unmanned aerial\nvehicles (UAVs) through the Madrid grid. Based on the comprehensive numerical\nevaluations, both proposed methods can provide the envisioned one meter 3D\npositioning accuracy even in the case of unsynchronized 5G network while\nsimultaneously tracking the clock offsets of network elements with a\nnanosecond-scale accuracy. \n\n"}
{"id": "1608.03819", "contents": "Title: DeepDiary: Automatic Caption Generation for Lifelogging Image Streams Abstract: Lifelogging cameras capture everyday life from a first-person perspective,\nbut generate so much data that it is hard for users to browse and organize\ntheir image collections effectively. In this paper, we propose to use automatic\nimage captioning algorithms to generate textual representations of these\ncollections. We develop and explore novel techniques based on deep learning to\ngenerate captions for both individual images and image streams, using temporal\nconsistency constraints to create summaries that are both more compact and less\nnoisy. We evaluate our techniques with quantitative and qualitative results,\nand apply captioning to an image retrieval application for finding potentially\nprivate images. Our results suggest that our automatic captioning algorithms,\nwhile imperfect, may work well enough to help users manage lifelogging photo\ncollections. \n\n"}
{"id": "1608.03932", "contents": "Title: Human Pose Estimation from Depth Images via Inference Embedded\n  Multi-task Learning Abstract: Human pose estimation (i.e., locating the body parts / joints of a person) is\na fundamental problem in human-computer interaction and multimedia\napplications. Significant progress has been made based on the development of\ndepth sensors, i.e., accessible human pose prediction from still depth images\n[32]. However, most of the existing approaches to this problem involve several\ncomponents/models that are independently designed and optimized, leading to\nsuboptimal performances. In this paper, we propose a novel inference-embedded\nmulti-task learning framework for predicting human pose from still depth\nimages, which is implemented with a deep architecture of neural networks.\nSpecifically, we handle two cascaded tasks: i) generating the heat (confidence)\nmaps of body parts via a fully convolutional network (FCN); ii) seeking the\noptimal configuration of body parts based on the detected body part proposals\nvia an inference built-in MatchNet [10], which measures the appearance and\ngeometric kinematic compatibility of body parts and embodies the dynamic\nprogramming inference as an extra network layer. These two tasks are jointly\noptimized. Our extensive experiments show that the proposed deep model\nsignificantly improves the accuracy of human pose estimation over other several\nstate-of-the-art methods or SDKs. We also release a large-scale dataset for\ncomparison, which includes 100K depth images under challenging scenarios. \n\n"}
{"id": "1608.04137", "contents": "Title: A second order dynamical system with Hessian-driven damping and penalty\n  term associated to variational inequalities Abstract: We consider the minimization of a convex objective function subject to the\nset of minima of another convex function, under the assumption that both\nfunctions are twice continuously differentiable. We approach this optimization\nproblem from a continuous perspective by means of a second order dynamical\nsystem with Hessian-driven damping and a penalty term corresponding to the\nconstrained function. By constructing appropriate energy functionals, we prove\nweak convergence of the trajectories generated by this differential equation to\na minimizer of the optimization problem as well as convergence for the\nobjective function values along the trajectories. The performed investigations\nrely on Lyapunov analysis in combination with the continuous version of the\nOpial Lemma. In case the objective function is strongly convex, we can even\nshow strong convergence of the trajectories. \n\n"}
{"id": "1608.04170", "contents": "Title: Every Filter Extracts A Specific Texture In Convolutional Neural\n  Networks Abstract: Many works have concentrated on visualizing and understanding the inner\nmechanism of convolutional neural networks (CNNs) by generating images that\nactivate some specific neurons, which is called deep visualization. However, it\nis still unclear what the filters extract from images intuitively. In this\npaper, we propose a modified code inversion algorithm, called feature map\ninversion, to understand the function of filter of interest in CNNs. We reveal\nthat every filter extracts a specific texture. The texture from higher layer\ncontains more colours and more intricate structures. We also demonstrate that\nstyle of images could be a combination of these texture primitives. Two methods\nare proposed to reallocate energy distribution of feature maps randomly and\npurposefully. Then, we inverse the modified code and generate images of diverse\nstyles. With these results, we provide an explanation about why Gram matrix of\nfeature maps \\cite{Gatys_2016_CVPR} could represent image style. \n\n"}
{"id": "1608.04348", "contents": "Title: Anomaly detection and classification for streaming data using PDEs Abstract: Nondominated sorting, also called Pareto Depth Analysis (PDA), is widely used\nin multi-objective optimization and has recently found important applications\nin multi-criteria anomaly detection. Recently, a partial differential equation\n(PDE) continuum limit was discovered for nondominated sorting leading to a very\nfast approximate sorting algorithm called PDE-based ranking. We propose in this\npaper a fast real-time streaming version of the PDA algorithm for anomaly\ndetection that exploits the computational advantages of PDE continuum limits.\nFurthermore, we derive new PDE continuum limits for sorting points within their\nnondominated layers and show how the new PDEs can be used to classify anomalies\nbased on which criterion was more significantly violated. We also prove\nstatistical convergence rates for PDE-based ranking, and present the results of\nnumerical experiments with both synthetic and real data. \n\n"}
{"id": "1608.04430", "contents": "Title: Sparsity Constrained Minimization via Mathematical Programming with\n  Equilibrium Constraints Abstract: Sparsity constrained minimization captures a wide spectrum of applications in\nboth machine learning and signal processing. This class of problems is\ndifficult to solve since it is NP-hard and existing solutions are primarily\nbased on Iterative Hard Thresholding (IHT). In this paper, we consider a class\nof continuous optimization techniques based on Mathematical Programs with\nEquilibrium Constraints (MPECs) to solve general sparsity constrained problems.\nSpecifically, we reformulate the problem as an equivalent biconvex MPEC, which\nwe can solve using an exact penalty method or an alternating direction method.\nWe elaborate on the merits of both proposed methods and analyze their\nconvergence properties. Finally, we demonstrate the effectiveness and\nversatility of our methods on several important problems, including feature\nselection, segmented regression, MRF optimization, trend filtering and impulse\nnoise removal. Extensive experiments show that our MPEC-based methods\noutperform state-of-the-art techniques, especially those based on IHT. \n\n"}
{"id": "1608.04460", "contents": "Title: Microcanonical thermodynamics in general physical theories Abstract: Microcanonical thermodynamics studies the operations that can be performed on\nsystems with well-defined energy. So far, this approach has been applied to\nclassical and quantum systems. Here we extend it to arbitrary physical\ntheories, proposing two requirements for the development of a general\nmicrocanonical framework. We then formulate three resource theories,\ncorresponding to three different sets of basic operations: i) random reversible\noperations, resulting from reversible dynamics with fluctuating parameters, ii)\nnoisy operations, generated by the interaction with ancillas in the\nmicrocanonical state, and iii) unital operations, defined as the operations\nthat preserve the microcanonical state. We focus our attention on a class of\nphysical theories, called sharp theories with purification, where these three\nsets of operations exhibit remarkable properties. Firstly, each set is\ncontained into the next. Secondly, the convertibility of states by unital\noperations is completely characterised by a majorisation criterion. Thirdly,\nthe three sets are equivalent in terms of state convertibility if and only if\nthe dynamics allowed by theory satisfy a suitable condition, which we call\nunrestricted reversibility. Under this condition, we derive a duality between\nthe resource theory of microcanonical thermodynamics and the resource theory of\npure bipartite entanglement. \n\n"}
{"id": "1608.05159", "contents": "Title: Multi-stage Object Detection with Group Recursive Learning Abstract: Most of existing detection pipelines treat object proposals independently and\npredict bounding box locations and classification scores over them separately.\nHowever, the important semantic and spatial layout correlations among proposals\nare often ignored, which are actually useful for more accurate object\ndetection. In this work, we propose a new EM-like group recursive learning\napproach to iteratively refine object proposals by incorporating such context\nof surrounding proposals and provide an optimal spatial configuration of object\ndetections. In addition, we propose to incorporate the weakly-supervised object\nsegmentation cues and region-based object detection into a multi-stage\narchitecture in order to fully exploit the learned segmentation features for\nbetter object detection in an end-to-end way. The proposed architecture\nconsists of three cascaded networks which respectively learn to perform\nweakly-supervised object segmentation, object proposal generation and recursive\ndetection refinement. Combining the group recursive learning and the\nmulti-stage architecture provides competitive mAPs of 78.6% and 74.9% on the\nPASCAL VOC2007 and VOC2012 datasets respectively, which outperforms many\nwell-established baselines [10] [20] significantly. \n\n"}
{"id": "1608.05269", "contents": "Title: Two-Timescale Stochastic Dispatch of Smart Distribution Grids Abstract: Smart distribution grids should efficiently integrate stochastic renewable\nresources while effecting voltage regulation. The design of energy management\nschemes is challenging, one of the reasons being that energy management is a\nmultistage problem where decisions are not all made at the same timescale and\nmust account for the variability during real-time operation. The joint dispatch\nof slow- and fast-timescale controls in a smart distribution grid is considered\nhere. The substation voltage, the energy exchanged with a main grid, and the\ngeneration schedules for small diesel generators have to be decided on a slow\ntimescale; whereas optimal photovoltaic inverter setpoints are found on a more\nfrequent basis. While inverter and looser voltage regulation limits are imposed\nat all times, tighter bus voltage constraints are enforced on the average or in\nprobability, thus enabling more efficient renewable integration. Upon\nreformulating the two-stage grid dispatch as a stochastic convex-concave\nproblem, two distribution-free schemes are put forth. An average dispatch\nalgorithm converges provably to the optimal two-stage decisions via a sequence\nof convex quadratic programs. Its non-convex probabilistic alternative entails\nsolving two slightly different convex problems and is numerically shown to\nconverge. Numerical tests on a real-world distribution feeder verify that both\nnovel data-driven schemes yield lower costs over competing alternatives. \n\n"}
{"id": "1608.06037", "contents": "Title: Lets keep it simple, Using simple architectures to outperform deeper and\n  more complex architectures Abstract: Major winning Convolutional Neural Networks (CNNs), such as AlexNet, VGGNet,\nResNet, GoogleNet, include tens to hundreds of millions of parameters, which\nimpose considerable computation and memory overhead. This limits their\npractical use for training, optimization and memory efficiency. On the\ncontrary, light-weight architectures, being proposed to address this issue,\nmainly suffer from low accuracy. These inefficiencies mostly stem from\nfollowing an ad hoc procedure. We propose a simple architecture, called\nSimpleNet, based on a set of designing principles, with which we empirically\nshow, a well-crafted yet simple and reasonably deep architecture can perform on\npar with deeper and more complex architectures. SimpleNet provides a good\ntradeoff between the computation/memory efficiency and the accuracy. Our simple\n13-layer architecture outperforms most of the deeper and complex architectures\nto date such as VGGNet, ResNet, and GoogleNet on several well-known benchmarks\nwhile having 2 to 25 times fewer number of parameters and operations. This\nmakes it very handy for embedded systems or systems with computational and\nmemory limitations. We achieved state-of-the-art result on CIFAR10\noutperforming several heavier architectures, near state of the art on MNIST and\ncompetitive results on CIFAR100 and SVHN. We also outperformed the much larger\nand deeper architectures such as VGGNet and popular variants of ResNets among\nothers on the ImageNet dataset. Models are made available at:\nhttps://github.com/Coderx7/SimpleNet \n\n"}
{"id": "1609.02452", "contents": "Title: End-to-End Eye Movement Detection Using Convolutional Neural Networks Abstract: Common computational methods for automated eye movement detection - i.e. the\ntask of detecting different types of eye movement in a continuous stream of\ngaze data - are limited in that they either involve thresholding on\nhand-crafted signal features, require individual detectors each only detecting\na single movement, or require pre-segmented data. We propose a novel approach\nfor eye movement detection that only involves learning a single detector\nend-to-end, i.e. directly from the continuous gaze data stream and\nsimultaneously for different eye movements without any manual feature crafting\nor segmentation. Our method is based on convolutional neural networks (CNN)\nthat recently demonstrated superior performance in a variety of tasks in\ncomputer vision, signal processing, and machine learning. We further introduce\na novel multi-participant dataset that contains scripted and free-viewing\nsequences of ground-truth annotated saccades, fixations, and smooth pursuits.\nWe show that our CNN-based method outperforms state-of-the-art baselines by a\nlarge margin on this challenging dataset, thereby underlining the significant\npotential of this approach for holistic, robust, and accurate eye movement\nprotocol analysis. \n\n"}
{"id": "1609.03285", "contents": "Title: Disciplined Multi-Convex Programming Abstract: A multi-convex optimization problem is one in which the variables can be\npartitioned into sets over which the problem is convex when the other variables\nare fixed. Multi-convex problems are generally solved approximately using\nvariations on alternating or cyclic minimization. Multi-convex problems arise\nin many applications, such as nonnegative matrix factorization, generalized low\nrank models, and structured control synthesis, to name just a few. In most\napplications to date the multi-convexity is simple to verify by hand. In this\npaper we study the automatic detection and verification of multi-convexity\nusing the ideas of disciplined convex programming. We describe an\nimplementation of our proposed method that detects and verifies\nmulti-convexity, and then invokes one of the general solution methods. \n\n"}
{"id": "1609.03600", "contents": "Title: Attack-resilient Estimation of Switched Nonlinear Stochastic\n  Cyber-Physical Systems Abstract: This paper studies attack-resilient estimation of a class of switched\nnonlinear systems subject to stochastic noises. The systems are threatened by\nboth of signal attacks and switching attacks. The problem is formulated as the\njoint estimation of states, attack vectors and modes of hidden-mode switched\nsystems. We propose an estimation algorithm which is composed of a bank of\nstate and attack vector estimators and a mode estimator. The mode estimator\nselects the most likely mode based on modes' posterior probabilities induced by\nthe discrepancies between obtained outputs and predicted outputs. We formally\nanalyze the stability of estimation errors in probability for the proposed\nestimator associated with the true mode when the hidden mode is time-invariant\nbut remains unknown. For hidden-mode switched linear systems, we discuss a way\nto reduce computational complexity which originates from unknown signal attack\nlocations. Lastly, we present numerical simulations on the IEEE 68-bus test\nsystem to show the estimator performance for time-varying modes with a regular\nmode set and a reduced mode set. \n\n"}
{"id": "1609.03677", "contents": "Title: Unsupervised Monocular Depth Estimation with Left-Right Consistency Abstract: Learning based methods have shown very promising results for the task of\ndepth estimation in single images. However, most existing approaches treat\ndepth prediction as a supervised regression problem and as a result, require\nvast quantities of corresponding ground truth depth data for training. Just\nrecording quality depth data in a range of environments is a challenging\nproblem. In this paper, we innovate beyond existing approaches, replacing the\nuse of explicit depth data during training with easier-to-obtain binocular\nstereo footage.\n  We propose a novel training objective that enables our convolutional neural\nnetwork to learn to perform single image depth estimation, despite the absence\nof ground truth depth data. Exploiting epipolar geometry constraints, we\ngenerate disparity images by training our network with an image reconstruction\nloss. We show that solving for image reconstruction alone results in poor\nquality depth images. To overcome this problem, we propose a novel training\nloss that enforces consistency between the disparities produced relative to\nboth the left and right images, leading to improved performance and robustness\ncompared to existing approaches. Our method produces state of the art results\nfor monocular depth estimation on the KITTI driving dataset, even outperforming\nsupervised methods that have been trained with ground truth depth. \n\n"}
{"id": "1609.04392", "contents": "Title: Learning Robust Features for Gait Recognition by Maximum Margin\n  Criterion Abstract: In the field of gait recognition from motion capture data, designing\nhuman-interpretable gait features is a common practice of many fellow\nresearchers. To refrain from ad-hoc schemes and to find maximally\ndiscriminative features we may need to explore beyond the limits of human\ninterpretability. This paper contributes to the state-of-the-art with a machine\nlearning approach for extracting robust gait features directly from raw joint\ncoordinates. The features are learned by a modification of Linear Discriminant\nAnalysis with Maximum Margin Criterion so that the identities are maximally\nseparated and, in combination with an appropriate classifier, used for gait\nrecognition. Experiments on the CMU MoCap database show that this method\noutperforms eight other relevant methods in terms of the distribution of\nbiometric templates in respective feature spaces expressed in four class\nseparability coefficients. Additional experiments indicate that this method is\na leading concept for rank-based classifier systems. \n\n"}
{"id": "1609.07042", "contents": "Title: Pose-Selective Max Pooling for Measuring Similarity Abstract: In this paper, we deal with two challenges for measuring the similarity of\nthe subject identities in practical video-based face recognition - the\nvariation of the head pose in uncontrolled environments and the computational\nexpense of processing videos. Since the frame-wise feature mean is unable to\ncharacterize the pose diversity among frames, we define and preserve the\noverall pose diversity and closeness in a video. Then, identity will be the\nonly source of variation across videos since the pose varies even within a\nsingle video. Instead of simply using all the frames, we select those faces\nwhose pose point is closest to the centroid of the K-means cluster containing\nthat pose point. Then, we represent a video as a bag of frame-wise deep face\nfeatures while the number of features has been reduced from hundreds to K.\nSince the video representation can well represent the identity, now we measure\nthe subject similarity between two videos as the max correlation among all\npossible pairs in the two bags of features. On the official 5,000 video-pairs\nof the YouTube Face dataset for face verification, our algorithm achieves a\ncomparable performance with VGG-face that averages over deep features of all\nframes. Other vision tasks can also benefit from the generic idea of employing\ngeometric cues to improve the descriptiveness of deep features. \n\n"}
{"id": "1609.07093", "contents": "Title: Neural Photo Editing with Introspective Adversarial Networks Abstract: The increasingly photorealistic sample quality of generative image models\nsuggests their feasibility in applications beyond image generation. We present\nthe Neural Photo Editor, an interface that leverages the power of generative\nneural networks to make large, semantically coherent changes to existing\nimages. To tackle the challenge of achieving accurate reconstructions without\nloss of feature quality, we introduce the Introspective Adversarial Network, a\nnovel hybridization of the VAE and GAN. Our model efficiently captures\nlong-range dependencies through use of a computational block based on\nweight-shared dilated convolutions, and improves generalization performance\nwith Orthogonal Regularization, a novel weight regularization method. We\nvalidate our contributions on CelebA, SVHN, and CIFAR-100, and produce samples\nand reconstructions with high visual fidelity. \n\n"}
{"id": "1609.07221", "contents": "Title: Convergence analysis of the direct extension of ADMM for multiple-block\n  separable convex minimization Abstract: Recently, the alternating direction method of multipliers (ADMM) has found\nmany efficient applications in various areas; and it has been shown that the\nconvergence is not guaranteed when it is directly extended to the\nmultiple-block case of separable convex minimization problems where there are\n$m\\ge 3$ functions without coupled variables in the objective. This fact has\ngiven great impetus to investigate various conditions on both the model and the\nalgorithm's parameter that can ensure the convergence of the direct extension\nof ADMM (abbreviated as \"e-ADMM\"). Despite some results under very strong\nconditions (e.g., at least $(m-1)$ functions should be strongly convex) that\nare applicable to the generic case with a general $m$, some others concentrate\non the special case of $m=3$ under the relatively milder condition that only\none function is assumed to be strongly convex. We focus on extending the\nconvergence analysis from the case of $m=3$ to the more general case of\n$m\\ge3$. That is, we show the convergence of e-ADMM for the case of $m\\ge 3$\nwith the assumption of only $(m-2)$ functions being strongly convex; and\nestablish its convergence rates in different scenarios such as the worst-case\nconvergence rates measured by iteration complexity and the asymptotically\nlinear convergence rate under stronger assumptions. Thus the convergence of\ne-ADMM for the general case of $m\\ge 4$ is proved; this result seems to be\nstill unknown even though it is intuitive given the known result of the case of\n$m=3$. Even for the special case of $m=3$, our convergence results turn out to\nbe more general than the exiting results that are derived specifically for the\ncase of $m=3$. \n\n"}
{"id": "1609.08661", "contents": "Title: Task Specific Adversarial Cost Function Abstract: The cost function used to train a generative model should fit the purpose of\nthe model. If the model is intended for tasks such as generating perceptually\ncorrect samples, it is beneficial to maximise the likelihood of a sample drawn\nfrom the model, Q, coming from the same distribution as the training data, P.\nThis is equivalent to minimising the Kullback-Leibler (KL) distance, KL[Q||P].\nHowever, if the model is intended for tasks such as retrieval or classification\nit is beneficial to maximise the likelihood that a sample drawn from the\ntraining data is captured by the model, equivalent to minimising KL[P||Q]. The\ncost function used in adversarial training optimises the Jensen-Shannon entropy\nwhich can be seen as an even interpolation between KL[Q||P] and KL[P||Q]. Here,\nwe propose an alternative adversarial cost function which allows easy tuning of\nthe model for either task. Our task specific cost function is evaluated on a\ndataset of hand-written characters in the following tasks: Generation,\nretrieval and one-shot learning. \n\n"}
{"id": "1610.00405", "contents": "Title: Seeing into Darkness: Scotopic Visual Recognition Abstract: Images are formed by counting how many photons traveling from a given set of\ndirections hit an image sensor during a given time interval. When photons are\nfew and far in between, the concept of `image' breaks down and it is best to\nconsider directly the flow of photons. Computer vision in this regime, which we\ncall `scotopic', is radically different from the classical image-based paradigm\nin that visual computations (classification, control, search) have to take\nplace while the stream of photons is captured and decisions may be taken as\nsoon as enough information is available. The scotopic regime is important for\nbiomedical imaging, security, astronomy and many other fields. Here we develop\na framework that allows a machine to classify objects with as few photons as\npossible, while maintaining the error rate below an acceptable threshold. A\ndynamic and asymptotically optimal speed-accuracy tradeoff is a key feature of\nthis framework. We propose and study an algorithm to optimize the tradeoff of a\nconvolutional network directly from lowlight images and evaluate on simulated\nimages from standard datasets. Surprisingly, scotopic systems can achieve\ncomparable classification performance as traditional vision systems while using\nless than 0.1% of the photons in a conventional image. In addition, we\ndemonstrate that our algorithms work even when the illuminance of the\nenvironment is unknown and varying. Last, we outline a spiking neural network\ncoupled with photon-counting sensors as a power-efficient hardware realization\nof scotopic algorithms. \n\n"}
{"id": "1610.00824", "contents": "Title: Real Time Fine-Grained Categorization with Accuracy and Interpretability Abstract: A well-designed fine-grained categorization system usually has three\ncontradictory requirements: accuracy (the ability to identify objects among\nsubordinate categories); interpretability (the ability to provide\nhuman-understandable explanation of recognition system behavior); and\nefficiency (the speed of the system). To handle the trade-off between accuracy\nand interpretability, we propose a novel \"Deeper Part-Stacked CNN\" architecture\narmed with interpretability by modeling subtle differences between object\nparts. The proposed architecture consists of a part localization network, a\ntwo-stream classification network that simultaneously encodes object-level and\npart-level cues, and a feature vectors fusion component. Specifically, the part\nlocalization network is implemented by exploring a new paradigm for key point\nlocalization that first samples a small number of representable pixels and then\ndetermine their labels via a convolutional layer followed by a softmax layer.\nWe also use a cropping layer to extract part features and propose a scale\nmean-max layer for feature fusion learning. Experimentally, our proposed method\noutperform state-of-the-art approaches both in part localization task and\nclassification task on Caltech-UCSD Birds-200-2011. Moreover, by adopting a set\nof sharing strategies between the computation of multiple object parts, our\nsingle model is fairly efficient running at 32 frames/sec. \n\n"}
{"id": "1610.01206", "contents": "Title: A Survey of Multi-View Representation Learning Abstract: Recently, multi-view representation learning has become a rapidly growing\ndirection in machine learning and data mining areas. This paper introduces two\ncategories for multi-view representation learning: multi-view representation\nalignment and multi-view representation fusion. Consequently, we first review\nthe representative methods and theories of multi-view representation learning\nbased on the perspective of alignment, such as correlation-based alignment.\nRepresentative examples are canonical correlation analysis (CCA) and its\nseveral extensions. Then from the perspective of representation fusion we\ninvestigate the advancement of multi-view representation learning that ranges\nfrom generative methods including multi-modal topic learning, multi-view sparse\ncoding, and multi-view latent space Markov networks, to neural network-based\nmethods including multi-modal autoencoders, multi-view convolutional neural\nnetworks, and multi-modal recurrent neural networks. Further, we also\ninvestigate several important applications of multi-view representation\nlearning. Overall, this survey aims to provide an insightful overview of\ntheoretical foundation and state-of-the-art developments in the field of\nmulti-view representation learning and to help researchers find the most\nappropriate tools for particular applications. \n\n"}
{"id": "1610.02680", "contents": "Title: Minimax Optimality of Shiryaev-Roberts Procedure for Quickest Drift\n  Change Detection of a Brownian motion Abstract: The problem of detecting a change in the drift of a Brownian motion is\nconsidered. The change point is assumed to have a modified exponential prior\ndistribution with unknown parameters. A worst-case analysis with respect to\nthese parameters is adopted leading to a min-max problem formulation.\nAnalytical and numerical justifications are provided towards establishing that\nthe Shiryaev-Roberts procedure with a specially designed starting point is\nexactly optimal for the proposed mathematical setup. \n\n"}
{"id": "1610.03303", "contents": "Title: The Phase Transition in 5 Point Energy Minimization Abstract: Let R_s(r)=sign(s)/r^s be the Riesz s-energy potential. (This is the usual\npower-law potential.) This monograph proves the existence of a computable\nnumber S=15.048... such that the triangular bi-pyramid is the unique minimizer\nwith respect to R_s, amongst all 5-point configurations on the sphere, if and\nonly if s lies in (-2,0) or (0,S). This establishes the existence of the\nlong-conjectured phase transition constant in 5-point energy minimization. \n\n"}
{"id": "1610.06204", "contents": "Title: A Reinforcement Learning Approach to the View Planning Problem Abstract: We present a Reinforcement Learning (RL) solution to the view planning\nproblem (VPP), which generates a sequence of view points that are capable of\nsensing all accessible area of a given object represented as a 3D model. In\ndoing so, the goal is to minimize the number of view points, making the VPP a\nclass of set covering optimization problem (SCOP). The SCOP is NP-hard, and the\ninapproximability results tell us that the greedy algorithm provides the best\napproximation that runs in polynomial time. In order to find a solution that is\nbetter than the greedy algorithm, (i) we introduce a novel score function by\nexploiting the geometry of the 3D model, (ii) we model an intuitive human\napproach to VPP using this score function, and (iii) we cast VPP as a Markovian\nDecision Process (MDP), and solve the MDP in RL framework using well-known RL\nalgorithms. In particular, we use SARSA, Watkins-Q and TD with function\napproximation to solve the MDP. We compare the results of our method with the\nbaseline greedy algorithm in an extensive set of test objects, and show that we\ncan out-perform the baseline in almost all cases. \n\n"}
{"id": "1610.06755", "contents": "Title: Switching in time-optimal problem with control in a ball Abstract: In this paper we analyse local regularity of time-optimal controls and\ntrajectories for an n-dimensional affine control system with a control\nparameter, taking values in a k-dimensional closed ball. In the case of k equal\nto n-1, we give sufficient conditions in terms of Lie bracket relations for all\noptimal controls to be smooth or to have only isolated jump discontinuities. \n\n"}
{"id": "1610.08026", "contents": "Title: Improved Upper Bounds on Systematic-Length for Linear Minimum Storage\n  Regenerating Codes Abstract: In this paper, we revisit the problem of finding the longest\nsystematic-length $k$ for a linear minimum storage regenerating (MSR) code with\noptimal repair of only systematic part, for a given per-node storage capacity\n$l$ and an arbitrary number of parity nodes $r$. We study the problem by\nfollowing a geometric analysis of linear subspaces and operators. First, a\nsimple quadratic bound is given, which implies that $k=r+2$ is the largest\nnumber of systematic nodes in the \\emph{scalar} scenario. Second, an\n$r$-based-log bound is derived, which is superior to the upper bound on\nlog-base $2$ in the prior work. Finally, an explicit upper bound depending on\nthe value of $\\frac{r^2}{l}$ is introduced, which further extends the\ncorresponding result in the literature. \n\n"}
{"id": "1610.09204", "contents": "Title: Judging a Book By its Cover Abstract: Book covers communicate information to potential readers, but can that same\ninformation be learned by computers? We propose using a deep Convolutional\nNeural Network (CNN) to predict the genre of a book based on the visual clues\nprovided by its cover. The purpose of this research is to investigate whether\nrelationships between books and their covers can be learned. However,\ndetermining the genre of a book is a difficult task because covers can be\nambiguous and genres can be overarching. Despite this, we show that a CNN can\nextract features and learn underlying design rules set by the designer to\ndefine a genre. Using machine learning, we can bring the large amount of\nresources available to the book cover design process. In addition, we present a\nnew challenging dataset that can be used for many pattern recognition tasks. \n\n"}
{"id": "1611.00448", "contents": "Title: Natural-Parameter Networks: A Class of Probabilistic Neural Networks Abstract: Neural networks (NN) have achieved state-of-the-art performance in various\napplications. Unfortunately in applications where training data is\ninsufficient, they are often prone to overfitting. One effective way to\nalleviate this problem is to exploit the Bayesian approach by using Bayesian\nneural networks (BNN). Another shortcoming of NN is the lack of flexibility to\ncustomize different distributions for the weights and neurons according to the\ndata, as is often done in probabilistic graphical models. To address these\nproblems, we propose a class of probabilistic neural networks, dubbed\nnatural-parameter networks (NPN), as a novel and lightweight Bayesian treatment\nof NN. NPN allows the usage of arbitrary exponential-family distributions to\nmodel the weights and neurons. Different from traditional NN and BNN, NPN takes\ndistributions as input and goes through layers of transformation before\nproducing distributions to match the target output distributions. As a Bayesian\ntreatment, efficient backpropagation (BP) is performed to learn the natural\nparameters for the distributions over both the weights and neurons. The output\ndistributions of each layer, as byproducts, may be used as second-order\nrepresentations for the associated tasks such as link prediction. Experiments\non real-world datasets show that NPN can achieve state-of-the-art performance. \n\n"}
{"id": "1611.00471", "contents": "Title: Dual Attention Networks for Multimodal Reasoning and Matching Abstract: We propose Dual Attention Networks (DANs) which jointly leverage visual and\ntextual attention mechanisms to capture fine-grained interplay between vision\nand language. DANs attend to specific regions in images and words in text\nthrough multiple steps and gather essential information from both modalities.\nBased on this framework, we introduce two types of DANs for multimodal\nreasoning and matching, respectively. The reasoning model allows visual and\ntextual attentions to steer each other during collaborative inference, which is\nuseful for tasks such as Visual Question Answering (VQA). In addition, the\nmatching model exploits the two attention mechanisms to estimate the similarity\nbetween images and sentences by focusing on their shared semantics. Our\nextensive experiments validate the effectiveness of DANs in combining vision\nand language, achieving the state-of-the-art performance on public benchmarks\nfor VQA and image-text matching. \n\n"}
{"id": "1611.01260", "contents": "Title: Learning Identity Mappings with Residual Gates Abstract: We propose a new layer design by adding a linear gating mechanism to shortcut\nconnections. By using a scalar parameter to control each gate, we provide a way\nto learn identity mappings by optimizing only one parameter. We build upon the\nmotivation behind Residual Networks, where a layer is reformulated in order to\nmake learning identity mappings less problematic to the optimizer. The\naugmentation introduces only one extra parameter per layer, and provides easier\noptimization by making degeneration into identity mappings simpler. We propose\na new model, the Gated Residual Network, which is the result when augmenting\nResidual Networks. Experimental results show that augmenting layers provides\nbetter optimization, increased performance, and more layer independence. We\nevaluate our method on MNIST using fully-connected networks, showing empirical\nindications that our augmentation facilitates the optimization of deep models,\nand that it provides high tolerance to full layer removal: the model retains\nover 90% of its performance even after half of its layers have been randomly\nremoved. We also evaluate our model on CIFAR-10 and CIFAR-100 using Wide Gated\nResNets, achieving 3.65% and 18.27% error, respectively. \n\n"}
{"id": "1611.02764", "contents": "Title: Inferring low-dimensional microstructure representations using\n  convolutional neural networks Abstract: We apply recent advances in machine learning and computer vision to a central\nproblem in materials informatics: The statistical representation of\nmicrostructural images. We use activations in a pre-trained convolutional\nneural network to provide a high-dimensional characterization of a set of\nsynthetic microstructural images. Next, we use manifold learning to obtain a\nlow-dimensional embedding of this statistical characterization. We show that\nthe low-dimensional embedding extracts the parameters used to generate the\nimages. According to a variety of metrics, the convolutional neural network\nmethod yields dramatically better embeddings than the analogous method derived\nfrom two-point correlations alone. \n\n"}
{"id": "1611.02862", "contents": "Title: The Little Engine that Could: Regularization by Denoising (RED) Abstract: Removal of noise from an image is an extensively studied problem in image\nprocessing. Indeed, the recent advent of sophisticated and highly effective\ndenoising algorithms lead some to believe that existing methods are touching\nthe ceiling in terms of noise removal performance. Can we leverage this\nimpressive achievement to treat other tasks in image processing? Recent work\nhas answered this question positively, in the form of the Plug-and-Play Prior\n($P^3$) method, showing that any inverse problem can be handled by sequentially\napplying image denoising steps. This relies heavily on the ADMM optimization\ntechnique in order to obtain this chained denoising interpretation.\n  Is this the only way in which tasks in image processing can exploit the image\ndenoising engine? In this paper we provide an alternative, more powerful and\nmore flexible framework for achieving the same goal. As opposed to the $P^3$\nmethod, we offer Regularization by Denoising (RED): using the denoising engine\nin defining the regularization of the inverse problem. We propose an explicit\nimage-adaptive Laplacian-based regularization functional, making the overall\nobjective functional clearer and better defined. With a complete flexibility to\nchoose the iterative optimization procedure for minimizing the above\nfunctional, RED is capable of incorporating any image denoising algorithm,\ntreat general inverse problems very effectively, and is guaranteed to converge\nto the globally optimal result. We test this approach and demonstrate\nstate-of-the-art results in the image deblurring and super-resolution problems. \n\n"}
{"id": "1611.02989", "contents": "Title: Bayesian data assimilation based on a family of outer measures Abstract: A flexible representation of uncertainty that remains within the standard\nframework of probabilistic measure theory is presented along with a study of\nits properties. This representation relies on a specific type of outer measure\nthat is based on the measure of a supremum, hence combining additive and highly\nsub-additive components. It is shown that this type of outer measure enables\nthe introduction of intuitive concepts such as pullback and general data\nassimilation operations. \n\n"}
{"id": "1611.03313", "contents": "Title: X-ray Scattering Image Classification Using Deep Learning Abstract: Visual inspection of x-ray scattering images is a powerful technique for\nprobing the physical structure of materials at the molecular scale. In this\npaper, we explore the use of deep learning to develop methods for automatically\nanalyzing x-ray scattering images. In particular, we apply Convolutional Neural\nNetworks and Convolutional Autoencoders for x-ray scattering image\nclassification. To acquire enough training data for deep learning, we use\nsimulation software to generate synthetic x-ray scattering images. Experiments\nshow that deep learning methods outperform previously published methods by 10\\%\non synthetic and real datasets. \n\n"}
{"id": "1611.04021", "contents": "Title: Leveraging Video Descriptions to Learn Video Question Answering Abstract: We propose a scalable approach to learn video-based question answering (QA):\nanswer a \"free-form natural language question\" about a video content. Our\napproach automatically harvests a large number of videos and descriptions\nfreely available online. Then, a large number of candidate QA pairs are\nautomatically generated from descriptions rather than manually annotated. Next,\nwe use these candidate QA pairs to train a number of video-based QA methods\nextended fromMN (Sukhbaatar et al. 2015), VQA (Antol et al. 2015), SA (Yao et\nal. 2015), SS (Venugopalan et al. 2015). In order to handle non-perfect\ncandidate QA pairs, we propose a self-paced learning procedure to iteratively\nidentify them and mitigate their effects in training. Finally, we evaluate\nperformance on manually generated video-based QA pairs. The results show that\nour self-paced learning procedure is effective, and the extended SS model\noutperforms various baselines. \n\n"}
{"id": "1611.04076", "contents": "Title: Least Squares Generative Adversarial Networks Abstract: Unsupervised learning with generative adversarial networks (GANs) has proven\nhugely successful. Regular GANs hypothesize the discriminator as a classifier\nwith the sigmoid cross entropy loss function. However, we found that this loss\nfunction may lead to the vanishing gradients problem during the learning\nprocess. To overcome such a problem, we propose in this paper the Least Squares\nGenerative Adversarial Networks (LSGANs) which adopt the least squares loss\nfunction for the discriminator. We show that minimizing the objective function\nof LSGAN yields minimizing the Pearson $\\chi^2$ divergence. There are two\nbenefits of LSGANs over regular GANs. First, LSGANs are able to generate higher\nquality images than regular GANs. Second, LSGANs perform more stable during the\nlearning process. We evaluate LSGANs on five scene datasets and the\nexperimental results show that the images generated by LSGANs are of better\nquality than the ones generated by regular GANs. We also conduct two comparison\nexperiments between LSGANs and regular GANs to illustrate the stability of\nLSGANs. \n\n"}
{"id": "1611.05244", "contents": "Title: Deep Transfer Learning for Person Re-identification Abstract: Person re-identification (Re-ID) poses a unique challenge to deep learning:\nhow to learn a deep model with millions of parameters on a small training set\nof few or no labels. In this paper, a number of deep transfer learning models\nare proposed to address the data sparsity problem. First, a deep network\narchitecture is designed which differs from existing deep Re-ID models in that\n(a) it is more suitable for transferring representations learned from large\nimage classification datasets, and (b) classification loss and verification\nloss are combined, each of which adopts a different dropout strategy. Second, a\ntwo-stepped fine-tuning strategy is developed to transfer knowledge from\nauxiliary datasets. Third, given an unlabelled Re-ID dataset, a novel\nunsupervised deep transfer learning model is developed based on co-training.\nThe proposed models outperform the state-of-the-art deep Re-ID models by large\nmargins: we achieve Rank-1 accuracy of 85.4\\%, 83.7\\% and 56.3\\% on CUHK03,\nMarket1501, and VIPeR respectively, whilst on VIPeR, our unsupervised model\n(45.1\\%) beats most supervised models. \n\n"}
{"id": "1611.05250", "contents": "Title: Real-Time Video Super-Resolution with Spatio-Temporal Networks and\n  Motion Compensation Abstract: Convolutional neural networks have enabled accurate image super-resolution in\nreal-time. However, recent attempts to benefit from temporal correlations in\nvideo super-resolution have been limited to naive or inefficient architectures.\nIn this paper, we introduce spatio-temporal sub-pixel convolution networks that\neffectively exploit temporal redundancies and improve reconstruction accuracy\nwhile maintaining real-time speed. Specifically, we discuss the use of early\nfusion, slow fusion and 3D convolutions for the joint processing of multiple\nconsecutive video frames. We also propose a novel joint motion compensation and\nvideo super-resolution algorithm that is orders of magnitude more efficient\nthan competing methods, relying on a fast multi-resolution spatial transformer\nmodule that is end-to-end trainable. These contributions provide both higher\naccuracy and temporally more consistent videos, which we confirm qualitatively\nand quantitatively. Relative to single-frame models, spatio-temporal networks\ncan either reduce the computational cost by 30% whilst maintaining the same\nquality or provide a 0.2dB gain for a similar computational cost. Results on\npublicly available datasets demonstrate that the proposed algorithms surpass\ncurrent state-of-the-art performance in both accuracy and efficiency. \n\n"}
{"id": "1611.05369", "contents": "Title: Fast On-Line Kernel Density Estimation for Active Object Localization Abstract: A major goal of computer vision is to enable computers to interpret visual\nsituations---abstract concepts (e.g., \"a person walking a dog,\" \"a crowd\nwaiting for a bus,\" \"a picnic\") whose image instantiations are linked more by\ntheir common spatial and semantic structure than by low-level visual\nsimilarity. In this paper, we propose a novel method for prior learning and\nactive object localization for this kind of knowledge-driven search in static\nimages. In our system, prior situation knowledge is captured by a set of\nflexible, kernel-based density estimations---a situation model---that represent\nthe expected spatial structure of the given situation. These estimations are\nefficiently updated by information gained as the system searches for relevant\nobjects, allowing the system to use context as it is discovered to narrow the\nsearch.\n  More specifically, at any given time in a run on a test image, our system\nuses image features plus contextual information it has discovered to identify a\nsmall subset of training images---an importance cluster---that is deemed most\nsimilar to the given test image, given the context. This subset is used to\ngenerate an updated situation model in an on-line fashion, using an efficient\nmultipole expansion technique.\n  As a proof of concept, we apply our algorithm to a highly varied and\nchallenging dataset consisting of instances of a \"dog-walking\" situation. Our\nresults support the hypothesis that dynamically-rendered, context-based\nprobability models can support efficient object localization in visual\nsituations. Moreover, our approach is general enough to be applied to diverse\nmachine learning paradigms requiring interpretable, probabilistic\nrepresentations generated from partially observed data. \n\n"}
{"id": "1611.05418", "contents": "Title: VisualBackProp: efficient visualization of CNNs Abstract: This paper proposes a new method, that we call VisualBackProp, for\nvisualizing which sets of pixels of the input image contribute most to the\npredictions made by the convolutional neural network (CNN). The method heavily\nhinges on exploring the intuition that the feature maps contain less and less\nirrelevant information to the prediction decision when moving deeper into the\nnetwork. The technique we propose was developed as a debugging tool for\nCNN-based systems for steering self-driving cars and is therefore required to\nrun in real-time, i.e. it was designed to require less computations than a\nforward propagation. This makes the presented visualization method a valuable\ndebugging tool which can be easily used during both training and inference. We\nfurthermore justify our approach with theoretical arguments and theoretically\nconfirm that the proposed method identifies sets of input pixels, rather than\nindividual pixels, that collaboratively contribute to the prediction. Our\ntheoretical findings stand in agreement with the experimental results. The\nempirical evaluation shows the plausibility of the proposed approach on the\nroad video data as well as in other applications and reveals that it compares\nfavorably to the layer-wise relevance propagation approach, i.e. it obtains\nsimilar visualization results and simultaneously achieves order of magnitude\nspeed-ups. \n\n"}
{"id": "1611.05594", "contents": "Title: SCA-CNN: Spatial and Channel-wise Attention in Convolutional Networks\n  for Image Captioning Abstract: Visual attention has been successfully applied in structural prediction tasks\nsuch as visual captioning and question answering. Existing visual attention\nmodels are generally spatial, i.e., the attention is modeled as spatial\nprobabilities that re-weight the last conv-layer feature map of a CNN encoding\nan input image. However, we argue that such spatial attention does not\nnecessarily conform to the attention mechanism --- a dynamic feature extractor\nthat combines contextual fixations over time, as CNN features are naturally\nspatial, channel-wise and multi-layer. In this paper, we introduce a novel\nconvolutional neural network dubbed SCA-CNN that incorporates Spatial and\nChannel-wise Attentions in a CNN. In the task of image captioning, SCA-CNN\ndynamically modulates the sentence generation context in multi-layer feature\nmaps, encoding where (i.e., attentive spatial locations at multiple layers) and\nwhat (i.e., attentive channels) the visual attention is. We evaluate the\nproposed SCA-CNN architecture on three benchmark image captioning datasets:\nFlickr8K, Flickr30K, and MSCOCO. It is consistently observed that SCA-CNN\nsignificantly outperforms state-of-the-art visual attention-based image\ncaptioning methods. \n\n"}
{"id": "1611.05644", "contents": "Title: Inverting The Generator Of A Generative Adversarial Network Abstract: Generative adversarial networks (GANs) learn to synthesise new samples from a\nhigh-dimensional distribution by passing samples drawn from a latent space\nthrough a generative network. When the high-dimensional distribution describes\nimages of a particular data set, the network should learn to generate visually\nsimilar image samples for latent variables that are close to each other in the\nlatent space. For tasks such as image retrieval and image classification, it\nmay be useful to exploit the arrangement of the latent space by projecting\nimages into it, and using this as a representation for discriminative tasks.\nGANs often consist of multiple layers of non-linear computations, making them\nvery difficult to invert. This paper introduces techniques for projecting image\nsamples into the latent space using any pre-trained GAN, provided that the\ncomputational graph is available. We evaluate these techniques on both MNIST\ndigits and Omniglot handwritten characters. In the case of MNIST digits, we\nshow that projections into the latent space maintain information about the\nstyle and the identity of the digit. In the case of Omniglot characters, we\nshow that even characters from alphabets that have not been seen during\ntraining may be projected well into the latent space; this suggests that this\napproach may have applications in one-shot learning. \n\n"}
{"id": "1611.05744", "contents": "Title: Compensating for Large In-Plane Rotations in Natural Images Abstract: Rotation invariance has been studied in the computer vision community\nprimarily in the context of small in-plane rotations. This is usually achieved\nby building invariant image features. However, the problem of achieving\ninvariance for large rotation angles remains largely unexplored. In this work,\nwe tackle this problem by directly compensating for large rotations, as opposed\nto building invariant features. This is inspired by the neuro-scientific\nconcept of mental rotation, which humans use to compare pairs of rotated\nobjects. Our contributions here are three-fold. First, we train a Convolutional\nNeural Network (CNN) to detect image rotations. We find that generic CNN\narchitectures are not suitable for this purpose. To this end, we introduce a\nconvolutional template layer, which learns representations for canonical\n'unrotated' images. Second, we use Bayesian Optimization to quickly sift\nthrough a large number of candidate images to find the canonical 'unrotated'\nimage. Third, we use this method to achieve robustness to large angles in an\nimage retrieval scenario. Our method is task-agnostic, and can be used as a\npre-processing step in any computer vision system. \n\n"}
{"id": "1611.05896", "contents": "Title: Answering Image Riddles using Vision and Reasoning through Probabilistic\n  Soft Logic Abstract: In this work, we explore a genre of puzzles (\"image riddles\") which involves\na set of images and a question. Answering these puzzles require both\ncapabilities involving visual detection (including object, activity\nrecognition) and, knowledge-based or commonsense reasoning. We compile a\ndataset of over 3k riddles where each riddle consists of 4 images and a\ngroundtruth answer. The annotations are validated using crowd-sourced\nevaluation. We also define an automatic evaluation metric to track future\nprogress. Our task bears similarity with the commonly known IQ tasks such as\nanalogy solving, sequence filling that are often used to test intelligence.\n  We develop a Probabilistic Reasoning-based approach that utilizes\nprobabilistic commonsense knowledge to answer these riddles with a reasonable\naccuracy. We demonstrate the results of our approach using both automatic and\nhuman evaluations. Our approach achieves some promising results for these\nriddles and provides a strong baseline for future attempts. We make the entire\ndataset and related materials publicly available to the community in\nImageRiddle Website (http://bit.ly/22f9Ala). \n\n"}
{"id": "1611.06345", "contents": "Title: Beyond Deep Residual Learning for Image Restoration: Persistent\n  Homology-Guided Manifold Simplification Abstract: The latest deep learning approaches perform better than the state-of-the-art\nsignal processing approaches in various image restoration tasks. However, if an\nimage contains many patterns and structures, the performance of these CNNs is\nstill inferior. To address this issue, here we propose a novel feature space\ndeep residual learning algorithm that outperforms the existing residual\nlearning. The main idea is originated from the observation that the performance\nof a learning algorithm can be improved if the input and/or label manifolds can\nbe made topologically simpler by an analytic mapping to a feature space. Our\nextensive numerical studies using denoising experiments and NTIRE single-image\nsuper-resolution (SISR) competition demonstrate that the proposed feature space\nresidual learning outperforms the existing state-of-the-art approaches.\nMoreover, our algorithm was ranked third in NTIRE competition with 5-10 times\nfaster computational time compared to the top ranked teams. The source code is\navailable on page : https://github.com/iorism/CNN.git \n\n"}
{"id": "1611.06474", "contents": "Title: Nazr-CNN: Fine-Grained Classification of UAV Imagery for Damage\n  Assessment Abstract: We propose Nazr-CNN1, a deep learning pipeline for object detection and\nfine-grained classification in images acquired from Unmanned Aerial Vehicles\n(UAVs) for damage assessment and monitoring. Nazr-CNN consists of two\ncomponents. The function of the first component is to localize objects (e.g.\nhouses or infrastructure) in an image by carrying out a pixel-level\nclassification. In the second component, a hidden layer of a Convolutional\nNeural Network (CNN) is used to encode Fisher Vectors (FV) of the segments\ngenerated from the first component in order to help discriminate between\ndifferent levels of damage. To showcase our approach we use data from UAVs that\nwere deployed to assess the level of damage in the aftermath of a devastating\ncyclone that hit the island of Vanuatu in 2015. The collected images were\nlabeled by a crowdsourcing effort and the labeling categories consisted of\nfine-grained levels of damage to built structures. Since our data set is\nrelatively small, a pre- trained network for pixel-level classification and FV\nencoding was used. Nazr-CNN attains promising results both for object detection\nand damage assessment suggesting that the integrated pipeline is robust in the\nface of small data sets and labeling errors by annotators. While the focus of\nNazr-CNN is on assessment of UAV images in a post-disaster scenario, our\nsolution is general and can be applied in many diverse settings. We show one\nsuch case of transfer learning to assess the level of damage in aerial images\ncollected after a typhoon in Philippines. \n\n"}
{"id": "1611.07119", "contents": "Title: Max-Margin Deep Generative Models for (Semi-)Supervised Learning Abstract: Deep generative models (DGMs) are effective on learning multilayered\nrepresentations of complex data and performing inference of input data by\nexploring the generative ability. However, it is relatively insufficient to\nempower the discriminative ability of DGMs on making accurate predictions. This\npaper presents max-margin deep generative models (mmDGMs) and a\nclass-conditional variant (mmDCGMs), which explore the strongly discriminative\nprinciple of max-margin learning to improve the predictive performance of DGMs\nin both supervised and semi-supervised learning, while retaining the generative\ncapability. In semi-supervised learning, we use the predictions of a max-margin\nclassifier as the missing labels instead of performing full posterior inference\nfor efficiency; we also introduce additional max-margin and label-balance\nregularization terms of unlabeled data for effectiveness. We develop an\nefficient doubly stochastic subgradient algorithm for the piecewise linear\nobjectives in different settings. Empirical results on various datasets\ndemonstrate that: (1) max-margin learning can significantly improve the\nprediction performance of DGMs and meanwhile retain the generative ability; (2)\nin supervised learning, mmDGMs are competitive to the best fully discriminative\nnetworks when employing convolutional neural networks as the generative and\nrecognition models; and (3) in semi-supervised learning, mmDCGMs can perform\nefficient inference and achieve state-of-the-art classification results on\nseveral benchmarks. \n\n"}
{"id": "1611.07661", "contents": "Title: Multigrid Neural Architectures Abstract: We propose a multigrid extension of convolutional neural networks (CNNs).\nRather than manipulating representations living on a single spatial grid, our\nnetwork layers operate across scale space, on a pyramid of grids. They consume\nmultigrid inputs and produce multigrid outputs; convolutional filters\nthemselves have both within-scale and cross-scale extent. This aspect is\ndistinct from simple multiscale designs, which only process the input at\ndifferent scales. Viewed in terms of information flow, a multigrid network\npasses messages across a spatial pyramid. As a consequence, receptive field\nsize grows exponentially with depth, facilitating rapid integration of context.\nMost critically, multigrid structure enables networks to learn internal\nattention and dynamic routing mechanisms, and use them to accomplish tasks on\nwhich modern CNNs fail.\n  Experiments demonstrate wide-ranging performance advantages of multigrid. On\nCIFAR and ImageNet classification tasks, flipping from a single grid to\nmultigrid within the standard CNN paradigm improves accuracy, while being\ncompute and parameter efficient. Multigrid is independent of other\narchitectural choices; we show synergy in combination with residual\nconnections. Multigrid yields dramatic improvement on a synthetic semantic\nsegmentation dataset. Most strikingly, relatively shallow multigrid networks\ncan learn to directly perform spatial transformation tasks, where, in contrast,\ncurrent CNNs fail. Together, our results suggest that continuous evolution of\nfeatures on a multigrid pyramid is a more powerful alternative to existing CNN\ndesigns on a flat grid. \n\n"}
{"id": "1611.07909", "contents": "Title: Image Segmentation Using Overlapping Group Sparsity Abstract: Sparse decomposition has been widely used for different applications, such as\nsource separation, image classification and image denoising. This paper\npresents a new algorithm for segmentation of an image into background and\nforeground text and graphics using sparse decomposition. First, the background\nis represented using a suitable smooth model, which is a linear combination of\na few smoothly varying basis functions, and the foreground text and graphics\nare modeled as a sparse component overlaid on the smooth background. Then the\nbackground and foreground are separated using a sparse decomposition framework\nand imposing some prior information, which promote the smoothness of\nbackground, and the sparsity and connectivity of foreground pixels. This\nalgorithm has been tested on a dataset of images extracted from HEVC standard\ntest sequences for screen content coding, and is shown to outperform prior\nmethods, including least absolute deviation fitting, k-means clustering based\nsegmentation in DjVu, and shape primitive extraction and coding algorithm. \n\n"}
{"id": "1611.08036", "contents": "Title: Robotic Grasp Detection using Deep Convolutional Neural Networks Abstract: Deep learning has significantly advanced computer vision and natural language\nprocessing. While there have been some successes in robotics using deep\nlearning, it has not been widely adopted. In this paper, we present a novel\nrobotic grasp detection system that predicts the best grasping pose of a\nparallel-plate robotic gripper for novel objects using the RGB-D image of the\nscene. The proposed model uses a deep convolutional neural network to extract\nfeatures from the scene and then uses a shallow convolutional neural network to\npredict the grasp configuration for the object of interest. Our multi-modal\nmodel achieved an accuracy of 89.21% on the standard Cornell Grasp Dataset and\nruns at real-time speeds. This redefines the state-of-the-art for robotic grasp\ndetection. \n\n"}
{"id": "1611.08207", "contents": "Title: Texture Synthesis with Spatial Generative Adversarial Networks Abstract: Generative adversarial networks (GANs) are a recent approach to train\ngenerative models of data, which have been shown to work particularly well on\nimage data. In the current paper we introduce a new model for texture synthesis\nbased on GAN learning. By extending the input noise distribution space from a\nsingle vector to a whole spatial tensor, we create an architecture with\nproperties well suited to the task of texture synthesis, which we call spatial\nGAN (SGAN). To our knowledge, this is the first successful completely\ndata-driven texture synthesis method based on GANs.\n  Our method has the following features which make it a state of the art\nalgorithm for texture synthesis: high image quality of the generated textures,\nvery high scalability w.r.t. the output texture size, fast real-time forward\ngeneration, the ability to fuse multiple diverse source images in complex\ntextures. To illustrate these capabilities we present multiple experiments with\ndifferent classes of texture images and use cases. We also discuss some\nlimitations of our method with respect to the types of texture images it can\nsynthesize, and compare it to other neural techniques for texture generation. \n\n"}
{"id": "1611.08272", "contents": "Title: InstanceCut: from Edges to Instances with MultiCut Abstract: This work addresses the task of instance-aware semantic segmentation. Our key\nmotivation is to design a simple method with a new modelling-paradigm, which\ntherefore has a different trade-off between advantages and disadvantages\ncompared to known approaches. Our approach, we term InstanceCut, represents the\nproblem by two output modalities: (i) an instance-agnostic semantic\nsegmentation and (ii) all instance-boundaries. The former is computed from a\nstandard convolutional neural network for semantic segmentation, and the latter\nis derived from a new instance-aware edge detection model. To reason globally\nabout the optimal partitioning of an image into instances, we combine these two\nmodalities into a novel MultiCut formulation. We evaluate our approach on the\nchallenging CityScapes dataset. Despite the conceptual simplicity of our\napproach, we achieve the best result among all published methods, and perform\nparticularly well for rare object classes. \n\n"}
{"id": "1611.08664", "contents": "Title: Semi-supervised Learning using Denoising Autoencoders for Brain Lesion\n  Detection and Segmentation Abstract: The work presented explores the use of denoising autoencoders (DAE) for brain\nlesion detection, segmentation and false positive reduction. Stacked denoising\nautoencoders (SDAE) were pre-trained using a large number of unlabeled patient\nvolumes and fine tuned with patches drawn from a limited number of patients\n(n=20, 40, 65). The results show negligible loss in performance even when SDAE\nwas fine tuned using 20 patients. Low grade glioma (LGG) segmentation was\nachieved using a transfer learning approach wherein a network pre-trained with\nHigh Grade Glioma (HGG) data was fine tuned using LGG image patches. The weakly\nsupervised SDAE (for HGG) and transfer learning based LGG network were also\nshown to generalize well and provide good segmentation on unseen BraTS 2013 &\nBraTS 2015 test data. An unique contribution includes a single layer DAE,\nreferred to as novelty detector(ND). ND was trained to accurately reconstruct\nnon-lesion patches using a mean squared error loss function. The reconstruction\nerror maps of test data were used to identify regions containing lesions. The\nerror maps were shown to assign unique error distributions to various\nconstituents of the glioma, enabling localization. The ND learns the non-lesion\nbrain accurately as it was also shown to provide good segmentation performance\non ischemic brain lesions in images from a different database. \n\n"}
{"id": "1611.09030", "contents": "Title: A modelling and computational study of the frustration index in signed\n  networks Abstract: Computing the frustration index of a signed graph is a key step toward\nsolving problems in many fields including social networks, political science,\nphysics, chemistry, and biology. The frustration index determines the distance\nof a network from a state of total structural balance. Although the definition\nof the frustration index goes back to the 1950's, its exact algorithmic\ncomputation, which is closely related to classic NP-hard graph problems, has\nonly become a focus in recent years. We develop three new binary linear\nprogramming models to compute the frustration index exactly and efficiently as\nthe solution to a global optimisation problem. Solving the models with\nprioritised branching and valid inequalities in Gurobi, we can compute the\nfrustration index of real signed networks with over 15000 edges in less than a\nminute on inexpensive hardware. We provide extensive performance analysis for\nboth random and real signed networks and show that our models outperform all\nexisting approaches by large factors. Based on solve time, algorithm output,\nand effective branching factor we highlight the superiority of our models to\nboth exact and heuristic methods in the literature. \n\n"}
{"id": "1611.09961", "contents": "Title: Semantic Facial Expression Editing using Autoencoded Flow Abstract: High-level manipulation of facial expressions in images --- such as changing\na smile to a neutral expression --- is challenging because facial expression\nchanges are highly non-linear, and vary depending on the appearance of the\nface. We present a fully automatic approach to editing faces that combines the\nadvantages of flow-based face manipulation with the more recent generative\ncapabilities of Variational Autoencoders (VAEs). During training, our model\nlearns to encode the flow from one expression to another over a low-dimensional\nlatent space. At test time, expression editing can be done simply using latent\nvector arithmetic. We evaluate our methods on two applications: 1) single-image\nfacial expression editing, and 2) facial expression interpolation between two\nimages. We demonstrate that our method generates images of higher perceptual\nquality than previous VAE and flow-based methods. \n\n"}
{"id": "1612.00534", "contents": "Title: Object Detection via Aspect Ratio and Context Aware Region-based\n  Convolutional Networks Abstract: Jointly integrating aspect ratio and context has been extensively studied and\nshown performance improvement in traditional object detection systems such as\nthe DPMs. It, however, has been largely ignored in deep neural network based\ndetection systems. This paper presents a method of integrating a mixture of\nobject models and region-based convolutional networks for accurate object\ndetection. Each mixture component accounts for both object aspect ratio and\nmulti-scale contextual information explicitly: (i) it exploits a mixture of\ntiling configurations in the RoI pooling to remedy the warping artifacts caused\nby a single type RoI pooling (e.g., with equally-sized 7 x 7 cells), and to\nrespect the underlying object shapes more; (ii) it \"looks from both the inside\nand the outside of a RoI\" by incorporating contextual information at two\nscales: global context pooled from the whole image and local context pooled\nfrom the surrounding of a RoI. To facilitate accurate detection, this paper\nproposes a multi-stage detection scheme for integrating the mixture of object\nmodels, which utilizes the detection results of the model at the previous stage\nas the proposals for the current in both training and testing. The proposed\nmethod is called the aspect ratio and context aware region-based convolutional\nnetwork (ARC-R-CNN). In experiments, ARC-R-CNN shows very competitive results\nwith Faster R-CNN [41] and R-FCN [10] on two datasets: the PASCAL VOC and the\nMicrosoft COCO. It obtains significantly better mAP performance using high IoU\nthresholds on both datasets. \n\n"}
{"id": "1612.00596", "contents": "Title: Learning to Search on Manifolds for 3D Pose Estimation of Articulated\n  Objects Abstract: This paper focuses on the challenging problem of 3D pose estimation of a\ndiverse spectrum of articulated objects from single depth images. A novel\nstructured prediction approach is considered, where 3D poses are represented as\nskeletal models that naturally operate on manifolds. Given an input depth\nimage, the problem of predicting the most proper articulation of underlying\nskeletal model is thus formulated as sequentially searching for the optimal\nskeletal configuration. This is subsequently addressed by convolutional neural\nnets trained end-to-end to render sequential prediction of the joint locations\nas regressing a set of tangent vectors of the underlying manifolds. Our\napproach is examined on various articulated objects including human hand,\nmouse, and fish benchmark datasets. Empirically it is shown to deliver highly\ncompetitive performance with respect to the state-of-the-arts, while operating\nin real-time (over 30 FPS). \n\n"}
{"id": "1612.00835", "contents": "Title: Scribbler: Controlling Deep Image Synthesis with Sketch and Color Abstract: Recently, there have been several promising methods to generate realistic\nimagery from deep convolutional networks. These methods sidestep the\ntraditional computer graphics rendering pipeline and instead generate imagery\nat the pixel level by learning from large collections of photos (e.g. faces or\nbedrooms). However, these methods are of limited utility because it is\ndifficult for a user to control what the network produces. In this paper, we\npropose a deep adversarial image synthesis architecture that is conditioned on\nsketched boundaries and sparse color strokes to generate realistic cars,\nbedrooms, or faces. We demonstrate a sketch based image synthesis system which\nallows users to 'scribble' over the sketch to indicate preferred color for\nobjects. Our network can then generate convincing images that satisfy both the\ncolor and the sketch constraints of user. The network is feed-forward which\nallows users to see the effect of their edits in real time. We compare to\nrecent work on sketch to image synthesis and show that our approach can\ngenerate more realistic, more diverse, and more controllable outputs. The\narchitecture is also effective at user-guided colorization of grayscale images. \n\n"}
{"id": "1612.02374", "contents": "Title: Automatic Detection of ADHD and ASD from Expressive Behaviour in RGBD\n  Data Abstract: Attention Deficit Hyperactivity Disorder (ADHD) and Autism Spectrum Disorder\n(ASD) are neurodevelopmental conditions which impact on a significant number of\nchildren and adults. Currently, the diagnosis of such disorders is done by\nexperts who employ standard questionnaires and look for certain behavioural\nmarkers through manual observation. Such methods for their diagnosis are not\nonly subjective, difficult to repeat, and costly but also extremely time\nconsuming. In this work, we present a novel methodology to aid diagnostic\npredictions about the presence/absence of ADHD and ASD by automatic visual\nanalysis of a person's behaviour. To do so, we conduct the questionnaires in a\ncomputer-mediated way while recording participants with modern RGBD\n(Colour+Depth) sensors. In contrast to previous automatic approaches which have\nfocussed only detecting certain behavioural markers, our approach provides a\nfully automatic end-to-end system for directly predicting ADHD and ASD in\nadults. Using state of the art facial expression analysis based on Dynamic Deep\nLearning and 3D analysis of behaviour, we attain classification rates of 96%\nfor Controls vs Condition (ADHD/ASD) group and 94% for Comorbid (ADHD+ASD) vs\nASD only group. We show that our system is a potentially useful time saving\ncontribution to the diagnostic field of ADHD and ASD. \n\n"}
{"id": "1612.04158", "contents": "Title: Automated Inference on Sociopsychological Impressions of Attractive\n  Female Faces Abstract: This article is a sequel to our earlier work [25]. The main objective of our\nresearch is to explore the potential of supervised machine learning in\nface-induced social computing and cognition, riding on the momentum of much\nheralded successes of face processing, analysis and recognition on the tasks of\nbiometric-based identification. We present a case study of automated\nstatistical inference on sociopsychological perceptions of female faces\ncontrolled for race, attractiveness, age and nationality. Our empirical\nevidences point to the possibility of training machine learning algorithms,\nusing example face images characterized by internet users, to predict\nperceptions of personality traits and demeanors. \n\n"}
{"id": "1612.04520", "contents": "Title: Single Image Action Recognition using Semantic Body Part Actions Abstract: In this paper, we propose a novel single image action recognition algorithm\nwhich is based on the idea of semantic body part actions. Unlike existing\nbottom up methods, we argue that the human action is a combination of\nmeaningful body part actions. In detail, we divide human body into five parts:\nhead, torso, arms, hands and legs. And for each of the body parts, we define\nseveral semantic body part actions, e.g., hand holding, hand waving. These\nsemantic body part actions are strongly related to the body actions, e.g.,\nwriting, and jogging. Based on the idea, we propose a deep neural network based\nsystem: first, body parts are localized by a Semi-FCN network. Second, for each\nbody parts, a Part Action Res-Net is used to predict semantic body part\nactions. And finally, we use SVM to fuse the body part actions and predict the\nentire body action. Experiments on two dataset: PASCAL VOC 2012 and Stanford-40\nreport mAP improvement from the state-of-the-art by 3.8% and 2.6% respectively. \n\n"}
{"id": "1612.04854", "contents": "Title: Temporal-Needle: A view and appearance invariant video descriptor Abstract: The ability to detect similar actions across videos can be very useful for\nreal-world applications in many fields. However, this task is still challenging\nfor existing systems, since videos that present the same action, can be taken\nfrom significantly different viewing directions, performed by different actors\nand backgrounds and under various video qualities. Video descriptors play a\nsignificant role in these systems. In this work we propose the\n\"temporal-needle\" descriptor which captures the dynamic behavior, while being\ninvariant to viewpoint and appearance. The descriptor is computed using multi\ntemporal scales of the video and by computing self-similarity for every patch\nthrough time in every temporal scale. The descriptor is computed for every\npixel in the video. However, to find similar actions across videos, we consider\nonly a small subset of the descriptors - the statistical significant\ndescriptors. This allow us to find good correspondences across videos more\nefficiently. Using the descriptor, we were able to detect the same behavior\nacross videos in a variety of scenarios. We demonstrate the use of the\ndescriptor in tasks such as temporal and spatial alignment, action detection\nand even show its potential in unsupervised video clustering into categories.\nIn this work we handled only videos taken with stationary cameras, but the\ndescriptor can be extended to handle moving camera as well. \n\n"}
{"id": "1612.06152", "contents": "Title: Few-Shot Object Recognition from Machine-Labeled Web Images Abstract: With the tremendous advances of Convolutional Neural Networks (ConvNets) on\nobject recognition, we can now obtain reliable enough machine-labeled\nannotations easily by predictions from off-the-shelf ConvNets. In this work, we\npresent an abstraction memory based framework for few-shot learning, building\nupon machine-labeled image annotations. Our method takes some large-scale\nmachine-annotated datasets (e.g., OpenImages) as an external memory bank. In\nthe external memory bank, the information is stored in the memory slots with\nthe form of key-value, where image feature is regarded as key and label\nembedding serves as value. When queried by the few-shot examples, our model\nselects visually similar data from the external memory bank, and writes the\nuseful information obtained from related external data into another memory\nbank, i.e., abstraction memory. Long Short-Term Memory (LSTM) controllers and\nattention mechanisms are utilized to guarantee the data written to the\nabstraction memory is correlated to the query example. The abstraction memory\nconcentrates information from the external memory bank, so that it makes the\nfew-shot recognition effective. In the experiments, we firstly confirm that our\nmodel can learn to conduct few-shot object recognition on clean human-labeled\ndata from ImageNet dataset. Then, we demonstrate that with our model,\nmachine-labeled image annotations are very effective and abundant resources to\nperform object recognition on novel categories. Experimental results show that\nour proposed model with machine-labeled annotations achieves great performance,\nonly with a gap of 1% between of the one with human-labeled annotations. \n\n"}
{"id": "1612.06361", "contents": "Title: Random linear systems with sparse solutions -- asymptotics and large\n  deviations Abstract: In this paper we revisit random linear under-determined systems with sparse\nsolutions. We consider $\\ell_1$ optimization heuristic known to work very well\nwhen used to solve these systems. A collection of fundamental results that\nrelate to its performance analysis in a statistical scenario is presented. We\nstart things off by recalling on now classical phase transition (PT) results\nthat we derived in \\cite{StojnicCSetam09,StojnicUpper10}. As these represent\nthe so-called breaking point characterizations, we now complement them by\nanalyzing the behavior in a zone around the breaking points in a sense\ntypically used in the study of the large deviation properties (LDP) in the\nclassical probability theory. After providing a conceptual solution to these\nproblems we attack them on a \"hardcore\" mathematical level attempting/hoping to\nbe able to obtain explicit solutions as elegant as those we obtained in\n\\cite{StojnicCSetam09,StojnicUpper10} (this time around though, the final\ncharacterizations were to be expected to be way more involved than in\n\\cite{StojnicCSetam09,StojnicUpper10}, simply, the ultimate goals are set much\nhigher and their achieving would provide a much richer collection of\ninformation about the $\\ell_1$'s behavior). Perhaps surprisingly, the final LDP\n$\\ell_1$ characterizations that we obtain happen to match the elegance of the\ncorresponding PT ones from \\cite{StojnicCSetam09,StojnicUpper10}. Moreover, as\nwe have done in \\cite{StojnicEquiv10}, here we also present a corresponding LDP\nset of results that can be obtained through an alternative high-dimensional\ngeometry approach. Finally, we also prove that the two types of\ncharacterizations, obtained through two substantially different mathematical\napproaches, match as one would hope that they do. \n\n"}
{"id": "1612.06933", "contents": "Title: Unsupervised Place Discovery for Visual Place Classification Abstract: In this study, we explore the use of deep convolutional neural networks\n(DCNNs) in visual place classification for robotic mapping and localization. An\nopen question is how to partition the robot's workspace into places to maximize\nthe performance (e.g., accuracy, precision, recall) of potential DCNN\nclassifiers. This is a chicken and egg problem: If we had a well-trained DCNN\nclassifier, it is rather easy to partition the robot's workspace into places,\nbut the training of a DCNN classifier requires a set of pre-defined place\nclasses. In this study, we address this problem and present several strategies\nfor unsupervised discovery of place classes (\"time cue,\" \"location cue,\"\n\"time-appearance cue,\" and \"location-appearance cue\"). We also evaluate the\nefficacy of the proposed methods using the publicly available University of\nMichigan North Campus Long-Term (NCLT) Dataset. \n\n"}
{"id": "1612.07289", "contents": "Title: Full-Duplex MIMO Small-Cell Networks with Interference Cancellation Abstract: Full-duplex (FD) technology is envisaged as a key component for future mobile\nbroadband networks due to its ability to boost the spectral efficiency. FD\nsystems can transmit and receive simultaneously on the same frequency at the\nexpense of residual self-interference (SI) and additional interference to the\nnetwork compared with half-duplex (HD) transmission. This paper analyzes the\nperformance of wireless networks with FD multi-antenna base stations (BSs) and\nHD user equipments (UEs) using stochastic geometry. Our analytical results\nquantify the success probability and the achievable spectral efficiency and\nindicate the amount of SI cancellation needed for beneficial FD operation. The\nadvantages of multi-antenna BSs/UEs are shown and the performance gains\nachieved by balancing desired signal power increase and interference\ncancellation are derived. The proposed framework aims at shedding light on the\nsystem-level gains of FD mode with respect to HD mode in terms of network\nthroughput, and provides design guidelines for the practical implementation of\nFD technology in large small-cell networks. \n\n"}
{"id": "1612.07902", "contents": "Title: LSE Precoders for Massive MIMO with Signal Constraints: Fundamental\n  Limits Abstract: This paper proposes the nonlinear Least Square Error (LSE) precoders for\nmultiuser MIMO broadcast channels. The output signals of LSE Precoders are\nlimited to be chosen from a predefined set which let these precoders address\nseveral constraints such as peak power limitation, constant envelope\ntransmission and discrete constellations. We study the large-system performance\nof these precoders via the replica method from statistical physics, and derive\na closed-form expression for the asymptotic distortion. Our results demonstrate\nthat an LSE precoder with the output peak-to-average power ratio of $3~{\\rm\ndB}$ can track the performance of the Regularized Zero Forcing (RZF) precoder\nclosely. As the peak-to-average power ratio reduces to one, the constant\nenvelope precoder is recovered. The investigations depict that the performance\nof the RZF precoder is achieved by the constant envelope precoder with $20\\%$\nof more transmit antennas. For $M$-PSK constellations, our analysis gives a\nlower-bound on the asymptotic distortion which is tight for moderate\nantenna-to-user ratios and deviates as the ratio grows. We improve this bound\nby deriving the replica solution under one-step of replica symmetry breaking.\nOur numerical investigations for this case show that the bound is tight for\nantenna-to-user ratios less than $5$. \n\n"}
{"id": "1701.00485", "contents": "Title: Two-Bit Networks for Deep Learning on Resource-Constrained Embedded\n  Devices Abstract: With the rapid proliferation of Internet of Things and intelligent edge\ndevices, there is an increasing need for implementing machine learning\nalgorithms, including deep learning, on resource-constrained mobile embedded\ndevices with limited memory and computation power. Typical large Convolutional\nNeural Networks (CNNs) need large amounts of memory and computational power,\nand cannot be deployed on embedded devices efficiently. We present Two-Bit\nNetworks (TBNs) for model compression of CNNs with edge weights constrained to\n(-2, -1, 1, 2), which can be encoded with two bits. Our approach can reduce the\nmemory usage and improve computational efficiency significantly while achieving\ngood performance in terms of classification accuracy, thus representing a\nreasonable tradeoff between model size and performance. \n\n"}
{"id": "1701.01981", "contents": "Title: Guessing Attacks on Distributed-Storage Systems Abstract: The secrecy of a distributed-storage system for passwords is studied. The\nencoder, Alice, observes a length-n password and describes it using two hints,\nwhich she stores in different locations. The legitimate receiver, Bob, observes\nboth hints. In one scenario the requirement is that the expected number of\nguesses it takes Bob to guess the password approach one as n tends to infinity,\nand in the other that the expected size of the shortest list that Bob must form\nto guarantee that it contain the password approach one. The eavesdropper, Eve,\nsees only one of the hints. Assuming that Alice cannot control which hints Eve\nobserves, the largest normalized (by n) exponent that can be guaranteed for the\nexpected number of guesses it takes Eve to guess the password is characterized\nfor each scenario. Key to the proof are new results on Arikan's guessing and\nBunte and Lapidoth's task-encoding problem; in particular, the paper\nestablishes a close relation between the two problems. A rate-distortion\nversion of the model is also discussed, as is a generalization that allows for\nAlice to produce {\\delta} (not necessarily two) hints, for Bob to observe {\\nu}\n(not necessarily two) of the hints, and for Eve to observe {\\eta} (not\nnecessarily one) of the hints. The generalized model is robust against {\\delta}\n- {\\nu} disk failures. \n\n"}
{"id": "1701.02892", "contents": "Title: Multivariate Regression with Grossly Corrupted Observations: A Robust\n  Approach and its Applications Abstract: This paper studies the problem of multivariate linear regression where a\nportion of the observations is grossly corrupted or is missing, and the\nmagnitudes and locations of such occurrences are unknown in priori. To deal\nwith this problem, we propose a new approach by explicitly consider the error\nsource as well as its sparseness nature. An interesting property of our\napproach lies in its ability of allowing individual regression output elements\nor tasks to possess their unique noise levels. Moreover, despite working with a\nnon-smooth optimization problem, our approach still guarantees to converge to\nits optimal solution. Experiments on synthetic data demonstrate the\ncompetitiveness of our approach compared with existing multivariate regression\nmodels. In addition, empirically our approach has been validated with very\npromising results on two exemplar real-world applications: The first concerns\nthe prediction of \\textit{Big-Five} personality based on user behaviors at\nsocial network sites (SNSs), while the second is 3D human hand pose estimation\nfrom depth images. The implementation of our approach and comparison methods as\nwell as the involved datasets are made publicly available in support of the\nopen-source and reproducible research initiatives. \n\n"}
{"id": "1701.03023", "contents": "Title: On the Tradeoff Region of Secure Exact-Repair Regenerating Codes Abstract: We consider the $(n,k,d,\\ell)$ secure exact-repair regenerating code problem,\nwhich generalizes the $(n,k,d)$ exact-repair regenerating code problem with the\nadditional constraint that the stored file needs to be kept\ninformation-theoretically secure against an eavesdropper, who can access the\ndata transmitted to regenerate a total of $\\ell$ different failed nodes. For\nall known results on this problem, the achievable tradeoff regions between the\nnormalized storage capacity and repair bandwidth have a single corner point,\nachieved by a scheme proposed by Shah, Rashmi and Kumar (the SRK point). Since\nthe achievable tradeoff regions of the exact-repair regenerating code problem\nwithout any secrecy constraints are known to have multiple corner points in\ngeneral, these existing results suggest a phase-change-like behavior, i.e.,\nenforcing a secrecy constraint ($\\ell\\geq 1$) immediately reduces the tradeoff\nregion to one with a single corner point. In this work, we first show that when\nthe secrecy parameter $\\ell$ is sufficiently large, the SRK point is indeed the\nonly corner point of the tradeoff region. However, when $\\ell$ is small, we\nshow that the tradeoff region can in fact have multiple corner points. In\nparticular, we establish a precise characterization of the tradeoff region for\nthe $(7,6,6,1)$ problem, which has exactly two corner points. Thus, a smooth\ntransition, instead of a phase-change-type of transition, should be expected as\nthe secrecy constraint is gradually strengthened. \n\n"}
{"id": "1701.04200", "contents": "Title: Distributionally Robust Stochastic Optimization with Dependence\n  Structure Abstract: Distributionally robust stochastic optimization (DRSO) is a framework for\ndecision-making problems under certainty, which finds solutions that perform\nwell for a chosen set of probability distributions. Many different approaches\nfor specifying a set of distributions have been proposed. The choice matters,\nbecause it affects the results, and the relative performance of different\nchoices depend on the characteristics of the problems. In this paper, we\nconsider problems in which different random variables exhibit some form of\ndependence, but the exact values of the parameters that represent the\ndependence are not known. We consider various sets of distributions that\nincorporate the dependence structure, and we study the corresponding DRSO\nproblems.\n  In the first part of the paper, we consider problems with linear dependence\nbetween random variables. We consider sets of distributions that are within a\nspecified Wasserstein distance of a nominal distribution, and that satisfy a\nsecond-order moment constraint. We obtain a tractable dual reformulation of the\ncorresponding DRSO problem. This approach is compared with the traditional\nmoment-based DRSO and Wasserstein-based DRSO with no moment constraints.\nNumerical experiments suggest that our new formulation has superior\nout-of-sample performance.\n  In the second part of the paper, we consider problems with various types of\nrank dependence between random variables, including rank dependence measured by\nSpearman's footrule distance between empirical rankings, comonotonic\ndistributions, box uncertainty for individual observations, and Wasserstein\ndistance between copulas associated with continuous distributions. We also\nobtain a dual reformulation of the DRSO problem. A desirable byproduct of the\nformulation is that it also avoids an issue associated with the one-sided\nmoment constraints in moment-based DRSO problems. \n\n"}
{"id": "1701.04466", "contents": "Title: Continuity of Channel Parameters and Operations under Various DMC\n  Topologies Abstract: We study the continuity of many channel parameters and operations under\nvarious topologies on the space of equivalent discrete memoryless channels\n(DMC). We show that mutual information, channel capacity, Bhattacharyya\nparameter, probability of error of a fixed code, and optimal probability of\nerror for a given code rate and blocklength, are continuous under various DMC\ntopologies. We also show that channel operations such as sums, products,\ninterpolations, and Ar{\\i}kan-style transformations are continuous. \n\n"}
{"id": "1701.05943", "contents": "Title: Structure of optimal strategies for remote estimation over\n  Gilbert-Elliott channel with feedback Abstract: We investigate remote estimation over a Gilbert-Elliot channel with feedback.\nWe assume that the channel state is observed by the receiver and fed back to\nthe transmitter with one unit delay. In addition, the transmitter gets ACK/NACK\nfeedback for successful/unsuccessful transmission. Using ideas from team\ntheory, we establish the structure of optimal transmission and estimation\nstrategies and identify a dynamic program to determine optimal strategies with\nthat structure. We then consider first-order autoregressive sources where the\nnoise process has unimodal and symmetric distribution. Using ideas from\nmajorization theory, we show that the optimal transmission strategy has a\nthreshold structure and the optimal estimation strategy is Kalman-like. \n\n"}
{"id": "1701.06342", "contents": "Title: Bayesian definition of random sequences with respect to conditional\n  probabilities Abstract: We study Martin-L\\\"{o}f random (ML-random) points on computable probability\nmeasures on sample and parameter spaces (Bayes models). We consider variants of\nconditional randomness defined by ML-randomness on Bayes models and those of\nconditional blind randomness. We show that variants of conditional blind\nrandomness are ill-defined from the Bayes statistical point of view. We prove\nthat if the sets of random sequences of uniformly computable parametric models\nare pairwise disjoint then there is a consistent estimator for the model.\nFinally, we present an algorithmic solution to a classical problem in Bayes\nstatistics, i.e., the posterior distributions converge weakly to almost all\nparameters if and only if the posterior distributions converge weakly to all\nML-random parameters. \n\n"}
{"id": "1701.06969", "contents": "Title: Error correction based on partial information Abstract: We consider the decoding of linear and array codes from errors when we are\nonly allowed to download a part of the codeword. More specifically, suppose\nthat we have encoded $k$ data symbols using an $(n,k)$ code with code length\n$n$ and dimension $k.$ During storage, some of the codeword coordinates might\nbe corrupted by errors. We aim to recover the original data by reading the\ncorrupted codeword with a limit on the transmitting bandwidth, namely, we can\nonly download an $\\alpha$ proportion of the corrupted codeword. For a given\n$\\alpha,$ our objective is to design a code and a decoding scheme such that we\ncan recover the original data from the largest possible number of errors. A\nnaive scheme is to read $\\alpha n$ coordinates of the codeword. This method\nused in conjunction with MDS codes guarantees recovery from any $\\lfloor(\\alpha\nn-k)/2\\rfloor$ errors. In this paper we show that we can instead read an\n$\\alpha$ proportion from each of the codeword's coordinates. For a\nwell-designed MDS code, this method can guarantee recovery from $\\lfloor\n(n-k/\\alpha)/2 \\rfloor$ errors, which is $1/\\alpha$ times more than the naive\nmethod, and is also the maximum number of errors that an $(n,k)$ code can\ncorrect by downloading only an $\\alpha$ proportion of the codeword. We present\ntwo families of such optimal constructions and decoding schemes. One is a\nReed-Solomon code with evaluation points in a subfield and the other is based\non Folded Reed-Solomon codes. We further show that both code constructions\nattain asymptotically optimal list decoding radius when downloading only a part\nof the corrupted codeword. We also construct an ensemble of random codes that\nwith high probability approaches the upper bound on the number of correctable\nerrors when the decoder downloads an $\\alpha$ proportion of the corrupted\ncodeword. \n\n"}
{"id": "1701.07153", "contents": "Title: Throughput Maximization for Wireless Powered Communications Harvesting\n  from Non-dedicated Sources Abstract: We consider the wireless powered communications where users harvest energy\nfrom non-dedicated sources. The user follows a harvest-then-transmit protocol:\nin first phase of a slot time the source node harvests energy from a nearby\nconventional Access Point, then transmit information to its destination node or\nrelay node in the second phase. We obtain the optimal\\textit{ harvesting ratio}\nto maximize the expected throughput for direct transmission (DT )and decode\nforward (DF) relay under outage constraint, respectively. Our results reveal\nthat the optimal harvest ratio for DT is dominated by the outage constraint\nwhile for DF relay, by the data causality . \n\n"}
{"id": "1701.08349", "contents": "Title: Supervised Deep Sparse Coding Networks Abstract: In this paper, we describe the deep sparse coding network (SCN), a novel deep\nnetwork that encodes intermediate representations with nonnegative sparse\ncoding. The SCN is built upon a number of cascading bottleneck modules, where\neach module consists of two sparse coding layers with relatively wide and slim\ndictionaries that are specialized to produce high dimensional discriminative\nfeatures and low dimensional representations for clustering, respectively.\nDuring training, both the dictionaries and regularization parameters are\noptimized with an end-to-end supervised learning algorithm based on multilevel\noptimization. Effectiveness of an SCN with seven bottleneck modules is verified\non several popular benchmark datasets. Remarkably, with few parameters to\nlearn, our SCN achieves 5.81% and 19.93% classification error rate on CIFAR-10\nand CIFAR-100, respectively. \n\n"}
{"id": "1702.00606", "contents": "Title: Joint Offloading and Computing Optimization in Wireless Powered\n  Mobile-Edge Computing Systems Abstract: Mobile-edge computing (MEC) and wireless power transfer (WPT) have been\nrecognized as promising techniques in the Internet of Things (IoT) era to\nprovide massive low-power wireless devices with enhanced computation capability\nand sustainable energy supply. In this paper, we propose a unified MEC-WPT\ndesign by considering a wireless powered multiuser MEC system, where a\nmulti-antenna access point (AP) (integrated with an MEC server) broadcasts\nwireless power to charge multiple users and each user node relies on the\nharvested energy to execute computation tasks. With MEC, these users can\nexecute their respective tasks locally by themselves or offload all or part of\nthem to the AP based on a time division multiple access (TDMA) protocol.\nBuilding on the proposed model, we develop an innovative framework to improve\nthe MEC performance, by jointly optimizing the energy transmit beamformer at\nthe AP, the central processing unit (CPU) frequencies and the numbers of\noffloaded bits at the users, as well as the time allocation among users. Under\nthis framework, we address a practical scenario where latency-limited\ncomputation is required. In this case, we develop an optimal resource\nallocation scheme that minimizes the AP's total energy consumption subject to\nthe users' individual computation latency constraints. Leveraging the\nstate-of-the-art optimization techniques, we derive the optimal solution in a\nsemi-closed form. Numerical results demonstrate the merits of the proposed\ndesign over alternative benchmark schemes. \n\n"}
{"id": "1702.01421", "contents": "Title: An extension of Chubanov's algorithm to symmetric cones Abstract: In this work we present an extension of Chubanov's algorithm to the case of\nhomogeneous feasibility problems over a symmetric cone K. As in Chubanov's\nmethod for linear feasibility problems, the algorithm consists of a basic\nprocedure and a step where the solutions are confined to the intersection of a\nhalf-space and K. Following an earlier work by Kitahara and Tsuchiya on second\norder cone feasibility problems, progress is measured through the volumes of\nthose intersections: when they become sufficiently small, we know it is time to\nstop. We never have to explicitly compute the volumes, it is only necessary to\nkeep track of the reductions between iterations. We show this is enough to\nobtain concrete upper bounds to the minimum eigenvalues of a scaled version of\nthe original feasibility problem. Another distinguishing feature of our\napproach is the usage of a spectral norm that takes into account the way that K\nis decomposed as simple cones. In several key cases, including semidefinite\nprogramming and second order cone programming, these norms make it possible to\nobtain better complexity bounds for the basic procedure when compared to a\nrecent approach by Pe\\~na and Soheili. Finally, in the appendix, we present a\ntranslation of the algorithm to the homogeneous feasibility problem in\nsemidefinite programming. \n\n"}
{"id": "1702.02295", "contents": "Title: Guided Optical Flow Learning Abstract: We study the unsupervised learning of CNNs for optical flow estimation using\nproxy ground truth data. Supervised CNNs, due to their immense learning\ncapacity, have shown superior performance on a range of computer vision\nproblems including optical flow prediction. They however require the ground\ntruth flow which is usually not accessible except on limited synthetic data.\nWithout the guidance of ground truth optical flow, unsupervised CNNs often\nperform worse as they are naturally ill-conditioned. We therefore propose a\nnovel framework in which proxy ground truth data generated from classical\napproaches is used to guide the CNN learning. The models are further refined in\nan unsupervised fashion using an image reconstruction loss. Our guided learning\napproach is competitive with or superior to state-of-the-art approaches on\nthree standard benchmark datasets yet is completely unsupervised and can run in\nreal time. \n\n"}
{"id": "1702.03507", "contents": "Title: Sense-and-Predict: Opportunistic MAC Based on Spatial Interference\n  Correlation for Cognitive Radio Networks Abstract: Opportunity detection at secondary transmitters (TXs) is a key technique\nenabling cognitive radio (CR) networks. Such detection however cannot guarantee\nreliable communication at secondary receivers (RXs), especially when their\nassociation distance is long. To cope with the issue, this paper proposes a\nnovel MAC called sense-and-predict (SaP), where each secondary TX decides\nwhether to access or not based on the prediction of the interference level at\nRX. Firstly, we provide the spatial interference correlation in a probabilistic\nform using stochastic geometry, and utilize it to maximize the area spectral\nefficiency (ASE) for secondary networks while guaranteeing the service quality\nof primary networks. Through simulations and testbed experiments using USRP,\nSaP is shown to always achieve ASE improvement compared with the conventional\nTX based sensing. \n\n"}
{"id": "1702.03828", "contents": "Title: Sharpness, Restart and Acceleration Abstract: The {\\L}ojasiewicz inequality shows that sharpness bounds on the minimum of\nconvex optimization problems hold almost generically. Sharpness directly\ncontrols the performance of restart schemes, as observed by Nemirovsky and\nNesterov (1985). The constants quantifying these sharpness bounds are of course\nunobservable, but we show that optimal restart strategies are robust, in the\nsense that, in some important cases, finding the best restart scheme only\nrequires a log scale grid search. Overall then, restart schemes generically\naccelerate accelerated first-order methods. \n\n"}
{"id": "1702.04479", "contents": "Title: Recognizing Dynamic Scenes with Deep Dual Descriptor based on Key Frames\n  and Key Segments Abstract: Recognizing dynamic scenes is one of the fundamental problems in scene\nunderstanding, which categorizes moving scenes such as a forest fire,\nlandslide, or avalanche. While existing methods focus on reliable capturing of\nstatic and dynamic information, few works have explored frame selection from a\ndynamic scene sequence. In this paper, we propose dynamic scene recognition\nusing a deep dual descriptor based on `key frames' and `key segments.' Key\nframes that reflect the feature distribution of the sequence with a small\nnumber are used for capturing salient static appearances. Key segments, which\nare captured from the area around each key frame, provide an additional\ndiscriminative power by dynamic patterns within short time intervals. To this\nend, two types of transferred convolutional neural network features are used in\nour approach. A fully connected layer is used to select the key frames and key\nsegments, while the convolutional layer is used to describe them. We conducted\nexperiments using public datasets as well as a new dataset comprised of 23\ndynamic scene classes with 10 videos per class. The evaluation results\ndemonstrated the state-of-the-art performance of the proposed method. \n\n"}
{"id": "1702.04680", "contents": "Title: Visual Discovery at Pinterest Abstract: Over the past three years Pinterest has experimented with several visual\nsearch and recommendation services, including Related Pins (2014), Similar\nLooks (2015), Flashlight (2016) and Lens (2017). This paper presents an\noverview of our visual discovery engine powering these services, and shares the\nrationales behind our technical and product decisions such as the use of object\ndetection and interactive user interfaces. We conclude that this visual\ndiscovery engine significantly improves engagement in both search and\nrecommendation tasks. \n\n"}
{"id": "1702.05197", "contents": "Title: Throughput-Optimal Broadcast in Wireless Networks with\n  Point-to-Multipoint Transmissions Abstract: We consider the problem of efficient packet dissemination in wireless\nnetworks with point-to-multi-point wireless broadcast channels. We propose a\ndynamic policy, which achieves the broadcast capacity of the network. This\npolicy is obtained by first transforming the original multi-hop network into a\nprecedence-relaxed virtual single-hop network and then finding an optimal\nbroadcast policy for the relaxed network. The resulting policy is shown to be\nthroughput-optimal for the original wireless network using a sample-path\nargument. We also prove the NP-completeness of the finite-horizon broadcast\nproblem, which is in contrast with the polynomial time solvability of the\nproblem with point-to-point channels. Illustrative simulation results\ndemonstrate the efficacy of the proposed broadcast policy in achieving the full\nbroadcast capacity with low delay. \n\n"}
{"id": "1702.05947", "contents": "Title: Cutting Planes for Families Implying Frankl's Conjecture Abstract: We find previously unknown families of sets which ensure Frankl's conjecture\nholds for all families that contain them using an algorithmic framework. The\nconjecture states that for any nonempty union-closed (UC) family there exists\nan element of the ground set in at least half the sets of the considered UC\nfamily. Poonen's Theorem characterizes the existence of weights which determine\nwhether a given UC family implies the conjecture for all UC families which\ncontain it. We design a cutting-plane method that computes the explicit weights\nwhich imply the existence conditions of Poonen's Theorem. This method enables\nus to answer several open questions regarding structural properties of UC\nfamilies, including the construction of a counterexample to a conjecture of\nMorris from 2006. \n\n"}
{"id": "1702.06318", "contents": "Title: Is Saki #delicious? The Food Perception Gap on Instagram and Its\n  Relation to Health Abstract: Food is an integral part of our life and what and how much we eat crucially\naffects our health. Our food choices largely depend on how we perceive certain\ncharacteristics of food, such as whether it is healthy, delicious or if it\nqualifies as a salad. But these perceptions differ from person to person and\none person's \"single lettuce leaf\" might be another person's \"side salad\".\nStudying how food is perceived in relation to what it actually is typically\ninvolves a laboratory setup. Here we propose to use recent advances in image\nrecognition to tackle this problem. Concretely, we use data for 1.9 million\nimages from Instagram from the US to look at systematic differences in how a\nmachine would objectively label an image compared to how a human subjectively\ndoes. We show that this difference, which we call the \"perception gap\", relates\nto a number of health outcomes observed at the county level. To the best of our\nknowledge, this is the first time that image recognition is being used to study\nthe \"misalignment\" of how people describe food images vs. what they actually\ndepict. \n\n"}
{"id": "1702.07025", "contents": "Title: Convolutional Neural Network Committees for Melanoma Classification with\n  Classical And Expert Knowledge Based Image Transforms Data Augmentation Abstract: Skin cancer is a major public health problem, as is the most common type of\ncancer and represents more than half of cancer diagnoses worldwide. Early\ndetection influences the outcome of the disease and motivates our work. We\ninvestigate the composition of CNN committees and data augmentation for the the\nISBI 2017 Melanoma Classification Challenge (named Skin Lesion Analysis towards\nMelanoma Detection) facing the peculiarities of dealing with such a small,\nunbalanced, biological database. For that, we explore committees of\nConvolutional Neural Networks trained over the ISBI challenge training dataset\nartificially augmented by both classical image processing transforms and image\nwarping guided by specialist knowledge about the lesion axis and improve the\nfinal classifier invariance to common melanoma variations. \n\n"}
{"id": "1702.07508", "contents": "Title: Toward high-performance online HCCR: a CNN approach with DropDistortion,\n  path signature and spatial stochastic max-pooling Abstract: This paper presents an investigation of several techniques that increase the\naccuracy of online handwritten Chinese character recognition (HCCR). We propose\na new training strategy named DropDistortion to train a deep convolutional\nneural network (DCNN) with distorted samples. DropDistortion gradually lowers\nthe degree of character distortion during training, which allows the DCNN to\nbetter generalize. Path signature is used to extract effective features for\nonline characters. Further improvement is achieved by employing spatial\nstochastic max-pooling as a method of feature map distortion and model\naveraging. Experiments were carried out on three publicly available datasets,\nnamely CASIA-OLHWDB 1.0, CASIA-OLHWDB 1.1, and the ICDAR2013 online HCCR\ncompetition dataset. The proposed techniques yield state-of-the-art recognition\naccuracies of 97.67%, 97.30%, and 97.99%, respectively. \n\n"}
{"id": "1702.08192", "contents": "Title: DeepNAT: Deep Convolutional Neural Network for Segmenting Neuroanatomy Abstract: We introduce DeepNAT, a 3D Deep convolutional neural network for the\nautomatic segmentation of NeuroAnaTomy in T1-weighted magnetic resonance\nimages. DeepNAT is an end-to-end learning-based approach to brain segmentation\nthat jointly learns an abstract feature representation and a multi-class\nclassification. We propose a 3D patch-based approach, where we do not only\npredict the center voxel of the patch but also neighbors, which is formulated\nas multi-task learning. To address a class imbalance problem, we arrange two\nnetworks hierarchically, where the first one separates foreground from\nbackground, and the second one identifies 25 brain structures on the\nforeground. Since patches lack spatial context, we augment them with\ncoordinates. To this end, we introduce a novel intrinsic parameterization of\nthe brain volume, formed by eigenfunctions of the Laplace-Beltrami operator. As\nnetwork architecture, we use three convolutional layers with pooling, batch\nnormalization, and non-linearities, followed by fully connected layers with\ndropout. The final segmentation is inferred from the probabilistic output of\nthe network with a 3D fully connected conditional random field, which ensures\nlabel agreement between close voxels. The roughly 2.7 million parameters in the\nnetwork are learned with stochastic gradient descent. Our results show that\nDeepNAT compares favorably to state-of-the-art methods. Finally, the purely\nlearning-based method may have a high potential for the adaptation to young,\nold, or diseased brains by fine-tuning the pre-trained network with a small\ntraining sample on the target application, where the availability of larger\ndatasets with manual annotations may boost the overall segmentation accuracy in\nthe future. \n\n"}
{"id": "1702.08734", "contents": "Title: Billion-scale similarity search with GPUs Abstract: Similarity search finds application in specialized database systems handling\ncomplex data such as images or videos, which are typically represented by\nhigh-dimensional features and require specific indexing structures. This paper\ntackles the problem of better utilizing GPUs for this task. While GPUs excel at\ndata-parallel tasks, prior approaches are bottlenecked by algorithms that\nexpose less parallelism, such as k-min selection, or make poor use of the\nmemory hierarchy.\n  We propose a design for k-selection that operates at up to 55% of theoretical\npeak performance, enabling a nearest neighbor implementation that is 8.5x\nfaster than prior GPU state of the art. We apply it in different similarity\nsearch scenarios, by proposing optimized design for brute-force, approximate\nand compressed-domain search based on product quantization. In all these\nsetups, we outperform the state of the art by large margins. Our implementation\nenables the construction of a high accuracy k-NN graph on 95 million images\nfrom the Yfcc100M dataset in 35 minutes, and of a graph connecting 1 billion\nvectors in less than 12 hours on 4 Maxwell Titan X GPUs. We have open-sourced\nour approach for the sake of comparison and reproducibility. \n\n"}
{"id": "1703.00405", "contents": "Title: Stability and performance analysis of linear positive systems with\n  delays using input-output methods Abstract: It is known that input-output approaches based on scaled small-gain theorems\nwith constant $D$-scalings and integral linear constraints are non-conservative\nfor the analysis of some classes of linear positive systems interconnected with\nuncertain linear operators. This dramatically contrasts with the case of\ngeneral linear systems with delays where input-output approaches provide, in\ngeneral, sufficient conditions only. Using these results we provide simple\nalternative proofs for many of the existing results on the stability of linear\npositive systems with discrete/distributed/neutral time-invariant/-varying\ndelays and linear difference equations. In particular, we give a simple proof\nfor the characterization of diagonal Riccati stability for systems with\ndiscrete-delays and generalize this equation to other types of delay systems.\nThe fact that all those results can be reproved in a very simple way\ndemonstrates the importance and the efficiency of the input-output framework\nfor the analysis of linear positive systems. The approach is also used to\nderive performance results evaluated in terms of the $L_1$-, $L_2$- and\n$L_\\infty$-gains. It is also flexible enough to be used for design purposes. \n\n"}
{"id": "1703.00845", "contents": "Title: Towards CNN Map Compression for camera relocalisation Abstract: This paper presents a study on the use of Convolutional Neural Networks for\ncamera relocalisation and its application to map compression. We follow state\nof the art visual relocalisation results and evaluate response to different\ndata inputs -- namely, depth, grayscale, RGB, spatial position and combinations\nof these. We use a CNN map representation and introduce the notion of CNN map\ncompression by using a smaller CNN architecture. We evaluate our proposal in a\nseries of publicly available datasets. This formulation allows us to improve\nrelocalisation accuracy by increasing the number of training trajectories while\nmaintaining a constant-size CNN. \n\n"}
{"id": "1703.01086", "contents": "Title: Arbitrary-Oriented Scene Text Detection via Rotation Proposals Abstract: This paper introduces a novel rotation-based framework for arbitrary-oriented\ntext detection in natural scene images. We present the Rotation Region Proposal\nNetworks (RRPN), which are designed to generate inclined proposals with text\norientation angle information. The angle information is then adapted for\nbounding box regression to make the proposals more accurately fit into the text\nregion in terms of the orientation. The Rotation Region-of-Interest (RRoI)\npooling layer is proposed to project arbitrary-oriented proposals to a feature\nmap for a text region classifier. The whole framework is built upon a\nregion-proposal-based architecture, which ensures the computational efficiency\nof the arbitrary-oriented text detection compared with previous text detection\nsystems. We conduct experiments using the rotation-based framework on three\nreal-world scene text detection datasets and demonstrate its superiority in\nterms of effectiveness and efficiency over previous approaches. \n\n"}
{"id": "1703.01208", "contents": "Title: Preserving Confidentiality in The Gaussian Broadcast Channel Using\n  Compute-and-Forward Abstract: We study the transmission of confidential messages across a wireless\nbroadcast channel with K>2 receivers and K helpers. The goal is to transmit all\nmessages reliably to their intended receivers while keeping them confidential\nfrom the unintended receivers. We design a codebook based on nested lattice\nstructure, cooperative jamming, lattice alignment, and i.i.d. coding. Moreover,\nwe exploit the asymmetric compute-and-forward decoding strategy to handle\nfinite SNR regimes. Unlike previous alignment schemes, our achievable rates are\nattainable at any finite SNR value. Also, we show that our scheme achieves the\noptimal sum secure degrees of freedom of 1 for the K-receiver Gaussian\nbroadcast channel with K confidential messages and K helpers. \n\n"}
{"id": "1703.01661", "contents": "Title: SegICP: Integrated Deep Semantic Segmentation and Pose Estimation Abstract: Recent robotic manipulation competitions have highlighted that sophisticated\nrobots still struggle to achieve fast and reliable perception of task-relevant\nobjects in complex, realistic scenarios. To improve these systems' perceptive\nspeed and robustness, we present SegICP, a novel integrated solution to object\nrecognition and pose estimation. SegICP couples convolutional neural networks\nand multi-hypothesis point cloud registration to achieve both robust pixel-wise\nsemantic segmentation as well as accurate and real-time 6-DOF pose estimation\nfor relevant objects. Our architecture achieves 1cm position error and\n<5^\\circ$ angle error in real time without an initial seed. We evaluate and\nbenchmark SegICP against an annotated dataset generated by motion capture. \n\n"}
{"id": "1703.01733", "contents": "Title: Position-based coding and convex splitting for private communication\n  over quantum channels Abstract: The classical-input quantum-output (cq) wiretap channel is a communication\nmodel involving a classical sender $X$, a legitimate quantum receiver $B$, and\na quantum eavesdropper $E$. The goal of a private communication protocol that\nuses such a channel is for the sender $X$ to transmit a message in such a way\nthat the legitimate receiver $B$ can decode it reliably, while the eavesdropper\n$E$ learns essentially nothing about which message was transmitted. The\n$\\varepsilon $-one-shot private capacity of a cq wiretap channel is equal to\nthe maximum number of bits that can be transmitted over the channel, such that\nthe privacy error is no larger than $\\varepsilon\\in(0,1)$. The present paper\nprovides a lower bound on the $\\varepsilon$-one-shot private classical\ncapacity, by exploiting the recently developed techniques of Anshu,\nDevabathini, Jain, and Warsi, called position-based coding and convex\nsplitting. The lower bound is equal to a difference of the hypothesis testing\nmutual information between $X$ and $B$ and the \"alternate\" smooth\nmax-information between $X$ and $E$. The one-shot lower bound then leads to a\nnon-trivial lower bound on the second-order coding rate for private classical\ncommunication over a memoryless cq wiretap channel. \n\n"}
{"id": "1703.02442", "contents": "Title: Detecting Cancer Metastases on Gigapixel Pathology Images Abstract: Each year, the treatment decisions for more than 230,000 breast cancer\npatients in the U.S. hinge on whether the cancer has metastasized away from the\nbreast. Metastasis detection is currently performed by pathologists reviewing\nlarge expanses of biological tissues. This process is labor intensive and\nerror-prone. We present a framework to automatically detect and localize tumors\nas small as 100 x 100 pixels in gigapixel microscopy images sized 100,000 x\n100,000 pixels. Our method leverages a convolutional neural network (CNN)\narchitecture and obtains state-of-the-art results on the Camelyon16 dataset in\nthe challenging lesion-level tumor detection task. At 8 false positives per\nimage, we detect 92.4% of the tumors, relative to 82.7% by the previous best\nautomated approach. For comparison, a human pathologist attempting exhaustive\nsearch achieved 73.2% sensitivity. We achieve image-level AUC scores above 97%\non both the Camelyon16 test set and an independent set of 110 slides. In\naddition, we discover that two slides in the Camelyon16 training set were\nerroneously labeled normal. Our approach could considerably reduce false\nnegative rates in metastasis detection. \n\n"}
{"id": "1703.04615", "contents": "Title: Recasting Residual-based Local Descriptors as Convolutional Neural\n  Networks: an Application to Image Forgery Detection Abstract: Local descriptors based on the image noise residual have proven extremely\neffective for a number of forensic applications, like forgery detection and\nlocalization. Nonetheless, motivated by promising results in computer vision,\nthe focus of the research community is now shifting on deep learning. In this\npaper we show that a class of residual-based descriptors can be actually\nregarded as a simple constrained convolutional neural network (CNN). Then, by\nrelaxing the constraints, and fine-tuning the net on a relatively small\ntraining set, we obtain a significant performance improvement with respect to\nthe conventional detector. \n\n"}
{"id": "1703.05161", "contents": "Title: Real-Time Panoramic Tracking for Event Cameras Abstract: Event cameras are a paradigm shift in camera technology. Instead of full\nframes, the sensor captures a sparse set of events caused by intensity changes.\nSince only the changes are transferred, those cameras are able to capture quick\nmovements of objects in the scene or of the camera itself. In this work we\npropose a novel method to perform camera tracking of event cameras in a\npanoramic setting with three degrees of freedom. We propose a direct camera\ntracking formulation, similar to state-of-the-art in visual odometry. We show\nthat the minimal information needed for simultaneous tracking and mapping is\nthe spatial position of events, without using the appearance of the imaged\nscene point. We verify the robustness to fast camera movements and dynamic\nobjects in the scene on a recently proposed dataset and self-recorded\nsequences. \n\n"}
{"id": "1703.05289", "contents": "Title: A clever elimination strategy for efficient minimal solvers Abstract: We present a new insight into the systematic generation of minimal solvers in\ncomputer vision, which leads to smaller and faster solvers. Many minimal\nproblem formulations are coupled sets of linear and polynomial equations where\nimage measurements enter the linear equations only. We show that it is useful\nto solve such systems by first eliminating all the unknowns that do not appear\nin the linear equations and then extending solutions to the rest of unknowns.\nThis can be generalized to fully non-linear systems by linearization via\nlifting. We demonstrate that this approach leads to more efficient solvers in\nthree problems of partially calibrated relative camera pose computation with\nunknown focal length and/or radial distortion. Our approach also generates new\ninteresting constraints on the fundamental matrices of partially calibrated\ncameras, which were not known before. \n\n"}
{"id": "1703.05830", "contents": "Title: Automatically identifying, counting, and describing wild animals in\n  camera-trap images with deep learning Abstract: Having accurate, detailed, and up-to-date information about the location and\nbehavior of animals in the wild would revolutionize our ability to study and\nconserve ecosystems. We investigate the ability to automatically, accurately,\nand inexpensively collect such data, which could transform many fields of\nbiology, ecology, and zoology into \"big data\" sciences. Motion sensor \"camera\ntraps\" enable collecting wildlife pictures inexpensively, unobtrusively, and\nfrequently. However, extracting information from these pictures remains an\nexpensive, time-consuming, manual task. We demonstrate that such information\ncan be automatically extracted by deep learning, a cutting-edge type of\nartificial intelligence. We train deep convolutional neural networks to\nidentify, count, and describe the behaviors of 48 species in the\n3.2-million-image Snapshot Serengeti dataset. Our deep neural networks\nautomatically identify animals with over 93.8% accuracy, and we expect that\nnumber to improve rapidly in years to come. More importantly, if our system\nclassifies only images it is confident about, our system can automate animal\nidentification for 99.3% of the data while still performing at the same 96.6%\naccuracy as that of crowdsourced teams of human volunteers, saving more than\n8.4 years (at 40 hours per week) of human labeling effort (i.e. over 17,000\nhours) on this 3.2-million-image dataset. Those efficiency gains immediately\nhighlight the importance of using deep neural networks to automate data\nextraction from camera-trap images. Our results suggest that this technology\ncould enable the inexpensive, unobtrusive, high-volume, and even real-time\ncollection of a wealth of information about vast numbers of animals in the\nwild. \n\n"}
{"id": "1703.06000", "contents": "Title: Semi-Supervised Deep Learning for Fully Convolutional Networks Abstract: Deep learning usually requires large amounts of labeled training data, but\nannotating data is costly and tedious. The framework of semi-supervised\nlearning provides the means to use both labeled data and arbitrary amounts of\nunlabeled data for training. Recently, semi-supervised deep learning has been\nintensively studied for standard CNN architectures. However, Fully\nConvolutional Networks (FCNs) set the state-of-the-art for many image\nsegmentation tasks. To the best of our knowledge, there is no existing\nsemi-supervised learning method for such FCNs yet. We lift the concept of\nauxiliary manifold embedding for semi-supervised learning to FCNs with the help\nof Random Feature Embedding. In our experiments on the challenging task of MS\nLesion Segmentation, we leverage the proposed framework for the purpose of\ndomain adaptation and report substantial improvements over the baseline model. \n\n"}
{"id": "1703.07339", "contents": "Title: Stochastic control on the half-line and applications to the optimal\n  dividend/consumption problem Abstract: We consider a stochastic control problem with the assumption that the system\nis controlled until the state process breaks the fixed barrier. Assuming some\ngeneral conditions, it is proved that the resulting Hamilton Jacobi Bellman\nequations has smooth solution. The aforementioned result is used to solve the\noptimal dividend and consumption problem. In the proof we use a fixed point\ntype argument, with an operator which is based on the stochastic representation\nfor a linear equation. \n\n"}
{"id": "1703.07737", "contents": "Title: In Defense of the Triplet Loss for Person Re-Identification Abstract: In the past few years, the field of computer vision has gone through a\nrevolution fueled mainly by the advent of large datasets and the adoption of\ndeep convolutional neural networks for end-to-end learning. The person\nre-identification subfield is no exception to this. Unfortunately, a prevailing\nbelief in the community seems to be that the triplet loss is inferior to using\nsurrogate losses (classification, verification) followed by a separate metric\nlearning step. We show that, for models trained from scratch as well as\npretrained ones, using a variant of the triplet loss to perform end-to-end deep\nmetric learning outperforms most other published methods by a large margin. \n\n"}
{"id": "1703.08570", "contents": "Title: Stochastic Methods for Composite and Weakly Convex Optimization Problems Abstract: We consider minimization of stochastic functionals that are compositions of a\n(potentially) non-smooth convex function $h$ and smooth function $c$ and, more\ngenerally, stochastic weakly-convex functionals. We develop a family of\nstochastic methods---including a stochastic prox-linear algorithm and a\nstochastic (generalized) sub-gradient procedure---and prove that, under mild\ntechnical conditions, each converges to first-order stationary points of the\nstochastic objective. We provide experiments further investigating our methods\non non-smooth phase retrieval problems; the experiments indicate the practical\neffectiveness of the procedures. \n\n"}
{"id": "1703.08617", "contents": "Title: Temporal Non-Volume Preserving Approach to Facial Age-Progression and\n  Age-Invariant Face Recognition Abstract: Modeling the long-term facial aging process is extremely challenging due to\nthe presence of large and non-linear variations during the face development\nstages. In order to efficiently address the problem, this work first decomposes\nthe aging process into multiple short-term stages. Then, a novel generative\nprobabilistic model, named Temporal Non-Volume Preserving (TNVP)\ntransformation, is presented to model the facial aging process at each stage.\nUnlike Generative Adversarial Networks (GANs), which requires an empirical\nbalance threshold, and Restricted Boltzmann Machines (RBM), an intractable\nmodel, our proposed TNVP approach guarantees a tractable density function,\nexact inference and evaluation for embedding the feature transformations\nbetween faces in consecutive stages. Our model shows its advantages not only in\ncapturing the non-linear age related variance in each stage but also producing\na smooth synthesis in age progression across faces. Our approach can model any\nface in the wild provided with only four basic landmark points. Moreover, the\nstructure can be transformed into a deep convolutional network while keeping\nthe advantages of probabilistic models with tractable log-likelihood density\nestimation. Our method is evaluated in both terms of synthesizing\nage-progressed faces and cross-age face verification and consistently shows the\nstate-of-the-art results in various face aging databases, i.e. FG-NET, MORPH,\nAginG Faces in the Wild (AGFW), and Cross-Age Celebrity Dataset (CACD). A\nlarge-scale face verification on Megaface challenge 1 is also performed to\nfurther show the advantages of our proposed approach. \n\n"}
{"id": "1703.08966", "contents": "Title: Mastering Sketching: Adversarial Augmentation for Structured Prediction Abstract: We present an integral framework for training sketch simplification networks\nthat convert challenging rough sketches into clean line drawings. Our approach\naugments a simplification network with a discriminator network, training both\nnetworks jointly so that the discriminator network discerns whether a line\ndrawing is a real training data or the output of the simplification network,\nwhich in turn tries to fool it. This approach has two major advantages. First,\nbecause the discriminator network learns the structure in line drawings, it\nencourages the output sketches of the simplification network to be more similar\nin appearance to the training sketches. Second, we can also train the\nsimplification network with additional unsupervised data, using the\ndiscriminator network as a substitute teacher. Thus, by adding only rough\nsketches without simplified line drawings, or only line drawings without the\noriginal rough sketches, we can improve the quality of the sketch\nsimplification. We show how our framework can be used to train models that\nsignificantly outperform the state of the art in the sketch simplification\ntask, despite using the same architecture for inference. We additionally\npresent an approach to optimize for a single image, which improves accuracy at\nthe cost of additional computation time. Finally, we show that, using the same\nframework, it is possible to train the network to perform the inverse problem,\ni.e., convert simple line sketches into pencil drawings, which is not possible\nusing the standard mean squared error loss. We validate our framework with two\nuser tests, where our approach is preferred to the state of the art in sketch\nsimplification 92.3% of the time and obtains 1.2 more points on a scale of 1 to\n5. \n\n"}
{"id": "1703.09211", "contents": "Title: Coherent Online Video Style Transfer Abstract: Training a feed-forward network for fast neural style transfer of images is\nproven to be successful. However, the naive extension to process video frame by\nframe is prone to producing flickering results. We propose the first end-to-end\nnetwork for online video style transfer, which generates temporally coherent\nstylized video sequences in near real-time. Two key ideas include an efficient\nnetwork by incorporating short-term coherence, and propagating short-term\ncoherence to long-term, which ensures the consistency over larger period of\ntime. Our network can incorporate different image stylization networks. We show\nthat the proposed method clearly outperforms the per-frame baseline both\nqualitatively and quantitatively. Moreover, it can achieve visually comparable\ncoherence to optimization-based video style transfer, but is three orders of\nmagnitudes faster in runtime. \n\n"}
{"id": "1703.09470", "contents": "Title: Learned Spectral Super-Resolution Abstract: We describe a novel method for blind, single-image spectral super-resolution.\nWhile conventional super-resolution aims to increase the spatial resolution of\nan input image, our goal is to spectrally enhance the input, i.e., generate an\nimage with the same spatial resolution, but a greatly increased number of\nnarrow (hyper-spectral) wave-length bands. Just like the spatial statistics of\nnatural images has rich structure, which one can exploit as prior to predict\nhigh-frequency content from a low resolution image, the same is also true in\nthe spectral domain: the materials and lighting conditions of the observed\nworld induce structure in the spectrum of wavelengths observed at a given\npixel. Surprisingly, very little work exists that attempts to use this\ndiagnosis and achieve blind spectral super-resolution from single images. We\nstart from the conjecture that, just like in the spatial domain, we can learn\nthe statistics of natural image spectra, and with its help generate finely\nresolved hyper-spectral images from RGB input. Technically, we follow the\ncurrent best practice and implement a convolutional neural network (CNN), which\nis trained to carry out the end-to-end mapping from an entire RGB image to the\ncorresponding hyperspectral image of equal size. We demonstrate spectral\nsuper-resolution both for conventional RGB images and for multi-spectral\nsatellite data, outperforming the state-of-the-art. \n\n"}
{"id": "1703.10714", "contents": "Title: Deep 3D Face Identification Abstract: We propose a novel 3D face recognition algorithm using a deep convolutional\nneural network (DCNN) and a 3D augmentation technique. The performance of 2D\nface recognition algorithms has significantly increased by leveraging the\nrepresentational power of deep neural networks and the use of large-scale\nlabeled training data. As opposed to 2D face recognition, training\ndiscriminative deep features for 3D face recognition is very difficult due to\nthe lack of large-scale 3D face datasets. In this paper, we show that transfer\nlearning from a CNN trained on 2D face images can effectively work for 3D face\nrecognition by fine-tuning the CNN with a relatively small number of 3D facial\nscans. We also propose a 3D face augmentation technique which synthesizes a\nnumber of different facial expressions from a single 3D face scan. Our proposed\nmethod shows excellent recognition results on Bosphorus, BU-3DFE, and 3D-TEC\ndatasets, without using hand-crafted features. The 3D identification using our\ndeep features also scales well for large databases. \n\n"}
{"id": "1704.00389", "contents": "Title: Hidden Two-Stream Convolutional Networks for Action Recognition Abstract: Analyzing videos of human actions involves understanding the temporal\nrelationships among video frames. State-of-the-art action recognition\napproaches rely on traditional optical flow estimation methods to pre-compute\nmotion information for CNNs. Such a two-stage approach is computationally\nexpensive, storage demanding, and not end-to-end trainable. In this paper, we\npresent a novel CNN architecture that implicitly captures motion information\nbetween adjacent frames. We name our approach hidden two-stream CNNs because it\nonly takes raw video frames as input and directly predicts action classes\nwithout explicitly computing optical flow. Our end-to-end approach is 10x\nfaster than its two-stage baseline. Experimental results on four challenging\naction recognition datasets: UCF101, HMDB51, THUMOS14 and ActivityNet v1.2 show\nthat our approach significantly outperforms the previous best real-time\napproaches. \n\n"}
{"id": "1704.00524", "contents": "Title: Block-Matching Convolutional Neural Network for Image Denoising Abstract: There are two main streams in up-to-date image denoising algorithms:\nnon-local self similarity (NSS) prior based methods and convolutional neural\nnetwork (CNN) based methods. The NSS based methods are favorable on images with\nregular and repetitive patterns while the CNN based methods perform better on\nirregular structures. In this paper, we propose a block-matching convolutional\nneural network (BMCNN) method that combines NSS prior and CNN. Initially,\nsimilar local patches in the input image are integrated into a 3D block. In\norder to prevent the noise from messing up the block matching, we first apply\nan existing denoising algorithm on the noisy image. The denoised image is\nemployed as a pilot signal for the block matching, and then denoising function\nfor the block is learned by a CNN structure. Experimental results show that the\nproposed BMCNN algorithm achieves state-of-the-art performance. In detail,\nBMCNN can restore both repetitive and irregular structures. \n\n"}
{"id": "1704.01137", "contents": "Title: DyVEDeep: Dynamic Variable Effort Deep Neural Networks Abstract: Deep Neural Networks (DNNs) have advanced the state-of-the-art in a variety\nof machine learning tasks and are deployed in increasing numbers of products\nand services. However, the computational requirements of training and\nevaluating large-scale DNNs are growing at a much faster pace than the\ncapabilities of the underlying hardware platforms that they are executed upon.\nIn this work, we propose Dynamic Variable Effort Deep Neural Networks\n(DyVEDeep) to reduce the computational requirements of DNNs during inference.\nPrevious efforts propose specialized hardware implementations for DNNs,\nstatically prune the network, or compress the weights. Complementary to these\napproaches, DyVEDeep is a dynamic approach that exploits the heterogeneity in\nthe inputs to DNNs to improve their compute efficiency with comparable\nclassification accuracy. DyVEDeep equips DNNs with dynamic effort mechanisms\nthat, in the course of processing an input, identify how critical a group of\ncomputations are to classify the input. DyVEDeep dynamically focuses its\ncompute effort only on the critical computa- tions, while skipping or\napproximating the rest. We propose 3 effort knobs that operate at different\nlevels of granularity viz. neuron, feature and layer levels. We build DyVEDeep\nversions for 5 popular image recognition benchmarks - one for CIFAR-10 and four\nfor ImageNet (AlexNet, OverFeat and VGG-16, weight-compressed AlexNet). Across\nall benchmarks, DyVEDeep achieves 2.1x-2.6x reduction in the number of scalar\noperations, which translates to 1.8x-2.3x performance improvement over a\nCaffe-based implementation, with < 0.5% loss in accuracy. \n\n"}
{"id": "1704.01262", "contents": "Title: Investigating Human Factors in Image Forgery Detection Abstract: In today's age of internet and social media, one can find an enormous volume\nof forged images on-line. These images have been used in the past to convey\nfalsified information and achieve harmful intentions. The spread and the effect\nof the social media only makes this problem more severe. While creating forged\nimages has become easier due to software advancements, there is no automated\nalgorithm which can reliably detect forgery.\n  Image forgery detection can be seen as a subset of image understanding\nproblem. Human performance is still the gold-standard for these type of\nproblems when compared to existing state-of-art automated algorithms. We\nconduct a subjective evaluation test with the aid of eye-tracker to investigate\ninto human factors associated with this problem. We compare the performance of\nan automated algorithm and humans for forgery detection problem. We also\ndevelop an algorithm which uses the data from the evaluation test to predict\nthe difficulty-level of an image (the difficulty-level of an image here denotes\nhow difficult it is for humans to detect forgery in an image. Terms such as\n\"Easy/difficult image\" will be used in the same context). The experimental\nresults presented in this paper should facilitate development of better\nalgorithms in the future. \n\n"}
{"id": "1704.01502", "contents": "Title: Weakly Supervised Dense Video Captioning Abstract: This paper focuses on a novel and challenging vision task, dense video\ncaptioning, which aims to automatically describe a video clip with multiple\ninformative and diverse caption sentences. The proposed method is trained\nwithout explicit annotation of fine-grained sentence to video region-sequence\ncorrespondence, but is only based on weak video-level sentence annotations. It\ndiffers from existing video captioning systems in three technical aspects.\nFirst, we propose lexical fully convolutional neural networks (Lexical-FCN)\nwith weakly supervised multi-instance multi-label learning to weakly link video\nregions with lexical labels. Second, we introduce a novel submodular\nmaximization scheme to generate multiple informative and diverse\nregion-sequences based on the Lexical-FCN outputs. A winner-takes-all scheme is\nadopted to weakly associate sentences to region-sequences in the training\nphase. Third, a sequence-to-sequence learning based language model is trained\nwith the weakly supervised information obtained through the association\nprocess. We show that the proposed method can not only produce informative and\ndiverse dense captions, but also outperform state-of-the-art single video\ncaptioning methods by a large margin. \n\n"}
{"id": "1704.02081", "contents": "Title: Evolution in Groups: A deeper look at synaptic cluster driven evolution\n  of deep neural networks Abstract: A promising paradigm for achieving highly efficient deep neural networks is\nthe idea of evolutionary deep intelligence, which mimics biological evolution\nprocesses to progressively synthesize more efficient networks. A crucial design\nfactor in evolutionary deep intelligence is the genetic encoding scheme used to\nsimulate heredity and determine the architectures of offspring networks. In\nthis study, we take a deeper look at the notion of synaptic cluster-driven\nevolution of deep neural networks which guides the evolution process towards\nthe formation of a highly sparse set of synaptic clusters in offspring\nnetworks. Utilizing a synaptic cluster-driven genetic encoding, the\nprobabilistic encoding of synaptic traits considers not only individual\nsynaptic properties but also inter-synaptic relationships within a deep neural\nnetwork. This process results in highly sparse offspring networks which are\nparticularly tailored for parallel computational devices such as GPUs and deep\nneural network accelerator chips. Comprehensive experimental results using four\nwell-known deep neural network architectures (LeNet-5, AlexNet, ResNet-56, and\nDetectNet) on two different tasks (object categorization and object detection)\ndemonstrate the efficiency of the proposed method. Cluster-driven genetic\nencoding scheme synthesizes networks that can achieve state-of-the-art\nperformance with significantly smaller number of synapses than that of the\noriginal ancestor network. ($\\sim$125-fold decrease in synapses for MNIST).\nFurthermore, the improved cluster efficiency in the generated offspring\nnetworks ($\\sim$9.71-fold decrease in clusters for MNIST and a $\\sim$8.16-fold\ndecrease in clusters for KITTI) is particularly useful for accelerated\nperformance on parallel computing hardware architectures such as those in GPUs\nand deep neural network accelerator chips. \n\n"}
{"id": "1704.02157", "contents": "Title: Multi-Scale Continuous CRFs as Sequential Deep Networks for Monocular\n  Depth Estimation Abstract: This paper addresses the problem of depth estimation from a single still\nimage. Inspired by recent works on multi- scale convolutional neural networks\n(CNN), we propose a deep model which fuses complementary information derived\nfrom multiple CNN side outputs. Different from previous methods, the\nintegration is obtained by means of continuous Conditional Random Fields\n(CRFs). In particular, we propose two different variations, one based on a\ncascade of multiple CRFs, the other on a unified graphical model. By designing\na novel CNN implementation of mean-field updates for continuous CRFs, we show\nthat both proposed models can be regarded as sequential deep networks and that\ntraining can be performed end-to-end. Through extensive experimental evaluation\nwe demonstrate the effective- ness of the proposed approach and establish new\nstate of the art results on publicly available datasets. \n\n"}
{"id": "1704.02249", "contents": "Title: Learned Watershed: End-to-End Learning of Seeded Segmentation Abstract: Learned boundary maps are known to outperform hand- crafted ones as a basis\nfor the watershed algorithm. We show, for the first time, how to train\nwatershed computation jointly with boundary map prediction. The estimator for\nthe merging priorities is cast as a neural network that is con- volutional\n(over space) and recurrent (over iterations). The latter allows learning of\ncomplex shape priors. The method gives the best known seeded segmentation\nresults on the CREMI segmentation challenge. \n\n"}
{"id": "1704.02268", "contents": "Title: Deep Unsupervised Similarity Learning using Partially Ordered Sets Abstract: Unsupervised learning of visual similarities is of paramount importance to\ncomputer vision, particularly due to lacking training data for fine-grained\nsimilarities. Deep learning of similarities is often based on relationships\nbetween pairs or triplets of samples. Many of these relations are unreliable\nand mutually contradicting, implying inconsistencies when trained without\nsupervision information that relates different tuples or triplets to each\nother. To overcome this problem, we use local estimates of reliable\n(dis-)similarities to initially group samples into compact surrogate classes\nand use local partial orders of samples to classes to link classes to each\nother. Similarity learning is then formulated as a partial ordering task with\nsoft correspondences of all samples to classes. Adopting a strategy of\nself-supervision, a CNN is trained to optimally represent samples in a mutually\nconsistent manner while updating the classes. The similarity learning and\ngrouping procedure are integrated in a single model and optimized jointly. The\nproposed unsupervised approach shows competitive performance on detailed pose\nestimation and object classification. \n\n"}
{"id": "1704.02685", "contents": "Title: Learning Important Features Through Propagating Activation Differences Abstract: The purported \"black box\" nature of neural networks is a barrier to adoption\nin applications where interpretability is essential. Here we present DeepLIFT\n(Deep Learning Important FeaTures), a method for decomposing the output\nprediction of a neural network on a specific input by backpropagating the\ncontributions of all neurons in the network to every feature of the input.\nDeepLIFT compares the activation of each neuron to its 'reference activation'\nand assigns contribution scores according to the difference. By optionally\ngiving separate consideration to positive and negative contributions, DeepLIFT\ncan also reveal dependencies which are missed by other approaches. Scores can\nbe computed efficiently in a single backward pass. We apply DeepLIFT to models\ntrained on MNIST and simulated genomic data, and show significant advantages\nover gradient-based methods. Video tutorial: http://goo.gl/qKb7pL, ICML slides:\nbit.ly/deeplifticmlslides, ICML talk: https://vimeo.com/238275076, code:\nhttp://goo.gl/RM8jvH. \n\n"}
{"id": "1704.03488", "contents": "Title: Learning Proximal Operators: Using Denoising Networks for Regularizing\n  Inverse Imaging Problems Abstract: While variational methods have been among the most powerful tools for solving\nlinear inverse problems in imaging, deep (convolutional) neural networks have\nrecently taken the lead in many challenging benchmarks. A remaining drawback of\ndeep learning approaches is their requirement for an expensive retraining\nwhenever the specific problem, the noise level, noise type, or desired measure\nof fidelity changes. On the contrary, variational methods have a plug-and-play\nnature as they usually consist of separate data fidelity and regularization\nterms.\n  In this paper we study the possibility of replacing the proximal operator of\nthe regularization used in many convex energy minimization algorithms by a\ndenoising neural network. The latter therefore serves as an implicit natural\nimage prior, while the data term can still be chosen independently. Using a\nfixed denoising neural network in exemplary problems of image deconvolution\nwith different blur kernels and image demosaicking, we obtain state-of-the-art\nreconstruction results. These indicate the high generalizability of our\napproach and a reduction of the need for problem-specific training.\nAdditionally, we discuss novel results on the analysis of possible optimization\nalgorithms to incorporate the network into, as well as the choices of algorithm\nparameters and their relation to the noise level the neural network is trained\non. \n\n"}
{"id": "1704.03920", "contents": "Title: Decomposition Algorithm for Distributionally Robust Optimization using\n  Wasserstein Metric Abstract: We study distributionally robust optimization (DRO) problems where the\nambiguity set is defined using the Wasserstein metric. We show that this class\nof DRO problems can be reformulated as semi-infinite programs. We give an\nexchange method to solve the reformulated problem for the general nonlinear\nmodel, and a central cutting-surface method for the convex case, assuming that\nwe have a separation oracle. We used a distributionally robust generalization\nof the logistic regression model to test our algorithm. Numerical experiments\non the distributionally robust logistic regression models show that the number\nof oracle calls are typically 20 ? 50 to achieve 5-digit precision. The\nsolution found by the model is generally better in its ability to predict with\na smaller standard error. \n\n"}
{"id": "1704.04365", "contents": "Title: Limited Feedback in Single and Multi-user MIMO Systems with Finite-Bit\n  ADCs Abstract: Communication systems with low-resolution analog-to-digital-converters (ADCs)\ncan exploit channel state information at the transmitter and receiver. This\npaper presents codebook designs and performance analyses for limited feedback\nMIMO systems with finite-bit ADCs. A point-to-point single-user channel is\nfirstly considered. When the received signal is sliced by 1-bit ADCs, the\nabsolute phase at the receiver is important to align the phase of the received\nsignals. A new codebook design for beamforming, which separately quantizes the\nchannel direction and the residual phase, is therefore proposed. For the\nmulti-bit case where the optimal transmission method is unknown, suboptimal\nGaussian signaling and eigenvector beamforming is assumed to obtain a lower\nbound of the achievable rate. It is found that to limit the rate loss, more\nfeedback bits are needed in the medium SNR regime than the low and high SNR\nregimes, which is quite different from the conventional infinite-bit ADC case.\nSecond, a multi-user system where a multiple-antenna transmitter sends signals\nto multiple single-antenna receivers with finite-bit ADCs is considered. Based\non the derived performance loss due to finite-bit ADCs and finite-bit CSI\nfeedback, the number of bits per feedback should increase linearly with the ADC\nresolution in order to restrict the rate loss. \n\n"}
{"id": "1704.05775", "contents": "Title: Deep Occlusion Reasoning for Multi-Camera Multi-Target Detection Abstract: People detection in single 2D images has improved greatly in recent years.\nHowever, comparatively little of this progress has percolated into multi-camera\nmulti-people tracking algorithms, whose performance still degrades severely\nwhen scenes become very crowded. In this work, we introduce a new architecture\nthat combines Convolutional Neural Nets and Conditional Random Fields to\nexplicitly model those ambiguities. One of its key ingredients are high-order\nCRF terms that model potential occlusions and give our approach its robustness\neven when many people are present. Our model is trained end-to-end and we show\nthat it outperforms several state-of-art algorithms on challenging scenes. \n\n"}
{"id": "1704.05796", "contents": "Title: Network Dissection: Quantifying Interpretability of Deep Visual\n  Representations Abstract: We propose a general framework called Network Dissection for quantifying the\ninterpretability of latent representations of CNNs by evaluating the alignment\nbetween individual hidden units and a set of semantic concepts. Given any CNN\nmodel, the proposed method draws on a broad data set of visual concepts to\nscore the semantics of hidden units at each intermediate convolutional layer.\nThe units with semantics are given labels across a range of objects, parts,\nscenes, textures, materials, and colors. We use the proposed method to test the\nhypothesis that interpretability of units is equivalent to random linear\ncombinations of units, then we apply our method to compare the latent\nrepresentations of various networks when trained to solve different supervised\nand self-supervised training tasks. We further analyze the effect of training\niterations, compare networks trained with different initializations, examine\nthe impact of network depth and width, and measure the effect of dropout and\nbatch normalization on the interpretability of deep visual representations. We\ndemonstrate that the proposed method can shed light on characteristics of CNN\nmodels and training methods that go beyond measurements of their discriminative\npower. \n\n"}
{"id": "1704.05838", "contents": "Title: Generative Face Completion Abstract: In this paper, we propose an effective face completion algorithm using a deep\ngenerative model. Different from well-studied background completion, the face\ncompletion task is more challenging as it often requires to generate\nsemantically new pixels for the missing key components (e.g., eyes and mouths)\nthat contain large appearance variations. Unlike existing nonparametric\nalgorithms that search for patches to synthesize, our algorithm directly\ngenerates contents for missing regions based on a neural network. The model is\ntrained with a combination of a reconstruction loss, two adversarial losses and\na semantic parsing loss, which ensures pixel faithfulness and local-global\ncontents consistency. With extensive experimental results, we demonstrate\nqualitatively and quantitatively that our model is able to deal with a large\narea of missing pixels in arbitrary shapes and generate realistic face\ncompletion results. \n\n"}
{"id": "1704.06036", "contents": "Title: End-to-end representation learning for Correlation Filter based tracking Abstract: The Correlation Filter is an algorithm that trains a linear template to\ndiscriminate between images and their translations. It is well suited to object\ntracking because its formulation in the Fourier domain provides a fast\nsolution, enabling the detector to be re-trained once per frame. Previous works\nthat use the Correlation Filter, however, have adopted features that were\neither manually designed or trained for a different task. This work is the\nfirst to overcome this limitation by interpreting the Correlation Filter\nlearner, which has a closed-form solution, as a differentiable layer in a deep\nneural network. This enables learning deep features that are tightly coupled to\nthe Correlation Filter. Experiments illustrate that our method has the\nimportant practical benefit of allowing lightweight architectures to achieve\nstate-of-the-art performance at high framerates. \n\n"}
{"id": "1704.06426", "contents": "Title: Massive MIMO Downlink 1-Bit Precoding with Linear Programming for PSK\n  Signaling Abstract: Quantized massive multiple-input-multiple-output (MIMO) systems are gaining\nmore interest due to their power efficiency. We present a new precoding\ntechnique to mitigate the multi-user interference and the quantization\ndistortions in a downlink multi-user (MU) multiple-input-single-output (MISO)\nsystem with 1-bit quantization at the transmitter. This work is restricted to\nPSK modulation schemes. The transmit signal vector is optimized for every\ndesired received vector taking into account the 1-bit quantization. The\noptimization is based on maximizing the safety margin to the decision\nthresholds of the PSK modulation. Simulation results show a significant gain in\nterms of the uncoded bit-error-ratio (BER) compared to the existing linear\nprecoding techniques. \n\n"}
{"id": "1704.06904", "contents": "Title: Residual Attention Network for Image Classification Abstract: In this work, we propose \"Residual Attention Network\", a convolutional neural\nnetwork using attention mechanism which can incorporate with state-of-art feed\nforward network architecture in an end-to-end training fashion. Our Residual\nAttention Network is built by stacking Attention Modules which generate\nattention-aware features. The attention-aware features from different modules\nchange adaptively as layers going deeper. Inside each Attention Module,\nbottom-up top-down feedforward structure is used to unfold the feedforward and\nfeedback attention process into a single feedforward process. Importantly, we\npropose attention residual learning to train very deep Residual Attention\nNetworks which can be easily scaled up to hundreds of layers. Extensive\nanalyses are conducted on CIFAR-10 and CIFAR-100 datasets to verify the\neffectiveness of every module mentioned above. Our Residual Attention Network\nachieves state-of-the-art object recognition performance on three benchmark\ndatasets including CIFAR-10 (3.90% error), CIFAR-100 (20.45% error) and\nImageNet (4.8% single model and single crop, top-5 error). Note that, our\nmethod achieves 0.6% top-1 accuracy improvement with 46% trunk depth and 69%\nforward FLOPs comparing to ResNet-200. The experiment also demonstrates that\nour network is robust against noisy labels. \n\n"}
{"id": "1704.06967", "contents": "Title: Proxy Templates for Inverse Compositional Photometric Bundle Adjustment Abstract: Recent advances in 3D vision have demonstrated the strengths of photometric\nbundle adjustment. By directly minimizing reprojected pixel errors, instead of\ngeometric reprojection errors, such methods can achieve sub-pixel alignment\naccuracy in both high and low textured regions. Typically, these problems are\nsolved using a forwards compositional Lucas-Kanade formulation parameterized by\n6-DoF rigid camera poses and a depth per point in the structure. For large\nproblems the most CPU-intensive component of the pipeline is the creation and\nfactorization of the Hessian matrix at each iteration. For many warps, the\ninverse compositional formulation can offer significant speed-ups since the\nHessian need only be inverted once. In this paper, we show that an ordinary\ninverse compositional formulation does not work for warps of this type of\nparameterization due to ill-conditioning of its partial derivatives. However,\nwe show that it is possible to overcome this limitation by introducing the\nconcept of a proxy template image. We show an order of magnitude improvement in\nspeed, with little effect on quality, going from forwards to inverse\ncompositional in our own photometric bundle adjustment method designed for\nobject-centric structure from motion. This means less processing time for large\nsystems or denser reconstructions under the same real-time constraints. We\nadditionally show that this theory can be readily applied to existing methods\nby integrating it with the recently released Direct Sparse Odometry SLAM\nalgorithm. \n\n"}
{"id": "1704.07575", "contents": "Title: Sharing deep generative representation for perceived image\n  reconstruction from human brain activity Abstract: Decoding human brain activities via functional magnetic resonance imaging\n(fMRI) has gained increasing attention in recent years. While encouraging\nresults have been reported in brain states classification tasks, reconstructing\nthe details of human visual experience still remains difficult. Two main\nchallenges that hinder the development of effective models are the perplexing\nfMRI measurement noise and the high dimensionality of limited data instances.\nExisting methods generally suffer from one or both of these issues and yield\ndissatisfactory results. In this paper, we tackle this problem by casting the\nreconstruction of visual stimulus as the Bayesian inference of missing view in\na multiview latent variable model. Sharing a common latent representation, our\njoint generative model of external stimulus and brain response is not only\n\"deep\" in extracting nonlinear features from visual images, but also powerful\nin capturing correlations among voxel activities of fMRI recordings. The\nnonlinearity and deep structure endow our model with strong representation\nability, while the correlations of voxel activities are critical for\nsuppressing noise and improving prediction. We devise an efficient variational\nBayesian method to infer the latent variables and the model parameters. To\nfurther improve the reconstruction accuracy, the latent representations of\ntesting instances are enforced to be close to that of their neighbours from the\ntraining set via posterior regularization. Experiments on three fMRI recording\ndatasets demonstrate that our approach can more accurately reconstruct visual\nstimuli. \n\n"}
{"id": "1704.07863", "contents": "Title: Multi-View Dynamic Facial Action Unit Detection Abstract: We propose a novel convolutional neural network approach to address the\nfine-grained recognition problem of multi-view dynamic facial action unit\ndetection. We leverage recent gains in large-scale object recognition by\nformulating the task of predicting the presence or absence of a specific action\nunit in a still image of a human face as holistic classification. We then\nexplore the design space of our approach by considering both shared and\nindependent representations for separate action units, and also different CNN\narchitectures for combining color and motion information. We then move to the\nnovel setup of the FERA 2017 Challenge, in which we propose a multi-view\nextension of our approach that operates by first predicting the viewpoint from\nwhich the video was taken, and then evaluating an ensemble of action unit\ndetectors that were trained for that specific viewpoint. Our approach is\nholistic, efficient, and modular, since new action units can be easily included\nin the overall system. Our approach significantly outperforms the baseline of\nthe FERA 2017 Challenge, with an absolute improvement of 14% on the F1-metric.\nAdditionally, it compares favorably against the winner of the FERA 2017\nchallenge. Code source is available at https://github.com/BCV-Uniandes/AUNets. \n\n"}
{"id": "1704.08165", "contents": "Title: A Generalization of Convolutional Neural Networks to Graph-Structured\n  Data Abstract: This paper introduces a generalization of Convolutional Neural Networks\n(CNNs) from low-dimensional grid data, such as images, to graph-structured\ndata. We propose a novel spatial convolution utilizing a random walk to uncover\nthe relations within the input, analogous to the way the standard convolution\nuses the spatial neighborhood of a pixel on the grid. The convolution has an\nintuitive interpretation, is efficient and scalable and can also be used on\ndata with varying graph structure. Furthermore, this generalization can be\napplied to many standard regression or classification problems, by learning the\nthe underlying graph. We empirically demonstrate the performance of the\nproposed CNN on MNIST, and challenge the state-of-the-art on Merck molecular\nactivity data set. \n\n"}
{"id": "1704.08572", "contents": "Title: Frequency-domain Compressive Channel Estimation for Frequency-Selective\n  Hybrid mmWave MIMO Systems Abstract: Channel estimation is useful in millimeter wave (mmWave) MIMO communication\nsystems. Channel state information allows optimized designs of precoders and\ncombiners under different metrics such as mutual information or\nsignal-to-interference-noise (SINR) ratio. At mmWave, MIMO precoders and\ncombiners are usually hybrid, since this architecture provides a means to\ntrade-off power consumption and achievable rate. Channel estimation is\nchallenging when using these architectures, however, since there is no direct\naccess to the outputs of the different antenna elements in the array. The MIMO\nchannel can only be observed through the analog combining network, which acts\nas a compression stage of the received signal. Most of prior work on channel\nestimation for hybrid architectures assumes a frequency-flat mmWave channel\nmodel. In this paper, we consider a frequency-selective mmWave channel and\npropose compressed-sensing-based strategies to estimate the channel in the\nfrequency domain. We evaluate different algorithms and compute their complexity\nto expose trade-offs in complexity-overhead-performance as compared to those of\nprevious approaches. \n\n"}
{"id": "1704.08686", "contents": "Title: Deep Functional Maps: Structured Prediction for Dense Shape\n  Correspondence Abstract: We introduce a new framework for learning dense correspondence between\ndeformable 3D shapes. Existing learning based approaches model shape\ncorrespondence as a labelling problem, where each point of a query shape\nreceives a label identifying a point on some reference domain; the\ncorrespondence is then constructed a posteriori by composing the label\npredictions of two input shapes. We propose a paradigm shift and design a\nstructured prediction model in the space of functional maps, linear operators\nthat provide a compact representation of the correspondence. We model the\nlearning process via a deep residual network which takes dense descriptor\nfields defined on two shapes as input, and outputs a soft map between the two\ngiven objects. The resulting correspondence is shown to be accurate on several\nchallenging benchmarks comprising multiple categories, synthetic models, real\nscans with acquisition artifacts, topological noise, and partiality. \n\n"}
{"id": "1704.08740", "contents": "Title: Improving Facial Attribute Prediction using Semantic Segmentation Abstract: Attributes are semantically meaningful characteristics whose applicability\nwidely crosses category boundaries. They are particularly important in\ndescribing and recognizing concepts where no explicit training example is\ngiven, \\textit{e.g., zero-shot learning}. Additionally, since attributes are\nhuman describable, they can be used for efficient human-computer interaction.\nIn this paper, we propose to employ semantic segmentation to improve facial\nattribute prediction. The core idea lies in the fact that many facial\nattributes describe local properties. In other words, the probability of an\nattribute to appear in a face image is far from being uniform in the spatial\ndomain. We build our facial attribute prediction model jointly with a deep\nsemantic segmentation network. This harnesses the localization cues learned by\nthe semantic segmentation to guide the attention of the attribute prediction to\nthe regions where different attributes naturally show up. As a result of this\napproach, in addition to recognition, we are able to localize the attributes,\ndespite merely having access to image level labels (weak supervision) during\ntraining. We evaluate our proposed method on CelebA and LFWA datasets and\nachieve superior results to the prior arts. Furthermore, we show that in the\nreverse problem, semantic face parsing improves when facial attributes are\navailable. That reaffirms the need to jointly model these two interconnected\ntasks. \n\n"}
{"id": "1705.00727", "contents": "Title: Hyperspectral Image Classification with Markov Random Fields and a\n  Convolutional Neural Network Abstract: This paper presents a new supervised classification algorithm for remotely\nsensed hyperspectral image (HSI) which integrates spectral and spatial\ninformation in a unified Bayesian framework. First, we formulate the HSI\nclassification problem from a Bayesian perspective. Then, we adopt a\nconvolutional neural network (CNN) to learn the posterior class distributions\nusing a patch-wise training strategy to better use the spatial information.\nNext, spatial information is further considered by placing a spatial smoothness\nprior on the labels. Finally, we iteratively update the CNN parameters using\nstochastic gradient decent (SGD) and update the class labels of all pixel\nvectors using an alpha-expansion min-cut-based algorithm. Compared with other\nstate-of-the-art methods, the proposed classification method achieves better\nperformance on one synthetic dataset and two benchmark HSI datasets in a number\nof experimental settings. \n\n"}
{"id": "1705.01809", "contents": "Title: Pixel Normalization from Numeric Data as Input to Neural Networks Abstract: Text to image transformation for input to neural networks requires\nintermediate steps. This paper attempts to present a new approach to pixel\nnormalization so as to convert textual data into image, suitable as input for\nneural networks. This method can be further improved by its Graphics Processing\nUnit (GPU) implementation to provide significant speedup in computational time. \n\n"}
{"id": "1705.02090", "contents": "Title: GRASS: Generative Recursive Autoencoders for Shape Structures Abstract: We introduce a novel neural network architecture for encoding and synthesis\nof 3D shapes, particularly their structures. Our key insight is that 3D shapes\nare effectively characterized by their hierarchical organization of parts,\nwhich reflects fundamental intra-shape relationships such as adjacency and\nsymmetry. We develop a recursive neural net (RvNN) based autoencoder to map a\nflat, unlabeled, arbitrary part layout to a compact code. The code effectively\ncaptures hierarchical structures of man-made 3D objects of varying structural\ncomplexities despite being fixed-dimensional: an associated decoder maps a code\nback to a full hierarchy. The learned bidirectional mapping is further tuned\nusing an adversarial setup to yield a generative model of plausible structures,\nfrom which novel structures can be sampled. Finally, our structure synthesis\nframework is augmented by a second trained module that produces fine-grained\npart geometry, conditioned on global and local structural context, leading to a\nfull generative pipeline for 3D shapes. We demonstrate that without\nsupervision, our network learns meaningful structural hierarchies adhering to\nperceptual grouping principles, produces compact codes which enable\napplications such as shape classification and partial matching, and supports\nshape synthesis and interpolation with significant variations in topology and\ngeometry. \n\n"}
{"id": "1705.02356", "contents": "Title: Solving (most) of a set of quadratic equalities: Composite optimization\n  for robust phase retrieval Abstract: We develop procedures, based on minimization of the composition $f(x) =\nh(c(x))$ of a convex function $h$ and smooth function $c$, for solving random\ncollections of quadratic equalities, applying our methodology to phase\nretrieval problems. We show that the prox-linear algorithm we develop can solve\nphase retrieval problems---even with adversarially faulty measurements---with\nhigh probability as soon as the number of measurements $m$ is a constant factor\nlarger than the dimension $n$ of the signal to be recovered. The algorithm\nrequires essentially no tuning---it consists of solving a sequence of convex\nproblems---and it is implementable without any particular assumptions on the\nmeasurements taken. We provide substantial experiments investigating our\nmethods, indicating the practical effectiveness of the procedures and showing\nthat they succeed with high probability as soon as $m / n \\ge 2$ when the\nsignal is real-valued. \n\n"}
{"id": "1705.05804", "contents": "Title: The Incremental Multiresolution Matrix Factorization Algorithm Abstract: Multiresolution analysis and matrix factorization are foundational tools in\ncomputer vision. In this work, we study the interface between these two\ndistinct topics and obtain techniques to uncover hierarchical block structure\nin symmetric matrices -- an important aspect in the success of many vision\nproblems. Our new algorithm, the incremental multiresolution matrix\nfactorization, uncovers such structure one feature at a time, and hence scales\nwell to large matrices. We describe how this multiscale analysis goes much\nfarther than what a direct global factorization of the data can identify. We\nevaluate the efficacy of the resulting factorizations for relative leveraging\nwithin regression tasks using medical imaging data. We also use the\nfactorization on representations learned by popular deep networks, providing\nevidence of their ability to infer semantic relationships even when they are\nnot explicitly trained to do so. We show that this algorithm can be used as an\nexploratory tool to improve the network architecture, and within numerous other\nsettings in vision. \n\n"}
{"id": "1705.06524", "contents": "Title: A fully dense and globally consistent 3D map reconstruction approach for\n  GI tract to enhance therapeutic relevance of the endoscopic capsule robot Abstract: In the gastrointestinal (GI) tract endoscopy field, ingestible wireless\ncapsule endoscopy is emerging as a novel, minimally invasive diagnostic\ntechnology for inspection of the GI tract and diagnosis of a wide range of\ndiseases and pathologies. Since the development of this technology, medical\ndevice companies and many research groups have made substantial progress in\nconverting passive capsule endoscopes to robotic active capsule endoscopes with\nmost of the functionality of current active flexible endoscopes. However,\nrobotic capsule endoscopy still has some challenges. In particular, the use of\nsuch devices to generate a precise three-dimensional (3D) mapping of the entire\ninner organ remains an unsolved problem. Such global 3D maps of inner organs\nwould help doctors to detect the location and size of diseased areas more\naccurately and intuitively, thus permitting more reliable diagnoses. To our\nknowledge, this paper presents the first complete pipeline for a complete 3D\nvisual map reconstruction of the stomach. The proposed pipeline is modular and\nincludes a preprocessing module, an image registration module, and a final\nshape-from-shading-based 3D reconstruction module; the 3D map is primarily\ngenerated by a combination of image stitching and shape-from-shading\ntechniques, and is updated in a frame-by-frame iterative fashion via capsule\nmotion inside the stomach. A comprehensive quantitative analysis of the\nproposed 3D reconstruction method is performed using an esophagus gastro\nduodenoscopy simulator, three different endoscopic cameras, and a 3D optical\nscanner. \n\n"}
{"id": "1705.06676", "contents": "Title: MUTAN: Multimodal Tucker Fusion for Visual Question Answering Abstract: Bilinear models provide an appealing framework for mixing and merging\ninformation in Visual Question Answering (VQA) tasks. They help to learn high\nlevel associations between question meaning and visual concepts in the image,\nbut they suffer from huge dimensionality issues. We introduce MUTAN, a\nmultimodal tensor-based Tucker decomposition to efficiently parametrize\nbilinear interactions between visual and textual representations. Additionally\nto the Tucker framework, we design a low-rank matrix-based decomposition to\nexplicitly constrain the interaction rank. With MUTAN, we control the\ncomplexity of the merging scheme while keeping nice interpretable fusion\nrelations. We show how our MUTAN model generalizes some of the latest VQA\narchitectures, providing state-of-the-art results. \n\n"}
{"id": "1705.07137", "contents": "Title: Deep De-Aliasing for Fast Compressive Sensing MRI Abstract: Fast Magnetic Resonance Imaging (MRI) is highly in demand for many clinical\napplications in order to reduce the scanning cost and improve the patient\nexperience. This can also potentially increase the image quality by reducing\nthe motion artefacts and contrast washout. However, once an image field of view\nand the desired resolution are chosen, the minimum scanning time is normally\ndetermined by the requirement of acquiring sufficient raw data to meet the\nNyquist-Shannon sampling criteria. Compressive Sensing (CS) theory has been\nperfectly matched to the MRI scanning sequence design with much less required\nraw data for the image reconstruction. Inspired by recent advances in deep\nlearning for solving various inverse problems, we propose a conditional\nGenerative Adversarial Networks-based deep learning framework for de-aliasing\nand reconstructing MRI images from highly undersampled data with great promise\nto accelerate the data acquisition process. By coupling an innovative content\nloss with the adversarial loss our de-aliasing results are more realistic.\nFurthermore, we propose a refinement learning procedure for training the\ngenerator network, which can stabilise the training with fast convergence and\nless parameter tuning. We demonstrate that the proposed framework outperforms\nstate-of-the-art CS-MRI methods, in terms of reconstruction error and\nperceptual image quality. In addition, our method can reconstruct each image in\n0.22ms--0.37ms, which is promising for real-time applications. \n\n"}
{"id": "1705.07384", "contents": "Title: Balanced Policy Evaluation and Learning Abstract: We present a new approach to the problems of evaluating and learning\npersonalized decision policies from observational data of past contexts,\ndecisions, and outcomes. Only the outcome of the enacted decision is available\nand the historical policy is unknown. These problems arise in personalized\nmedicine using electronic health records and in internet advertising. Existing\napproaches use inverse propensity weighting (or, doubly robust versions) to\nmake historical outcome (or, residual) data look like it were generated by a\nnew policy being evaluated or learned. But this relies on a plug-in approach\nthat rejects data points with a decision that disagrees with the new policy,\nleading to high variance estimates and ineffective learning. We propose a new,\nbalance-based approach that too makes the data look like the new policy but\ndoes so directly by finding weights that optimize for balance between the\nweighted data and the target policy in the given, finite sample, which is\nequivalent to minimizing worst-case or posterior conditional mean square error.\nOur policy learner proceeds as a two-level optimization problem over policies\nand weights. We demonstrate that this approach markedly outperforms existing\nones both in evaluation and learning, which is unsurprising given the wider\nsupport of balance-based weights. We establish extensive theoretical\nconsistency guarantees and regret bounds that support this empirical success. \n\n"}
{"id": "1705.09422", "contents": "Title: Text-Independent Speaker Verification Using 3D Convolutional Neural\n  Networks Abstract: In this paper, a novel method using 3D Convolutional Neural Network (3D-CNN)\narchitecture has been proposed for speaker verification in the text-independent\nsetting. One of the main challenges is the creation of the speaker models. Most\nof the previously-reported approaches create speaker models based on averaging\nthe extracted features from utterances of the speaker, which is known as the\nd-vector system. In our paper, we propose an adaptive feature learning by\nutilizing the 3D-CNNs for direct speaker model creation in which, for both\ndevelopment and enrollment phases, an identical number of spoken utterances per\nspeaker is fed to the network for representing the speakers' utterances and\ncreation of the speaker model. This leads to simultaneously capturing the\nspeaker-related information and building a more robust system to cope with\nwithin-speaker variation. We demonstrate that the proposed method significantly\noutperforms the traditional d-vector verification system. Moreover, the\nproposed system can also be an alternative to the traditional d-vector system\nwhich is a one-shot speaker modeling system by utilizing 3D-CNNs. \n\n"}
{"id": "1705.09451", "contents": "Title: Algorithmic clothing: hybrid recommendation, from street-style-to-shop Abstract: In this paper we detail Cortexica's (https://www.cortexica.com)\nrecommendation framework -- particularly, we describe how a hybrid visual\nrecommender system can be created by combining conditional random fields for\nsegmentation and deep neural networks for object localisation and feature\nrepresentation. The recommendation system that is built after localisation,\nsegmentation and classification has two properties -- first, it is knowledge\nbased in the sense that it learns pairwise preference/occurrence matrix by\nutilising knowledge from experts (images from fashion blogs) and second, it is\ncontent-based as it utilises a deep learning based framework for learning\nfeature representation. Such a construct is especially useful when there is a\nscarcity of user preference data, that forms the foundation of many\ncollaborative recommendation algorithms. \n\n"}
{"id": "1705.10284", "contents": "Title: Feature Incay for Representation Regularization Abstract: Softmax loss is widely used in deep neural networks for multi-class\nclassification, where each class is represented by a weight vector, a sample is\nrepresented as a feature vector, and the feature vector has the largest\nprojection on the weight vector of the correct category when the model\ncorrectly classifies a sample. To ensure generalization, weight decay that\nshrinks the weight norm is often used as regularizer. Different from\ntraditional learning algorithms where features are fixed and only weights are\ntunable, features are also tunable as representation learning in deep learning.\nThus, we propose feature incay to also regularize representation learning,\nwhich favors feature vectors with large norm when the samples can be correctly\nclassified. With the feature incay, feature vectors are further pushed away\nfrom the origin along the direction of their corresponding weight vectors,\nwhich achieves better inter-class separability. In addition, the proposed\nfeature incay encourages intra-class compactness along the directions of weight\nvectors by increasing the small feature norm faster than the large ones.\nEmpirical results on MNIST, CIFAR10 and CIFAR100 demonstrate feature incay can\nimprove the generalization ability. \n\n"}
{"id": "1705.10589", "contents": "Title: Jeffrey's prior sampling of deep sigmoidal networks Abstract: Neural networks have been shown to have a remarkable ability to uncover low\ndimensional structure in data: the space of possible reconstructed images form\na reduced model manifold in image space. We explore this idea directly by\nanalyzing the manifold learned by Deep Belief Networks and Stacked Denoising\nAutoencoders using Monte Carlo sampling. The model manifold forms an only\nslightly elongated hyperball with actual reconstructed data appearing\npredominantly on the boundaries of the manifold. In connection with the results\nwe present, we discuss problems of sampling high-dimensional manifolds as well\nas recent work [M. Transtrum, G. Hart, and P. Qiu, Submitted (2014)] discussing\nthe relation between high dimensional geometry and model reduction. \n\n"}
{"id": "1706.00051", "contents": "Title: Deep Generative Adversarial Networks for Compressed Sensing Automates\n  MRI Abstract: Magnetic resonance image (MRI) reconstruction is a severely ill-posed linear\ninverse task demanding time and resource intensive computations that can\nsubstantially trade off {\\it accuracy} for {\\it speed} in real-time imaging. In\naddition, state-of-the-art compressed sensing (CS) analytics are not cognizant\nof the image {\\it diagnostic quality}. To cope with these challenges we put\nforth a novel CS framework that permeates benefits from generative adversarial\nnetworks (GAN) to train a (low-dimensional) manifold of diagnostic-quality MR\nimages from historical patients. Leveraging a mixture of least-squares (LS)\nGANs and pixel-wise $\\ell_1$ cost, a deep residual network with skip\nconnections is trained as the generator that learns to remove the {\\it\naliasing} artifacts by projecting onto the manifold. LSGAN learns the texture\ndetails, while $\\ell_1$ controls the high-frequency noise. A multilayer\nconvolutional neural network is then jointly trained based on diagnostic\nquality images to discriminate the projection quality. The test phase performs\nfeed-forward propagation over the generator network that demands a very low\ncomputational overhead. Extensive evaluations are performed on a large\ncontrast-enhanced MR dataset of pediatric patients. In particular, images rated\nbased on expert radiologists corroborate that GANCS retrieves high contrast\nimages with detailed texture relative to conventional CS, and pixel-wise\nschemes. In addition, it offers reconstruction under a few milliseconds, two\norders of magnitude faster than state-of-the-art CS-MRI schemes. \n\n"}
{"id": "1706.00153", "contents": "Title: Cross-modal Common Representation Learning by Hybrid Transfer Network Abstract: DNN-based cross-modal retrieval is a research hotspot to retrieve across\ndifferent modalities as image and text, but existing methods often face the\nchallenge of insufficient cross-modal training data. In single-modal scenario,\nsimilar problem is usually relieved by transferring knowledge from large-scale\nauxiliary datasets (as ImageNet). Knowledge from such single-modal datasets is\nalso very useful for cross-modal retrieval, which can provide rich general\nsemantic information that can be shared across different modalities. However,\nit is challenging to transfer useful knowledge from single-modal (as image)\nsource domain to cross-modal (as image/text) target domain. Knowledge in source\ndomain cannot be directly transferred to both two different modalities in\ntarget domain, and the inherent cross-modal correlation contained in target\ndomain provides key hints for cross-modal retrieval which should be preserved\nduring transfer process. This paper proposes Cross-modal Hybrid Transfer\nNetwork (CHTN) with two subnetworks: Modal-sharing transfer subnetwork utilizes\nthe modality in both source and target domains as a bridge, for transferring\nknowledge to both two modalities simultaneously; Layer-sharing correlation\nsubnetwork preserves the inherent cross-modal semantic correlation to further\nadapt to cross-modal retrieval task. Cross-modal data can be converted to\ncommon representation by CHTN for retrieval, and comprehensive experiment on 3\ndatasets shows its effectiveness. \n\n"}
{"id": "1706.00631", "contents": "Title: Dual-reference Face Retrieval Abstract: Face retrieval has received much attention over the past few decades, and\nmany efforts have been made in retrieving face images against pose,\nillumination, and expression variations. However, the conventional works fail\nto meet the requirements of a potential and novel task --- retrieving a\nperson's face image at a specific age, especially when the specific 'age' is\nnot given as a numeral, i.e. 'retrieving someone's image at the similar age\nperiod shown by another person's image'. To tackle this problem, we propose a\ndual reference face retrieval framework in this paper, where the system takes\ntwo inputs: an identity reference image which indicates the target identity and\nan age reference image which reflects the target age. In our framework, the raw\nimages are first projected on a joint manifold, which preserves both the age\nand identity locality. Then two similarity metrics of age and identity are\nexploited and optimized by utilizing our proposed quartet-based model. The\nexperiments show promising results, outperforming hierarchical methods. \n\n"}
{"id": "1706.01789", "contents": "Title: Deep Alignment Network: A convolutional neural network for robust face\n  alignment Abstract: In this paper, we propose Deep Alignment Network (DAN), a robust face\nalignment method based on a deep neural network architecture. DAN consists of\nmultiple stages, where each stage improves the locations of the facial\nlandmarks estimated by the previous stage. Our method uses entire face images\nat all stages, contrary to the recently proposed face alignment methods that\nrely on local patches. This is possible thanks to the use of landmark heatmaps\nwhich provide visual information about landmark locations estimated at the\nprevious stages of the algorithm. The use of entire face images rather than\npatches allows DAN to handle face images with large variation in head pose and\ndifficult initializations. An extensive evaluation on two publicly available\ndatasets shows that DAN reduces the state-of-the-art failure rate by up to 70%.\nOur method has also been submitted for evaluation as part of the Menpo\nchallenge. \n\n"}
{"id": "1706.01912", "contents": "Title: Full Quantification of Left Ventricle via Deep Multitask Learning\n  Network Respecting Intra- and Inter-Task Relatedness Abstract: Cardiac left ventricle (LV) quantification is among the most clinically\nimportant tasks for identification and diagnosis of cardiac diseases, yet still\na challenge due to the high variability of cardiac structure and the complexity\nof temporal dynamics. Full quantification, i.e., to simultaneously quantify all\nLV indices including two areas (cavity and myocardium), six regional wall\nthicknesses (RWT), three LV dimensions, and one cardiac phase, is even more\nchallenging since the uncertain relatedness intra and inter each type of\nindices may hinder the learning procedure from better convergence and\ngeneralization. In this paper, we propose a newly-designed multitask learning\nnetwork (FullLVNet), which is constituted by a deep convolution neural network\n(CNN) for expressive feature embedding of cardiac structure; two followed\nparallel recurrent neural network (RNN) modules for temporal dynamic modeling;\nand four linear models for the final estimation. During the final estimation,\nboth intra- and inter-task relatedness are modeled to enforce improvement of\ngeneralization: 1) respecting intra-task relatedness, group lasso is applied to\neach of the regression tasks for sparse and common feature selection and\nconsistent prediction; 2) respecting inter-task relatedness, three phase-guided\nconstraints are proposed to penalize violation of the temporal behavior of the\nobtained LV indices. Experiments on MR sequences of 145 subjects show that\nFullLVNet achieves high accurate prediction with our intra- and inter-task\nrelatedness, leading to MAE of 190mm$^2$, 1.41mm, 2.68mm for average areas,\nRWT, dimensions and error rate of 10.4\\% for the phase classification. This\nendows our method a great potential in comprehensive clinical assessment of\nglobal, regional and dynamic cardiac function. \n\n"}
{"id": "1706.03646", "contents": "Title: Point Linking Network for Object Detection Abstract: Object detection is a core problem in computer vision. With the development\nof deep ConvNets, the performance of object detectors has been dramatically\nimproved. The deep ConvNets based object detectors mainly focus on regressing\nthe coordinates of bounding box, e.g., Faster-R-CNN, YOLO and SSD. Different\nfrom these methods that considering bounding box as a whole, we propose a novel\nobject bounding box representation using points and links and implemented using\ndeep ConvNets, termed as Point Linking Network (PLN). Specifically, we regress\nthe corner/center points of bounding-box and their links using a fully\nconvolutional network; then we map the corner points and their links back to\nmultiple bounding boxes; finally an object detection result is obtained by\nfusing the multiple bounding boxes. PLN is naturally robust to object occlusion\nand flexible to object scale variation and aspect ratio variation. In the\nexperiments, PLN with the Inception-v2 model achieves state-of-the-art\nsingle-model and single-scale results on the PASCAL VOC 2007, the PASCAL VOC\n2012 and the COCO detection benchmarks without bells and whistles. The source\ncode will be released. \n\n"}
{"id": "1706.03686", "contents": "Title: Image Crowd Counting Using Convolutional Neural Network and Markov\n  Random Field Abstract: In this paper, we propose a method called Convolutional Neural Network-Markov\nRandom Field (CNN-MRF) to estimate the crowd count in a still image. We first\ndivide the dense crowd visible image into overlapping patches and then use a\ndeep convolutional neural network to extract features from each patch image,\nfollowed by a fully connected neural network to regress the local patch crowd\ncount. Since the local patches have overlapping portions, the crowd count of\nthe adjacent patches has a high correlation. We use this correlation and the\nMarkov random field to smooth the counting results of the local patches.\nExperiments show that our approach significantly outperforms the\nstate-of-the-art methods on UCF and Shanghaitech crowd counting datasets. \n\n"}
{"id": "1706.04970", "contents": "Title: A convolutional autoencoder approach for mining features in cellular\n  electron cryo-tomograms and weakly supervised coarse segmentation Abstract: Cellular electron cryo-tomography enables the 3D visualization of cellular\norganization in the near-native state and at submolecular resolution. However,\nthe contents of cellular tomograms are often complex, making it difficult to\nautomatically isolate different in situ cellular components. In this paper, we\npropose a convolutional autoencoder-based unsupervised approach to provide a\ncoarse grouping of 3D small subvolumes extracted from tomograms. We demonstrate\nthat the autoencoder can be used for efficient and coarse characterization of\nfeatures of macromolecular complexes and surfaces, such as membranes. In\naddition, the autoencoder can be used to detect non-cellular features related\nto sample preparation and data collection, such as carbon edges from the grid\nand tomogram boundaries. The autoencoder is also able to detect patterns that\nmay indicate spatial interactions between cellular components. Furthermore, we\ndemonstrate that our autoencoder can be used for weakly supervised semantic\nsegmentation of cellular components, requiring a very small amount of manual\nannotation. \n\n"}
{"id": "1706.05208", "contents": "Title: Self-ensembling for visual domain adaptation Abstract: This paper explores the use of self-ensembling for visual domain adaptation\nproblems. Our technique is derived from the mean teacher variant (Tarvainen et\nal., 2017) of temporal ensembling (Laine et al;, 2017), a technique that\nachieved state of the art results in the area of semi-supervised learning. We\nintroduce a number of modifications to their approach for challenging domain\nadaptation scenarios and evaluate its effectiveness. Our approach achieves\nstate of the art results in a variety of benchmarks, including our winning\nentry in the VISDA-2017 visual domain adaptation challenge. In small image\nbenchmarks, our algorithm not only outperforms prior art, but can also achieve\naccuracy that is close to that of a classifier trained in a supervised fashion. \n\n"}
{"id": "1706.05671", "contents": "Title: Rate of convergence of the Nesterov accelerated gradient method in the\n  subcritical case $\\alpha \\leq 3$ Abstract: In a Hilbert space setting $\\mathcal H$, given $\\Phi: \\mathcal H \\to \\mathbb\nR$ a convex continuously differentiable function, and $\\alpha$ a positive\nparameter, we consider the inertial system with Asymptotic Vanishing Damping\n\\begin{equation*}\n  \\mbox{(AVD)}_{\\alpha} \\quad \\quad \\ddot{x}(t) + \\frac{\\alpha}{t} \\dot{x}(t) +\n\\nabla \\Phi (x(t)) =0. \\end{equation*}\n  Depending on the value of $ \\alpha $ with respect to 3, we give a complete\npicture of the convergence properties as $t \\to + \\infty$ of the trajectories\ngenerated by $\\mbox{(AVD)}_{\\alpha}$, as well as iterations of the\ncorresponding algorithms. Our main result concerns the subcritical case $\\alpha\n\\leq 3$, where we show that $\\Phi (x(t))-\\min \\Phi = \\mathcal O\n(t^{-\\frac{2}{3}\\alpha})$.\n  Then we examine the convergence of trajectories to optimal solutions. As a\nnew result, in the one-dimensional framework, for the critical value $\\alpha =\n3 $, we prove the convergence of the trajectories without any restrictive\nhypothesis on the convex function $\\Phi $. In the second part of this paper, we\nstudy the convergence properties of the associated forward-backward inertial\nalgorithms. They aim to solve structured convex minimization problems of the\nform $\\min \\left\\lbrace \\Theta:= \\Phi + \\Psi \\right\\rbrace$, with $\\Phi$ smooth\nand $\\Psi$ nonsmooth. The continuous dynamics serves as a guideline for this\nstudy. We obtain a similar rate of convergence for the sequence of iterates\n$(x_k)$: for $\\alpha \\leq 3$ we have $\\Theta (x_k)-\\min \\Theta = \\mathcal O\n(k^{-p})$ for all $p <\\frac{2\\alpha}{3}$ , and for $\\alpha > 3$ \\ $\\Theta\n(x_k)-\\min \\Theta = o (k^{-2})$ . We conclude this study by showing that the\nresults are robust with respect to external perturbations. \n\n"}
{"id": "1706.06905", "contents": "Title: Learnable pooling with Context Gating for video classification Abstract: Current methods for video analysis often extract frame-level features using\npre-trained convolutional neural networks (CNNs). Such features are then\naggregated over time e.g., by simple temporal averaging or more sophisticated\nrecurrent neural networks such as long short-term memory (LSTM) or gated\nrecurrent units (GRU). In this work we revise existing video representations\nand study alternative methods for temporal aggregation. We first explore\nclustering-based aggregation layers and propose a two-stream architecture\naggregating audio and visual features. We then introduce a learnable non-linear\nunit, named Context Gating, aiming to model interdependencies among network\nactivations. Our experimental results show the advantage of both improvements\nfor the task of video classification. In particular, we evaluate our method on\nthe large-scale multi-modal Youtube-8M v2 dataset and outperform all other\nmethods in the Youtube 8M Large-Scale Video Understanding challenge. \n\n"}
{"id": "1706.07156", "contents": "Title: Comparison of Time-Frequency Representations for Environmental Sound\n  Classification using Convolutional Neural Networks Abstract: Recent successful applications of convolutional neural networks (CNNs) to\naudio classification and speech recognition have motivated the search for\nbetter input representations for more efficient training. Visual displays of an\naudio signal, through various time-frequency representations such as\nspectrograms offer a rich representation of the temporal and spectral structure\nof the original signal. In this letter, we compare various popular signal\nprocessing methods to obtain this representation, such as short-time Fourier\ntransform (STFT) with linear and Mel scales, constant-Q transform (CQT) and\ncontinuous Wavelet transform (CWT), and assess their impact on the\nclassification performance of two environmental sound datasets using CNNs. This\nstudy supports the hypothesis that time-frequency representations are valuable\nin learning useful features for sound classification. Moreover, the actual\ntransformation used is shown to impact the classification accuracy, with\nMel-scaled STFT outperforming the other discussed methods slightly and baseline\nMFCC features to a large degree. Additionally, we observe that the optimal\nwindow size during transformation is dependent on the characteristics of the\naudio signal and architecturally, 2D convolution yielded better results in most\ncases compared to 1D. \n\n"}
{"id": "1706.07680", "contents": "Title: Training Adversarial Discriminators for Cross-channel Abnormal Event\n  Detection in Crowds Abstract: Abnormal crowd behaviour detection attracts a large interest due to its\nimportance in video surveillance scenarios. However, the ambiguity and the lack\nof sufficient abnormal ground truth data makes end-to-end training of large\ndeep networks hard in this domain. In this paper we propose to use Generative\nAdversarial Nets (GANs), which are trained to generate only the normal\ndistribution of the data. During the adversarial GAN training, a discriminator\n(D) is used as a supervisor for the generator network (G) and vice versa. At\ntesting time we use D to solve our discriminative task (abnormality detection),\nwhere D has been trained without the need of manually-annotated abnormal data.\nMoreover, in order to prevent G learn a trivial identity function, we use a\ncross-channel approach, forcing G to transform raw-pixel data in motion\ninformation and vice versa. The quantitative results on standard benchmarks\nshow that our method outperforms previous state-of-the-art methods in both the\nframe-level and the pixel-level evaluation. \n\n"}
{"id": "1706.07960", "contents": "Title: Encoding Video and Label Priors for Multi-label Video Classification on\n  YouTube-8M dataset Abstract: YouTube-8M is the largest video dataset for multi-label video classification.\nIn order to tackle the multi-label classification on this challenging dataset,\nit is necessary to solve several issues such as temporal modeling of videos,\nlabel imbalances, and correlations between labels. We develop a deep neural\nnetwork model, which consists of four components: the frame encoder, the\nclassification layer, the label processing layer, and the loss function. We\nintroduce our newly proposed methods and discusses how existing models operate\nin the YouTube-8M Classification Task, what insights they have, and why they\nsucceed (or fail) to achieve good performance. Most of the models we proposed\nare very high compared to the baseline models, and the ensemble of the models\nwe used is 8th in the Kaggle Competition. \n\n"}
{"id": "1706.08124", "contents": "Title: Scalable multimodal convolutional networks for brain tumour segmentation Abstract: Brain tumour segmentation plays a key role in computer-assisted surgery. Deep\nneural networks have increased the accuracy of automatic segmentation\nsignificantly, however these models tend to generalise poorly to different\nimaging modalities than those for which they have been designed, thereby\nlimiting their applications. For example, a network architecture initially\ndesigned for brain parcellation of monomodal T1 MRI can not be easily\ntranslated into an efficient tumour segmentation network that jointly utilises\nT1, T1c, Flair and T2 MRI. To tackle this, we propose a novel scalable\nmultimodal deep learning architecture using new nested structures that\nexplicitly leverage deep features within or across modalities. This aims at\nmaking the early layers of the architecture structured and sparse so that the\nfinal architecture becomes scalable to the number of modalities. We evaluate\nthe scalable architecture for brain tumour segmentation and give evidence of\nits regularisation effect compared to the conventional concatenation approach. \n\n"}
{"id": "1706.08126", "contents": "Title: ToolNet: Holistically-Nested Real-Time Segmentation of Robotic Surgical\n  Tools Abstract: Real-time tool segmentation from endoscopic videos is an essential part of\nmany computer-assisted robotic surgical systems and of critical importance in\nrobotic surgical data science. We propose two novel deep learning architectures\nfor automatic segmentation of non-rigid surgical instruments. Both methods take\nadvantage of automated deep-learning-based multi-scale feature extraction while\ntrying to maintain an accurate segmentation quality at all resolutions. The two\nproposed methods encode the multi-scale constraint inside the network\narchitecture. The first proposed architecture enforces it by cascaded\naggregation of predictions and the second proposed network does it by means of\na holistically-nested architecture where the loss at each scale is taken into\naccount for the optimization process. As the proposed methods are for real-time\nsemantic labeling, both present a reduced number of parameters. We propose the\nuse of parametric rectified linear units for semantic labeling in these small\narchitectures to increase the regularization ability of the design and maintain\nthe segmentation accuracy without overfitting the training sets. We compare the\nproposed architectures against state-of-the-art fully convolutional networks.\nWe validate our methods using existing benchmark datasets, including ex vivo\ncases with phantom tissue and different robotic surgical instruments present in\nthe scene. Our results show a statistically significant improved Dice\nSimilarity Coefficient over previous instrument segmentation methods. We\nanalyze our design choices and discuss the key drivers for improving accuracy. \n\n"}
{"id": "1706.08218", "contents": "Title: YoTube: Searching Action Proposal via Recurrent and Static Regression\n  Networks Abstract: In this paper, we present YoTube-a novel network fusion framework for\nsearching action proposals in untrimmed videos, where each action proposal\ncorresponds to a spatialtemporal video tube that potentially locates one human\naction. Our method consists of a recurrent YoTube detector and a static YoTube\ndetector, where the recurrent YoTube explores the regression capability of RNN\nfor candidate bounding boxes predictions using learnt temporal dynamics and the\nstatic YoTube produces the bounding boxes using rich appearance cues in a\nsingle frame. Both networks are trained using rgb and optical flow in order to\nfully exploit the rich appearance, motion and temporal context, and their\noutputs are fused to produce accurate and robust proposal boxes. Action\nproposals are finally constructed by linking these boxes using dynamic\nprogramming with a novel trimming method to handle the untrimmed video\neffectively and efficiently. Extensive experiments on the challenging UCF-101\nand UCF-Sports datasets show that our proposed technique obtains superior\nperformance compared with the state-of-the-art. \n\n"}
{"id": "1706.08482", "contents": "Title: Deep Network Flow for Multi-Object Tracking Abstract: Data association problems are an important component of many computer vision\napplications, with multi-object tracking being one of the most prominent\nexamples. A typical approach to data association involves finding a graph\nmatching or network flow that minimizes a sum of pairwise association costs,\nwhich are often either hand-crafted or learned as linear functions of fixed\nfeatures. In this work, we demonstrate that it is possible to learn features\nfor network-flow-based data association via backpropagation, by expressing the\noptimum of a smoothed network flow problem as a differentiable function of the\npairwise association costs. We apply this approach to multi-object tracking\nwith a network flow formulation. Our experiments demonstrate that we are able\nto successfully learn all cost functions for the association problem in an\nend-to-end fashion, which outperform hand-crafted costs in all settings. The\nintegration and combination of various sources of inputs becomes easy and the\ncost functions can be learned entirely from data, alleviating tedious\nhand-designing of costs. \n\n"}
{"id": "1706.08789", "contents": "Title: Auto-Encoder Guided GAN for Chinese Calligraphy Synthesis Abstract: In this paper, we investigate the Chinese calligraphy synthesis problem:\nsynthesizing Chinese calligraphy images with specified style from standard\nfont(eg. Hei font) images (Fig. 1(a)). Recent works mostly follow the stroke\nextraction and assemble pipeline which is complex in the process and limited by\nthe effect of stroke extraction. We treat the calligraphy synthesis problem as\nan image-to-image translation problem and propose a deep neural network based\nmodel which can generate calligraphy images from standard font images directly.\nBesides, we also construct a large scale benchmark that contains various styles\nfor Chinese calligraphy synthesis. We evaluate our method as well as some\nbaseline methods on the proposed dataset, and the experimental results\ndemonstrate the effectiveness of our proposed model. \n\n"}
{"id": "1706.09318", "contents": "Title: Retinal Vessel Segmentation in Fundoscopic Images with Generative\n  Adversarial Networks Abstract: Retinal vessel segmentation is an indispensable step for automatic detection\nof retinal diseases with fundoscopic images. Though many approaches have been\nproposed, existing methods tend to miss fine vessels or allow false positives\nat terminal branches. Let alone under-segmentation, over-segmentation is also\nproblematic when quantitative studies need to measure the precise width of\nvessels. In this paper, we present a method that generates the precise map of\nretinal vessels using generative adversarial training. Our methods achieve dice\ncoefficient of 0.829 on DRIVE dataset and 0.834 on STARE dataset which is the\nstate-of-the-art performance on both datasets. \n\n"}
{"id": "1706.09719", "contents": "Title: Iterative Spectral Clustering for Unsupervised Object Localization Abstract: This paper addresses the problem of unsupervised object localization in an\nimage. Unlike previous supervised and weakly supervised algorithms that require\nbounding box or image level annotations for training classifiers in order to\nlearn features representing the object, we propose a simple yet effective\ntechnique for localization using iterative spectral clustering. This iterative\nspectral clustering approach along with appropriate cluster selection strategy\nin each iteration naturally helps in searching of object region in the image.\nIn order to estimate the final localization window, we group the proposals\nobtained from the iterative spectral clustering step based on the perceptual\nsimilarity, and average the coordinates of the proposals from the top scoring\ngroups. We benchmark our algorithm on challenging datasets like Object\nDiscovery and PASCAL VOC 2007, achieving an average CorLoc percentage of 51%\nand 35% respectively which is comparable to various other weakly supervised\nalgorithms despite being completely unsupervised. \n\n"}
{"id": "1706.09858", "contents": "Title: What's Mine is Yours: Pretrained CNNs for Limited Training Sonar ATR Abstract: Finding mines in Sonar imagery is a significant problem with a great deal of\nrelevance for seafaring military and commercial endeavors. Unfortunately, the\nlack of enormous Sonar image data sets has prevented automatic target\nrecognition (ATR) algorithms from some of the same advances seen in other\ncomputer vision fields. Namely, the boom in convolutional neural nets (CNNs)\nwhich have been able to achieve incredible results - even surpassing human\nactors - has not been an easily feasible route for many practitioners of Sonar\nATR. We demonstrate the power of one avenue to incorporating CNNs into Sonar\nATR: transfer learning. We first show how well a straightforward, flexible CNN\nfeature-extraction strategy can be used to obtain impressive if not\nstate-of-the-art results. Secondly, we propose a way to utilize the powerful\ntransfer learning approach towards multiple instance target detection and\nidentification within a provided synthetic aperture Sonar data set. \n\n"}
{"id": "1707.00251", "contents": "Title: Where to Play: Retrieval of Video Segments using Natural-Language\n  Queries Abstract: In this paper, we propose a new approach for retrieval of video segments\nusing natural language queries. Unlike most previous approaches such as\nconcept-based methods or rule-based structured models, the proposed method uses\nimage captioning model to construct sentential queries for visual information.\nIn detail, our approach exploits multiple captions generated by visual features\nin each image with `Densecap'. Then, the similarities between captions of\nadjacent images are calculated, which is used to track semantically similar\ncaptions over multiple frames. Besides introducing this novel idea of 'tracking\nby captioning', the proposed method is one of the first approaches that uses a\nlanguage generation model learned by neural networks to construct semantic\nquery describing the relations and properties of visual information. To\nevaluate the effectiveness of our approach, we have created a new evaluation\ndataset, which contains about 348 segments of scenes in 20 movie-trailers.\nThrough quantitative and qualitative evaluation, we show that our method is\neffective for retrieval of video segments using natural language queries. \n\n"}
{"id": "1707.00577", "contents": "Title: Generalization Properties of Doubly Stochastic Learning Algorithms Abstract: Doubly stochastic learning algorithms are scalable kernel methods that\nperform very well in practice. However, their generalization properties are not\nwell understood and their analysis is challenging since the corresponding\nlearning sequence may not be in the hypothesis space induced by the kernel. In\nthis paper, we provide an in-depth theoretical analysis for different variants\nof doubly stochastic learning algorithms within the setting of nonparametric\nregression in a reproducing kernel Hilbert space and considering the square\nloss. Particularly, we derive convergence results on the generalization error\nfor the studied algorithms either with or without an explicit penalty term. To\nthe best of our knowledge, the derived results for the unregularized variants\nare the first of this kind, while the results for the regularized variants\nimprove those in the literature. The novelties in our proof are a sample error\nbound that requires controlling the trace norm of a cumulative operator, and a\nrefined analysis of bounding initial error. \n\n"}
{"id": "1707.01086", "contents": "Title: Discriminative Localization in CNNs for Weakly-Supervised Segmentation\n  of Pulmonary Nodules Abstract: Automated detection and segmentation of pulmonary nodules on lung computed\ntomography (CT) scans can facilitate early lung cancer diagnosis. Existing\nsupervised approaches for automated nodule segmentation on CT scans require\nvoxel-based annotations for training, which are labor- and time-consuming to\nobtain. In this work, we propose a weakly-supervised method that generates\naccurate voxel-level nodule segmentation trained with image-level labels only.\nBy adapting a convolutional neural network (CNN) trained for image\nclassification, our proposed method learns discriminative regions from the\nactivation maps of convolution units at different scales, and identifies the\ntrue nodule location with a novel candidate-screening framework. Experimental\nresults on the public LIDC-IDRI dataset demonstrate that, our weakly-supervised\nnodule segmentation framework achieves competitive performance compared to a\nfully-supervised CNN-based segmentation method. \n\n"}
{"id": "1707.04642", "contents": "Title: Recognizing Abnormal Heart Sounds Using Deep Learning Abstract: The work presented here applies deep learning to the task of automated\ncardiac auscultation, i.e. recognizing abnormalities in heart sounds. We\ndescribe an automated heart sound classification algorithm that combines the\nuse of time-frequency heat map representations with a deep convolutional neural\nnetwork (CNN). Given the cost-sensitive nature of misclassification, our CNN\narchitecture is trained using a modified loss function that directly optimizes\nthe trade-off between sensitivity and specificity. We evaluated our algorithm\nat the 2016 PhysioNet Computing in Cardiology challenge where the objective was\nto accurately classify normal and abnormal heart sounds from single, short,\npotentially noisy recordings. Our entry to the challenge achieved a final\nspecificity of 0.95, sensitivity of 0.73 and overall score of 0.84. We achieved\nthe greatest specificity score out of all challenge entries and, using just a\nsingle CNN, our algorithm differed in overall score by only 0.02 compared to\nthe top place finisher, which used an ensemble approach. \n\n"}
{"id": "1707.04791", "contents": "Title: Non-Asymptotic Analysis of Robust Control from Coarse-Grained\n  Identification Abstract: This work explores the trade-off between the number of samples required to\naccurately build models of dynamical systems and the degradation of performance\nin various control objectives due to a coarse approximation. In particular, we\nshow that simple models can be easily fit from input/output data and are\nsufficient for achieving various control objectives. We derive bounds on the\nnumber of noisy input/output samples from a stable linear time-invariant system\nthat are sufficient to guarantee that the corresponding finite impulse response\napproximation is close to the true system in the $\\mathcal{H}_\\infty$-norm. We\ndemonstrate that these demands are lower than those derived in prior art which\naimed to accurately identify dynamical models. We also explore how different\nphysical input constraints, such as power constraints, affect the sample\ncomplexity. Finally, we show how our analysis fits within the established\nframework of robust control, by demonstrating how a controller designed for an\napproximate system provably meets performance objectives on the true system. \n\n"}
{"id": "1707.05911", "contents": "Title: Recognizing and Curating Photo Albums via Event-Specific Image\n  Importance Abstract: Automatic organization of personal photos is a problem with many real world\nap- plications, and can be divided into two main tasks: recognizing the event\ntype of the photo collection, and selecting interesting images from the\ncollection. In this paper, we attempt to simultaneously solve both tasks:\nalbum-wise event recognition and image- wise importance prediction. We\ncollected an album dataset with both event type labels and image importance\nlabels, refined from an existing CUFED dataset. We propose a hybrid system\nconsisting of three parts: A siamese network-based event-specific image\nimportance prediction, a Convolutional Neural Network (CNN) that recognizes the\nevent type, and a Long Short-Term Memory (LSTM)-based sequence level event\nrecognizer. We propose an iterative updating procedure for event type and image\nimportance score prediction. We experimentally verified that image importance\nscore prediction and event type recognition can each help the performance of\nthe other. \n\n"}
{"id": "1707.06029", "contents": "Title: Supervising Neural Attention Models for Video Captioning by Human Gaze\n  Data Abstract: The attention mechanisms in deep neural networks are inspired by human's\nattention that sequentially focuses on the most relevant parts of the\ninformation over time to generate prediction output. The attention parameters\nin those models are implicitly trained in an end-to-end manner, yet there have\nbeen few trials to explicitly incorporate human gaze tracking to supervise the\nattention models. In this paper, we investigate whether attention models can\nbenefit from explicit human gaze labels, especially for the task of video\ncaptioning. We collect a new dataset called VAS, consisting of movie clips, and\ncorresponding multiple descriptive sentences along with human gaze tracking\ndata. We propose a video captioning model named Gaze Encoding Attention Network\n(GEAN) that can leverage gaze tracking information to provide the spatial and\ntemporal attention for sentence generation. Through evaluation of language\nsimilarity metrics and human assessment via Amazon mechanical Turk, we\ndemonstrate that spatial attentions guided by human gaze data indeed improve\nthe performance of multiple captioning methods. Moreover, we show that the\nproposed approach achieves the state-of-the-art performance for both gaze\nprediction and video captioning not only in our VAS dataset but also in\nstandard datasets (e.g. LSMDC and Hollywood2). \n\n"}
{"id": "1707.06399", "contents": "Title: Adaptive Feeding: Achieving Fast and Accurate Detections by Adaptively\n  Combining Object Detectors Abstract: Object detection aims at high speed and accuracy simultaneously. However,\nfast models are usually less accurate, while accurate models cannot satisfy our\nneed for speed. A fast model can be 10 times faster but 50\\% less accurate than\nan accurate model. In this paper, we propose Adaptive Feeding (AF) to combine a\nfast (but less accurate) detector and an accurate (but slow) detector, by\nadaptively determining whether an image is easy or hard and choosing an\nappropriate detector for it. In practice, we build a cascade of detectors,\nincluding the AF classifier which make the easy vs. hard decision and the two\ndetectors. The AF classifier can be tuned to obtain different tradeoff between\nspeed and accuracy, which has negligible training time and requires no\nadditional training data. Experimental results on the PASCAL VOC, MS COCO and\nCaltech Pedestrian datasets confirm that AF has the ability to achieve\ncomparable speed as the fast detector and comparable accuracy as the accurate\none at the same time. As an example, by combining the fast SSD300 with the\naccurate SSD500 detector, AF leads to 50\\% speedup over SSD500 with the same\nprecision on the VOC2007 test set. \n\n"}
{"id": "1707.06825", "contents": "Title: Evaluation of Hashing Methods Performance on Binary Feature Descriptors Abstract: In this paper we evaluate performance of data-dependent hashing methods on\nbinary data. The goal is to find a hashing method that can effectively produce\nlower dimensional binary representation of 512-bit FREAK descriptors. A\nrepresentative sample of recent unsupervised, semi-supervised and supervised\nhashing methods was experimentally evaluated on large datasets of labelled\nbinary FREAK feature descriptors. \n\n"}
{"id": "1707.07074", "contents": "Title: What-and-Where to Match: Deep Spatially Multiplicative Integration\n  Networks for Person Re-identification Abstract: Matching pedestrians across disjoint camera views, known as person\nre-identification (re-id), is a challenging problem that is of importance to\nvisual recognition and surveillance. Most existing methods exploit local\nregions within spatial manipulation to perform matching in local\ncorrespondence. However, they essentially extract \\emph{fixed} representations\nfrom pre-divided regions for each image and perform matching based on the\nextracted representation subsequently. For models in this pipeline, local finer\npatterns that are crucial to distinguish positive pairs from negative ones\ncannot be captured, and thus making them underperformed. In this paper, we\npropose a novel deep multiplicative integration gating function, which answers\nthe question of \\emph{what-and-where to match} for effective person re-id. To\naddress \\emph{what} to match, our deep network emphasizes common local patterns\nby learning joint representations in a multiplicative way. The network\ncomprises two Convolutional Neural Networks (CNNs) to extract convolutional\nactivations, and generates relevant descriptors for pedestrian matching. This\nthus, leads to flexible representations for pair-wise images. To address\n\\emph{where} to match, we combat the spatial misalignment by performing\nspatially recurrent pooling via a four-directional recurrent neural network to\nimpose spatial dependency over all positions with respect to the entire image.\nThe proposed network is designed to be end-to-end trainable to characterize\nlocal pairwise feature interactions in a spatially aligned manner. To\ndemonstrate the superiority of our method, extensive experiments are conducted\nover three benchmark data sets: VIPeR, CUHK03 and Market-1501. \n\n"}
{"id": "1707.07248", "contents": "Title: Towards Good Practices for Deep 3D Hand Pose Estimation Abstract: 3D hand pose estimation from single depth image is an important and\nchallenging problem for human-computer interaction. Recently deep convolutional\nnetworks (ConvNet) with sophisticated design have been employed to address it,\nbut the improvement over traditional random forest based methods is not so\napparent. To exploit the good practice and promote the performance for hand\npose estimation, we propose a tree-structured Region Ensemble Network (REN) for\ndirectly 3D coordinate regression. It first partitions the last convolution\noutputs of ConvNet into several grid regions. The results from separate\nfully-connected (FC) regressors on each regions are then integrated by another\nFC layer to perform the estimation. By exploitation of several training\nstrategies including data augmentation and smooth $L_1$ loss, proposed REN can\nsignificantly improve the performance of ConvNet to localize hand joints. The\nexperimental results demonstrate that our approach achieves the best\nperformance among state-of-the-art algorithms on three public hand pose\ndatasets. We also experiment our methods on fingertip detection and human pose\ndatasets and obtain state-of-the-art accuracy. \n\n"}
{"id": "1707.07432", "contents": "Title: LV-ROVER: Lexicon Verified Recognizer Output Voting Error Reduction Abstract: Offline handwritten text line recognition is a hard task that requires both\nan efficient optical character recognizer and language model. Handwriting\nrecognition state of the art methods are based on Long Short Term Memory (LSTM)\nrecurrent neural networks (RNN) coupled with the use of linguistic knowledge.\nMost of the proposed approaches in the literature focus on improving one of the\ntwo components and use constraint, dedicated to a database lexicon. However,\nstate of the art performance is achieved by combining multiple optical models,\nand possibly multiple language models with the Recognizer Output Voting Error\nReduction (ROVER) framework. Though handwritten line recognition with ROVER has\nbeen implemented by combining only few recognizers because training multiple\ncomplete recognizers is hard. In this paper we propose a Lexicon Verified\nROVER: LV-ROVER, that has a reduce complexity compare to the original one and\nthat can combine hundreds of recognizers without language models. We achieve\nstate of the art for handwritten line text on the RIMES dataset. \n\n"}
{"id": "1707.07584", "contents": "Title: Joint Background Reconstruction and Foreground Segmentation via A\n  Two-stage Convolutional Neural Network Abstract: Foreground segmentation in video sequences is a classic topic in computer\nvision. Due to the lack of semantic and prior knowledge, it is difficult for\nexisting methods to deal with sophisticated scenes well. Therefore, in this\npaper, we propose an end-to-end two-stage deep convolutional neural network\n(CNN) framework for foreground segmentation in video sequences. In the first\nstage, a convolutional encoder-decoder sub-network is employed to reconstruct\nthe background images and encode rich prior knowledge of background scenes. In\nthe second stage, the reconstructed background and current frame are input into\na multi-channel fully-convolutional sub-network (MCFCN) for accurate foreground\nsegmentation. In the two-stage CNN, the reconstruction loss and segmentation\nloss are jointly optimized. The background images and foreground objects are\noutput simultaneously in an end-to-end way. Moreover, by incorporating the\nprior semantic knowledge of foreground and background in the pre-training\nprocess, our method could restrain the background noise and keep the integrity\nof foreground objects at the same time. Experiments on CDNet 2014 show that our\nmethod outperforms the state-of-the-art by 4.9%. \n\n"}
{"id": "1707.07890", "contents": "Title: Spatiotemporal Modeling for Crowd Counting in Videos Abstract: Region of Interest (ROI) crowd counting can be formulated as a regression\nproblem of learning a mapping from an image or a video frame to a crowd density\nmap. Recently, convolutional neural network (CNN) models have achieved\npromising results for crowd counting. However, even when dealing with video\ndata, CNN-based methods still consider each video frame independently, ignoring\nthe strong temporal correlation between neighboring frames. To exploit the\notherwise very useful temporal information in video sequences, we propose a\nvariant of a recent deep learning model called convolutional LSTM (ConvLSTM)\nfor crowd counting. Unlike the previous CNN-based methods, our method fully\ncaptures both spatial and temporal dependencies. Furthermore, we extend the\nConvLSTM model to a bidirectional ConvLSTM model which can access long-range\ninformation in both directions. Extensive experiments using four publicly\navailable datasets demonstrate the reliability of our approach and the\neffectiveness of incorporating temporal information to boost the accuracy of\ncrowd counting. In addition, we also conduct some transfer learning experiments\nto show that once our model is trained on one dataset, its learning experience\ncan be transferred easily to a new dataset which consists of only very few\nvideo frames for model adaptation. \n\n"}
{"id": "1707.08364", "contents": "Title: Deep Interactive Region Segmentation and Captioning Abstract: With recent innovations in dense image captioning, it is now possible to\ndescribe every object of the scene with a caption while objects are determined\nby bounding boxes. However, interpretation of such an output is not trivial due\nto the existence of many overlapping bounding boxes. Furthermore, in current\ncaptioning frameworks, the user is not able to involve personal preferences to\nexclude out of interest areas. In this paper, we propose a novel hybrid deep\nlearning architecture for interactive region segmentation and captioning where\nthe user is able to specify an arbitrary region of the image that should be\nprocessed. To this end, a dedicated Fully Convolutional Network (FCN) named\nLyncean FCN (LFCN) is trained using our special training data to isolate the\nUser Intention Region (UIR) as the output of an efficient segmentation. In\nparallel, a dense image captioning model is utilized to provide a wide variety\nof captions for that region. Then, the UIR will be explained with the caption\nof the best match bounding box. To the best of our knowledge, this is the first\nwork that provides such a comprehensive output. Our experiments show the\nsuperiority of the proposed approach over state-of-the-art interactive\nsegmentation methods on several well-known datasets. In addition, replacement\nof the bounding boxes with the result of the interactive segmentation leads to\na better understanding of the dense image captioning output as well as accuracy\nenhancement for the object detection in terms of Intersection over Union (IoU). \n\n"}
{"id": "1707.08926", "contents": "Title: Non-Coherent Detection for Diffusive Molecular Communications Abstract: We study non-coherent detection schemes for molecular communication (MC)\nsystems that do not require knowledge of the channel state information (CSI).\nIn particular, we first derive the optimal maximum likelihood (ML)\nmultiple-symbol (MS) detector for MC systems. As a special case of the optimal\nMS detector, we show that the optimal ML symbol-by-symbol (SS) detector can be\nequivalently written in the form of a threshold-based detector, where the\noptimal decision threshold is constant and depends only on the statistics of\nthe MC channel. The main challenge of the MS detector is the complexity\nassociated with the calculation of the optimal detection metric. To overcome\nthis issue, we propose an approximate MS detection metric which can be\nexpressed in closed form. To reduce complexity even further, we develop a\nnon-coherent decision-feedback (DF) detector and a suboptimal blind detector.\nFinally, we derive analytical expressions for the bit error rate (BER) of the\noptimal SS detector, as well as upper and lower bounds for the BER of the\noptimal MS detector. Simulation results confirm the analysis and reveal the\neffectiveness of the proposed optimal and suboptimal detection schemes compared\nto a benchmark scheme that assumes perfect CSI knowledge, particularly when the\nnumber of observations used for detection is sufficiently large. \n\n"}
{"id": "1708.00938", "contents": "Title: Associative Domain Adaptation Abstract: We propose associative domain adaptation, a novel technique for end-to-end\ndomain adaptation with neural networks, the task of inferring class labels for\nan unlabeled target domain based on the statistical properties of a labeled\nsource domain. Our training scheme follows the paradigm that in order to\neffectively derive class labels for the target domain, a network should produce\nstatistically domain invariant embeddings, while minimizing the classification\nerror on the labeled source domain. We accomplish this by reinforcing\nassociations between source and target data directly in embedding space. Our\nmethod can easily be added to any existing classification network with no\nstructural and almost no computational overhead. We demonstrate the\neffectiveness of our approach on various benchmarks and achieve\nstate-of-the-art results across the board with a generic convolutional neural\nnetwork architecture not specifically tuned to the respective tasks. Finally,\nwe show that the proposed association loss produces embeddings that are more\neffective for domain adaptation compared to methods employing maximum mean\ndiscrepancy as a similarity measure in embedding space. \n\n"}
{"id": "1708.01022", "contents": "Title: When Kernel Methods meet Feature Learning: Log-Covariance Network for\n  Action Recognition from Skeletal Data Abstract: Human action recognition from skeletal data is a hot research topic and\nimportant in many open domain applications of computer vision, thanks to\nrecently introduced 3D sensors. In the literature, naive methods simply\ntransfer off-the-shelf techniques from video to the skeletal representation.\nHowever, the current state-of-the-art is contended between to different\nparadigms: kernel-based methods and feature learning with (recurrent) neural\nnetworks. Both approaches show strong performances, yet they exhibit heavy, but\ncomplementary, drawbacks. Motivated by this fact, our work aims at combining\ntogether the best of the two paradigms, by proposing an approach where a\nshallow network is fed with a covariance representation. Our intuition is that,\nas long as the dynamics is effectively modeled, there is no need for the\nclassification network to be deep nor recurrent in order to score favorably. We\nvalidate this hypothesis in a broad experimental analysis over 6 publicly\navailable datasets. \n\n"}
{"id": "1708.01072", "contents": "Title: A Sparse Completely Positive Relaxation of the Modularity Maximization\n  for Community Detection Abstract: In this paper, we consider the community detection problem under either the\nstochastic block model (SBM) assumption or the degree-correlated stochastic\nblock model (DCSBM) assumption. The modularity maximization formulation for the\ncommunity detection problem is NP-hard in general. In this paper, we propose a\nsparse and low-rank completely positive relaxation for the modularity\nmaximization problem, we then develop an efficient row-by-row (RBR) type block\ncoordinate descent (BCD) algorithm to solve the relaxation and prove an\n$\\mathcal{O}(1/\\sqrt{N})$ convergence rate to a stationary point where $N$ is\nthe number of iterations. A fast rounding scheme is constructed to retrieve the\ncommunity structure from the solution. Non-asymptotic high probability bounds\non the misclassification rate are established to justify our approach. We\nfurther develop an asynchronous parallel RBR algorithm to speed up the\nconvergence. Extensive numerical experiments on both synthetic and real world\nnetworks show that the proposed approach enjoys advantages in both clustering\naccuracy and numerical efficiency. Our numerical results indicate that the\nnewly proposed method is a quite competitive alternative for community\ndetection on sparse networks with over 50 million nodes. \n\n"}
{"id": "1708.01204", "contents": "Title: Improved Speech Reconstruction from Silent Video Abstract: Speechreading is the task of inferring phonetic information from visually\nobserved articulatory facial movements, and is a notoriously difficult task for\nhumans to perform. In this paper we present an end-to-end model based on a\nconvolutional neural network (CNN) for generating an intelligible and\nnatural-sounding acoustic speech signal from silent video frames of a speaking\nperson. We train our model on speakers from the GRID and TCD-TIMIT datasets,\nand evaluate the quality and intelligibility of reconstructed speech using\ncommon objective measurements. We show that speech predictions from the\nproposed model attain scores which indicate significantly improved quality over\nexisting models. In addition, we show promising results towards reconstructing\nspeech from an unconstrained dictionary. \n\n"}
{"id": "1708.01231", "contents": "Title: Optimal constants for a non-local approximation of Sobolev norms and\n  total variation Abstract: We consider the family of non-local and non-convex functionals proposed and\ninvestigated by J. Bourgain, H. Brezis and H.-M. Nguyen in a series of papers\nof the last decade. It was known that this family of functionals\nGamma-converges to a suitable multiple of the Sobolev norm or the total\nvariation, depending on the summability exponent, but the exact constants and\nthe structure of recovery families were still unknown, even in dimension one.\n  We prove a Gamma-convergence result with explicit values of the constants in\nany space dimension. We also show the existence of recovery families consisting\nof smooth functions with compact support.\n  The key point is reducing the problem first to dimension one, and then to a\nfinite combinatorial rearrangement inequality. \n\n"}
{"id": "1708.01241", "contents": "Title: DSOD: Learning Deeply Supervised Object Detectors from Scratch Abstract: We present Deeply Supervised Object Detector (DSOD), a framework that can\nlearn object detectors from scratch. State-of-the-art object objectors rely\nheavily on the off-the-shelf networks pre-trained on large-scale classification\ndatasets like ImageNet, which incurs learning bias due to the difference on\nboth the loss functions and the category distributions between classification\nand detection tasks. Model fine-tuning for the detection task could alleviate\nthis bias to some extent but not fundamentally. Besides, transferring\npre-trained models from classification to detection between discrepant domains\nis even more difficult (e.g. RGB to depth images). A better solution to tackle\nthese two critical problems is to train object detectors from scratch, which\nmotivates our proposed DSOD. Previous efforts in this direction mostly failed\ndue to much more complicated loss functions and limited training data in object\ndetection. In DSOD, we contribute a set of design principles for training\nobject detectors from scratch. One of the key findings is that deep\nsupervision, enabled by dense layer-wise connections, plays a critical role in\nlearning a good detector. Combining with several other principles, we develop\nDSOD following the single-shot detection (SSD) framework. Experiments on PASCAL\nVOC 2007, 2012 and MS COCO datasets demonstrate that DSOD can achieve better\nresults than the state-of-the-art solutions with much more compact models. For\ninstance, DSOD outperforms SSD on all three benchmarks with real-time detection\nspeed, while requires only 1/2 parameters to SSD and 1/10 parameters to Faster\nRCNN. Our code and models are available at: https://github.com/szq0214/DSOD . \n\n"}
{"id": "1708.01566", "contents": "Title: Augmented Reality Meets Computer Vision : Efficient Data Generation for\n  Urban Driving Scenes Abstract: The success of deep learning in computer vision is based on availability of\nlarge annotated datasets. To lower the need for hand labeled images, virtually\nrendered 3D worlds have recently gained popularity. Creating realistic 3D\ncontent is challenging on its own and requires significant human effort. In\nthis work, we propose an alternative paradigm which combines real and synthetic\ndata for learning semantic instance segmentation and object detection models.\nExploiting the fact that not all aspects of the scene are equally important for\nthis task, we propose to augment real-world imagery with virtual objects of the\ntarget category. Capturing real-world images at large scale is easy and cheap,\nand directly provides real background appearances without the need for creating\ncomplex 3D models of the environment. We present an efficient procedure to\naugment real images with virtual objects. This allows us to create realistic\ncomposite images which exhibit both realistic background appearance and a large\nnumber of complex object arrangements. In contrast to modeling complete 3D\nenvironments, our augmentation approach requires only a few user interactions\nin combination with 3D shapes of the target object. Through extensive\nexperimentation, we conclude the right set of parameters to produce augmented\ndata which can maximally enhance the performance of instance segmentation\nmodels. Further, we demonstrate the utility of our approach on training\nstandard deep models for semantic instance segmentation and object detection of\ncars in outdoor driving scenes. We test the models trained on our augmented\ndata on the KITTI 2015 dataset, which we have annotated with pixel-accurate\nground truth, and on Cityscapes dataset. Our experiments demonstrate that\nmodels trained on augmented imagery generalize better than those trained on\nsynthetic data or models trained on limited amount of annotated real data. \n\n"}
{"id": "1708.02349", "contents": "Title: Temporal Context Network for Activity Localization in Videos Abstract: We present a Temporal Context Network (TCN) for precise temporal localization\nof human activities. Similar to the Faster-RCNN architecture, proposals are\nplaced at equal intervals in a video which span multiple temporal scales. We\npropose a novel representation for ranking these proposals. Since pooling\nfeatures only inside a segment is not sufficient to predict activity\nboundaries, we construct a representation which explicitly captures context\naround a proposal for ranking it. For each temporal segment inside a proposal,\nfeatures are uniformly sampled at a pair of scales and are input to a temporal\nconvolutional neural network for classification. After ranking proposals,\nnon-maximum suppression is applied and classification is performed to obtain\nfinal detections. TCN outperforms state-of-the-art methods on the ActivityNet\ndataset and the THUMOS14 dataset. \n\n"}
{"id": "1708.02371", "contents": "Title: Successive Quadratic Upper-Bounding for Discrete Mean-Risk Minimization\n  and Network Interdiction Abstract: The advances in conic optimization have led to its increased utilization for\nmodeling data uncertainty. In particular, conic mean-risk optimization gained\nprominence in probabilistic and robust optimization. Whereas the corresponding\nconic models are solved efficiently over convex sets, their discrete\ncounterparts are intractable. In this paper, we give a highly effective\nsuccessive quadratic upper-bounding procedure for discrete mean-risk\nminimization problems. The procedure is based on a reformulation of the\nmean-risk problem through the perspective of its convex quadratic term.\nComputational experiments conducted on the network interdiction problem with\nstochastic capacities show that the proposed approach yields solutions within\n1-2% of optimality in a small fraction of the time required by exact search\nalgorithms. We demonstrate the value of the proposed approach for constructing\nefficient frontiers of flow-at-risk vs. interdiction cost for varying\nconfidence levels. \n\n"}
{"id": "1708.02659", "contents": "Title: Gramian Tensor Decomposition via Semidefinite Programming Abstract: In this paper we examine a symmetric tensor decomposition problem, the\nGramian decomposition, posed as a rank minimization problem. We study the\nrelaxation of the problem and consider cases when the relaxed solution is a\nsolution to the original problem. In some instances of tensor rank and order,\nwe prove generically that the solution to the relaxation will be optimal in the\noriginal. In other cases, we present interesting examples and approaches that\ndemonstrate the intricacy of this problem. \n\n"}
{"id": "1708.02932", "contents": "Title: SUBIC: A supervised, structured binary code for image search Abstract: For large-scale visual search, highly compressed yet meaningful\nrepresentations of images are essential. Structured vector quantizers based on\nproduct quantization and its variants are usually employed to achieve such\ncompression while minimizing the loss of accuracy. Yet, unlike binary hashing\nschemes, these unsupervised methods have not yet benefited from the\nsupervision, end-to-end learning and novel architectures ushered in by the deep\nlearning revolution. We hence propose herein a novel method to make deep\nconvolutional neural networks produce supervised, compact, structured binary\ncodes for visual search. Our method makes use of a novel block-softmax\nnon-linearity and of batch-based entropy losses that together induce structure\nin the learned encodings. We show that our method outperforms state-of-the-art\ncompact representations based on deep hashing or structured quantization in\nsingle and cross-domain category retrieval, instance retrieval and\nclassification. We make our code and models publicly available online. \n\n"}
{"id": "1708.04366", "contents": "Title: Deep Edge-Aware Saliency Detection Abstract: There has been profound progress in visual saliency thanks to the deep\nlearning architectures, however, there still exist three major challenges that\nhinder the detection performance for scenes with complex compositions, multiple\nsalient objects, and salient objects of diverse scales. In particular, output\nmaps of the existing methods remain low in spatial resolution causing blurred\nedges due to the stride and pooling operations, networks often neglect\ndescriptive statistical and handcrafted priors that have potential to\ncomplement saliency detection results, and deep features at different layers\nstay mainly desolate waiting to be effectively fused to handle multi-scale\nsalient objects. In this paper, we tackle these issues by a new fully\nconvolutional neural network that jointly learns salient edges and saliency\nlabels in an end-to-end fashion. Our framework first employs convolutional\nlayers that reformulate the detection task as a dense labeling problem, then\nintegrates handcrafted saliency features in a hierarchical manner into lower\nand higher levels of the deep network to leverage available information for\nmulti-scale response, and finally refines the saliency map through dilated\nconvolutions by imposing context. In this way, the salient edge priors are\nefficiently incorporated and the output resolution is significantly improved\nwhile keeping the memory requirements low, leading to cleaner and sharper\nobject boundaries. Extensive experimental analyses on ten benchmarks\ndemonstrate that our framework achieves consistently superior performance and\nattains robustness for complex scenes in comparison to the very recent\nstate-of-the-art approaches. \n\n"}
{"id": "1708.04672", "contents": "Title: DeformNet: Free-Form Deformation Network for 3D Shape Reconstruction\n  from a Single Image Abstract: 3D reconstruction from a single image is a key problem in multiple\napplications ranging from robotic manipulation to augmented reality. Prior\nmethods have tackled this problem through generative models which predict 3D\nreconstructions as voxels or point clouds. However, these methods can be\ncomputationally expensive and miss fine details. We introduce a new\ndifferentiable layer for 3D data deformation and use it in DeformNet to learn a\nmodel for 3D reconstruction-through-deformation. DeformNet takes an image\ninput, searches the nearest shape template from a database, and deforms the\ntemplate to match the query image. We evaluate our approach on the ShapeNet\ndataset and show that - (a) the Free-Form Deformation layer is a powerful new\nbuilding block for Deep Learning models that manipulate 3D data (b) DeformNet\nuses this FFD layer combined with shape retrieval for smooth and\ndetail-preserving 3D reconstruction of qualitatively plausible point clouds\nwith respect to a single query image (c) compared to other state-of-the-art 3D\nreconstruction methods, DeformNet quantitatively matches or outperforms their\nbenchmarks by significant margins. For more information, visit:\nhttps://deformnet-site.github.io/DeformNet-website/ . \n\n"}
{"id": "1708.04811", "contents": "Title: Language Identification Using Deep Convolutional Recurrent Neural\n  Networks Abstract: Language Identification (LID) systems are used to classify the spoken\nlanguage from a given audio sample and are typically the first step for many\nspoken language processing tasks, such as Automatic Speech Recognition (ASR)\nsystems. Without automatic language detection, speech utterances cannot be\nparsed correctly and grammar rules cannot be applied, causing subsequent speech\nrecognition steps to fail. We propose a LID system that solves the problem in\nthe image domain, rather than the audio domain. We use a hybrid Convolutional\nRecurrent Neural Network (CRNN) that operates on spectrogram images of the\nprovided audio snippets. In extensive experiments we show, that our model is\napplicable to a range of noisy scenarios and can easily be extended to\npreviously unknown languages, while maintaining its classification accuracy. We\nrelease our code and a large scale training set for LID systems to the\ncommunity. \n\n"}
{"id": "1708.05851", "contents": "Title: Image2song: Song Retrieval via Bridging Image Content and Lyric Words Abstract: Image is usually taken for expressing some kinds of emotions or purposes,\nsuch as love, celebrating Christmas. There is another better way that combines\nthe image and relevant song to amplify the expression, which has drawn much\nattention in the social network recently. Hence, the automatic selection of\nsongs should be expected. In this paper, we propose to retrieve semantic\nrelevant songs just by an image query, which is named as the image2song\nproblem. Motivated by the requirements of establishing correlation in\nsemantic/content, we build a semantic-based song retrieval framework, which\nlearns the correlation between image content and lyric words. This model uses a\nconvolutional neural network to generate rich tags from image regions, a\nrecurrent neural network to model lyric, and then establishes correlation via a\nmulti-layer perceptron. To reduce the content gap between image and lyric, we\npropose to make the lyric modeling focus on the main image content via a tag\nattention. We collect a dataset from the social-sharing multimodal data to\nstudy the proposed problem, which consists of (image, music clip, lyric)\ntriplets. We demonstrate that our proposed model shows noticeable results in\nthe image2song retrieval task and provides suitable songs. Besides, the\nsong2image task is also performed. \n\n"}
{"id": "1708.06012", "contents": "Title: Product Matrix Minimum Storage Regenerating Codes with Flexible Number\n  of Helpers Abstract: In coding for distributed storage systems, efficient data reconstruction and\nrepair through accessing a predefined number of arbitrarily chosen storage\nnodes is guaranteed by regenerating codes. Traditionally, code parameters,\nspecially the number of helper nodes participating in a repair process, are\npredetermined. However, depending on the state of the system and network\ntraffic, it is desirable to adapt such parameters accordingly in order to\nminimize the cost of repair. In this work a class of regenerating codes with\nminimum storage is introduced that can simultaneously operate at the optimal\nrepair bandwidth, for a wide range of exact repair mechanisms, based on\ndifferent number of helper nodes. \n\n"}
{"id": "1708.06500", "contents": "Title: Sparsity Invariant CNNs Abstract: In this paper, we consider convolutional neural networks operating on sparse\ninputs with an application to depth upsampling from sparse laser scan data.\nFirst, we show that traditional convolutional networks perform poorly when\napplied to sparse data even when the location of missing data is provided to\nthe network. To overcome this problem, we propose a simple yet effective sparse\nconvolution layer which explicitly considers the location of missing data\nduring the convolution operation. We demonstrate the benefits of the proposed\nnetwork architecture in synthetic and real experiments with respect to various\nbaseline approaches. Compared to dense baselines, the proposed sparse\nconvolution network generalizes well to novel datasets and is invariant to the\nlevel of sparsity in the data. For our evaluation, we derive a novel dataset\nfrom the KITTI benchmark, comprising 93k depth annotated RGB images. Our\ndataset allows for training and evaluating depth upsampling and depth\nprediction techniques in challenging real-world settings and will be made\navailable upon publication. \n\n"}
{"id": "1708.06734", "contents": "Title: Representation Learning by Learning to Count Abstract: We introduce a novel method for representation learning that uses an\nartificial supervision signal based on counting visual primitives. This\nsupervision signal is obtained from an equivariance relation, which does not\nrequire any manual annotation. We relate transformations of images to\ntransformations of the representations. More specifically, we look for the\nrepresentation that satisfies such relation rather than the transformations\nthat match a given representation. In this paper, we use two image\ntransformations in the context of counting: scaling and tiling. The first\ntransformation exploits the fact that the number of visual primitives should be\ninvariant to scale. The second transformation allows us to equate the total\nnumber of visual primitives in each tile to that in the whole image. These two\ntransformations are combined in one constraint and used to train a neural\nnetwork with a contrastive loss. The proposed task produces representations\nthat perform on par or exceed the state of the art in transfer learning\nbenchmarks. \n\n"}
{"id": "1708.07089", "contents": "Title: Predicting Aesthetic Score Distribution through Cumulative\n  Jensen-Shannon Divergence Abstract: Aesthetic quality prediction is a challenging task in the computer vision\ncommunity because of the complex interplay with semantic contents and\nphotographic technologies. Recent studies on the powerful deep learning based\naesthetic quality assessment usually use a binary high-low label or a numerical\nscore to represent the aesthetic quality. However the scalar representation\ncannot describe well the underlying varieties of the human perception of\naesthetics. In this work, we propose to predict the aesthetic score\ndistribution (i.e., a score distribution vector of the ordinal basic human\nratings) using Deep Convolutional Neural Network (DCNN). Conventional DCNNs\nwhich aim to minimize the difference between the predicted scalar numbers or\nvectors and the ground truth cannot be directly used for the ordinal basic\nrating distribution. Thus, a novel CNN based on the Cumulative distribution\nwith Jensen-Shannon divergence (CJS-CNN) is presented to predict the aesthetic\nscore distribution of human ratings, with a new reliability-sensitive learning\nmethod based on the kurtosis of the score distribution, which eliminates the\nrequirement of the original full data of human ratings (without normalization).\nExperimental results on large scale aesthetic dataset demonstrate the\neffectiveness of our introduced CJS-CNN in this task. \n\n"}
{"id": "1708.08190", "contents": "Title: A Probabilistic Quality Representation Approach to Deep Blind Image\n  Quality Prediction Abstract: Blind image quality assessment (BIQA) remains a very challenging problem due\nto the unavailability of a reference image. Deep learning based BIQA methods\nhave been attracting increasing attention in recent years, yet it remains a\ndifficult task to train a robust deep BIQA model because of the very limited\nnumber of training samples with human subjective scores. Most existing methods\nlearn a regression network to minimize the prediction error of a scalar image\nquality score. However, such a scheme ignores the fact that an image will\nreceive divergent subjective scores from different subjects, which cannot be\nadequately represented by a single scalar number. This is particularly true on\ncomplex, real-world distorted images. Moreover, images may broadly differ in\ntheir distributions of assigned subjective scores. Recognizing this, we propose\na new representation of perceptual image quality, called probabilistic quality\nrepresentation (PQR), to describe the image subjective score distribution,\nwhereby a more robust loss function can be employed to train a deep BIQA model.\nThe proposed PQR method is shown to not only speed up the convergence of deep\nmodel training, but to also greatly improve the achievable level of quality\nprediction accuracy relative to scalar quality score regression methods. The\nsource code is available at https://github.com/HuiZeng/BIQA_Toolbox. \n\n"}
{"id": "1708.09006", "contents": "Title: Pix2face: Direct 3D Face Model Estimation Abstract: An efficient, fully automatic method for 3D face shape and pose estimation in\nunconstrained 2D imagery is presented. The proposed method jointly estimates a\ndense set of 3D landmarks and facial geometry using a single pass of a modified\nversion of the popular \"U-Net\" neural network architecture. Additionally, we\npropose a method for directly estimating a set of 3D Morphable Model (3DMM)\nparameters, using the estimated 3D landmarks and geometry as constraints in a\nsimple linear system. Qualitative modeling results are presented, as well as\nquantitative evaluation of predicted 3D face landmarks in unconstrained video\nsequences. \n\n"}
{"id": "1708.09105", "contents": "Title: Simultaneously Color-Depth Super-Resolution with Conditional Generative\n  Adversarial Network Abstract: Recently, Generative Adversarial Network (GAN) has been found wide\napplications in style transfer, image-to-image translation and image\nsuper-resolution. In this paper, a color-depth conditional GAN is proposed to\nconcurrently resolve the problems of depth super-resolution and color\nsuper-resolution in 3D videos. Firstly, given the low-resolution depth image\nand low-resolution color image, a generative network is proposed to leverage\nmutual information of color image and depth image to enhance each other in\nconsideration of the geometry structural dependency of color-depth image in the\nsame scene. Secondly, three loss functions, including data loss, total\nvariation loss, and 8-connected gradient difference loss are introduced to\ntrain this generative network in order to keep generated images close to the\nreal ones, in addition to the adversarial loss. Experimental results\ndemonstrate that the proposed approach produces high-quality color image and\ndepth image from low-quality image pair, and it is superior to several other\nleading methods. Besides, we use the same neural network framework to resolve\nthe problem of image smoothing and edge detection at the same time. \n\n"}
{"id": "1708.09522", "contents": "Title: Action Classification and Highlighting in Videos Abstract: Inspired by recent advances in neural machine translation, that jointly align\nand translate using encoder-decoder networks equipped with attention, we\npropose an attentionbased LSTM model for human activity recognition. Our model\njointly learns to classify actions and highlight frames associated with the\naction, by attending to salient visual information through a jointly learned\nsoft-attention networks. We explore attention informed by various forms of\nvisual semantic features, including those encoding actions, objects and scenes.\nWe qualitatively show that soft-attention can learn to effectively attend to\nimportant objects and scene information correlated with specific human actions.\nFurther, we show that, quantitatively, our attention-based LSTM outperforms the\nvanilla LSTM and CNN models used by stateof-the-art methods. On a large-scale\nyoutube video dataset, ActivityNet, our model outperforms competing methods in\naction classification. \n\n"}
{"id": "1708.09585", "contents": "Title: ICDAR2017 Competition on Reading Chinese Text in the Wild (RCTW-17) Abstract: Chinese is the most widely used language in the world. Algorithms that read\nChinese text in natural images facilitate applications of various kinds.\nDespite the large potential value, datasets and competitions in the past\nprimarily focus on English, which bares very different characteristics than\nChinese. This report introduces RCTW, a new competition that focuses on Chinese\ntext reading. The competition features a large-scale dataset with 12,263\nannotated images. Two tasks, namely text localization and end-to-end\nrecognition, are set up. The competition took place from January 20 to May 31,\n2017. 23 valid submissions were received from 19 teams. This report includes\ndataset description, task definitions, evaluation protocols, and results\nsummaries and analysis. Through this competition, we call for more future\nresearch on the Chinese text reading problem. The official website for the\ncompetition is http://rctw.vlrlab.net \n\n"}
{"id": "1709.00308", "contents": "Title: A Comprehensive Survey of Deep Learning in Remote Sensing: Theories,\n  Tools and Challenges for the Community Abstract: In recent years, deep learning (DL), a re-branding of neural networks (NNs),\nhas risen to the top in numerous areas, namely computer vision (CV), speech\nrecognition, natural language processing, etc. Whereas remote sensing (RS)\npossesses a number of unique challenges, primarily related to sensors and\napplications, inevitably RS draws from many of the same theories as CV; e.g.,\nstatistics, fusion, and machine learning, to name a few. This means that the RS\ncommunity should be aware of, if not at the leading edge of, of advancements\nlike DL. Herein, we provide the most comprehensive survey of state-of-the-art\nRS DL research. We also review recent new developments in the DL field that can\nbe used in DL for RS. Namely, we focus on theories, tools and challenges for\nthe RS community. Specifically, we focus on unsolved challenges and\nopportunities as it relates to (i) inadequate data sets, (ii)\nhuman-understandable solutions for modelling physical phenomena, (iii) Big\nData, (iv) non-traditional heterogeneous data sources, (v) DL architectures and\nlearning algorithms for spectral, spatial and temporal data, (vi) transfer\nlearning, (vii) an improved theoretical understanding of DL systems, (viii)\nhigh barriers to entry, and (ix) training and optimizing the DL. \n\n"}
{"id": "1709.01421", "contents": "Title: Multi-label Class-imbalanced Action Recognition in Hockey Videos via 3D\n  Convolutional Neural Networks Abstract: Automatic analysis of the video is one of most complex problems in the fields\nof computer vision and machine learning. A significant part of this research\ndeals with (human) activity recognition (HAR) since humans, and the activities\nthat they perform, generate most of the video semantics. Video-based HAR has\napplications in various domains, but one of the most important and challenging\nis HAR in sports videos. Some of the major issues include high inter- and\nintra-class variations, large class imbalance, the presence of both group\nactions and single player actions, and recognizing simultaneous actions, i.e.,\nthe multi-label learning problem. Keeping in mind these challenges and the\nrecent success of CNNs in solving various computer vision problems, in this\nwork, we implement a 3D CNN based multi-label deep HAR system for multi-label\nclass-imbalanced action recognition in hockey videos. We test our system for\ntwo different scenarios: an ensemble of $k$ binary networks vs. a single\n$k$-output network, on a publicly available dataset. We also compare our\nresults with the system that was originally designed for the chosen dataset.\nExperimental results show that the proposed approach performs better than the\nexisting solution. \n\n"}
{"id": "1709.02043", "contents": "Title: The Mating Rituals of Deep Neural Networks: Learning Compact Feature\n  Representations through Sexual Evolutionary Synthesis Abstract: Evolutionary deep intelligence was recently proposed as a method for\nachieving highly efficient deep neural network architectures over successive\ngenerations. Drawing inspiration from nature, we propose the incorporation of\nsexual evolutionary synthesis. Rather than the current asexual synthesis of\nnetworks, we aim to produce more compact feature representations by\nsynthesizing more diverse and generalizable offspring networks in subsequent\ngenerations via the combination of two parent networks. Experimental results\nwere obtained using the MNIST and CIFAR-10 datasets, and showed improved\narchitectural efficiency and comparable testing accuracy relative to the\nbaseline asexual evolutionary neural networks. In particular, the network\nsynthesized via sexual evolutionary synthesis for MNIST had approximately\ndouble the architectural efficiency (cluster efficiency of 34.29X and synaptic\nefficiency of 258.37X) in comparison to the network synthesized via asexual\nevolutionary synthesis, with both networks achieving a testing accuracy of\n~97%. \n\n"}
{"id": "1709.02917", "contents": "Title: Sublinear-Time Algorithms for Compressive Phase Retrieval Abstract: In the compressive phase retrieval problem, or phaseless compressed sensing,\nor compressed sensing from intensity only measurements, the goal is to\nreconstruct a sparse or approximately $k$-sparse vector $x \\in \\mathbb{R}^n$\ngiven access to $y= |\\Phi x|$, where $|v|$ denotes the vector obtained from\ntaking the absolute value of $v\\in\\mathbb{R}^n$ coordinate-wise. In this paper\nwe present sublinear-time algorithms for different variants of the compressive\nphase retrieval problem which are akin to the variants considered for the\nclassical compressive sensing problem in theoretical computer science. Our\nalgorithms use pure combinatorial techniques and near-optimal number of\nmeasurements. \n\n"}
{"id": "1709.03009", "contents": "Title: How to Train a CAT: Learning Canonical Appearance Transformations for\n  Direct Visual Localization Under Illumination Change Abstract: Direct visual localization has recently enjoyed a resurgence in popularity\nwith the increasing availability of cheap mobile computing power. The\ncompetitive accuracy and robustness of these algorithms compared to\nstate-of-the-art feature-based methods, as well as their natural ability to\nyield dense maps, makes them an appealing choice for a variety of mobile\nrobotics applications. However, direct methods remain brittle in the face of\nappearance change due to their underlying assumption of photometric\nconsistency, which is commonly violated in practice. In this paper, we propose\nto mitigate this problem by training deep convolutional encoder-decoder models\nto transform images of a scene such that they correspond to a previously-seen\ncanonical appearance. We validate our method in multiple environments and\nillumination conditions using high-fidelity synthetic RGB-D datasets, and\nintegrate the trained models into a direct visual localization pipeline,\nyielding improvements in visual odometry (VO) accuracy through time-varying\nillumination conditions, as well as improved metric relocalization performance\nunder illumination change, where conventional methods normally fail. We further\nprovide a preliminary investigation of transfer learning from synthetic to real\nenvironments in a localization context. An open-source implementation of our\nmethod using PyTorch is available at https://github.com/utiasSTARS/cat-net. \n\n"}
{"id": "1709.03541", "contents": "Title: Robust period estimation using mutual information for multi-band light\n  curves in the synoptic survey era Abstract: The Large Synoptic Survey Telescope (LSST) will produce an unprecedented\namount of light curves using six optical bands. Robust and efficient methods\nthat can aggregate data from multidimensional sparsely-sampled time series are\nneeded. In this paper we present a new method for light curve period estimation\nbased on the quadratic mutual information (QMI). The proposed method does not\nassume a particular model for the light curve nor its underlying probability\ndensity and it is robust to non-Gaussian noise and outliers. By combining the\nQMI from several bands the true period can be estimated even when no\nsingle-band QMI yields the period. Period recovery performance as a function of\naverage magnitude and sample size is measured using 30,000 synthetic multi-band\nlight curves of RR Lyrae and Cepheid variables generated by the LSST Operations\nand Catalog simulators. The results show that aggregating information from\nseveral bands is highly beneficial in LSST sparsely-sampled time series,\nobtaining an absolute increase in period recovery rate up to 50%. We also show\nthat the QMI is more robust to noise and light curve length (sample size) than\nthe multiband generalizations of the Lomb Scargle and Analysis of Variance\nperiodograms, recovering the true period in 10-30% more cases than its\ncompetitors. A python package containing efficient Cython implementations of\nthe QMI and other methods is provided. \n\n"}
{"id": "1709.04060", "contents": "Title: Streamlined Deployment for Quantized Neural Networks Abstract: Running Deep Neural Network (DNN) models on devices with limited\ncomputational capability is a challenge due to large compute and memory\nrequirements. Quantized Neural Networks (QNNs) have emerged as a potential\nsolution to this problem, promising to offer most of the DNN accuracy benefits\nwith much lower computational cost. However, harvesting these benefits on\nexisting mobile CPUs is a challenge since operations on highly quantized\ndatatypes are not natively supported in most instruction set architectures\n(ISAs). In this work, we first describe a streamlining flow to convert all QNN\ninference operations to integer ones. Afterwards, we provide techniques based\non processing one bit position at a time (bit-serial) to show how QNNs can be\nefficiently deployed using common bitwise operations. We demonstrate the\npotential of QNNs on mobile CPUs with microbenchmarks and on a quantized\nAlexNet, which is 3.5x faster than an optimized 8-bit baseline. Our bit-serial\nmatrix multiplication library is available on GitHub at https://git.io/vhshn \n\n"}
{"id": "1709.04215", "contents": "Title: Long time behavior of the master equation in mean-field game theory Abstract: Mean Field Game (MFG) systems describe equilibrium configurations in games\nwith infinitely many interacting controllers. We are interested in the behavior\nof this system as the horizon becomes large, or as the discount factor tends to\n$0$. We show that, in the two cases, the asymptotic behavior of the Mean Field\nGame system is strongly related with the long time behavior of the so-called\nmaster equation and with the vanishing discount limit of the discounted master\nequation, respectively. Both equations are nonlinear transport equations in the\nspace of measures. We prove the existence of a solution to an ergodic master\nequation, towards which the time-dependent master equation converges as the\nhorizon becomes large, and towards which the discounted master equation\nconverges as the discount factor tends to $0$. The whole analysis is based on\nthe obtention of new estimates for the exponential rates of convergence of the\ntime-dependent MFG system and the discounted MFG system. \n\n"}
{"id": "1709.04496", "contents": "Title: An Exploration of 2D and 3D Deep Learning Techniques for Cardiac MR\n  Image Segmentation Abstract: Accurate segmentation of the heart is an important step towards evaluating\ncardiac function. In this paper, we present a fully automated framework for\nsegmentation of the left (LV) and right (RV) ventricular cavities and the\nmyocardium (Myo) on short-axis cardiac MR images. We investigate various 2D and\n3D convolutional neural network architectures for this task. We investigate the\nsuitability of various state-of-the art 2D and 3D convolutional neural network\narchitectures, as well as slight modifications thereof, for this task.\nExperiments were performed on the ACDC 2017 challenge training dataset\ncomprising cardiac MR images of 100 patients, where manual reference\nsegmentations were made available for end-diastolic (ED) and end-systolic (ES)\nframes. We find that processing the images in a slice-by-slice fashion using 2D\nnetworks is beneficial due to a relatively large slice thickness. However, the\nexact network architecture only plays a minor role. We report mean Dice\ncoefficients of $0.950$ (LV), $0.893$ (RV), and $0.899$ (Myo), respectively\nwith an average evaluation time of 1.1 seconds per volume on a modern GPU. \n\n"}
{"id": "1709.05436", "contents": "Title: Scene-centric Joint Parsing of Cross-view Videos Abstract: Cross-view video understanding is an important yet under-explored area in\ncomputer vision. In this paper, we introduce a joint parsing framework that\nintegrates view-centric proposals into scene-centric parse graphs that\nrepresent a coherent scene-centric understanding of cross-view scenes. Our key\nobservations are that overlapping fields of views embed rich appearance and\ngeometry correlations and that knowledge fragments corresponding to individual\nvision tasks are governed by consistency constraints available in commonsense\nknowledge. The proposed joint parsing framework represents such correlations\nand constraints explicitly and generates semantic scene-centric parse graphs.\nQuantitative experiments show that scene-centric predictions in the parse graph\noutperform view-centric predictions. \n\n"}
{"id": "1709.05746", "contents": "Title: Adversarial Discriminative Sim-to-real Transfer of Visuo-motor Policies Abstract: Various approaches have been proposed to learn visuo-motor policies for\nreal-world robotic applications. One solution is first learning in simulation\nthen transferring to the real world. In the transfer, most existing approaches\nneed real-world images with labels. However, the labelling process is often\nexpensive or even impractical in many robotic applications. In this paper, we\npropose an adversarial discriminative sim-to-real transfer approach to reduce\nthe cost of labelling real data. The effectiveness of the approach is\ndemonstrated with modular networks in a table-top object reaching task where a\n7 DoF arm is controlled in velocity mode to reach a blue cuboid in clutter\nthrough visual observations. The adversarial transfer approach reduced the\nlabelled real data requirement by 50%. Policies can be transferred to real\nenvironments with only 93 labelled and 186 unlabelled real images. The\ntransferred visuo-motor policies are robust to novel (not seen in training)\nobjects in clutter and even a moving target, achieving a 97.8% success rate and\n1.8 cm control accuracy. \n\n"}
{"id": "1709.06248", "contents": "Title: Look Wider to Match Image Patches with Convolutional Neural Networks Abstract: When a human matches two images, the viewer has a natural tendency to view\nthe wide area around the target pixel to obtain clues of right correspondence.\nHowever, designing a matching cost function that works on a large window in the\nsame way is difficult. The cost function is typically not intelligent enough to\ndiscard the information irrelevant to the target pixel, resulting in\nundesirable artifacts. In this paper, we propose a novel learn a stereo\nmatching cost with a large-sized window. Unlike conventional pooling layers\nwith strides, the proposed per-pixel pyramid-pooling layer can cover a large\narea without a loss of resolution and detail. Therefore, the learned matching\ncost function can successfully utilize the information from a large area\nwithout introducing the fattening effect. The proposed method is robust despite\nthe presence of weak textures, depth discontinuity, illumination, and exposure\ndifference. The proposed method achieves near-peak performance on the\nMiddlebury benchmark. \n\n"}
{"id": "1709.07944", "contents": "Title: MR Acquisition-Invariant Representation Learning Abstract: Voxelwise classification approaches are popular and effective methods for\ntissue quantification in brain magnetic resonance imaging (MRI) scans. However,\ngeneralization of these approaches is hampered by large differences between\nsets of MRI scans such as differences in field strength, vendor or acquisition\nprotocols. Due to this acquisition related variation, classifiers trained on\ndata from a specific scanner fail or under-perform when applied to data that\nwas acquired differently. In order to address this lack of generalization, we\npropose a Siamese neural network (MRAI-net) to learn a representation that\nminimizes the between-scanner variation, while maintaining the contrast between\nbrain tissues necessary for brain tissue quantification. The proposed MRAI-net\nwas evaluated on both simulated and real MRI data. After learning the MR\nacquisition invariant representation, any supervised classification model that\nuses feature vectors can be applied. In this paper, we provide a proof of\nprinciple, which shows that a linear classifier applied on the MRAI\nrepresentation is able to outperform supervised convolutional neural network\nclassifiers for tissue classification when little target training data is\navailable. \n\n"}
{"id": "1709.09049", "contents": "Title: From a monotone probabilistic scheme to a probabilistic max-plus\n  algorithm for solving Hamilton-Jacobi-Bellman equations Abstract: In a previous work (Akian, Fodjo, 2016), we introduced a lower complexity\nprobabilistic max-plus numerical method for solving fully nonlinear\nHamilton-Jacobi-Bellman equations associated to diffusion control problems\ninvolving a finite set-valued (or switching) control and possibly a\ncontinuum-valued control. This method was based on the idempotent expansion\nproperties obtained by McEneaney, Kaise and Han (2011) and on the numerical\nprobabilistic method proposed by Fahim, Touzi and Warin (2011) for solving some\nfully nonlinear parabolic partial differential equations. A difficulty of the\nalgorithm of Fahim, Touzi and Warin is in the critical constraints imposed on\nthe Hamiltonian to ensure the monotonicity of the scheme, hence the convergence\nof the algorithm. Here, we propose a new \"probabilistic scheme\" which is\nmonotone under rather weak assumptions, including the case of strongly elliptic\nPDE with bounded coefficients. This allows us to apply our probabilistic\nmax-plus method in more general situations. We illustrate this on the\nevaluation of the superhedging price of an option under uncertain correlation\nmodel with several underlying stocks and changing sign cross gamma, and\nconsider in particular the case of 5 stocks leading to a PDE in dimension 5. \n\n"}
{"id": "1710.00772", "contents": "Title: Optimal Resource Allocation in Ultra-low Power Fog-computing SWIPT-based\n  Networks Abstract: In this paper, we consider a fog computing system consisting of a\nmulti-antenna access point (AP), an ultra-low power (ULP) single antenna device\nand a fog server. The ULP device is assumed to be capable of both energy\nharvesting (EH) and information decoding (ID) using a time-switching\nsimultaneous wireless information and power transfer (SWIPT) scheme. The ULP\ndevice deploys the harvested energy for ID and either local computing or\noffloading the computations to the fog server depending on which strategy is\nmost energy efficient. In this scenario, we optimize the time slots devoted to\nEH, ID and local computation as well as the time slot and power required for\nthe offloading to minimize the energy cost of the ULP device. Numerical results\nare provided to study the effectiveness of the optimized fog computing system\nand the relevant challenges. \n\n"}
{"id": "1710.01103", "contents": "Title: Isotropic and Steerable Wavelets in N Dimensions. A multiresolution\n  analysis framework for ITK Abstract: This document describes the implementation of the external module\nITKIsotropicWavelets, a multiresolution (MRA) analysis framework using\nisotropic and steerable wavelets in the frequency domain. This framework\nprovides the backbone for state of the art filters for denoising, feature\ndetection or phase analysis in N-dimensions. It focus on reusability, and\nhighly decoupled modules for easy extension and implementation of new filters,\nand it contains a filter for multiresolution phase analysis,\n  The backbone of the multi-scale analysis is provided by an isotropic\nband-limited wavelet pyramid, and the detection of directional features is\nprovided by coupling the pyramid with a generalized Riesz transform. The\ngeneralized Riesz transform of order N behaves like a smoothed version of the\nNth order derivatives of the signal. Also, it is steerable: its components\nimpulse responses can be rotated to any spatial orientation, reducing\ncomputation time when detecting directional features. \n\n"}
{"id": "1710.01168", "contents": "Title: Fast Fine-grained Image Classification via Weakly Supervised\n  Discriminative Localization Abstract: Fine-grained image classification is to recognize hundreds of subcategories\nin each basic-level category. Existing methods employ discriminative\nlocalization to find the key distinctions among subcategories. However, they\ngenerally have two limitations: (1) Discriminative localization relies on\nregion proposal methods to hypothesize the locations of discriminative regions,\nwhich are time-consuming. (2) The training of discriminative localization\ndepends on object or part annotations, which are heavily labor-consuming. It is\nhighly challenging to address the two key limitations simultaneously, and\nexisting methods only focus on one of them. Therefore, we propose a weakly\nsupervised discriminative localization approach (WSDL) for fast fine-grained\nimage classification to address the two limitations at the same time, and its\nmain advantages are: (1) n-pathway end-to-end discriminative localization\nnetwork is designed to improve classification speed, which simultaneously\nlocalizes multiple different discriminative regions for one image to boost\nclassification accuracy, and shares full-image convolutional features generated\nby region proposal network to accelerate the process of generating region\nproposals as well as reduce the computation of convolutional operation. (2)\nMulti-level attention guided localization learning is proposed to localize\ndiscriminative regions with different focuses automatically, without using\nobject and part annotations, avoiding the labor consumption. Different level\nattentions focus on different characteristics of the image, which are\ncomplementary and boost the classification accuracy. Both are jointly employed\nto simultaneously improve classification speed and eliminate dependence on\nobject and part annotations. Compared with state-of-the-art methods on 2\nwidely-used fine-grained image classification datasets, our WSDL approach\nachieves the best performance. \n\n"}
{"id": "1710.01559", "contents": "Title: Monitoring tool usage in surgery videos using boosted convolutional and\n  recurrent neural networks Abstract: This paper investigates the automatic monitoring of tool usage during a\nsurgery, with potential applications in report generation, surgical training\nand real-time decision support. Two surgeries are considered: cataract surgery,\nthe most common surgical procedure, and cholecystectomy, one of the most common\ndigestive surgeries. Tool usage is monitored in videos recorded either through\na microscope (cataract surgery) or an endoscope (cholecystectomy). Following\nstate-of-the-art video analysis solutions, each frame of the video is analyzed\nby convolutional neural networks (CNNs) whose outputs are fed to recurrent\nneural networks (RNNs) in order to take temporal relationships between events\ninto account. Novelty lies in the way those CNNs and RNNs are trained.\nComputational complexity prevents the end-to-end training of \"CNN+RNN\" systems.\nTherefore, CNNs are usually trained first, independently from the RNNs. This\napproach is clearly suboptimal for surgical tool analysis: many tools are very\nsimilar to one another, but they can generally be differentiated based on past\nevents. CNNs should be trained to extract the most useful visual features in\ncombination with the temporal context. A novel boosting strategy is proposed to\nachieve this goal: the CNN and RNN parts of the system are simultaneously\nenriched by progressively adding weak classifiers (either CNNs or RNNs) trained\nto improve the overall classification accuracy. Experiments were performed in a\ndataset of 50 cataract surgery videos and a dataset of 80 cholecystectomy\nvideos. Very good classification performance are achieved in both datasets:\ntool usage could be labeled with an average area under the ROC curve of $A_z =\n0.9961$ and $A_z = 0.9939$, respectively, in offline mode (using past, present\nand future information), and $A_z = 0.9957$ and $A_z = 0.9936$, respectively,\nin online mode (using past and present information only). \n\n"}
{"id": "1710.02039", "contents": "Title: Integrating Boundary and Center Correlation Filters for Visual Tracking\n  with Aspect Ratio Variation Abstract: The aspect ratio variation frequently appears in visual tracking and has a\nsevere influence on performance. Although many correlation filter (CF)-based\ntrackers have also been suggested for scale adaptive tracking, few studies have\nbeen given to handle the aspect ratio variation for CF trackers. In this paper,\nwe make the first attempt to address this issue by introducing a family of 1D\nboundary CFs to localize the left, right, top, and bottom boundaries in videos.\nThis allows us cope with the aspect ratio variation flexibly during tracking.\nSpecifically, we present a novel tracking model to integrate 1D Boundary and 2D\nCenter CFs (IBCCF) where boundary and center filters are enforced by a\nnear-orthogonality regularization term. To optimize our IBCCF model, we develop\nan alternating direction method of multipliers. Experiments on several datasets\nshow that IBCCF can effectively handle aspect ratio variation, and achieves\nstate-of-the-art performance in terms of accuracy and robustness. \n\n"}
{"id": "1710.03023", "contents": "Title: An automatic deep learning approach for coronary artery calcium\n  segmentation Abstract: Coronary artery calcium (CAC) is a significant marker of atherosclerosis and\ncardiovascular events. In this work we present a system for the automatic\nquantification of calcium score in ECG-triggered non-contrast enhanced cardiac\ncomputed tomography (CT) images. The proposed system uses a supervised deep\nlearning algorithm, i.e. convolutional neural network (CNN) for the\nsegmentation and classification of candidate lesions as coronary or not,\npreviously extracted in the region of the heart using a cardiac atlas. We\ntrained our network with 45 CT volumes; 18 volumes were used to validate the\nmodel and 56 to test it. Individual lesions were detected with a sensitivity of\n91.24%, a specificity of 95.37% and a positive predicted value (PPV) of 90.5%;\ncomparing calcium score obtained by the system and calcium score manually\nevaluated by an expert operator, a Pearson coefficient of 0.983 was obtained. A\nhigh agreement (Cohen's k = 0.879) between manual and automatic risk prediction\nwas also observed. These results demonstrated that convolutional neural\nnetworks can be effectively applied for the automatic segmentation and\nclassification of coronary calcifications. \n\n"}
{"id": "1710.05133", "contents": "Title: Tracking Moving Agents via Inexact Online Gradient Descent Algorithm Abstract: Multi-agent systems are being increasingly deployed in challenging\nenvironments for performing complex tasks such as multi-target tracking,\nsearch-and-rescue, and intrusion detection. Notwithstanding the computational\nlimitations of individual robots, such systems rely on collaboration to sense\nand react to the environment. This paper formulates the generic target tracking\nproblem as a time-varying optimization problem and puts forth an inexact online\ngradient descent method for solving it sequentially. The performance of the\nproposed algorithm is studied by characterizing its dynamic regret, a notion\ncommon to the online learning literature. Building upon the existing results,\nwe provide improved regret rates that not only allow non-strongly convex costs\nbut also explicating the role of the cumulative gradient error. Two distinct\nclasses of problems are considered: one in which the objective function adheres\nto a quadratic growth condition, and another where the objective function is\nconvex but the variable belongs to a compact domain. For both cases, results\nare developed while allowing the error to be either adversarial or arising from\na white noise process. Further, the generality of the proposed framework is\ndemonstrated by developing online variants of existing stochastic gradient\nalgorithms and interpreting them as special cases of the proposed inexact\ngradient method. The efficacy of the proposed inexact gradient framework is\nestablished on a multi-agent multi-target tracking problem, while its\nflexibility is exemplified by generating online movie recommendations for\nMovielens $10$M dataset. \n\n"}
{"id": "1710.05285", "contents": "Title: CNNComparator: Comparative Analytics of Convolutional Neural Networks Abstract: Convolutional neural networks (CNNs) are widely used in many image\nrecognition tasks due to their extraordinary performance. However, training a\ngood CNN model can still be a challenging task. In a training process, a CNN\nmodel typically learns a large number of parameters over time, which usually\nresults in different performance. Often, it is difficult to explore the\nrelationships between the learned parameters and the model performance due to a\nlarge number of parameters and different random initializations. In this paper,\nwe present a visual analytics approach to compare two different snapshots of a\ntrained CNN model taken after different numbers of epochs, so as to provide\nsome insight into the design or the training of a better CNN model. Our system\ncompares snapshots by exploring the differences in operation parameters and the\ncorresponding blob data at different levels. A case study has been conducted to\ndemonstrate the effectiveness of our system. \n\n"}
{"id": "1710.06094", "contents": "Title: Multi-Tenant C-RAN With Spectrum Pooling: Downlink Optimization Under\n  Privacy Constraints Abstract: Spectrum pooling allows multiple operators, or tenants, to share the same\nfrequency bands. This work studies the optimization of spectrum pooling for the\ndownlink of a multi-tenant Cloud Radio Access Network (C-RAN) system in the\npresence of inter-tenant privacy constraints. The spectrum available for\ndownlink transmission is partitioned into private and shared subbands, and the\nparticipating operators cooperate to serve the user equipments (UEs) on the\nshared subband. The network of each operator consists of a cloud processor (CP)\nthat is connected to proprietary radio units (RUs) by means of finite-capacity\nfronthaul links. In order to enable interoperator cooperation, the CPs of the\nparticipating operators are also connected by finite-capacity backhaul links.\nInter-operator cooperation may hence result in loss of privacy. Fronthaul and\nbackhaul links are used to transfer quantized baseband signals. Standard\nquantization is considered first. Then, a novel approach based on the idea of\ncorrelating quantization noise signals across RUs of different operators is\nproposed to control the trade-off between distortion at UEs and inter-operator\nprivacy. The problem of optimizing the bandwidth allocation, precoding, and\nfronthaul/backhaul compression strategies is tackled under constraints on\nbackhaul and fronthaul capacity, as well as on per-RU transmit power and\ninter-operator privacy. For both cases, the optimization problems are tackled\nusing the concave convex procedure (CCCP), and extensive numerical results are\nprovided. \n\n"}
{"id": "1710.07084", "contents": "Title: Emerging from Water: Underwater Image Color Correction Based on Weakly\n  Supervised Color Transfer Abstract: Underwater vision suffers from severe effects due to selective attenuation\nand scattering when light propagates through water. Such degradation not only\naffects the quality of underwater images but limits the ability of vision\ntasks. Different from existing methods which either ignore the wavelength\ndependency of the attenuation or assume a specific spectral profile, we tackle\ncolor distortion problem of underwater image from a new view. In this letter,\nwe propose a weakly supervised color transfer method to correct color\ndistortion, which relaxes the need of paired underwater images for training and\nallows for the underwater images unknown where were taken. Inspired by\nCycle-Consistent Adversarial Networks, we design a multi-term loss function\nincluding adversarial loss, cycle consistency loss, and SSIM (Structural\nSimilarity Index Measure) loss, which allows the content and structure of the\ncorrected result the same as the input, but the color as if the image was taken\nwithout the water. Experiments on underwater images captured under diverse\nscenes show that our method produces visually pleasing results, even\noutperforms the art-of-the-state methods. Besides, our method can improve the\nperformance of vision tasks. \n\n"}
{"id": "1710.08011", "contents": "Title: ActivityNet Challenge 2017 Summary Abstract: The ActivityNet Large Scale Activity Recognition Challenge 2017 Summary:\nresults and challenge participants papers. \n\n"}
{"id": "1710.09490", "contents": "Title: Complete 3D Scene Parsing from an RGBD Image Abstract: One major goal of vision is to infer physical models of objects, surfaces,\nand their layout from sensors. In this paper, we aim to interpret indoor scenes\nfrom one RGBD image. Our representation encodes the layout of orthogonal walls\nand the extent of objects, modeled with CAD-like 3D shapes. We parse both the\nvisible and occluded portions of the scene and all observable objects,\nproducing a complete 3D parse. Such a scene interpretation is useful for\nrobotics and visual reasoning, but difficult to produce due to the well-known\nchallenge of segmentation, the high degree of occlusion, and the diversity of\nobjects in indoor scenes. We take a data-driven approach, generating sets of\npotential object regions, matching to regions in training images, and\ntransferring and aligning associated 3D models while encouraging fit to\nobservations and spatial consistency. We use support inference to aid\ninterpretation and propose a retrieval scheme that uses convolutional neural\nnetworks (CNNs) to classify regions and retrieve objects with similar shapes.\nWe demonstrate the performance of our method on our newly annotated NYUd v2\ndataset with detailed 3D shapes. \n\n"}
{"id": "1710.10948", "contents": "Title: Sound Source Localization in a Multipath Environment Using Convolutional\n  Neural Networks Abstract: The propagation of sound in a shallow water environment is characterized by\nboundary reflections from the sea surface and sea floor. These reflections\nresult in multiple (indirect) sound propagation paths, which can degrade the\nperformance of passive sound source localization methods. This paper proposes\nthe use of convolutional neural networks (CNNs) for the localization of sources\nof broadband acoustic radiated noise (such as motor vessels) in shallow water\nmultipath environments. It is shown that CNNs operating on cepstrogram and\ngeneralized cross-correlogram inputs are able to more reliably estimate the\ninstantaneous range and bearing of transiting motor vessels when the source\nlocalization performance of conventional passive ranging methods is degraded.\nThe ensuing improvement in source localization performance is demonstrated\nusing real data collected during an at-sea experiment. \n\n"}
{"id": "1710.11176", "contents": "Title: CrescendoNet: A Simple Deep Convolutional Neural Network with Ensemble\n  Behavior Abstract: We introduce a new deep convolutional neural network, CrescendoNet, by\nstacking simple building blocks without residual connections. Each Crescendo\nblock contains independent convolution paths with increased depths. The numbers\nof convolution layers and parameters are only increased linearly in Crescendo\nblocks. In experiments, CrescendoNet with only 15 layers outperforms almost all\nnetworks without residual connections on benchmark datasets, CIFAR10, CIFAR100,\nand SVHN. Given sufficient amount of data as in SVHN dataset, CrescendoNet with\n15 layers and 4.1M parameters can match the performance of DenseNet-BC with 250\nlayers and 15.3M parameters. CrescendoNet provides a new way to construct high\nperformance deep convolutional neural networks without residual connections.\nMoreover, through investigating the behavior and performance of subnetworks in\nCrescendoNet, we note that the high performance of CrescendoNet may come from\nits implicit ensemble behavior, which differs from the FractalNet that is also\na deep convolutional neural network without residual connections. Furthermore,\nthe independence between paths in CrescendoNet allows us to introduce a new\npath-wise training procedure, which can reduce the memory needed for training. \n\n"}
{"id": "1710.11370", "contents": "Title: Capacity-Achieving PIR Schemes with Optimal Sub-Packetization Abstract: Suppose a database containing $M$ records is replicated across $N$ servers,\nand a user wants to privately retrieve one record by accessing the servers such\nthat identity of the retrieved record is secret against any up to $T$ servers.\nA scheme designed for this purpose is called a private information retrieval\n(PIR) scheme. In practice, capacity-achieving and small sub-packetization are\nboth desired for PIR schemes, because the former implies the highest download\nrate and the latter usually means simple realization.\n  For general values of $N,T,M$, the only known capacity-achieving PIR scheme\nwas designed by Sun and Jafar in 2016 with sub-packetization $N^M$. In this\npaper, we design a linear capacity-achieving PIR scheme with much smaller\nsub-packetization $dn^{M-1}$, where $d={\\rm gcd}(N,T)$ and $n=N/d$.\nFurthermore, we prove that for any linear capacity-achieving PIR scheme it must\nhave sub-packetization no less than $dn^{M-1}$, implying our scheme has the\noptimal sub-packetization. Moreover, comparing with Sun and Jafar's scheme, our\nscheme reduces the field size by a factor of $\\frac{1}{Nd^{M-2}}$. \n\n"}
{"id": "1711.00677", "contents": "Title: Understanding and Predicting The Attractiveness of Human Action Shot Abstract: Selecting attractive photos from a human action shot sequence is quite\nchallenging, because of the subjective nature of the \"attractiveness\", which is\nmainly a combined factor of human pose in action and the background. Prior\nworks have actively studied high-level image attributes including\ninterestingness, memorability, popularity, and aesthetics. However, none of\nthem has ever studied the \"attractiveness\" of human action shot. In this paper,\nwe present the first study of the \"attractiveness\" of human action shots by\ntaking a systematic data-driven approach. Specifically, we create a new\naction-shot dataset composed of about 8000 high quality action-shot photos. We\nfurther conduct rich crowd-sourced human judge studies on Amazon Mechanical\nTurk(AMT) in terms of global attractiveness of a single photo, and relative\nattractiveness of a pair of photos. A deep Siamese network with a novel hybrid\ndistribution matching loss was further proposed to fully exploit both types of\nratings. Extensive experiments reveal that (1) the property of action shot\nattractiveness is subjective but predicable (2) our proposed method is both\nefficient and effective for predicting the attractive human action shots. \n\n"}
{"id": "1711.00987", "contents": "Title: Analysis of Biased Stochastic Gradient Descent Using Sequential\n  Semidefinite Programs Abstract: We present a convergence rate analysis for biased stochastic gradient descent\n(SGD), where individual gradient updates are corrupted by computation errors.\nWe develop stochastic quadratic constraints to formulate a small linear matrix\ninequality (LMI) whose feasible points lead to convergence bounds of biased\nSGD. Based on this LMI condition, we develop a sequential minimization approach\nto analyze the intricate trade-offs that couple stepsize selection, convergence\nrate, optimization accuracy, and robustness to gradient inaccuracy. We also\nprovide feasible points for this LMI and obtain theoretical formulas that\nquantify the convergence properties of biased SGD under various assumptions on\nthe loss functions. \n\n"}
{"id": "1711.01110", "contents": "Title: A Rudimentary Model for Low-Latency Anonymous Communication Systems Abstract: In this paper we present a rudimentary model for low-latency anonymous\ncommunication systems. Specifically, we study distributed OR algorithm as an\nabstract of the system. Based on our model, we give several satisfactory lower\nbounds of anonymity leakage of a deterministic OR algorithm. Some of them\nreveal a trade-off between anonymity and communication complexity. For the\nrandomized OR algorithm, we only give a relatively trivial but possibly tight\nlower bound when leaving out communication complexity. And we find the\nrelationship between our model and some open case in the study of secret\nsharing scheme, if considering communication complexity. \n\n"}
{"id": "1711.01468", "contents": "Title: Ensembles of Multiple Models and Architectures for Robust Brain Tumour\n  Segmentation Abstract: Deep learning approaches such as convolutional neural nets have consistently\noutperformed previous methods on challenging tasks such as dense, semantic\nsegmentation. However, the various proposed networks perform differently, with\nbehaviour largely influenced by architectural choices and training settings.\nThis paper explores Ensembles of Multiple Models and Architectures (EMMA) for\nrobust performance through aggregation of predictions from a wide range of\nmethods. The approach reduces the influence of the meta-parameters of\nindividual models and the risk of overfitting the configuration to a particular\ndatabase. EMMA can be seen as an unbiased, generic deep learning model which is\nshown to yield excellent performance, winning the first position in the BRATS\n2017 competition among 50+ participating teams. \n\n"}
{"id": "1711.01619", "contents": "Title: Enlarged Controllability of Riemann-Liouville Fractional Differential\n  Equations Abstract: We investigate exact enlarged controllability for time fractional diffusion\nsystems of Riemann-Liouville type. The Hilbert uniqueness method is used to\nprove exact enlarged controllability for both cases of zone and pointwise\nactuators. A penalization method is given and the minimum energy control is\ncharacterized. \n\n"}
{"id": "1711.02010", "contents": "Title: Artificial Generation of Big Data for Improving Image Classification: A\n  Generative Adversarial Network Approach on SAR Data Abstract: Very High Spatial Resolution (VHSR) large-scale SAR image databases are still\nan unresolved issue in the Remote Sensing field. In this work, we propose such\na dataset and use it to explore patch-based classification in urban and\nperiurban areas, considering 7 distinct semantic classes. In this context, we\ninvestigate the accuracy of large CNN classification models and pre-trained\nnetworks for SAR imaging systems. Furthermore, we propose a Generative\nAdversarial Network (GAN) for SAR image generation and test, whether the\nsynthetic data can actually improve classification accuracy. \n\n"}
{"id": "1711.02838", "contents": "Title: Stochastic Cubic Regularization for Fast Nonconvex Optimization Abstract: This paper proposes a stochastic variant of a classic algorithm---the\ncubic-regularized Newton method [Nesterov and Polyak 2006]. The proposed\nalgorithm efficiently escapes saddle points and finds approximate local minima\nfor general smooth, nonconvex functions in only\n$\\mathcal{\\tilde{O}}(\\epsilon^{-3.5})$ stochastic gradient and stochastic\nHessian-vector product evaluations. The latter can be computed as efficiently\nas stochastic gradients. This improves upon the\n$\\mathcal{\\tilde{O}}(\\epsilon^{-4})$ rate of stochastic gradient descent. Our\nrate matches the best-known result for finding local minima without requiring\nany delicate acceleration or variance-reduction techniques. \n\n"}
{"id": "1711.03179", "contents": "Title: Multi-stage Suture Detection for Robot Assisted Anastomosis based on\n  Deep Learning Abstract: In robotic surgery, task automation and learning from demonstration combined\nwith human supervision is an emerging trend for many new surgical robot\nplatforms. One such task is automated anastomosis, which requires bimanual\nneedle handling and suture detection. Due to the complexity of the surgical\nenvironment and varying patient anatomies, reliable suture detection is\ndifficult, which is further complicated by occlusion and thread topologies. In\nthis paper, we propose a multi-stage framework for suture thread detection\nbased on deep learning. Fully convolutional neural networks are used to obtain\nthe initial detection and the overlapping status of suture thread, which are\nlater fused with the original image to learn a gradient road map of the thread.\nBased on the gradient road map, multiple segments of the thread are extracted\nand linked to form the whole thread using a curvilinear structure detector.\nExperiments on two different types of sutures demonstrate the accuracy of the\nproposed framework. \n\n"}
{"id": "1711.03291", "contents": "Title: Portfolio Optimization and Model Predictive Control: A Kinetic Approach Abstract: In this paper, we introduce a large system of interacting financial agents in\nwhich each agent is faced with the decision of how to allocate his capital\nbetween a risky stock or a risk-less bond. The investment decision of\ninvestors, derived through an optimization, drives the stock price. The model\nhas been inspired by the econophysical Levy-Levy-Solomon model (Economics\nLetters, 45). The goal of this work is to gain insights into the stock price\nand wealth distribution. We especially want to discover the causes for the\nappearance of power-laws in financial data. We follow a kinetic approach\nsimilar to (D. Maldarella, L. Pareschi, Physica A, 391) and derive the mean\nfield limit of our microscopic agent dynamics. The novelty in our approach is\nthat the financial agents apply model predictive control (MPC) to approximate\nand solve the optimization of their utility function. Interestingly, the MPC\napproach gives a mathematical connection between the two opponent economic\nconcepts of modeling financial agents to be rational or boundedly rational.\nFurthermore, this is to our knowledge the first kinetic portfolio model which\nconsiders a wealth and stock price distribution simultaneously. Due to our\nkinetic approach, we can study the wealth and price distribution on a\nmesoscopic level. The wealth distribution is characterized by a lognormal law.\nFor the stock price distribution, we can either observe a lognormal behavior in\nthe case of long-term investors or a power-law in the case of high-frequency\ntrader. Furthermore, the stock return data exhibits a fat-tail, which is a well\nknown characteristic of real financial data. \n\n"}
{"id": "1711.04161", "contents": "Title: End-to-end Video-level Representation Learning for Action Recognition Abstract: From the frame/clip-level feature learning to the video-level representation\nbuilding, deep learning methods in action recognition have developed rapidly in\nrecent years. However, current methods suffer from the confusion caused by\npartial observation training, or without end-to-end learning, or restricted to\nsingle temporal scale modeling and so on. In this paper, we build upon\ntwo-stream ConvNets and propose Deep networks with Temporal Pyramid Pooling\n(DTPP), an end-to-end video-level representation learning approach, to address\nthese problems. Specifically, at first, RGB images and optical flow stacks are\nsparsely sampled across the whole video. Then a temporal pyramid pooling layer\nis used to aggregate the frame-level features which consist of spatial and\ntemporal cues. Lastly, the trained model has compact video-level representation\nwith multiple temporal scales, which is both global and sequence-aware.\nExperimental results show that DTPP achieves the state-of-the-art performance\non two challenging video action datasets: UCF101 and HMDB51, either by ImageNet\npre-training or Kinetics pre-training. \n\n"}
{"id": "1711.04293", "contents": "Title: Hand Gesture Recognition with Leap Motion Abstract: The recent introduction of depth cameras like Leap Motion Controller allows\nresearchers to exploit the depth information to recognize hand gesture more\nrobustly. This paper proposes a novel hand gesture recognition system with Leap\nMotion Controller. A series of features are extracted from Leap Motion tracking\ndata, we feed these features along with HOG feature extracted from sensor\nimages into a multi-class SVM classifier to recognize performed gesture,\ndimension reduction and feature weighted fusion are also discussed. Our results\nshow that our model is much more accurate than previous work. \n\n"}
{"id": "1711.04323", "contents": "Title: High-Order Attention Models for Visual Question Answering Abstract: The quest for algorithms that enable cognitive abilities is an important part\nof machine learning. A common trait in many recently investigated\ncognitive-like tasks is that they take into account different data modalities,\nsuch as visual and textual input. In this paper we propose a novel and\ngenerally applicable form of attention mechanism that learns high-order\ncorrelations between various data modalities. We show that high-order\ncorrelations effectively direct the appropriate attention to the relevant\nelements in the different data modalities that are required to solve the joint\ntask. We demonstrate the effectiveness of our high-order attention mechanism on\nthe task of visual question answering (VQA), where we achieve state-of-the-art\nperformance on the standard VQA dataset. \n\n"}
{"id": "1711.05376", "contents": "Title: Sliced Wasserstein Distance for Learning Gaussian Mixture Models Abstract: Gaussian mixture models (GMM) are powerful parametric tools with many\napplications in machine learning and computer vision. Expectation maximization\n(EM) is the most popular algorithm for estimating the GMM parameters. However,\nEM guarantees only convergence to a stationary point of the log-likelihood\nfunction, which could be arbitrarily worse than the optimal solution. Inspired\nby the relationship between the negative log-likelihood function and the\nKullback-Leibler (KL) divergence, we propose an alternative formulation for\nestimating the GMM parameters using the sliced Wasserstein distance, which\ngives rise to a new algorithm. Specifically, we propose minimizing the\nsliced-Wasserstein distance between the mixture model and the data distribution\nwith respect to the GMM parameters. In contrast to the KL-divergence, the\nenergy landscape for the sliced-Wasserstein distance is more well-behaved and\ntherefore more suitable for a stochastic gradient descent scheme to obtain the\noptimal GMM parameters. We show that our formulation results in parameter\nestimates that are more robust to random initializations and demonstrate that\nit can estimate high-dimensional data distributions more faithfully than the EM\nalgorithm. \n\n"}
{"id": "1711.05862", "contents": "Title: Real-Time Document Image Classification using Deep CNN and Extreme\n  Learning Machines Abstract: This paper presents an approach for real-time training and testing for\ndocument image classification. In production environments, it is crucial to\nperform accurate and (time-)efficient training. Existing deep learning\napproaches for classifying documents do not meet these requirements, as they\nrequire much time for training and fine-tuning the deep architectures.\nMotivated from Computer Vision, we propose a two-stage approach. The first\nstage trains a deep network that works as feature extractor and in the second\nstage, Extreme Learning Machines (ELMs) are used for classification. The\nproposed approach outperforms all previously reported structural and deep\nlearning based methods with a final accuracy of 83.24% on Tobacco-3482 dataset,\nleading to a relative error reduction of 25% when compared to a previous\nConvolutional Neural Network (CNN) based approach (DeepDocClassifier). More\nimportantly, the training time of the ELM is only 1.176 seconds and the overall\nprediction time for 2,482 images is 3.066 seconds. As such, this novel approach\nmakes deep learning-based document classification suitable for large-scale\nreal-time applications. \n\n"}
{"id": "1711.05971", "contents": "Title: Learning to Find Good Correspondences Abstract: We develop a deep architecture to learn to find good correspondences for\nwide-baseline stereo. Given a set of putative sparse matches and the camera\nintrinsics, we train our network in an end-to-end fashion to label the\ncorrespondences as inliers or outliers, while simultaneously using them to\nrecover the relative pose, as encoded by the essential matrix. Our architecture\nis based on a multi-layer perceptron operating on pixel coordinates rather than\ndirectly on the image, and is thus simple and small. We introduce a novel\nnormalization technique, called Context Normalization, which allows us to\nprocess each data point separately while imbuing it with global information,\nand also makes the network invariant to the order of the correspondences. Our\nexperiments on multiple challenging datasets demonstrate that our method is\nable to drastically improve the state of the art with little training data. \n\n"}
{"id": "1711.06439", "contents": "Title: Towards dense volumetric pancreas segmentation in CT using 3D fully\n  convolutional networks Abstract: Pancreas segmentation in computed tomography imaging has been historically\ndifficult for automated methods because of the large shape and size variations\nbetween patients. In this work, we describe a custom-build 3D fully\nconvolutional network (FCN) that can process a 3D image including the whole\npancreas and produce an automatic segmentation. We investigate two variations\nof the 3D FCN architecture; one with concatenation and one with summation skip\nconnections to the decoder part of the network. We evaluate our methods on a\ndataset from a clinical trial with gastric cancer patients, including 147\ncontrast enhanced abdominal CT scans acquired in the portal venous phase. Using\nthe summation architecture, we achieve an average Dice score of 89.7 $\\pm$ 3.8\n(range [79.8, 94.8]) % in testing, achieving the new state-of-the-art\nperformance in pancreas segmentation on this dataset. \n\n"}
{"id": "1711.07155", "contents": "Title: Let Features Decide for Themselves: Feature Mask Network for Person\n  Re-identification Abstract: Person re-identification aims at establishing the identity of a pedestrian\nfrom a gallery that contains images of multiple people obtained from a\nmulti-camera system. Many challenges such as occlusions, drastic lighting and\npose variations across the camera views, indiscriminate visual appearances,\ncluttered backgrounds, imperfect detections, motion blur, and noise make this\ntask highly challenging. While most approaches focus on learning features and\nmetrics to derive better representations, we hypothesize that both local and\nglobal contextual cues are crucial for an accurate identity matching. To this\nend, we propose a Feature Mask Network (FMN) that takes advantage of ResNet\nhigh-level features to predict a feature map mask and then imposes it on the\nlow-level features to dynamically reweight different object parts for a locally\naware feature representation. This serves as an effective attention mechanism\nby allowing the network to focus on local details selectively. Given the\nresemblance of person re-identification with classification and retrieval\ntasks, we frame the network training as a multi-task objective optimization,\nwhich further improves the learned feature descriptions. We conduct experiments\non Market-1501, DukeMTMC-reID and CUHK03 datasets, where the proposed approach\nrespectively achieves significant improvements of $5.3\\%$, $9.1\\%$ and $10.7\\%$\nin mAP measure relative to the state-of-the-art. \n\n"}
{"id": "1711.07190", "contents": "Title: Block-Cyclic Stochastic Coordinate Descent for Deep Neural Networks Abstract: We present a stochastic first-order optimization algorithm, named BCSC, that\nadds a cyclic constraint to stochastic block-coordinate descent. It uses\ndifferent subsets of the data to update different subsets of the parameters,\nthus limiting the detrimental effect of outliers in the training set. Empirical\ntests in benchmark datasets show that our algorithm outperforms\nstate-of-the-art optimization methods in both accuracy as well as convergence\nspeed. The improvements are consistent across different architectures, and can\nbe combined with other training techniques and regularization methods. \n\n"}
{"id": "1711.07354", "contents": "Title: Convergent Block Coordinate Descent for Training Tikhonov Regularized\n  Deep Neural Networks Abstract: By lifting the ReLU function into a higher dimensional space, we develop a\nsmooth multi-convex formulation for training feed-forward deep neural networks\n(DNNs). This allows us to develop a block coordinate descent (BCD) training\nalgorithm consisting of a sequence of numerically well-behaved convex\noptimizations. Using ideas from proximal point methods in convex analysis, we\nprove that this BCD algorithm will converge globally to a stationary point with\nR-linear convergence rate of order one. In experiments with the MNIST database,\nDNNs trained with this BCD algorithm consistently yielded better test-set error\nrates than identical DNN architectures trained via all the stochastic gradient\ndescent (SGD) variants in the Caffe toolbox. \n\n"}
{"id": "1711.07653", "contents": "Title: Proximal Alternating Direction Network: A Globally Converged Deep\n  Unrolling Framework Abstract: Deep learning models have gained great success in many real-world\napplications. However, most existing networks are typically designed in\nheuristic manners, thus lack of rigorous mathematical principles and\nderivations. Several recent studies build deep structures by unrolling a\nparticular optimization model that involves task information. Unfortunately,\ndue to the dynamic nature of network parameters, their resultant deep\npropagation networks do \\emph{not} possess the nice convergence property as the\noriginal optimization scheme does. This paper provides a novel proximal\nunrolling framework to establish deep models by integrating experimentally\nverified network architectures and rich cues of the tasks. More importantly, we\n\\emph{prove in theory} that 1) the propagation generated by our unrolled deep\nmodel globally converges to a critical-point of a given variational energy, and\n2) the proposed framework is still able to learn priors from training data to\ngenerate a convergent propagation even when task information is only partially\navailable. Indeed, these theoretical results are the best we can ask for,\nunless stronger assumptions are enforced. Extensive experiments on various\nreal-world applications verify the theoretical convergence and demonstrate the\neffectiveness of designed deep models. \n\n"}
{"id": "1711.07714", "contents": "Title: Residual Parameter Transfer for Deep Domain Adaptation Abstract: The goal of Deep Domain Adaptation is to make it possible to use Deep Nets\ntrained in one domain where there is enough annotated training data in another\nwhere there is little or none. Most current approaches have focused on learning\nfeature representations that are invariant to the changes that occur when going\nfrom one domain to the other, which means using the same network parameters in\nboth domains. While some recent algorithms explicitly model the changes by\nadapting the network parameters, they either severely restrict the possible\ndomain changes, or significantly increase the number of model parameters.\n  By contrast, we introduce a network architecture that includes auxiliary\nresidual networks, which we train to predict the parameters in the domain with\nlittle annotated data from those in the other one. This architecture enables us\nto flexibly preserve the similarities between domains where they exist and\nmodel the differences when necessary. We demonstrate that our approach yields\nhigher accuracy than state-of-the-art methods without undue complexity. \n\n"}
{"id": "1711.07752", "contents": "Title: Repulsion Loss: Detecting Pedestrians in a Crowd Abstract: Detecting individual pedestrians in a crowd remains a challenging problem\nsince the pedestrians often gather together and occlude each other in\nreal-world scenarios. In this paper, we first explore how a state-of-the-art\npedestrian detector is harmed by crowd occlusion via experimentation, providing\ninsights into the crowd occlusion problem. Then, we propose a novel bounding\nbox regression loss specifically designed for crowd scenes, termed repulsion\nloss. This loss is driven by two motivations: the attraction by target, and the\nrepulsion by other surrounding objects. The repulsion term prevents the\nproposal from shifting to surrounding objects thus leading to more crowd-robust\nlocalization. Our detector trained by repulsion loss outperforms all the\nstate-of-the-art methods with a significant improvement in occlusion cases. \n\n"}
{"id": "1711.07846", "contents": "Title: Functional Map of the World Abstract: We present a new dataset, Functional Map of the World (fMoW), which aims to\ninspire the development of machine learning models capable of predicting the\nfunctional purpose of buildings and land use from temporal sequences of\nsatellite images and a rich set of metadata features. The metadata provided\nwith each image enables reasoning about location, time, sun angles, physical\nsizes, and other features when making predictions about objects in the image.\nOur dataset consists of over 1 million images from over 200 countries. For each\nimage, we provide at least one bounding box annotation containing one of 63\ncategories, including a \"false detection\" category. We present an analysis of\nthe dataset along with baseline approaches that reason about metadata and\ntemporal views. Our data, code, and pretrained models have been made publicly\navailable. \n\n"}
{"id": "1711.08241", "contents": "Title: 3D Point Cloud Classification and Segmentation using 3D Modified Fisher\n  Vector Representation for Convolutional Neural Networks Abstract: The point cloud is gaining prominence as a method for representing 3D shapes,\nbut its irregular format poses a challenge for deep learning methods. The\ncommon solution of transforming the data into a 3D voxel grid introduces its\nown challenges, mainly large memory size. In this paper we propose a novel 3D\npoint cloud representation called 3D Modified Fisher Vectors (3DmFV). Our\nrepresentation is hybrid as it combines the discrete structure of a grid with\ncontinuous generalization of Fisher vectors, in a compact and computationally\nefficient way. Using the grid enables us to design a new CNN architecture for\npoint cloud classification and part segmentation. In a series of experiments we\ndemonstrate competitive performance or even better than state-of-the-art on\nchallenging benchmark datasets. \n\n"}
{"id": "1711.08364", "contents": "Title: ForestHash: Semantic Hashing With Shallow Random Forests and Tiny\n  Convolutional Networks Abstract: Hash codes are efficient data representations for coping with the ever\ngrowing amounts of data. In this paper, we introduce a random forest semantic\nhashing scheme that embeds tiny convolutional neural networks (CNN) into\nshallow random forests, with near-optimal information-theoretic code\naggregation among trees. We start with a simple hashing scheme, where random\ntrees in a forest act as hashing functions by setting `1' for the visited tree\nleaf, and `0' for the rest. We show that traditional random forests fail to\ngenerate hashes that preserve the underlying similarity between the trees,\nrendering the random forests approach to hashing challenging. To address this,\nwe propose to first randomly group arriving classes at each tree split node\ninto two groups, obtaining a significantly simplified two-class classification\nproblem, which can be handled using a light-weight CNN weak learner. Such\nrandom class grouping scheme enables code uniqueness by enforcing each class to\nshare its code with different classes in different trees. A non-conventional\nlow-rank loss is further adopted for the CNN weak learners to encourage code\nconsistency by minimizing intra-class variations and maximizing inter-class\ndistance for the two random class groups. Finally, we introduce an\ninformation-theoretic approach for aggregating codes of individual trees into a\nsingle hash code, producing a near-optimal unique hash for each class. The\nproposed approach significantly outperforms state-of-the-art hashing methods\nfor image retrieval tasks on large-scale public datasets, while performing at\nthe level of other state-of-the-art image classification techniques while\nutilizing a more compact and efficient scalable representation. This work\nproposes a principled and robust procedure to train and deploy in parallel an\nensemble of light-weight CNNs, instead of simply going deeper. \n\n"}
{"id": "1711.08389", "contents": "Title: Conditional Image-Text Embedding Networks Abstract: This paper presents an approach for grounding phrases in images which jointly\nlearns multiple text-conditioned embeddings in a single end-to-end model. In\norder to differentiate text phrases into semantically distinct subspaces, we\npropose a concept weight branch that automatically assigns phrases to\nembeddings, whereas prior works predefine such assignments. Our proposed\nsolution simplifies the representation requirements for individual embeddings\nand allows the underrepresented concepts to take advantage of the shared\nrepresentations before feeding them into concept-specific layers. Comprehensive\nexperiments verify the effectiveness of our approach across three phrase\ngrounding datasets, Flickr30K Entities, ReferIt Game, and Visual Genome, where\nwe obtain a (resp.) 4%, 3%, and 4% improvement in grounding performance over a\nstrong region-phrase embedding baseline. \n\n"}
{"id": "1711.08561", "contents": "Title: Adversarial Feature Augmentation for Unsupervised Domain Adaptation Abstract: Recent works showed that Generative Adversarial Networks (GANs) can be\nsuccessfully applied in unsupervised domain adaptation, where, given a labeled\nsource dataset and an unlabeled target dataset, the goal is to train powerful\nclassifiers for the target samples. In particular, it was shown that a GAN\nobjective function can be used to learn target features indistinguishable from\nthe source ones. In this work, we extend this framework by (i) forcing the\nlearned feature extractor to be domain-invariant, and (ii) training it through\ndata augmentation in the feature space, namely performing feature augmentation.\nWhile data augmentation in the image space is a well established technique in\ndeep learning, feature augmentation has not yet received the same level of\nattention. We accomplish it by means of a feature generator trained by playing\nthe GAN minimax game against source features. Results show that both enforcing\ndomain-invariance and performing feature augmentation lead to superior or\ncomparable performance to state-of-the-art results in several unsupervised\ndomain adaptation benchmarks. \n\n"}
{"id": "1711.09313", "contents": "Title: DeepRadiologyNet: Radiologist Level Pathology Detection in CT Head\n  Images Abstract: We describe a system to automatically filter clinically significant findings\nfrom computerized tomography (CT) head scans, operating at performance levels\nexceeding that of practicing radiologists. Our system, named DeepRadiologyNet,\nbuilds on top of deep convolutional neural networks (CNNs) trained using\napproximately 3.5 million CT head images gathered from over 24,000 studies\ntaken from January 1, 2015 to August 31, 2015 and January 1, 2016 to April 30\n2016 in over 80 clinical sites. For our initial system, we identified 30\nphenomenological traits to be recognized in the CT scans. To test the system,\nwe designed a clinical trial using over 4.8 million CT head images (29,925\nstudies), completely disjoint from the training and validation set, interpreted\nby 35 US Board Certified radiologists with specialized CT head experience. We\nmeasured clinically significant error rates to ascertain whether the\nperformance of DeepRadiologyNet was comparable to or better than that of US\nBoard Certified radiologists. DeepRadiologyNet achieved a clinically\nsignificant miss rate of 0.0367% on automatically selected high-confidence\nstudies. Thus, DeepRadiologyNet enables significant reduction in the workload\nof human radiologists by automatically filtering studies and reporting on the\nhigh-confidence ones at an operating point well below the literal error rate\nfor US Board Certified radiologists, estimated at 0.82%. \n\n"}
{"id": "1711.10151", "contents": "Title: Recurrent Segmentation for Variable Computational Budgets Abstract: State-of-the-art systems for semantic image segmentation use feed-forward\npipelines with fixed computational costs. Building an image segmentation system\nthat works across a range of computational budgets is challenging and\ntime-intensive as new architectures must be designed and trained for every\ncomputational setting. To address this problem we develop a recurrent neural\nnetwork that successively improves prediction quality with each iteration.\nImportantly, the RNN may be deployed across a range of computational budgets by\nmerely running the model for a variable number of iterations. We find that this\narchitecture is uniquely suited for efficiently segmenting videos. By\nexploiting the segmentation of past frames, the RNN can perform video\nsegmentation at similar quality but reduced computational cost compared to\nstate-of-the-art image segmentation methods. When applied to static images in\nthe PASCAL VOC 2012 and Cityscapes segmentation datasets, the RNN traces out a\nspeed-accuracy curve that saturates near the performance of state-of-the-art\nsegmentation methods. \n\n"}
{"id": "1711.11200", "contents": "Title: Embedded Real-Time Fall Detection Using Deep Learning For Elderly Care Abstract: This paper proposes a real-time embedded fall detection system using a\nDVS(Dynamic Vision Sensor) that has never been used for traditional fall\ndetection, a dataset for fall detection using that, and a DVS-TN(DVS-Temporal\nNetwork). The first contribution is building a DVS Falls Dataset, which made\nour network to recognize a much greater variety of falls than the existing\ndatasets that existed before and solved privacy issues using the DVS. Secondly,\nwe introduce the DVS-TN : optimized deep learning network to detect falls using\nDVS. Finally, we implemented a fall detection system which can run on\nlow-computing H/W with real-time, and tested on DVS Falls Dataset that takes\ninto account various falls situations. Our approach achieved 95.5% on the\nF1-score and operates at 31.25 FPS on NVIDIA Jetson TX1 board. \n\n"}
{"id": "1711.11217", "contents": "Title: Future Person Localization in First-Person Videos Abstract: We present a new task that predicts future locations of people observed in\nfirst-person videos. Consider a first-person video stream continuously recorded\nby a wearable camera. Given a short clip of a person that is extracted from the\ncomplete stream, we aim to predict that person's location in future frames. To\nfacilitate this future person localization ability, we make the following three\nkey observations: a) First-person videos typically involve significant\nego-motion which greatly affects the location of the target person in future\nframes; b) Scales of the target person act as a salient cue to estimate a\nperspective effect in first-person videos; c) First-person videos often capture\npeople up-close, making it easier to leverage target poses (e.g., where they\nlook) for predicting their future locations. We incorporate these three\nobservations into a prediction framework with a multi-stream\nconvolution-deconvolution architecture. Experimental results reveal our method\nto be effective on our new dataset as well as on a public social interaction\ndataset. \n\n"}
{"id": "1711.11249", "contents": "Title: ArbiText: Arbitrary-Oriented Text Detection in Unconstrained Scene Abstract: Arbitrary-oriented text detection in the wild is a very challenging task, due\nto the aspect ratio, scale, orientation, and illumination variations. In this\npaper, we propose a novel method, namely Arbitrary-oriented Text (or ArbText\nfor short) detector, for efficient text detection in unconstrained natural\nscene images. Specifically, we first adopt the circle anchors rather than the\nrectangular ones to represent bounding boxes, which is more robust to\norientation variations. Subsequently, we incorporate a pyramid pooling module\ninto the Single Shot MultiBox Detector framework, in order to simultaneously\nexplore the local and global visual information, which can, therefore, generate\nmore confidential detection results. Experiments on established scene-text\ndatasets, such as the ICDAR 2015 and MSRA-TD500 datasets, have demonstrated the\nsupe rior performance of the proposed method, compared to the state-of-the-art\napproaches. \n\n"}
{"id": "1712.01358", "contents": "Title: Long-Term Visual Object Tracking Benchmark Abstract: We propose a new long video dataset (called Track Long and Prosper - TLP) and\nbenchmark for single object tracking. The dataset consists of 50 HD videos from\nreal world scenarios, encompassing a duration of over 400 minutes (676K\nframes), making it more than 20 folds larger in average duration per sequence\nand more than 8 folds larger in terms of total covered duration, as compared to\nexisting generic datasets for visual tracking. The proposed dataset paves a way\nto suitably assess long term tracking performance and train better deep\nlearning architectures (avoiding/reducing augmentation, which may not reflect\nreal world behaviour). We benchmark the dataset on 17 state of the art trackers\nand rank them according to tracking accuracy and run time speeds. We further\npresent thorough qualitative and quantitative evaluation highlighting the\nimportance of long term aspect of tracking. Our most interesting observations\nare (a) existing short sequence benchmarks fail to bring out the inherent\ndifferences in tracking algorithms which widen up while tracking on long\nsequences and (b) the accuracy of trackers abruptly drops on challenging long\nsequences, suggesting the potential need of research efforts in the direction\nof long-term tracking. \n\n"}
{"id": "1712.01628", "contents": "Title: On Deterministic Sampling Patterns for Robust Low-Rank Matrix Completion Abstract: In this letter, we study the deterministic sampling patterns for the\ncompletion of low rank matrix, when corrupted with a sparse noise, also known\nas robust matrix completion. We extend the recent results on the deterministic\nsampling patterns in the absence of noise based on the geometric analysis on\nthe Grassmannian manifold. A special case where each column has a certain\nnumber of noisy entries is considered, where our probabilistic analysis\nperforms very efficiently. Furthermore, assuming that the rank of the original\nmatrix is not given, we provide an analysis to determine if the rank of a valid\ncompletion is indeed the actual rank of the data corrupted with sparse noise by\nverifying some conditions. \n\n"}
{"id": "1712.02066", "contents": "Title: Automatic Segmentation and Overall Survival Prediction in Gliomas using\n  Fully Convolutional Neural Network and Texture Analysis Abstract: In this paper, we use a fully convolutional neural network (FCNN) for the\nsegmentation of gliomas from Magnetic Resonance Images (MRI). A fully\nautomatic, voxel based classification was achieved by training a 23 layer deep\nFCNN on 2-D slices extracted from patient volumes. The network was trained on\nslices extracted from 130 patients and validated on 50 patients. For the task\nof survival prediction, texture and shape based features were extracted from T1\npost contrast volume to train an XGBoost regressor. On BraTS 2017 validation\nset, the proposed scheme achieved a mean whole tumor, tumor core and active\ndice score of 0.83, 0.69 and 0.69 respectively and an accuracy of 52% for the\noverall survival prediction. \n\n"}
{"id": "1712.02466", "contents": "Title: On Sub-Packetization and Access Number of Capacity-Achieving PIR Schemes\n  for MDS Coded Non-Colluding Servers Abstract: Consider the problem of private information retrieval (PIR) over a\ndistributed storage system where $M$ records are stored across $N$ servers by\nusing an $[N,K]$ MDS code. For simplicity, this problem is usually referred as\nthe coded PIR problem. In 2016, Banawan and Ulukus designed the first\ncapacity-achieving coded PIR scheme with sub-packetization $KN^{M}$ and access\nnumber $MKN^{M}$, where capacity characterizes the minimal download size for\nretrieving per unit of data, and sub-packetization and access number are two\nmetrics closely related to implementation complexity. In this paper, we focus\non minimizing the sub-packetization and the access number for linear\ncapacity-achieving coded PIR schemes. We first determine the lower bounds on\nsub-packetization and access number, which are $Kn^{M-1}$ and $MKn^{M-1}$,\nrespectively, in the nontrivial cases (i.e. $N\\!>\\!K\\!\\geq\\!1$ and $M\\!>\\!1$),\nwhere $n\\!=\\!N/{\\rm gcd}(N,K)$. We then design a general linear\ncapacity-achieving coded PIR scheme to simultaneously attain these two bounds,\nimplying tightness of both bounds. \n\n"}
{"id": "1712.03121", "contents": "Title: Simultaneous Hand Pose and Skeleton Bone-Lengths Estimation from a\n  Single Depth Image Abstract: Articulated hand pose estimation is a challenging task for human-computer\ninteraction. The state-of-the-art hand pose estimation algorithms work only\nwith one or a few subjects for which they have been calibrated or trained.\nParticularly, the hybrid methods based on learning followed by model fitting or\nmodel based deep learning do not explicitly consider varying hand shapes and\nsizes. In this work, we introduce a novel hybrid algorithm for estimating the\n3D hand pose as well as bone-lengths of the hand skeleton at the same time,\nfrom a single depth image. The proposed CNN architecture learns hand pose\nparameters and scale parameters associated with the bone-lengths\nsimultaneously. Subsequently, a new hybrid forward kinematics layer employs\nboth parameters to estimate 3D joint positions of the hand. For end-to-end\ntraining, we combine three public datasets NYU, ICVL and MSRA-2015 in one\nunified format to achieve large variation in hand shapes and sizes. Among\nhybrid methods, our method shows improved accuracy over the state-of-the-art on\nthe combined dataset and the ICVL dataset that contain multiple subjects. Also,\nour algorithm is demonstrated to work well with unseen images. \n\n"}
{"id": "1712.03380", "contents": "Title: A Deep Recurrent Framework for Cleaning Motion Capture Data Abstract: We present a deep, bidirectional, recurrent framework for cleaning noisy and\nincomplete motion capture data. It exploits temporal coherence and joint\ncorrelations to infer adaptive filters for each joint in each frame. A single\nmodel can be trained to denoise a heterogeneous mix of action types, under\nsubstantial amounts of noise. A signal that has both noise and gaps is\npreprocessed with a second bidirectional network that synthesizes missing\nframes from surrounding context. The approach handles a wide variety of noise\ntypes and long gaps, does not rely on knowledge of the noise distribution, and\noperates in a streaming setting. We validate our approach through extensive\nevaluations on noise both in joint angles and in joint positions, and show that\nit improves upon various alternatives. \n\n"}
{"id": "1712.03491", "contents": "Title: 3D Facial Expression Reconstruction using Cascaded Regression Abstract: This paper proposes a novel model fitting algorithm for 3D facial expression\nreconstruction from a single image. Face expression reconstruction from a\nsingle image is a challenging task in computer vision. Most state-of-the-art\nmethods fit the input image to a 3D Morphable Model (3DMM). These methods need\nto solve a stochastic problem and cannot deal with expression and pose\nvariations. To solve this problem, we adopt a 3D face expression model and use\na combined feature which is robust to scale, rotation and different lighting\nconditions. The proposed method applies a cascaded regression framework to\nestimate parameters for the 3DMM. 2D landmarks are detected and used to\ninitialize the 3D shape and mapping matrices. In each iteration, residues\nbetween the current 3DMM parameters and the ground truth are estimated and then\nused to update the 3D shapes. The mapping matrices are also calculated based on\nthe updated shapes and 2D landmarks. HOG features of the local patches and\ndisplacements between 3D landmark projections and 2D landmarks are exploited.\nCompared with existing methods, the proposed method is robust to expression and\npose changes and can reconstruct higher fidelity 3D face shape. \n\n"}
{"id": "1712.03866", "contents": "Title: Using a single RGB frame for real time 3D hand pose estimation in the\n  wild Abstract: We present a method for the real-time estimation of the full 3D pose of one\nor more human hands using a single commodity RGB camera. Recent work in the\narea has displayed impressive progress using RGBD input. However, since the\nintroduction of RGBD sensors, there has been little progress for the case of\nmonocular color input. We capitalize on the latest advancements of deep\nlearning, combining them with the power of generative hand pose estimation\ntechniques to achieve real-time monocular 3D hand pose estimation in\nunrestricted scenarios. More specifically, given an RGB image and the relevant\ncamera calibration information, we employ a state-of-the-art detector to\nlocalize hands. Given a crop of a hand in the image, we run the pretrained\nnetwork of OpenPose for hands to estimate the 2D location of hand joints.\nFinally, non-linear least-squares minimization fits a 3D model of the hand to\nthe estimated 2D joint positions, recovering the 3D hand pose. Extensive\nexperimental results provide comparison to the state of the art as well as\nqualitative assessment of the method in the wild. \n\n"}
{"id": "1712.04621", "contents": "Title: The Effectiveness of Data Augmentation in Image Classification using\n  Deep Learning Abstract: In this paper, we explore and compare multiple solutions to the problem of\ndata augmentation in image classification. Previous work has demonstrated the\neffectiveness of data augmentation through simple techniques, such as cropping,\nrotating, and flipping input images. We artificially constrain our access to\ndata to a small subset of the ImageNet dataset, and compare each data\naugmentation technique in turn. One of the more successful data augmentations\nstrategies is the traditional transformations mentioned above. We also\nexperiment with GANs to generate images of different styles. Finally, we\npropose a method to allow a neural net to learn augmentations that best improve\nthe classifier, which we call neural augmentation. We discuss the successes and\nshortcomings of this method on various datasets. \n\n"}
{"id": "1712.05790", "contents": "Title: Deep Burst Denoising Abstract: Noise is an inherent issue of low-light image capture, one which is\nexacerbated on mobile devices due to their narrow apertures and small sensors.\nOne strategy for mitigating noise in a low-light situation is to increase the\nshutter time of the camera, thus allowing each photosite to integrate more\nlight and decrease noise variance. However, there are two downsides of long\nexposures: (a) bright regions can exceed the sensor range, and (b) camera and\nscene motion will result in blurred images. Another way of gathering more light\nis to capture multiple short (thus noisy) frames in a \"burst\" and intelligently\nintegrate the content, thus avoiding the above downsides. In this paper, we use\nthe burst-capture strategy and implement the intelligent integration via a\nrecurrent fully convolutional deep neural net (CNN). We build our novel,\nmultiframe architecture to be a simple addition to any single frame denoising\nmodel, and design to handle an arbitrary number of noisy input frames. We show\nthat it achieves state of the art denoising results on our burst dataset,\nimproving on the best published multi-frame techniques, such as VBM4D and\nFlexISP. Finally, we explore other applications of image enhancement by\nintegrating content from multiple frames and demonstrate that our DNN\narchitecture generalizes well to image super-resolution. \n\n"}
{"id": "1712.06424", "contents": "Title: Learning to Write Stylized Chinese Characters by Reading a Handful of\n  Examples Abstract: Automatically writing stylized Chinese characters is an attractive yet\nchallenging task due to its wide applicabilities. In this paper, we propose a\nnovel framework named Style-Aware Variational Auto-Encoder (SA-VAE) to flexibly\ngenerate Chinese characters. Specifically, we propose to capture the different\ncharacteristics of a Chinese character by disentangling the latent features\ninto content-related and style-related components. Considering of the complex\nshapes and structures, we incorporate the structure information as prior\nknowledge into our framework to guide the generation. Our framework shows a\npowerful one-shot/low-shot generalization ability by inferring the style\ncomponent given a character with unseen style. To the best of our knowledge,\nthis is the first attempt to learn to write new-style Chinese characters by\nobserving only one or a few examples. Extensive experiments demonstrate its\neffectiveness in generating different stylized Chinese characters by fusing the\nfeature vectors corresponding to different contents and styles, which is of\nsignificant importance in real-world applications. \n\n"}
{"id": "1712.06715", "contents": "Title: Deformable Classifiers Abstract: Geometric variations of objects, which do not modify the object class, pose a\nmajor challenge for object recognition. These variations could be rigid as well\nas non-rigid transformations. In this paper, we design a framework for training\ndeformable classifiers, where latent transformation variables are introduced,\nand a transformation of the object image to a reference instantiation is\ncomputed in terms of the classifier output, separately for each class. The\nclassifier outputs for each class, after transformation, are compared to yield\nthe final decision. As a by-product of the classification this yields a\ntransformation of the input object to a reference pose, which can be used for\ndownstream tasks such as the computation of object support. We apply a two-step\ntraining mechanism for our framework, which alternates between optimizing over\nthe latent transformation variables and the classifier parameters to minimize\nthe loss function. We show that multilayer perceptrons, also known as deep\nnetworks, are well suited for this approach and achieve state of the art\nresults on the rotated MNIST and the Google Earth dataset, and produce\ncompetitive results on MNIST and CIFAR-10 when training on smaller subsets of\ntraining data. \n\n"}
{"id": "1712.07420", "contents": "Title: Finding Competitive Network Architectures Within a Day Using UCT Abstract: The design of neural network architectures for a new data set is a laborious\ntask which requires human deep learning expertise. In order to make deep\nlearning available for a broader audience, automated methods for finding a\nneural network architecture are vital. Recently proposed methods can already\nachieve human expert level performances. However, these methods have run times\nof months or even years of GPU computing time, ignoring hardware constraints as\nfaced by many researchers and companies. We propose the use of Monte Carlo\nplanning in combination with two different UCT (upper confidence bound applied\nto trees) derivations to search for network architectures. We adapt the UCT\nalgorithm to the needs of network architecture search by proposing two ways of\nsharing information between different branches of the search tree. In an\nempirical study we are able to demonstrate that this method is able to find\ncompetitive networks for MNIST, SVHN and CIFAR-10 in just a single GPU day.\nExtending the search time to five GPU days, we are able to outperform human\narchitectures and our competitors which consider the same types of layers. \n\n"}
{"id": "1712.07436", "contents": "Title: Incremental Adversarial Domain Adaptation for Continually Changing\n  Environments Abstract: Continuous appearance shifts such as changes in weather and lighting\nconditions can impact the performance of deployed machine learning models.\nWhile unsupervised domain adaptation aims to address this challenge, current\napproaches do not utilise the continuity of the occurring shifts. In\nparticular, many robotics applications exhibit these conditions and thus\nfacilitate the potential to incrementally adapt a learnt model over minor\nshifts which integrate to massive differences over time. Our work presents an\nadversarial approach for lifelong, incremental domain adaptation which benefits\nfrom unsupervised alignment to a series of intermediate domains which\nsuccessively diverge from the labelled source domain. We empirically\ndemonstrate that our incremental approach improves handling of large appearance\nchanges, e.g. day to night, on a traversable-path segmentation task compared\nwith a direct, single alignment step approach. Furthermore, by approximating\nthe feature distribution for the source domain with a generative adversarial\nnetwork, the deployment module can be rendered fully independent of retaining\npotentially large amounts of the related source training data for only a minor\nreduction in performance. \n\n"}
{"id": "1712.08002", "contents": "Title: Human Action Recognition: Pose-based Attention draws focus to Hands Abstract: We propose a new spatio-temporal attention based mechanism for human action\nrecognition able to automatically attend to the hands most involved into the\nstudied action and detect the most discriminative moments in an action.\nAttention is handled in a recurrent manner employing Recurrent Neural Network\n(RNN) and is fully-differentiable. In contrast to standard soft-attention based\nmechanisms, our approach does not use the hidden RNN state as input to the\nattention model. Instead, attention distributions are extracted using external\ninformation: human articulated pose. We performed an extensive ablation study\nto show the strengths of this approach and we particularly studied the\nconditioning aspect of the attention mechanism. We evaluate the method on the\nlargest currently available human action recognition dataset, NTU-RGB+D, and\nreport state-of-the-art results. Other advantages of our model are certain\naspects of explanability, as the spatial and temporal attention distributions\nat test time allow to study and verify on which parts of the input data the\nmethod focuses. \n\n"}
{"id": "1712.08015", "contents": "Title: Risk-Based Distributionally Robust Optimal Power Flow With Dynamic Line\n  Rating Abstract: In this paper, we propose a risk-based data-driven approach to optimal power\nflow (DROPF) with dynamic line rating. The risk terms, including penalties for\nload shedding, wind generation curtailment and line overload, are embedded into\nthe objective function. To hedge against the uncertainties on wind generation\ndata and line rating data, we consider a distributionally robust approach. The\nambiguity set is based on second-order moment and Wasserstein distance, which\ncaptures the correlations between wind generation outputs and line ratings, and\nis robust to data perturbation. We show that the proposed DROPF model can be\nreformulated as a conic program. Considering the relatively large number of\nconstraints involved, an approximation of the proposed DROPF model is\nsuggested, which significantly reduces the computational costs. A Wasserstein\ndistance constrained DROPF and its tractable reformulation are also provided\nfor practical large-scale test systems. Simulation results on the 5-bus, the\nIEEE 118-bus and the Polish 2736-bus test systems validate the effectiveness of\nthe proposed models. \n\n"}
{"id": "1712.08263", "contents": "Title: Using LIP to Gloss Over Faces in Single-Stage Face Detection Networks Abstract: This work shows that it is possible to fool/attack recent state-of-the-art\nface detectors which are based on the single-stage networks. Successfully\nattacking face detectors could be a serious malware vulnerability when\ndeploying a smart surveillance system utilizing face detectors. We show that\nexisting adversarial perturbation methods are not effective to perform such an\nattack, especially when there are multiple faces in the input image. This is\nbecause the adversarial perturbation specifically generated for one face may\ndisrupt the adversarial perturbation for another face. In this paper, we call\nthis problem the Instance Perturbation Interference (IPI) problem. This IPI\nproblem is addressed by studying the relationship between the deep neural\nnetwork receptive field and the adversarial perturbation. As such, we propose\nthe Localized Instance Perturbation (LIP) that uses adversarial perturbation\nconstrained to the Effective Receptive Field (ERF) of a target to perform the\nattack. Experiment results show the LIP method massively outperforms existing\nadversarial perturbation generation methods -- often by a factor of 2 to 10. \n\n"}
{"id": "1712.08832", "contents": "Title: Large-Scale Object Discovery and Detector Adaptation from Unlabeled\n  Video Abstract: We explore object discovery and detector adaptation based on unlabeled video\nsequences captured from a mobile platform. We propose a fully automatic\napproach for object mining from video which builds upon a generic object\ntracking approach. By applying this method to three large video datasets from\nautonomous driving and mobile robotics scenarios, we demonstrate its robustness\nand generality. Based on the object mining results, we propose a novel approach\nfor unsupervised object discovery by appearance-based clustering. We show that\nthis approach successfully discovers interesting objects relevant to driving\nscenarios. In addition, we perform self-supervised detector adaptation in order\nto improve detection performance on the KITTI dataset for existing categories.\nOur approach has direct relevance for enabling large-scale object learning for\nautonomous driving. \n\n"}
{"id": "1712.09025", "contents": "Title: Domain Adaptation Meets Disentangled Representation Learning and Style\n  Transfer Abstract: Many methods have been proposed to solve the domain adaptation problem\nrecently. However, the success of them implicitly funds on the assumption that\nthe information of domains are fully transferrable. If the assumption is not\nsatisfied, the effect of negative transfer may degrade domain adaptation. In\nthis paper, a better learning network has been proposed by considering three\ntasks - domain adaptation, disentangled representation, and style transfer\nsimultaneously. Firstly, the learned features are disentangled into common\nparts and specific parts. The common parts represent the transferrable\nfeatures, which are suitable for domain adaptation with less negative transfer.\nConversely, the specific parts characterize the unique style of each individual\ndomain. Based on this, the new concept of feature exchange across domains,\nwhich can not only enhance the transferability of common features but also be\nuseful for image style transfer, is introduced. These designs allow us to\nintroduce five types of training objectives to realize the three challenging\ntasks at the same time. The experimental results show that our architecture can\nbe adaptive well to full transfer learning and partial transfer learning upon a\nwell-learned disentangled representation. Besides, the trained network also\ndemonstrates high potential to generate style-transferred images. \n\n"}
{"id": "1712.09532", "contents": "Title: Consensus-based Sequence Training for Video Captioning Abstract: Captioning models are typically trained using the cross-entropy loss.\nHowever, their performance is evaluated on other metrics designed to better\ncorrelate with human assessments. Recently, it has been shown that\nreinforcement learning (RL) can directly optimize these metrics in tasks such\nas captioning. However, this is computationally costly and requires specifying\na baseline reward at each step to make training converge. We propose a fast\napproach to optimize one's objective of interest through the REINFORCE\nalgorithm. First we show that, by replacing model samples with ground-truth\nsentences, RL training can be seen as a form of weighted cross-entropy loss,\ngiving a fast, RL-based pre-training algorithm. Second, we propose to use the\nconsensus among ground-truth captions of the same video as the baseline reward.\nThis can be computed very efficiently. We call the complete proposal\nConsensus-based Sequence Training (CST). Applied to the MSRVTT video captioning\nbenchmark, our proposals train significantly faster than comparable methods and\nestablish a new state-of-the-art on the task, improving the CIDEr score from\n47.3 to 54.2. \n\n"}
{"id": "1801.00222", "contents": "Title: Limitation of SDMA in Ultra-Dense Small Cell Networks Abstract: Benefitting from multi-user gain brought by multi-antenna techniques, space\ndivision multiple access (SDMA) is capable of significantly enhancing spatial\nthroughput (ST) in wireless networks. Nevertheless, we show in this letter\nthat, even when SDMA is applied, ST would diminish to be zero in ultra-dense\nnetworks (UDN), where small cell base stations (BSs) are fully densified. More\nimportantly, we compare the performance of SDMA, single-user beamforming\n(SU-BF) (one user is served in each cell) and full SDMA (the number of served\nusers equals the number of equipped antennas). Surprisingly, it is shown that\nSU-BF achieves the highest ST and critical density, beyond which ST starts to\ndegrade, in UDN. The results in this work could shed light on the fundamental\nlimitation of SDMA in UDN. \n\n"}
{"id": "1801.01780", "contents": "Title: Probabilistic max-plus schemes for solving Hamilton-Jacobi-Bellman\n  equations Abstract: We consider fully nonlinear Hamilton-Jacobi-Bellman equations associated to\ndiffusion control problems involving a finite set-valued (or switching) control\nand possibly a continuum-valued control. In previous works (Akian, Fodjo, 2016\nand 2017), we introduced a lower complexity probabilistic numerical algorithm\nfor such equations by combining max-plus and numerical probabilistic\napproaches. The max-plus approach is in the spirit of the one of McEneaney,\nKaise and Han (2011), and is based on the distributivity of monotone operators\nwith respect to suprema. The numerical probabilistic approach is in the spirit\nof the one proposed by Fahim, Touzi and Warin (2011). A difficulty of the\nlatter algorithm was in the critical constraints imposed on the Hamiltonian to\nensure the monotonicity of the scheme, hence the convergence of the algorithm.\nHere, we present new probabilistic schemes which are monotone under rather weak\nassumptions, and show error estimates for these schemes. These estimates will\nbe used in further works to study the probabilistic max-plus method. \n\n"}
{"id": "1801.02108", "contents": "Title: SBNet: Sparse Blocks Network for Fast Inference Abstract: Conventional deep convolutional neural networks (CNNs) apply convolution\noperators uniformly in space across all feature maps for hundreds of layers -\nthis incurs a high computational cost for real-time applications. For many\nproblems such as object detection and semantic segmentation, we are able to\nobtain a low-cost computation mask, either from a priori problem knowledge, or\nfrom a low-resolution segmentation network. We show that such computation masks\ncan be used to reduce computation in the high-resolution main network. Variants\nof sparse activation CNNs have previously been explored on small-scale tasks\nand showed no degradation in terms of object classification accuracy, but often\nmeasured gains in terms of theoretical FLOPs without realizing a practical\nspeed-up when compared to highly optimized dense convolution implementations.\nIn this work, we leverage the sparsity structure of computation masks and\npropose a novel tiling-based sparse convolution algorithm. We verified the\neffectiveness of our sparse CNN on LiDAR-based 3D object detection, and we\nreport significant wall-clock speed-ups compared to dense convolution without\nnoticeable loss of accuracy. \n\n"}
{"id": "1801.02781", "contents": "Title: Minimum Throughput Maximization in UAV-Aided Wireless Powered\n  Communication Networks Abstract: This paper investigates unmanned aerial vehicle (UAV)-aided wireless powered\ncommunication network (WPCN) systems where a mobile access point (AP) at the\nUAV serves multiple energy-constrained ground terminals (GTs). Specifically,\nthe UAVs first charge the GTs by transmitting the wireless energy transfer\n(WET) signals in the downlink. Then, by utilizing the harvested wireless energy\nfrom the UAVs, the GTs send their uplink wireless information transmission\n(WIT) signals to the UAVs. In this paper, depending on the operations of the\nUAVs, we adopt two different scenarios, namely integrated UAV and separated UAV\nWPCNs. First, in the integrated UAV WPCN, a UAV acts as a hybrid AP in which\nboth energy transfer and information reception are processed at a single UAV.\nIn contrast, for the separated UAV WPCN, we consider two UAVs each of which\nbehaves as an energy AP and an information AP independently, and thus the\nenergy transfer and the information decoding are separately performed at two\ndifferent UAVs. For both systems, we jointly optimize the trajectories of the\nUAVs, the uplink power control, and the time resource allocation for the WET\nand the WIT to maximize the minimum throughput of the GTs. Since the formulated\nproblems are non-convex, we apply the concave-convex procedure by deriving\nappropriate convex bounds for non-convex constraints. As a result, we propose\niterative algorithms which efficiently identify a local optimal solution for\nthe minimum throughput maximization problems. Simulation results verify the\nefficiency of the proposed algorithms compared to conventional schemes. \n\n"}
{"id": "1801.03116", "contents": "Title: Existence and continuity of solution trajectories of generalized\n  equations with application in electronics Abstract: We consider a special form of parametric generalized equations arising from\nelectronic circuits with AC sources and study the effect of perturbing the\ninput signal on solution trajectories. Using methods of variational analysis\nand strong metric regularity property of an auxiliary map, we are able to prove\nthe regularity properties of the solution trajectories inherited by the input\nsignal. Furthermore, we establish the existence of continuous solution\ntrajectories for the perturbed problem. This can be achieved via a result of\nuniform strong metric regularity for the auxiliary map.\n  Key words and phrases: generalized equations, electronic circuits, strong\nmetric regularity, uniform strong metric regularity, perturbations. \n\n"}
{"id": "1801.03655", "contents": "Title: Can Negligible Cooperation Increase Network Capacity? The Average-Error\n  Case Abstract: In communication networks, cooperative strategies are coding schemes where\nnetwork nodes work together to improve network performance metrics such as\nsum-rate. This work studies encoder cooperation in the setting of a discrete\nmultiple access channel with two encoders and a single decoder. A node in the\nnetwork that is connected to both encoders via rate-limited links, referred to\nas the cooperation facilitator (CF), enables the cooperation strategy.\nPreviously, the authors presented a class of multiple access channels where the\naverage-error sum-capacity has an infinite derivative in the limit where CF\noutput link capacities approach zero. The authors also demonstrated that for\nsome channels, the maximal-error sum-capacity is not continuous at the point\nwhere the output link capacities of the CF equal zero. This work shows that the\nthe average-error sum-capacity is continuous when CF output link capacities\nconverge to zero; that is, the infinite derivative of the average-error\nsum-capacity is not a result of its discontinuity as in the maximal-error case. \n\n"}
{"id": "1801.04985", "contents": "Title: Routeing properties in a Gibbsian model for highly dense multihop\n  networks Abstract: We investigate a probabilistic model for routeing in a multihop ad-hoc\ncommunication network, where each user sends a message to the base station.\nMessages travel in hops via other users, used as relays. Their trajectories are\nchosen at random according to a Gibbs distribution, which favours trajectories\nwith low interference, measured in terms of signal-to-interference ratio. This\nmodel was introduced in our earlier paper [KT18], where we expressed, in the\nlimit of a high density of users, the typical distribution of the family of\ntrajectories in terms of a law of large numbers. In the present work, we derive\nits qualitative properties. We analytically identify the emerging typical\nscenarios in three extreme regimes. We analyse the typical number of hops and\nthe typical length of a hop, and the deviation of the trajectory from the\nstraight line, (1) in the limit of a large communication area and large\ndistances, and (2) in the limit of a strong interference weight. In both\nregimes, the typical trajectory approaches a straight line quickly, in regime\n(1) with equal hop lengths. Interestingly, in regime (1), the typical length of\na hop diverges logarithmically in the distance of the transmitter to the base\nstation. We further analyse (3) local and global repulsive effects of a densely\npopulated subarea on the trajectories. \n\n"}
{"id": "1801.05568", "contents": "Title: Image Captioning using Deep Neural Architectures Abstract: Automatically creating the description of an image using any natural\nlanguages sentence like English is a very challenging task. It requires\nexpertise of both image processing as well as natural language processing. This\npaper discuss about different available models for image captioning task. We\nhave also discussed about how the advancement in the task of object recognition\nand machine translation has greatly improved the performance of image\ncaptioning model in recent years. In addition to that we have discussed how\nthis model can be implemented. In the end, we have also evaluated the\nperformance of model using standard evaluation matrices. \n\n"}
{"id": "1801.05599", "contents": "Title: Additive Margin Softmax for Face Verification Abstract: In this paper, we propose a conceptually simple and geometrically\ninterpretable objective function, i.e. additive margin Softmax (AM-Softmax),\nfor deep face verification. In general, the face verification task can be\nviewed as a metric learning problem, so learning large-margin face features\nwhose intra-class variation is small and inter-class difference is large is of\ngreat importance in order to achieve good performance. Recently, Large-margin\nSoftmax and Angular Softmax have been proposed to incorporate the angular\nmargin in a multiplicative manner. In this work, we introduce a novel additive\nangular margin for the Softmax loss, which is intuitively appealing and more\ninterpretable than the existing works. We also emphasize and discuss the\nimportance of feature normalization in the paper. Most importantly, our\nexperiments on LFW BLUFR and MegaFace show that our additive margin softmax\nloss consistently performs better than the current state-of-the-art methods\nusing the same network architecture and training dataset. Our code has also\nbeen made available at https://github.com/happynear/AMSoftmax \n\n"}
{"id": "1801.05606", "contents": "Title: Multi-View Stereo 3D Edge Reconstruction Abstract: This paper presents a novel method for the reconstruction of 3D edges in\nmulti-view stereo scenarios. Previous research in the field typically relied on\nvideo sequences and limited the reconstruction process to either straight\nline-segments, or edge-points, i.e., 3D points that correspond to image edges.\nWe instead propose a system, denoted as EdgeGraph3D, able to recover both\nstraight and curved 3D edges from an unordered image sequence. A second\ncontribution of this work is a graph-based representation for 2D edges that\nallows the identification of the most structurally significant edges detected\nin an image. We integrate EdgeGraph3D in a multi-view stereo reconstruction\npipeline and analyze the benefits provided by 3D edges to the accuracy of the\nrecovered surfaces. We evaluate the effectiveness of our approach on multiple\ndatasets from two different collections in the multi-view stereo literature.\nExperimental results demonstrate the ability of EdgeGraph3D to work in presence\nof strong illumination changes and reflections, which are usually detrimental\nto the effectiveness of classical photometric reconstruction systems. \n\n"}
{"id": "1801.05944", "contents": "Title: PTB-TIR: A Thermal Infrared Pedestrian Tracking Benchmark Abstract: Thermal infrared (TIR) pedestrian tracking is one of the important components\namong numerous applications of computer vision, which has a major advantage: it\ncan track pedestrians in total darkness. The ability to evaluate the TIR\npedestrian tracker fairly, on a benchmark dataset, is significant for the\ndevelopment of this field. However, there is not a benchmark dataset. In this\npaper, we develop a TIR pedestrian tracking dataset for the TIR pedestrian\ntracker evaluation. The dataset includes 60 thermal sequences with manual\nannotations. Each sequence has nine attribute labels for the attribute based\nevaluation. In addition to the dataset, we carry out the large-scale evaluation\nexperiments on our benchmark dataset using nine publicly available trackers.\nThe experimental results help us understand the strengths and weaknesses of\nthese trackers.In addition, in order to gain more insight into the TIR\npedestrian tracker, we divide its functions into three components: feature\nextractor, motion model, and observation model. Then, we conduct three\ncomparison experiments on our benchmark dataset to validate how each component\naffects the tracker's performance. The findings of these experiments provide\nsome guidelines for future research. The dataset and evaluation toolkit can be\ndownloaded at {https://github.com/QiaoLiuHit/PTB-TIR_Evaluation_toolkit}. \n\n"}
{"id": "1801.07388", "contents": "Title: Let's Dance: Learning From Online Dance Videos Abstract: In recent years, deep neural network approaches have naturally extended to\nthe video domain, in their simplest case by aggregating per-frame\nclassifications as a baseline for action recognition. A majority of the work in\nthis area extends from the imaging domain, leading to visual-feature heavy\napproaches on temporal data. To address this issue we introduce \"Let's Dance\",\na 1000 video dataset (and growing) comprised of 10 visually overlapping dance\ncategories that require motion for their classification. We stress the\nimportant of human motion as a key distinguisher in our work given that, as we\nshow in this work, visual information is not sufficient to classify\nmotion-heavy categories. We compare our datasets' performance using imaging\ntechniques with UCF-101 and demonstrate this inherent difficulty. We present a\ncomparison of numerous state-of-the-art techniques on our dataset using three\ndifferent representations (video, optical flow and multi-person pose data) in\norder to analyze these approaches. We discuss the motion parameterization of\neach of them and their value in learning to categorize online dance videos.\nLastly, we release this dataset (and its three representations) for the\nresearch community to use. \n\n"}
{"id": "1801.07829", "contents": "Title: Dynamic Graph CNN for Learning on Point Clouds Abstract: Point clouds provide a flexible geometric representation suitable for\ncountless applications in computer graphics; they also comprise the raw output\nof most 3D data acquisition devices. While hand-designed features on point\nclouds have long been proposed in graphics and vision, however, the recent\noverwhelming success of convolutional neural networks (CNNs) for image analysis\nsuggests the value of adapting insight from CNN to the point cloud world. Point\nclouds inherently lack topological information so designing a model to recover\ntopology can enrich the representation power of point clouds. To this end, we\npropose a new neural network module dubbed EdgeConv suitable for CNN-based\nhigh-level tasks on point clouds including classification and segmentation.\nEdgeConv acts on graphs dynamically computed in each layer of the network. It\nis differentiable and can be plugged into existing architectures. Compared to\nexisting modules operating in extrinsic space or treating each point\nindependently, EdgeConv has several appealing properties: It incorporates local\nneighborhood information; it can be stacked applied to learn global shape\nproperties; and in multi-layer systems affinity in feature space captures\nsemantic characteristics over potentially long distances in the original\nembedding. We show the performance of our model on standard benchmarks\nincluding ModelNet40, ShapeNetPart, and S3DIS. \n\n"}
{"id": "1801.08267", "contents": "Title: Visual Weather Temperature Prediction Abstract: In this paper, we attempt to employ convolutional recurrent neural networks\nfor weather temperature estimation using only image data. We study ambient\ntemperature estimation based on deep neural networks in two scenarios a)\nestimating temperature of a single outdoor image, and b) predicting temperature\nof the last image in an image sequence. In the first scenario, visual features\nare extracted by a convolutional neural network trained on a large-scale image\ndataset. We demonstrate that promising performance can be obtained, and analyze\nhow volume of training data influences performance. In the second scenario, we\nconsider the temporal evolution of visual appearance, and construct a recurrent\nneural network to predict the temperature of the last image in a given image\nsequence. We obtain better prediction accuracy compared to the state-of-the-art\nmodels. Further, we investigate how performance varies when information is\nextracted from different scene regions, and when images are captured in\ndifferent daytime hours. Our approach further reinforces the idea of using only\nvisual information for cost efficient weather prediction in the future. \n\n"}
{"id": "1801.08297", "contents": "Title: NDDR-CNN: Layerwise Feature Fusing in Multi-Task CNNs by Neural\n  Discriminative Dimensionality Reduction Abstract: In this paper, we propose a novel Convolutional Neural Network (CNN)\nstructure for general-purpose multi-task learning (MTL), which enables\nautomatic feature fusing at every layer from different tasks. This is in\ncontrast with the most widely used MTL CNN structures which empirically or\nheuristically share features on some specific layers (e.g., share all the\nfeatures except the last convolutional layer). The proposed layerwise feature\nfusing scheme is formulated by combining existing CNN components in a novel\nway, with clear mathematical interpretability as discriminative dimensionality\nreduction, which is referred to as Neural Discriminative Dimensionality\nReduction (NDDR). Specifically, we first concatenate features with the same\nspatial resolution from different tasks according to their channel dimension.\nThen, we show that the discriminative dimensionality reduction can be fulfilled\nby 1x1 Convolution, Batch Normalization, and Weight Decay in one CNN. The use\nof existing CNN components ensures the end-to-end training and the\nextensibility of the proposed NDDR layer to various state-of-the-art CNN\narchitectures in a \"plug-and-play\" manner. The detailed ablation analysis shows\nthat the proposed NDDR layer is easy to train and also robust to different\nhyperparameters. Experiments on different task sets with various base network\narchitectures demonstrate the promising performance and desirable\ngeneralizability of our proposed method. The code of our paper is available at\nhttps://github.com/ethanygao/NDDR-CNN. \n\n"}
{"id": "1801.08577", "contents": "Title: Effective Building Block Design for Deep Convolutional Neural Networks\n  using Search Abstract: Deep learning has shown promising results on many machine learning tasks but\nDL models are often complex networks with large number of neurons and layers,\nand recently, complex layer structures known as building blocks. Finding the\nbest deep model requires a combination of finding both the right architecture\nand the correct set of parameters appropriate for that architecture. In\naddition, this complexity (in terms of layer types, number of neurons, and\nnumber of layers) also present problems with generalization since larger\nnetworks are easier to overfit to the data. In this paper, we propose a search\nframework for finding effective architectural building blocks for convolutional\nneural networks (CNN). Our approach is much faster at finding models that are\nclose to state-of-the-art in performance. In addition, the models discovered by\nour approach are also smaller than models discovered by similar techniques. We\nachieve these twin advantages by designing our search space in such a way that\nit searches over a reduced set of state-of-the-art building blocks for CNNs\nincluding residual block, inception block, inception-residual block, ResNeXt\nblock and many others. We apply this technique to generate models for multiple\nimage datasets and show that these models achieve performance comparable to\nstate-of-the-art (and even surpassing the state-of-the-art in one case). We\nalso show that learned models are transferable between datasets. \n\n"}
{"id": "1801.08985", "contents": "Title: Object category learning and retrieval with weak supervision Abstract: We consider the problem of retrieving objects from image data and learning to\nclassify them into meaningful semantic categories with minimal supervision. To\nthat end, we propose a fully differentiable unsupervised deep clustering\napproach to learn semantic classes in an end-to-end fashion without individual\nclass labeling using only unlabeled object proposals. The key contributions of\nour work are 1) a kmeans clustering objective where the clusters are learned as\nparameters of the network and are represented as memory units, and 2)\nsimultaneously building a feature representation, or embedding, while learning\nto cluster it. This approach shows promising results on two popular computer\nvision datasets: on CIFAR10 for clustering objects, and on the more complex and\nchallenging Cityscapes dataset for semantically discovering classes which\nvisually correspond to cars, people, and bicycles. Currently, the only\nsupervision provided is segmentation objectness masks, but this method can be\nextended to use an unsupervised objectness-based object generation mechanism\nwhich will make the approach completely unsupervised. \n\n"}
{"id": "1801.09242", "contents": "Title: Joint Voxel and Coordinate Regression for Accurate 3D Facial Landmark\n  Localization Abstract: 3D face shape is more expressive and viewpoint-consistent than its 2D\ncounterpart. However, 3D facial landmark localization in a single image is\nchallenging due to the ambiguous nature of landmarks under 3D perspective.\nExisting approaches typically adopt a suboptimal two-step strategy, performing\n2D landmark localization followed by depth estimation. In this paper, we\npropose the Joint Voxel and Coordinate Regression (JVCR) method for 3D facial\nlandmark localization, addressing it more effectively in an end-to-end fashion.\nFirst, a compact volumetric representation is proposed to encode the per-voxel\nlikelihood of positions being the 3D landmarks. The dimensionality of such a\nrepresentation is fixed regardless of the number of target landmarks, so that\nthe curse of dimensionality could be avoided. Then, a stacked hourglass network\nis adopted to estimate the volumetric representation from coarse to fine,\nfollowed by a 3D convolution network that takes the estimated volume as input\nand regresses 3D coordinates of the face shape. In this way, the 3D structural\nconstraints between landmarks could be learned by the neural network in a more\nefficient manner. Moreover, the proposed pipeline enables end-to-end training\nand improves the robustness and accuracy of 3D facial landmark localization.\nThe effectiveness of our approach is validated on the 3DFAW and AFLW2000-3D\ndatasets. Experimental results show that the proposed method achieves\nstate-of-the-art performance in comparison with existing methods. \n\n"}
{"id": "1801.09468", "contents": "Title: DeepSIC: Deep Semantic Image Compression Abstract: Incorporating semantic information into the codecs during image compression\ncan significantly reduce the repetitive computation of fundamental semantic\nanalysis (such as object recognition) in client-side applications. The same\npractice also enable the compressed code to carry the image semantic\ninformation during storage and transmission. In this paper, we propose a\nconcept called Deep Semantic Image Compression (DeepSIC) and put forward two\nnovel architectures that aim to reconstruct the compressed image and generate\ncorresponding semantic representations at the same time. The first architecture\nperforms semantic analysis in the encoding process by reserving a portion of\nthe bits from the compressed code to store the semantic representations. The\nsecond performs semantic analysis in the decoding step with the feature maps\nthat are embedded in the compressed code. In both architectures, the feature\nmaps are shared by the compression and the semantic analytics modules. To\nvalidate our approaches, we conduct experiments on the publicly available\nbenchmarking datasets and achieve promising results. We also provide a thorough\nanalysis of the advantages and disadvantages of the proposed technique. \n\n"}
{"id": "1801.10068", "contents": "Title: Deep Adversarial Attention Alignment for Unsupervised Domain Adaptation:\n  the Benefit of Target Expectation Maximization Abstract: In this paper, we make two contributions to unsupervised domain adaptation\n(UDA) using the convolutional neural network (CNN). First, our approach\ntransfers knowledge in all the convolutional layers through attention\nalignment. Most previous methods align high-level representations, e.g.,\nactivations of the fully connected (FC) layers. In these methods, however, the\nconvolutional layers which underpin critical low-level domain knowledge cannot\nbe updated directly towards reducing domain discrepancy. Specifically, we\nassume that the discriminative regions in an image are relatively invariant to\nimage style changes. Based on this assumption, we propose an attention\nalignment scheme on all the target convolutional layers to uncover the\nknowledge shared by the source domain. Second, we estimate the posterior label\ndistribution of the unlabeled data for target network training. Previous\nmethods, which iteratively update the pseudo labels by the target network and\nrefine the target network by the updated pseudo labels, are vulnerable to label\nestimation errors. Instead, our approach uses category distribution to\ncalculate the cross-entropy loss for training, thereby ameliorating the error\naccumulation of the estimated labels. The two contributions allow our approach\nto outperform the state-of-the-art methods by +2.6% on the Office-31 dataset. \n\n"}
{"id": "1801.10121", "contents": "Title: Image Captioning at Will: A Versatile Scheme for Effectively Injecting\n  Sentiments into Image Descriptions Abstract: Automatic image captioning has recently approached human-level performance\ndue to the latest advances in computer vision and natural language\nunderstanding. However, most of the current models can only generate plain\nfactual descriptions about the content of a given image. However, for human\nbeings, image caption writing is quite flexible and diverse, where additional\nlanguage dimensions, such as emotion, humor and language styles, are often\nincorporated to produce diverse, emotional, or appealing captions. In\nparticular, we are interested in generating sentiment-conveying image\ndescriptions, which has received little attention. The main challenge is how to\neffectively inject sentiments into the generated captions without altering the\nsemantic matching between the visual content and the generated descriptions. In\nthis work, we propose two different models, which employ different schemes for\ninjecting sentiments into image captions. Compared with the few existing\napproaches, the proposed models are much simpler and yet more effective. The\nexperimental results show that our model outperform the state-of-the-art models\nin generating sentimental (i.e., sentiment-bearing) image captions. In\naddition, we can also easily manipulate the model by assigning different\nsentiments to the testing image to generate captions with the corresponding\nsentiments. \n\n"}
{"id": "1802.01873", "contents": "Title: Every Smile is Unique: Landmark-Guided Diverse Smile Generation Abstract: Each smile is unique: one person surely smiles in different ways (e.g.,\nclosing/opening the eyes or mouth). Given one input image of a neutral face,\ncan we generate multiple smile videos with distinctive characteristics? To\ntackle this one-to-many video generation problem, we propose a novel deep\nlearning architecture named Conditional Multi-Mode Network (CMM-Net). To better\nencode the dynamics of facial expressions, CMM-Net explicitly exploits facial\nlandmarks for generating smile sequences. Specifically, a variational\nauto-encoder is used to learn a facial landmark embedding. This single\nembedding is then exploited by a conditional recurrent network which generates\na landmark embedding sequence conditioned on a specific expression (e.g.,\nspontaneous smile). Next, the generated landmark embeddings are fed into a\nmulti-mode recurrent landmark generator, producing a set of landmark sequences\nstill associated to the given smile class but clearly distinct from each other.\nFinally, these landmark sequences are translated into face videos. Our\nexperimental results demonstrate the effectiveness of our CMM-Net in generating\nrealistic videos of multiple smile expressions. \n\n"}
{"id": "1802.02018", "contents": "Title: Orthogonally Regularized Deep Networks For Image Super-resolution Abstract: Deep learning methods, in particular trained Convolutional Neural Networks\n(CNNs) have recently been shown to produce compelling state-of-the-art results\nfor single image Super-Resolution (SR). Invariably, a CNN is learned to map the\nlow resolution (LR) image to its corresponding high resolution (HR) version in\nthe spatial domain. Aiming for faster inference and more efficient solutions\nthan solving the SR problem in the spatial domain, we propose a novel network\nstructure for learning the SR mapping function in an image transform domain,\nspecifically the Discrete Cosine Transform (DCT). As a first contribution, we\nshow that DCT can be integrated into the network structure as a Convolutional\nDCT (CDCT) layer. We further extend the network to allow the CDCT layer to\nbecome trainable (i.e. optimizable). Because this layer represents an image\ntransform, we enforce pairwise orthogonality constraints on the individual\nbasis functions/filters. This Orthogonally Regularized Deep SR network (ORDSR)\nsimplifies the SR task by taking advantage of image transform domain while\nadapting the design of transform basis to the training image set. \n\n"}
{"id": "1802.02207", "contents": "Title: Automated dataset generation for image recognition using the example of\n  taxonomy Abstract: This master thesis addresses the subject of automatically generating a\ndataset for image recognition, which takes a lot of time when being done\nmanually. As the thesis was written with motivation from the context of the\nbiodiversity workgroup at the City University of Applied Sciences Bremen, the\nclassification of taxonomic entries was chosen as an exemplary use case. In\norder to automate the dataset creation, a prototype was conceptualized and\nimplemented after working out knowledge basics and analyzing requirements for\nit. It makes use of an pre-trained abstract artificial intelligence which is\nable to sort out images that do not contain the desired content. Subsequent to\nthe implementation and the automated dataset creation resulting from it, an\nevaluation was performed. Other, manually collected datasets were compared to\nthe one the prototype produced in means of specifications and accuracy. The\nresults were more than satisfactory and showed that automatically generating a\ndataset for image recognition is not only possible, but also might be a decent\nalternative to spending time and money in doing this task manually. At the very\nend of this work, an idea of how to use the principle of employing abstract\nartificial intelligences for step-by-step classification of deeper taxonomic\nlayers in a productive system is presented and discussed. \n\n"}
{"id": "1802.02208", "contents": "Title: Automatic Pavement Crack Detection Based on Structured Prediction with\n  the Convolutional Neural Network Abstract: Automated pavement crack detection is a challenging task that has been\nresearched for decades due to the complicated pavement conditions in real\nworld. In this paper, a supervised method based on deep learning is proposed,\nwhich has the capability of dealing with different pavement conditions.\nSpecifically, a convolutional neural network (CNN) is used to learn the\nstructure of the cracks from raw images, without any preprocessing. Small\npatches are extracted from crack images as inputs to generate a large training\ndatabase, a CNN is trained and crack detection is modeled as a multi-label\nclassification problem. Typically, crack pixels are much fewer than non-crack\npixels. To deal with the problem with severely imbalanced data, a strategy with\nmodifying the ratio of positive to negative samples is proposed. The method is\ntested on two public databases and compared with five existing methods.\nExperimental results show that it outperforms the other methods. \n\n"}
{"id": "1802.02485", "contents": "Title: BROJA-2PID: A robust estimator for bivariate partial information\n  decomposition Abstract: Makkeh, Theis, and Vicente found in [8] that Cone Programming model is the\nmost robust to compute the Bertschinger et al. partial information decompostion\n(BROJA PID) measure [1]. We developed a production-quality robust software that\ncomputes the BROJA PID measure based on the Cone Programming model. In this\npaper, we prove the important property of strong duality for the Cone Program\nand prove an equivalence between the Cone Program and the original Convex\nproblem. Then describe in detail our software and how to use it.\\newline\\indent \n\n"}
{"id": "1802.02532", "contents": "Title: A Spatial Mapping Algorithm with Applications in Deep Learning-Based\n  Structure Classification Abstract: Convolutional Neural Network (CNN)-based machine learning systems have made\nbreakthroughs in feature extraction and image recognition tasks in two\ndimensions (2D). Although there is significant ongoing work to apply CNN\ntechnology to domains involving complex 3D data, the success of such efforts\nhas been constrained, in part, by limitations in data representation\ntechniques. Most current approaches rely upon low-resolution 3D models,\nstrategic limitation of scope in the 3D space, or the application of lossy\nprojection techniques to allow for the use of 2D CNNs. To address this issue,\nwe present a mapping algorithm that converts 3D structures to 2D and 1D data\ngrids by mapping a traversal of a 3D space-filling curve to the traversal of\ncorresponding 2D and 1D curves. We explore the performance of 2D and 1D CNNs\ntrained on data encoded with our method versus comparable volumetric CNNs\noperating upon raw 3D data from a popular benchmarking dataset. Our experiments\ndemonstrate that both 2D and 1D representations of 3D data generated via our\nmethod preserve a significant proportion of the 3D data's features in forms\nlearnable by CNNs. Furthermore, we demonstrate that our method of encoding 3D\ndata into lower-dimensional representations allows for decreased CNN training\ntime cost, increased original 3D model rendering resolutions, and supports\nincreased numbers of data channels when compared to purely volumetric\napproaches. This demonstration is accomplished in the context of a structural\nbiology classification task wherein we train 3D, 2D, and 1D CNNs on examples of\ntwo homologous branches within the Ras protein family. The essential\ncontribution of this paper is the introduction of a dimensionality-reduction\nmethod that may ease the application of powerful deep learning tools to domains\ncharacterized by complex structural data. \n\n"}
{"id": "1802.03494", "contents": "Title: AMC: AutoML for Model Compression and Acceleration on Mobile Devices Abstract: Model compression is a critical technique to efficiently deploy neural\nnetwork models on mobile devices which have limited computation resources and\ntight power budgets. Conventional model compression techniques rely on\nhand-crafted heuristics and rule-based policies that require domain experts to\nexplore the large design space trading off among model size, speed, and\naccuracy, which is usually sub-optimal and time-consuming. In this paper, we\npropose AutoML for Model Compression (AMC) which leverage reinforcement\nlearning to provide the model compression policy. This learning-based\ncompression policy outperforms conventional rule-based compression policy by\nhaving higher compression ratio, better preserving the accuracy and freeing\nhuman labor. Under 4x FLOPs reduction, we achieved 2.7% better accuracy than\nthe handcrafted model compression policy for VGG-16 on ImageNet. We applied\nthis automated, push-the-button compression pipeline to MobileNet and achieved\n1.81x speedup of measured inference latency on an Android phone and 1.43x\nspeedup on the Titan XP GPU, with only 0.1% loss of ImageNet Top-1 accuracy. \n\n"}
{"id": "1802.03528", "contents": "Title: Coverless information hiding based on Generative Model Abstract: A new coverless image information hiding method based on generative model is\nproposed, we feed the secret image to the generative model database, and\ngenerate a meaning-normal and independent image different from the secret\nimage, then, the generated image is transmitted to the receiver and is fed to\nthe generative model database to generate another image visually the same as\nthe secret image. So we only need to transmit the meaning-normal image which is\nnot related to the secret image, and we can achieve the same effect as the\ntransmission of the secret image. This is the first time to propose the\ncoverless image information hiding method based on generative model, compared\nwith the traditional image steganography, the transmitted image does not embed\nany information of the secret image in this method, therefore, can effectively\nresist steganalysis tools. Experimental results show that our method has high\ncapacity, safety and reliability. \n\n"}
{"id": "1802.04084", "contents": "Title: Randomized Block Cubic Newton Method Abstract: We study the problem of minimizing the sum of three convex functions: a\ndifferentiable, twice-differentiable and a non-smooth term in a high\ndimensional setting. To this effect we propose and analyze a randomized block\ncubic Newton (RBCN) method, which in each iteration builds a model of the\nobjective function formed as the sum of the natural models of its three\ncomponents: a linear model with a quadratic regularizer for the differentiable\nterm, a quadratic model with a cubic regularizer for the twice differentiable\nterm, and perfect (proximal) model for the nonsmooth term. Our method in each\niteration minimizes the model over a random subset of blocks of the search\nvariable. RBCN is the first algorithm with these properties, generalizing\nseveral existing methods, matching the best known bounds in all special cases.\nWe establish ${\\cal O}(1/\\epsilon)$, ${\\cal O}(1/\\sqrt{\\epsilon})$ and ${\\cal\nO}(\\log (1/\\epsilon))$ rates under different assumptions on the component\nfunctions. Lastly, we show numerically that our method outperforms the\nstate-of-the-art on a variety of machine learning problems, including cubically\nregularized least-squares, logistic regression with constraints, and Poisson\nregression. \n\n"}
{"id": "1802.04636", "contents": "Title: Modeling of Facial Aging and Kinship: A Survey Abstract: Computational facial models that capture properties of facial cues related to\naging and kinship increasingly attract the attention of the research community,\nenabling the development of reliable methods for age progression, age\nestimation, age-invariant facial characterization, and kinship verification\nfrom visual data. In this paper, we review recent advances in modeling of\nfacial aging and kinship. In particular, we provide an up-to date, complete\nlist of available annotated datasets and an in-depth analysis of geometric,\nhand-crafted, and learned facial representations that are used for facial aging\nand kinship characterization. Moreover, evaluation protocols and metrics are\nreviewed and notable experimental results for each surveyed task are analyzed.\nThis survey allows us to identify challenges and discuss future research\ndirections for the development of robust facial models in real-world\nconditions. \n\n"}
{"id": "1802.05342", "contents": "Title: Spatial Coherence of Oriented White Matter Microstructure: Applications\n  to White Matter Regions Associated with Genetic Similarity Abstract: We present a method to discover differences between populations with respect\nto the spatial coherence of their oriented white matter microstructure in\narbitrarily shaped white matter regions. This method is applied to diffusion\nMRI scans of a subset of the Human Connectome Project dataset: 57 pairs of\nmonozygotic and 52 pairs of dizygotic twins. After controlling for\nmorphological similarity between twins, we identify 3.7% of all white matter as\nbeing associated with genetic similarity (35.1k voxels, $p < 10^{-4}$, false\ndiscovery rate 1.5%), 75% of which spatially clusters into twenty-two\ncontiguous white matter regions. Furthermore, we show that the orientation\nsimilarity within these regions generalizes to a subset of 47 pairs of non-twin\nsiblings, and show that these siblings are on average as similar as dizygotic\ntwins. The regions are located in deep white matter including the superior\nlongitudinal fasciculus, the optic radiations, the middle cerebellar peduncle,\nthe corticospinal tract, and within the anterior temporal lobe, as well as the\ncerebellum, brain stem, and amygdalae.\n  These results extend previous work using undirected fractional anisotrophy\nfor measuring putative heritable influences in white matter. Our\nmultidirectional extension better accounts for crossing fiber connections\nwithin voxels. This bottom up approach has at its basis a novel measurement of\ncoherence within neighboring voxel dyads between subjects, and avoids some of\nthe fundamental ambiguities encountered with tractographic approaches to white\nmatter analysis that estimate global connectivity. \n\n"}
{"id": "1802.05584", "contents": "Title: Convolutional Analysis Operator Learning: Acceleration and Convergence Abstract: Convolutional operator learning is gaining attention in many signal\nprocessing and computer vision applications. Learning kernels has mostly relied\non so-called patch-domain approaches that extract and store many overlapping\npatches across training signals. Due to memory demands, patch-domain methods\nhave limitations when learning kernels from large datasets -- particularly with\nmulti-layered structures, e.g., convolutional neural networks -- or when\napplying the learned kernels to high-dimensional signal recovery problems. The\nso-called convolution approach does not store many overlapping patches, and\nthus overcomes the memory problems particularly with careful algorithmic\ndesigns; it has been studied within the \"synthesis\" signal model, e.g.,\nconvolutional dictionary learning. This paper proposes a new convolutional\nanalysis operator learning (CAOL) framework that learns an analysis sparsifying\nregularizer with the convolution perspective, and develops a new convergent\nBlock Proximal Extrapolated Gradient method using a Majorizer (BPEG-M) to solve\nthe corresponding block multi-nonconvex problems. To learn diverse filters\nwithin the CAOL framework, this paper introduces an orthogonality constraint\nthat enforces a tight-frame filter condition, and a regularizer that promotes\ndiversity between filters. Numerical experiments show that, with sharp\nmajorizers, BPEG-M significantly accelerates the CAOL convergence rate compared\nto the state-of-the-art block proximal gradient (BPG) method. Numerical\nexperiments for sparse-view computational tomography show that a convolutional\nsparsifying regularizer learned via CAOL significantly improves reconstruction\nquality compared to a conventional edge-preserving regularizer. Using more and\nwider kernels in a learned regularizer better preserves edges in reconstructed\nimages. \n\n"}
{"id": "1802.05747", "contents": "Title: Systematic Weight Pruning of DNNs using Alternating Direction Method of\n  Multipliers Abstract: We present a systematic weight pruning framework of deep neural networks\n(DNNs) using the alternating direction method of multipliers (ADMM). We first\nformulate the weight pruning problem of DNNs as a constrained nonconvex\noptimization problem, and then adopt the ADMM framework for systematic weight\npruning. We show that ADMM is highly suitable for weight pruning due to the\ncomputational efficiency it offers. We achieve a much higher compression ratio\ncompared with prior work while maintaining the same test accuracy, together\nwith a faster convergence rate. Our models are released at\nhttps://github.com/KaiqiZhang/admm-pruning \n\n"}
{"id": "1802.05763", "contents": "Title: ASP:A Fast Adversarial Attack Example Generation Framework based on\n  Adversarial Saliency Prediction Abstract: With the excellent accuracy and feasibility, the Neural Networks have been\nwidely applied into the novel intelligent applications and systems. However,\nwith the appearance of the Adversarial Attack, the NN based system performance\nbecomes extremely vulnerable:the image classification results can be\narbitrarily misled by the adversarial examples, which are crafted images with\nhuman unperceivable pixel-level perturbation. As this raised a significant\nsystem security issue, we implemented a series of investigations on the\nadversarial attack in this work: We first identify an image's pixel\nvulnerability to the adversarial attack based on the adversarial saliency\nanalysis. By comparing the analyzed saliency map and the adversarial\nperturbation distribution, we proposed a new evaluation scheme to\ncomprehensively assess the adversarial attack precision and efficiency. Then,\nwith a novel adversarial saliency prediction method, a fast adversarial example\ngeneration framework, namely \"ASP\", is proposed with significant attack\nefficiency improvement and dramatic computation cost reduction. Compared to the\nprevious methods, experiments show that ASP has at most 12 times speed-up for\nadversarial example generation, 2 times lower perturbation rate, and high\nattack success rate of 87% on both MNIST and Cifar10. ASP can be also well\nutilized to support the data-hungry NN adversarial training. By reducing the\nattack success rate as much as 90%, ASP can quickly and effectively enhance the\ndefense capability of NN based system to the adversarial attacks. \n\n"}
{"id": "1802.07129", "contents": "Title: Deep BCD-Net Using Identical Encoding-Decoding CNN Structures for\n  Iterative Image Recovery Abstract: In \"extreme\" computational imaging that collects extremely undersampled or\nnoisy measurements, obtaining an accurate image within a reasonable computing\ntime is challenging. Incorporating image mapping convolutional neural networks\n(CNN) into iterative image recovery has great potential to resolve this issue.\nThis paper 1) incorporates image mapping CNN using identical convolutional\nkernels in both encoders and decoders into a block coordinate descent (BCD)\nsignal recovery method and 2) applies alternating direction method of\nmultipliers to train the aforementioned image mapping CNN. We refer to the\nproposed recurrent network as BCD-Net using identical encoding-decoding CNN\nstructures. Numerical experiments show that, for a) denoising low\nsignal-to-noise-ratio images and b) extremely undersampled magnetic resonance\nimaging, the proposed BCD-Net achieves significantly more accurate image\nrecovery, compared to BCD-Net using distinct encoding-decoding structures\nand/or the conventional image recovery model using both wavelets and total\nvariation. \n\n"}
{"id": "1802.07303", "contents": "Title: MoNet: Moments Embedding Network Abstract: Bilinear pooling has been recently proposed as a feature encoding layer,\nwhich can be used after the convolutional layers of a deep network, to improve\nperformance in multiple vision tasks. Different from conventional global\naverage pooling or fully connected layer, bilinear pooling gathers 2nd order\ninformation in a translation invariant fashion. However, a serious drawback of\nthis family of pooling layers is their dimensionality explosion. Approximate\npooling methods with compact properties have been explored towards resolving\nthis weakness. Additionally, recent results have shown that significant\nperformance gains can be achieved by adding 1st order information and applying\nmatrix normalization to regularize unstable higher order information. However,\ncombining compact pooling with matrix normalization and other order information\nhas not been explored until now. In this paper, we unify bilinear pooling and\nthe global Gaussian embedding layers through the empirical moment matrix. In\naddition, we propose a novel sub-matrix square-root layer, which can be used to\nnormalize the output of the convolution layer directly and mitigate the\ndimensionality problem with off-the-shelf compact pooling methods. Our\nexperiments on three widely used fine-grained classification datasets\nillustrate that our proposed architecture, MoNet, can achieve similar or better\nperformance than with the state-of-art G2DeNet. Furthermore, when combined with\ncompact pooling technique, MoNet obtains comparable performance with encoded\nfeatures with 96% less dimensions. \n\n"}
{"id": "1802.07789", "contents": "Title: Semantic Segmentation Refinement by Monte Carlo Region Growing of High\n  Confidence Detections Abstract: Despite recent improvements using fully convolutional networks, in general,\nthe segmentation produced by most state-of-the-art semantic segmentation\nmethods does not show satisfactory adherence to the object boundaries. We\npropose a method to refine the segmentation results generated by such deep\nlearning models. Our method takes as input the confidence scores generated by a\npixel-dense segmentation network and re-labels pixels with low confidence\nlevels. The re-labeling approach employs a region growing mechanism that\naggregates these pixels to neighboring areas with high confidence scores and\nsimilar appearance. In order to correct the labels of pixels that were\nincorrectly classified with high confidence level by the semantic segmentation\nalgorithm, we generate multiple region growing steps through a Monte Carlo\nsampling of the seeds of the regions. Our method improves the accuracy of a\nstate-of-the-art fully convolutional semantic segmentation approach on the\npublicly available COCO and PASCAL datasets, and it shows significantly better\nresults on selected sequences of the finely-annotated DAVIS dataset. \n\n"}
{"id": "1802.07796", "contents": "Title: Continuous Relaxation of MAP Inference: A Nonconvex Perspective Abstract: In this paper, we study a nonconvex continuous relaxation of MAP inference in\ndiscrete Markov random fields (MRFs). We show that for arbitrary MRFs, this\nrelaxation is tight, and a discrete stationary point of it can be easily\nreached by a simple block coordinate descent algorithm. In addition, we study\nthe resolution of this relaxation using popular gradient methods, and further\npropose a more effective solution using a multilinear decomposition framework\nbased on the alternating direction method of multipliers (ADMM). Experiments on\nmany real-world problems demonstrate that the proposed ADMM significantly\noutperforms other nonconvex relaxation based methods, and compares favorably\nwith state of the art MRF optimization algorithms in different settings. \n\n"}
{"id": "1802.07918", "contents": "Title: Video Person Re-identification by Temporal Residual Learning Abstract: In this paper, we propose a novel feature learning framework for video person\nre-identification (re-ID). The proposed framework largely aims to exploit the\nadequate temporal information of video sequences and tackle the poor spatial\nalignment of moving pedestrians. More specifically, for exploiting the temporal\ninformation, we design a temporal residual learning (TRL) module to\nsimultaneously extract the generic and specific features of consecutive frames.\nThe TRL module is equipped with two bi-directional LSTM (BiLSTM), which are\nrespectively responsible to describe a moving person in different aspects,\nproviding complementary information for better feature representations. To deal\nwith the poor spatial alignment in video re-ID datasets, we propose a\nspatial-temporal transformer network (ST^2N) module. Transformation parameters\nin the ST^2N module are learned by leveraging the high-level semantic\ninformation of the current frame as well as the temporal context knowledge from\nother frames. The proposed ST^2N module with less learnable parameters allows\neffective person alignments under significant appearance changes. Extensive\nexperimental results on the large-scale MARS, PRID2011, ILIDS-VID and SDU-VID\ndatasets demonstrate that the proposed method achieves consistently superior\nperformance and outperforms most of the very recent state-of-the-art methods. \n\n"}
{"id": "1802.08941", "contents": "Title: Gradient Primal-Dual Algorithm Converges to Second-Order Stationary\n  Solutions for Nonconvex Distributed Optimization Abstract: In this work, we study two first-order primal-dual based algorithms, the\nGradient Primal-Dual Algorithm (GPDA) and the Gradient Alternating Direction\nMethod of Multipliers (GADMM), for solving a class of linearly constrained\nnon-convex optimization problems. We show that with random initialization of\nthe primal and dual variables, both algorithms are able to compute second-order\nstationary solutions (ss2) with probability one. This is the first result\nshowing that primal-dual algorithm is capable of finding ss2 when only using\nfirst-order information, it also extends the existing results for first-order,\nbut primal-only algorithms.\n  An important implication of our result is that it also gives rise to the\nfirst global convergence result to the ss2, for two classes of unconstrained\ndistributed non-convex learning problems over multi-agent networks. \n\n"}
{"id": "1802.09153", "contents": "Title: PBGen: Partial Binarization of Deconvolution-Based Generators for Edge\n  Intelligence Abstract: This work explores the binarization of the deconvolution-based generator in a\nGAN for memory saving and speedup of image construction. Our study suggests\nthat different from convolutional neural networks (including the discriminator)\nwhere all layers can be binarized, only some of the layers in the generator can\nbe binarized without significant performance loss. Supported by theoretical\nanalysis and verified by experiments, a direct metric based on the dimension of\ndeconvolution operations is established, which can be used to quickly decide\nwhich layers in the generator can be binarized. Our results also indicate that\nboth the generator and the discriminator should be binarized simultaneously for\nbalanced competition and better performance. Experimental results based on\nCelebA suggest that directly applying state-of-the-art binarization techniques\nto all the layers of the generator will lead to 2.83$\\times$ performance loss\nmeasured by sliced Wasserstein distance compared with the original generator,\nwhile applying them to selected layers only can yield up to 25.81$\\times$\nsaving in memory consumption, and 1.96$\\times$ and 1.32$\\times$ speedup in\ninference and training respectively with little performance loss. \n\n"}
{"id": "1802.09972", "contents": "Title: Fusion of Multispectral Data Through Illumination-aware Deep Neural\n  Networks for Pedestrian Detection Abstract: Multispectral pedestrian detection has received extensive attention in recent\nyears as a promising solution to facilitate robust human target detection for\naround-the-clock applications (e.g. security surveillance and autonomous\ndriving). In this paper, we demonstrate illumination information encoded in\nmultispectral images can be utilized to significantly boost performance of\npedestrian detection. A novel illumination-aware weighting mechanism is present\nto accurately depict illumination condition of a scene. Such illumination\ninformation is incorporated into two-stream deep convolutional neural networks\nto learn multispectral human-related features under different illumination\nconditions (daytime and nighttime). Moreover, we utilized illumination\ninformation together with multispectral data to generate more accurate semantic\nsegmentation which are used to boost pedestrian detection accuracy. Putting all\nof the pieces together, we present a powerful framework for multispectral\npedestrian detection based on multi-task learning of illumination-aware\npedestrian detection and semantic segmentation. Our proposed method is trained\nend-to-end using a well-designed multi-task loss function and outperforms\nstate-of-the-art approaches on KAIST multispectral pedestrian dataset. \n\n"}
{"id": "1802.10038", "contents": "Title: Improving OCR Accuracy on Early Printed Books by combining Pretraining,\n  Voting, and Active Learning Abstract: We combine three methods which significantly improve the OCR accuracy of OCR\nmodels trained on early printed books: (1) The pretraining method utilizes the\ninformation stored in already existing models trained on a variety of typesets\n(mixed models) instead of starting the training from scratch. (2) Performing\ncross fold training on a single set of ground truth data (line images and their\ntranscriptions) with a single OCR engine (OCRopus) produces a committee whose\nmembers then vote for the best outcome by also taking the top-N alternatives\nand their intrinsic confidence values into account. (3) Following the principle\nof maximal disagreement we select additional training lines which the voters\ndisagree most on, expecting them to offer the highest information gain for a\nsubsequent training (active learning). Evaluations on six early printed books\nyielded the following results: On average the combination of pretraining and\nvoting improved the character accuracy by 46% when training five folds starting\nfrom the same mixed model. This number rose to 53% when using different models\nfor pretraining, underlining the importance of diverse voters. Incorporating\nactive learning improved the obtained results by another 16% on average\n(evaluated on three of the six books). Overall, the proposed methods lead to an\naverage error rate of 2.5% when training on only 60 lines. Using a substantial\nground truth pool of 1,000 lines brought the error rate down even further to\nless than 1% on average. \n\n"}
{"id": "1802.10557", "contents": "Title: General-type discrete self-adjoint Dirac systems: explicit solutions of\n  direct and inverse problems, asymptotics of Verblunsky-type coefficients and\n  stability of solving inverse problem Abstract: We consider discrete self-adjoint Dirac systems determined by the potentials\n(sequences) $\\{C_k\\}$ such that the matrices $C_k$ are positive definite and\n$j$-unitary, where $j$ is a diagonal $m\\times m$ matrix and has $m_1$ entries\n$1$ and $m_2$ entries $-1$ ($m_1+m_2=m$) on the main diagonal. We construct\nsystems with rational Weyl functions and explicitly solve inverse problem to\nrecover systems from the contractive rational Weyl functions. Moreover, we\nstudy the stability of this procedure. The matrices $C_k$ (in the potentials)\nare so called Halmos extensions of the Verblunsky-type coefficients $\\rho_k$.\nWe show that in the case of the contractive rational Weyl functions the\ncoefficients $\\rho_k$ tend to zero and the matrices $C_k$ tend to the indentity\nmatrix $I_m$. \n\n"}
{"id": "1802.10560", "contents": "Title: Novelty Detection with GAN Abstract: The ability of a classifier to recognize unknown inputs is important for many\nclassification-based systems. We discuss the problem of simultaneous\nclassification and novelty detection, i.e. determining whether an input is from\nthe known set of classes and from which specific class, or from an unknown\ndomain and does not belong to any of the known classes. We propose a method\nbased on the Generative Adversarial Networks (GAN) framework. We show that a\nmulti-class discriminator trained with a generator that generates samples from\na mixture of nominal and novel data distributions is the optimal novelty\ndetector. We approximate that generator with a mixture generator trained with\nthe Feature Matching loss and empirically show that the proposed method\noutperforms conventional methods for novelty detection. Our findings\ndemonstrate a simple, yet powerful new application of the GAN framework for the\ntask of novelty detection. \n\n"}
{"id": "1803.00389", "contents": "Title: Poisson Image Denoising Using Best Linear Prediction: A Post-processing\n  Framework Abstract: In this paper, we address the problem of denoising images degraded by Poisson\nnoise. We propose a new patch-based approach based on best linear prediction to\nestimate the underlying clean image. A simplified prediction formula is derived\nfor Poisson observations, which requires the covariance matrix of the\nunderlying clean patch. We use the assumption that similar patches in a\nneighborhood share the same covariance matrix, and we use off-the-shelf Poisson\ndenoising methods in order to obtain an initial estimate of the covariance\nmatrices. Our method can be seen as a post-processing step for Poisson\ndenoising methods and the results show that it improves upon several Poisson\ndenoising methods by relevant margins. \n\n"}
{"id": "1803.00397", "contents": "Title: Satellite imagery analysis for operational damage assessment in\n  Emergency situations Abstract: When major disaster occurs the questions are raised how to estimate the\ndamage in time to support the decision making process and relief efforts by\nlocal authorities or humanitarian teams. In this paper we consider the use of\nMachine Learning and Computer Vision on remote sensing imagery to improve time\nefficiency of assessment of damaged buildings in disaster affected area. We\npropose a general workflow that can be useful in various disaster management\napplications, and demonstrate the use of the proposed workflow for the\nassessment of the damage caused by the wildfires in California in 2017. \n\n"}
{"id": "1803.00557", "contents": "Title: The 2018 DAVIS Challenge on Video Object Segmentation Abstract: We present the 2018 DAVIS Challenge on Video Object Segmentation, a public\ncompetition specifically designed for the task of video object segmentation. It\nbuilds upon the DAVIS 2017 dataset, which was presented in the previous edition\nof the DAVIS Challenge, and added 100 videos with multiple objects per sequence\nto the original DAVIS 2016 dataset. Motivated by the analysis of the results of\nthe 2017 edition, the main track of the competition will be the same than in\nthe previous edition (segmentation given the full mask of the objects in the\nfirst frame -- semi-supervised scenario). This edition, however, also adds an\ninteractive segmentation teaser track, where the participants will interact\nwith a web service simulating the input of a human that provides scribbles to\niteratively improve the result. \n\n"}
{"id": "1803.01906", "contents": "Title: Abnormality Detection in Mammography using Deep Convolutional Neural\n  Networks Abstract: Breast cancer is the most common cancer in women worldwide. The most common\nscreening technology is mammography. To reduce the cost and workload of\nradiologists, we propose a computer aided detection approach for classifying\nand localizing calcifications and masses in mammogram images. To improve on\nconventional approaches, we apply deep convolutional neural networks (CNN) for\nautomatic feature learning and classifier building. In computer-aided\nmammography, deep CNN classifiers cannot be trained directly on full mammogram\nimages because of the loss of image details from resizing at input layers.\nInstead, our classifiers are trained on labelled image patches and then adapted\nto work on full mammogram images for localizing the abnormalities.\nState-of-the-art deep convolutional neural networks are compared on their\nperformance of classifying the abnormalities. Experimental results indicate\nthat VGGNet receives the best overall accuracy at 92.53\\% in classifications.\nFor localizing abnormalities, ResNet is selected for computing class activation\nmaps because it is ready to be deployed without structural change or further\ntraining. Our approach demonstrates that deep convolutional neural network\nclassifiers have remarkable localization capabilities despite no supervision on\nthe location of abnormalities is provided. \n\n"}
{"id": "1803.02988", "contents": "Title: Rethinking Feature Distribution for Loss Functions in Image\n  Classification Abstract: We propose a large-margin Gaussian Mixture (L-GM) loss for deep neural\nnetworks in classification tasks. Different from the softmax cross-entropy\nloss, our proposal is established on the assumption that the deep features of\nthe training set follow a Gaussian Mixture distribution. By involving a\nclassification margin and a likelihood regularization, the L-GM loss\nfacilitates both a high classification performance and an accurate modeling of\nthe training feature distribution. As such, the L-GM loss is superior to the\nsoftmax loss and its major variants in the sense that besides classification,\nit can be readily used to distinguish abnormal inputs, such as the adversarial\nexamples, based on their features' likelihood to the training feature\ndistribution. Extensive experiments on various recognition benchmarks like\nMNIST, CIFAR, ImageNet and LFW, as well as on adversarial examples demonstrate\nthe effectiveness of our proposal. \n\n"}
{"id": "1803.03310", "contents": "Title: Generalization in Metric Learning: Should the Embedding Layer be the\n  Embedding Layer? Abstract: This work studies deep metric learning under small to medium scale data as we\nbelieve that better generalization could be a contributing factor to the\nimprovement of previous fine-grained image retrieval methods; it should be\nconsidered when designing future techniques. In particular, we investigate\nusing other layers in a deep metric learning system (besides the embedding\nlayer) for feature extraction and analyze how well they perform on training\ndata and generalize to testing data. From this study, we suggest a new\nregularization practice where one can add or choose a more optimal layer for\nfeature extraction. State-of-the-art performance is demonstrated on 3\nfine-grained image retrieval benchmarks: Cars-196, CUB-200-2011, and Stanford\nOnline Product. \n\n"}
{"id": "1803.04347", "contents": "Title: Classifying Online Dating Profiles on Tinder using FaceNet Facial\n  Embeddings Abstract: A method to produce personalized classification models to automatically\nreview online dating profiles on Tinder is proposed, based on the user's\nhistorical preference. The method takes advantage of a FaceNet facial\nclassification model to extract features which may be related to facial\nattractiveness. The embeddings from a FaceNet model were used as the features\nto describe an individual's face. A user reviewed 8,545 online dating profiles.\nFor each reviewed online dating profile, a feature set was constructed from the\nprofile images which contained just one face. Two approaches are presented to\ngo from the set of features for each face, to a set of profile features. A\nsimple logistic regression trained on the embeddings from just 20 profiles\ncould obtain a 65% validation accuracy. A point of diminishing marginal returns\nwas identified to occur around 80 profiles, at which the model accuracy of 73%\nwould only improve marginally after reviewing a significant number of\nadditional profiles. \n\n"}
{"id": "1803.04626", "contents": "Title: Maintaining Natural Image Statistics with the Contextual Loss Abstract: Maintaining natural image statistics is a crucial factor in restoration and\ngeneration of realistic looking images. When training CNNs, photorealism is\nusually attempted by adversarial training (GAN), that pushes the output images\nto lie on the manifold of natural images. GANs are very powerful, but not\nperfect. They are hard to train and the results still often suffer from\nartifacts. In this paper we propose a complementary approach, that could be\napplied with or without GAN, whose goal is to train a feed-forward CNN to\nmaintain natural internal statistics. We look explicitly at the distribution of\nfeatures in an image and train the network to generate images with natural\nfeature distributions. Our approach reduces by orders of magnitude the number\nof images required for training and achieves state-of-the-art results on both\nsingle-image super-resolution, and high-resolution surface normal estimation. \n\n"}
{"id": "1803.05541", "contents": "Title: Context-Aware Mixed Reality: A Framework for Ubiquitous Interaction Abstract: Mixed Reality (MR) is a powerful interactive technology that yields new types\nof user experience. We present a semantic based interactive MR framework that\nexceeds the current geometry level approaches, a step change in generating\nhigh-level context-aware interactions. Our key insight is to build semantic\nunderstanding in MR that not only can greatly enhance user experience through\nobject-specific behaviours, but also pave the way for solving complex\ninteraction design challenges. The framework generates semantic properties of\nthe real world environment through dense scene reconstruction and deep image\nunderstanding. We demonstrate our approach with a material-aware prototype\nsystem for generating context-aware physical interactions between the real and\nthe virtual objects. Quantitative and qualitative evaluations are carried out\nand the results show that the framework delivers accurate and fast semantic\ninformation in interactive MR environment, providing effective semantic level\ninteractions. \n\n"}
{"id": "1803.05753", "contents": "Title: What Catches the Eye? Visualizing and Understanding Deep Saliency Models Abstract: Deep convolutional neural networks have demonstrated high performances for\nfixation prediction in recent years. How they achieve this, however, is less\nexplored and they remain to be black box models. Here, we attempt to shed light\non the internal structure of deep saliency models and study what features they\nextract for fixation prediction. Specifically, we use a simple yet powerful\narchitecture, consisting of only one CNN and a single resolution input,\ncombined with a new loss function for pixel-wise fixation prediction during\nfree viewing of natural scenes. We show that our simple method is on par or\nbetter than state-of-the-art complicated saliency models. Furthermore, we\npropose a method, related to saliency model evaluation metrics, to visualize\ndeep models for fixation prediction. Our method reveals the inner\nrepresentations of deep models for fixation prediction and provides evidence\nthat saliency, as experienced by humans, is likely to involve high-level\nsemantic knowledge in addition to low-level perceptual cues. Our results can be\nuseful to measure the gap between current saliency models and the human\ninter-observer model and to build new models to close this gap. \n\n"}
{"id": "1803.05872", "contents": "Title: Virtual CNN Branching: Efficient Feature Ensemble for Person\n  Re-Identification Abstract: In this paper we introduce an ensemble method for convolutional neural\nnetwork (CNN), called \"virtual branching,\" which can be implemented with nearly\nno additional parameters and computation on top of standard CNNs. We propose\nour method in the context of person re-identification (re-ID). Our CNN model\nconsists of shared bottom layers, followed by \"virtual\" branches, where neurons\nfrom a block of regular convolutional and fully-connected layers are\npartitioned into multiple sets. Each virtual branch is trained with different\ndata to specialize in different aspects, e.g., a specific body region or pose\norientation. In this way, robust ensemble representations are obtained against\nhuman body misalignment, deformations, or variations in viewing angles, at\nnearly no any additional cost. The proposed method achieves competitive\nperformance on multiple person re-ID benchmark datasets, including Market-1501,\nCUHK03, and DukeMTMC-reID. \n\n"}
{"id": "1803.06355", "contents": "Title: A Low-rank Tensor Regularization Strategy for Hyperspectral Unmixing Abstract: Tensor-based methods have recently emerged as a more natural and effective\nformulation to address many problems in hyperspectral imaging. In hyperspectral\nunmixing (HU), low-rank constraints on the abundance maps have been shown to\nact as a regularization which adequately accounts for the multidimensional\nstructure of the underlying signal. However, imposing a strict low-rank\nconstraint for the abundance maps does not seem to be adequate, as important\ninformation that may be required to represent fine scale abundance behavior may\nbe discarded. This paper introduces a new low-rank tensor regularization that\nadequately captures the low-rank structure underlying the abundance maps\nwithout hindering the flexibility of the solution. Simulation results with\nsynthetic and real data show that the the extra flexibility introduced by the\nproposed regularization significantly improves the unmixing results. \n\n"}
{"id": "1803.06629", "contents": "Title: Cross-modality image synthesis from unpaired data using CycleGAN:\n  Effects of gradient consistency loss and training data size Abstract: CT is commonly used in orthopedic procedures. MRI is used along with CT to\nidentify muscle structures and diagnose osteonecrosis due to its superior soft\ntissue contrast. However, MRI has poor contrast for bone structures. Clearly,\nit would be helpful if a corresponding CT were available, as bone boundaries\nare more clearly seen and CT has standardized (i.e., Hounsfield) units.\nTherefore, we aim at MR-to-CT synthesis. The CycleGAN was successfully applied\nto unpaired CT and MR images of the head, these images do not have as much\nvariation of intensity pairs as do images in the pelvic region due to the\npresence of joints and muscles. In this paper, we extended the CycleGAN\napproach by adding the gradient consistency loss to improve the accuracy at the\nboundaries. We conducted two experiments. To evaluate image synthesis, we\ninvestigated dependency of image synthesis accuracy on 1) the number of\ntraining data and 2) the gradient consistency loss. To demonstrate the\napplicability of our method, we also investigated a segmentation accuracy on\nsynthesized images. \n\n"}
{"id": "1803.06929", "contents": "Title: Stochastic filtering and optimal control of pure jump Markov processes\n  with noise-free partial observation Abstract: We consider an infinite horizon optimal control problem for a pure jump\nMarkov process $X$, taking values in a complete and separable metric space $I$,\nwith noise-free partial observation. The observation process is defined as $Y_t\n= h(X_t)$, $t \\geq 0$, where $h$ is a given map defined on $I$. The observation\nis noise-free in the sense that the only source of randomness is the process\n$X$ itself. The aim is to minimize a discounted cost functional. In the first\npart of the paper we write down an explicit filtering equation and characterize\nthe filtering process as a Piecewise Deterministic Process. In the second part,\nafter transforming the original control problem with partial observation into\none with complete observation (the separated problem) using filtering\nequations, we prove the equivalence of the original and separated problems\nthrough an explicit formula linking their respective value functions. The value\nfunction of the separated problem is also characterized as the unique fixed\npoint of a suitably defined contraction mapping. \n\n"}
{"id": "1803.06984", "contents": "Title: Robust Optimization for Electricity Generation Abstract: We consider a robust optimization problem in an electric power system under\nuncertain demand and availability of renewable energy resources. Solving the\ndeterministic alternating current optimal power flow (ACOPF) problem has been\nconsidered challenging since the 1960s due to its nonconvexity. Linear\napproximation of the AC power flow system sees pervasive use, but does not\nguarantee a physically feasible system configuration. In recent years, various\nconvex relaxation schemes for the ACOPF problem have been investigated, and\nunder some assumptions, a physically feasible solution can be recovered. Based\non these convex relaxations, we construct a robust convex optimization problem\nwith recourse to solve for optimal controllable injections (fossil fuel,\nnuclear, etc.) in electric power systems under uncertainty (renewable energy\ngeneration, demand fluctuation, etc.). We propose a cutting-plane method to\nsolve this robust optimization problem, and we establish convergence and other\ndesirable properties. Experimental results indicate that our robust convex\nrelaxation of the ACOPF problem can provide a tight lower bound. \n\n"}
{"id": "1803.07289", "contents": "Title: Flex-Convolution (Million-Scale Point-Cloud Learning Beyond Grid-Worlds) Abstract: Traditional convolution layers are specifically designed to exploit the\nnatural data representation of images -- a fixed and regular grid. However,\nunstructured data like 3D point clouds containing irregular neighborhoods\nconstantly breaks the grid-based data assumption. Therefore applying\nbest-practices and design choices from 2D-image learning methods towards\nprocessing point clouds are not readily possible. In this work, we introduce a\nnatural generalization flex-convolution of the conventional convolution layer\nalong with an efficient GPU implementation. We demonstrate competitive\nperformance on rather small benchmark sets using fewer parameters and lower\nmemory consumption and obtain significant improvements on a million-scale\nreal-world dataset. Ours is the first which allows to efficiently process 7\nmillion points concurrently. \n\n"}
{"id": "1803.07818", "contents": "Title: Phase Retrieval via Sensor Network Localization Abstract: The problem of phase retrieval is revisited and studied from a fresh\nperspective. In particular, we establish a connection between the phase\nretrieval problem and the sensor network localization problem, which allows us\nto utilize the vast theoretical and algorithmic literature on the latter to\ntackle the former. Leveraging this connection, we develop a two-stage algorithm\nfor phase retrieval that can provably recover the desired signal. In both\nsparse and dense settings, our proposed algorithm improves upon prior\napproaches simultaneously in the number of required measurements for recovery\nand the reconstruction time. We present numerical results to corroborate our\ntheory and to demonstrate the efficiency of the proposed algorithm. As a side\nresult, we propose a new form of phase retrieval problem and connect it to the\ncomplex rigidity theory proposed by Gortler and Thurston. \n\n"}
{"id": "1803.07913", "contents": "Title: HATS: Histograms of Averaged Time Surfaces for Robust Event-based Object\n  Classification Abstract: Event-based cameras have recently drawn the attention of the Computer Vision\ncommunity thanks to their advantages in terms of high temporal resolution, low\npower consumption and high dynamic range, compared to traditional frame-based\ncameras. These properties make event-based cameras an ideal choice for\nautonomous vehicles, robot navigation or UAV vision, among others. However, the\naccuracy of event-based object classification algorithms, which is of crucial\nimportance for any reliable system working in real-world conditions, is still\nfar behind their frame-based counterparts. Two main reasons for this\nperformance gap are: 1. The lack of effective low-level representations and\narchitectures for event-based object classification and 2. The absence of large\nreal-world event-based datasets. In this paper we address both problems. First,\nwe introduce a novel event-based feature representation together with a new\nmachine learning architecture. Compared to previous approaches, we use local\nmemory units to efficiently leverage past temporal information and build a\nrobust event-based representation. Second, we release the first large\nreal-world event-based dataset for object classification. We compare our method\nto the state-of-the-art with extensive experiments, showing better\nclassification performance and real-time computation. \n\n"}
{"id": "1803.07950", "contents": "Title: End-to-End Video Captioning with Multitask Reinforcement Learning Abstract: Although end-to-end (E2E) learning has led to impressive progress on a\nvariety of visual understanding tasks, it is often impeded by hardware\nconstraints (e.g., GPU memory) and is prone to overfitting. When it comes to\nvideo captioning, one of the most challenging benchmark tasks in computer\nvision, those limitations of E2E learning are especially amplified by the fact\nthat both the input videos and output captions are lengthy sequences. Indeed,\nstate-of-the-art methods for video captioning process video frames by\nconvolutional neural networks and generate captions by unrolling recurrent\nneural networks. If we connect them in an E2E manner, the resulting model is\nboth memory-consuming and data-hungry, making it extremely hard to train. In\nthis paper, we propose a multitask reinforcement learning approach to training\nan E2E video captioning model. The main idea is to mine and construct as many\neffective tasks (e.g., attributes, rewards, and the captions) as possible from\nthe human captioned videos such that they can jointly regulate the search space\nof the E2E neural network, from which an E2E video captioning model can be\nfound and generalized to the testing phase. To the best of our knowledge, this\nis the first video captioning model that is trained end-to-end from the raw\nvideo input to the caption output. Experimental results show that such a model\noutperforms existing ones to a large margin on two benchmark video captioning\ndatasets. \n\n"}
{"id": "1803.08103", "contents": "Title: A Unified Framework for Multi-View Multi-Class Object Pose Estimation Abstract: One core challenge in object pose estimation is to ensure accurate and robust\nperformance for large numbers of diverse foreground objects amidst complex\nbackground clutter. In this work, we present a scalable framework for\naccurately inferring six Degree-of-Freedom (6-DoF) pose for a large number of\nobject classes from single or multiple views. To learn discriminative pose\nfeatures, we integrate three new capabilities into a deep Convolutional Neural\nNetwork (CNN): an inference scheme that combines both classification and pose\nregression based on a uniform tessellation of the Special Euclidean group in\nthree dimensions (SE(3)), the fusion of class priors into the training process\nvia a tiled class map, and an additional regularization using deep supervision\nwith an object mask. Further, an efficient multi-view framework is formulated\nto address single-view ambiguity. We show that this framework consistently\nimproves the performance of the single-view network. We evaluate our method on\nthree large-scale benchmarks: YCB-Video, JHUScene-50 and ObjectNet-3D. Our\napproach achieves competitive or superior performance over the current\nstate-of-the-art methods. \n\n"}
{"id": "1803.08208", "contents": "Title: Single-Shot Bidirectional Pyramid Networks for High-Quality Object\n  Detection Abstract: Recent years have witnessed many exciting achievements for object detection\nusing deep learning techniques. Despite achieving significant progresses, most\nexisting detectors are designed to detect objects with relatively low-quality\nprediction of locations, i.e., often trained with the threshold of Intersection\nover Union (IoU) set to 0.5 by default, which can yield low-quality or even\nnoisy detections. It remains an open challenge for how to devise and train a\nhigh-quality detector that can achieve more precise localization (i.e.,\nIoU$>$0.5) without sacrificing the detection performance. In this paper, we\npropose a novel single-shot detection framework of Bidirectional Pyramid\nNetworks (BPN) towards high-quality object detection, which consists of two\nnovel components: (i) a Bidirectional Feature Pyramid structure for more\neffective and robust feature representations; and (ii) a Cascade Anchor\nRefinement to gradually refine the quality of predesigned anchors for more\neffective training. Our experiments showed that the proposed BPN achieves the\nbest performances among all the single-stage object detectors on both PASCAL\nVOC and MS COCO datasets, especially for high-quality detections. \n\n"}
{"id": "1803.08542", "contents": "Title: Aligning Across Large Gaps in Time Abstract: We present a method of temporally-invariant image registration for outdoor\nscenes, with invariance across time of day, across seasonal variations, and\nacross decade-long periods, for low- and high-texture scenes. Our method can be\nuseful for applications in remote sensing, GPS-denied UAV localization, 3D\nreconstruction, and many others. Our method leverages a recently proposed\napproach to image registration, where fully-convolutional neural networks are\nused to create feature maps which can be registered using the\nInverse-Composition Lucas-Kanade algorithm (ICLK). We show that invariance that\nis learned from satellite imagery can be transferable to time-lapse data\ncaptured by webcams mounted on buildings near ground-level. \n\n"}
{"id": "1803.08995", "contents": "Title: Iterative Low-Rank Approximation for CNN Compression Abstract: Deep convolutional neural networks contain tens of millions of parameters,\nmaking them impossible to work efficiently on embedded devices. We propose\niterative approach of applying low-rank approximation to compress deep\nconvolutional neural networks. Since classification and object detection are\nthe most favored tasks for embedded devices, we demonstrate the effectiveness\nof our approach by compressing AlexNet, VGG-16, YOLOv2 and Tiny YOLO networks.\nOur results show the superiority of the proposed method compared to\nnon-repetitive ones. We demonstrate higher compression ratio providing less\naccuracy loss. \n\n"}
{"id": "1803.09093", "contents": "Title: Comparing Generative Adversarial Network Techniques for Image Creation\n  and Modification Abstract: Generative adversarial networks (GANs) have demonstrated to be successful at\ngenerating realistic real-world images. In this paper we compare various GAN\ntechniques, both supervised and unsupervised. The effects on training stability\nof different objective functions are compared. We add an encoder to the\nnetwork, making it possible to encode images to the latent space of the GAN.\nThe generator, discriminator and encoder are parameterized by deep\nconvolutional neural networks. For the discriminator network we experimented\nwith using the novel Capsule Network, a state-of-the-art technique for\ndetecting global features in images. Experiments are performed using a digit\nand face dataset, with various visualizations illustrating the results. The\nresults show that using the encoder network it is possible to reconstruct\nimages. With the conditional GAN we can alter visual attributes of generated or\nencoded images. The experiments with the Capsule Network as discriminator\nresult in generated images of a lower quality, compared to a standard\nconvolutional neural network. \n\n"}
{"id": "1803.09196", "contents": "Title: Learning Type-Aware Embeddings for Fashion Compatibility Abstract: Outfits in online fashion data are composed of items of many different types\n(e.g. top, bottom, shoes) that share some stylistic relationship with one\nanother. A representation for building outfits requires a method that can learn\nboth notions of similarity (for example, when two tops are interchangeable) and\ncompatibility (items of possibly different type that can go together in an\noutfit). This paper presents an approach to learning an image embedding that\nrespects item type, and jointly learns notions of item similarity and\ncompatibility in an end-to-end model. To evaluate the learned representation,\nwe crawled 68,306 outfits created by users on the Polyvore website. Our\napproach obtains 3-5% improvement over the state-of-the-art on outfit\ncompatibility prediction and fill-in-the-blank tasks using our dataset, as well\nas an established smaller dataset, while supporting a variety of useful\nqueries. \n\n"}
{"id": "1803.09466", "contents": "Title: Regularizing Deep Hashing Networks Using GAN Generated Fake Images Abstract: Recently, deep-networks-based hashing (deep hashing) has become a leading\napproach for large-scale image retrieval. It aims to learn a compact bitwise\nrepresentation for images via deep networks, so that similar images are mapped\nto nearby hash codes. Since a deep network model usually has a large number of\nparameters, it may probably be too complicated for the training data we have,\nleading to model over-fitting. To address this issue, in this paper, we propose\na simple two-stage pipeline to learn deep hashing models, by regularizing the\ndeep hashing networks using fake images. The first stage is to generate fake\nimages from the original training set without extra data, via a generative\nadversarial network (GAN). In the second stage, we propose a deep architec-\nture to learn hash functions, in which we use a maximum-entropy based loss to\nincorporate the newly created fake images by the GAN. We show that this loss\nacts as a strong regularizer of the deep architecture, by penalizing\nlow-entropy output hash codes. This loss can also be interpreted as a model\nensemble by simultaneously training many network models with massive weight\nsharing but over different training sets. Empirical evaluation results on\nseveral benchmark datasets show that the proposed method has superior\nperformance gains over state-of-the-art hashing methods. \n\n"}
{"id": "1803.09820", "contents": "Title: A disciplined approach to neural network hyper-parameters: Part 1 --\n  learning rate, batch size, momentum, and weight decay Abstract: Although deep learning has produced dazzling successes for applications of\nimage, speech, and video processing in the past few years, most trainings are\nwith suboptimal hyper-parameters, requiring unnecessarily long training times.\nSetting the hyper-parameters remains a black art that requires years of\nexperience to acquire. This report proposes several efficient ways to set the\nhyper-parameters that significantly reduce training time and improves\nperformance. Specifically, this report shows how to examine the training\nvalidation/test loss function for subtle clues of underfitting and overfitting\nand suggests guidelines for moving toward the optimal balance point. Then it\ndiscusses how to increase/decrease the learning rate/momentum to speed up\ntraining. Our experiments show that it is crucial to balance every manner of\nregularization for each dataset and architecture. Weight decay is used as a\nsample regularizer to show how its optimal value is tightly coupled with the\nlearning rates and momentums. Files to help replicate the results reported here\nare available. \n\n"}
{"id": "1803.10630", "contents": "Title: Person re-identification with fusion of hand-crafted and deep pose-based\n  body region features Abstract: Person re-identification (re-ID) aims to accurately re- trieve a person from\na large-scale database of images cap- tured across multiple cameras. Existing\nworks learn deep representations using a large training subset of unique per-\nsons. However, identifying unseen persons is critical for a good re-ID\nalgorithm. Moreover, the misalignment be- tween person crops to detection\nerrors or pose variations leads to poor feature matching. In this work, we\npresent a fusion of handcrafted features and deep feature representa- tion\nlearned using multiple body parts to complement the global body features that\nachieves high performance on un- seen test images. Pose information is used to\ndetect body regions that are passed through Convolutional Neural Net- works\n(CNN) to guide feature learning. Finally, a metric learning step enables robust\ndistance matching on a dis- criminative subspace. Experimental results on 4\npopular re-ID benchmark datasets namely VIPer, DukeMTMC-reID, Market-1501 and\nCUHK03 show that the proposed method achieves state-of-the-art performance in\nimage-based per- son re-identification. \n\n"}
{"id": "1803.11335", "contents": "Title: On the classification of linear complementary dual codes Abstract: We give a complete classification of binary linear complementary dual codes\nof lengths up to $13$ and ternary linear complementary dual codes of lengths up\nto $10$. \n\n"}
{"id": "1803.11438", "contents": "Title: Reconstruction Network for Video Captioning Abstract: In this paper, the problem of describing visual contents of a video sequence\nwith natural language is addressed. Unlike previous video captioning work\nmainly exploiting the cues of video contents to make a language description, we\npropose a reconstruction network (RecNet) with a novel\nencoder-decoder-reconstructor architecture, which leverages both the forward\n(video to sentence) and backward (sentence to video) flows for video\ncaptioning. Specifically, the encoder-decoder makes use of the forward flow to\nproduce the sentence description based on the encoded video semantic features.\nTwo types of reconstructors are customized to employ the backward flow and\nreproduce the video features based on the hidden state sequence generated by\nthe decoder. The generation loss yielded by the encoder-decoder and the\nreconstruction loss introduced by the reconstructor are jointly drawn into\ntraining the proposed RecNet in an end-to-end fashion. Experimental results on\nbenchmark datasets demonstrate that the proposed reconstructor can boost the\nencoder-decoder models and leads to significant gains in video caption\naccuracy. \n\n"}
{"id": "1804.00021", "contents": "Title: Hierarchical Transfer Convolutional Neural Networks for Image\n  Classification Abstract: In this paper, we address the issue of how to enhance the generalization\nperformance of convolutional neural networks (CNN) in the early learning stage\nfor image classification. This is motivated by real-time applications that\nrequire the generalization performance of CNN to be satisfactory within limited\ntraining time. In order to achieve this, a novel hierarchical transfer CNN\nframework is proposed. It consists of a group of shallow CNNs and a cloud CNN,\nwhere the shallow CNNs are trained firstly and then the first layers of the\ntrained shallow CNNs are used to initialize the first layer of the cloud CNN.\nThis method will boost the generalization performance of the cloud CNN\nsignificantly, especially during the early stage of training. Experiments using\nCIFAR-10 and ImageNet datasets are performed to examine the proposed method.\nResults demonstrate the improvement of testing accuracy is 12% on average and\nas much as 20% for the CIFAR-10 case while 5% testing accuracy improvement for\nthe ImageNet case during the early stage of learning. It is also shown that\nuniversal improvements of testing accuracy are obtained across different\nsettings of dropout and number of shallow CNNs. \n\n"}
{"id": "1804.00105", "contents": "Title: Visual Question Reasoning on General Dependency Tree Abstract: The collaborative reasoning for understanding each image-question pair is\nvery critical but under-explored for an interpretable Visual Question Answering\n(VQA) system. Although very recent works also tried the explicit compositional\nprocesses to assemble multiple sub-tasks embedded in the questions, their\nmodels heavily rely on the annotations or hand-crafted rules to obtain valid\nreasoning layout, leading to either heavy labor or poor performance on\ncomposition reasoning. In this paper, to enable global context reasoning for\nbetter aligning image and language domains in diverse and unrestricted cases,\nwe propose a novel reasoning network called Adversarial Composition Modular\nNetwork (ACMN). This network comprises of two collaborative modules: i) an\nadversarial attention module to exploit the local visual evidence for each word\nparsed from the question; ii) a residual composition module to compose the\npreviously mined evidence. Given a dependency parse tree for each question, the\nadversarial attention module progressively discovers salient regions of one\nword by densely combining regions of child word nodes in an adversarial manner.\nThen residual composition module merges the hidden representations of an\narbitrary number of children through sum pooling and residual connection. Our\nACMN is thus capable of building an interpretable VQA system that gradually\ndives the image cues following a question-driven reasoning route and makes\nglobal reasoning by incorporating the learned knowledge of all attention\nmodules in a principled manner. Experiments on relational datasets demonstrate\nthe superiority of our ACMN and visualization results show the explainable\ncapability of our reasoning system. \n\n"}
{"id": "1804.00248", "contents": "Title: SampleAhead: Online Classifier-Sampler Communication for Learning from\n  Synthesized Data Abstract: State-of-the-art techniques of artificial intelligence, in particular deep\nlearning, are mostly data-driven. However, collecting and manually labeling a\nlarge scale dataset is both difficult and expensive. A promising alternative is\nto introduce synthesized training data, so that the dataset size can be\nsignificantly enlarged with little human labor. But, this raises an important\nproblem in active vision: given an {\\bf infinite} data space, how to\neffectively sample a {\\bf finite} subset to train a visual classifier? This\npaper presents an approach for learning from synthesized data effectively. The\nmotivation is straightforward -- increasing the probability of seeing difficult\ntraining data. We introduce a module named {\\bf SampleAhead} to formulate the\nlearning process into an online communication between a {\\em classifier} and a\n{\\em sampler}, and update them iteratively. In each round, we adjust the\nsampling distribution according to the classification results, and train the\nclassifier using the data sampled from the updated distribution. Experiments\nare performed by introducing synthesized images rendered from ShapeNet models\nto assist PASCAL3D+ classification. Our approach enjoys higher classification\naccuracy, especially in the scenario of a limited number of training samples.\nThis demonstrates its efficiency in exploring the infinite data space. \n\n"}
{"id": "1804.00256", "contents": "Title: One-Two-One Networks for Compression Artifacts Reduction in Remote\n  Sensing Abstract: Compression artifacts reduction (CAR) is a challenging problem in the field\nof remote sensing. Most recent deep learning based methods have demonstrated\nsuperior performance over the previous hand-crafted methods. In this paper, we\npropose an end-to-end one-two-one (OTO) network, to combine different deep\nmodels, i.e., summation and difference models, to solve the CAR problem.\nParticularly, the difference model motivated by the Laplacian pyramid is\ndesigned to obtain the high frequency information, while the summation model\naggregates the low frequency information. We provide an in-depth investigation\ninto our OTO architecture based on the Taylor expansion, which shows that these\ntwo kinds of information can be fused in a nonlinear scheme to gain more\ncapacity of handling complicated image compression artifacts, especially the\nblocking effect in compression. Extensive experiments are conducted to\ndemonstrate the superior performance of the OTO networks, as compared to the\nstate-of-the-arts on remote sensing datasets and other benchmark datasets. \n\n"}
{"id": "1804.00521", "contents": "Title: CompNet: Complementary Segmentation Network for Brain MRI Extraction Abstract: Brain extraction is a fundamental step for most brain imaging studies. In\nthis paper, we investigate the problem of skull stripping and propose\ncomplementary segmentation networks (CompNets) to accurately extract the brain\nfrom T1-weighted MRI scans, for both normal and pathological brain images. The\nproposed networks are designed in the framework of encoder-decoder networks and\nhave two pathways to learn features from both the brain tissue and its\ncomplementary part located outside of the brain. The complementary pathway\nextracts the features in the non-brain region and leads to a robust solution to\nbrain extraction from MRIs with pathologies, which do not exist in our training\ndataset. We demonstrate the effectiveness of our networks by evaluating them on\nthe OASIS dataset, resulting in the state of the art performance under the\ntwo-fold cross-validation setting. Moreover, the robustness of our networks is\nverified by testing on images with introduced pathologies and by showing its\ninvariance to unseen brain pathologies. In addition, our complementary network\ndesign is general and can be extended to address other image segmentation\nproblems with better generalization. \n\n"}
{"id": "1804.00792", "contents": "Title: Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks Abstract: Data poisoning is an attack on machine learning models wherein the attacker\nadds examples to the training set to manipulate the behavior of the model at\ntest time. This paper explores poisoning attacks on neural nets. The proposed\nattacks use \"clean-labels\"; they don't require the attacker to have any control\nover the labeling of training data. They are also targeted; they control the\nbehavior of the classifier on a $\\textit{specific}$ test instance without\ndegrading overall classifier performance. For example, an attacker could add a\nseemingly innocuous image (that is properly labeled) to a training set for a\nface recognition engine, and control the identity of a chosen person at test\ntime. Because the attacker does not need to control the labeling function,\npoisons could be entered into the training set simply by leaving them on the\nweb and waiting for them to be scraped by a data collection bot.\n  We present an optimization-based method for crafting poisons, and show that\njust one single poison image can control classifier behavior when transfer\nlearning is used. For full end-to-end training, we present a \"watermarking\"\nstrategy that makes poisoning reliable using multiple ($\\approx$50) poisoned\ntraining instances. We demonstrate our method by generating poisoned frog\nimages from the CIFAR dataset and using them to manipulate image classifiers. \n\n"}
{"id": "1804.00946", "contents": "Title: Unsupervised Learning of Sequence Representations by Autoencoders Abstract: Sequence data is challenging for machine learning approaches, because the\nlengths of the sequences may vary between samples. In this paper, we present an\nunsupervised learning model for sequence data, called the Integrated Sequence\nAutoencoder (ISA), to learn a fixed-length vectorial representation by\nminimizing the reconstruction error. Specifically, we propose to integrate two\nclassical mechanisms for sequence reconstruction which takes into account both\nthe global silhouette information and the local temporal dependencies.\nFurthermore, we propose a stop feature that serves as a temporal stamp to guide\nthe reconstruction process, which results in a higher-quality representation.\nThe learned representation is able to effectively summarize not only the\napparent features, but also the underlying and high-level style information.\nTake for example a speech sequence sample: our ISA model can not only recognize\nthe spoken text (apparent feature), but can also discriminate the speaker who\nutters the audio (more high-level style). One promising application of the ISA\nmodel is that it can be readily used in the semi-supervised learning scenario,\nin which a large amount of unlabeled data is leveraged to extract high-quality\nsequence representations and thus to improve the performance of the subsequent\nsupervised learning tasks on limited labeled data. \n\n"}
{"id": "1804.01438", "contents": "Title: Learning Discriminative Features with Multiple Granularities for Person\n  Re-Identification Abstract: The combination of global and partial features has been an essential solution\nto improve discriminative performances in person re-identification (Re-ID)\ntasks. Previous part-based methods mainly focus on locating regions with\nspecific pre-defined semantics to learn local representations, which increases\nlearning difficulty but not efficient or robust to scenarios with large\nvariances. In this paper, we propose an end-to-end feature learning strategy\nintegrating discriminative information with various granularities. We carefully\ndesign the Multiple Granularity Network (MGN), a multi-branch deep network\narchitecture consisting of one branch for global feature representations and\ntwo branches for local feature representations. Instead of learning on semantic\nregions, we uniformly partition the images into several stripes, and vary the\nnumber of parts in different local branches to obtain local feature\nrepresentations with multiple granularities. Comprehensive experiments\nimplemented on the mainstream evaluation datasets including Market-1501,\nDukeMTMC-reid and CUHK03 indicate that our method has robustly achieved\nstate-of-the-art performances and outperformed any existing approaches by a\nlarge margin. For example, on Market-1501 dataset in single query mode, we\nachieve a state-of-the-art result of Rank-1/mAP=96.6%/94.2% after re-ranking. \n\n"}
{"id": "1804.01438", "contents": "Title: Learning Discriminative Features with Multiple Granularities for Person\n  Re-Identification Abstract: The combination of global and partial features has been an essential solution\nto improve discriminative performances in person re-identification (Re-ID)\ntasks. Previous part-based methods mainly focus on locating regions with\nspecific pre-defined semantics to learn local representations, which increases\nlearning difficulty but not efficient or robust to scenarios with large\nvariances. In this paper, we propose an end-to-end feature learning strategy\nintegrating discriminative information with various granularities. We carefully\ndesign the Multiple Granularity Network (MGN), a multi-branch deep network\narchitecture consisting of one branch for global feature representations and\ntwo branches for local feature representations. Instead of learning on semantic\nregions, we uniformly partition the images into several stripes, and vary the\nnumber of parts in different local branches to obtain local feature\nrepresentations with multiple granularities. Comprehensive experiments\nimplemented on the mainstream evaluation datasets including Market-1501,\nDukeMTMC-reid and CUHK03 indicate that our method has robustly achieved\nstate-of-the-art performances and outperformed any existing approaches by a\nlarge margin. For example, on Market-1501 dataset in single query mode, we\nachieve a state-of-the-art result of Rank-1/mAP=96.6%/94.2% after re-ranking. \n\n"}
{"id": "1804.02729", "contents": "Title: Distributed Non-Convex First-Order Optimization and Information\n  Processing: Lower Complexity Bounds and Rate Optimal Algorithms Abstract: We consider a class of popular distributed non-convex optimization problems,\nin which agents connected by a network $\\mathcal{G}$ collectively optimize a\nsum of smooth (possibly non-convex) local objective functions. We address the\nfollowing question: if the agents can only access the gradients of local\nfunctions, what are the fastest rates that any distributed algorithms can\nachieve, and how to achieve those rates.\n  First, we show that there exist difficult problem instances, such that it\ntakes a class of distributed first-order methods at least\n$\\mathcal{O}(1/\\sqrt{\\xi(\\mathcal{G})} \\times \\bar{L} /{\\epsilon})$\ncommunication rounds to achieve certain $\\epsilon$-solution [where\n$\\xi(\\mathcal{G})$ denotes the spectral gap of the graph Laplacian matrix, and\n$\\bar{L}$ is some Lipschitz constant]. Second, we propose (near) optimal\nmethods whose rates match the developed lower rate bound (up to a polylog\nfactor). The key in the algorithm design is to properly embed the classical\npolynomial filtering techniques into modern first-order algorithms. To the best\nof our knowledge, this is the first time that lower rate bounds and optimal\nmethods have been developed for distributed non-convex optimization problems. \n\n"}
{"id": "1804.03281", "contents": "Title: Recurrent Neural Networks for Person Re-identification Revisited Abstract: The task of person re-identification has recently received rising attention\ndue to the high performance achieved by new methods based on deep learning. In\nparticular, in the context of video-based re-identification, many\nstate-of-the-art works have explored the use of Recurrent Neural Networks\n(RNNs) to process input sequences. In this work, we revisit this tool by\nderiving an approximation which reveals the small effect of recurrent\nconnections, leading to a much simpler feed-forward architecture. Using the\nsame parameters as the recurrent version, our proposed feed-forward\narchitecture obtains very similar accuracy. More importantly, our model can be\ncombined with a new training process to significantly improve re-identification\nperformance. Our experiments demonstrate that the proposed models converge\nsubstantially faster than recurrent ones, with accuracy improvements by up to\n5% on two datasets. The performance achieved is better or on par with other\nRNN-based person re-identification techniques. \n\n"}
{"id": "1804.03360", "contents": "Title: Reference-Conditioned Super-Resolution by Neural Texture Transfer Abstract: With the recent advancement in deep learning, we have witnessed a great\nprogress in single image super-resolution. However, due to the significant\ninformation loss of the image downscaling process, it has become extremely\nchallenging to further advance the state-of-the-art, especially for large\nupscaling factors. This paper explores a new research direction in super\nresolution, called reference-conditioned super-resolution, in which a reference\nimage containing desired high-resolution texture details is provided besides\nthe low-resolution image. We focus on transferring the high-resolution texture\nfrom reference images to the super-resolution process without the constraint of\ncontent similarity between reference and target images, which is a key\ndifference from previous example-based methods. Inspired by recent work on\nimage stylization, we address the problem via neural texture transfer. We\ndesign an end-to-end trainable deep model which generates detail enriched\nresults by adaptively fusing the content from the low-resolution image with the\ntexture patterns from the reference image. We create a benchmark dataset for\nthe general research of reference-based super-resolution, which contains\nreference images paired with low-resolution inputs with varying degrees of\nsimilarity. Both objective and subjective evaluations demonstrate the great\npotential of using reference images as well as the superiority of our results\nover other state-of-the-art methods. \n\n"}
{"id": "1804.04241", "contents": "Title: Capsules for Object Segmentation Abstract: Convolutional neural networks (CNNs) have shown remarkable results over the\nlast several years for a wide range of computer vision tasks. A new\narchitecture recently introduced by Sabour et al., referred to as a capsule\nnetworks with dynamic routing, has shown great initial results for digit\nrecognition and small image classification. The success of capsule networks\nlies in their ability to preserve more information about the input by replacing\nmax-pooling layers with convolutional strides and dynamic routing, allowing for\npreservation of part-whole relationships in the data. This preservation of the\ninput is demonstrated by reconstructing the input from the output capsule\nvectors. Our work expands the use of capsule networks to the task of object\nsegmentation for the first time in the literature. We extend the idea of\nconvolutional capsules with locally-connected routing and propose the concept\nof deconvolutional capsules. Further, we extend the masked reconstruction to\nreconstruct the positive input class. The proposed\nconvolutional-deconvolutional capsule network, called SegCaps, shows strong\nresults for the task of object segmentation with substantial decrease in\nparameter space. As an example application, we applied the proposed SegCaps to\nsegment pathological lungs from low dose CT scans and compared its accuracy and\nefficiency with other U-Net-based architectures. SegCaps is able to handle\nlarge image sizes (512 x 512) as opposed to baseline capsules (typically less\nthan 32 x 32). The proposed SegCaps reduced the number of parameters of U-Net\narchitecture by 95.4% while still providing a better segmentation accuracy. \n\n"}
{"id": "1804.04272", "contents": "Title: Deep Neural Networks Motivated by Partial Differential Equations Abstract: Partial differential equations (PDEs) are indispensable for modeling many\nphysical phenomena and also commonly used for solving image processing tasks.\nIn the latter area, PDE-based approaches interpret image data as\ndiscretizations of multivariate functions and the output of image processing\nalgorithms as solutions to certain PDEs. Posing image processing problems in\nthe infinite dimensional setting provides powerful tools for their analysis and\nsolution. Over the last few decades, the reinterpretation of classical image\nprocessing problems through the PDE lens has been creating multiple celebrated\napproaches that benefit a vast area of tasks including image segmentation,\ndenoising, registration, and reconstruction.\n  In this paper, we establish a new PDE-interpretation of a class of deep\nconvolutional neural networks (CNN) that are commonly used to learn from\nspeech, image, and video data. Our interpretation includes convolution residual\nneural networks (ResNet), which are among the most promising approaches for\ntasks such as image classification having improved the state-of-the-art\nperformance in prestigious benchmark challenges. Despite their recent\nsuccesses, deep ResNets still face some critical challenges associated with\ntheir design, immense computational costs and memory requirements, and lack of\nunderstanding of their reasoning.\n  Guided by well-established PDE theory, we derive three new ResNet\narchitectures that fall into two new classes: parabolic and hyperbolic CNNs. We\ndemonstrate how PDE theory can provide new insights and algorithms for deep\nlearning and demonstrate the competitiveness of three new CNN architectures\nusing numerical experiments. \n\n"}
{"id": "1804.04804", "contents": "Title: Learning Deep Sketch Abstraction Abstract: Human free-hand sketches have been studied in various contexts including\nsketch recognition, synthesis and fine-grained sketch-based image retrieval\n(FG-SBIR). A fundamental challenge for sketch analysis is to deal with\ndrastically different human drawing styles, particularly in terms of\nabstraction level. In this work, we propose the first stroke-level sketch\nabstraction model based on the insight of sketch abstraction as a process of\ntrading off between the recognizability of a sketch and the number of strokes\nused to draw it. Concretely, we train a model for abstract sketch generation\nthrough reinforcement learning of a stroke removal policy that learns to\npredict which strokes can be safely removed without affecting recognizability.\nWe show that our abstraction model can be used for various sketch analysis\ntasks including: (1) modeling stroke saliency and understanding the decision of\nsketch recognition models, (2) synthesizing sketches of variable abstraction\nfor a given category, or reference object instance in a photo, and (3) training\na FG-SBIR model with photos only, bypassing the expensive photo-sketch pair\ncollection step. \n\n"}
{"id": "1804.05275", "contents": "Title: Horizontal Pyramid Matching for Person Re-identification Abstract: Despite the remarkable recent progress, person re-identification (Re-ID)\napproaches are still suffering from the failure cases where the discriminative\nbody parts are missing. To mitigate such cases, we propose a simple yet\neffective Horizontal Pyramid Matching (HPM) approach to fully exploit various\npartial information of a given person, so that correct person candidates can be\nstill identified even even some key parts are missing. Within the HPM, we make\nthe following contributions to produce a more robust feature representation for\nthe Re-ID task: 1) we learn to classify using partial feature representations\nat different horizontal pyramid scales, which successfully enhance the\ndiscriminative capabilities of various person parts; 2) we exploit average and\nmax pooling strategies to account for person-specific discriminative\ninformation in a global-local manner. To validate the effectiveness of the\nproposed HPM, extensive experiments are conducted on three popular benchmarks,\nincluding Market-1501, DukeMTMC-ReID and CUHK03. In particular, we achieve mAP\nscores of 83.1%, 74.5% and 59.7% on these benchmarks, which are the new\nstate-of-the-arts. Our code is available on Github \n\n"}
{"id": "1804.05338", "contents": "Title: Attention-Gated Networks for Improving Ultrasound Scan Plane Detection Abstract: In this work, we apply an attention-gated network to real-time automated scan\nplane detection for fetal ultrasound screening. Scan plane detection in fetal\nultrasound is a challenging problem due the poor image quality resulting in low\ninterpretability for both clinicians and automated algorithms. To solve this,\nwe propose incorporating self-gated soft-attention mechanisms. A soft-attention\nmechanism generates a gating signal that is end-to-end trainable, which allows\nthe network to contextualise local information useful for prediction. The\nproposed attention mechanism is generic and it can be easily incorporated into\nany existing classification architectures, while only requiring a few\nadditional parameters. We show that, when the base network has a high capacity,\nthe incorporated attention mechanism can provide efficient object localisation\nwhile improving the overall performance. When the base network has a low\ncapacity, the method greatly outperforms the baseline approach and\nsignificantly reduces false positives. Lastly, the generated attention maps\nallow us to understand the model's reasoning process, which can also be used\nfor weakly supervised object localisation. \n\n"}
{"id": "1804.05810", "contents": "Title: ShapeShifter: Robust Physical Adversarial Attack on Faster R-CNN Object\n  Detector Abstract: Given the ability to directly manipulate image pixels in the digital input\nspace, an adversary can easily generate imperceptible perturbations to fool a\nDeep Neural Network (DNN) image classifier, as demonstrated in prior work. In\nthis work, we propose ShapeShifter, an attack that tackles the more challenging\nproblem of crafting physical adversarial perturbations to fool image-based\nobject detectors like Faster R-CNN. Attacking an object detector is more\ndifficult than attacking an image classifier, as it needs to mislead the\nclassification results in multiple bounding boxes with different scales.\nExtending the digital attack to the physical world adds another layer of\ndifficulty, because it requires the perturbation to be robust enough to survive\nreal-world distortions due to different viewing distances and angles, lighting\nconditions, and camera limitations. We show that the Expectation over\nTransformation technique, which was originally proposed to enhance the\nrobustness of adversarial perturbations in image classification, can be\nsuccessfully adapted to the object detection setting. ShapeShifter can generate\nadversarially perturbed stop signs that are consistently mis-detected by Faster\nR-CNN as other objects, posing a potential threat to autonomous vehicles and\nother safety-critical computer vision systems. \n\n"}
{"id": "1804.06215", "contents": "Title: DetNet: A Backbone network for Object Detection Abstract: Recent CNN based object detectors, no matter one-stage methods like YOLO,\nSSD, and RetinaNe or two-stage detectors like Faster R-CNN, R-FCN and FPN are\nusually trying to directly finetune from ImageNet pre-trained models designed\nfor image classification. There has been little work discussing on the backbone\nfeature extractor specifically designed for the object detection. More\nimportantly, there are several differences between the tasks of image\nclassification and object detection. 1. Recent object detectors like FPN and\nRetinaNet usually involve extra stages against the task of image classification\nto handle the objects with various scales. 2. Object detection not only needs\nto recognize the category of the object instances but also spatially locate the\nposition. Large downsampling factor brings large valid receptive field, which\nis good for image classification but compromises the object location ability.\nDue to the gap between the image classification and object detection, we\npropose DetNet in this paper, which is a novel backbone network specifically\ndesigned for object detection. Moreover, DetNet includes the extra stages\nagainst traditional backbone network for image classification, while maintains\nhigh spatial resolution in deeper layers. Without any bells and whistles,\nstate-of-the-art results have been obtained for both object detection and\ninstance segmentation on the MSCOCO benchmark based on our DetNet~(4.8G FLOPs)\nbackbone. The code will be released for the reproduction. \n\n"}
{"id": "1804.06786", "contents": "Title: Quantifying the visual concreteness of words and topics in multimodal\n  datasets Abstract: Multimodal machine learning algorithms aim to learn visual-textual\ncorrespondences. Previous work suggests that concepts with concrete visual\nmanifestations may be easier to learn than concepts with abstract ones. We give\nan algorithm for automatically computing the visual concreteness of words and\ntopics within multimodal datasets. We apply the approach in four settings,\nranging from image captions to images/text scraped from historical books. In\naddition to enabling explorations of concepts in multimodal datasets, our\nconcreteness scores predict the capacity of machine learning algorithms to\nlearn textual/visual relationships. We find that 1) concrete concepts are\nindeed easier to learn; 2) the large number of algorithms we consider have\nsimilar failure cases; 3) the precise positive relationship between\nconcreteness and performance varies between datasets. We conclude with\nrecommendations for using concreteness scores to facilitate future multimodal\nresearch. \n\n"}
{"id": "1804.07237", "contents": "Title: Multi-view Hybrid Embedding: A Divide-and-Conquer Approach Abstract: We present a novel cross-view classification algorithm where the gallery and\nprobe data come from different views. A popular approach to tackle this problem\nis the multi-view subspace learning (MvSL) that aims to learn a latent subspace\nshared by multi-view data. Despite promising results obtained on some\napplications, the performance of existing methods deteriorates dramatically\nwhen the multi-view data is sampled from nonlinear manifolds or suffers from\nheavy outliers. To circumvent this drawback, motivated by the\nDivide-and-Conquer strategy, we propose Multi-view Hybrid Embedding (MvHE), a\nunique method of dividing the problem of cross-view classification into three\nsubproblems and building one model for each subproblem. Specifically, the first\nmodel is designed to remove view discrepancy, whereas the second and third\nmodels attempt to discover the intrinsic nonlinear structure and to increase\ndiscriminability in intra-view and inter-view samples respectively. The kernel\nextension is conducted to further boost the representation power of MvHE.\nExtensive experiments are conducted on four benchmark datasets. Our methods\ndemonstrate overwhelming advantages against the state-of-the-art MvSL based\ncross-view classification approaches in terms of classification accuracy and\nrobustness. \n\n"}
{"id": "1804.07795", "contents": "Title: Stochastic subgradient method converges on tame functions Abstract: This work considers the question: what convergence guarantees does the\nstochastic subgradient method have in the absence of smoothness and convexity?\nWe prove that the stochastic subgradient method, on any semialgebraic locally\nLipschitz function, produces limit points that are all first-order stationary.\nMore generally, our result applies to any function with a Whitney stratifiable\ngraph. In particular, this work endows the stochastic subgradient method, and\nits proximal extension, with rigorous convergence guarantees for a wide class\nof problems arising in data science---including all popular deep learning\narchitectures. \n\n"}
{"id": "1804.07839", "contents": "Title: Large Scale Automated Reading of Frontal and Lateral Chest X-Rays using\n  Dual Convolutional Neural Networks Abstract: The MIMIC-CXR dataset is (to date) the largest released chest x-ray dataset\nconsisting of 473,064 chest x-rays and 206,574 radiology reports collected from\n63,478 patients. We present the results of training and evaluating a collection\nof deep convolutional neural networks on this dataset to recognize multiple\ncommon thorax diseases. To the best of our knowledge, this is the first work\nthat trains CNNs for this task on such a large collection of chest x-ray\nimages, which is over four times the size of the largest previously released\nchest x-ray corpus (ChestX-Ray14). We describe and evaluate individual CNN\nmodels trained on frontal and lateral CXR view types. In addition, we present a\nnovel DualNet architecture that emulates routine clinical practice by\nsimultaneously processing both frontal and lateral CXR images obtained from a\nradiological exam. Our DualNet architecture shows improved performance in\nrecognizing findings in CXR images when compared to applying separate baseline\nfrontal and lateral classifiers. \n\n"}
{"id": "1804.08281", "contents": "Title: Memory Matching Networks for One-Shot Image Recognition Abstract: In this paper, we introduce the new ideas of augmenting Convolutional Neural\nNetworks (CNNs) with Memory and learning to learn the network parameters for\nthe unlabelled images on the fly in one-shot learning. Specifically, we present\nMemory Matching Networks (MM-Net) --- a novel deep architecture that explores\nthe training procedure, following the philosophy that training and test\nconditions must match. Technically, MM-Net writes the features of a set of\nlabelled images (support set) into memory and reads from memory when performing\ninference to holistically leverage the knowledge in the set. Meanwhile, a\nContextual Learner employs the memory slots in a sequential manner to predict\nthe parameters of CNNs for unlabelled images. The whole architecture is trained\nby once showing only a few examples per class and switching the learning from\nminibatch to minibatch, which is tailored for one-shot learning when presented\nwith a few examples of new categories at test time. Unlike the conventional\none-shot learning approaches, our MM-Net could output one unified model\nirrespective of the number of shots and categories. Extensive experiments are\nconducted on two public datasets, i.e., Omniglot and \\emph{mini}ImageNet, and\nsuperior results are reported when compared to state-of-the-art approaches.\nMore remarkably, our MM-Net improves one-shot accuracy on Omniglot from 98.95%\nto 99.28% and from 49.21% to 53.37% on \\emph{mini}ImageNet. \n\n"}
{"id": "1804.08292", "contents": "Title: MVTec D2S: Densely Segmented Supermarket Dataset Abstract: We introduce the Densely Segmented Supermarket (D2S) dataset, a novel\nbenchmark for instance-aware semantic segmentation in an industrial domain. It\ncontains 21,000 high-resolution images with pixel-wise labels of all object\ninstances. The objects comprise groceries and everyday products from 60\ncategories. The benchmark is designed such that it resembles the real-world\nsetting of an automatic checkout, inventory, or warehouse system. The training\nimages only contain objects of a single class on a homogeneous background,\nwhile the validation and test sets are much more complex and diverse. To\nfurther benchmark the robustness of instance segmentation methods, the scenes\nare acquired with different lightings, rotations, and backgrounds. We ensure\nthat there are no ambiguities in the labels and that every instance is labeled\ncomprehensively. The annotations are pixel-precise and allow using crops of\nsingle instances for articial data augmentation. The dataset covers several\nchallenges highly relevant in the field, such as a limited amount of training\ndata and a high diversity in the test and validation sets. The evaluation of\nstate-of-the-art object detection and instance segmentation methods on D2S\nreveals significant room for improvement. \n\n"}
{"id": "1804.08378", "contents": "Title: BrainSlug: Transparent Acceleration of Deep Learning Through Depth-First\n  Parallelism Abstract: Neural network frameworks such as PyTorch and TensorFlow are the workhorses\nof numerous machine learning applications ranging from object recognition to\nmachine translation. While these frameworks are versatile and straightforward\nto use, the training of and inference in deep neural networks is resource\n(energy, compute, and memory) intensive. In contrast to recent works focusing\non algorithmic enhancements, we introduce BrainSlug, a framework that\ntransparently accelerates neural network workloads by changing the default\nlayer-by-layer processing to a depth-first approach, reducing the amount of\ndata required by the computations and thus improving the performance of the\navailable hardware caches. BrainSlug achieves performance improvements of up to\n41.1% on CPUs and 35.7% on GPUs. These optimizations come at zero cost to the\nuser as they do not require hardware changes and only need tiny adjustments to\nthe software. \n\n"}
{"id": "1804.08659", "contents": "Title: Fingerprint Match in Box Abstract: We open source fingerprint Match in Box, a complete end-to-end fingerprint\nrecognition system embedded within a 4 inch cube. Match in Box stands in\ncontrast to a typical bulky and expensive proprietary fingerprint recognition\nsystem which requires sending a fingerprint image to an external host for\nprocessing and subsequent spoof detection and matching. In particular, Match in\nBox is a first of a kind, portable, low-cost, and easy-to-assemble fingerprint\nreader with an enrollment database embedded within the reader's memory and open\nsource fingerprint spoof detector, feature extractor, and matcher all running\non the reader's internal vision processing unit (VPU). An onboard touch screen\nand rechargeable battery pack make this device extremely portable and ideal for\napplying both fingerprint authentication (1:1 comparison) and fingerprint\nidentification (1:N search) to applications (vaccination tracking, food and\nbenefit distribution programs, human trafficking prevention) in rural\ncommunities, especially in developing countries. We also show that Match in Box\nis suited for capturing neonate fingerprints due to its high resolution (1900\nppi) cameras. \n\n"}
{"id": "1804.08758", "contents": "Title: Switchable Temporal Propagation Network Abstract: Videos contain highly redundant information between frames. Such redundancy\nhas been extensively studied in video compression and encoding, but is less\nexplored for more advanced video processing. In this paper, we propose a\nlearnable unified framework for propagating a variety of visual properties of\nvideo images, including but not limited to color, high dynamic range (HDR), and\nsegmentation information, where the properties are available for only a few\nkey-frames. Our approach is based on a temporal propagation network (TPN),\nwhich models the transition-related affinity between a pair of frames in a\npurely data-driven manner. We theoretically prove two essential factors for\nTPN: (a) by regularizing the global transformation matrix as orthogonal, the\n\"style energy\" of the property can be well preserved during propagation; (b)\nsuch regularization can be achieved by the proposed switchable TPN with\nbi-directional training on pairs of frames. We apply the switchable TPN to\nthree tasks: colorizing a gray-scale video based on a few color key-frames,\ngenerating an HDR video from a low dynamic range (LDR) video and a few HDR\nframes, and propagating a segmentation mask from the first frame in videos.\nExperimental results show that our approach is significantly more accurate and\nefficient than the state-of-the-art methods. \n\n"}
{"id": "1804.10822", "contents": "Title: A Bimodal Learning Approach to Assist Multi-sensory Effects\n  Synchronization Abstract: In mulsemedia applications, traditional media content (text, image, audio,\nvideo, etc.) can be related to media objects that target other human senses\n(e.g., smell, haptics, taste). Such applications aim at bridging the virtual\nand real worlds through sensors and actuators. Actuators are responsible for\nthe execution of sensory effects (e.g., wind, heat, light), which produce\nsensory stimulations on the users. In these applications sensory stimulation\nmust happen in a timely manner regarding the other traditional media content\nbeing presented. For example, at the moment in which an explosion is presented\nin the audiovisual content, it may be adequate to activate actuators that\nproduce heat and light. It is common to use some declarative multimedia\nauthoring language to relate the timestamp in which each media object is to be\npresented to the execution of some sensory effect. One problem in this setting\nis that the synchronization of media objects and sensory effects is done\nmanually by the author(s) of the application, a process which is time-consuming\nand error prone. In this paper, we present a bimodal neural network\narchitecture to assist the synchronization task in mulsemedia applications. Our\napproach is based on the idea that audio and video signals can be used\nsimultaneously to identify the timestamps in which some sensory effect should\nbe executed. Our learning architecture combines audio and video signals for the\nprediction of scene components. For evaluation purposes, we construct a dataset\nbased on Google's AudioSet. We provide experiments to validate our bimodal\narchitecture. Our results show that the bimodal approach produces better\nresults when compared to several variants of unimodal architectures. \n\n"}
{"id": "1805.00355", "contents": "Title: Sample-to-Sample Correspondence for Unsupervised Domain Adaptation Abstract: The assumption that training and testing samples are generated from the same\ndistribution does not always hold for real-world machine-learning applications.\nThe procedure of tackling this discrepancy between the training (source) and\ntesting (target) domains is known as domain adaptation. We propose an\nunsupervised version of domain adaptation that considers the presence of only\nunlabelled data in the target domain. Our approach centers on finding\ncorrespondences between samples of each domain. The correspondences are\nobtained by treating the source and target samples as graphs and using a convex\ncriterion to match them. The criteria used are first-order and second-order\nsimilarities between the graphs as well as a class-based regularization. We\nhave also developed a computationally efficient routine for the convex\noptimization, thus allowing the proposed method to be used widely. To verify\nthe effectiveness of the proposed method, computer simulations were conducted\non synthetic, image classification and sentiment classification datasets.\nResults validated that the proposed local sample-to-sample matching method\nout-performs traditional moment-matching methods and is competitive with\nrespect to current local domain-adaptation methods. \n\n"}
{"id": "1805.00385", "contents": "Title: Boosting Self-Supervised Learning via Knowledge Transfer Abstract: In self-supervised learning, one trains a model to solve a so-called pretext\ntask on a dataset without the need for human annotation. The main objective,\nhowever, is to transfer this model to a target domain and task. Currently, the\nmost effective transfer strategy is fine-tuning, which restricts one to use the\nsame model or parts thereof for both pretext and target tasks. In this paper,\nwe present a novel framework for self-supervised learning that overcomes\nlimitations in designing and comparing different tasks, models, and data\ndomains. In particular, our framework decouples the structure of the\nself-supervised model from the final task-specific fine-tuned model. This\nallows us to: 1) quantitatively assess previously incompatible models including\nhandcrafted features; 2) show that deeper neural network models can learn\nbetter representations from the same pretext task; 3) transfer knowledge\nlearned with a deep model to a shallower one and thus boost its learning. We\nuse this framework to design a novel self-supervised task, which achieves\nstate-of-the-art performance on the common benchmarks in PASCAL VOC 2007,\nILSVRC12 and Places by a significant margin. Our learned features shrink the\nmAP gap between models trained via self-supervised learning and supervised\nlearning from 5.9% to 2.6% in object detection on PASCAL VOC 2007. \n\n"}
{"id": "1805.00599", "contents": "Title: Placement Delivery Array Design via Attention-Based Deep Neural Network Abstract: A decentralized coded caching scheme has been proposed by Maddah-Ali and\nNiesen, and has been shown to alleviate the load of networks. Recently,\nplacement delivery array (PDA) was proposed to characterize the coded caching\nscheme. In this paper, a neural architecture is first proposed to learn the\nconstruction of PDAs. Our model solves the problem of variable size PDAs using\nmechanism of neural attention and reinforcement learning. It differs from the\nprevious attempts in that, instead of using combined optimization algorithms to\nget PDAs, it uses sequence-to-sequence model to learn construct PDAs. Numerical\nresults are given to demonstrate that the proposed method can effectively\nimplement coded caching. We also show that the complexity of our method to\nconstruct PDAs is low. \n\n"}
{"id": "1805.00676", "contents": "Title: Text to Image Synthesis Using Generative Adversarial Networks Abstract: Generating images from natural language is one of the primary applications of\nrecent conditional generative models. Besides testing our ability to model\nconditional, highly dimensional distributions, text to image synthesis has many\nexciting and practical applications such as photo editing or computer-aided\ncontent creation. Recent progress has been made using Generative Adversarial\nNetworks (GANs). This material starts with a gentle introduction to these\ntopics and discusses the existent state of the art models. Moreover, I propose\nWasserstein GAN-CLS, a new model for conditional image generation based on the\nWasserstein distance which offers guarantees of stability. Then, I show how the\nnovel loss function of Wasserstein GAN-CLS can be used in a Conditional\nProgressive Growing GAN. In combination with the proposed loss, the model\nboosts by 7.07% the best Inception Score (on the Caltech birds dataset) of the\nmodels which use only the sentence-level visual semantics. The only model which\nperforms better than the Conditional Wasserstein Progressive Growing GAN is the\nrecently proposed AttnGAN which uses word-level visual semantics as well. \n\n"}
{"id": "1805.00706", "contents": "Title: On the Structure of Interlinked Cycle Structures with Interlocked Outer\n  Cycles Abstract: For index coding problems with special structure on the side-information\ngraphs called Interlinked Cycle (IC) structures index codes have been proposed\nin the literature (C. Thapa, L. Ong, and S. Johnson, \"Interlinked Cycles for\nIndex Coding: Generalizing Cycles and Cliques\", in IEEE Trans. Inf. Theory,\nvol. 63, no. 6, Jun. 2017, with a correction in \"Interlinked Cycles for Index\nCoding: Generalizing Cycles and Cliques\", in arxiv (arxiv:1603.00092v2 [cs.IT]\n25 Feb 2018)). Recently (S. Sasi and B.S. Rajan, \"On Optimal Index Codes for\nInterlinked Cycle Structures with Outer Cycles,\" in arxiv (arXiv:1804.09120v1\n[cs.IT]), 24 Apr 2018) for a generalization of IC structures called IC\nstructures with interlocked outer cycles optimal length index codes have been\nreported and it is shown that the optimal length depends on the maximum number\nof disjoint outer cycles. In this paper we discuss certain structural\nproperties of IC structures with interlocked outer cycles and provide a simple\nalgorithm to find the maximum number of disjoint outer cycles. \n\n"}
{"id": "1805.00862", "contents": "Title: Spectral clustering algorithms for the detection of clusters in\n  block-cyclic and block-acyclic graphs Abstract: We propose two spectral algorithms for partitioning nodes in directed graphs\nrespectively with a cyclic and an acyclic pattern of connection between groups\nof nodes. Our methods are based on the computation of extremal eigenvalues of\nthe transition matrix associated to the directed graph. The two algorithms\noutperform state-of-the art methods for directed graph clustering on synthetic\ndatasets, including methods based on blockmodels, bibliometric symmetrization\nand random walks. Our algorithms have the same space complexity as classical\nspectral clustering algorithms for undirected graphs and their time complexity\nis also linear in the number of edges in the graph. One of our methods is\napplied to a trophic network based on predator-prey relationships. It\nsuccessfully extracts common categories of preys and predators encountered in\nfood chains. The same method is also applied to highlight the hierarchical\nstructure of a worldwide network of Autonomous Systems depicting business\nagreements between Internet Service Providers. \n\n"}
{"id": "1805.00980", "contents": "Title: SaaS: Speed as a Supervisor for Semi-supervised Learning Abstract: We introduce the SaaS Algorithm for semi-supervised learning, which uses\nlearning speed during stochastic gradient descent in a deep neural network to\nmeasure the quality of an iterative estimate of the posterior probability of\nunknown labels. Training speed in supervised learning correlates strongly with\nthe percentage of correct labels, so we use it as an inference criterion for\nthe unknown labels, without attempting to infer the model parameters at first.\nDespite its simplicity, SaaS achieves state-of-the-art results in\nsemi-supervised learning benchmarks. \n\n"}
{"id": "1805.01290", "contents": "Title: Multi-task Learning of Cascaded CNN for Facial Attribute Classification Abstract: Recently, facial attribute classification (FAC) has attracted significant\nattention in the computer vision community. Great progress has been made along\nwith the availability of challenging FAC datasets. However, conventional FAC\nmethods usually firstly pre-process the input images (i.e., perform face\ndetection and alignment) and then predict facial attributes. These methods\nignore the inherent dependencies among these tasks (i.e., face detection,\nfacial landmark localization and FAC). Moreover, some methods using\nconvolutional neural network are trained based on the fixed loss weights\nwithout considering the differences between facial attributes. In order to\naddress the above problems, we propose a novel multi-task learning of cas-\ncaded convolutional neural network method, termed MCFA, for predicting multiple\nfacial attributes simultaneously. Specifically, the proposed method takes\nadvantage of three cascaded sub-networks (i.e., S_Net, M_Net and L_Net\ncorresponding to the neural networks under different scales) to jointly train\nmultiple tasks in a coarse-to-fine manner, which can achieve end-to-end\noptimization. Furthermore, the proposed method automatically assigns the loss\nweight to each facial attribute based on a novel dynamic weighting scheme, thus\nmaking the proposed method concentrate on predicting the more difficult facial\nattributes. Experimental results show that the proposed method outperforms\nseveral state-of-the-art FAC methods on the challenging CelebA and LFWA\ndatasets. \n\n"}
{"id": "1805.01890", "contents": "Title: RMDL: Random Multimodel Deep Learning for Classification Abstract: The continually increasing number of complex datasets each year necessitates\never improving machine learning methods for robust and accurate categorization\nof these data. This paper introduces Random Multimodel Deep Learning (RMDL): a\nnew ensemble, deep learning approach for classification. Deep learning models\nhave achieved state-of-the-art results across many domains. RMDL solves the\nproblem of finding the best deep learning structure and architecture while\nsimultaneously improving robustness and accuracy through ensembles of deep\nlearning architectures. RDML can accept as input a variety data to include\ntext, video, images, and symbolic. This paper describes RMDL and shows test\nresults for image and text data including MNIST, CIFAR-10, WOS, Reuters, IMDB,\nand 20newsgroup. These test results show that RDML produces consistently better\nperformance than standard methods over a broad range of data types and\nclassification problems. \n\n"}
{"id": "1805.02475", "contents": "Title: Comparative evaluation of instrument segmentation and tracking methods\n  in minimally invasive surgery Abstract: Intraoperative segmentation and tracking of minimally invasive instruments is\na prerequisite for computer- and robotic-assisted surgery. Since additional\nhardware like tracking systems or the robot encoders are cumbersome and lack\naccuracy, surgical vision is evolving as promising techniques to segment and\ntrack the instruments using only the endoscopic images. However, what is\nmissing so far are common image data sets for consistent evaluation and\nbenchmarking of algorithms against each other. The paper presents a comparative\nvalidation study of different vision-based methods for instrument segmentation\nand tracking in the context of robotic as well as conventional laparoscopic\nsurgery. The contribution of the paper is twofold: we introduce a comprehensive\nvalidation data set that was provided to the study participants and present the\nresults of the comparative validation study. Based on the results of the\nvalidation study, we arrive at the conclusion that modern deep learning\napproaches outperform other methods in instrument segmentation tasks, but the\nresults are still not perfect. Furthermore, we show that merging results from\ndifferent methods actually significantly increases accuracy in comparison to\nthe best stand-alone method. On the other hand, the results of the instrument\ntracking task show that this is still an open challenge, especially during\nchallenging scenarios in conventional laparoscopic surgery. \n\n"}
{"id": "1805.02481", "contents": "Title: MEGAN: Mixture of Experts of Generative Adversarial Networks for\n  Multimodal Image Generation Abstract: Recently, generative adversarial networks (GANs) have shown promising\nperformance in generating realistic images. However, they often struggle in\nlearning complex underlying modalities in a given dataset, resulting in\npoor-quality generated images. To mitigate this problem, we present a novel\napproach called mixture of experts GAN (MEGAN), an ensemble approach of\nmultiple generator networks. Each generator network in MEGAN specializes in\ngenerating images with a particular subset of modalities, e.g., an image class.\nInstead of incorporating a separate step of handcrafted clustering of multiple\nmodalities, our proposed model is trained through an end-to-end learning of\nmultiple generators via gating networks, which is responsible for choosing the\nappropriate generator network for a given condition. We adopt the categorical\nreparameterization trick for a categorical decision to be made in selecting a\ngenerator while maintaining the flow of the gradients. We demonstrate that\nindividual generators learn different and salient subparts of the data and\nachieve a multiscale structural similarity (MS-SSIM) score of 0.2470 for CelebA\nand a competitive unsupervised inception score of 8.33 in CIFAR-10. \n\n"}
{"id": "1805.02523", "contents": "Title: Detecting Traffic Lights by Single Shot Detection Abstract: Recent improvements in object detection are driven by the success of\nconvolutional neural networks (CNN). They are able to learn rich features\noutperforming hand-crafted features. So far, research in traffic light\ndetection mainly focused on hand-crafted features, such as color, shape or\nbrightness of the traffic light bulb. This paper presents a deep learning\napproach for accurate traffic light detection in adapting a single shot\ndetection (SSD) approach. SSD performs object proposals creation and\nclassification using a single CNN. The original SSD struggles in detecting very\nsmall objects, which is essential for traffic light detection. By our\nadaptations it is possible to detect objects much smaller than ten pixels\nwithout increasing the input image size. We present an extensive evaluation on\nthe DriveU Traffic Light Dataset (DTLD). We reach both, high accuracy and low\nfalse positive rates. The trained model is real-time capable with ten frames\nper second on a Nvidia Titan Xp. Code has been made available at\nhttps://github.com/julimueller/tl_ssd. \n\n"}
{"id": "1805.04384", "contents": "Title: Exploiting Images for Video Recognition with Hierarchical Generative\n  Adversarial Networks Abstract: Existing deep learning methods of video recognition usually require a large\nnumber of labeled videos for training. But for a new task, videos are often\nunlabeled and it is also time-consuming and labor-intensive to annotate them.\nInstead of human annotation, we try to make use of existing fully labeled\nimages to help recognize those videos. However, due to the problem of domain\nshifts and heterogeneous feature representations, the performance of\nclassifiers trained on images may be dramatically degraded for video\nrecognition tasks. In this paper, we propose a novel method, called\nHierarchical Generative Adversarial Networks (HiGAN), to enhance recognition in\nvideos (i.e., target domain) by transferring knowledge from images (i.e.,\nsource domain). The HiGAN model consists of a \\emph{low-level} conditional GAN\nand a \\emph{high-level} conditional GAN. By taking advantage of these two-level\nadversarial learning, our method is capable of learning a domain-invariant\nfeature representation of source images and target videos. Comprehensive\nexperiments on two challenging video recognition datasets (i.e. UCF101 and\nHMDB51) demonstrate the effectiveness of the proposed method when compared with\nthe existing state-of-the-art domain adaptation methods. \n\n"}
{"id": "1805.05062", "contents": "Title: Token-level and sequence-level loss smoothing for RNN language models Abstract: Despite the effectiveness of recurrent neural network language models, their\nmaximum likelihood estimation suffers from two limitations. It treats all\nsentences that do not match the ground truth as equally poor, ignoring the\nstructure of the output space. Second, it suffers from \"exposure bias\": during\ntraining tokens are predicted given ground-truth sequences, while at test time\nprediction is conditioned on generated output sequences. To overcome these\nlimitations we build upon the recent reward augmented maximum likelihood\napproach \\ie sequence-level smoothing that encourages the model to predict\nsentences close to the ground truth according to a given performance metric. We\nextend this approach to token-level loss smoothing, and propose improvements to\nthe sequence-level smoothing approach. Our experiments on two different tasks,\nimage captioning and machine translation, show that token-level and\nsequence-level loss smoothing are complementary, and significantly improve\nresults. \n\n"}
{"id": "1805.05719", "contents": "Title: Optimal convergence rates for Nesterov acceleration Abstract: In this paper, we study the behavior of solutions of the ODE associated to\nNesterov acceleration. It is well-known since the pioneering work of Nesterov\nthat the rate of convergence $O(1/t^2)$ is optimal for the class of convex\nfunctions with Lipschitz gradient. In this work, we show that better\nconvergence rates can be obtained with some additional geometrical conditions,\nsuch as \\L ojasiewicz property. More precisely, we prove the optimal\nconvergence rates that can be obtained depending on the geometry of the\nfunction $F$ to minimize. The convergence rates are new, and they shed new\nlight on the behavior of Nesterov acceleration schemes. We prove in particular\nthat the classical Nesterov scheme may provide convergence rates that are worse\nthan the classical gradient descent scheme on sharp functions: for instance,\nthe convergence rate for strongly convex functions is not geometric for the\nclassical Nesterov scheme (while it is the case for the gradient descent\nalgorithm). This shows that applying the classical Nesterov acceleration on\nconvex functions without looking more at the geometrical properties of the\nobjective functions may lead to sub-optimal algorithms. \n\n"}
{"id": "1805.05727", "contents": "Title: 2sRanking-CNN: A 2-stage ranking-CNN for diagnosis of glaucoma from\n  fundus images using CAM-extracted ROI as an intermediate input Abstract: Glaucoma is a disease in which the optic nerve is chronically damaged by the\nelevation of the intra-ocular pressure, resulting in visual field defect.\nTherefore, it is important to monitor and treat suspected patients before they\nare confirmed with glaucoma. In this paper, we propose a 2-stage ranking-CNN\nthat classifies fundus images as normal, suspicious, and glaucoma. Furthermore,\nwe propose a method of using the class activation map as a mask filter and\ncombining it with the original fundus image as an intermediate input. Our\nresults have improved the average accuracy by about 10% over the existing\n3-class CNN and ranking-CNN, and especially improved the sensitivity of\nsuspicious class by more than 20% over 3-class CNN. In addition, the extracted\nROI was also found to overlap with the diagnostic criteria of the physician.\nThe method we propose is expected to be efficiently applied to any medical data\nwhere there is a suspicious condition between normal and disease. \n\n"}
{"id": "1805.06386", "contents": "Title: Neural Multi-scale Image Compression Abstract: This study presents a new lossy image compression method that utilizes the\nmulti-scale features of natural images. Our model consists of two networks:\nmulti-scale lossy autoencoder and parallel multi-scale lossless coder. The\nmulti-scale lossy autoencoder extracts the multi-scale image features to\nquantized variables and the parallel multi-scale lossless coder enables rapid\nand accurate lossless coding of the quantized variables via encoding/decoding\nthe variables in parallel. Our proposed model achieves comparable performance\nto the state-of-the-art model on Kodak and RAISE-1k dataset images, and it\nencodes a PNG image of size $768 \\times 512$ in 70 ms with a single GPU and a\nsingle CPU process and decodes it into a high-fidelity image in approximately\n200 ms. \n\n"}
{"id": "1805.06523", "contents": "Title: End-to-end Learning of a Convolutional Neural Network via Deep Tensor\n  Decomposition Abstract: In this paper we study the problem of learning the weights of a deep\nconvolutional neural network. We consider a network where convolutions are\ncarried out over non-overlapping patches with a single kernel in each layer. We\ndevelop an algorithm for simultaneously learning all the kernels from the\ntraining data. Our approach dubbed Deep Tensor Decomposition (DeepTD) is based\non a rank-1 tensor decomposition. We theoretically investigate DeepTD under a\nrealizable model for the training data where the inputs are chosen i.i.d. from\na Gaussian distribution and the labels are generated according to planted\nconvolutional kernels. We show that DeepTD is data-efficient and provably works\nas soon as the sample size exceeds the total number of convolutional weights in\nthe network. We carry out a variety of numerical experiments to investigate the\neffectiveness of DeepTD and verify our theoretical findings. \n\n"}
{"id": "1805.07281", "contents": "Title: An Unsupervised Approach to Solving Inverse Problems using Generative\n  Adversarial Networks Abstract: Solving inverse problems continues to be a challenge in a wide array of\napplications ranging from deblurring, image inpainting, source separation etc.\nMost existing techniques solve such inverse problems by either explicitly or\nimplicitly finding the inverse of the model. The former class of techniques\nrequire explicit knowledge of the measurement process which can be unrealistic,\nand rely on strong analytical regularizers to constrain the solution space,\nwhich often do not generalize well. The latter approaches have had remarkable\nsuccess in part due to deep learning, but require a large collection of\nsource-observation pairs, which can be prohibitively expensive. In this paper,\nwe propose an unsupervised technique to solve inverse problems with generative\nadversarial networks (GANs). Using a pre-trained GAN in the space of source\nsignals, we show that one can reliably recover solutions to under determined\nproblems in a `blind' fashion, i.e., without knowledge of the measurement\nprocess. We solve this by making successive estimates on the model and the\nsolution in an iterative fashion. We show promising results in three\nchallenging applications -- blind source separation, image deblurring, and\nrecovering an image from its edge map, and perform better than several\nbaselines. \n\n"}
{"id": "1805.07567", "contents": "Title: Optimizing the F-measure for Threshold-free Salient Object Detection Abstract: Current CNN-based solutions to salient object detection (SOD) mainly rely on\nthe optimization of cross-entropy loss (CELoss). Then the quality of detected\nsaliency maps is often evaluated in terms of F-measure. In this paper, we\ninvestigate an interesting issue: can we consistently use the F-measure\nformulation in both training and evaluation for SOD? By reformulating the\nstandard F-measure we propose the relaxed F-measure which is differentiable\nw.r.t the posterior and can be easily appended to the back of CNNs as the loss\nfunction. Compared to the conventional cross-entropy loss of which the\ngradients decrease dramatically in the saturated area, our loss function, named\nFLoss, holds considerable gradients even when the activation approaches the\ntarget. Consequently, the FLoss can continuously force the network to produce\npolarized activations. Comprehensive benchmarks on several popular datasets\nshow that FLoss outperforms the state-of-the-art with a considerable margin.\nMore specifically, due to the polarized predictions, our method is able to\nobtain high-quality saliency maps without carefully tuning the optimal\nthreshold, showing significant advantages in real-world applications. \n\n"}
{"id": "1805.08249", "contents": "Title: Classifier-agnostic saliency map extraction Abstract: Currently available methods for extracting saliency maps identify parts of\nthe input which are the most important to a specific fixed classifier. We show\nthat this strong dependence on a given classifier hinders their performance. To\naddress this problem, we propose classifier-agnostic saliency map extraction,\nwhich finds all parts of the image that any classifier could use, not just one\ngiven in advance. We observe that the proposed approach extracts higher quality\nsaliency maps than prior work while being conceptually simple and easy to\nimplement. The method sets the new state of the art result for localization\ntask on the ImageNet data, outperforming all existing weakly-supervised\nlocalization techniques, despite not using the ground truth labels at the\ninference time. The code reproducing the results is available at\nhttps://github.com/kondiz/casme .\n  The final version of this manuscript is published in Computer Vision and\nImage Understanding and is available online at\nhttps://doi.org/10.1016/j.cviu.2020.102969 . \n\n"}
{"id": "1805.08587", "contents": "Title: Deep Feature Aggregation and Image Re-ranking with Heat Diffusion for\n  Image Retrieval Abstract: Image retrieval based on deep convolutional features has demonstrated\nstate-of-the-art performance in popular benchmarks. In this paper, we present a\nunified solution to address deep convolutional feature aggregation and image\nre-ranking by simulating the dynamics of heat diffusion. A distinctive problem\nin image retrieval is that repetitive or \\emph{bursty} features tend to\ndominate final image representations, resulting in representations less\ndistinguishable. We show that by considering each deep feature as a heat\nsource, our unsupervised aggregation method is able to avoid\nover-representation of \\emph{bursty} features. We additionally provide a\npractical solution for the proposed aggregation method and further show the\nefficiency of our method in experimental evaluation. Inspired by the\naforementioned deep feature aggregation method, we also propose a method to\nre-rank a number of top ranked images for a given query image by considering\nthe query as the heat source. Finally, we extensively evaluate the proposed\napproach with pre-trained and fine-tuned deep networks on common public\nbenchmarks and show superior performance compared to previous work. \n\n"}
{"id": "1805.08805", "contents": "Title: Resource Aware Person Re-identification across Multiple Resolutions Abstract: Not all people are equally easy to identify: color statistics might be enough\nfor some cases while others might require careful reasoning about high- and\nlow-level details. However, prevailing person re-identification(re-ID) methods\nuse one-size-fits-all high-level embeddings from deep convolutional networks\nfor all cases. This might limit their accuracy on difficult examples or makes\nthem needlessly expensive for the easy ones. To remedy this, we present a new\nperson re-ID model that combines effective embeddings built on multiple\nconvolutional network layers, trained with deep-supervision. On traditional\nre-ID benchmarks, our method improves substantially over the previous\nstate-of-the-art results on all five datasets that we evaluate on. We then\npropose two new formulations of the person re-ID problem under\nresource-constraints, and show how our model can be used to effectively trade\noff accuracy and computation in the presence of resource constraints. Code and\npre-trained models are available at https://github.com/mileyan/DARENet. \n\n"}
{"id": "1805.09028", "contents": "Title: Efficient Relaxations for Dense CRFs with Sparse Higher Order Potentials Abstract: Dense conditional random fields (CRFs) have become a popular framework for\nmodelling several problems in computer vision such as stereo correspondence and\nmulti-class semantic segmentation. By modelling long-range interactions, dense\nCRFs provide a labelling that captures finer detail than their sparse\ncounterparts. Currently, the state-of-the-art algorithm performs mean-field\ninference using a filter-based method but fails to provide a strong theoretical\nguarantee on the quality of the solution. A question naturally arises as to\nwhether it is possible to obtain a maximum a posteriori (MAP) estimate of a\ndense CRF using a principled method. Within this paper, we show that this is\nindeed possible. We will show that, by using a filter-based method, continuous\nrelaxations of the MAP problem can be optimised efficiently using\nstate-of-the-art algorithms. Specifically, we will solve a quadratic\nprogramming (QP) relaxation using the Frank-Wolfe algorithm and a linear\nprogramming (LP) relaxation by developing a proximal minimisation framework. By\nexploiting labelling consistency in the higher-order potentials and utilising\nthe filter-based method, we are able to formulate the above algorithms such\nthat each iteration has a complexity linear in the number of classes and random\nvariables. The presented algorithms can be applied to any labelling problem\nusing a dense CRF with sparse higher-order potentials. In this paper, we use\nsemantic segmentation as an example application as it demonstrates the ability\nof the algorithm to scale to dense CRFs with large dimensions. We perform\nexperiments on the Pascal dataset to indicate that the presented algorithms are\nable to attain lower energies than the mean-field inference method. \n\n"}
{"id": "1805.09403", "contents": "Title: A Projection Approach to Equality Constrained Iterative Linear Quadratic\n  Optimal Control Abstract: This paper presents a state and state-input constrained variant of the\ndiscrete-time iterative Linear Quadratic Regulator (iLQR) algorithm, with\nlinear time-complexity in the number of time steps. The approach is based on a\nprojection of the control input onto the nullspace of the linearized\nconstraints. We derive a fully constraint-compliant feedforward-feedback\ncontrol update rule, for which we can solve efficiently with Riccati-style\ndifference equations. We assume that the relative degree of all constraints in\nthe discrete-time system model is equal to one, which often holds for robotics\nproblems employing rigid-body dynamic models. Simulation examples, including a\n6 DoF robotic arm, are given to validate and illustrate the performance of the\nmethod. \n\n"}
{"id": "1805.09639", "contents": "Title: Online Regularized Nonlinear Acceleration Abstract: Regularized nonlinear acceleration (RNA) estimates the minimum of a function\nby post-processing iterates from an algorithm such as the gradient method. It\ncan be seen as a regularized version of Anderson acceleration, a classical\nacceleration scheme from numerical analysis. The new scheme provably improves\nthe rate of convergence of fixed step gradient descent, and its empirical\nperformance is comparable to that of quasi-Newton methods. However, RNA cannot\naccelerate faster multistep algorithms like Nesterov's method and often\ndiverges in this context. Here, we adapt RNA to overcome these issues, so that\nour scheme can be used on fast algorithms such as gradient methods with\nmomentum. We show optimal complexity bounds for quadratics and asymptotically\noptimal rates on general convex minimization problems. Moreover, this new\nscheme works online, i.e., extrapolated solution estimates can be reinjected at\neach iteration, significantly improving numerical performance over classical\naccelerated methods. \n\n"}
{"id": "1805.09687", "contents": "Title: Corpus Conversion Service: A machine learning platform to ingest\n  documents at scale [Poster abstract] Abstract: Over the past few decades, the amount of scientific articles and technical\nliterature has increased exponentially in size. Consequently, there is a great\nneed for systems that can ingest these documents at scale and make their\ncontent discoverable. Unfortunately, both the format of these documents (e.g.\nthe PDF format or bitmap images) as well as the presentation of the data (e.g.\ncomplex tables) make the extraction of qualitative and quantitive data\nextremely challenging. We present a platform to ingest documents at scale which\nis powered by Machine Learning techniques and allows the user to train custom\nmodels on document collections. We show precision/recall results greater than\n97% with regard to conversion to structured formats, as well as scaling\nevidence for each of the microservices constituting the platform. \n\n"}
{"id": "1805.09785", "contents": "Title: Entropy and mutual information in models of deep neural networks Abstract: We examine a class of deep learning models with a tractable method to compute\ninformation-theoretic quantities. Our contributions are three-fold: (i) We show\nhow entropies and mutual informations can be derived from heuristic statistical\nphysics methods, under the assumption that weight matrices are independent and\northogonally-invariant. (ii) We extend particular cases in which this result is\nknown to be rigorously exact by providing a proof for two-layers networks with\nGaussian random weights, using the recently introduced adaptive interpolation\nmethod. (iii) We propose an experiment framework with generative models of\nsynthetic datasets, on which we train deep neural networks with a weight\nconstraint designed so that the assumption in (i) is verified during learning.\nWe study the behavior of entropies and mutual informations throughout learning\nand conclude that, in the proposed setting, the relationship between\ncompression and generalization remains elusive. \n\n"}
{"id": "1805.10561", "contents": "Title: Adversarial Constraint Learning for Structured Prediction Abstract: Constraint-based learning reduces the burden of collecting labels by having\nusers specify general properties of structured outputs, such as constraints\nimposed by physical laws. We propose a novel framework for simultaneously\nlearning these constraints and using them for supervision, bypassing the\ndifficulty of using domain expertise to manually specify constraints. Learning\nrequires a black-box simulator of structured outputs, which generates valid\nlabels, but need not model their corresponding inputs or the input-label\nrelationship. At training time, we constrain the model to produce outputs that\ncannot be distinguished from simulated labels by adversarial training.\nProviding our framework with a small number of labeled inputs gives rise to a\nnew semi-supervised structured prediction model; we evaluate this model on\nmultiple tasks --- tracking, pose estimation and time series prediction --- and\nfind that it achieves high accuracy with only a small number of labeled inputs.\nIn some cases, no labels are required at all. \n\n"}
{"id": "1805.10737", "contents": "Title: Deep Adversarial Context-Aware Landmark Detection for Ultrasound Imaging Abstract: Real-time localization of prostate gland in trans-rectal ultrasound images is\na key technology that is required to automate the ultrasound guided prostate\nbiopsy procedures. In this paper, we propose a new deep learning based approach\nwhich is aimed at localizing several prostate landmarks efficiently and\nrobustly. We propose a multitask learning approach primarily to make the\noverall algorithm more contextually aware. In this approach, we not only\nconsider the explicit learning of landmark locations, but also build-in a\nmechanism to learn the contour of the prostate. This multitask learning is\nfurther coupled with an adversarial arm to promote the generation of feasible\nstructures. We have trained this network using ~4000 labeled trans-rectal\nultrasound images and tested on an independent set of images with ground truth\nlandmark locations. We have achieved an overall Dice score of 92.6% for the\nadversarially trained multitask approach, which is significantly better than\nthe Dice score of 88.3% obtained by only learning of landmark locations. The\noverall mean distance error using the adversarial multitask approach has also\nimproved by 20% while reducing the standard deviation of the error compared to\nlearning landmark locations only. In terms of computational complexity both\napproaches can process the images in real-time using standard computer with a\nstandard CUDA enabled GPU. \n\n"}
{"id": "1805.10864", "contents": "Title: Versatile Auxiliary Regressor with Generative Adversarial network\n  (VAR+GAN) Abstract: Being able to generate constrained samples is one of the most appealing\napplications of the deep generators. Conditional generators are one of the\nsuccessful implementations of such models wherein the created samples are\nconstrained to a specific class. In this work, the application of these\nnetworks is extended to regression problems wherein the conditional generator\nis restrained to any continuous aspect of the data. A new loss function is\npresented for the regression network and also implementations for generating\nfaces with any particular set of landmarks is provided. \n\n"}
{"id": "1805.11770", "contents": "Title: AutoZOOM: Autoencoder-based Zeroth Order Optimization Method for\n  Attacking Black-box Neural Networks Abstract: Recent studies have shown that adversarial examples in state-of-the-art image\nclassifiers trained by deep neural networks (DNN) can be easily generated when\nthe target model is transparent to an attacker, known as the white-box setting.\nHowever, when attacking a deployed machine learning service, one can only\nacquire the input-output correspondences of the target model; this is the\nso-called black-box attack setting. The major drawback of existing black-box\nattacks is the need for excessive model queries, which may give a false sense\nof model robustness due to inefficient query designs. To bridge this gap, we\npropose a generic framework for query-efficient black-box attacks. Our\nframework, AutoZOOM, which is short for Autoencoder-based Zeroth Order\nOptimization Method, has two novel building blocks towards efficient black-box\nattacks: (i) an adaptive random gradient estimation strategy to balance query\ncounts and distortion, and (ii) an autoencoder that is either trained offline\nwith unlabeled data or a bilinear resizing operation for attack acceleration.\nExperimental results suggest that, by applying AutoZOOM to a state-of-the-art\nblack-box attack (ZOO), a significant reduction in model queries can be\nachieved without sacrificing the attack success rate and the visual quality of\nthe resulting adversarial examples. In particular, when compared to the\nstandard ZOO method, AutoZOOM can consistently reduce the mean query counts in\nfinding successful adversarial examples (or reaching the same distortion level)\nby at least 93% on MNIST, CIFAR-10 and ImageNet datasets, leading to novel\ninsights on adversarial robustness. \n\n"}
{"id": "1805.11895", "contents": "Title: RLS Recovery with Asymmetric Penalty: Fundamental Limits and Algorithmic\n  Approaches Abstract: This paper studies regularized least square recovery of signals whose\nsamples' prior distributions are nonidentical, e.g., signals with time-variant\nsparsity. For this model, Bayesian framework suggests to regularize the least\nsquares term with an asymmetric penalty. We investigate this problem in two\nrespects: First, we characterize the asymptotic performance via the replica\nmethod and then discuss algorithmic approaches to the problem. Invoking the\nasymptotic characterization of the performance, we propose a tuning strategy to\noptimally tune the algorithmic approaches for recovery. To demonstrate\napplications of the results, the particular example of BPSK recovery is\ninvestigated and the efficiency of the proposed strategy is depicted in the\nshadow of results available in the literature \n\n"}
{"id": "1805.12279", "contents": "Title: Bayesian Pose Graph Optimization via Bingham Distributions and Tempered\n  Geodesic MCMC Abstract: We introduce Tempered Geodesic Markov Chain Monte Carlo (TG-MCMC) algorithm\nfor initializing pose graph optimization problems, arising in various scenarios\nsuch as SFM (structure from motion) or SLAM (simultaneous localization and\nmapping). TG-MCMC is first of its kind as it unites asymptotically global\nnon-convex optimization on the spherical manifold of quaternions with posterior\nsampling, in order to provide both reliable initial poses and uncertainty\nestimates that are informative about the quality of individual solutions. We\ndevise rigorous theoretical convergence guarantees for our method and\nextensively evaluate it on synthetic and real benchmark datasets. Besides its\nelegance in formulation and theory, we show that our method is robust to\nmissing data, noise and the estimated uncertainties capture intuitive\nproperties of the data. \n\n"}
{"id": "1805.12358", "contents": "Title: Light Field Denoising via Anisotropic Parallax Analysis in a CNN\n  Framework Abstract: Light field (LF) cameras provide perspective information of scenes by taking\ndirectional measurements of the focusing light rays. The raw outputs are\nusually dark with additive camera noise, which impedes subsequent processing\nand applications. We propose a novel LF denoising framework based on\nanisotropic parallax analysis (APA). Two convolutional neural networks are\njointly designed for the task: first, the structural parallax synthesis network\npredicts the parallax details for the entire LF based on a set of anisotropic\nparallax features. These novel features can efficiently capture the high\nfrequency perspective components of a LF from noisy observations. Second, the\nview-dependent detail compensation network restores non-Lambertian variation to\neach LF view by involving view-specific spatial energies. Extensive experiments\nshow that the proposed APA LF denoiser provides a much better denoising\nperformance than state-of-the-art methods in terms of visual quality and in\npreservation of parallax details. \n\n"}
{"id": "1805.12395", "contents": "Title: Deep Learning with unsupervised data labeling for weeds detection on UAV\n  images Abstract: In modern agriculture, usually weeds control consists in spraying herbicides\nall over the agricultural field. This practice involves significant waste and\ncost of herbicide for farmers and environmental pollution. One way to reduce\nthe cost and environmental impact is to allocate the right doses of herbicide\nat the right place and at the right time (Precision Agriculture). Nowadays,\nUnmanned Aerial Vehicle (UAV) is becoming an interesting acquisition system for\nweeds localization and management due to its ability to obtain the images of\nthe entire agricultural field with a very high spatial resolution and at low\ncost. Despite the important advances in UAV acquisition systems, automatic\nweeds detection remains a challenging problem because of its strong similarity\nwith the crops. Recently Deep Learning approach has shown impressive results in\ndifferent complex classification problem. However, this approach needs a\ncertain amount of training data but, creating large agricultural datasets with\npixel-level annotations by expert is an extremely time consuming task. In this\npaper, we propose a novel fully automatic learning method using Convolutional\nNeuronal Networks (CNNs) with unsupervised training dataset collection for\nweeds detection from UAV images. The proposed method consists in three main\nphases. First we automatically detect the crop lines and using them to identify\nthe interline weeds. In the second phase, interline weeds are used to\nconstitute the training dataset. Finally, we performed CNNs on this dataset to\nbuild a model able to detect the crop and weeds in the images. The results\nobtained are comparable to the traditional supervised training data labeling.\nThe accuracy gaps are 1.5% in the spinach field and 6% in the bean field. \n\n"}
{"id": "1806.00186", "contents": "Title: Video Description: A Survey of Methods, Datasets and Evaluation Metrics Abstract: Video description is the automatic generation of natural language sentences\nthat describe the contents of a given video. It has applications in human-robot\ninteraction, helping the visually impaired and video subtitling. The past few\nyears have seen a surge of research in this area due to the unprecedented\nsuccess of deep learning in computer vision and natural language processing.\nNumerous methods, datasets and evaluation metrics have been proposed in the\nliterature, calling the need for a comprehensive survey to focus research\nefforts in this flourishing new direction. This paper fills the gap by\nsurveying the state of the art approaches with a focus on deep learning models;\ncomparing benchmark datasets in terms of their domains, number of classes, and\nrepository size; and identifying the pros and cons of various evaluation\nmetrics like SPICE, CIDEr, ROUGE, BLEU, METEOR, and WMD. Classical video\ndescription approaches combined subject, object and verb detection with\ntemplate based language models to generate sentences. However, the release of\nlarge datasets revealed that these methods can not cope with the diversity in\nunconstrained open domain videos. Classical approaches were followed by a very\nshort era of statistical methods which were soon replaced with deep learning,\nthe current state of the art in video description. Our survey shows that\ndespite the fast-paced developments, video description research is still in its\ninfancy due to the following reasons. Analysis of video description models is\nchallenging because it is difficult to ascertain the contributions, towards\naccuracy or errors, of the visual features and the adopted language model in\nthe final description. Existing datasets neither contain adequate visual\ndiversity nor complexity of linguistic structures. Finally, current evaluation\nmetrics ... \n\n"}
{"id": "1806.00202", "contents": "Title: Online Learning with Inexact Proximal Online Gradient Descent Algorithms Abstract: We consider non-differentiable dynamic optimization problems such as those\narising in robotics and subspace tracking. Given the computational constraints\nand the time-varying nature of the problem, a low-complexity algorithm is\ndesirable, while the accuracy of the solution may only increase slowly over\ntime. We put forth the proximal online gradient descent (OGD) algorithm for\ntracking the optimum of a composite objective function comprising of a\ndifferentiable loss function and a non-differentiable regularizer. An online\nlearning framework is considered and the gradient of the loss function is\nallowed to be erroneous. Both, the gradient error as well as the dynamics of\nthe function optimum or target are adversarial and the performance of the\ninexact proximal OGD is characterized in terms of its dynamic regret, expressed\nin terms of the cumulative error and path length of the target. The proposed\ninexact proximal OGD is generalized for application to large-scale problems\nwhere the loss function has a finite sum structure. In such cases, evaluation\nof the full gradient may not be viable and a variance reduced version is\nproposed that allows the component functions to be sub-sampled. The efficacy of\nthe proposed algorithms is tested on the problem of formation control in\nrobotics and on the dynamic foreground-background separation problem in video. \n\n"}
{"id": "1806.00406", "contents": "Title: New Gramians for Linear Switched Systems: Reachability, Observability,\n  and Model Reduction Abstract: In this paper, we propose new algebraic Gramians for continuous-time linear\nswitched systems, which satisfy generalized Lyapunov equations. The main\ncontribution of this work is twofold. First, we show that the ranges of those\nGramians encode the reachability and observability spaces of a linear switched\nsystem. As a consequence, a simple Gramian-based criterion for reachability and\nobservability is established. Second, a balancing-based model order reduction\ntechnique is proposed and, under some sufficient conditions, stability\npreservation and an error bound are shown. Finally, the efficiency of the\nproposed method is illustrated by means of numerical examples. \n\n"}
{"id": "1806.00466", "contents": "Title: Surgical Activity Recognition in Robot-Assisted Radical Prostatectomy\n  using Deep Learning Abstract: Adverse surgical outcomes are costly to patients and hospitals. Approaches to\nbenchmark surgical care are often limited to gross measures across the entire\nprocedure despite the performance of particular tasks being largely responsible\nfor undesirable outcomes. In order to produce metrics from tasks as opposed to\nthe whole procedure, methods to recognize automatically individual surgical\ntasks are needed. In this paper, we propose several approaches to recognize\nsurgical activities in robot-assisted minimally invasive surgery using deep\nlearning. We collected a clinical dataset of 100 robot-assisted radical\nprostatectomies (RARP) with 12 tasks each and propose `RP-Net', a modified\nversion of InceptionV3 model, for image based surgical activity recognition. We\nachieve an average precision of 80.9% and average recall of 76.7% across all\ntasks using RP-Net which out-performs all other RNN and CNN based models\nexplored in this paper. Our results suggest that automatic surgical activity\nrecognition during RARP is feasible and can be the foundation for advanced\nanalytics. \n\n"}
{"id": "1806.00580", "contents": "Title: Detecting Adversarial Examples via Key-based Network Abstract: Though deep neural networks have achieved state-of-the-art performance in\nvisual classification, recent studies have shown that they are all vulnerable\nto the attack of adversarial examples. Small and often imperceptible\nperturbations to the input images are sufficient to fool the most powerful deep\nneural networks. Various defense methods have been proposed to address this\nissue. However, they either require knowledge on the process of generating\nadversarial examples, or are not robust against new attacks specifically\ndesigned to penetrate the existing defense. In this work, we introduce\nkey-based network, a new detection-based defense mechanism to distinguish\nadversarial examples from normal ones based on error correcting output codes,\nusing the binary code vectors produced by multiple binary classifiers applied\nto randomly chosen label-sets as signatures to match normal images and reject\nadversarial examples. In contrast to existing defense methods, the proposed\nmethod does not require knowledge of the process for generating adversarial\nexamples and can be applied to defend against different types of attacks. For\nthe practical black-box and gray-box scenarios, where the attacker does not\nknow the encoding scheme, we show empirically that key-based network can\neffectively detect adversarial examples generated by several state-of-the-art\nattacks. \n\n"}
{"id": "1806.00737", "contents": "Title: Content-based Video Relevance Prediction Challenge: Data, Protocol, and\n  Baseline Abstract: Video relevance prediction is one of the most important tasks for online\nstreaming service. Given the relevance of videos and viewer feedbacks, the\nsystem can provide personalized recommendations, which will help the user\ndiscover more content of interest. In most online service, the computation of\nvideo relevance table is based on users' implicit feedback, e.g. watch and\nsearch history. However, this kind of method performs poorly for \"cold-start\"\nproblems - when a new video is added to the library, the recommendation system\nneeds to bootstrap the video relevance score with very little user behavior\nknown. One promising approach to solve it is analyzing video content itself,\ni.e. predicting video relevance by video frame, audio, subtitle and metadata.\nIn this paper, we describe a challenge on Content-based Video Relevance\nPrediction (CBVRP) that is hosted by Hulu in the ACM Multimedia Conference\n2018. In this challenge, Hulu drives the study on an open problem of exploiting\ncontent characteristics directly from original video for video relevance\nprediction. We provide massive video assets and ground truth relevance derived\nfrom our really system, to build up a common platform for algorithm development\nand performance evaluation. \n\n"}
{"id": "1806.01225", "contents": "Title: Image reconstruction through metamorphosis Abstract: This article adapts the framework of metamorphosis to solve inverse problems\nin imaging that includes joint reconstruction and image registration. The\ndeformations in question have two components, one that is a geometric\ndeformation moving intensities and the other a deformation of intensity values\nitself, which, e.g., allows for appearance of a new structure. The idea\ndeveloped here is to reconstruct an image from noisy and indirect observations\nby registering, via metamorphosis, a template to the observed data. Unlike a\nregistration with only geometrical changes, this framework gives good results\nwhen intensities of the template are poorly chosen. We show that this method is\na well-defined regularisation method (proving existence, stability and\nconvergence) and present several numerical examples. \n\n"}
{"id": "1806.01376", "contents": "Title: Factorized Adversarial Networks for Unsupervised Domain Adaptation Abstract: In this paper, we propose Factorized Adversarial Networks (FAN) to solve\nunsupervised domain adaptation problems for image classification tasks. Our\nnetworks map the data distribution into a latent feature space, which is\nfactorized into a domain-specific subspace that contains domain-specific\ncharacteristics and a task-specific subspace that retains category information,\nfor both source and target domains, respectively. Unsupervised domain\nadaptation is achieved by adversarial training to minimize the discrepancy\nbetween the distributions of two task-specific subspaces from source and target\ndomains. We demonstrate that the proposed approach outperforms state-of-the-art\nmethods on multiple benchmark datasets used in the literature for unsupervised\ndomain adaptation. Furthermore, we collect two real-world tagging datasets that\nare much larger than existing benchmark datasets, and get significant\nimprovement upon baselines, proving the practical value of our approach. \n\n"}
{"id": "1806.01484", "contents": "Title: 3D Human Pose Estimation with 2D Marginal Heatmaps Abstract: Automatically determining three-dimensional human pose from monocular RGB\nimage data is a challenging problem. The two-dimensional nature of the input\nresults in intrinsic ambiguities which make inferring depth particularly\ndifficult. Recently, researchers have demonstrated that the flexible\nstatistical modelling capabilities of deep neural networks are sufficient to\nmake such inferences with reasonable accuracy. However, many of these models\nuse coordinate output techniques which are memory-intensive, not\ndifferentiable, and/or do not spatially generalise well. We propose\nimprovements to 3D coordinate prediction which avoid the aforementioned\nundesirable traits by predicting 2D marginal heatmaps under an augmented\nsoft-argmax scheme. Our resulting model, MargiPose, produces visually coherent\nheatmaps whilst maintaining differentiability. We are also able to achieve\nstate-of-the-art accuracy on publicly available 3D human pose estimation data. \n\n"}
{"id": "1806.01673", "contents": "Title: Recurrent Convolutional Fusion for RGB-D Object Recognition Abstract: Providing machines with the ability to recognize objects like humans has\nalways been one of the primary goals of machine vision. The introduction of\nRGB-D cameras has paved the way for a significant leap forward in this\ndirection thanks to the rich information provided by these sensors. However,\nthe machine vision community still lacks an effective method to synergically\nuse the RGB and depth data to improve object recognition. In order to take a\nstep in this direction, we introduce a novel end-to-end architecture for RGB-D\nobject recognition called recurrent convolutional fusion (RCFusion). Our method\ngenerates compact and highly discriminative multi-modal features by combining\ncomplementary RGB and depth information representing different levels of\nabstraction. Extensive experiments on two popular datasets, RGB-D Object\nDataset and JHUIT-50, show that RCFusion significantly outperforms\nstate-of-the-art approaches in both the object categorization and instance\nrecognition tasks. \n\n"}
{"id": "1806.01799", "contents": "Title: Survey and Taxonomy of Lossless Graph Compression and Space-Efficient\n  Graph Representations Abstract: Various graphs such as web or social networks may contain up to trillions of\nedges. Compressing such datasets can accelerate graph processing by reducing\nthe amount of I/O accesses and the pressure on the memory subsystem. Yet,\nselecting a proper compression method is challenging as there exist a plethora\nof techniques, algorithms, domains, and approaches in compressing graphs. To\nfacilitate this, we present a survey and taxonomy on lossless graph compression\nthat is the first, to the best of our knowledge, to exhaustively analyze this\ndomain. Moreover, our survey does not only categorize existing schemes, but\nalso explains key ideas, discusses formal underpinning in selected works, and\ndescribes the space of the existing compression schemes using three dimensions:\nareas of research (e.g., compressing web graphs), techniques (e.g., gap\nencoding), and features (e.g., whether or not a given scheme targets dynamic\ngraphs). Our survey can be used as a guide to select the best lossless\ncompression scheme in a given setting. \n\n"}
{"id": "1806.02015", "contents": "Title: Distributed Hypothesis Testing with Privacy Constraints Abstract: We revisit the distributed hypothesis testing (or hypothesis testing with\ncommunication constraints) problem from the viewpoint of privacy. Instead of\nobserving the raw data directly, the transmitter observes a sanitized or\nrandomized version of it. We impose an upper bound on the mutual information\nbetween the raw and randomized data. Under this scenario, the receiver, which\nis also provided with side information, is required to make a decision on\nwhether the null or alternative hypothesis is in effect. We first provide a\ngeneral lower bound on the type-II exponent for an arbitrary pair of\nhypotheses. Next, we show that if the distribution under the alternative\nhypothesis is the product of the marginals of the distribution under the null\n(i.e., testing against independence), then the exponent is known exactly.\nMoreover, we show that the strong converse property holds. Using ideas from\nEuclidean information theory, we also provide an approximate expression for the\nexponent when the communication rate is low and the privacy level is high.\nFinally, we illustrate our results with a binary and a Gaussian example. \n\n"}
{"id": "1806.02299", "contents": "Title: DPatch: An Adversarial Patch Attack on Object Detectors Abstract: Object detectors have emerged as an indispensable module in modern computer\nvision systems. In this work, we propose DPatch -- a black-box\nadversarial-patch-based attack towards mainstream object detectors (i.e. Faster\nR-CNN and YOLO). Unlike the original adversarial patch that only manipulates\nimage-level classifier, our DPatch simultaneously attacks the bounding box\nregression and object classification so as to disable their predictions.\nCompared to prior works, DPatch has several appealing properties: (1) DPatch\ncan perform both untargeted and targeted effective attacks, degrading the mAP\nof Faster R-CNN and YOLO from 75.10% and 65.7% down to below 1%, respectively.\n(2) DPatch is small in size and its attacking effect is location-independent,\nmaking it very practical to implement real-world attacks. (3) DPatch\ndemonstrates great transferability among different detectors as well as\ntraining datasets. For example, DPatch that is trained on Faster R-CNN can\neffectively attack YOLO, and vice versa. Extensive evaluations imply that\nDPatch can perform effective attacks under black-box setup, i.e., even without\nthe knowledge of the attacked network's architectures and parameters.\nSuccessful realization of DPatch also illustrates the intrinsic vulnerability\nof the modern detector architectures to such patch-based adversarial attacks. \n\n"}
{"id": "1806.04265", "contents": "Title: Accurate and Robust Neural Networks for Security Related Applications\n  Exampled by Face Morphing Attacks Abstract: Artificial neural networks tend to learn only what they need for a task. A\nmanipulation of the training data can counter this phenomenon. In this paper,\nwe study the effect of different alterations of the training data, which limit\nthe amount and position of information that is available for the decision\nmaking. We analyze the accuracy and robustness against semantic and black box\nattacks on the networks that were trained on different training data\nmodifications for the particular example of morphing attacks. A morphing attack\nis an attack on a biometric facial recognition system where the system is\nfooled to match two different individuals with the same synthetic face image.\nSuch a synthetic image can be created by aligning and blending images of the\ntwo individuals that should be matched with this image. \n\n"}
{"id": "1806.04498", "contents": "Title: The Unusual Effectiveness of Averaging in GAN Training Abstract: We examine two different techniques for parameter averaging in GAN training.\nMoving Average (MA) computes the time-average of parameters, whereas\nExponential Moving Average (EMA) computes an exponentially discounted sum.\nWhilst MA is known to lead to convergence in bilinear settings, we provide the\n-- to our knowledge -- first theoretical arguments in support of EMA. We show\nthat EMA converges to limit cycles around the equilibrium with vanishing\namplitude as the discount parameter approaches one for simple bilinear games\nand also enhances the stability of general GAN training. We establish\nexperimentally that both techniques are strikingly effective in the\nnon-convex-concave GAN setting as well. Both improve inception and FID scores\non different architectures and for different GAN objectives. We provide\ncomprehensive experimental results across a range of datasets -- mixture of\nGaussians, CIFAR-10, STL-10, CelebA and ImageNet -- to demonstrate its\neffectiveness. We achieve state-of-the-art results on CIFAR-10 and produce\nclean CelebA face images.\\footnote{~The code is available at\n\\url{https://github.com/yasinyazici/EMA_GAN}} \n\n"}
{"id": "1806.05030", "contents": "Title: Visually grounded cross-lingual keyword spotting in speech Abstract: Recent work considered how images paired with speech can be used as\nsupervision for building speech systems when transcriptions are not available.\nWe ask whether visual grounding can be used for cross-lingual keyword spotting:\ngiven a text keyword in one language, the task is to retrieve spoken utterances\ncontaining that keyword in another language. This could enable searching\nthrough speech in a low-resource language using text queries in a high-resource\nlanguage. As a proof-of-concept, we use English speech with German queries: we\nuse a German visual tagger to add keyword labels to each training image, and\nthen train a neural network to map English speech to German keywords. Without\nseeing parallel speech-transcriptions or translations, the model achieves a\nprecision at ten of 58%. We show that most erroneous retrievals contain\nequivalent or semantically relevant keywords; excluding these would improve\nP@10 to 91%. \n\n"}
{"id": "1806.05341", "contents": "Title: From Trailers to Storylines: An Efficient Way to Learn from Movies Abstract: The millions of movies produced in the human history are valuable resources\nfor computer vision research. However, learning a vision model from movie data\nwould meet with serious difficulties. A major obstacle is the computational\ncost -- the length of a movie is often over one hour, which is substantially\nlonger than the short video clips that previous study mostly focuses on. In\nthis paper, we explore an alternative approach to learning vision models from\nmovies. Specifically, we consider a framework comprised of a visual module and\na temporal analysis module. Unlike conventional learning methods, the proposed\napproach learns these modules from different sets of data -- the former from\ntrailers while the latter from movies. This allows distinctive visual features\nto be learned within a reasonable budget while still preserving long-term\ntemporal structures across an entire movie. We construct a large-scale dataset\nfor this study and define a series of tasks on top. Experiments on this dataset\nshowed that the proposed method can substantially reduce the training time\nwhile obtaining highly effective features and coherent temporal structures. \n\n"}
{"id": "1806.05660", "contents": "Title: Interactive Classification for Deep Learning Interpretation Abstract: We present an interactive system enabling users to manipulate images to\nexplore the robustness and sensitivity of deep learning image classifiers.\nUsing modern web technologies to run in-browser inference, users can remove\nimage features using inpainting algorithms and obtain new classifications in\nreal time, which allows them to ask a variety of \"what if\" questions by\nexperimentally modifying images and seeing how the model reacts. Our system\nallows users to compare and contrast what image regions humans and machine\nlearning models use for classification, revealing a wide range of surprising\nresults ranging from spectacular failures (e.g., a \"water bottle\" image becomes\na \"concert\" when removing a person) to impressive resilience (e.g., a \"baseball\nplayer\" image remains correctly classified even without a glove or base). We\ndemonstrate our system at The 2018 Conference on Computer Vision and Pattern\nRecognition (CVPR) for the audience to try it live. Our system is open-sourced\nat https://github.com/poloclub/interactive-classification. A video demo is\navailable at https://youtu.be/llub5GcOF6w. \n\n"}
{"id": "1806.06004", "contents": "Title: Partially-Supervised Image Captioning Abstract: Image captioning models are becoming increasingly successful at describing\nthe content of images in restricted domains. However, if these models are to\nfunction in the wild - for example, as assistants for people with impaired\nvision - a much larger number and variety of visual concepts must be\nunderstood. To address this problem, we teach image captioning models new\nvisual concepts from labeled images and object detection datasets. Since image\nlabels and object classes can be interpreted as partial captions, we formulate\nthis problem as learning from partially-specified sequence data. We then\npropose a novel algorithm for training sequence models, such as recurrent\nneural networks, on partially-specified sequences which we represent using\nfinite state automata. In the context of image captioning, our method lifts the\nrestriction that previously required image captioning models to be trained on\npaired image-sentence corpora only, or otherwise required specialized model\narchitectures to take advantage of alternative data modalities. Applying our\napproach to an existing neural captioning model, we achieve state of the art\nresults on the novel object captioning task using the COCO dataset. We further\nshow that we can train a captioning model to describe new visual concepts from\nthe Open Images dataset while maintaining competitive COCO evaluation scores. \n\n"}
{"id": "1806.06594", "contents": "Title: Deep Recurrent Neural Network for Multi-target Filtering Abstract: This paper addresses the problem of fixed motion and measurement models for\nmulti-target filtering using an adaptive learning framework. This is performed\nby defining target tuples with random finite set terminology and utilisation of\nrecurrent neural networks with a long short-term memory architecture. A novel\ndata association algorithm compatible with the predicted tracklet tuples is\nproposed, enabling the update of occluded targets, in addition to assigning\nbirth, survival and death of targets. The algorithm is evaluated over a\ncommonly used filtering simulation scenario, with highly promising results. \n\n"}
{"id": "1806.06811", "contents": "Title: Temporal coherence-based self-supervised learning for laparoscopic\n  workflow analysis Abstract: In order to provide the right type of assistance at the right time,\ncomputer-assisted surgery systems need context awareness. To achieve this,\nmethods for surgical workflow analysis are crucial. Currently, convolutional\nneural networks provide the best performance for video-based workflow analysis\ntasks. For training such networks, large amounts of annotated data are\nnecessary. However, collecting a sufficient amount of data is often costly,\ntime-consuming, and not always feasible. In this paper, we address this problem\nby presenting and comparing different approaches for self-supervised\npretraining of neural networks on unlabeled laparoscopic videos using temporal\ncoherence. We evaluate our pretrained networks on Cholec80, a publicly\navailable dataset for surgical phase segmentation, on which a maximum F1 score\nof 84.6 was reached. Furthermore, we were able to achieve an increase of the F1\nscore of up to 10 points when compared to a non-pretrained neural network. \n\n"}
{"id": "1806.06939", "contents": "Title: Bayesian Prediction of Future Street Scenes through Importance Sampling\n  based Optimization Abstract: For autonomous agents to successfully operate in the real world, anticipation\nof future events and states of their environment is a key competence. This\nproblem can be formalized as a sequence prediction problem, where a number of\nobservations are used to predict the sequence into the future. However,\nreal-world scenarios demand a model of uncertainty of such predictions, as\nfuture states become increasingly uncertain and multi-modal -- in particular on\nlong time horizons. This makes modelling and learning challenging. We cast\nstate of the art semantic segmentation and future prediction models based on\ndeep learning into a Bayesian formulation that in turn allows for a full\nBayesian treatment of the prediction problem. We present a new sampling scheme\nfor this model that draws from the success of variational autoencoders by\nincorporating a recognition network. In the experiments we show that our model\noutperforms prior work in accuracy of the predicted segmentation and provides\ncalibrated probabilities that also better capture the multi-modal aspects of\npossible future states of street scenes. \n\n"}
{"id": "1806.08054", "contents": "Title: Error Compensated Quantized SGD and its Applications to Large-scale\n  Distributed Optimization Abstract: Large-scale distributed optimization is of great importance in various\napplications. For data-parallel based distributed learning, the inter-node\ngradient communication often becomes the performance bottleneck. In this paper,\nwe propose the error compensated quantized stochastic gradient descent\nalgorithm to improve the training efficiency. Local gradients are quantized to\nreduce the communication overhead, and accumulated quantization error is\nutilized to speed up the convergence. Furthermore, we present theoretical\nanalysis on the convergence behaviour, and demonstrate its advantage over\ncompetitors. Extensive experiments indicate that our algorithm can compress\ngradients by a factor of up to two magnitudes without performance degradation. \n\n"}
{"id": "1806.08990", "contents": "Title: Stroke-based Character Reconstruction Abstract: Background elimination for noisy character images or character images from\nreal scene is still a challenging problem, due to the bewildering backgrounds,\nuneven illumination, low resolution and different distortions. We propose a\nstroke-based character reconstruction(SCR) method that use a weighted quadratic\nBezier curve(WQBC) to represent strokes of a character. Only training on our\nsynthetic data, our stroke extractor can achieve excellent reconstruction\neffect in real scenes. Meanwhile. It can also help achieve great ability in\ndefending adversarial attacks of character recognizers. \n\n"}
{"id": "1806.09071", "contents": "Title: Walrasian Equilibrium and Centralized Distributed Optimization from the\n  point of view of Modern Convex Optimization Methods on the Example of\n  Resource Allocation Problem Abstract: We consider the resource allocation problem and its numerical solution. The\nfollowing constructions are demonstrated: 1) Walrasian price-adjustment\nmechanism for determining the equilibrium; 2) Decentralized role of the prices;\n3) Slater's method for price restrictions (dual Lagrange multipliers); 4) A new\nmechanism for determining equilibrium prices, in which prices are fully\ncontrolled not by Center (Government), but by economic agents -- nodes\n(factories). In economic literature the convergence of the considered methods\nis only proved. In contrast, this paper provides an accurate analysis of the\nconvergence rate of the described procedures for determining the equilibrium.\nThe analysis is based on the primal-dual nature of the suggested algorithms.\nMore precisely, in this article we propose the economic interpretation of the\nfollowing numerical primal-dual methods of convex optimization: dichotomy and\nsubgradient projection method. Numerical experiments conclude the paper. \n\n"}
{"id": "1806.09408", "contents": "Title: Transmission-Constrained Unit Commitment Abstract: The unit commitment with transmission constraints in the alternating-current\n(AC) model is a challenging mixed-integer non-linear optimisation problem. We\npresent an approach based on decomposition of a Mixed-Integer Semidefinite\nProgramming (MISDP) problem into a mixed-integer quadratic (MIQP) master\nproblem and a semidefinite programming (SDP) sub-problem. Between the master\nproblem and the sub-problem, we pass novel classes of cuts. We analyse finite\nconvergence to the optimum of the MISDP and report promising computational\nresults on a test case from the Canary Islands, Spain. \n\n"}
{"id": "1806.09507", "contents": "Title: Semi-Automatic RECIST Labeling on CT Scans with Cascaded Convolutional\n  Neural Networks Abstract: Response evaluation criteria in solid tumors (RECIST) is the standard\nmeasurement for tumor extent to evaluate treatment responses in cancer\npatients. As such, RECIST annotations must be accurate. However, RECIST\nannotations manually labeled by radiologists require professional knowledge and\nare time-consuming, subjective, and prone to inconsistency among different\nobservers. To alleviate these problems, we propose a cascaded convolutional\nneural network based method to semi-automatically label RECIST annotations and\ndrastically reduce annotation time. The proposed method consists of two stages:\nlesion region normalization and RECIST estimation. We employ the spatial\ntransformer network (STN) for lesion region normalization, where a localization\nnetwork is designed to predict the lesion region and the transformation\nparameters with a multi-task learning strategy. For RECIST estimation, we adapt\nthe stacked hourglass network (SHN), introducing a relationship constraint loss\nto improve the estimation precision. STN and SHN can both be learned in an\nend-to-end fashion. We train our system on the DeepLesion dataset, obtaining a\nconsensus model trained on RECIST annotations performed by multiple\nradiologists over a multi-year period. Importantly, when judged against the\ninter-reader variability of two additional radiologist raters, our system\nperforms more stably and with less variability, suggesting that RECIST\nannotations can be reliably obtained with reduced labor and time. \n\n"}
{"id": "1806.09613", "contents": "Title: Attention-based Few-Shot Person Re-identification Using Meta Learning Abstract: In this paper, we investigate the challenging task of person\nre-identification from a new perspective and propose an end-to-end\nattention-based architecture for few-shot re-identification through\nmeta-learning. The motivation for this task lies in the fact that humans, can\nusually identify another person after just seeing that given person a few times\n(or even once) by attending to their memory. On the other hand, the unique\nnature of the person re-identification problem, i.e., only few examples exist\nper identity and new identities always appearing during testing, calls for a\nfew shot learning architecture with the capacity of handling new identities.\nHence, we frame the problem within a meta-learning setting, where a neural\nnetwork based meta-learner is trained to optimize a learner i.e., an\nattention-based matching function. Another challenge of the person\nre-identification problem is the small inter-class difference between different\nidentities and large intra-class difference of the same identity. In order to\nincrease the discriminative power of the model, we propose a new\nattention-based feature encoding scheme that takes into account the critical\nintra-view and cross-view relationship of images. We refer to the proposed\nAttention-based Re-identification Metalearning model as ARM. Extensive\nevaluations demonstrate the advantages of the ARM as compared to the\nstate-of-the-art on the challenging PRID2011, CUHK01, CUHK03 and Market1501\ndatasets. \n\n"}
{"id": "1806.10359", "contents": "Title: Context Proposals for Saliency Detection Abstract: One of the fundamental properties of a salient object region is its contrast\nwith the immediate context. The problem is that numerous object regions exist\nwhich potentially can all be salient. One way to prevent an exhaustive search\nover all object regions is by using object proposal algorithms. These return a\nlimited set of regions which are most likely to contain an object. Several\nsaliency estimation methods have used object proposals. However, they focus on\nthe saliency of the proposal only, and the importance of its immediate context\nhas not been evaluated.\n  In this paper, we aim to improve salient object detection. Therefore, we\nextend object proposal methods with context proposals, which allow to\nincorporate the immediate context in the saliency computation. We propose\nseveral saliency features which are computed from the context proposals. In the\nexperiments, we evaluate five object proposal methods for the task of saliency\nsegmentation, and find that Multiscale Combinatorial Grouping outperforms the\nothers. Furthermore, experiments show that the proposed context features\nimprove performance, and that our method matches results on the FT datasets and\nobtains competitive results on three other datasets (PASCAL-S, MSRA-B and\nECSSD). \n\n"}
{"id": "1806.10419", "contents": "Title: MTBI Identification From Diffusion MR Images Using Bag of Adversarial\n  Visual Features Abstract: In this work, we propose bag of adversarial features (BAF) for identifying\nmild traumatic brain injury (MTBI) patients from their diffusion magnetic\nresonance images (MRI) (obtained within one month of injury) by incorporating\nunsupervised feature learning techniques. MTBI is a growing public health\nproblem with an estimated incidence of over 1.7 million people annually in US.\nDiagnosis is based on clinical history and symptoms, and accurate, concrete\nmeasures of injury are lacking. Unlike most of previous works, which use\nhand-crafted features extracted from different parts of brain for MTBI\nclassification, we employ feature learning algorithms to learn more\ndiscriminative representation for this task. A major challenge in this field\nthus far is the relatively small number of subjects available for training.\nThis makes it difficult to use an end-to-end convolutional neural network to\ndirectly classify a subject from MR images. To overcome this challenge, we\nfirst apply an adversarial auto-encoder (with convolutional structure) to learn\npatch-level features, from overlapping image patches extracted from different\nbrain regions. We then aggregate these features through a bag-of-word approach.\nWe perform an extensive experimental study on a dataset of 227 subjects\n(including 109 MTBI patients, and 118 age and sex matched healthy controls),\nand compare the bag-of-deep-features with several previous approaches. Our\nexperimental results show that the BAF significantly outperforms earlier works\nrelying on the mean values of MR metrics in selected brain regions. \n\n"}
{"id": "1806.10457", "contents": "Title: Physics-based Scene-level Reasoning for Object Pose Estimation in\n  Clutter Abstract: This paper focuses on vision-based pose estimation for multiple rigid objects\nplaced in clutter, especially in cases involving occlusions and objects resting\non each other. Progress has been achieved recently in object recognition given\nadvancements in deep learning. Nevertheless, such tools typically require a\nlarge amount of training data and significant manual effort to label objects.\nThis limits their applicability in robotics, where solutions must scale to a\nlarge number of objects and variety of conditions. Moreover, the combinatorial\nnature of the scenes that could arise from the placement of multiple objects is\nhard to capture in the training dataset. Thus, the learned models might not\nproduce the desired level of precision required for tasks, such as robotic\nmanipulation. This work proposes an autonomous process for pose estimation that\nspans from data generation to scene-level reasoning and self-learning. In\nparticular, the proposed framework first generates a labeled dataset for\ntraining a Convolutional Neural Network (CNN) for object detection in clutter.\nThese detections are used to guide a scene-level optimization process, which\nconsiders the interactions between the different objects present in the clutter\nto output pose estimates of high precision. Furthermore, confident estimates\nare used to label online real images from multiple views and re-train the\nprocess in a self-learning pipeline. Experimental results indicate that this\nprocess is quickly able to identify in cluttered scenes physically-consistent\nobject poses that are more precise than the ones found by reasoning over\nindividual instances of objects. Furthermore, the quality of pose estimates\nincreases over time given the self-learning process. \n\n"}
{"id": "1806.10779", "contents": "Title: Differentiable Learning-to-Normalize via Switchable Normalization Abstract: We address a learning-to-normalize problem by proposing Switchable\nNormalization (SN), which learns to select different normalizers for different\nnormalization layers of a deep neural network. SN employs three distinct scopes\nto compute statistics (means and variances) including a channel, a layer, and a\nminibatch. SN switches between them by learning their importance weights in an\nend-to-end manner. It has several good properties. First, it adapts to various\nnetwork architectures and tasks (see Fig.1). Second, it is robust to a wide\nrange of batch sizes, maintaining high performance even when small minibatch is\npresented (e.g. 2 images/GPU). Third, SN does not have sensitive\nhyper-parameter, unlike group normalization that searches the number of groups\nas a hyper-parameter. Without bells and whistles, SN outperforms its\ncounterparts on various challenging benchmarks, such as ImageNet, COCO,\nCityScapes, ADE20K, and Kinetics. Analyses of SN are also presented. We hope SN\nwill help ease the usage and understand the normalization techniques in deep\nlearning. The code of SN has been made available in\nhttps://github.com/switchablenorms/. \n\n"}
{"id": "1806.10890", "contents": "Title: Efficient CNN Implementation for Eye-Gaze Estimation on\n  Low-Power/Low-Quality Consumer Imaging Systems Abstract: Accurate and efficient eye gaze estimation is important for emerging consumer\nelectronic systems such as driver monitoring systems and novel user interfaces.\nSuch systems are required to operate reliably in difficult, unconstrained\nenvironments with low power consumption and at minimal cost. In this paper a\nnew hardware friendly, convolutional neural network model with minimal\ncomputational requirements is introduced and assessed for efficient\nappearance-based gaze estimation. The model is tested and compared against\nexisting appearance based CNN approaches, achieving better eye gaze accuracy\nwith significantly fewer computational requirements. A brief updated literature\nreview is also provided. \n\n"}
{"id": "1807.00110", "contents": "Title: Linear and sublinear convergence rates for a subdifferentiable\n  distributed deterministic asynchronous Dykstra's algorithm Abstract: In two earlier papers, we designed a distributed deterministic asynchronous\nalgorithm for minimizing the sum of subdifferentiable and proximable functions\nand a regularizing quadratic on time-varying graphs based on Dykstra's\nalgorithm, or block coordinate dual ascent. Each node in the distributed\noptimization problem is the sum of a known regularizing quadratic and a\nfunction to be minimized. In this paper, we prove sublinear convergence rates\nfor the general algorithm, and a linear rate of convergence if the function on\neach node is smooth with Lipschitz gradient. \n\n"}
{"id": "1807.00652", "contents": "Title: PointSIFT: A SIFT-like Network Module for 3D Point Cloud Semantic\n  Segmentation Abstract: Recently, 3D understanding research sheds light on extracting features from\npoint cloud directly, which requires effective shape pattern description of\npoint clouds. Inspired by the outstanding 2D shape descriptor SIFT, we design a\nmodule called PointSIFT that encodes information of different orientations and\nis adaptive to scale of shape. Specifically, an orientation-encoding unit is\ndesigned to describe eight crucial orientations, and multi-scale representation\nis achieved by stacking several orientation-encoding units. PointSIFT module\ncan be integrated into various PointNet-based architecture to improve the\nrepresentation ability. Extensive experiments show our PointSIFT-based\nframework outperforms state-of-the-art method on standard benchmark datasets.\nThe code and trained model will be published accompanied by this paper. \n\n"}
{"id": "1807.00655", "contents": "Title: On the Tradeoff Between Accuracy and Complexity in Blind Detection of\n  Polar Codes Abstract: Polar codes are a recent family of error-correcting codes with a number of\ndesirable characteristics. Their disruptive nature is illustrated by their\nrapid adoption in the $5^{th}$-generation mobile-communication standard, where\nthey are used to protect control messages. In this work, we describe a\ntwo-stage system tasked with identifying the location of control messages that\nconsists of a detection and selection stage followed by a decoding one. The\nfirst stage spurs the need for polar-code detection algorithms with variable\neffort to balance complexity between the two stages. We illustrate this idea of\nvariable effort for multiple detection algorithms aimed at the first stage. We\npropose three novel blind detection methods based on belief-propagation\ndecoding inspired by early-stopping criteria. Then we show how their\nreliability improves with the number of decoding iterations to highlight the\npossible tradeoffs between accuracy and complexity. Additionally, we show\nsimilar tradeoffs for a detection method from previous work. In a setup where\nonly one block encoded with the polar code of interest is present among many\nother blocks, our results notably show that, depending on the complexity\nbudget, a variable number of undesirable blocks can be dismissed while\nachieving a missed-detection rate in line with the block-error rate of a\ncomplex decoding algorithm. \n\n"}
{"id": "1807.01110", "contents": "Title: Regional enlarged observability of Caputo fractional differential\n  equations Abstract: We consider the regional enlarged observability problem for fractional\nevolution differential equations involving Caputo derivatives. Using the\nHilbert Uniqueness Method, we show that it is possible to rebuild the initial\nstate between two prescribed functions only in an internal subregion of the\nwhole domain. Finally, an example is provided to illustrate the theory. \n\n"}
{"id": "1807.01251", "contents": "Title: Training behavior of deep neural network in frequency domain Abstract: Why deep neural networks (DNNs) capable of overfitting often generalize well\nin practice is a mystery [#zhang2016understanding]. To find a potential\nmechanism, we focus on the study of implicit biases underlying the training\nprocess of DNNs. In this work, for both real and synthetic datasets, we\nempirically find that a DNN with common settings first quickly captures the\ndominant low-frequency components, and then relatively slowly captures the\nhigh-frequency ones. We call this phenomenon Frequency Principle (F-Principle).\nThe F-Principle can be observed over DNNs of various structures, activation\nfunctions, and training algorithms in our experiments. We also illustrate how\nthe F-Principle help understand the effect of early-stopping as well as the\ngeneralization of DNNs. This F-Principle potentially provides insights into a\ngeneral principle underlying DNN optimization and generalization. \n\n"}
{"id": "1807.02371", "contents": "Title: End-to-End Race Driving with Deep Reinforcement Learning Abstract: We present research using the latest reinforcement learning algorithm for\nend-to-end driving without any mediated perception (object recognition, scene\nunderstanding). The newly proposed reward and learning strategies lead together\nto faster convergence and more robust driving using only RGB image from a\nforward facing camera. An Asynchronous Actor Critic (A3C) framework is used to\nlearn the car control in a physically and graphically realistic rally game,\nwith the agents evolving simultaneously on tracks with a variety of road\nstructures (turns, hills), graphics (seasons, location) and physics (road\nadherence). A thorough evaluation is conducted and generalization is proven on\nunseen tracks and using legal speed limits. Open loop tests on real sequences\nof images show some domain adaption capability of our method. \n\n"}
{"id": "1807.02480", "contents": "Title: A Fully Convolutional Two-Stream Fusion Network for Interactive Image\n  Segmentation Abstract: In this paper, we propose a novel fully convolutional two-stream fusion\nnetwork (FCTSFN) for interactive image segmentation. The proposed network\nincludes two sub-networks: a two-stream late fusion network (TSLFN) that\npredicts the foreground at a reduced resolution, and a multi-scale refining\nnetwork (MSRN) that refines the foreground at full resolution. The TSLFN\nincludes two distinct deep streams followed by a fusion network. The intuition\nis that, since user interactions are more direct information on\nforeground/background than the image itself, the two-stream structure of the\nTSLFN reduces the number of layers between the pure user interaction features\nand the network output, allowing the user interactions to have a more direct\nimpact on the segmentation result. The MSRN fuses the features from different\nlayers of TSLFN with different scales, in order to seek the local to global\ninformation on the foreground to refine the segmentation result at full\nresolution. We conduct comprehensive experiments on four benchmark datasets.\nThe results show that the proposed network achieves competitive performance\ncompared to current state-of-the-art interactive image segmentation methods \n\n"}
{"id": "1807.02758", "contents": "Title: Image Super-Resolution Using Very Deep Residual Channel Attention\n  Networks Abstract: Convolutional neural network (CNN) depth is of crucial importance for image\nsuper-resolution (SR). However, we observe that deeper networks for image SR\nare more difficult to train. The low-resolution inputs and features contain\nabundant low-frequency information, which is treated equally across channels,\nhence hindering the representational ability of CNNs. To solve these problems,\nwe propose the very deep residual channel attention networks (RCAN).\nSpecifically, we propose a residual in residual (RIR) structure to form very\ndeep network, which consists of several residual groups with long skip\nconnections. Each residual group contains some residual blocks with short skip\nconnections. Meanwhile, RIR allows abundant low-frequency information to be\nbypassed through multiple skip connections, making the main network focus on\nlearning high-frequency information. Furthermore, we propose a channel\nattention mechanism to adaptively rescale channel-wise features by considering\ninterdependencies among channels. Extensive experiments show that our RCAN\nachieves better accuracy and visual improvements against state-of-the-art\nmethods. \n\n"}
{"id": "1807.02855", "contents": "Title: Semi-parametric Image Inpainting Abstract: This paper introduces a semi-parametric approach to image inpainting for\nirregular holes. The nonparametric part consists of an external image database.\nDuring test time database is used to retrieve a supplementary image, similar to\nthe input masked picture, and utilize it as auxiliary information for the deep\nneural network. Further, we propose a novel method of generating masks with\nirregular holes and present public dataset with such masks. Experiments on\nCelebA-HQ dataset show that our semi-parametric method yields more realistic\nresults than previous approaches, which is confirmed by the user study. \n\n"}
{"id": "1807.02925", "contents": "Title: Vehicle Image Generation Going Well with The Surroundings Abstract: Since the generative neural networks have made a breakthrough in the image\ngeneration problem, lots of researches on their applications have been studied\nsuch as image restoration, style transfer and image completion. However, there\nhas been few research generating objects in uncontrolled real-world\nenvironments. In this paper, we propose a novel approach for vehicle image\ngeneration in real-world scenes. Using a subnetwork based on a precedent work\nof image completion, our model makes the shape of an object. Details of objects\nare trained by an additional colorization and refinement subnetwork, resulting\nin a better quality of generated objects. Unlike many other works, our method\ndoes not require any segmentation layout but still makes a plausible vehicle in\nthe image. We evaluate our method by using images from Berkeley Deep Drive\n(BDD) and Cityscape datasets, which are widely used for object detection and\nimage segmentation problems. The adequacy of the generated images by the\nproposed method has also been evaluated using a widely utilized object\ndetection algorithm and the FID score. \n\n"}
{"id": "1807.03021", "contents": "Title: Verisimilar Image Synthesis for Accurate Detection and Recognition of\n  Texts in Scenes Abstract: The requirement of large amounts of annotated images has become one grand\nchallenge while training deep neural network models for various visual\ndetection and recognition tasks. This paper presents a novel image synthesis\ntechnique that aims to generate a large amount of annotated scene text images\nfor training accurate and robust scene text detection and recognition models.\nThe proposed technique consists of three innovative designs. First, it realizes\n\"semantic coherent\" synthesis by embedding texts at semantically sensible\nregions within the background image, where the semantic coherence is achieved\nby leveraging the semantic annotations of objects and image regions that have\nbeen created in the prior semantic segmentation research. Second, it exploits\nvisual saliency to determine the embedding locations within each semantic\nsensible region, which coincides with the fact that texts are often placed\naround homogeneous regions for better visibility in scenes. Third, it designs\nan adaptive text appearance model that determines the color and brightness of\nembedded texts by learning from the feature of real scene text images\nadaptively. The proposed technique has been evaluated over five public datasets\nand the experiments show its superior performance in training accurate and\nrobust scene text detection and recognition models. \n\n"}
{"id": "1807.03116", "contents": "Title: Deep Global-Connected Net With The Generalized Multi-Piecewise ReLU\n  Activation in Deep Learning Abstract: Recent Progress has shown that exploitation of hidden layer neurons in\nconvolution neural networks incorporating with a carefully designed activation\nfunction can yield better classification results in the field of computer\nvision. The paper firstly introduces a novel deep learning architecture aiming\nto mitigate the gradient-vanishing problem, in which the earlier hidden layer\nneurons could be directly connected with the last hidden layer and feed into\nthe last layer for classification. We then design a generalized linear\nrectifier function as the activation function that can approximate arbitrary\ncomplex functions via training of the parameters. We will show that our design\ncan achieve similar performance in a number of object recognition and video\naction benchmark tasks, under significantly less number of parameters and\nshallower network infrastructure, which is not only promising in training in\nterms of computation burden and memory usage, but is also applicable to\nlow-computation, low-memory mobile scenarios. \n\n"}
{"id": "1807.03343", "contents": "Title: Complex Fully Convolutional Neural Networks for MR Image Reconstruction Abstract: Undersampling the k-space data is widely adopted for acceleration of Magnetic\nResonance Imaging (MRI). Current deep learning based approaches for supervised\nlearning of MRI image reconstruction employ real-valued operations and\nrepresentations by treating complex valued k-space/spatial-space as real\nvalues. In this paper, we propose complex dense fully convolutional neural\nnetwork ($\\mathbb{C}$DFNet) for learning to de-alias the reconstruction\nartifacts within undersampled MRI images. We fashioned a densely-connected\nfully convolutional block tailored for complex-valued inputs by introducing\ndedicated layers such as complex convolution, batch normalization,\nnon-linearities etc. $\\mathbb{C}$DFNet leverages the inherently complex-valued\nnature of input k-space and learns richer representations. We demonstrate\nimproved perceptual quality and recovery of anatomical structures through\n$\\mathbb{C}$DFNet in contrast to its real-valued counterparts. \n\n"}
{"id": "1807.03401", "contents": "Title: High-Resolution Mammogram Synthesis using Progressive Generative\n  Adversarial Networks Abstract: The ability to generate synthetic medical images is useful for data\naugmentation, domain transfer, and out-of-distribution detection. However,\ngenerating realistic, high-resolution medical images is challenging,\nparticularly for Full Field Digital Mammograms (FFDM), due to the textural\nheterogeneity, fine structural details and specific tissue properties. In this\npaper, we explore the use of progressively trained generative adversarial\nnetworks (GANs) to synthesize mammograms, overcoming the underlying\ninstabilities when training such adversarial models. This work is the first to\nshow that generation of realistic synthetic medical images is feasible at up to\n1280x1024 pixels, the highest resolution achieved for medical image synthesis,\nenabling visualizations within standard mammographic hanging protocols. We hope\nthis work can serve as a useful guide and facilitate further research on GANs\nin the medical imaging domain. \n\n"}
{"id": "1807.03528", "contents": "Title: Deep Underwater Image Enhancement Abstract: In an underwater scene, wavelength-dependent light absorption and scattering\ndegrade the visibility of images, causing low contrast and distorted color\ncasts. To address this problem, we propose a convolutional neural network based\nimage enhancement model, i.e., UWCNN, which is trained efficiently using a\nsynthetic underwater image database. Unlike the existing works that require the\nparameters of underwater imaging model estimation or impose inflexible\nframeworks applicable only for specific scenes, our model directly reconstructs\nthe clear latent underwater image by leveraging on an automatic end-to-end and\ndata-driven training mechanism. Compliant with underwater imaging models and\noptical properties of underwater scenes, we first synthesize ten different\nmarine image databases. Then, we separately train multiple UWCNN models for\neach underwater image formation type. Experimental results on real-world and\nsynthetic underwater images demonstrate that the presented method generalizes\nwell on different underwater scenes and outperforms the existing methods both\nqualitatively and quantitatively. Besides, we conduct an ablation study to\ndemonstrate the effect of each component in our network. \n\n"}
{"id": "1807.04199", "contents": "Title: Optimal control problems with oscillations, concentrations and\n  discontinuities Abstract: Optimal control problems with oscillations (chattering controls) and\nconcentrations (impulsive controls) can have integral performance criteria such\nthat concentration of the control signal occurs at a discontinuity of the state\nsignal. Techniques from functional analysis (anisotropic parametrized measures)\nare applied to give a precise meaning of the integral cost and to allow for the\nsound application of numerical methods. We show how this can be combined with\nthe Lasserre hierarchy of semidefinite programming relaxations. \n\n"}
{"id": "1807.04418", "contents": "Title: Subsampled Turbulence Removal Network Abstract: We present a deep-learning approach to restore a sequence of\nturbulence-distorted video frames from turbulent deformations and space-time\nvarying blurs. Instead of requiring a massive training sample size in deep\nnetworks, we purpose a training strategy that is based on a new data\naugmentation method to model turbulence from a relatively small dataset. Then\nwe introduce a subsampled method to enhance the restoration performance of the\npresented GAN model. The contributions of the paper is threefold: first, we\nintroduce a simple but effective data augmentation algorithm to model the\nturbulence in real life for training in the deep network; Second, we firstly\npurpose the Wasserstein GAN combined with $\\ell_1$ cost for successful\nrestoration of turbulence-corrupted video sequence; Third, we combine the\nsubsampling algorithm to filter out strongly corrupted frames to generate a\nvideo sequence with better quality. \n\n"}
{"id": "1807.04428", "contents": "Title: Convergence Rate of Block-Coordinate Maximization Burer-Monteiro Method\n  for Solving Large SDPs Abstract: Semidefinite programming (SDP) with diagonal constraints arise in many\noptimization problems, such as Max-Cut, community detection and group\nsynchronization. Although SDPs can be solved to arbitrary precision in\npolynomial time, generic convex solvers do not scale well with the dimension of\nthe problem. In order to address this issue, Burer and Monteiro proposed to\nreduce the dimension of the problem by appealing to a low-rank factorization\nand solve the subsequent non-convex problem instead. In this paper, we present\ncoordinate ascent based methods to solve this non-convex problem with provable\nconvergence guarantees. More specifically, we prove that the block-coordinate\nmaximization algorithm applied to the non-convex Burer-Monteiro method globally\nconverges to a first-order stationary point with a sublinear rate without any\nassumptions on the problem. We further show that this algorithm converges\nlinearly around a local maximum provided that the objective function exhibits\nquadratic decay. We establish that this condition generically holds when the\nrank of the factorization is sufficiently large. Furthermore, incorporating\nLanczos method to the block-coordinate maximization, we propose an algorithm\nthat is guaranteed to return a solution that provides $1-O(1/r)$ approximation\nto the original SDP without any assumptions, where $r$ is the rank of the\nfactorization. This approximation ratio is known to be optimal (up to\nconstants) under the unique games conjecture, and we can explicitly quantify\nthe number of iterations to obtain such a solution. \n\n"}
{"id": "1807.05284", "contents": "Title: Survey on Deep Learning Techniques for Person Re-Identification Task Abstract: Intelligent video-surveillance is currently an active research field in\ncomputer vision and machine learning techniques. It provides useful tools for\nsurveillance operators and forensic video investigators. Person\nre-identification (PReID) is one among these tools. It consists of recognizing\nwhether an individual has already been observed over a camera in a network or\nnot. This tool can also be employed in various possible applications such as\noff-line retrieval of all the video-sequences showing an individual of interest\nwhose image is given a query, and online pedestrian tracking over multiple\ncamera views. To this aim, many techniques have been proposed to increase the\nperformance of PReID. Among the systems, many researchers utilized deep neural\nnetworks (DNNs) because of their better performance and fast execution at test\ntime. Our objective is to provide for future researchers the work being done on\nPReID to date. Therefore, we summarized state-of-the-art DNN models being used\nfor this task. A brief description of each model along with their evaluation on\na set of benchmark datasets is given. Finally, a detailed comparison is\nprovided among these models followed by some limitations that can work as\nguidelines for future research. \n\n"}
{"id": "1807.05380", "contents": "Title: 3D Hand Pose Estimation using Simulation and Partial-Supervision with a\n  Shared Latent Space Abstract: Tremendous amounts of expensive annotated data are a vital ingredient for\nstate-of-the-art 3d hand pose estimation. Therefore, synthetic data has been\npopularized as annotations are automatically available. However, models trained\nonly with synthetic samples do not generalize to real data, mainly due to the\ngap between the distribution of synthetic and real data. In this paper, we\npropose a novel method that seeks to predict the 3d position of the hand using\nboth synthetic and partially-labeled real data. Accordingly, we form a shared\nlatent space between three modalities: synthetic depth image, real depth image,\nand pose. We demonstrate that by carefully learning the shared latent space, we\ncan find a regression model that is able to generalize to real data. As such,\nwe show that our method produces accurate predictions in both semi-supervised\nand unsupervised settings. Additionally, the proposed model is capable of\ngenerating novel, meaningful, and consistent samples from all of the three\ndomains. We evaluate our method qualitatively and quantitively on two highly\ncompetitive benchmarks (i.e., NYU and ICVL) and demonstrate its superiority\nover the state-of-the-art methods. The source code will be made available at\nhttps://github.com/masabdi/LSPS. \n\n"}
{"id": "1807.05520", "contents": "Title: Deep Clustering for Unsupervised Learning of Visual Features Abstract: Clustering is a class of unsupervised learning methods that has been\nextensively applied and studied in computer vision. Little work has been done\nto adapt it to the end-to-end training of visual features on large scale\ndatasets. In this work, we present DeepCluster, a clustering method that\njointly learns the parameters of a neural network and the cluster assignments\nof the resulting features. DeepCluster iteratively groups the features with a\nstandard clustering algorithm, k-means, and uses the subsequent assignments as\nsupervision to update the weights of the network. We apply DeepCluster to the\nunsupervised training of convolutional neural networks on large datasets like\nImageNet and YFCC100M. The resulting model outperforms the current state of the\nart by a significant margin on all the standard benchmarks. \n\n"}
{"id": "1807.05597", "contents": "Title: Deep Learning for Semantic Segmentation on Minimal Hardware Abstract: Deep learning has revolutionised many fields, but it is still challenging to\ntransfer its success to small mobile robots with minimal hardware.\nSpecifically, some work has been done to this effect in the RoboCup humanoid\nfootball domain, but results that are performant and efficient and still\ngenerally applicable outside of this domain are lacking. We propose an approach\nconceptually different from those taken previously. It is based on semantic\nsegmentation and does achieve these desired properties. In detail, it is being\nable to process full VGA images in real-time on a low-power mobile processor.\nIt can further handle multiple image dimensions without retraining, it does not\nrequire specific domain knowledge for achieving a high frame rate and it is\napplicable on a minimal mobile hardware. \n\n"}
{"id": "1807.06233", "contents": "Title: Robust Deep Multi-modal Learning Based on Gated Information Fusion\n  Network Abstract: The goal of multi-modal learning is to use complimentary information on the\nrelevant task provided by the multiple modalities to achieve reliable and\nrobust performance. Recently, deep learning has led significant improvement in\nmulti-modal learning by allowing for the information fusion in the intermediate\nfeature levels. This paper addresses a problem of designing robust deep\nmulti-modal learning architecture in the presence of imperfect modalities. We\nintroduce deep fusion architecture for object detection which processes each\nmodality using the separate convolutional neural network (CNN) and constructs\nthe joint feature map by combining the intermediate features from the CNNs. In\norder to facilitate the robustness to the degraded modalities, we employ the\ngated information fusion (GIF) network which weights the contribution from each\nmodality according to the input feature maps to be fused. The weights are\ndetermined through the convolutional layers followed by a sigmoid function and\ntrained along with the information fusion network in an end-to-end fashion. Our\nexperiments show that the proposed GIF network offers the additional\narchitectural flexibility to achieve robust performance in handling some\ndegraded modalities, and show a significant performance improvement based on\nSingle Shot Detector (SSD) for KITTI dataset using the proposed fusion network\nand data augmentation schemes. \n\n"}
{"id": "1807.07295", "contents": "Title: Operator-in-the-Loop Deep Sequential Multi-camera Feature Fusion for\n  Person Re-identification Abstract: Given a target image as query, person re-identification systems retrieve a\nranked list of candidate matches on a per-camera basis. In deployed systems, a\nhuman operator scans these lists and labels sighted targets by touch or\nmouse-based selection. However, classical re-id approaches generate per-camera\nlists independently. Therefore, target identifications by operator in a subset\nof cameras cannot be utilized to improve ranking of the target in remaining set\nof network cameras. To address this shortcoming, we propose a novel sequential\nmulti-camera re-id approach. The proposed approach can accommodate human\noperator inputs and provides early gains via a monotonic improvement in target\nranking. At the heart of our approach is a fusion function which operates on\ndeep feature representations of query and candidate matches. We formulate an\noptimization procedure custom-designed to incrementally improve query\nrepresentation. Since existing evaluation methods cannot be directly adopted to\nour setting, we also propose two novel evaluation protocols. The results on two\nlarge-scale re-id datasets (Market-1501, DukeMTMC-reID) demonstrate that our\nmulti-camera method significantly outperforms baselines and other popular\nfeature fusion schemes. Additionally, we conduct a comparative subject-based\nstudy of human operator performance. The superior operator performance enabled\nby our approach makes a compelling case for its integration into deployable\nvideo-surveillance systems. \n\n"}
{"id": "1807.07437", "contents": "Title: Selective Zero-Shot Classification with Augmented Attributes Abstract: In this paper, we introduce a selective zero-shot classification problem: how\ncan the classifier avoid making dubious predictions? Existing attribute-based\nzero-shot classification methods are shown to work poorly in the selective\nclassification scenario. We argue the under-complete human defined attribute\nvocabulary accounts for the poor performance. We propose a selective zero-shot\nclassifier based on both the human defined and the automatically discovered\nresidual attributes. The proposed classifier is constructed by firstly learning\nthe defined and the residual attributes jointly. Then the predictions are\nconducted within the subspace of the defined attributes. Finally, the\nprediction confidence is measured by both the defined and the residual\nattributes. Experiments conducted on several benchmarks demonstrate that our\nclassifier produces a superior performance to other methods under the\nrisk-coverage trade-off metric. \n\n"}
{"id": "1807.07796", "contents": "Title: 3D-LMNet: Latent Embedding Matching for Accurate and Diverse 3D Point\n  Cloud Reconstruction from a Single Image Abstract: 3D reconstruction from single view images is an ill-posed problem. Inferring\nthe hidden regions from self-occluded images is both challenging and ambiguous.\nWe propose a two-pronged approach to address these issues. To better\nincorporate the data prior and generate meaningful reconstructions, we propose\n3D-LMNet, a latent embedding matching approach for 3D reconstruction. We first\ntrain a 3D point cloud auto-encoder and then learn a mapping from the 2D image\nto the corresponding learnt embedding. To tackle the issue of uncertainty in\nthe reconstruction, we predict multiple reconstructions that are consistent\nwith the input view. This is achieved by learning a probablistic latent space\nwith a novel view-specific diversity loss. Thorough quantitative and\nqualitative analysis is performed to highlight the significance of the proposed\napproach. We outperform state-of-the-art approaches on the task of single-view\n3D reconstruction on both real and synthetic datasets while generating multiple\nplausible reconstructions, demonstrating the generalizability and utility of\nour approach. \n\n"}
{"id": "1807.07805", "contents": "Title: Continuous-Time Accelerated Methods via a Hybrid Control Lens Abstract: Treating optimization methods as dynamical systems can be traced back\ncenturies ago in order to comprehend the notions and behaviors of optimization\nmethods. Lately, this mind set has become the driving force to design new\noptimization methods. Inspired by the recent dynamical system viewpoint of\nNesterov's fast method, we propose two classes of fast methods, formulated as\nhybrid control systems, to obtain pre-specified exponential convergence rate.\nAlternative to the existing fast methods which are parametric-in-time second\norder differential equations, we dynamically synthesize feedback controls in a\nstate-dependent manner. Namely, in the first class the damping term is viewed\nas the control input, while in the second class the amplitude with which the\ngradient of the objective function impacts the dynamics serves as the\ncontroller. The objective function requires to satisfy the so-called\nPolyak--{\\L}ojasiewicz inequality which effectively implies no local optima and\na certain gradient-domination property. Moreover, we establish that both hybrid\nstructures possess Zeno-free solution trajectories. We finally provide a\nmechanism to determine the discretization step size to attain an exponential\nconvergence rate. \n\n"}
{"id": "1807.08370", "contents": "Title: SiGAN: Siamese Generative Adversarial Network for Identity-Preserving\n  Face Hallucination Abstract: Despite generative adversarial networks (GANs) can hallucinate\nphoto-realistic high-resolution (HR) faces from low-resolution (LR) faces, they\ncannot guarantee preserving the identities of hallucinated HR faces, making the\nHR faces poorly recognizable. To address this problem, we propose a Siamese GAN\n(SiGAN) to reconstruct HR faces that visually resemble their corresponding\nidentities. On top of a Siamese network, the proposed SiGAN consists of a pair\nof two identical generators and one discriminator. We incorporate\nreconstruction error and identity label information in the loss function of\nSiGAN in a pairwise manner. By iteratively optimizing the loss functions of the\ngenerator pair and discriminator of SiGAN, we cannot only achieve\nphoto-realistic face reconstruction, but also ensures the reconstructed\ninformation is useful for identity recognition. Experimental results\ndemonstrate that SiGAN significantly outperforms existing face hallucination\nGANs in objective face verification performance, while achieving\nphoto-realistic reconstruction. Moreover, for input LR faces from unknown\nidentities who are not included in training, SiGAN can still do a good job. \n\n"}
{"id": "1807.10706", "contents": "Title: Diagnosing Error in Temporal Action Detectors Abstract: Despite the recent progress in video understanding and the continuous rate of\nimprovement in temporal action localization throughout the years, it is still\nunclear how far (or close?) we are to solving the problem. To this end, we\nintroduce a new diagnostic tool to analyze the performance of temporal action\ndetectors in videos and compare different methods beyond a single scalar\nmetric. We exemplify the use of our tool by analyzing the performance of the\ntop rewarded entries in the latest ActivityNet action localization challenge.\nOur analysis shows that the most impactful areas to work on are: strategies to\nbetter handle temporal context around the instances, improving the robustness\nw.r.t. the instance absolute and relative size, and strategies to reduce the\nlocalization errors. Moreover, our experimental analysis finds the lack of\nagreement among annotator is not a major roadblock to attain progress in the\nfield. Our diagnostic tool is publicly available to keep fueling the minds of\nother researchers with additional insights about their algorithms. \n\n"}
{"id": "1807.10712", "contents": "Title: Semi-convolutional Operators for Instance Segmentation Abstract: Object detection and instance segmentation are dominated by region-based\nmethods such as Mask RCNN. However, there is a growing interest in reducing\nthese problems to pixel labeling tasks, as the latter could be more efficient,\ncould be integrated seamlessly in image-to-image network architectures as used\nin many other tasks, and could be more accurate for objects that are not well\napproximated by bounding boxes. In this paper we show theoretically and\nempirically that constructing dense pixel embeddings that can separate object\ninstances cannot be easily achieved using convolutional operators. At the same\ntime, we show that simple modifications, which we call semi-convolutional, have\na much better chance of succeeding at this task. We use the latter to show a\nconnection to Hough voting as well as to a variant of the bilateral kernel that\nis spatially steered by a convolutional network. We demonstrate that these\noperators can also be used to improve approaches such as Mask RCNN,\ndemonstrating better segmentation of complex biological shapes and PASCAL VOC\ncategories than achievable by Mask RCNN alone. \n\n"}
{"id": "1807.10915", "contents": "Title: Unsupervised Adversarial Depth Estimation using Cycled Generative\n  Networks Abstract: While recent deep monocular depth estimation approaches based on supervised\nregression have achieved remarkable performance, costly ground truth\nannotations are required during training. To cope with this issue, in this\npaper we present a novel unsupervised deep learning approach for predicting\ndepth maps and show that the depth estimation task can be effectively tackled\nwithin an adversarial learning framework. Specifically, we propose a deep\ngenerative network that learns to predict the correspondence field i.e. the\ndisparity map between two image views in a calibrated stereo camera setting.\nThe proposed architecture consists of two generative sub-networks jointly\ntrained with adversarial learning for reconstructing the disparity map and\norganized in a cycle such as to provide mutual constraints and supervision to\neach other. Extensive experiments on the publicly available datasets KITTI and\nCityscapes demonstrate the effectiveness of the proposed model and competitive\nresults with state of the art methods. The code and trained model are available\non https://github.com/andrea-pilzer/unsup-stereo-depthGAN. \n\n"}
{"id": "1807.11037", "contents": "Title: Efficient Uncertainty Estimation for Semantic Segmentation in Videos Abstract: Uncertainty estimation in deep learning becomes more important recently. A\ndeep learning model can't be applied in real applications if we don't know\nwhether the model is certain about the decision or not. Some literature\nproposes the Bayesian neural network which can estimate the uncertainty by\nMonte Carlo Dropout (MC dropout). However, MC dropout needs to forward the\nmodel $N$ times which results in $N$ times slower. For real-time applications\nsuch as a self-driving car system, which needs to obtain the prediction and the\nuncertainty as fast as possible, so that MC dropout becomes impractical. In\nthis work, we propose the region-based temporal aggregation (RTA) method which\nleverages the temporal information in videos to simulate the sampling\nprocedure. Our RTA method with Tiramisu backbone is 10x faster than the MC\ndropout with Tiramisu backbone ($N=5$). Furthermore, the uncertainty estimation\nobtained by our RTA method is comparable to MC dropout's uncertainty estimation\non pixel-level and frame-level metrics. \n\n"}
{"id": "1807.11130", "contents": "Title: Geo-Supervised Visual Depth Prediction Abstract: We propose using global orientation from inertial measurements, and the bias\nit induces on the shape of objects populating the scene, to inform visual 3D\nreconstruction. We test the effect of using the resulting prior in depth\nprediction from a single image, where the normal vectors to surfaces of objects\nof certain classes tend to align with gravity or be orthogonal to it. Adding\nsuch a prior to baseline methods for monocular depth prediction yields\nimprovements beyond the state-of-the-art and illustrates the power of gravity\nas a supervisory signal. \n\n"}
{"id": "1807.11190", "contents": "Title: Distributed Stochastic Optimization in Networks with Low Informational\n  Exchange Abstract: We consider a distributed stochastic optimization problem in networks with\nfinite number of nodes. Each node adjusts its action to optimize the global\nutility of the network, which is defined as the sum of local utilities of all\nnodes. Gradient descent method is a common technique to solve the optimization\nproblem, while the computation of the gradient may require much information\nexchange. In this paper, we consider that each node can only have a noisy\nnumerical observation of its local utility, of which the closed-form expression\nis not available. This assumption is quite realistic, especially when the\nsystem is too complicated or constantly changing. Nodes may exchange the\nobservation of their local utilities to estimate the global utility at each\ntimeslot. We propose stochastic perturbation based distributed algorithms under\nthe assumptions whether each node has collected local utilities of all or only\npart of the other nodes. We use tools from stochastic approximation to prove\nthat both algorithms converge to the optimum. The convergence rate of the\nalgorithms is also derived. Although the proposed algorithms can be applied to\ngeneral optimization problems, we perform simulations considering power control\nin wireless networks and present numerical results to corroborate our claim. \n\n"}
{"id": "1807.11195", "contents": "Title: Multi-Fiber Networks for Video Recognition Abstract: In this paper, we aim to reduce the computational cost of spatio-temporal\ndeep neural networks, making them run as fast as their 2D counterparts while\npreserving state-of-the-art accuracy on video recognition benchmarks. To this\nend, we present the novel Multi-Fiber architecture that slices a complex neural\nnetwork into an ensemble of lightweight networks or fibers that run through the\nnetwork. To facilitate information flow between fibers we further incorporate\nmultiplexer modules and end up with an architecture that reduces the\ncomputational cost of 3D networks by an order of magnitude, while increasing\nrecognition performance at the same time. Extensive experimental results show\nthat our multi-fiber architecture significantly boosts the efficiency of\nexisting convolution networks for both image and video recognition tasks,\nachieving state-of-the-art performance on UCF-101, HMDB-51 and Kinetics\ndatasets. Our proposed model requires over 9x and 13x less computations than\nthe I3D and R(2+1)D models, respectively, yet providing higher accuracy. \n\n"}
{"id": "1807.11272", "contents": "Title: Uncertainty Quantification in CNN-Based Surface Prediction Using Shape\n  Priors Abstract: Surface reconstruction is a vital tool in a wide range of areas of medical\nimage analysis and clinical research. Despite the fact that many methods have\nproposed solutions to the reconstruction problem, most, due to their\ndeterministic nature, do not directly address the issue of quantifying\nuncertainty associated with their predictions. We remedy this by proposing a\nnovel probabilistic deep learning approach capable of simultaneous surface\nreconstruction and associated uncertainty prediction. The method incorporates\nprior shape information in the form of a principal component analysis (PCA)\nmodel. Experiments using the UK Biobank data show that our probabilistic\napproach outperforms an analogous deterministic PCA-based method in the task of\n2D organ delineation and quantifies uncertainty by formulating distributions\nover predicted surface vertex positions. \n\n"}
{"id": "1807.11553", "contents": "Title: Reach-Avoid Problems via Sum-of-Squares Optimization and Dynamic\n  Programming Abstract: Reach-avoid problems involve driving a system to a set of desirable\nconfigurations while keeping it away from undesirable ones. Providing\nmathematical guarantees for such scenarios is challenging but have numerous\npotential practical applications. Due to the challenges, analysis of\nreach-avoid problems involves making trade-offs between generality of system\ndynamics, generality of problem setups, optimality of solutions, and\ncomputational complexity. In this paper, we combine sum-of-squares optimization\nand dynamic programming to address the reach-avoid problem, and provide a\nconservative solution that maintains reaching and avoidance guarantees. Our\nmethod is applicable to polynomial system dynamics and to general problem\nsetups, and is more computationally scalable than previous related methods.\nThrough a numerical example involving two single integrators, we validate our\nproposed theory and compare our method to Hamilton-Jacobi reachability. Having\nvalidated our theory, we demonstrate the computational scalability of our\nmethod by computing the reach-avoid set of a system involving two kinematic\ncars. \n\n"}
{"id": "1807.11888", "contents": "Title: Deep End-to-end Fingerprint Denoising and Inpainting Abstract: This work describes our winning solution for the Chalearn LAP In-painting\nCompetition Track 3 - Fingerprint Denoising and In-painting. The objective of\nthis competition is to reduce noise, remove the background pattern and replace\nmissing parts of fingerprint images in order to simplify the verification made\nby humans or third-party software. In this paper, we use a U-Net like CNN model\nthat performs all those steps end-to-end after being trained on the competition\ndata in a fully supervised way. This architecture and training procedure\nachieved the best results on all three metrics of the competition. \n\n"}
{"id": "1807.11939", "contents": "Title: Entanglement cost and quantum channel simulation Abstract: This paper proposes a revised definition for the entanglement cost of a\nquantum channel $\\mathcal{N}$. In particular, it is defined here to be the\nsmallest rate at which entanglement is required, in addition to free classical\ncommunication, in order to simulate $n$ calls to $\\mathcal{N}$, such that the\nmost general discriminator cannot distinguish the $n$ calls to $\\mathcal{N}$\nfrom the simulation. The most general discriminator is one who tests the\nchannels in a sequential manner, one after the other, and this discriminator is\nknown as a quantum tester [Chiribella et al., Phys. Rev. Lett., 101, 060401\n(2008)] or one who is implementing a quantum co-strategy [Gutoski et al., Symp.\nTh. Comp., 565 (2007)]. As such, the proposed revised definition of\nentanglement cost of a quantum channel leads to a rate that cannot be smaller\nthan the previous notion of a channel's entanglement cost [Berta et al., IEEE\nTrans. Inf. Theory, 59, 6779 (2013)], in which the discriminator is limited to\ndistinguishing parallel uses of the channel from the simulation. Under this\nrevised notion, I prove that the entanglement cost of certain\nteleportation-simulable channels is equal to the entanglement cost of their\nunderlying resource states. Then I find single-letter formulas for the\nentanglement cost of some fundamental channel models, including dephasing,\nerasure, three-dimensional Werner--Holevo channels, epolarizing channels\n(complements of depolarizing channels), as well as single-mode pure-loss and\npure-amplifier bosonic Gaussian channels. These examples demonstrate that the\nresource theory of entanglement for quantum channels is not reversible.\nFinally, I discuss how to generalize the basic notions to arbitrary resource\ntheories. \n\n"}
{"id": "1808.00277", "contents": "Title: Non-Orthogonal Multiple Access for 5G and Beyond Abstract: Driven by the rapid escalation of the wireless capacity requirements imposed\nby advanced multimedia applications (e.g., ultra-high-definition video, virtual\nreality etc.), as well as the dramatically increasing demand for user access\nrequired for the Internet of Things (IoT), the fifth generation (5G) networks\nface challenges in terms of supporting large-scale heterogeneous data traffic.\nNon-orthogonal multiple access (NOMA), which has been recently proposed for the\n3rd generation partnership projects long-term evolution advanced (3GPP-LTE-A),\nconstitutes a promising technology of addressing the above-mentioned challenges\nin 5G networks by accommodating several users within the same orthogonal\nresource block. By doing so, significant bandwidth efficiency enhancement can\nbe attained over conventional orthogonal multiple access (OMA) techniques. This\nmotivated numerous researchers to dedicate substantial research contributions\nto this field. In this context, we provide a comprehensive overview of the\nstate-of-the-art in power-domain multiplexing aided NOMA, with a focus on the\ntheoretical NOMA principles, multiple antenna aided NOMA design, on the\ninterplay between NOMA and cooperative transmission, on the resource control of\nNOMA, on the co-existence of NOMA with other emerging potential 5G techniques\nand on the comparison with other NOMA variants. We highlight the main\nadvantages of power-domain multiplexing NOMA compared to other existing NOMA\ntechniques. We summarize the challenges of existing research contributions of\nNOMA and provide potential solutions. Finally, we offer some design guidelines\nfor NOMA systems and identify promising research opportunities for the future. \n\n"}
{"id": "1808.00519", "contents": "Title: Orthogonal Time Frequency Space Modulation Abstract: This paper introduces a new two-dimensional modulation technique called\nOrthogonal Time Frequency Space (OTFS) modulation. OTFS has the novel and\nimportant feature of being designed in the delay-Doppler domain. When coupled\nwith a suitable equalizer, OTFS modulation is able to exploit the full channel\ndiversity over both time and frequency. Moreover, it converts the fading,\ntime-varying wireless channel experienced by modulated signals such as OFDM\ninto a time-independent channel with a complex channel gain that is essentially\nconstant for all symbols.\n  This design obviates the need for transmitter adaptation, and greatly\nsimplifies system operation. The paper describes the basic operating principles\nof OTFS as well as a possible implementation as an overlay to current or\nanticipated standardized systems. OTFS is shown to provide significant\nperformance improvement in systems with high Doppler, short packets, and/or\nlarge antenna array. In particular, simulation results indicate at least\nseveral dB of block error rate performance improvement for OTFS over OFDM in\nall of these settings. \n\n"}
{"id": "1808.00769", "contents": "Title: Sparse and Dense Data with CNNs: Depth Completion and Semantic\n  Segmentation Abstract: Convolutional neural networks are designed for dense data, but vision data is\noften sparse (stereo depth, point clouds, pen stroke, etc.). We present a\nmethod to handle sparse depth data with optional dense RGB, and accomplish\ndepth completion and semantic segmentation changing only the last layer. Our\nproposal efficiently learns sparse features without the need of an additional\nvalidity mask. We show how to ensure network robustness to varying input\nsparsities. Our method even works with densities as low as 0.8% (8 layer\nlidar), and outperforms all published state-of-the-art on the Kitti depth\ncompletion benchmark. \n\n"}
{"id": "1808.01933", "contents": "Title: On the Duality and File Size Hierarchy of Fractional Repetition Codes Abstract: Distributed storage systems that deploy erasure codes can provide better\nfeatures such as lower storage overhead and higher data reliability. In this\npaper, we focus on fractional repetition (FR) codes, which are a class of\nstorage codes characterized by the features of uncoded exact repair and minimum\nrepair bandwidth. We study the duality of FR codes, and investigate the\nrelationship between the supported file size of an FR code and its dual code.\nBased on the established relationship, we derive an improved dual bound on the\nsupported file size of FR codes. We further show that FR codes constructed from\n$t$-designs are optimal when the size of the stored file is sufficiently large.\nMoreover, we present the tensor product technique for combining FR codes, and\nelaborate on the file size hierarchy of resulting codes. \n\n"}
{"id": "1808.02861", "contents": "Title: Choose Your Neuron: Incorporating Domain Knowledge through\n  Neuron-Importance Abstract: Individual neurons in convolutional neural networks supervised for\nimage-level classification tasks have been shown to implicitly learn\nsemantically meaningful concepts ranging from simple textures and shapes to\nwhole or partial objects - forming a \"dictionary\" of concepts acquired through\nthe learning process. In this work we introduce a simple, efficient zero-shot\nlearning approach based on this observation. Our approach, which we call Neuron\nImportance-AwareWeight Transfer (NIWT), learns to map domain knowledge about\nnovel \"unseen\" classes onto this dictionary of learned concepts and then\noptimizes for network parameters that can effectively combine these concepts -\nessentially learning classifiers by discovering and composing learned semantic\nconcepts in deep networks. Our approach shows improvements over previous\napproaches on the CUBirds and AWA2 generalized zero-shot learning benchmarks.\nWe demonstrate our approach on a diverse set of semantic inputs as external\ndomain knowledge including attributes and natural language captions. Moreover\nby learning inverse mappings, NIWT can provide visual and textual explanations\nfor the predictions made by the newly learned classifiers and provide neuron\nnames. Our code is available at\nhttps://github.com/ramprs/neuron-importance-zsl. \n\n"}
{"id": "1808.03203", "contents": "Title: Data Rates for Network Linear Equations Abstract: In this paper, we study network linear equations subject to digital\ncommunications with a finite data rate, where each node is associated with one\nequation from a system of linear equations. Each node holds a dynamic state and\ninteracts with its neighbors through an undirected connected graph, where along\neach link the pair of nodes share information. Due to the data-rate constraint,\neach node builds an encoder-decoder pair, with which it produces transmitted\nmessage with a zooming-in finite-level uniform quantizer and also generates\nestimates of its neighbors' states from the received signals. We then propose a\ndistributed quantized algorithm and show that when the network linear equations\nadmit a unique solution, each node's state is driven to that solution\nexponentially. We further establish the asymptotic rate of convergence, which\nshows that a larger number of quantization levels leads to a faster convergence\nrate but is still fundamentally bounded by the inherent network structure and\nthe linear equations. When a unique least-squares solution exists, we show that\nthe algorithm can compute such a solution with a suitably selected time-varying\nstep size inherited from the encoder and zooming-in quantizer dynamics. In both\ncases, a minimal data rate is shown to be enough for guaranteeing the desired\nconvergence when the step sizes are properly chosen. These results assure the\napplicability of various network linear equation solvers in the literature when\npeer-to-peer communication is digital. \n\n"}
{"id": "1808.04336", "contents": "Title: Vision-Based Preharvest Yield Mapping for Apple Orchards Abstract: We present an end-to-end computer vision system for mapping yield in an apple\norchard using images captured from a single camera. Our proposed system is\nplatform independent and does not require any specific lighting conditions. Our\nmain technical contributions are 1)~a semi-supervised clustering algorithm that\nutilizes colors to identify apples and 2)~an unsupervised clustering method\nthat utilizes spatial properties to estimate fruit counts from apple clusters\nhaving arbitrarily complex geometry. Additionally, we utilize camera motion to\nmerge the counts across multiple views. We verified the performance of our\nalgorithms by conducting multiple field trials on three tree rows consisting of\n$252$ trees at the University of Minnesota Horticultural Research Center.\nResults indicate that the detection method achieves $F_1$-measure $.95 -.97$\nfor multiple color varieties and lighting conditions. The counting method\nachieves an accuracy of $89\\%-98\\%$. Additionally, we report merged fruit\ncounts from both sides of the tree rows. Our yield estimation method achieves\nan overall accuracy of $91.98\\% - 94.81\\%$ across different datasets. \n\n"}
{"id": "1808.04480", "contents": "Title: Fast Convergence for Object Detection by Learning how to Combine Error\n  Functions Abstract: In this paper, we introduce an innovative method to improve the convergence\nspeed and accuracy of object detection neural networks. Our approach,\nCONVERGE-FAST-AUXNET, is based on employing multiple, dependent loss metrics\nand weighting them optimally using an on-line trained auxiliary network.\nExperiments are performed in the well-known RoboCup@Work challenge environment.\nA fully convolutional segmentation network is trained on detecting objects'\npickup points. We empirically obtain an approximate measure for the rate of\nsuccess of a robotic pickup operation based on the accuracy of the object\ndetection network. Our experiments show that adding an optimally weighted\nEuclidean distance loss to a network trained on the commonly used Intersection\nover Union (IoU) metric reduces the convergence time by 42.48%. The estimated\npickup rate is improved by 39.90%. Compared to state-of-the-art task weighting\nmethods, the improvement is 24.5% in convergence, and 15.8% on the estimated\npickup rate. \n\n"}
{"id": "1808.04589", "contents": "Title: DeepNeuro: an open-source deep learning toolbox for neuroimaging Abstract: Translating neural networks from theory to clinical practice has unique\nchallenges, specifically in the field of neuroimaging. In this paper, we\npresent DeepNeuro, a deep learning framework that is best-suited to putting\ndeep learning algorithms for neuroimaging in practical usage with a minimum of\nfriction. We show how this framework can be used to both design and train\nneural network architectures, as well as modify state-of-the-art architectures\nin a flexible and intuitive way. We display the pre- and postprocessing\nfunctions common in the medical imaging community that DeepNeuro offers to\nensure consistent performance of networks across variable users, institutions,\nand scanners. And we show how pipelines created in DeepNeuro can be concisely\npackaged into shareable Docker containers and command-line interfaces using\nDeepNeuro's pipeline resources. \n\n"}
{"id": "1808.04618", "contents": "Title: On Robustness of Massive MIMO Systems Against Passive Eavesdropping\n  under Antenna Selection Abstract: In massive MIMO wiretap settings, the base station can significantly suppress\neavesdroppers by narrow beamforming toward legitimate terminals. Numerical\ninvestigations show that by this approach, secrecy is obtained at no\nsignificant cost. We call this property of massive MIMO systems `secrecy for\nfree' and show that it not only holds when all the transmit antennas at the\nbase station are employed, but also when only a single antenna is set active.\nUsing linear precoding, the information leakage to the eavesdroppers can be\nsufficiently diminished, when the total number of available transmit antennas\nat the base station grows large, even when only a fixed number of them are\nselected. This result indicates that passive eavesdropping has no significant\nimpact on massive MIMO systems, regardless of the number of active transmit\nantennas. \n\n"}
{"id": "1808.04905", "contents": "Title: Multi-Sector and Multi-Panel Performance in 5G mmWave Cellular Networks Abstract: The next generation of cellular networks (5G) will exploit the mmWave\nspectrum to increase the available capacity. Communication at such high\nfrequencies, however, suffers from high path loss and blockage, therefore\ndirectional transmissions using antenna arrays and dense deployments are\nneeded. Thus, when evaluating the performance of mmWave mobile networks, it is\nnecessary to accurately model the complex channel, the directionality of the\ntransmission, but also the interplay that these elements can have with the\nwhole protocol stack, both in the radio access and in the higher layers. In\nthis paper, we improve the channel model abstraction of the mmWave module for\nns-3, by introducing the support of a more realistic antenna array model,\ncompliant with 3GPP NR requirements, and of multiple antenna arrays at the base\nstations and mobile handsets. We then study the end-to-end performance of a\nmmWave cellular network by varying the channel and antenna array\nconfigurations, and show that increasing the number of antenna arrays and,\nconsequently, the number of sectors is beneficial for both throughput and\nlatency. \n\n"}
{"id": "1808.05022", "contents": "Title: A Dense-Depth Representation for VLAD descriptors in Content-Based Image\n  Retrieval Abstract: The recent advances brought by deep learning allowed to improve the\nperformance in image retrieval tasks. Through the many convolutional layers,\navailable in a Convolutional Neural Network (CNN), it is possible to obtain a\nhierarchy of features from the evaluated image. At every step, the patches\nextracted are smaller than the previous levels and more representative.\nFollowing this idea, this paper introduces a new detector applied on the\nfeature maps extracted from pre-trained CNN. Specifically, this approach lets\nto increase the number of features in order to increase the performance of the\naggregation algorithms like the most famous and used VLAD embedding. The\nproposed approach is tested on different public datasets: Holidays, Oxford5k,\nParis6k and UKB. \n\n"}
{"id": "1808.05071", "contents": "Title: Ensemble of Convolutional Neural Networks for Dermoscopic Images\n  Classification Abstract: In this report, we are presenting our automated prediction system for disease\nclassification within dermoscopic images. The proposed solution is based on\ndeep learning, where we employed transfer learning strategy on VGG16 and\nGoogLeNet architectures. The key feature of our solution is preprocessing based\nprimarily on image augmentation and colour normalization. The solution was\nevaluated on Task 3: Lesion Diagnosis of the ISIC 2018: Skin Lesion Analysis\nTowards Melanoma Detection. \n\n"}
{"id": "1808.05671", "contents": "Title: On the Convergence of Adaptive Gradient Methods for Nonconvex\n  Optimization Abstract: Adaptive gradient methods are workhorses in deep learning. However, the\nconvergence guarantees of adaptive gradient methods for nonconvex optimization\nhave not been thoroughly studied. In this paper, we provide a fine-grained\nconvergence analysis for a general class of adaptive gradient methods including\nAMSGrad, RMSProp and AdaGrad. For smooth nonconvex functions, we prove that\nadaptive gradient methods in expectation converge to a first-order stationary\npoint. Our convergence rate is better than existing results for adaptive\ngradient methods in terms of dimension. In addition, we also prove high\nprobability bounds on the convergence rates of AMSGrad, RMSProp as well as\nAdaGrad, which have not been established before. Our analyses shed light on\nbetter understanding the mechanism behind adaptive gradient methods in\noptimizing nonconvex objectives. \n\n"}
{"id": "1808.07935", "contents": "Title: Deconvolutional Networks for Point-Cloud Vehicle Detection and Tracking\n  in Driving Scenarios Abstract: Vehicle detection and tracking is a core ingredient for developing autonomous\ndriving applications in urban scenarios. Recent image-based Deep Learning (DL)\ntechniques are obtaining breakthrough results in these perceptive tasks.\nHowever, DL research has not yet advanced much towards processing 3D point\nclouds from lidar range-finders. These sensors are very common in autonomous\nvehicles since, despite not providing as semantically rich information as\nimages, their performance is more robust under harsh weather conditions than\nvision sensors. In this paper we present a full vehicle detection and tracking\nsystem that works with 3D lidar information only. Our detection step uses a\nConvolutional Neural Network (CNN) that receives as input a featured\nrepresentation of the 3D information provided by a Velodyne HDL-64 sensor and\nreturns a per-point classification of whether it belongs to a vehicle or not.\nThe classified point cloud is then geometrically processed to generate\nobservations for a multi-object tracking system implemented via a number of\nMulti-Hypothesis Extended Kalman Filters (MH-EKF) that estimate the position\nand velocity of the surrounding vehicles. The system is thoroughly evaluated on\nthe KITTI tracking dataset, and we show the performance boost provided by our\nCNN-based vehicle detector over a standard geometric approach. Our lidar-based\napproach uses about a 4% of the data needed for an image-based detector with\nsimilarly competitive results. \n\n"}
{"id": "1808.08024", "contents": "Title: Decision fusion with multiple spatial supports by conditional random\n  fields Abstract: Classification of remotely sensed images into land cover or land use is\nhighly dependent on geographical information at least at two levels. First,\nland cover classes are observed in a spatially smooth domain separated by sharp\nregion boundaries. Second, land classes and observation scale are also tightly\nintertwined: they tend to be consistent within areas of homogeneous appearance,\nor regions, in the sense that all pixels within a roof should be classified as\nroof, independently on the spatial support used for the classification. In this\npaper, we follow these two observations and encode them as priors in an energy\nminimization framework based on conditional random fields (CRFs), where\nclassification results obtained at pixel and region levels are\nprobabilistically fused. The aim is to enforce the final maps to be consistent\nnot only in their own spatial supports (pixel and region) but also across\nsupports, i.e., by getting the predictions on the pixel lattice and on the set\nof regions to agree. To this end, we define an energy function with three\nterms: 1) a data term for the individual elements in each support\n(support-specific nodes); 2) spatial regularization terms in a neighborhood for\neach of the supports (support-specific edges); and 3) a regularization term\nbetween individual pixels and the region containing each of them (intersupports\nedges). We utilize these priors in a unified energy minimization problem that\ncan be optimized by standard solvers. The proposed 2LCRF model consists of a\nCRF defined over a bipartite graph, i.e., two interconnected layers within a\nsingle graph accounting for interlattice connections. \n\n"}
{"id": "1808.08127", "contents": "Title: Recalibrating Fully Convolutional Networks with Spatial and Channel\n  'Squeeze & Excitation' Blocks Abstract: In a wide range of semantic segmentation tasks, fully convolutional neural\nnetworks (F-CNNs) have been successfully leveraged to achieve state-of-the-art\nperformance. Architectural innovations of F-CNNs have mainly been on improving\nspatial encoding or network connectivity to aid gradient flow. In this article,\nwe aim towards an alternate direction of recalibrating the learned feature maps\nadaptively; boosting meaningful features while suppressing weak ones. The\nrecalibration is achieved by simple computational blocks that can be easily\nintegrated in F-CNNs architectures. We draw our inspiration from the recently\nproposed 'squeeze & excitation' (SE) modules for channel recalibration for\nimage classification. Towards this end, we introduce three variants of SE\nmodules for segmentation, (i) squeezing spatially and exciting channel-wise,\n(ii) squeezing channel-wise and exciting spatially and (iii) joint spatial and\nchannel 'squeeze & excitation'. We effectively incorporate the proposed SE\nblocks in three state-of-the-art F-CNNs and demonstrate a consistent\nimprovement of segmentation accuracy on three challenging benchmark datasets.\nImportantly, SE blocks only lead to a minimal increase in model complexity of\nabout 1.5%, while the Dice score increases by 4-9% in the case of U-Net. Hence,\nwe believe that SE blocks can be an integral part of future F-CNN\narchitectures. \n\n"}
{"id": "1808.08308", "contents": "Title: ParaNet - Using Dense Blocks for Early Inference Abstract: DenseNets have been shown to be a competitive model among recent\nconvolutional network architectures. These networks utilize Dense Blocks, which\nare groups of densely connected layers where the output of a hidden layer is\nfed in as the input of every other layer following it. In this paper, we aim to\nimprove certain aspects of DenseNet, especially when it comes to practicality.\nWe introduce ParaNet, a new architecture that constructs three pipelines which\nallow for early inference. We additionally introduce a cascading mechanism such\nthat different pipelines are able to share parameters, as well as logit\nmatching between the outputs of the pipelines. We separately evaluate each of\nthe newly introduced mechanisms of ParaNet, then evaluate our proposed\narchitecture on CIFAR-100. \n\n"}
{"id": "1808.08931", "contents": "Title: Smoothed Dilated Convolutions for Improved Dense Prediction Abstract: Dilated convolutions, also known as atrous convolutions, have been widely\nexplored in deep convolutional neural networks (DCNNs) for various dense\nprediction tasks. However, dilated convolutions suffer from the gridding\nartifacts, which hampers the performance. In this work, we propose two simple\nyet effective degridding methods by studying a decomposition of dilated\nconvolutions. Unlike existing models, which explore solutions by focusing on a\nblock of cascaded dilated convolutional layers, our methods address the\ngridding artifacts by smoothing the dilated convolution itself. In addition, we\npoint out that the two degridding approaches are intrinsically related and\ndefine separable and shared (SS) operations, which generalize the proposed\nmethods. We further explore SS operations in view of operations on graphs and\npropose the SS output layer, which is able to smooth the entire DCNNs by only\nreplacing the output layer. We evaluate our degridding methods and the SS\noutput layer thoroughly, and visualize the smoothing effect through effective\nreceptive field analysis. Results show that our methods degridding yield\nconsistent improvements on the performance of dense prediction tasks, while\nadding negligible amounts of extra training parameters. And the SS output layer\nimproves the performance significantly and is very efficient in terms of number\nof training parameters. \n\n"}
{"id": "1808.09128", "contents": "Title: Multiple Lane Detection Algorithm Based on Optimised Dense Disparity Map\n  Estimation Abstract: Lane detection is very important for self-driving vehicles. In recent years,\ncomputer stereo vision has been prevalently used to enhance the accuracy of the\nlane detection systems. This paper mainly presents a multiple lane detection\nalgorithm developed based on optimised dense disparity map estimation, where\nthe disparity information obtained at time t_{n} is utilised to optimise the\nprocess of disparity estimation at time t_{n+1}. This is achieved by estimating\nthe road model at time t_{n} and then controlling the search range for the\ndisparity estimation at time t_{n+1}. The lanes are then detected using our\npreviously published algorithm, where the vanishing point information is used\nto model the lanes. The experimental results illustrate that the runtime of the\ndisparity estimation is reduced by around 37% and the accuracy of the lane\ndetection is about 99%. \n\n"}
{"id": "1808.09769", "contents": "Title: Tensor Alignment Based Domain Adaptation for Hyperspectral Image\n  Classification Abstract: This paper presents a tensor alignment (TA) based domain adaptation method\nfor hyperspectral image (HSI) classification. To be specific, HSIs in both\ndomains are first segmented into superpixels and tensors of both domains are\nconstructed to include neighboring samples from single superpixel. Then we\nconsider the subspace invariance between two domains as projection matrices and\noriginal tensors are projected as core tensors with lower dimensions into the\ninvariant tensor subspace by applying Tucker decomposition. To preserve\ngeometric information in original tensors, we employ a manifold regularization\nterm for core tensors into the decomposition progress. The projection matrices\nand core tensors are solved in an alternating optimization manner and the\nconvergence of TA algorithm is analyzed. In addition, a post-processing\nstrategy is defined via pure samples extraction for each superpixel to further\nimprove classification performance. Experimental results on four real HSIs\ndemonstrate that the proposed method can achieve better performance compared\nwith the state-of-the-art subspace learning methods when a limited amount of\nsource labeled samples are available. \n\n"}
{"id": "1808.09892", "contents": "Title: Top-down Attention Recurrent VLAD Encoding for Action Recognition in\n  Videos Abstract: Most recent approaches for action recognition from video leverage deep\narchitectures to encode the video clip into a fixed length representation\nvector that is then used for classification. For this to be successful, the\nnetwork must be capable of suppressing irrelevant scene background and extract\nthe representation from the most discriminative part of the video. Our\ncontribution builds on the observation that spatio-temporal patterns\ncharacterizing actions in videos are highly correlated with objects and their\nlocation in the video. We propose Top-down Attention Action VLAD (TA-VLAD), a\ndeep recurrent architecture with built-in spatial attention that performs\ntemporally aggregated VLAD encoding for action recognition from videos. We\nadopt a top-down approach of attention, by using class specific activation maps\nobtained from a deep CNN pre-trained for image classification, to weight\nappearance features before encoding them into a fixed-length video descriptor\nusing Gated Recurrent Units. Our method achieves state of the art recognition\naccuracy on HMDB51 and UCF101 benchmarks. \n\n"}
{"id": "1808.10032", "contents": "Title: The Impact of Preprocessing on Deep Representations for Iris Recognition\n  on Unconstrained Environments Abstract: The use of iris as a biometric trait is widely used because of its high level\nof distinction and uniqueness. Nowadays, one of the major research challenges\nrelies on the recognition of iris images obtained in visible spectrum under\nunconstrained environments. In this scenario, the acquired iris are affected by\ncapture distance, rotation, blur, motion blur, low contrast and specular\nreflection, creating noises that disturb the iris recognition systems. Besides\ndelineating the iris region, usually preprocessing techniques such as\nnormalization and segmentation of noisy iris images are employed to minimize\nthese problems. But these techniques inevitably run into some errors. In this\ncontext, we propose the use of deep representations, more specifically,\narchitectures based on VGG and ResNet-50 networks, for dealing with the images\nusing (and not) iris segmentation and normalization. We use transfer learning\nfrom the face domain and also propose a specific data augmentation technique\nfor iris images. Our results show that the approach using non-normalized and\nonly circle-delimited iris images reaches a new state of the art in the\nofficial protocol of the NICE.II competition, a subset of the UBIRIS database,\none of the most challenging databases on unconstrained environments, reporting\nan average Equal Error Rate (EER) of 13.98% which represents an absolute\nreduction of about 5%. \n\n"}
{"id": "1808.10710", "contents": "Title: MobiBits: Multimodal Mobile Biometric Database Abstract: This paper presents a novel database comprising representations of five\ndifferent biometric characteristics, collected in a mobile, unconstrained or\nsemi-constrained setting with three different mobile devices, including\ncharacteristics previously unavailable in existing datasets, namely hand\nimages, thermal hand images, and thermal face images, all acquired with a\nmobile, off-the-shelf device. In addition to this collection of data we perform\nan extensive set of experiments providing insight on benchmark recognition\nperformance that can be achieved with these data, carried out with existing\ncommercial and academic biometric solutions. This is the first known to us\nmobile biometric database introducing samples of biometric traits such as\nthermal hand images and thermal face images. We hope that this contribution\nwill make a valuable addition to the already existing databases and enable new\nexperiments and studies in the field of mobile authentication. The MobiBits\ndatabase is made publicly available to the research community at no cost for\nnon-commercial purposes. \n\n"}
{"id": "1809.00076", "contents": "Title: 3D Segmentation with Exponential Logarithmic Loss for Highly Unbalanced\n  Object Sizes Abstract: With the introduction of fully convolutional neural networks, deep learning\nhas raised the benchmark for medical image segmentation on both speed and\naccuracy, and different networks have been proposed for 2D and 3D segmentation\nwith promising results. Nevertheless, most networks only handle relatively\nsmall numbers of labels (<10), and there are very limited works on handling\nhighly unbalanced object sizes especially in 3D segmentation. In this paper, we\npropose a network architecture and the corresponding loss function which\nimprove segmentation of very small structures. By combining skip connections\nand deep supervision with respect to the computational feasibility of 3D\nsegmentation, we propose a fast converging and computationally efficient\nnetwork architecture for accurate segmentation. Furthermore, inspired by the\nconcept of focal loss, we propose an exponential logarithmic loss which\nbalances the labels not only by their relative sizes but also by their\nsegmentation difficulties. We achieve an average Dice coefficient of 82% on\nbrain segmentation with 20 labels, with the ratio of the smallest to largest\nobject sizes as 0.14%. Less than 100 epochs are required to reach such\naccuracy, and segmenting a 128x128x128 volume only takes around 0.4 s. \n\n"}
{"id": "1809.00241", "contents": "Title: Activity Recognition on a Large Scale in Short Videos - Moments in Time\n  Dataset Abstract: Moments capture a huge part of our lives. Accurate recognition of these\nmoments is challenging due to the diverse and complex interpretation of the\nmoments. Action recognition refers to the act of classifying the desired\naction/activity present in a given video. In this work, we perform experiments\non Moments in Time dataset to recognize accurately activities occurring in 3\nsecond clips. We use state of the art techniques for visual, auditory and\nspatio temporal localization and develop method to accurately classify the\nactivity in the Moments in Time dataset. Our novel approach of using Visual\nBased Textual features and fusion techniques performs well providing an overall\n89.23 % Top - 5 accuracy on the 20 classes - a significant improvement over the\nBaseline TRN model. \n\n"}
{"id": "1809.01124", "contents": "Title: Straight to the Facts: Learning Knowledge Base Retrieval for Factual\n  Visual Question Answering Abstract: Question answering is an important task for autonomous agents and virtual\nassistants alike and was shown to support the disabled in efficiently\nnavigating an overwhelming environment. Many existing methods focus on\nobservation-based questions, ignoring our ability to seamlessly combine\nobserved content with general knowledge. To understand interactions with a\nknowledge base, a dataset has been introduced recently and keyword matching\ntechniques were shown to yield compelling results despite being vulnerable to\nmisconceptions due to synonyms and homographs. To address this issue, we\ndevelop a learning-based approach which goes straight to the facts via a\nlearned embedding space. We demonstrate state-of-the-art results on the\nchallenging recently introduced fact-based visual question answering dataset,\noutperforming competing methods by more than 5%. \n\n"}
{"id": "1809.02940", "contents": "Title: Automated Strabismus Detection for Telemedicine Applications Abstract: Strabismus is one of the most influential ophthalmologic diseases in human's\nlife. Timely detection of strabismus contributes to its prognosis and\ntreatment. Telemedicine, which has great potential to alleviate the growing\ndemand of the diagnosis of ophthalmologic diseases, is an effective method to\nachieve timely strabismus detection. In this paper, a tele strabismus dataset\nis established by the ophthalmologists. Then an end-to-end framework named as\nRF-CNN is proposed to achieve automated strabismus detection on the established\ntele strabismus dataset. RF-CNN first performs eye region segmentation on each\nindividual image, and further classifies the segmented eye regions with deep\nneural networks. The experimental results on the established tele strabismus\ndataset demonstrates that the proposed RF-CNN can have a good performance on\nautomated strabismus detection for telemedicine application. Code is made\npublicly available at:\nhttps://github.com/jieWeiLu/Strabismus-Detection-for-Telemedicine-Application. \n\n"}
{"id": "1809.03314", "contents": "Title: A Robotic Auto-Focus System based on Deep Reinforcement Learning Abstract: Considering its advantages in dealing with high-dimensional visual input and\nlearning control policies in discrete domain, Deep Q Network (DQN) could be an\nalternative method of traditional auto-focus means in the future. In this\npaper, based on Deep Reinforcement Learning, we propose an end-to-end approach\nthat can learn auto-focus policies from visual input and finish at a clear spot\nautomatically. We demonstrate that our method - discretizing the action space\nwith coarse to fine steps and applying DQN is not only a solution to auto-focus\nbut also a general approach towards vision-based control problems. Separate\nphases of training in virtual and real environments are applied to obtain an\neffective model. Virtual experiments, which are carried out after the virtual\ntraining phase, indicates that our method could achieve 100% accuracy on a\ncertain view with different focus range. Further training on real robots could\neliminate the deviation between the simulator and real scenario, leading to\nreliable performances in real applications. \n\n"}
{"id": "1809.03676", "contents": "Title: Unbiasing Semantic Segmentation For Robot Perception using Synthetic\n  Data Feature Transfer Abstract: Robot perception systems need to perform reliable image segmentation in\nreal-time on noisy, raw perception data. State-of-the-art segmentation\napproaches use large CNN models and carefully constructed datasets; however,\nthese models focus on accuracy at the cost of real-time inference. Furthermore,\nthe standard semantic segmentation datasets are not large enough for training\nCNNs without augmentation and are not representative of noisy, uncurated robot\nperception data. We propose improving the performance of real-time segmentation\nframeworks on robot perception data by transferring features learned from\nsynthetic segmentation data. We show that pretraining real-time segmentation\narchitectures with synthetic segmentation data instead of ImageNet improves\nfine-tuning performance by reducing the bias learned in pretraining and closing\nthe \\textit{transfer gap} as a result. Our experiments show that our real-time\nrobot perception models pretrained on synthetic data outperform those\npretrained on ImageNet for every scale of fine-tuning data examined. Moreover,\nthe degree to which synthetic pretraining outperforms ImageNet pretraining\nincreases as the availability of robot data decreases, making our approach\nattractive for robotics domains where dataset collection is hard and/or\nexpensive. \n\n"}
{"id": "1809.05408", "contents": "Title: Socially Aware Kalman Neural Networks for Trajectory Prediction Abstract: Trajectory prediction is a critical technique in the navigation of robots and\nautonomous vehicles. However, the complex traffic and dynamic uncertainties\nyield challenges in the effectiveness and robustness in modeling. We purpose a\ndata-driven approach socially aware Kalman neural networks (SAKNN) where the\ninteraction layer and the Kalman layer are embedded in the architecture,\nresulting in a class of architectures with huge potential to directly learn\nfrom high variance sensor input and robustly generate low variance outcomes.\nThe evaluation of our approach on NGSIM dataset demonstrates that SAKNN\nperforms state-of-the-art on prediction effectiveness in a relatively long-term\nhorizon and significantly improves the signal-to-noise ratio of the predicted\nsignal. \n\n"}
{"id": "1809.05611", "contents": "Title: A study on the use of Boundary Equilibrium GAN for Approximate\n  Frontalization of Unconstrained Faces to aid in Surveillance Abstract: Face frontalization is the process of synthesizing frontal facing views of\nfaces given its angled poses. We implement a generative adversarial network\n(GAN) with spherical linear interpolation (Slerp) for frontalization of\nunconstrained facial images. Our special focus is intended towards the\ngeneration of approximate frontal faces of the side posed images captured from\nsurveillance cameras. Specifically, the present work is a comprehensive study\non the implementation of an auto-encoder based Boundary Equilibrium GAN (BEGAN)\nto generate frontal faces using an interpolation of a side view face and its\nmirrored view. To increase the quality of the interpolated output we implement\na BEGAN with Slerp. This approach could produce a promising output along with a\nfaster and more stable training for the model. The BEGAN model additionally has\na balanced generator-discriminator combination, which prevents mode collapse\nalong with a global convergence measure. It is expected that such an\napproximate face generation model would be able to replace face composites used\nin surveillance and crime detection. \n\n"}
{"id": "1809.05895", "contents": "Title: Primal-dual accelerated gradient methods with small-dimensional\n  relaxation oracle Abstract: In this paper, a new variant of accelerated gradient descent is proposed. The\npro-posed method does not require any information about the objective function,\nusesexact line search for the practical accelerations of convergence, converges\naccordingto the well-known lower bounds for both convex and non-convex\nobjective functions,possesses primal-dual properties and can be applied in the\nnon-euclidian set-up. Asfar as we know this is the rst such method possessing\nall of the above properties atthe same time. We also present a universal\nversion of the method which is applicableto non-smooth problems. We demonstrate\nhow in practice one can efficiently use thecombination of line-search and\nprimal-duality by considering a convex optimizationproblem with a simple\nstructure (for example, linearly constrained). \n\n"}
{"id": "1809.05966", "contents": "Title: Exploring the Vulnerability of Single Shot Module in Object Detectors\n  via Imperceptible Background Patches Abstract: Recent works succeeded to generate adversarial perturbations on the entire\nimage or the object of interests to corrupt CNN based object detectors. In this\npaper, we focus on exploring the vulnerability of the Single Shot Module (SSM)\ncommonly used in recent object detectors, by adding small perturbations to\npatches in the background outside the object. The SSM is referred to the Region\nProposal Network used in a two-stage object detector or the single-stage object\ndetector itself. The SSM is typically a fully convolutional neural network\nwhich generates output in a single forward pass. Due to the excessive\nconvolutions used in SSM, the actual receptive field is larger than the object\nitself. As such, we propose a novel method to corrupt object detectors by\ngenerating imperceptible patches only in the background. Our method can find a\nfew background patches for perturbation, which can effectively decrease true\npositives and dramatically increase false positives. Efficacy is demonstrated\non 5 two-stage object detectors and 8 single-stage object detectors on the MS\nCOCO 2014 dataset. Results indicate that perturbations with small distortions\noutside the bounding box of object region can still severely damage the\ndetection performance. \n\n"}
{"id": "1809.06201", "contents": "Title: Player Experience Extraction from Gameplay Video Abstract: The ability to extract the sequence of game events for a given player's\nplay-through has traditionally required access to the game's engine or source\ncode. This serves as a barrier to researchers, developers, and hobbyists who\nmight otherwise benefit from these game logs. In this paper we present two\napproaches to derive game logs from game video via convolutional neural\nnetworks and transfer learning. We evaluate the approaches in a Super Mario\nBros. clone, Mega Man and Skyrim. Our results demonstrate our approach\noutperforms random forest and other transfer baselines. \n\n"}
{"id": "1809.06227", "contents": "Title: Improving Reinforcement Learning Based Image Captioning with Natural\n  Language Prior Abstract: Recently, Reinforcement Learning (RL) approaches have demonstrated advanced\nperformance in image captioning by directly optimizing the metric used for\ntesting. However, this shaped reward introduces learning biases, which reduces\nthe readability of generated text. In addition, the large sample space makes\ntraining unstable and slow. To alleviate these issues, we propose a simple\ncoherent solution that constrains the action space using an n-gram language\nprior. Quantitative and qualitative evaluations on benchmarks show that RL with\nthe simple add-on module performs favorably against its counterpart in terms of\nboth readability and speed of convergence. Human evaluation results show that\nour model is more human readable and graceful. The implementation will become\npublicly available upon the acceptance of the paper. \n\n"}
{"id": "1809.07196", "contents": "Title: Characterising Across-Stack Optimisations for Deep Convolutional Neural\n  Networks Abstract: Convolutional Neural Networks (CNNs) are extremely computationally demanding,\npresenting a large barrier to their deployment on resource-constrained devices.\nSince such systems are where some of their most useful applications lie (e.g.\nobstacle detection for mobile robots, vision-based medical assistive\ntechnology), significant bodies of work from both machine learning and systems\ncommunities have attempted to provide optimisations that will make CNNs\navailable to edge devices. In this paper we unify the two viewpoints in a Deep\nLearning Inference Stack and take an across-stack approach by implementing and\nevaluating the most common neural network compression techniques (weight\npruning, channel pruning, and quantisation) and optimising their parallel\nexecution with a range of programming approaches (OpenMP, OpenCL) and hardware\narchitectures (CPU, GPU). We provide comprehensive Pareto curves to instruct\ntrade-offs under constraints of accuracy, execution time, and memory space. \n\n"}
{"id": "1809.07586", "contents": "Title: A Fast and Accurate System for Face Detection, Identification, and\n  Verification Abstract: The availability of large annotated datasets and affordable computation power\nhave led to impressive improvements in the performance of CNNs on various\nobject detection and recognition benchmarks. These, along with a better\nunderstanding of deep learning methods, have also led to improved capabilities\nof machine understanding of faces. CNNs are able to detect faces, locate facial\nlandmarks, estimate pose, and recognize faces in unconstrained images and\nvideos. In this paper, we describe the details of a deep learning pipeline for\nunconstrained face identification and verification which achieves\nstate-of-the-art performance on several benchmark datasets. We propose a novel\nface detector, Deep Pyramid Single Shot Face Detector (DPSSD), which is fast\nand capable of detecting faces with large scale variations (especially tiny\nfaces). We give design details of the various modules involved in automatic\nface recognition: face detection, landmark localization and alignment, and face\nidentification/verification. We provide evaluation results of the proposed face\ndetector on challenging unconstrained face detection datasets. Then, we present\nexperimental results for IARPA Janus Benchmarks A, B and C (IJB-A, IJB-B,\nIJB-C), and the Janus Challenge Set 5 (CS5). \n\n"}
{"id": "1809.07677", "contents": "Title: Real Time Dense Depth Estimation by Fusing Stereo with Sparse Depth\n  Measurements Abstract: We present an approach to depth estimation that fuses information from a\nstereo pair with sparse range measurements derived from a LIDAR sensor or a\nrange camera. The goal of this work is to exploit the complementary strengths\nof the two sensor modalities, the accurate but sparse range measurements and\nthe ambiguous but dense stereo information. These two sources are effectively\nand efficiently fused by combining ideas from anisotropic diffusion and\nsemi-global matching.\n  We evaluate our approach on the KITTI 2015 and Middlebury 2014 datasets,\nusing randomly sampled ground truth range measurements as our sparse depth\ninput. We achieve significant performance improvements with a small fraction of\nrange measurements on both datasets. We also provide qualitative results from\nour platform using the PMDTec Monstar sensor. Our entire pipeline runs on an\nNVIDIA TX-2 platform at 5Hz on 1280x1024 stereo images with 128 disparity\nlevels. \n\n"}
{"id": "1809.07921", "contents": "Title: Adversarial 3D Human Pose Estimation via Multimodal Depth Supervision Abstract: In this paper, a novel deep-learning based framework is proposed to infer 3D\nhuman poses from a single image. Specifically, a two-phase approach is\ndeveloped. We firstly utilize a generator with two branches for the extraction\nof explicit and implicit depth information respectively. During the training\nprocess, an adversarial scheme is also employed to further improve the\nperformance. The implicit and explicit depth information with the estimated 2D\njoints generated by a widely used estimator, in the second step, are together\nfed into a deep 3D pose regressor for the final pose generation. Our method\nachieves MPJPE of 58.68mm on the ECCV2018 3D Human Pose Estimation Challenge. \n\n"}
{"id": "1809.08365", "contents": "Title: A Unified Framework for the Tractable Analysis of Multi-Antenna Wireless\n  Networks Abstract: Densifying networks and deploying more antennas at each access point are two\nprincipal ways to boost the capacity of wireless networks. However, the\ncomplicated distributions of the signal power and the accumulated interference\npower, largely induced by various space-time processing techniques, make it\nhighly challenging to quantitatively characterize the performance of\nmulti-antenna networks. In this paper, using tools from stochastic geometry, a\nunified framework is developed for the analysis of such networks. The major\nresults are two innovative representations of the coverage probability, which\nmake the analysis of multi-antenna networks almost as tractable as the\nsingle-antenna case. One is expressed as an $\\ell_1$-induced norm of a Toeplitz\nmatrix, and the other is given in a finite sum form. With a compact\nrepresentation, the former incorporates many existing analytical results on\nsingle- and multi-antenna networks as special cases, and leads to tractable\nexpressions for evaluating the coverage probability in both ad hoc and cellular\nnetworks. While the latter is more complicated for numerical evaluation, it\nhelps analytically gain key design insights. In particular, it helps prove that\nthe coverage probability of ad hoc networks is a monotonically decreasing\nconvex function of the transmitter density and that there exists a peak value\nof the coverage improvement when increasing the number of transmit antennas. On\nthe other hand, in multi-antenna cellular networks, it is shown that the\ncoverage probability is independent of the transmitter density and that the\noutage probability decreases exponentially as the number of transmit antennas\nincreases. \n\n"}
{"id": "1809.08625", "contents": "Title: Unsupervised Learning of Dense Optical Flow, Depth and Egomotion from\n  Sparse Event Data Abstract: In this work we present a lightweight, unsupervised learning pipeline for\n\\textit{dense} depth, optical flow and egomotion estimation from sparse event\noutput of the Dynamic Vision Sensor (DVS). To tackle this low level vision\ntask, we use a novel encoder-decoder neural network architecture - ECN.\n  Our work is the first monocular pipeline that generates dense depth and\noptical flow from sparse event data only. The network works in self-supervised\nmode and has just 150k parameters. We evaluate our pipeline on the MVSEC self\ndriving dataset and present results for depth, optical flow and and egomotion\nestimation. Due to the lightweight design, the inference part of the network\nruns at 250 FPS on a single GPU, making the pipeline ready for realtime\nrobotics applications. Our experiments demonstrate significant improvements\nupon previous works that used deep learning on event data, as well as the\nability of our pipeline to perform well during both day and night. \n\n"}
{"id": "1809.08720", "contents": "Title: Synchronization of Kuramoto Oscillators: Inverse Taylor Expansions Abstract: Synchronization in networks of coupled oscillators is a widely studied topic\nwith extensive scientific and engineering applications. In this paper, we study\nthe frequency synchronization problem for networks of Kuramoto oscillators with\narbitrary topology and heterogeneous edge weights. We propose a novel\nequivalent transcription for the equilibrium synchronization equation. Using\nthis transcription, we develop a power series expansion to compute the\nsynchronized solution of the Kuramoto model as well as a sufficient condition\nfor the strong convergence of this series expansion. Truncating the power\nseries provides (i) an efficient approximation scheme for computing the\nsynchronized solution, and (ii) a simple-to-check, statistically-correct\nhierarchy of increasingly accurate synchronization tests. This hierarchy of\ntests provides a theoretical foundation for and generalizes the best-known\napproximate synchronization test in the literature. Our numerical experiments\nillustrate the accuracy and the computational efficiency of the truncated\nseries approximation compared to existing iterative methods and existing\nsynchronization tests. \n\n"}
{"id": "1809.09237", "contents": "Title: Nonconvex Robust Low-rank Matrix Recovery Abstract: In this paper we study the problem of recovering a low-rank matrix from a\nnumber of random linear measurements that are corrupted by outliers taking\narbitrary values. We consider a nonsmooth nonconvex formulation of the problem,\nin which we explicitly enforce the low-rank property of the solution by using a\nfactored representation of the matrix variable and employ an $\\ell_1$-loss\nfunction to robustify the solution against outliers. We show that even when a\nconstant fraction (which can be up to almost half) of the measurements are\narbitrarily corrupted, as long as certain measurement operators arising from\nthe measurement model satisfy the so-called $\\ell_1/\\ell_2$-restricted isometry\nproperty, the ground-truth matrix can be exactly recovered from any global\nminimum of the resulting optimization problem. Furthermore, we show that the\nobjective function of the optimization problem is sharp and weakly convex.\nConsequently, a subgradient Method (SubGM) with geometrically diminishing step\nsizes will converge linearly to the ground-truth matrix when suitably\ninitialized. We demonstrate the efficacy of the SubGM for the nonconvex robust\nlow-rank matrix recovery problem with various numerical experiments. \n\n"}
{"id": "1809.09758", "contents": "Title: Confidence Inference for Focused Learning in Stereo Matching Abstract: In this paper, we present confidence inference approachin an unsupervised way\nin stereo matching. Deep Neu-ral Networks (DNNs) have recently been achieving\nstate-of-the-art performance. However, it is often hard to tellwhether the\ntrained model was making sensible predictionsor just guessing at random. To\naddress this problem, westart from a probabilistic interpretation of theL1loss\nusedin stereo matching, which inherently assumes an indepen-dent and identical\n(aka i.i.d.) Laplacian distribution. Weshow that with the newly introduced\ndense confidence map,the identical assumption is relaxed. Intuitively, the\nvari-ance in the Laplacian distribution is large for low confidentpixels while\nsmall for high-confidence pixels. In practice,the network learns\ntoattenuatelow-confidence pixels (e.g.,noisy input, occlusions, featureless\nregions) andfocusonhigh-confidence pixels. Moreover, it can be observed\nfromexperiments that the focused learning is very helpful in find-ing a better\nconvergence state of the trained model, reduc-ing over-fitting on a given\ndataset. \n\n"}
{"id": "1809.10280", "contents": "Title: Unsupervised Person Image Synthesis in Arbitrary Poses Abstract: We present a novel approach for synthesizing photo-realistic images of people\nin arbitrary poses using generative adversarial learning. Given an input image\nof a person and a desired pose represented by a 2D skeleton, our model renders\nthe image of the same person under the new pose, synthesizing novel views of\nthe parts visible in the input image and hallucinating those that are not seen.\nThis problem has recently been addressed in a supervised manner, i.e., during\ntraining the ground truth images under the new poses are given to the network.\nWe go beyond these approaches by proposing a fully unsupervised strategy. We\ntackle this challenging scenario by splitting the problem into two principal\nsubtasks. First, we consider a pose conditioned bidirectional generator that\nmaps back the initially rendered image to the original pose, hence being\ndirectly comparable to the input image without the need to resort to any\ntraining image. Second, we devise a novel loss function that incorporates\ncontent and style terms, and aims at producing images of high perceptual\nquality. Extensive experiments conducted on the DeepFashion dataset demonstrate\nthat the images rendered by our model are very close in appearance to those\nobtained by fully supervised approaches. \n\n"}
{"id": "1810.00434", "contents": "Title: CaTDet: Cascaded Tracked Detector for Efficient Object Detection from\n  Video Abstract: Detecting objects in a video is a compute-intensive task. In this paper we\npropose CaTDet, a system to speedup object detection by leveraging the temporal\ncorrelation in video. CaTDet consists of two DNN models that form a cascaded\ndetector, and an additional tracker to predict regions of interests based on\nhistoric detections. We also propose a new metric, mean Delay(mD), which is\ndesigned for latency-critical video applications. Experiments on the KITTI\ndataset show that CaTDet reduces operation count by 5.1-8.7x with the same mean\nAverage Precision(mAP) as the single-model Faster R-CNN detector and incurs\nadditional delay of 0.3 frame. On CityPersons dataset, CaTDet achieves 13.0x\nreduction in operations with 0.8% mAP loss. \n\n"}
{"id": "1810.00609", "contents": "Title: One-Click Annotation with Guided Hierarchical Object Detection Abstract: The increase in data collection has made data annotation an interesting and\nvaluable task in the contemporary world. This paper presents a new methodology\nfor quickly annotating data using click-supervision and hierarchical object\ndetection. The proposed work is semi-automatic in nature where the task of\nannotations is split between the human and a neural network. We show that our\nimproved method of annotation reduces the time, cost and mental stress on a\nhuman annotator. The research also highlights how our method performs better\nthan the current approach in different circumstances such as variation in\nnumber of objects, object size and different datasets. Our approach also\nproposes a new method of using object detectors making it suitable for data\nannotation task. The experiment conducted on PASCAL VOC dataset revealed that\nannotation created from our approach achieves a mAP of 0.995 and a recall of\n0.903. The Our Approach has shown an overall improvement by 8.5%, 18.6% in mean\naverage precision and recall score for KITTI and 69.6%, 36% for CITYSCAPES\ndataset. The proposed framework is 3-4 times faster as compared to the standard\nannotation method. \n\n"}
{"id": "1810.00746", "contents": "Title: Bayesian Prediction of Future Street Scenes using Synthetic Likelihoods Abstract: For autonomous agents to successfully operate in the real world, the ability\nto anticipate future scene states is a key competence. In real-world scenarios,\nfuture states become increasingly uncertain and multi-modal, particularly on\nlong time horizons. Dropout based Bayesian inference provides a computationally\ntractable, theoretically well grounded approach to learn likely\nhypotheses/models to deal with uncertain futures and make predictions that\ncorrespond well to observations -- are well calibrated. However, it turns out\nthat such approaches fall short to capture complex real-world scenes, even\nfalling behind in accuracy when compared to the plain deterministic approaches.\nThis is because the used log-likelihood estimate discourages diversity. In this\nwork, we propose a novel Bayesian formulation for anticipating future scene\nstates which leverages synthetic likelihoods that encourage the learning of\ndiverse models to accurately capture the multi-modal nature of future scene\nstates. We show that our approach achieves accurate state-of-the-art\npredictions and calibrated probabilities through extensive experiments for\nscene anticipation on Cityscapes dataset. Moreover, we show that our approach\ngeneralizes across diverse tasks such as digit generation and precipitation\nforecasting. \n\n"}
{"id": "1810.00774", "contents": "Title: Geometric Constellation Shaping for Fiber Optic Communication Systems\n  via End-to-end Learning Abstract: In this paper, an unsupervised machine learning method for geometric\nconstellation shaping is investigated. By embedding a differentiable fiber\nchannel model within two neural networks, the learning algorithm is optimizing\nfor a geometric constellation shape. The learned constellations yield improved\nperformance to state-of-the-art geometrically shaped constellations, and\ninclude an implicit trade-off between amplification noise and nonlinear\neffects. Further, the method allows joint optimization of system parameters,\nsuch as the optimal launch power, simultaneously with the constellation shape.\nAn experimental demonstration validates the findings. Improved performances are\nreported, up to 0.13 bit/4D in simulation and experimentally up to 0.12 bit/4D. \n\n"}
{"id": "1810.00953", "contents": "Title: Improved robustness to adversarial examples using Lipschitz\n  regularization of the loss Abstract: We augment adversarial training (AT) with worst case adversarial training\n(WCAT) which improves adversarial robustness by 11% over the current\nstate-of-the-art result in the $\\ell_2$ norm on CIFAR-10. We obtain verifiable\naverage case and worst case robustness guarantees, based on the expected and\nmaximum values of the norm of the gradient of the loss. We interpret\nadversarial training as Total Variation Regularization, which is a fundamental\ntool in mathematical image processing, and WCAT as Lipschitz regularization. \n\n"}
{"id": "1810.01069", "contents": "Title: Cloud Chaser: Real Time Deep Learning Computer Vision on Low Computing\n  Power Devices Abstract: Internet of Things(IoT) devices, mobile phones, and robotic systems are often\ndenied the power of deep learning algorithms due to their limited computing\npower. However, to provide time-critical services such as emergency response,\nhome assistance, surveillance, etc, these devices often need real-time analysis\nof their camera data. This paper strives to offer a viable approach to\nintegrate high-performance deep learning-based computer vision algorithms with\nlow-resource and low-power devices by leveraging the computing power of the\ncloud. By offloading the computation work to the cloud, no dedicated hardware\nis needed to enable deep neural networks on existing low computing power\ndevices. A Raspberry Pi based robot, Cloud Chaser, is built to demonstrate the\npower of using cloud computing to perform real-time vision tasks. Furthermore,\nto reduce latency and improve real-time performance, compression algorithms are\nproposed and evaluated for streaming real-time video frames to the cloud. \n\n"}
{"id": "1810.01118", "contents": "Title: Sinkhorn AutoEncoders Abstract: Optimal transport offers an alternative to maximum likelihood for learning\ngenerative autoencoding models. We show that minimizing the p-Wasserstein\ndistance between the generator and the true data distribution is equivalent to\nthe unconstrained min-min optimization of the p-Wasserstein distance between\nthe encoder aggregated posterior and the prior in latent space, plus a\nreconstruction error. We also identify the role of its trade-off hyperparameter\nas the capacity of the generator: its Lipschitz constant. Moreover, we prove\nthat optimizing the encoder over any class of universal approximators, such as\ndeterministic neural networks, is enough to come arbitrarily close to the\noptimum. We therefore advertise this framework, which holds for any metric\nspace and prior, as a sweet-spot of current generative autoencoding objectives.\nWe then introduce the Sinkhorn auto-encoder (SAE), which approximates and\nminimizes the p-Wasserstein distance in latent space via backprogation through\nthe Sinkhorn algorithm. SAE directly works on samples, i.e. it models the\naggregated posterior as an implicit distribution, with no need for a\nreparameterization trick for gradients estimations. SAE is thus able to work\nwith different metric spaces and priors with minimal adaptations. We\ndemonstrate the flexibility of SAE on latent spaces with different geometries\nand priors and compare with other methods on benchmark data sets. \n\n"}
{"id": "1810.01733", "contents": "Title: A deep learning pipeline for product recognition on store shelves Abstract: Recognition of grocery products in store shelves poses peculiar challenges.\nFirstly, the task mandates the recognition of an extremely high number of\ndifferent items, in the order of several thousands for medium-small shops, with\nmany of them featuring small inter and intra class variability. Then, available\nproduct databases usually include just one or a few studio-quality images per\nproduct (referred to herein as reference images), whilst at test time\nrecognition is performed on pictures displaying a portion of a shelf containing\nseveral products and taken in the store by cheap cameras (referred to as query\nimages). Moreover, as the items on sale in a store as well as their appearance\nchange frequently over time, a practical recognition system should handle\nseamlessly new products/packages. Inspired by recent advances in object\ndetection and image retrieval, we propose to leverage on state of the art\nobject detectors based on deep learning to obtain an initial productagnostic\nitem detection. Then, we pursue product recognition through a similarity search\nbetween global descriptors computed on reference and cropped query images. To\nmaximize performance, we learn an ad-hoc global descriptor by a CNN trained on\nreference images based on an image embedding loss. Our system is\ncomputationally expensive at training time but can perform recognition rapidly\nand accurately at test time. \n\n"}
{"id": "1810.02068", "contents": "Title: Towards Fast and Energy-Efficient Binarized Neural Network Inference on\n  FPGA Abstract: Binarized Neural Network (BNN) removes bitwidth redundancy in classical CNN\nby using a single bit (-1/+1) for network parameters and intermediate\nrepresentations, which has greatly reduced the off-chip data transfer and\nstorage overhead. However, a large amount of computation redundancy still\nexists in BNN inference. By analyzing local properties of images and the\nlearned BNN kernel weights, we observe an average of $\\sim$78% input similarity\nand $\\sim$59% weight similarity among weight kernels, measured by our proposed\nmetric in common network architectures. Thus there does exist redundancy that\ncan be exploited to further reduce the amount of on-chip computations.\n  Motivated by the observation, in this paper, we proposed two types of fast\nand energy-efficient architectures for BNN inference. We also provide analysis\nand insights to pick the better strategy of these two for different datasets\nand network models. By reusing the results from previous computation, much\ncycles for data buffer access and computations can be skipped. By experiments,\nwe demonstrate that 80% of the computation and 40% of the buffer access can be\nskipped by exploiting BNN similarity. Thus, our design can achieve 17%\nreduction in total power consumption, 54% reduction in on-chip power\nconsumption and 2.4$\\times$ maximum speedup, compared to the baseline without\napplying our reuse technique. Our design also shows 1.9$\\times$ more\narea-efficiency compared to state-of-the-art BNN inference design. We believe\nour deployment of BNN on FPGA leads to a promising future of running deep\nlearning models on mobile devices. \n\n"}
{"id": "1810.03040", "contents": "Title: Tight-and-cheap conic relaxation for the optimal reactive power dispatch\n  problem Abstract: The optimal reactive power dispatch (ORPD) problem is an alternating current\noptimal power flow (ACOPF) problem where discrete control devices for\nregulating the reactive power, such as shunt elements and tap changers, are\nconsidered. The ORPD problem is modelled as a mixed-integer nonlinear\noptimization problem and its complexity is increased compared to the ACOPF\nproblem, which is highly nonconvex and generally hard to solve. Recently,\nconvex relaxations of the ACOPF problem have attracted a significant interest\nsince they can lead to global optimality. We propose a tight conic relaxation\nof the ORPD problem and show that a round-off technique applied with this\nrelaxation leads to near-global optimal solutions with very small guaranteed\noptimality gaps, unlike with the nonconvex continuous relaxation. We report\ncomputational results on selected MATPOWER test cases with up to 3375 buses. \n\n"}
{"id": "1810.03275", "contents": "Title: TV-regularized CT Reconstruction and Metal Artifact Reduction Using\n  Inequality Constraints with Preconditioning Abstract: Total variation(TV) regularization is applied to X-Ray computed\ntomography(CT) in an effort to reduce metal artifacts. Tikhonov regularization\nwith $L^2$ data fidelity term and total variation regularization is augmented\nin this novel model by inequality constraints on sinogram data affected by\nmetal to model errors caused by metal. The formulated problem is discretized\nand solved using the Chambolle-Pock algorithm. Faster convergence is achieved\nusing preconditioning in a Douglas-Rachford spitting method as well as Advanced\nDirection Method of Multipliers(ADMM). The methods are applied to real and\nsynthetic data demonstrating feasibility of the model to reduce metal\nartifacts. Technical details of CT data used and its processing are given in\nthe appendix. \n\n"}
{"id": "1810.03649", "contents": "Title: Overcoming Language Priors in Visual Question Answering with Adversarial\n  Regularization Abstract: Modern Visual Question Answering (VQA) models have been shown to rely heavily\non superficial correlations between question and answer words learned during\ntraining such as overwhelmingly reporting the type of room as kitchen or the\nsport being played as tennis, irrespective of the image. Most alarmingly, this\nshortcoming is often not well reflected during evaluation because the same\nstrong priors exist in test distributions; however, a VQA system that fails to\nground questions in image content would likely perform poorly in real-world\nsettings. In this work, we present a novel regularization scheme for VQA that\nreduces this effect. We introduce a question-only model that takes as input the\nquestion encoding from the VQA model and must leverage language biases in order\nto succeed. We then pose training as an adversarial game between the VQA model\nand this question-only adversary -- discouraging the VQA model from capturing\nlanguage biases in its question encoding. Further,we leverage this\nquestion-only model to estimate the increase in model confidence after\nconsidering the image, which we maximize explicitly to encourage visual\ngrounding. Our approach is a model agnostic training procedure and simple to\nimplement. We show empirically that it can improve performance significantly on\na bias-sensitive split of the VQA dataset for multiple base models -- achieving\nstate-of-the-art on this task. Further, on standard VQA tasks, our approach\nshows significantly less drop in accuracy compared to existing bias-reducing\nVQA models. \n\n"}
{"id": "1810.04158", "contents": "Title: Seeing Beyond Appearance - Mapping Real Images into Geometrical Domains\n  for Unsupervised CAD-based Recognition Abstract: While convolutional neural networks are dominating the field of computer\nvision, one usually does not have access to the large amount of domain-relevant\ndata needed for their training. It thus became common to use available\nsynthetic samples along domain adaptation schemes to prepare algorithms for the\ntarget domain. Tackling this problem from a different angle, we introduce a\npipeline to map unseen target samples into the synthetic domain used to train\ntask-specific methods. Denoising the data and retaining only the features these\nrecognition algorithms are familiar with, our solution greatly improves their\nperformance. As this mapping is easier to learn than the opposite one (ie to\nlearn to generate realistic features to augment the source samples), we\ndemonstrate how our whole solution can be trained purely on augmented synthetic\ndata, and still perform better than methods trained with domain-relevant\ninformation (eg real images or realistic textures for the 3D models). Applying\nour approach to object recognition from texture-less CAD data, we present a\ncustom generative network which fully utilizes the purely geometrical\ninformation to learn robust features and achieve a more refined mapping for\nunseen color images. \n\n"}
{"id": "1810.04250", "contents": "Title: Bird Species Classification using Transfer Learning with Multistage\n  Training Abstract: Bird species classification has received more and more attention in the field\nof computer vision, for its promising applications in biology and environmental\nstudies. Recognizing bird species is difficult due to the challenges of\ndiscriminative region localization and fine-grained feature learning. In this\npaper, we have introduced a Transfer learning based method with multistage\ntraining. We have used both Pre-Trained Mask-RCNN and an ensemble model\nconsisting of Inception Nets (InceptionV3 & InceptionResNetV2 ) to get\nlocalization and species of the bird from the images respectively. Our final\nmodel achieves an F1 score of 0.5567 or 55.67 % on the dataset provided in CVIP\n2018 Challenge. \n\n"}
{"id": "1810.05162", "contents": "Title: Characterizing Adversarial Examples Based on Spatial Consistency\n  Information for Semantic Segmentation Abstract: Deep Neural Networks (DNNs) have been widely applied in various recognition\ntasks. However, recently DNNs have been shown to be vulnerable against\nadversarial examples, which can mislead DNNs to make arbitrary incorrect\npredictions. While adversarial examples are well studied in classification\ntasks, other learning problems may have different properties. For instance,\nsemantic segmentation requires additional components such as dilated\nconvolutions and multiscale processing. In this paper, we aim to characterize\nadversarial examples based on spatial context information in semantic\nsegmentation. We observe that spatial consistency information can be\npotentially leveraged to detect adversarial examples robustly even when a\nstrong adaptive attacker has access to the model and detection strategies. We\nalso show that adversarial examples based on attacks considered within the\npaper barely transfer among models, even though transferability is common in\nclassification. Our observations shed new light on developing adversarial\nattacks and defenses to better understand the vulnerabilities of DNNs. \n\n"}
{"id": "1810.05874", "contents": "Title: Embedded deep learning in ophthalmology: Making ophthalmic imaging\n  smarter Abstract: Deep learning has recently gained high interest in ophthalmology, due to its\nability to detect clinically significant features for diagnosis and prognosis.\nDespite these significant advances, little is known about the ability of\nvarious deep learning systems to be embedded within ophthalmic imaging devices,\nallowing automated image acquisition. In this work, we will review the existing\nand future directions for \"active acquisition\" embedded deep learning, leading\nto as high quality images with little intervention by the human operator. In\nclinical practice, the improved image quality should translate into more robust\ndeep learning-based clinical diagnostics. Embedded deep learning will be\nenabled by the constantly improving hardware performance with low cost. We will\nbriefly review possible computation methods in larger clinical systems.\nBriefly, they can be included in a three-layer framework composed of edge, fog\nand cloud layers, the former being performed at a device-level. Improved edge\nlayer performance via \"active acquisition\" serves as an automatic data curation\noperator translating to better quality data in electronic health records\n(EHRs), as well as on the cloud layer, for improved deep learning-based\nclinical data mining. \n\n"}
{"id": "1810.06415", "contents": "Title: Virtualization of tissue staining in digital pathology using an\n  unsupervised deep learning approach Abstract: Histopathological evaluation of tissue samples is a key practice in patient\ndiagnosis and drug development, especially in oncology. Historically,\nHematoxylin and Eosin (H&E) has been used by pathologists as a gold standard\nstaining. However, in many cases, various target specific stains, including\nimmunohistochemistry (IHC), are needed in order to highlight specific\nstructures in the tissue. As tissue is scarce and staining procedures are\ntedious, it would be beneficial to generate images of stained tissue virtually.\nVirtual staining could also generate in-silico multiplexing of different stains\non the same tissue segment. In this paper, we present a sample application that\ngenerates FAP-CK virtual IHC images from Ki67-CD8 real IHC images using an\nunsupervised deep learning approach based on CycleGAN. We also propose a method\nto deal with tiling artifacts caused by normalization layers and we validate\nour approach by comparing the results of tissue analysis algorithms for virtual\nand real images. \n\n"}
{"id": "1810.06990", "contents": "Title: LRW-1000: A Naturally-Distributed Large-Scale Benchmark for Lip Reading\n  in the Wild Abstract: Large-scale datasets have successively proven their fundamental importance in\nseveral research fields, especially for early progress in some emerging topics.\nIn this paper, we focus on the problem of visual speech recognition, also known\nas lipreading, which has received increasing interest in recent years. We\npresent a naturally-distributed large-scale benchmark for lip reading in the\nwild, named LRW-1000, which contains 1,000 classes with 718,018 samples from\nmore than 2,000 individual speakers. Each class corresponds to the syllables of\na Mandarin word composed of one or several Chinese characters. To the best of\nour knowledge, it is currently the largest word-level lipreading dataset and\nalso the only public large-scale Mandarin lipreading dataset. This dataset aims\nat covering a \"natural\" variability over different speech modes and imaging\nconditions to incorporate challenges encountered in practical applications. It\nhas shown a large variation in this benchmark in several aspects, including the\nnumber of samples in each class, video resolution, lighting conditions, and\nspeakers' attributes such as pose, age, gender, and make-up. Besides providing\na detailed description of the dataset and its collection pipeline, we evaluate\nseveral typical popular lipreading methods and perform a thorough analysis of\nthe results from several aspects. The results demonstrate the consistency and\nchallenges of our dataset, which may open up some new promising directions for\nfuture work. \n\n"}
{"id": "1810.07121", "contents": "Title: Multiple Interactions Made Easy (MIME): Large Scale Demonstrations Data\n  for Imitation Abstract: In recent years, we have seen an emergence of data-driven approaches in\nrobotics. However, most existing efforts and datasets are either in simulation\nor focus on a single task in isolation such as grasping, pushing or poking. In\norder to make progress and capture the space of manipulation, we would need to\ncollect a large-scale dataset of diverse tasks such as pouring, opening\nbottles, stacking objects etc. But how does one collect such a dataset? In this\npaper, we present the largest available robotic-demonstration dataset (MIME)\nthat contains 8260 human-robot demonstrations over 20 different robotic tasks\n(https://sites.google.com/view/mimedataset). These tasks range from the simple\ntask of pushing objects to the difficult task of stacking household objects.\nOur dataset consists of videos of human demonstrations and kinesthetic\ntrajectories of robot demonstrations. We also propose to use this dataset for\nthe task of mapping 3rd person video features to robot trajectories.\nFurthermore, we present two different approaches using this dataset and\nevaluate the predicted robot trajectories against ground-truth trajectories. We\nhope our dataset inspires research in multiple areas including visual\nimitation, trajectory prediction, and multi-task robotic learning. \n\n"}
{"id": "1810.08352", "contents": "Title: Super-pixel cloud detection using Hierarchical Fusion CNN Abstract: Cloud detection plays a very important role in the process of remote sensing\nimages. This paper designs a super-pixel level cloud detection method based on\nconvolutional neural network (CNN) and deep forest. Firstly, remote sensing\nimages are segmented into super-pixels through the combination of SLIC and\nSEEDS. Structured forests is carried out to compute edge probability of each\npixel, based on which super-pixels are segmented more precisely. Segmented\nsuper-pixels compose a super-pixel level remote sensing database. Though cloud\ndetection is essentially a binary classification problem, our database is\nlabeled into four categories: thick cloud, cirrus cloud, building and other\nculture, to improve the generalization ability of our proposed models.\nSecondly, super-pixel level database is used to train our cloud detection\nmodels based on CNN and deep forest. Considering super-pixel level remote\nsensing images contain less semantic information compared with general object\nclassification database, we propose a Hierarchical Fusion CNN (HFCNN). It takes\nfull advantage of low-level features like color and texture information and is\nmore applicable to cloud detection task. In test phase, every super-pixel in\nremote sensing images is classified by our proposed models and then combined to\nrecover final binary mask by our proposed distance metric, which is used to\ndetermine ambiguous super-pixels. Experimental results show that, compared with\nconventional methods, HFCNN can achieve better precision and recall. \n\n"}
{"id": "1810.08771", "contents": "Title: Image Inpainting via Generative Multi-column Convolutional Neural\n  Networks Abstract: In this paper, we propose a generative multi-column network for image\ninpainting. This network synthesizes different image components in a parallel\nmanner within one stage. To better characterize global structures, we design a\nconfidence-driven reconstruction loss while an implicit diversified MRF\nregularization is adopted to enhance local details. The multi-column network\ncombined with the reconstruction and MRF loss propagates local and global\ninformation derived from context to the target inpainting regions. Extensive\nexperiments on challenging street view, face, natural objects and scenes\nmanifest that our method produces visual compelling results even without\npreviously common post-processing. \n\n"}
{"id": "1810.09012", "contents": "Title: C2A: Crowd Consensus Analytics for Virtual Colonoscopy Abstract: We present a medical crowdsourcing visual analytics platform called C{$^2$}A\nto visualize, classify and filter crowdsourced clinical data. More\nspecifically, C$^2$A is used to build consensus on a clinical diagnosis by\nvisualizing crowd responses and filtering out anomalous activity. Crowdsourcing\nmedical applications have recently shown promise where the non-expert users\n(the crowd) were able to achieve accuracy similar to the medical experts. This\nhas the potential to reduce interpretation/reading time and possibly improve\naccuracy by building a consensus on the findings beforehand and letting the\nmedical experts make the final diagnosis. In this paper, we focus on a virtual\ncolonoscopy (VC) application with the clinical technicians as our target users,\nand the radiologists acting as consultants and classifying segments as benign\nor malignant. In particular, C$^2$A is used to analyze and explore crowd\nresponses on video segments, created from fly-throughs in the virtual colon.\nC$^2$A provides several interactive visualization components to build crowd\nconsensus on video segments, to detect anomalies in the crowd data and in the\nVC video segments, and finally, to improve the non-expert user's work quality\nand performance by A/B testing for the optimal crowdsourcing platform and\napplication-specific parameters. Case studies and domain experts feedback\ndemonstrate the effectiveness of our framework in improving workers' output\nquality, the potential to reduce the radiologists' interpretation time, and\nhence, the potential to improve the traditional clinical workflow by marking\nthe majority of the video segments as benign based on the crowd consensus. \n\n"}
{"id": "1810.09660", "contents": "Title: Large scale visual place recognition with sub-linear storage growth Abstract: Robotic and animal mapping systems share many of the same objectives and\nchallenges, but differ in one key aspect: where much of the research in robotic\nmapping has focused on solving the data association problem, the grid cell\nneurons underlying maps in the mammalian brain appear to intentionally break\ndata association by encoding many locations with a single grid cell neuron. One\npotential benefit of this intentional aliasing is both sub-linear map storage\nand computational requirements growth with environment size, which we\ndemonstrated in a previous proof-of-concept study that detected and encoded\nmutually complementary co-prime pattern frequencies in the visual map data. In\nthis research, we solve several of the key theoretical and practical\nlimitations of that prototype model and achieve significantly better sub-linear\nstorage growth, a factor reduction in storage requirements per map location,\nscalability to large datasets on standard compute equipment and improved\nrobustness to environments with visually challenging appearance change. These\nimprovements are achieved through several innovations including a flexible\nuser-driven choice mechanism for the periodic patterns underlying the new\nencoding method, a parallelized chunking technique that splits the map into\nsub-sections processed in parallel and a novel feature selection approach that\nselects only the image information most relevant to the encoded temporal\npatterns. We evaluate our techniques on two large benchmark datasets with the\ncomparison to the previous state-of-the-art system, as well as providing a\ndetailed analysis of system performance with respect to parameters such as\nrequired precision performance and the number of cyclic patterns encoded. \n\n"}
{"id": "1810.09821", "contents": "Title: Self-Erasing Network for Integral Object Attention Abstract: Recently, adversarial erasing for weakly-supervised object attention has been\ndeeply studied due to its capability in localizing integral object regions.\nHowever, such a strategy raises one key problem that attention regions will\ngradually expand to non-object regions as training iterations continue, which\nsignificantly decreases the quality of the produced attention maps. To tackle\nsuch an issue as well as promote the quality of object attention, we introduce\na simple yet effective Self-Erasing Network (SeeNet) to prohibit attentions\nfrom spreading to unexpected background regions. In particular, SeeNet\nleverages two self-erasing strategies to encourage networks to use reliable\nobject and background cues for learning to attention. In this way, integral\nobject regions can be effectively highlighted without including much more\nbackground regions. To test the quality of the generated attention maps, we\nemploy the mined object regions as heuristic cues for learning semantic\nsegmentation models. Experiments on Pascal VOC well demonstrate the superiority\nof our SeeNet over other state-of-the-art methods. \n\n"}
{"id": "1810.10948", "contents": "Title: Training Generative Adversarial Networks Via Turing Test Abstract: In this article, we introduce a new mode for training Generative Adversarial\nNetworks (GANs). Rather than minimizing the distance of evidence distribution\n$\\tilde{p}(x)$ and the generative distribution $q(x)$, we minimize the distance\nof $\\tilde{p}(x_r)q(x_f)$ and $\\tilde{p}(x_f)q(x_r)$. This adversarial pattern\ncan be interpreted as a Turing test in GANs. It allows us to use information of\nreal samples during training generator and accelerates the whole training\nprocedure. We even find that just proportionally increasing the size of\ndiscriminator and generator, it succeeds on 256x256 resolution without\nadjusting hyperparameters carefully. \n\n"}
{"id": "1810.11229", "contents": "Title: Null-controllability and control cost estimates for the heat equation on\n  unbounded and large bounded domains Abstract: We survey recent results on the control problem for the heat equation on\nunbounded and large bounded domains. First we formulate new uncertainty\nrelations, respectively spectral inequalities. Then we present an abstract\ncontrol cost estimate which improves upon earlier results. It is particularly\ninteresting when combined with the earlier mentioned spectral inequalities\nsince it yields sharp control cost bounds in several asymptotic regimes. We\nalso show that control problems on unbounded domains can be approximated by\ncorresponding problems on a sequence of bounded domains forming an exhaustion.\nOur results apply also for the generalized heat equation associated with a\nSchr\\\"odinger semigroup. \n\n"}
{"id": "1810.11731", "contents": "Title: Real-time Action Recognition with Dissimilarity-based Training of\n  Specialized Module Networks Abstract: This paper addresses the problem of real-time action recognition in trimmed\nvideos, for which deep neural networks have defined the state-of-the-art\nperformance in the recent literature. For attaining higher recognition\naccuracies with efficient computations, researchers have addressed the various\naspects of limitations in the recognition pipeline. This includes network\narchitecture, the number of input streams (where additional streams augment the\ncolor information), the cost function to be optimized, in addition to others.\nThe literature has always aimed, though, at assigning the adopted network (or\nnetworks, in case of multiple streams) the task of recognizing the whole number\nof action classes in the dataset at hand. We propose to train multiple\nspecialized module networks instead. Each module is trained to recognize a\nsubset of the action classes. Towards this goal, we present a\ndissimilarity-based optimized procedure for distributing the action classes\nover the modules, which can be trained simultaneously offline. On two standard\ndatasets--UCF-101 and HMDB-51--the proposed method demonstrates a comparable\nperformance, that is superior in some aspects, to the state-of-the-art, and\nthat satisfies the real-time constraint. We achieved 72.5\\% accuracy on the\nchallenging HMDB-51 dataset. By assigning fewer and unalike classes to each\nmodule network, this research paves the way to benefit from light-weight\narchitectures without compromising recognition accuracy. \n\n"}
{"id": "1810.11789", "contents": "Title: A Tube-based MPC Scheme for Interaction Control of Underwater Vehicle\n  Manipulator Systems Abstract: Over the last years, the development of Autonomous Underwater Vehicles (AUV)\nwith attached robotic manipulators, the so-called Underwater Vehicle\nManipulator System (UVMS), has gained significant research attention, due to\nthe ability of interaction with underwater environments. In such applications,\nforce/torque controllers which guarantee that the end-effector of the UVMS\napplies desired forces/torques towards the environment, should be designed in a\nway that state and input constraints are taken into consideration. Furthermore,\ndue to their complicated structure, unmodeled dynamics as well as external\ndisturbances may arise. Motivated by this, we proposed a robust Model Predicted\nControl Methodology (NMPC) methodology which can handle the aforementioned\nconstraints in an efficient way and it guarantees that the end-effector is\nexerting the desired forces/torques towards the environment. Simulation results\nverify the validity of the proposed framework. \n\n"}
{"id": "1810.11819", "contents": "Title: Object Tracking in Hyperspectral Videos with Convolutional Features and\n  Kernelized Correlation Filter Abstract: Target tracking in hyperspectral videos is a new research topic. In this\npaper, a novel method based on convolutional network and Kernelized Correlation\nFilter (KCF) framework is presented for tracking objects of interest in\nhyperspectral videos. We extract a set of normalized three-dimensional cubes\nfrom the target region as fixed convolution filters which contain spectral\ninformation surrounding a target. The feature maps generated by convolutional\noperations are combined to form a three-dimensional representation of an\nobject, thereby providing effective encoding of local spectral-spatial\ninformation. We show that a simple two-layer convolutional networks is\nsufficient to learn robust representations without the need of offline training\nwith a large dataset. In the tracking step, KCF is adopted to distinguish\ntargets from neighboring environment. Experimental results demonstrate that the\nproposed method performs well on sample hyperspectral videos, and outperforms\nseveral state-of-the-art methods tested on grayscale and color videos in the\nsame scene. \n\n"}
{"id": "1810.12440", "contents": "Title: TallyQA: Answering Complex Counting Questions Abstract: Most counting questions in visual question answering (VQA) datasets are\nsimple and require no more than object detection. Here, we study algorithms for\ncomplex counting questions that involve relationships between objects,\nattribute identification, reasoning, and more. To do this, we created TallyQA,\nthe world's largest dataset for open-ended counting. We propose a new algorithm\nfor counting that uses relation networks with region proposals. Our method lets\nrelation networks be efficiently used with high-resolution imagery. It yields\nstate-of-the-art results compared to baseline and recent systems on both\nTallyQA and the HowMany-QA benchmark. \n\n"}
{"id": "1810.12941", "contents": "Title: Joint detection and matching of feature points in multimodal images Abstract: In this work, we propose a novel Convolutional Neural Network (CNN)\narchitecture for the joint detection and matching of feature points in images\nacquired by different sensors using a single forward pass. The resulting\nfeature detector is tightly coupled with the feature descriptor, in contrast to\nclassical approaches (SIFT, etc.), where the detection phase precedes and\ndiffers from computing the descriptor. Our approach utilizes two CNN\nsubnetworks, the first being a Siamese CNN and the second, consisting of dual\nnon-weight-sharing CNNs. This allows simultaneous processing and fusion of the\njoint and disjoint cues in the multimodal image patches. The proposed approach\nis experimentally shown to outperform contemporary state-of-the-art schemes\nwhen applied to multiple datasets of multimodal images. It is also shown to\nprovide repeatable feature points detections across multisensor images,\noutperforming state-of-the-art detectors. To the best of our knowledge, it is\nthe first unified approach for the detection and matching of such images. \n\n"}
{"id": "1811.00344", "contents": "Title: Analyzing Perception-Distortion Tradeoff using Enhanced Perceptual\n  Super-resolution Network Abstract: Convolutional neural network (CNN) based methods have recently achieved great\nsuccess for image super-resolution (SR). However, most deep CNN based SR models\nattempt to improve distortion measures (e.g. PSNR, SSIM, IFC, VIF) while\nresulting in poor quantified perceptual quality (e.g. human opinion score,\nno-reference quality measures such as NIQE). Few works have attempted to\nimprove the perceptual quality at the cost of performance reduction in\ndistortion measures. A very recent study has revealed that distortion and\nperceptual quality are at odds with each other and there is always a trade-off\nbetween the two. Often the restoration algorithms that are superior in terms of\nperceptual quality, are inferior in terms of distortion measures. Our work\nattempts to analyze the trade-off between distortion and perceptual quality for\nthe problem of single image SR. To this end, we use the well-known SR\narchitecture-enhanced deep super-resolution (EDSR) network and show that it can\nbe adapted to achieve better perceptual quality for a specific range of the\ndistortion measure. While the original network of EDSR was trained to minimize\nthe error defined based on per-pixel accuracy alone, we train our network using\na generative adversarial network framework with EDSR as the generator module.\nOur proposed network, called enhanced perceptual super-resolution network\n(EPSR), is trained with a combination of mean squared error loss, perceptual\nloss, and adversarial loss. Our experiments reveal that EPSR achieves the\nstate-of-the-art trade-off between distortion and perceptual quality while the\nexisting methods perform well in either of these measures alone. \n\n"}
{"id": "1811.00445", "contents": "Title: CariGAN: Caricature Generation through Weakly Paired Adversarial\n  Learning Abstract: Caricature generation is an interesting yet challenging task. The primary\ngoal is to generate plausible caricatures with reasonable exaggerations given\nface images. Conventional caricature generation approaches mainly use low-level\ngeometric transformations such as image warping to generate exaggerated images,\nwhich lack richness and diversity in terms of content and style. The recent\nprogress in generative adversarial networks (GANs) makes it possible to learn\nan image-to-image transformation from data, so that richer contents and styles\ncan be generated. However, directly applying the GAN-based models to this task\nleads to unsatisfactory results because there is a large variance in the\ncaricature distribution. Moreover, some models require strictly paired training\ndata which largely limits their usage scenarios. In this paper, we propose\nCariGAN overcome these problems. Instead of training on paired data, CariGAN\nlearns transformations only from weakly paired images. Specifically, to enforce\nreasonable exaggeration and facial deformation, facial landmarks are adopted as\nan additional condition to constrain the generated image. Furthermore, an\nattention mechanism is introduced to encourage our model to focus on the key\nfacial parts so that more vivid details in these regions can be generated.\nFinally, a Diversity Loss is proposed to encourage the model to produce diverse\nresults to help alleviate the `mode collapse' problem of the conventional\nGAN-based models. Extensive experiments on a new large-scale `WebCaricature'\ndataset show that the proposed CariGAN can generate more plausible caricatures\nwith larger diversity compared with the state-of-the-art models. \n\n"}
{"id": "1811.00522", "contents": "Title: Linear Quadratic Mean Field Games -- Part I: The Asymptotic Solvability\n  Problem Abstract: This paper investigates the so-called asymptotic solvability problem in\nlinear quadratic (LQ) mean field games. The model has asymptotic solvability if\nfor all sufficiently large population sizes, the corresponding game has a set\nof feedback Nash strategies subject to a mild regularity requirement. We\nprovide a necessary and sufficient condition and show that in this case the\nsolution converges to a mean field limit. This is accomplished by developing a\nre-scaling method to derive a low dimensional ordinary differential equation\n(ODE) system, where a non-symmetric Riccati ODE has a central role. \n\n"}
{"id": "1811.01396", "contents": "Title: Handwriting Recognition in Low-resource Scripts using Adversarial\n  Learning Abstract: Handwritten Word Recognition and Spotting is a challenging field dealing with\nhandwritten text possessing irregular and complex shapes. The design of deep\nneural network models makes it necessary to extend training datasets in order\nto introduce variations and increase the number of samples; word-retrieval is\ntherefore very difficult in low-resource scripts. Much of the existing\nliterature comprises preprocessing strategies which are seldom sufficient to\ncover all possible variations. We propose the Adversarial Feature Deformation\nModule (AFDM) that learns ways to elastically warp extracted features in a\nscalable manner. The AFDM is inserted between intermediate layers and trained\nalternatively with the original framework, boosting its capability to better\nlearn highly informative features rather than trivial ones. We test our\nmeta-framework, which is built on top of popular word-spotting and\nword-recognition frameworks and enhanced by the AFDM, not only on extensive\nLatin word datasets but also sparser Indic scripts. We record results for\nvarying training data sizes, and observe that our enhanced network generalizes\nmuch better in the low-data regime; the overall word-error rates and mAP scores\nare observed to improve as well. \n\n"}
{"id": "1811.01926", "contents": "Title: contextual: Evaluating Contextual Multi-Armed Bandit Problems in R Abstract: Over the past decade, contextual bandit algorithms have been gaining in\npopularity due to their effectiveness and flexibility in solving sequential\ndecision problems---from online advertising and finance to clinical trial\ndesign and personalized medicine. At the same time, there are, as of yet,\nsurprisingly few options that enable researchers and practitioners to simulate\nand compare the wealth of new and existing bandit algorithms in a standardized\nway. To help close this gap between analytical research and empirical\nevaluation the current paper introduces the object-oriented R package\n\"contextual\": a user-friendly and, through its object-oriented structure,\neasily extensible framework that facilitates parallelized comparison of\ncontextual and context-free bandit policies through both simulation and offline\nanalysis. \n\n"}
{"id": "1811.01988", "contents": "Title: Strong mixed-integer programming formulations for trained neural\n  networks Abstract: We present strong mixed-integer programming (MIP) formulations for\nhigh-dimensional piecewise linear functions that correspond to trained neural\nnetworks. These formulations can be used for a number of important tasks, such\nas verifying that an image classification network is robust to adversarial\ninputs, or solving decision problems where the objective function is a machine\nlearning model. We present a generic framework, which may be of independent\ninterest, that provides a way to construct sharp or ideal formulations for the\nmaximum of d affine functions over arbitrary polyhedral input domains. We apply\nthis result to derive MIP formulations for a number of the most popular\nnonlinear operations (e.g. ReLU and max pooling) that are strictly stronger\nthan other approaches from the literature. We corroborate this computationally,\nshowing that our formulations are able to offer substantial improvements in\nsolve time on verification tasks for image classification networks. \n\n"}
{"id": "1811.02233", "contents": "Title: Weakly Supervised Scene Parsing with Point-based Distance Metric\n  Learning Abstract: Semantic scene parsing is suffering from the fact that pixel-level\nannotations are hard to be collected. To tackle this issue, we propose a\nPoint-based Distance Metric Learning (PDML) in this paper. PDML does not\nrequire dense annotated masks and only leverages several labeled points that\nare much easier to obtain to guide the training process. Concretely, we\nleverage semantic relationship among the annotated points by encouraging the\nfeature representations of the intra- and inter-category points to keep\nconsistent, i.e. points within the same category should have more similar\nfeature representations compared to those from different categories. We\nformulate such a characteristic into a simple distance metric loss, which\ncollaborates with the point-wise cross-entropy loss to optimize the deep neural\nnetworks. Furthermore, to fully exploit the limited annotations, distance\nmetric learning is conducted across different training images instead of simply\nadopting an image-dependent manner. We conduct extensive experiments on two\nchallenging scene parsing benchmarks of PASCAL-Context and ADE 20K to validate\nthe effectiveness of our PDML, and competitive mIoU scores are achieved. \n\n"}
{"id": "1811.03233", "contents": "Title: Knowledge Transfer via Distillation of Activation Boundaries Formed by\n  Hidden Neurons Abstract: An activation boundary for a neuron refers to a separating hyperplane that\ndetermines whether the neuron is activated or deactivated. It has been long\nconsidered in neural networks that the activations of neurons, rather than\ntheir exact output values, play the most important role in forming\nclassification friendly partitions of the hidden feature space. However, as far\nas we know, this aspect of neural networks has not been considered in the\nliterature of knowledge transfer. In this paper, we propose a knowledge\ntransfer method via distillation of activation boundaries formed by hidden\nneurons. For the distillation, we propose an activation transfer loss that has\nthe minimum value when the boundaries generated by the student coincide with\nthose by the teacher. Since the activation transfer loss is not differentiable,\nwe design a piecewise differentiable loss approximating the activation transfer\nloss. By the proposed method, the student learns a separating boundary between\nactivation region and deactivation region formed by each neuron in the teacher.\nThrough the experiments in various aspects of knowledge transfer, it is\nverified that the proposed method outperforms the current state-of-the-art. \n\n"}
{"id": "1811.03879", "contents": "Title: Cross and Learn: Cross-Modal Self-Supervision Abstract: In this paper we present a self-supervised method for representation learning\nutilizing two different modalities. Based on the observation that cross-modal\ninformation has a high semantic meaning we propose a method to effectively\nexploit this signal. For our approach we utilize video data since it is\navailable on a large scale and provides easily accessible modalities given by\nRGB and optical flow. We demonstrate state-of-the-art performance on highly\ncontested action recognition datasets in the context of self-supervised\nlearning. We show that our feature representation also transfers to other tasks\nand conduct extensive ablation studies to validate our core contributions. Code\nand model can be found at https://github.com/nawidsayed/Cross-and-Learn. \n\n"}
{"id": "1811.04303", "contents": "Title: PolyNeuron: Automatic Neuron Discovery via Learned Polyharmonic Spline\n  Activations Abstract: Automated deep neural network architecture design has received a significant\namount of recent attention. However, this attention has not been equally shared\nby one of the fundamental building blocks of a deep neural network, the\nneurons. In this study, we propose PolyNeuron, a novel automatic neuron\ndiscovery approach based on learned polyharmonic spline activations. More\nspecifically, PolyNeuron revolves around learning polyharmonic splines,\ncharacterized by a set of control points, that represent the activation\nfunctions of the neurons in a deep neural network. A relaxed variant of\nPolyNeuron, which we term PolyNeuron-R, loosens the constraints imposed by\nPolyNeuron to reduce the computational complexity for discovering the neuron\nactivation functions in an automated manner. Experiments show both PolyNeuron\nand PolyNeuron-R lead to networks that have improved or comparable performance\non multiple network architectures (LeNet-5 and ResNet-20) using different\ndatasets (MNIST and CIFAR10). As such, automatic neuron discovery approaches\nsuch as PolyNeuron is a worthy direction to explore. \n\n"}
{"id": "1811.04515", "contents": "Title: External optimal control of nonlocal PDEs Abstract: Very recently M. Warma has shown that for nonlocal PDEs associated with the\nfractional Laplacian, the classical notion of controllability from the boundary\ndoes not make sense and therefore it must be replaced by a control that is\nlocalized outside the open set where the PDE is solved. Having learned from the\nabove mentioned result, in this paper we introduce a new class of source\nidentification and optimal control problems where the source/control is located\noutside the observation domain where the PDE is satisfied. The classical\ndiffusion models lack this flexibility as they assume that the source/control\nis located either inside or on the boundary. This is essentially due to the\nlocality property of the underlying operators. We use the nonlocality of the\nfractional operator to create a framework that now allows placing a\nsource/control outside the observation domain. We consider the Dirichlet, Robin\nand Neumann source identification or optimal control problems. These problems\nrequire dealing with the nonlocal normal derivative (that we shall call\ninteraction operator). We create a functional analytic framework and show\nwell-posedness and derive the first order optimality conditions for these\nproblems. We introduce a new approach to approximate, with convergence rate,\nthe Dirichlet problem with nonzero exterior condition. The numerical examples\nconfirm our theoretical findings and illustrate the practicality of our\napproach. \n\n"}
{"id": "1811.05014", "contents": "Title: NeXtVLAD: An Efficient Neural Network to Aggregate Frame-level Features\n  for Large-scale Video Classification Abstract: This paper introduces a fast and efficient network architecture, NeXtVLAD, to\naggregate frame-level features into a compact feature vector for large-scale\nvideo classification. Briefly speaking, the basic idea is to decompose a\nhigh-dimensional feature into a group of relatively low-dimensional vectors\nwith attention before applying NetVLAD aggregation over time. This NeXtVLAD\napproach turns out to be both effective and parameter efficient in aggregating\ntemporal information. In the 2nd Youtube-8M video understanding challenge, a\nsingle NeXtVLAD model with less than 80M parameters achieves a GAP score of\n0.87846 in private leaderboard. A mixture of 3 NeXtVLAD models results in\n0.88722, which is ranked 3rd over 394 teams. The code is publicly available at\nhttps://github.com/linrongc/youtube-8m. \n\n"}
{"id": "1811.06277", "contents": "Title: Image declipping with deep networks Abstract: We present a deep network to recover pixel values lost to clipping. The\nclipped area of the image is typically a uniform area of minimum or maximum\nbrightness, losing image detail and color fidelity. The degree to which the\nclipping is visually noticeable depends on the amount by which values were\nclipped, and the extent of the clipped area. Clipping may occur in any (or all)\nof the pixel's color channels. Although clipped pixels are common and occur to\nsome degree in almost every image we tested, current automatic solutions have\nonly partial success in repairing clipped pixels and work only in limited cases\nsuch as only with overexposure (not under-exposure) and when some of the color\nchannels are not clipped. Using neural networks and their ability to model\nnatural images allows our neural network, DeclipNet, to reconstruct data in\nclipped regions producing state of the art results. \n\n"}
{"id": "1811.06498", "contents": "Title: Adjusting for Confounding in Unsupervised Latent Representations of\n  Images Abstract: Biological imaging data are often partially confounded or contain unwanted\nvariability. Examples of such phenomena include variable lighting across\nmicroscopy image captures, stain intensity variation in histological slides,\nand batch effects for high throughput drug screening assays. Therefore, to\ndevelop \"fair\" models which generalise well to unseen examples, it is crucial\nto learn data representations that are insensitive to nuisance factors of\nvariation. In this paper, we present a strategy based on adversarial training,\ncapable of learning unsupervised representations invariant to confounders. As\nan empirical validation of our method, we use deep convolutional autoencoders\nto learn unbiased cellular representations from microscopy imaging. \n\n"}
{"id": "1811.06666", "contents": "Title: Ground Plane Polling for 6DoF Pose Estimation of Objects on the Road Abstract: This paper introduces an approach to produce accurate 3D detection boxes for\nobjects on the ground using single monocular images. We do so by merging 2D\nvisual cues, 3D object dimensions, and ground plane constraints to produce\nboxes that are robust against small errors and incorrect predictions. First, we\ntrain a single-shot convolutional neural network (CNN) that produces multiple\nvisual and geometric cues of interest: 2D bounding boxes, 2D keypoints of\ninterest, coarse object orientations and object dimensions. Subsets of these\ncues are then used to poll probable ground planes from a pre-computed database\nof ground planes, to identify the \"best fit\" plane with highest consensus. Once\nidentified, the \"best fit\" plane provides enough constraints to successfully\nconstruct the desired 3D detection box, without directly predicting the 6DoF\npose of the object. The entire ground plane polling (GPP) procedure is\nconstructed as a non-parametrized layer of the CNN that outputs the desired\n\"best fit\" plane and the corresponding 3D keypoints, which together define the\nfinal 3D bounding box. Doing so allows us to poll thousands of different ground\nplane configurations without adding considerable overhead, while also creating\na single CNN that directly produces the desired output without the need for\npost processing. We evaluate our method on the 2D detection and orientation\nestimation benchmark from the challenging KITTI dataset, and provide additional\ncomparisons for 3D metrics of importance. This single-stage, single-pass CNN\nresults in superior localization and orientation estimation compared to more\ncomplex and computationally expensive monocular approaches. \n\n"}
{"id": "1811.07266", "contents": "Title: DeepConsensus: using the consensus of features from multiple layers to\n  attain robust image classification Abstract: We consider a classifier whose test set is exposed to various perturbations\nthat are not present in the training set. These test samples still contain\nenough features to map them to the same class as their unperturbed counterpart.\nCurrent architectures exhibit rapid degradation of accuracy when trained on\nstandard datasets but then used to classify perturbed samples of that data. To\naddress this, we present a novel architecture named DeepConsensus that\nsignificantly improves generalization to these test-time perturbations. Our key\ninsight is that deep neural networks should directly consider summaries of low\nand high level features when making classifications. Existing convolutional\nneural networks can be augmented with DeepConsensus, leading to improved\nresistance against large and small perturbations on MNIST, EMNIST,\nFashionMNIST, CIFAR10 and SVHN datasets. \n\n"}
{"id": "1811.07311", "contents": "Title: Regularized adversarial examples for model interpretability Abstract: As machine learning algorithms continue to improve, there is an increasing\nneed for explaining why a model produces a certain prediction for a certain\ninput. In recent years, several methods for model interpretability have been\ndeveloped, aiming to provide explanation of which subset regions of the model\ninput is the main reason for the model prediction. In parallel, a significant\nresearch community effort is occurring in recent years for developing\nadversarial example generation methods for fooling models, while not altering\nthe true label of the input,as it would have been classified by a human\nannotator. In this paper, we bridge the gap between adversarial example\ngeneration and model interpretability, and introduce a modification to the\nadversarial example generation process which encourages better\ninterpretability. We analyze the proposed method on a public medical imaging\ndataset, both quantitatively and qualitatively, and show that it significantly\noutperforms the leading known alternative method. Our suggested method is\nsimple to implement, and can be easily plugged into most common adversarial\nexample generation frameworks. Additionally, we propose an explanation quality\nmetric - $APE$ - \"Adversarial Perturbative Explanation\", which measures how\nwell an explanation describes model decisions. \n\n"}
{"id": "1811.07389", "contents": "Title: Privacy Preserving Utility Mining: A Survey Abstract: In big data era, the collected data usually contains rich information and\nhidden knowledge. Utility-oriented pattern mining and analytics have shown a\npowerful ability to explore these ubiquitous data, which may be collected from\nvarious fields and applications, such as market basket analysis, retail,\nclick-stream analysis, medical analysis, and bioinformatics. However, analysis\nof these data with sensitive private information raises privacy concerns. To\nachieve better trade-off between utility maximizing and privacy preserving,\nPrivacy-Preserving Utility Mining (PPUM) has become a critical issue in recent\nyears. In this paper, we provide a comprehensive overview of PPUM. We first\npresent the background of utility mining, privacy-preserving data mining and\nPPUM, then introduce the related preliminaries and problem formulation of PPUM,\nas well as some key evaluation criteria for PPUM. In particular, we present and\ndiscuss the current state-of-the-art PPUM algorithms, as well as their\nadvantages and deficiencies in detail. Finally, we highlight and discuss some\ntechnical challenges and open directions for future research on PPUM. \n\n"}
{"id": "1811.07441", "contents": "Title: CompoNet: Learning to Generate the Unseen by Part Synthesis and\n  Composition Abstract: Data-driven generative modeling has made remarkable progress by leveraging\nthe power of deep neural networks. A reoccurring challenge is how to enable a\nmodel to generate a rich variety of samples from the entire target\ndistribution, rather than only from a distribution confined to the training\ndata. In other words, we would like the generative model to go beyond the\nobserved samples and learn to generate ``unseen'', yet still plausible, data.\nIn our work, we present CompoNet, a generative neural network for 2D or 3D\nshapes that is based on a part-based prior, where the key idea is for the\nnetwork to synthesize shapes by varying both the shape parts and their\ncompositions. Treating a shape not as an unstructured whole, but as a\n(re-)composable set of deformable parts, adds a combinatorial dimension to the\ngenerative process to enrich the diversity of the output, encouraging the\ngenerator to venture more into the ``unseen''. We show that our part-based\nmodel generates richer variety of plausible shapes compared with baseline\ngenerative models. To this end, we introduce two quantitative metrics to\nevaluate the diversity of a generative model and assess how well the generated\ndata covers both the training data and unseen data from the same target\ndistribution. Code is available at https://github.com/nschor/CompoNet. \n\n"}
{"id": "1811.07456", "contents": "Title: Larger Norm More Transferable: An Adaptive Feature Norm Approach for\n  Unsupervised Domain Adaptation Abstract: Domain adaptation enables the learner to safely generalize into novel\nenvironments by mitigating domain shifts across distributions. Previous works\nmay not effectively uncover the underlying reasons that would lead to the\ndrastic model degradation on the target task. In this paper, we empirically\nreveal that the erratic discrimination of the target domain mainly stems from\nits much smaller feature norms with respect to that of the source domain. To\nthis end, we propose a novel parameter-free Adaptive Feature Norm approach. We\ndemonstrate that progressively adapting the feature norms of the two domains to\na large range of values can result in significant transfer gains, implying that\nthose task-specific features with larger norms are more transferable. Our\nmethod successfully unifies the computation of both standard and partial domain\nadaptation with more robustness against the negative transfer issue. Without\nbells and whistles but a few lines of code, our method substantially lifts the\nperformance on the target task and exceeds state-of-the-arts by a large margin\n(11.5% on Office-Home and 17.1% on VisDA2017). We hope our simple yet effective\napproach will shed some light on the future research of transfer learning. Code\nis available at https://github.com/jihanyang/AFN. \n\n"}
{"id": "1811.07480", "contents": "Title: Global and Local Sensitivity Guided Key Salient Object Re-augmentation\n  for Video Saliency Detection Abstract: The existing still-static deep learning based saliency researches do not\nconsider the weighting and highlighting of extracted features from different\nlayers, all features contribute equally to the final saliency decision-making.\nSuch methods always evenly detect all \"potentially significant regions\" and\nunable to highlight the key salient object, resulting in detection failure of\ndynamic scenes. In this paper, based on the fact that salient areas in videos\nare relatively small and concentrated, we propose a \\textbf{key salient object\nre-augmentation method (KSORA) using top-down semantic knowledge and bottom-up\nfeature guidance} to improve detection accuracy in video scenes. KSORA includes\ntwo sub-modules (WFE and KOS): WFE processes local salient feature selection\nusing bottom-up strategy, while KOS ranks each object in global fashion by\ntop-down statistical knowledge, and chooses the most critical object area for\nlocal enhancement. The proposed KSORA can not only strengthen the saliency\nvalue of the local key salient object but also ensure global saliency\nconsistency. Results on three benchmark datasets suggest that our model has the\ncapability of improving the detection accuracy on complex scenes. The\nsignificant performance of KSORA, with a speed of 17FPS on modern GPUs, has\nbeen verified by comparisons with other ten state-of-the-art algorithms. \n\n"}
{"id": "1811.07626", "contents": "Title: Beyond Attributes: Adversarial Erasing Embedding Network for Zero-shot\n  Learning Abstract: In this paper, an adversarial erasing embedding network with the guidance of\nhigh-order attributes (AEEN-HOA) is proposed for going further to solve the\nchallenging ZSL/GZSL task. AEEN-HOA consists of two branches, i.e., the upper\nstream is capable of erasing some initially discovered regions, then the\nhigh-order attribute supervision is incorporated to characterize the\nrelationship between the class attributes. Meanwhile, the bottom stream is\ntrained by taking the current background regions to train the same attribute.\nAs far as we know, it is the first time of introducing the erasing operations\ninto the ZSL task. In addition, we first propose a class attribute activation\nmap for the visualization of ZSL output, which shows the relationship between\nclass attribute feature and attention map. Experiments on four standard\nbenchmark datasets demonstrate the superiority of AEEN-HOA framework. \n\n"}
{"id": "1811.07958", "contents": "Title: Tukey-Inspired Video Object Segmentation Abstract: We investigate the problem of strictly unsupervised video object\nsegmentation, i.e., the separation of a primary object from background in video\nwithout a user-provided object mask or any training on an annotated dataset. We\nfind foreground objects in low-level vision data using a John Tukey-inspired\nmeasure of \"outlierness\". This Tukey-inspired measure also estimates the\nreliability of each data source as video characteristics change (e.g., a camera\nstarts moving). The proposed method achieves state-of-the-art results for\nstrictly unsupervised video object segmentation on the challenging DAVIS\ndataset. Finally, we use a variant of the Tukey-inspired measure to combine the\noutput of multiple segmentation methods, including those using supervision\nduring training, runtime, or both. This collectively more robust method of\nsegmentation improves the Jaccard measure of its constituent methods by as much\nas 28%. \n\n"}
{"id": "1811.08004", "contents": "Title: Photorealistic Facial Synthesis in the Dimensional Affect Space Abstract: This paper presents a novel approach for synthesizing facial affect, which is\nbased on our annotating 600,000 frames of the 4DFAB database in terms of\nvalence and arousal. The input of this approach is a pair of these emotional\nstate descriptors and a neutral 2D image of a person to whom the corresponding\naffect will be synthesized. Given this target pair, a set of 3D facial meshes\nis selected, which is used to build a blendshape model and generate the new\nfacial affect. To synthesize the affect on the 2D neutral image, 3DMM fitting\nis performed and the reconstructed face is deformed to generate the target\nfacial expressions. Last, the new face is rendered into the original image.\nBoth qualitative and quantitative experimental studies illustrate the\ngeneration of realistic images, when the neutral image is sampled from a\nvariety of well known databases, such as the Aff-Wild, AFEW, Multi-PIE,\nAFEW-VA, BU-3DFE, Bosphorus. \n\n"}
{"id": "1811.08126", "contents": "Title: Adversarial Feedback Loop Abstract: Thanks to their remarkable generative capabilities, GANs have gained great\npopularity, and are used abundantly in state-of-the-art methods and\napplications. In a GAN based model, a discriminator is trained to learn the\nreal data distribution. To date, it has been used only for training purposes,\nwhere it's utilized to train the generator to provide real-looking outputs. In\nthis paper we propose a novel method that makes an explicit use of the\ndiscriminator in test-time, in a feedback manner in order to improve the\ngenerator results. To the best of our knowledge it is the first time a\ndiscriminator is involved in test-time. We claim that the discriminator holds\nsignificant information on the real data distribution, that could be useful for\ntest-time as well, a potential that has not been explored before.\n  The approach we propose does not alter the conventional training stage. At\ntest-time, however, it transfers the output from the generator into the\ndiscriminator, and uses feedback modules (convolutional blocks) to translate\nthe features of the discriminator layers into corrections to the features of\nthe generator layers, which are used eventually to get a better generator\nresult. Our method can contribute to both conditional and unconditional GANs.\nAs demonstrated by our experiments, it can improve the results of\nstate-of-the-art networks for super-resolution, and image generation. \n\n"}
{"id": "1811.08139", "contents": "Title: Adversarial point set registration Abstract: We present a novel approach to point set registration which is based on\none-shot adversarial learning. The idea of the algorithm is inspired by recent\nsuccesses of generative adversarial networks. Treating the point clouds as\nthree-dimensional probability distributions, we develop a one-shot adversarial\noptimization procedure, in which we train a critic neural network to\ndistinguish between source and target point sets, while simultaneously learning\nthe parameters of the transformation to trick the critic into confusing the\npoints. In contrast to most existing algorithms for point set registration,\nours does not rely on any correspondences between the point clouds. We\ndemonstrate the performance of the algorithm on several challenging benchmarks\nand compare it to the existing baselines. \n\n"}
{"id": "1811.08150", "contents": "Title: Effect of Depth and Width on Local Minima in Deep Learning Abstract: In this paper, we analyze the effects of depth and width on the quality of\nlocal minima, without strong over-parameterization and simplification\nassumptions in the literature. Without any simplification assumption, for deep\nnonlinear neural networks with the squared loss, we theoretically show that the\nquality of local minima tends to improve towards the global minimum value as\ndepth and width increase. Furthermore, with a locally-induced structure on deep\nnonlinear neural networks, the values of local minima of neural networks are\ntheoretically proven to be no worse than the globally optimal values of\ncorresponding classical machine learning models. We empirically support our\ntheoretical observation with a synthetic dataset as well as MNIST, CIFAR-10 and\nSVHN datasets. When compared to previous studies with strong\nover-parameterization assumptions, the results in this paper do not require\nover-parameterization, and instead show the gradual effects of\nover-parameterization as consequences of general results. \n\n"}
{"id": "1811.09188", "contents": "Title: Ergodicity analysis and antithetic integral control of a class of\n  stochastic reaction networks with delays Abstract: Delays are an important phenomenon arising in a wide variety of real world\nsystems. They occur in biological models because of diffusion effects or as\nsimplifying modeling elements. We propose here to consider delayed stochastic\nreaction networks. The difficulty here lies in the fact that the state-space of\na delayed reaction network is infinite-dimensional, which makes their analysis\nmore involved. We demonstrate here that a particular class of stochastic\ntime-varying delays, namely those that follow a phase-type distribution, can be\nexactly implemented in terms of a chemical reaction network. Hence, any\ndelay-free network can be augmented to incorporate those delays through the\naddition of delay-species and delay-reactions. Hence, for this class of\nstochastic delays, which can be used to approximate any delay distribution\narbitrarily accurately, the state-space remains finite-dimensional and,\ntherefore, standard tools developed for standard reaction network still apply.\nIn particular, we demonstrate that for unimolecular mass-action reaction\nnetworks that the delayed stochastic reaction network is ergodic if and only if\nthe non-delayed network is ergodic as well. Bimolecular reactions are more\ndifficult to consider but an analogous result is also obtained. These results\ntell us that delays that are phase-type distributed, regardless of their\ndistribution, are not harmful to the ergodicity property of reaction networks.\nWe also prove that the presence of those delays adds convolution terms in the\nmoment equation but does not change the value of the stationary means compared\nto the delay-free case. Finally, the control of a certain class of delayed\nstochastic reaction network using a delayed antithetic integral controller is\nconsidered. It is proven that this controller achieves its goal provided that\nthe delay-free network satisfy the conditions of ergodicity and\noutput-controllability. \n\n"}
{"id": "1811.09521", "contents": "Title: Complementary Segmentation of Primary Video Objects with Reversible\n  Flows Abstract: Segmenting primary objects in a video is an important yet challenging problem\nin computer vision, as it exhibits various levels of foreground/background\nambiguities. To reduce such ambiguities, we propose a novel formulation via\nexploiting foreground and background context as well as their complementary\nconstraint. Under this formulation, a unified objective function is further\ndefined to encode each cue. For implementation, we design a Complementary\nSegmentation Network (CSNet) with two separate branches, which can\nsimultaneously encode the foreground and background information along with\njoint spatial constraints. The CSNet is trained on massive images with manually\nannotated salient objects in an end-to-end manner. By applying CSNet on each\nvideo frame, the spatial foreground and background maps can be initialized. To\nenforce temporal consistency effectively and efficiently, we divide each frame\ninto superpixels and construct neighborhood reversible flow that reflects the\nmost reliable temporal correspondences between superpixels in far-away frames.\nWith such flow, the initialized foregroundness and backgroundness can be\npropagated along the temporal dimension so that primary video objects gradually\npop-out and distractors are well suppressed. Extensive experimental results on\nthree video datasets show that the proposed approach achieves impressive\nperformance in comparisons with 18 state-of-the-art models. \n\n"}
{"id": "1811.09652", "contents": "Title: Generalised Entropies and Metric-Invariant Optimal Countermeasures for\n  Information Leakage under Symmetric Constraints Abstract: We introduce a novel generalization of entropy and conditional entropy from\nwhich most definitions from the literature can be derived as particular cases.\nWithin this general framework, we investigate the problem of designing\ncountermeasures for information leakage. In particular, we seek\nmetric-invariant solutions, i.e., they are robust against the choice of entropy\nfor quantifying the leakage. The problem can be modelled as an information\nchannel from the system to an adversary, and the countermeasures can be seen as\nmodifying this channel in order to minimise the amount of information that the\noutputs reveal about the inputs. Our main result is to fully solve the problem\nunder the highly symmetrical design constraint that the number of inputs that\ncan produce the same output is capped. Our proof is constructive and the\noptimal channels and the minimum leakage are derived in closed form. \n\n"}
{"id": "1811.09916", "contents": "Title: Generating Realistic Training Images Based on Tonality-Alignment\n  Generative Adversarial Networks for Hand Pose Estimation Abstract: Hand pose estimation from a monocular RGB image is an important but\nchallenging task. The main factor affecting its performance is the lack of a\nsufficiently large training dataset with accurate hand-keypoint annotations. In\nthis work, we circumvent this problem by proposing an effective method for\ngenerating realistic hand poses and show that state-of-the-art algorithms for\nhand pose estimation can be greatly improved by utilizing the generated hand\nposes as training data. Specifically, we first adopt an augmented reality (AR)\nsimulator to synthesize hand poses with accurate hand-keypoint labels. Although\nthe synthetic hand poses come with precise joint labels, eliminating the need\nof manual annotations, they look unnatural and are not the ideal training data.\nTo produce more realistic hand poses, we propose to blend a synthetic hand pose\nwith a real background, such as arms and sleeves. To this end, we develop\ntonality-alignment generative adversarial networks (TAGANs), which align the\ntonality and color distributions between synthetic hand poses and real\nbackgrounds, and can generate high quality hand poses. We evaluate TAGAN on\nthree benchmarks, including the RHP, STB, and CMU-PS hand pose datasets. With\nthe aid of the synthesized poses, our method performs favorably against the\nstate-of-the-arts in both 2D and 3D hand pose estimations. \n\n"}
{"id": "1811.10004", "contents": "Title: Visual Attention on the Sun: What Do Existing Models Actually Predict? Abstract: Visual attention prediction is a classic problem that seems to be well\naddressed in the deep learning era. One compelling concern, however, gradually\narise along with the rapidly growing performance scores over existing visual\nattention datasets: do existing deep models really capture the inherent\nmechanism of human visual attention? To address this concern, this paper\nproposes a new dataset, named VASUN, that records the free-viewing human\nattention on solar images. Different from previous datasets, images in VASUN\ncontain many irregular visual patterns that existing deep models have never\nseen. By benchmarking existing models on VASUN, we find the performances of\nmany state-of-the-art deep models drop remarkably, while many classic shallow\nmodels perform impressively. From these results, we find that the significant\nperformance advance of existing deep attention models may come from their\ncapabilities of memorizing and predicting the occurrence of some specific\nvisual patterns other than learning the inherent mechanism of human visual\nattention. In addition, we also train several baseline models on VASUN to\ndemonstrate the feasibility and key issues of predicting visual attention on\nthe sun. These baseline models, together with the proposed dataset, can be used\nto revisit the problem of visual attention prediction from a novel perspective\nthat are complementary to existing ones. \n\n"}
{"id": "1811.10427", "contents": "Title: MR-GAN: Manifold Regularized Generative Adversarial Networks Abstract: Despite the growing interest in generative adversarial networks (GANs),\ntraining GANs remains a challenging problem, both from a theoretical and a\npractical standpoint. To address this challenge, in this paper, we propose a\nnovel way to exploit the unique geometry of the real data, especially the\nmanifold information. More specifically, we design a method to regularize GAN\ntraining by adding an additional regularization term referred to as manifold\nregularizer. The manifold regularizer forces the generator to respect the\nunique geometry of the real data manifold and generate high quality data.\nFurthermore, we theoretically prove that the addition of this regularization\nterm in any class of GANs including DCGAN and Wasserstein GAN leads to improved\nperformance in terms of generalization, existence of equilibrium, and\nstability. Preliminary experiments show that the proposed manifold\nregularization helps in avoiding mode collapse and leads to stable training. \n\n"}
{"id": "1811.10551", "contents": "Title: Similarity-preserving Image-image Domain Adaptation for Person\n  Re-identification Abstract: This article studies the domain adaptation problem in person\nre-identification (re-ID) under a \"learning via translation\" framework,\nconsisting of two components, 1) translating the labeled images from the source\nto the target domain in an unsupervised manner, 2) learning a re-ID model using\nthe translated images. The objective is to preserve the underlying human\nidentity information after image translation, so that translated images with\nlabels are effective for feature learning on the target domain. To this end, we\npropose a similarity preserving generative adversarial network (SPGAN) and its\nend-to-end trainable version, eSPGAN. Both aiming at similarity preserving,\nSPGAN enforces this property by heuristic constraints, while eSPGAN does so by\noptimally facilitating the re-ID model learning. More specifically, SPGAN\nseparately undertakes the two components in the \"learning via translation\"\nframework. It first preserves two types of unsupervised similarity, namely,\nself-similarity of an image before and after translation, and\ndomain-dissimilarity of a translated source image and a target image. It then\nlearns a re-ID model using existing networks. In comparison, eSPGAN seamlessly\nintegrates image translation and re-ID model learning. During the end-to-end\ntraining of eSPGAN, re-ID learning guides image translation to preserve the\nunderlying identity information of an image. Meanwhile, image translation\nimproves re-ID learning by providing identity-preserving training samples of\nthe target domain style. In the experiment, we show that identities of the fake\nimages generated by SPGAN and eSPGAN are well preserved. Based on this, we\nreport the new state-of-the-art domain adaptation results on two large-scale\nperson re-ID datasets. \n\n"}
{"id": "1811.10669", "contents": "Title: GANsfer Learning: Combining labelled and unlabelled data for GAN based\n  data augmentation Abstract: Medical imaging is a domain which suffers from a paucity of manually\nannotated data for the training of learning algorithms. Manually delineating\npathological regions at a pixel level is a time consuming process, especially\nin 3D images, and often requires the time of a trained expert. As a result,\nsupervised machine learning solutions must make do with small amounts of\nlabelled data, despite there often being additional unlabelled data available.\nWhilst of less value than labelled images, these unlabelled images can contain\npotentially useful information. In this paper we propose combining both\nlabelled and unlabelled data within a GAN framework, before using the resulting\nnetwork to produce images for use when training a segmentation network. We\nexplore the task of deep grey matter multi-class segmentation in an AD dataset\nand show that the proposed method leads to a significant improvement in\nsegmentation results, particularly in cases where the amount of labelled data\nis restricted. We show that this improvement is largely driven by a greater\nability to segment the structures known to be the most affected by AD, thereby\ndemonstrating the benefits of exposing the system to more examples of\npathological anatomical variation. We also show how a shift in domain of the\ntraining data from young and healthy towards older and more pathological\nexamples leads to better segmentations of the latter cases, and that this leads\nto a significant improvement in the ability for the computed segmentations to\nstratify cases of AD. \n\n"}
{"id": "1811.10884", "contents": "Title: UnDEMoN 2.0: Improved Depth and Ego Motion Estimation through Deep Image\n  Sampling Abstract: In this paper, we provide an improved version of UnDEMoN model for depth and\nego motion estimation from monocular images. The improvement is achieved by\ncombining the standard bi-linear sampler with a deep network based image\nsampling model (DIS-NET) to provide better image reconstruction capabilities on\nwhich the depth estimation accuracy depends in un-supervised learning models.\nWhile DIS-NET provides higher order regression and larger input search space,\nthe bi-linear sampler provides geometric constraints necessary for reducing the\nsize of the solution space for an ill-posed problem of this kind. This\ncombination is shown to provide significant improvement in depth and pose\nestimation accuracy outperforming all existing state-of-the-art methods in this\ncategory. In addition, the modified network uses far less number of tunable\nparameters making it one of the lightest deep network model for depth\nestimation. The proposed model is labeled as \"UnDEMoN 2.0\" indicating an\nimprovement over the existing UnDEMoN model. The efficacy of the proposed model\nis demonstrated through rigorous experimental analysis on the standard KITTI\ndataset. \n\n"}
{"id": "1811.11209", "contents": "Title: Iterative Transformer Network for 3D Point Cloud Abstract: 3D point cloud is an efficient and flexible representation of 3D structures.\nRecently, neural networks operating on point clouds have shown superior\nperformance on 3D understanding tasks such as shape classification and part\nsegmentation. However, performance on such tasks is evaluated on complete\nshapes aligned in a canonical frame, while real world 3D data are partial and\nunaligned. A key challenge in learning from partial, unaligned point cloud data\nis to learn features that are invariant or equivariant with respect to\ngeometric transformations. To address this challenge, we propose the Iterative\nTransformer Network (IT-Net), a network module that canonicalizes the pose of a\npartial object with a series of 3D rigid transformations predicted in an\niterative fashion. We demonstrate the efficacy of IT-Net as an anytime pose\nestimator from partial point clouds without using complete object models.\nFurther, we show that IT-Net achieves superior performance over alternative 3D\ntransformer networks on various tasks, such as partial shape classification and\nobject part segmentation. \n\n"}
{"id": "1811.11582", "contents": "Title: Continuous Trade-off Optimization between Fast and Accurate Deep Face\n  Detectors Abstract: Although deep neural networks offer better face detection results than\nshallow or handcrafted models, their complex architectures come with higher\ncomputational requirements and slower inference speeds than shallow neural\nnetworks. In this context, we study five straightforward approaches to achieve\nan optimal trade-off between accuracy and speed in face detection. All the\napproaches are based on separating the test images in two batches, an easy\nbatch that is fed to a faster face detector and a difficult batch that is fed\nto a more accurate yet slower detector. We conduct experiments on the AFW and\nthe FDDB data sets, using MobileNet-SSD as the fast face detector and S3FD\n(Single Shot Scale-invariant Face Detector) as the accurate face detector, both\nmodels being pre-trained on the WIDER FACE data set. Our experiments show that\nthe proposed difficulty metrics compare favorably to a random split of the\nimages. \n\n"}
{"id": "1811.11637", "contents": "Title: Adaptive Stochastic Variance Reduction for Subsampled Newton Method with\n  Cubic Regularization Abstract: The cubic regularized Newton method of Nesterov and Polyak has become\nincreasingly popular for non-convex optimization because of its capability of\nfinding an approximate local solution with second-order guarantee. Several\nrecent works extended this method to the setting of minimizing the average of N\nsmooth functions by replacing the exact gradients and Hessians with subsampled\napproximations. It has been shown that the total Hessian sample complexity can\nbe reduced to be sublinear in N per iteration by leveraging stochastic variance\nreduction techniques. We present an adaptive variance reduction scheme for\nsubsampled Newton method with cubic regularization, and show that the expected\nHessian sample complexity is O(N + N^{2/3}\\epsilon^{-3/2}) for finding an\n(\\epsilon,\\epsilon^{1/2})-approximate local solution (in terms of first and\nsecond-order guarantees respectively). Moreover, we show that the same Hessian\nsample complexity retains with fixed sample sizes if exact gradients are used.\nThe techniques of our analysis are different from previous works in that we do\nnot rely on high probability bounds based on matrix concentration inequalities.\nInstead, we derive and utilize bounds on the 3rd and 4th order moments of the\naverage of random matrices, which are of independent interest on their own. \n\n"}
{"id": "1811.12004", "contents": "Title: Real-time 2D Multi-Person Pose Estimation on CPU: Lightweight OpenPose Abstract: In this work we adapt multi-person pose estimation architecture to use it on\nedge devices. We follow the bottom-up approach from OpenPose, the winner of\nCOCO 2016 Keypoints Challenge, because of its decent quality and robustness to\nnumber of people inside the frame. With proposed network design and optimized\npost-processing code the full solution runs at 28 frames per second (fps) on\nIntel$\\unicode{xAE}$ NUC 6i7KYB mini PC and 26 fps on Core$^{TM}$ i7-6850K CPU.\nThe network model has 4.1M parameters and 9 billions floating-point operations\n(GFLOPs) complexity, which is just ~15% of the baseline 2-stage OpenPose with\nalmost the same quality. The code and model are available as a part of\nIntel$\\unicode{xAE}$ OpenVINO$^{TM}$ Toolkit. \n\n"}
{"id": "1811.12030", "contents": "Title: Grid R-CNN Abstract: This paper proposes a novel object detection framework named Grid R-CNN,\nwhich adopts a grid guided localization mechanism for accurate object\ndetection. Different from the traditional regression based methods, the Grid\nR-CNN captures the spatial information explicitly and enjoys the position\nsensitive property of fully convolutional architecture. Instead of using only\ntwo independent points, we design a multi-point supervision formulation to\nencode more clues in order to reduce the impact of inaccurate prediction of\nspecific points. To take the full advantage of the correlation of points in a\ngrid, we propose a two-stage information fusion strategy to fuse feature maps\nof neighbor grid points. The grid guided localization approach is easy to be\nextended to different state-of-the-art detection frameworks. Grid R-CNN leads\nto high quality object localization, and experiments demonstrate that it\nachieves a 4.1% AP gain at IoU=0.8 and a 10.0% AP gain at IoU=0.9 on COCO\nbenchmark compared to Faster R-CNN with Res50 backbone and FPN architecture. \n\n"}
{"id": "1811.12506", "contents": "Title: 3D Semi-Supervised Learning with Uncertainty-Aware Multi-View\n  Co-Training Abstract: While making a tremendous impact in various fields, deep neural networks\nusually require large amounts of labeled data for training which are expensive\nto collect in many applications, especially in the medical domain. Unlabeled\ndata, on the other hand, is much more abundant. Semi-supervised learning\ntechniques, such as co-training, could provide a powerful tool to leverage\nunlabeled data. In this paper, we propose a novel framework, uncertainty-aware\nmulti-view co-training (UMCT), to address semi-supervised learning on 3D data,\nsuch as volumetric data from medical imaging. In our work, co-training is\nachieved by exploiting multi-viewpoint consistency of 3D data. We generate\ndifferent views by rotating or permuting the 3D data and utilize asymmetrical\n3D kernels to encourage diversified features in different sub-networks. In\naddition, we propose an uncertainty-weighted label fusion mechanism to estimate\nthe reliability of each view's prediction with Bayesian deep learning. As one\nview requires the supervision from other views in co-training, our\nself-adaptive approach computes a confidence score for the prediction of each\nunlabeled sample in order to assign a reliable pseudo label. Thus, our approach\ncan take advantage of unlabeled data during training. We show the effectiveness\nof our proposed semi-supervised method on several public datasets from medical\nimage segmentation tasks (NIH pancreas & LiTS liver tumor dataset). Meanwhile,\na fully-supervised method based on our approach achieved state-of-the-art\nperformances on both the LiTS liver tumor segmentation and the Medical\nSegmentation Decathlon (MSD) challenge, demonstrating the robustness and value\nof our framework, even when fully supervised training is feasible. \n\n"}
{"id": "1811.12641", "contents": "Title: Transferable Adversarial Attacks for Image and Video Object Detection Abstract: Adversarial examples have been demonstrated to threaten many computer vision\ntasks including object detection. However, the existing attacking methods for\nobject detection have two limitations: poor transferability, which denotes that\nthe generated adversarial examples have low success rate to attack other kinds\nof detection methods, and high computation cost, which means that they need\nmore time to generate an adversarial image, and therefore are difficult to deal\nwith the video data. To address these issues, we utilize a generative mechanism\nto obtain the adversarial image and video. In this way, the processing time is\nreduced. To enhance the transferability, we destroy the feature maps extracted\nfrom the feature network, which usually constitutes the basis of object\ndetectors. The proposed method is based on the Generative Adversarial Network\n(GAN) framework, where we combine the high-level class loss and low-level\nfeature loss to jointly train the adversarial example generator. A series of\nexperiments conducted on PASCAL VOC and ImageNet VID datasets show that our\nmethod can efficiently generate image and video adversarial examples, and more\nimportantly, these adversarial examples have better transferability, and thus,\nare able to simultaneously attack two kinds of representative object detection\nmodels: proposal based models like Faster-RCNN, and regression based models\nlike SSD. \n\n"}
{"id": "1811.12766", "contents": "Title: Model-blind Video Denoising Via Frame-to-frame Training Abstract: Modeling the processing chain that has produced a video is a difficult\nreverse engineering task, even when the camera is available. This makes model\nbased video processing a still more complex task. In this paper we propose a\nfully blind video denoising method, with two versions off-line and on-line.\nThis is achieved by fine-tuning a pre-trained AWGN denoising network to the\nvideo with a novel frame-to-frame training strategy. Our denoiser can be used\nwithout knowledge of the origin of the video or burst and the post processing\nsteps applied from the camera sensor. The on-line process only requires a\ncouple of frames before achieving visually-pleasing results for a wide range of\nperturbations. It nonetheless reaches state of the art performance for standard\nGaussian noise, and can be used off-line with still better performance. \n\n"}
{"id": "1811.12786", "contents": "Title: TextMountain: Accurate Scene Text Detection via Instance Segmentation Abstract: In this paper, we propose a novel scene text detection method named\nTextMountain. The key idea of TextMountain is making full use of border-center\ninformation. Different from previous works that treat center-border as a binary\nclassification problem, we predict text center-border probability (TCBP) and\ntext center-direction (TCD). The TCBP is just like a mountain whose top is text\ncenter and foot is text border. The mountaintop can separate text instances\nwhich cannot be easily achieved using semantic segmentation map and its rising\ndirection can plan a road to top for each pixel on mountain foot at the group\nstage. The TCD helps TCBP learning better. Our label rules will not lead to the\nambiguous problem with the transformation of angle, so the proposed method is\nrobust to multi-oriented text and can also handle well with curved text. In\ninference stage, each pixel at the mountain foot needs to search the path to\nthe mountaintop and this process can be efficiently completed in parallel,\nyielding the efficiency of our method compared with others. The experiments on\nMLT, ICDAR2015, RCTW-17 and SCUT-CTW1500 databases demonstrate that the\nproposed method achieves better or comparable performance in terms of both\naccuracy and efficiency. It is worth mentioning our method achieves an\nF-measure of 76.85% on MLT which outperforms the previous methods by a large\nmargin. Code will be made available. \n\n"}
{"id": "1811.12813", "contents": "Title: Real Time Bangladeshi Sign Language Detection using Faster R-CNN Abstract: Bangladeshi Sign Language (BdSL) is a commonly used medium of communication\nfor the hearing-impaired people in Bangladesh. Developing a real time system to\ndetect these signs from images is a great challenge. In this paper, we present\na technique to detect BdSL from images that performs in real time. Our method\nuses Convolutional Neural Network based object detection technique to detect\nthe presence of signs in the image region and to recognize its class. For this\npurpose, we adopted Faster Region-based Convolutional Network approach and\ndeveloped a dataset $-$ BdSLImset $-$ to train our system. Previous research\nworks in detecting BdSL generally depend on external devices while most of the\nother vision-based techniques do not perform efficiently in real time. Our\napproach, however, is free from such limitations and the experimental results\ndemonstrate that the proposed method successfully identifies and recognizes\nBangladeshi signs in real time. \n\n"}
{"id": "1812.00535", "contents": "Title: Beyond Inferring Class Representatives: User-Level Privacy Leakage From\n  Federated Learning Abstract: Federated learning, i.e., a mobile edge computing framework for deep\nlearning, is a recent advance in privacy-preserving machine learning, where the\nmodel is trained in a decentralized manner by the clients, i.e., data curators,\npreventing the server from directly accessing those private data from the\nclients. This learning mechanism significantly challenges the attack from the\nserver side. Although the state-of-the-art attacking techniques that\nincorporated the advance of Generative adversarial networks (GANs) could\nconstruct class representatives of the global data distribution among all\nclients, it is still challenging to distinguishably attack a specific client\n(i.e., user-level privacy leakage), which is a stronger privacy threat to\nprecisely recover the private data from a specific client. This paper gives the\nfirst attempt to explore user-level privacy leakage against the federated\nlearning by the attack from a malicious server. We propose a framework\nincorporating GAN with a multi-task discriminator, which simultaneously\ndiscriminates category, reality, and client identity of input samples. The\nnovel discrimination on client identity enables the generator to recover user\nspecified private data. Unlike existing works that tend to interfere the\ntraining process of the federated learning, the proposed method works\n\"invisibly\" on the server side. The experimental results demonstrate the\neffectiveness of the proposed attacking approach and the superior to the\nstate-of-the-art. \n\n"}
{"id": "1812.00558", "contents": "Title: Subregularity of subdifferential mappings relative to the critical set\n  and KL property of exponent 1/2 Abstract: For a proper extended real-valued function, this work focuses on the\nrelationship between the subregularity of its subdifferential mapping relative\nto the critical set and its KL property of exponent 1/2. When the function is\nlsc convex, we establish the equivalence between them under the continuous\nassumption on the critical set. Then, for the uniformly prox-regular function,\nunder its continuity on the local minimum set, the KL property of exponent 1/2\non the local minimum set is shown to be equivalent to the subregularity of its\nsubdifferential relative to this set. Moreover, for this class of nonconvex\nfunctions, under a separation assumption of stationary values, we show that the\nsubregularity of its subdifferential relative to the critical set also implies\nits KL property of exponent $1/2$. These results provide a bridge for the two\nkinds of regularity, and their application is illustrated by examples. \n\n"}
{"id": "1812.00572", "contents": "Title: Practical Window Setting Optimization for Medical Image Deep Learning Abstract: The recent advancements in deep learning have allowed for numerous\napplications in computed tomography (CT), with potential to improve diagnostic\naccuracy, speed of interpretation, and clinical efficiency. However, the deep\nlearning community has to date neglected window display settings - a key\nfeature of clinical CT interpretation and opportunity for additional\noptimization. Here we propose a window setting optimization (WSO) module that\nis fully trainable with convolutional neural networks (CNNs) to find optimal\nwindow settings for clinical performance. Our approach was inspired by the\nmethod commonly used by practicing radiologists to interpret CT images by\nadjusting window settings to increase the visualization of certain pathologies.\nOur approach provides optimal window ranges to enhance the conspicuity of\nabnormalities, and was used to enable performance enhancement for intracranial\nhemorrhage and urinary stone detection. On each task, the WSO model\noutperformed models trained over the full range of Hounsfield unit values in CT\nimages, as well as images windowed with pre-defined settings. The WSO module\ncan be readily applied to any analysis of CT images, and can be further\ngeneralized to tasks on other medical imaging modalities. \n\n"}
{"id": "1812.00812", "contents": "Title: Mapping Informal Settlements in Developing Countries with\n  Multi-resolution, Multi-spectral Data Abstract: Detecting and mapping informal settlements encompasses several of the United\nNations sustainable development goals. This is because informal settlements are\nhome to the most socially and economically vulnerable people on the planet.\nThus, understanding where these settlements are is of paramount importance to\nboth government and non-government organizations (NGOs), such as the United\nNations Children's Fund (UNICEF), who can use this information to deliver\neffective social and economic aid. We propose two effective methods for\ndetecting and mapping the locations of informal settlements. One uses only\nlow-resolution (LR), freely available, Sentinel-2 multispectral satellite\nimagery with noisy annotations, whilst the other is a deep learning approach\nthat uses only costly very-high-resolution (VHR) satellite imagery. To our\nknowledge, we are the first to map informal settlements successfully with\nlow-resolution satellite imagery. We extensively evaluate and compare the\nproposed methods. Please find additional material at\nhttps://frontierdevelopmentlab.github.io/informal-settlements/. \n\n"}
{"id": "1812.01228", "contents": "Title: Solving multi-resource allocation and location problems in disaster\n  management through linear programming Abstract: In this paper we propose a new efficient linear programming based approach\nfor multi-resource allocation and location problems in disaster management.\nSuch problems require an integer solution and therefore, in most cases, the\ncomputations rely on integer and mixed-integer linear programming solvers. In\ngeneral, these solvers can not handle large scaled problem. In this paper we\ndemonstrate that there exists a large class of disaster management problems\nwhose exact solutions can be obtained by applying the simplex method (linear\nprogramming). The results of numerical experiments are provided. Another\nimportant contribution of this paper is related to general cluster analysis and\nallocation. Namely, we demonstrate that the classical $k$-medoid clustering\nmethod can be implemented using linear programming techniques (simplex method)\nwithout relying on integer solvers. \n\n"}
{"id": "1812.01288", "contents": "Title: FaceFeat-GAN: a Two-Stage Approach for Identity-Preserving Face\n  Synthesis Abstract: The advance of Generative Adversarial Networks (GANs) enables realistic face\nimage synthesis. However, synthesizing face images that preserve facial\nidentity as well as have high diversity within each identity remains\nchallenging. To address this problem, we present FaceFeat-GAN, a novel\ngenerative model that improves both image quality and diversity by using two\nstages. Unlike existing single-stage models that map random noise to image\ndirectly, our two-stage synthesis includes the first stage of diverse feature\ngeneration and the second stage of feature-to-image rendering. The competitions\nbetween generators and discriminators are carefully designed in both stages\nwith different objective functions. Specially, in the first stage, they compete\nin the feature domain to synthesize various facial features rather than images.\nIn the second stage, they compete in the image domain to render photo-realistic\nimages that contain high diversity but preserve identity. Extensive experiments\nshow that FaceFeat-GAN generates images that not only retain identity\ninformation but also have high diversity and quality, significantly\noutperforming previous methods. \n\n"}
{"id": "1812.01675", "contents": "Title: Deep quench approximation and optimal control of general Cahn-Hilliard\n  systems with fractional operators and double obstacle potentials Abstract: The paper arXiv:1804.11290 contains well-posedness and regularity results for\na system of evolutionary operator equations having the structure of a\nCahn-Hilliard system. The operators appearing in the system equations were\nfractional versions in the spectral sense of general linear operators A and B\nhaving compact resolvents and are densely defined, unbounded, selfadjoint, and\nmonotone in a Hilbert space of functions defined in a smooth domain. The\nassociated double-well potentials driving the phase separation process modeled\nby the Cahn-Hilliard system could be of a very general type that includes\nstandard physically meaningful cases such as polynomial, logarithmic, and\ndouble obstacle nonlinearities. In the subsequent paper arXiv:1807.03218, an\nanalysis of distributed optimal control problems was performed for such\nevolutionary systems, where only the differentiable case of certain polynomial\nand logarithmic double-well potentials could be admitted. Results concerning\nexistence of optimizers and first-order necessary optimality conditions were\nderived. In the present paper, we complement these results by studying a\ndistributed control problem for such evolutionary systems in the case of\nnondifferentiable nonlinearities of double obstacle type. For such\nnonlinearities, it is well known that the standard constraint qualifications\ncannot be applied to construct appropriate Lagrange multipliers. To overcome\nthis difficulty, we follow here the so-called \"deep quench\" method. We first\ngive a general convergence analysis of the deep quench approximation that\nincludes an error estimate and then demonstrate that its use leads in the\ndouble obstacle case to appropriate first-order necessary optimality conditions\nin terms of a variational inequality and the associated adjoint state system. \n\n"}
{"id": "1812.02019", "contents": "Title: Dynamic Spatio-temporal Graph-based CNNs for Traffic Prediction Abstract: Forecasting future traffic flows from previous ones is a challenging problem\nbecause of their complex and dynamic nature of spatio-temporal structures. Most\nexisting graph-based CNNs attempt to capture the static relations while largely\nneglecting the dynamics underlying sequential data. In this paper, we present\ndynamic spatio-temporal graph-based CNNs (DST-GCNNs) by learning expressive\nfeatures to represent spatio-temporal structures and predict future traffic\nflows from surveillance video data. In particular, DST-GCNN is a two stream\nnetwork. In the flow prediction stream, we present a novel graph-based\nspatio-temporal convolutional layer to extract features from a graph\nrepresentation of traffic flows. Then several such layers are stacked together\nto predict future flows over time. Meanwhile, the relations between traffic\nflows in the graph are often time variant as the traffic condition changes over\ntime. To capture the graph dynamics, we use the graph prediction stream to\npredict the dynamic graph structures, and the predicted structures are fed into\nthe flow prediction stream. Experiments on real datasets demonstrate that the\nproposed model achieves competitive performances compared with the other\nstate-of-the-art methods. \n\n"}
{"id": "1812.02510", "contents": "Title: ForensicTransfer: Weakly-supervised Domain Adaptation for Forgery\n  Detection Abstract: Distinguishing manipulated from real images is becoming increasingly\ndifficult as new sophisticated image forgery approaches come out by the day.\nNaive classification approaches based on Convolutional Neural Networks (CNNs)\nshow excellent performance in detecting image manipulations when they are\ntrained on a specific forgery method. However, on examples from unseen\nmanipulation approaches, their performance drops significantly. To address this\nlimitation in transferability, we introduce Forensic-Transfer (FT). We devise a\nlearning-based forensic detector which adapts well to new domains, i.e., novel\nmanipulation methods and can handle scenarios where only a handful of fake\nexamples are available during training. To this end, we learn a forensic\nembedding based on a novel autoencoder-based architecture that can be used to\ndistinguish between real and fake imagery. The learned embedding acts as a form\nof anomaly detector; namely, an image manipulated from an unseen method will be\ndetected as fake provided it maps sufficiently far away from the cluster of\nreal images. Comparing to prior works, FT shows significant improvements in\ntransferability, which we demonstrate in a series of experiments on\ncutting-edge benchmarks. For instance, on unseen examples, we achieve up to 85%\nin terms of accuracy, and with only a handful of seen examples, our performance\nalready reaches around 95%. \n\n"}
{"id": "1812.02664", "contents": "Title: Recursive Visual Attention in Visual Dialog Abstract: Visual dialog is a challenging vision-language task, which requires the agent\nto answer multi-round questions about an image. It typically needs to address\ntwo major problems: (1) How to answer visually-grounded questions, which is the\ncore challenge in visual question answering (VQA); (2) How to infer the\nco-reference between questions and the dialog history. An example of visual\nco-reference is: pronouns (\\eg, ``they'') in the question (\\eg, ``Are they on\nor off?'') are linked with nouns (\\eg, ``lamps'') appearing in the dialog\nhistory (\\eg, ``How many lamps are there?'') and the object grounded in the\nimage. In this work, to resolve the visual co-reference for visual dialog, we\npropose a novel attention mechanism called Recursive Visual Attention (RvA).\nSpecifically, our dialog agent browses the dialog history until the agent has\nsufficient confidence in the visual co-reference resolution, and refines the\nvisual attention recursively. The quantitative and qualitative experimental\nresults on the large-scale VisDial v0.9 and v1.0 datasets demonstrate that the\nproposed RvA not only outperforms the state-of-the-art methods, but also\nachieves reasonable recursion and interpretable attention maps without\nadditional annotations. The code is available at\n\\url{https://github.com/yuleiniu/rva}. \n\n"}
{"id": "1812.02897", "contents": "Title: Improved Search Strategies with Application to Estimating Facial\n  Blendshape Parameters Abstract: It is well known that popular optimization techniques can lead to overfitting\nor even a lack of convergence altogether; thus, practitioners often utilize ad\nhoc regularization terms added to the energy functional. When carefully\ncrafted, these regularizations can produce compelling results. However,\nregularization changes both the energy landscape and the solution to the\noptimization problem, which can result in underfitting. Surprisingly, many\npractitioners both add regularization and claim that their model lacks the\nexpressivity to fit the data. Motivated by a geometric interpretation of the\nlinearized search space, we propose an approach that ameliorates overfitting\nwithout the need for regularization terms that restrict the expressiveness of\nthe underlying model. We illustrate the efficacy of our approach on\nminimization problems related to three-dimensional facial expression estimation\nwhere overfitting clouds semantic understanding and regularization may lead to\nunderfitting that misses or misinterprets subtle expressions. \n\n"}
{"id": "1812.03050", "contents": "Title: Graph Cut Segmentation Methods Revisited with a Quantum Algorithm Abstract: The design and performance of computer vision algorithms are greatly\ninfluenced by the hardware on which they are implemented. CPUs, multi-core\nCPUs, FPGAs and GPUs have inspired new algorithms and enabled existing ideas to\nbe realized. This is notably the case with GPUs, which has significantly\nchanged the landscape of computer vision research through deep learning. As the\nend of Moores law approaches, researchers and hardware manufacturers are\nexploring alternative hardware computing paradigms. Quantum computers are a\nvery promising alternative and offer polynomial or even exponential speed-ups\nover conventional computing for some problems. This paper presents a novel\napproach to image segmentation that uses new quantum computing hardware.\nSegmentation is formulated as a graph cut problem that can be mapped to the\nquantum approximate optimization algorithm (QAOA). This algorithm can be\nimplemented on current and near-term quantum computers. Encouraging results are\npresented on artificial and medical imaging data. This represents an important,\npractical step towards leveraging quantum computers for computer vision. \n\n"}
{"id": "1812.05262", "contents": "Title: ELASTIC: Improving CNNs with Dynamic Scaling Policies Abstract: Scale variation has been a challenge from traditional to modern approaches in\ncomputer vision. Most solutions to scale issues have a similar theme: a set of\nintuitive and manually designed policies that are generic and fixed (e.g. SIFT\nor feature pyramid). We argue that the scaling policy should be learned from\ndata. In this paper, we introduce ELASTIC, a simple, efficient and yet very\neffective approach to learn a dynamic scale policy from data. We formulate the\nscaling policy as a non-linear function inside the network's structure that (a)\nis learned from data, (b) is instance specific, (c) does not add extra\ncomputation, and (d) can be applied on any network architecture. We applied\nELASTIC to several state-of-the-art network architectures and showed consistent\nimprovement without extra (sometimes even lower) computation on ImageNet\nclassification, MSCOCO multi-label classification, and PASCAL VOC semantic\nsegmentation. Our results show major improvement for images with scale\nchallenges. Our code is available here: https://github.com/allenai/elastic \n\n"}
{"id": "1812.06271", "contents": "Title: PVSNet: Palm Vein Authentication Siamese Network Trained using Triplet\n  Loss and Adaptive Hard Mining by Learning Enforced Domain Specific Features Abstract: Designing an end-to-end deep learning network to match the biometric features\nwith limited training samples is an extremely challenging task. To address this\nproblem, we propose a new way to design an end-to-end deep CNN framework i.e.,\nPVSNet that works in two major steps: first, an encoder-decoder network is used\nto learn generative domain-specific features followed by a Siamese network in\nwhich convolutional layers are pre-trained in an unsupervised fashion as an\nautoencoder. The proposed model is trained via triplet loss function that is\nadjusted for learning feature embeddings in a way that minimizes the distance\nbetween embedding-pairs from the same subject and maximizes the distance with\nthose from different subjects, with a margin. In particular, a triplet Siamese\nmatching network using an adaptive margin based hard negative mining has been\nsuggested. The hyper-parameters associated with the training strategy, like the\nadaptive margin, have been tuned to make the learning more effective on\nbiometric datasets. In extensive experimentation, the proposed network\noutperforms most of the existing deep learning solutions on three type of\ntypical vein datasets which clearly demonstrates the effectiveness of our\nproposed method. \n\n"}
{"id": "1812.07003", "contents": "Title: 3D-SIS: 3D Semantic Instance Segmentation of RGB-D Scans Abstract: We introduce 3D-SIS, a novel neural network architecture for 3D semantic\ninstance segmentation in commodity RGB-D scans. The core idea of our method is\nto jointly learn from both geometric and color signal, thus enabling accurate\ninstance predictions. Rather than operate solely on 2D frames, we observe that\nmost computer vision applications have multi-view RGB-D input available, which\nwe leverage to construct an approach for 3D instance segmentation that\neffectively fuses together these multi-modal inputs. Our network leverages\nhigh-resolution RGB input by associating 2D images with the volumetric grid\nbased on the pose alignment of the 3D reconstruction. For each image, we first\nextract 2D features for each pixel with a series of 2D convolutions; we then\nbackproject the resulting feature vector to the associated voxel in the 3D\ngrid. This combination of 2D and 3D feature learning allows significantly\nhigher accuracy object detection and instance segmentation than\nstate-of-the-art alternatives. We show results on both synthetic and real-world\npublic benchmarks, achieving an improvement in mAP of over 13 on real-world\ndata. \n\n"}
{"id": "1812.07032", "contents": "Title: Boundary loss for highly unbalanced segmentation Abstract: Widely used loss functions for CNN segmentation, e.g., Dice or cross-entropy,\nare based on integrals over the segmentation regions. Unfortunately, for highly\nunbalanced segmentations, such regional summations have values that differ by\nseveral orders of magnitude across classes, which affects training performance\nand stability. We propose a boundary loss, which takes the form of a distance\nmetric on the space of contours, not regions. This can mitigate the\ndifficulties of highly unbalanced problems because it uses integrals over the\ninterface between regions instead of unbalanced integrals over the regions.\nFurthermore, a boundary loss complements regional information. Inspired by\ngraph-based optimization techniques for computing active-contour flows, we\nexpress a non-symmetric $L_2$ distance on the space of contours as a regional\nintegral, which avoids completely local differential computations involving\ncontour points. This yields a boundary loss expressed with the regional softmax\nprobability outputs of the network, which can be easily combined with standard\nregional losses and implemented with any existing deep network architecture for\nN-D segmentation. We report comprehensive evaluations and comparisons on\ndifferent unbalanced problems, showing that our boundary loss can yield\nsignificant increases in performances while improving training stability. Our\ncode is publicly available: https://github.com/LIVIAETS/surface-loss . \n\n"}
{"id": "1812.07742", "contents": "Title: Cross-Database Micro-Expression Recognition: A Benchmark Abstract: Cross-database micro-expression recognition (CDMER) is one of recently\nemerging and interesting problem in micro-expression analysis. CDMER is more\nchallenging than the conventional micro-expression recognition (MER), because\nthe training and testing samples in CDMER come from different micro-expression\ndatabases, resulting in the inconsistency of the feature distributions between\nthe training and testing sets. In this paper, we contribute to this topic from\nthree aspects. First, we establish a CDMER experimental evaluation protocol\naiming to allow the researchers to conveniently work on this topic and provide\na standard platform for evaluating their proposed methods. Second, we conduct\nbenchmark experiments by using NINE state-of-the-art domain adaptation (DA)\nmethods and SIX popular spatiotemporal descriptors for respectively\ninvestigating CDMER problem from two different perspectives. Third, we propose\na novel DA method called region selective transfer regression (RSTR) to deal\nwith the CDMER task. Our RSTR takes advantage of one important cue for\nrecognizing micro-expressions, i.e., the different contributions of the facial\nlocal regions in MER. The overall superior performance of RSTR demonstrates\nthat taking into consideration the important cues benefiting MER, e.g., the\nfacial local region information, contributes to develop effective DA methods\nfor dealing with CDMER problem. \n\n"}
{"id": "1812.07809", "contents": "Title: Found in Translation: Learning Robust Joint Representations by Cyclic\n  Translations Between Modalities Abstract: Multimodal sentiment analysis is a core research area that studies speaker\nsentiment expressed from the language, visual, and acoustic modalities. The\ncentral challenge in multimodal learning involves inferring joint\nrepresentations that can process and relate information from these modalities.\nHowever, existing work learns joint representations by requiring all modalities\nas input and as a result, the learned representations may be sensitive to noisy\nor missing modalities at test time. With the recent success of sequence to\nsequence (Seq2Seq) models in machine translation, there is an opportunity to\nexplore new ways of learning joint representations that may not require all\ninput modalities at test time. In this paper, we propose a method to learn\nrobust joint representations by translating between modalities. Our method is\nbased on the key insight that translation from a source to a target modality\nprovides a method of learning joint representations using only the source\nmodality as input. We augment modality translations with a cycle consistency\nloss to ensure that our joint representations retain maximal information from\nall modalities. Once our translation model is trained with paired multimodal\ndata, we only need data from the source modality at test time for final\nsentiment prediction. This ensures that our model remains robust from\nperturbations or missing information in the other modalities. We train our\nmodel with a coupled translation-prediction objective and it achieves new\nstate-of-the-art results on multimodal sentiment analysis datasets: CMU-MOSI,\nICT-MMMO, and YouTube. Additional experiments show that our model learns\nincreasingly discriminative joint representations with more input modalities\nwhile maintaining robustness to missing or perturbed modalities. \n\n"}
{"id": "1812.07907", "contents": "Title: PnP-AdaNet: Plug-and-Play Adversarial Domain Adaptation Network with a\n  Benchmark at Cross-modality Cardiac Segmentation Abstract: Deep convolutional networks have demonstrated the state-of-the-art\nperformance on various medical image computing tasks. Leveraging images from\ndifferent modalities for the same analysis task holds clinical benefits.\nHowever, the generalization capability of deep models on test data with\ndifferent distributions remain as a major challenge. In this paper, we propose\nthe PnPAdaNet (plug-and-play adversarial domain adaptation network) for\nadapting segmentation networks between different modalities of medical images,\ne.g., MRI and CT. We propose to tackle the significant domain shift by aligning\nthe feature spaces of source and target domains in an unsupervised manner.\nSpecifically, a domain adaptation module flexibly replaces the early encoder\nlayers of the source network, and the higher layers are shared between domains.\nWith adversarial learning, we build two discriminators whose inputs are\nrespectively multi-level features and predicted segmentation masks. We have\nvalidated our domain adaptation method on cardiac structure segmentation in\nunpaired MRI and CT. The experimental results with comprehensive ablation\nstudies demonstrate the excellent efficacy of our proposed PnP-AdaNet.\nMoreover, we introduce a novel benchmark on the cardiac dataset for the task of\nunsupervised cross-modality domain adaptation. We will make our code and\ndatabase publicly available, aiming to promote future studies on this\nchallenging yet important research topic in medical imaging. \n\n"}
{"id": "1812.08094", "contents": "Title: Shallow Cue Guided Deep Visual Tracking via Mixed Models Abstract: In this paper, a robust visual tracking approach via mixed model based\nconvolutional neural networks (SDT) is developed. In order to handle abrupt or\nfast motion, a prior map is generated to facilitate the localization of region\nof interest (ROI) before the deep tracker is performed. A top-down saliency\nmodel with nineteen shallow cues are employed to construct the prior map with\nonline learnt combination weights. Moreover, apart from a holistic deep\nlearner, four local networks are also trained to learn different components of\nthe target. The generated four local heat maps will facilitate to rectify the\nholistic map by eliminating the distracters to avoid drifting. Furthermore, to\nguarantee the instance for online update of high quality, a prioritised update\nstrategy is implemented by casting the problem into a label noise problem. The\nselection probability is designed by considering both confidence values and\nbio-inspired memory for temporal information integration. Experiments are\nconducted qualitatively and quantitatively on a set of challenging image\nsequences. Comparative study demonstrates that the proposed algorithm\noutperforms other state-of-the-art methods. \n\n"}
{"id": "1812.08789", "contents": "Title: Steerable $e$PCA: Rotationally Invariant Exponential Family PCA Abstract: In photon-limited imaging, the pixel intensities are affected by photon count\nnoise. Many applications, such as 3-D reconstruction using correlation analysis\nin X-ray free electron laser (XFEL) single molecule imaging, require an\naccurate estimation of the covariance of the underlying 2-D clean images.\nAccurate estimation of the covariance from low-photon count images must take\ninto account that pixel intensities are Poisson distributed, hence the\nclassical sample covariance estimator is sub-optimal. Moreover, in single\nmolecule imaging, including in-plane rotated copies of all images could further\nimprove the accuracy of covariance estimation. In this paper we introduce an\nefficient and accurate algorithm for covariance matrix estimation of count\nnoise 2-D images, including their uniform planar rotations and possibly\nreflections. Our procedure, steerable $e$PCA, combines in a novel way two\nrecently introduced innovations. The first is a methodology for principal\ncomponent analysis (PCA) for Poisson distributions, and more generally,\nexponential family distributions, called $e$PCA. The second is steerable PCA, a\nfast and accurate procedure for including all planar rotations for PCA. The\nresulting principal components are invariant to the rotation and reflection of\nthe input images. We demonstrate the efficiency and accuracy of steerable\n$e$PCA in numerical experiments involving simulated XFEL datasets and rotated\nYale B face data. \n\n"}
{"id": "1812.09737", "contents": "Title: End-to-end Learning for Graph Decomposition Abstract: We propose a novel end-to-end trainable framework for the graph decomposition\nproblem. The minimum cost multicut problem is first converted to an\nunconstrained binary cubic formulation where cycle consistency constraints are\nincorporated into the objective function. The new optimization problem can be\nviewed as a Conditional Random Field (CRF) in which the random variables are\nassociated with the binary edge labels of the initial graph and the hard\nconstraints are introduced in the CRF as high-order potentials. The parameters\nof a standard Neural Network and the fully differentiable CRF are optimized in\nan end-to-end manner. Furthermore, our method utilizes the cycle constraints as\nmeta-supervisory signals during the learning of the deep feature\nrepresentations by taking the dependencies between the output random variables\ninto account. We present analyses of the end-to-end learned representations,\nshowing the impact of the joint training, on the task of clustering images of\nMNIST. We also validate the effectiveness of our approach both for the feature\nlearning and the final clustering on the challenging task of real-world\nmulti-person pose estimation. \n\n"}
{"id": "1812.10217", "contents": "Title: Seeing isn't Believing: Practical Adversarial Attack Against Object\n  Detectors Abstract: In this paper, we presented systematic solutions to build robust and\npractical AEs against real world object detectors. Particularly, for Hiding\nAttack (HA), we proposed the feature-interference reinforcement (FIR) method\nand the enhanced realistic constraints generation (ERG) to enhance robustness,\nand for Appearing Attack (AA), we proposed the nested-AE, which combines two\nAEs together to attack object detectors in both long and short distance. We\nalso designed diverse styles of AEs to make AA more surreptitious. Evaluation\nresults show that our AEs can attack the state-of-the-art real-time object\ndetectors (i.e., YOLO V3 and faster-RCNN) at the success rate up to 92.4% with\nvarying distance from 1m to 25m and angles from -60{\\deg} to 60{\\deg}. Our AEs\nare also demonstrated to be highly transferable, capable of attacking another\nthree state-of-the-art black-box models with high success rate. \n\n"}
{"id": "1812.10366", "contents": "Title: A Poisson-Gaussian Denoising Dataset with Real Fluorescence Microscopy\n  Images Abstract: Fluorescence microscopy has enabled a dramatic development in modern biology.\nDue to its inherently weak signal, fluorescence microscopy is not only much\nnoisier than photography, but also presented with Poisson-Gaussian noise where\nPoisson noise, or shot noise, is the dominating noise source. To get clean\nfluorescence microscopy images, it is highly desirable to have effective\ndenoising algorithms and datasets that are specifically designed to denoise\nfluorescence microscopy images. While such algorithms exist, no such datasets\nare available. In this paper, we fill this gap by constructing a dataset - the\nFluorescence Microscopy Denoising (FMD) dataset - that is dedicated to\nPoisson-Gaussian denoising. The dataset consists of 12,000 real fluorescence\nmicroscopy images obtained with commercial confocal, two-photon, and wide-field\nmicroscopes and representative biological samples such as cells, zebrafish, and\nmouse brain tissues. We use image averaging to effectively obtain ground truth\nimages and 60,000 noisy images with different noise levels. We use this dataset\nto benchmark 10 representative denoising algorithms and find that deep learning\nmethods have the best performance. To our knowledge, this is the first real\nmicroscopy image dataset for Poisson-Gaussian denoising purposes and it could\nbe an important tool for high-quality, real-time denoising applications in\nbiomedical research. \n\n"}
{"id": "1812.10587", "contents": "Title: Learning Dynamic Generator Model by Alternating Back-Propagation Through\n  Time Abstract: This paper studies the dynamic generator model for spatial-temporal processes\nsuch as dynamic textures and action sequences in video data. In this model,\neach time frame of the video sequence is generated by a generator model, which\nis a non-linear transformation of a latent state vector, where the non-linear\ntransformation is parametrized by a top-down neural network. The sequence of\nlatent state vectors follows a non-linear auto-regressive model, where the\nstate vector of the next frame is a non-linear transformation of the state\nvector of the current frame as well as an independent noise vector that\nprovides randomness in the transition. The non-linear transformation of this\ntransition model can be parametrized by a feedforward neural network. We show\nthat this model can be learned by an alternating back-propagation through time\nalgorithm that iteratively samples the noise vectors and updates the parameters\nin the transition model and the generator model. We show that our training\nmethod can learn realistic models for dynamic textures and action patterns. \n\n"}
{"id": "1812.10592", "contents": "Title: Eyes on the Prize: Improved Biological Surface Registration via Forward\n  Propagation Abstract: Many algorithms for surface registration risk producing significant errors if\nsurfaces are significantly nonisometric. Manifold learning has been shown to be\neffective at improving registration quality, using information from an entire\ncollection of surfaces to correct issues present in pairwise registrations.\nThese methods, however, are not robust to changes in the collection of\nsurfaces, or do not produce accurate registrations at a resolution high enough\nfor subsequent downstream analysis. We propose a novel algorithm for\nefficiently registering such collections given initial correspondences with\nvarying degrees of accuracy. By combining the initial information with recent\ndevelopments in manifold learning, we employ a simple metric condition to\nconstruct a measure on the space of correspondences between any pair of shapes\nin our collection, which we then use to distill soft correspondences. We\ndemonstrate that this measure can improve correspondence accuracy between\nfeature points compared to currently employed, less robust methods on a diverse\ndataset of surfaces from evolutionary biology. We then show how our methods can\nbe used, in combination with recent sampling and interpolation methods, to\ncompute accurate and consistent homeomorphisms between surfaces. \n\n"}
{"id": "1812.11317", "contents": "Title: Support Vector Guided Softmax Loss for Face Recognition Abstract: Face recognition has witnessed significant progresses due to the advances of\ndeep convolutional neural networks (CNNs), the central challenge of which, is\nfeature discrimination. To address it, one group tries to exploit mining-based\nstrategies (\\textit{e.g.}, hard example mining and focal loss) to focus on the\ninformative examples. The other group devotes to designing margin-based loss\nfunctions (\\textit{e.g.}, angular, additive and additive angular margins) to\nincrease the feature margin from the perspective of ground truth class. Both of\nthem have been well-verified to learn discriminative features. However, they\nsuffer from either the ambiguity of hard examples or the lack of discriminative\npower of other classes. In this paper, we design a novel loss function, namely\nsupport vector guided softmax loss (SV-Softmax), which adaptively emphasizes\nthe mis-classified points (support vectors) to guide the discriminative\nfeatures learning. So the developed SV-Softmax loss is able to eliminate the\nambiguity of hard examples as well as absorb the discriminative power of other\nclasses, and thus results in more discrimiantive features. To the best of our\nknowledge, this is the first attempt to inherit the advantages of mining-based\nand margin-based losses into one framework. Experimental results on several\nbenchmarks have demonstrated the effectiveness of our approach over\nstate-of-the-arts. \n\n"}
{"id": "1812.11842", "contents": "Title: Do GANs leave artificial fingerprints? Abstract: In the last few years, generative adversarial networks (GAN) have shown\ntremendous potential for a number of applications in computer vision and\nrelated fields. With the current pace of progress, it is a sure bet they will\nsoon be able to generate high-quality images and videos, virtually\nindistinguishable from real ones. Unfortunately, realistic GAN-generated images\npose serious threats to security, to begin with a possible flood of fake\nmultimedia, and multimedia forensic countermeasures are in urgent need. In this\nwork, we show that each GAN leaves its specific fingerprint in the images it\ngenerates, just like real-world cameras mark acquired images with traces of\ntheir photo-response non-uniformity pattern. Source identification experiments\nwith several popular GANs show such fingerprints to represent a precious asset\nfor forensic analyses. \n\n"}
{"id": "1901.00063", "contents": "Title: Extreme Relative Pose Estimation for RGB-D Scans via Scene Completion Abstract: Estimating the relative rigid pose between two RGB-D scans of the same\nunderlying environment is a fundamental problem in computer vision, robotics,\nand computer graphics. Most existing approaches allow only limited maximum\nrelative pose changes since they require considerable overlap between the input\nscans. We introduce a novel deep neural network that extends the scope to\nextreme relative poses, with little or even no overlap between the input scans.\nThe key idea is to infer more complete scene information about the underlying\nenvironment and match on the completed scans. In particular, instead of only\nperforming scene completion from each individual scan, our approach alternates\nbetween relative pose estimation and scene completion. This allows us to\nperform scene completion by utilizing information from both input scans at late\niterations, resulting in better results for both scene completion and relative\npose estimation. Experimental results on benchmark datasets show that our\napproach leads to considerable improvements over state-of-the-art approaches\nfor relative pose estimation. In particular, our approach provides encouraging\nrelative pose estimates even between non-overlapping scans. \n\n"}
{"id": "1901.00616", "contents": "Title: Volumetric Convolution: Automatic Representation Learning in Unit Ball Abstract: Convolution is an efficient technique to obtain abstract feature\nrepresentations using hierarchical layers in deep networks. Although performing\nconvolution in Euclidean geometries is fairly straightforward, its extension to\nother topological spaces---such as a sphere ($\\mathbb{S}^2$) or a unit ball\n($\\mathbb{B}^3$)---entails unique challenges. In this work, we propose a novel\n`\\emph{volumetric convolution}' operation that can effectively convolve\narbitrary functions in $\\mathbb{B}^3$. We develop a theoretical framework for\n\\emph{volumetric convolution} based on Zernike polynomials and efficiently\nimplement it as a differentiable and an easily pluggable layer for deep\nnetworks. Furthermore, our formulation leads to derivation of a novel formula\nto measure the symmetry of a function in $\\mathbb{B}^3$ around an arbitrary\naxis, that is useful in 3D shape analysis tasks. We demonstrate the efficacy of\nproposed volumetric convolution operation on a possible use-case i.e., 3D\nobject recognition task. \n\n"}
{"id": "1901.01021", "contents": "Title: Transformed $\\ell_1$ Regularization for Learning Sparse Deep Neural\n  Networks Abstract: Deep neural networks (DNNs) have achieved extraordinary success in numerous\nareas. However, to attain this success, DNNs often carry a large number of\nweight parameters, leading to heavy costs of memory and computation resources.\nOverfitting is also likely to happen in such network when the training data are\ninsufficient. These shortcomings severely hinder the application of DNNs in\nresource-constrained platforms. In fact, many network weights are known to be\nredundant and can be removed from the network without much loss of performance.\nTo this end, we introduce a new non-convex integrated transformed $\\ell_1$\nregularizer to promote sparsity for DNNs, which removes both redundant\nconnections and unnecessary neurons simultaneously. To be specific, we apply\nthe transformed $\\ell_1$ to the matrix space of network weights and utilize it\nto remove redundant connections. Besides, group sparsity is also employed as an\nauxiliary to remove unnecessary neurons. An efficient stochastic proximal\ngradient algorithm is presented to solve the new model at the same time. To the\nbest of our knowledge, this is the first work to utilize a non-convex\nregularizer in sparse optimization based method to promote sparsity for DNNs.\nExperiments on several public datasets demonstrate the effectiveness of the\nproposed method. \n\n"}
{"id": "1901.01535", "contents": "Title: RayNet: Learning Volumetric 3D Reconstruction with Ray Potentials Abstract: In this paper, we consider the problem of reconstructing a dense 3D model\nusing images captured from different views. Recent methods based on\nconvolutional neural networks (CNN) allow learning the entire task from data.\nHowever, they do not incorporate the physics of image formation such as\nperspective geometry and occlusion. Instead, classical approaches based on\nMarkov Random Fields (MRF) with ray-potentials explicitly model these physical\nprocesses, but they cannot cope with large surface appearance variations across\ndifferent viewpoints. In this paper, we propose RayNet, which combines the\nstrengths of both frameworks. RayNet integrates a CNN that learns\nview-invariant feature representations with an MRF that explicitly encodes the\nphysics of perspective projection and occlusion. We train RayNet end-to-end\nusing empirical risk minimization. We thoroughly evaluate our approach on\nchallenging real-world datasets and demonstrate its benefits over a piece-wise\ntrained baseline, hand-crafted models as well as other learning-based\napproaches. \n\n"}
{"id": "1901.01570", "contents": "Title: Transductive Zero-Shot Learning with Visual Structure Constraint Abstract: To recognize objects of the unseen classes, most existing Zero-Shot\nLearning(ZSL) methods first learn a compatible projection function between the\ncommon semantic space and the visual space based on the data of source seen\nclasses, then directly apply it to the target unseen classes. However, in real\nscenarios, the data distribution between the source and target domain might not\nmatch well, thus causing the well-known \\textbf{domain shift} problem. Based on\nthe observation that visual features of test instances can be separated into\ndifferent clusters, we propose a new visual structure constraint on class\ncenters for transductive ZSL, to improve the generality of the projection\nfunction (i.e. alleviate the above domain shift problem). Specifically, three\ndifferent strategies (symmetric Chamfer-distance, Bipartite matching distance,\nand Wasserstein distance) are adopted to align the projected unseen semantic\ncenters and visual cluster centers of test instances. We also propose a new\ntraining strategy to handle the real cases where many unrelated images exist in\nthe test dataset, which is not considered in previous methods. Experiments on\nmany widely used datasets demonstrate that the proposed visual structure\nconstraint can bring substantial performance gain consistently and achieve\nstate-of-the-art results. The source code is available at\n\\url{https://github.com/raywzy/VSC}. \n\n"}
{"id": "1901.01939", "contents": "Title: GASL: Guided Attention for Sparsity Learning in Deep Neural Networks Abstract: The main goal of network pruning is imposing sparsity on the neural network\nby increasing the number of parameters with zero value in order to reduce the\narchitecture size and the computational speedup. In most of the previous\nresearch works, sparsity is imposed stochastically without considering any\nprior knowledge of the weights distribution or other internal network\ncharacteristics. Enforcing too much sparsity may induce accuracy drop due to\nthe fact that a lot of important elements might have been eliminated. In this\npaper, we propose Guided Attention for Sparsity Learning (GASL) to achieve (1)\nmodel compression by having less number of elements and speed-up; (2) prevent\nthe accuracy drop by supervising the sparsity operation via a guided attention\nmechanism and (3) introduce a generic mechanism that can be adapted for any\ntype of architecture; Our work is aimed at providing a framework based on\ninterpretable attention mechanisms for imposing structured and non-structured\nsparsity in deep neural networks. For Cifar-100 experiments, we achieved the\nstate-of-the-art sparsity level and 2.91x speedup with competitive accuracy\ncompared to the best method. For MNIST and LeNet architecture we also achieved\nthe highest sparsity and speedup level. \n\n"}
{"id": "1901.02446", "contents": "Title: Panoptic Feature Pyramid Networks Abstract: The recently introduced panoptic segmentation task has renewed our\ncommunity's interest in unifying the tasks of instance segmentation (for thing\nclasses) and semantic segmentation (for stuff classes). However, current\nstate-of-the-art methods for this joint task use separate and dissimilar\nnetworks for instance and semantic segmentation, without performing any shared\ncomputation. In this work, we aim to unify these methods at the architectural\nlevel, designing a single network for both tasks. Our approach is to endow Mask\nR-CNN, a popular instance segmentation method, with a semantic segmentation\nbranch using a shared Feature Pyramid Network (FPN) backbone. Surprisingly,\nthis simple baseline not only remains effective for instance segmentation, but\nalso yields a lightweight, top-performing method for semantic segmentation. In\nthis work, we perform a detailed study of this minimally extended version of\nMask R-CNN with FPN, which we refer to as Panoptic FPN, and show it is a robust\nand accurate baseline for both tasks. Given its effectiveness and conceptual\nsimplicity, we hope our method can serve as a strong baseline and aid future\nresearch in panoptic segmentation. \n\n"}
{"id": "1901.02596", "contents": "Title: MSR: Multi-Scale Shape Regression for Scene Text Detection Abstract: State-of-the-art scene text detection techniques predict quadrilateral boxes\nthat are prone to localization errors while dealing with straight or curved\ntext lines of different orientations and lengths in scenes. This paper presents\na novel multi-scale shape regression network (MSR) that is capable of locating\ntext lines of different lengths, shapes and curvatures in scenes. The proposed\nMSR detects scene texts by predicting dense text boundary points that\ninherently capture the location and shape of text lines accurately and are also\nmore tolerant to the variation of text line length as compared with the state\nof the arts using proposals or segmentation. Additionally, the multi-scale\nnetwork extracts and fuses features at different scales which demonstrates\nsuperb tolerance to the text scale variation. Extensive experiments over\nseveral public datasets show that the proposed MSR obtains superior detection\nperformance for both curved and straight text lines of different lengths and\norientations. \n\n"}
{"id": "1901.03762", "contents": "Title: Using Scene Graph Context to Improve Image Generation Abstract: Generating realistic images from scene graphs asks neural networks to be able\nto reason about object relationships and compositionality. As a relatively new\ntask, how to properly ensure the generated images comply with scene graphs or\nhow to measure task performance remains an open question. In this paper, we\npropose to harness scene graph context to improve image generation from scene\ngraphs. We introduce a scene graph context network that pools features\ngenerated by a graph convolutional neural network that are then provided to\nboth the image generation network and the adversarial loss. With the context\nnetwork, our model is trained to not only generate realistic looking images,\nbut also to better preserve non-spatial object relationships. We also define\ntwo novel evaluation metrics, the relation score and the mean opinion relation\nscore, for this task that directly evaluate scene graph compliance. We use both\nquantitative and qualitative studies to demonstrate that our pro-posed model\noutperforms the state-of-the-art on this challenging task. \n\n"}
{"id": "1901.05686", "contents": "Title: Image Enhancement Network Trained by Using HDR images Abstract: In this paper, a novel image enhancement network is proposed, where HDR\nimages are used for generating training data for our network. Most of\nconventional image enhancement methods, including Retinex based methods, do not\ntake into account restoring lost pixel values caused by clipping and\nquantizing. In addition, recently proposed CNN based methods still have a\nlimited scope of application or a limited performance, due to network\narchitectures. In contrast, the proposed method have a higher performance and a\nsimpler network architecture than existing CNN based methods. Moreover, the\nproposed method enables us to restore lost pixel values. Experimental results\nshow that the proposed method can provides higher-quality images than\nconventional image enhancement methods including a CNN based method, in terms\nof TMQI and NIQE. \n\n"}
{"id": "1901.06010", "contents": "Title: Degrees of Freedom Region of the $(M,N_1,N_2)$ MIMO Broadcast Channel\n  with Partial CSIT: An Application of Sum-set Inequalities Based on Aligned\n  Image Sets Abstract: The degrees of freedom (DoF) region is characterized for the $2$-user\nmultiple input multiple output (MIMO) broadcast channel (BC), where the\ntransmitter is equipped with $M$ antennas, the two receivers are equipped with\n$N_1$ and $N_2$ antennas, and the levels of channel state information at the\ntransmitter (CSIT) for the two users are parameterized by $\\beta_1, \\beta_2$,\nrespectively. The achievability of the DoF region was established by Hao,\nRassouli and Clerckx, but no proof of optimality was heretofore available. The\nproof of optimality is provided in this work with the aid of sum-set\ninequalities based on the aligned image sets (AIS) approach. \n\n"}
{"id": "1901.06013", "contents": "Title: FARSA: Fully Automated Roadway Safety Assessment Abstract: This paper addresses the task of road safety assessment. An emerging approach\nfor conducting such assessments in the United States is through the US Road\nAssessment Program (usRAP), which rates roads from highest risk (1 star) to\nlowest (5 stars). Obtaining these ratings requires manual, fine-grained\nlabeling of roadway features in street-level panoramas, a slow and costly\nprocess. We propose to automate this process using a deep convolutional neural\nnetwork that directly estimates the star rating from a street-level panorama,\nrequiring milliseconds per image at test time. Our network also estimates many\nother road-level attributes, including curvature, roadside hazards, and the\ntype of median. To support this, we incorporate task-specific attention layers\nso the network can focus on the panorama regions that are most useful for a\nparticular task. We evaluated our approach on a large dataset of real-world\nimages from two US states. We found that incorporating additional tasks, and\nusing a semi-supervised training approach, significantly reduced overfitting\nproblems, allowed us to optimize more layers of the network, and resulted in\nhigher accuracy. \n\n"}
{"id": "1901.06111", "contents": "Title: CRDN: Cascaded Residual Dense Networks for Dynamic MR Imaging with\n  Edge-enhanced Loss Constraint Abstract: Dynamic magnetic resonance (MR) imaging has generated great research\ninterest, as it can provide both spatial and temporal information for clinical\ndiagnosis. However, slow imaging speed or long scanning time is still one of\nthe challenges for dynamic MR imaging. Most existing methods reconstruct\nDynamic MR images from incomplete k-space data under the guidance of compressed\nsensing (CS) or low rank theory, which suffer from long iterative\nreconstruction time. Recently, deep learning has shown great potential in\naccelerating dynamic MR. Our previous work proposed a dynamic MR imaging method\nwith both k-space and spatial prior knowledge integrated via multi-supervised\nnetwork training. Nevertheless, there was still a certain degree of smooth in\nthe reconstructed images at high acceleration factors. In this work, we propose\ncascaded residual dense networks for dynamic MR imaging with edge-enhance loss\nconstraint, dubbed as CRDN. Specifically, the cascaded residual dense networks\nfully exploit the hierarchical features from all the convolutional layers with\nboth local and global feature fusion. We further utilize the total variation\n(TV) loss function, which has the edge enhancement properties, for training the\nnetworks. \n\n"}
{"id": "1901.06706", "contents": "Title: Visual Entailment: A Novel Task for Fine-Grained Image Understanding Abstract: Existing visual reasoning datasets such as Visual Question Answering (VQA),\noften suffer from biases conditioned on the question, image or answer\ndistributions. The recently proposed CLEVR dataset addresses these limitations\nand requires fine-grained reasoning but the dataset is synthetic and consists\nof similar objects and sentence structures across the dataset.\n  In this paper, we introduce a new inference task, Visual Entailment (VE) -\nconsisting of image-sentence pairs whereby a premise is defined by an image,\nrather than a natural language sentence as in traditional Textual Entailment\ntasks. The goal of a trained VE model is to predict whether the image\nsemantically entails the text. To realize this task, we build a dataset SNLI-VE\nbased on the Stanford Natural Language Inference corpus and Flickr30k dataset.\nWe evaluate various existing VQA baselines and build a model called Explainable\nVisual Entailment (EVE) system to address the VE task. EVE achieves up to 71%\naccuracy and outperforms several other state-of-the-art VQA based models.\nFinally, we demonstrate the explainability of EVE through cross-modal attention\nvisualizations. The SNLI-VE dataset is publicly available at\nhttps://github.com/ necla-ml/SNLI-VE. \n\n"}
{"id": "1901.07821", "contents": "Title: Rethinking Lossy Compression: The Rate-Distortion-Perception Tradeoff Abstract: Lossy compression algorithms are typically designed and analyzed through the\nlens of Shannon's rate-distortion theory, where the goal is to achieve the\nlowest possible distortion (e.g., low MSE or high SSIM) at any given bit rate.\nHowever, in recent years, it has become increasingly accepted that \"low\ndistortion\" is not a synonym for \"high perceptual quality\", and in fact\noptimization of one often comes at the expense of the other. In light of this\nunderstanding, it is natural to seek for a generalization of rate-distortion\ntheory which takes perceptual quality into account. In this paper, we adopt the\nmathematical definition of perceptual quality recently proposed by Blau &\nMichaeli (2018), and use it to study the three-way tradeoff between rate,\ndistortion, and perception. We show that restricting the perceptual quality to\nbe high, generally leads to an elevation of the rate-distortion curve, thus\nnecessitating a sacrifice in either rate or distortion. We prove several\nfundamental properties of this triple-tradeoff, calculate it in closed form for\na Bernoulli source, and illustrate it visually on a toy MNIST example. \n\n"}
{"id": "1901.07827", "contents": "Title: Towards Compact ConvNets via Structure-Sparsity Regularized Filter\n  Pruning Abstract: The success of convolutional neural networks (CNNs) in computer vision\napplications has been accompanied by a significant increase of computation and\nmemory costs, which prohibits its usage on resource-limited environments such\nas mobile or embedded devices. To this end, the research of CNN compression has\nrecently become emerging. In this paper, we propose a novel filter pruning\nscheme, termed structured sparsity regularization (SSR), to simultaneously\nspeedup the computation and reduce the memory overhead of CNNs, which can be\nwell supported by various off-the-shelf deep learning libraries. Concretely,\nthe proposed scheme incorporates two different regularizers of structured\nsparsity into the original objective function of filter pruning, which fully\ncoordinates the global outputs and local pruning operations to adaptively prune\nfilters. We further propose an Alternative Updating with Lagrange Multipliers\n(AULM) scheme to efficiently solve its optimization. AULM follows the principle\nof ADMM and alternates between promoting the structured sparsity of CNNs and\noptimizing the recognition loss, which leads to a very efficient solver (2.5x\nto the most recent work that directly solves the group sparsity-based\nregularization). Moreover, by imposing the structured sparsity, the online\ninference is extremely memory-light, since the number of filters and the output\nfeature maps are simultaneously reduced. The proposed scheme has been deployed\nto a variety of state-of-the-art CNN structures including LeNet, AlexNet, VGG,\nResNet and GoogLeNet over different datasets. Quantitative results demonstrate\nthat the proposed scheme achieves superior performance over the\nstate-of-the-art methods. We further demonstrate the proposed compression\nscheme for the task of transfer learning, including domain adaptation and\nobject detection, which also show exciting performance gains over the\nstate-of-the-arts. \n\n"}
{"id": "1901.08379", "contents": "Title: Using CycleGANs for effectively reducing image variability across OCT\n  devices and improving retinal fluid segmentation Abstract: Optical coherence tomography (OCT) has become the most important imaging\nmodality in ophthalmology. A substantial amount of research has recently been\ndevoted to the development of machine learning (ML) models for the\nidentification and quantification of pathological features in OCT images. Among\nthe several sources of variability the ML models have to deal with, a major\nfactor is the acquisition device, which can limit the ML model's\ngeneralizability. In this paper, we propose to reduce the image variability\nacross different OCT devices (Spectralis and Cirrus) by using CycleGAN, an\nunsupervised unpaired image transformation algorithm. The usefulness of this\napproach is evaluated in the setting of retinal fluid segmentation, namely\nintraretinal cystoid fluid (IRC) and subretinal fluid (SRF). First, we train a\nsegmentation model on images acquired with a source OCT device. Then we\nevaluate the model on (1) source, (2) target and (3) transformed versions of\nthe target OCT images. The presented transformation strategy shows an F1 score\nof 0.4 (0.51) for IRC (SRF) segmentations. Compared with traditional\ntransformation approaches, this means an F1 score gain of 0.2 (0.12). \n\n"}
{"id": "1901.09005", "contents": "Title: Revisiting Self-Supervised Visual Representation Learning Abstract: Unsupervised visual representation learning remains a largely unsolved\nproblem in computer vision research. Among a big body of recently proposed\napproaches for unsupervised learning of visual representations, a class of\nself-supervised techniques achieves superior performance on many challenging\nbenchmarks. A large number of the pretext tasks for self-supervised learning\nhave been studied, but other important aspects, such as the choice of\nconvolutional neural networks (CNN), has not received equal attention.\nTherefore, we revisit numerous previously proposed self-supervised models,\nconduct a thorough large scale study and, as a result, uncover multiple crucial\ninsights. We challenge a number of common practices in selfsupervised visual\nrepresentation learning and observe that standard recipes for CNN design do not\nalways translate to self-supervised representation learning. As part of our\nstudy, we drastically boost the performance of previously proposed techniques\nand outperform previously published state-of-the-art results by a large margin. \n\n"}
{"id": "1901.09109", "contents": "Title: DADAM: A Consensus-based Distributed Adaptive Gradient Method for Online\n  Optimization Abstract: Adaptive gradient-based optimization methods such as \\textsc{Adagrad},\n\\textsc{Rmsprop}, and \\textsc{Adam} are widely used in solving large-scale\nmachine learning problems including deep learning. A number of schemes have\nbeen proposed in the literature aiming at parallelizing them, based on\ncommunications of peripheral nodes with a central node, but incur high\ncommunications cost. To address this issue, we develop a novel consensus-based\ndistributed adaptive moment estimation method (\\textsc{Dadam}) for online\noptimization over a decentralized network that enables data parallelization, as\nwell as decentralized computation. The method is particularly useful, since it\ncan accommodate settings where access to local data is allowed. Further, as\nestablished theoretically in this work, it can outperform centralized adaptive\nalgorithms, for certain classes of loss functions used in applications. We\nanalyze the convergence properties of the proposed algorithm and provide a\ndynamic regret bound on the convergence rate of adaptive moment estimation\nmethods in both stochastic and deterministic settings. Empirical results\ndemonstrate that \\textsc{Dadam} works also well in practice and compares\nfavorably to competing online optimization methods. \n\n"}
{"id": "1901.09197", "contents": "Title: Deep Convolutional Encoder-Decoders with Aggregated Multi-Resolution\n  Skip Connections for Skin Lesion Segmentation Abstract: The prevalence of skin melanoma is rapidly increasing as well as the recorded\ndeath cases of its patients. Automatic image segmentation tools play an\nimportant role in providing standardized computer-assisted analysis for skin\nmelanoma patients. Current state-of-the-art segmentation methods are based on\nfully convolutional neural networks, which utilize an encoder-decoder approach.\nHowever, these methods produce coarse segmentation masks due to the loss of\nlocation information during the encoding layers. Inspired by Pyramid Scene\nParsing Network (PSP-Net), we propose an encoder-decoder model that utilizes\npyramid pooling modules in the deep skip connections which aggregate the global\ncontext and compensate for the lost spatial information. We trained and\nvalidated our approach using ISIC 2018: Skin Lesion Analysis Towards Melanoma\nDetection grand challenge dataset. Our approach showed a validation accuracy\nwith a Jaccard index of 0.837, which outperforms U-Net. We believe that with\nthis reported reliable accuracy, this method can be introduced for clinical\npractice. \n\n"}
{"id": "1901.09507", "contents": "Title: A Lagrangian Policy for Optimal Energy Storage Control Abstract: This paper presents a millisecond-level look-ahead control algorithm for\nenergy storage with constant space complexity and worst-case linear run-time\ncomplexity. The algorithm connects the optimal control with the Lagrangian\nmultiplier associated with the state-of-charge constraint. It is compared to\nsolving look-ahead control using a state-of-the-art convex optimization solver.\nSimulation results show that both methods obtain the same control result, while\nthe proposed algorithm runs up to 100,000 times faster and solves most problems\nwithin one millisecond. The theoretical results from developing this algorithm\nalso provide key insights into designing optimal energy storage control schemes\nat the centralized system level as well as under distributed settings. \n\n"}
{"id": "1901.09653", "contents": "Title: Optimal inflow control penalizing undersupply in transport systems with\n  uncertain demands Abstract: We are concerned with optimal control strategies subject to uncertain\ndemands. An Ornstein-Uhlenbeck process describes the uncertain demand. The\ntransport within the supply system is modeled by the linear advection equation.\nWe consider different approaches to control the produced amount at a given time\nto meet the stochastic demand in an optimal way. In particular, we introduce an\nundersupply penalty and analyze its effect on the optimal output in a numerical\nsimulation study. \n\n"}
{"id": "1901.09972", "contents": "Title: Heartbeat Anomaly Detection using Adversarial Oversampling Abstract: Cardiovascular diseases are one of the most common causes of death in the\nworld. Prevention, knowledge of previous cases in the family, and early\ndetection is the best strategy to reduce this fact. Different machine learning\napproaches to automatic diagnostic are being proposed to this task. As in most\nhealth problems, the imbalance between examples and classes is predominant in\nthis problem and affects the performance of the automated solution. In this\npaper, we address the classification of heartbeats images in different\ncardiovascular diseases. We propose a two-dimensional Convolutional Neural\nNetwork for classification after using a InfoGAN architecture for generating\nsynthetic images to unbalanced classes. We call this proposal Adversarial\nOversampling and compare it with the classical oversampling methods as SMOTE,\nADASYN, and RandomOversampling. The results show that the proposed approach\nimproves the classifier performance for the minority classes without harming\nthe performance in the balanced classes. \n\n"}
{"id": "1901.10112", "contents": "Title: Evaluating Generalization Ability of Convolutional Neural Networks and\n  Capsule Networks for Image Classification via Top-2 Classification Abstract: Image classification is a challenging problem which aims to identify the\ncategory of object in the image. In recent years, deep Convolutional Neural\nNetworks (CNNs) have been applied to handle this task, and impressive\nimprovement has been achieved. However, some research showed the output of CNNs\ncan be easily altered by adding relatively small perturbations to the input\nimage, such as modifying few pixels. Recently, Capsule Networks (CapsNets) are\nproposed, which can help eliminating this limitation. Experiments on MNIST\ndataset revealed that capsules can better characterize the features of object\nthan CNNs. But it's hard to find a suitable quantitative method to compare the\ngeneralization ability of CNNs and CapsNets. In this paper, we propose a new\nimage classification task called Top-2 classification to evaluate the\ngeneralization ability of CNNs and CapsNets. The models are trained on single\nlabel image samples same as the traditional image classification task. But in\nthe test stage, we randomly concatenate two test image samples which contain\ndifferent labels, and then use the trained models to predict the top-2 labels\non the unseen newly-created two label image samples. This task can provide us\nprecise quantitative results to compare the generalization ability of CNNs and\nCapsNets. Back to the CapsNet, because it uses Full Connectivity (FC) mechanism\namong all capsules, it requires many parameters. To reduce the number of\nparameters, we introduce the Parameter-Sharing (PS) mechanism between capsules.\nExperiments on five widely used benchmark image datasets demonstrate the method\nsignificantly reduces the number of parameters, without losing the\neffectiveness of extracting features. Further, on the Top-2 classification\ntask, the proposed PS CapsNets obtain impressive higher accuracy compared to\nthe traditional CNNs and FC CapsNets by a large margin. \n\n"}
{"id": "1901.10170", "contents": "Title: Mask-RCNN and U-net Ensembled for Nuclei Segmentation Abstract: Nuclei segmentation is both an important and in some ways ideal task for\nmodern computer vision methods, e.g. convolutional neural networks. While\nrecent developments in theory and open-source software have made these tools\neasier to implement, expert knowledge is still required to choose the right\nmodel architecture and training setup. We compare two popular segmentation\nframeworks, U-Net and Mask-RCNN in the nuclei segmentation task and find that\nthey have different strengths and failures. To get the best of both worlds, we\ndevelop an ensemble model to combine their predictions that can outperform both\nmodels by a significant margin and should be considered when aiming for best\nnuclei segmentation performance. \n\n"}
{"id": "1901.10277", "contents": "Title: High-Quality Self-Supervised Deep Image Denoising Abstract: We describe a novel method for training high-quality image denoising models\nbased on unorganized collections of corrupted images. The training does not\nneed access to clean reference images, or explicit pairs of corrupted images,\nand can thus be applied in situations where such data is unacceptably expensive\nor impossible to acquire. We build on a recent technique that removes the need\nfor reference data by employing networks with a \"blind spot\" in the receptive\nfield, and significantly improve two key aspects: image quality and training\nefficiency. Our result quality is on par with state-of-the-art neural network\ndenoisers in the case of i.i.d. additive Gaussian noise, and not far behind\nwith Poisson and impulse noise. We also successfully handle cases where\nparameters of the noise model are variable and/or unknown in both training and\nevaluation data. \n\n"}
{"id": "1901.11284", "contents": "Title: Capturing Object Detection Uncertainty in Multi-Layer Grid Maps Abstract: We propose a deep convolutional object detector for automated driving\napplications that also estimates classification, pose and shape uncertainty of\neach detected object. The input consists of a multi-layer grid map which is\nwell-suited for sensor fusion, free-space estimation and machine learning.\nBased on the estimated pose and shape uncertainty we approximate object hulls\nwith bounded collision probability which we find helpful for subsequent\ntrajectory planning tasks. We train our models based on the KITTI object\ndetection data set. In a quantitative and qualitative evaluation some models\nshow a similar performance and superior robustness compared to previously\ndeveloped object detectors. However, our evaluation also points to undesired\ndata set properties which should be addressed when training data-driven models\nor creating new data sets. \n\n"}
{"id": "cond-mat/0608312", "contents": "Title: On Cavity Approximations for Graphical Models Abstract: We reformulate the Cavity Approximation (CA), a class of algorithms recently\nintroduced for improving the Bethe approximation estimates of marginals in\ngraphical models. In our new formulation, which allows for the treatment of\nmultivalued variables, a further generalization to factor graphs with arbitrary\norder of interaction factors is explicitly carried out, and a message passing\nalgorithm that implements the first order correction to the Bethe approximation\nis described. Furthermore we investigate an implementation of the CA for\npairwise interactions. In all cases considered we could confirm that CA[k] with\nincreasing $k$ provides a sequence of approximations of markedly increasing\nprecision. Furthermore in some cases we could also confirm the general\nexpectation that the approximation of order $k$, whose computational complexity\nis $O(N^{k+1})$ has an error that scales as $1/N^{k+1}$ with the size of the\nsystem. We discuss the relation between this approach and some recent\ndevelopments in the field. \n\n"}
{"id": "cs/0411014", "contents": "Title: Rate Distortion and Denoising of Individual Data Using Kolmogorov\n  complexity Abstract: We examine the structure of families of distortion balls from the perspective\nof Kolmogorov complexity. Special attention is paid to the canonical\nrate-distortion function of a source word which returns the minimal Kolmogorov\ncomplexity of all distortion balls containing that word subject to a bound on\ntheir cardinality. This canonical rate-distortion function is related to the\nmore standard algorithmic rate-distortion function for the given distortion\nmeasure. Examples are given of list distortion, Hamming distortion, and\nEuclidean distortion. The algorithmic rate-distortion function can behave\ndifferently from Shannon's rate-distortion function. To this end, we show that\nthe canonical rate-distortion function can and does assume a wide class of\nshapes (unlike Shannon's); we relate low algorithmic mutual information to low\nKolmogorov complexity (and consequently suggest that certain aspects of the\nmutual information formulation of Shannon's rate-distortion function behave\ndifferently than would an analogous formulation using algorithmic mutual\ninformation); we explore the notion that low Kolmogorov complexity distortion\nballs containing a given word capture the interesting properties of that word\n(which is hard to formalize in Shannon's theory) and this suggests an approach\nto denoising; and, finally, we show that the different behavior of the\nrate-distortion curves of individual source words to some extent disappears\nafter averaging over the source words. \n\n"}
{"id": "cs/0510009", "contents": "Title: Tree-Based Construction of LDPC Codes Having Good Pseudocodeword Weights Abstract: We present a tree-based construction of LDPC codes that have minimum\npseudocodeword weight equal to or almost equal to the minimum distance, and\nperform well with iterative decoding. The construction involves enumerating a\n$d$-regular tree for a fixed number of layers and employing a connection\nalgorithm based on permutations or mutually orthogonal Latin squares to close\nthe tree. Methods are presented for degrees $d=p^s$ and $d = p^s+1$, for $p$ a\nprime. One class corresponds to the well-known finite-geometry and finite\ngeneralized quadrangle LDPC codes; the other codes presented are new. We also\npresent some bounds on pseudocodeword weight for $p$-ary LDPC codes. Treating\nthese codes as $p$-ary LDPC codes rather than binary LDPC codes improves their\nrates, minimum distances, and pseudocodeword weights, thereby giving a new\nimportance to the finite geometry LDPC codes where $p > 2$. \n\n"}
{"id": "cs/0701120", "contents": "Title: Algorithmic Complexity Bounds on Future Prediction Errors Abstract: We bound the future loss when predicting any (computably) stochastic sequence\nonline. Solomonoff finitely bounded the total deviation of his universal\npredictor $M$ from the true distribution $mu$ by the algorithmic complexity of\n$mu$. Here we assume we are at a time $t>1$ and already observed $x=x_1...x_t$.\nWe bound the future prediction performance on $x_{t+1}x_{t+2}...$ by a new\nvariant of algorithmic complexity of $mu$ given $x$, plus the complexity of the\nrandomness deficiency of $x$. The new complexity is monotone in its condition\nin the sense that this complexity can only decrease if the condition is\nprolonged. We also briefly discuss potential generalizations to Bayesian model\nclasses and to classification problems. \n\n"}
{"id": "math/0108163", "contents": "Title: Interval straight line fitting Abstract: I consider the task of experimental data fitting. Unlike the traditional\napproach I do not try to minimize any functional based on available\nexperimental information, instead the minimization problem is replaced with\nconstraint satisfaction procedure, which produces the interval hull of\nsolutions of desired type. The method, called 'box slicing algorithm', is\ndescribed in details. The results obtained this way need not to be labeled with\nconfidence level of any kind, they are simply certain (guaranteed). The method\neasily handles the case with uncertainties in one or both variables. There is\nno need for, always more or less arbitrary, weighting the experimental data.\nThe approach is directly applicable to other experimental data processing\nproblems like outliers detection or finding the straight line, which is tangent\nto the experimental curve. \n\n"}
{"id": "math/0204189", "contents": "Title: State-Space Controller Design for the Fractional-Order Regulated System Abstract: In this paper we will present a mathematical description and analysis of a\nfractional-order regulated system in the state space and the state-space\ncontroller design based on placing the closed-loop poles on the complex plane.\nPresented are the results of simulations and stability investigation of this\nsystem. \n\n"}
{"id": "math/0503448", "contents": "Title: Max-plus (A,B)-invariant spaces and control of timed discrete event\n  systems Abstract: The concept of (A,B)-invariant subspace (or controlled invariant) of a linear\ndynamical system is extended to linear systems over the max-plus semiring.\nAlthough this extension presents several difficulties, which are similar to\nthose encountered in the same kind of extension to linear dynamical systems\nover rings, it appears capable of providing solutions to many control problems\nlike in the cases of linear systems over fields or rings. Sufficient conditions\nare given for computing the maximal (A,B)-invariant subspace contained in a\ngiven space and the existence of linear state feedbacks is discussed. An\napplication to the study of transportation networks which evolve according to a\ntimetable is considered. \n\n"}
{"id": "math/0506124", "contents": "Title: Relative entropy and the multi-variable multi-dimensional moment problem Abstract: Entropy-like functionals on operator algebras have been studied since the\npioneering work of von Neumann, Umegaki, Lindblad, and Lieb. The most\nwell-known are the von Neumann entropy $trace (\\rho\\log \\rho)$ and a\ngeneralization of the Kullback-Leibler distance $trace (\\rho \\log \\rho - \\rho\n\\log \\sigma)$, refered to as quantum relative entropy and used to quantify\ndistance between states of a quantum system. The purpose of this paper is to\nexplore these as regularizing functionals in seeking solutions to\nmulti-variable and multi-dimensional moment problems. It will be shown that\nextrema can be effectively constructed via a suitable homotopy. The homotopy\napproach leads naturally to a further generalization and a description of all\nthe solutions to such moment problems. This is accomplished by a\nrenormalization of a Riemannian metric induced by entropy functionals. As an\napplication we discuss the inverse problem of describing power spectra which\nare consistent with second-order statistics, which has been the main motivation\nbehind the present work. \n\n"}
