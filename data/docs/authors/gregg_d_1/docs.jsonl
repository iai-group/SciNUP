{"id": "0704.1269", "contents": "Title: Phase Transitions in the Coloring of Random Graphs Abstract: We consider the problem of coloring the vertices of a large sparse random\ngraph with a given number of colors so that no adjacent vertices have the same\ncolor. Using the cavity method, we present a detailed and systematic analytical\nstudy of the space of proper colorings (solutions).\n  We show that for a fixed number of colors and as the average vertex degree\n(number of constraints) increases, the set of solutions undergoes several phase\ntransitions similar to those observed in the mean field theory of glasses.\nFirst, at the clustering transition, the entropically dominant part of the\nphase space decomposes into an exponential number of pure states so that beyond\nthis transition a uniform sampling of solutions becomes hard. Afterward, the\nspace of solutions condenses over a finite number of the largest states and\nconsequently the total entropy of solutions becomes smaller than the annealed\none. Another transition takes place when in all the entropically dominant\nstates a finite fraction of nodes freezes so that each of these nodes is\nallowed a single color in all the solutions inside the state. Eventually, above\nthe coloring threshold, no more solutions are available. We compute all the\ncritical connectivities for Erdos-Renyi and regular random graphs and determine\ntheir asymptotic values for large number of colors.\n  Finally, we discuss the algorithmic consequences of our findings. We argue\nthat the onset of computational hardness is not associated with the clustering\ntransition and we suggest instead that the freezing transition might be the\nrelevant phenomenon. We also discuss the performance of a simple local Walk-COL\nalgorithm and of the belief propagation algorithm in the light of our results. \n\n"}
{"id": "0704.1694", "contents": "Title: Locally Decodable Codes From Nice Subsets of Finite Fields and Prime\n  Factors of Mersenne Numbers Abstract: A k-query Locally Decodable Code (LDC) encodes an n-bit message x as an N-bit\ncodeword C(x), such that one can probabilistically recover any bit x_i of the\nmessage by querying only k bits of the codeword C(x), even after some constant\nfraction of codeword bits has been corrupted. The major goal of LDC related\nresearch is to establish the optimal trade-off between length and query\ncomplexity of such codes.\n  Recently [Y] introduced a novel technique for constructing locally decodable\ncodes and vastly improved the upper bounds for code length. The technique is\nbased on Mersenne primes. In this paper we extend the work of [Y] and argue\nthat further progress via these methods is tied to progress on an old number\ntheory question regarding the size of the largest prime factors of Mersenne\nnumbers.\n  Specifically, we show that every Mersenne number m=2^t-1 that has a prime\nfactor p>m^\\gamma yields a family of k(\\gamma)-query locally decodable codes of\nlength Exp(n^{1/t}). Conversely, if for some fixed k and all \\epsilon > 0 one\ncan use the technique of [Y] to obtain a family of k-query LDCs of length\nExp(n^\\epsilon); then infinitely many Mersenne numbers have prime factors arger\nthan known currently. \n\n"}
{"id": "0705.2876", "contents": "Title: An online algorithm for generating fractal hash chains applied to\n  digital chains of custody Abstract: This paper gives an online algorithm for generating Jakobsson's fractal hash\nchains. Our new algorithm compliments Jakobsson's fractal hash chain algorithm\nfor preimage traversal since his algorithm assumes the entire hash chain is\nprecomputed and a particular list of Ceiling(log n) hash elements or pebbles\nare saved. Our online algorithm for hash chain traversal incrementally\ngenerates a hash chain of n hash elements without knowledge of n before it\nstarts. For any n, our algorithm stores only the Ceiling(log n) pebbles which\nare precisely the inputs for Jakobsson's amortized hash chain preimage\ntraversal algorithm. This compact representation is useful to generate,\ntraverse, and store a number of large digital hash chains on a small and\nconstrained device. We also give an application using both Jakobsson's and our\nnew algorithm applied to digital chains of custody for validating dynamically\nchanging forensics data. \n\n"}
{"id": "0708.4284", "contents": "Title: Optimal Per-Edge Processing Times in the Semi-Streaming Model Abstract: We present semi-streaming algorithms for basic graph problems that have\noptimal per-edge processing times and therefore surpass all previous\nsemi-streaming algorithms for these tasks. The semi-streaming model, which is\nappropriate when dealing with massive graphs, forbids random access to the\ninput and restricts the memory to O(n*polylog n) bits.\n  Particularly, the formerly best per-edge processing times for finding the\nconnected components and a bipartition are O(alpha(n)), for determining\nk-vertex and k-edge connectivity O(k^2n) and O(n*log n) respectively for any\nconstant k and for computing a minimum spanning forest O(log n). All these time\nbounds we reduce to O(1).\n  Every presented algorithm determines a solution asymptotically as fast as the\nbest corresponding algorithm up to date in the classical RAM model, which\ntherefore cannot convert the advantage of unlimited memory and random access\ninto superior computing times for these problems. \n\n"}
{"id": "0712.1402", "contents": "Title: Reconstruction of Markov Random Fields from Samples: Some Easy\n  Observations and Algorithms Abstract: Markov random fields are used to model high dimensional distributions in a\nnumber of applied areas. Much recent interest has been devoted to the\nreconstruction of the dependency structure from independent samples from the\nMarkov random fields. We analyze a simple algorithm for reconstructing the\nunderlying graph defining a Markov random field on $n$ nodes and maximum degree\n$d$ given observations. We show that under mild non-degeneracy conditions it\nreconstructs the generating graph with high probability using $\\Theta(d\n\\epsilon^{-2}\\delta^{-4} \\log n)$ samples where $\\epsilon,\\delta$ depend on the\nlocal interactions. For most local interaction $\\eps,\\delta$ are of order\n$\\exp(-O(d))$.\n  Our results are optimal as a function of $n$ up to a multiplicative constant\ndepending on $d$ and the strength of the local interactions. Our results seem\nto be the first results for general models that guarantee that {\\em the}\ngenerating model is reconstructed. Furthermore, we provide explicit $O(n^{d+2}\n\\epsilon^{-2}\\delta^{-4} \\log n)$ running time bound. In cases where the\nmeasure on the graph has correlation decay, the running time is $O(n^2 \\log n)$\nfor all fixed $d$. We also discuss the effect of observing noisy samples and\nshow that as long as the noise level is low, our algorithm is effective. On the\nother hand, we construct an example where large noise implies\nnon-identifiability even for generic noise and interactions. Finally, we\nbriefly show that in some simple cases, models with hidden nodes can also be\nrecovered. \n\n"}
{"id": "0712.1959", "contents": "Title: Delaunay Edge Flips in Dense Surface Triangulations Abstract: Delaunay flip is an elegant, simple tool to convert a triangulation of a\npoint set to its Delaunay triangulation. The technique has been researched\nextensively for full dimensional triangulations of point sets. However, an\nimportant case of triangulations which are not full dimensional is surface\ntriangulations in three dimensions. In this paper we address the question of\nconverting a surface triangulation to a subcomplex of the Delaunay\ntriangulation with edge flips. We show that the surface triangulations which\nclosely approximate a smooth surface with uniform density can be transformed to\na Delaunay triangulation with a simple edge flip algorithm. The condition on\nuniformity becomes less stringent with increasing density of the triangulation.\nIf the condition is dropped completely, the flip algorithm still terminates\nalthough the output surface triangulation becomes \"almost Delaunay\" instead of\nexactly Delaunay. \n\n"}
{"id": "0712.4213", "contents": "Title: Exact Quantum Algorithms for the Leader Election Problem Abstract: This paper gives the first separation of quantum and classical pure (i.e.,\nnon-cryptographic) computing abilities with no restriction on the amount of\navailable computing resources, by considering the exact solvability of a\ncelebrated unsolvable problem in classical distributed computing, the ``leader\nelection problem'' on anonymous networks. The goal of the leader election\nproblem is to elect a unique leader from among distributed parties. The paper\nconsiders this problem for anonymous networks, in which each party has the same\nidentifier. It is well-known that no classical algorithm can solve exactly\n(i.e., in bounded time without error) the leader election problem in anonymous\nnetworks, even if it is given the number of parties. This paper gives two\nquantum algorithms that, given the number of parties, can exactly solve the\nproblem for any network topology in polynomial rounds and polynomial\ncommunication/time complexity with respect to the number of parties, when the\nparties are connected by quantum communication links. \n\n"}
{"id": "0801.0514", "contents": "Title: New results on Noncommutative and Commutative Polynomial Identity\n  Testing Abstract: Using ideas from automata theory we design a new efficient (deterministic)\nidentity test for the \\emph{noncommutative} polynomial identity testing problem\n(first introduced and studied in \\cite{RS05,BW05}). We also apply this idea to\nthe reconstruction of black-box noncommuting algebraic branching programs.\nAssuming the black-box model allows us to query the ABP for the output at any\ngiven gate, we can reconstruct an (equivalent) ABP in deterministic polynomial\ntime. Finally, we explore commutative identity testing when the coefficients of\nthe input polynomial come from an arbitrary finite commutative ring with unity. \n\n"}
{"id": "0804.4666", "contents": "Title: Combining geometry and combinatorics: A unified approach to sparse\n  signal recovery Abstract: There are two main algorithmic approaches to sparse signal recovery:\ngeometric and combinatorial. The geometric approach starts with a geometric\nconstraint on the measurement matrix and then uses linear programming to decode\ninformation about the signal from its measurements. The combinatorial approach\nconstructs the measurement matrix and a combinatorial decoding algorithm to\nmatch. We present a unified approach to these two classes of sparse signal\nrecovery algorithms.\n  The unifying elements are the adjacency matrices of high-quality unbalanced\nexpanders. We generalize the notion of Restricted Isometry Property (RIP),\ncrucial to compressed sensing results for signal recovery, from the Euclidean\nnorm to the l_p norm for p about 1, and then show that unbalanced expanders are\nessentially equivalent to RIP-p matrices.\n  From known deterministic constructions for such matrices, we obtain new\ndeterministic measurement matrix constructions and algorithms for signal\nrecovery which, compared to previous deterministic algorithms, are superior in\neither the number of measurements or in noise tolerance. \n\n"}
{"id": "0806.2274", "contents": "Title: Exposing Multi-Relational Networks to Single-Relational Network Analysis\n  Algorithms Abstract: Many, if not most network analysis algorithms have been designed specifically\nfor single-relational networks; that is, networks in which all edges are of the\nsame type. For example, edges may either represent \"friendship,\" \"kinship,\" or\n\"collaboration,\" but not all of them together. In contrast, a multi-relational\nnetwork is a network with a heterogeneous set of edge labels which can\nrepresent relationships of various types in a single data structure. While\nmulti-relational networks are more expressive in terms of the variety of\nrelationships they can capture, there is a need for a general framework for\ntransferring the many single-relational network analysis algorithms to the\nmulti-relational domain. It is not sufficient to execute a single-relational\nnetwork analysis algorithm on a multi-relational network by simply ignoring\nedge labels. This article presents an algebra for mapping multi-relational\nnetworks to single-relational networks, thereby exposing them to\nsingle-relational network analysis algorithms. \n\n"}
{"id": "0806.4790", "contents": "Title: AMS Without 4-Wise Independence on Product Domains Abstract: In their seminal work, Alon, Matias, and Szegedy introduced several sketching\ntechniques, including showing that 4-wise independence is sufficient to obtain\ngood approximations of the second frequency moment. In this work, we show that\ntheir sketching technique can be extended to product domains $[n]^k$ by using\nthe product of 4-wise independent functions on $[n]$. Our work extends that of\nIndyk and McGregor, who showed the result for $k = 2$. Their primary motivation\nwas the problem of identifying correlations in data streams. In their model, a\nstream of pairs $(i,j) \\in [n]^2$ arrive, giving a joint distribution $(X,Y)$,\nand they find approximation algorithms for how close the joint distribution is\nto the product of the marginal distributions under various metrics, which\nnaturally corresponds to how close $X$ and $Y$ are to being independent. By\nusing our technique, we obtain a new result for the problem of approximating\nthe $\\ell_2$ distance between the joint distribution and the product of the\nmarginal distributions for $k$-ary vectors, instead of just pairs, in a single\npass. Our analysis gives a randomized algorithm that is a $(1 \\pm \\epsilon)$\napproximation (with probability $1-\\delta$) that requires space logarithmic in\n$n$ and $m$ and proportional to $3^k$. \n\n"}
{"id": "0808.0163", "contents": "Title: Twice-Ramanujan Sparsifiers Abstract: We prove that every graph has a spectral sparsifier with a number of edges\nlinear in its number of vertices. As linear-sized spectral sparsifiers of\ncomplete graphs are expanders, our sparsifiers of arbitrary graphs can be\nviewed as generalizations of expander graphs. In particular, we prove that for\nevery $d>1$ and every undirected, weighted graph $G=(V,E,w)$ on $n$ vertices,\nthere exists a weighted graph $H=(V,F,\\tilde{w})$ with at most $\\lceil d(n-1)\n\\rceil$ edges such that for every $x \\in \\mathbb{R}^{V}$, \\[ x^{T}L_{G}x \\leq\nx^{T}L_{H}x \\leq (\\frac{d+1+2\\sqrt{d}}{d+1-2\\sqrt{d}})\\cdot x^{T}L_{G}x \\]\nwhere $L_{G}$ and $L_{H}$ are the Laplacian matrices of $G$ and $H$,\nrespectively. Thus, $H$ approximates $G$ spectrally at least as well as a\nRamanujan expander with $dn/2$ edges approximates the complete graph. We give\nan elementary deterministic polynomial time algorithm for constructing $H$. \n\n"}
{"id": "0808.1549", "contents": "Title: The Peculiar Phase Structure of Random Graph Bisection Abstract: The mincut graph bisection problem involves partitioning the n vertices of a\ngraph into disjoint subsets, each containing exactly n/2 vertices, while\nminimizing the number of \"cut\" edges with an endpoint in each subset. When\nconsidered over sparse random graphs, the phase structure of the graph\nbisection problem displays certain familiar properties, but also some\nsurprises. It is known that when the mean degree is below the critical value of\n2 log 2, the cutsize is zero with high probability. We study how the minimum\ncutsize increases with mean degree above this critical threshold, finding a new\nanalytical upper bound that improves considerably upon previous bounds.\nCombined with recent results on expander graphs, our bound suggests the unusual\nscenario that random graph bisection is replica symmetric up to and beyond the\ncritical threshold, with a replica symmetry breaking transition possibly taking\nplace above the threshold. An intriguing algorithmic consequence is that\nalthough the problem is NP-hard, we can find near-optimal cutsizes (whose ratio\nto the optimal value approaches 1 asymptotically) in polynomial time for\ntypical instances near the phase transition. \n\n"}
{"id": "0809.4332", "contents": "Title: From one solution of a 3-satisfiability formula to a solution cluster:\n  Frozen variables and entropy Abstract: A solution to a 3-satisfiability (3-SAT) formula can be expanded into a\ncluster, all other solutions of which are reachable from this one through a\nsequence of single-spin flips. Some variables in the solution cluster are\nfrozen to the same spin values by one of two different mechanisms: frozen-core\nformation and long-range frustrations. While frozen cores are identified by a\nlocal whitening algorithm, long-range frustrations are very difficult to trace,\nand they make an entropic belief-propagation (BP) algorithm fail to converge.\nFor BP to reach a fixed point the spin values of a tiny fraction of variables\n(chosen according to the whitening algorithm) are externally fixed during the\niteration. From the calculated entropy values, we infer that, for a large\nrandom 3-SAT formula with constraint density close to the satisfiability\nthreshold, the solutions obtained by the survey-propagation or the walksat\nalgorithm belong neither to the most dominating clusters of the formula nor to\nthe most abundant clusters. This work indicates that a single solution cluster\nof a random 3-SAT formula may have further community structures. \n\n"}
{"id": "0810.4946", "contents": "Title: FPT Algorithms and Kernels for the Directed $k$-Leaf Problem Abstract: A subgraph $T$ of a digraph $D$ is an {\\em out-branching} if $T$ is an\noriented spanning tree with only one vertex of in-degree zero (called the {\\em\nroot}). The vertices of $T$ of out-degree zero are {\\em leaves}. In the {\\sc\nDirected $k$-Leaf} Problem, we are given a digraph $D$ and an integral\nparameter $k$, and we are to decide whether $D$ has an out-branching with at\nleast $k$ leaves. Recently, Kneis et al. (2008) obtained an algorithm for the\nproblem of running time $4^{k}\\cdot n^{O(1)}$. We describe a new algorithm for\nthe problem of running time $3.72^{k}\\cdot n^{O(1)}$. In {\\sc Rooted Directed\n$k$-Leaf} Problem, apart from $D$ and $k$, we are given a vertex $r$ of $D$ and\nwe are to decide whether $D$ has an out-branching rooted at $r$ with at least\n$k$ leaves. Very recently, Fernau et al. (2008) found an $O(k^3)$-size kernel\nfor {\\sc Rooted Directed $k$-Leaf}. In this paper, we obtain an $O(k)$ kernel\nfor {\\sc Rooted Directed $k$-Leaf} restricted to acyclic digraphs. \n\n"}
{"id": "0811.3055", "contents": "Title: Exact phase transition of backtrack-free search with implications on the\n  power of greedy algorithms Abstract: Backtracking is a basic strategy to solve constraint satisfaction problems\n(CSPs). A satisfiable CSP instance is backtrack-free if a solution can be found\nwithout encountering any dead-end during a backtracking search, implying that\nthe instance is easy to solve. We prove an exact phase transition of\nbacktrack-free search in some random CSPs, namely in Model RB and in Model RD.\nThis is the first time an exact phase transition of backtrack-free search can\nbe identified on some random CSPs. Our technical results also have interesting\nimplications on the power of greedy algorithms, on the width of random\nhypergraphs and on the exact satisfiability threshold of random CSPs. \n\n"}
{"id": "0812.1061", "contents": "Title: Decidability of the Equivalence of Multi-Letter Quantum Finite Automata Abstract: Multi-letter {\\it quantum finite automata} (QFAs) were a quantum variant of\nclassical {\\it one-way multi-head finite automata} (J. Hromkovi\\v{c}, Acta\nInformatica 19 (1983) 377-384), and it has been shown that this new one-way\nQFAs (multi-letter QFAs) can accept with no error some regular languages\n$(a+b)^{*}b$ that are unacceptable by the previous one-way QFAs. In this paper,\nwe study the decidability of the equivalence of multi-letter QFAs, and the main\ntechnical contributions are as follows: (1) We show that any two automata, a\n$k_{1}$-letter QFA ${\\cal A}_1$ and a $k_{2}$-letter QFA ${\\cal A}_2$, over the\nsame input alphabet $\\Sigma$ are equivalent if and only if they are\n$(n^2m^{k-1}-m^{k-1}+k)$-equivalent, where $m=|\\Sigma|$ is the cardinality of\n$\\Sigma$, $k=\\max(k_{1},k_{2})$, and $n=n_{1}+n_{2}$, with $n_{1}$ and $n_{2}$\nbeing the numbers of states of ${\\cal A}_{1}$ and ${\\cal A}_{2}$, respectively.\nWhen $k=1$, we obtain the decidability of equivalence of measure-once QFAs in\nthe literature. It is worth mentioning that our technical method is essentially\ndifferent from that for the decidability of the case of single input alphabet\n(i.e., $m=1$). (2) However, if we determine the equivalence of multi-letter\nQFAs by checking all strings of length not more than $ n^2m^{k-1}-m^{k-1}+k$,\nthen the worst time complexity is exponential, i.e.,\n$O(n^6m^{n^2m^{k-1}-m^{k-1}+2k-1})$. Therefore, we design a polynomial-time\n$O(m^{2k-1}n^{8}+km^kn^{6})$ algorithm for determining the equivalence of any\ntwo multi-letter QFAs. Here, the time complexity is concerning the number of\nstates in the multi-letter QFAs, and $k$ is thought of as a constant. \n\n"}
{"id": "0812.2137", "contents": "Title: A Factor 3/2 Approximation for Generalized Steiner Tree Problem with\n  Distances One and Two Abstract: We design a 3/2 approximation algorithm for the Generalized Steiner Tree\nproblem (GST) in metrics with distances 1 and 2. This is the first polynomial\ntime approximation algorithm for a wide class of non-geometric metric GST\ninstances with approximation factor below 2. \n\n"}
{"id": "0901.1849", "contents": "Title: Randomized Self-Assembly for Exact Shapes Abstract: Working in Winfree's abstract tile assembly model, we show that a\nconstant-size tile assembly system can be programmed through relative tile\nconcentrations to build an n x n square with high probability, for any\nsufficiently large n. This answers an open question of Kao and Schweller\n(Randomized Self-Assembly for Approximate Shapes, ICALP 2008), who showed how\nto build an approximately n x n square using tile concentration programming,\nand asked whether the approximation could be made exact with high probability.\nWe show how this technique can be modified to answer another question of Kao\nand Schweller, by showing that a constant-size tile assembly system can be\nprogrammed through tile concentrations to assemble arbitrary finite *scaled\nshapes*, which are shapes modified by replacing each point with a c x c block\nof points, for some integer c. Furthermore, we exhibit a smooth tradeoff\nbetween specifying bits of n via tile concentrations versus specifying them via\nhard-coded tile types, which allows tile concentration programming to be\nemployed for specifying a fraction of the bits of \"input\" to a tile assembly\nsystem, under the constraint that concentrations can only be specified to a\nlimited precision. Finally, to account for some unrealistic aspects of the tile\nconcentration programming model, we show how to modify the construction to use\nonly concentrations that are arbitrarily close to uniform. \n\n"}
{"id": "0901.3348", "contents": "Title: Nuclear norm minimization for the planted clique and biclique problems Abstract: We consider the problems of finding a maximum clique in a graph and finding a\nmaximum-edge biclique in a bipartite graph. Both problems are NP-hard. We write\nboth problems as matrix-rank minimization and then relax them using the nuclear\nnorm. This technique, which may be regarded as a generalization of compressive\nsensing, has recently been shown to be an effective way to solve rank\noptimization problems. In the special cases that the input graph has a planted\nclique or biclique (i.e., a single large clique or biclique plus diversionary\nedges), our algorithm successfully provides an exact solution to the original\ninstance. For each problem, we provide two analyses of when our algorithm\nsucceeds. In the first analysis, the diversionary edges are placed by an\nadversary. In the second, they are placed at random. In the case of random\nedges for the planted clique problem, we obtain the same bound as Alon,\nKrivelevich and Sudakov as well as Feige and Krauthgamer, but we use different\ntechniques. \n\n"}
{"id": "0903.5328", "contents": "Title: A Stochastic View of Optimal Regret through Minimax Duality Abstract: We study the regret of optimal strategies for online convex optimization\ngames. Using von Neumann's minimax theorem, we show that the optimal regret in\nthis adversarial setting is closely related to the behavior of the empirical\nminimization algorithm in a stochastic process setting: it is equal to the\nmaximum, over joint distributions of the adversary's action sequence, of the\ndifference between a sum of minimal expected losses and the minimal empirical\nloss. We show that the optimal regret has a natural geometric interpretation,\nsince it can be viewed as the gap in Jensen's inequality for a concave\nfunctional--the minimizer over the player's actions of expected loss--defined\non a set of probability distributions. We use this expression to obtain upper\nand lower bounds on the regret of an optimal strategy for a variety of online\nlearning problems. Our method provides upper bounds without the need to\nconstruct a learning algorithm; the lower bounds provide explicit optimal\nstrategies for the adversary. \n\n"}
{"id": "0904.0727", "contents": "Title: (Meta) Kernelization Abstract: In a parameterized problem, every instance I comes with a positive integer k.\nThe problem is said to admit a polynomial kernel if, in polynomial time, one\ncan reduce the size of the instance I to a polynomial in k, while preserving\nthe answer. In this work we give two meta-theorems on kernelzation. The first\ntheorem says that all problems expressible in Counting Monadic Second Order\nLogic and satisfying a coverability property admit a polynomial kernel on\ngraphs of bounded genus. Our second result is that all problems that have\nfinite integer index and satisfy a weaker coverability property admit a linear\nkernel on graphs of bounded genus. These theorems unify and extend all\npreviously known kernelization results for planar graph problems. \n\n"}
{"id": "0904.3251", "contents": "Title: On evaluation of permanents Abstract: We study the time and space complexity of matrix permanents over rings and\nsemirings. \n\n"}
{"id": "0906.1356", "contents": "Title: A Probabilistic Approach to Problems Parameterized Above or Below Tight\n  Bounds Abstract: We introduce a new approach for establishing fixed-parameter tractability of\nproblems parameterized above tight lower bounds. To illustrate the approach we\nconsider three problems of this type of unknown complexity that were introduced\nby Mahajan, Raman and Sikdar (J. Comput. Syst. Sci. 75, 2009). We show that a\ngeneralization of one of the problems and non-trivial special cases of the\nother two are fixed-parameter tractable. \n\n"}
{"id": "0907.1724", "contents": "Title: Inapproximability of the Tutte polynomial of a planar graph Abstract: The Tutte polynomial of a graph G is a two-variable polynomial T(G;x,y) that\nencodes many interesting properties of the graph. We study the complexity of\nthe following problem, for rationals x and y: given as input a planar graph G,\ndetermine T(G;x,y). Vertigan completely mapped the complexity of exactly\ncomputing the Tutte polynomial of a planar graph. He showed that the problem\ncan be solved in polynomial time if (x,y) is on the hyperbola H_q given by\n(x-1)(y-1)=q for q=1 or q=2 or if (x,y) is one of the two special points\n(x,y)=(-1,-1) or (x,y)=(1,1). Otherwise, the problem is #P-hard. In this paper,\nwe consider the problem of approximating T(G;x,y), in the usual sense of \"fully\npolynomial randomised approximation scheme\" or FPRAS. Roughly speaking, an\nFPRAS is required to produce, in polynomial time and with high probability, an\nanswer that has small relative error. Assuming that NP is different from RP, we\nshow that there is no FPRAS for the Tutte polynomial in a large portion of the\n(x,y) plane. In particular, there is no FPRAS if x>1, y<-1 or if y>1, x<-1 or\nif x<0, y<0 and q>5. Also, there is no FPRAS if x<1, y<1 and q=3. For q>5, our\nresult is intriguing because it shows that there is no FPRAS at\n(x,y)=(1-q/(1+epsilon),-epsilon) for any positive epsilon but it leaves open\nthe limit point epsilon=0, which corresponds to approximately counting\nq-colourings of a planar graph. \n\n"}
{"id": "0907.2627", "contents": "Title: Finding Fullerene Patches in Polynomial Time Abstract: We consider the following question, motivated by the enumeration of\nfullerenes. A fullerene patch is a 2-connected plane graph G in which inner\nfaces have length 5 or 6, non-boundary vertices have degree 3, and boundary\nvertices have degree 2 or 3. The degree sequence along the boundary is called\nthe boundary code of G. We show that the question whether a given sequence S is\na boundary code of some fullerene patch can be answered in polynomial time when\nsuch patches have at most five 5-faces. We conjecture that our algorithm gives\nthe correct answer for any number of 5-faces, and sketch how to extend the\nalgorithm to the problem of counting the number of different patches with a\ngiven boundary code. \n\n"}
{"id": "0907.3076", "contents": "Title: On Brambles, Grid-Like Minors, and Parameterized Intractability of\n  Monadic Second-Order Logic Abstract: Brambles were introduced as the dual notion to treewidth, one of the most\ncentral concepts of the graph minor theory of Robertson and Seymour. Recently,\nGrohe and Marx showed that there are graphs G, in which every bramble of order\nlarger than the square root of the treewidth is of exponential size in |G|. On\nthe positive side, they show the existence of polynomial-sized brambles of the\norder of the square root of the treewidth, up to log factors. We provide the\nfirst polynomial time algorithm to construct a bramble in general graphs and\nachieve this bound, up to log-factors. We use this algorithm to construct\ngrid-like minors, a replacement structure for grid-minors recently introduced\nby Reed and Wood, in polynomial time. Using the grid-like minors, we introduce\nthe notion of a perfect bramble and an algorithm to find one in polynomial\ntime. Perfect brambles are brambles with a particularly simple structure and\nthey also provide us with a subgraph that has bounded degree and still large\ntreewidth; we use them to obtain a meta-theorem on deciding certain\nparameterized subgraph-closed problems on general graphs in time singly\nexponential in the parameter.\n  The second part of our work deals with providing a lower bound to Courcelle's\nfamous theorem, stating that every graph property that can be expressed by a\nsentence in monadic second-order logic (MSO), can be decided by a linear time\nalgorithm on classes of graphs of bounded treewidth. Using our results from the\nfirst part of our work we establish a strong lower bound for tractability of\nMSO on classes of colored graphs. \n\n"}
{"id": "0907.3583", "contents": "Title: Web of Lossy Adapters for Interface Interoperability: An Algorithm and\n  NP-completeness of Minimization Abstract: By using different interface adapters for different methods, it is possible\nto construct a maximally covering web of interface adapters which incurs\nminimum loss during interface adaptation. We introduce a polynomial-time\nalgorithm that can achieve this. However, we also show that minimizing the\nnumber of adapters included in a maximally covering web of interface adapters\nis an NP-complete problem. \n\n"}
{"id": "0907.4283", "contents": "Title: Domination Problems in Nowhere-Dense Classes of Graphs Abstract: We investigate the parameterized complexity of generalisations and variations\nof the dominating set problem on classes of graphs that are nowhere dense. In\nparticular, we show that the distance-d dominating-set problem, also known as\nthe (k,d)-centres problem, is fixed-parameter tractable on any class that is\nnowhere dense and closed under induced subgraphs. This generalises known\nresults about the dominating set problem on H-minor free classes, classes with\nlocally excluded minors and classes of graphs of bounded expansion. A key\nfeature of our proof is that it is based simply on the fact that these graph\nclasses are uniformly quasi-wide, and does not rely on a structural\ndecomposition. Our result also establishes that the distance-d dominating-set\nproblem is FPT on classes of bounded expansion, answering a question of Ne{\\v\ns}et{\\v{r}}il and Ossona de Mendez. \n\n"}
{"id": "0908.4499", "contents": "Title: Monadic second-order model-checking on decomposable matroids Abstract: A notion of branch-width, which generalizes the one known for graphs, can be\ndefined for matroids. We first give a proof of the polynomial time\nmodel-checking of monadic second-order formulas on representable matroids of\nbounded branch-width, by reduction to monadic second-order formulas on trees.\nThis proof is much simpler than the one previously known. We also provide a\nlink between our logical approach and a grammar that allows to build matroids\nof bounded branch-width. Finally, we introduce a new class of non-necessarily\nrepresentable matroids, described by a grammar and on which monadic\nsecond-order formulas can be checked in linear time. \n\n"}
{"id": "0909.4969", "contents": "Title: MACH: Fast Randomized Tensor Decompositions Abstract: Tensors naturally model many real world processes which generate multi-aspect\ndata. Such processes appear in many different research disciplines, e.g,\nchemometrics, computer vision, psychometrics and neuroimaging analysis. Tensor\ndecompositions such as the Tucker decomposition are used to analyze\nmulti-aspect data and extract latent factors, which capture the multilinear\ndata structure. Such decompositions are powerful mining tools, for extracting\npatterns from large data volumes. However, most frequently used algorithms for\nsuch decompositions involve the computationally expensive Singular Value\nDecomposition.\n  In this paper we propose MACH, a new sampling algorithm to compute such\ndecompositions. Our method is of significant practical value for tensor\nstreams, such as environmental monitoring systems, IP traffic matrices over\ntime, where large amounts of data are accumulated and the analysis is\ncomputationally intensive but also in \"post-mortem\" data analysis cases where\nthe tensor does not fit in the available memory. We provide the theoretical\nanalysis of our proposed method, and verify its efficacy in monitoring system\napplications. \n\n"}
{"id": "0910.3376", "contents": "Title: Quantum Proofs for Classical Theorems Abstract: Alongside the development of quantum algorithms and quantum complexity theory\nin recent years, quantum techniques have also proved instrumental in obtaining\nresults in classical (non-quantum) areas. In this paper we survey these results\nand the quantum toolbox they use. \n\n"}
{"id": "0911.3195", "contents": "Title: Efficient Distributed Random Walks with Applications Abstract: We focus on the problem of performing random walks efficiently in a\ndistributed network. Given bandwidth constraints, the goal is to minimize the\nnumber of rounds required to obtain a random walk sample. We first present a\nfast sublinear time distributed algorithm for performing random walks whose\ntime complexity is sublinear in the length of the walk. Our algorithm performs\na random walk of length $\\ell$ in $\\tilde{O}(\\sqrt{\\ell D})$ rounds (with high\nprobability) on an undirected network, where $D$ is the diameter of the\nnetwork. This improves over the previous best algorithm that ran in\n$\\tilde{O}(\\ell^{2/3}D^{1/3})$ rounds (Das Sarma et al., PODC 2009). We further\nextend our algorithms to efficiently perform $k$ independent random walks in\n$\\tilde{O}(\\sqrt{k\\ell D} + k)$ rounds. We then show that there is a\nfundamental difficulty in improving the dependence on $\\ell$ any further by\nproving a lower bound of $\\Omega(\\sqrt{\\frac{\\ell}{\\log \\ell}} + D)$ under a\ngeneral model of distributed random walk algorithms. Our random walk algorithms\nare useful in speeding up distributed algorithms for a variety of applications\nthat use random walks as a subroutine. We present two main applications. First,\nwe give a fast distributed algorithm for computing a random spanning tree (RST)\nin an arbitrary (undirected) network which runs in $\\tilde{O}(\\sqrt{m}D)$\nrounds (with high probability; here $m$ is the number of edges). Our second\napplication is a fast decentralized algorithm for estimating mixing time and\nrelated parameters of the underlying network. Our algorithm is fully\ndecentralized and can serve as a building block in the design of\ntopologically-aware networks. \n\n"}
{"id": "0912.0850", "contents": "Title: Grammar-Based Compression in a Streaming Model Abstract: We show that, given a string $s$ of length $n$, with constant memory and\nlogarithmic passes over a constant number of streams we can build a\ncontext-free grammar that generates $s$ and only $s$ and whose size is within\nan $\\Oh{\\min (g \\log g, \\sqrt{n \\log g})}$-factor of the minimum $g$. This\nstands in contrast to our previous result that, with polylogarithmic memory and\npolylogarithmic passes over a single stream, we cannot build such a grammar\nwhose size is within any polynomial of $g$. \n\n"}
{"id": "0912.1457", "contents": "Title: A survey on algorithmic aspects of modular decomposition Abstract: The modular decomposition is a technique that applies but is not restricted\nto graphs. The notion of module naturally appears in the proofs of many graph\ntheoretical theorems. Computing the modular decomposition tree is an important\npreprocessing step to solve a large number of combinatorial optimization\nproblems. Since the first polynomial time algorithm in the early 70's, the\nalgorithmic of the modular decomposition has known an important development.\nThis paper survey the ideas and techniques that arose from this line of\nresearch. \n\n"}
{"id": "0912.3563", "contents": "Title: Belief propagation for graph partitioning Abstract: We study the belief propagation algorithm for the graph bi-partitioning\nproblem, i.e. the ground state of the ferromagnetic Ising model at a fixed\nmagnetization. Application of a message passing scheme to a model with a fixed\nglobal parameter is not banal and we show that the magnetization can in fact be\nfixed in a local way within the belief propagation equations. Our method\nprovides the full phase diagram of the bi-partitioning problem on random\ngraphs, as well as an efficient heuristic solver that we anticipate to be\nuseful in a wide range of application of the partitioning problem. \n\n"}
{"id": "1001.2767", "contents": "Title: Universally Optimal Privacy Mechanisms for Minimax Agents Abstract: A scheme that publishes aggregate information about sensitive data must\nresolve the trade-off between utility to information consumers and privacy of\nthe database participants. Differential privacy is a well-established\ndefinition of privacy--this is a universal guarantee against all attackers,\nwhatever their side-information or intent. In this paper, we present a\nuniversal treatment of utility based on the standard minimax rule from decision\ntheory (in contrast to the utility model in, which is Bayesian). In our model,\ninformation consumers are minimax (risk-averse) agents, each possessing some\nside-information about the query, and each endowed with a loss-function which\nmodels their tolerance to inaccuracies. Further, information consumers are\nrational in the sense that they actively combine information from the mechanism\nwith their side-information in a way that minimizes their loss. Under this\nassumption of rational behavior, we show that for every fixed count query, a\ncertain geometric mechanism is universally optimal for all minimax information\nconsumers. Additionally, our solution makes it possible to release query\nresults at multiple levels of privacy in a collusion-resistant manner. \n\n"}
{"id": "1001.5019", "contents": "Title: Lower Bounds for the Complexity of Monadic Second-Order Logic Abstract: Courcelle's famous theorem from 1990 states that any property of graphs\ndefinable in monadic second-order logic (MSO) can be decided in linear time on\nany class of graphs of bounded treewidth, or in other words, MSO is\nfixed-parameter tractable in linear time on any such class of graphs. From a\nlogical perspective, Courcelle's theorem establishes a sufficient condition, or\nan upper bound, for tractability of MSO-model checking.\n  Whereas such upper bounds on the complexity of logics have received\nsignificant attention in the literature, almost nothing is known about\ncorresponding lower bounds. In this paper we establish a strong lower bound for\nthe complexity of monadic second-order logic. In particular, we show that if C\nis any class of graphs which is closed under taking subgraphs and whose\ntreewidth is not bounded by a polylogarithmic function (in fact, $\\log^c n$ for\nsome small c suffices) then MSO-model checking is intractable on C (under a\nsuitable assumption from complexity theory). \n\n"}
{"id": "1003.0079", "contents": "Title: Non-Sparse Regularization for Multiple Kernel Learning Abstract: Learning linear combinations of multiple kernels is an appealing strategy\nwhen the right choice of features is unknown. Previous approaches to multiple\nkernel learning (MKL) promote sparse kernel combinations to support\ninterpretability and scalability. Unfortunately, this 1-norm MKL is rarely\nobserved to outperform trivial baselines in practical applications. To allow\nfor robust kernel mixtures, we generalize MKL to arbitrary norms. We devise new\ninsights on the connection between several existing MKL formulations and\ndevelop two efficient interleaved optimization strategies for arbitrary norms,\nlike p-norms with p>1. Empirically, we demonstrate that the interleaved\noptimization strategies are much faster compared to the commonly used wrapper\napproaches. A theoretical analysis and an experiment on controlled artificial\ndata experiment sheds light on the appropriateness of sparse, non-sparse and\n$\\ell_\\infty$-norm MKL in various scenarios. Empirical applications of p-norm\nMKL to three real-world problems from computational biology show that\nnon-sparse MKL achieves accuracies that go beyond the state-of-the-art. \n\n"}
{"id": "1003.4719", "contents": "Title: Introduction to clarithmetic I Abstract: \"Clarithmetic\" is a generic name for formal number theories similar to Peano\narithmetic, but based on computability logic (see\nhttp://www.cis.upenn.edu/~giorgi/cl.html) instead of the more traditional\nclassical or intuitionistic logics. Formulas of clarithmetical theories\nrepresent interactive computational problems, and their \"truth\" is understood\nas existence of an algorithmic solution. Imposing various complexity\nconstraints on such solutions yields various versions of clarithmetic. The\npresent paper introduces a system of clarithmetic for polynomial time\ncomputability, which is shown to be sound and complete. Sound in the sense that\nevery theorem T of the system represents an interactive number-theoretic\ncomputational problem with a polynomial time solution and, furthermore, such a\nsolution can be efficiently extracted from a proof of T. And complete in the\nsense that every interactive number-theoretic problem with a polynomial time\nsolution is represented by some theorem T of the system. The paper is written\nin a semitutorial style and targets readers with no prior familiarity with\ncomputability logic. \n\n"}
{"id": "1004.0351", "contents": "Title: An Oblivious Spanning Tree for Buy-at-Bulk Network Design Problems Abstract: We consider the problem of constructing a single spanning tree for the\nsingle-source buy-at-bulk network design problem for doubling-dimension graphs.\nWe compute a spanning tree to route a set of demands (or data) along a graph to\nor from a designated root node. The demands could be aggregated at (or\nsymmetrically distributed to) intermediate nodes where the fusion-cost is\nspecified by a non-negative concave function $f$. We describe a novel approach\nfor developing an oblivious spanning tree in the sense that it is independent\nof the number of data sources (or demands) and cost function at intermediate\nnodes. To our knowledge, this is the first paper to propose a single spanning\ntree solution to this problem (as opposed to multiple overlay trees). There has\nbeen no prior work where the tree is oblivious to both the fusion cost function\nand the set of sources (demands). We present a deterministic, polynomial-time\nalgorithm for constructing a spanning tree in low doubling graphs that\nguarantees $\\log^{3}D\\cdot\\log n$-approximation over the optimal cost, where\n$D$ is the diameter of the graph and $n$ the total number of nodes. With\nconstant fusion-cost function our spanning tree gives a $O(\\log^3\nD)$-approximation for every Steiner tree to the root. \n\n"}
{"id": "1004.1434", "contents": "Title: On the uselessness of quantum queries Abstract: Given a prior probability distribution over a set of possible oracle\nfunctions, we define a number of queries to be useless for determining some\nproperty of the function if the probability that the function has the property\nis unchanged after the oracle responds to the queries. A familiar example is\nthe parity of a uniformly random Boolean-valued function over $\\{1,2,...,N\\}$,\nfor which $N-1$ classical queries are useless. We prove that if $2k$ classical\nqueries are useless for some oracle problem, then $k$ quantum queries are also\nuseless. For such problems, which include classical threshold secret sharing\nschemes, our result also gives a new way to obtain a lower bound on the quantum\nquery complexity, even in cases where neither the function nor the property to\nbe determined is Boolean. \n\n"}
{"id": "1004.1485", "contents": "Title: Are there any good digraph width measures? Abstract: Several different measures for digraph width have appeared in the last few\nyears. However, none of them shares all the \"nice\" properties of treewidth:\nFirst, being \\emph{algorithmically useful} i.e. admitting polynomial-time\nalgorithms for all $\\MS1$-definable problems on digraphs of bounded width. And,\nsecond, having nice \\emph{structural properties} i.e. being monotone under\ntaking subdigraphs and some form of arc contractions. As for the former,\n(undirected) $\\MS1$ seems to be the least common denominator of all reasonably\nexpressive logical languages on digraphs that can speak about the edge/arc\nrelation on the vertex set.The latter property is a necessary condition for a\nwidth measure to be characterizable by some version of the cops-and-robber game\ncharacterizing the ordinary treewidth. Our main result is that \\emph{any\nreasonable} algorithmically useful and structurally nice digraph measure cannot\nbe substantially different from the treewidth of the underlying undirected\ngraph. Moreover, we introduce \\emph{directed topological minors} and argue that\nthey are the weakest useful notion of minors for digraphs. \n\n"}
{"id": "1005.2791", "contents": "Title: A note on concentration of submodular functions Abstract: We survey a few concentration inequalities for submodular and fractionally\nsubadditive functions of independent random variables, implied by the entropy\nmethod for self-bounding functions. The power of these concentration bounds is\nthat they are dimension-free, in particular implying standard deviation\nO(\\sqrt{\\E[f]}) rather than O(\\sqrt{n}) which can be obtained for any\n1-Lipschitz function of n variables. \n\n"}
{"id": "1005.3616", "contents": "Title: Conflict-Free Coloring and its Applications Abstract: Let $H=(V,E)$ be a hypergraph. A {\\em conflict-free} coloring of $H$ is an\nassignment of colors to $V$ such that in each hyperedge $e \\in E$ there is at\nleast one uniquely-colored vertex. This notion is an extension of the classical\ngraph coloring. Such colorings arise in the context of frequency assignment to\ncellular antennae, in battery consumption aspects of sensor networks, in RFID\nprotocols and several other fields, and has been the focus of many recent\nresearch papers. In this paper, we survey this notion and its combinatorial and\nalgorithmic aspects. \n\n"}
{"id": "1005.5520", "contents": "Title: The potential to improve the choice: list conflict-free coloring for\n  geometric hypergraphs Abstract: Given a geometric hypergraph (or a range-space) $H=(V,\\cal E)$, a coloring of\nits vertices is said to be conflict-free if for every hyperedge $S \\in \\cal E$\nthere is at least one vertex in $S$ whose color is distinct from the colors of\nall other vertices in $S$. The study of this notion is motivated by frequency\nassignment problems in wireless networks. We study the list-coloring (or\nchoice) version of this notion. In this version, each vertex is associated with\na set of (admissible) colors and it is allowed to be colored only with colors\nfrom its set. List coloring arises naturally in the context of wireless\nnetworks.\n  Our main result is a list coloring algorithm based on a new potential method.\nThe algorithm produces a stronger unique-maximum coloring, in which colors are\npositive integers and the maximum color in every hyperedge occurs uniquely. As\na corollary, we provide asymptotically sharp bounds on the size of the lists\nrequired to assure the existence of such unique-maximum colorings for many\ngeometric hypergraphs (e.g., discs or pseudo-discs in the plane or points with\nrespect to discs). Moreover, we provide an algorithm, such that, given a family\nof lists with the appropriate sizes, computes such a coloring from these lists. \n\n"}
{"id": "1006.1003", "contents": "Title: Fast simulation of large-scale growth models Abstract: We give an algorithm that computes the final state of certain growth models\nwithout computing all intermediate states. Our technique is based on a \"least\naction principle\" which characterizes the odometer function of the growth\nprocess. Starting from an approximation for the odometer, we successively\ncorrect under- and overestimates and provably arrive at the correct final\nstate.\n  Internal diffusion-limited aggregation (IDLA) is one of the models amenable\nto our technique. The boundary fluctuations in IDLA were recently proved to be\nat most logarithmic in the size of the growth cluster, but the constant in\nfront of the logarithm is still not known. As an application of our method, we\ncalculate the size of fluctuations over two orders of magnitude beyond previous\nsimulations, and use the results to estimate this constant. \n\n"}
{"id": "1006.1419", "contents": "Title: Understanding the Quantum Computational Speed-up via De-quantisation Abstract: While it seems possible that quantum computers may allow for algorithms\noffering a computational speed-up over classical algorithms for some problems,\nthe issue is poorly understood. We explore this computational speed-up by\ninvestigating the ability to de-quantise quantum algorithms into classical\nsimulations of the algorithms which are as efficient in both time and space as\nthe original quantum algorithms.\n  The process of de-quantisation helps formulate conditions to determine if a\nquantum algorithm provides a real speed-up over classical algorithms. These\nconditions can be used to develop new quantum algorithms more effectively (by\navoiding features that could allow the algorithm to be efficiently classically\nsimulated), as well as providing the potential to create new classical\nalgorithms (by using features which have proved valuable for quantum\nalgorithms).\n  Results on many different methods of de-quantisations are presented, as well\nas a general formal definition of de-quantisation. De-quantisations employing\nhigher-dimensional classical bits, as well as those using matrix-simulations,\nput emphasis on entanglement in quantum algorithms; a key result is that any\nalgorithm in which the entanglement is bounded is de-quantisable. These methods\nare contrasted with the stabiliser formalism de-quantisations due to the\nGottesman-Knill Theorem, as well as those which take advantage of the topology\nof the circuit for a quantum algorithm.\n  The benefits of the different methods are contrasted, and the importance of a\nrange of techniques is emphasised. We further discuss some features of quantum\nalgorithms which current de-quantisation methods do not cover. \n\n"}
{"id": "1006.2951", "contents": "Title: The Complexity of Proving Chaoticity and the Church-Turing Thesis Abstract: Proving the chaoticity of some dynamical systems is equivalent to solving the\nhardest problems in mathematics. Conversely, one argues that it is not\nunconceivable that classical physical systems may \"compute the hard or even the\nincomputable\" by measuring observables which correspond to computationally hard\nor even incomputable problems. \n\n"}
{"id": "1006.4396", "contents": "Title: Faster Algorithms for Feedback Arc Set Tournament, Kemeny Rank\n  Aggregation and Betweenness Tournament Abstract: We study fixed parameter algorithms for three problems: Kemeny rank\naggregation, feedback arc set tournament, and betweenness tournament. For\nKemeny rank aggregation we give an algorithm with runtime O*(2^O(sqrt{OPT})),\nwhere n is the number of candidates, OPT is the cost of the optimal ranking,\nand O* hides polynomial factors. This is a dramatic improvement on the\npreviously best known runtime of O*(2^O(OPT)). For feedback arc set tournament\nwe give an algorithm with runtime O*(2^O(sqrt{OPT})), an improvement on the\npreviously best known O*(OPT^O(sqrt{OPT})) (Alon, Lokshtanov and Saurabh 2009).\nFor betweenness tournament we give an algorithm with runtime\nO*(2^O(sqrt{OPT/n})), where n is the number of vertices and OPT is the optimal\ncost. This improves on the previously known O*(OPT^O(OPT^{1/3}))$ (Saurabh\n2009), especially when OPT is small. Unusually we can solve instances with OPT\nas large as n (log n)^2 in polynomial time! \n\n"}
{"id": "1007.1604", "contents": "Title: Infectious Random Walks Abstract: We study the dynamics of information (or virus) dissemination by $m$ mobile\nagents performing independent random walks on an $n$-node grid. We formulate\nour results in terms of two scenarios: broadcasting and gossiping. In the\nbroadcasting scenario, the mobile agents are initially placed uniformly at\nrandom among the grid nodes. At time 0, one agent is informed of a rumor and\nstarts a random walk. When an informed agent meets an uninformed agent, the\nlatter becomes informed and starts a new random walk. We study the broadcasting\ntime of the system, that is, the time it takes for all agents to know the\nrumor. In the gossiping scenario, each agent is given a distinct rumor at time\n0 and all agents start random walks. When two agents meet, they share all\nrumors they are aware of. We study the gossiping time of the system, that is,\nthe time it takes for all agents to know all rumors. We prove that both the\nbroadcasting and the gossiping times are $\\tilde\\Theta(n/\\sqrt{m})$ w.h.p.,\nthus achieving a tight characterization up to logarithmic factors. Previous\nresults for the grid provided bounds which were weaker and only concerned\naverage times. In the context of virus infection, a corollary of our results is\nthat static and dynamically moving agents are infected at about the same speed. \n\n"}
{"id": "1007.4191", "contents": "Title: Fast Moment Estimation in Data Streams in Optimal Space Abstract: We give a space-optimal algorithm with update time\nO(log^2(1/eps)loglog(1/eps)) for (1+eps)-approximating the pth frequency\nmoment, 0 < p < 2, of a length-n vector updated in a data stream. This provides\na nearly exponential improvement in the update time complexity over the\nprevious space-optimal algorithm of [Kane-Nelson-Woodruff, SODA 2010], which\nhad update time Omega(1/eps^2). \n\n"}
{"id": "1008.4563", "contents": "Title: Shortest paths between shortest paths and independent sets Abstract: We study problems of reconfiguration of shortest paths in graphs. We prove\nthat the shortest reconfiguration sequence can be exponential in the size of\nthe graph and that it is NP-hard to compute the shortest reconfiguration\nsequence even when we know that the sequence has polynomial length. Moreover,\nwe also study reconfiguration of independent sets in three different models and\nanalyze relationships between these models, observing that shortest path\nreconfiguration is a special case of independent set reconfiguration in perfect\ngraphs, under any of the three models. Finally, we give polynomial results for\nrestricted classes of graphs (even-hole-free and $P_4$-free graphs). \n\n"}
{"id": "1009.0246", "contents": "Title: Explicit Proofs and The Flip Abstract: This article describes a formal strategy of geometric complexity theory (GCT)\nto resolve the {\\em self referential paradox} in the $P$ vs. $NP$ and related\nproblems. The strategy, called the {\\em flip}, is to go for {\\em explicit\nproofs} of these problems. By an explicit proof we mean a proof that constructs\nproof certificates of hardness that are easy to verify, construct and decode.\nThe main result in this paper says that (1) any proof of the arithmetic\nimplication of the $P$ vs. $NP$ conjecture is close to an explicit proof in the\nsense that it can be transformed into an explicit proof by proving in addition\nthat arithmetic circuit identity testing can be derandomized in a blackbox\nfashion, and (2) stronger forms of these arithmetic hardness and\nderandomization conjectures together imply a polynomial time algorithm for a\nformidable explicit construction problem in algebraic geometry. This may\nexplain why these conjectures, which look so elementary at the surface, have\nturned out to be so hard. \n\n"}
{"id": "1009.1381", "contents": "Title: A Branch-and-Reduce Algorithm for Finding a Minimum Independent\n  Dominating Set Abstract: An independent dominating set D of a graph G = (V,E) is a subset of vertices\nsuch that every vertex in V \\ D has at least one neighbor in D and D is an\nindependent set, i.e. no two vertices of D are adjacent in G. Finding a minimum\nindependent dominating set in a graph is an NP-hard problem. Whereas it is hard\nto cope with this problem using parameterized and approximation algorithms,\nthere is a simple exact O(1.4423^n)-time algorithm solving the problem by\nenumerating all maximal independent sets. In this paper we improve the latter\nresult, providing the first non trivial algorithm computing a minimum\nindependent dominating set of a graph in time O(1.3569^n). Furthermore, we give\na lower bound of \\Omega(1.3247^n) on the worst-case running time of this\nalgorithm, showing that the running time analysis is almost tight. \n\n"}
{"id": "1009.2242", "contents": "Title: Minimal-memory realization of pearl-necklace encoders of general quantum\n  convolutional codes Abstract: Quantum convolutional codes, like their classical counterparts, promise to\noffer higher error correction performance than block codes of equivalent\nencoding complexity, and are expected to find important applications in\nreliable quantum communication where a continuous stream of qubits is\ntransmitted. Grassl and Roetteler devised an algorithm to encode a quantum\nconvolutional code with a \"pearl-necklace encoder.\" Despite their theoretical\nsignificance as a neat way of representing quantum convolutional codes, they\nare not well-suited to practical realization. In fact, there is no\nstraightforward way to implement any given pearl-necklace structure. This paper\ncloses the gap between theoretical representation and practical implementation.\nIn our previous work, we presented an efficient algorithm for finding a\nminimal-memory realization of a pearl-necklace encoder for\nCalderbank-Shor-Steane (CSS) convolutional codes. This work extends our\nprevious work and presents an algorithm for turning a pearl-necklace encoder\nfor a general (non-CSS) quantum convolutional code into a realizable quantum\nconvolutional encoder. We show that a minimal-memory realization depends on the\ncommutativity relations between the gate strings in the pearl-necklace encoder.\nWe find a realization by means of a weighted graph which details the\nnon-commutative paths through the pearl-necklace. The weight of the longest\npath in this graph is equal to the minimal amount of memory needed to implement\nthe encoder. The algorithm has a polynomial-time complexity in the number of\ngate strings in the pearl-necklace encoder. \n\n"}
{"id": "1010.1595", "contents": "Title: Using parallel computation to improve Independent Metropolis--Hastings\n  based estimation Abstract: In this paper, we consider the implications of the fact that parallel\nraw-power can be exploited by a generic Metropolis--Hastings algorithm if the\nproposed values are independent. In particular, we present improvements to the\nindependent Metropolis--Hastings algorithm that significantly decrease the\nvariance of any estimator derived from the MCMC output, for a null computing\ncost since those improvements are based on a fixed number of target density\nevaluations. Furthermore, the techniques developed in this paper do not\njeopardize the Markovian convergence properties of the algorithm, since they\nare based on the Rao--Blackwell principles of Gelfand and Smith (1990), already\nexploited in Casella and Robert (1996), Atchade and Perron (2005) and Douc and\nRobert (2010). We illustrate those improvements both on a toy normal example\nand on a classical probit regression model, but stress the fact that they are\napplicable in any case where the independent Metropolis-Hastings is applicable. \n\n"}
{"id": "1010.5016", "contents": "Title: A Unified Framework for Testing Linear-Invariant Properties Abstract: The study of the interplay between the testability of properties of Boolean\nfunctions and the invariances acting on their domain which preserve the\nproperty was initiated by Kaufman and Sudan (STOC 2008). Invariance with\nrespect to F_2-linear transformations is arguably the most common symmetry\nexhibited by natural properties of Boolean functions on the hypercube. Hence,\nan important goal in Property Testing is to describe necessary and sufficient\nconditions for the testability of linear-invariant properties. This direction\nwas explicitly proposed for investigation in a recent survey of Sudan.\n  We obtain the following results:\n  1. We show that every linear-invariant property that can be characterized by\nforbidding induced solutions to a (possibly infinite) set of linear equations\ncan be tested with one-sided error.\n  2. We show that every linear-invariant property that can be tested with\none-sided error can be characterized by forbidding induced solutions to a\n(possibly infinite) set of systems of linear equations.\n  We conjecture that our result from item (1) can be extended to cover systems\nof linear equations. We further show that the validity of this conjecture would\nhave the following implications:\n  1. It would imply that every linear-invariant property that is closed under\nrestrictions to linear subspaces is testable with one-sided error. Such a\nresult would unify several previous results on testing Boolean functions, such\nas the testability of low-degree polynomials and of Fourier dimensionality.\n  2. It would imply that a linear-invariant property P is testable with\none-sided error if and only if P is closed under restrictions to linear\nsubspaces, thus resolving Sudan's problem. \n\n"}
{"id": "1011.0217", "contents": "Title: On Selective Unboundedness of VASS Abstract: Numerous properties of vector addition systems with states amount to checking\nthe (un)boundedness of some selective feature (e.g., number of reversals, run\nlength). Some of these features can be checked in exponential space by using\nRackoff's proof or its variants, combined with Savitch's theorem. However, the\nquestion is still open for many others, e.g., reversal-boundedness. In the\npaper, we introduce the class of generalized unboundedness properties that can\nbe verified in exponential space by extending Rackoff's technique, sometimes in\nan unorthodox way. We obtain new optimal upper bounds, for example for\nplace-boundedness problem, reversal-boundedness detection (several variants\nexist), strong promptness detection problem and regularity detection. Our\nanalysis is sufficiently refined so as we also obtain a polynomial-space bound\nwhen the dimension is fixed. \n\n"}
{"id": "1011.0468", "contents": "Title: Efficient Triangle Counting in Large Graphs via Degree-based Vertex\n  Partitioning Abstract: The number of triangles is a computationally expensive graph statistic which\nis frequently used in complex network analysis (e.g., transitivity ratio), in\nvarious random graph models (e.g., exponential random graph model) and in\nimportant real world applications such as spam detection, uncovering of the\nhidden thematic structure of the Web and link recommendation. Counting\ntriangles in graphs with millions and billions of edges requires algorithms\nwhich run fast, use small amount of space, provide accurate estimates of the\nnumber of triangles and preferably are parallelizable.\n  In this paper we present an efficient triangle counting algorithm which can\nbe adapted to the semistreaming model. The key idea of our algorithm is to\ncombine the sampling algorithm of Tsourakakis et al. and the partitioning of\nthe set of vertices into a high degree and a low degree subset respectively as\nin the Alon, Yuster and Zwick work treating each set appropriately. We obtain a\nrunning time $O \\left(m + \\frac{m^{3/2} \\Delta \\log{n}}{t \\epsilon^2} \\right)$\nand an $\\epsilon$ approximation (multiplicative error), where $n$ is the number\nof vertices, $m$ the number of edges and $\\Delta$ the maximum number of\ntriangles an edge is contained.\n  Furthermore, we show how this algorithm can be adapted to the semistreaming\nmodel with space usage $O\\left(m^{1/2}\\log{n} + \\frac{m^{3/2} \\Delta \\log{n}}{t\n\\epsilon^2} \\right)$ and a constant number of passes (three) over the graph\nstream. We apply our methods in various networks with several millions of edges\nand we obtain excellent results. Finally, we propose a random projection based\nmethod for triangle counting and provide a sufficient condition to obtain an\nestimate with low variance. \n\n"}
{"id": "1011.2787", "contents": "Title: Parallel approximation of min-max problems Abstract: This paper presents an efficient parallel approximation scheme for a new\nclass of min-max problems. The algorithm is derived from the matrix\nmultiplicative weights update method and can be used to find near-optimal\nstrategies for competitive two-party classical or quantum interactions in which\na referee exchanges any number of messages with one party followed by any\nnumber of additional messages with the other. It considerably extends the class\nof interactions which admit parallel solutions, demonstrating for the first\ntime the existence of a parallel algorithm for an interaction in which one\nparty reacts adaptively to the other.\n  As a consequence, we prove that several competing-provers complexity classes\ncollapse to PSPACE such as QRG(2), SQG and two new classes called DIP and DQIP.\nA special case of our result is a parallel approximation scheme for a specific\nclass of semidefinite programs whose feasible region consists of lists of\nsemidefinite matrices that satisfy a transcript-like consistency condition.\nApplied to this special case, our algorithm yields a direct polynomial-space\nsimulation of multi-message quantum interactive proofs resulting in a\nfirst-principles proof of QIP=PSPACE. \n\n"}
{"id": "1011.3843", "contents": "Title: Magnetic Towers of Hanoi and their Optimal Solutions Abstract: The Magnetic Tower of Hanoi puzzle - a modified \"base 3\" version of the\nclassical Tower of Hanoi puzzle as described in earlier papers, is actually a\nsmall set of independent sister-puzzles, depending on the \"pre-coloring\"\ncombination of the tower's posts. Starting with Red facing up on a Source post,\nworking through an Intermediate - colored or Neutral post, and ending Blue\nfacing up on a Destination post, we identify the different pre-coloring\ncombinations in (S,I,D) order. The Tower's pre-coloring combinations are\n{[(R,B,B) / (R,R,B)] ; [(R,B,N) / (N,R,B)] ; [(N,B,N) / (N,R,N)] ; [R,N,B] ;\n[(R,N,N) / (N,N,B)] ; [N,N,N]}. In this paper we investigate these\nsister-puzzles, identify the algorithm that optimally solves each pre-colored\npuzzle, and prove its Optimality. As it turns out, five of the six algorithms,\nchallenging on their own, are part of the algorithm solving the \"natural\", Free\nMagnetic Tower of Hanoi puzzle [N,N,N]. We start by showing that the N-disk\nColored Tower [(R,B,B) / (R,R,B)] is solved by (3^N - 1)/2 moves. Defining\n\"Algorithm Duration\" as the ratio of number of algorithm-moves solving the\npuzzle to the number of algorithm-moves solving the Colored Tower, we find the\nDuration-Limits for all sister-puzzles. In the order of the list above they are\n{[1] ; [10/11] ; [10/11] ; [8/11] ; [7/11] ; [20/33]}. Thus, the Duration-Limit\nof the Optimal Algorithm solving the Free Magnetic Tower of Hanoi puzzle is\n20/33 or 606 0/00. On the road to optimally solve this colorful Magnetic\npuzzle, we hit other \"forward-moving\" puzzle-solving algorithms. Overall we\nlooked at 10 pairs of integer sequences. Of the twenty integer sequences, five\nare listed in the On-line Encyclopedia of Integer Sequences, the other fifteen\n- not yet. The large set of different solutions is a clear indication to the\nfreedom-of-wondering that makes this Magnetic Tower of Hanoi puzzle so\ncolorful. \n\n"}
{"id": "1011.6267", "contents": "Title: Computing multiway cut within the given excess over the largest minimum\n  isolating cut Abstract: Let $(G,T)$ be an instance of the (vertex) multiway cut problem where $G$ is\na graph and $T$ is a set of terminals. For $t \\in T$, a set of nonterminal\nvertices separating $t$ from $T \\setminus \\{T\\}$ is called an \\emph{isolating\ncut} of $t$. The largest among all the smallest isolating cuts is a natural\nlower bound for a multiway cut of $(G,T)$. Denote this lower bound by $m$ and\nlet $k$ be an integer.\n  In this paper we propose an $O(kn^{k+3})$ algorithm that computes a multiway\ncut of $(G,T)$ of size at most $m+k$ or reports that there is no such multiway\ncut. The core of the proposed algorithm is the following combinatorial result.\nLet $G$ be a graph and let $X,Y$ be two disjoint subsets of vertices of $G$.\nLet $m$ be the smallest size of a vertex $X-Y$ separator. Then, for the given\ninteger $k$, the number of \\emph{important} $X-Y$ separators \\cite{MarxTCS} of\nsize at most $m+k$ is at most $\\sum_{i=0}^k{n \\choose i}$. \n\n"}
{"id": "1012.0531", "contents": "Title: Categorical Tensor Network States Abstract: We examine the use of string diagrams and the mathematics of category theory\nin the description of quantum states by tensor networks. This approach lead to\na unification of several ideas, as well as several results and methods that\nhave not previously appeared in either side of the literature. Our approach\nenabled the development of a tensor network framework allowing a solution to\nthe quantum decomposition problem which has several appealing features.\nSpecifically, given an n-body quantum state S, we present a new and general\nmethod to factor S into a tensor network of clearly defined building blocks. We\nuse the solution to expose a previously unknown and large class of quantum\nstates which we prove can be sampled efficiently and exactly. This general\nframework of categorical tensor network states, where a combination of generic\nand algebraically defined tensors appear, enhances the theory of tensor network\nstates. \n\n"}
{"id": "1012.0729", "contents": "Title: Agnostic Learning of Monomials by Halfspaces is Hard Abstract: We prove the following strong hardness result for learning: Given a\ndistribution of labeled examples from the hypercube such that there exists a\nmonomial consistent with $(1-\\eps)$ of the examples, it is NP-hard to find a\nhalfspace that is correct on $(1/2+\\eps)$ of the examples, for arbitrary\nconstants $\\eps > 0$. In learning theory terms, weak agnostic learning of\nmonomials is hard, even if one is allowed to output a hypothesis from the much\nbigger concept class of halfspaces. This hardness result subsumes a long line\nof previous results, including two recent hardness results for the proper\nlearning of monomials and halfspaces. As an immediate corollary of our result\nwe show that weak agnostic learning of decision lists is NP-hard.\n  Our techniques are quite different from previous hardness proofs for\nlearning. We define distributions on positive and negative examples for\nmonomials whose first few moments match. We use the invariance principle to\nargue that regular halfspaces (all of whose coefficients have small absolute\nvalue relative to the total $\\ell_2$ norm) cannot distinguish between\ndistributions whose first few moments match. For highly non-regular subspaces,\nwe use a structural lemma from recent work on fooling halfspaces to argue that\nthey are ``junta-like'' and one can zero out all but the top few coefficients\nwithout affecting the performance of the halfspace. The top few coefficients\nform the natural list decoding of a halfspace in the context of dictatorship\ntests/Label Cover reductions.\n  We note that unlike previous invariance principle based proofs which are only\nknown to give Unique-Games hardness, we are able to reduce from a version of\nLabel Cover problem that is known to be NP-hard. This has inspired follow-up\nwork on bypassing the Unique Games conjecture in some optimal geometric\ninapproximability results. \n\n"}
{"id": "1012.1886", "contents": "Title: Sublinear Time, Measurement-Optimal, Sparse Recovery For All Abstract: An approximate sparse recovery system in ell_1 norm formally consists of\nparameters N, k, epsilon an m-by-N measurement matrix, Phi, and a decoding\nalgorithm, D. Given a vector, x, where x_k denotes the optimal k-term\napproximation to x, the system approximates x by hat_x = D(Phi.x), which must\nsatisfy\n  ||hat_x - x||_1 <= (1+epsilon)||x - x_k||_1.\n  Among the goals in designing such systems are minimizing m and the runtime of\nD. We consider the \"forall\" model, in which a single matrix Phi is used for all\nsignals x.\n  All previous algorithms that use the optimal number m=O(k log(N/k)) of\nmeasurements require superlinear time Omega(N log(N/k)). In this paper, we give\nthe first algorithm for this problem that uses the optimum number of\nmeasurements (up to a constant factor) and runs in sublinear time o(N) when\nk=o(N), assuming access to a data structure requiring space and preprocessing\nO(N). \n\n"}
{"id": "1012.2291", "contents": "Title: Bitwise Quantum Min-Entropy Sampling and New Lower Bounds for Random\n  Access Codes Abstract: Min-entropy sampling gives a bound on the min-entropy of a randomly chosen\nsubset of a string, given a bound on the min-entropy of the whole string.\nK\\\"onig and Renner showed a min-entropy sampling theorem that holds relative to\nquantum knowledge. Their result achieves the optimal rate, but it can only be\napplied if the bits are sampled in blocks, and only gives weak bounds for the\nnon-smooth min-entropy. We give two new quantum min-entropy sampling theorems\nthat do not have the above weaknesses. The first theorem shows that the result\nby K\\\"onig and Renner also applies to bitwise sampling, and the second theorem\ngives a strong bound for the non-smooth min-entropy. Our results imply a new\nlower bound for k-out-of-n random access codes: while previous results by\nBen-Aroya, Regev, and de Wolf showed that the decoding probability is\nexponentially small in k if the storage rate is smaller than 0.7, our results\nimply that this holds for any storage rate strictly smaller than 1, which is\noptimal. \n\n"}
{"id": "1012.2825", "contents": "Title: Improved distance queries in planar graphs Abstract: There are several known data structures that answer distance queries between\ntwo arbitrary vertices in a planar graph. The tradeoff is among preprocessing\ntime, storage space and query time. In this paper we present three data\nstructures that answer such queries, each with its own advantage over previous\ndata structures. The first one improves the query time of data structures of\nlinear space. The second improves the preprocessing time of data structures\nwith a space bound of O(n^(4/3)) or higher while matching the best known query\ntime. The third data structure improves the query time for a similar range of\nspace bounds, at the expense of a longer preprocessing time. The techniques\nthat we use include modifying the parameters of planar graph decompositions,\ncombining the different advantages of existing data structures, and using the\nMonge property for finding minimum elements of matrices. \n\n"}
{"id": "1012.4767", "contents": "Title: Multiple-source multiple-sink maximum flow in planar graphs Abstract: In this paper we show an O(n^(3/2) log^2 n) time algorithm for finding a\nmaximum flow in a planar graph with multiple sources and multiple sinks. This\nis the fastest algorithm whose running time depends only on the number of\nvertices in the graph. For general (non-planar) graphs the multiple-source\nmultiple-sink version of the maximum flow problem is as difficult as the\nstandard single-source single-sink version. However, the standard reduction\ndoes not preserve the planarity of the graph, and it is not known how to\ngeneralize existing maximum flow algorithms for planar graphs to the\nmultiple-source multiple-sink maximum flow problem. \n\n"}
{"id": "1012.5870", "contents": "Title: Multiple-Source Multiple-Sink Maximum Flow in Directed Planar Graphs in\n  $O(n^{1.5} \\log n)$ Time Abstract: We give an $O(n^{1.5} \\log n)$ algorithm that, given a directed planar graph\nwith arc capacities, a set of source nodes and a set of sink nodes, finds a\nmaximum flow from the sources to the sinks. \n\n"}
{"id": "1101.2245", "contents": "Title: Invertible Bloom Lookup Tables Abstract: We present a version of the Bloom filter data structure that supports not\nonly the insertion, deletion, and lookup of key-value pairs, but also allows a\ncomplete listing of its contents with high probability, as long the number of\nkey-value pairs is below a designed threshold. Our structure allows the number\nof key-value pairs to greatly exceed this threshold during normal operation.\nExceeding the threshold simply temporarily prevents content listing and reduces\nthe probability of a successful lookup. If later entries are deleted to return\nthe structure below the threshold, everything again functions appropriately. We\nalso show that simple variations of our structure are robust to certain\nstandard errors, such as the deletion of a key without a corresponding\ninsertion or the insertion of two distinct values for a key. The properties of\nour structure make it suitable for several applications, including database and\nnetworking applications that we highlight. \n\n"}
{"id": "1101.3682", "contents": "Title: Diversification improves interpolation Abstract: We consider the problem of interpolating an unknown multivariate polynomial\nwith coefficients taken from a finite field or as numerical approximations of\ncomplex numbers. Building on the recent work of Garg and Schost, we improve on\nthe best-known algorithm for interpolation over large finite fields by\npresenting a Las Vegas randomized algorithm that uses fewer black box\nevaluations. Using related techniques, we also address numerical interpolation\nof sparse polynomials with complex coefficients, and provide the first provably\nstable algorithm (in the sense of relative error) for this problem, at the cost\nof modestly more evaluations. A key new technique is a randomization which\nmakes all coefficients of the unknown polynomial distinguishable, producing\nwhat we call a diverse polynomial. Another departure from most previous\napproaches is that our algorithms do not rely on root finding as a subroutine.\nWe show how these improvements affect the practical performance with trial\nimplementations. \n\n"}
{"id": "1102.0805", "contents": "Title: Asymptotics of the chromatic number for quasi-line graphs Abstract: As proved by Kahn, the chromatic number and fractional chromatic number of a\nline graph agree asymptotically. That is, for any line graph $G$ we have\n$\\chi(G) \\leq (1+o(1))\\chi_f(G)$. We extend this result to quasi-line graphs,\nan important subclass of claw-free graphs. Furthermore we prove that we can\nconstruct a colouring that achieves this bound in polynomial time, giving us an\nasymptotic approximation algorithm for the chromatic number of quasi-line\ngraphs. \n\n"}
{"id": "1102.3635", "contents": "Title: Rapid mixing of subset Glauber dynamics on graphs of bounded tree-width Abstract: Motivated by the `subgraphs world' view of the ferromagnetic Ising model, we\ndevelop a general approach to studying mixing times of Glauber dynamics based\non subset expansion expressions for a class of graph polynomials. With a\ncanonical paths argument, we demonstrate that the chains defined within this\nframework mix rapidly upon graphs of bounded tree-width. This extends known\nresults on rapid mixing for the Tutte polynomial, the adjacency-rank\n($R_2$-)polynomial and the interlace polynomial. \n\n"}
{"id": "1102.3749", "contents": "Title: Approximation Algorithms for Correlated Knapsacks and Non-Martingale\n  Bandits Abstract: In the stochastic knapsack problem, we are given a knapsack of size B, and a\nset of jobs whose sizes and rewards are drawn from a known probability\ndistribution. However, we know the actual size and reward only when the job\ncompletes. How should we schedule jobs to maximize the expected total reward?\nWe know O(1)-approximations when we assume that (i) rewards and sizes are\nindependent random variables, and (ii) we cannot prematurely cancel jobs. What\ncan we say when either or both of these assumptions are changed?\n  The stochastic knapsack problem is of interest in its own right, but\ntechniques developed for it are applicable to other stochastic packing\nproblems. Indeed, ideas for this problem have been useful for budgeted learning\nproblems, where one is given several arms which evolve in a specified\nstochastic fashion with each pull, and the goal is to pull the arms a total of\nB times to maximize the reward obtained. Much recent work on this problem focus\non the case when the evolution of the arms follows a martingale, i.e., when the\nexpected reward from the future is the same as the reward at the current state.\nWhat can we say when the rewards do not form a martingale?\n  In this paper, we give constant-factor approximation algorithms for the\nstochastic knapsack problem with correlations and/or cancellations, and also\nfor budgeted learning problems where the martingale condition is not satisfied.\nIndeed, we can show that previously proposed LP relaxations have large\nintegrality gaps. We propose new time-indexed LP relaxations, and convert the\nfractional solutions into distributions over strategies, and then use the LP\nvalues and the time ordering information from these strategies to devise a\nrandomized adaptive scheduling algorithm. We hope our LP formulation and\ndecomposition methods may provide a new way to address other correlated bandit\nproblems with more general contexts. \n\n"}
{"id": "1102.3766", "contents": "Title: Derandomizing HSSW Algorithm for 3-SAT Abstract: We present a (full) derandomization of HSSW algorithm for 3-SAT, proposed by\nHofmeister, Sch\\\"oning, Schuler, and Watanabe in [STACS'02]. Thereby, we obtain\nan O(1.3303^n)-time deterministic algorithm for 3-SAT, which is currently\nfastest. \n\n"}
{"id": "1102.5538", "contents": "Title: Pseudo-random graphs and bit probe schemes with one-sided error Abstract: We study probabilistic bit-probe schemes for the membership problem. Given a\nset A of at most n elements from the universe of size m we organize such a\nstructure that queries of type \"Is x in A?\" can be answered very quickly.\nH.Buhrman, P.B.Miltersen, J.Radhakrishnan, and S.Venkatesh proposed a bit-probe\nscheme based on expanders. Their scheme needs space of $O(n\\log m)$ bits, and\nrequires to read only one randomly chosen bit from the memory to answer a\nquery. The answer is correct with high probability with two-sided errors. In\nthis paper we show that for the same problem there exists a bit-probe scheme\nwith one-sided error that needs space of $O(n\\log^2 m+\\poly(\\log m))$ bits. The\ndifference with the model of Buhrman, Miltersen, Radhakrishnan, and Venkatesh\nis that we consider a bit-probe scheme with an auxiliary word. This means that\nin our scheme the memory is split into two parts of different size: the main\nstorage of $O(n\\log^2 m)$ bits and a short word of $\\log^{O(1)}m$ bits that is\npre-computed once for the stored set A and `cached'. To answer a query \"Is x in\nA?\" we allow to read the whole cached word and only one bit from the main\nstorage. For some reasonable values of parameters our space bound is better\nthan what can be achieved by any scheme without cached data. \n\n"}
{"id": "1103.0040", "contents": "Title: From Convex Optimization to Randomized Mechanisms: Toward Optimal\n  Combinatorial Auctions Abstract: We design an expected polynomial-time, truthful-in-expectation,\n(1-1/e)-approximation mechanism for welfare maximization in a fundamental class\nof combinatorial auctions. Our results apply to bidders with valuations that\nare m matroid rank sums (MRS), which encompass most concrete examples of\nsubmodular functions studied in this context, including coverage functions,\nmatroid weighted-rank functions, and convex combinations thereof. Our\napproximation factor is the best possible, even for known and explicitly given\ncoverage valuations, assuming P != NP. Ours is the first\ntruthful-in-expectation and polynomial-time mechanism to achieve a\nconstant-factor approximation for an NP-hard welfare maximization problem in\ncombinatorial auctions with heterogeneous goods and restricted valuations.\n  Our mechanism is an instantiation of a new framework for designing\napproximation mechanisms based on randomized rounding algorithms. A typical\nsuch algorithm first optimizes over a fractional relaxation of the original\nproblem, and then randomly rounds the fractional solution to an integral one.\nWith rare exceptions, such algorithms cannot be converted into truthful\nmechanisms. The high-level idea of our mechanism design framework is to\noptimize directly over the (random) output of the rounding algorithm, rather\nthan over the input to the rounding algorithm. This approach leads to\ntruthful-in-expectation mechanisms, and these mechanisms can be implemented\nefficiently when the corresponding objective function is concave. For bidders\nwith MRS valuations, we give a novel randomized rounding algorithm that leads\nto both a concave objective function and a (1-1/e)-approximation of the optimal\nwelfare. \n\n"}
{"id": "1103.1127", "contents": "Title: Computation and Spacetime Structure Abstract: We investigate the relationship between computation and spacetime structure,\nfocussing on the role of closed timelike curves (CTCs) in promoting\ncomputational speedup. We note first that CTC traversal can be interpreted in\ntwo distinct ways, depending on ones understanding of spacetime. Focussing on\none interpretation leads us to develop a toy universe in which no CTC can be\ntraversed more than once, whence no computational speedup is possible.\nFocussing on the second (and more standard) interpretation leads to the\nsurprising conclusion that CTCs act as perfect information repositories: just\nas black holes have entropy, so do CTCs. If we also assume that P is not equal\nto NP, we find that all observers agree that, even if unbounded time travel\nexisted in their youth, this capability eventually vanishes as they grow older.\nThus the computational assumption \"P is not NP\" is also an assumption\nconcerning cosmological structure. \n\n"}
{"id": "1104.0882", "contents": "Title: Xheal: Localized Self-healing using Expanders Abstract: We consider the problem of self-healing in reconfigurable networks (e.g.\npeer-to-peer and wireless mesh networks) that are under repeated attack by an\nomniscient adversary and propose a fully distributed algorithm, Xheal that\nmaintains good expansion and spectral properties of the network, also keeping\nthe network connected. Moreover, Xheal does this while allowing only low\nstretch and degree increase per node. Thus, the algorithm heals global\nproperties while only doing local changes and using only local information.\n  Our work improves over the self-healing algorithms 'Forgiving tree'[PODC\n2008] and 'Forgiving graph'[PODC 2009] (using a similar model) in that we are\nable to give guarantees on degree and stretch, while at the same time\npreserving the expansion and spectral properties of the network. These repairs\npreserve the invariants in the following sense. At any point in the algorithm,\nthe expansion of the graph will be either `better' than the expansion of the\ngraph formed by considering only the adversarial insertions (not the\nadversarial deletions) or the expansion will be, at least, a constant. Also,\nthe stretch i.e. the distance between any pair of nodes in the healed graph is\nno more than a $O(\\log n)$ factor. Similarly, at any point, a node $v$ whose\ndegree would have been $d$ in the graph with adversarial insertions only, will\nhave degree at most $O(\\kappa d)$ in the actual graph, for a small parameter\n$\\kappa$. We also provide bounds on the second smallest eigenvalue of the\nLaplacian which captures key properties such as mixing time, conductance,\ncongestion in routing etc. Our distributed data structure has low amortized\nlatency and bandwidth requirements. \n\n"}
{"id": "1104.2076", "contents": "Title: A Note On Estimating the Spectral Norm of A Matrix Efficiently Abstract: We give an efficient algorithm which can obtain a relative error\napproximation to the spectral norm of a matrix, combining the power iteration\nmethod with some techniques from matrix reconstruction which use random\nsampling. \n\n"}
{"id": "1104.4618", "contents": "Title: Minimum cell connection and separation in line segment arrangements Abstract: We study the complexity of the following cell connection and separation\nproblems in segment arrangements. Given a set of straight-line segments in the\nplane and two points $a$ and $b$ in different cells of the induced arrangement:\n  (i) compute the minimum number of segments one needs to remove so that there\nis a path connecting $a$ to $b$ that does not intersect any of the remaining\nsegments; (ii) compute the minimum number of segments one needs to remove so\nthat the arrangement induced by the remaining segments has a single cell; (iii)\ncompute the minimum number of segments one needs to retain so that any path\nconnecting $a$ to $b$ intersects some of the retained segments.\n  We show that problems (i) and (ii) are NP-hard and discuss some special,\ntractable cases. Most notably, we provide a linear-time algorithm for a variant\nof problem (i) where the path connecting $a$ to $b$ must stay inside a given\npolygon $P$ with a constant number of holes, the segments are contained in $P$,\nand the endpoints of the segments are on the boundary of $P$. For problem (iii)\nwe provide a cubic-time algorithm. \n\n"}
{"id": "1104.5226", "contents": "Title: Parallelism and Time in Hierarchical Self-Assembly Abstract: We study the role that parallelism plays in time complexity of Winfree's\nabstract Tile Assembly Model (aTAM), a model of molecular algorithmic\nself-assembly. In the \"hierarchical\" aTAM, two assemblies, both consisting of\nmultiple tiles, are allowed to aggregate together, whereas in the \"seeded\"\naTAM, tiles attach one at a time to a growing assembly. Adleman, Cheng, Goel,\nand Huang (\"Running Time and Program Size for Self-Assembled Squares\", STOC\n2001) showed how to assemble an n x n square in O(n) time in the seeded aTAM\nusing O(log n / log log n) unique tile types, where both of these parameters\nare optimal. They asked whether the hierarchical aTAM could allow a tile system\nto use the ability to form large assemblies in parallel before they attach to\nbreak the Omega(n) lower bound for assembly time. We show that there is a tile\nsystem with the optimal O(log n / log log n) tile types that assembles an n x n\nsquare using O(log^2 n) parallel \"stages\", which is close to the optimal\nOmega(log n) stages, forming the final n x n square from four n/2 x n/2\nsquares, which are themselves recursively formed from n/4 x n/4 squares, etc.\nHowever, despite this nearly maximal parallelism, the system requires\nsuperlinear time to assemble the square. We extend the definition of *partial\norder tile systems* studied by Adleman et al. in a natural way to hierarchical\nassembly and show that no hierarchical partial order tile system can build any\nshape with diameter N in less than time Omega(N), demonstrating that in this\ncase the hierarchical model affords no speedup whatsoever over the seeded\nmodel. We strengthen the Omega(N) time lower bound for deterministic seeded\nsystems of Adleman et al. to nondeterministic seeded systems. Finally, we show\nthat for infinitely many n, a tile system can assemble an n x n' rectangle,\nwith n > n', in time O(n^{4/5} log n), breaking the linear-time lower bound. \n\n"}
{"id": "1104.5257", "contents": "Title: The Complexity of Surjective Homomorphism Problems -- a Survey Abstract: We survey known results about the complexity of surjective homomorphism\nproblems, studied in the context of related problems in the literature such as\nlist homomorphism, retraction and compaction. In comparison with these\nproblems, surjective homomorphism problems seem to be harder to classify and we\nexamine especially three concrete problems that have arisen from the\nliterature, two of which remain of open complexity. \n\n"}
{"id": "1105.2391", "contents": "Title: LP-Based Approximation Algorithms for Traveling Salesman Path Problems Abstract: This paper has been merged into 1110.4604. \n\n"}
{"id": "1105.4593", "contents": "Title: Submodular Function Maximization via the Multilinear Relaxation and\n  Contention Resolution Schemes Abstract: We consider the problem of maximizing a non-negative submodular set function\n$f:2^N \\rightarrow \\mathbb{R}_+$ over a ground set $N$ subject to a variety of\npacking type constraints including (multiple) matroid constraints, knapsack\nconstraints, and their intersections. In this paper we develop a general\nframework that allows us to derive a number of new results, in particular when\n$f$ may be a non-monotone function. Our algorithms are based on (approximately)\nmaximizing the multilinear extension $F$ of $f$ over a polytope $P$ that\nrepresents the constraints, and then effectively rounding the fractional\nsolution. Although this approach has been used quite successfully, it has been\nlimited in some important ways. We overcome these limitations as follows.\n  First, we give constant factor approximation algorithms to maximize $F$ over\na down-closed polytope $P$ described by an efficient separation oracle.\nPreviously this was known only for monotone functions. For non-monotone\nfunctions, a constant factor was known only when the polytope was either the\nintersection of a fixed number of knapsack constraints or a matroid polytope.\nSecond, we show that contention resolution schemes are an effective way to\nround a fractional solution, even when $f$ is non-monotone. In particular,\ncontention resolution schemes for different polytopes can be combined to handle\nthe intersection of different constraints. Via LP duality we show that a\ncontention resolution scheme for a constraint is related to the correlation gap\nof weighted rank functions of the constraint. This leads to an optimal\ncontention resolution scheme for the matroid polytope.\n  Our results provide a broadly applicable framework for maximizing linear and\nsubmodular functions subject to independence constraints. We give several\nillustrative examples. Contention resolution schemes may find other\napplications. \n\n"}
{"id": "1105.4924", "contents": "Title: Multiscale Geometric Methods for Data Sets II: Geometric\n  Multi-Resolution Analysis Abstract: Data sets are often modeled as point clouds in $R^D$, for $D$ large. It is\noften assumed that the data has some interesting low-dimensional structure, for\nexample that of a $d$-dimensional manifold $M$, with $d$ much smaller than $D$.\nWhen $M$ is simply a linear subspace, one may exploit this assumption for\nencoding efficiently the data by projecting onto a dictionary of $d$ vectors in\n$R^D$ (for example found by SVD), at a cost $(n+D)d$ for $n$ data points. When\n$M$ is nonlinear, there are no \"explicit\" constructions of dictionaries that\nachieve a similar efficiency: typically one uses either random dictionaries, or\ndictionaries obtained by black-box optimization. In this paper we construct\ndata-dependent multi-scale dictionaries that aim at efficient encoding and\nmanipulating of the data. Their construction is fast, and so are the algorithms\nthat map data points to dictionary coefficients and vice versa. In addition,\ndata points are guaranteed to have a sparse representation in terms of the\ndictionary. We think of dictionaries as the analogue of wavelets, but for\napproximating point clouds rather than functions. \n\n"}
{"id": "1105.6073", "contents": "Title: Reducts of Ramsey structures Abstract: One way of studying a relational structure is to investigate functions which\nare related to that structure and which leave certain aspects of the structure\ninvariant. Examples are the automorphism group, the self-embedding monoid, the\nendomorphism monoid, or the polymorphism clone of a structure. Such functions\ncan be particularly well understood when the relational structure is countably\ninfinite and has a first-order definition in another relational structure which\nhas a finite language, is totally ordered and homogeneous, and has the Ramsey\nproperty. This is because in this situation, Ramsey theory provides the\ncombinatorial tool for analyzing these functions -- in a certain sense, it\nallows to represent such functions by functions on finite sets.\n  This is a survey of results in model theory and theoretical computer science\nobtained recently by the authors in this context. In model theory, we approach\nthe problem of classifying the reducts of countably infinite ordered\nhomogeneous Ramsey structures in a finite language, and certain decidability\nquestions connected with such reducts. In theoretical computer science, we use\nthe same combinatorial methods in order to classify the computational\ncomplexity for various classes of infinite-domain constraint satisfaction\nproblems. While the first set of applications is obviously of an infinitary\ncharacter, the second set concerns genuinely finitary problems -- their\nunifying feature is that the same tools from Ramsey theory are used in their\nsolution. \n\n"}
{"id": "1107.0088", "contents": "Title: Sparse Sums of Positive Semidefinite Matrices Abstract: Recently there has been much interest in \"sparsifying\" sums of rank one\nmatrices: modifying the coefficients such that only a few are nonzero, while\napproximately preserving the matrix that results from the sum. Results of this\nsort have found applications in many different areas, including sparsifying\ngraphs. In this paper we consider the more general problem of sparsifying sums\nof positive semidefinite matrices that have arbitrary rank.\n  We give several algorithms for solving this problem. The first algorithm is\nbased on the method of Batson, Spielman and Srivastava (2009). The second\nalgorithm is based on the matrix multiplicative weights update method of Arora\nand Kale (2007). We also highlight an interesting connection between these two\nalgorithms.\n  Our algorithms have numerous applications. We show how they can be used to\nconstruct graph sparsifiers with auxiliary constraints, sparsifiers of\nhypergraphs, and sparse solutions to semidefinite programs. \n\n"}
{"id": "1107.0789", "contents": "Title: Distributed Matrix Completion and Robust Factorization Abstract: If learning methods are to scale to the massive sizes of modern datasets, it\nis essential for the field of machine learning to embrace parallel and\ndistributed computing. Inspired by the recent development of matrix\nfactorization methods with rich theory but poor computational complexity and by\nthe relative ease of mapping matrices onto distributed architectures, we\nintroduce a scalable divide-and-conquer framework for noisy matrix\nfactorization. We present a thorough theoretical analysis of this framework in\nwhich we characterize the statistical errors introduced by the \"divide\" step\nand control their magnitude in the \"conquer\" step, so that the overall\nalgorithm enjoys high-probability estimation guarantees comparable to those of\nits base algorithm. We also present experiments in collaborative filtering and\nvideo background modeling that demonstrate the near-linear to superlinear\nspeed-ups attainable with this approach. \n\n"}
{"id": "1107.2021", "contents": "Title: Multi-Instance Learning with Any Hypothesis Class Abstract: In the supervised learning setting termed Multiple-Instance Learning (MIL),\nthe examples are bags of instances, and the bag label is a function of the\nlabels of its instances. Typically, this function is the Boolean OR. The\nlearner observes a sample of bags and the bag labels, but not the instance\nlabels that determine the bag labels. The learner is then required to emit a\nclassification rule for bags based on the sample. MIL has numerous\napplications, and many heuristic algorithms have been used successfully on this\nproblem, each adapted to specific settings or applications. In this work we\nprovide a unified theoretical analysis for MIL, which holds for any underlying\nhypothesis class, regardless of a specific application or problem domain. We\nshow that the sample complexity of MIL is only poly-logarithmically dependent\non the size of the bag, for any underlying hypothesis class. In addition, we\nintroduce a new PAC-learning algorithm for MIL, which uses a regular supervised\nlearning algorithm as an oracle. We prove that efficient PAC-learning for MIL\ncan be generated from any efficient non-MIL supervised learning algorithm that\nhandles one-sided error. The computational complexity of the resulting\nalgorithm is only polynomially dependent on the bag size. \n\n"}
{"id": "1107.2188", "contents": "Title: The Simulated Greedy Algorithm for Several Submodular Matroid Secretary\n  Problems Abstract: We study the matroid secretary problems with submodular valuation functions.\nIn these problems, the elements arrive in random order. When one element\narrives, we have to make an immediate and irrevocable decision on whether to\naccept it or not. The set of accepted elements must form an {\\em independent\nset} in a predefined matroid. Our objective is to maximize the value of the\naccepted elements. In this paper, we focus on the case that the valuation\nfunction is a non-negative and monotonically non-decreasing submodular\nfunction.\n  We introduce a general algorithm for such {\\em submodular matroid secretary\nproblems}. In particular, we obtain constant competitive algorithms for the\ncases of laminar matroids and transversal matroids. Our algorithms can be\nfurther applied to any independent set system defined by the intersection of a\n{\\em constant} number of laminar matroids, while still achieving constant\ncompetitive ratios. Notice that laminar matroids generalize uniform matroids\nand partition matroids.\n  On the other hand, when the underlying valuation function is linear, our\nalgorithm achieves a competitive ratio of 9.6 for laminar matroids, which\nsignificantly improves the previous result. \n\n"}
{"id": "1108.0617", "contents": "Title: QMA variants with polynomially many provers Abstract: We study three variants of multi-prover quantum Merlin-Arthur proof systems.\nWe first show that the class of problems that can be efficiently verified using\npolynomially many quantum proofs, each of logarithmic-size, is exactly MQA\n(also known as QCMA), the class of problems which can be efficiently verified\nvia a classical proof and a quantum verifier. We then study the class\nBellQMA(poly), characterized by a verifier who first applies unentangled,\nnonadaptive measurements to each of the polynomially many proofs, followed by\nan arbitrary but efficient quantum verification circuit on the resulting\nmeasurement outcomes. We show that if the number of outcomes per nonadaptive\nmeasurement is a polynomially-bounded function, then the expressive power of\nthe proof system is exactly QMA. Finally, we study a class equivalent to\nQMA(m), denoted SepQMA(m), where the verifier's measurement operator\ncorresponding to outcome \"accept\" is a fully separable operator across the m\nquantum proofs. Using cone programming duality, we give an alternate proof of a\nresult of Harrow and Montanaro [FOCS, pp. 633--642 (2010)] that shows a perfect\nparallel repetition theorem for SepQMA(m) for any m. \n\n"}
{"id": "1108.4803", "contents": "Title: Constraint Satisfaction Problems Parameterized Above or Below Tight\n  Bounds: A Survey Abstract: We consider constraint satisfaction problems parameterized above or below\ntight bounds. One example is MaxSat parameterized above $m/2$: given a CNF\nformula $F$ with $m$ clauses, decide whether there is a truth assignment that\nsatisfies at least $m/2+k$ clauses, where $k$ is the parameter. Among other\nproblems we deal with are MaxLin2-AA (given a system of linear equations over\n$\\mathbb{F}_2$ in which each equation has a positive integral weight, decide\nwhether there is an assignment to the variables that satisfies equations of\ntotal weight at least $W/2+k$, where $W$ is the total weight of all equations),\nMax-$r$-Lin2-AA (the same as MaxLin2-AA, but each equation has at most $r$\nvariables, where $r$ is a constant) and Max-$r$-Sat-AA (given a CNF formula $F$\nwith $m$ clauses in which each clause has at most $r$ literals, decide whether\nthere is a truth assignment satisfying at least $\\sum_{i=1}^m(1-2^{r_i})+k$\nclauses, where $k$ is the parameter, $r_i$ is the number of literals in Clause\n$i$, and $r$ is a constant). We also consider Max-$r$-CSP-AA, a natural\ngeneralization of both Max-$r$-Lin2-AA and Max-$r$-Sat-AA, order (or,\npermutation) constraint satisfaction problems of arities 2 and 3 parameterized\nabove the average value and some other problems related to MaxSat. We discuss\nresults, both polynomial kernels and parameterized algorithms, obtained for the\nproblems mainly in the last few years as well as some open questions. \n\n"}
{"id": "1109.4729", "contents": "Title: Parameterized Complexity of Firefighting Revisited Abstract: The Firefighter problem is to place firefighters on the vertices of a graph\nto prevent a fire with known starting point from lighting up the entire graph.\nIn each time step, a firefighter may be permanently placed on an unburned\nvertex and the fire spreads to its neighborhood in the graph in so far no\nfirefighters are protecting those vertices. The goal is to let as few vertices\nburn as possible. This problem is known to be NP-complete, even when restricted\nto bipartite graphs or to trees of maximum degree three. Initial study showed\nthe Firefighter problem to be fixed-parameter tractable on trees in various\nparameterizations. We complete these results by showing that the problem is in\nFPT on general graphs when parameterized by the number of burned vertices, but\nhas no polynomial kernel on trees, resolving an open problem. Conversely, we\nshow that the problem is W[1]-hard when parameterized by the number of unburned\nvertices, even on bipartite graphs. For both parameterizations, we additionally\ngive refined algorithms on trees, improving on the running times of the known\nalgorithms. \n\n"}
{"id": "1109.5036", "contents": "Title: Testing first-order properties for subclasses of sparse graphs Abstract: We present a linear-time algorithm for deciding first-order (FO) properties\nin classes of graphs with bounded expansion, a notion recently introduced by\nNesetril and Ossona de Mendez. This generalizes several results from the\nliterature, because many natural classes of graphs have bounded expansion:\ngraphs of bounded tree-width, all proper minor-closed classes of graphs, graphs\nof bounded degree, graphs with no subgraph isomorphic to a subdivision of a\nfixed graph, and graphs that can be drawn in a fixed surface in such a way that\neach edge crosses at most a constant number of other edges. We deduce that\nthere is an almost linear-time algorithm for deciding FO properties in classes\nof graphs with locally bounded expansion.\n  More generally, we design a dynamic data structure for graphs belonging to a\nfixed class of graphs of bounded expansion. After a linear-time initialization\nthe data structure allows us to test an FO property in constant time, and the\ndata structure can be updated in constant time after addition/deletion of an\nedge, provided the list of possible edges to be added is known in advance and\ntheir simultaneous addition results in a graph in the class. All our results\nalso hold for relational structures and are based on the seminal result of\nNesetril and Ossona de Mendez on the existence of low tree-depth colorings. \n\n"}
{"id": "1109.5579", "contents": "Title: Strong convergence of partial match queries in random quadtrees Abstract: We prove that the rescaled costs of partial match queries in a random\ntwo-dimensional quadtree converge almost surely towards a random limit which is\nidentified as the terminal value of a martingale. Our approach shares many\nsimilarities with the theory of self-similar fragmentations. \n\n"}
{"id": "1110.1785", "contents": "Title: Voting with Limited Information and Many Alternatives Abstract: The traditional axiomatic approach to voting is motivated by the problem of\nreconciling differences in subjective preferences. In contrast, a dominant line\nof work in the theory of voting over the past 15 years has considered a\ndifferent kind of scenario, also fundamental to voting, in which there is a\ngenuinely \"best\" outcome that voters would agree on if they only had enough\ninformation. This type of scenario has its roots in the classical Condorcet\nJury Theorem; it includes cases such as jurors in a criminal trial who all want\nto reach the correct verdict but disagree in their inferences from the\navailable evidence, or a corporate board of directors who all want to improve\nthe company's revenue, but who have different information that favors different\noptions.\n  This style of voting leads to a natural set of questions: each voter has a\n{\\em private signal} that provides probabilistic information about which option\nis best, and a central question is whether a simple plurality voting system,\nwhich tabulates votes for different options, can cause the group decision to\narrive at the correct option. We show that plurality voting is powerful enough\nto achieve this: there is a way for voters to map their signals into votes for\noptions in such a way that --- with sufficiently many voters --- the correct\noption receives the greatest number of votes with high probability. We show\nfurther, however, that any process for achieving this is inherently expensive\nin the number of voters it requires: succeeding in identifying the correct\noption with probability at least $1 - \\eta$ requires $\\Omega(n^3 \\epsilon^{-2}\n\\log \\eta^{-1})$ voters, where $n$ is the number of options and $\\epsilon$ is a\ndistributional measure of the minimum difference between the options. \n\n"}
{"id": "1110.1894", "contents": "Title: On the Efficiency of Influence-and-Exploit Strategies for Revenue\n  Maximization under Positive Externalities Abstract: We study the problem of revenue maximization in the marketing model for\nsocial networks introduced by (Hartline, Mirrokni, Sundararajan, WWW '08). We\nrestrict our attention to the Uniform Additive Model and mostly focus on\nInfluence-and-Exploit (IE) marketing strategies. We obtain a comprehensive\ncollection of results on the efficiency and the approximability of IE\nstrategies, which also imply a significant improvement on the best known\napproximation ratios for revenue maximization. Specifically, we show that in\nthe Uniform Additive Model, both computing the optimal marketing strategy and\ncomputing the best IE strategy are $\\NP$-hard for undirected social networks.\nWe observe that allowing IE strategies to offer prices smaller than the myopic\nprice in the exploit step leads to a measurable improvement on their\nperformance. Thus, we show that the best IE strategy approximates the maximum\nrevenue within a factor of 0.911 for undirected and of roughly 0.553 for\ndirected networks. Moreover, we present a natural generalization of IE\nstrategies, with more than two pricing classes, and show that they approximate\nthe maximum revenue within a factor of roughly 0.7 for undirected and of\nroughly 0.35 for directed networks. Utilizing a connection between good IE\nstrategies and large cuts in the underlying social network, we obtain\npolynomial-time algorithms that approximate the revenue of the best IE strategy\nwithin a factor of roughly 0.9. Hence, we significantly improve on the best\nknown approximation ratio for revenue maximization to 0.8229 for undirected and\nto 0.5011 for directed networks (from 2/3 and 1/3, respectively, by Hartline et\nal.). \n\n"}
{"id": "1110.1915", "contents": "Title: Further hardness results on the rainbow vertex-connection number of\n  graphs Abstract: A vertex-colored graph $G$ is {\\it rainbow vertex-connected} if any pair of\nvertices in $G$ are connected by a path whose internal vertices have distinct\ncolors, which was introduced by Krivelevich and Yuster. The {\\it rainbow\nvertex-connection number} of a connected graph $G$, denoted by $rvc(G)$, is the\nsmallest number of colors that are needed in order to make $G$ rainbow\nvertex-connected. In a previous paper we showed that it is NP-Complete to\ndecide whether a given graph $G$ has $rvc(G)=2$. In this paper we show that for\nevery integer $k\\geq 2$, deciding whether $rvc(G)\\leq k$ is NP-Hard. We also\nshow that for any fixed integer $k\\geq 2$, this problem belongs to NP-class,\nand so it becomes NP-Complete. \n\n"}
{"id": "1110.2828", "contents": "Title: Testing perfection is hard Abstract: A graph property P is strongly testable if for every fixed \\epsilon>0 there\nis a one-sided \\epsilon-tester for P whose query complexity is bounded by a\nfunction of \\epsilon. In classifying the strongly testable graph properties,\nthe first author and Shapira showed that any hereditary graph property (such as\nP the family of perfect graphs) is strongly testable. A property is easily\ntestable if it is strongly testable with query complexity bounded by a\npolynomial function of \\epsilon^{-1}, and otherwise it is hard. One of our main\nresults shows that testing perfectness is hard. The proof shows that testing\nperfectness is at least as hard as testing triangle-freeness, which is hard. On\nthe other hand, we show that induced P_3-freeness is easily testable. This\nsettles one of the two exceptional graphs, the other being C_4 (and its\ncomplement), left open in the characterization by the first author and Shapira\nof graphs H for which induced H-freeness is easily testable. \n\n"}
{"id": "1110.4493", "contents": "Title: Improved Grammar-Based Compressed Indexes Abstract: We introduce the first grammar-compressed representation of a sequence that\nsupports searches in time that depends only logarithmically on the size of the\ngrammar. Given a text $T[1..u]$ that is represented by a (context-free) grammar\nof $n$ (terminal and nonterminal) symbols and size $N$ (measured as the sum of\nthe lengths of the right hands of the rules), a basic grammar-based\nrepresentation of $T$ takes $N\\lg n$ bits of space. Our representation requires\n$2N\\lg n + N\\lg u + \\epsilon\\, n\\lg n + o(N\\lg n)$ bits of space, for any\n$0<\\epsilon \\le 1$. It can find the positions of the $occ$ occurrences of a\npattern of length $m$ in $T$ in $O((m^2/\\epsilon)\\lg (\\frac{\\lg u}{\\lg n})\n+occ\\lg n)$ time, and extract any substring of length $\\ell$ of $T$ in time\n$O(\\ell+h\\lg(N/h))$, where $h$ is the height of the grammar tree. \n\n"}
{"id": "1111.0623", "contents": "Title: Beating Randomized Response on Incoherent Matrices Abstract: Computing accurate low rank approximations of large matrices is a fundamental\ndata mining task. In many applications however the matrix contains sensitive\ninformation about individuals. In such case we would like to release a low rank\napproximation that satisfies a strong privacy guarantee such as differential\nprivacy. Unfortunately, to date the best known algorithm for this task that\nsatisfies differential privacy is based on naive input perturbation or\nrandomized response: Each entry of the matrix is perturbed independently by a\nsufficiently large random noise variable, a low rank approximation is then\ncomputed on the resulting matrix.\n  We give (the first) significant improvements in accuracy over randomized\nresponse under the natural and necessary assumption that the matrix has low\ncoherence. Our algorithm is also very efficient and finds a constant rank\napproximation of an m x n matrix in time O(mn). Note that even generating the\nnoise matrix required for randomized response already requires time O(mn). \n\n"}
{"id": "1111.0965", "contents": "Title: Many Sparse Cuts via Higher Eigenvalues Abstract: Cheeger's fundamental inequality states that any edge-weighted graph has a\nvertex subset $S$ such that its expansion (a.k.a. conductance) is bounded as\nfollows: \\[ \\phi(S) \\defeq \\frac{w(S,\\bar{S})}{\\min \\set{w(S), w(\\bar{S})}}\n\\leq 2\\sqrt{\\lambda_2} \\] where $w$ is the total edge weight of a subset or a\ncut and $\\lambda_2$ is the second smallest eigenvalue of the normalized\nLaplacian of the graph. Here we prove the following natural generalization: for\nany integer $k \\in [n]$, there exist $ck$ disjoint subsets $S_1, ..., S_{ck}$,\nsuch that \\[ \\max_i \\phi(S_i) \\leq C \\sqrt{\\lambda_{k} \\log k} \\] where\n$\\lambda_i$ is the $i^{th}$ smallest eigenvalue of the normalized Laplacian and\n$c<1,C>0$ are suitable absolute constants. Our proof is via a polynomial-time\nalgorithm to find such subsets, consisting of a spectral projection and a\nrandomized rounding. As a consequence, we get the same upper bound for the\nsmall set expansion problem, namely for any $k$, there is a subset $S$ whose\nweight is at most a $\\bigO(1/k)$ fraction of the total weight and $\\phi(S) \\le\nC \\sqrt{\\lambda_k \\log k}$. Both results are the best possible up to constant\nfactors.\n  The underlying algorithmic problem, namely finding $k$ subsets such that the\nmaximum expansion is minimized, besides extending sparse cuts to more than one\nsubset, appears to be a natural clustering problem in its own right. \n\n"}
{"id": "1111.1055", "contents": "Title: Multi-way spectral partitioning and higher-order Cheeger inequalities Abstract: A basic fact in spectral graph theory is that the number of connected\ncomponents in an undirected graph is equal to the multiplicity of the\neigenvalue zero in the Laplacian matrix of the graph. In particular, the graph\nis disconnected if and only if there are at least two eigenvalues equal to\nzero. Cheeger's inequality and its variants provide an approximate version of\nthe latter fact; they state that a graph has a sparse cut if and only if there\nare at least two eigenvalues that are close to zero.\n  It has been conjectured that an analogous characterization holds for higher\nmultiplicities, i.e., there are $k$ eigenvalues close to zero if and only if\nthe vertex set can be partitioned into $k$ subsets, each defining a sparse cut.\nWe resolve this conjecture. Our result provides a theoretical justification for\nclustering algorithms that use the bottom $k$ eigenvectors to embed the\nvertices into $\\mathbb R^k$, and then apply geometric considerations to the\nembedding.\n  We also show that these techniques yield a nearly optimal tradeoff between\nthe expansion of sets of size $\\approx n/k$, and the $k$th smallest eigenvalue\nof the normalized Laplacian matrix, denoted $\\lambda_k$. In particular, we show\nthat in every graph there is a set of size at most $2n/k$ which has expansion\nat most $O(\\sqrt{\\lambda_k \\log k})$. This bound is tight, up to constant\nfactors, for the \"noisy hypercube\" graphs. \n\n"}
{"id": "1111.1462", "contents": "Title: Uselessness for an Oracle Model with Internal Randomness Abstract: We consider a generalization of the standard oracle model in which the oracle\nacts on the target with a permutation selected according to internal random\ncoins. We describe several problems that are impossible to solve classically\nbut can be solved by a quantum algorithm using a single query; we show that\nsuch infinity-vs-one separations between classical and quantum query\ncomplexities can be constructed from much weaker separations.\n  We also give conditions to determine when oracle problems---either in the\nstandard model, or in any of the generalizations we consider---cannot be solved\nwith success probability better than random guessing would achieve. In the\noracle model with internal randomness where the goal is to gain any nonzero\nadvantage over guessing, we prove (roughly speaking) that $k$ quantum queries\nare equivalent in power to $2k$ classical queries, thus extending results of\nMeyer and Pommersheim. \n\n"}
{"id": "1111.1750", "contents": "Title: Near Linear-Work Parallel SDD Solvers, Low-Diameter Decomposition, and\n  Low-Stretch Subgraphs Abstract: We present the design and analysis of a near linear-work parallel algorithm\nfor solving symmetric diagonally dominant (SDD) linear systems. On input of a\nSDD $n$-by-$n$ matrix $A$ with $m$ non-zero entries and a vector $b$, our\nalgorithm computes a vector $\\tilde{x}$ such that $\\norm[A]{\\tilde{x} - A^+b}\n\\leq \\vareps \\cdot \\norm[A]{A^+b}$ in $O(m\\log^{O(1)}{n}\\log{\\frac1\\epsilon})$\nwork and $O(m^{1/3+\\theta}\\log \\frac1\\epsilon)$ depth for any fixed $\\theta >\n0$.\n  The algorithm relies on a parallel algorithm for generating low-stretch\nspanning trees or spanning subgraphs. To this end, we first develop a parallel\ndecomposition algorithm that in polylogarithmic depth and $\\otilde(|E|)$ work,\npartitions a graph into components with polylogarithmic diameter such that only\na small fraction of the original edges are between the components. This can be\nused to generate low-stretch spanning trees with average stretch\n$O(n^{\\alpha})$ in $O(n^{1+\\alpha})$ work and $O(n^{\\alpha})$ depth.\nAlternatively, it can be used to generate spanning subgraphs with\npolylogarithmic average stretch in $\\otilde(|E|)$ work and polylogarithmic\ndepth. We apply this subgraph construction to derive a parallel linear system\nsolver. By using this solver in known applications, our results imply improved\nparallel randomized algorithms for several problems, including single-source\nshortest paths, maximum flow, minimum-cost flow, and approximate maximum flow. \n\n"}
{"id": "1111.5442", "contents": "Title: Improved Lower Bounds for the Shortest Superstring and Related Problems Abstract: We study the approximation hardness of the Shortest Superstring, the Maximal\nCompression and the Maximum Asymmetric Traveling Salesperson (MAX-ATSP)\nproblem. We introduce a new reduction method that produces strongly restricted\ninstances of the Shortest Superstring problem, in which the maximal orbit size\nis eight (with no character appearing more than eight times) and all given\nstrings having length four. Based on this reduction method, we are able to\nimprove the best up to now known approximation lower bound for the Shortest\nSuperstring problem and the Maximal Compression problem by an order of\nmagnitude. The results imply also an improved approximation lower bound for the\nMAX-ATSP problem. \n\n"}
{"id": "1111.5986", "contents": "Title: The Clique Problem in Ray Intersection Graphs Abstract: Ray intersection graphs are intersection graphs of rays, or halflines, in the\nplane. We show that any planar graph has an even subdivision whose complement\nis a ray intersection graph. The construction can be done in polynomial time\nand implies that finding a maximum clique in a segment intersection graph is\nNP-hard. This solves a 21-year old open problem posed by Kratochv\\'il and\nNe\\v{s}et\\v{r}il. \n\n"}
{"id": "1111.6990", "contents": "Title: Shortest Non-trivial Cycles in Directed and Undirected Surface Graphs Abstract: Let G be a graph embedded on a surface of genus g with b boundary cycles. We\ndescribe algorithms to compute multiple types of non-trivial cycles in G, using\ndifferent techniques depending on whether or not G is an undirected graph. If G\nis undirected, then we give an algorithm to compute a shortest non-separating\ncycle in 2^O(g) n log log n time. Similar algorithms are given to compute a\nshortest non-contractible or non-null-homologous cycle in 2^O(g+b) n log log n\ntime. Our algorithms for undirected G combine an algorithm of Kutz with known\ntechniques for efficiently enumerating homotopy classes of curves that may be\nshortest non-trivial cycles.\n  Our main technical contributions in this work arise from assuming G is a\ndirected graph with possibly asymmetric edge weights. For this case, we give an\nalgorithm to compute a shortest non-contractible cycle in G in O((g^3 + g b)n\nlog n) time. In order to achieve this time bound, we use a restriction of the\ninfinite cyclic cover that may be useful in other contexts. We also describe an\nalgorithm to compute a shortest non-null-homologous cycle in G in O((g^2 + g\nb)n log n) time, extending a known algorithm of Erickson to compute a shortest\nnon-separating cycle. In both the undirected and directed cases, our algorithms\nimprove the best time bounds known for many values of g and b. \n\n"}
{"id": "1112.1313", "contents": "Title: The Target Set Selection Problem on Cycle Permutation Graphs,\n  Generalized Petersen Graphs and Torus Cordalis Abstract: In this paper we consider a fundamental problem in the area of viral\nmarketing, called T{\\scriptsize ARGET} S{\\scriptsize ET} S{\\scriptsize\nELECTION} problem.\n  In a a viral marketing setting, social networks are modeled by graphs with\npotential customers of a new product as vertices and friend relationships as\nedges, where each vertex $v$ is assigned a threshold value $\\theta(v)$. The\nthresholds represent the different latent tendencies of customers (vertices) to\nbuy the new product when their friend (neighbors) do.\n  Consider a repetitive process on social network $(G,\\theta)$ where each\nvertex $v$ is associated with two states, active and inactive, which indicate\nwhether $v$ is persuaded into buying the new product. Suppose we are given a\ntarget set $S\\subseteq V(G)$. Initially, all vertices in $G$ are inactive. At\ntime step 0, we choose all vertices in $S$ to become active.\n  Then, at every time step $t>0$, all vertices that were active in time step\n$t-1$ remain active, and we activate any vertex $v$ if at least $\\theta(v)$ of\nits neighbors were active at time step $t-1$. The activation process terminates\nwhen no more vertices can get activated. We are interested in the following\noptimization problem, called T{\\scriptsize ARGET} S{\\scriptsize ET}\nS{\\scriptsize ELECTION}: Finding a target set $S$ of smallest possible size\nthat activates all vertices of $G$. There is an important and well-studied\nthreshold called strict majority threshold, where for every vertex $v$ in $G$\nwe have $\\theta(v)=\\lceil{(d(v) +1)/2}\\rceil$ and $d(v)$ is the degree of $v$\nin $G$. In this paper, we consider the T{\\scriptsize ARGET} S{\\scriptsize ET}\nS{\\scriptsize ELECTION} problem under strict majority thresholds and focus on\nthree popular regular network structures: cycle permutation graphs, generalized\nPetersen graphs and torus cordalis. \n\n"}
{"id": "1112.1538", "contents": "Title: Jungles, bundles, and fixed parameter tractability Abstract: We give a fixed-parameter tractable (FPT) approximation algorithm computing\nthe path-width of a tournament, and more generally, of a semi-complete digraph.\nBased on this result, we prove that topological containment and rooted\nimmersion problems are FPT on semi-complete digraphs. \n\n"}
{"id": "1112.2489", "contents": "Title: Effective de Rham Cohomology - The Hypersurface Case Abstract: We prove an effective bound for the degrees of generators of the algebraic de\nRham cohomology of smooth affine hypersurfaces. In particular, we show that the\nde Rham cohomology H_dR^p(X) of a smooth hypersurface X of degree d in C^n can\nbe generated by differential forms of degree d^O(pn). This result is relevant\nfor the algorithmic computation of the cohomology, but is also motivated by\nquestions in the theory of ordinary differential equations related to the\ninfinitesimal Hilbert 16th problem. \n\n"}
{"id": "1112.4131", "contents": "Title: Uncommon Suffix Tries Abstract: Common assumptions on the source producing the words inserted in a suffix\ntrie with $n$ leaves lead to a $\\log n$ height and saturation level. We provide\nan example of a suffix trie whose height increases faster than a power of $n$\nand another one whose saturation level is negligible with respect to $\\log n$.\nBoth are built from VLMC (Variable Length Markov Chain) probabilistic sources;\nthey are easily extended to families of sources having the same properties. The\nfirst example corresponds to a \"logarithmic infinite comb\" and enjoys a non\nuniform polynomial mixing. The second one corresponds to a \"factorial infinite\ncomb\" for which mixing is uniform and exponential. \n\n"}
{"id": "1112.5947", "contents": "Title: Random Context and Semi-Conditional Insertion-Deletion Systems Abstract: In this article we introduce the operations of insertion and deletion working\nin a random-context and semi-conditional manner. We show that the conditional\nuse of rules strictly increase the computational power. In the case of\nsemi-conditional insertion-deletion systems context-free insertion and deletion\nrules of one symbol are sufficient to get the computational completeness. In\nthe random context case our results expose an asymmetry between the\ncomputational power of insertion and deletion rules: systems of size $(2,0,0;\n1,1,0)$ are computationally complete, while systems of size $(1,1,0;2,0,0)$\n(and more generally of size $(1,1,0;p,1,1)$) are not. This is particularly\ninteresting because other control mechanisms like graph-control or matrix\ncontrol used together with insertion-deletion systems do not present such\nasymmetry. \n\n"}
{"id": "1201.0127", "contents": "Title: Faster Subset Selection for Matrices and Applications Abstract: We study subset selection for matrices defined as follows: given a matrix\n$\\matX \\in \\R^{n \\times m}$ ($m > n$) and an oversampling parameter $k$ ($n \\le\nk \\le m$), select a subset of $k$ columns from $\\matX$ such that the\npseudo-inverse of the subsampled matrix has as smallest norm as possible. In\nthis work, we focus on the Frobenius and the spectral matrix norms. We describe\nseveral novel (deterministic and randomized) approximation algorithms for this\nproblem with approximation bounds that are optimal up to constant factors.\nAdditionally, we show that the combinatorial problem of finding a low-stretch\nspanning tree in an undirected graph corresponds to subset selection, and\ndiscuss various implications of this reduction. \n\n"}
{"id": "1201.0365", "contents": "Title: Lower bounding edit distances between permutations Abstract: A number of fields, including the study of genome rearrangements and the\ndesign of interconnection networks, deal with the connected problems of sorting\npermutations in \"as few moves as possible\", using a given set of allowed\noperations, or computing the number of moves the sorting process requires,\noften referred to as the \\emph{distance} of the permutation. These operations\noften act on just one or two segments of the permutation, e.g. by reversing one\nsegment or exchanging two segments. The \\emph{cycle graph} of the permutation\nto sort is a fundamental tool in the theory of genome rearrangements, and has\nproved useful in settling the complexity of many variants of the above\nproblems. In this paper, we present an algebraic reinterpretation of the cycle\ngraph of a permutation $\\pi$ as an even permutation $\\bar{\\pi}$, and show how\nto reformulate our sorting problems in terms of particular factorisations of\nthe latter permutation. Using our framework, we recover known results in a\nsimple and unified way, and obtain a new lower bound on the \\emph{prefix\ntransposition distance} (where a \\emph{prefix transposition} displaces the\ninitial segment of a permutation), which is shown to outperform previous\nresults. Moreover, we use our approach to improve the best known lower bound on\nthe \\emph{prefix transposition diameter} from $2n/3$ to $\\lfloor3n/4\\rfloor$,\nand investigate a few relations between some statistics on $\\pi$ and\n$\\bar{\\pi}$. \n\n"}
{"id": "1201.0856", "contents": "Title: Complexity Classification in Infinite-Domain Constraint Satisfaction Abstract: A constraint satisfaction problem (CSP) is a computational problem where the\ninput consists of a finite set of variables and a finite set of constraints,\nand where the task is to decide whether there exists a satisfying assignment of\nvalues to the variables. Depending on the type of constraints that we allow in\nthe input, a CSP might be tractable, or computationally hard. In recent years,\ngeneral criteria have been discovered that imply that a CSP is polynomial-time\ntractable, or that it is NP-hard. Finite-domain CSPs have become a major common\nresearch focus of graph theory, artificial intelligence, and finite model\ntheory. It turned out that the key questions for complexity classification of\nCSPs are closely linked to central questions in universal algebra.\n  This thesis studies CSPs where the variables can take values from an infinite\ndomain. This generalization enhances dramatically the range of computational\nproblems that can be modeled as a CSP. Many problems from areas that have so\nfar seen no interaction with constraint satisfaction theory can be formulated\nusing infinite domains, e.g. problems from temporal and spatial reasoning,\nphylogenetic reconstruction, and operations research.\n  It turns out that the universal-algebraic approach can also be applied to\nstudy large classes of infinite-domain CSPs, yielding elegant complexity\nclassification results. A new tool in this thesis that becomes relevant\nparticularly for infinite domains is Ramsey theory. We demonstrate the\nfeasibility of our approach with two complete complexity classification\nresults: one on CSPs in temporal reasoning, the other on a generalization of\nSchaefer's theorem for propositional logic to logic over graphs. We also study\nthe limits of complexity classification, and present classes of computational\nproblems provably do not exhibit a complexity dichotomy into hard and easy\nproblems. \n\n"}
{"id": "1201.3318", "contents": "Title: Notes on Bit-reversal Broadcast Scheduling Abstract: This report contains revision and extension of some results about RBO\n[arXiv:1108.5095]. RBO is a simple and efficient broadcast scheduling of $n =\n2^k$ uniform frames for battery powered radio receivers. Each frame contains a\nkey from some arbitrary linearly ordered universe. The broadcast cycle -- a\nsequence of frames sorted by the keys and permuted by $k$-bit reversal -- is\ntransmitted in a round robin fashion by the broadcaster. At arbitrary time\nduring the transmission, the receiver may start a simple protocol that reports\nto him all the frames with the keys that are contained in a specified interval\nof the key values $[K', K\"]$. RBO receives at most $2 k + 1$ other frames' keys\nbefore receiving the first key from $[K', K\"]$ or noticing that there are no\nsuch keys in the broadcast cycle. As a simple corollary, $4 k + 2$ is upper\nbound the number of keys outside $[K', K\"]$ that will ever be received. In\nunreliable network the expected number of efforts to receive such frames is\nbounded by $(8 k + 4) / p + 2 (1 - p) / p^2$, where $p$ is probability of\nsuccessful reception, and the reception rate of the requested frames is $p$ --\nthe highest possible. The receiver's protocol state consists of the values $k$,\n$K'$ and $K\"$, one wake-up timer and two other $k$-bit variables. Its only\nnontrivial computation -- the computation of the next wake-up time slot -- can\nbe performed in $O (k)$ simple operations, such as arithmetic/bit-wise\noperations on $k$-bit numbers, using only constant number of $k$-bit variables. \n\n"}
{"id": "1201.4603", "contents": "Title: Rainbow Connectivity of Sparse Random Graphs Abstract: An edge colored graph $G$ is rainbow edge connected if any two vertices are\nconnected by a path whose edges have distinct colors. The rainbow connectivity\nof a connected graph $G$, denoted by $rc(G)$, is the smallest number of colors\nthat are needed in order to make $G$ rainbow connected.\n  In this work we study the rainbow connectivity of binomial random graphs at\nthe connectivity threshold $p=\\frac{\\log n+\\om}{n}$ where $\\om=\\om(n)\\to\\infty$\nand ${\\om}=o(\\log{n})$ and of random $r$-regular graphs where $r \\geq 3$ is a\nfixed integer. Specifically, we prove that the rainbow connectivity $rc(G)$ of\n$G=G(n,p)$ satisfies $rc(G) \\sim \\max\\set{Z_1,diameter(G)}$ with high\nprobability (\\whp). Here $Z_1$ is the number of vertices in $G$ whose degree\nequals 1 and the diameter of $G$ is asymptotically equal to $\\diam$ \\whp.\nFinally, we prove that the rainbow connectivity $rc(G)$ of the random\n$r$-regular graph $G=G(n,r)$ satisfies $rc(G) =O(\\log^2{n})$ \\whp. \n\n"}
{"id": "1202.0302", "contents": "Title: Kernels on Sample Sets via Nonparametric Divergence Estimates Abstract: Most machine learning algorithms, such as classification or regression, treat\nthe individual data point as the object of interest. Here we consider extending\nmachine learning algorithms to operate on groups of data points. We suggest\ntreating a group of data points as an i.i.d. sample set from an underlying\nfeature distribution for that group. Our approach employs kernel machines with\na kernel on i.i.d. sample sets of vectors. We define certain kernel functions\non pairs of distributions, and then use a nonparametric estimator to\nconsistently estimate those functions based on sample sets. The projection of\nthe estimated Gram matrix to the cone of symmetric positive semi-definite\nmatrices enables us to use kernel machines for classification, regression,\nanomaly detection, and low-dimensional embedding in the space of distributions.\nWe present several numerical experiments both on real and simulated datasets to\ndemonstrate the advantages of our new approach. \n\n"}
{"id": "1202.2624", "contents": "Title: A linear-time algorithm for finding a complete graph minor in a dense\n  graph Abstract: Let g(t) be the minimum number such that every graph G with average degree\nd(G) \\geq g(t) contains a K_{t}-minor. Such a function is known to exist, as\noriginally shown by Mader. Kostochka and Thomason independently proved that\ng(t) \\in \\Theta(t*sqrt{log t}). This article shows that for all fixed \\epsilon\n> 0 and fixed sufficiently large t \\geq t(\\epsilon), if d(G) \\geq\n(2+\\epsilon)g(t) then we can find this K_{t}-minor in linear time. This\nimproves a previous result by Reed and Wood who gave a linear-time algorithm\nwhen d(G) \\geq 2^{t-2}. \n\n"}
{"id": "1202.3367", "contents": "Title: Faster Approximate Multicommodity Flow Using Quadratically Coupled Flows Abstract: The maximum multicommodity flow problem is a natural generalization of the\nmaximum flow problem to route multiple distinct flows. Obtaining a $1-\\epsilon$\napproximation to the multicommodity flow problem on graphs is a well-studied\nproblem. In this paper we present an adaptation of recent advances in\nsingle-commodity flow algorithms to this problem. As the underlying linear\nsystems in the electrical problems of multicommodity flow problems are no\nlonger Laplacians, our approach is tailored to generate specialized systems\nwhich can be preconditioned and solved efficiently using Laplacians. Given an\nundirected graph with m edges and k commodities, we give algorithms that find\n$1-\\epsilon$ approximate solutions to the maximum concurrent flow problem and\nthe maximum weighted multicommodity flow problem in time\n$\\tilde{O}(m^{4/3}\\poly(k,\\epsilon^{-1}))$. \n\n"}
{"id": "1202.4301", "contents": "Title: Algebraic Independence in Positive Characteristic -- A p-Adic Calculus Abstract: A set of multivariate polynomials, over a field of zero or large\ncharacteristic, can be tested for algebraic independence by the well-known\nJacobian criterion. For fields of other characteristic p>0, there is no\nanalogous characterization known. In this paper we give the first such\ncriterion. Essentially, it boils down to a non-degeneracy condition on a lift\nof the Jacobian polynomial over (an unramified extension of) the ring of p-adic\nintegers.\n  Our proof builds on the de Rham-Witt complex, which was invented by Illusie\n(1979) for crystalline cohomology computations, and we deduce a natural\ngeneralization of the Jacobian. This new avatar we call the Witt-Jacobian. In\nessence, we show how to faithfully differentiate polynomials over F_p (i.e.\nsomehow avoid dx^p/dx=0) and thus capture algebraic independence.\n  We apply the new criterion to put the problem of testing algebraic\nindependence in the complexity class NP^#P (previously best was PSPACE). Also,\nwe give a modest application to the problem of identity testing in algebraic\ncomplexity theory. \n\n"}
{"id": "1202.4419", "contents": "Title: Induced Disjoint Paths in Claw-Free Graphs Abstract: Paths P1,...,Pk in a graph G=(V,E) are said to be mutually induced if for any\n1 <= i < j <= k, Pi and Pj have neither common vertices nor adjacent vertices\n(except perhaps their end-vertices). The Induced Disjoint Paths problem is to\ntest whether a graph G with k pairs of specified vertices (si,ti) contains k\nmutually induced paths Pi such that Pi connects si and ti for i=1,...,k. We\nshow that this problem is fixed-parameter tractable for claw-free graphs when\nparameterized by k. Several related problems, such as the k-in-a-Path problem,\nare proven to be fixed-parameter tractable for claw-free graphs as well. We\nshow that an improvement of these results in certain directions is unlikely,\nfor example by noting that the Induced Disjoint Paths problem cannot have a\npolynomial kernel for line graphs (a type of claw-free graphs), unless NP\n\\subseteq coNP/poly. Moreover, the problem becomes NP-complete, even when k=2,\nfor the more general class of K_1,4-free graphs. Finally, we show that the\nn^O(k)-time algorithm of Fiala et al. for testing whether a claw-free graph\ncontains some k-vertex graph H as a topological induced minor is essentially\noptimal by proving that this problem is W[1]-hard even if G and H are line\ngraphs. \n\n"}
{"id": "1202.4504", "contents": "Title: A Constant Factor Approximation Algorithm for Reordering Buffer\n  Management Abstract: In the reordering buffer management problem (RBM) a sequence of $n$ colored\nitems enters a buffer with limited capacity $k$. When the buffer is full, one\nitem is removed to the output sequence, making room for the next input item.\nThis step is repeated until the input sequence is exhausted and the buffer is\nempty. The objective is to find a sequence of removals that minimizes the total\nnumber of color changes in the output sequence. The problem formalizes numerous\napplications in computer and production systems, and is known to be NP-hard.\n  We give the first constant factor approximation guarantee for RBM. Our\nalgorithm is based on an intricate \"rounding\" of the solution to an LP\nrelaxation for RBM, so it also establishes a constant upper bound on the\nintegrality gap of this relaxation. Our results improve upon the best previous\nbound of $O(\\sqrt{\\log k})$ of Adamaszek et al. (STOC 2011) that used different\nmethods and gave an online algorithm. Our constant factor approximation beats\nthe super-constant lower bounds on the competitive ratio given by Adamaszek et\nal. This is the first demonstration of an offline algorithm for RBM that is\nprovably better than any online algorithm. \n\n"}
{"id": "1202.5049", "contents": "Title: Efficient Algorithms for Solving Hypergraphic Steiner Tree Relaxations\n  in Quasi-Bipartite Instances Abstract: We consider the Steiner tree problem in quasi-bipartite graphs, where no two\nSteiner vertices are connected by an edge. For this class of instances, we\npresent an efficient algorithm to exactly solve the so called directed\ncomponent relaxation (DCR), a specific form of hypergraphic LP relaxation that\nwas instrumental in the recent break-through result by Byrka et al. [BGRS10]\n(STOC 2010). Our algorithm hinges on an efficiently computable map from extreme\npoints of the bidirected cut relaxation to feasible solutions of (DCR). As a\nconsequence, together with [BGRS10] we immediately obtain an efficient\n73/60-approximation for quasi-bipartite Steiner tree instances. We also present\na particularly simple (BCR)-based random sampling algorithm that achieves a\nperformance guarantee slightly better than 77/60. \n\n"}
{"id": "1202.5749", "contents": "Title: Fixed-parameter tractability of multicut in directed acyclic graphs Abstract: The MULTICUT problem, given a graph G, a set of terminal pairs T={(s_i,t_i) |\n1 <= i <= r} and an integer p, asks whether one can find a cutset consisting of\nat most p non-terminal vertices that separates all the terminal pairs, i.e.,\nafter removing the cutset, t_i is not reachable from s_i for each 1 <= i <= r.\nThe fixed-parameter tractability of MULTICUT in undirected graphs,\nparameterized by the size of the cutset only, has been recently proven by Marx\nand Razgon (STOC'11) and, independently, by Bousquet et al. (STOC'11), after\nresisting attacks as a long-standing open problem. In this paper we prove that\nMULTICUT is fixed-parameter tractable on directed acyclic graphs, when\nparameterized both by the size of the cutset and the number of terminal pairs.\nWe complement this result by showing that this is implausible for\nparameterization by the size of the cutset only, as this version of the problem\nremains W[1]-hard. \n\n"}
{"id": "1202.5888", "contents": "Title: An efficient algorithm for the diameter of Cayley graphs generated by\n  transposition trees Abstract: A problem of practical and theoretical interest is to determine or estimate\nthe diameter of various families of Cayley networks. The previously known\nestimate for the diameter of Cayley graphs generated by transposition trees is\nan upper bound given in the oft-cited paper of Akers and Krishnamurthy (1989).\nIn this work, we first assess the performance of their upper bound. We show\nthat for every $n$, there exists a tree on $n$ vertices, such that the\ndifference between the upper bound and the true diameter value is at least\n$n-4$.\n  Evaluating their upper bound takes time $\\Omega(n!)$. In this paper, we\nprovide an algorithm that obtains an estimate of the diameter, but which\nrequires only time $O(n^2)$; furthermore, the value obtained by our algorithm\nis less than or equal to the previously known diameter upper bound. Such an\nimprovement to polynomial time, while still performing at least as well as the\nprevious bound, is possible because our algorithm works directly with the\ntransposition tree on $n$ vertices and does not require examining any of the\npermutations. We also provide a tree for which the value computed by our\nalgorithm is not necessarily unique, which is an important result because such\nexamples are quite rare. For all families of trees we have investigated so far,\neach of the possible values computed by our algorithm happens to also be an\nupper bound on the diameter. \n\n"}
{"id": "1202.6101", "contents": "Title: Maximum Inner-Product Search using Tree Data-structures Abstract: The problem of {\\em efficiently} finding the best match for a query in a\ngiven set with respect to the Euclidean distance or the cosine similarity has\nbeen extensively studied in literature. However, a closely related problem of\nefficiently finding the best match with respect to the inner product has never\nbeen explored in the general setting to the best of our knowledge. In this\npaper we consider this general problem and contrast it with the existing\nbest-match algorithms. First, we propose a general branch-and-bound algorithm\nusing a tree data structure. Subsequently, we present a dual-tree algorithm for\nthe case where there are multiple queries. Finally we present a new data\nstructure for increasing the efficiency of the dual-tree algorithm. These\nbranch-and-bound algorithms involve novel bounds suited for the purpose of\nbest-matching with inner products. We evaluate our proposed algorithms on a\nvariety of data sets from various applications, and exhibit up to five orders\nof magnitude improvement in query time over the naive search technique. \n\n"}
{"id": "1203.0833", "contents": "Title: Faster Parameterized Algorithms using Linear Programming Abstract: We investigate the parameterized complexity of Vertex Cover parameterized by\nthe difference between the size of the optimal solution and the value of the\nlinear programming (LP) relaxation of the problem. By carefully analyzing the\nchange in the LP value in the branching steps, we argue that combining\npreviously known preprocessing rules with the most straightforward branching\nalgorithm yields an $O^*((2.618)^k)$ algorithm for the problem. Here $k$ is the\nexcess of the vertex cover size over the LP optimum, and we write $O^*(f(k))$\nfor a time complexity of the form $O(f(k)n^{O(1)})$, where $f (k)$ grows\nexponentially with $k$. We proceed to show that a more sophisticated branching\nalgorithm achieves a runtime of $O^*(2.3146^k)$.\n  Following this, using known and new reductions, we give $O^*(2.3146^k)$\nalgorithms for the parameterized versions of Above Guarantee Vertex Cover, Odd\nCycle Transversal, Split Vertex Deletion and Almost 2-SAT, and an\n$O^*(1.5214^k)$ algorithm for Ko\\\"nig Vertex Deletion, Vertex Cover Param by\nOCT and Vertex Cover Param by KVD. These algorithms significantly improve the\nbest known bounds for these problems. The most notable improvement is the new\nbound for Odd Cycle Transversal - this is the first algorithm which beats the\ndependence on $k$ of the seminal $O^*(3^k)$ algorithm of Reed, Smith and Vetta.\nFinally, using our algorithm, we obtain a kernel for the standard\nparameterization of Vertex Cover with at most $2k - c \\log k$ vertices. Our\nkernel is simpler than previously known kernels achieving the same size bound. \n\n"}
{"id": "1203.2888", "contents": "Title: Report on \"Mathematical Aspects of P vs. NP and its Variants.\" Abstract: This is a report on a workshop held August 1 to August 5, 2011 at the\nInstitute for Computational and Experimental Research in Mathematics (ICERM) at\nBrown University, Providence, Rhode Island, organized by Saugata Basu, Joseph\nM. Landsberg, and J. Maurice Rojas. We provide overviews of the more recent\nresults presented at the workshop, including some works-in-progress as well as\ntentative and intriguing ideas for new directions. The main themes we discuss\nare representation theory and geometry in the Mulmuley-Sohoni Geometric\nComplexity Theory Program, and number theory and other ideas in the\nBlum-Shub-Smale model. \n\n"}
{"id": "1203.3578", "contents": "Title: Iterative rounding approximation algorithms for degree-bounded\n  node-connectivity network design Abstract: We consider the problem of finding a minimum edge cost subgraph of a graph\nsatisfying both given node-connectivity requirements and degree upper bounds on\nnodes. We present an iterative rounding algorithm of the biset LP relaxation\nfor this problem. For directed graphs and $k$-out-connectivity requirements\nfrom a root, our algorithm computes a solution that is a 2-approximation on the\ncost, and the degree of each node $v$ in the solution is at most $2b(v) + O(k)$\nwhere $b(v)$ is the degree upper bound on $v$. For undirected graphs and\nelement-connectivity requirements with maximum connectivity requirement $k$,\nour algorithm computes a solution that is a $4$-approximation on the cost, and\nthe degree of each node $v$ in the solution is at most $4b(v)+O(k)$. These\nratios improve the previous $O(\\log k)$-approximation on the cost and $O(2^k\nb(v))$ approximation on the degrees. Our algorithms can be used to improve\napproximation ratios for other node-connectivity problems such as undirected\n$k$-out-connectivity, directed and undirected $k$-connectivity, and undirected\nrooted $k$-connectivity and subset $k$-connectivity. \n\n"}
{"id": "1203.4041", "contents": "Title: When the Cut Condition is Enough: A Complete Characterization for\n  Multiflow Problems in Series-Parallel Networks Abstract: Let $G=(V,E)$ be a supply graph and $H=(V,F)$ a demand graph defined on the\nsame set of vertices. An assignment of capacities to the edges of $G$ and\ndemands to the edges of $H$ is said to satisfy the \\emph{cut condition} if for\nany cut in the graph, the total demand crossing the cut is no more than the\ntotal capacity crossing it. The pair $(G,H)$ is called \\emph{cut-sufficient} if\nfor any assignment of capacities and demands that satisfy the cut condition,\nthere is a multiflow routing the demands defined on $H$ within the network with\ncapacities defined on $G$. We prove a previous conjecture, which states that\nwhen the supply graph $G$ is series-parallel, the pair $(G,H)$ is\ncut-sufficient if and only if $(G,H)$ does not contain an \\emph{odd spindle} as\na minor; that is, if it is impossible to contract edges of $G$ and delete edges\nof $G$ and $H$ so that $G$ becomes the complete bipartite graph $K_{2,p}$, with\n$p\\geq 3$ odd, and $H$ is composed of a cycle connecting the $p$ vertices of\ndegree 2, and an edge connecting the two vertices of degree $p$. We further\nprove that if the instance is \\emph{Eulerian} --- that is, the demands and\ncapacities are integers and the total of demands and capacities incident to\neach vertex is even --- then the multiflow problem has an integral solution. We\nprovide a polynomial-time algorithm to find an integral solution in this case.\nIn order to prove these results, we formulate properties of tight cuts (cuts\nfor which the cut condition inequality is tight) in cut-sufficient pairs. We\nbelieve these properties might be useful in extending our results to planar\ngraphs. \n\n"}
{"id": "1203.5453", "contents": "Title: Optimal Private Halfspace Counting via Discrepancy Abstract: A range counting problem is specified by a set $P$ of size $|P| = n$ of\npoints in $\\mathbb{R}^d$, an integer weight $x_p$ associated to each point $p\n\\in P$, and a range space ${\\cal R} \\subseteq 2^{P}$. Given a query range $R\n\\in {\\cal R}$, the target output is $R(\\vec{x}) = \\sum_{p \\in R}{x_p}$. Range\ncounting for different range spaces is a central problem in Computational\nGeometry.\n  We study $(\\epsilon, \\delta)$-differentially private algorithms for range\ncounting. Our main results are for the range space given by hyperplanes, that\nis, the halfspace counting problem. We present an $(\\epsilon,\n\\delta)$-differentially private algorithm for halfspace counting in $d$\ndimensions which achieves $O(n^{1-1/d})$ average squared error. This contrasts\nwith the $\\Omega(n)$ lower bound established by the classical result of Dinur\nand Nissim [PODS 2003] for arbitrary subset counting queries. We also show a\nmatching lower bound on average squared error for any $(\\epsilon,\n\\delta)$-differentially private algorithm for halfspace counting. Both bounds\nare obtained using discrepancy theory. For the lower bound, we use a modified\ndiscrepancy measure and bound approximation of $(\\epsilon,\n\\delta)$-differentially private algorithms for range counting queries in terms\nof this discrepancy. We also relate the modified discrepancy measure to\nclassical combinatorial discrepancy, which allows us to exploit known\ndiscrepancy lower bounds. This approach also yields a lower bound of\n$\\Omega((\\log n)^{d-1})$ for $(\\epsilon, \\delta)$-differentially private\northogonal range counting in $d$ dimensions, the first known superconstant\nlower bound for this problem. For the upper bound, we use an approach inspired\nby partial coloring methods for proving discrepancy upper bounds, and obtain\n$(\\epsilon, \\delta)$-differentially private algorithms for range counting with\npolynomially bounded shatter function range spaces. \n\n"}
{"id": "1203.6020", "contents": "Title: How to solve kSAT in polynomial time Abstract: With using of multi-nary logic analytic formulas proposition that \"kSAT is in\nP and could be solved in $O(n^{3.5})$\" was proved \n\n"}
{"id": "1204.0062", "contents": "Title: Improved matrix algorithms via the Subsampled Randomized Hadamard\n  Transform Abstract: Several recent randomized linear algebra algorithms rely upon fast dimension\nreduction methods. A popular choice is the Subsampled Randomized Hadamard\nTransform (SRHT). In this article, we address the efficacy, in the Frobenius\nand spectral norms, of an SRHT-based low-rank matrix approximation technique\nintroduced by Woolfe, Liberty, Rohklin, and Tygert. We establish a slightly\nbetter Frobenius norm error bound than currently available, and a much sharper\nspectral norm error bound (in the presence of reasonable decay of the singular\nvalues). Along the way, we produce several results on matrix operations with\nSRHTs (such as approximate matrix multiplication) that may be of independent\ninterest. Our approach builds upon Tropp's in \"Improved analysis of the\nSubsampled Randomized Hadamard Transform\". \n\n"}
{"id": "1204.1939", "contents": "Title: Random walks which prefer unvisited edges. Exploring high girth even\n  degree expanders in linear time Abstract: We consider a modified random walk which uses unvisited edges whenever\npossible, and makes a simple random walk otherwise. We call such a walk an\nedge-process. We assume there is a rule A, which tells the walk which unvisited\nedge to use whenever there is a choice. In the simplest case, A is a uniform\nrandom choice over unvisited edges incident with the current walk position.\nHowever we do not exclude arbitrary choices of rule A. For example, the rule\ncould be determined on-line by an adversary, or could vary from vertex to\nvertex.\n  For even degree expander graphs, of bounded maximum degree, we have the\nfollowing result. Let G be an n vertex even degree expander graph, for which\nevery vertex is in at least one vertex induced cycle of length L. Any\nedge-process on G has cover time (n+ (n log n)/L). This result is independent\nof the rule A used to select the order of the unvisited edges, which can be\nchosen on-line by an adversary.\n  As an example, With high probability, random r-regular graphs, (r at least 4,\neven), are expanders for which L = Omega(log n). Thus, for almost all such\ngraphs, the vertex cover time of the edge-process is Theta(n). This improves\nthe vertex cover time of such graphs by a factor of log n, compared to the\nOmega(n log n) cover time of any weighted random walk. \n\n"}
{"id": "1204.3488", "contents": "Title: Efficient sub-5 approximations for minimum dominating sets in unit disk\n  graphs Abstract: A unit disk graph is the intersection graph of n congruent disks in the\nplane. Dominating sets in unit disk graphs are widely studied due to their\napplication in wireless ad-hoc networks. Because the minimum dominating set\nproblem for unit disk graphs is NP-hard, numerous approximation algorithms have\nbeen proposed in the literature, including some PTAS. However, since the\nproposal of a linear-time 5-approximation algorithm in 1995, the lack of\nefficient algorithms attaining better approximation factors has aroused\nattention. We introduce a linear-time O(n+m) approximation algorithm that takes\nthe usual adjacency representation of the graph as input and outputs a\n44/9-approximation. This approximation factor is also attained by a second\nalgorithm, which takes the geometric representation of the graph as input and\nruns in O(n log n) time regardless of the number of edges. Additionally, we\npropose a 43/9-approximation which can be obtained in O(n^2 m) time given only\nthe graph's adjacency representation. It is noteworthy that the dominating sets\nobtained by our algorithms are also independent sets. \n\n"}
{"id": "1204.4666", "contents": "Title: Finding Small Sparse Cuts Locally by Random Walk Abstract: We study the problem of finding a small sparse cut in an undirected graph.\nGiven an undirected graph G=(V,E) and a parameter k <= |E|, the small sparsest\ncut problem is to find a subset of vertices S with minimum conductance among\nall sets with volume at most k. Using ideas developed in local graph\npartitioning algorithms, we obtain the following bicriteria approximation\nalgorithms for the small sparsest cut problem:\n  - If there is a subset U with conductance \\phi and vol(U) <= k, then there is\na polynomial time algorithm to find a set S with conductance\nO(\\sqrt{\\phi/\\epsilon}) and vol(S) <= k^{1+\\epsilon} for any \\epsilon > 1/k.\n  - If there is a subset U with conductance \\phi and vol(U) <= k, then there is\na polynomial time algorithm to find a set S with conductance O(\\sqrt{\\phi\nln(k)/\\epsilon}) and vol(S) <= (1+\\epsilon)k for any \\epsilon > 2ln(k)/k.\n  These algorithms can be implemented locally using truncated random walk, with\nrunning time almost linear to the output size. This provides a local graph\npartitioning algorithm with a better conductance guarantee when k is sublinear. \n\n"}
{"id": "1204.5823", "contents": "Title: A Bicriteria Approximation for the Reordering Buffer Problem Abstract: In the reordering buffer problem (RBP), a server is asked to process a\nsequence of requests lying in a metric space. To process a request the server\nmust move to the corresponding point in the metric. The requests can be\nprocessed slightly out of order; in particular, the server has a buffer of\ncapacity k which can store up to k requests as it reads in the sequence. The\ngoal is to reorder the requests in such a manner that the buffer constraint is\nsatisfied and the total travel cost of the server is minimized. The RBP arises\nin many applications that require scheduling with a limited buffer capacity,\nsuch as scheduling a disk arm in storage systems, switching colors in paint\nshops of a car manufacturing plant, and rendering 3D images in computer\ngraphics.\n  We study the offline version of RBP and develop bicriteria approximations.\nWhen the underlying metric is a tree, we obtain a solution of cost no more than\n9OPT using a buffer of capacity 4k + 1 where OPT is the cost of an optimal\nsolution with buffer capacity k. Constant factor approximations were known\npreviously only for the uniform metric (Avigdor-Elgrabli et al., 2012). Via\nrandomized tree embeddings, this implies an O(log n) approximation to cost and\nO(1) approximation to buffer size for general metrics. Previously the best\nknown algorithm for arbitrary metrics by Englert et al. (2007) provided an\nO(log^2 k log n) approximation without violating the buffer constraint. \n\n"}
{"id": "1205.0458", "contents": "Title: Better Balance by Being Biased: A 0.8776-Approximation for Max Bisection Abstract: Recently Raghavendra and Tan (SODA 2012) gave a 0.85-approximation algorithm\nfor the Max Bisection problem. We improve their algorithm to a\n0.8776-approximation. As Max Bisection is hard to approximate within\n$\\alpha_{GW} + \\epsilon \\approx 0.8786$ under the Unique Games Conjecture\n(UGC), our algorithm is nearly optimal. We conjecture that Max Bisection is\napproximable within $\\alpha_{GW}-\\epsilon$, i.e., the bisection constraint\n(essentially) does not make Max Cut harder.\n  We also obtain an optimal algorithm (assuming the UGC) for the analogous\nvariant of Max 2-Sat. Our approximation ratio for this problem exactly matches\nthe optimal approximation ratio for Max 2-Sat, i.e., $\\alpha_{LLZ} + \\epsilon\n\\approx 0.9401$, showing that the bisection constraint does not make Max 2-Sat\nharder. This improves on a 0.93-approximation for this problem due to\nRaghavendra and Tan. \n\n"}
{"id": "1205.0968", "contents": "Title: Information Complexity versus Corruption and Applications to\n  Orthogonality and Gap-Hamming Abstract: Three decades of research in communication complexity have led to the\ninvention of a number of techniques to lower bound randomized communication\ncomplexity. The majority of these techniques involve properties of large\nsubmatrices (rectangles) of the truth-table matrix defining a communication\nproblem. The only technique that does not quite fit is information complexity,\nwhich has been investigated over the last decade. Here, we connect information\ncomplexity to one of the most powerful \"rectangular\" techniques: the\nrecently-introduced smooth corruption (or \"smooth rectangle\") bound. We show\nthat the former subsumes the latter under rectangular input distributions. We\nconjecture that this subsumption holds more generally, under arbitrary\ndistributions, which would resolve the long-standing direct sum question for\nrandomized communication. As an application, we obtain an optimal $\\Omega(n)$\nlower bound on the information complexity---under the {\\em uniform\ndistribution}---of the so-called orthogonality problem (ORT), which is in turn\nclosely related to the much-studied Gap-Hamming-Distance (GHD). The proof of\nthis bound is along the lines of recent communication lower bounds for GHD, but\nwe encounter a surprising amount of additional technical detail. \n\n"}
{"id": "1205.2761", "contents": "Title: Multi-Prover Quantum Merlin-Arthur Proof Systems with Small Gap Abstract: This paper studies multiple-proof quantum Merlin-Arthur (QMA) proof systems\nin the setting when the completeness-soundness gap is small. Small means that\nwe only lower-bound the gap with an inverse-exponential function of the input\nlength, or with an even smaller function. Using the protocol of Blier and Tapp\n[arXiv:0709.0738], we show that in this case the proof system has the same\nexpressive power as non-deterministic exponential time (NEXP). Since\nsingle-proof QMA proof systems, with the same bound on the gap, have expressive\npower at most exponential time (EXP), we get a separation between single and\nmulti-prover proof systems in the 'small-gap setting', under the assumption\nthat EXP is not equal to NEXP. This implies, among others, the nonexistence of\ncertain operators called disentanglers (defined by Aaronson et al.\n[arXiv:0804.0802]), with good approximation parameters.\n  We also show that in this setting the proof system has the same expressive\npower if we restrict the verifier to be able to perform only Bell-measurements,\ni.e., using a BellQMA verifier. This is not known to hold in the usual setting,\nwhen the gap is bounded by an inverse-polynomial function of the input length.\nTo show this we use the protocol of Chen and Drucker [arXiv:1011.0716]. The\nonly caveat here is that we need at least a linear amount of proofs to achieve\nthe power of NEXP, while in the previous setting two proofs were enough.\n  We also study the case when the proof-lengths are only logarithmic in the\ninput length and observe that in some cases the expressive power decreases.\nHowever, we show that it doesn't decrease further if we make the proof lengths\nto be even shorter. \n\n"}
{"id": "1205.5075", "contents": "Title: Efficient Sparse Group Feature Selection via Nonconvex Optimization Abstract: Sparse feature selection has been demonstrated to be effective in handling\nhigh-dimensional data. While promising, most of the existing works use convex\nmethods, which may be suboptimal in terms of the accuracy of feature selection\nand parameter estimation. In this paper, we expand a nonconvex paradigm to\nsparse group feature selection, which is motivated by applications that require\nidentifying the underlying group structure and performing feature selection\nsimultaneously. The main contributions of this article are twofold: (1)\nstatistically, we introduce a nonconvex sparse group feature selection model\nwhich can reconstruct the oracle estimator. Therefore, consistent feature\nselection and parameter estimation can be achieved; (2) computationally, we\npropose an efficient algorithm that is applicable to large-scale problems.\nNumerical results suggest that the proposed nonconvex method compares favorably\nagainst its competitors on synthetic data and real-world applications, thus\nachieving desired goal of delivering high performance. \n\n"}
{"id": "1206.1623", "contents": "Title: Proximal Newton-type methods for minimizing composite functions Abstract: We generalize Newton-type methods for minimizing smooth functions to handle a\nsum of two convex functions: a smooth function and a nonsmooth function with a\nsimple proximal mapping. We show that the resulting proximal Newton-type\nmethods inherit the desirable convergence behavior of Newton-type methods for\nminimizing smooth functions, even when search directions are computed\ninexactly. Many popular methods tailored to problems arising in bioinformatics,\nsignal processing, and statistical learning are special cases of proximal\nNewton-type methods, and our analysis yields new convergence results for some\nof these methods. \n\n"}
{"id": "1206.4912", "contents": "Title: Preprocessing Subgraph and Minor Problems: When Does a Small Vertex\n  Cover Help? Abstract: We prove a number of results around kernelization of problems parameterized\nby the size of a given vertex cover of the input graph. We provide three sets\nof simple general conditions characterizing problems admitting kernels of\npolynomial size. Our characterizations not only give generic explanations for\nthe existence of many known polynomial kernels for problems like q-Coloring,\nOdd Cycle Transversal, Chordal Deletion, Eta Transversal, or Long Path,\nparameterized by the size of a vertex cover, but also imply new polynomial\nkernels for problems like F-Minor-Free Deletion, which is to delete at most k\nvertices to obtain a graph with no minor from a fixed finite set F.\n  While our characterization captures many interesting problems, the\nkernelization complexity landscape of parameterizations by vertex cover is much\nmore involved. We demonstrate this by several results about induced subgraph\nand minor containment testing, which we find surprising. While it was known\nthat testing for an induced complete subgraph has no polynomial kernel unless\nNP is in coNP/poly, we show that the problem of testing if a graph contains a\ncomplete graph on t vertices as a minor admits a polynomial kernel. On the\nother hand, it was known that testing for a path on t vertices as a minor\nadmits a polynomial kernel, but we show that testing for containment of an\ninduced path on t vertices is unlikely to admit a polynomial kernel. \n\n"}
{"id": "1206.5941", "contents": "Title: Kernelization Lower Bounds By Cross-Composition Abstract: We introduce the cross-composition framework for proving kernelization lower\nbounds. A classical problem L AND/OR-cross-composes into a parameterized\nproblem Q if it is possible to efficiently construct an instance of Q with\npolynomially bounded parameter value that expresses the logical AND or OR of a\nsequence of instances of L. Building on work by Bodlaender et al. (ICALP 2008)\nand using a result by Fortnow and Santhanam (STOC 2008) with a refinement by\nDell and van Melkebeek (STOC 2010), we show that if an NP-hard problem\nOR-cross-composes into a parameterized problem Q then Q does not admit a\npolynomial kernel unless NP \\subseteq coNP/poly and the polynomial hierarchy\ncollapses. Similarly, an AND-cross-composition for Q rules out polynomial\nkernels for Q under Bodlaender et al.'s AND-distillation conjecture.\n  Our technique generalizes and strengthens the recent techniques of using\ncomposition algorithms and of transferring the lower bounds via polynomial\nparameter transformations. We show its applicability by proving kernelization\nlower bounds for a number of important graphs problems with structural\n(non-standard) parameterizations, e.g., Clique, Chromatic Number, Weighted\nFeedback Vertex Set, and Weighted Odd Cycle Transversal do not admit polynomial\nkernels with respect to the vertex cover number of the input graphs unless the\npolynomial hierarchy collapses, contrasting the fact that these problems are\ntrivially fixed-parameter tractable for this parameter.\n  After learning of our results, several teams of authors have successfully\napplied the cross-composition framework to different parameterized problems.\nFor completeness, our presentation of the framework includes several extensions\nbased on this follow-up work. For example, we show how a relaxed version of\nOR-cross-compositions may be used to give lower bounds on the degree of the\npolynomial in the kernel size. \n\n"}
{"id": "1206.6381", "contents": "Title: Shortest path distance in random k-nearest neighbor graphs Abstract: Consider a weighted or unweighted k-nearest neighbor graph that has been\nbuilt on n data points drawn randomly according to some density p on R^d. We\nstudy the convergence of the shortest path distance in such graphs as the\nsample size tends to infinity. We prove that for unweighted kNN graphs, this\ndistance converges to an unpleasant distance function on the underlying space\nwhose properties are detrimental to machine learning. We also study the\nbehavior of the shortest path distance in weighted kNN graphs. \n\n"}
{"id": "1207.1831", "contents": "Title: Optimal Euclidean spanners: really short, thin and lanky Abstract: In a seminal STOC'95 paper, titled \"Euclidean spanners: short, thin and\nlanky\", Arya et al. devised a construction of Euclidean $(1+\\eps)$-spanners\nthat achieves constant degree, diameter $O(\\log n)$, and weight $O(\\log^2 n)\n\\cdot \\omega(MST)$, and has running time $O(n \\cdot \\log n)$. This construction\napplies to $n$-point constant-dimensional Euclidean spaces. Moreover, Arya et\nal. conjectured that the weight bound can be improved by a logarithmic factor,\nwithout increasing the degree and the diameter of the spanner, and within the\nsame running time.\n  This conjecture of Arya et al. became a central open problem in the area of\nEuclidean spanners.\n  In this paper we resolve the long-standing conjecture of Arya et al. in the\naffirmative. Specifically, we present a construction of spanners with the same\nstretch, degree, diameter, and running time, as in Arya et al.'s result, but\nwith optimal weight $O(\\log n) \\cdot \\omega(MST)$.\n  Moreover, our result is more general in three ways. First, we demonstrate\nthat the conjecture holds true not only in constant-dimensional Euclidean\nspaces, but also in doubling metrics. Second, we provide a general tradeoff\nbetween the three involved parameters, which is tight in the entire range.\nThird, we devise a transformation that decreases the lightness of spanners in\ngeneral metrics, while keeping all their other parameters in check. Our main\nresult is obtained as a corollary of this transformation. \n\n"}
{"id": "1207.4537", "contents": "Title: Reduction from non-injective hidden shift problem to injective hidden\n  shift problem Abstract: We introduce a simple tool that can be used to reduce non-injective instances\nof the hidden shift problem over arbitrary group to injective instances over\nthe same group. In particular, we show that the average-case non-injective\nhidden shift problem admit this reduction. We show similar results for\n(non-injective) hidden shift problem for bent functions. We generalize the\nnotion of influence and show how it relates to applicability of this tool for\ndoing reductions. In particular, these results can be used to simplify the main\nresults by Gavinsky, Roetteler, and Roland about the hidden shift problem for\nthe Boolean-valued functions and bent functions, and also to generalize their\nresults to non-Boolean domains (thereby answering an open question that they\npose). \n\n"}
{"id": "1207.6549", "contents": "Title: Analysis of an exhaustive search algorithm in random graphs and the\n  n^{c\\log n} -asymptotics Abstract: We analyze the cost used by a naive exhaustive search algorithm for finding a\nmaximum independent set in random graphs under the usual G_{n,p} -model where\neach possible edge appears independently with the same probability p. The\nexpected cost turns out to be of the less common asymptotic order n^{c\\log n},\nwhich we explore from several different perspectives. Also we collect many\ninstances where such an order appears, from algorithmics to analysis, from\nprobability to algebra. The limiting distribution of the cost required by the\nalgorithm under a purely idealized random model is proved to be normal. The\napproach we develop is of some generality and is amenable for other graph\nalgorithms. \n\n"}
{"id": "1207.6655", "contents": "Title: A 2D Nearest-Neighbor Quantum Architecture for Factoring in\n  Polylogarithmic Depth Abstract: We contribute a 2D nearest-neighbor quantum architecture for Shor's algorithm\nto factor an $n$-bit number in $O(\\log^2(n))$ depth. Our implementation uses\nparallel phase estimation, constant-depth fanout and teleportation, and\nconstant-depth carry-save modular addition. We derive upper bounds on the\ncircuit resources of our architecture under a new 2D nearest-neighbor model\nwhich allows a classical controller and parallel, communicating modules. We\nalso contribute a novel constant-depth circuit for unbounded quantum unfanout\nin our new model. Finally, we provide a comparison to all previous\nnearest-neighbor factoring implementations. Our circuit results in an\nexponential improvement in nearest-neighbor circuit depth at the cost of a\npolynomial increase in circuit size and width. \n\n"}
{"id": "1208.0526", "contents": "Title: Optimization hardness as transient chaos in an analog approach to\n  constraint satisfaction Abstract: Boolean satisfiability [1] (k-SAT) is one of the most studied optimization\nproblems, as an efficient (that is, polynomial-time) solution to k-SAT (for\n$k\\geq 3$) implies efficient solutions to a large number of hard optimization\nproblems [2,3]. Here we propose a mapping of k-SAT into a deterministic\ncontinuous-time dynamical system with a unique correspondence between its\nattractors and the k-SAT solution clusters. We show that beyond a constraint\ndensity threshold, the analog trajectories become transiently chaotic [4-7],\nand the boundaries between the basins of attraction [8] of the solution\nclusters become fractal [7-9], signaling the appearance of optimization\nhardness [10]. Analytical arguments and simulations indicate that the system\nalways finds solutions for satisfiable formulae even in the frozen regimes of\nrandom 3-SAT [11] and of locked occupation problems [12] (considered among the\nhardest algorithmic benchmarks); a property partly due to the system's\nhyperbolic [4,13] character. The system finds solutions in polynomial\ncontinuous-time, however, at the expense of exponential fluctuations in its\nenergy function. \n\n"}
{"id": "1208.2767", "contents": "Title: Boolean networks synchronism sensitivity and XOR circulant networks\n  convergence time Abstract: In this paper are presented first results of a theoretical study on the role\nof non-monotone interactions in Boolean automata networks. We propose to\nanalyse the contribution of non-monotony to the diversity and complexity in\ntheir dynamical behaviours according to two axes. The first one consists in\nsupporting the idea that non-monotony has a peculiar influence on the\nsensitivity to synchronism of such networks. It leads us to the second axis\nthat presents preliminary results and builds an understanding of the dynamical\nbehaviours, in particular concerning convergence times, of specific\nnon-monotone Boolean automata networks called XOR circulant networks. \n\n"}
{"id": "1208.5639", "contents": "Title: Convex Integer Optimization by Constantly Many Linear Counterparts Abstract: In this article we study convex integer maximization problems with composite\nobjective functions of the form $f(Wx)$, where $f$ is a convex function on\n$\\R^d$ and $W$ is a $d\\times n$ matrix with small or binary entries, over\nfinite sets $S\\subset \\Z^n$ of integer points presented by an oracle or by\nlinear inequalities.\n  Continuing the line of research advanced by Uri Rothblum and his colleagues\non edge-directions, we introduce here the notion of {\\em edge complexity} of\n$S$, and use it to establish polynomial and constant upper bounds on the number\nof vertices of the projection $\\conv(WS)$ and on the number of linear\noptimization counterparts needed to solve the above convex problem.\n  Two typical consequences are the following. First, for any $d$, there is a\nconstant $m(d)$ such that the maximum number of vertices of the projection of\nany matroid $S\\subset\\{0,1\\}^n$ by any binary $d\\times n$ matrix $W$ is $m(d)$\nregardless of $n$ and $S$; and the convex matroid problem reduces to $m(d)$\ngreedily solvable linear counterparts. In particular, $m(2)=8$. Second, for any\n$d,l,m$, there is a constant $t(d;l,m)$ such that the maximum number of\nvertices of the projection of any three-index $l\\times m\\times n$\ntransportation polytope for any $n$ by any binary $d\\times(l\\times m\\times n)$\nmatrix $W$ is $t(d;l,m)$; and the convex three-index transportation problem\nreduces to $t(d;l,m)$ linear counterparts solvable in polynomial time. \n\n"}
{"id": "1208.6271", "contents": "Title: Graph Symmetry Detection and Canonical Labeling: Differences and\n  Synergies Abstract: Symmetries of combinatorial objects are known to complicate search\nalgorithms, but such obstacles can often be removed by detecting symmetries\nearly and discarding symmetric subproblems. Canonical labeling of combinatorial\nobjects facilitates easy equivalence checking through quick matching. All\nexisting canonical labeling software also finds symmetries, but the fastest\nsymmetry-finding software does not perform canonical labeling. In this work, we\ncontrast the two problems and dissect typical algorithms to identify their\nsimilarities and differences. We then develop a novel approach to canonical\nlabeling where symmetries are found first and then used to speed up the\ncanonical labeling algorithms. Empirical results show that this approach\noutperforms state-of-the-art canonical labelers. \n\n"}
{"id": "1208.6589", "contents": "Title: Efficient Computation of the Permanent of Block Factorizable Matrices Abstract: We present an efficient algorithm for computing the permanent for matrices of\nsize N that can written as a product of L block diagonal matrices with blocks\nof size at most 2. For fixed L, the time and space resources scale linearly in\nN, with a prefactor that scales exponentially in L. This class of matrices\ncontains banded matrices with banded inverse. We show that such a factorization\ninto a product of block diagonal matrices gives rise to a circuit acting on a\nHilbert space with a tensor product structure and that the permanent is equal\nto the transition amplitude of this circuit and a product basis state. In this\ncorrespondence, a block diagonal matrix gives rise to one layer of the circuit,\nwhere each block to a gate acting either on a single tensor component or on two\nadjacent tensor components. This observation allows us to adopt matrix product\nstates, a computational method from condensed matter physics and quantum\ninformation theory used to simulate quantum systems, to evaluate the transition\namplitude. \n\n"}
{"id": "1209.1688", "contents": "Title: Rank Centrality: Ranking from Pair-wise Comparisons Abstract: The question of aggregating pair-wise comparisons to obtain a global ranking\nover a collection of objects has been of interest for a very long time: be it\nranking of online gamers (e.g. MSR's TrueSkill system) and chess players,\naggregating social opinions, or deciding which product to sell based on\ntransactions. In most settings, in addition to obtaining a ranking, finding\n`scores' for each object (e.g. player's rating) is of interest for\nunderstanding the intensity of the preferences.\n  In this paper, we propose Rank Centrality, an iterative rank aggregation\nalgorithm for discovering scores for objects (or items) from pair-wise\ncomparisons. The algorithm has a natural random walk interpretation over the\ngraph of objects with an edge present between a pair of objects if they are\ncompared; the score, which we call Rank Centrality, of an object turns out to\nbe its stationary probability under this random walk. To study the efficacy of\nthe algorithm, we consider the popular Bradley-Terry-Luce (BTL) model\n(equivalent to the Multinomial Logit (MNL) for pair-wise comparisons) in which\neach object has an associated score which determines the probabilistic outcomes\nof pair-wise comparisons between objects. In terms of the pair-wise marginal\nprobabilities, which is the main subject of this paper, the MNL model and the\nBTL model are identical. We bound the finite sample error rates between the\nscores assumed by the BTL model and those estimated by our algorithm. In\nparticular, the number of samples required to learn the score well with high\nprobability depends on the structure of the comparison graph. When the\nLaplacian of the comparison graph has a strictly positive spectral gap, e.g.\neach item is compared to a subset of randomly chosen items, this leads to\ndependence on the number of samples that is nearly order-optimal. \n\n"}
{"id": "1209.3523", "contents": "Title: Eight-Fifth Approximation for TSP Paths Abstract: We prove the approximation ratio 8/5 for the metric $\\{s,t\\}$-path-TSP\nproblem, and more generally for shortest connected $T$-joins.\n  The algorithm that achieves this ratio is the simple \"Best of Many\" version\nof Christofides' algorithm (1976), suggested by An, Kleinberg and Shmoys\n(2012), which consists in determining the best Christofides $\\{s,t\\}$-tour out\nof those constructed from a family $\\Fscr_{>0}$ of trees having a convex\ncombination dominated by an optimal solution $x^*$ of the fractional\nrelaxation. They give the approximation guarantee $\\frac{\\sqrt{5}+1}{2}$ for\nsuch an $\\{s,t\\}$-tour, which is the first improvement after the 5/3 guarantee\nof Hoogeveen's Christofides type algorithm (1991). Cheriyan, Friggstad and Gao\n(2012) extended this result to a 13/8-approximation of shortest connected\n$T$-joins, for $|T|\\ge 4$.\n  The ratio 8/5 is proved by simplifying and improving the approach of An,\nKleinberg and Shmoys that consists in completing $x^*/2$ in order to dominate\nthe cost of \"parity correction\" for spanning trees. We partition the edge-set\nof each spanning tree in $\\Fscr_{>0}$ into an $\\{s,t\\}$-path (or more\ngenerally, into a $T$-join) and its complement, which induces a decomposition\nof $x^*$. This decomposition can be refined and then efficiently used to\ncomplete $x^*/2$ without using linear programming or particular properties of\n$T$, but by adding to each cut deficient for $x^*/2$ an individually tailored\nexplicitly given vector, inherent in $x^*$.\n  A simple example shows that the Best of Many Christofides algorithm may not\nfind a shorter $\\{s,t\\}$-tour than 3/2 times the incidentally common optima of\nthe problem and of its fractional relaxation. \n\n"}
{"id": "1209.4865", "contents": "Title: The arithmetic complexity of tensor contractions Abstract: We investigate the algebraic complexity of tensor calulus. We consider a\ngeneralization of iterated matrix product to tensors and show that the\nresulting formulas exactly capture VP, the class of polynomial families\nefficiently computable by arithmetic circuits. This gives a natural and robust\ncharacterization of this complexity class that despite its naturalness is not\nvery well understood so far. \n\n"}
{"id": "1209.5818", "contents": "Title: Fast Algorithms for the Maximum Clique Problem on Massive Sparse Graphs Abstract: The maximum clique problem is a well known NP-Hard problem with applications\nin data mining, network analysis, informatics, and many other areas. Although\nthere exist several algorithms with acceptable runtimes for certain classes of\ngraphs, many of them are infeasible for massive graphs. We present a new exact\nalgorithm that employs novel pruning techniques to very quickly find maximum\ncliques in large sparse graphs. Extensive experiments on several types of\nsynthetic and real-world graphs show that our new algorithm is up to several\norders of magnitude faster than existing algorithms for most instances. We also\npresent a heuristic variant that runs orders of magnitude faster than the exact\nalgorithm, while providing optimal or near-optimal solutions. \n\n"}
{"id": "1209.5993", "contents": "Title: Geometric Complexity Theory V: Efficient algorithms for Noether\n  Normalization Abstract: We study a basic algorithmic problem in algebraic geometry, which we call\nNNL, of constructing a normalizing map as per Noether's Normalization Lemma.\nFor general explicit varieties, as formally defined in this paper, we give a\nrandomized polynomial-time Monte Carlo algorithm for this problem. For some\ninteresting cases of explicit varieties, we give deterministic quasi-polynomial\ntime algorithms. These may be contrasted with the standard EXPSPACE-algorithms\nfor these problems in computational algebraic geometry.\n  In particular, we show that:\n  (1) The categorical quotient for any finite dimensional representation $V$ of\n$SL_m$, with constant $m$, is explicit in characteristic zero.\n  (2) NNL for this categorical quotient can be solved deterministically in time\nquasi-polynomial in the dimension of $V$.\n  (3) The categorical quotient of the space of $r$-tuples of $m \\times m$\nmatrices by the simultaneous conjugation action of $SL_m$ is explicit in any\ncharacteristic.\n  (4) NNL for this categorical quotient can be solved deterministically in time\nquasi-polynomial in $m$ and $r$ in any characteristic $p$ not in $[2,\\ m/2]$.\n  (5) NNL for every explicit variety in zero or large enough characteristic can\nbe solved deterministically in quasi-polynomial time, assuming the hardness\nhypothesis for the permanent in geometric complexity theory.\n  The last result leads to a geometric complexity theory approach to put NNL\nfor every explicit variety in P. \n\n"}
{"id": "1210.2698", "contents": "Title: Improved Approximation Lower Bounds for Vertex Cover on Power Law Graphs\n  and Some Generalizations Abstract: We prove new explicit inapproximability results for the Vertex Cover Problem\non the Power Law Graphs and some functional generalizations of that class of\ngraphs. Our results depend on special bounded degree amplifier constructions\nfor those classes of graphs and could be also of independent interest. \n\n"}
{"id": "1210.8014", "contents": "Title: Volume Rendering of AMR Simulations Abstract: High-resolution simulations often rely on the Adaptive Mesh Resolution (AMR)\ntechnique to optimize memory consumption versus attainable precision. While\nthis technique allows for dramatic improvements in terms of computing\nperformance, the analysis and visualization of its data outputs remain\nchallenging. The lack of effective volume renderers for the octree-based AMR\nused by the RAMSES simulation program has led to the development of the\nsolutions presented in this paper. Two custom algorithms are discussed, based\non the splatting and the ray-casting techniques. Their usage is illustrated in\nthe context of the visualization of a high-resolution, 6000-processor\nsimulation of a Milky Way-like galaxy. Performance obtained in terms of memory\nmanagement and parallelism speedup are presented. \n\n"}
{"id": "1211.1043", "contents": "Title: Soft (Gaussian CDE) regression models and loss functions Abstract: Regression, unlike classification, has lacked a comprehensive and effective\napproach to deal with cost-sensitive problems by the reuse (and not a\nre-training) of general regression models. In this paper, a wide variety of\ncost-sensitive problems in regression (such as bids, asymmetric losses and\nrejection rules) can be solved effectively by a lightweight but powerful\napproach, consisting of: (1) the conversion of any traditional one-parameter\ncrisp regression model into a two-parameter soft regression model, seen as a\nnormal conditional density estimator, by the use of newly-introduced enrichment\nmethods; and (2) the reframing of an enriched soft regression model to new\ncontexts by an instance-dependent optimisation of the expected loss derived\nfrom the conditional normal distribution. \n\n"}
{"id": "1211.5414", "contents": "Title: Analysis of a randomized approximation scheme for matrix multiplication Abstract: This note gives a simple analysis of a randomized approximation scheme for\nmatrix multiplication proposed by Sarlos (2006) based on a random rotation\nfollowed by uniform column sampling. The result follows from a matrix version\nof Bernstein's inequality and a tail inequality for quadratic forms in\nsubgaussian random vectors. \n\n"}
{"id": "1211.7110", "contents": "Title: Algorithms for discovering and proving theorems about permutation\n  patterns Abstract: We present an algorithm, called BiSC, that describes the patterns avoided by\na given set of permutations. It automatically conjectures the statements of\nknown theorems such as the descriptions of stack-sortable (Knuth 1975) and\nWest-2-stack-sortable permutations (West 1990), smooth (Lakshmibai and Sandhya\n1990) and forest-like permutations (Bousquet-Melou and Butler 2007), and simsun\npermutations (Branden and Claesson 2011). The algorithm has also been used to\ndiscover new theorems and conjectures related to Young tableaux,\nWilf-equivalences and sorting devices. We further give algorithms to prove a\ncomplete description of preimages of pattern classes under certain sorting\ndevices. These generalize an algorithm of Claesson and Ulfarsson (2012) and\nallow us to prove a linear time algorithm for finding occurrences of the\npattern 4312. \n\n"}
{"id": "1212.0979", "contents": "Title: A Combinatorial Polynomial Algorithm for the Linear Arrow-Debreu Market Abstract: We present the first combinatorial polynomial time algorithm for computing\nthe equilibrium of the Arrow-Debreu market model with linear utilities. \n\n"}
{"id": "1212.3517", "contents": "Title: Inapproximability of Dominating Set in Power Law Graphs Abstract: We give logarithmic lower bounds for the approximability of the Minimum\nDominating Set problem in connected (alpha,beta)-Power Law Graphs. We give also\na best up to now upper approximation bound on the problem for the case of the\nparameters beta>2. We develop also a new functional method for proving lower\napproximation bounds and display a sharp phase transition between\napproximability and inapproximability of the underlying problem. This method\ncould also be of independent interest. \n\n"}
{"id": "1212.4777", "contents": "Title: A Practical Algorithm for Topic Modeling with Provable Guarantees Abstract: Topic models provide a useful method for dimensionality reduction and\nexploratory data analysis in large text corpora. Most approaches to topic model\ninference have been based on a maximum likelihood objective. Efficient\nalgorithms exist that approximate this objective, but they have no provable\nguarantees. Recently, algorithms have been introduced that provide provable\nbounds, but these algorithms are not practical because they are inefficient and\nnot robust to violations of model assumptions. In this paper we present an\nalgorithm for topic model inference that is both provable and practical. The\nalgorithm produces results comparable to the best MCMC implementations while\nrunning orders of magnitude faster. \n\n"}
{"id": "1212.5156", "contents": "Title: Nonparametric ridge estimation Abstract: We study the problem of estimating the ridges of a density function. Ridge\nestimation is an extension of mode finding and is useful for understanding the\nstructure of a density. It can also be used to find hidden structure in point\ncloud data. We show that, under mild regularity conditions, the ridges of the\nkernel density estimator consistently estimate the ridges of the true density.\nWhen the data are noisy measurements of a manifold, we show that the ridges are\nclose and topologically similar to the hidden manifold. To find the estimated\nridges in practice, we adapt the modified mean-shift algorithm proposed by\nOzertem and Erdogmus [J. Mach. Learn. Res. 12 (2011) 1249-1286]. Some numerical\nexperiments verify that the algorithm is accurate. \n\n"}
{"id": "1301.0114", "contents": "Title: Tree-based Arithmetic and Compressed Representations of Giant Numbers Abstract: Can we do arithmetic in a completely different way, with a radically\ndifferent data structure? Could this approach provide practical benefits, like\noperations on giant numbers while having an average performance similar to\ntraditional bitstring representations?\n  While answering these questions positively, our tree based representation\ndescribed in this paper comes with a few extra benefits: it compresses giant\nnumbers such that, for instance, the largest known prime number as well as its\nrelated perfect number are represented as trees of small sizes. The same also\napplies to Fermat numbers and important computations like exponentiation of two\nbecome constant time operations.\n  At the same time, succinct representations of sparse sets, multisets and\nsequences become possible through bijections to our tree-represented natural\nnumbers. \n\n"}
{"id": "1301.3780", "contents": "Title: Bounds on the Size of Sound Monotone Switching Networks Accepting\n  Permutation Sets of Directed Trees Abstract: In this paper, we prove almost tight bounds on the size of sound monotone\nswitching networks accepting permutations sets of directed trees. This roughly\ncorresponds to proving almost tight bounds bounds on the monotone memory\nefficiency of the directed ST-connectivity problem for the special case in\nwhich the input graph is guaranteed to have no path from s to t or be\nisomorphic to a specific directed tree. \n\n"}
{"id": "1302.0072", "contents": "Title: Dynamic 2D Dictionary Matching in Small Space Abstract: The dictionary matching problem preprocesses a set of patterns and finds all\noccurrences of each of the patterns in a text when it is provided. We focus on\nthe dynamic setting, in which patterns can be inserted to and removed from the\ndictionary, without reprocessing the entire dictionary. This article presents\nthe first algorithm that performs \\emph{dynamic} dictionary matching on\ntwo-dimensional data within small space. The time complexity of our algorithm\nis almost linear. The only slowdown is incurred by querying the compressed\nself-index that replaces the dictionary. The dictionary is updated in time\nproportional to the size of the pattern that is being inserted to or removed\nfrom the dictionary. Our algorithm is suitable for rectangular patterns that\nare of uniform size in one dimension. \n\n"}
{"id": "1302.0275", "contents": "Title: A strong direct product theorem for the tribes function via the\n  smooth-rectangle bound Abstract: The main result of this paper is an optimal strong direct product result for\nthe two-party public-coin randomized communication complexity of the Tribes\nfunction. This is proved by providing an alternate proof of the optimal lower\nbound of \\Omega(n) for the randomised communication complexity of the Tribes\nfunction using the so-called smooth-rectangle bound, introduced by Jain and\nKlauck [JK10]. The optimal \\Omega(n) lower bound for Tribes was originally\nproved by Jayram, Kumar and Sivakumar [JKS03], using a more powerful lower\nbound technique, namely the information complexity bound. The information\ncomplexity bound is known to be at least as strong a lower bound method as the\nsmooth-rectangle bound [KLL+12]. On the other hand, we are not aware of any\nfunction or relation for which the smooth-rectangle bound is (asymptotically)\nsmaller than its public-coin randomized communication complexity. The optimal\ndirect product for Tribes is obtained by combining our smooth-rectangle bound\nfor tribes with the strong direct product result of Jain and Yao [JY12] in\nterms of smooth-rectangle bound. \n\n"}
{"id": "1302.2576", "contents": "Title: The trace norm constrained matrix-variate Gaussian process for multitask\n  bipartite ranking Abstract: We propose a novel hierarchical model for multitask bipartite ranking. The\nproposed approach combines a matrix-variate Gaussian process with a generative\nmodel for task-wise bipartite ranking. In addition, we employ a novel trace\nconstrained variational inference approach to impose low rank structure on the\nposterior matrix-variate Gaussian process. The resulting posterior covariance\nfunction is derived in closed form, and the posterior mean function is the\nsolution to a matrix-variate regression with a novel spectral elastic net\nregularizer. Further, we show that variational inference for the trace\nconstrained matrix-variate Gaussian process combined with maximum likelihood\nparameter estimation for the bipartite ranking model is jointly convex. Our\nmotivating application is the prioritization of candidate disease genes. The\ngoal of this task is to aid the identification of unobserved associations\nbetween human genes and diseases using a small set of observed associations as\nwell as kernels induced by gene-gene interaction networks and disease\nontologies. Our experimental results illustrate the performance of the proposed\nmodel on real world datasets. Moreover, we find that the resulting low rank\nsolution improves the computational scalability of training and testing as\ncompared to baseline models. \n\n"}
{"id": "1302.4713", "contents": "Title: Constrained Signaling in Auction Design Abstract: We consider the problem of an auctioneer who faces the task of selling a good\n(drawn from a known distribution) to a set of buyers, when the auctioneer does\nnot have the capacity to describe to the buyers the exact identity of the good\nthat he is selling. Instead, he must come up with a constrained signalling\nscheme: a (non injective) mapping from goods to signals, that satisfies the\nconstraints of his setting. For example, the auctioneer may be able to\ncommunicate only a bounded length message for each good, or he might be legally\nconstrained in how he can advertise the item being sold. Each candidate\nsignaling scheme induces an incomplete-information game among the buyers, and\nthe goal of the auctioneer is to choose the signaling scheme and accompanying\nauction format that optimizes welfare. In this paper, we use techniques from\nsubmodular function maximization and no-regret learning to give algorithms for\ncomputing constrained signaling schemes for a variety of constrained signaling\nproblems. \n\n"}
{"id": "1302.6256", "contents": "Title: Parallel Maximum Clique Algorithms with Applications to Network Analysis\n  and Storage Abstract: We propose a fast, parallel maximum clique algorithm for large sparse graphs\nthat is designed to exploit characteristics of social and information networks.\nThe method exhibits a roughly linear runtime scaling over real-world networks\nranging from 1000 to 100 million nodes. In a test on a social network with 1.8\nbillion edges, the algorithm finds the largest clique in about 20 minutes. Our\nmethod employs a branch and bound strategy with novel and aggressive pruning\ntechniques. For instance, we use the core number of a vertex in combination\nwith a good heuristic clique finder to efficiently remove the vast majority of\nthe search space. In addition, we parallelize the exploration of the search\ntree. During the search, processes immediately communicate changes to upper and\nlower bounds on the size of maximum clique, which occasionally results in a\nsuper-linear speedup because vertices with large search spaces can be pruned by\nother processes. We apply the algorithm to two problems: to compute temporal\nstrong components and to compress graphs. \n\n"}
{"id": "1303.0478", "contents": "Title: Monomial Testing and Applications Abstract: In this paper, we devise two algorithms for the problem of testing\n$q$-monomials of degree $k$ in any multivariate polynomial represented by a\ncircuit, regardless of the primality of $q$. One is an $O^*(2^k)$ time\nrandomized algorithm. The other is an $O^*(12.8^k)$ time deterministic\nalgorithm for the same $q$-monomial testing problem but requiring the\npolynomials to be represented by tree-like circuits. Several applications of\n$q$-monomial testing are also given, including a deterministic $O^*(12.8^{mk})$\nupper bound for the $m$-set $k$-packing problem. \n\n"}
{"id": "1303.5197", "contents": "Title: Multi-dimensional sparse structured signal approximation using split\n  Bregman iterations Abstract: The paper focuses on the sparse approximation of signals using overcomplete\nrepresentations, such that it preserves the (prior) structure of\nmulti-dimensional signals. The underlying optimization problem is tackled using\na multi-dimensional split Bregman optimization approach. An extensive empirical\nevaluation shows how the proposed approach compares to the state of the art\ndepending on the signal features. \n\n"}
{"id": "1303.5217", "contents": "Title: Optimal Partitioning for Dual-Pivot Quicksort Abstract: Dual-pivot quicksort refers to variants of classical quicksort where in the\npartitioning step two pivots are used to split the input into three segments.\nThis can be done in different ways, giving rise to different algorithms.\nRecently, a dual-pivot algorithm proposed by Yaroslavskiy received much\nattention, because a variant of it replaced the well-engineered quicksort\nalgorithm in Sun's Java 7 runtime library. Nebel and Wild (ESA 2012) analyzed\nthis algorithm and showed that on average it uses 1.9n ln n + O(n) comparisons\nto sort an input of size n, beating standard quicksort, which uses 2n ln n +\nO(n) comparisons. We introduce a model that captures all dual-pivot algorithms,\ngive a unified analysis, and identify new dual-pivot algorithms that minimize\nthe average number of key comparisons among all possible algorithms up to a\nlinear term. This minimum is 1.8n ln n + O(n). For the case that the pivots are\nchosen from a small sample, we include a comparison of dual-pivot quicksort and\nclassical quicksort. Specifically, we show that dual-pivot quicksort benefits\nfrom a skewed choice of pivots. We experimentally evaluate our algorithms and\ncompare them to Yaroslavskiy's algorithm and the recently described three-pivot\nquicksort algorithm of Kushagra et al. (ALENEX 2014). \n\n"}
{"id": "1303.6437", "contents": "Title: New Inapproximability Bounds for TSP Abstract: In this paper, we study the approximability of the metric Traveling Salesman\nProblem (TSP) and prove new explicit inapproximability bounds for that problem.\nThe best up to now known hardness of approximation bounds were 185/184 for the\nsymmetric case (due to Lampis) and 117/116 for the asymmetric case (due to\nPapadimitriou and Vempala). We construct here two new bounded occurrence CSP\nreductions which improve these bounds to 123/122 and 75/74, respectively. The\nlatter bound is the first improvement in more than a decade for the case of the\nasymmetric TSP. One of our main tools, which may be of independent interest, is\na new construction of a bounded degree wheel amplifier used in the proof of our\nresults. \n\n"}
{"id": "1304.0378", "contents": "Title: Fully Dynamic $(1+\\epsilon)$-Approximate Matchings Abstract: We present the first data structures that maintain near optimal maximum\ncardinality and maximum weighted matchings on sparse graphs in sublinear time\nper update. Our main result is a data structure that maintains a $(1+\\epsilon)$\napproximation of maximum matching under edge insertions/deletions in worst case\n$O(\\sqrt{m}\\epsilon^{-2})$ time per update. This improves the 3/2 approximation\ngiven in [Neiman,Solomon,STOC 2013] which runs in similar time. The result is\nbased on two ideas. The first is to re-run a static algorithm after a chosen\nnumber of updates to ensure approximation guarantees. The second is to\njudiciously trim the graph to a smaller equivalent one whenever possible.\n  We also study extensions of our approach to the weighted setting, and combine\nit with known frameworks to obtain arbitrary approximation ratios. For a\nconstant $\\epsilon$ and for graphs with edge weights between 1 and N, we design\nan algorithm that maintains an $(1+\\epsilon)$-approximate maximum weighted\nmatching in $O(\\sqrt{m} \\log N)$ time per update. The only previous result for\nmaintaining weighted matchings on dynamic graphs has an approximation ratio of\n4.9108, and was shown in [Anand,Baswana,Gupta,Sen, FSTTCS 2012, arXiv 2012]. \n\n"}
{"id": "1304.4243", "contents": "Title: Range Counting Coresets for Uncertain Data Abstract: We study coresets for various types of range counting queries on uncertain\ndata. In our model each uncertain point has a probability density describing\nits location, sometimes defined as k distinct locations. Our goal is to\nconstruct a subset of the uncertain points, including their locational\nuncertainty, so that range counting queries can be answered by just examining\nthis subset. We study three distinct types of queries. RE queries return the\nexpected number of points in a query range. RC queries return the number of\npoints in the range with probability at least a threshold. RQ queries returns\nthe probability that fewer than some threshold fraction of the points are in\nthe range. In both RC and RQ coresets the threshold is provided as part of the\nquery. And for each type of query we provide coreset constructions with\napproximation-size tradeoffs. We show that random sampling can be used to\nconstruct each type of coreset, and we also provide significantly improved\nbounds using discrepancy-based approaches on axis-aligned range queries. \n\n"}
{"id": "1304.4626", "contents": "Title: Efficient Computation of Representative Sets with Applications in\n  Parameterized and Exact Algorithms Abstract: We give two algorithms computing representative families of linear and\nuniform matroids and demonstrate how to use representative families for\ndesigning single-exponential parameterized and exact exponential time\nalgorithms. The applications of our approach include\n  - LONGEST DIRECTED CYCLE\n  - MINIMUM EQUIVALENT GRAPH (MEG)\n  - Algorithms on graphs of bounded treewidth\n  -k-PATH, k-TREE, and more generally, k-SUBGRAPH ISOMORPHISM, where the\nk-vertex pattern graph is of constant treewidth. \n\n"}
{"id": "1304.5164", "contents": "Title: Dequantizing read-once quantum formulas Abstract: Quantum formulas, defined by Yao [FOCS '93], are the quantum analogs of\nclassical formulas, i.e., classical circuits in which all gates have fanout\none. We show that any read-once quantum formula over a gate set that contains\nall single-qubit gates is equivalent to a read-once classical formula of the\nsame size and depth over an analogous classical gate set. For example, any\nread-once quantum formula over Toffoli and single-qubit gates is equivalent to\na read-once classical formula over Toffoli and NOT gates. We then show that the\nequivalence does not hold if the read-once restriction is removed. To show the\npower of quantum formulas without the read-once restriction, we define a new\nmodel of computation called the one-qubit model and show that it can compute\nall boolean functions. This model may also be of independent interest. \n\n"}
{"id": "1304.5583", "contents": "Title: Distributed Low-rank Subspace Segmentation Abstract: Vision problems ranging from image clustering to motion segmentation to\nsemi-supervised learning can naturally be framed as subspace segmentation\nproblems, in which one aims to recover multiple low-dimensional subspaces from\nnoisy and corrupted input data. Low-Rank Representation (LRR), a convex\nformulation of the subspace segmentation problem, is provably and empirically\naccurate on small problems but does not scale to the massive sizes of modern\nvision datasets. Moreover, past work aimed at scaling up low-rank matrix\nfactorization is not applicable to LRR given its non-decomposable constraints.\nIn this work, we propose a novel divide-and-conquer algorithm for large-scale\nsubspace segmentation that can cope with LRR's non-decomposable constraints and\nmaintains LRR's strong recovery guarantees. This has immediate implications for\nthe scalability of subspace segmentation, which we demonstrate on a benchmark\nface recognition dataset and in simulations. We then introduce novel\napplications of LRR-based subspace segmentation to large-scale semi-supervised\nlearning for multimedia event detection, concept detection, and image tagging.\nIn each case, we obtain state-of-the-art results and order-of-magnitude speed\nups. \n\n"}
{"id": "1304.6321", "contents": "Title: A O(c^k n) 5-Approximation Algorithm for Treewidth Abstract: We give an algorithm that for an input n-vertex graph G and integer k>0, in\ntime 2^[O(k)]n either outputs that the treewidth of G is larger than k, or\ngives a tree decomposition of G of width at most 5k+4. This is the first\nalgorithm providing a constant factor approximation for treewidth which runs in\ntime single-exponential in k and linear in n. Treewidth based computations are\nsubroutines of numerous algorithms. Our algorithm can be used to speed up many\nsuch algorithms to work in time which is single-exponential in the treewidth\nand linear in the input size. \n\n"}
{"id": "1304.6800", "contents": "Title: Approximation Hardness of Graphic TSP on Cubic Graphs Abstract: We prove explicit approximation hardness results for the Graphic TSP on cubic\nand subcubic graphs as well as the new inapproximability bounds for the\ncorresponding instances of the (1,2)-TSP. The proof technique uses new modular\nconstructions of simulating gadgets for the restricted cubic and subcubic\ninstances. The modular constructions used in the paper could be also of\nindependent interest. \n\n"}
{"id": "1304.8046", "contents": "Title: Sophistication vs Logical Depth Abstract: Sophistication and logical depth are two measures that express how\ncomplicated the structure in a string is. Sophistication is defined as the\nminimal complexity of a computable function that defines a two-part description\nfor the string that is shortest within some precision; the second can be\ndefined as the minimal computation time of a program that is shortest within\nsome precision. We show that the Busy Beaver function of the sophistication of\na string exceeds its logical depth with logarithmically bigger precision, and\nthat logical depth exceeds the Busy Beaver function of sophistication with\nlogarithmically bigger precision. We also show that this is not true if the\nprecision is only increased by a constant (when the notions are defined with\nplain Kolmogorov complexity). Finally we show that sophistication is unstable\nin its precision: constant variations can change its value by a linear term in\nthe length of the string. \n\n"}
{"id": "1305.1535", "contents": "Title: Probabilistic Constructions of Computable Objects and a Computable\n  Version of Lov\\'asz Local Lemma Abstract: A nonconstructive proof can be used to prove the existence of an object with\nsome properties without providing an explicit example of such an object. A\nspecial case is a probabilistic proof where we show that an object with\nrequired properties appears with some positive probability in some random\nprocess. Can we use such arguments to prove the existence of a computable\ninfinite object? Sometimes yes: following [8], we show how the notion of a\nlayerwise computable mapping can be used to prove a computable version of\nLov\\'asz local lemma. (A survey of Moser-Tardos proof is included to make the\npaper self-contained.) \n\n"}
{"id": "1305.2777", "contents": "Title: Fingerprints in Compressed Strings Abstract: The Karp-Rabin fingerprint of a string is a type of hash value that due to\nits strong properties has been used in many string algorithms. In this paper we\nshow how to construct a data structure for a string $S$ of size $N$ compressed\nby a context-free grammar of size $n$ that answers fingerprint queries. That\nis, given indices $i$ and $j$, the answer to a query is the fingerprint of the\nsubstring $S[i,j]$. We present the first O(n) space data structures that answer\nfingerprint queries without decompressing any characters. For Straight Line\nPrograms (SLP) we get $O(\\log N)$ query time, and for Linear SLPs (an SLP\nderivative that captures LZ78 compression and its variations) we get $O(\\log\n\\log N)$ query time. Hence, our data structures has the same time and space\ncomplexity as for random access in SLPs. We utilize the fingerprint data\nstructures to solve the longest common extension problem in query time $O(\\log\nN \\log \\lce)$ and $O(\\log \\lce \\log\\log \\lce + \\log\\log N)$ for SLPs and Linear\nSLPs, respectively. Here, $\\lce$ denotes the length of the LCE. \n\n"}
{"id": "1305.6376", "contents": "Title: Fractional Pebbling Game Lower Bounds Abstract: Fractional pebbling is a generalization of black-white pebbling introduced\nrecently. In this reasearch paper we solve an open problem by proving a tight\nlower bound on the pebble weight required to fractionally pebble a balanced\nd-ary tree of height h. This bound has close ties with branching programs and\nthe separation of P from NL. \n\n"}
{"id": "1307.0556", "contents": "Title: The Complexity of Counting Homomorphisms to Cactus Graphs Modulo 2 Abstract: A homomorphism from a graph G to a graph H is a function from V(G) to V(H)\nthat preserves edges. Many combinatorial structures that arise in mathematics\nand computer science can be represented naturally as graph homomorphisms and as\nweighted sums of graph homomorphisms. In this paper, we study the complexity of\ncounting homomorphisms modulo 2. The complexity of modular counting was\nintroduced by Papadimitriou and Zachos and it has been pioneered by Valiant who\nfamously introduced a problem for which counting modulo 7 is easy but counting\nmodulo 2 is intractable. Modular counting provides a rich setting in which to\nstudy the structure of homomorphism problems. In this case, the structure of\nthe graph H has a big influence on the complexity of the problem. Thus, our\napproach is graph-theoretic. We give a complete solution for the class of\ncactus graphs, which are connected graphs in which every edge belongs to at\nmost one cycle. Cactus graphs arise in many applications such as the modelling\nof wireless sensor networks and the comparison of genomes. We show that, for\nsome cactus graphs H, counting homomorphisms to H modulo 2 can be done in\npolynomial time. For every other fixed cactus graph H, the problem is complete\nfor the complexity class parity-P which is a wide complexity class to which\nevery problem in the polynomial hierarchy can be reduced (using randomised\nreductions). Determining which H lead to tractable problems can be done in\npolynomial time. Our result builds upon the work of Faben and Jerrum, who gave\na dichotomy for the case in which H is a tree. \n\n"}
{"id": "1307.0836", "contents": "Title: Strong equivalence of reversible circuits is coNP-complete Abstract: It is well-known that deciding equivalence of logic circuits is a\ncoNP-complete problem. As a corollary, the problem of deciding weak equivalence\nof reversible circuits, i.e. ignoring the ancilla bits, is also coNP-complete.\nThe complexity of deciding strong equivalence, including the ancilla bits, is\nless obvious and may depend on gate set. Here we use Barrington's theorem to\nshow that deciding strong equivalence of reversible circuits built from the\nFredkin gate is coNP-complete. This implies coNP-completeness of deciding\nstrong equivalence for other commonly used universal reversible gate sets,\nincluding any gate set that includes the Toffoli or Fredkin gate. \n\n"}
{"id": "1307.2724", "contents": "Title: The technique of in-place associative sorting Abstract: In the first place, a novel, yet straightforward in-place integer\nvalue-sorting algorithm is presented. It sorts in linear time using constant\namount of additional memory for storing counters and indices beside the input\narray. The technique is inspired from the principal idea behind one of the\nordinal theories of \"serial order in behavior\" and explained by the analogy\nwith the three main stages in the formation and retrieval of memory in\ncognitive neuroscience: (i) practicing, (ii) storage and (iii) retrieval. It is\nfurther improved in terms of time complexity as well as specialized for\ndistinct integers, though still improper for rank-sorting.\n  Afterwards, another novel, yet straightforward technique is introduced which\nmakes this efficient value-sorting technique proper for rank-sorting. Hence,\ngiven an array of n elements each have an integer key, the technique sorts the\nelements according to their integer keys in linear time using only constant\namount of additional memory. The devised technique is very practical and\nefficient outperforming bucket sort, distribution counting sort and address\ncalculation sort family of algorithms making it attractive in almost every case\neven when space is not a critical resource. \n\n"}
{"id": "1307.7811", "contents": "Title: A Novel Combinatorial Method for Estimating Transcript Expression with\n  RNA-Seq: Bounding the Number of Paths Abstract: RNA-Seq technology offers new high-throughput ways for transcript\nidentification and quantification based on short reads, and has recently\nattracted great interest. The problem is usually modeled by a weighted splicing\ngraph whose nodes stand for exons and whose edges stand for split alignments to\nthe exons. The task consists of finding a number of paths, together with their\nexpression levels, which optimally explain the coverages of the graph under\nvarious fitness functions, such least sum of squares. In (Tomescu et al.\nRECOMB-seq 2013) we showed that under general fitness functions, if we allow a\npolynomially bounded number of paths in an optimal solution, this problem can\nbe solved in polynomial time by a reduction to a min-cost flow program. In this\npaper we further refine this problem by asking for a bounded number k of paths\nthat optimally explain the splicing graph. This problem becomes NP-hard in the\nstrong sense, but we give a fast combinatorial algorithm based on dynamic\nprogramming for it. In order to obtain a practical tool, we implement three\noptimizations and heuristics, which achieve better performance on real data,\nand similar or better performance on simulated data, than state-of-the-art\ntools Cufflinks, IsoLasso and SLIDE. Our tool, called Traph, is available at\nhttp://www.cs.helsinki.fi/gsa/traph/ \n\n"}
{"id": "1308.1009", "contents": "Title: Sign Stable Projections, Sign Cauchy Projections and Chi-Square Kernels Abstract: The method of stable random projections is popular for efficiently computing\nthe Lp distances in high dimension (where 0<p<=2), using small space. Because\nit adopts nonadaptive linear projections, this method is naturally suitable\nwhen the data are collected in a dynamic streaming fashion (i.e., turnstile\ndata streams). In this paper, we propose to use only the signs of the projected\ndata and analyze the probability of collision (i.e., when the two signs\ndiffer). We derive a bound of the collision probability which is exact when p=2\nand becomes less sharp when p moves away from 2. Interestingly, when p=1 (i.e.,\nCauchy random projections), we show that the probability of collision can be\naccurately approximated as functions of the chi-square similarity. For example,\nwhen the (un-normalized) data are binary, the maximum approximation error of\nthe collision probability is smaller than 0.0192. In text and vision\napplications, the chi-square similarity is a popular measure for nonnegative\ndata when the features are generated from histograms. Our experiments confirm\nthat the proposed method is promising for large-scale learning applications. \n\n"}
{"id": "1308.1575", "contents": "Title: The Parameterised Complexity of Counting Connected Subgraphs and Graph\n  Motifs Abstract: We introduce a class of parameterised counting problems on graphs, p-#Induced\nSubgraph With Property(\\Phi), which generalises a number of problems which have\npreviously been studied. This paper focusses on the case in which \\Phi defines\na family of graphs whose edge-minimal elements all have bounded treewidth; this\nincludes the special case in which \\Phi describes the property of being\nconnected. We show that exactly counting the number of connected induced\nk-vertex subgraphs in an n-vertex graph is #W[1]-hard, but on the other hand\nthere exists an FPTRAS for the problem; more generally, we show that there\nexists an FPTRAS for p-#Induced Subgraph With Property(\\Phi) whenever \\Phi is\nmonotone and all the minimal graphs satisfying \\Phi have bounded treewidth. We\nthen apply these results to a counting version of the Graph Motif problem. \n\n"}
{"id": "1308.1762", "contents": "Title: Spatial mixing and approximation algorithms for graphs with bounded\n  connective constant Abstract: The hard core model in statistical physics is a probability distribution on\nindependent sets in a graph in which the weight of any independent set I is\nproportional to lambda^(|I|), where lambda > 0 is the vertex activity. We show\nthat there is an intimate connection between the connective constant of a graph\nand the phenomenon of strong spatial mixing (decay of correlations) for the\nhard core model; specifically, we prove that the hard core model with vertex\nactivity lambda < lambda_c(Delta + 1) exhibits strong spatial mixing on any\ngraph of connective constant Delta, irrespective of its maximum degree, and\nhence derive an FPTAS for the partition function of the hard core model on such\ngraphs. Here lambda_c(d) := d^d/(d-1)^(d+1) is the critical activity for the\nuniqueness of the Gibbs measure of the hard core model on the infinite d-ary\ntree. As an application, we show that the partition function can be efficiently\napproximated with high probability on graphs drawn from the random graph model\nG(n,d/n) for all lambda < e/d, even though the maximum degree of such graphs is\nunbounded with high probability.\n  We also improve upon Weitz's bounds for strong spatial mixing on bounded\ndegree graphs (Weitz, 2006) by providing a computationally simple method which\nuses known estimates of the connective constant of a lattice to obtain bounds\non the vertex activities lambda for which the hard core model on the lattice\nexhibits strong spatial mixing. Using this framework, we improve upon these\nbounds for several lattices including the Cartesian lattice in dimensions 3 and\nhigher.\n  Our techniques also allow us to relate the threshold for the uniqueness of\nthe Gibbs measure on a general tree to its branching factor (Lyons, 1989). \n\n"}
{"id": "1308.3665", "contents": "Title: On Sparsification for Computing Treewidth Abstract: We investigate whether an n-vertex instance (G,k) of Treewidth, asking\nwhether the graph G has treewidth at most k, can efficiently be made sparse\nwithout changing its answer. By giving a special form of OR-cross-composition,\nwe prove that this is unlikely: if there is an e > 0 and a polynomial-time\nalgorithm that reduces n-vertex Treewidth instances to equivalent instances, of\nan arbitrary problem, with O(n^{2-e}) bits, then NP is in coNP/poly and the\npolynomial hierarchy collapses to its third level.\n  Our sparsification lower bound has implications for structural\nparameterizations of Treewidth: parameterizations by measures that do not\nexceed the vertex count, cannot have kernels with O(k^{2-e}) bits for any e >\n0, unless NP is in coNP/poly. Motivated by the question of determining the\noptimal kernel size for Treewidth parameterized by vertex cover, we improve the\nO(k^3)-vertex kernel from Bodlaender et al. (STACS 2011) to a kernel with\nO(k^2) vertices. Our improved kernel is based on a novel form of\ntreewidth-invariant set. We use the q-expansion lemma of Fomin et al. (STACS\n2011) to find such sets efficiently in graphs whose vertex count is\nsuperquadratic in their vertex cover number. \n\n"}
{"id": "1308.4469", "contents": "Title: External Memory Algorithms For Path Traversal in Graphs Abstract: This thesis presents a number of results related to path traversal in trees\nand graphs. In particular, we focus on data structures which allow such\ntraversals to be performed efficiently in the external memory setting. In\naddition, for trees and planar graphs the data structures we present are\nsuccinct. Our tree structures permit efficient bottom-up path traversal in\nrooted trees of arbitrary degree and efficient top-down path traversal in\nbinary trees. In the graph setting, we permit efficient traversal of an\narbitrary path in bounded degree planar graphs. Our data structures for both\ntrees and graphs match or slightly improve current best results for external\nmemory path traversal in these settings while at the same time improving space\nbounds due to the succinct nature of our data structures. Employing our path\ntraversal structure for bounded degree planar graphs, we describe a number of\nuseful applications of this technique for triangular meshes in R^2. As an\nextension of the R^2 representation for triangular meshes we also present an\nefficient external memory representation for well-shaped tetrahedral meshes in\nR^3. The external memory representation we present is based on a partitioning\nscheme that matches the current best-known results for well-shaped tetrahedral\nmeshes. We describe applications of path traversal in tetrahedral meshes which\nare made efficient in the external memory setting using our structure. Finally,\nwe present a result on using jump-and-walk point location in well-shaped meshes\nin both R^2 and R^3. We demonstrate that, given an approximate nearest\nneighbour from among the vertices of a mesh, locating the simplex containing\nthe query point involves a constant length walk (path traversal) in the mesh. \n\n"}
{"id": "1309.0249", "contents": "Title: Algorithms versus Circuit Lower Bounds Abstract: Different techniques have been used to prove several transference theorems of\nthe form \"nontrivial algorithms for a circuit class C yield circuit lower\nbounds against C\". In this survey we revisit many of these results. We discuss\nhow circuit lower bounds can be obtained from derandomization, compression,\nlearning, and satisfiability algorithms. We also cover the connection between\ncircuit lower bounds and useful properties, a notion that turns out to be\nfundamental in the context of these transference theorems. Along the way, we\nobtain a few new results, simplify several proofs, and show connections\ninvolving different frameworks. We hope that our presentation will serve as a\nself-contained introduction for those interested in pursuing research in this\narea. \n\n"}
{"id": "1309.1418", "contents": "Title: Algorithmic Data Analytics, Small Data Matters and Correlation versus\n  Causation Abstract: This is a review of aspects of the theory of algorithmic information that may\ncontribute to a framework for formulating questions related to complex highly\nunpredictable systems. We start by contrasting Shannon Entropy and\nKolmogorov-Chaitin complexity epitomizing the difference between correlation\nand causation to then move onto surveying classical results from algorithmic\ncomplexity and algorithmic probability, highlighting their deep connection to\nthe study of automata frequency distributions. We end showing how long-range\nalgorithmic predicting models for economic and biological systems may require\ninfinite computation but locally approximated short-range estimations are\npossible thereby showing how small data can deliver important insights into\nimportant features of complex \"Big Data\". \n\n"}
{"id": "1309.1507", "contents": "Title: A Quantized Johnson Lindenstrauss Lemma: The Finding of Buffon's Needle Abstract: In 1733, Georges-Louis Leclerc, Comte de Buffon in France, set the ground of\ngeometric probability theory by defining an enlightening problem: What is the\nprobability that a needle thrown randomly on a ground made of equispaced\nparallel strips lies on two of them? In this work, we show that the solution to\nthis problem, and its generalization to $N$ dimensions, allows us to discover a\nquantized form of the Johnson-Lindenstrauss (JL) Lemma, i.e., one that combines\na linear dimensionality reduction procedure with a uniform quantization of\nprecision $\\delta>0$. In particular, given a finite set $\\mathcal S \\subset\n\\mathbb R^N$ of $S$ points and a distortion level $\\epsilon>0$, as soon as $M >\nM_0 = O(\\epsilon^{-2} \\log S)$, we can (randomly) construct a mapping from\n$(\\mathcal S, \\ell_2)$ to $(\\delta\\mathbb Z^M, \\ell_1)$ that approximately\npreserves the pairwise distances between the points of $\\mathcal S$.\nInterestingly, compared to the common JL Lemma, the mapping is quasi-isometric\nand we observe both an additive and a multiplicative distortions on the\nembedded distances. These two distortions, however, decay as $O(\\sqrt{(\\log\nS)/M})$ when $M$ increases. Moreover, for coarse quantization, i.e., for high\n$\\delta$ compared to the set radius, the distortion is mainly additive, while\nfor small $\\delta$ we tend to a Lipschitz isometric embedding. Finally, we\nprove the existence of a \"nearly\" quasi-isometric embedding of $(\\mathcal S,\n\\ell_2)$ into $(\\delta\\mathbb Z^M, \\ell_2)$. This one involves a non-linear\ndistortion of the $\\ell_2$-distance in $\\mathcal S$ that vanishes for distant\npoints in this set. Noticeably, the additive distortion in this case is slower,\nand decays as $O(\\sqrt[4]{(\\log S)/M})$. \n\n"}
{"id": "1309.1645", "contents": "Title: Fast ranking algorithm for very large data Abstract: In this paper, we propose a new ranking method inspired from previous results\non the diffusion approach to solve linear equation. We describe new\nmathematical equations corresponding to this method and show through\nexperimental results the potential computational gain. This ranking method is\nalso compared to the well known PageRank model. \n\n"}
{"id": "1309.3533", "contents": "Title: Mixed Membership Models for Time Series Abstract: In this article we discuss some of the consequences of the mixed membership\nperspective on time series analysis. In its most abstract form, a mixed\nmembership model aims to associate an individual entity with some set of\nattributes based on a collection of observed data. Although much of the\nliterature on mixed membership models considers the setting in which\nexchangeable collections of data are associated with each member of a set of\nentities, it is equally natural to consider problems in which an entire time\nseries is viewed as an entity and the goal is to characterize the time series\nin terms of a set of underlying dynamic attributes or \"dynamic regimes\".\nIndeed, this perspective is already present in the classical hidden Markov\nmodel, where the dynamic regimes are referred to as \"states\", and the\ncollection of states realized in a sample path of the underlying process can be\nviewed as a mixed membership characterization of the observed time series. Our\ngoal here is to review some of the richer modeling possibilities for time\nseries that are provided by recent developments in the mixed membership\nframework. \n\n"}
{"id": "1309.4713", "contents": "Title: SEFE with No Mapping via Large Induced Outerplane Graphs in Plane Graphs Abstract: We show that every $n$-vertex planar graph admits a simultaneous embedding\nwith no mapping and with fixed edges with any $(n/2)$-vertex planar graph. In\norder to achieve this result, we prove that every $n$-vertex plane graph has an\ninduced outerplane subgraph containing at least $n/2$ vertices. Also, we show\nthat every $n$-vertex planar graph and every $n$-vertex planar partial 3-tree\nadmit a simultaneous embedding with no mapping and with fixed edges. \n\n"}
{"id": "1309.7739", "contents": "Title: Communication complexity of promise problems and their applications to\n  finite automata Abstract: Equality and disjointness are two of the most studied problems in\ncommunication complexity. They have been studied for both classical and also\nquantum communication and for various models and modes of communication.\nBuhrman et al. [Buh98] proved that the exact quantum communication complexity\nfor a promise version of the equality problem is ${\\bf O}(\\log {n})$ while the\nclassical deterministic communication complexity is $n+1$ for two-way\ncommunication, which was the first impressively large (exponential) gap between\nquantum and classical (deterministic and probabilistic) communication\ncomplexity. If an error is tolerated, both quantum and probabilistic\ncommunication complexities for equality are ${\\bf O}(\\log {n})$. However, even\nif an error is tolerated, the gaps between quantum (probabilistic) and\ndeterministic complexity are not larger than quadratic for the disjointness\nproblem. It is therefore interesting to ask whether there are some promise\nversions of the disjointness problem for which bigger gaps can be shown. We\ngive a positive answer to such a question. Namely, we prove that there exists\nan exponential gap between quantum (even probabilistic) communication\ncomplexity and classical deterministic communication complexity of some\nspecific versions of the disjointness problem.\n  Klauck [Kla00] proved, for any language, that the state complexity of exact\nquantum/classical finite automata, which is a general model of one-way quantum\nfinite automata, is not less than the state complexity of an equivalent one-way\ndeterministic finite automata (1DFA). In this paper we show, using a\ncommunication complexity result, that situation may be different for some\npromise problems. Namely, we show for certain promise problem that the gap\nbetween the state complexity of exact one-way quantum finite automata and 1DFA\ncan be exponential. \n\n"}
{"id": "1310.1533", "contents": "Title: CAM: Causal additive models, high-dimensional order search and penalized\n  regression Abstract: We develop estimation for potentially high-dimensional additive structural\nequation models. A key component of our approach is to decouple order search\namong the variables from feature or edge selection in a directed acyclic graph\nencoding the causal structure. We show that the former can be done with\nnonregularized (restricted) maximum likelihood estimation while the latter can\nbe efficiently addressed using sparse regression techniques. Thus, we\nsubstantially simplify the problem of structure search and estimation for an\nimportant class of causal models. We establish consistency of the (restricted)\nmaximum likelihood estimator for low- and high-dimensional scenarios, and we\nalso allow for misspecification of the error distribution. Furthermore, we\ndevelop an efficient computational algorithm which can deal with many\nvariables, and the new method's accuracy and performance is illustrated on\nsimulated and real data. \n\n"}
{"id": "1310.1845", "contents": "Title: On triangulating k-outerplanar graphs Abstract: A k-outerplanar graph is a graph that can be drawn in the plane without\ncrossing such that after k-fold removal of the vertices on the outer-face there\nare no vertices left. In this paper, we study how to triangulate a\nk-outerplanar graph while keeping its outerplanarity small. Specifically, we\nshow that not all k-outerplanar graphs can be triangulated so that the result\nis k-outerplanar, but they can be triangulated so that the result is\n(k+1)-outerplanar. \n\n"}
{"id": "1310.2118", "contents": "Title: Finding Dominators via Disjoint Set Union Abstract: The problem of finding dominators in a directed graph has many important\napplications, notably in global optimization of computer code. Although linear\nand near-linear-time algorithms exist, they use sophisticated data structures.\nWe develop an algorithm for finding dominators that uses only a \"static tree\"\ndisjoint set data structure in addition to simple lists and maps. The algorithm\nruns in near-linear or linear time, depending on the implementation of the\ndisjoint set data structure. We give several versions of the algorithm,\nincluding one that computes loop nesting information (needed in many kinds of\nglobal code optimization) and that can be made self-certifying, so that the\ncorrectness of the computed dominators is very easy to verify. \n\n"}
{"id": "1311.0224", "contents": "Title: Faster Algorithms For Vertex Partitioning Problems Parameterized by\n  Clique-width Abstract: Many NP-hard problems, such as Dominating Set, are FPT parameterized by\nclique-width. For graphs of clique-width $k$ given with a $k$-expression,\nDominating Set can be solved in $4^k n^{O(1)}$ time. However, no FPT algorithm\nis known for computing an optimal $k$-expression. For a graph of clique-width\n$k$, if we rely on known algorithms to compute a $(2^{3k}-1)$-expression via\nrank-width and then solving Dominating Set using the $(2^{3k}-1)$-expression,\nthe above algorithm will only give a runtime of $4^{2^{3k}} n^{O(1)}$. There\nhave been results which overcome this exponential jump; the best known\nalgorithm can solve Dominating Set in time $2^{O(k^2)} n^{O(1)}$ by avoiding\nconstructing a $k$-expression [Bui-Xuan, Telle, and Vatshelle. Fast dynamic\nprogramming for locally checkable vertex subset and vertex partitioning\nproblems. Theoret. Comput. Sci., 2013. doi:10.1016/j.tcs.2013.01.009]. We\nimprove this to $2^{O(k\\log k)}n^{O(1)}$. Indeed, we show that for a graph of\nclique-width $k$, a large class of domination and partitioning problems\n(LC-VSP), including Dominating Set, can be solved in $2^{O(k\\log{k})}\nn^{O(1)}$. Our main tool is a variant of rank-width using the rank of a $0$-$1$\nmatrix over the rational field instead of the binary field. \n\n"}
{"id": "1311.0776", "contents": "Title: The Composition Theorem for Differential Privacy Abstract: Sequential querying of differentially private mechanisms degrades the overall\nprivacy level. In this paper, we answer the fundamental question of\ncharacterizing the level of overall privacy degradation as a function of the\nnumber of queries and the privacy levels maintained by each privatization\nmechanism. Our solution is complete: we prove an upper bound on the overall\nprivacy level and construct a sequence of privatization mechanisms that\nachieves this bound. The key innovation is the introduction of an operational\ninterpretation of differential privacy (involving hypothesis testing) and the\nuse of new data processing inequalities. Our result improves over the\nstate-of-the-art, and has immediate applications in several problems studied in\nthe literature including differentially private multi-party computation. \n\n"}
{"id": "1311.1780", "contents": "Title: Learned-Norm Pooling for Deep Feedforward and Recurrent Neural Networks Abstract: In this paper we propose and investigate a novel nonlinear unit, called $L_p$\nunit, for deep neural networks. The proposed $L_p$ unit receives signals from\nseveral projections of a subset of units in the layer below and computes a\nnormalized $L_p$ norm. We notice two interesting interpretations of the $L_p$\nunit. First, the proposed unit can be understood as a generalization of a\nnumber of conventional pooling operators such as average, root-mean-square and\nmax pooling widely used in, for instance, convolutional neural networks (CNN),\nHMAX models and neocognitrons. Furthermore, the $L_p$ unit is, to a certain\ndegree, similar to the recently proposed maxout unit (Goodfellow et al., 2013)\nwhich achieved the state-of-the-art object recognition results on a number of\nbenchmark datasets. Secondly, we provide a geometrical interpretation of the\nactivation function based on which we argue that the $L_p$ unit is more\nefficient at representing complex, nonlinear separating boundaries. Each $L_p$\nunit defines a superelliptic boundary, with its exact shape defined by the\norder $p$. We claim that this makes it possible to model arbitrarily shaped,\ncurved boundaries more efficiently by combining a few $L_p$ units of different\norders. This insight justifies the need for learning different orders for each\nunit in the model. We empirically evaluate the proposed $L_p$ units on a number\nof datasets and show that multilayer perceptrons (MLP) consisting of the $L_p$\nunits achieve the state-of-the-art results on a number of benchmark datasets.\nFurthermore, we evaluate the proposed $L_p$ unit on the recently proposed deep\nrecurrent neural networks (RNN). \n\n"}
{"id": "1311.2110", "contents": "Title: Curvature and Optimal Algorithms for Learning and Minimizing Submodular\n  Functions Abstract: We investigate three related and important problems connected to machine\nlearning: approximating a submodular function everywhere, learning a submodular\nfunction (in a PAC-like setting [53]), and constrained minimization of\nsubmodular functions. We show that the complexity of all three problems depends\non the 'curvature' of the submodular function, and provide lower and upper\nbounds that refine and improve previous results [3, 16, 18, 52]. Our proof\ntechniques are fairly generic. We either use a black-box transformation of the\nfunction (for approximation and learning), or a transformation of algorithms to\nuse an appropriate surrogate function (for minimization). Curiously, curvature\nhas been known to influence approximations for submodular maximization [7, 55],\nbut its effect on minimization, approximation and learning has hitherto been\nopen. We complete this picture, and also support our theoretical claims by\nempirical results. \n\n"}
{"id": "1311.3297", "contents": "Title: The Bose-Hubbard model is QMA-complete Abstract: The Bose-Hubbard model is a system of interacting bosons that live on the\nvertices of a graph. The particles can move between adjacent vertices and\nexperience a repulsive on-site interaction. The Hamiltonian is determined by a\nchoice of graph that specifies the geometry in which the particles move and\ninteract. We prove that approximating the ground energy of the Bose-Hubbard\nmodel on a graph at fixed particle number is QMA-complete. In our QMA-hardness\nproof, we encode the history of an n-qubit computation in the subspace with at\nmost one particle per site (i.e., hard-core bosons). This feature, along with\nthe well-known mapping between hard-core bosons and spin systems, lets us prove\na related result for a class of 2-local Hamiltonians defined by graphs that\ngeneralizes the XY model. By avoiding the use of perturbation theory in our\nanalysis, we circumvent the need to multiply terms in the Hamiltonian by large\ncoefficients. \n\n"}
{"id": "1311.4821", "contents": "Title: On the Complexity of Random Satisfiability Problems with Planted\n  Solutions Abstract: The problem of identifying a planted assignment given a random $k$-SAT\nformula consistent with the assignment exhibits a large algorithmic gap: while\nthe planted solution becomes unique and can be identified given a formula with\n$O(n\\log n)$ clauses, there are distributions over clauses for which the best\nknown efficient algorithms require $n^{k/2}$ clauses. We propose and study a\nunified model for planted $k$-SAT, which captures well-known special cases. An\ninstance is described by a planted assignment $\\sigma$ and a distribution on\nclauses with $k$ literals. We define its distribution complexity as the largest\n$r$ for which the distribution is not $r$-wise independent ($1 \\le r \\le k$ for\nany distribution with a planted assignment).\n  Our main result is an unconditional lower bound, tight up to logarithmic\nfactors, for statistical (query) algorithms [Kearns 1998, Feldman et. al 2012],\nmatching known upper bounds, which, as we show, can be implemented using a\nstatistical algorithm. Since known approaches for problems over distributions\nhave statistical analogues (spectral, MCMC, gradient-based, convex optimization\netc.), this lower bound provides a rigorous explanation of the observed\nalgorithmic gap. The proof introduces a new general technique for the analysis\nof statistical query algorithms. It also points to a geometric paring\nphenomenon in the space of all planted assignments.\n  We describe consequences of our lower bounds to Feige's refutation hypothesis\n[Feige 2002] and to lower bounds on general convex programs that solve planted\n$k$-SAT. Our bounds also extend to other planted $k$-CSP models, and, in\nparticular, provide concrete evidence for the security of Goldreich's one-way\nfunction and the associated pseudorandom generator when used with a\nsufficiently hard predicate [Goldreich 2000]. \n\n"}
{"id": "1311.4839", "contents": "Title: Ferromagnetic Potts Model: Refined #BIS-hardness and Related Results Abstract: Recent results establish for 2-spin antiferromagnetic systems that the\ncomputational complexity of approximating the partition function on graphs of\nmaximum degree D undergoes a phase transition that coincides with the\nuniqueness phase transition on the infinite D-regular tree. For the\nferromagnetic Potts model we investigate whether analogous hardness results\nhold. Goldberg and Jerrum showed that approximating the partition function of\nthe ferromagnetic Potts model is at least as hard as approximating the number\nof independent sets in bipartite graphs (#BIS-hardness). We improve this\nhardness result by establishing it for bipartite graphs of maximum degree D. We\nfirst present a detailed picture for the phase diagram for the infinite\nD-regular tree, giving a refined picture of its first-order phase transition\nand establishing the critical temperature for the coexistence of the disordered\nand ordered phases. We then prove for all temperatures below this critical\ntemperature that it is #BIS-hard to approximate the partition function on\nbipartite graphs of maximum degree D. As a corollary, it is #BIS-hard to\napproximate the number of k-colorings on bipartite graphs of maximum degree D\nwhen k <= D/(2 ln D).\n  The #BIS-hardness result for the ferromagnetic Potts model uses random\nbipartite regular graphs as a gadget in the reduction. The analysis of these\nrandom graphs relies on recent connections between the maxima of the\nexpectation of their partition function, attractive fixpoints of the associated\ntree recursions, and induced matrix norms. We extend these connections to\nrandom regular graphs for all ferromagnetic models and establish the Bethe\nprediction for every ferromagnetic spin system on random regular graphs. We\nalso prove for the ferromagnetic Potts model that the Swendsen-Wang algorithm\nis torpidly mixing on random D-regular graphs at the critical temperature for\nlarge q. \n\n"}
{"id": "1311.5694", "contents": "Title: Computing the multilinear factors of lacunary polynomials without\n  heights Abstract: We present a deterministic algorithm which computes the multilinear factors\nof multivariate lacunary polynomials over number fields. Its complexity is\npolynomial in $\\ell^n$ where $\\ell$ is the lacunary size of the input\npolynomial and $n$ its number of variables, that is in particular polynomial in\nthe logarithm of its degree. We also provide a randomized algorithm for the\nsame problem of complexity polynomial in $\\ell$ and $n$.\n  Over other fields of characteristic zero and finite fields of large\ncharacteristic, our algorithms compute the multilinear factors having at least\nthree monomials of multivariate polynomials. Lower bounds are provided to\nexplain the limitations of our algorithm. As a by-product, we also design\npolynomial-time deterministic polynomial identity tests for families of\npolynomials which were not known to admit any.\n  Our results are based on so-called Gap Theorem which reduce high-degree\nfactorization to repeated low-degree factorizations. While previous algorithms\nused Gap Theorems expressed in terms of the heights of the coefficients, our\nGap Theorems only depend on the exponents of the polynomials. This makes our\nalgorithms more elementary and general, and faster in most cases. \n\n"}
{"id": "1311.7631", "contents": "Title: Absorption Time of the Moran Process Abstract: The Moran process models the spread of mutations in populations on graphs. We\ninvestigate the absorption time of the process, which is the time taken for a\nmutation introduced at a randomly chosen vertex to either spread to the whole\npopulation, or to become extinct. It is known that the expected absorption time\nfor an advantageous mutation is O(n^4) on an n-vertex undirected graph, which\nallows the behaviour of the process on undirected graphs to be analysed using\nthe Markov chain Monte Carlo method. We show that this does not extend to\ndirected graphs by exhibiting an infinite family of directed graphs for which\nthe expected absorption time is exponential in the number of vertices. However,\nfor regular directed graphs, we show that the expected absorption time is\nOmega(n log n) and O(n^2). We exhibit families of graphs matching these bounds\nand give improved bounds for other families of graphs, based on isoperimetric\nnumber. Our results are obtained via stochastic dominations which we\ndemonstrate by establishing a coupling in a related continuous-time model. The\ncoupling also implies several natural domination results regarding the fixation\nprobability of the original (discrete-time) process, resolving a conjecture of\nShakarian, Roos and Johnson. \n\n"}
{"id": "1312.0036", "contents": "Title: Weak Parity Abstract: We study the query complexity of Weak Parity: the problem of computing the\nparity of an n-bit input string, where one only has to succeed on a 1/2+eps\nfraction of input strings, but must do so with high probability on those inputs\nwhere one does succeed. It is well-known that n randomized queries and n/2\nquantum queries are needed to compute parity on all inputs. But surprisingly,\nwe give a randomized algorithm for Weak Parity that makes only\nO(n/log^0.246(1/eps)) queries, as well as a quantum algorithm that makes only\nO(n/sqrt(log(1/eps))) queries. We also prove a lower bound of\nOmega(n/log(1/eps)) in both cases; and using extremal combinatorics, prove\nlower bounds of Omega(log n) in the randomized case and Omega(sqrt(log n)) in\nthe quantum case for any eps>0. We show that improving our lower bounds is\nintimately related to two longstanding open problems about Boolean functions:\nthe Sensitivity Conjecture, and the relationships between query complexity and\npolynomial degree. \n\n"}
{"id": "1312.2173", "contents": "Title: Bounds on Double-Sided Myopic Algorithms for Unconstrained Non-monotone\n  Submodular Maximization Abstract: Unconstrained submodular maximization captures many NP-hard combinatorial\noptimization problems, including Max-Cut, Max-Di-Cut, and variants of facility\nlocation problems. Recently, Buchbinder et al. presented a surprisingly simple\nlinear time randomized greedy-like online algorithm that achieves a constant\napproximation ratio of 1/2, matching optimally the hardness result of Feige et\nal.. Motivated by the algorithm of Buchbinder et al., we introduce a precise\nalgorithmic model called double-sided myopic algorithms. We show that while the\nalgorithm of Buchbinder et al. can be realized as a randomized online\ndouble-sided myopic algorithm, no such deterministic algorithm, even with\nadaptive ordering, can achieve the same approximation ratio. With respect to\nthe Max-Di-Cut problem, we relate the Buchbinder et al. algorithm and our\nmyopic framework to the online algorithm and inapproximation of Bar-Noy and\nLampis. \n\n"}
{"id": "1312.2226", "contents": "Title: On two Algorithmic Problems about Synchronizing Automata Abstract: Under the assumption $\\mathcal{P} \\neq \\mathcal{NP}$, we prove that two\nnatural problems from the theory of synchronizing automata cannot be solved in\npolynomial time. The first problem is to decide whether a given reachable\npartial automaton is synchronizing. The second one is, given an $n$-state\nbinary complete synchronizing automaton, to compute its reset threshold within\nperformance ratio less than $d \\ln{(n)}$ for a specific constant $d>0$. \n\n"}
{"id": "1312.4116", "contents": "Title: Quantum Algorithm to Solve a Maze: Converting the Maze Problem into a\n  Search Problem Abstract: We propose a different methodology towards approaching a Maze problem. We\nconvert the problem into a Quantum Search Problem (QSP), and its solutions are\nsought for using the iterative Grover's Search Algorithm. Though the category\nof mazes we are looking at are of the NP complete class, we have redirected\nsuch a NP complete problem into a QSP. Our solution deals with two dimensional\nperfect mazes with no closed loops. We encode all possible individual paths\nfrom the starting point of the maze into a quantum register. A quantum fitness\noperator applied on the register encodes each individual with its fitness\nvalue. We propose an oracle design which marks all the individuals above a\ncertain fitness value and use the Grover search algorithm to find one of the\nmarked states. Iterating over this method, we approach towards the optimum\nsolution. \n\n"}
{"id": "1312.4628", "contents": "Title: Counting Triangulations and other Crossing-free Structures via Onion\n  Layers Abstract: Let $P$ be a set of $n$ points in the plane. A crossing-free structure on $P$\nis a plane graph with vertex set $P$. Examples of crossing-free structures\ninclude triangulations of $P$, spanning cycles of $P$, also known as\npolygonalizations of $P$, among others. In this paper we develop a general\ntechnique for computing the number of crossing-free structures of an input set\n$P$. We apply the technique to obtain algorithms for computing the number of\ntriangulations, matchings, and spanning cycles of $P$. The running time of our\nalgorithms is upper bounded by $n^{O(k)}$, where $k$ is the number of onion\nlayers of $P$. In particular, for $k = O(1)$ our algorithms run in polynomial\ntime. In addition, we show that our algorithm for counting triangulations is\nnever slower than $O^{*}(3.1414^{n})$, even when $k = \\Theta(n)$. Given that\nthere are several well-studied configurations of points with at least\n$\\Omega(3.464^{n})$ triangulations, and some even with $\\Omega(8^{n})$\ntriangulations, our algorithm asymptotically outperforms any enumeration\nalgorithm for such instances. In fact, it is widely believed that any set of\n$n$ points must have at least $\\Omega(3.464^{n})$ triangulations. If this is\ntrue, then our algorithm is strictly sub-linear in the number of triangulations\ncounted. We also show that our techniques are general enough to solve the\n\"Restricted-Triangulation-Counting-Problem\", which we prove to be $W[2]$-hard\nin the parameter $k$. This implies a \"no free lunch\" result: In order to be\nfixed-parameter tractable, our general algorithm must rely on additional\nproperties that are specific to the considered class of structures. \n\n"}
{"id": "1312.5604", "contents": "Title: Learning Transformations for Classification Forests Abstract: This work introduces a transformation-based learner model for classification\nforests. The weak learner at each split node plays a crucial role in a\nclassification tree. We propose to optimize the splitting objective by learning\na linear transformation on subspaces using nuclear norm as the optimization\ncriteria. The learned linear transformation restores a low-rank structure for\ndata from the same class, and, at the same time, maximizes the separation\nbetween different classes, thereby improving the performance of the split\nfunction. Theoretical and experimental results support the proposed framework. \n\n"}
{"id": "1312.5686", "contents": "Title: Complexity Hierarchies Beyond Elementary Abstract: We introduce a hierarchy of fast-growing complexity classes and show its\nsuitability for completeness statements of many non elementary problems. This\nhierarchy allows the classification of many decision problems with a\nnon-elementary complexity, which occur naturally in logic, combinatorics,\nformal languages, verification, etc., with complexities ranging from simple\ntowers of exponentials to Ackermannian and beyond. \n\n"}
{"id": "1312.6447", "contents": "Title: Incremental Network Design with Maximum Flows Abstract: We study an incremental network design problem, where in each time period of\nthe planning horizon an arc can be added to the network and a maximum flow\nproblem is solved, and where the objective is to maximize the cumulative flow\nover the entire planning horizon. After presenting two mixed integer\nprogramming (MIP) formulations for this NP-complete problem, we describe\nseveral heuristics and prove performance bounds for some special cases. In a\nseries of computational experiments, we compare the performance of the MIP\nformulations as well as the heuristics. \n\n"}
{"id": "1312.6713", "contents": "Title: Path Finding II : An \\~O(m sqrt(n)) Algorithm for the Minimum Cost Flow\n  Problem Abstract: In this paper we present an $\\tilde{O}(m\\sqrt{n}\\log^{O(1)}U)$ time algorithm\nfor solving the maximum flow problem on directed graphs with $m$ edges, $n$\nvertices, and capacity ratio $U$. This improves upon the previous fastest\nrunning time of\n$O(m\\min\\left(n^{2/3},m^{1/2}\\right)\\log\\left(n^{2}/m\\right)\\log U)$ achieved\nover 15 years ago by Goldberg and Rao. In the special case of solving dense\ndirected unit capacity graphs our algorithm improves upon the previous fastest\nrunning times of of $O(\\min\\{m^{3/2},mn^{^{2/3}}\\})$ achieved by Even and\nTarjan and Karzanov over 35 years ago and of $\\tilde{O}(m^{10/7})$ achieved\nrecently by M\\k{a}dry.\n  We achieve these results through the development and application of a new\ngeneral interior point method that we believe is of independent interest. The\nnumber of iterations required by this algorithm is better than that predicted\nby analyzing the best self-concordant barrier of the feasible region. By\napplying this method to the linear programming formulations of maximum flow,\nminimum cost flow, and lossy generalized minimum cost flow and applying\nanalysis by Daitch and Spielman we achieve running time of\n$\\tilde{O}(m\\sqrt{n}\\log^{O(1)}(U/\\epsilon))$ for these problems as well.\nFurthermore, our algorithm is parallelizable and using a recent nearly linear\ntime work polylogarithmic depth Laplacian system solver of Spielman and Peng we\nachieve a $\\tilde{O}(\\sqrt{n}\\log^{O(1)}(U/\\epsilon))$ depth algorithm and\n$\\tilde{O}(m\\sqrt{n}\\log^{O(1)}(U/\\epsilon))$ work algorithm for solving these\nproblems. \n\n"}
{"id": "1312.7605", "contents": "Title: Constraint Satisfaction with Counting Quantifiers 2 Abstract: We study constraint satisfaction problems (CSPs) in the presence of counting\nquantifiers $\\exists^{\\geq j}$, asserting the existence of $j$ distinct\nwitnesses for the variable in question. As a continuation of our previous (CSR\n2012) paper, we focus on the complexity of undirected graph templates. As our\nmain contribution, we settle the two principal open questions proposed in (CSR\n2012). Firstly, we complete the classification of clique templates by proving a\nfull trichotomy for all possible combinations of counting quantifiers and\nclique sizes, placing each case either in P, NP-complete or Pspace-complete.\nThis involves resolution of the cases in which we have the single quantifier\n$\\exists^{\\geq j}$ on the clique $K_{2j}$. Secondly, we confirm a conjecture\nfrom (CSR 2012), which proposes a full dichotomy for $\\exists$ and\n$\\exists^{\\geq 2}$ on all finite undirected graphs. The main thrust of this\nsecond result is the solution of the complexity for the infinite path which we\nprove is a polynomial-time solvable problem. By adapting the algorithm for the\ninfinite path we are then able to solve the problem for finite paths, and then\ntrees and forests. Thus as a corollary to this work, combining with the other\ncases from (CSR 2012), we obtain a full dichotomy for $\\exists$ and\n$\\exists^{\\geq 2}$ quantifiers on finite graphs, each such problem being either\nin P or NP-hard. Finally, we persevere with the work of (CSR 2012) in exploring\ncases in which there is dichotomy between P and Pspace-complete, in contrast\nwith situations in which the intermediate NP-completeness may appear. \n\n"}
{"id": "1401.0189", "contents": "Title: On the Limits of Depth Reduction at Depth 3 Over Small Finite Fields Abstract: Recently, Gupta et.al. [GKKS2013] proved that over Q any $n^{O(1)}$-variate\nand $n$-degree polynomial in VP can also be computed by a depth three\n$\\Sigma\\Pi\\Sigma$ circuit of size $2^{O(\\sqrt{n}\\log^{3/2}n)}$. Over fixed-size\nfinite fields, Grigoriev and Karpinski proved that any $\\Sigma\\Pi\\Sigma$\ncircuit that computes $Det_n$ (or $Perm_n$) must be of size $2^{\\Omega(n)}$\n[GK1998]. In this paper, we prove that over fixed-size finite fields, any\n$\\Sigma\\Pi\\Sigma$ circuit for computing the iterated matrix multiplication\npolynomial of $n$ generic matrices of size $n\\times n$, must be of size\n$2^{\\Omega(n\\log n)}$. The importance of this result is that over fixed-size\nfields there is no depth reduction technique that can be used to compute all\nthe $n^{O(1)}$-variate and $n$-degree polynomials in VP by depth 3 circuits of\nsize $2^{o(n\\log n)}$. The result [GK1998] can only rule out such a possibility\nfor depth 3 circuits of size $2^{o(n)}$.\n  We also give an example of an explicit polynomial ($NW_{n,\\epsilon}(X)$) in\nVNP (not known to be in VP), for which any $\\Sigma\\Pi\\Sigma$ circuit computing\nit (over fixed-size fields) must be of size $2^{\\Omega(n\\log n)}$. The\npolynomial we consider is constructed from the combinatorial design. An\ninteresting feature of this result is that we get the first examples of two\npolynomials (one in VP and one in VNP) such that they have provably stronger\ncircuit size lower bounds than Permanent in a reasonably strong model of\ncomputation.\n  Next, we prove that any depth 4\n$\\Sigma\\Pi^{[O(\\sqrt{n})]}\\Sigma\\Pi^{[\\sqrt{n}]}$ circuit computing\n$NW_{n,\\epsilon}(X)$ (over any field) must be of size $2^{\\Omega(\\sqrt{n}\\log\nn)}$. To the best of our knowledge, the polynomial $NW_{n,\\epsilon}(X)$ is the\nfirst example of an explicit polynomial in VNP such that it requires\n$2^{\\Omega(\\sqrt{n}\\log n)}$ size depth four circuits, but no known matching\nupper bound. \n\n"}
{"id": "1401.0417", "contents": "Title: Faster SVD-Truncated Least-Squares Regression Abstract: We develop a fast algorithm for computing the \"SVD-truncated\" regularized\nsolution to the least-squares problem: $ \\min_{\\x} \\TNorm{\\matA \\x - \\b}. $ Let\n$\\matA_k$ of rank $k$ be the best rank $k$ matrix computed via the SVD of\n$\\matA$. Then, the SVD-truncated regularized solution is: $ \\x_k =\n\\pinv{\\matA}_k \\b. $ If $\\matA$ is $m \\times n$, then, it takes $O(m n\n\\min\\{m,n\\})$ time to compute $\\x_k $ using the SVD of \\math{\\matA}. We give an\napproximation algorithm for \\math{\\x_k} which constructs a rank-\\math{k}\napproximation $\\tilde{\\matA}_{k}$ and computes $ \\tilde{\\x}_{k} =\n\\pinv{\\tilde\\matA}_{k} \\b$ in roughly $O(\\nnz(\\matA) k \\log n)$ time. Our\nalgorithm uses a randomized variant of the subspace iteration. We show that,\nwith high probability: $ \\TNorm{\\matA \\tilde{\\x}_{k} - \\b} \\approx \\TNorm{\\matA\n\\x_k - \\b}$ and $\\TNorm{\\x_k - \\tilde\\x_k} \\approx 0. $ \n\n"}
{"id": "1401.0579", "contents": "Title: More Algorithms for Provable Dictionary Learning Abstract: In dictionary learning, also known as sparse coding, the algorithm is given\nsamples of the form $y = Ax$ where $x\\in \\mathbb{R}^m$ is an unknown random\nsparse vector and $A$ is an unknown dictionary matrix in $\\mathbb{R}^{n\\times\nm}$ (usually $m > n$, which is the overcomplete case). The goal is to learn $A$\nand $x$. This problem has been studied in neuroscience, machine learning,\nvisions, and image processing. In practice it is solved by heuristic algorithms\nand provable algorithms seemed hard to find. Recently, provable algorithms were\nfound that work if the unknown feature vector $x$ is $\\sqrt{n}$-sparse or even\nsparser. Spielman et al. \\cite{DBLP:journals/jmlr/SpielmanWW12} did this for\ndictionaries where $m=n$; Arora et al. \\cite{AGM} gave an algorithm for\novercomplete ($m >n$) and incoherent matrices $A$; and Agarwal et al.\n\\cite{DBLP:journals/corr/AgarwalAN13} handled a similar case but with weaker\nguarantees.\n  This raised the problem of designing provable algorithms that allow sparsity\n$\\gg \\sqrt{n}$ in the hidden vector $x$. The current paper designs algorithms\nthat allow sparsity up to $n/poly(\\log n)$. It works for a class of matrices\nwhere features are individually recoverable, a new notion identified in this\npaper that may motivate further work.\n  The algorithm runs in quasipolynomial time because they use limited\nenumeration. \n\n"}
{"id": "1401.1771", "contents": "Title: Simple linear algorithms for mining graph cores Abstract: Batagelj and Zaversnik proposed a linear algorithm for the well-known\n$k$-core decomposition problem. However, when $k$-cores are desired for a given\n$k$, we find that a simple linear algorithm requiring no sorting works for\nmining $k$-cores. In addition, this algorithm can be extended to mine $(k_1,\nk_2,\\ldots, k_p)$-cores from $p$-partite graphs in linear time, and this mining\napproach can be efficiently implemented in a distributed computing environment\nwith a lower message complexity bound in comparison with the best known method\nof distributed $k$-core decomposition. \n\n"}
{"id": "1401.2436", "contents": "Title: Hardness of robust graph isomorphism, Lasserre gaps, and asymmetry of\n  random graphs Abstract: Building on work of Cai, F\\\"urer, and Immerman \\cite{CFI92}, we show two\nhardness results for the Graph Isomorphism problem. First, we show that there\nare pairs of nonisomorphic $n$-vertex graphs $G$ and $H$ such that any\nsum-of-squares (SOS) proof of nonisomorphism requires degree $\\Omega(n)$. In\nother words, we show an $\\Omega(n)$-round integrality gap for the Lasserre SDP\nrelaxation. In fact, we show this for pairs $G$ and $H$ which are not even\n$(1-10^{-14})$-isomorphic. (Here we say that two $n$-vertex, $m$-edge graphs\n$G$ and $H$ are $\\alpha$-isomorphic if there is a bijection between their\nvertices which preserves at least $\\alpha m$ edges.) Our second result is that\nunder the {\\sc R3XOR} Hypothesis \\cite{Fei02} (and also any of a class of\nhypotheses which generalize the {\\sc R3XOR} Hypothesis), the \\emph{robust}\nGraph Isomorphism problem is hard. I.e.\\ for every $\\epsilon > 0$, there is no\nefficient algorithm which can distinguish graph pairs which are\n$(1-\\epsilon)$-isomorphic from pairs which are not even\n$(1-\\epsilon_0)$-isomorphic for some universal constant $\\epsilon_0$. Along the\nway we prove a robust asymmetry result for random graphs and hypergraphs which\nmay be of independent interest. \n\n"}
{"id": "1401.2514", "contents": "Title: QoS Constrained Optimal Sink and Relay Placement in Planned Wireless\n  Sensor Networks Abstract: We are given a set of sensors at given locations, a set of potential\nlocations for placing base stations (BSs, or sinks), and another set of\npotential locations for placing wireless relay nodes. There is a cost for\nplacing a BS and a cost for placing a relay. The problem we consider is to\nselect a set of BS locations, a set of relay locations, and an association of\nsensor nodes with the selected BS locations, so that number of hops in the path\nfrom each sensor to its BS is bounded by hmax, and among all such feasible\nnetworks, the cost of the selected network is the minimum. The hop count bound\nsuffices to ensure a certain probability of the data being delivered to the BS\nwithin a given maximum delay under a light traffic model. We observe that the\nproblem is NP-Hard, and is hard to even approximate within a constant factor.\nFor this problem, we propose a polynomial time approximation algorithm\n(SmartSelect) based on a relay placement algorithm proposed in our earlier\nwork, along with a modification of the greedy algorithm for weighted set cover.\nWe have analyzed the worst case approximation guarantee for this algorithm. We\nhave also proposed a polynomial time heuristic to improve upon the solution\nprovided by SmartSelect. Our numerical results demonstrate that the algorithms\nprovide good quality solutions using very little computation time in various\nrandomly generated network scenarios. \n\n"}
{"id": "1401.2662", "contents": "Title: Directed Width Parameters and Circumference of Digraphs Abstract: We prove that the directed treewidth, DAG-width and Kelly-width of a digraph\nare bounded above by its circumference plus one. \n\n"}
{"id": "1401.3615", "contents": "Title: Performance Engineering for a Medical Imaging Application on the Intel\n  Xeon Phi Accelerator Abstract: We examine the Xeon Phi, which is based on Intel's Many Integrated Cores\narchitecture, for its suitability to run the FDK algorithm--the most commonly\nused algorithm to perform the 3D image reconstruction in cone-beam computed\ntomography. We study the challenges of efficiently parallelizing the\napplication and means to enable sensible data sharing between threads despite\nthe lack of a shared last level cache. Apart from parallelization, SIMD\nvectorization is critical for good performance on the Xeon Phi; we perform\nvarious micro-benchmarks to investigate the platform's new set of vector\ninstructions and put a special emphasis on the newly introduced vector gather\ncapability. We refine a previous performance model for the application and\nadapt it for the Xeon Phi to validate the performance of our optimized\nhand-written assembly implementation, as well as the performance of several\ndifferent auto-vectorization approaches. \n\n"}
{"id": "1401.3916", "contents": "Title: Quantum Hamiltonian Complexity Abstract: Constraint satisfaction problems are a central pillar of modern computational\ncomplexity theory. This survey provides an introduction to the rapidly growing\nfield of Quantum Hamiltonian Complexity, which includes the study of quantum\nconstraint satisfaction problems. Over the past decade and a half, this field\nhas witnessed fundamental breakthroughs, ranging from the establishment of a\n\"Quantum Cook-Levin Theorem\" to deep insights into the structure of 1D\nlow-temperature quantum systems via so-called area laws. Our aim here is to\nprovide a computer science-oriented introduction to the subject in order to\nhelp bridge the language barrier between computer scientists and physicists in\nthe field. As such, we include the following in this survey: (1) The\nmotivations and history of the field, (2) a glossary of condensed matter\nphysics terms explained in computer-science friendly language, (3) overviews of\ncentral ideas from condensed matter physics, such as indistinguishable\nparticles, mean field theory, tensor networks, and area laws, and (4) brief\nexpositions of selected computer science-based results in the area. For\nexample, as part of the latter, we provide a novel information theoretic\npresentation of Bravyi's polynomial time algorithm for Quantum 2-SAT. \n\n"}
{"id": "1401.6694", "contents": "Title: Multivariate sparse interpolation using randomized Kronecker\n  substitutions Abstract: We present new techniques for reducing a multivariate sparse polynomial to a\nunivariate polynomial. The reduction works similarly to the classical and\nwidely-used Kronecker substitution, except that we choose the degrees randomly\nbased on the number of nonzero terms in the multivariate polynomial, that is,\nits sparsity. The resulting univariate polynomial often has a significantly\nlower degree than the Kronecker substitution polynomial, at the expense of a\nsmall number of term collisions. As an application, we give a new algorithm for\nmultivariate interpolation which uses these new techniques along with any\nexisting univariate interpolation algorithm. \n\n"}
{"id": "1402.2589", "contents": "Title: Partitioning Perfect Graphs into Stars Abstract: The partition of graphs into \"nice\" subgraphs is a central algorithmic\nproblem with strong ties to matching theory. We study the partitioning of\nundirected graphs into same-size stars, a problem known to be NP-complete even\nfor the case of stars on three vertices. We perform a thorough computational\ncomplexity study of the problem on subclasses of perfect graphs and identify\nseveral polynomial-time solvable cases, for example, on interval graphs and\nbipartite permutation graphs, and also NP-complete cases, for example, on grid\ngraphs and chordal graphs. \n\n"}
{"id": "1402.3573", "contents": "Title: Tree 3-spanners of diameter at most 5 Abstract: Tree spanners approximate distances within graphs; a subtree of a graph is a\ntree $t$-spanner of the graph if and only if for every pair of vertices their\ndistance in the subtree is at most $t$ times their distance in the graph. When\na graph contains a subtree of diameter at most $t$, then trivially admits a\ntree $t$-spanner. Now, determining whether a graph admits a tree $t$-spanner of\ndiameter at most $t+1$ is an NP complete problem, when $t\\geq 4$, and it is\ntractable, when $t\\leq 3$. Although it is not known whether it is tractable to\ndecide graphs that admit a tree 3-spanner of any diameter, an efficient\nalgorithm to determine graphs that admit a tree 3-spanner of diameter at most 5\nis presented. Moreover, it is proved that if a graph of diameter at most 3\nadmits a tee 3-spanner, then it admits a tree 3-spanner of diameter at most 5.\nHence, this algorithm decides tree 3-spanner admissibility of diameter at most\n3 graphs. \n\n"}
{"id": "1402.5857", "contents": "Title: The challenges of unbounded treewidth in parameterised subgraph counting\n  problems Abstract: Parameterised subgraph counting problems are the most thoroughly studied\ntopic in the theory of parameterised counting, and there has been significant\nrecent progress in this area. Many of the existing tractability results for\nparameterised problems which involve finding or counting subgraphs with\nparticular properties rely on bounding the treewidth of these subgraphs in some\nsense; here, we prove a number of hardness results for the situation in which\nthis bounded treewidth condition does not hold, resulting in dichotomies for\nsome special cases of the general subgraph counting problem. The paper also\ngives a thorough survey of known results on this subject and the methods used,\nas well as discussing the relationships both between multicolour and uncoloured\nversions of subgraph counting problems, and between exact counting, approximate\ncounting and the corresponding decision problems. \n\n"}
{"id": "1402.5904", "contents": "Title: On the Integrality Ratio of the Subtour LP for Euclidean TSP Abstract: A long standing conjecture says that the integrality ratio of the subtour LP\nfor metric TSP is $4/3$. A well known family of graphic TSP instances achieves\nthis lower bound asymptotically. For Euclidean TSP the best known lower bound\non the integrality ratio was $8/7$. We improve this value by presenting a\nfamily of Euclidean TSP instances for which the integrality ratio of the\nsubtour LP converges to 4/3. \n\n"}
{"id": "1402.6310", "contents": "Title: Approximating the Cubicity of Trees Abstract: Cubicity of a graph $G$ is the smallest dimension $d$, for which $G$ is a\nunit disc graph in ${\\mathbb{R}}^d$, under the $l^\\infty$ metric, i.e. $G$ can\nbe represented as an intersection graph of $d$-dimensional (axis-parallel) unit\nhypercubes. We call such an intersection representation a $d$-dimensional cube\nrepresentation of $G$. Computing cubicity is known to be inapproximable in\npolynomial time, within an $O(n^{1-\\epsilon})$ factor for any $\\epsilon >0$,\nunless NP=ZPP.\n  In this paper, we present a randomized algorithm that runs in polynomial time\nand computes cube representations of trees, of dimension within a constant\nfactor of the optimum. It is also shown that the cubicity of trees can be\napproximated within a constant factor in deterministic polynomial time, if the\ncube representation is not required to be computed. As far as we know, this is\nthe first constant factor approximation algorithm for computing the cubicity of\ntrees. It is not yet clear whether computing the cubicity of trees is NP-hard\nor not. \n\n"}
{"id": "1402.7224", "contents": "Title: On low treewidth graphs and supertrees Abstract: Compatibility of unrooted phylogenetic trees is a well studied problem in\nphylogenetics. It asks to determine whether for a set of k input trees there\nexists a larger tree (called a supertree) that contains the topologies of all k\ninput trees. When any such supertree exists we call the instance compatible and\notherwise incompatible. It is known that the problem is NP-hard and FPT,\nalthough a constructive FPT algorithm is not known. It has been shown that\nwhenever the treewidth of an auxiliary structure known as the display graph is\nstrictly larger than the number of input trees, the instance is incompatible.\nHere we show that whenever the treewidth of the display graph is at most 2, the\ninstance is compatible. Furthermore, we give a polynomial-time algorithm to\nconstruct a supertree in this case. Finally, we demonstrate both compatible and\nincompatible instances that have display graphs with treewidth 3, highlighting\nthat the treewidth of the display graph is (on its own) not sufficient to\ndetermine compatibility. \n\n"}
{"id": "1403.0885", "contents": "Title: Standard Simplices and Pluralities are Not the Most Noise Stable Abstract: The Standard Simplex Conjecture and the Plurality is Stablest Conjecture are\ntwo conjectures stating that certain partitions are optimal with respect to\nGaussian and discrete noise stability respectively. These two conjectures are\nnatural generalizations of the Gaussian noise stability result by Borell (1985)\nand the Majority is Stablest Theorem (2004). Here we show that the standard\nsimplex is not the most stable partition in Gaussian space and that Plurality\nis not the most stable low influence partition in discrete space for every\nnumber of parts $k \\geq 3$, for every value $\\rho \\neq 0$ of the noise and for\nevery prescribed measures for the different parts as long as they are not all\nequal to $1/k$. Our results do not contradict the original statements of the\nPlurality is Stablest and Standard Simplex Conjectures in their original\nstatements concerning partitions to sets of equal measure. However, they\nindicate that if these conjectures are true, their veracity and their proofs\nwill crucially rely on assuming that the sets are of equal measures, in stark\ncontrast to Borell's result, the Majority is Stablest Theorem and many other\nresults in isoperimetric theory. Given our results it is natural to ask for\n(conjectured) partitions achieving the optimum noise stability. \n\n"}
{"id": "1403.1081", "contents": "Title: Linear rank-width of distance-hereditary graphs I. A polynomial-time\n  algorithm Abstract: Linear rank-width is a linearized variation of rank-width, and it is deeply\nrelated to matroid path-width. In this paper, we show that the linear\nrank-width of every $n$-vertex distance-hereditary graph, equivalently a graph\nof rank-width at most $1$, can be computed in time $\\mathcal{O}(n^2\\cdot \\log_2\nn)$, and a linear layout witnessing the linear rank-width can be computed with\nthe same time complexity. As a corollary, we show that the path-width of every\n$n$-element matroid of branch-width at most $2$ can be computed in time\n$\\mathcal{O}(n^2\\cdot \\log_2 n)$, provided that the matroid is given by an\nindependent set oracle.\n  To establish this result, we present a characterization of the linear\nrank-width of distance-hereditary graphs in terms of their canonical split\ndecompositions. This characterization is similar to the known characterization\nof the path-width of forests given by Ellis, Sudborough, and Turner [The vertex\nseparation and search number of a graph. Inf. Comput., 113(1):50--79, 1994].\nHowever, different from forests, it is non-trivial to relate substructures of\nthe canonical split decomposition of a graph with some substructures of the\ngiven graph. We introduce a notion of `limbs' of canonical split\ndecompositions, which correspond to certain vertex-minors of the original\ngraph, for the right characterization. \n\n"}
{"id": "1403.1515", "contents": "Title: Linear Recognition of Almost Interval Graphs Abstract: Let $\\mbox{interval} + k v$, $\\mbox{interval} + k e$, and $\\mbox{interval} -\nk e$ denote the classes of graphs that can be obtained from some interval graph\nby adding $k$ vertices, adding $k$ edges, and deleting $k$ edges, respectively.\nWhen $k$ is small, these graph classes are called almost interval graphs. They\nare well motivated from computational biology, where the data ought to be\nrepresented by an interval graph while we can only expect an almost interval\ngraph for the best. For any fixed $k$, we give linear-time algorithms for\nrecognizing all these classes, and in the case of membership, our algorithms\nprovide also a specific interval graph as evidence. When $k$ is part of the\ninput, these problems are also known as graph modification problems, all\nNP-complete. Our results imply that they are fixed-parameter tractable\nparameterized by $k$, thereby resolving the long-standing open problem on the\nparameterized complexity of recognizing $\\mbox{interval}+ k e$, first asked by\nBodlaender et al. [Bioinformatics, 11:49--57, 1995]. Moreover, our algorithms\nfor recognizing $\\mbox{interval}+ k v$ and $\\mbox{interval}- k e$ run in times\n$O(6^k \\cdot (n + m))$ and $O(8^k \\cdot (n + m))$, (where $n$ and $m$ stand for\nthe numbers of vertices and edges respectively in the input graph,)\nsignificantly improving the $O(k^{2k}\\cdot n^3m)$-time algorithm of Heggernes\net al. [STOC 2007] and the $O(10^k \\cdot n^9)$-time algorithm of Cao and Marx\n[SODA 2014] respectively. \n\n"}
{"id": "1403.4153", "contents": "Title: A family of polycyclic groups over which the uniform conjugacy problem\n  is NP-complete Abstract: In this paper we study the conjugacy problem in polycyclic groups. Our main\nresult is that we construct polycyclic groups $G_n$ whose conjugacy problem is\nat least as hard as the subset sum problem with $n$ indeterminates. As such,\nthe conjugacy problem over the groups $G_n$ is NP-complete where the parameters\nof the problem are taken in terms of $n$ and the length of the elements given\non input. \n\n"}
{"id": "1403.6135", "contents": "Title: Differentially Private Convex Optimization with Piecewise Affine\n  Objectives Abstract: Differential privacy is a recently proposed notion of privacy that provides\nstrong privacy guarantees without any assumptions on the adversary. The paper\nstudies the problem of computing a differentially private solution to convex\noptimization problems whose objective function is piecewise affine. Such\nproblem is motivated by applications in which the affine functions that define\nthe objective function contain sensitive user information. We propose several\nprivacy preserving mechanisms and provide analysis on the trade-offs between\noptimality and the level of privacy for these mechanisms. Numerical experiments\nare also presented to evaluate their performance in practice. \n\n"}
{"id": "1404.0286", "contents": "Title: Wear Minimization for Cuckoo Hashing: How Not to Throw a Lot of Eggs\n  into One Basket Abstract: We study wear-leveling techniques for cuckoo hashing, showing that it is\npossible to achieve a memory wear bound of $\\log\\log n+O(1)$ after the\ninsertion of $n$ items into a table of size $Cn$ for a suitable constant $C$\nusing cuckoo hashing. Moreover, we study our cuckoo hashing method empirically,\nshowing that it significantly improves on the memory wear performance for\nclassic cuckoo hashing and linear probing in practice. \n\n"}
{"id": "1404.1008", "contents": "Title: Spectral concentration and greedy k-clustering Abstract: A popular graph clustering method is to consider the embedding of an input\ngraph into R^k induced by the first k eigenvectors of its Laplacian, and to\npartition the graph via geometric manipulations on the resulting metric space.\nDespite the practical success of this methodology, there is limited\nunderstanding of several heuristics that follow this framework. We provide\ntheoretical justification for one such natural and computationally efficient\nvariant.\n  Our result can be summarized as follows. A partition of a graph is called\nstrong if each cluster has small external conductance, and large internal\nconductance. We present a simple greedy spectral clustering algorithm which\nreturns a partition that is provably close to a suitably strong partition,\nprovided that such a partition exists. A recent result shows that strong\npartitions exist for graphs with a sufficiently large spectral gap between the\nk-th and (k+1)-st eigenvalues. Taking this together with our main theorem gives\na spectral algorithm which finds a partition close to a strong one for graphs\nwith large enough spectral gap. We also show how this simple greedy algorithm\ncan be implemented in near-linear time for any fixed k and error guarantee.\nFinally, we evaluate our algorithm on some real-world and synthetic inputs. \n\n"}
{"id": "1404.1950", "contents": "Title: On the power of homogeneous depth 4 arithmetic circuits Abstract: We prove exponential lower bounds on the size of homogeneous depth 4\narithmetic circuits computing an explicit polynomial in $VP$. Our results hold\nfor the {\\it Iterated Matrix Multiplication} polynomial - in particular we show\nthat any homogeneous depth 4 circuit computing the $(1,1)$ entry in the product\nof $n$ generic matrices of dimension $n^{O(1)}$ must have size\n$n^{\\Omega(\\sqrt{n})}$.\n  Our results strengthen previous works in two significant ways.\n  Our lower bounds hold for a polynomial in $VP$. Prior to our work, Kayal et\nal [KLSS14] proved an exponential lower bound for homogeneous depth 4 circuits\n(over fields of characteristic zero) computing a poly in $VNP$. The best known\nlower bounds for a depth 4 homogeneous circuit computing a poly in $VP$ was the\nbound of $n^{\\Omega(\\log n)}$ by [LSS, KLSS14].Our exponential lower bounds\nalso give the first exponential separation between general arithmetic circuits\nand homogeneous depth 4 arithmetic circuits. In particular they imply that the\ndepth reduction results of Koiran [Koi12] and Tavenas [Tav13] are tight even\nfor reductions to general homogeneous depth 4 circuits (without the restriction\nof bounded bottom fanin).\n  Our lower bound holds over all fields. The lower bound of [KLSS14] worked\nonly over fields of characteristic zero. Prior to our work, the best lower\nbound for homogeneous depth 4 circuits over fields of positive characteristic\nwas $n^{\\Omega(\\log n)}$ [LSS, KLSS14]. \n\n"}
{"id": "1404.3820", "contents": "Title: Circuit complexity, proof complexity, and polynomial identity testing Abstract: We introduce a new algebraic proof system, which has tight connections to\n(algebraic) circuit complexity. In particular, we show that any\nsuper-polynomial lower bound on any Boolean tautology in our proof system\nimplies that the permanent does not have polynomial-size algebraic circuits\n(VNP is not equal to VP). As a corollary to the proof, we also show that\nsuper-polynomial lower bounds on the number of lines in Polynomial Calculus\nproofs (as opposed to the usual measure of number of monomials) imply the\nPermanent versus Determinant Conjecture. Note that, prior to our work, there\nwas no proof system for which lower bounds on an arbitrary tautology implied\nany computational lower bound.\n  Our proof system helps clarify the relationships between previous algebraic\nproof systems, and begins to shed light on why proof complexity lower bounds\nfor various proof systems have been so much harder than lower bounds on the\ncorresponding circuit classes. In doing so, we highlight the importance of\npolynomial identity testing (PIT) for understanding proof complexity.\n  More specifically, we introduce certain propositional axioms satisfied by any\nBoolean circuit computing PIT. We use these PIT axioms to shed light on\nAC^0[p]-Frege lower bounds, which have been open for nearly 30 years, with no\nsatisfactory explanation as to their apparent difficulty. We show that either:\na) Proving super-polynomial lower bounds on AC^0[p]-Frege implies VNP does not\nhave polynomial-size circuits of depth d - a notoriously open question for d at\nleast 4 - thus explaining the difficulty of lower bounds on AC^0[p]-Frege, or\nb) AC^0[p]-Frege cannot efficiently prove the depth d PIT axioms, and hence we\nhave a lower bound on AC^0[p]-Frege.\n  Using the algebraic structure of our proof system, we propose a novel way to\nextend techniques from algebraic circuit complexity to prove lower bounds in\nproof complexity. \n\n"}
{"id": "1404.4344", "contents": "Title: Improved Analysis of Deterministic Load-Balancing Schemes Abstract: We consider the problem of deterministic load balancing of tokens in the\ndiscrete model. A set of $n$ processors is connected into a $d$-regular\nundirected network. In every time step, each processor exchanges some of its\ntokens with each of its neighbors in the network. The goal is to minimize the\ndiscrepancy between the number of tokens on the most-loaded and the\nleast-loaded processor as quickly as possible.\n  Rabani et al. (1998) present a general technique for the analysis of a wide\nclass of discrete load balancing algorithms. Their approach is to characterize\nthe deviation between the actual loads of a discrete balancing algorithm with\nthe distribution generated by a related Markov chain. The Markov chain can also\nbe regarded as the underlying model of a continuous diffusion algorithm. Rabani\net al. showed that after time $T = O(\\log (Kn)/\\mu)$, any algorithm of their\nclass achieves a discrepancy of $O(d\\log n/\\mu)$, where $\\mu$ is the spectral\ngap of the transition matrix of the graph, and $K$ is the initial load\ndiscrepancy in the system.\n  In this work we identify some natural additional conditions on deterministic\nbalancing algorithms, resulting in a class of algorithms reaching a smaller\ndiscrepancy. This class contains well-known algorithms, eg., the Rotor-Router.\nSpecifically, we introduce the notion of cumulatively fair load-balancing\nalgorithms where in any interval of consecutive time steps, the total number of\ntokens sent out over an edge by a node is the same (up to constants) for all\nadjacent edges. We prove that algorithms which are cumulatively fair and where\nevery node retains a sufficient part of its load in each step, achieve a\ndiscrepancy of $O(\\min\\{d\\sqrt{\\log n/\\mu},d\\sqrt{n}\\})$ in time $O(T)$. We\nalso show that in general neither of these assumptions may be omitted without\nincreasing discrepancy. We then show by a combinatorial potential reduction\nargument that any cumulatively fair scheme satisfying some additional\nassumptions achieves a discrepancy of $O(d)$ almost as quickly as the\ncontinuous diffusion process. This positive result applies to some of the\nsimplest and most natural discrete load balancing schemes. \n\n"}
{"id": "1404.4749", "contents": "Title: Decoding binary node labels from censored edge measurements: Phase\n  transition and efficient recovery Abstract: We consider the problem of clustering a graph $G$ into two communities by\nobserving a subset of the vertex correlations. Specifically, we consider the\ninverse problem with observed variables $Y=B_G x \\oplus Z$, where $B_G$ is the\nincidence matrix of a graph $G$, $x$ is the vector of unknown vertex variables\n(with a uniform prior) and $Z$ is a noise vector with Bernoulli$(\\varepsilon)$\ni.i.d. entries. All variables and operations are Boolean. This model is\nmotivated by coding, synchronization, and community detection problems. In\nparticular, it corresponds to a stochastic block model or a correlation\nclustering problem with two communities and censored edges. Without noise,\nexact recovery (up to global flip) of $x$ is possible if and only the graph $G$\nis connected, with a sharp threshold at the edge probability $\\log(n)/n$ for\nErd\\H{o}s-R\\'enyi random graphs. The first goal of this paper is to determine\nhow the edge probability $p$ needs to scale to allow exact recovery in the\npresence of noise. Defining the degree (oversampling) rate of the graph by\n$\\alpha =np/\\log(n)$, it is shown that exact recovery is possible if and only\nif $\\alpha >2/(1-2\\varepsilon)^2+ o(1/(1-2\\varepsilon)^2)$. In other words,\n$2/(1-2\\varepsilon)^2$ is the information theoretic threshold for exact\nrecovery at low-SNR. In addition, an efficient recovery algorithm based on\nsemidefinite programming is proposed and shown to succeed in the threshold\nregime up to twice the optimal rate. For a deterministic graph $G$, defining\nthe degree rate as $\\alpha=d/\\log(n)$, where $d$ is the minimum degree of the\ngraph, it is shown that the proposed method achieves the rate $\\alpha>\n4((1+\\lambda)/(1-\\lambda)^2)/(1-2\\varepsilon)^2+ o(1/(1-2\\varepsilon)^2)$,\nwhere $1-\\lambda$ is the spectral gap of the graph $G$. \n\n"}
{"id": "1404.5236", "contents": "Title: Sum-of-squares proofs and the quest toward optimal algorithms Abstract: In order to obtain the best-known guarantees, algorithms are traditionally\ntailored to the particular problem we want to solve. Two recent developments,\nthe Unique Games Conjecture (UGC) and the Sum-of-Squares (SOS) method,\nsurprisingly suggest that this tailoring is not necessary and that a single\nefficient algorithm could achieve best possible guarantees for a wide range of\ndifferent problems.\n  The Unique Games Conjecture (UGC) is a tantalizing conjecture in\ncomputational complexity, which, if true, will shed light on the complexity of\na great many problems. In particular this conjecture predicts that a single\nconcrete algorithm provides optimal guarantees among all efficient algorithms\nfor a large class of computational problems.\n  The Sum-of-Squares (SOS) method is a general approach for solving systems of\npolynomial constraints. This approach is studied in several scientific\ndisciplines, including real algebraic geometry, proof complexity, control\ntheory, and mathematical programming, and has found applications in fields as\ndiverse as quantum information theory, formal verification, game theory and\nmany others.\n  We survey some connections that were recently uncovered between the Unique\nGames Conjecture and the Sum-of-Squares method. In particular, we discuss new\ntools to rigorously bound the running time of the SOS method for obtaining\napproximate solutions to hard optimization problems, and how these tools give\nthe potential for the sum-of-squares method to provide new guarantees for many\nproblems of interest, and possibly to even refute the UGC. \n\n"}
{"id": "1405.0329", "contents": "Title: Forbidden Induced Subgraphs of Normal Helly Circular-Arc Graphs:\n  Characterization and Detection Abstract: A normal Helly circular-arc graph is the intersection graph of arcs on a\ncircle of which no three or less arcs cover the whole circle. Lin, Soulignac,\nand Szwarcfiter [Discrete Appl. Math. 2013] characterized circular-arc graphs\nthat are not normal Helly circular-arc graphs, and used it to develop the first\nrecognition algorithm for this graph class. As open problems, they ask for the\nforbidden induced subgraph characterization and a direct recognition algorithm\nfor normal Helly circular-arc graphs, both of which are resolved by the current\npaper. Moreover, when the input is not a normal Helly circular-arc graph, our\nrecognition algorithm finds in linear time a minimal forbidden induced subgraph\nas certificate. \n\n"}
{"id": "1405.2424", "contents": "Title: Identification, location-domination and metric dimension on interval and\n  permutation graphs. II. Algorithms and complexity Abstract: We consider the problems of finding optimal identifying codes, (open)\nlocating-dominating sets and resolving sets (denoted IDENTIFYING CODE, (OPEN)\nLOCATING-DOMINATING SET and METRIC DIMENSION) of an interval or a permutation\ngraph. In these problems, one asks to distinguish all vertices of a graph by a\nsubset of the vertices, using either the neighbourhood within the solution set\nor the distances to the solution vertices. Using a general reduction for this\nclass of problems, we prove that the decision problems associated to these four\nnotions are NP-complete, even for interval graphs of diameter $2$ and\npermutation graphs of diameter $2$. While IDENTIFYING CODE and (OPEN)\nLOCATING-DOMINATING SET are trivially fixed-parameter-tractable when\nparameterized by solution size, it is known that in the same setting METRIC\nDIMENSION is $W[2]$-hard. We show that for interval graphs, this\nparameterization of METRIC DIMENSION is fixed-parameter-tractable. \n\n"}
{"id": "1405.3726", "contents": "Title: Topic words analysis based on LDA model Abstract: Social network analysis (SNA), which is a research field describing and\nmodeling the social connection of a certain group of people, is popular among\nnetwork services. Our topic words analysis project is a SNA method to visualize\nthe topic words among emails from Obama.com to accounts registered in Columbus,\nOhio. Based on Latent Dirichlet Allocation (LDA) model, a popular topic model\nof SNA, our project characterizes the preference of senders for target group of\nreceptors. Gibbs sampling is used to estimate topic and word distribution. Our\ntraining and testing data are emails from the carbon-free server\nDatagreening.com. We use parallel computing tool BashReduce for word processing\nand generate related words under each latent topic to discovers typical\ninformation of political news sending specially to local Columbus receptors.\nRunning on two instances using paralleling tool BashReduce, our project\ncontributes almost 30% speedup processing the raw contents, comparing with\nprocessing contents on one instance locally. Also, the experimental result\nshows that the LDA model applied in our project provides precision rate 53.96%\nhigher than TF-IDF model finding target words, on the condition that\nappropriate size of topic words list is selected. \n\n"}
{"id": "1405.4917", "contents": "Title: An Algebraic Hardness Criterion for Surjective Constraint Satisfaction Abstract: The constraint satisfaction problem (CSP) on a relational structure B is to\ndecide, given a set of constraints on variables where the relations come from\nB, whether or not there is a assignment to the variables satisfying all of the\nconstraints; the surjective CSP is the variant where one decides the existence\nof a surjective satisfying assignment onto the universe of B. We present an\nalgebraic condition on the polymorphism clone of B and prove that it is\nsufficient for the hardness of the surjective CSP on a finite structure B, in\nthe sense that this problem admits a reduction from a certain fixed-structure\nCSP. To our knowledge, this is the first result that allows one to use\nalgebraic information from a relational structure B to infer information on the\ncomplexity hardness of surjective constraint satisfaction on B. A corollary of\nour result is that, on any finite non-trivial structure having only essentially\nunary polymorphisms, surjective constraint satisfaction is NP-complete. \n\n"}
{"id": "1405.6791", "contents": "Title: Agnostic Learning of Disjunctions on Symmetric Distributions Abstract: We consider the problem of approximating and learning disjunctions (or\nequivalently, conjunctions) on symmetric distributions over $\\{0,1\\}^n$.\nSymmetric distributions are distributions whose PDF is invariant under any\npermutation of the variables. We give a simple proof that for every symmetric\ndistribution $\\mathcal{D}$, there exists a set of $n^{O(\\log{(1/\\epsilon)})}$\nfunctions $\\mathcal{S}$, such that for every disjunction $c$, there is function\n$p$, expressible as a linear combination of functions in $\\mathcal{S}$, such\nthat $p$ $\\epsilon$-approximates $c$ in $\\ell_1$ distance on $\\mathcal{D}$ or\n$\\mathbf{E}_{x \\sim \\mathcal{D}}[ |c(x)-p(x)|] \\leq \\epsilon$. This directly\ngives an agnostic learning algorithm for disjunctions on symmetric\ndistributions that runs in time $n^{O( \\log{(1/\\epsilon)})}$. The best known\nprevious bound is $n^{O(1/\\epsilon^4)}$ and follows from approximation of the\nmore general class of halfspaces (Wimmer, 2010). We also show that there exists\na symmetric distribution $\\mathcal{D}$, such that the minimum degree of a\npolynomial that $1/3$-approximates the disjunction of all $n$ variables is\n$\\ell_1$ distance on $\\mathcal{D}$ is $\\Omega( \\sqrt{n})$. Therefore the\nlearning result above cannot be achieved via $\\ell_1$-regression with a\npolynomial basis used in most other agnostic learning algorithms.\n  Our technique also gives a simple proof that for any product distribution\n$\\mathcal{D}$ and every disjunction $c$, there exists a polynomial $p$ of\ndegree $O(\\log{(1/\\epsilon)})$ such that $p$ $\\epsilon$-approximates $c$ in\n$\\ell_1$ distance on $\\mathcal{D}$. This was first proved by Blais et al.\n(2008) via a more involved argument. \n\n"}
{"id": "1405.7112", "contents": "Title: Optimal query complexity for estimating the trace of a matrix Abstract: Given an implicit $n\\times n$ matrix $A$ with oracle access $x^TA x$ for any\n$x\\in \\mathbb{R}^n$, we study the query complexity of randomized algorithms for\nestimating the trace of the matrix. This problem has many applications in\nquantum physics, machine learning, and pattern matching. Two metrics are\ncommonly used for evaluating the estimators: i) variance; ii) a high\nprobability multiplicative-approximation guarantee. Almost all the known\nestimators are of the form $\\frac{1}{k}\\sum_{i=1}^k x_i^T A x_i$ for $x_i\\in\n\\mathbb{R}^n$ being i.i.d. for some special distribution.\n  Our main results are summarized as follows. We give an exact characterization\nof the minimum variance unbiased estimator in the broad class of linear\nnonadaptive estimators (which subsumes all the existing known estimators). We\nalso consider the query complexity lower bounds for any (possibly nonlinear and\nadaptive) estimators: (1) We show that any estimator requires\n$\\Omega(1/\\epsilon)$ queries to have a guarantee of variance at most\n$\\epsilon$. (2) We show that any estimator requires\n$\\Omega(\\frac{1}{\\epsilon^2}\\log \\frac{1}{\\delta})$ queries to achieve a\n$(1\\pm\\epsilon)$-multiplicative approximation guarantee with probability at\nleast $1 - \\delta$. Both above lower bounds are asymptotically tight.\n  As a corollary, we also resolve a conjecture in the seminal work of Avron and\nToledo (Journal of the ACM 2011) regarding the sample complexity of the\nGaussian Estimator. \n\n"}
{"id": "1405.7375", "contents": "Title: Tensor Network Contractions for #SAT Abstract: The computational cost of counting the number of solutions satisfying a\nBoolean formula, which is a problem instance of #SAT, has proven subtle to\nquantify. Even when finding individual satisfying solutions is computationally\neasy (e.g. 2-SAT, which is in P), determining the number of solutions is\n#P-hard. Recently, computational methods simulating quantum systems experienced\nadvancements due to the development of tensor network algorithms and associated\nquantum physics-inspired techniques. By these methods, we give an algorithm\nusing an axiomatic tensor contraction language for n-variable #SAT instances\nwith complexity $O((g+cd)^{O(1)} 2^c)$ where $c$ is the number of COPY-tensors,\n$g$ is the number of gates, and $d$ is the maximal degree of any COPY-tensor.\nThus, counting problems can be solved efficiently when their tensor network\nexpression has at most $O(\\log c)$ COPY-tensors and polynomial fan-out. This\nframework also admits an intuitive proof of a variant of the Tovey conjecture\n(the r,1-SAT instance of the Dubois-Tovey theorem). This study increases the\ntheory, expressiveness and application of tensor based algorithmic tools and\nprovides an alternative insight on these problems which have a long history in\nstatistical physics and computer science. \n\n"}
{"id": "1405.7381", "contents": "Title: On computation with 'probabilities' modulo k Abstract: We propose a framework to study models of computation of indeterministic\ndata, represented by abstract \"distributions\". In these distributions,\nprobabilities are replaced by \"amplitudes\" drawn from a fixed semi-ring $S$, of\nwhich the non-negative reals, the complex numbers, finite fields $\\mathbb\nF_{p^r}$, and cyclic rings $\\mathbb Z_k$ are examples. Varying $S$ yields\ndifferent models of computation, which we may investigate to better understand\nthe (likely) difference in power between randomised and quantum computation.\nThe \"modal quantum states\" of Schumacher and Westmoreland [arXiv:1010.2929] are\nexamples of such distributions, for $S$ a finite field. For $S = \\mathbb F_2$,\nWillcock and Sabry [arXiv:1102.3587] show that UNIQUE-SAT is solvable by\npolynomial-time uniform circuit families consisting of invertible gates. We\ncharacterize the decision problems solvable by polynomial uniform circuit\nfamilies, using either invertible or \"unitary\" transformations over cyclic\nrings $S = \\mathbb Z_k$, or (in the case that $k$ is a prime power) finite\nfields $S = \\mathbb F_k$. In particular, for $k$ a prime power, these are\nprecisely the problems in the class $\\mathsf{Mod}_k\\mathsf P$. \n\n"}
{"id": "1405.7849", "contents": "Title: Very narrow quantum OBDDs and width hierarchies for classical OBDDs Abstract: We present several results on comparative complexity for different variants\nof OBDD models.\n  - We present some results on comparative complexity of classical and quantum\nOBDDs. We consider a partial function depending on parameter k such that for\nany k > 0 this function is computed by an exact quantum OBDD of width 2 but any\nclassical OBDD (deterministic or stable bounded error probabilistic) needs\nwidth 2k+1.\n  - We consider quantum and classical nondeterminism. We show that quantum\nnondeterminism can be more efficient than classical one. In particular, an\nexplicit function is presented which is computed by a quantum nondeterministic\nOBDD with constant width but any classical nondeterministic OBDD for this\nfunction needs non-constant width.\n  - We also present new hierarchies on widths of deterministic and\nnon-deterministic OBDDs. We focus both on small and large widths. \n\n"}
{"id": "1406.3514", "contents": "Title: Limits of CSP Problems and Efficient Parameter Testing Abstract: We present a unified framework on the limits of constraint satisfaction\nproblems (CSPs) and efficient parameter testing which depends only on array\nexchangeability and the method of cut decomposition without recourse to the\nweakly regular partitions. In particular, we formulate and prove a\nrepresentation theorem for compact colored $r$-uniform directed hypergraph\n($r$-graph) limits, and apply this to $r$CSP limits. We investigate the sample\ncomplexity of testable $r$-graph parameters, we discuss the generalized ground\nstate energies and demonstrate that they are efficiently testable. \n\n"}
{"id": "1406.4566", "contents": "Title: Guaranteed Scalable Learning of Latent Tree Models Abstract: We present an integrated approach for structure and parameter estimation in\nlatent tree graphical models. Our overall approach follows a\n\"divide-and-conquer\" strategy that learns models over small groups of variables\nand iteratively merges onto a global solution. The structure learning involves\ncombinatorial operations such as minimum spanning tree construction and local\nrecursive grouping; the parameter learning is based on the method of moments\nand on tensor decompositions. Our method is guaranteed to correctly recover the\nunknown tree structure and the model parameters with low sample complexity for\nthe class of linear multivariate latent tree models which includes discrete and\nGaussian distributions, and Gaussian mixtures. Our bulk asynchronous parallel\nalgorithm is implemented in parallel and the parallel computation complexity\nincreases only logarithmically with the number of variables and linearly with\ndimensionality of each variable. \n\n"}
{"id": "1406.6353", "contents": "Title: A Lower Bound of $2^n$ Conditional Branches for Boolean Satisfiability\n  on Post Machines Abstract: We establish a lower bound of $2^n$ conditional branches for deciding the\nsatisfiability of the conjunction of any two Boolean formulas from a set called\na full representation of Boolean functions of $n$ variables - a set containing\na Boolean formula to represent each Boolean function of $n$ variables. The\ncontradiction proof first assumes that there exists a Post machine (Post's\nFormulation 1) that correctly decides the satisfiability of the conjunction of\nany two Boolean formulas from such a set by following an execution path that\nincludes fewer than $2^n$ conditional branches. By using multiple runs of this\nPost machine, with one run for each Boolean function of $n$ variables, the\nproof derives a contradiction by showing that this Post machine is unable to\ncorrectly decide the satisfiability of the conjunction of at least one pair of\nBoolean formulas from a full representation of $n$-variable Boolean functions\nif the machine executes fewer than $2^n$ conditional branches. This lower bound\nof $2^n$ conditional branches holds for any full representation of Boolean\nfunctions of $n$ variables, even if a full representation consists solely of\nminimized Boolean formulas derived by a Boolean minimization method. We discuss\nwhy the lower bound fails to hold for satisfiability of certain restricted\nformulas, such as 2CNF satisfiability, XOR-SAT, and HORN-SAT. We also relate\nthe lower bound to 3CNF satisfiability. The lower bound does not depend on\nsequentiality of access to the boxes in the symbol space and will hold even if\na machine is capable of non-sequential access. \n\n"}
{"id": "1406.6567", "contents": "Title: Fixed-Parameter Tractability of Token Jumping on Planar Graphs Abstract: Suppose that we are given two independent sets $I_0$ and $I_r$ of a graph\nsuch that $|I_0| = |I_r|$, and imagine that a token is placed on each vertex in\n$I_0$. The token jumping problem is to determine whether there exists a\nsequence of independent sets which transforms $I_0$ into $I_r$ so that each\nindependent set in the sequence results from the previous one by moving exactly\none token to another vertex. This problem is known to be PSPACE-complete even\nfor planar graphs of maximum degree three, and W[1]-hard for general graphs\nwhen parameterized by the number of tokens. In this paper, we present a\nfixed-parameter algorithm for the token jumping problem on planar graphs, where\nthe parameter is only the number of tokens. Furthermore, the algorithm can be\nmodified so that it finds a shortest sequence for a yes-instance. The same\nscheme of the algorithms can be applied to a wider class of graphs,\n$K_{3,t}$-free graphs for any fixed integer $t \\ge 3$, and it yields\nfixed-parameter algorithms. \n\n"}
{"id": "1406.6576", "contents": "Title: Linear-Time Algorithm for Sliding Tokens on Trees Abstract: Suppose that we are given two independent sets $I_b$ and $I_r$ of a graph\nsuch that $|I_b|=|I_r|$, and imagine that a token is placed on each vertex in\n$I_b$. Then, the sliding token problem is to determine whether there exists a\nsequence of independent sets which transforms $I_b$ into $I_r$ so that each\nindependent set in the sequence results from the previous one by sliding\nexactly one token along an edge in the graph. This problem is known to be\nPSPACE-complete even for planar graphs, and also for bounded treewidth graphs.\nIn this paper, we thus study the problem restricted to trees, and give the\nfollowing three results: (1) the decision problem is solvable in linear time;\n(2) for a yes-instance, we can find in quadratic time an actual sequence of\nindependent sets between $I_b$ and $I_r$ whose length (i.e., the number of\ntoken-slides) is quadratic; and (3) there exists an infinite family of\ninstances on paths for which any sequence requires quadratic length. \n\n"}
{"id": "1406.7279", "contents": "Title: Guruswami-Sinop Rounding without Higher Level Lasserre Abstract: Guruswami and Sinop give a $O(1/\\delta)$ approximation guarantee for the\nnon-uniform Sparsest Cut problem by solving $O(r)$-level Lasserre semidefinite\nconstraints, provided that the generalized eigenvalues of the Laplacians of the\ncost and demand graphs satisfy a certain spectral condition, namely,\n$\\lambda_{r+1} \\geq \\Phi^{*}/(1-\\delta)$. Their key idea is a rounding\ntechnique that first maps a vector-valued solution to $[0, 1]$ using\nappropriately scaled projections onto Lasserre vectors. In this paper, we show\nthat similar projections and analysis can be obtained using only $\\ell_{2}^{2}$\ntriangle inequality constraints. This results in a $O(r/\\delta^{2})$\napproximation guarantee for the non-uniform Sparsest Cut problem by adding only\n$\\ell_{2}^{2}$ triangle inequality constraints to the usual semidefinite\nprogram, provided that the same spectral condition, $\\lambda_{r+1} \\geq\n\\Phi^{*}/(1-\\delta)$, holds. \n\n"}
{"id": "1406.7398", "contents": "Title: A framework for good SAT translations, with applications to CNF\n  representations of XOR constraints Abstract: We present a general framework for good CNF-representations of boolean\nconstraints, to be used for translating decision problems into SAT problems\n(i.e., deciding satisfiability for conjunctive normal forms). We apply it to\nthe representation of systems of XOR-constraints, also known as systems of\nlinear equations over the two-element field, or systems of parity constraints.\n  The general framework defines the notion of \"representation\", and provides\nseveral methods to measure the quality of the representation by the complexity\n(\"hardness\") needed for making implicit \"knowledge\" of the representation\nexplicit (to a SAT-solving mechanism). We obtain general upper and lower\nbounds.\n  Applied to systems of XOR-constraints, we show a super-polynomial lower bound\non \"good\" representations under very general circumstances. A corresponding\nupper bound shows fixed-parameter tractability in the number of constraints.\n  The measurement underlying this upper bound ignores the auxiliary variables\nneeded for shorter representations of XOR-constraints. Improved upper bounds\n(for special cases) take them into account, and a rich picture begins to\nemerge, under the various hardness measurements. \n\n"}
{"id": "1407.1746", "contents": "Title: Sum-of-squares hierarchy lower bounds for symmetric formulations Abstract: We introduce a method for proving Sum-of-Squares (SoS)/ Lasserre hierarchy\nlower bounds when the initial problem formulation exhibits a high degree of\nsymmetry. Our main technical theorem allows us to reduce the study of the\npositive semidefiniteness to the analysis of \"well-behaved\" univariate\npolynomial inequalities.\n  We illustrate the technique on two problems, one unconstrained and the other\nwith constraints. More precisely, we give a short elementary proof of\nGrigoriev/Laurent lower bound for finding the integer cut polytope of the\ncomplete graph. We also show that the SoS hierarchy requires a non-constant\nnumber of rounds to improve the initial integrality gap of 2 for the\nMin-Knapsack linear program strengthened with cover inequalities. \n\n"}
{"id": "1407.2178", "contents": "Title: Restricted Isometry Property for General p-Norms Abstract: The Restricted Isometry Property (RIP) is a fundamental property of a matrix\nwhich enables sparse recovery. Informally, an $m \\times n$ matrix satisfies RIP\nof order $k$ for the $\\ell_p$ norm, if $\\|Ax\\|_p \\approx \\|x\\|_p$ for every\nvector $x$ with at most $k$ non-zero coordinates.\n  For every $1 \\leq p < \\infty$ we obtain almost tight bounds on the minimum\nnumber of rows $m$ necessary for the RIP property to hold. Prior to this work,\nonly the cases $p = 1$, $1 + 1 / \\log k$, and $2$ were studied. Interestingly,\nour results show that the case $p = 2$ is a \"singularity\" point: the optimal\nnumber of rows $m$ is $\\widetilde{\\Theta}(k^{p})$ for all $p\\in\n[1,\\infty)\\setminus \\{2\\}$, as opposed to $\\widetilde{\\Theta}(k)$ for $k=2$.\n  We also obtain almost tight bounds for the column sparsity of RIP matrices\nand discuss implications of our results for the Stable Sparse Recovery problem. \n\n"}
{"id": "1407.3200", "contents": "Title: Lock-in Problem for Parallel Rotor-router Walks Abstract: The rotor-router model, also called the Propp machine, was introduced as a\ndeterministic alternative to the random walk. In this model, a group of\nidentical tokens are initially placed at nodes of the graph. Each node\nmaintains a cyclic ordering of the outgoing arcs, and during consecutive turns\nthe tokens are propagated along arcs chosen according to this ordering in\nround-robin fashion. The behavior of the model is fully deterministic. Yanovski\net al.(2003) proved that a single rotor-router walk on any graph with m edges\nand diameter $D$ stabilizes to a traversal of an Eulerian circuit on the set of\nall 2m directed arcs on the edge set of the graph, and that such periodic\nbehaviour of the system is achieved after an initial transient phase of at most\n2mD steps. The case of multiple parallel rotor-routers was studied\nexperimentally, leading Yanovski et al. to the conjecture that a system of $k\n\\textgreater{} 1$ parallel walks also stabilizes with a period of length at\nmost $2m$ steps. In this work we disprove this conjecture, showing that the\nperiod of parallel rotor-router walks can in fact, be superpolynomial in the\nsize of graph. On the positive side, we provide a characterization of the\nperiodic behavior of parallel router walks, in terms of a structural property\nof stable states called a subcycle decomposition. This property provides us the\ntools to efficiently detect whether a given system configuration corresponds to\nthe transient or to the limit behavior of the system. Moreover, we provide\npolynomial upper bounds of $O(m^4 D^2 + mD \\log k)$ and $O(m^5 k^2)$ on the\nnumber of steps it takes for the system to stabilize. Thus, we are able to\npredict any future behavior of the system using an algorithm that takes\npolynomial time and space. In addition, we show that there exists a separation\nbetween the stabilization time of the single-walk and multiple-walk\nrotor-router systems, and that for some graphs the latter can be asymptotically\nlarger even for the case of $k = 2$ walks. \n\n"}
{"id": "1407.5298", "contents": "Title: How the Experts Algorithm Can Help Solve LPs Online Abstract: We consider the problem of solving packing/covering LPs online, when the\ncolumns of the constraint matrix are presented in random order. This problem\nhas received much attention and the main focus is to figure out how large the\nright-hand sides of the LPs have to be (compared to the entries on the\nleft-hand side of the constraints) to allow $(1+\\epsilon)$-approximations\nonline. It is known that the right-hand sides have to be $\\Omega(\\epsilon^{-2}\n\\log m)$ times the left-hand sides, where $m$ is the number of constraints.\n  In this paper we give a primal-dual algorithm that achieve this bound for\nmixed packing/covering LPs. Our algorithms construct dual solutions using a\nregret-minimizing online learning algorithm in a black-box fashion, and use\nthem to construct primal solutions. The adversarial guarantee that holds for\nthe constructed duals helps us to take care of most of the correlations that\narise in the algorithm; the remaining correlations are handled via martingale\nconcentration and maximal inequalities. These ideas lead to conceptually simple\nand modular algorithms, which we hope will be useful in other contexts. \n\n"}
{"id": "1407.6692", "contents": "Title: 2-Server PIR with sub-polynomial communication Abstract: A 2-server Private Information Retrieval (PIR) scheme allows a user to\nretrieve the $i$th bit of an $n$-bit database replicated among two servers\n(which do not communicate) while not revealing any information about $i$ to\neither server. In this work we construct a 1-round 2-server PIR with total\ncommunication cost $n^{O({\\sqrt{\\log\\log n/\\log n}})}$. This improves over the\ncurrently known 2-server protocols which require $O(n^{1/3})$ communication and\nmatches the communication cost of known 3-server PIR schemes. Our improvement\ncomes from reducing the number of servers in existing protocols, based on\nMatching Vector Codes, from 3 or 4 servers to 2. This is achieved by viewing\nthese protocols in an algebraic way (using polynomial interpolation) and\nextending them using partial derivatives. \n\n"}
{"id": "1407.6756", "contents": "Title: Higher Lower Bounds from the 3SUM Conjecture Abstract: The 3SUM conjecture has proven to be a valuable tool for proving conditional\nlower bounds on dynamic data structures and graph problems. This line of work\nwas initiated by P\\v{a}tra\\c{s}cu (STOC 2010) who reduced 3SUM to an offline\nSetDisjointness problem. However, the reduction introduced by P\\v{a}tra\\c{s}cu\nsuffers from several inefficiencies, making it difficult to obtain tight\nconditional lower bounds from the 3SUM conjecture.\n  In this paper we address many of the deficiencies of P\\v{a}tra\\c{s}cu's\nframework. We give new and efficient reductions from 3SUM to offline\nSetDisjointness and offline SetIntersection (the reporting version of\nSetDisjointness) which leads to polynomially higher lower bounds on several\nproblems. Using our reductions, we are able to show the essential optimality of\nseveral algorithms, assuming the 3SUM conjecture.\n  - Chiba and Nishizeki's $O(m\\alpha)$-time algorithm (SICOMP 1985) for\nenumerating all triangles in a graph with arboricity/degeneracy $\\alpha$ is\nessentially optimal, for any $\\alpha$.\n  - Bj{\\o}rklund, Pagh, Williams, and Zwick's algorithm (ICALP 2014) for\nlisting $t$ triangles is essentially optimal (assuming the matrix\nmultiplication exponent is $\\omega=2$).\n  - Any static data structure for SetDisjointness that answers queries in\nconstant time must spend $\\Omega(N^{2-o(1)})$ time in preprocessing, where $N$\nis the size of the set system.\n  These statements were unattainable via P\\v{a}tra\\c{s}cu's reductions.\n  We also introduce several new reductions from 3SUM to pattern matching\nproblems and dynamic graph problems. Of particular interest are new conditional\nlower bounds for dynamic versions of Maximum Cardinality Matching, which\nintroduce a new technique for obtaining amortized lower bounds. \n\n"}
{"id": "1407.6846", "contents": "Title: The Power of Two Choices with Simple Tabulation Abstract: The power of two choices is a classic paradigm for load balancing when\nassigning $m$ balls to $n$ bins. When placing a ball, we pick two bins\naccording to two hash functions $h_0$ and $h_1$, and place the ball in the\nleast loaded bin. Assuming fully random hash functions, when $m=O(n)$, Azar et\nal.~[STOC'94] proved that the maximum load is $\\lg \\lg n + O(1)$ with high\nprobability.\n  In this paper, we investigate the power of two choices when the hash\nfunctions $h_0$ and $h_1$ are implemented with simple tabulation, which is a\nvery efficient hash function evaluated in constant time. Following their\nanalysis of Cuckoo hashing [J.ACM'12], P\\v{a}tra\\c{s}cu and Thorup claimed that\nthe expected maximum load with simple tabulation is $O(\\lg\\lg n)$. This did not\ninclude any high probability guarantee, so the load balancing was not yet to be\ntrusted.\n  Here, we show that with simple tabulation, the maximum load is $O(\\lg\\lg n)$\nwith high probability, giving the first constant time hash function with this\nguarantee. We also give a concrete example where, unlike with fully random\nhashing, the maximum load is not bounded by $\\lg \\lg n + O(1)$, or even\n$(1+o(1))\\lg \\lg n$ with high probability. Finally, we show that the expected\nmaximum load is $\\lg \\lg n + O(1)$, just like with fully random hashing. \n\n"}
{"id": "1407.7740", "contents": "Title: Privacy and Truthful Equilibrium Selection for Aggregative Games Abstract: We study a very general class of games --- multi-dimensional aggregative\ngames --- which in particular generalize both anonymous games and weighted\ncongestion games. For any such game that is also large, we solve the\nequilibrium selection problem in a strong sense. In particular, we give an\nefficient weak mediator: a mechanism which has only the power to listen to\nreported types and provide non-binding suggested actions, such that (a) it is\nan asymptotic Nash equilibrium for every player to truthfully report their type\nto the mediator, and then follow its suggested action; and (b) that when\nplayers do so, they end up coordinating on a particular asymptotic pure\nstrategy Nash equilibrium of the induced complete information game. In fact,\ntruthful reporting is an ex-post Nash equilibrium of the mediated game, so our\nsolution applies even in settings of incomplete information, and even when\nplayer types are arbitrary or worst-case (i.e. not drawn from a common prior).\nWe achieve this by giving an efficient differentially private algorithm for\ncomputing a Nash equilibrium in such games. The rates of convergence to\nequilibrium in all of our results are inverse polynomial in the number of\nplayers $n$. We also apply our main results to a multi-dimensional market game.\n  Our results can be viewed as giving, for a rich class of games, a more robust\nversion of the Revelation Principle, in that we work with weaker informational\nassumptions (no common prior), yet provide a stronger solution concept (ex-post\nNash versus Bayes Nash equilibrium). In comparison to previous work, our main\nconceptual contribution is showing that weak mediators are a game theoretic\nobject that exist in a wide variety of games -- previously, they were only\nknown to exist in traffic routing games. \n\n"}
{"id": "1407.7887", "contents": "Title: Going for Speed: Sublinear Algorithms for Dense r-CSPs Abstract: We give new sublinear and parallel algorithms for the extensively studied\nproblem of approximating n-variable r-CSPs (constraint satisfaction problems\nwith constraints of arity r up to an additive error. The running time of our\nalgorithms is O(n/\\epsilon^2) + 2^O(1/\\epsilon^2) for Boolean r-CSPs and O(k^4\nn / \\epsilon^2) + 2^O(log k / \\epsilon^2) for r-CSPs with constraints on\nvariables over an alphabet of size k. For any constant k this gives optimal\ndependence on n in the running time unconditionally, while the exponent in the\ndependence on 1/\\epsilon is polynomially close to the lower bound under the\nexponential-time hypothesis, which is 2^\\Omega(\\epsilon^(-1/2)).\n  For Max-Cut this gives an exponential improvement in dependence on 1/\\epsilon\ncompared to the sublinear algorithms of Goldreich, Goldwasser and Ron (JACM'98)\nand a linear speedup in n compared to the algorithms of Mathieu and Schudy\n(SODA'08). For the maximization version of k-Correlation Clustering problem our\nrunning time is O(k^4 n / \\epsilon^2) + k^O(1/\\epsilon^2), improving the\npreviously best n k^{O(1/\\epsilon^3 log k/\\epsilon) by Guruswami and Giotis\n(SODA'06). \n\n"}
{"id": "1408.1376", "contents": "Title: Factorization Norms and Hereditary Discrepancy Abstract: The $\\gamma_2$ norm of a real $m\\times n$ matrix $A$ is the minimum number\n$t$ such that the column vectors of $A$ are contained in a $0$-centered\nellipsoid $E\\subseteq\\mathbb{R}^m$ which in turn is contained in the hypercube\n$[-t, t]^m$. We prove that this classical quantity approximates the\n\\emph{hereditary discrepancy} $\\mathrm{herdisc}\\ A$ as follows: $\\gamma_2(A) =\n{O(\\log m)}\\cdot \\mathrm{herdisc}\\ A$ and $\\mathrm{herdisc}\\ A = O(\\sqrt{\\log\nm}\\,)\\cdot\\gamma_2(A) $. Since $\\gamma_2$ is polynomial-time computable, this\ngives a polynomial-time approximation algorithm for hereditary discrepancy.\nBoth inequalities are shown to be asymptotically tight.\n  We then demonstrate on several examples the power of the $\\gamma_2$ norm as a\ntool for proving lower and upper bounds in discrepancy theory. Most notably, we\nprove a new lower bound of $\\Omega(\\log^{d-1} n)$ for the \\emph{$d$-dimensional\nTusn\\'ady problem}, asking for the combinatorial discrepancy of an $n$-point\nset in $\\mathbb{R}^d$ with respect to axis-parallel boxes. For $d>2$, this\nimproves the previous best lower bound, which was of order approximately\n$\\log^{(d-1)/2}n$, and it comes close to the best known upper bound of\n$O(\\log^{d+1/2}n)$, for which we also obtain a new, very simple proof. \n\n"}
{"id": "1408.2425", "contents": "Title: Hypergraph Markov Operators, Eigenvalues and Approximation Algorithms Abstract: The celebrated Cheeger's Inequality \\cite{am85,a86} establishes a bound on\nthe expansion of a graph via its spectrum. This inequality is central to a rich\nspectral theory of graphs, based on studying the eigenvalues and eigenvectors\nof the adjacency matrix (and other related matrices) of graphs. It has remained\nopen to define a suitable spectral model for hypergraphs whose spectra can be\nused to estimate various combinatorial properties of the hypergraph.\n  In this paper we introduce a new hypergraph Laplacian operator (generalizing\nthe Laplacian matrix of graphs)and study its spectra. We prove a Cheeger-type\ninequality for hypergraphs, relating the second smallest eigenvalue of this\noperator to the expansion of the hypergraph. We bound other hypergraph\nexpansion parameters via higher eigenvalues of this operator. We give bounds on\nthe diameter of the hypergraph as a function of the second smallest eigenvalue\nof the Laplacian operator. The Markov process underlying the Laplacian operator\ncan be viewed as a dispersion process on the vertices of the hypergraph that\nmight be of independent interest. We bound the {\\em Mixing-time} of this\nprocess as a function of the second smallest eigenvalue of the Laplacian\noperator. All these results are generalizations of the corresponding results\nfor graphs.\n  We show that there can be no linear operator for hypergraphs whose spectra\ncaptures hypergraph expansion in a Cheeger-like manner. For any $k$, we give a\npolynomial time algorithm to compute an approximation to the $k^{th}$ smallest\neigenvalue of the operator. We show that this approximation factor is optimal\nunder the SSE hypothesis (introduced by \\cite{rs10}) for constant values of\n$k$.\n  Finally, using the factor preserving reduction from vertex expansion in\ngraphs to hypergraph expansion, we show that all our results for hypergraphs\nextend to vertex expansion in graphs. \n\n"}
{"id": "1408.3467", "contents": "Title: Evaluating Visual Properties via Robust HodgeRank Abstract: Nowadays, how to effectively evaluate visual properties has become a popular\ntopic for fine-grained visual comprehension. In this paper we study the problem\nof how to estimate such visual properties from a ranking perspective with the\nhelp of the annotators from online crowdsourcing platforms. The main challenges\nof our task are two-fold. On one hand, the annotations often contain\ncontaminated information, where a small fraction of label flips might ruin the\nglobal ranking of the whole dataset. On the other hand, considering the large\ndata capacity, the annotations are often far from being complete. What is\nworse, there might even exist imbalanced annotations where a small subset of\nsamples are frequently annotated. Facing such challenges, we propose a robust\nranking framework based on the principle of Hodge decomposition of imbalanced\nand incomplete ranking data. According to the HodgeRank theory, we find that\nthe major source of the contamination comes from the cyclic ranking component\nof the Hodge decomposition. This leads us to an outlier detection formulation\nas sparse approximations of the cyclic ranking projection. Taking a step\nfurther, it facilitates a novel outlier detection model as Huber's LASSO in\nrobust statistics. Moreover, simple yet scalable algorithms are developed based\non Linearized Bregman Iteration to achieve an even less biased estimator.\nStatistical consistency of outlier detection is established in both cases under\nnearly the same conditions. Our studies are supported by experiments with both\nsimulated examples and real-world data. The proposed framework provides us a\npromising tool for robust ranking with large scale crowdsourcing data arising\nfrom computer vision. \n\n"}
{"id": "1408.5274", "contents": "Title: Deciding game invariance Abstract: Duch\\^ene and Rigo introduced the notion of invariance for take-away games on\nheaps. Roughly speaking, these are games whose rulesets do not depend on the\nposition. Given a sequence $S$ of positive tuples of integers, the question of\nwhether there exists an invariant game having $S$ as set of\n$\\mathcal{P}$-positions is relevant. In particular, it was recently proved by\nLarsson et al. that if $S$ is a pair of complementary Beatty sequences, then\nthe answer to this question is always positive. In this paper, we show that for\na fairly large set of sequences (expressed by infinite words), the answer to\nthis question is decidable. \n\n"}
{"id": "1408.5425", "contents": "Title: Heat and Noise on Cubes and Spheres: The Sensitivity of Randomly Rotated\n  Polynomial Threshold Functions Abstract: We establish a precise relationship between spherical harmonics and Fourier\nbasis functions over a hypercube randomly embedded in the sphere. In\nparticular, we give a bound on the expected Boolean noise sensitivity of a\nrandomly rotated function in terms of its \"spherical sensitivity,\" which we\ndefine according to its evolution under the spherical heat equation. As an\napplication, we prove an average case of the Gotsman-Linial conjecture,\nbounding the sensitivity of polynomial threshold functions subjected to a\nrandom rotation. \n\n"}
{"id": "1409.1534", "contents": "Title: Algorithms in Real Algebraic Geometry: A Survey Abstract: We survey both old and new developments in the theory of algorithms in real\nalgebraic geometry -- starting from effective quantifier elimination in the\nfirst order theory of reals due to Tarski and Seidenberg, to more recent\nalgorithms for computing topological invariants of semi-algebraic sets. We\nemphasize throughout the complexity aspects of these algorithms and also\ndiscuss the computational hardness of the underlying problems. We also describe\nsome recent results linking the computational hardness of decision problems in\nthe first order theory of the reals, with that of computing certain topological\ninvariants of semi-algebraic sets. Even though we mostly concentrate on exact\nalgorithms, we also discuss some numerical approaches involving semi-definite\nprogramming that have gained popularity in recent times. \n\n"}
{"id": "1409.2290", "contents": "Title: Computational Complexity, Phase Transitions, and Message-Passing for\n  Community Detection Abstract: We take a whirlwind tour of problems and techniques at the boundary of\ncomputer science and statistical physics. We start with a brief description of\nP, NP, and NP-completeness. We then discuss random graphs, including the\nemergence of the giant component and the k-core, using techniques from\nbranching processes and differential equations. Using these tools as well as\nthe second moment method, we give upper and lower bounds on the critical clause\ndensity for random k-SAT. We end with community detection in networks,\nvariational methods, the Bethe free energy, belief propagation, the\ndetectability transition, and the non-backtracking matrix. \n\n"}
{"id": "1409.3182", "contents": "Title: Ground state connectivity of local Hamiltonians Abstract: The study of ground state energies of local Hamiltonians has played a\nfundamental role in quantum complexity theory. In this paper, we take a new\ndirection by introducing the physically motivated notion of \"ground state\nconnectivity\" of local Hamiltonians, which captures problems in areas ranging\nfrom quantum stabilizer codes to quantum memories. Roughly, \"ground state\nconnectivity\" corresponds to the natural question: Given two ground states\n|{\\psi}> and |{\\phi}> of a local Hamiltonian H, is there an \"energy barrier\"\n(with respect to H) along any sequence of local operations mapping |{\\psi}> to\n|{\\phi}>? We show that the complexity of this question can range from\nQCMA-complete to PSPACE-complete, as well as NEXP-complete for an appropriately\ndefined \"succinct\" version of the problem. As a result, we obtain a natural\nQCMA-complete problem, a goal which has generally proven difficult since the\nconception of QCMA over a decade ago. Our proofs rely on a new technical tool,\nthe Traversal Lemma, which analyzes the Hilbert space a local unitary evolution\nmust traverse under certain conditions. We show that this lemma is essentially\ntight with respect to the length of the unitary evolution in question. \n\n"}
{"id": "1409.4080", "contents": "Title: Algorithmic complexity for psychology: A user-friendly implementation of\n  the coding theorem method Abstract: Kolmogorov-Chaitin complexity has long been believed to be impossible to\napproximate when it comes to short sequences (e.g. of length 5-50). However,\nwith the newly developed \\emph{coding theorem method} the complexity of strings\nof length 2-11 can now be numerically estimated. We present the theoretical\nbasis of algorithmic complexity for short strings (ACSS) and describe an\nR-package providing functions based on ACSS that will cover psychologists'\nneeds and improve upon previous methods in three ways: (1) ACSS is now\navailable not only for binary strings, but for strings based on up to 9\ndifferent symbols, (2) ACSS no longer requires time-consuming computing, and\n(3) a new approach based on ACSS gives access to an estimation of the\ncomplexity of strings of any length. Finally, three illustrative examples show\nhow these tools can be applied to psychology. \n\n"}
{"id": "1409.7458", "contents": "Title: Beyond Maximum Likelihood: from Theory to Practice Abstract: Maximum likelihood is the most widely used statistical estimation technique.\nRecent work by the authors introduced a general methodology for the\nconstruction of estimators for functionals in parametric models, and\ndemonstrated improvements - both in theory and in practice - over the maximum\nlikelihood estimator (MLE), particularly in high dimensional scenarios\ninvolving parameter dimension comparable to or larger than the number of\nsamples. This approach to estimation, building on results from approximation\ntheory, is shown to yield minimax rate-optimal estimators for a wide class of\nfunctionals, implementable with modest computational requirements. In a\nnutshell, a message of this recent work is that, for a wide class of\nfunctionals, the performance of these essentially optimal estimators with $n$\nsamples is comparable to that of the MLE with $n \\ln n$ samples.\n  In the present paper, we highlight the applicability of the aforementioned\nmethodology to statistical problems beyond functional estimation, and show that\nit can yield substantial gains. For example, we demonstrate that for learning\ntree-structured graphical models, our approach achieves a significant reduction\nof the required data size compared with the classical Chow--Liu algorithm,\nwhich is an implementation of the MLE, to achieve the same accuracy. The key\nstep in improving the Chow--Liu algorithm is to replace the empirical mutual\ninformation with the estimator for mutual information proposed by the authors.\nFurther, applying the same replacement approach to classical Bayesian network\nclassification, the resulting classifiers uniformly outperform the previous\nclassifiers on 26 widely used datasets. \n\n"}
{"id": "1410.1703", "contents": "Title: A Truthful Mechanism for the Generalized Assignment Problem Abstract: We propose a truthful-in-expectation, $(1-1/e)$-approximation mechanism for a\nstrategic variant of the generalized assignment problem (GAP). In GAP, a set of\nitems has to be optimally assigned to a set of bins without exceeding the\ncapacity of any singular bin. In the strategic variant of the problem we study,\nvalues for assigning items to bins are the private information of bidders and\nthe mechanism should provide bidders with incentives to truthfully report their\nvalues. The approximation ratio of the mechanism is a significant improvement\nover the approximation ratio of the existing truthful mechanism for GAP.\n  The proposed mechanism comprises a novel convex optimization program as the\nallocation rule as well as an appropriate payment rule. To implement the convex\nprogram in polynomial time, we propose a fractional local search algorithm\nwhich approximates the optimal solution within an arbitrarily small error\nleading to an approximately truthful-in-expectation mechanism. The presented\nalgorithm improves upon the existing optimization algorithms for GAP in terms\nof simplicity and runtime while the approximation ratio closely matches the\nbest approximation ratio given for GAP when all inputs are publicly known. \n\n"}
{"id": "1410.3886", "contents": "Title: Tighter Low-rank Approximation via Sampling the Leveraged Element Abstract: In this work, we propose a new randomized algorithm for computing a low-rank\napproximation to a given matrix. Taking an approach different from existing\nliterature, our method first involves a specific biased sampling, with an\nelement being chosen based on the leverage scores of its row and column, and\nthen involves weighted alternating minimization over the factored form of the\nintended low-rank matrix, to minimize error only on these samples. Our method\ncan leverage input sparsity, yet produce approximations in {\\em spectral} (as\nopposed to the weaker Frobenius) norm; this combines the best aspects of\notherwise disparate current results, but with a dependence on the condition\nnumber $\\kappa = \\sigma_1/\\sigma_r$. In particular we require $O(nnz(M) +\n\\frac{n\\kappa^2 r^5}{\\epsilon^2})$ computations to generate a rank-$r$\napproximation to $M$ in spectral norm. In contrast, the best existing method\nrequires $O(nnz(M)+ \\frac{nr^2}{\\epsilon^4})$ time to compute an approximation\nin Frobenius norm. Besides the tightness in spectral norm, we have a better\ndependence on the error $\\epsilon$. Our method is naturally and highly\nparallelizable.\n  Our new approach enables two extensions that are interesting on their own.\nThe first is a new method to directly compute a low-rank approximation (in\nefficient factored form) to the product of two given matrices; it computes a\nsmall random set of entries of the product, and then executes weighted\nalternating minimization (as before) on these. The sampling strategy is\ndifferent because now we cannot access leverage scores of the product matrix\n(but instead have to work with input matrices). The second extension is an\nimproved algorithm with smaller communication complexity for the distributed\nPCA setting (where each server has small set of rows of the matrix, and want to\ncompute low rank approximation with small amount of communication with other\nservers). \n\n"}
{"id": "1410.5297", "contents": "Title: A Polynomial Time Algorithm For The Conjugacy Decision and Search\n  Problems in Free Abelian-by-Infinite Cyclic Groups Abstract: In this paper we introduce a polynomial time algorithm that solves both the\nconjugacy decision and search problems in free abelian-by-infinite cyclic\ngroups where the input is elements in normal form. We do this by adapting the\nwork of Bogopolski, Martino, Maslakova, and Ventura in\n\\cite{bogopolski2006conjugacy} and Bogopolski, Martino, and Ventura in\n\\cite{bogopolski2010orbit}, to free abelian-by-infinite cyclic groups, and in\ncertain cases apply a polynomial time algorithm for the orbit problem over\n$\\Z^n$ by Kannan and Lipton. \n\n"}
{"id": "1410.5518", "contents": "Title: On Symmetric and Asymmetric LSHs for Inner Product Search Abstract: We consider the problem of designing locality sensitive hashes (LSH) for\ninner product similarity, and of the power of asymmetric hashes in this\ncontext. Shrivastava and Li argue that there is no symmetric LSH for the\nproblem and propose an asymmetric LSH based on different mappings for query and\ndatabase points. However, we show there does exist a simple symmetric LSH that\nenjoys stronger guarantees and better empirical performance than the asymmetric\nLSH they suggest. We also show a variant of the settings where asymmetry is\nin-fact needed, but there a different asymmetric LSH is required. \n\n"}
{"id": "1410.8202", "contents": "Title: Binary Determinantal Complexity Abstract: We prove that for writing the 3 by 3 permanent polynomial as a determinant of\na matrix consisting only of zeros, ones, and variables as entries, a 7 by 7\nmatrix is required. Our proof is computer based and uses the enumeration of\nbipartite graphs. Furthermore, we analyze sequences of polynomials that are\ndeterminants of polynomially sized matrices consisting only of zeros, ones, and\nvariables. We show that these are exactly the sequences in the complexity class\nof constant free polynomially sized (weakly) skew circuits. \n\n"}
{"id": "1411.1977", "contents": "Title: Towards an Isomorphism Dichotomy for Hereditary Graph Classes Abstract: In this paper we resolve the complexity of the isomorphism problem on all but\nfinitely many of the graph classes characterized by two forbidden induced\nsubgraphs. To this end we develop new techniques applicable for the structural\nand algorithmic analysis of graphs. First, we develop a methodology to show\nisomorphism completeness of the isomorphism problem on graph classes by\nproviding a general framework unifying various reduction techniques. Second, we\ngeneralize the concept of the modular decomposition to colored graphs, allowing\nfor non-standard decompositions. We show that, given a suitable decomposition\nfunctor, the graph isomorphism problem reduces to checking isomorphism of\ncolored prime graphs. Third, we extend the techniques of bounded color valence\nand hypergraph isomorphism on hypergraphs of bounded color size as follows. We\nsay a colored graph has generalized color valence at most k if, after removing\nall vertices in color classes of size at most k, for each color class C every\nvertex has at most k neighbors in C or at most k non-neighbors in C. We show\nthat isomorphism of graphs of bounded generalized color valence can be solved\nin polynomial time. \n\n"}
{"id": "1411.2647", "contents": "Title: Asynchronous Approximation of a Single Component of the Solution to a\n  Linear System Abstract: We present a distributed asynchronous algorithm for approximating a single\ncomponent of the solution to a system of linear equations $Ax = b$, where $A$\nis a positive definite real matrix, and $b \\in \\mathbb{R}^n$. This is\nequivalent to solving for $x_i$ in $x = Gx + z$ for some $G$ and $z$ such that\nthe spectral radius of $G$ is less than 1. Our algorithm relies on the Neumann\nseries characterization of the component $x_i$, and is based on residual\nupdates. We analyze our algorithm within the context of a cloud computation\nmodel, in which the computation is split into small update tasks performed by\nsmall processors with shared access to a distributed file system. We prove a\nrobust asymptotic convergence result when the spectral radius $\\rho(|G|) < 1$,\nregardless of the precise order and frequency in which the update tasks are\nperformed. We provide convergence rate bounds which depend on the order of\nupdate tasks performed, analyzing both deterministic update rules via counting\nweighted random walks, as well as probabilistic update rules via concentration\nbounds. The probabilistic analysis requires analyzing the product of random\nmatrices which are drawn from distributions that are time and path dependent.\nWe specifically consider the setting where $n$ is large, yet $G$ is sparse,\ne.g., each row has at most $d$ nonzero entries. This is motivated by\napplications in which $G$ is derived from the edge structure of an underlying\ngraph. Our results prove that if the local neighborhood of the graph does not\ngrow too quickly as a function of $n$, our algorithm can provide significant\nreduction in computation cost as opposed to any algorithm which computes the\nglobal solution vector $x$. Our algorithm obtains an $\\epsilon \\|x\\|_2$\nadditive approximation for $x_i$ in constant time with respect to the size of\nthe matrix when the maximum row sparsity $d = O(1)$ and $1/(1-\\|G\\|_2) = O(1)$. \n\n"}
{"id": "1411.2718", "contents": "Title: Variable-Order de Bruijn Graphs Abstract: The de Bruijn graph $G_K$ of a set of strings $S$ is a key data structure in\ngenome assembly that represents overlaps between all the $K$-length substrings\nof $S$. Construction and navigation of the graph is a space and time bottleneck\nin practice and the main hurdle for assembling large, eukaryote genomes. This\nproblem is compounded by the fact that state-of-the-art assemblers do not build\nthe de Bruijn graph for a single order (value of $K$) but for multiple values\nof $K$. More precisely, they build $d$ de Bruijn graphs, each with a specific\norder, i.e., $G_{K_1}, G_{K_2}, ..., G_{K_d}$. Although, this paradigm\nincreases the quality of the assembly produced, it increases the memory by a\nfactor of $d$ in most cases. In this paper, we show how to augment a succinct\nde Bruijn graph representation by Bowe et al. (Proc. WABI, 2012) to support new\noperations that let us change order on the fly, effectively representing all de\nBruijn graphs of order up to some maximum $K$ in a single data structure. Our\nexperiments show our variable-order de Bruijn graph only modestly increases\nspace usage, construction time, and navigation time compared to a single order\ngraph. \n\n"}
{"id": "1411.3530", "contents": "Title: Cheeger constants, structural balance, and spectral clustering analysis\n  for signed graphs Abstract: We introduce a family of multi-way Cheeger-type constants $\\{h_k^{\\sigma},\nk=1,2,\\ldots, n\\}$ on a signed graph $\\Gamma=(G,\\sigma)$ such that\n$h_k^{\\sigma}=0$ if and only if $\\Gamma$ has $k$ balanced connected components.\nThese constants are switching invariant and bring together in a unified\nviewpoint a number of important graph-theoretical concepts, including the\nclassical Cheeger constant, those measures of bipartiteness introduced by\nDesai-Rao, Trevisan, Bauer-Jost, respectively, on unsigned graphs,, and the\nfrustration index (originally called the line index of balance by Harary) on\nsigned graphs. We further unify the (higher-order or improved) Cheeger and dual\nCheeger inequalities for unsigned graphs as well as the underlying algorithmic\nproof techniques by establishing their corresponding versions on signed graphs.\nIn particular, we develop a spectral clustering method for finding $k$\nalmost-balanced subgraphs, each defining a sparse cut. The proper metric for\nsuch a clustering is the metric on a real projective space. We also prove\nestimates of the extremal eigenvalues of signed Laplace matrix in terms of\nnumber of signed triangles ($3$-cycles). \n\n"}
{"id": "1411.4074", "contents": "Title: Improving Monte Carlo randomized approximation schemes Abstract: Consider a central problem in randomized approximation schemes that use a\nMonte Carlo approach. Given a sequence of independent, identically distributed\nrandom variables $X_1,X_2,\\ldots$ with mean $\\mu$ and standard deviation at\nmost $c \\mu$, where $c$ is a known constant, and $\\epsilon,\\delta > 0$, create\nan estimate $\\hat \\mu$ for $\\mu$ such that $\\text{P}(|\\hat \\mu - \\mu| >\n\\epsilon \\mu) \\leq \\delta$. This technique has been used for building\nrandomized approximation schemes for the volume of a convex body, the permanent\nof a nonnegative matrix, the number of linear extensions of a poset, the\npartition function of the Ising model and many other problems. Existing methods\nuse (to the leading order) $19.35 (c/\\epsilon)^2 \\ln(\\delta^{-1})$ samples.\nThis is the best possible number up to the constant factor, and it is an open\nquestion as to what is the best constant possible. This work gives an easy to\napply estimate that only uses $6.96 (c/\\epsilon)^2 \\ln(\\delta^{-1})$ samples in\nthe leading order. \n\n"}
{"id": "1411.6049", "contents": "Title: Quantum Invariants of 3-manifolds and NP vs #P Abstract: The computational complexity class #P captures the difficulty of counting the\nsatisfying assignments to a boolean formula. In this work, we use basic tools\nfrom quantum computation to give a proof that the SO(3)\nWitten-Reshetikhin-Turaev (WRT) invariant of 3-manifolds is #P-hard to\ncalculate. We then apply this result to a question about the combinatorics of\nHeegaard splittings, motivated by analogous work on link diagrams by M.\nFreedman. We show that, if $\\#\\text{P}\\neq\\text{FP}^\\text{NP}$, then there\nexist infinitely many Heegaard splittings which cannot be made logarithmically\nthin by local WRT-preserving moves, except perhaps via a superpolynomial number\nof steps. We also outline two extensions of the above results. First, adapting\na result of Kuperberg, we show that any presentation-independent approximation\nof WRT is also #P-hard. Second, we sketch out how all of our results can be\ntranslated to the setting of triangulations and Turaev-Viro invariants. \n\n"}
{"id": "1412.0325", "contents": "Title: Matchings with lower quotas: Algorithms and complexity Abstract: We study a natural generalization of the maximum weight many-to-one matching\nproblem. We are given an undirected bipartite graph $G= (A \\cup P, E)$ with\nweights on the edges in $E$, and with lower and upper quotas on the vertices in\n$P$. We seek a maximum weight many-to-one matching satisfying two sets of\nconstraints: vertices in $A$ are incident to at most one matching edge, while\nvertices in $P$ are either unmatched or they are incident to a number of\nmatching edges between their lower and upper quota. This problem, which we call\nmaximum weight many-to-one matching with lower and upper quotas (WMLQ), has\napplications to the assignment of students to projects within university\ncourses, where there are constraints on the minimum and maximum numbers of\nstudents that must be assigned to each project.\n  In this paper, we provide a comprehensive analysis of the complexity of WMLQ\nfrom the viewpoints of classic polynomial time algorithms, fixed-parameter\ntractability, as well as approximability. We draw the line between NP-hard and\npolynomially tractable instances in terms of degree and quota constraints and\nprovide efficient algorithms to solve the tractable ones. We further show that\nthe problem can be solved in polynomial time for instances with bounded\ntreewidth; however, the corresponding runtime is exponential in the treewidth\nwith the maximum upper quota $u_{max}$ as basis, and we prove that this\ndependence is necessary unless FPT = W[1]. The approximability of WMLQ is also\ndiscussed: we present an approximation algorithm for the general case with\nperformance guarantee $u_{\\max}+1$, which is asymptotically best possible\nunless P = NP. Finally, we elaborate on how most of our positive results carry\nover to matchings in arbitrary graphs with lower quotas. \n\n"}
{"id": "1412.0969", "contents": "Title: Settling Some Open Problems on 2-Player Symmetric Nash Equilibria Abstract: Over the years, researchers have studied the complexity of several decision\nversions of Nash equilibrium in (symmetric) two-player games (bimatrix games).\nTo the best of our knowledge, the last remaining open problem of this sort is\nthe following; it was stated by Papadimitriou in 2007: find a non-symmetric\nNash equilibrium (NE) in a symmetric game. We show that this problem is\nNP-complete and the problem of counting the number of non-symmetric NE in a\nsymmetric game is #P-complete.\n  In 2005, Kannan and Theobald defined the \"rank of a bimatrix game\"\nrepresented by matrices (A, B) to be rank(A+B) and asked whether a NE can be\ncomputed in rank 1 games in polynomial time. Observe that the rank 0 case is\nprecisely the zero sum case, for which a polynomial time algorithm follows from\nvon Neumann's reduction of such games to linear programming. In 2011, Adsul et.\nal. obtained an algorithm for rank 1 games; however, it does not solve the case\nof symmetric rank 1 games. We resolve this problem. \n\n"}
{"id": "1412.4646", "contents": "Title: Fewer runs than word length Abstract: The work takes another look at the number of runs that a string might contain\nand provides an alternative proof for the bound. We also propose another\nstronger conjecture that states that, for a fixed order on the alphabet, within\nevery factor of a word there are at most as many occurrences of Lyndon roots\ncorresponding to runs in a word as the length of the factor (only first such\noccurrences for each run are considered). \n\n"}
{"id": "1412.6686", "contents": "Title: On the Entity Hardening Problem in Multi-layered Interdependent Networks Abstract: The power grid and the communication network are highly interdependent on\neach other for their well being. In recent times the research community has\nshown significant interest in modeling such interdependent networks and\nstudying the impact of failures on these networks. Although a number of models\nhave been proposed, many of them are simplistic in nature and fail to capture\nthe complex interdependencies that exist between the entities of these\nnetworks. To overcome the limitations, recently an Implicative Interdependency\nModel that utilizes Boolean Logic, was proposed and a number of problems were\nstudied. In this paper we study the entity hardening problem, where by entity\nhardening we imply the ability of the network operator to ensure that an\nadversary (be it Nature or human) cannot take a network entity from operative\nto inoperative state. Given that the network operator with a limited budget can\nonly harden k entities, the goal of the entity hardening problem is to identify\nthe set of k entities whose hardening will ensure maximum benefit for the\noperator, i.e. maximally reduce the ability of the adversary to degrade the\nnetwork. We show that the problem is solvable in polynomial time for some\ncases, whereas for others it is NP-complete. We provide the optimal solution\nusing ILP, and propose a heuristic approach to solve the problem. We evaluate\nthe efficacy of our heuristic using power and communication network data of\nMaricopa County, Arizona. The experiments show that our heuristic almost always\nproduces near optimal results. \n\n"}
{"id": "1501.00011", "contents": "Title: Why now is the right time to study quantum computing Abstract: Quantum computing is a good way to justify difficult physics experiments. But\nuntil quantum computers are built, do computer scientists need to know anything\nabout quantum information? In fact, quantum computing is not merely a recipe\nfor new computing devices, but a new way of looking at the world that has been\nastonishingly intellectually productive. In this article, I'll talk about where\nquantum computing came from, what it is, and what we can learn from it. \n\n"}
{"id": "1501.00033", "contents": "Title: Parallel repetition for entangled k-player games via fast quantum search Abstract: We present two parallel repetition theorems for the entangled value of\nmulti-player, one-round free games (games where the inputs come from a product\ndistribution). Our first theorem shows that for a $k$-player free game $G$ with\nentangled value $\\mathrm{val}^*(G) = 1 - \\epsilon$, the $n$-fold repetition of\n$G$ has entangled value $\\mathrm{val}^*(G^{\\otimes n})$ at most $(1 -\n\\epsilon^{3/2})^{\\Omega(n/sk^4)}$, where $s$ is the answer length of any\nplayer. In contrast, the best known parallel repetition theorem for the\nclassical value of two-player free games is $\\mathrm{val}(G^{\\otimes n}) \\leq\n(1 - \\epsilon^2)^{\\Omega(n/s)}$, due to Barak, et al. (RANDOM 2009). This\nsuggests the possibility of a separation between the behavior of entangled and\nclassical free games under parallel repetition.\n  Our second theorem handles the broader class of free games $G$ where the\nplayers can output (possibly entangled) quantum states. For such games, the\nrepeated entangled value is upper bounded by $(1 -\n\\epsilon^2)^{\\Omega(n/sk^2)}$. We also show that the dependence of the exponent\non $k$ is necessary: we exhibit a $k$-player free game $G$ and $n \\geq 1$ such\nthat $\\mathrm{val}^*(G^{\\otimes n}) \\geq \\mathrm{val}^*(G)^{n/k}$.\n  Our analysis exploits the novel connection between communication protocols\nand quantum parallel repetition, first explored by Chailloux and Scarpa (ICALP\n2014). We demonstrate that better communication protocols yield better parallel\nrepetition theorems: our first theorem crucially uses a quantum search protocol\nby Aaronson and Ambainis, which gives a quadratic speed-up for distributed\nsearch problems. Finally, our results apply to a broader class of games than\nwere previously considered before; in particular, we obtain the first parallel\nrepetition theorem for entangled games involving more than two players, and for\ngames involving quantum outputs. \n\n"}
{"id": "1501.01571", "contents": "Title: An Introduction to Matrix Concentration Inequalities Abstract: In recent years, random matrices have come to play a major role in\ncomputational mathematics, but most of the classical areas of random matrix\ntheory remain the province of experts. Over the last decade, with the advent of\nmatrix concentration inequalities, research has advanced to the point where we\ncan conquer many (formerly) challenging problems with a page or two of\narithmetic. The aim of this monograph is to describe the most successful\nmethods from this area along with some interesting examples that these\ntechniques can illuminate. \n\n"}
{"id": "1501.01910", "contents": "Title: What is Cook's theorem? Abstract: In this paper, we make a preliminary interpretation of Cook's theorem\npresented in [1]. This interpretation reveals cognitive biases in the proof of\nCook's theorem that arise from the attempt of constructing a formula in CNF to\nrepresent a computation of a nondeterministic Turing machine. Such cognitive\nbiases are due to the lack of understanding about the essence of\nnondeterminism, and lead to the confusion between different levels of\nnondeterminism and determinism, thus cause the loss of nondeterminism from the\nNP-completeness theory. The work shows that Cook's theorem is the origin of the\nloss of nondeterminism in terms of the equivalence of the two definitions of\nNP, the one defining NP as the class of problems solvable by a nondeterministic\nTuring machine in polynomial time, and the other defining NP as the class of\nproblems verifiable by a deterministic Turing machine in polynomial time.\nTherefore, we argue that fundamental difficulties in understanding P versus NP\nlie firstly at cognition level, then logic level. \n\n"}
{"id": "1501.02285", "contents": "Title: Interval Selection in the Streaming Model Abstract: A set of intervals is independent when the intervals are pairwise disjoint.\nIn the interval selection problem we are given a set $\\mathbb{I}$ of intervals\nand we want to find an independent subset of intervals of largest cardinality.\nLet $\\alpha(\\mathbb{I})$ denote the cardinality of an optimal solution. We\ndiscuss the estimation of $\\alpha(\\mathbb{I})$ in the streaming model, where we\nonly have one-time, sequential access to the input intervals, the endpoints of\nthe intervals lie in $\\{1,...,n \\}$, and the amount of the memory is\nconstrained.\n  For intervals of different sizes, we provide an algorithm in the data stream\nmodel that computes an estimate $\\hat\\alpha$ of $\\alpha(\\mathbb{I})$ that, with\nprobability at least $2/3$, satisfies $\\tfrac 12(1-\\varepsilon)\n\\alpha(\\mathbb{I}) \\le \\hat\\alpha \\le \\alpha(\\mathbb{I})$. For same-length\nintervals, we provide another algorithm in the data stream model that computes\nan estimate $\\hat\\alpha$ of $\\alpha(\\mathbb{I})$ that, with probability at\nleast $2/3$, satisfies $\\tfrac 23(1-\\varepsilon) \\alpha(\\mathbb{I}) \\le\n\\hat\\alpha \\le \\alpha(\\mathbb{I})$. The space used by our algorithms is bounded\nby a polynomial in $\\varepsilon^{-1}$ and $\\log n$. We also show that no better\nestimations can be achieved using $o(n)$ bits of storage.\n  We also develop new, approximate solutions to the interval selection problem,\nwhere we want to report a feasible solution, that use $O(\\alpha(\\mathbb{I}))$\nspace. Our algorithms for the interval selection problem match the optimal\nresults by Emek, Halld{\\'o}rsson and Ros{\\'e}n [Space-Constrained Interval\nSelection, ICALP 2012], but are much simpler. \n\n"}
{"id": "1501.06626", "contents": "Title: Manipulating the Probabilistic Serial Rule Abstract: The probabilistic serial (PS) rule is one of the most prominent randomized\nrules for the assignment problem. It is well-known for its superior fairness\nand welfare properties. However, PS is not immune to manipulative behaviour by\nthe agents. We initiate the study of the computational complexity of an agent\nmanipulating the PS rule. We show that computing an expected utility better\nresponse is NP- hard. On the other hand, we present a polynomial-time algorithm\nto compute a lexicographic best response. For the case of two agents, we show\nthat even an expected utility best response can be computed in polynomial time.\nOur result for the case of two agents relies on an interesting connection with\nsequential allocation of discrete objects. \n\n"}
{"id": "1501.07053", "contents": "Title: Quadratic-Time Hardness of LCS and other Sequence Similarity Measures Abstract: Two important similarity measures between sequences are the longest common\nsubsequence (LCS) and the dynamic time warping distance (DTWD). The\ncomputations of these measures for two given sequences are central tasks in a\nvariety of applications. Simple dynamic programming algorithms solve these\ntasks in $O(n^2)$ time, and despite an extensive amount of research, no\nalgorithms with significantly better worst case upper bounds are known.\n  In this paper, we show that an $O(n^{2-\\epsilon})$ time algorithm, for some\n$\\epsilon>0$, for computing the LCS or the DTWD of two sequences of length $n$\nover a constant size alphabet, refutes the popular Strong Exponential Time\nHypothesis (SETH). Moreover, we show that computing the LCS of $k$ strings over\nan alphabet of size $O(k)$ cannot be done in $O(n^{k-\\epsilon})$ time, for any\n$\\epsilon>0$, under SETH. Finally, we also address the time complexity of\napproximating the DTWD of two strings in truly subquadratic time. \n\n"}
{"id": "1501.07195", "contents": "Title: The Logic of Counting Query Answers Abstract: We consider the problem of counting the number of answers to a first-order\nformula on a finite structure. We present and study an extension of first-order\nlogic in which algorithms for this counting problem can be naturally and\nconveniently expressed, in senses that are made precise and that are motivated\nby the wish to understand tractable cases of the counting problem. \n\n"}
{"id": "1502.02551", "contents": "Title: Deep Learning with Limited Numerical Precision Abstract: Training of large-scale deep neural networks is often constrained by the\navailable computational resources. We study the effect of limited precision\ndata representation and computation on neural network training. Within the\ncontext of low-precision fixed-point computations, we observe the rounding\nscheme to play a crucial role in determining the network's behavior during\ntraining. Our results show that deep networks can be trained using only 16-bit\nwide fixed-point number representation when using stochastic rounding, and\nincur little to no degradation in the classification accuracy. We also\ndemonstrate an energy-efficient hardware accelerator that implements\nlow-precision fixed-point arithmetic with stochastic rounding. \n\n"}
{"id": "1502.02800", "contents": "Title: Fast integer multiplication using generalized Fermat primes Abstract: For almost 35 years, Sch{\\\"o}nhage-Strassen's algorithm has been the fastest\nalgorithm known for multiplying integers, with a time complexity O(n $\\times$\nlog n $\\times$ log log n) for multiplying n-bit inputs. In 2007, F{\\\"u}rer\nproved that there exists K > 1 and an algorithm performing this operation in\nO(n $\\times$ log n $\\times$ K log n). Recent work by Harvey, van der Hoeven,\nand Lecerf showed that this complexity estimate can be improved in order to get\nK = 8, and conjecturally K = 4. Using an alternative algorithm, which relies on\narithmetic modulo generalized Fermat primes, we obtain conjecturally the same\nresult K = 4 via a careful complexity analysis in the deterministic multitape\nTuring model. \n\n"}
{"id": "1502.05675", "contents": "Title: NP-Hardness and Inapproximability of Sparse PCA Abstract: We give a reduction from {\\sc clique} to establish that sparse PCA is\nNP-hard. The reduction has a gap which we use to exclude an FPTAS for sparse\nPCA (unless P=NP). Under weaker complexity assumptions, we also exclude\npolynomial constant-factor approximation algorithms. \n\n"}
{"id": "1502.05983", "contents": "Title: Sorting Networks: The Final Countdown Abstract: In this paper we extend the knowledge on the problem of empirically searching\nfor sorting networks of minimal depth. We present new search space pruning\ntechniques for the last four levels of a candidate sorting network by\nconsidering only the output set representation of a network. We present an\nalgorithm for checking whether an $n$-input sorting network of depth $d$ exists\nby considering the minimal up to permutation and reflection itemsets at each\nlevel and using the pruning at the last four levels. We experimentally\nevaluated this algorithm to find the optimal depth sorting networks for all $n\n\\leq 12$. \n\n"}
{"id": "1503.00278", "contents": "Title: An Introduction to Temporal Graphs: An Algorithmic Perspective Abstract: A \\emph{temporal graph} is, informally speaking, a graph that changes with\ntime. When time is discrete and only the relationships between the\nparticipating entities may change and not the entities themselves, a temporal\ngraph may be viewed as a sequence $G_1,G_2\\ldots,G_l$ of static graphs over the\nsame (static) set of nodes $V$. Though static graphs have been extensively\nstudied, for their temporal generalization we are still far from having a\nconcrete set of structural and algorithmic principles. Recent research shows\nthat many graph properties and problems become radically different and usually\nsubstantially more difficult when an extra time dimension in added to them.\nMoreover, there is already a rich and rapidly growing set of modern systems and\napplications that can be naturally modeled and studied via temporal graphs.\nThis, further motivates the need for the development of a temporal extension of\ngraph theory. We survey here recent results on temporal graphs and temporal\ngraph problems that have appeared in the Computer Science community. \n\n"}
{"id": "1503.00833", "contents": "Title: The complexity of dominating set reconfiguration Abstract: Suppose that we are given two dominating sets $D_s$ and $D_t$ of a graph $G$\nwhose cardinalities are at most a given threshold $k$. Then, we are asked\nwhether there exists a sequence of dominating sets of $G$ between $D_s$ and\n$D_t$ such that each dominating set in the sequence is of cardinality at most\n$k$ and can be obtained from the previous one by either adding or deleting\nexactly one vertex. This problem is known to be PSPACE-complete in general. In\nthis paper, we study the complexity of this decision problem from the viewpoint\nof graph classes. We first prove that the problem remains PSPACE-complete even\nfor planar graphs, bounded bandwidth graphs, split graphs, and bipartite\ngraphs. We then give a general scheme to construct linear-time algorithms and\nshow that the problem can be solved in linear time for cographs, trees, and\ninterval graphs. Furthermore, for these tractable cases, we can obtain a\ndesired sequence such that the number of additions and deletions is bounded by\n$O(n)$, where $n$ is the number of vertices in the input graph. \n\n"}
{"id": "1503.02835", "contents": "Title: Polynomial-time approximability of the k-Sink Location problem Abstract: A dynamic network ${\\cal N} = (G,c,\\tau,S)$ where $G=(V,E)$ is a graph,\nintegers $\\tau(e)$ and $c(e)$ represent, for each edge $e\\in E$, the time\nrequired to traverse edge $e$ and its nonnegative capacity, and the set\n$S\\subseteq V$ is a set of sources. In the $k$-{\\sc Sink Location} problem, one\nis given as input a dynamic network ${\\cal N}$ where every source $u\\in S$ is\ngiven a nonnegative supply value $\\sigma(u)$. The task is then to find a set of\nsinks $X = \\{x_1,\\ldots,x_k\\}$ in $G$ that minimizes the routing time of all\nsupply to $X$. Note that, in the case where $G$ is an undirected graph, the\noptimal position of the sinks in $X$ needs not be at vertices, and can be\nlocated along edges. Hoppe and Tardos showed that, given an instance of\n$k$-{\\sc Sink Location} and a set of $k$ vertices $X\\subseteq V$, one can find\nan optimal routing scheme of all the supply in $G$ to $X$ in polynomial time,\nin the case where graph $G$ is directed. Note that when $G$ is directed, this\nsuffices to obtain polynomial-time solvability of the $k$-{\\sc Sink Location}\nproblem, since any optimal position will be located at vertices of $G$.\nHowever, the computational complexity of the $k$-{\\sc Sink Location} problem on\ngeneral undirected graphs is still open. In this paper, we show that the\n$k$-{\\sc Sink Location} problem admits a fully polynomial-time approximation\nscheme (FPTAS) for every fixed $k$, and that the problem is $W[1]$-hard when\nparameterized by $k$. \n\n"}
{"id": "1503.05977", "contents": "Title: Dynamic Data Structures for Document Collections and Graphs Abstract: In the dynamic indexing problem, we must maintain a changing collection of\ntext documents so that we can efficiently support insertions, deletions, and\npattern matching queries. We are especially interested in developing efficient\ndata structures that store and query the documents in compressed form. All\nprevious compressed solutions to this problem rely on answering rank and select\nqueries on a dynamic sequence of symbols. Because of the lower bound in\n[Fredman and Saks, 1989], answering rank queries presents a bottleneck in\ncompressed dynamic indexing. In this paper we show how this lower bound can be\ncircumvented using our new framework. We demonstrate that the gap between\nstatic and dynamic variants of the indexing problem can be almost closed. Our\nmethod is based on a novel framework for adding dynamism to static compressed\ndata structures. Our framework also applies more generally to dynamizing other\nproblems. We show, for example, how our framework can be applied to develop\ncompressed representations of dynamic graphs and binary relations. \n\n"}
{"id": "1503.06381", "contents": "Title: Balancing Communication for Multi-party Interactive Coding Abstract: We consider interactive coding in a setting where $n$ parties wish to compute\na joint function of their inputs via an interactive protocol over imperfect\nchannels. We assume that adversarial errors can comprise a\n$\\mathcal{O}(\\frac{1}{n})$ fraction of the total communication, occurring\nanywhere on the communication network. Our goal is to maintain a constant\nmultiplicative overhead in the total communication required, as compared to the\nerror-free setting, and also to balance the workload over the different\nparties. We build upon the prior protocol of Jain, Kalai, and Lewko, but while\nthat protocol relies on a single coordinator to shoulder a heavy burden\nthroughout the protocol, we design a mechanism to pass the coordination duties\nfrom party to party, resulting in a more even distribution of communication\nover the course of the computation. \n\n"}
{"id": "1503.07310", "contents": "Title: The Complexity of Phylogeny Constraint Satisfaction Problems Abstract: We systematically study the computational complexity of a broad class of\ncomputational problems in phylogenetic reconstruction. The class contains for\nexample the rooted triple consistency problem, forbidden subtree problems, the\nquartet consistency problem, and many other problems studied in the\nbioinformatics literature. The studied problems can be described as\n\\emph{constraint satisfaction problems} where the constraints have a\nfirst-order definition over the rooted triple relation. We show that every such\nphylogeny problem can be solved in polynomial time or is NP-complete. On the\nalgorithmic side, we generalize a well-known polynomial-time algorithm of Aho,\nSagiv, Szymanski, and Ullman for the rooted triple consistency problem. Our\nalgorithm repeatedly solves linear equation systems to construct a solution in\npolynomial time. We then show that every phylogeny problem that cannot be\nsolved by our algorithm is NP-complete. Our classification establishes a\ndichotomy for a large class of infinite structures that we believe is of\nindependent interest in universal algebra, model theory, and topology. The\nproof of our main result combines results and techniques from various research\nareas: a recent classification of the model-complete cores of the reducts of\nthe homogeneous binary branching C-relation, Leeb's Ramsey theorem for rooted\ntrees, and universal algebra. \n\n"}
{"id": "1503.08019", "contents": "Title: Optimality of Fast Matching Algorithms for Random Networks with\n  Applications to Structural Controllability Abstract: Network control refers to a very large and diverse set of problems including\ncontrollability of linear time-invariant dynamical systems, where the objective\nis to select an appropriate input to steer the network to a desired state.\nThere are many notions of controllability, one of them being structural\ncontrollability, which is intimately connected to finding maximum matchings on\nthe underlying network topology. In this work, we study fast, scalable\nalgorithms for finding maximum matchings for a large class of random networks.\nFirst, we illustrate that degree distribution random networks are realistic\nmodels for real networks in terms of structural controllability. Subsequently,\nwe analyze a popular, fast and practical heuristic due to Karp and Sipser as\nwell as a simplification of it. For both heuristics, we establish asymptotic\noptimality and provide results concerning the asymptotic size of maximum\nmatchings for an extensive class of random networks. \n\n"}
{"id": "1504.00429", "contents": "Title: Gradual Release of Sensitive Data under Differential Privacy Abstract: We introduce the problem of releasing sensitive data under differential\nprivacy when the privacy level is subject to change over time. Existing work\nassumes that privacy level is determined by the system designer as a fixed\nvalue before sensitive data is released. For certain applications, however,\nusers may wish to relax the privacy level for subsequent releases of the same\ndata after either a re-evaluation of the privacy concerns or the need for\nbetter accuracy. Specifically, given a database containing sensitive data, we\nassume that a response $y_1$ that preserves $\\epsilon_{1}$-differential privacy\nhas already been published. Then, the privacy level is relaxed to $\\epsilon_2$,\nwith $\\epsilon_2 > \\epsilon_1$, and we wish to publish a more accurate response\n$y_2$ while the joint response $(y_1, y_2)$ preserves $\\epsilon_2$-differential\nprivacy. How much accuracy is lost in the scenario of gradually releasing two\nresponses $y_1$ and $y_2$ compared to the scenario of releasing a single\nresponse that is $\\epsilon_{2}$-differentially private? Our results show that\nthere exists a composite mechanism that achieves \\textit{no loss} in accuracy.\nWe consider the case in which the private data lies within $\\mathbb{R}^{n}$\nwith an adjacency relation induced by the $\\ell_{1}$-norm, and we focus on\nmechanisms that approximate identity queries. We show that the same accuracy\ncan be achieved in the case of gradual release through a mechanism whose\noutputs can be described by a \\textit{lazy Markov stochastic process}. This\nstochastic process has a closed form expression and can be efficiently sampled.\nOur results are applicable beyond identity queries. To this end, we demonstrate\nthat our results can be applied in several cases, including Google's RAPPOR\nproject, trading of sensitive data, and controlled transmission of private data\nin a social network. \n\n"}
{"id": "1504.01130", "contents": "Title: Proving the Herman-Protocol Conjecture Abstract: Herman's self-stabilisation algorithm, introduced 25 years ago, is a\nwell-studied synchronous randomised protocol for enabling a ring of $N$\nprocesses collectively holding any odd number of tokens to reach a stable state\nin which a single token remains. Determining the worst-case expected time to\nstabilisation is the central outstanding open problem about this protocol. It\nis known that there is a constant $h$ such that any initial configuration has\nexpected stabilisation time at most $h N^2$. Ten years ago, McIver and Morgan\nestablished a lower bound of $4/27 \\approx 0.148$ for $h$, achieved with three\nequally-spaced tokens, and conjectured this to be the optimal value of $h$. A\nseries of papers over the last decade gradually reduced the upper bound on $h$,\nwith the present record (achieved in 2014) standing at approximately $0.156$.\nIn this paper, we prove McIver and Morgan's conjecture and establish that $h =\n4/27$ is indeed optimal. \n\n"}
{"id": "1504.01431", "contents": "Title: If the Current Clique Algorithms are Optimal, so is Valiant's Parser Abstract: The CFG recognition problem is: given a context-free grammar $\\mathcal{G}$\nand a string $w$ of length $n$, decide if $w$ can be obtained from\n$\\mathcal{G}$. This is the most basic parsing question and is a core computer\nscience problem. Valiant's parser from 1975 solves the problem in\n$O(n^{\\omega})$ time, where $\\omega<2.373$ is the matrix multiplication\nexponent. Dozens of parsing algorithms have been proposed over the years, yet\nValiant's upper bound remains unbeaten. The best combinatorial algorithms have\nmildly subcubic $O(n^3/\\log^3{n})$ complexity.\n  Lee (JACM'01) provided evidence that fast matrix multiplication is needed for\nCFG parsing, and that very efficient and practical algorithms might be hard or\neven impossible to obtain. Lee showed that any algorithm for a more general\nparsing problem with running time $O(|\\mathcal{G}|\\cdot n^{3-\\varepsilon})$ can\nbe converted into a surprising subcubic algorithm for Boolean Matrix\nMultiplication. Unfortunately, Lee's hardness result required that the grammar\nsize be $|\\mathcal{G}|=\\Omega(n^6)$. Nothing was known for the more relevant\ncase of constant size grammars.\n  In this work, we prove that any improvement on Valiant's algorithm, even for\nconstant size grammars, either in terms of runtime or by avoiding the\ninefficiencies of fast matrix multiplication, would imply a breakthrough\nalgorithm for the $k$-Clique problem: given a graph on $n$ nodes, decide if\nthere are $k$ that form a clique.\n  Besides classifying the complexity of a fundamental problem, our reduction\nhas led us to similar lower bounds for more modern and well-studied cubic time\nproblems for which faster algorithms are highly desirable in practice: RNA\nFolding, a central problem in computational biology, and Dyck Language Edit\nDistance, answering an open question of Saha (FOCS'14). \n\n"}
{"id": "1504.01459", "contents": "Title: A Complete Worst-Case Analysis of Heapsort with Experimental\n  Verification of Its Results, A manuscript (MS) Abstract: A rigorous proof is presented that the number of comparisons of keys\nperformed in the worst case by ${\\tt Heapsort}$ on any array of size $N \\geq 2$\nis equal to: $ 2 (N-1)\\, ( \\, \\lg \\frac{N-1}{2} +\\varepsilon \\, ) - 2s_2(N) -\ne_2(N) + \\min (\\lfloor \\lg (N-1) \\rfloor, 2) + 6 + c, $ where $ \\varepsilon $,\ngiven by: $\\varepsilon = 1 + \\lceil \\lg \\, (N-1) \\rceil - \\lg \\, (N-1) -\n2^{\\lceil \\lg \\, (N-1) \\rceil - \\lg \\, (N-1)} ,$ is a function of $ N $ with\nthe minimum value 0 and and the supremum value $\\delta = 1 - \\lg e + \\lg \\lg e\n\\approx 0.0860713320559342$, $s_2(N)$ is the sum of all digits of the binary\nrepresentation of $N$, $e_2(N)$ is the exponent of $2$ in the prime\nfactorization of $N$, and $ c $ is a binary function on the set of integers\ndefined by: $c = 1$, if $N \\leq 2 ^{\\lceil \\lg N \\rceil} - 4$, and $c = 0$,\notherwise. An algorithm that generates worst-case input arrays of any size $ N\n\\geq 2 $ for ${\\tt Heapsort}$ is offered. The algorithm has been implemented in\nJava, runs in $O( N \\log N )$ time, and allows for precise experimental\nverification of the above formula. \n\n"}
{"id": "1504.02146", "contents": "Title: Discrete Stochastic Submodular Maximization: Adaptive vs. Non-Adaptive\n  vs. Offline Abstract: We consider the problem of stochastic monotone submodular function\nmaximization, subject to constraints. We give results on adaptivity gaps, and\non the gap between the optimal offline and online solutions. We present a\nprocedure that transforms a decision tree (adaptive algorithm) into a\nnon-adaptive chain. We prove that this chain achieves at least ${\\tau}$ times\nthe utility of the decision tree, over a product distribution and binary state\nspace, where ${\\tau} = \\min_{i,j} \\Pr[x_i=j]$. This proves an adaptivity gap of\n$1/{\\tau}$ (which is $2$ in the case of a uniform distribution) for the problem\nof stochastic monotone submodular maximization subject to state-independent\nconstraints. For a cardinality constraint, we prove that a simple adaptive\ngreedy algorithm achieves an approximation factor of $(1-1/e^{\\tau})$ with\nrespect to the optimal offline solution; previously, it has been proven that\nthe algorithm achieves an approximation factor of $(1-1/e)$ with respect to the\noptimal adaptive online solution. Finally, we show that there exists a\nnon-adaptive solution for the stochastic max coverage problem that is within a\nfactor $(1-1/e)$ of the optimal adaptive solution and within a factor of\n${\\tau}(1-1/e)$ of the optimal offline solution. \n\n"}
{"id": "1504.03391", "contents": "Title: Tight Bounds on Low-degree Spectral Concentration of Submodular and XOS\n  functions Abstract: Submodular and fractionally subadditive (or equivalently XOS) functions play\na fundamental role in combinatorial optimization, algorithmic game theory and\nmachine learning. Motivated by learnability of these classes of functions from\nrandom examples, we consider the question of how well such functions can be\napproximated by low-degree polynomials in $\\ell_2$ norm over the uniform\ndistribution. This question is equivalent to understanding of the concentration\nof Fourier weight on low-degree coefficients, a central concept in Fourier\nanalysis. We show that\n  1. For any submodular function $f:\\{0,1\\}^n \\rightarrow [0,1]$, there is a\npolynomial of degree $O(\\log (1/\\epsilon) / \\epsilon^{4/5})$ approximating $f$\nwithin $\\epsilon$ in $\\ell_2$, and there is a submodular function that requires\ndegree $\\Omega(1/\\epsilon^{4/5})$.\n  2. For any XOS function $f:\\{0,1\\}^n \\rightarrow [0,1]$, there is a\npolynomial of degree $O(1/\\epsilon)$ and there exists an XOS function that\nrequires degree $\\Omega(1/\\epsilon)$.\n  This improves on previous approaches that all showed an upper bound of\n$O(1/\\epsilon^2)$ for submodular and XOS functions. The best previous lower\nbound was $\\Omega(1/\\epsilon^{2/3})$ for monotone submodular functions. Our\ntechniques reveal new structural properties of submodular and XOS functions and\nthe upper bounds lead to nearly optimal PAC learning algorithms for these\nclasses of functions. \n\n"}
{"id": "1504.03856", "contents": "Title: Sparse multivariate polynomial interpolation in the basis of Schubert\n  polynomials Abstract: Schubert polynomials were discovered by A. Lascoux and M. Sch\\\"utzenberger in\nthe study of cohomology rings of flag manifolds in 1980's. These polynomials\ngeneralize Schur polynomials, and form a linear basis of multivariate\npolynomials. In 2003, Lenart and Sottile introduced skew Schubert polynomials,\nwhich generalize skew Schur polynomials, and expand in the Schubert basis with\nthe generalized Littlewood-Richardson coefficients.\n  In this paper we initiate the study of these two families of polynomials from\nthe perspective of computational complexity theory. We first observe that skew\nSchubert polynomials, and therefore Schubert polynomials, are in $\\CountP$\n(when evaluating on non-negative integral inputs) and $\\VNP$.\n  Our main result is a deterministic algorithm that computes the expansion of a\npolynomial $f$ of degree $d$ in $\\Z[x_1, \\dots, x_n]$ in the basis of Schubert\npolynomials, assuming an oracle computing Schubert polynomials. This algorithm\nruns in time polynomial in $n$, $d$, and the bit size of the expansion. This\ngeneralizes, and derandomizes, the sparse interpolation algorithm of symmetric\npolynomials in the Schur basis by Barvinok and Fomin (Advances in Applied\nMathematics, 18(3):271--285). In fact, our interpolation algorithm is general\nenough to accommodate any linear basis satisfying certain natural properties.\n  Applications of the above results include a new algorithm that computes the\ngeneralized Littlewood-Richardson coefficients. \n\n"}
{"id": "1504.04650", "contents": "Title: A Faster FPTAS for the Unbounded Knapsack Problem Abstract: The Unbounded Knapsack Problem (UKP) is a well-known variant of the famous\n0-1 Knapsack Problem (0-1 KP). In contrast to 0-1 KP, an arbitrary number of\ncopies of every item can be taken in UKP. Since UKP is NP-hard, fully\npolynomial time approximation schemes (FPTAS) are of great interest. Such\nalgorithms find a solution arbitrarily close to the optimum $\\mathrm{OPT}(I)$,\ni.e. of value at least $(1-\\varepsilon) \\mathrm{OPT}(I)$ for $\\varepsilon > 0$,\nand have a running time polynomial in the input length and\n$\\frac{1}{\\varepsilon}$. For over thirty years, the best FPTAS was due to\nLawler with a running time in $O(n + \\frac{1}{\\varepsilon^3})$ and a space\ncomplexity in $O(n + \\frac{1}{\\varepsilon^2})$, where $n$ is the number of\nknapsack items. We present an improved FPTAS with a running time in $O(n +\n\\frac{1}{\\varepsilon^2} \\log^3 \\frac{1}{\\varepsilon})$ and a space bound in\n$O(n + \\frac{1}{\\varepsilon} \\log^2 \\frac{1}{\\varepsilon})$. This directly\nimproves the running time of the fastest known approximation schemes for Bin\nPacking and Strip Packing, which have to approximately solve UKP instances as\nsubproblems. \n\n"}
{"id": "1504.06135", "contents": "Title: Complexity of Propositional Logics in Team Semantics Abstract: We classify the computational complexity of the satisfiability, validity and\nmodel-checking problems for propositional independence, inclusion, and team\nlogic. Our main result shows that the satisfiability and validity problems for\npropositional team logic are complete for alternating exponential-time with\npolynomially many alternations. \n\n"}
{"id": "1504.07697", "contents": "Title: Polynomial Factorization over Finite Fields By Computing Euler-Poincare\n  Characteristics of Drinfeld Modules Abstract: We propose and rigorously analyze two randomized algorithms to factor\nunivariate polynomials over finite fields using rank $2$ Drinfeld modules. The\nfirst algorithm estimates the degree of an irreducible factor of a polynomial\nfrom Euler-Poincare characteristics of random Drinfeld modules. Knowledge of a\nfactor degree allows one to rapidly extract all factors of that degree. As a\nconsequence, the problem of factoring polynomials over finite fields in time\nnearly linear in the degree is reduced to finding Euler-Poincare\ncharacteristics of random Drinfeld modules with high probability. Notably, the\nworst case complexity of polynomial factorization over finite fields is reduced\nto the average case complexity of a problem concerning Drinfeld modules. The\nsecond algorithm is a random Drinfeld module analogue of Berlekamp's algorithm.\nDuring the course of its analysis, we prove a new bound on degree distributions\nin factorization patterns of polynomials over finite fields in certain short\nintervals. \n\n"}
{"id": "1504.07999", "contents": "Title: Average-case complexity versus approximate simulation of commuting\n  quantum computations Abstract: We use the class of commuting quantum computations known as IQP\n(Instantaneous Quantum Polynomial time) to strengthen the conjecture that\nquantum computers are hard to simulate classically. We show that, if either of\ntwo plausible average-case hardness conjectures holds, then IQP computations\nare hard to simulate classically up to constant additive error. One conjecture\nrelates to the hardness of estimating the complex-temperature partition\nfunction for random instances of the Ising model; the other concerns\napproximating the number of zeroes of random low-degree polynomials. We observe\nthat both conjectures can be shown to be valid in the setting of worst-case\ncomplexity. We arrive at these conjectures by deriving spin-based\ngeneralisations of the Boson Sampling problem that avoid the so-called\npermanent anticoncentration conjecture. \n\n"}
{"id": "1505.00477", "contents": "Title: Kernel Spectral Clustering and applications Abstract: In this chapter we review the main literature related to kernel spectral\nclustering (KSC), an approach to clustering cast within a kernel-based\noptimization setting. KSC represents a least-squares support vector machine\nbased formulation of spectral clustering described by a weighted kernel PCA\nobjective. Just as in the classifier case, the binary clustering model is\nexpressed by a hyperplane in a high dimensional space induced by a kernel. In\naddition, the multi-way clustering can be obtained by combining a set of binary\ndecision functions via an Error Correcting Output Codes (ECOC) encoding scheme.\nBecause of its model-based nature, the KSC method encompasses three main steps:\ntraining, validation, testing. In the validation stage model selection is\nperformed to obtain tuning parameters, like the number of clusters present in\nthe data. This is a major advantage compared to classical spectral clustering\nwhere the determination of the clustering parameters is unclear and relies on\nheuristics. Once a KSC model is trained on a small subset of the entire data,\nit is able to generalize well to unseen test points. Beyond the basic\nformulation, sparse KSC algorithms based on the Incomplete Cholesky\nDecomposition (ICD) and $L_0$, $L_1, L_0 + L_1$, Group Lasso regularization are\nreviewed. In that respect, we show how it is possible to handle large scale\ndata. Also, two possible ways to perform hierarchical clustering and a soft\nclustering method are presented. Finally, real-world applications such as image\nsegmentation, power load time-series clustering, document clustering and big\ndata learning are considered. \n\n"}
{"id": "1505.00662", "contents": "Title: Optimal Learning via the Fourier Transform for Sums of Independent\n  Integer Random Variables Abstract: We study the structure and learnability of sums of independent integer random\nvariables (SIIRVs). For $k \\in \\mathbb{Z}_{+}$, a $k$-SIIRV of order $n \\in\n\\mathbb{Z}_{+}$ is the probability distribution of the sum of $n$ independent\nrandom variables each supported on $\\{0, 1, \\dots, k-1\\}$. We denote by ${\\cal\nS}_{n,k}$ the set of all $k$-SIIRVs of order $n$.\n  In this paper, we tightly characterize the sample and computational\ncomplexity of learning $k$-SIIRVs. More precisely, we design a computationally\nefficient algorithm that uses $\\widetilde{O}(k/\\epsilon^2)$ samples, and learns\nan arbitrary $k$-SIIRV within error $\\epsilon,$ in total variation distance.\nMoreover, we show that the {\\em optimal} sample complexity of this learning\nproblem is $\\Theta((k/\\epsilon^2)\\sqrt{\\log(1/\\epsilon)}).$ Our algorithm\nproceeds by learning the Fourier transform of the target $k$-SIIRV in its\neffective support. Its correctness relies on the {\\em approximate sparsity} of\nthe Fourier transform of $k$-SIIRVs -- a structural property that we establish,\nroughly stating that the Fourier transform of $k$-SIIRVs has small magnitude\noutside a small set.\n  Along the way we prove several new structural results about $k$-SIIRVs. As\none of our main structural contributions, we give an efficient algorithm to\nconstruct a sparse {\\em proper} $\\epsilon$-cover for ${\\cal S}_{n,k},$ in total\nvariation distance. We also obtain a novel geometric characterization of the\nspace of $k$-SIIRVs. Our characterization allows us to prove a tight lower\nbound on the size of $\\epsilon$-covers for ${\\cal S}_{n,k}$, and is the key\ningredient in our tight sample complexity lower bound.\n  Our approach of exploiting the sparsity of the Fourier transform in\ndistribution learning is general, and has recently found additional\napplications. \n\n"}
{"id": "1505.00709", "contents": "Title: Parameterized Approximation Algorithms for Packing Problems Abstract: In the past decade, many parameterized algorithms were developed for packing\nproblems. Our goal is to obtain tradeoffs that improve the running times of\nthese algorithms at the cost of computing approximate solutions. Consider a\npacking problem for which there is no known algorithm with approximation ratio\n$\\alpha$, and a parameter $k$. If the value of an optimal solution is at least\n$k$, we seek a solution of value at least $\\alpha k$; otherwise, we seek an\narbitrary solution. Clearly, if the best known parameterized algorithm that\nfinds a solution of value $t$ runs in time $O^*(f(t))$ for some function $f$,\nwe are interested in running times better than $O^*(f(\\alpha k))$. We present\ntradeoffs between running times and approximation ratios for the $P_2$-Packing,\n$3$-Set $k$-Packing and $3$-Dimensional $k$-Matching problems. Our tradeoffs\nare based on combinations of several known results, as well as a computation of\n\"approximate lopsided universal sets.\" \n\n"}
{"id": "1505.02205", "contents": "Title: A lower bound for the determinantal complexity of a hypersurface Abstract: We prove that the determinantal complexity of a hypersurface of degree $d >\n2$ is bounded below by one more than the codimension of the singular locus,\nprovided that this codimension is at least $5$. As a result, we obtain that the\ndeterminantal complexity of the $3 \\times 3$ permanent is $7$. We also prove\nthat for $n> 3$, there is no nonsingular hypersurface in $\\mathbf{P}^n$ of\ndegree $d$ that has an expression as a determinant of a $d \\times d$ matrix of\nlinear forms while on the other hand for $n \\le 3$, a general determinantal\nexpression is nonsingular. Finally, we answer a question of Ressayre by showing\nthat the determinantal complexity of the unique (singular) cubic surface\ncontaining a single line is $5$. \n\n"}
{"id": "1505.02371", "contents": "Title: Computing maximal autarkies with few and simple oracle queries Abstract: We consider the algorithmic task of computing a maximal autarky for a\nclause-set F, i.e., a partial assignment which satisfies every clause of F it\ntouches, and where this property is destroyed by adding any non-empty set of\nfurther assignments. We employ SAT solvers as oracles, using various\ncapabilities. Using the standard SAT oracle, log_2(n(F)) oracle calls suffice,\nwhere n(F) is the number of variables, but the drawback is that (translated)\ncardinality constraints are employed, which makes this approach less efficient\nin practice. Using an extended SAT oracle, motivated by the capabilities of\nmodern SAT solvers, we show how to compute maximal autarkies with 2 n(F)^{1/2}\nsimpler oracle calls. This novel algorithm combines the previous two main\napproaches, based on the autarky-resolution duality and on SAT translations. \n\n"}
{"id": "1505.02372", "contents": "Title: On Asymptotic Gate Complexity and Depth of Reversible Circuits With\n  Additional Memory Abstract: The reversible logic can be used in various research areas, e.g. quantum\ncomputation, cryptography and signal processing. In the paper we study\nreversible logic circuits with additional inputs, which consist of NOT, CNOT\nand C\\textsuperscript{2}NOT gates. We consider a set $F(n,q)$ of all\ntransformations $\\mathbb B^n \\to \\mathbb B^n$ that can be realized by\nreversible circuits with $(n+q)$ inputs. An analogue of Lupanov's method for\nthe synthesis of reversible logic circuits with additional inputs is described.\nWe prove upper asymptotic bounds for the Shannon gate complexity function\n$L(n,q)$ and the depth function $D(n,q)$ in case of $q > 0$: $L(n,q_0) \\lesssim\n2^n$ if $q_0 \\sim n 2^{n-o(n)}$ and $D(n,q_1) \\lesssim 3n$ if $q_1 \\sim 2^n$. \n\n"}
{"id": "1505.02462", "contents": "Title: Soft-Deep Boltzmann Machines Abstract: We present a layered Boltzmann machine (BM) that can better exploit the\nadvantages of a distributed representation. It is widely believed that deep BMs\n(DBMs) have far greater representational power than its shallow counterpart,\nrestricted Boltzmann machines (RBMs). However, this expectation on the\nsupremacy of DBMs over RBMs has not ever been validated in a theoretical\nfashion. In this paper, we provide both theoretical and empirical evidences\nthat the representational power of DBMs can be actually rather limited in\ntaking advantages of distributed representations. We propose an approximate\nmeasure for the representational power of a BM regarding to the efficiency of a\ndistributed representation. With this measure, we show a surprising fact that\nDBMs can make inefficient use of distributed representations. Based on these\nobservations, we propose an alternative BM architecture, which we dub soft-deep\nBMs (sDBMs). We show that sDBMs can more efficiently exploit the distributed\nrepresentations in terms of the measure. Experiments demonstrate that sDBMs\noutperform several state-of-the-art models, including DBMs, in generative tasks\non binarized MNIST and Caltech-101 silhouettes. \n\n"}
{"id": "1505.03737", "contents": "Title: Isomorphism Testing for Graphs of Bounded Rank Width Abstract: We give an algorithm that, for every fixed k, decides isomorphism of graphs\nof rank width at most k in polynomial time. As the clique width of a graph is\nbounded in terms of its rank width, we also obtain a polynomial time\nisomorphism test for graph classes of bounded clique width. \n\n"}
{"id": "1505.04198", "contents": "Title: Greedy Matching: Guarantees and Limitations Abstract: Since Tinhofer proposed the MinGreedy algorithm for maximum cardinality\nmatching in 1984, several experimental studies found the randomized algorithm\nto perform excellently for various classes of random graphs and benchmark\ninstances. In contrast, only few analytical results are known. We show that\nMinGreedy cannot improve on the trivial approximation ratio 1/2 whp., even for\nbipartite graphs. Our hard inputs seem to require a small number of high-degree\nnodes.\n  This motivates an investigation of greedy algorithms on graphs with maximum\ndegree D: We show that MinGreedy achieves a (D-1)/(2D-3)-approximation for\ngraphs with D=3 and for D-regular graphs, and a guarantee of (D-1/2)/(2D-2) for\ngraphs with maximum degree D. Interestingly, our bounds even hold for the\ndeterministic MinGreedy that breaks all ties arbitrarily.\n  Moreover, we investigate the limitations of the greedy paradigm, using the\nmodel of priority algorithms introduced by Borodin, Nielsen, and Rackoff. We\nstudy deterministic priority algorithms and prove a\n(D-1)/(2D-3)-inapproximability result for graphs with maximum degree D; thus,\nthese greedy algorithms do not achieve a 1/2+eps-approximation and in\nparticular the 2/3-approximation obtained by the deterministic MinGreedy for\nD=3 is optimal in this class. For k-uniform hypergraphs we show a tight\n1/k-inapproximability bound. We also study fully randomized priority algorithms\nand give a 5/6-inapproximability bound. Thus, they cannot compete with matching\nalgorithms of other paradigms. \n\n"}
{"id": "1506.01972", "contents": "Title: Improved SVRG for Non-Strongly-Convex or Sum-of-Non-Convex Objectives Abstract: Many classical algorithms are found until several years later to outlive the\nconfines in which they were conceived, and continue to be relevant in\nunforeseen settings. In this paper, we show that SVRG is one such method: being\noriginally designed for strongly convex objectives, it is also very robust in\nnon-strongly convex or sum-of-non-convex settings.\n  More precisely, we provide new analysis to improve the state-of-the-art\nrunning times in both settings by either applying SVRG or its novel variant.\nSince non-strongly convex objectives include important examples such as Lasso\nor logistic regression, and sum-of-non-convex objectives include famous\nexamples such as stochastic PCA and is even believed to be related to training\ndeep neural nets, our results also imply better performances in these\napplications. \n\n"}
{"id": "1506.02903", "contents": "Title: Mixing Time Estimation in Reversible Markov Chains from a Single Sample\n  Path Abstract: This article provides the first procedure for computing a fully\ndata-dependent interval that traps the mixing time $t_{\\text{mix}}$ of a finite\nreversible ergodic Markov chain at a prescribed confidence level. The interval\nis computed from a single finite-length sample path from the Markov chain, and\ndoes not require the knowledge of any parameters of the chain. This stands in\ncontrast to previous approaches, which either only provide point estimates, or\nrequire a reset mechanism, or additional prior knowledge. The interval is\nconstructed around the relaxation time $t_{\\text{relax}}$, which is strongly\nrelated to the mixing time, and the width of the interval converges to zero\nroughly at a $\\sqrt{n}$ rate, where $n$ is the length of the sample path. Upper\nand lower bounds are given on the number of samples required to achieve\nconstant-factor multiplicative accuracy. The lower bounds indicate that, unless\nfurther restrictions are placed on the chain, no procedure can achieve this\naccuracy level before seeing each state at least $\\Omega(t_{\\text{relax}})$\ntimes on the average. Finally, future directions of research are identified. \n\n"}
{"id": "1506.06272", "contents": "Title: Aligning where to see and what to tell: image caption with region-based\n  attention and scene factorization Abstract: Recent progress on automatic generation of image captions has shown that it\nis possible to describe the most salient information conveyed by images with\naccurate and meaningful sentences. In this paper, we propose an image caption\nsystem that exploits the parallel structures between images and sentences. In\nour model, the process of generating the next word, given the previously\ngenerated ones, is aligned with the visual perception experience where the\nattention shifting among the visual regions imposes a thread of visual\nordering. This alignment characterizes the flow of \"abstract meaning\", encoding\nwhat is semantically shared by both the visual scene and the text description.\nOur system also makes another novel modeling contribution by introducing\nscene-specific contexts that capture higher-level semantic information encoded\nin an image. The contexts adapt language models for word generation to specific\nscene types. We benchmark our system and contrast to published results on\nseveral popular datasets. We show that using either region-based attention or\nscene-specific contexts improves systems without those components. Furthermore,\ncombining these two modeling ingredients attains the state-of-the-art\nperformance. \n\n"}
{"id": "1506.06737", "contents": "Title: Spectral Thresholds in the Bipartite Stochastic Block Model Abstract: We consider a bipartite stochastic block model on vertex sets $V_1$ and\n$V_2$, with planted partitions in each, and ask at what densities efficient\nalgorithms can recover the partition of the smaller vertex set.\n  When $|V_2| \\gg |V_1|$, multiple thresholds emerge. We first locate a sharp\nthreshold for detection of the partition, in the sense of the results of\n\\cite{mossel2012stochastic,mossel2013proof} and \\cite{massoulie2014community}\nfor the stochastic block model. We then show that at a higher edge density, the\nsingular vectors of the rectangular biadjacency matrix exhibit a localization /\ndelocalization phase transition, giving recovery above the threshold and no\nrecovery below. Nevertheless, we propose a simple spectral algorithm, Diagonal\nDeletion SVD, which recovers the partition at a nearly optimal edge density.\n  The bipartite stochastic block model studied here was used by\n\\cite{feldman2014algorithm} to give a unified algorithm for recovering planted\npartitions and assignments in random hypergraphs and random $k$-SAT formulae\nrespectively. Our results give the best known bounds for the clause density at\nwhich solutions can be found efficiently in these models as well as showing a\nbarrier to further improvement via this reduction to the bipartite block model. \n\n"}
{"id": "1506.07002", "contents": "Title: Parallel repetition and concentration for (sub-)no-signalling games via\n  a flexible constrained de Finetti reduction Abstract: We use a recently discovered constrained de Finetti reduction (aka\n\"Post-Selection Lemma\") to study the parallel repetition of multi-player\nnon-local games under no-signalling strategies. Since the technique allows us\nto reduce general strategies to independent plays, we obtain parallel\nrepetition (corresponding to winning all rounds) in the same way as exponential\nconcentration of the probability to win a fraction larger than the value of the\ngame.\n  Our proof technique leads us naturally to a relaxation of no-signalling (NS)\nstrategies, which we dub sub-no-signalling (SNOS). While for two players the\ntwo concepts coincide, they differ for three or more players. Our results are\nmost complete and satisfying for arbitrary number of sub-no-signalling players,\nwhere we get universal parallel repetition and concentration for any game,\nwhile the no-signalling case is obtained as a corollary, but only for games\nwith \"full support\". \n\n"}
{"id": "1506.07216", "contents": "Title: Communication Lower Bounds for Statistical Estimation Problems via a\n  Distributed Data Processing Inequality Abstract: We study the tradeoff between the statistical error and communication cost of\ndistributed statistical estimation problems in high dimensions. In the\ndistributed sparse Gaussian mean estimation problem, each of the $m$ machines\nreceives $n$ data points from a $d$-dimensional Gaussian distribution with\nunknown mean $\\theta$ which is promised to be $k$-sparse. The machines\ncommunicate by message passing and aim to estimate the mean $\\theta$. We\nprovide a tight (up to logarithmic factors) tradeoff between the estimation\nerror and the number of bits communicated between the machines. This directly\nleads to a lower bound for the distributed \\textit{sparse linear regression}\nproblem: to achieve the statistical minimax error, the total communication is\nat least $\\Omega(\\min\\{n,d\\}m)$, where $n$ is the number of observations that\neach machine receives and $d$ is the ambient dimension. These lower results\nimprove upon [Sha14,SD'14] by allowing multi-round iterative communication\nmodel. We also give the first optimal simultaneous protocol in the dense case\nfor mean estimation.\n  As our main technique, we prove a \\textit{distributed data processing\ninequality}, as a generalization of usual data processing inequalities, which\nmight be of independent interest and useful for other problems. \n\n"}
{"id": "1506.07329", "contents": "Title: Polyhedral aspects of Submodularity, Convexity and Concavity Abstract: Seminal work by Edmonds and Lovasz shows the strong connection between\nsubmodularity and convexity. Submodular functions have tight modular lower\nbounds, and subdifferentials in a manner akin to convex functions. They also\nadmit poly-time algorithms for minimization and satisfy the Fenchel duality\ntheorem and the Discrete Seperation Theorem, both of which are fundamental\ncharacteristics of convex functions. Submodular functions also show signs\nsimilar to concavity. Submodular maximization, though NP hard, admits constant\nfactor approximation guarantees. Concave functions composed with modular\nfunctions are submodular, and they also satisfy diminishing returns property.\nThis manuscript provides a more complete picture on the relationship between\nsubmodularity with convexity and concavity, by extending many of the results\nconnecting submodularity with convexity to the concave aspects of\nsubmodularity. We first show the existence of superdifferentials, and\nefficiently computable tight modular upper bounds of a submodular function.\nWhile we show that it is hard to characterize this polyhedron, we obtain inner\nand outer bounds on the superdifferential along with certain specific and\nuseful supergradients. We then investigate forms of concave extensions of\nsubmodular functions and show interesting relationships to submodular\nmaximization. We next show connections between optimality conditions over the\nsuperdifferentials and submodular maximization, and show how forms of\napproximate optimality conditions translate into approximation factors for\nmaximization. We end this paper by studying versions of the discrete seperation\ntheorem and the Fenchel duality theorem when seen from the concave point of\nview. In every case, we relate our results to the existing results from the\nconvex point of view, thereby improving the analysis of the relationship\nbetween submodularity, convexity, and concavity. \n\n"}
{"id": "1506.08105", "contents": "Title: Modelling of directional data using Kent distributions Abstract: The modelling of data on a spherical surface requires the consideration of\ndirectional probability distributions. To model asymmetrically distributed data\non a three-dimensional sphere, Kent distributions are often used. The moment\nestimates of the parameters are typically used in modelling tasks involving\nKent distributions. However, these lack a rigorous statistical treatment. The\nfocus of the paper is to introduce a Bayesian estimation of the parameters of\nthe Kent distribution which has not been carried out in the literature, partly\nbecause of its complex mathematical form. We employ the Bayesian\ninformation-theoretic paradigm of Minimum Message Length (MML) to bridge this\ngap and derive reliable estimators. The inferred parameters are subsequently\nused in mixture modelling of Kent distributions. The problem of inferring the\nsuitable number of mixture components is also addressed using the MML\ncriterion. We demonstrate the superior performance of the derived MML-based\nparameter estimates against the traditional estimators. We apply the MML\nprinciple to infer mixtures of Kent distributions to model empirical data\ncorresponding to protein conformations. We demonstrate the effectiveness of\nKent models to act as improved descriptors of protein structural data as\ncompared to commonly used von Mises-Fisher distributions. \n\n"}
{"id": "1506.08752", "contents": "Title: On Tightly Bounding the Dubins Traveling Salesman's Optimum Abstract: The Dubins Traveling Salesman Problem (DTSP) has generated significant\ninterest over the last decade due to its occurrence in several civil and\nmilitary surveillance applications. Currently, there is no algorithm that can\nfind an optimal solution to the problem. In addition, relaxing the motion\nconstraints and solving the resulting Euclidean TSP (ETSP) provides the only\nlower bound available for the problem. However, in many problem instances, the\nlower bound computed by solving the ETSP is far below the cost of the feasible\nsolutions obtained by some well-known algorithms for the DTSP. This article\naddresses this fundamental issue and presents the first systematic procedure\nfor developing tight lower bounds for the DTSP. \n\n"}
{"id": "1507.00662", "contents": "Title: On the Approximability of Digraph Ordering Abstract: Given an n-vertex digraph D = (V, A) the Max-k-Ordering problem is to compute\na labeling $\\ell : V \\to [k]$ maximizing the number of forward edges, i.e.\nedges (u,v) such that $\\ell$(u) < $\\ell$(v). For different values of k, this\nreduces to Maximum Acyclic Subgraph (k=n), and Max-Dicut (k=2). This work\nstudies the approximability of Max-k-Ordering and its generalizations,\nmotivated by their applications to job scheduling with soft precedence\nconstraints. We give an LP rounding based 2-approximation algorithm for\nMax-k-Ordering for any k={2,..., n}, improving on the known\n2k/(k-1)-approximation obtained via random assignment. The tightness of this\nrounding is shown by proving that for any k={2,..., n} and constant\n$\\varepsilon > 0$, Max-k-Ordering has an LP integrality gap of 2 -\n$\\varepsilon$ for $n^{\\Omega\\left(1/\\log\\log k\\right)}$ rounds of the\nSherali-Adams hierarchy.\n  A further generalization of Max-k-Ordering is the restricted maximum acyclic\nsubgraph problem or RMAS, where each vertex v has a finite set of allowable\nlabels $S_v \\subseteq \\mathbb{Z}^+$. We prove an LP rounding based\n$4\\sqrt{2}/(\\sqrt{2}+1) \\approx 2.344$ approximation for it, improving on the\n$2\\sqrt{2} \\approx 2.828$ approximation recently given by Grandoni et al.\n(Information Processing Letters, Vol. 115(2), Pages 182-185, 2015). In fact,\nour approximation algorithm also works for a general version where the\nobjective counts the edges which go forward by at least a positive offset\nspecific to each edge.\n  The minimization formulation of digraph ordering is DAG edge deletion or\nDED(k), which requires deleting the minimum number of edges from an n-vertex\ndirected acyclic graph (DAG) to remove all paths of length k. We show that\nboth, the LP relaxation and a local ratio approach for DED(k) yield\nk-approximation for any $k\\in [n]$. \n\n"}
{"id": "1507.00824", "contents": "Title: D-MFVI: Distributed Mean Field Variational Inference using Bregman ADMM Abstract: Bayesian models provide a framework for probabilistic modelling of complex\ndatasets. However, many of such models are computationally demanding especially\nin the presence of large datasets. On the other hand, in sensor network\napplications, statistical (Bayesian) parameter estimation usually needs\ndistributed algorithms, in which both data and computation are distributed\nacross the nodes of the network. In this paper we propose a general framework\nfor distributed Bayesian learning using Bregman Alternating Direction Method of\nMultipliers (B-ADMM). We demonstrate the utility of our framework, with Mean\nField Variational Bayes (MFVB) as the primitive for distributed Matrix\nFactorization (MF) and distributed affine structure from motion (SfM). \n\n"}
{"id": "1507.01083", "contents": "Title: Interactive certificate for the verification of Wiedemann's Krylov\n  sequence: application to the certification of the determinant, the minimal\n  and the characteristic polynomials of sparse matrices Abstract: Certificates to a linear algebra computation are additional data structures\nfor each output, which can be used by a-possibly randomized- verification\nalgorithm that proves the correctness of each output. Wiede-mann's algorithm\nprojects the Krylov sequence obtained by repeatedly multiplying a vector by a\nmatrix to obtain a linearly recurrent sequence. The minimal polynomial of this\nsequence divides the minimal polynomial of the matrix. For instance, if the\n$n\\times n$ input matrix is sparse with n 1+o(1) non-zero entries, the\ncomputation of the sequence is quadratic in the dimension of the matrix while\nthe computation of the minimal polynomial is n 1+o(1), once that projected\nKrylov sequence is obtained. In this paper we give algorithms that compute\ncertificates for the Krylov sequence of sparse or structured $n\\times n$\nmatrices over an abstract field, whose Monte Carlo verification complexity can\nbe made essentially linear. As an application this gives certificates for the\ndeterminant, the minimal and characteristic polynomials of sparse or structured\nmatrices at the same cost. \n\n"}
{"id": "1507.02615", "contents": "Title: Optimal Auctions vs. Anonymous Pricing Abstract: For selling a single item to agents with independent but non-identically\ndistributed values, the revenue optimal auction is complex. With respect to it,\nHartline and Roughgarden (2009) showed that the approximation factor of the\nsecond-price auction with an anonymous reserve is between two and four. We\nconsider the more demanding problem of approximating the revenue of the ex ante\nrelaxation of the auction problem by posting an anonymous price (while supplies\nlast) and prove that their worst-case ratio is e. As a corollary, the\nupper-bound of anonymous pricing or anonymous reserves versus the optimal\nauction improves from four to $e$. We conclude that, up to an $e$ factor,\ndiscrimination and simultaneity are unimportant for driving revenue in\nsingle-item auctions. \n\n"}
{"id": "1507.02805", "contents": "Title: On the Connectedness of Clash-free Timetables Abstract: We investigate the connectedness of clash-free timetables with respect to the\nKempe-exchange operation. This investigation is related to the connectedness of\nthe search space of timetabling problem instances, which is a desirable\nproperty, for example for two-step algorithms using the Kempe-exchange during\nthe optimization step. The theoretical framework for our investigations is\nbased on the study of reconfiguration graphs, which model the search space of\ntimetabling problems. We contribute to this framework by including timeslot\navailability requirements in the analysis and we derive improved conditions for\nthe connectedness of clash-free timetables in this setting. We apply the\ntheoretical insights to establish the connectedness of clash-free timetables\nfor a number of benchmark instances. \n\n"}
{"id": "1507.03046", "contents": "Title: An efficient tree decomposition method for permanents and mixed\n  discriminants Abstract: We present an efficient algorithm to compute permanents, mixed discriminants\nand hyperdeterminants of structured matrices and multidimensional arrays\n(tensors). We describe the sparsity structure of an array in terms of a graph,\nand we assume that its treewidth, denoted as $\\omega$, is small. Our algorithm\nrequires $O(n 2^\\omega)$ arithmetic operations to compute permanents, and\n$O(n^2 + n 3^\\omega)$ for mixed discriminants and hyperdeterminants. We finally\nshow that mixed volume computation continues to be hard under bounded treewidth\nassumptions. \n\n"}
{"id": "1507.03269", "contents": "Title: Tensor principal component analysis via sum-of-squares proofs Abstract: We study a statistical model for the tensor principal component analysis\nproblem introduced by Montanari and Richard: Given a order-$3$ tensor $T$ of\nthe form $T = \\tau \\cdot v_0^{\\otimes 3} + A$, where $\\tau \\geq 0$ is a\nsignal-to-noise ratio, $v_0$ is a unit vector, and $A$ is a random noise\ntensor, the goal is to recover the planted vector $v_0$. For the case that $A$\nhas iid standard Gaussian entries, we give an efficient algorithm to recover\n$v_0$ whenever $\\tau \\geq \\omega(n^{3/4} \\log(n)^{1/4})$, and certify that the\nrecovered vector is close to a maximum likelihood estimator, all with high\nprobability over the random choice of $A$. The previous best algorithms with\nprovable guarantees required $\\tau \\geq \\Omega(n)$.\n  In the regime $\\tau \\leq o(n)$, natural tensor-unfolding-based spectral\nrelaxations for the underlying optimization problem break down (in the sense\nthat their integrality gap is large). To go beyond this barrier, we use convex\nrelaxations based on the sum-of-squares method. Our recovery algorithm proceeds\nby rounding a degree-$4$ sum-of-squares relaxations of the\nmaximum-likelihood-estimation problem for the statistical model. To complement\nour algorithmic results, we show that degree-$4$ sum-of-squares relaxations\nbreak down for $\\tau \\leq O(n^{3/4}/\\log(n)^{1/4})$, which demonstrates that\nimproving our current guarantees (by more than logarithmic factors) would\nrequire new techniques or might even be intractable.\n  Finally, we show how to exploit additional problem structure in order to\nsolve our sum-of-squares relaxations, up to some approximation, very\nefficiently. Our fastest algorithm runs in nearly-linear time using shifted\n(matrix) power iteration and has similar guarantees as above. The analysis of\nthis algorithm also confirms a variant of a conjecture of Montanari and Richard\nabout singular vectors of tensor unfoldings. \n\n"}
{"id": "1507.04774", "contents": "Title: Fast clique minor generation in Chimera qubit connectivity graphs Abstract: The current generation of D-Wave quantum annealing processor is designed to\nminimize the energy of an Ising spin configuration whose pairwise interactions\nlie on the edges of a {\\em Chimera} graph $\\mathcal C_{M,N,L}$. In order to\nsolve an Ising spin problem with arbitrary pairwise interaction structure, the\ncorresponding graph must be minor-embedded into a Chimera graph. We define a\ncombinatorial class of {\\em native clique minors} in Chimera graphs with vertex\nimages of uniform, near minimal size, and provide a polynomial-time algorithm\nthat finds a maximum native clique minor in a given induced subgraph of a\nChimera graph. These minors allow improvement over recent work and have\nimmediate practical applications in the field of quantum annealing. \n\n"}
{"id": "1507.06970", "contents": "Title: Perturbed Iterate Analysis for Asynchronous Stochastic Optimization Abstract: We introduce and analyze stochastic optimization methods where the input to\neach gradient update is perturbed by bounded noise. We show that this framework\nforms the basis of a unified approach to analyze asynchronous implementations\nof stochastic optimization algorithms.In this framework, asynchronous\nstochastic optimization algorithms can be thought of as serial methods\noperating on noisy inputs. Using our perturbed iterate framework, we provide\nnew analyses of the Hogwild! algorithm and asynchronous stochastic coordinate\ndescent, that are simpler than earlier analyses, remove many assumptions of\nprevious models, and in some cases yield improved upper bounds on the\nconvergence rates. We proceed to apply our framework to develop and analyze\nKroMagnon: a novel, parallel, sparse stochastic variance-reduced gradient\n(SVRG) algorithm. We demonstrate experimentally on a 16-core machine that the\nsparse and parallel version of SVRG is in some cases more than four orders of\nmagnitude faster than the standard SVRG algorithm. \n\n"}
{"id": "1507.07225", "contents": "Title: Spatial mixing and approximate counting for Potts model on graphs with\n  bounded average degree Abstract: We propose a notion of contraction function for a family of graphs and\nestablish its connection to the strong spatial mixing for spin systems. More\nspecifically, we show that for anti-ferromagnetic Potts model on families of\ngraphs characterized by a specific contraction function, the model exhibits\nstrong spatial mixing, and if further the graphs exhibit certain local sparsity\nwhich are very natural and easy to satisfy by typical sparse graphs, then we\nalso have FPTAS for computing the partition function.\n  This new characterization of strong spatial mixing of multi-spin system does\nnot require maximum degree of the graphs to be bounded, but instead it relates\nthe decay of correlation of the model to a notion of effective average degree\nmeasured by the contraction of a function on the family of graphs. It also\ngeneralizes other notion of effective average degree which may determine the\nstrong spatial mixing, such as the connective constant, whose connection to\nstrong spatial mixing is only known for very simple models and is not\nextendable to general spin systems.\n  As direct consequences: (1) we obtain FPTAS for the partition function of\n$q$-state anti-ferromagnetic Potts model with activity $0\\le\\beta<1$ on graphs\nof maximum degree bounded by $d$ when $q> 3(1-\\beta)d+1$, improving the\nprevious best bound $\\beta> 3(1-\\beta)d$ and asymptotically approaching the\ninapproximability threshold $q=(1-\\beta)d$, and (2) we obtain an efficient\nsampler (in the same sense of fully polynomial-time almost uniform sampler,\nFPAUS) for the Potts model on Erd\\H{o}s-R\\'enyi random graph\n$\\mathcal{G}(n,d/n)$ with sufficiently large constant $d$, provided that $q>\n3(1-\\beta)d+4$. In particular when $\\beta=0$, the sampler becomes an FPAUS for\nfor proper $q$-coloring in $\\mathcal{G}(n,d/n)$ with $q> 3d+4$, improving the\ncurrent best bound $q> 5.5d$ for FPAUS for $q$-coloring in\n$\\mathcal{G}(n,d/n)$. \n\n"}
{"id": "1507.07237", "contents": "Title: A Deterministic Algorithm for Maximizing Submodular Functions Abstract: The problem of maximizing a non-negative submodular function was introduced\nby Feige, Mirrokni, and Vondrak [FOCS'07] who provided a deterministic\nlocal-search based algorithm that guarantees an approximation ratio of $\\frac 1\n3$, as well as a randomized $\\frac 2 5$-approximation algorithm. An extensive\nline of research followed and various algorithms with improving approximation\nratios were developed, all of them are randomized. Finally, Buchbinder et al.\n[FOCS'12] presented a randomized $\\frac 1 2$-approximation algorithm, which is\nthe best possible.\n  This paper gives the first deterministic algorithm for maximizing a\nnon-negative submodular function that achieves an approximation ratio better\nthan $\\frac 1 3$. The approximation ratio of our algorithm is $\\frac 2 5$. Our\nalgorithm is based on recursive composition of solutions obtained by the local\nsearch algorithm of Feige et al. We show that the $\\frac 2 5$ approximation\nratio can be guaranteed when the recursion depth is $2$, and leave open the\nquestion of whether the approximation ratio improves as the recursion depth\nincreases. \n\n"}
{"id": "1507.07368", "contents": "Title: Almost Optimal Cover-Free Families Abstract: Roughly speaking, an $(n,(r,s))$-Cover Free Family (CFF) is a small set of\n$n$-bit strings such that: \"in any $d:=r+s$ indices we see all patterns of\nweight $r$\". CFFs have been of interest for a long time both in discrete\nmathematics as part of block design theory, and in theoretical computer science\nwhere they have found a variety of applications, for example, in parametrized\nalgorithms where they were introduced in the recent breakthrough work of Fomin,\nLokshtanov and Saurabh under the name `lopsided universal sets'.\n  In this paper we give the first explicit construction of cover-free families\nof optimal size up to lower order multiplicative terms, {for any $r$ and $s$}.\nIn fact, our construction time is almost linear in the size of the family.\nBefore our work, such a result existed only for $r=d^{o(1)}$. and $r=\n\\omega(d/(\\log\\log d\\log\\log\\log d))$. As a sample application, we improve the\nrunning times of parameterized algorithms from the recent work of Gabizon,\nLokshtanov and Pilipczuk. \n\n"}
{"id": "1508.01746", "contents": "Title: Using Deep Learning for Detecting Spoofing Attacks on Speech Signals Abstract: It is well known that speaker verification systems are subject to spoofing\nattacks. The Automatic Speaker Verification Spoofing and Countermeasures\nChallenge -- ASVSpoof2015 -- provides a standard spoofing database, containing\nattacks based on synthetic speech, along with a protocol for experiments. This\npaper describes CPqD's systems submitted to the ASVSpoof2015 Challenge, based\non deep neural networks, working both as a classifier and as a feature\nextraction module for a GMM and a SVM classifier. Results show the validity of\nthis approach, achieving less than 0.5\\% EER for known attacks. \n\n"}
{"id": "1508.01928", "contents": "Title: A variational approach to the consistency of spectral clustering Abstract: This paper establishes the consistency of spectral approaches to data\nclustering. We consider clustering of point clouds obtained as samples of a\nground-truth measure. A graph representing the point cloud is obtained by\nassigning weights to edges based on the distance between the points they\nconnect. We investigate the spectral convergence of both unnormalized and\nnormalized graph Laplacians towards the appropriate operators in the continuum\ndomain. We obtain sharp conditions on how the connectivity radius can be scaled\nwith respect to the number of sample points for the spectral convergence to\nhold.\n  We also show that the discrete clusters obtained via spectral clustering\nconverge towards a continuum partition of the ground truth measure. Such\ncontinuum partition minimizes a functional describing the continuum analogue of\nthe graph-based spectral partitioning. Our approach, based on variational\nconvergence, is general and flexible. \n\n"}
{"id": "1508.02439", "contents": "Title: Unified Acceleration Method for Packing and Covering Problems via\n  Diameter Reduction Abstract: The linear coupling method was introduced recently by Allen-Zhu and Orecchia\nfor solving convex optimization problems with first order methods, and it\nprovides a conceptually simple way to integrate a gradient descent step and\nmirror descent step in each iteration. The high-level approach of the linear\ncoupling method is very flexible, and it has shown initial promise by providing\nimproved algorithms for packing and covering linear programs. Somewhat\nsurprisingly, however, while the dependence of the convergence rate on the\nerror parameter $\\epsilon$ for packing problems was improved to\n$O(1/\\epsilon)$, which corresponds to what accelerated gradient methods are\ndesigned to achieve, the dependence for covering problems was only improved to\n$O(1/\\epsilon^{1.5})$, and even that required a different more complicated\nalgorithm. Given the close connections between packing and covering problems\nand since previous algorithms for these very related problems have led to the\nsame $\\epsilon$ dependence, this discrepancy is surprising, and it leaves open\nthe question of the exact role that the linear coupling is playing in\ncoordinating the complementary gradient and mirror descent step of the\nalgorithm. In this paper, we clarify these issues for linear coupling\nalgorithms for packing and covering linear programs, illustrating that the\nlinear coupling method can lead to improved $O(1/\\epsilon)$ dependence for both\npacking and covering problems in a unified manner, i.e., with the same\nalgorithm and almost identical analysis. Our main technical result is a novel\ndiameter reduction method for covering problems that is of independent interest\nand that may be useful in applying the accelerated linear coupling method to\nother combinatorial problems. \n\n"}
{"id": "1508.02440", "contents": "Title: Energy Structure of Optimal Positional Strategies in Mean Payoff Games Abstract: This note studies structural aspects concerning Optimal Positional Strategies\n(OPSs) in Mean Payoff Games (MPGs), it is a contribution to understanding the\nrelationship between OPSs in MPGs and Small Energy-Progress Measures (SEPMs) in\nreweighted Energy Games (EGs). Firstly, it is observed that the space of all\nOPSs, $\\texttt{opt}_{\\Gamma}\\Sigma^M_0$, admits a unique complete decomposition\nin terms of so-called extremal-SEPM{s} in reweighted EG{s}; this points out\nwhat we called the \"Energy-Lattice $\\mathcal{X}^*_{\\Gamma}$ of\n$\\texttt{opt}_{\\Gamma}\\Sigma^M_0$\". Secondly, it is offered a pseudo-polynomial\ntotal-time recursive procedure for enumerating (w/o repetitions) all the\nelements of $\\mathcal{X}^*_{\\Gamma}$, and for computing the corresponding\npartitioning of $\\texttt{opt}_{\\Gamma}\\Sigma^M_0$. It is observed that the\ncorresponding recursion tree defines an additional lattice\n$\\mathcal{B}^*_{\\Gamma}$, whose elements are certain subgames $\\Gamma'\\subseteq\n\\Gamma$ that we call basic subgames. The extremal-SEPMs of a given \\MPG\n$\\Gamma$ coincide with the least-SEPMs of the basic subgames of $\\Gamma$; so,\n$\\mathcal{X}^*_{\\Gamma}$ is the energy-lattice comprising all and only the\nleast-SEPMs of the \\emph{basic} subgames of $\\Gamma$. The complexity of the\nproposed enumeration for both $\\mathcal{B}^*_{\\Gamma}$ and\n$\\mathcal{X}^*_{\\Gamma}$ is $O(|V|^3|E|W |\\mathcal{B}^*_{\\Gamma}|)$ total time\nand $O(|V||E|)+\\Theta\\big(|E| \\mathcal{B}^*_{\\Gamma}|\\big)$ working space.\nFinally, it is constructed an \\MPG $\\Gamma$ for which $|\\mathcal{B}^*_{\\Gamma}|\n> |\\mathcal{X}^*_\\Gamma|$, this proves that $\\mathcal{B}^*_{\\Gamma}$ and\n$\\mathcal{X}^*_\\Gamma$ are not isomorphic. \n\n"}
{"id": "1508.04095", "contents": "Title: Algorithmic Aspects of Optimal Channel Coding Abstract: A central question in information theory is to determine the maximum success\nprobability that can be achieved in sending a fixed number of messages over a\nnoisy channel. This was first studied in the pioneering work of Shannon who\nestablished a simple expression characterizing this quantity in the limit of\nmultiple independent uses of the channel. Here we consider the general setting\nwith only one use of the channel. We observe that the maximum success\nprobability can be expressed as the maximum value of a submodular function.\nUsing this connection, we establish the following results:\n  1. There is a simple greedy polynomial-time algorithm that computes a code\nachieving a (1-1/e)-approximation of the maximum success probability. Moreover,\nfor this problem it is NP-hard to obtain an approximation ratio strictly better\nthan (1-1/e).\n  2. Shared quantum entanglement between the sender and the receiver can\nincrease the success probability by a factor of at most 1/(1-1/e). In addition,\nthis factor is tight if one allows an arbitrary non-signaling box between the\nsender and the receiver.\n  3. We give tight bounds on the one-shot performance of the meta-converse of\nPolyanskiy-Poor-Verdu. \n\n"}
{"id": "1508.04816", "contents": "Title: Reducing multi-qubit interactions in adiabatic quantum computation\n  without adding auxiliary qubits. Part 1: The \"deduc-reduc\" method and its\n  application to quantum factorization of numbers Abstract: Adiabatic quantum computing has recently been used to factor 56153 [Dattani &\nBryans, arXiv:1411.6758] at room temperature, which is orders of magnitude\nlarger than any number attempted yet using Shor's algorithm (circuit-based\nquantum computation). However, this number is still vastly smaller than RSA-768\nwhich is the largest number factored thus far on a classical computer. We\naddress a major issue arising in the scaling of adiabatic quantum factorization\nto much larger numbers. Namely, the existence of many 4-qubit, 3-qubit and\n2-qubit interactions in the Hamiltonians. We showcase our method on various\nexamples, one of which shows that we can remove 94% of the 4-qubit interactions\nand 83% of the 3-qubit interactions in the factorization of a 25-digit number\nwith almost no effort, without adding any auxiliary qubits. Our method is not\nlimited to quantum factoring. Its importance extends to the wider field of\ndiscrete optimization. Any CSP (constraint-satisfiability problem),\npsuedo-boolean optimization problem, or QUBO (quadratic unconstrained Boolean\noptimization) problem can in principle benefit from the \"deduction-reduction\"\nmethod which we introduce in this paper. We provide an open source code which\ntakes in a Hamiltonian (or a discrete discrete function which needs to be\noptimized), and returns a Hamiltonian that has the same unique ground state(s),\nno new auxiliary variables, and as few multi-qubit (multi-variable) terms as\npossible with deduc-reduc. \n\n"}
{"id": "1508.05170", "contents": "Title: Adaptive Online Learning Abstract: We propose a general framework for studying adaptive regret bounds in the\nonline learning framework, including model selection bounds and data-dependent\nbounds. Given a data- or model-dependent bound we ask, \"Does there exist some\nalgorithm achieving this bound?\" We show that modifications to recently\nintroduced sequential complexity measures can be used to answer this question\nby providing sufficient conditions under which adaptive rates can be achieved.\nIn particular each adaptive rate induces a set of so-called offset complexity\nmeasures, and obtaining small upper bounds on these quantities is sufficient to\ndemonstrate achievability. A cornerstone of our analysis technique is the use\nof one-sided tail inequalities to bound suprema of offset random processes.\n  Our framework recovers and improves a wide variety of adaptive bounds\nincluding quantile bounds, second-order data-dependent bounds, and small loss\nbounds. In addition we derive a new type of adaptive bound for online linear\noptimization based on the spectral norm, as well as a new online PAC-Bayes\ntheorem that holds for countably infinite sets. \n\n"}
{"id": "1508.06019", "contents": "Title: Dense Subset Sum may be the hardest Abstract: The Subset Sum problem asks whether a given set of $n$ positive integers\ncontains a subset of elements that sum up to a given target $t$. It is an\noutstanding open question whether the $O^*(2^{n/2})$-time algorithm for Subset\nSum by Horowitz and Sahni [J. ACM 1974] can be beaten in the worst-case setting\nby a \"truly faster\", $O^*(2^{(0.5-\\delta)n})$-time algorithm, with some\nconstant $\\delta > 0$. Continuing an earlier work [STACS 2015], we study Subset\nSum parameterized by the maximum bin size $\\beta$, defined as the largest\nnumber of subsets of the $n$ input integers that yield the same sum. For every\n$\\epsilon > 0$ we give a truly faster algorithm for instances with $\\beta \\leq\n2^{(0.5-\\epsilon)n}$, as well as instances with $\\beta \\geq 2^{0.661n}$.\nConsequently, we also obtain a characterization in terms of the popular density\nparameter $n/\\log_2 t$: if all instances of density at least $1.003$ admit a\ntruly faster algorithm, then so does every instance. This goes against the\ncurrent intuition that instances of density 1 are the hardest, and therefore is\na step toward answering the open question in the affirmative. Our results stem\nfrom novel combinations of earlier algorithms for Subset Sum and a study of an\nextremal question in additive combinatorics connected to the problem of\nUniquely Decodable Code Pairs in information theory. \n\n"}
{"id": "1508.06847", "contents": "Title: The maximum time of 2-neighbour bootstrap percolation in grid graphs and\n  some parameterized results Abstract: In 2-neighborhood bootstrap percolation on a graph $G$, an infection spreads\naccording to the following deterministic rule: infected vertices of $G$ remain\ninfected forever and in consecutive rounds healthy vertices with at least two\nalready infected neighbors become infected. Percolation occurs if eventually\nevery vertex is infected. The maximum time $t(G)$ is the maximum number of\nrounds needed to eventually infect the entire vertex set. In 2013, it was\nproved by Benevides et al \\cite{eurocomb13} that $t(G)$ is NP-hard for planar\ngraphs and that deciding whether $t(G)\\geq k$ is polynomial time solvable for\n$k\\leq 2$, but is NP-complete for $k\\geq 4$. They left two open problems about\nthe complexity for $k=3$ and for planar bipartite graphs. In 2014, we solved\nthe first problem\\cite{wg2014}. In this paper, we solve the second one by\nproving that $t(G)$ is NP-complete even in grid graphs with maximum degree 3.\nWe also prove that $t(G)$ is polynomial time solvable for solid grid graphs\nwith maximum degree 3. Moreover, we prove that the percolation time problem is\nW[1]-hard on the treewidth of the graph, but it is fixed parameter tractable\nwith parameters treewidth$+k$ and maxdegree$+k$. \n\n"}
{"id": "1509.02503", "contents": "Title: An introduction to geometric complexity theory Abstract: I survey methods from differential geometry, algebraic geometry and\nrepresentation theory relevant for the permanent v. determinant problem from\ncomputer science, an algebraic analog of the P v. NP problem. \n\n"}
{"id": "1509.04344", "contents": "Title: Stable Nash Equilibria in the Gale-Shapley Matching Game Abstract: In this article we study the stable marriage game induced by the\nmen-proposing Gale-Shapley algorithm. Our setting is standard: all the lists\nare complete and the matching mechanism is the men-proposing Gale-Shapley\nalgorithm. It is well known that in this setting, men cannot cheat, but women\ncan. In fact, Teo, Sethuraman and Tan \\cite{TST01}, show that there is a\npolynomial time algorithm to obtain, for a given strategy (the set of all\nlists) $Q$ and a woman $w$, the best partner attainable by changing her list.\nHowever, what if the resulting matching is not stable with respect to $Q$?\nObviously, such a matching would be vulnerable to further manipulation, but is\nnot mentioned in \\cite{TST01}. In this paper, we consider (safe) manipulation\nthat implies a stable matching in a most general setting. Specifically, our\ngoal is to decide for a given $Q$, if w can manipulate her list to obtain a\nstrictly better partner with respect to the true strategy $P$ (which may be\ndifferent from $Q$), and also the outcome is a stable matching for $P$. \n\n"}
{"id": "1509.05065", "contents": "Title: Estimating operator norms using covering nets Abstract: We present several polynomial- and quasipolynomial-time approximation schemes\nfor a large class of generalized operator norms. Special cases include the\n$2\\rightarrow q$ norm of matrices for $q>2$, the support function of the set of\nseparable quantum states, finding the least noisy output of\nentanglement-breaking quantum channels, and approximating the injective tensor\nnorm for a map between two Banach spaces whose factorization norm through\n$\\ell_1^n$ is bounded.\n  These reproduce and in some cases improve upon the performance of previous\nalgorithms by Brand\\~ao-Christandl-Yard and followup work, which were based on\nthe Sum-of-Squares hierarchy and whose analysis used techniques from quantum\ninformation such as the monogamy principle of entanglement. Our algorithms, by\ncontrast, are based on brute force enumeration over carefully chosen covering\nnets. These have the advantage of using less memory, having much simpler proofs\nand giving new geometric insights into the problem. Net-based algorithms for\nsimilar problems were also presented by Shi-Wu and Barak-Kelner-Steurer, but in\neach case with a run-time that is exponential in the rank of some matrix. We\nachieve polynomial or quasipolynomial runtimes by using the much smaller nets\nthat exist in $\\ell_1$ spaces. This principle has been used in learning theory,\nwhere it is known as Maurey's empirical method. \n\n"}
{"id": "1509.05662", "contents": "Title: Metric $1$-median selection: Query complexity vs. approximation ratio Abstract: Consider the problem of finding a point in a metric space\n$(\\{1,2,\\ldots,n\\},d)$ with the minimum average distance to other points. We\nshow that this problem has no deterministic $o(n^{1+1/(h-1)})$-query\n$(2h-\\Omega(1))$-approximation algorithms for any constant\n$h\\in\\mathbb{Z}^+\\setminus\\{1\\}$. \n\n"}
{"id": "1509.05725", "contents": "Title: Backdoors into Heterogeneous Classes of SAT and CSP Abstract: In this paper we extend the classical notion of strong and weak backdoor sets\nfor SAT and CSP by allowing that different instantiations of the backdoor\nvariables result in instances that belong to different base classes; the union\nof the base classes forms a heterogeneous base class. Backdoor sets to\nheterogeneous base classes can be much smaller than backdoor sets to\nhomogeneous ones, hence they are much more desirable but possibly harder to\nfind. We draw a detailed complexity landscape for the problem of detecting\nstrong and weak backdoor sets into heterogeneous base classes for SAT and CSP. \n\n"}
{"id": "1509.05806", "contents": "Title: Abelian Hypergroups and Quantum Computation Abstract: Motivated by a connection, described here for the first time, between the\nhidden normal subgroup problem (HNSP) and abelian hypergroups (algebraic\nobjects that model collisions of physical particles), we develop a stabilizer\nformalism using abelian hypergroups and an associated classical simulation\ntheorem (a la Gottesman-Knill). Using these tools, we develop the first\nprovably efficient quantum algorithm for finding hidden subhypergroups of\nnilpotent abelian hypergroups and, via the aforementioned connection, a new,\nhypergroup-based algorithm for the HNSP on nilpotent groups. We also give\nefficient methods for manipulating non-unitary, non-monomial stabilizers and an\nadaptive Fourier sampling technique of general interest. \n\n"}
{"id": "1509.06430", "contents": "Title: Parallel algorithms and concentration bounds for the Lovasz Local Lemma\n  via witness DAGs Abstract: The Lov\\'{a}sz Local Lemma (LLL) is a cornerstone principle in the\nprobabilistic method of combinatorics, and a seminal algorithm of Moser &\nTardos (2010) provides an efficient randomized algorithm to implement it. This\ncan be parallelized to give an algorithm that uses polynomially many processors\nand runs in $O(\\log^3 n)$ time on an EREW PRAM, stemming from $O(\\log n)$\nadaptive computations of a maximal independent set (MIS). Chung et al. (2014)\ndeveloped faster local and parallel algorithms, potentially running in time\n$O(\\log^2 n)$, but these algorithms require more stringent conditions than the\nLLL.\n  We give a new parallel algorithm that works under essentially the same\nconditions as the original algorithm of Moser & Tardos but uses only a single\nMIS computation, thus running in $O(\\log^2 n)$ time on an EREW PRAM. This can\nbe derandomized to give an NC algorithm running in time $O(\\log^2 n)$ as well,\nspeeding up a previous NC LLL algorithm of Chandrasekaran et al. (2013).\n  We also provide improved and tighter bounds on the run-times of the\nsequential and parallel resampling-based algorithms originally developed by\nMoser & Tardos. These apply to any problem instance in which the tighter\nShearer LLL criterion is satisfied. \n\n"}
{"id": "1509.08216", "contents": "Title: Fast Algorithms for Finding Pattern Avoiders and Counting Pattern\n  Occurrences in Permutations Abstract: Given a set $\\Pi$ of permutation patterns of length at most $k$, we present\nan algorithm for building $S_{\\le n}(\\Pi)$, the set of permutations of length\nat most $n$ avoiding the patterns in $\\Pi$, in time $O(|S_{\\le n - 1}(\\Pi)|\n\\cdot k + |S_{n}(\\Pi)|)$. Additionally, we present an $O(n!k)$-time algorithm\nfor counting the number of copies of patterns from $\\Pi$ in each permutation in\n$S_n$. Surprisingly, when $|\\Pi| = 1$, this runtime can be improved to $O(n!)$,\nspending only constant time per permutation. Whereas the previous best\nalgorithms, based on generate-and-check, take exponential time per permutation\nanalyzed, all of our algorithms take time at most polynomial per outputted\npermutation.\n  If we want to solve only the enumerative variant of each problem, computing\n$|S_{\\le n}(\\Pi)|$ or tallying permutations according to $\\Pi$-patterns, rather\nthan to store information about every permutation, then all of our algorithms\ncan be implemented in $O(n^{k+1}k)$ space.\n  Using our algorithms, we generated $|S_5(\\Pi)|, \\ldots, |S_{16}(\\Pi)|$ for\neach $\\Pi \\subseteq S_4$ with $|\\Pi| > 4$, and analyzed OEIS matches. We\nobtained a number of potentially novel pattern-avoidance conjectures.\n  Our algorithms extend to considering permutations in any set closed under\nstandardization of subsequences. Our algorithms also partially adapt to\nconsidering vincular patterns. \n\n"}
{"id": "1509.08333", "contents": "Title: High-dimensional Time Series Prediction with Missing Values Abstract: High-dimensional time series prediction is needed in applications as diverse\nas demand forecasting and climatology. Often, such applications require methods\nthat are both highly scalable, and deal with noisy data in terms of corruptions\nor missing values. Classical time series methods usually fall short of handling\nboth these issues. In this paper, we propose to adapt matrix matrix completion\napproaches that have previously been successfully applied to large scale noisy\ndata, but which fail to adequately model high-dimensional time series due to\ntemporal dependencies. We present a novel temporal regularized matrix\nfactorization (TRMF) framework which supports data-driven temporal dependency\nlearning and enables forecasting ability to our new matrix factorization\napproach. TRMF is highly general, and subsumes many existing matrix\nfactorization approaches for time series data. We make interesting connections\nto graph regularized matrix factorization methods in the context of learning\nthe dependencies. Experiments on both real and synthetic data show that TRMF\noutperforms several existing approaches for common time series tasks. \n\n"}
{"id": "1510.00149", "contents": "Title: Deep Compression: Compressing Deep Neural Networks with Pruning, Trained\n  Quantization and Huffman Coding Abstract: Neural networks are both computationally intensive and memory intensive,\nmaking them difficult to deploy on embedded systems with limited hardware\nresources. To address this limitation, we introduce \"deep compression\", a three\nstage pipeline: pruning, trained quantization and Huffman coding, that work\ntogether to reduce the storage requirement of neural networks by 35x to 49x\nwithout affecting their accuracy. Our method first prunes the network by\nlearning only the important connections. Next, we quantize the weights to\nenforce weight sharing, finally, we apply Huffman coding. After the first two\nsteps we retrain the network to fine tune the remaining connections and the\nquantized centroids. Pruning, reduces the number of connections by 9x to 13x;\nQuantization then reduces the number of bits that represent each connection\nfrom 32 to 5. On the ImageNet dataset, our method reduced the storage required\nby AlexNet by 35x, from 240MB to 6.9MB, without loss of accuracy. Our method\nreduced the size of VGG-16 by 49x from 552MB to 11.3MB, again with no loss of\naccuracy. This allows fitting the model into on-chip SRAM cache rather than\noff-chip DRAM memory. Our compression method also facilitates the use of\ncomplex neural networks in mobile applications where application size and\ndownload bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU,\ncompressed network has 3x to 4x layerwise speedup and 3x to 7x better energy\nefficiency. \n\n"}
{"id": "1510.02173", "contents": "Title: Data-Efficient Learning of Feedback Policies from Image Pixels using\n  Deep Dynamical Models Abstract: Data-efficient reinforcement learning (RL) in continuous state-action spaces\nusing very high-dimensional observations remains a key challenge in developing\nfully autonomous systems. We consider a particularly important instance of this\nchallenge, the pixels-to-torques problem, where an RL agent learns a\nclosed-loop control policy (\"torques\") from pixel information only. We\nintroduce a data-efficient, model-based reinforcement learning algorithm that\nlearns such a closed-loop policy directly from pixel information. The key\ningredient is a deep dynamical model for learning a low-dimensional feature\nembedding of images jointly with a predictive model in this low-dimensional\nfeature space. Joint learning is crucial for long-term predictions, which lie\nat the core of the adaptive nonlinear model predictive control strategy that we\nuse for closed-loop control. Compared to state-of-the-art RL methods for\ncontinuous states and actions, our approach learns quickly, scales to\nhigh-dimensional state spaces, is lightweight and an important step toward\nfully autonomous end-to-end learning from pixels to torques. \n\n"}
{"id": "1510.04676", "contents": "Title: How Good is Multi-Pivot Quicksort? Abstract: Multi-Pivot Quicksort refers to variants of classical quicksort where in the\npartitioning step $k$ pivots are used to split the input into $k + 1$ segments.\nFor many years, multi-pivot quicksort was regarded as impractical, but in 2009\na 2-pivot approach by Yaroslavskiy, Bentley, and Bloch was chosen as the\nstandard sorting algorithm in Sun's Java 7. In 2014 at ALENEX, Kushagra et al.\nintroduced an even faster algorithm that uses three pivots. This paper studies\nwhat possible advantages multi-pivot quicksort might offer in general. The\ncontributions are as follows: Natural comparison-optimal algorithms for\nmulti-pivot quicksort are devised and analyzed. The analysis shows that the\nbenefits of using multiple pivots with respect to the average comparison count\nare marginal and these strategies are inferior to simpler strategies such as\nthe well known median-of-$k$ approach. A substantial part of the partitioning\ncost is caused by rearranging elements. A rigorous analysis of an algorithm for\nrearranging elements in the partitioning step is carried out, observing mainly\nhow often array cells are accessed during partitioning. The algorithm behaves\nbest if 3 to 5 pivots are used. Experiments show that this translates into good\ncache behavior and is closest to predicting observed running times of\nmulti-pivot quicksort algorithms. Finally, it is studied how choosing pivots\nfrom a sample affects sorting cost. The study is theoretical in the sense that\nalthough the findings motivate design recommendations for multipivot quicksort\nalgorithms that lead to running time improvements over known algorithms in an\nexperimental setting, these improvements are small. \n\n"}
{"id": "1510.05137", "contents": "Title: Integrality Gaps and Approximation Algorithms for Dispersers and\n  Bipartite Expanders Abstract: We study the problem of approximating the quality of a disperser. A bipartite\ngraph $G$ on $([N],[M])$ is a $(\\rho N,(1-\\delta)M)$-disperser if for any\nsubset $S\\subseteq [N]$ of size $\\rho N$, the neighbor set $\\Gamma(S)$ contains\nat least $(1-\\delta)M$ distinct vertices. Our main results are strong\nintegrality gaps in the Lasserre hierarchy and an approximation algorithm for\ndispersers.\n  \\begin{enumerate}\n  \\item For any $\\alpha>0$, $\\delta>0$, and a random bipartite graph $G$ with\nleft degree $D=O(\\log N)$, we prove that the Lasserre hierarchy cannot\ndistinguish whether $G$ is an $(N^{\\alpha},(1-\\delta)M)$-disperser or not an\n$(N^{1-\\alpha},\\delta M)$-disperser.\n  \\item For any $\\rho>0$, we prove that there exist infinitely many constants\n$d$ such that the Lasserre hierarchy cannot distinguish whether a random\nbipartite graph $G$ with right degree $d$ is a $(\\rho N,\n(1-(1-\\rho)^d)M)$-disperser or not a $(\\rho N, (1-\\Omega(\\frac{1-\\rho}{\\rho d +\n1-\\rho}))M)$-disperser. We also provide an efficient algorithm to find a subset\nof size exact $\\rho N$ that has an approximation ratio matching the integrality\ngap within an extra loss of\n$\\frac{\\min\\{\\frac{\\rho}{1-\\rho},\\frac{1-\\rho}{\\rho}\\}}{\\log d}$.\n\\end{enumerate}\n  Our method gives an integrality gap in the Lasserre hierarchy for bipartite\nexpanders with left degree~$D$. $G$ on $([N],[M])$ is a $(\\rho N,a)$-expander\nif for any subset $S\\subseteq [N]$ of size $\\rho N$, the neighbor set\n$\\Gamma(S)$ contains at least $a \\cdot \\rho N$ distinct vertices. We prove that\nfor any constant $\\epsilon>0$, there exist constants $\\epsilon'<\\epsilon,\\rho,$\nand $D$ such that the Lasserre hierarchy cannot distinguish whether a bipartite\ngraph on $([N],[M])$ with left degree $D$ is a $(\\rho N,\n(1-\\epsilon')D)$-expander or not a $(\\rho N, (1-\\epsilon)D)$-expander. \n\n"}
{"id": "1510.05886", "contents": "Title: Approximation Algorithm for Minimum Weight Connected $m$-Fold Dominating\n  Set Abstract: Using connected dominating set (CDS) to serve as a virtual backbone in a\nwireless networks can save energy and reduce interference. Since nodes may fail\ndue to accidental damage or energy depletion, it is desirable that the virtual\nbackbone has some fault-tolerance. A $k$-connected $m$-fold dominating set\n($(k,m)$-CDS) of a graph $G$ is a node set $D$ such that every node in\n$V\\setminus D$ has at least $m$ neighbors in $D$ and the subgraph of $G$\ninduced by $D$ is $k$-connected. Using $(k,m)$-CDS can tolerate the failure of\n$\\min\\{k-1,m-1\\}$ nodes. In this paper, we study Minimum Weight $(1,m)$-CDS\nproblem ($(1,m)$-MWCDS), and present an\n$(H(\\delta+m)+2H(\\delta-1))$-approximation algorithm, where $\\delta$ is the\nmaximum degree of the graph and $H(\\cdot)$ is the Harmonic number. Notice that\nthere is a $1.35\\ln n$-approximation algorithm for the $(1,1)$-MWCDS problem,\nwhere $n$ is the number of nodes in the graph. Though our constant in $O(\\ln\n\\cdot)$ is larger than 1.35, $n$ is replaced by $\\delta$. Such a replacement\nenables us to obtain a $(6.67+\\varepsilon)$-approximation for the $(1,m)$-MWCDS\nproblem on unit disk graphs. \n\n"}
{"id": "1511.01287", "contents": "Title: Local Conflict Coloring Abstract: Locally finding a solution to symmetry-breaking tasks such as\nvertex-coloring, edge-coloring, maximal matching, maximal independent set,\netc., is a long-standing challenge in distributed network computing. More\nrecently, it has also become a challenge in the framework of centralized local\ncomputation. We introduce conflict coloring as a general symmetry-breaking task\nthat includes all the aforementioned tasks as specific instantiations ---\nconflict coloring includes all locally checkable labeling tasks from\n[Naor\\&Stockmeyer, STOC 1993]. Conflict coloring is characterized by two\nparameters $l$ and $d$, where the former measures the amount of freedom given\nto the nodes for selecting their colors, and the latter measures the number of\nconstraints which colors of adjacent nodes are subject to.We show that, in the\nstandard LOCAL model for distributed network computing, if $l/d \\textgreater{}\n\\Delta$, then conflict coloring can be solved in $\\tilde\nO(\\sqrt{\\Delta})+\\log^*n$ rounds in $n$-node graphs with maximum degree\n$\\Delta$, where $\\tilde O$ ignores the polylog factors in $\\Delta$. The\ndependency in~$n$ is optimal, as a consequence of the $\\Omega(\\log^*n)$ lower\nbound by [Linial, SIAM J. Comp. 1992] for $(\\Delta+1)$-coloring. An important\nspecial case of our result is a significant improvement over the best known\nalgorithm for distributed $(\\Delta+1)$-coloring due to [Barenboim, PODC 2015],\nwhich required $\\tilde O(\\Delta^{3/4})+\\log^*n$ rounds. Improvements for other\nvariants of coloring, including $(\\Delta+1)$-list-coloring,\n$(2\\Delta-1)$-edge-coloring, $T$-coloring, etc., also follow from our general\nresult on conflict coloring. Likewise, in the framework of centralized local\ncomputation algorithms (LCAs), our general result yields an LCA which requires\na smaller number of probes than the previously best known algorithm for\nvertex-coloring, and works for a wide range of coloring problems. \n\n"}
{"id": "1511.02460", "contents": "Title: Graph Isomorphism for Bounded Genus Graphs In Linear Time Abstract: For every integer $g$, isomorphism of graphs of Euler genus at most $g$ can\nbe decided in linear time.\n  This improves previously known algorithms whose time complexity is $n^{O(g)}$\n(shown in early 1980's), and in fact, this is the first fixed-parameter\ntractable algorithm for the graph isomorphism problem for bounded genus graphs\nin terms of the Euler genus $g$. Our result also generalizes the seminal result\nof Hopcroft and Wong in 1974, which says that the graph isomorphism problem can\nbe decided in linear time for planar graphs.\n  Our proof is quite lengthly and complicated, but if we are satisfied with an\n$O(n^3)$ time algorithm for the same problem, the proof is shorter and easier. \n\n"}
{"id": "1511.02927", "contents": "Title: Fundamental invariants of orbit closures Abstract: For several objects of interest in geometric complexity theory, namely for\nthe determinant, the permanent, the product of variables, the power sum, the\nunit tensor, and the matrix multiplication tensor, we introduce and study a\nfundamental SL-invariant function that relates the coordinate ring of the orbit\nwith the coordinate ring of its closure. For the power sums we can write down\nthis fundamental invariant explicitly in most cases. Our constructions\ngeneralize the two Aronhold invariants on ternary cubics. For the other objects\nwe identify the invariant function conditional on intriguing combinatorial\nproblems much like the well-known Alon-Tarsi conjecture on Latin squares. We\nprovide computer calculations in small dimensions for these cases. As a main\ntool for our analysis, we determine the stabilizers, and we establish the\npolystability of all the mentioned forms and tensors (including the generic\nones). \n\n"}
{"id": "1511.03675", "contents": "Title: Membership in moment polytopes is in NP and coNP Abstract: We show that the problem of deciding membership in the moment polytope\nassociated with a finite-dimensional unitary representation of a compact,\nconnected Lie group is in NP and coNP. This is the first non-trivial result on\nthe computational complexity of this problem, which naively amounts to a\nquadratically-constrained program. Our result applies in particular to the\nKronecker polytopes, and therefore to the problem of deciding positivity of the\nstretched Kronecker coefficients. In contrast, it has recently been shown that\ndeciding positivity of a single Kronecker coefficient is NP-hard in general\n[Ikenmeyer, Mulmuley and Walter, arXiv:1507.02955]. We discuss the consequences\nof our work in the context of complexity theory and the quantum marginal\nproblem. \n\n"}
{"id": "1511.04561", "contents": "Title: 8-Bit Approximations for Parallelism in Deep Learning Abstract: The creation of practical deep learning data-products often requires\nparallelization across processors and computers to make deep learning feasible\non large data sets, but bottlenecks in communication bandwidth make it\ndifficult to attain good speedups through parallelism. Here we develop and test\n8-bit approximation algorithms which make better use of the available bandwidth\nby compressing 32-bit gradients and nonlinear activations to 8-bit\napproximations. We show that these approximations do not decrease predictive\nperformance on MNIST, CIFAR10, and ImageNet for both model and data parallelism\nand provide a data transfer speedup of 2x relative to 32-bit parallelism. We\nbuild a predictive model for speedups based on our experimental data, verify\nits validity on known speedup data, and show that we can obtain a speedup of\n50x and more on a system of 96 GPUs compared to a speedup of 23x for 32-bit. We\ncompare our data types with other methods and show that 8-bit approximations\nachieve state-of-the-art speedups for model parallelism. Thus 8-bit\napproximation is an efficient method to parallelize convolutional networks on\nvery large systems of GPUs. \n\n"}
{"id": "1511.05053", "contents": "Title: A Polynomial Lower Bound for Testing Monotonicity Abstract: We show that every algorithm for testing $n$-variate Boolean functions for\nmonotonicity must have query complexity $\\tilde{\\Omega}(n^{1/4})$. All previous\nlower bounds for this problem were designed for non-adaptive algorithms and, as\na result, the best previous lower bound for general (possibly adaptive)\nmonotonicity testers was only $\\Omega(\\log n)$. Combined with the query\ncomplexity of the non-adaptive monotonicity tester of Khot, Minzer, and Safra\n(FOCS 2015), our lower bound shows that adaptivity can result in at most a\nquadratic reduction in the query complexity for testing monotonicity.\n  By contrast, we show that there is an exponential gap between the query\ncomplexity of adaptive and non-adaptive algorithms for testing regular linear\nthreshold functions (LTFs) for monotonicity. Chen, De, Servedio, and Tan (STOC\n2015) recently showed that non-adaptive algorithms require almost\n$\\Omega(n^{1/2})$ queries for this task. We introduce a new adaptive\nmonotonicity testing algorithm which has query complexity $O(\\log n)$ when the\ninput is a regular LTF. \n\n"}
{"id": "1511.05514", "contents": "Title: Better $s$-$t$-Tours by Gao Trees Abstract: We consider the $s$-$t$-path TSP: given a finite metric space with two\nelements $s$ and $t$, we look for a path from $s$ to $t$ that contains all the\nelements and has minimum total distance. We improve the approximation ratio for\nthis problem from 1.599 to 1.566. Like previous algorithms, we solve the\nnatural LP relaxation and represent an optimum solution $x^*$ as a convex\ncombination of spanning trees. Gao showed that there exists a spanning tree in\nthe support of $x^*$ that has only one edge in each narrow cut (i.e., each cut\n$C$ with $x^*(C)<2$). Our main theorem says that the spanning trees in the\nconvex combination can be chosen such that many of them are such \"Gao trees''. \n\n"}
{"id": "1511.05690", "contents": "Title: Fast Computation on Semirings Isomorphic to $(\\times, \\max)$ on\n  $\\mathbb{R}_+$ Abstract: Important problems across multiple disciplines involve computations on the\nsemiring $(\\times, \\max)$ (or its equivalents, the negated version $(\\times,\n\\min)$), the log-transformed version $(+, \\max)$, or the negated\nlog-transformed version $(+, \\min)$): max-convolution, all-pairs shortest paths\nin a weighted graph, and finding the largest $k$ values in $x_i+y_j$ for two\nlists $x$ and $y$. However, fast algorithms such as those enabling FFT\nconvolution, sub-cubic matrix multiplication, \\emph{etc.}, require inverse\noperations, and thus cannot be computed on semirings. This manuscript\ngeneralizes recent advances on max-convolution: in this approach a small family\nof $p$-norm rings are used to efficiently approximate results on a nonnegative\nsemiring. The general approach can be used to easily compute sub-cubic\nestimates of the all-pairs shortest paths in a graph with nonnegative edge\nweights and sub-quadratic estimates of the top $k$ values in $x_i+y_j$ when $x$\nand $y$ are nonnegative. These methods are fast in practice and can benefit\nfrom coarse-grained parallelization. \n\n"}
{"id": "1511.06037", "contents": "Title: Enumeration and Random Generation of Unlabeled Classes of Graphs: A\n  Practical Study of Cycle Pointing and the Dissymmetry Theorem Abstract: Our work studies the enumeration and random generation of unlabeled\ncombinatorial classes of unrooted graphs. While the technique of vertex\npointing provides a straightforward procedure for analyzing a labeled class of\nunrooted graphs by first studying its rooted counterpart, the existence of\nnontrivial symmetries in the unlabeled case causes this technique to break\ndown. Instead, techniques such as the dissymmetry theorem (of Otter) and cycle\npointing (of Bodirsky et al.) have emerged in the unlabeled case, with the\nformer providing an enumeration of the class and the latter providing both an\nenumeration and an unbiased sampler. In this work, we extend the power of the\ndissymmetry theorem by showing that it in fact provides a Boltzmann sampler for\nthe class in question. We then present an exposition of the cycle pointing\ntechnique, with a focus on the enumeration and random generation of the\nunderlying unpointed class. Finally, we apply cycle pointing to enumerate and\nimplement samplers for the classes of distance-hereditary graphs and three-leaf\npower graphs. \n\n"}
{"id": "1511.07263", "contents": "Title: Input Sparsity Time Low-Rank Approximation via Ridge Leverage Score\n  Sampling Abstract: We present a new algorithm for finding a near optimal low-rank approximation\nof a matrix $A$ in $O(nnz(A))$ time. Our method is based on a recursive\nsampling scheme for computing a representative subset of $A$'s columns, which\nis then used to find a low-rank approximation.\n  This approach differs substantially from prior $O(nnz(A))$ time algorithms,\nwhich are all based on fast Johnson-Lindenstrauss random projections. It\nmatches the guarantees of these methods while offering a number of advantages.\n  Not only are sampling algorithms faster for sparse and structured data, but\nthey can also be applied in settings where random projections cannot. For\nexample, we give new single-pass streaming algorithms for the column subset\nselection and projection-cost preserving sample problems. Our method has also\nbeen used to give the fastest algorithms for provably approximating kernel\nmatrices [MM16]. \n\n"}
{"id": "1511.08270", "contents": "Title: On the hardness of learning sparse parities Abstract: This work investigates the hardness of computing sparse solutions to systems\nof linear equations over F_2. Consider the k-EvenSet problem: given a\nhomogeneous system of linear equations over F_2 on n variables, decide if there\nexists a nonzero solution of Hamming weight at most k (i.e. a k-sparse\nsolution). While there is a simple O(n^{k/2})-time algorithm for it,\nestablishing fixed parameter intractability for k-EvenSet has been a notorious\nopen problem. Towards this goal, we show that unless k-Clique can be solved in\nn^{o(k)} time, k-EvenSet has no poly(n)2^{o(sqrt{k})} time algorithm and no\npolynomial time algorithm when k = (log n)^{2+eta} for any eta > 0.\n  Our work also shows that the non-homogeneous generalization of the problem --\nwhich we call k-VectorSum -- is W[1]-hard on instances where the number of\nequations is O(k log n), improving on previous reductions which produced\nOmega(n) equations. We also show that for any constant eps > 0, given a system\nof O(exp(O(k))log n) linear equations, it is W[1]-hard to decide if there is a\nk-sparse linear form satisfying all the equations or if every function on at\nmost k-variables (k-junta) satisfies at most (1/2 + eps)-fraction of the\nequations. In the setting of computational learning, this shows hardness of\napproximate non-proper learning of k-parities. In a similar vein, we use the\nhardness of k-EvenSet to show that that for any constant d, unless k-Clique can\nbe solved in n^{o(k)} time there is no poly(m, n)2^{o(sqrt{k}) time algorithm\nto decide whether a given set of m points in F_2^n satisfies: (i) there exists\na non-trivial k-sparse homogeneous linear form evaluating to 0 on all the\npoints, or (ii) any non-trivial degree d polynomial P supported on at most k\nvariables evaluates to zero on approx. Pr_{F_2^n}[P(z) = 0] fraction of the\npoints i.e., P is fooled by the set of points. \n\n"}
{"id": "1511.09229", "contents": "Title: Efficient Deterministic Single Round Document Exchange for Edit Distance Abstract: Suppose that we have two parties that possess each a binary string. Suppose\nthat the length of the first string (document) is $n$ and that the two strings\n(documents) have edit distance (minimal number of deletes, inserts and\nsubstitutions needed to transform one string into the other) at most $k$. The\nproblem we want to solve is to devise an efficient protocol in which the first\nparty sends a single message that allows the second party to guess the first\nparty's string. In this paper we show an efficient deterministic protocol for\nthis problem. The protocol runs in time $O(n\\cdot \\mathtt{polylog}(n))$ and has\nmessage size $O(k^2+k\\log^2n)$ bits. To the best of our knowledge, ours is the\nfirst efficient deterministic protocol for this problem, if efficiency is\nmeasured in both the message size and the running time. As an immediate\napplication of our new protocol, we show a new error correcting code that is\nefficient even for large numbers of (adversarial) edit errors. \n\n"}
{"id": "1511.09360", "contents": "Title: On the Complexity of Multi-Parameterized Cluster Editing Abstract: The Cluster Editing problem seeks a transformation of a given undirected\ngraph into a disjoint union of cliques via a minimum number of edge additions\nor deletions. A multi-parameterized version of the problem is studied,\nfeaturing a number of input parameters that bound the amount of both\nedge-additions and deletions per single vertex, as well as the size of a\nclique-cluster. We show that the problem remains NP-hard even when only one\nedge can be deleted and at most two edges can be added per vertex. However, the\nnew formulation allows us to solve Cluster Editing (exactly) in polynomial time\nwhen the number of edge-edit operations per vertex is smaller than half the\nminimum cluster size. In other words, Correlation Clustering can be solved\nefficiently when the number of false positives/negatives per single data\nelement is expected to be small compared to the minimum cluster size. As a\nbyproduct, we obtain a kernelization algorithm that delivers linear-size\nkernels when the two edge-edit bounds are small constants. \n\n"}
{"id": "1512.00482", "contents": "Title: Characterization and Complexity Results on Jumping Finite Automata Abstract: In a jumping finite automaton, the input head can jump to an arbitrary\nposition within the remaining input after reading and consuming a symbol.\n  We characterize the corresponding class of languages in terms of special\nshuffle expressions and survey other equivalent notions from the existing\nliterature.\n  Moreover, we present several results concerning computational hardness and\nalgorithms for parsing and other basic tasks concerning jumping finite\nautomata. \n\n"}
{"id": "1512.02068", "contents": "Title: Minimum Cut of Directed Planar Graphs in O(nloglogn) Time Abstract: We give an $O(n \\log \\log n)$ time algorithm for computing the minimum cut\n(or equivalently, the shortest cycle) of a weighted directed planar graph. This\nimproves the previous fastest $O(n\\log^3 n)$ solution. Interestingly, while in\nundirected planar graphs both min-cut and min $st$-cut have $O(n \\log \\log n)$\nsolutions, in directed planar graphs our result makes min-cut faster than min\n$st$-cut, which currently requires $O(n \\log n)$. \n\n"}
{"id": "1512.04047", "contents": "Title: Parameterizing edge modification problems above lower bounds Abstract: We study the parameterized complexity of a variant of the $F$-free Editing\nproblem: Given a graph $G$ and a natural number $k$, is it possible to modify\nat most $k$ edges in $G$ so that the resulting graph contains no induced\nsubgraph isomorphic to $F$? In our variant, the input additionally contains a\nvertex-disjoint packing $\\mathcal{H}$ of induced subgraphs of $G$, which\nprovides a lower bound $h(\\mathcal{H})$ on the number of edge modifications\nrequired to transform $G$ into an $F$-free graph. While earlier works used the\nnumber $k$ as parameter or structural parameters of the input graph $G$, we\nconsider instead the parameter $\\ell:=k-h(\\mathcal{H})$, that is, the number of\nedge modifications above the lower bound $h(\\mathcal{H})$. We develop a\nframework of generic data reduction rules to show fixed-parameter tractability\nwith respect to $\\ell$ for $K_3$-Free Editing, Feedback Arc Set in Tournaments,\nand Cluster Editing when the packing $\\mathcal{H}$ contains subgraphs with\nbounded solution size. For $K_3$-Free Editing, we also prove NP-hardness in\ncase of edge-disjoint packings of $K_3$s and $\\ell=0$, while for $K_q$-Free\nEditing and $q\\ge 6$, NP-hardness for $\\ell=0$ even holds for vertex-disjoint\npackings of $K_q$s. In addition, we provide NP-hardness results for $F$-free\nVertex Deletion, were the aim is to delete a minimum number of vertices to make\nthe input graph $F$-free. \n\n"}
{"id": "1512.04932", "contents": "Title: Strong reductions for extended formulations Abstract: We generalize the reduction mechanism for linear programming problems and\nsemidefinite programming problems from [arXiv:1410.8816] in two ways 1)\nrelaxing the requirement of affineness and 2) extending to fractional\noptimization problems. As applications we provide several new LP-hardness and\nSDP-hardness results, e.g., for the SparsestCut problem, the BalancedSeparator\nproblem, the MaxCut problem and the Matching problem on 3-regular graphs. We\nalso provide a new, very strong Lasserre integrality gap result for the\nIndependentSet problem, which is strictly greater than the best known LP\napproximation, showing that the Lasserre hierarchy does not always provide the\ntightest SDP relaxation. \n\n"}
{"id": "1512.05411", "contents": "Title: Non-Local Probes Do Not Help with Graph Problems Abstract: This work bridges the gap between distributed and centralised models of\ncomputing in the context of sublinear-time graph algorithms. A priori, typical\ncentralised models of computing (e.g., parallel decision trees or centralised\nlocal algorithms) seem to be much more powerful than distributed\nmessage-passing algorithms: centralised algorithms can directly probe any part\nof the input, while in distributed algorithms nodes can only communicate with\ntheir immediate neighbours. We show that for a large class of graph problems,\nthis extra freedom does not help centralised algorithms at all: for example,\nefficient stateless deterministic centralised local algorithms can be simulated\nwith efficient distributed message-passing algorithms. In particular, this\nenables us to transfer existing lower bound results from distributed algorithms\nto centralised local algorithms. \n\n"}
{"id": "1512.06678", "contents": "Title: Solving $k$-SUM using few linear queries Abstract: The $k$-SUM problem is given $n$ input real numbers to determine whether any\n$k$ of them sum to zero. The problem is of tremendous importance in the\nemerging field of complexity theory within $P$, and it is in particular open\nwhether it admits an algorithm of complexity $O(n^c)$ with $c<\\lceil\n\\frac{k}{2} \\rceil$. Inspired by an algorithm due to Meiser (1993), we show\nthat there exist linear decision trees and algebraic computation trees of depth\n$O(n^3\\log^3 n)$ solving $k$-SUM. Furthermore, we show that there exists a\nrandomized algorithm that runs in $\\tilde{O}(n^{\\lceil \\frac{k}{2} \\rceil+8})$\ntime, and performs $O(n^3\\log^3 n)$ linear queries on the input. Thus, we show\nthat it is possible to have an algorithm with a runtime almost identical (up to\nthe $+8$) to the best known algorithm but for the first time also with the\nnumber of queries on the input a polynomial that is independent of $k$. The\n$O(n^3\\log^3 n)$ bound on the number of linear queries is also a tighter bound\nthan any known algorithm solving $k$-SUM, even allowing unlimited total time\noutside of the queries. By simultaneously achieving few queries to the input\nwithout significantly sacrificing runtime vis-\\`{a}-vis known algorithms, we\ndeepen the understanding of this canonical problem which is a cornerstone of\ncomplexity-within-$P$.\n  We also consider a range of tradeoffs between the number of terms involved in\nthe queries and the depth of the decision tree. In particular, we prove that\nthere exist $o(n)$-linear decision trees of depth $o(n^4)$. \n\n"}
{"id": "1601.01597", "contents": "Title: Grafalgo - A Library of Graph Algorithms and Supporting Data Structures\n  (revised) Abstract: This report provides an (updated) overview of {\\sl Grafalgo}, an open-source\nlibrary of graph algorithms and the data structures used to implement them. The\nprograms in this library were originally written to support a graduate class in\nadvanced data structures and algorithms at Washington University. Because the\ncode's primary purpose was pedagogical, it was written to be as straightforward\nas possible, while still being highly efficient. Grafalgo is implemented in C++\nand incorporates some features of C++11.\n  The library is available on an open-source basis and may be downloaded from\nhttps://code.google.com/p/grafalgo/. Source code documentation is at\nwww.arl.wustl.edu/\\textasciitilde jst/doc/grafalgo. While not designed as\nproduction code, the library is suitable for use in larger systems, so long as\nits limitations are understood. The readability of the code also makes it\nrelatively straightforward to extend it for other purposes. \n\n"}
{"id": "1601.03038", "contents": "Title: Linear time algorithm for computing the rank of divisors on cactus\n  graphs Abstract: Rank of divisor on graph was introduced in 2007 and it quickly attracts many\nattentions. Recently, in 2015 the problem for computing this quantity was\nproved to be NP-hard. In this paper, we describe a linear time algorithm for\nthis problem limited on cactus graphs. \n\n"}
{"id": "1601.03095", "contents": "Title: Submodular Optimization under Noise Abstract: We consider the problem of maximizing a monotone submodular function under\nnoise. There has been a great deal of work on optimization of submodular\nfunctions under various constraints, resulting in algorithms that provide\ndesirable approximation guarantees. In many applications, however, we do not\nhave access to the submodular function we aim to optimize, but rather to some\nerroneous or noisy version of it. This raises the question of whether provable\nguarantees are obtainable in presence of error and noise. We provide initial\nanswers, by focusing on the question of maximizing a monotone submodular\nfunction under a cardinality constraint when given access to a noisy oracle of\nthe function. We show that:\n  - For a cardinality constraint $k \\geq 2$, there is an approximation\nalgorithm whose approximation ratio is arbitrarily close to $1-1/e$;\n  - For $k=1$ there is an algorithm whose approximation ratio is arbitrarily\nclose to $1/2$. No randomized algorithm can obtain an approximation ratio\nbetter than $1/2+o(1)$;\n  -If the noise is adversarial, no non-trivial approximation guarantee can be\nobtained. \n\n"}
{"id": "1601.03603", "contents": "Title: Protection of flows under targeted attacks Abstract: Due to the importance of robustness in many real-world optimization problems,\nthe field of robust optimization has gained a lot of attention over the past\ndecade. We concentrate on maximum flow problems and introduce a novel robust\noptimization model which, compared to known models from the literature,\nfeatures several advantageous properties: (i) We consider a general class of\npath-based flow problems which can be used to model a large variety of network\nrouting problems (and other packing problems). (ii) We aim at solutions that\nare robust against targeted attacks by a potent adversary who may attack any\nflow path of his choice on any edge of the network. (iii) In contrast to\nprevious robust maximum flow models, for which no efficient algorithms are\nknown, optimal robust flows for the most important basic variants of our model\ncan be found in polynomial time.\n  We also consider generalizations where the flow player can spend a budget to\nprotect the network against the interdictor. Here, we show that the problem can\nbe solved efficiently when the interdiction costs are determined by the flow\nplayer from scratch. However, the problem becomes hard to approximate when the\nflow player has to improve an initial protection infrastructure. \n\n"}
{"id": "1601.04366", "contents": "Title: Learning the kernel matrix via predictive low-rank approximations Abstract: Efficient and accurate low-rank approximations of multiple data sources are\nessential in the era of big data. The scaling of kernel-based learning\nalgorithms to large datasets is limited by the O(n^2) computation and storage\ncomplexity of the full kernel matrix, which is required by most of the recent\nkernel learning algorithms.\n  We present the Mklaren algorithm to approximate multiple kernel matrices\nlearn a regression model, which is entirely based on geometrical concepts. The\nalgorithm does not require access to full kernel matrices yet it accounts for\nthe correlations between all kernels. It uses Incomplete Cholesky\ndecomposition, where pivot selection is based on least-angle regression in the\ncombined, low-dimensional feature space. The algorithm has linear complexity in\nthe number of data points and kernels. When explicit feature space induced by\nthe kernel can be constructed, a mapping from the dual to the primal Ridge\nregression weights is used for model interpretation.\n  The Mklaren algorithm was tested on eight standard regression datasets. It\noutperforms contemporary kernel matrix approximation approaches when learning\nwith multiple kernels. It identifies relevant kernels, achieving highest\nexplained variance than other multiple kernel learning methods for the same\nnumber of iterations. Test accuracy, equivalent to the one using full kernel\nmatrices, was achieved with at significantly lower approximation ranks. A\ndifference in run times of two orders of magnitude was observed when either the\nnumber of samples or kernels exceeds 3000. \n\n"}
{"id": "1601.04520", "contents": "Title: A Dichotomy for First-Order Reducts of Unary Structures Abstract: Many natural decision problems can be formulated as constraint satisfaction\nproblems for reducts $\\mathbb{A}$ of finitely bounded homogeneous structures.\nThis class of problems is a large generalisation of the class of CSPs over\nfinite domains. Our first result is a general polynomial-time reduction from\nsuch infinite-domain CSPs to finite-domain CSPs. We use this reduction to\nobtain new powerful polynomial-time tractability conditions that can be\nexpressed in terms of the topological polymorphism clone of $\\mathbb{A}$.\nMoreover, we study the subclass $\\mathcal{C}$ of CSPs for structures\n$\\mathbb{A}$ that are reducts of a structure with a unary language. Also this\nclass $\\mathcal{C}$ properly extends the class of all finite-domain CSPs. We\napply our new tractability conditions to prove the general tractability\nconjecture of Bodirsky and Pinsker for reducts of finitely bounded homogeneous\nstructures for the class $\\mathcal{C}$. \n\n"}
{"id": "1601.06207", "contents": "Title: Rectified Gaussian Scale Mixtures and the Sparse Non-Negative Least\n  Squares Problem Abstract: In this paper, we develop a Bayesian evidence maximization framework to solve\nthe sparse non-negative least squares (S-NNLS) problem. We introduce a family\nof probability densities referred to as the Rectified Gaussian Scale Mixture\n(R- GSM) to model the sparsity enforcing prior distribution for the solution.\nThe R-GSM prior encompasses a variety of heavy-tailed densities such as the\nrectified Laplacian and rectified Student- t distributions with a proper choice\nof the mixing density. We utilize the hierarchical representation induced by\nthe R-GSM prior and develop an evidence maximization framework based on the\nExpectation-Maximization (EM) algorithm. Using the EM based method, we estimate\nthe hyper-parameters and obtain a point estimate for the solution. We refer to\nthe proposed method as rectified sparse Bayesian learning (R-SBL). We provide\nfour R- SBL variants that offer a range of options for computational complexity\nand the quality of the E-step computation. These methods include the Markov\nchain Monte Carlo EM, linear minimum mean-square-error estimation, approximate\nmessage passing and a diagonal approximation. Using numerical experiments, we\nshow that the proposed R-SBL method outperforms existing S-NNLS solvers in\nterms of both signal and support recovery performance, and is also very robust\nagainst the structure of the design matrix. \n\n"}
{"id": "1602.00422", "contents": "Title: Packed Compact Tries: A Fast and Efficient Data Structure for Online\n  String Processing Abstract: In this paper, we present a new data structure called the packed compact trie\n(packed c-trie) which stores a set $S$ of $k$ strings of total length $n$ in $n\n\\log\\sigma + O(k \\log n)$ bits of space and supports fast pattern matching\nqueries and updates, where $\\sigma$ is the size of an alphabet. Assume that\n$\\alpha = \\log_\\sigma n$ letters are packed in a single machine word on the\nstandard word RAM model, and let $f(k,n)$ denote the query and update times of\nthe dynamic predecessor/successor data structure of our choice which stores $k$\nintegers from universe $[1,n]$ in $O(k \\log n)$ bits of space. Then, given a\nstring of length $m$, our packed c-tries support pattern matching queries and\ninsert/delete operations in $O(\\frac{m}{\\alpha} f(k,n))$ worst-case time and in\n$O(\\frac{m}{\\alpha} + f(k,n))$ expected time. Our experiments show that our\npacked c-tries are faster than the standard compact tries (a.k.a. Patricia\ntrees) on real data sets. As an application of our packed c-trie, we show that\nthe sparse suffix tree for a string of length $n$ over prefix codes with $k$\nsampled positions, such as evenly-spaced and word delimited sparse suffix\ntrees, can be constructed online in $O((\\frac{n}{\\alpha} + k) f(k,n))$\nworst-case time and $O(\\frac{n}{\\alpha} + k f(k,n))$ expected time with $n \\log\n\\sigma + O(k \\log n)$ bits of space. When $k = O(\\frac{n}{\\alpha})$, by using\nthe state-of-the-art dynamic predecessor/successor data structures, we obtain\nsub-linear time construction algorithms using only $O(\\frac{n}{\\alpha})$ bits\nof space in both cases. We also discuss an application of our packed c-tries to\nonline LZD factorization. \n\n"}
{"id": "1602.02747", "contents": "Title: Independent sets and cuts in large-girth regular graphs Abstract: We present a local algorithm producing an independent set of expected size\n$0.44533n$ on large-girth 3-regular graphs and $0.40407n$ on large-girth\n4-regular graphs. We also construct a cut (or bisection or bipartite subgraph)\nwith $1.34105n$ edges on large-girth 3-regular graphs. These decrease the gaps\nbetween the best known upper and lower bounds from $0.0178$ to $0.01$, from\n$0.0242$ to $0.0123$ and from $0.0724$ to $0.0616$, respectively. We are using\nlocal algorithms, therefore, the method also provides upper bounds for the\nfractional coloring numbers of $1 / 0.44533 \\approx 2.24554$ and $1 / 0.40407\n\\approx 2.4748$ and fractional edge coloring number $1.5 / 1.34105 \\approx\n1.1185$. Our algorithms are applications of the technique introduced by Hoppen\nand Wormald. \n\n"}
{"id": "1602.04274", "contents": "Title: Systematic and Deterministic Graph-Minor Embedding for Cartesian\n  Products of Graphs Abstract: The limited connectivity of current and next-generation quantum annealers\nmotivates the need for efficient graph-minor embedding methods. These methods\nallow non-native problems to be adapted to the target annealer's architecture.\nThe overhead of the widely used heuristic techniques is quickly proving to be a\nsignificant bottleneck for solving real-world applications. To alleviate this\ndifficulty, we propose a systematic and deterministic embedding method,\nexploiting the structures of both the input graph of the specific problem and\nthe quantum annealer. We focus on the specific case of the Cartesian product of\ntwo complete graphs, a regular structure that occurs in many problems. We\ndivide the embedding problem by first embedding one of the factors of the\nCartesian product in a repeatable pattern. The resulting simplified problem\nconsists of the placement and connecting together of these copies to reach a\nvalid solution. Aside from the obvious advantage of a systematic and\ndeterministic approach with respect to speed and efficiency, the embeddings\nproduced are easily scaled for larger processors and show desirable properties\nfor the number of qubits used and the chain length distribution. To conclude,\nwe briefly address the problem of circumventing inoperable qubits by presenting\npossible extensions of the method. \n\n"}
{"id": "1602.04505", "contents": "Title: Quasi-4-Connected Components Abstract: We introduce a new decomposition of a graphs into quasi-4-connected\ncomponents, where we call a graph quasi-4-connected if it is 3-connected and it\nonly has separations of order 3 that remove a single vertex. Moreover, we give\na cubic time algorithm computing the decomposition of a given graph.\n  Our decomposition into quasi-4-connected components refines the well-known\ndecompositions of graphs into biconnected and triconnected components. We\nrelate our decomposition to Robertson and Seymour's theory of tangles by\nestablishing a correspondence between the quasi-4-connected components of a\ngraph and its tangles of order 4. \n\n"}
{"id": "1602.04995", "contents": "Title: On the Density of non-Simple 3-Planar Graphs Abstract: A \\emph{$k$-planar graph} is a graph that can be drawn in the plane such that\nevery edge is crossed at most $k$ times. For $k \\leq 4$, Pach and T\\'oth proved\na bound of $(k+3)(n-2)$ on the total number of edges of a $k$-planar graph,\nwhich is tight for $k=1,2$. For $k=3$, the bound of $6n-12$ has been improved\nto $\\frac{11}{2}n-11$ and has been shown to be optimal up to an additive\nconstant for simple graphs. In this paper, we prove that the bound of\n$\\frac{11}{2}n-11$ edges also holds for non-simple $3$-planar graphs that admit\ndrawings in which non-homotopic parallel edges and self-loops are allowed.\nBased on this result, a characterization of \\emph{optimal $3$-planar graphs}\n(that is, $3$-planar graphs with $n$ vertices and exactly $\\frac{11}{2}n-11$\nedges) might be possible, as to the best of our knowledge the densest known\nsimple $3$-planar is not known to be optimal. \n\n"}
{"id": "1602.05852", "contents": "Title: Consensus in Rooted Dynamic Networks with Short-Lived Stability Abstract: We consider the problem of solving consensus using deterministic algorithms\nin a synchronous dynamic network with unreliable, directional point-to-point\nlinks, which are under the control of a message adversary. In contrast to a\nlarge body of existing work that focuses on oblivious message adversaries where\nthe communication graphs are picked from a predefined set, we consider message\nadversaries where guarantees about stable periods that occur only eventually\ncan be expressed. We reveal to what extent such eventual stability is necessary\nand sufficient, that is, we present the shortest period of stability that\npermits solving consensus, a result that should prove quite useful in systems\nthat exhibit erratic boot-up phases or recover after repeatedly occurring,\nmassive transient faults. Contrary to the case of longer stability periods,\nwhere we show how standard algorithmic techniques for solving consensus can be\nemployed, the short-lived nature of the stability phase forces us to use more\nunusual algorithmic methods that avoid waiting explicitly for the stability\nperiod to occur. \n\n"}
{"id": "1602.05897", "contents": "Title: Toward Deeper Understanding of Neural Networks: The Power of\n  Initialization and a Dual View on Expressivity Abstract: We develop a general duality between neural networks and compositional\nkernels, striving towards a better understanding of deep learning. We show that\ninitial representations generated by common random initializations are\nsufficiently rich to express all functions in the dual kernel space. Hence,\nthough the training objective is hard to optimize in the worst case, the\ninitial weights form a good starting point for optimization. Our dual view also\nreveals a pragmatic and aesthetic perspective of neural networks and\nunderscores their expressive power. \n\n"}
{"id": "1602.05908", "contents": "Title: Efficient approaches for escaping higher order saddle points in\n  non-convex optimization Abstract: Local search heuristics for non-convex optimizations are popular in applied\nmachine learning. However, in general it is hard to guarantee that such\nalgorithms even converge to a local minimum, due to the existence of\ncomplicated saddle point structures in high dimensions. Many functions have\ndegenerate saddle points such that the first and second order derivatives\ncannot distinguish them with local optima. In this paper we use higher order\nderivatives to escape these saddle points: we design the first efficient\nalgorithm guaranteed to converge to a third order local optimum (while existing\ntechniques are at most second order). We also show that it is NP-hard to extend\nthis further to finding fourth order local optima. \n\n"}
{"id": "1602.06922", "contents": "Title: Fast Cross-Polytope Locality-Sensitive Hashing Abstract: We provide a variant of cross-polytope locality sensitive hashing with\nrespect to angular distance which is provably optimal in asymptotic sensitivity\nand enjoys $\\mathcal{O}(d \\ln d )$ hash computation time. Building on a recent\nresult (by Andoni, Indyk, Laarhoven, Razenshteyn, Schmidt, 2015), we show that\noptimal asymptotic sensitivity for cross-polytope LSH is retained even when the\ndense Gaussian matrix is replaced by a fast Johnson-Lindenstrauss transform\nfollowed by discrete pseudo-rotation, reducing the hash computation time from\n$\\mathcal{O}(d^2)$ to $\\mathcal{O}(d \\ln d )$. Moreover, our scheme achieves\nthe optimal rate of convergence for sensitivity. By incorporating a\nlow-randomness Johnson-Lindenstrauss transform, our scheme can be modified to\nrequire only $\\mathcal{O}(\\ln^9(d))$ random bits \n\n"}
{"id": "1602.07876", "contents": "Title: On Satisfiability Problems with a Linear Structure Abstract: It was recently shown \\cite{STV} that satisfiability is polynomially solvable\nwhen the incidence graph is an interval bipartite graph (an interval graph\nturned into a bipartite graph by omitting all edges within each partite set).\nHere we relax this condition in several directions: First, we show that it\nholds for $k$-interval bigraphs, bipartite graphs which can be converted to\ninterval bipartite graphs by adding to each node of one side at most $k$ edges;\nthe same result holds for the counting and the weighted maximization version of\nsatisfiability. Second, given two linear orders, one for the variables and one\nfor the clauses, we show how to find, in polynomial time, the smallest $k$ such\nthat there is a $k$-interval bigraph compatible with these two orders. On the\nnegative side we prove that, barring complexity collapses, no such extensions\nare possible for CSPs more general than satisfiability. We also show\nNP-hardness of recognizing 1-interval bigraphs. \n\n"}
{"id": "1602.08268", "contents": "Title: Construction of Gene and Species Trees from Sequence Data incl.\n  Orthologs, Paralogs, and Xenologs Abstract: Phylogenetic reconstruction aims at finding plausible hypotheses of the\nevolutionary history of genes or species based on genomic sequence information.\nThe distinction of orthologous genes (genes that having a common ancestry and\ndiverged after a speciation) is crucial and lies at the heart of many genomic\nstudies. However, existing methods that rely only on 1:1 orthologs to infer\nspecies trees are strongly restricted to a small set of allowed genes that\nprovide information about the species tree. The use of larger gene sets that\nconsist in addition of non-orthologous genes (e.g. so-called paralogous or\nxenologous genes) considerably increases the information about the evolutionary\nhistory of the respective species. In this work, we introduce a novel method to\ncompute species phylogenies based on sequence data including orthologs,\nparalogs or even xenologs. \n\n"}
{"id": "1603.00928", "contents": "Title: Trainyard is NP-Hard Abstract: Recently, due to the widespread diffusion of smart-phones, mobile puzzle\ngames have experienced a huge increase in their popularity. A successful puzzle\nhas to be both captivating and challenging, and it has been suggested that this\nfeatures are somehow related to their computational complexity \\cite{Eppstein}.\nIndeed, many puzzle games --such as Mah-Jongg, Sokoban, Candy Crush, and 2048,\nto name a few-- are known to be NP-hard \\cite{CondonFLS97,\nculberson1999sokoban, GualaLN14, Mehta14a}. In this paper we consider\nTrainyard: a popular mobile puzzle game whose goal is to get colored trains\nfrom their initial stations to suitable destination stations. We prove that the\nproblem of determining whether there exists a solution to a given Trainyard\nlevel is NP-hard. We also \\href{http://trainyard.isnphard.com}{provide} an\nimplementation of our hardness reduction. \n\n"}
{"id": "1603.01191", "contents": "Title: A fixed-parameter algorithm for a routing open shop problem: unit\n  processing times, few machines and locations Abstract: The open shop problem is to find a minimum makespan schedule to process each\njob $J_i$ on each machine $M_q$ for $p_{iq}$ time such that, at any time, each\nmachine processes at most one job and each job is processed by at most one\nmachine. We study a problem variant in which the jobs are located in the\nvertices of an edge-weighted graph. The weights determine the time needed for\nthe machines to travel between jobs in different vertices. We show that the\nproblem with $m$ machines and $n$ unit-time jobs in $g$ vertices is solvable in\n$2^{O(gm^2\\log gm)}+O(mn\\log n)$ time. \n\n"}
{"id": "1603.02752", "contents": "Title: Best-of-K Bandits Abstract: This paper studies the Best-of-K Bandit game: At each time the player chooses\na subset S among all N-choose-K possible options and observes reward max(X(i) :\ni in S) where X is a random vector drawn from a joint distribution. The\nobjective is to identify the subset that achieves the highest expected reward\nwith high probability using as few queries as possible. We present\ndistribution-dependent lower bounds based on a particular construction which\nforce a learner to consider all N-choose-K subsets, and match naive extensions\nof known upper bounds in the bandit setting obtained by treating each subset as\na separate arm. Nevertheless, we present evidence that exhaustive search may be\navoided for certain, favorable distributions because the influence of\nhigh-order order correlations may be dominated by lower order statistics.\nFinally, we present an algorithm and analysis for independent arms, which\nmitigates the surprising non-trivial information occlusion that occurs due to\nonly observing the max in the subset. This may inform strategies for more\ngeneral dependent measures, and we complement these result with independent-arm\nlower bounds. \n\n"}
{"id": "1603.03178", "contents": "Title: Near-Optimal Sample Complexity Bounds for Circulant Binary Embedding Abstract: Binary embedding is the problem of mapping points from a high-dimensional\nspace to a Hamming cube in lower dimension while preserving pairwise distances.\nAn efficient way to accomplish this is to make use of fast embedding techniques\ninvolving Fourier transform e.g.~circulant matrices. While binary embedding has\nbeen studied extensively, theoretical results on fast binary embedding are\nrather limited. In this work, we build upon the recent literature to obtain\nsignificantly better dependencies on the problem parameters. A set of $N$\npoints in $\\mathbb{R}^n$ can be properly embedded into the Hamming cube $\\{\\pm\n1\\}^k$ with $\\delta$ distortion, by using $k\\sim\\delta^{-3}\\log N$ samples\nwhich is optimal in the number of points $N$ and compares well with the optimal\ndistortion dependency $\\delta^{-2}$. Our optimal embedding result applies in\nthe regime $\\log N\\lesssim n^{1/3}$. Furthermore, if the looser condition $\\log\nN\\lesssim \\sqrt{n}$ holds, we show that all but an arbitrarily small fraction\nof the points can be optimally embedded. We believe our techniques can be\nuseful to obtain improved guarantees for other nonlinear embedding problems. \n\n"}
{"id": "1603.07340", "contents": "Title: Peeling and Nibbling the Cactus: Subexponential-Time Algorithms for\n  Counting Triangulations and Related Problems Abstract: Given a set of $n$ points $S$ in the plane, a triangulation $T$ of $S$ is a\nmaximal set of non-crossing segments with endpoints in $S$. We present an\nalgorithm that computes the number of triangulations on a given set of $n$\npoints in time $n^{(11+ o(1))\\sqrt{n} }$, significantly improving the previous\nbest running time of $O(2^n n^2)$ by Alvarez and Seidel [SoCG 2013]. Our main\ntool is identifying separators of size $O(\\sqrt{n})$ of a triangulation in a\ncanonical way. The definition of the separators are based on the decomposition\nof the triangulation into nested layers (\"cactus graphs\"). Based on the above\nalgorithm, we develop a simple and formal framework to count other non-crossing\nstraight-line graphs in $n^{O(\\sqrt{n})}$ time. We demonstrate the usefulness\nof the framework by applying it to counting non-crossing Hamilton cycles,\nspanning trees, perfect matchings, $3$-colorable triangulations, connected\ngraphs, cycle decompositions, quadrangulations, $3$-regular graphs, and more. \n\n"}
{"id": "1603.08976", "contents": "Title: Local Search Yields a PTAS for k-Means in Doubling Metrics Abstract: The most well known and ubiquitous clustering problem encountered in nearly\nevery branch of science is undoubtedly $k$-means: given a set of data points\nand a parameter $k$, select $k$ centres and partition the data points into $k$\nclusters around these centres so that the sum of squares of distances of the\npoints to their cluster centre is minimized. Typically these data points lie\n$\\mathbb{R}^d$ for some $d\\geq 2$.\n  $k$-means and the first algorithms for it were introduced in the 1950's.\nSince then, hundreds of papers have studied this problem and many algorithms\nhave been proposed for it. The most commonly used algorithm is known as\nLloyd-Forgy, which is also referred to as \"the\" $k$-means algorithm, and\nvarious extensions of it often work very well in practice. However, they may\nproduce solutions whose cost is arbitrarily large compared to the optimum\nsolution. Kanungo et al. [2004] analyzed a simple local search heuristic to get\na polynomial-time algorithm with approximation ratio $9+\\epsilon$ for any fixed\n$\\epsilon>0$ for $k$-means in Euclidean space.\n  Finding an algorithm with a better approximation guarantee has remained one\nof the biggest open questions in this area, in particular whether one can get a\ntrue PTAS for fixed dimension Euclidean space. We settle this problem by\nshowing that a simple local search algorithm provides a PTAS for $k$-means in\n$\\mathbb{R}^d$ for any fixed $d$. More precisely, for any error parameter\n$\\epsilon>0$, the local search algorithm that considers swaps of up to\n$\\rho=d^{O(d)}\\cdot{\\epsilon}^{-O(d/\\epsilon)}$ centres at a time finds a\nsolution using exactly $k$ centres whose cost is at most a\n$(1+\\epsilon)$-factor greater than the optimum.\n  Finally, we provide the first demonstration that local search yields a PTAS\nfor the uncapacitated facility location problem and $k$-median with non-uniform\nopening costs in doubling metrics. \n\n"}
{"id": "1603.09128", "contents": "Title: Bilingual Learning of Multi-sense Embeddings with Discrete Autoencoders Abstract: We present an approach to learning multi-sense word embeddings relying both\non monolingual and bilingual information. Our model consists of an encoder,\nwhich uses monolingual and bilingual context (i.e. a parallel sentence) to\nchoose a sense for a given word, and a decoder which predicts context words\nbased on the chosen sense. The two components are estimated jointly. We observe\nthat the word representations induced from bilingual data outperform the\nmonolingual counterparts across a range of evaluation tasks, even though\ncrosslingual information is not available at test time. \n\n"}
{"id": "1604.01429", "contents": "Title: The Price of Differential Privacy for Low-Rank Factorization Abstract: In this paper, we study what price one has to pay to release {\\em\ndifferentially private low-rank factorization} of a matrix. We consider various\nsettings that are close to the real world applications of low-rank\nfactorization: (i) the manner in which matrices are updated (row by row or in\nan arbitrary manner), (ii) whether matrices are distributed or not, and (iii)\nhow the output is produced (once at the end of all updates, also known as {\\em\none-shot algorithms} or continually). Even though these settings are well\nstudied without privacy, surprisingly, there are no private algorithm for these\nsettings (except when a matrix is updated row by row). We present the first set\nof differentially private algorithms for all these settings.\n  Our algorithms when private matrix is updated in an arbitrary manner promise\ndifferential privacy with respect to two stronger privacy guarantees than\npreviously studied, use space and time {\\em comparable} to the non-private\nalgorithm, and achieve {\\em optimal accuracy}. To complement our positive\nresults, we also prove that the space required by our algorithms is optimal up\nto logarithmic factors. When data matrices are distributed over multiple\nservers, we give a non-interactive differentially private algorithm with\ncommunication cost independent of dimension. In concise, we give algorithms\nthat incur optimal cost. We also perform experiments to verify that all our\nalgorithms perform well in practice and outperform the best known algorithms\nuntil now for large range of parameters. We give experimental results for total\napproximation error and additive error for varying dimensions, $\\alpha$ and\n$k$. \n\n"}
{"id": "1604.01839", "contents": "Title: Clustering Via Crowdsourcing Abstract: In recent years, crowdsourcing, aka human aided computation has emerged as an\neffective platform for solving problems that are considered complex for\nmachines alone. Using human is time-consuming and costly due to monetary\ncompensations. Therefore, a crowd based algorithm must judiciously use any\ninformation computed through an automated process, and ask minimum number of\nquestions to the crowd adaptively.\n  One such problem which has received significant attention is {\\em entity\nresolution}. Formally, we are given a graph $G=(V,E)$ with unknown edge set $E$\nwhere $G$ is a union of $k$ (again unknown, but typically large $O(n^\\alpha)$,\nfor $\\alpha>0$) disjoint cliques $G_i(V_i, E_i)$, $i =1, \\dots, k$. The goal is\nto retrieve the sets $V_i$s by making minimum number of pair-wise queries $V\n\\times V\\to\\{\\pm1\\}$ to an oracle (the crowd). When the answer to each query is\ncorrect, e.g. via resampling, then this reduces to finding connected components\nin a graph. On the other hand, when crowd answers may be incorrect, it\ncorresponds to clustering over minimum number of noisy inputs. Even, with\nperfect answers, a simple lower and upper bound of $\\Theta(nk)$ on query\ncomplexity can be shown. A major contribution of this paper is to reduce the\nquery complexity to linear or even sublinear in $n$ when mild side information\nis provided by a machine, and even in presence of crowd errors which are not\ncorrectable via resampling. We develop new information theoretic lower bounds\non the query complexity of clustering with side information and errors, and our\nupper bounds closely match with them. Our algorithms are naturally\nparallelizable, and also give near-optimal bounds on the number of adaptive\nrounds required to match the query complexity. \n\n"}
{"id": "1604.02486", "contents": "Title: The Salesman's Improved Paths: 3/2+1/34 Integrality Gap and\n  Approximation Ratio Abstract: We give a new, strongly polynomial-time algorithm and improved analysis for\nthe metric $s-t$ path TSP. It finds a tour of cost less than 1.53 times the\noptimum of the subtour elimination LP, while known examples show that 1.5 is a\nlower bound for the integrality gap.\n  A key new idea is the deletion of some edges of Christofides' trees, which is\nthen accompanied by novel arguments of the analysis: edge-deletion disconnects\nthe trees, which are then partly reconnected by `parity correction'. We show\nthat the arising `connectivity correction' can be achieved for a minor extra\ncost.\n  On the one hand this algorithm and analysis extend previous tools such as the\nbest-of-many Christofides algorithm. On the other hand, powerful new tools are\nsolicited, such as a flow problem for analyzing the reconnection cost, and the\nconstruction of a set of more and more restrictive spanning trees, each of\nwhich can still be found by the greedy algorithm. We show that these trees can\nreplace the convex combination of spanning trees in the best-of-may\nChristofides algorithm.\n  These new methods lead to improving the integrality ratio and approximation\nguarantee below 1.53, as it is already sketched in the preliminary shortened\nversion of this article that appeared in FOCS 2016. The algorithm and analysis\nhave been significantly simplified in the current article, and details of\nproofs and explanations have been added. \n\n"}
{"id": "1604.04827", "contents": "Title: h-Index Manipulation by Undoing Merges Abstract: The h-index is an important bibliographic measure used to assess the\nperformance of researchers. Dutiful researchers merge different versions of\ntheir articles in their Google Scholar profile even though this can decrease\ntheir h-index. In this article, we study the manipulation of the h-index by\nundoing such merges. In contrast to manipulation by merging articles (van\nBevern et al. [Artif. Intel. 240:19-35, 2016]) such manipulation is harder to\ndetect. We present numerous results on computational complexity (from\nlinear-time algorithms to parameterized computational hardness results) and\nempirically indicate that at least small improvements of the h-index by\nsplitting merged articles are unfortunately easily achievable. \n\n"}
{"id": "1604.06707", "contents": "Title: Loopless Gray Code Enumeration and the Tower of Bucharest Abstract: We give new algorithms for generating all n-tuples over an alphabet of m\nletters, changing only one letter at a time (Gray codes). These algorithms are\nbased on the connection with variations of the Towers of Hanoi game. Our\nalgorithms are loopless, in the sense that the next change can be determined in\na constant number of steps, and they can be implemented in hardware. We also\ngive another family of loopless algorithms that is based on the idea of working\nahead and saving the work in a buffer. \n\n"}
{"id": "1604.07467", "contents": "Title: Estimating Weighted Matchings in $o(n)$ Space Abstract: We consider the problem of estimating the weight of a maximum weighted\nmatching of a weighted graph $G(V,E)$ whose edges are revealed in a streaming\nfashion. We develop a reduction from the maximum weighted matching problem to\nthe maximum cardinality matching problem that only doubles the approximation\nfactor of a streaming algorithm developed for the maximum cardinality matching\nproblem. Our results hold for the insertion-only and the dynamic (i.e,\ninsertion and deletion) edge-arrival streaming models. The previous best-known\nreduction is due to Bury and Schwiegelshohn (ESA 2015) who develop an algorithm\nwhose approximation guarantee scales by a polynomial factor.\n  As an application, we obtain improved estimators for weighted planar graphs\nand, more generally, for weighted bounded-arboricity graphs, by feeding into\nour reduction the recent estimators due to Esfandiari et al. (SODA 2015) and to\nChitnis et al. (SODA 2016). In particular, we obtain a\n$(48+\\epsilon)$-approximation estimator for the weight of a maximum weighted\nmatching in planar graphs. \n\n"}
{"id": "1604.07724", "contents": "Title: Assessing the Computational Complexity of Multi-Layer Subgraph Detection Abstract: Multi-layer graphs consist of several graphs (layers) over the same vertex\nset. They are motivated by real-world problems where entities (vertices) are\nassociated via multiple types of relationships (edges in different layers). We\nchart the border of computational (in)tractability for the class of subgraph\ndetection problems on multi-layer graphs, including fundamental problems such\nas maximum matching, finding certain clique relaxations (motivated by community\ndetection), or path problems. Mostly encountering hardness results, sometimes\neven for two or three layers, we can also spot some islands of tractability. \n\n"}
{"id": "1605.00058", "contents": "Title: Strongly Refuting Random CSPs Below the Spectral Threshold Abstract: Random constraint satisfaction problems (CSPs) are known to exhibit threshold\nphenomena: given a uniformly random instance of a CSP with $n$ variables and\n$m$ clauses, there is a value of $m = \\Omega(n)$ beyond which the CSP will be\nunsatisfiable with high probability. Strong refutation is the problem of\ncertifying that no variable assignment satisfies more than a constant fraction\nof clauses; this is the natural algorithmic problem in the unsatisfiable regime\n(when $m/n = \\omega(1)$).\n  Intuitively, strong refutation should become easier as the clause density\n$m/n$ grows, because the contradictions introduced by the random clauses become\nmore locally apparent. For CSPs such as $k$-SAT and $k$-XOR, there is a\nlong-standing gap between the clause density at which efficient strong\nrefutation algorithms are known, $m/n \\ge \\widetilde O(n^{k/2-1})$, and the\nclause density at which instances become unsatisfiable with high probability,\n$m/n = \\omega (1)$.\n  In this paper, we give spectral and sum-of-squares algorithms for strongly\nrefuting random $k$-XOR instances with clause density $m/n \\ge \\widetilde\nO(n^{(k/2-1)(1-\\delta)})$ in time $\\exp(\\widetilde O(n^{\\delta}))$ or in\n$\\widetilde O(n^{\\delta})$ rounds of the sum-of-squares hierarchy, for any\n$\\delta \\in [0,1)$ and any integer $k \\ge 3$. Our algorithms provide a smooth\ntransition between the clause density at which polynomial-time algorithms are\nknown at $\\delta = 0$, and brute-force refutation at the satisfiability\nthreshold when $\\delta = 1$. We also leverage our $k$-XOR results to obtain\nstrong refutation algorithms for SAT (or any other Boolean CSP) at similar\nclause densities. Our algorithms match the known sum-of-squares lower bounds\ndue to Grigoriev and Schonebeck, up to logarithmic factors.\n  Additionally, we extend our techniques to give new results for certifying\nupper bounds on the injective tensor norm of random tensors. \n\n"}
{"id": "1605.01079", "contents": "Title: Newer method of string comparison: the Modified Moving Contracting\n  Window Pattern Algorithm Abstract: This paper presents a new algorithm, the Modified Moving Contracting Window\nPattern Algorithm (CMCWPM), for the calculation of field similarity. It\nstrongly relies on previous work by Yang et al. (2001), correcting previous\nwork in which characters marked as inaccessible for further pattern matching\nwere not treated as boundaries between subfields, occasionally leading to\nhigher than expected scores of field similarity. A reference Python\nimplementation is provided. \n\n"}
{"id": "1605.01717", "contents": "Title: Negative-Weight Shortest Paths and Unit Capacity Minimum Cost Flow in\n  $\\tilde{O}(m^{10/7} \\log W)$ Time Abstract: In this paper, we study a set of combinatorial optimization problems on\nweighted graphs: the shortest path problem with negative weights, the weighted\nperfect bipartite matching problem, the unit-capacity minimum-cost maximum flow\nproblem and the weighted perfect bipartite $b$-matching problem under the\nassumption that $\\Vert b\\Vert_1=O(m)$. We show that each one of these four\nproblems can be solved in $\\tilde{O}(m^{10/7}\\log W)$ time, where $W$ is the\nabsolute maximum weight of an edge in the graph, which gives the first in over\n25 years polynomial improvement in their sparse-graph time complexity.\n  At a high level, our algorithms build on the interior-point method-based\nframework developed by Madry (FOCS 2013) for solving unit-capacity maximum flow\nproblem. We develop a refined way to analyze this framework, as well as provide\nnew variants of the underlying preconditioning and perturbation techniques.\nConsequently, we are able to extend the whole interior-point method-based\napproach to make it applicable in the weighted graph regime. \n\n"}
{"id": "1605.02882", "contents": "Title: An Algorithm for Koml\\'os Conjecture Matching Banaszczyk's bound Abstract: We consider the problem of finding a low discrepancy coloring for sparse set\nsystems where each element lies in at most t sets. We give an efficient\nalgorithm that finds a coloring with discrepancy O((t log n)^{1/2}), matching\nthe best known non-constructive bound for the problem due to Banaszczyk. The\nprevious algorithms only achieved an O(t^{1/2} log n) bound. The result also\nextends to the more general Koml\\'{o}s setting and gives an algorithmic\nO(log^{1/2} n) bound. \n\n"}
{"id": "1605.03266", "contents": "Title: A Quantum Approach to the Unique Sink Orientation Problem Abstract: We consider quantum algorithms for the unique sink orientation problem on\ncubes. This problem is widely considered to be of intermediate computational\ncomplexity. This is because there no known polynomial algorithm (classical or\nquantum) from the problem and yet it arrises as part of a series of problems\nfor which it being intractable would imply complexity theoretic collapses. We\ngive a reduction which proves that if one can efficiently evaluate the kth\npower of the unique sink orientation outmap, then there exists a polynomial\ntime quantum algorithm for the unique sink orientation problem on cubes. \n\n"}
{"id": "1605.06402", "contents": "Title: Ristretto: Hardware-Oriented Approximation of Convolutional Neural\n  Networks Abstract: Convolutional neural networks (CNN) have achieved major breakthroughs in\nrecent years. Their performance in computer vision have matched and in some\nareas even surpassed human capabilities. Deep neural networks can capture\ncomplex non-linear features; however this ability comes at the cost of high\ncomputational and memory requirements. State-of-art networks require billions\nof arithmetic operations and millions of parameters. To enable embedded devices\nsuch as smartphones, Google glasses and monitoring cameras with the astonishing\npower of deep learning, dedicated hardware accelerators can be used to decrease\nboth execution time and power consumption. In applications where fast\nconnection to the cloud is not guaranteed or where privacy is important,\ncomputation needs to be done locally. Many hardware accelerators for deep\nneural networks have been proposed recently. A first important step of\naccelerator design is hardware-oriented approximation of deep networks, which\nenables energy-efficient inference. We present Ristretto, a fast and automated\nframework for CNN approximation. Ristretto simulates the hardware arithmetic of\na custom hardware accelerator. The framework reduces the bit-width of network\nparameters and outputs of resource-intense layers, which reduces the chip area\nfor multiplication units significantly. Alternatively, Ristretto can remove the\nneed for multipliers altogether, resulting in an adder-only arithmetic. The\ntool fine-tunes trimmed networks to achieve high classification accuracy. Since\ntraining of deep neural networks can be time-consuming, Ristretto uses highly\noptimized routines which run on the GPU. This enables fast compression of any\ngiven network. Given a maximum tolerance of 1%, Ristretto can successfully\ncondense CaffeNet and SqueezeNet to 8-bit. The code for Ristretto is available. \n\n"}
{"id": "1605.06636", "contents": "Title: Deep Transfer Learning with Joint Adaptation Networks Abstract: Deep networks have been successfully applied to learn transferable features\nfor adapting models from a source domain to a different target domain. In this\npaper, we present joint adaptation networks (JAN), which learn a transfer\nnetwork by aligning the joint distributions of multiple domain-specific layers\nacross domains based on a joint maximum mean discrepancy (JMMD) criterion.\nAdversarial training strategy is adopted to maximize JMMD such that the\ndistributions of the source and target domains are made more distinguishable.\nLearning can be performed by stochastic gradient descent with the gradients\ncomputed by back-propagation in linear-time. Experiments testify that our model\nyields state of the art results on standard datasets. \n\n"}
{"id": "1605.06702", "contents": "Title: On cap sets and the group-theoretic approach to matrix multiplication Abstract: In 2003, Cohn and Umans described a framework for proving upper bounds on the\nexponent $\\omega$ of matrix multiplication by reducing matrix multiplication to\ngroup algebra multiplication, and in 2005 Cohn, Kleinberg, Szegedy, and Umans\nproposed specific conjectures for how to obtain $\\omega=2$. In this paper we\nrule out obtaining $\\omega=2$ in this framework from abelian groups of bounded\nexponent. To do this we bound the size of tricolored sum-free sets in such\ngroups, extending the breakthrough results of Croot, Lev, Pach, Ellenberg, and\nGijswijt on cap sets. As a byproduct of our proof, we show that a variant of\ntensor rank due to Tao gives a quantitative understanding of the notion of\nunstable tensor from geometric invariant theory. \n\n"}
{"id": "1605.06852", "contents": "Title: The Greedy Spanner is Existentially Optimal Abstract: The greedy spanner is arguably the simplest and most well-studied spanner\nconstruction. Experimental results demonstrate that it is at least as good as\nany other spanner construction, in terms of both the size and weight\nparameters. However, a rigorous proof for this statement has remained elusive.\n  In this work we fill in the theoretical gap via a surprisingly simple\nobservation: The greedy spanner is \\emph{existentially optimal} (or\nexistentially near-optimal) for several important graph families, in terms of\nboth the size and weight. Roughly speaking, the greedy spanner is said to be\nexistentially optimal (or near-optimal) for a graph family $\\mathcal G$ if the\nworst performance of the greedy spanner over all graphs in $\\mathcal G$ is just\nas good (or nearly as good) as the worst performance of an optimal spanner over\nall graphs in $\\mathcal G$.\n  Focusing on the weight parameter, the state-of-the-art spanner constructions\nfor both general graphs (due to Chechik and Wulff-Nilsen [SODA'16]) and\ndoubling metrics (due to Gottlieb [FOCS'15]) are complex. Plugging our\nobservation on these results, we conclude that the greedy spanner achieves\nnear-optimal weight guarantees for both general graphs and doubling metrics,\nthus resolving two longstanding conjectures in the area.\n  Further, we observe that approximate-greedy spanners are existentially\nnear-optimal as well. Consequently, we provide an $O(n \\log n)$-time\nconstruction of $(1+\\epsilon)$-spanners for doubling metrics with constant\nlightness and degree. Our construction improves Gottlieb's construction, whose\nruntime is $O(n \\log^2 n)$ and whose number of edges and degree are unbounded,\nand remarkably, it matches the state-of-the-art Euclidean result (due to\nGudmundsson et al.\\ [SICOMP'02]) in all the involved parameters (up to\ndependencies on $\\epsilon$ and the dimension). \n\n"}
{"id": "1605.06855", "contents": "Title: Smart broadcasting: Do you want to be seen? Abstract: Many users in online social networks are constantly trying to gain attention\nfrom their followers by broadcasting posts to them. These broadcasters are\nlikely to gain greater attention if their posts can remain visible for a longer\nperiod of time among their followers' most recent feeds. Then when to post? In\nthis paper, we study the problem of smart broadcasting using the framework of\ntemporal point processes, where we model users feeds and posts as discrete\nevents occurring in continuous time. Based on such continuous-time model, then\nchoosing a broadcasting strategy for a user becomes a problem of designing the\nconditional intensity of her posting events. We derive a novel formula which\nlinks this conditional intensity with the visibility of the user in her\nfollowers' feeds. Furthermore, by exploiting this formula, we develop an\nefficient convex optimization framework for the when-to-post problem. Our\nmethod can find broadcasting strategies that reach a desired visibility level\nwith provable guarantees. We experimented with data gathered from Twitter, and\nshow that our framework can consistently make broadcasters' post more visible\nthan alternatives. \n\n"}
{"id": "1605.07084", "contents": "Title: Low-Sensitivity Functions from Unambiguous Certificates Abstract: We provide new query complexity separations against sensitivity for total\nBoolean functions: a power $3$ separation between deterministic (and even\nrandomized or quantum) query complexity and sensitivity, and a power $2.22$\nseparation between certificate complexity and sensitivity. We get these\nseparations by using a new connection between sensitivity and a seemingly\nunrelated measure called one-sided unambiguous certificate complexity\n($UC_{min}$). We also show that $UC_{min}$ is lower-bounded by fractional block\nsensitivity, which means we cannot use these techniques to get a\nsuper-quadratic separation between $bs(f)$ and $s(f)$. We also provide a\nquadratic separation between the tree-sensitivity and decision tree complexity\nof Boolean functions, disproving a conjecture of Gopalan, Servedio, Tal, and\nWigderson (CCC 2016).\n  Along the way, we give a power $1.22$ separation between certificate\ncomplexity and one-sided unambiguous certificate complexity, improving the\npower $1.128$ separation due to G\\\"o\\\"os (FOCS 2015). As a consequence, we\nobtain an improved $\\Omega(\\log^{1.22} n)$ lower-bound on the\nco-nondeterministic communication complexity of the Clique vs. Independent Set\nproblem. \n\n"}
{"id": "1605.07422", "contents": "Title: Computing Web-scale Topic Models using an Asynchronous Parameter Server Abstract: Topic models such as Latent Dirichlet Allocation (LDA) have been widely used\nin information retrieval for tasks ranging from smoothing and feedback methods\nto tools for exploratory search and discovery. However, classical methods for\ninferring topic models do not scale up to the massive size of today's publicly\navailable Web-scale data sets. The state-of-the-art approaches rely on custom\nstrategies, implementations and hardware to facilitate their asynchronous,\ncommunication-intensive workloads.\n  We present APS-LDA, which integrates state-of-the-art topic modeling with\ncluster computing frameworks such as Spark using a novel asynchronous parameter\nserver. Advantages of this integration include convenient usage of existing\ndata processing pipelines and eliminating the need for disk writes as data can\nbe kept in memory from start to finish. Our goal is not to outperform highly\ncustomized implementations, but to propose a general high-performance topic\nmodeling framework that can easily be used in today's data processing\npipelines. We compare APS-LDA to the existing Spark LDA implementations and\nshow that our system can, on a 480-core cluster, process up to 135 times more\ndata and 10 times more topics without sacrificing model quality. \n\n"}
{"id": "1605.07503", "contents": "Title: A novel algorithm for solving the Decision Boolean Satisfiability\n  Problem without algebra Abstract: This paper depicts an algorithm for solving the Decision Boolean\nSatisfiability Problem using the binary numerical properties of a Special\nDecision Satisfiability Problem, parallel execution, object oriented, and short\ntermination. The two operations: expansion and simplification are used to\nexplains why using algebra grows the resolution steps. It is proved that its\ncomplexity has an upper bound of $2^{n-1}$ where $n$ is the number of logical\nvariables of the given problem. \n\n"}
{"id": "1605.08540", "contents": "Title: Induced Minor Free Graphs: Isomorphism and Clique-width Abstract: Given two graphs $G$ and $H$, we say that $G$ contains $H$ as an induced\nminor if a graph isomorphic to $H$ can be obtained from $G$ by a sequence of\nvertex deletions and edge contractions. We study the complexity of Graph\nIsomorphism on graphs that exclude a fixed graph as an induced minor. More\nprecisely, we determine for every graph $H$ that Graph Isomorphism is\npolynomial-time solvable on $H$-induced-minor-free graphs or that it is\nGI-complete. Additionally, we classify those graphs $H$ for which\n$H$-induced-minor-free graphs have bounded clique-width. These two results\ncomplement similar dichotomies for graphs that exclude a fixed graph as an\ninduced subgraph, minor, or subgraph. \n\n"}
{"id": "1605.09046", "contents": "Title: TripleSpin - a generic compact paradigm for fast machine learning\n  computations Abstract: We present a generic compact computational framework relying on structured\nrandom matrices that can be applied to speed up several machine learning\nalgorithms with almost no loss of accuracy. The applications include new fast\nLSH-based algorithms, efficient kernel computations via random feature maps,\nconvex optimization algorithms, quantization techniques and many more. Certain\nmodels of the presented paradigm are even more compressible since they apply\nonly bit matrices. This makes them suitable for deploying on mobile devices.\nAll our findings come with strong theoretical guarantees. In particular, as a\nbyproduct of the presented techniques and by using relatively new\nBerry-Esseen-type CLT for random vectors, we give the first theoretical\nguarantees for one of the most efficient existing LSH algorithms based on the\n$\\textbf{HD}_{3}\\textbf{HD}_{2}\\textbf{HD}_{1}$ structured matrix (\"Practical\nand Optimal LSH for Angular Distance\"). These guarantees as well as theoretical\nresults for other aforementioned applications follow from the same general\ntheoretical principle that we present in the paper. Our structured family\ncontains as special cases all previously considered structured schemes,\nincluding the recently introduced $P$-model. Experimental evaluation confirms\nthe accuracy and efficiency of TripleSpin matrices. \n\n"}
{"id": "1605.09721", "contents": "Title: CYCLADES: Conflict-free Asynchronous Machine Learning Abstract: We present CYCLADES, a general framework for parallelizing stochastic\noptimization algorithms in a shared memory setting. CYCLADES is asynchronous\nduring shared model updates, and requires no memory locking mechanisms, similar\nto HOGWILD!-type algorithms. Unlike HOGWILD!, CYCLADES introduces no conflicts\nduring the parallel execution, and offers a black-box analysis for provable\nspeedups across a large family of algorithms. Due to its inherent conflict-free\nnature and cache locality, our multi-core implementation of CYCLADES\nconsistently outperforms HOGWILD!-type algorithms on sufficiently sparse\ndatasets, leading to up to 40% speedup gains compared to the HOGWILD!\nimplementation of SGD, and up to 5x gains over asynchronous implementations of\nvariance reduction algorithms. \n\n"}
{"id": "1606.01316", "contents": "Title: Provable Burer-Monteiro factorization for a class of norm-constrained\n  matrix problems Abstract: We study the projected gradient descent method on low-rank matrix problems\nwith a strongly convex objective. We use the Burer-Monteiro factorization\napproach to implicitly enforce low-rankness; such factorization introduces\nnon-convexity in the objective. We focus on constraint sets that include both\npositive semi-definite (PSD) constraints and specific matrix norm-constraints.\nSuch criteria appear in quantum state tomography and phase retrieval\napplications.\n  We show that non-convex projected gradient descent favors local linear\nconvergence in the factored space. We build our theory on a novel descent\nlemma, that non-trivially extends recent results on the unconstrained problem.\nThe resulting algorithm is Projected Factored Gradient Descent, abbreviated as\nProjFGD, and shows superior performance compared to state of the art on quantum\nstate tomography and sparse phase retrieval applications. \n\n"}
{"id": "1606.02786", "contents": "Title: Maximum Selection and Sorting with Adversarial Comparators and an\n  Application to Density Estimation Abstract: We study maximum selection and sorting of $n$ numbers using pairwise\ncomparators that output the larger of their two inputs if the inputs are more\nthan a given threshold apart, and output an adversarially-chosen input\notherwise. We consider two adversarial models. A non-adaptive adversary that\ndecides on the outcomes in advance based solely on the inputs, and an adaptive\nadversary that can decide on the outcome of each query depending on previous\nqueries and outcomes.\n  Against the non-adaptive adversary, we derive a maximum-selection algorithm\nthat uses at most $2n$ comparisons in expectation, and a sorting algorithm that\nuses at most $2n \\ln n$ comparisons in expectation. These numbers are within\nsmall constant factors from the best possible. Against the adaptive adversary,\nwe propose a maximum-selection algorithm that uses $\\Theta(n\\log\n(1/{\\epsilon}))$ comparisons to output a correct answer with probability at\nleast $1-\\epsilon$. The existence of this algorithm affirmatively resolves an\nopen problem of Ajtai, Feldman, Hassadim, and Nelson.\n  Our study was motivated by a density-estimation problem where, given samples\nfrom an unknown underlying distribution, we would like to find a distribution\nin a known class of $n$ candidate distributions that is close to underlying\ndistribution in $\\ell_1$ distance. Scheffe's algorithm outputs a distribution\nat an $\\ell_1$ distance at most 9 times the minimum and runs in time\n$\\Theta(n^2\\log n)$. Using maximum selection, we propose an algorithm with the\nsame approximation guarantee but run time of $\\Theta(n\\log n)$. \n\n"}
{"id": "1606.03878", "contents": "Title: Device-independent dimension tests in the prepare-and-measure scenario Abstract: Analyzing the dimension of an unknown quantum system in a device-independent\nmanner, i.e., using only the measurement statistics, is a fundamental task in\nquantum physics and quantum information theory. In this paper, we consider this\nproblem in the prepare-and-measure scenario. Specifically, we provide a lower\nbound on the dimension of the prepared quantum systems which is a function that\nonly depends on the measurement statistics. Furthermore, we show that our bound\nperforms well on several examples. {In particular}, we show that our bound\nprovides new insights into the notion of dimension witness, and we also use it\nto show that the sets of restricted-dimensional prepare-and-measure\ncorrelations are not always convex. \n\n"}
{"id": "1606.05615", "contents": "Title: Guaranteed Non-convex Optimization: Submodular Maximization over\n  Continuous Domains Abstract: Submodular continuous functions are a category of (generally)\nnon-convex/non-concave functions with a wide spectrum of applications. We\ncharacterize these functions and demonstrate that they can be maximized\nefficiently with approximation guarantees. Specifically, i) We introduce the\nweak DR property that gives a unified characterization of submodularity for all\nset, integer-lattice and continuous functions; ii) for maximizing monotone\nDR-submodular continuous functions under general down-closed convex\nconstraints, we propose a Frank-Wolfe variant with $(1-1/e)$ approximation\nguarantee, and sub-linear convergence rate; iii) for maximizing general\nnon-monotone submodular continuous functions subject to box constraints, we\npropose a DoubleGreedy algorithm with $1/3$ approximation guarantee. Submodular\ncontinuous functions naturally find applications in various real-world\nsettings, including influence and revenue maximization with continuous\nassignments, sensor energy management, multi-resolution data summarization,\nfacility location, etc. Experimental results show that the proposed algorithms\nefficiently generate superior solutions compared to baseline algorithms. \n\n"}
{"id": "1606.08087", "contents": "Title: A width parameter useful for chordal and co-comparability graphs Abstract: We investigate new graph classes of bounded mim-width, strictly extending\ninterval graphs and permutation graphs. The graphs $K_t \\boxminus K_t$ and $K_t\n\\boxminus S_t$ are graphs obtained from the disjoint union of two cliques of\nsize $t$, and one clique of size $t$ and one independent set of size $t$\nrespectively, by adding a perfect matching. We prove that : (1) interval graphs\nare $(K_3\\boxminus S_3)$-free chordal graphs; and $(K_t\\boxminus S_t)$-free\nchordal graphs have mim-width at most $t-1$, (2) permutation graphs are\n$(K_3\\boxminus K_3)$-free co-comparability graphs; and $(K_t\\boxminus\nK_t)$-free co-comparability graphs have mim-width at most $t-1$, (3) chordal\ngraphs and co-comparability graphs have unbounded mim-width in general. We\nobtain several algorithmic consequences; for instance, while Minimum Dominating\nSet is NP-complete on chordal graphs, it can be solved in time\n$n^{\\mathcal{O}(t)}$ on $(K_t\\boxminus S_t)$-free chordal graphs. The third\nstatement strengthens a result of Belmonte and Vatshelle stating that either\nthose classes do not have constant mim-width or a decomposition with constant\nmim-width cannot be computed in polynomial time unless $P=NP$. We generalize\nthese ideas to bigger graph classes. We introduce a new width parameter\nsim-width, of stronger modelling power than mim-width, by making a small change\nin the definition of mim-width. We prove that chordal graphs and\nco-comparability graphs have sim-width at most 1. We investigate a way to bound\nmim-width for graphs of bounded sim-width by excluding $K_t\\boxminus K_t$ and\n$K_t\\boxminus S_t$ as induced minors or induced subgraphs, and give algorithmic\nconsequences. Lastly, we show that circle graphs have unbounded sim-width, and\nthus also unbounded mim-width. \n\n"}
{"id": "1606.09282", "contents": "Title: Learning without Forgetting Abstract: When building a unified vision system or gradually adding new capabilities to\na system, the usual assumption is that training data for all tasks is always\navailable. However, as the number of tasks grows, storing and retraining on\nsuch data becomes infeasible. A new problem arises where we add new\ncapabilities to a Convolutional Neural Network (CNN), but the training data for\nits existing capabilities are unavailable. We propose our Learning without\nForgetting method, which uses only new task data to train the network while\npreserving the original capabilities. Our method performs favorably compared to\ncommonly used feature extraction and fine-tuning adaption techniques and\nperforms similarly to multitask learning that uses original task data we assume\nunavailable. A more surprising observation is that Learning without Forgetting\nmay be able to replace fine-tuning with similar old and new task datasets for\nimproved new task performance. \n\n"}
{"id": "1606.09449", "contents": "Title: Clique-Width and Directed Width Measures for Answer-Set Programming Abstract: Disjunctive Answer Set Programming (ASP) is a powerful declarative\nprogramming paradigm whose main decision problems are located on the second\nlevel of the polynomial hierarchy. Identifying tractable fragments and\ndeveloping efficient algorithms for such fragments are thus important\nobjectives in order to complement the sophisticated ASP systems available to\ndate. Hard problems can become tractable if some problem parameter is bounded\nby a fixed constant; such problems are then called fixed-parameter tractable\n(FPT). While several FPT results for ASP exist, parameters that relate to\ndirected or signed graphs representing the program at hand have been neglected\nso far. In this paper, we first give some negative observations showing that\ndirected width measures on the dependency graph of a program do not lead to FPT\nresults. We then consider the graph parameter of signed clique-width and\npresent a novel dynamic programming algorithm that is FPT w.r.t. this\nparameter. Clique-width is more general than the well-known treewidth, and, to\nthe best of our knowledge, ours is the first FPT algorithm for bounded\nclique-width for reasoning problems beyond SAT. \n\n"}
{"id": "1607.01167", "contents": "Title: Deterministic polynomial-time approximation algorithms for partition\n  functions and graph polynomials Abstract: In this paper we show a new way of constructing deterministic polynomial-time\napproximation algorithms for computing complex-valued evaluations of a large\nclass of graph polynomials on bounded degree graphs. In particular, our\napproach works for the Tutte polynomial and independence polynomial, as well as\npartition functions of complex-valued spin and edge-coloring models.\n  More specifically, we define a large class of graph polynomials $\\mathcal C$\nand show that if $p\\in \\cal C$ and there is a disk $D$ centered at zero in the\ncomplex plane such that $p(G)$ does not vanish on $D$ for all bounded degree\ngraphs $G$, then for each $z$ in the interior of $D$ there exists a\ndeterministic polynomial-time approximation algorithm for evaluating $p(G)$ at\n$z$. This gives an explicit connection between absence of zeros of graph\npolynomials and the existence of efficient approximation algorithms, allowing\nus to show new relationships between well-known conjectures.\n  Our work builds on a recent line of work initiated by. Barvinok, which\nprovides a new algorithmic approach besides the existing Markov chain Monte\nCarlo method and the correlation decay method for these types of problems. \n\n"}
{"id": "1607.02184", "contents": "Title: Maximizing the Sum of Radii of Disjoint Balls or Disks Abstract: Finding nonoverlapping balls with given centers in any metric space,\nmaximizing the sum of radii of the balls, can be expressed as a linear program.\nIts dual linear program expresses the problem of finding a minimum-weight set\nof cycles (allowing 2-cycles) covering all vertices in a complete geometric\ngraph. For points in a Euclidean space of any finite dimension~$d$, with any\nconvex distance function on this space, this graph can be replaced by a sparse\nsubgraph obeying a separator theorem. This graph structure leads to an\nalgorithm for finding the optimum set of balls in time $O(n^{2-1/d})$,\nimproving the $O(n^3)$ time of a naive cycle cover algorithm. As a subroutine,\nwe provide an algorithm for weighted bipartite matching in graphs with\nseparators, which speeds up the best previous algorithm for this problem on\nplanar bipartite graphs from $O(n^{3/2}\\log n)$ to $O(n^{3/2})$ time. We also\nshow how to constrain the balls to all have radius at least a given threshold\nvalue, and how to apply our radius-sum optimization algorithms to the problem\nof embedding a finite metric space into a star metric minimizing the average\ndistance to the hub. \n\n"}
{"id": "1607.02922", "contents": "Title: Characterization and recognition of proper tagged probe interval graphs Abstract: Interval graphs were used in the study of genomics by the famous molecular\nbiologist Benzer. Later on probe interval graphs were introduced by Zhang as a\ngeneralization of interval graphs for the study of cosmid contig mapping of\nDNA.\n  A tagged probe interval graph (briefly, TPIG) is motivated by similar\napplications to genomics, where the set of vertices is partitioned into two\nsets, namely, probes and nonprobes and there is an interval on the real line\ncorresponding to each vertex. The graph has an edge between two probe vertices\nif their corresponding intervals intersect, has an edge between a probe vertex\nand a nonprobe vertex if the interval corresponding to a nonprobe vertex\ncontains at least one end point of the interval corresponding to a probe vertex\nand the set of non-probe vertices is an independent set. This class of graphs\nhave been defined nearly two decades ago, but till today there is no known\nrecognition algorithm for it.\n  In this paper, we consider a natural subclass of TPIG, namely, the class of\nproper tagged probe interval graphs (in short PTPIG). We present\ncharacterization and a linear time recognition algorithm for PTPIG. To obtain\nthis characterization theorem we introduce a new concept called canonical\nsequence for proper interval graphs, which, we belief, has an independent\ninterest in the study of proper interval graphs. Also to obtain the recognition\nalgorithm for PTPIG, we introduce and solve a variation of consecutive $1$'s\nproblem, namely, oriented consecutive $1$'s problem and some variations of\nPQ-tree algorithm. We also discuss the interrelations between the classes of\nPTPIG and TPIG with probe interval graphs and probe proper interval graphs. \n\n"}
{"id": "1607.04431", "contents": "Title: Edge-Orders Abstract: Canonical orderings and their relatives such as st-numberings have been used\nas a key tool in algorithmic graph theory for the last decades. Recently, a\nunifying concept behind all these orders has been shown: they can be described\nby a graph decomposition into parts that have a prescribed vertex-connectivity.\n  Despite extensive interest in canonical orderings, no analogue of this\nunifying concept is known for edge-connectivity. In this paper, we establish\nsuch a concept named edge-orders and show how to compute (1,1)-edge-orders of\n2-edge-connected graphs as well as (2,1)-edge-orders of 3-edge-connected graphs\nin linear time, respectively. While the former can be seen as the edge-variants\nof st-numberings, the latter are the edge-variants of Mondshein sequences and\nnon-separating ear decompositions. The methods that we use for obtaining such\nedge-orders differ considerably in almost all details from the ones used for\ntheir vertex-counterparts, as different graph-theoretic constructions are used\nin the inductive proof and standard reductions from edge- to\nvertex-connectivity are bound to fail.\n  As a first application, we consider the famous Edge-Independent Spanning Tree\nConjecture, which asserts that every k-edge-connected graph contains k rooted\nspanning trees that are pairwise edge-independent. We illustrate the impact of\nthe above edge-orders by deducing algorithms that construct 2- and 3-edge\nindependent spanning trees of 2- and 3-edge-connected graphs, the latter of\nwhich improves the best known running time from O(n^2) to linear time. \n\n"}
{"id": "1607.04940", "contents": "Title: An optimization approach to locally-biased graph algorithms Abstract: Locally-biased graph algorithms are algorithms that attempt to find local or\nsmall-scale structure in a large data graph. In some cases, this can be\naccomplished by adding some sort of locality constraint and calling a\ntraditional graph algorithm; but more interesting are locally-biased graph\nalgorithms that compute answers by running a procedure that does not even look\nat most of the input graph. This corresponds more closely to what practitioners\nfrom various data science domains do, but it does not correspond well with the\nway that algorithmic and statistical theory is typically formulated. Recent\nwork from several research communities has focused on developing locally-biased\ngraph algorithms that come with strong complementary algorithmic and\nstatistical theory and that are useful in practice in downstream data science\napplications. We provide a review and overview of this work, highlighting\ncommonalities between seemingly-different approaches, and highlighting\npromising directions for future work. \n\n"}
{"id": "1607.05112", "contents": "Title: Minimum cycle and homology bases of surface embedded graphs Abstract: We study the problems of finding a minimum cycle basis (a minimum weight set\nof cycles that form a basis for the cycle space) and a minimum homology basis\n(a minimum weight set of cycles that generates the $1$-dimensional\n($\\mathbb{Z}_2$)-homology classes) of an undirected graph embedded on a\nsurface. The problems are closely related, because the minimum cycle basis of a\ngraph contains its minimum homology basis, and the minimum homology basis of\nthe $1$-skeleton of any graph is exactly its minimum cycle basis.\n  For the minimum cycle basis problem, we give a deterministic\n$O(n^\\omega+2^{2g}n^2+m)$-time algorithm for graphs embedded on an orientable\nsurface of genus $g$. The best known existing algorithms for surface embedded\ngraphs are those for general graphs: an $O(m^\\omega)$ time Monte Carlo\nalgorithm and a deterministic $O(nm^2/\\log n + n^2 m)$ time algorithm. For the\nminimum homology basis problem, we give a deterministic $O((g+b)^3 n \\log n +\nm)$-time algorithm for graphs embedded on an orientable or non-orientable\nsurface of genus $g$ with $b$ boundary components, assuming shortest paths are\nunique, improving on existing algorithms for many values of $g$ and $n$. The\nassumption of unique shortest paths can be avoided with high probability using\nrandomization or deterministically by increasing the running time of the\nhomology basis algorithm by a factor of $O(\\log n)$. \n\n"}
{"id": "1607.05527", "contents": "Title: An Approximation Algorithm for the Art Gallery Problem Abstract: Given a simple polygon $\\mathcal{P}$ on $n$ vertices, two points $x,y$ in\n$\\mathcal{P}$ are said to be visible to each other if the line segment between\n$x$ and $y$ is contained in $\\mathcal{P}$. The Point Guard Art Gallery problem\nasks for a minimum set $S$ such that every point in $\\mathcal{P}$ is visible\nfrom a point in $S$. The set $S$ is referred to as guards. Assuming integer\ncoordinates and a specific general position assumption, we present the first\n$O(\\log \\text{OPT})$-approximation algorithm for the point guard problem for\nsimple polygons. This algorithm combines ideas of a paper of Efrat and\nHar-Peled [Inf. Process. Lett. 2006] and Deshpande et. al. [WADS 2007]. We also\npoint out a mistake in the latter. \n\n"}
{"id": "1607.05597", "contents": "Title: Distributed Construction of Purely Additive Spanners Abstract: This paper studies the complexity of distributed construction of purely\nadditive spanners in the CONGEST model. We describe algorithms for building\nsuch spanners in several cases. Because of the need to simultaneously make\ndecisions at far apart locations, the algorithms use additional mechanisms\ncompared to their sequential counterparts.\n  We complement our algorithms with a lower bound on the number of rounds\nrequired for computing pairwise spanners. The standard reductions from\nset-disjointness and equality seem unsuitable for this task because no specific\nedge needs to be removed from the graph. Instead, to obtain our lower bound, we\ndefine a new communication complexity problem that reduces to computing a\nsparse spanner, and prove a lower bound on its communication complexity using\ninformation theory. This technique significantly extends the current toolbox\nused for obtaining lower bounds for the CONGEST model, and we believe it may\nfind additional applications. \n\n"}
{"id": "1607.06017", "contents": "Title: Doubly Accelerated Methods for Faster CCA and Generalized\n  Eigendecomposition Abstract: We study $k$-GenEV, the problem of finding the top $k$ generalized\neigenvectors, and $k$-CCA, the problem of finding the top $k$ vectors in\ncanonical-correlation analysis. We propose algorithms $\\mathtt{LazyEV}$ and\n$\\mathtt{LazyCCA}$ to solve the two problems with running times linearly\ndependent on the input size and on $k$.\n  Furthermore, our algorithms are DOUBLY-ACCELERATED: our running times depend\nonly on the square root of the matrix condition number, and on the square root\nof the eigengap. This is the first such result for both $k$-GenEV or $k$-CCA.\nWe also provide the first gap-free results, which provide running times that\ndepend on $1/\\sqrt{\\varepsilon}$ rather than the eigengap. \n\n"}
{"id": "1607.07676", "contents": "Title: Complexity of Token Swapping and its Variants Abstract: In the Token Swapping problem we are given a graph with a token placed on\neach vertex. Each token has exactly one destination vertex, and we try to move\nall the tokens to their destinations, using the minimum number of swaps, i.e.,\noperations of exchanging the tokens on two adjacent vertices. As the main\nresult of this paper, we show that Token Swapping is $W[1]$-hard parameterized\nby the length $k$ of a shortest sequence of swaps. In fact, we prove that, for\nany computable function $f$, it cannot be solved in time $f(k)n^{o(k / \\log\nk)}$ where $n$ is the number of vertices of the input graph, unless the ETH\nfails. This lower bound almost matches the trivial $n^{O(k)}$-time algorithm.\n  We also consider two generalizations of the Token Swapping, namely Colored\nToken Swapping (where the tokens have different colors and tokens of the same\ncolor are indistinguishable), and Subset Token Swapping (where each token has a\nset of possible destinations). To complement the hardness result, we prove that\neven the most general variant, Subset Token Swapping, is FPT in nowhere-dense\ngraph classes.\n  Finally, we consider the complexities of all three problems in very\nrestricted classes of graphs: graphs of bounded treewidth and diameter, stars,\ncliques, and paths, trying to identify the borderlines between polynomial and\nNP-hard cases. \n\n"}
{"id": "1607.07837", "contents": "Title: First Efficient Convergence for Streaming k-PCA: a Global, Gap-Free, and\n  Near-Optimal Rate Abstract: We study streaming principal component analysis (PCA), that is to find, in\n$O(dk)$ space, the top $k$ eigenvectors of a $d\\times d$ hidden matrix $\\bf\n\\Sigma$ with online vectors drawn from covariance matrix $\\bf \\Sigma$.\n  We provide $\\textit{global}$ convergence for Oja's algorithm which is\npopularly used in practice but lacks theoretical understanding for $k>1$. We\nalso provide a modified variant $\\mathsf{Oja}^{++}$ that runs $\\textit{even\nfaster}$ than Oja's. Our results match the information theoretic lower bound in\nterms of dependency on error, on eigengap, on rank $k$, and on dimension $d$,\nup to poly-log factors. In addition, our convergence rate can be made gap-free,\nthat is proportional to the approximation error and independent of the\neigengap.\n  In contrast, for general rank $k$, before our work (1) it was open to design\nany algorithm with efficient global convergence rate; and (2) it was open to\ndesign any algorithm with (even local) gap-free convergence rate in $O(dk)$\nspace. \n\n"}
{"id": "1607.08456", "contents": "Title: Kernel functions based on triplet comparisons Abstract: Given only information in the form of similarity triplets \"Object A is more\nsimilar to object B than to object C\" about a data set, we propose two ways of\ndefining a kernel function on the data set. While previous approaches construct\na low-dimensional Euclidean embedding of the data set that reflects the given\nsimilarity triplets, we aim at defining kernel functions that correspond to\nhigh-dimensional embeddings. These kernel functions can subsequently be used to\napply any kernel method to the data set. \n\n"}
{"id": "1607.08806", "contents": "Title: Trimming and gluing Gray codes Abstract: We consider the algorithmic problem of generating each subset of\n$[n]:=\\{1,2,\\ldots,n\\}$ whose size is in some interval $[k,l]$, $0\\leq k\\leq\nl\\leq n$, exactly once (cyclically) by repeatedly adding or removing a single\nelement, or by exchanging a single element. For $k=0$ and $l=n$ this is the\nclassical problem of generating all $2^n$ subsets of $[n]$ by element\nadditions/removals, and for $k=l$ this is the classical problem of generating\nall $\\binom{n}{k}$ subsets of $[n]$ by element exchanges. We prove the\nexistence of such cyclic minimum-change enumerations for a large range of\nvalues $n$, $k$, and $l$, improving upon and generalizing several previous\nresults. For all these existential results we provide optimal algorithms to\ncompute the corresponding Gray codes in constant $\\mathcal{O}(1)$ time per\ngenerated set and $\\mathcal{O}(n)$ space. Rephrased in terms of graph theory,\nour results establish the existence of (almost) Hamilton cycles in the subgraph\nof the $n$-dimensional cube $Q_n$ induced by all levels $[k,l]$. We reduce all\nremaining open cases to a generalized version of the middle levels conjecture,\nwhich asserts that the subgraph of $Q_{2k+1}$ induced by all levels\n$[k-c,k+1+c]$, $c\\in\\{0,1,\\ldots,k\\}$, has a Hamilton cycle. We also prove an\napproximate version of this generalized conjecture, showing that this graph has\na cycle that visits a $(1-o(1))$-fraction of all vertices. \n\n"}
{"id": "1608.01292", "contents": "Title: Approximating set multi-covers Abstract: Johnson and Lov\\'asz and Stein proved independently that any hypergraph\nsatisfies $\\tau\\leq (1+\\ln \\Delta)\\tau^{\\ast}$, where $\\tau$ is the transversal\nnumber, $\\tau^{\\ast}$ is its fractional version, and $\\Delta$ denotes the\nmaximum degree. We prove $\\tau_f\\leq c \\tau^{\\ast}\\max\\{\\ln \\Delta, f\\}$ for\nthe $f$-fold transversal number $\\tau_f$. Similarly to Johnson, Lov\\'asz and\nStein, we also show that this bound can be achieved non-probabilistically,\nusing a greedy algorithm.\n  As a combinatorial application, we prove an estimate on how fast $\\tau_f/f$\nconverges to $\\tau^{\\ast}$. As a geometric application, we obtain an upper\nbound on the minimal density of an $f$-fold covering of the $d$-dimensional\nEuclidean space by translates of any convex body. \n\n"}
{"id": "1608.01628", "contents": "Title: Binarisation for Valued Constraint Satisfaction Problems Abstract: We study methods for transforming valued constraint satisfaction problems\n(VCSPs) to binary VCSPs. First, we show that the standard dual encoding\npreserves many aspects of the algebraic properties that capture the\ncomputational complexity of VCSPs. Second, we extend the reduction of CSPs to\nbinary CSPs described by Bulin et al. [LMCS'15] to VCSPs. This reduction\nestablishes that VCSPs over a fixed valued constraint language are\npolynomial-time equivalent to Minimum-Cost Homomorphism Problems over a fixed\ndigraph. \n\n"}
{"id": "1608.01747", "contents": "Title: A Distance for HMMs based on Aggregated Wasserstein Metric and State\n  Registration Abstract: We propose a framework, named Aggregated Wasserstein, for computing a\ndissimilarity measure or distance between two Hidden Markov Models with state\nconditional distributions being Gaussian. For such HMMs, the marginal\ndistribution at any time spot follows a Gaussian mixture distribution, a fact\nexploited to softly match, aka register, the states in two HMMs. We refer to\nsuch HMMs as Gaussian mixture model-HMM (GMM-HMM). The registration of states\nis inspired by the intrinsic relationship of optimal transport and the\nWasserstein metric between distributions. Specifically, the components of the\nmarginal GMMs are matched by solving an optimal transport problem where the\ncost between components is the Wasserstein metric for Gaussian distributions.\nThe solution of the optimization problem is a fast approximation to the\nWasserstein metric between two GMMs. The new Aggregated Wasserstein distance is\na semi-metric and can be computed without generating Monte Carlo samples. It is\ninvariant to relabeling or permutation of the states. This distance quantifies\nthe dissimilarity of GMM-HMMs by measuring both the difference between the two\nmarginal GMMs and the difference between the two transition matrices. Our new\ndistance is tested on the tasks of retrieval and classification of time series.\nExperiments on both synthetic data and real data have demonstrated its\nadvantages in terms of accuracy as well as efficiency in comparison with\nexisting distances based on the Kullback-Leibler divergence. \n\n"}
{"id": "1608.03023", "contents": "Title: Stochastic Rank-1 Bandits Abstract: We propose stochastic rank-$1$ bandits, a class of online learning problems\nwhere at each step a learning agent chooses a pair of row and column arms, and\nreceives the product of their values as a reward. The main challenge of the\nproblem is that the individual values of the row and column are unobserved. We\nassume that these values are stochastic and drawn independently. We propose a\ncomputationally-efficient algorithm for solving our problem, which we call\nRank1Elim. We derive a $O((K + L) (1 / \\Delta) \\log n)$ upper bound on its\n$n$-step regret, where $K$ is the number of rows, $L$ is the number of columns,\nand $\\Delta$ is the minimum of the row and column gaps; under the assumption\nthat the mean row and column rewards are bounded away from zero. To the best of\nour knowledge, we present the first bandit algorithm that finds the maximum\nentry of a rank-$1$ matrix whose regret is linear in $K + L$, $1 / \\Delta$, and\n$\\log n$. We also derive a nearly matching lower bound. Finally, we evaluate\nRank1Elim empirically on multiple problems. We observe that it leverages the\nstructure of our problems and can learn near-optimal solutions even if our\nmodeling assumptions are mildly violated. \n\n"}
{"id": "1608.03245", "contents": "Title: On the Complexity of Closest Pair via Polar-Pair of Point-Sets Abstract: Every graph $G$ can be represented by a collection of equi-radii spheres in a\n$d$-dimensional metric $\\Delta$ such that there is an edge $uv$ in $G$ if and\nonly if the spheres corresponding to $u$ and $v$ intersect. The smallest\ninteger $d$ such that $G$ can be represented by a collection of spheres (all of\nthe same radius) in $\\Delta$ is called the sphericity of $G$, and if the\ncollection of spheres are non-overlapping, then the value $d$ is called the\ncontact-dimension of $G$. In this paper, we study the sphericity and contact\ndimension of the complete bipartite graph $K_{n,n}$ in various $L^p$-metrics\nand consequently connect the complexity of the monochromatic closest pair and\nbichromatic closest pair problems. \n\n"}
{"id": "1608.03270", "contents": "Title: Faster Algorithms for Computing the Stationary Distribution, Simulating\n  Random Walks, and More Abstract: In this paper, we provide faster algorithms for computing various fundamental\nquantities associated with random walks on a directed graph, including the\nstationary distribution, personalized PageRank vectors, hitting times, and\nescape probabilities. In particular, on a directed graph with $n$ vertices and\n$m$ edges, we show how to compute each quantity in time\n$\\tilde{O}(m^{3/4}n+mn^{2/3})$, where the $\\tilde{O}$ notation suppresses\npolylogarithmic factors in $n$, the desired accuracy, and the appropriate\ncondition number (i.e. the mixing time or restart probability).\n  Our result improves upon the previous fastest running times for these\nproblems; previous results either invoke a general purpose linear system solver\non a $n\\times n$ matrix with $m$ non-zero entries, or depend polynomially on\nthe desired error or natural condition number associated with the problem (i.e.\nthe mixing time or restart probability). For sparse graphs, we obtain a running\ntime of $\\tilde{O}(n^{7/4})$, breaking the $O(n^{2})$ barrier of the best\nrunning time one could hope to achieve using fast matrix multiplication.\n  We achieve our result by providing a similar running time improvement for\nsolving directed Laplacian systems, a natural directed or asymmetric analog of\nthe well studied symmetric or undirected Laplacian systems. We show how to\nsolve such systems in time $\\tilde{O}(m^{3/4}n+mn^{2/3})$, and efficiently\nreduce a broad range of problems to solving $\\tilde{O}(1)$ directed Laplacian\nsystems on Eulerian graphs. We hope these results and our analysis open the\ndoor for further study into directed spectral graph theory. \n\n"}
{"id": "1608.03439", "contents": "Title: Finding Large Set Covers Faster via the Representation Method Abstract: The worst-case fastest known algorithm for the Set Cover problem on universes\nwith $n$ elements still essentially is the simple $O^*(2^n)$-time dynamic\nprogramming algorithm, and no non-trivial consequences of an $O^*(1.01^n)$-time\nalgorithm are known. Motivated by this chasm, we study the following natural\nquestion: Which instances of Set Cover can we solve faster than the simple\ndynamic programming algorithm? Specifically, we give a Monte Carlo algorithm\nthat determines the existence of a set cover of size $\\sigma n$ in\n$O^*(2^{(1-\\Omega(\\sigma^4))n})$ time. Our approach is also applicable to Set\nCover instances with exponentially many sets: By reducing the task of finding\nthe chromatic number $\\chi(G)$ of a given $n$-vertex graph $G$ to Set Cover in\nthe natural way, we show there is an $O^*(2^{(1-\\Omega(\\sigma^4))n})$-time\nrandomized algorithm that given integer $s=\\sigma n$, outputs NO if $\\chi(G) >\ns$ and YES with constant probability if $\\chi(G)\\leq s-1$.\n  On a high level, our results are inspired by the `representation method' of\nHowgrave-Graham and Joux~[EUROCRYPT'10] and obtained by only evaluating a\nrandomly sampled subset of the table entries of a dynamic programming\nalgorithm. \n\n"}
{"id": "1608.04036", "contents": "Title: Greedy Maximization Framework for Graph-based Influence Functions Abstract: The study of graph-based submodular maximization problems was initiated in a\nseminal work of Kempe, Kleinberg, and Tardos (2003): An {\\em influence}\nfunction of subsets of nodes is defined by the graph structure and the aim is\nto find subsets of seed nodes with (approximately) optimal tradeoff of size and\ninfluence. Applications include viral marketing, monitoring, and active\nlearning of node labels. This powerful formulation was studied for\n(generalized) {\\em coverage} functions, where the influence of a seed set on a\nnode is the maximum utility of a seed item to the node, and for pairwise {\\em\nutility} based on reachability, distances, or reverse ranks.\n  We define a rich class of influence functions which unifies and extends\nprevious work beyond coverage functions and specific utility functions. We\npresent a meta-algorithm for approximate greedy maximization with strong\napproximation quality guarantees and worst-case near-linear computation for all\nfunctions in our class. Our meta-algorithm generalizes a recent design by Cohen\net al (2014) that was specific for distance-based coverage functions. \n\n"}
{"id": "1608.04236", "contents": "Title: Generative and Discriminative Voxel Modeling with Convolutional Neural\n  Networks Abstract: When working with three-dimensional data, choice of representation is key. We\nexplore voxel-based models, and present evidence for the viability of\nvoxellated representations in applications including shape modeling and object\nclassification. Our key contributions are methods for training voxel-based\nvariational autoencoders, a user interface for exploring the latent space\nlearned by the autoencoder, and a deep convolutional neural network\narchitecture for object classification. We address challenges unique to\nvoxel-based representations, and empirically evaluate our models on the\nModelNet benchmark, where we demonstrate a 51.5% relative improvement in the\nstate of the art for object classification. \n\n"}
{"id": "1608.04829", "contents": "Title: Quantum Merlin-Arthur with noisy channel Abstract: What happens if in QMA the quantum channel between Merlin and Arthur is\nnoisy? It is not difficult to show that such a modification does not change the\ncomputational power as long as the noise is not too strong so that errors are\ncorrectable with high probability, since if Merlin encodes the witness state in\na quantum error-correction code and sends it to Arthur, Arthur can correct the\nerror caused by the noisy channel. If we further assume that Arthur can do only\nsingle-qubit measurements, however, the problem becomes nontrivial, since in\nthis case Arthur cannot do the universal quantum computation by himself. In\nthis paper, we show that such a restricted complexity class is still equivalent\nto QMA. To show it, we use measurement-based quantum computing: honest Merlin\nsends the graph state to Arthur, and Arthur does fault-tolerant\nmeasurement-based quantum computing on the noisy graph state with only\nsingle-qubit measurements. By measuring stabilizer operators, Arthur also\nchecks the correctness of the graph state. Although this idea itself was\nalready used in several previous papers, these results cannot be directly used\nto the present case, since the test that checks the graph state used in these\npapers is so strict that even honest Merlin is rejected with high probability\nif the channel is noisy. We therefore introduce a more relaxed test that can\naccept not only the ideal graph state but also noisy graph states that are\nerror-correctable. \n\n"}
{"id": "1608.05983", "contents": "Title: Inverting Variational Autoencoders for Improved Generative Accuracy Abstract: Recent advances in semi-supervised learning with deep generative models have\nshown promise in generalizing from small labeled datasets\n($\\mathbf{x},\\mathbf{y}$) to large unlabeled ones ($\\mathbf{x}$). In the case\nwhere the codomain has known structure, a large unfeatured dataset\n($\\mathbf{y}$) is potentially available. We develop a parameter-efficient, deep\nsemi-supervised generative model for the purpose of exploiting this untapped\ndata source. Empirical results show improved performance in disentangling\nlatent variable semantics as well as improved discriminative prediction on\nMartian spectroscopic and handwritten digit domains. \n\n"}
{"id": "1608.06253", "contents": "Title: Multi-Dueling Bandits and Their Application to Online Ranker Evaluation Abstract: New ranking algorithms are continually being developed and refined,\nnecessitating the development of efficient methods for evaluating these\nrankers. Online ranker evaluation focuses on the challenge of efficiently\ndetermining, from implicit user feedback, which ranker out of a finite set of\nrankers is the best. Online ranker evaluation can be modeled by dueling ban-\ndits, a mathematical model for online learning under limited feedback from\npairwise comparisons. Comparisons of pairs of rankers is performed by\ninterleaving their result sets and examining which documents users click on.\nThe dueling bandits model addresses the key issue of which pair of rankers to\ncompare at each iteration, thereby providing a solution to the\nexploration-exploitation trade-off. Recently, methods for simultaneously\ncomparing more than two rankers have been developed. However, the question of\nwhich rankers to compare at each iteration was left open. We address this\nquestion by proposing a generalization of the dueling bandits model that uses\nsimultaneous comparisons of an unrestricted number of rankers. We evaluate our\nalgorithm on synthetic data and several standard large-scale online ranker\nevaluation datasets. Our experimental results show that the algorithm yields\norders of magnitude improvement in performance compared to stateof- the-art\ndueling bandit algorithms. \n\n"}
{"id": "1608.06724", "contents": "Title: On the Sensitivity Complexity of $k$-Uniform Hypergraph Properties Abstract: In this paper we investigate the sensitivity complexity of hypergraph\nproperties. We present a $k$-uniform hypergraph property with sensitivity\ncomplexity $O(n^{\\lceil k/3\\rceil})$ for any $k\\geq3$, where $n$ is the number\nof vertices. Moreover, we can do better when $k\\equiv1$ (mod 3) by presenting a\n$k$-uniform hypergraph property with sensitivity $O(n^{\\lceil k/3\\rceil-1/2})$.\nThis result disproves a conjecture of Babai~\\cite{Babai}, which conjectures\nthat the sensitivity complexity of $k$-uniform hypergraph properties is at\nleast $\\Omega(n^{k/2})$. We also investigate the sensitivity complexity of\nother symmetric functions and show that for many classes of transitive Boolean\nfunctions the minimum achievable sensitivity complexity can be $O(N^{1/3})$,\nwhere $N$ is the number of variables. Finally, we give a lower bound for\nsensitivity of $k$-uniform hypergraph properties, which implies the {\\em\nsensitivity conjecture} of $k$-uniform hypergraph properties for any constant\n$k$. \n\n"}
{"id": "1608.06819", "contents": "Title: Pricing and Optimization in Shared Vehicle Systems: An Approximation\n  Framework Abstract: Optimizing shared vehicle systems (bike/scooter/car/ride-sharing) is more\nchallenging compared to traditional resource allocation settings due to the\npresence of \\emph{complex network externalities} -- changes in the\ndemand/supply at any location affect future supply throughout the system within\nshort timescales. These externalities are well captured by steady-state\nMarkovian models, which are therefore widely used to analyze such systems.\nHowever, using such models to design pricing and other control policies is\ncomputationally difficult since the resulting optimization problems are\nhigh-dimensional and non-convex.\n  To this end, we develop a \\emph{rigorous approximation framework} for shared\nvehicle systems, providing a unified approach for a wide range of controls\n(pricing, matching, rebalancing), objective functions (throughput, revenue,\nwelfare), and system constraints (travel-times, welfare benchmarks,\nposted-price constraints). Our approach is based on the analysis of natural\nconvex relaxations, and obtains as special cases existing approximate-optimal\npolicies for limited settings, asymptotic-optimality results, and heuristic\npolicies. The resulting guarantees are non-asymptotic and parametric, and\nprovide operational insights into the design of real-world systems. In\nparticular, for any shared vehicle system with $n$ stations and $m$ vehicles,\nour framework obtains an approximation ratio of $1+(n-1)/m$, which is\nparticularly meaningful when $m/n$, the average number of vehicles per station,\nis large, as is often the case in practice. \n\n"}
{"id": "1608.07251", "contents": "Title: Large-scale Collaborative Imaging Genetics Studies of Risk Genetic\n  Factors for Alzheimer's Disease Across Multiple Institutions Abstract: Genome-wide association studies (GWAS) offer new opportunities to identify\ngenetic risk factors for Alzheimer's disease (AD). Recently, collaborative\nefforts across different institutions emerged that enhance the power of many\nexisting techniques on individual institution data. However, a major barrier to\ncollaborative studies of GWAS is that many institutions need to preserve\nindividual data privacy. To address this challenge, we propose a novel\ndistributed framework, termed Local Query Model (LQM) to detect risk SNPs for\nAD across multiple research institutions. To accelerate the learning process,\nwe propose a Distributed Enhanced Dual Polytope Projection (D-EDPP) screening\nrule to identify irrelevant features and remove them from the optimization. To\nthe best of our knowledge, this is the first successful run of the\ncomputationally intensive model selection procedure to learn a consistent model\nacross different institutions without compromising their privacy while ranking\nthe SNPs that may collectively affect AD. Empirical studies are conducted on\n809 subjects with 5.9 million SNP features which are distributed across three\nindividual institutions. D-EDPP achieved a 66-fold speed-up by effectively\nidentifying irrelevant features. \n\n"}
{"id": "1608.07413", "contents": "Title: $\\chi$-bounds, operations and chords Abstract: A \\emph{long unichord} in a graph is an edge that is the unique chord of some\ncycle of length at least 5. A graph is \\emph{long-unichord-free} if it does not\ncontain any long-unichord. We prove a structure theorem for long-unichord-free\ngraph. We give an $O(n^4m)$-time algorithm to recognize them. We show that any\nlong-unichord-free graph $G$ can be colored with at most $O(\\omega^3)$ colors,\nwhere $\\omega$ is the maximum number of pairwise adjacent vertices in $G$. \n\n"}
{"id": "1608.07568", "contents": "Title: Graphic TSP in cubic graphs Abstract: We present a polynomial-time 9/7-approximation algorithm for the graphic TSP\nfor cubic graphs, which improves the previously best approximation factor of\n1.3 for 2-connected cubic graphs and drops the requirement of 2-connectivity at\nthe same time. To design our algorithm, we prove that every simple 2-connected\ncubic n-vertex graph contains a spanning closed walk of length at most 9n/7-1,\nand that such a walk can be found in polynomial time. \n\n"}
{"id": "1608.07647", "contents": "Title: Elementary polytopes with high lift-and-project ranks for strong\n  positive semidefinite operators Abstract: We consider operators acting on convex subsets of the unit hypercube. These\noperators are used in constructing convex relaxations of combinatorial\noptimization problems presented as a 0,1 integer programming problem or a 0,1\npolynomial optimization problem. Our focus is mostly on operators that, when\nexpressed as a lift-and-project operator, involve the use of semidefiniteness\nconstraints in the lifted space, including operators due to Lasserre and\nvariants of the Sherali--Adams and Bienstock--Zuckerberg operators. We study\nthe performance of these semidefinite-optimization-based lift-and-project\noperators on some elementary polytopes --- hypercubes that are chipped (at\nleast one vertex of the hypercube removed by intersection with a closed\nhalfspace) or cropped (all $2^n$ vertices of the hypercube removed by\nintersection with $2^n$ closed halfspaces) to varying degrees of severity\n$\\rho$. We prove bounds on $\\rho$ where these operators would perform badly on\nthe aforementioned examples. We also show that the integrality gap of the\nchipped hypercube is invariant under the application of several\nlift-and-project operators of varying strengths. \n\n"}
{"id": "1609.00048", "contents": "Title: Practical sketching algorithms for low-rank matrix approximation Abstract: This paper describes a suite of algorithms for constructing low-rank\napproximations of an input matrix from a random linear image of the matrix,\ncalled a sketch. These methods can preserve structural properties of the input\nmatrix, such as positive-semidefiniteness, and they can produce approximations\nwith a user-specified rank. The algorithms are simple, accurate, numerically\nstable, and provably correct. Moreover, each method is accompanied by an\ninformative error bound that allows users to select parameters a priori to\nachieve a given approximation quality. These claims are supported by numerical\nexperiments with real and synthetic data. \n\n"}
{"id": "1609.00090", "contents": "Title: Attribute Truss Community Search Abstract: Recently, community search over graphs has attracted significant attention\nand many algorithms have been developed for finding dense subgraphs from large\ngraphs that contain given query nodes. In applications such as analysis of\nprotein protein interaction (PPI) networks, citation graphs, and collaboration\nnetworks, nodes tend to have attributes. Unfortunately, previously developed\ncommunity search algorithms ignore these attributes and result in communities\nwith poor cohesion w.r.t. their node attributes. In this paper, we study the\nproblem of attribute-driven community search, that is, given an undirected\ngraph $G$ where nodes are associated with attributes, and an input query $Q$\nconsisting of nodes $V_q$ and attributes $W_q$, find the communities containing\n$V_q$, in which most community members are densely inter-connected and have\nsimilar attributes.\n  We formulate our problem of finding attributed truss communities (ATC), as\nfinding all connected and close k-truss subgraphs containing $V_q$, that are\nlocally maximal and have the largest attribute relevance score among such\nsubgraphs. We design a novel attribute relevance score function and establish\nits desirable properties. The problem is shown to be NP-hard. However, we\ndevelop an efficient greedy algorithmic framework, which finds a maximal\n$k$-truss containing $V_q$, and then iteratively removes the nodes with the\nleast popular attributes and shrinks the graph so as to satisfy community\nconstraints. We also build an elegant index to maintain the known $k$-truss\nstructure and attribute information, and propose efficient query processing\nalgorithms. Extensive experiments on large real-world networks with\nground-truth communities shows the efficiency and effectiveness of our proposed\nmethods. \n\n"}
{"id": "1609.00750", "contents": "Title: Predicting Signed Edges with $O(n^{1+o(1)} \\log{n})$ Queries Abstract: Social networks and interactions in social media involve both positive and\nnegative relationships. Signed graphs capture both types of relationships:\npositive edges correspond to pairs of \"friends\", and negative edges to pairs of\n\"foes\". The {\\em edge sign prediction problem}, which aims to predict whether\nan interaction between a pair of nodes will be positive or negative, is an\nimportant graph mining task for which many heuristics have recently been\nproposed \\cite{leskovec2010predicting,leskovec2010signed}.\n  Motivated by social balance theory, we model the edge sign prediction problem\nas a noisy correlation clustering problem with two clusters. We are allowed to\nquery each pair of nodes whether they belong to the same cluster or not, but\nthe answer to the query is corrupted with some probability $0<q<\\frac{1}{2}$.\nLet $c=\\frac{1}{2}-q$ be the gap. We provide an algorithm that recovers the\nclustering with high probability in the presence of noise for any constant gap\n$c$ with $O(n^{1+\\tfrac{1}{\\log\\log{n}}}\\log{n})$ queries. Our algorithm uses\nsimple breadth first search as its main algorithmic primitive. Finally, we\nprovide a novel generalization to $k \\geq 3$ clusters and prove that our\ntechniques can recover the clustering if the gap is constant in this\ngeneralized setting. \n\n"}
{"id": "1609.07780", "contents": "Title: Linear kernels for edge deletion problems to immersion-closed graph\n  classes Abstract: Suppose $\\mathcal{F}$ is a finite family of graphs. We consider the following\nmeta-problem, called $\\mathcal{F}$-Immersion Deletion: given a graph $G$ and\ninteger $k$, decide whether the deletion of at most $k$ edges of $G$ can result\nin a graph that does not contain any graph from $\\mathcal{F}$ as an immersion.\nThis problem is a close relative of the $\\mathcal{F}$-Minor Deletion problem\nstudied by Fomin et al. [FOCS 2012], where one deletes vertices in order to\nremove all minor models of graphs from $\\mathcal{F}$.\n  We prove that whenever all graphs from $\\mathcal{F}$ are connected and at\nleast one graph of $\\mathcal{F}$ is planar and subcubic, then the\n$\\mathcal{F}$-Immersion Deletion problem admits: a constant-factor\napproximation algorithm running in time $O(m^3 \\cdot n^3 \\cdot \\log m)$; a\nlinear kernel that can be computed in time $O(m^4 \\cdot n^3 \\cdot \\log m)$; and\na $O(2^{O(k)} + m^4 \\cdot n^3 \\cdot \\log m)$-time fixed-parameter algorithm,\nwhere $n,m$ count the vertices and edges of the input graph.\n  These results mirror the findings of Fomin et al. [FOCS 2012], who obtained a\nsimilar set of algorithmic results for $\\mathcal{F}$-Minor Deletion, under the\nassumption that at least one graph from $\\mathcal{F}$ is planar. An important\ndifference is that we are able to obtain a linear kernel for\n$\\mathcal{F}$-Immersion Deletion, while the exponent of the kernel of Fomin et\nal. for $\\mathcal{F}$-Minor Deletion depends heavily on the family\n$\\mathcal{F}$. In fact, this dependence is unavoidable under plausible\ncomplexity assumptions, as proven by Giannopoulou et al. [ICALP 2015]. This\nreveals that the kernelization complexity of $\\mathcal{F}$-Immersion Deletion\nis quite different than that of $\\mathcal{F}$-Minor Deletion. \n\n"}
{"id": "1610.00082", "contents": "Title: PTAS for Ordered Instances of Resource Allocation Problems with\n  Restrictions on Inclusions Abstract: We consider the problem of allocating a set $I$ of $m$ indivisible resources\n(items) to a set $P$ of $n$ customers (players) competing for the resources.\nEach resource $j \\in I$ has a same value $v_j > 0$ for a subset of customers\ninterested in $j$, and zero value for the remaining customers. The utility\nreceived by each customer is the sum of the values of the resources allocated\nto her. The goal is to find a feasible allocation of the resources to the\ninterested customers such that for the Max-Min allocation problem (Min-Max\nallocation problem) the minimum of the utilities (maximum of the utilities)\nreceived by the customers is maximized (minimized). The Max-Min allocation\nproblem is also known as the \\textit{Fair Allocation problem}, or the\n\\textit{Santa Claus problem}. The Min-Max allocation problem is the problem of\nScheduling on Unrelated Parallel Machines, and is also known as the $R \\, | \\,\n| C_{\\max}$ problem.\n  In this paper, we are interested in instances of the problem that admit a\nPolynomial Time Approximation Scheme (PTAS). We show that an ordering property\non the resources and the customers is important and paves the way for a PTAS.\nFor the Max-Min allocation problem, we start with instances of the problem that\ncan be viewed as a \\textit{convex bipartite graph}; a bipartite graph for which\nthere exists an ordering of the resources such that each customer is interested\nin (has a positive evaluation for) a set of \\textit{consecutive} resources. We\ndemonstrate a PTAS for the inclusion-free cases. This class of instances is\nequivalent to the class of bipartite permutation graphs. For the Min-Max\nallocation problem, we also obtain a PTAS for inclusion-free instances. These\ninstances are not only of theoretical interest but also have practical\napplications. \n\n"}
{"id": "1610.00353", "contents": "Title: $O(m^9)$ network flow LP model of the Assignment Problem polytope with\n  applications to hard combinatorial optimization problems Abstract: In this paper, we present a new, network flow LP model of the standard\nAssignment Problem (AP) polytope. The model is not meant to be competitive with\nexisting standard procedures for solving the AP, as its complexity order of\nsize is $O(m^9)$, where m is the number of assignments. However, it allows for\nhard combinatorial optimization problems (COPs) to be solved as Assignment\nProblems (APs), including, in particular, the Quadratic, Cubic, Quartic,\nQuintic, and Sextic Assignment Problems, as well as the Traveling Salesman\nProblem and many of its variations. Hence, in particular, the model re-affirms\n\"P = NP.\" Illustrations are provided for the Linear Assignment (LAP), Quadratic\nAssignment (QAP), and Traveling Salesman (TSP) problems. Issues pertaining to\nthe extended formulations \"barriers\" for the LP modeling of hard COPs are not\ndiscussed in this paper because the developments are focused on the Assignment\nProblem polytope only, and also the applicability/non-applicability of those\n\"barriers\" are thoroughly addressed in a separate paper* in which it is shown\nthat, in an optimization context, these \"barriers\" have no pertinence for a\nmodel which projects to the AP polytope, provided appropriate costs can be\nattached to the non-superfluous variables of the model. Hence, the issues of\nthe \"barriers\" are left out of this paper essentially for the sake of space.\n  *: Diaby, M., M. Karwan, and L. Sun [2024]. On modeling NP-Complete problems\nas polynomial-sized linear programs: Escaping/Side-stepping the \"barriers.\"\nAvailable at: arXiv:2304.07716 [cc.CC]. \n\n"}
{"id": "1610.00581", "contents": "Title: Time and Space Efficient Quantum Algorithms for Detecting Cycles and\n  Testing Bipartiteness Abstract: We study space and time efficient quantum algorithms for two graph problems\n-- deciding whether an $n$-vertex graph is a forest, and whether it is\nbipartite. Via a reduction to the s-t connectivity problem, we describe quantum\nalgorithms for deciding both properties in $\\tilde{O}(n^{3/2})$ time and using\n$O(\\log n)$ classical and quantum bits of storage in the adjacency matrix\nmodel. We then present quantum algorithms for deciding the two properties in\nthe adjacency array model, which run in time $\\tilde{O}(n\\sqrt{d_m})$ and also\nrequire $O(\\log n)$ space, where $d_m$ is the maximum degree of any vertex in\nthe input graph. \n\n"}
{"id": "1610.01132", "contents": "Title: A Non-generative Framework and Convex Relaxations for Unsupervised\n  Learning Abstract: We give a novel formal theoretical framework for unsupervised learning with\ntwo distinctive characteristics. First, it does not assume any generative model\nand based on a worst-case performance metric. Second, it is comparative, namely\nperformance is measured with respect to a given hypothesis class. This allows\nto avoid known computational hardness results and improper algorithms based on\nconvex relaxations. We show how several families of unsupervised learning\nmodels, which were previously only analyzed under probabilistic assumptions and\nare otherwise provably intractable, can be efficiently learned in our framework\nby convex optimization. \n\n"}
{"id": "1610.01861", "contents": "Title: Efficient Best-Response Computation for Strategic Network Formation\n  under Attack Abstract: Inspired by real world examples, e.g. the Internet, researchers have\nintroduced an abundance of strategic games to study natural phenomena in\nnetworks. Unfortunately, almost all of these games have the conceptual drawback\nof being computationally intractable, i.e. computing a best response strategy\nor checking if an equilibrium is reached is NP-hard. Thus, a main challenge in\nthe field is to find tractable realistic network formation models.\n  We address this challenge by investigating a very recently introduced model\nby Goyal et al. [WINE'16] which focuses on robust networks in the presence of a\nstrong adversary who attacks (and kills) nodes in the network and lets this\nattack spread virus-like to neighboring nodes and their neighbors. Our main\nresult is to establish that this natural model is one of the few exceptions\nwhich are both realistic and computationally tractable. In particular, we\nanswer an open question of Goyal et al. by providing an efficient algorithm for\ncomputing a best response strategy, which implies that deciding whether the\ngame has reached a Nash equilibrium can be done efficiently as well. Our\nalgorithm essentially solves the problem of computing a minimal connection to a\nnetwork which maximizes the reachability while hedging against severe attacks\non the network infrastructure and may thus be of independent interest. \n\n"}
{"id": "1610.03029", "contents": "Title: Lower bounds for CSP refutation by SDP hierarchies Abstract: For a $k$-ary predicate $P$, a random instance of CSP$(P)$ with $n$ variables\nand $m$ constraints is unsatisfiable with high probability when $m \\gg n$. The\nnatural algorithmic task in this regime is \\emph{refutation}: finding a proof\nthat a given random instance is unsatisfiable. Recent work of Allen et al.\nsuggests that the difficulty of refuting CSP$(P)$ using an SDP is determined by\na parameter $\\mathrm{cmplx}(P)$, the smallest $t$ for which there does not\nexist a $t$-wise uniform distribution over satisfying assignments to $P$. In\nparticular they show that random instances of CSP$(P)$ with $m \\gg\nn^{\\mathrm{cmplx(P)}/2}$ can be refuted efficiently using an SDP.\n  In this work, we give evidence that $n^{\\mathrm{cmplx}(P)/2}$ constraints are\nalso \\emph{necessary} for refutation using SDPs. Specifically, we show that if\n$P$ supports a $(t-1)$-wise uniform distribution over satisfying assignments,\nthen the Sherali-Adams$_+$ and Lov\\'{a}sz-Schrijver$_+$ SDP hierarchies cannot\nrefute a random instance of CSP$(P)$ in polynomial time for any $m \\leq\nn^{t/2-\\epsilon}$. \n\n"}
{"id": "1610.04317", "contents": "Title: Approximate Counting, the Lovasz Local Lemma and Inference in Graphical\n  Models Abstract: In this paper we introduce a new approach for approximately counting in\nbounded degree systems with higher-order constraints. Our main result is an\nalgorithm to approximately count the number of solutions to a CNF formula\n$\\Phi$ when the width is logarithmic in the maximum degree. This closes an\nexponential gap between the known upper and lower bounds.\n  Moreover our algorithm extends straightforwardly to approximate sampling,\nwhich shows that under Lov\\'asz Local Lemma-like conditions it is not only\npossible to find a satisfying assignment, it is also possible to generate one\napproximately uniformly at random from the set of all satisfying assignments.\nOur approach is a significant departure from earlier techniques in approximate\ncounting, and is based on a framework to bootstrap an oracle for computing\nmarginal probabilities on individual variables. Finally, we give an application\nof our results to show that it is algorithmically possible to sample from the\nposterior distribution in an interesting class of graphical models. \n\n"}
{"id": "1610.05825", "contents": "Title: NP-hard sets are not sparse unless P=NP: An exposition of a simple proof\n  of Mahaney's Theorem, with applications Abstract: Mahaney's Theorem states that, assuming $\\mathsf{P} \\neq \\mathsf{NP}$, no\nNP-hard set can have a polynomially bounded number of yes-instances at each\ninput length. We give an exposition of a very simple unpublished proof of\nManindra Agrawal whose ideas appear in Agrawal-Arvind (\"Geometric sets of low\ninformation content,\" Theoret. Comp. Sci., 1996). This proof is so simple that\nit can easily be taught to undergraduates or a general graduate CS audience -\nnot just theorists! - in about 10 minutes, which the author has done\nsuccessfully several times. We also include applications of Mahaney's Theorem\nto fundamental questions that bright undergraduates would ask which could be\nused to fill the remaining hour of a lecture, as well as an application (due to\nIkenmeyer, Mulmuley, and Walter, arXiv:1507.02955) to the representation theory\nof the symmetric group and the Geometric Complexity Theory Program. To this\nauthor, the fact that sparsity results on NP-complete sets have an application\nto classical questions in representation theory says that they are not only a\ngem of classical theoretical computer science, but indeed a gem of mathematics. \n\n"}
{"id": "1610.05897", "contents": "Title: Fully Dynamic Algorithm for Top-$k$ Densest Subgraphs Abstract: Given a large graph, the densest-subgraph problem asks to find a subgraph\nwith maximum average degree. When considering the top-$k$ version of this\nproblem, a na\\\"ive solution is to iteratively find the densest subgraph and\nremove it in each iteration. However, such a solution is impractical due to\nhigh processing cost. The problem is further complicated when dealing with\ndynamic graphs, since adding or removing an edge requires re-running the\nalgorithm. In this paper, we study the top-$k$ densest-subgraph problem in the\nsliding-window model and propose an efficient fully-dynamic algorithm. The\ninput of our algorithm consists of an edge stream, and the goal is to find the\nnode-disjoint subgraphs that maximize the sum of their densities. In contrast\nto existing state-of-the-art solutions that require iterating over the entire\ngraph upon any update, our algorithm profits from the observation that updates\nonly affect a limited region of the graph. Therefore, the top-$k$ densest\nsubgraphs are maintained by only applying local updates. We provide a\ntheoretical analysis of the proposed algorithm and show empirically that the\nalgorithm often generates denser subgraphs than state-of-the-art competitors.\nExperiments show an improvement in efficiency of up to five orders of magnitude\ncompared to state-of-the-art solutions. \n\n"}
{"id": "1610.06539", "contents": "Title: Linear separation of connected dominating sets in graphs Abstract: A connected dominating set in a graph is a dominating set of vertices that\ninduces a connected subgraph. Following analogous studies in the literature\nrelated to independent sets, dominating sets, and total dominating sets, we\nstudy in this paper the class of graphs in which the connected dominating sets\ncan be separated from the other vertex subsets by a linear weight function.\nMore precisely, we say that a graph is connected-domishold if it admits\nnon-negative real weights associated to its vertices such that a set of\nvertices is a connected dominating set if and only if the sum of the\ncorresponding weights exceeds a certain threshold. We characterize the graphs\nin this non-hereditary class in terms of a property of the set of minimal\ncutsets of the graph. We give several characterizations for the hereditary\ncase, that is, when each connected induced subgraph is required to be\nconnected-domishold. The characterization by forbidden induced subgraphs\nimplies that the class properly generalizes two well known classes of chordal\ngraphs, the block graphs and the trivially perfect graphs. Finally, we study\ncertain algorithmic aspects of connected-domishold graphs. Building on\nconnections with minimal cutsets and properties of the derived hypergraphs and\nBoolean functions, we show that our approach leads to new polynomially solvable\ncases of the weighted connected dominating set problem. \n\n"}
{"id": "1610.07737", "contents": "Title: Categorical Complexity Abstract: We introduce a notion of complexity of diagrams (and in particular of objects\nand morphisms) in an arbitrary category, as well as a notion of complexity of\nfunctors between categories equipped with complexity functions. We discuss\nseveral examples of this new definition in categories of wide common interest,\nsuch as finite sets, Boolean functions, topological spaces, vector spaces,\nsemi-linear and semi-algebraic sets, graded algebras, affine and projective\nvarieties and schemes, and modules over polynomial rings. We show that on one\nhand categorical complexity recovers in several settings classical notions of\nnon-uniform computational complexity (such as circuit complexity), while on the\nother hand it has features which make it mathematically more natural. We also\npostulate that studying functor complexity is the categorical analog of\nclassical questions in complexity theory about separating different complexity\nclasses. \n\n"}
{"id": "1610.07770", "contents": "Title: Online Submodular Maximization with Free Disposal: Randomization Beats\n  0.25 for Partition Matroids Abstract: We study the online submodular maximization problem with free disposal under\na matroid constraint. Elements from some ground set arrive one by one in\nrounds, and the algorithm maintains a feasible set that is independent in the\nunderlying matroid. In each round when a new element arrives, the algorithm may\naccept the new element into its feasible set and possibly remove elements from\nit, provided that the resulting set is still independent. The goal is to\nmaximize the value of the final feasible set under some monotone submodular\nfunction, to which the algorithm has oracle access.\n  For $k$-uniform matroids, we give a deterministic algorithm with competitive\nratio at least $0.2959$, and the ratio approaches $\\frac{1}{\\alpha_\\infty}\n\\approx 0.3178$ as $k$ approaches infinity, improving the previous best ratio\nof $0.25$ by Chakrabarti and Kale (IPCO 2014), Buchbinder et al. (SODA 2015)\nand Chekuri et al. (ICALP 2015). We also show that our algorithm is optimal\namong a class of deterministic monotone algorithms that accept a new arriving\nelement only if the objective is strictly increased.\n  Further, we prove that no deterministic monotone algorithm can be strictly\nbetter than $0.25$-competitive even for partition matroids, the most modest\ngeneralization of $k$-uniform matroids, matching the competitive ratio by\nChakrabarti and Kale (IPCO 2014) and Chekuri et al. (ICALP 2015).\nInterestingly, we show that randomized algorithms are strictly more powerful by\ngiving a (non-monotone) randomized algorithm for partition matroids with ratio\n$\\frac{1}{\\alpha_\\infty} \\approx 0.3178$. \n\n"}
{"id": "1610.08738", "contents": "Title: Compressive K-means Abstract: The Lloyd-Max algorithm is a classical approach to perform K-means\nclustering. Unfortunately, its cost becomes prohibitive as the training dataset\ngrows large. We propose a compressive version of K-means (CKM), that estimates\ncluster centers from a sketch, i.e. from a drastically compressed\nrepresentation of the training dataset. We demonstrate empirically that CKM\nperforms similarly to Lloyd-Max, for a sketch size proportional to the number\nof cen-troids times the ambient dimension, and independent of the size of the\noriginal dataset. Given the sketch, the computational complexity of CKM is also\nindependent of the size of the dataset. Unlike Lloyd-Max which requires several\nreplicates, we further demonstrate that CKM is almost insensitive to\ninitialization. For a large dataset of 10^7 data points, we show that CKM can\nrun two orders of magnitude faster than five replicates of Lloyd-Max, with\nsimilar clustering performance on artificial data. Finally, CKM achieves lower\nclassification errors on handwritten digits classification. \n\n"}
{"id": "1610.08906", "contents": "Title: Logarithmic Query Complexity for Approximate Nash Computation in Large\n  Games Abstract: We investigate the problem of equilibrium computation for \"large\" $n$-player\ngames. Large games have a Lipschitz-type property that no single player's\nutility is greatly affected by any other individual player's actions. In this\npaper, we mostly focus on the case where any change of strategy by a player\ncauses other players' payoffs to change by at most $\\frac{1}{n}$. We study\nalgorithms having query access to the game's payoff function, aiming to find\n$\\epsilon$-Nash equilibria. We seek algorithms that obtain $\\epsilon$ as small\nas possible, in time polynomial in $n$.\n  Our main result is a randomised algorithm that achieves $\\epsilon$\napproaching $\\frac{1}{8}$ for 2-strategy games in a {\\em completely uncoupled}\nsetting, where each player observes her own payoff to a query, and adjusts her\nbehaviour independently of other players' payoffs/actions. $O(\\log n)$\nrounds/queries are required. We also show how to obtain a slight improvement\nover $\\frac{1}{8}$, by introducing a small amount of communication between the\nplayers.\n  Finally, we give extension of our results to large games with more than two\nstrategies per player, and alternative largeness parameters. \n\n"}
{"id": "1611.01090", "contents": "Title: General and Fractional Hypertree Decompositions: Hard and Easy Cases Abstract: Hypertree decompositions, as well as the more powerful generalized hypertree\ndecompositions (GHDs), and the yet more general fractional hypertree\ndecompositions (FHD) are hypergraph decomposition methods successfully used for\nanswering conjunctive queries and for the solution of constraint satisfaction\nproblems. Every hypergraph H has a width relative to each of these\ndecomposition methods: its hypertree width hw(H), its generalized hypertree\nwidth ghw(H), and its fractional hypertree width fhw(H), respectively.\n  It is known that hw(H) <= k can be checked in polynomial time for fixed k,\nwhile checking ghw(H) <= k is NP-complete for any k greater than or equal to 3.\nThe complexity of checking fhw(H) <= k for a fixed k has been open for more\nthan a decade.\n  We settle this open problem by showing that checking fhw(H) <= k is\nNP-complete, even for k=2. The same construction allows us to prove also the\nNP-completeness of checking ghw(H) <= k for k=2. After proving these hardness\nresults, we identify meaningful restrictions, for which checking for bounded\nghw or fhw becomes tractable. \n\n"}
{"id": "1611.03225", "contents": "Title: Sharper Bounds for Regularized Data Fitting Abstract: We study matrix sketching methods for regularized variants of linear\nregression, low rank approximation, and canonical correlation analysis. Our\nmain focus is on sketching techniques which preserve the objective function\nvalue for regularized problems, which is an area that has remained largely\nunexplored. We study regularization both in a fairly broad setting, and in the\nspecific context of the popular and widely used technique of ridge\nregularization; for the latter, as applied to each of these problems, we show\nalgorithmic resource bounds in which the {\\em statistical dimension} appears in\nplaces where in previous bounds the rank would appear. The statistical\ndimension is always smaller than the rank, and decreases as the amount of\nregularization increases. In particular, for the ridge low-rank approximation\nproblem $\\min_{Y,X} \\lVert YX - A \\rVert_F^2 + \\lambda \\lVert Y\\rVert_F^2 +\n\\lambda\\lVert X \\rVert_F^2$, where $Y\\in\\mathbb{R}^{n\\times k}$ and\n$X\\in\\mathbb{R}^{k\\times d}$, we give an approximation algorithm needing \\[\nO(\\mathtt{nnz}(A)) + \\tilde{O}((n+d)\\varepsilon^{-1}k \\min\\{k,\n\\varepsilon^{-1}\\mathtt{sd}_\\lambda(Y^*)\\})+\n\\mathtt{poly}(\\mathtt{sd}_\\lambda(Y^*) \\varepsilon^{-1}) \\] time, where\n$s_{\\lambda}(Y^*)\\le k$ is the statistical dimension of $Y^*$, $Y^*$ is an\noptimal $Y$, $\\varepsilon$ is an error parameter, and $\\mathtt{nnz}(A)$ is the\nnumber of nonzero entries of $A$.This is faster than prior work, even when\n$\\lambda=0$.\n  We also study regularization in a much more general setting. For example, we\nobtain sketching-based algorithms for the low-rank approximation problem\n$\\min_{X,Y} \\lVert YX - A \\rVert_F^2 + f(Y,X)$ where $f(\\cdot,\\cdot)$ is a\nregularizing function satisfying some very general conditions (chiefly,\ninvariance under orthogonal transformations). \n\n"}
{"id": "1611.03383", "contents": "Title: Disentangling factors of variation in deep representations using\n  adversarial training Abstract: We introduce a conditional generative model for learning to disentangle the\nhidden factors of variation within a set of labeled observations, and separate\nthem into complementary codes. One code summarizes the specified factors of\nvariation associated with the labels. The other summarizes the remaining\nunspecified variability. During training, the only available source of\nsupervision comes from our ability to distinguish among different observations\nbelonging to the same class. Examples of such observations include images of a\nset of labeled objects captured at different viewpoints, or recordings of set\nof speakers dictating multiple phrases. In both instances, the intra-class\ndiversity is the source of the unspecified factors of variation: each object is\nobserved at multiple viewpoints, and each speaker dictates multiple phrases.\nLearning to disentangle the specified factors from the unspecified ones becomes\neasier when strong supervision is possible. Suppose that during training, we\nhave access to pairs of images, where each pair shows two different objects\ncaptured from the same viewpoint. This source of alignment allows us to solve\nour task using existing methods. However, labels for the unspecified factors\nare usually unavailable in realistic scenarios where data acquisition is not\nstrictly controlled. We address the problem of disentanglement in this more\ngeneral setting by combining deep convolutional autoencoders with a form of\nadversarial training. Both factors of variation are implicitly captured in the\norganization of the learned embedding space, and can be used for solving\nsingle-image analogies. Experimental results on synthetic and real datasets\nshow that the proposed method is capable of generalizing to unseen classes and\nintra-class variabilities. \n\n"}
{"id": "1611.03473", "contents": "Title: Statistical Query Lower Bounds for Robust Estimation of High-dimensional\n  Gaussians and Gaussian Mixtures Abstract: We describe a general technique that yields the first {\\em Statistical Query\nlower bounds} for a range of fundamental high-dimensional learning problems\ninvolving Gaussian distributions. Our main results are for the problems of (1)\nlearning Gaussian mixture models (GMMs), and (2) robust (agnostic) learning of\na single unknown Gaussian distribution. For each of these problems, we show a\n{\\em super-polynomial gap} between the (information-theoretic) sample\ncomplexity and the computational complexity of {\\em any} Statistical Query\nalgorithm for the problem. Our SQ lower bound for Problem (1) is qualitatively\nmatched by known learning algorithms for GMMs. Our lower bound for Problem (2)\nimplies that the accuracy of the robust learning algorithm\nin~\\cite{DiakonikolasKKLMS16} is essentially best possible among all\npolynomial-time SQ algorithms.\n  Our SQ lower bounds are attained via a unified moment-matching technique that\nis useful in other contexts and may be of broader interest. Our technique\nyields nearly-tight lower bounds for a number of related unsupervised\nestimation problems. Specifically, for the problems of (3) robust covariance\nestimation in spectral norm, and (4) robust sparse mean estimation, we\nestablish a quadratic {\\em statistical--computational tradeoff} for SQ\nalgorithms, matching known upper bounds. Finally, our technique can be used to\nobtain tight sample complexity lower bounds for high-dimensional {\\em testing}\nproblems. Specifically, for the classical problem of robustly {\\em testing} an\nunknown mean (known covariance) Gaussian, our technique implies an\ninformation-theoretic sample lower bound that scales {\\em linearly} in the\ndimension. Our sample lower bound matches the sample complexity of the\ncorresponding robust {\\em learning} problem and separates the sample complexity\nof robust testing from standard (non-robust) testing. \n\n"}
{"id": "1611.04548", "contents": "Title: Real Stable Polynomials and Matroids: Optimization and Counting Abstract: A great variety of fundamental optimization and counting problems arising in\ncomputer science, mathematics and physics can be reduced to one of the\nfollowing computational tasks involving polynomials and set systems: given an\n$m$-variate real polynomial $g$ and a family of subsets $B$ of $[m]$, (1) find\n$S\\in B$ such that the monomial in $g$ corresponding to $S$ has the largest\ncoefficient in $g$, or (2) compute the sum of coefficients of monomials in $g$\ncorresponding to all the sets in $B$. Special cases of these problems, such as\ncomputing permanents, sampling from DPPs and maximizing subdeterminants have\nbeen topics of recent interest in theoretical computer science.\n  In this paper we present a general convex programming framework geared to\nsolve both of these problems. We show that roughly, when $g$ is a real stable\npolynomial with non-negative coefficients and $B$ is a matroid, the integrality\ngap of our relaxation is finite and depends only on $m$ (and not on the\ncoefficients of g).\n  Prior to our work, such results were known only in sporadic cases that relied\non the structure of $g$ and $B$; it was not even clear if one could formulate a\nconvex relaxation that has a finite integrality gap beyond these special cases.\nTwo notable examples are a result by Gurvits on the van der Waerden conjecture\nfor real stable $g$ when $B$ is a single element and a result by Nikolov and\nSingh for multilinear real stable polynomials when $B$ is a partition matroid.\nOur work, which encapsulates most interesting cases of $g$ and $B$, benefits\nfrom both - we were inspired by the latter in deriving the right convex\nprogramming relaxation and the former in establishing the integrality gap.\nHowever, proving our results requires significant extensions of both; in that\nprocess we come up with new notions and connections between stable polynomials\nand matroids which should be of independent interest. \n\n"}
{"id": "1611.05248", "contents": "Title: Incremental and Fully Dynamic Subgraph Connectivity For Emergency\n  Planning Abstract: During the last 10 years it has become popular to study dynamic graph\nproblems in a emergency planning or sensitivity setting: Instead of considering\nthe general fully dynamic problem, we only have to process a single batch\nupdate of size $d$; after the update we have to answer queries.\n  In this paper, we consider the dynamic subgraph connectivity problem with\nsensitivity $d$: We are given a graph of which some vertices are activated and\nsome are deactivated. After that we get a single update in which the states of\nup to d vertices are changed. Then we get a sequence of connectivity queries in\nthe subgraph of activated vertices.\n  We present the first fully dynamic algorithm for this problem which has an\nupdate and query time only slightly worse than the best decremental algorithm.\nIn addition, we present the first incremental algorithm which is tight with\nrespect to the best known conditional lower bound; moreover, the algorithm is\nsimple and we believe it is implementable and efficient in practice. \n\n"}
{"id": "1611.06500", "contents": "Title: Incremental Exact Min-Cut in Poly-logarithmic Amortized Update Time Abstract: We present a deterministic incremental algorithm for \\textit{exactly}\nmaintaining the size of a minimum cut with $\\widetilde{O}(1)$ amortized time\nper edge insertion and $O(1)$ query time. This result partially answers an open\nquestion posed by Thorup [Combinatorica 2007]. It also stays in sharp contrast\nto a polynomial conditional lower-bound for the fully-dynamic weighted minimum\ncut problem. Our algorithm is obtained by combining a recent sparsification\ntechnique of Kawarabayashi and Thorup [STOC 2015] and an exact incremental\nalgorithm of Henzinger [J. of Algorithm 1997]. We also study space-efficient\nincremental algorithms for the minimum cut problem. Concretely, we show that\nthere exists an ${O}(n\\log n/\\varepsilon^2)$ space Monte-Carlo algorithm that\ncan process a stream of edge insertions starting from an empty graph, and with\nhigh probability, the algorithm maintains a $(1+\\varepsilon)$-approximation to\nthe minimum cut. The algorithm has $\\widetilde{O}(1)$ amortized update-time and\nconstant query-time. \n\n"}
{"id": "1611.06759", "contents": "Title: Emergence of Compositional Representations in Restricted Boltzmann\n  Machines Abstract: Extracting automatically the complex set of features composing real\nhigh-dimensional data is crucial for achieving high performance in\nmachine--learning tasks. Restricted Boltzmann Machines (RBM) are empirically\nknown to be efficient for this purpose, and to be able to generate distributed\nand graded representations of the data. We characterize the structural\nconditions (sparsity of the weights, low effective temperature, nonlinearities\nin the activation functions of hidden units, and adaptation of fields\nmaintaining the activity in the visible layer) allowing RBM to operate in such\na compositional phase. Evidence is provided by the replica analysis of an\nadequate statistical ensemble of random RBMs and by RBM trained on the\nhandwritten digits dataset MNIST. \n\n"}
{"id": "1611.06815", "contents": "Title: Uniquely restricted matchings and edge colorings Abstract: A matching in a graph is uniquely restricted if no other matching covers\nexactly the same set of vertices. This notion was defined by Golumbic, Hirst,\nand Lewenstein and studied in a number of articles. Our contribution is\ntwofold. We provide approximation algorithms for computing a uniquely\nrestricted matching of maximum size in some bipartite graphs. In particular, we\nachieve a ratio of $9/5$ for subcubic bipartite graphs, improving over a\n$2$-approximation algorithm proposed by Mishra. Furthermore, we study the\nuniquely restricted chromatic index of a graph, defined as the minimum number\nof uniquely restricted matchings into which its edge set can be partitioned. We\nprovide tight upper bounds in terms of the maximum degree and characterize all\nextremal graphs. Our constructive proofs yield efficient algorithms to\ndetermine the corresponding edge colorings. \n\n"}
{"id": "1611.07012", "contents": "Title: GRAM: Graph-based Attention Model for Healthcare Representation Learning Abstract: Deep learning methods exhibit promising performance for predictive modeling\nin healthcare, but two important challenges remain: -Data insufficiency:Often\nin healthcare predictive modeling, the sample size is insufficient for deep\nlearning methods to achieve satisfactory results. -Interpretation:The\nrepresentations learned by deep learning methods should align with medical\nknowledge. To address these challenges, we propose a GRaph-based Attention\nModel, GRAM that supplements electronic health records (EHR) with hierarchical\ninformation inherent to medical ontologies. Based on the data volume and the\nontology structure, GRAM represents a medical concept as a combination of its\nancestors in the ontology via an attention mechanism. We compared predictive\nperformance (i.e. accuracy, data needs, interpretability) of GRAM to various\nmethods including the recurrent neural network (RNN) in two sequential\ndiagnoses prediction tasks and one heart failure prediction task. Compared to\nthe basic RNN, GRAM achieved 10% higher accuracy for predicting diseases rarely\nobserved in the training data and 3% improved area under the ROC curve for\npredicting heart failure using an order of magnitude less training data.\nAdditionally, unlike other methods, the medical concept representations learned\nby GRAM are well aligned with the medical ontology. Finally, GRAM exhibits\nintuitive attention behaviors by adaptively generalizing to higher level\nconcepts when facing data insufficiency at the lower level concepts. \n\n"}
{"id": "1611.07169", "contents": "Title: Quasi-regular sequences and optimal schedules for security games Abstract: We study security games in which a defender commits to a mixed strategy for\nprotecting a finite set of targets of different values. An attacker, knowing\nthe defender's strategy, chooses which target to attack and for how long. If\nthe attacker spends time $t$ at a target $i$ of value $\\alpha_i$, and if he\nleaves before the defender visits the target, his utility is $t \\cdot \\alpha_i\n$; if the defender visits before he leaves, his utility is 0. The defender's\ngoal is to minimize the attacker's utility. The defender's strategy consists of\na schedule for visiting the targets; it takes her unit time to switch between\ntargets. Such games are a simplified model of a number of real-world scenarios\nsuch as protecting computer networks from intruders, crops from thieves, etc.\n  We show that optimal defender play for this continuous time security games\nreduces to the solution of a combinatorial question regarding the existence of\ninfinite sequences over a finite alphabet, with the following properties for\neach symbol $i$: (1) $i$ constitutes a prescribed fraction $p_i$ of the\nsequence. (2) The occurrences of $i$ are spread apart close to evenly, in that\nthe ratio of the longest to shortest interval between consecutive occurrences\nis bounded by a parameter $K$. We call such sequences $K$-quasi-regular.\n  We show that, surprisingly, $2$-quasi-regular sequences suffice for optimal\ndefender play. What is more, even randomized $2$-quasi-regular sequences\nsuffice for optimality. We show that such sequences always exist, and can be\ncalculated efficiently.\n  The question of the least $K$ for which deterministic $K$-quasi-regular\nsequences exist is fascinating. Using an ergodic theoretical approach, we show\nthat deterministic $3$-quasi-regular sequences always exist. For $2 \\leq K < 3$\nwe do not know whether deterministic $K$-quasi-regular sequences always exist. \n\n"}
{"id": "1611.08229", "contents": "Title: Fast Orthonormal Sparsifying Transforms Based on Householder Reflectors Abstract: Dictionary learning is the task of determining a data-dependent transform\nthat yields a sparse representation of some observed data. The dictionary\nlearning problem is non-convex, and usually solved via computationally complex\niterative algorithms. Furthermore, the resulting transforms obtained generally\nlack structure that permits their fast application to data. To address this\nissue, this paper develops a framework for learning orthonormal dictionaries\nwhich are built from products of a few Householder reflectors. Two algorithms\nare proposed to learn the reflector coefficients: one that considers a\nsequential update of the reflectors and one with a simultaneous update of all\nreflectors that imposes an additional internal orthogonal constraint. The\nproposed methods have low computational complexity and are shown to converge to\nlocal minimum points which can be described in terms of the spectral properties\nof the matrices involved. The resulting dictionaries balance between the\ncomputational complexity and the quality of the sparse representations by\ncontrolling the number of Householder reflectors in their product. Simulations\nof the proposed algorithms are shown in the image processing setting where\nwell-known fast transforms are available for comparisons. The proposed\nalgorithms have favorable reconstruction error and the advantage of a fast\nimplementation relative to the classical, unstructured, dictionaries. \n\n"}
{"id": "1611.08400", "contents": "Title: Nondeterministic Communication Complexity of Random Boolean Functions Abstract: We study nondeterministic communication complexity and related concepts\n(fooling sets, fractional covering number) of random functions $f\\colon X\\times\nY \\to \\{0,1\\}$ where each value is chosen to be 1 independently with\nprobability $p=p(n)$, $n := |X|=|Y|$. \n\n"}
{"id": "1612.00100", "contents": "Title: Noise-Tolerant Life-Long Matrix Completion via Adaptive Sampling Abstract: We study the problem of recovering an incomplete $m\\times n$ matrix of rank\n$r$ with columns arriving online over time. This is known as the problem of\nlife-long matrix completion, and is widely applied to recommendation system,\ncomputer vision, system identification, etc. The challenge is to design\nprovable algorithms tolerant to a large amount of noises, with small sample\ncomplexity. In this work, we give algorithms achieving strong guarantee under\ntwo realistic noise models. In bounded deterministic noise, an adversary can\nadd any bounded yet unstructured noise to each column. For this problem, we\npresent an algorithm that returns a matrix of a small error, with sample\ncomplexity almost as small as the best prior results in the noiseless case. For\nsparse random noise, where the corrupted columns are sparse and drawn randomly,\nwe give an algorithm that exactly recovers an $\\mu_0$-incoherent matrix by\nprobability at least $1-\\delta$ with sample complexity as small as\n$O\\left(\\mu_0rn\\log (r/\\delta)\\right)$. This result advances the\nstate-of-the-art work and matches the lower bound in a worst case. We also\nstudy the scenario where the hidden matrix lies on a mixture of subspaces and\nshow that the sample complexity can be even smaller. Our proposed algorithms\nperform well experimentally in both synthetic and real-world datasets. \n\n"}
{"id": "1612.01064", "contents": "Title: Trained Ternary Quantization Abstract: Deep neural networks are widely used in machine learning applications.\nHowever, the deployment of large neural networks models can be difficult to\ndeploy on mobile devices with limited power budgets. To solve this problem, we\npropose Trained Ternary Quantization (TTQ), a method that can reduce the\nprecision of weights in neural networks to ternary values. This method has very\nlittle accuracy degradation and can even improve the accuracy of some models\n(32, 44, 56-layer ResNet) on CIFAR-10 and AlexNet on ImageNet. And our AlexNet\nmodel is trained from scratch, which means it's as easy as to train normal full\nprecision model. We highlight our trained quantization method that can learn\nboth ternary values and ternary assignment. During inference, only ternary\nvalues (2-bit weights) and scaling factors are needed, therefore our models are\nnearly 16x smaller than full-precision models. Our ternary models can also be\nviewed as sparse binary weight networks, which can potentially be accelerated\nwith custom circuit. Experiments on CIFAR-10 show that the ternary models\nobtained by trained quantization method outperform full-precision models of\nResNet-32,44,56 by 0.04%, 0.16%, 0.36%, respectively. On ImageNet, our model\noutperforms full-precision AlexNet model by 0.3% of Top-1 accuracy and\noutperforms previous ternary models by 3%. \n\n"}
{"id": "1612.01527", "contents": "Title: Matrix multiplication algorithms from group orbits Abstract: We show how to construct highly symmetric algorithms for matrix\nmultiplication. In particular, we consider algorithms which decompose the\nmatrix multiplication tensor into a sum of rank-1 tensors, where the\ndecomposition itself consists of orbits under some finite group action. We show\nhow to use the representation theory of the corresponding group to derive\nsimple constraints on the decomposition, which we solve by hand for n=2,3,4,5,\nrecovering Strassen's algorithm (in a particularly symmetric form) and new\nalgorithms for larger n. While these new algorithms do not improve the known\nupper bounds on tensor rank or the matrix multiplication exponent, they are\nbeautiful in their own right, and we point out modifications of this idea that\ncould plausibly lead to further improvements. Our constructions also suggest\nfurther patterns that could be mined for new algorithms, including a\ntantalizing connection with lattices. In particular, using lattices we give the\nmost transparent proof to date of Strassen's algorithm; the same proof works\nfor all n, to yield a decomposition with $n^3 - n + 1$ terms. \n\n"}
{"id": "1612.01817", "contents": "Title: Pseudodeterministic Constructions in Subexponential Time Abstract: We study pseudodeterministic constructions, i.e., randomized algorithms which\noutput the same solution on most computation paths. We establish\nunconditionally that there is an infinite sequence $\\{p_n\\}_{n \\in \\mathbb{N}}$\nof increasing primes and a randomized algorithm $A$ running in expected\nsub-exponential time such that for each $n$, on input $1^{|p_n|}$, $A$ outputs\n$p_n$ with probability $1$. In other words, our result provides a\npseudodeterministic construction of primes in sub-exponential time which works\ninfinitely often.\n  This result follows from a much more general theorem about\npseudodeterministic constructions. A property $Q \\subseteq \\{0,1\\}^{*}$ is\n$\\gamma$-dense if for large enough $n$, $|Q \\cap \\{0,1\\}^n| \\geq \\gamma 2^n$.\nWe show that for each $c > 0$ at least one of the following holds: (1) There is\na pseudodeterministic polynomial time construction of a family $\\{H_n\\}$ of\nsets, $H_n \\subseteq \\{0,1\\}^n$, such that for each $(1/n^c)$-dense property $Q\n\\in \\mathsf{DTIME}(n^c)$ and every large enough $n$, $H_n \\cap Q \\neq\n\\emptyset$; or (2) There is a deterministic sub-exponential time construction\nof a family $\\{H'_n\\}$ of sets, $H'_n \\subseteq \\{0,1\\}^n$, such that for each\n$(1/n^c)$-dense property $Q \\in \\mathsf{DTIME}(n^c)$ and for infinitely many\nvalues of $n$, $H'_n \\cap Q \\neq \\emptyset$.\n  We provide further algorithmic applications that might be of independent\ninterest. Perhaps intriguingly, while our main results are unconditional, they\nhave a non-constructive element, arising from a sequence of applications of the\nhardness versus randomness paradigm. \n\n"}
{"id": "1612.04689", "contents": "Title: An Integer Interior Point Method for Min-Cost Flow Using Arc\n  Contractions and Deletions Abstract: We present an interior point method for the min-cost flow problem that uses\narc contractions and deletions to steer clear from the boundary of the polytope\nwhen path-following methods come too close. We obtain a randomized algorithm\nrunning in expected $\\tilde O( m^{3/2} )$ time that only visits integer lattice\npoints in the vicinity of the central path of the polytope. This enables us to\nuse integer arithmetic like classical combinatorial algorithms typically do. We\nprovide explicit bounds on the size of the numbers that appear during all\ncomputations. By presenting an integer arithmetic interior point algorithm we\navoid the tediousness of floating point error analysis and achieve a method\nthat is guaranteed to be free of any numerical issues. We thereby eliminate one\nof the drawbacks of numerical methods in contrast to combinatorial min-cost\nflow algorithms that still yield the most efficient implementations in\npractice, despite their inferior worst-case time complexity. \n\n"}
{"id": "1612.04794", "contents": "Title: Algorithms for Automatic Ranking of Participants and Tasks in an\n  Anonymized Contest Abstract: We introduce a new set of problems based on the Chain Editing problem. In our\nversion of Chain Editing, we are given a set of anonymous participants and a\nset of undisclosed tasks that every participant attempts. For each\nparticipant-task pair, we know whether the participant has succeeded at the\ntask or not. We assume that participants vary in their ability to solve tasks,\nand that tasks vary in their difficulty to be solved. In an ideal world,\nstronger participants should succeed at a superset of tasks that weaker\nparticipants succeed at. Similarly, easier tasks should be completed\nsuccessfully by a superset of participants who succeed at harder tasks. In\nreality, it can happen that a stronger participant fails at a task that a\nweaker participants succeeds at. Our goal is to find a perfect nesting of the\nparticipant-task relations by flipping a minimum number of participant-task\nrelations, implying such a \"nearest perfect ordering\" to be the one that is\nclosest to the truth of participant strengths and task difficulties. Many\nvariants of the problem are known to be NP-hard.\n  We propose six natural $k$-near versions of the Chain Editing problem and\nclassify their complexity. The input to a $k$-near Chain Editing problem\nincludes an initial ordering of the participants (or tasks) that we are\nrequired to respect by moving each participant (or task) at most $k$ positions\nfrom the initial ordering. We obtain surprising results on the complexity of\nthe six $k$-near problems: Five of the problems are polynomial-time solvable\nusing dynamic programming, but one of them is NP-hard. \n\n"}
{"id": "1612.07162", "contents": "Title: Supercritical Space-Width Trade-offs for Resolution Abstract: We show that there are CNF formulas which can be refuted in resolution in\nboth small space and small width, but for which any small-width proof must have\nspace exceeding by far the linear worst-case upper bound. This significantly\nstrengthens the space-width trade-offs in [Ben-Sasson '09]}, and provides one\nmore example of trade-offs in the \"supercritical\" regime above worst case\nrecently identified by [Razborov '16]. We obtain our results by using\nRazborov's new hardness condensation technique and combining it with the space\nlower bounds in [Ben-Sasson and Nordstrom '08]. \n\n"}
{"id": "1612.08192", "contents": "Title: An Improved Homomorphism Preservation Theorem From Lower Bounds in\n  Circuit Complexity Abstract: Previous work of the author [39] showed that the Homomorphism Preservation\nTheorem of classical model theory remains valid when its statement is\nrestricted to finite structures. In this paper, we give a new proof of this\nresult via a reduction to lower bounds in circuit complexity, specifically on\nthe AC$^0$ formula size of the colored subgraph isomorphism problem. Formally,\nwe show the following: if a first-order sentence $\\Phi$ of quantifier-rank $k$\nis preserved under homomorphisms on finite structures, then it is equivalent on\nfinite structures to an existential-positive sentence $\\Psi$ of quantifier-rank\n$k^{O(1)}$. Quantitatively, this improves the result of [39], where the upper\nbound on the quantifier-rank of $\\Psi$ is a non-elementary function of $k$. \n\n"}
{"id": "1701.01394", "contents": "Title: On spectral partitioning of signed graphs Abstract: We argue that the standard graph Laplacian is preferable for spectral\npartitioning of signed graphs compared to the signed Laplacian. Simple examples\ndemonstrate that partitioning based on signs of components of the leading\neigenvectors of the signed Laplacian may be meaningless, in contrast to\npartitioning based on the Fiedler vector of the standard graph Laplacian for\nsigned graphs. We observe that negative eigenvalues are beneficial for spectral\npartitioning of signed graphs, making the Fiedler vector easier to compute. \n\n"}
{"id": "1701.02720", "contents": "Title: Towards End-to-End Speech Recognition with Deep Convolutional Neural\n  Networks Abstract: Convolutional Neural Networks (CNNs) are effective models for reducing\nspectral variations and modeling spectral correlations in acoustic features for\nautomatic speech recognition (ASR). Hybrid speech recognition systems\nincorporating CNNs with Hidden Markov Models/Gaussian Mixture Models\n(HMMs/GMMs) have achieved the state-of-the-art in various benchmarks.\nMeanwhile, Connectionist Temporal Classification (CTC) with Recurrent Neural\nNetworks (RNNs), which is proposed for labeling unsegmented sequences, makes it\nfeasible to train an end-to-end speech recognition system instead of hybrid\nsettings. However, RNNs are computationally expensive and sometimes difficult\nto train. In this paper, inspired by the advantages of both CNNs and the CTC\napproach, we propose an end-to-end speech framework for sequence labeling, by\ncombining hierarchical CNNs with CTC directly without recurrent connections. By\nevaluating the approach on the TIMIT phoneme recognition task, we show that the\nproposed model is not only computationally efficient, but also competitive with\nthe existing baseline systems. Moreover, we argue that CNNs have the capability\nto model temporal correlations with appropriate context information. \n\n"}
{"id": "1701.03308", "contents": "Title: Sampling and Reconstruction Using Bloom Filters Abstract: In this paper, we address the problem of sampling from a set and\nreconstructing a set stored as a Bloom filter. To the best of our knowledge our\nwork is the first to address this question. We introduce a novel hierarchical\ndata structure called BloomSampleTree that helps us design efficient algorithms\nto extract an almost uniform sample from the set stored in a Bloom filter and\nalso allows us to reconstruct the set efficiently. In the case where the hash\nfunctions used in the Bloom filter implementation are partially invertible, in\nthe sense that it is easy to calculate the set of elements that map to a\nparticular hash value, we propose a second, more space-efficient method called\nHashInvert for the reconstruction. We study the properties of these two methods\nboth analytically as well as experimentally. We provide bounds on run times for\nboth methods and sample quality for the BloomSampleTree based algorithm, and\nshow through an extensive experimental evaluation that our methods are\nefficient and effective. \n\n"}
{"id": "1701.03493", "contents": "Title: Subgaussian Tail Bounds via Stability Arguments Abstract: Sums of independent, bounded random variables concentrate around their\nexpectation approximately as well a Gaussian of the same variance. Well known\nresults of this form include the Bernstein, Hoeffding, and Chernoff\ninequalities and many others. We present an alternative proof of these tail\nbounds based on what we call a stability argument, which avoids bounding the\nmoment generating function or higher-order moments of the distribution. Our\nstability argument is inspired by recent work on the generalization properties\nof differential privacy and their connection to adaptive data analysis (Bassily\net al., STOC 2016). \n\n"}
{"id": "1701.03856", "contents": "Title: The flip Markov chain for connected regular graphs Abstract: Mahlmann and Schindelhauer (2005) defined a Markov chain which they called\n$k$-Flipper, and showed that it is irreducible on the set of all connected\nregular graphs of a given degree (at least 3). We study the 1-Flipper chain,\nwhich we call the flip chain, and prove that the flip chain converges rapidly\nto the uniform distribution over connected $2r$-regular graphs with $n$\nvertices, where $n\\geq 8$ and $r = r(n)\\geq 2$. Formally, we prove that the\ndistribution of the flip chain will be within $\\varepsilon$ of uniform in total\nvariation distance after $\\text{poly}(n,r,\\log(\\varepsilon^{-1}))$ steps. This\npolynomial upper bound on the mixing time is given explicitly, and improves\nmarkedly on a previous bound given by Feder et al.(2006). We achieve this\nimprovement by using a direct two-stage canonical path construction, which we\ndefine in a general setting.\n  This work has applications to decentralised networks based on random regular\nconnected graphs of even degree, as a self-stabilising protocol in which nodes\nspontaneously perform random flips in order to repair the network. \n\n"}
{"id": "1701.04086", "contents": "Title: The complexity of quantified constraints using the algebraic formulation Abstract: Let A be an idempotent algebra on a finite domain. We combine results of\nChen, Zhuk and Carvalho et al. to argue that if A satisfies the polynomially\ngenerated powers property (PGP), then QCSP(Inv(A)) is in NP. We then use the\nresult of Zhuk to prove a converse, that if QCSP(Inv(A)) satisfies the\nexponentially generated powers property (EGP), then QCSP(Inv(A)) is co-NP-hard.\nSince Zhuk proved that only PGP and EGP are possible, we derive a full\ndichotomy for the QCSP, justifying the moral correctness of what we term the\nChen Conjecture.\n  We examine in closer detail the situation for domains of size three. Over any\nfinite domain, the only type of PGP that can occur is switchability.\nSwitchability was introduced by Chen as a generalisation of the already-known\nCollapsibility. For three-element domain algebras A that are Switchable, we\nprove that for every finite subset Delta of Inv(A), Pol(Delta) is Collapsible.\nThe significance of this is that, for QCSP on finite structures (over\nthree-element domain), all QCSP tractability explained by Switchability is\nalready explained by Collapsibility.\n  Finally, we present a three-element domain complexity classification\nvignette, using known as well as derived results. \n\n"}
{"id": "1701.04777", "contents": "Title: The fast parallel algorithm for CNF SAT without algebra Abstract: A novel parallel algorithm for solving the classical Decision Boolean\nSatisfiability problem with clauses in conjunctive normal form is depicted. My\napproach for solving SAT is without using algebra or other computational search\nstrategies such as branch and bound, back-forward, tree representation, etc.\nThe method is based on the special class of SAT problems, Simple SAT (SSAT).\nThe algorithm's design includes parallel execution, object oriented, and short\ntermination as my previous versions but it keep track of the tested\nunsatisfactory binary values to improve the efficiency and to favor short\ntermination. The resulting algorithm is linear with respect to the number of\nclauses plus a process data on the partial solutions of the subproblems SSAT of\nan arbitrary SAT and it is bounded by $2^{n}$ iterations where $n$ is the\nnumber of logical variables. The novelty for the solution of arbitrary SAT\nproblems is a linear algorithm, such its complexity is less or equal than the\nalgorithms of the state of the art for solving SAT. \n\n"}
{"id": "1701.05243", "contents": "Title: How to Find a Joint Probability Distribution of Minimum Entropy (almost)\n  given the Marginals Abstract: Given two discrete random variables $X$ and $Y$, with probability\ndistributions ${\\bf p} =(p_1, \\ldots , p_n)$ and ${\\bf q}=(q_1, \\ldots , q_m)$,\nrespectively, denote by ${\\cal C}({\\bf p}, {\\bf q})$ the set of all couplings\nof ${\\bf p}$ and ${\\bf q}$, that is, the set of all bivariate probability\ndistributions that have ${\\bf p}$ and ${\\bf q}$ as marginals. In this paper, we\nstudy the problem of finding the joint probability distribution in ${\\cal\nC}({\\bf p}, {\\bf q})$ of minimum entropy (equivalently, the joint probability\ndistribution that maximizes the mutual information between $X$ and $Y$), and we\ndiscuss several situations where the need for this kind of optimization\nnaturally arises. Since the optimization problem is known to be NP-hard, we\ngive an efficient algorithm to find a joint probability distribution in ${\\cal\nC}({\\bf p}, {\\bf q})$ with entropy exceeding the minimum possible by at most 1,\nthus providing an approximation algorithm with additive approximation factor of\n1. Leveraging on this algorithm, we extend our result to the problem of finding\na minimum--entropy joint distribution of arbitrary $k\\geq 2$ discrete random\nvariables $X_1, \\ldots , X_k$, consistent with the known $k$ marginal\ndistributions of $X_1, \\ldots , X_k$. In this case, our approximation algorithm\nhas an additive approximation factor of $\\log k$. We also discuss some related\napplications of our findings. \n\n"}
{"id": "1701.05654", "contents": "Title: Bayesian Network Learning via Topological Order Abstract: We propose a mixed integer programming (MIP) model and iterative algorithms\nbased on topological orders to solve optimization problems with acyclic\nconstraints on a directed graph. The proposed MIP model has a significantly\nlower number of constraints compared to popular MIP models based on cycle\nelimination constraints and triangular inequalities. The proposed iterative\nalgorithms use gradient descent and iterative reordering approaches,\nrespectively, for searching topological orders. A computational experiment is\npresented for the Gaussian Bayesian network learning problem, an optimization\nproblem minimizing the sum of squared errors of regression models with L1\npenalty over a feature network with application of gene network inference in\nbioinformatics. \n\n"}
{"id": "1702.01284", "contents": "Title: New cardinality estimation algorithms for HyperLogLog sketches Abstract: This paper presents new methods to estimate the cardinalities of data sets\nrecorded by HyperLogLog sketches. A theoretically motivated extension to the\noriginal estimator is presented that eliminates the bias for small and large\ncardinalities. Based on the maximum likelihood principle a second unbiased\nmethod is derived together with a robust and efficient numerical algorithm to\ncalculate the estimate. The maximum likelihood approach can also be applied to\nmore than a single HyperLogLog sketch. In particular, it is shown that it gives\nmore precise cardinality estimates for union, intersection, or relative\ncomplements of two sets that are both represented by HyperLogLog sketches\ncompared to the conventional technique using the inclusion-exclusion principle.\nAll the new methods are demonstrated and verified by extensive simulations. \n\n"}
{"id": "1702.01286", "contents": "Title: An Adaptive Sublinear-Time Block Sparse Fourier Transform Abstract: The problem of approximately computing the $k$ dominant Fourier coefficients\nof a vector $X$ quickly, and using few samples in time domain, is known as the\nSparse Fourier Transform (sparse FFT) problem. A long line of work on the\nsparse FFT has resulted in algorithms with $O(k\\log n\\log (n/k))$ runtime\n[Hassanieh et al., STOC'12] and $O(k\\log n)$ sample complexity [Indyk et al.,\nFOCS'14]. These results are proved using non-adaptive algorithms, and the\nlatter $O(k\\log n)$ sample complexity result is essentially the best possible\nunder the sparsity assumption alone.\n  This paper revisits the sparse FFT problem with the added twist that the\nsparse coefficients approximately obey a $(k_0,k_1)$-block sparse model. In\nthis model, signal frequencies are clustered in $k_0$ intervals with width\n$k_1$ in Fourier space, where $k= k_0k_1$ is the total sparsity. Signals\narising in applications are often well approximated by this model with $k_0\\ll\nk$.\n  Our main result is the first sparse FFT algorithm for $(k_0, k_1)$-block\nsparse signals with the sample complexity of $O^*(k_0k_1 + k_0\\log(1+ k_0)\\log\nn)$ at constant signal-to-noise ratios, and sublinear runtime. A similar sample\ncomplexity was previously achieved in the works on model-based compressive\nsensing using random Gaussian measurements, but used $\\Omega(n)$ runtime. To\nthe best of our knowledge, our result is the first sublinear-time algorithm for\nmodel based compressed sensing, and the first sparse FFT result that goes below\nthe $O(k\\log n)$ sample complexity bound.\n  Our algorithm crucially uses {\\em adaptivity} to achieve the improved sample\ncomplexity bound, and we prove that adaptivity is in fact necessary if Fourier\nmeasurements are used: Any non-adaptive algorithm must use $\\Omega(k_0k_1\\log\n\\frac{n}{k_0k_1})$ samples for the $(k_0,k_1$)-block sparse model, ruling out\nimprovements over the vanilla sparsity assumption. \n\n"}
{"id": "1702.03773", "contents": "Title: Overlapping Community Detection by Local Decentralised Vertex-centred\n  Process Abstract: This paper focuses on the identification of overlapping communities, allowing\nnodes to simultaneously belong to several communities, in a decentralised way.\nTo that aim it proposes LOCNeSs, an algorithm specially designed to run in a\ndecentralised environment and to limit propagation, two essential\ncharacteristics to be applied in mobile networks. It is based on the\nexploitation of the preferential attachment mechanism in networks. Experimental\nresults show that LOCNeSs is stable and achieves good overlapping vertex\nidentification. \n\n"}
{"id": "1702.06110", "contents": "Title: Density Independent Algorithms for Sparsifying $k$-Step Random Walks Abstract: We give faster algorithms for producing sparse approximations of the\ntransition matrices of $k$-step random walks on undirected, weighted graphs.\nThese transition matrices also form graphs, and arise as intermediate objects\nin a variety of graph algorithms. Our improvements are based on a better\nunderstanding of processes that sample such walks, as well as tighter bounds on\nkey weights underlying these sampling processes. On a graph with $n$ vertices\nand $m$ edges, our algorithm produces a graph with about $n\\log{n}$ edges that\napproximates the $k$-step random walk graph in about $m + n \\log^4{n}$ time. In\norder to obtain this runtime bound, we also revisit \"density independent\"\nalgorithms for sparsifying graphs whose runtime overhead is expressed only in\nterms of the number of vertices. \n\n"}
{"id": "1702.06237", "contents": "Title: Exact tensor completion with sum-of-squares Abstract: We obtain the first polynomial-time algorithm for exact tensor completion\nthat improves over the bound implied by reduction to matrix completion. The\nalgorithm recovers an unknown 3-tensor with $r$ incoherent, orthogonal\ncomponents in $\\mathbb R^n$ from $r\\cdot \\tilde O(n^{1.5})$ randomly observed\nentries of the tensor. This bound improves over the previous best one of\n$r\\cdot \\tilde O(n^{2})$ by reduction to exact matrix completion. Our bound\nalso matches the best known results for the easier problem of approximate\ntensor completion (Barak & Moitra, 2015).\n  Our algorithm and analysis extends seminal results for exact matrix\ncompletion (Candes & Recht, 2009) to the tensor setting via the sum-of-squares\nmethod. The main technical challenge is to show that a small number of randomly\nchosen monomials are enough to construct a degree-3 polynomial with precisely\nplanted orthogonal global optima over the sphere and that this fact can be\ncertified within the sum-of-squares proof system. \n\n"}
{"id": "1702.06723", "contents": "Title: Compact linear programs for 2SAT Abstract: For each integer $n$ we present an explicit formulation of a compact linear\nprogram, with $O(n^3)$ variables and constraints, which determines the\nsatisfiability of any 2SAT formula with $n$ boolean variables by a single\nlinear optimization. This contrasts with the fact that the natural polytope for\nthis problem, formed from the convex hull of all satisfiable formulas and their\nsatisfying assignments, has superpolynomial extension complexity. Our\nformulation is based on multicommodity flows. We also discuss connections of\nthese results to the stable matching problem. \n\n"}
{"id": "1702.07665", "contents": "Title: Truthful Mechanisms for Delivery with Mobile Agents Abstract: We study the game-theoretic task of selecting mobile agents to deliver\nmultiple items on a network. An instance is given by $m$ messages (physical\nobjects) which have to be transported between specified source-target pairs in\na weighted undirected graph, and $k$ mobile heterogeneous agents, each being\nable to transport one message at a time. Following a recent model by\n[B\\\"artschi et al. 2016], each agent $i$ consumes energy proportional to the\ndistance it travels in the graph, where the different rates of energy\nconsumption are given by weight factors $w_i$. We are interested in optimizing\nor approximating the total energy consumption over all selected agents.\n  Unlike previous research, we assume the weights to be private values known\nonly to the respective agents. We present three different mechanisms which\nselect, route and pay the agents in a truthful way that guarantees voluntary\nparticipation of the agents, while approximating the optimum energy consumption\nby a constant factor. To this end we analyze a previous structural result and\nan approximation algorithm given by [B\\\"artschi et al. 2017]. Finally, we show\nthat for some instances in the case of a single package, the sum of the\npayments can be bounded in terms of the optimum. \n\n"}
{"id": "1702.08489", "contents": "Title: Depth Separation for Neural Networks Abstract: Let $f:\\mathbb{S}^{d-1}\\times \\mathbb{S}^{d-1}\\to\\mathbb{S}$ be a function of\nthe form $f(\\mathbf{x},\\mathbf{x}') = g(\\langle\\mathbf{x},\\mathbf{x}'\\rangle)$\nfor $g:[-1,1]\\to \\mathbb{R}$. We give a simple proof that shows that poly-size\ndepth two neural networks with (exponentially) bounded weights cannot\napproximate $f$ whenever $g$ cannot be approximated by a low degree polynomial.\nMoreover, for many $g$'s, such as $g(x)=\\sin(\\pi d^3x)$, the number of neurons\nmust be $2^{\\Omega\\left(d\\log(d)\\right)}$. Furthermore, the result holds\nw.r.t.\\ the uniform distribution on $\\mathbb{S}^{d-1}\\times \\mathbb{S}^{d-1}$.\nAs many functions of the above form can be well approximated by poly-size depth\nthree networks with poly-bounded weights, this establishes a separation between\ndepth two and depth three networks w.r.t.\\ the uniform distribution on\n$\\mathbb{S}^{d-1}\\times \\mathbb{S}^{d-1}$. \n\n"}
{"id": "1703.00862", "contents": "Title: Binarized Convolutional Landmark Localizers for Human Pose Estimation\n  and Face Alignment with Limited Resources Abstract: Our goal is to design architectures that retain the groundbreaking\nperformance of CNNs for landmark localization and at the same time are\nlightweight, compact and suitable for applications with limited computational\nresources. To this end, we make the following contributions: (a) we are the\nfirst to study the effect of neural network binarization on localization tasks,\nnamely human pose estimation and face alignment. We exhaustively evaluate\nvarious design choices, identify performance bottlenecks, and more importantly\npropose multiple orthogonal ways to boost performance. (b) Based on our\nanalysis, we propose a novel hierarchical, parallel and multi-scale residual\narchitecture that yields large performance improvement over the standard\nbottleneck block while having the same number of parameters, thus bridging the\ngap between the original network and its binarized counterpart. (c) We perform\na large number of ablation studies that shed light on the properties and the\nperformance of the proposed block. (d) We present results for experiments on\nthe most challenging datasets for human pose estimation and face alignment,\nreporting in many cases state-of-the-art performance. Code can be downloaded\nfrom https://www.adrianbulat.com/binary-cnn-landmarks \n\n"}
{"id": "1703.01054", "contents": "Title: When Hashes Met Wedges: A Distributed Algorithm for Finding High\n  Similarity Vectors Abstract: Finding similar user pairs is a fundamental task in social networks, with\nnumerous applications in ranking and personalization tasks such as link\nprediction and tie strength detection. A common manifestation of user\nsimilarity is based upon network structure: each user is represented by a\nvector that represents the user's network connections, where pairwise cosine\nsimilarity among these vectors defines user similarity. The predominant task\nfor user similarity applications is to discover all similar pairs that have a\npairwise cosine similarity value larger than a given threshold $\\tau$. In\ncontrast to previous work where $\\tau$ is assumed to be quite close to 1, we\nfocus on recommendation applications where $\\tau$ is small, but still\nmeaningful. The all pairs cosine similarity problem is computationally\nchallenging on networks with billions of edges, and especially so for settings\nwith small $\\tau$. To the best of our knowledge, there is no practical solution\nfor computing all user pairs with, say $\\tau = 0.2$ on large social networks,\neven using the power of distributed algorithms.\n  Our work directly addresses this challenge by introducing a new algorithm ---\nWHIMP --- that solves this problem efficiently in the MapReduce model. The key\ninsight in WHIMP is to combine the \"wedge-sampling\" approach of Cohen-Lewis for\napproximate matrix multiplication with the SimHash random projection techniques\nof Charikar. We provide a theoretical analysis of WHIMP, proving that it has\nnear optimal communication costs while maintaining computation cost comparable\nwith the state of the art. We also empirically demonstrate WHIMP's scalability\nby computing all highly similar pairs on four massive data sets, and show that\nit accurately finds high similarity pairs. In particular, we note that WHIMP\nsuccessfully processes the entire Twitter network, which has tens of billions\nof edges. \n\n"}
{"id": "1703.01127", "contents": "Title: On the Behavior of Convolutional Nets for Feature Extraction Abstract: Deep neural networks are representation learning techniques. During training,\na deep net is capable of generating a descriptive language of unprecedented\nsize and detail in machine learning. Extracting the descriptive language coded\nwithin a trained CNN model (in the case of image data), and reusing it for\nother purposes is a field of interest, as it provides access to the visual\ndescriptors previously learnt by the CNN after processing millions of images,\nwithout requiring an expensive training phase. Contributions to this field\n(commonly known as feature representation transfer or transfer learning) have\nbeen purely empirical so far, extracting all CNN features from a single layer\nclose to the output and testing their performance by feeding them to a\nclassifier. This approach has provided consistent results, although its\nrelevance is limited to classification tasks. In a completely different\napproach, in this paper we statistically measure the discriminative power of\nevery single feature found within a deep CNN, when used for characterizing\nevery class of 11 datasets. We seek to provide new insights into the behavior\nof CNN features, particularly the ones from convolutional layers, as this can\nbe relevant for their application to knowledge representation and reasoning.\nOur results confirm that low and middle level features may behave differently\nto high level features, but only under certain conditions. We find that all CNN\nfeatures can be used for knowledge representation purposes both by their\npresence or by their absence, doubling the information a single CNN feature may\nprovide. We also study how much noise these features may include, and propose a\nthresholding approach to discard most of it. All these insights have a direct\napplication to the generation of CNN embedding spaces. \n\n"}
{"id": "1703.01804", "contents": "Title: Orthogonalized ALS: A Theoretically Principled Tensor Decomposition\n  Algorithm for Practical Use Abstract: The popular Alternating Least Squares (ALS) algorithm for tensor\ndecomposition is efficient and easy to implement, but often converges to poor\nlocal optima---particularly when the weights of the factors are non-uniform. We\npropose a modification of the ALS approach that is as efficient as standard\nALS, but provably recovers the true factors with random initialization under\nstandard incoherence assumptions on the factors of the tensor. We demonstrate\nthe significant practical superiority of our approach over traditional ALS for\na variety of tasks on synthetic data---including tensor factorization on exact,\nnoisy and over-complete tensors, as well as tensor completion---and for\ncomputing word embeddings from a third-order word tri-occurrence tensor. \n\n"}
{"id": "1703.02136", "contents": "Title: English Conversational Telephone Speech Recognition by Humans and\n  Machines Abstract: One of the most difficult speech recognition tasks is accurate recognition of\nhuman to human communication. Advances in deep learning over the last few years\nhave produced major speech recognition improvements on the representative\nSwitchboard conversational corpus. Word error rates that just a few years ago\nwere 14% have dropped to 8.0%, then 6.6% and most recently 5.8%, and are now\nbelieved to be within striking range of human performance. This then raises two\nissues - what IS human performance, and how far down can we still drive speech\nrecognition error rates? A recent paper by Microsoft suggests that we have\nalready achieved human performance. In trying to verify this statement, we\nperformed an independent set of human performance measurements on two\nconversational tasks and found that human performance may be considerably\nbetter than what was earlier reported, giving the community a significantly\nharder goal to achieve. We also report on our own efforts in this area,\npresenting a set of acoustic and language modeling techniques that lowered the\nword error rate of our own English conversational telephone LVCSR system to the\nlevel of 5.5%/10.3% on the Switchboard/CallHome subsets of the Hub5 2000\nevaluation, which - at least at the writing of this paper - is a new\nperformance milestone (albeit not at what we measure to be human performance!).\nOn the acoustic side, we use a score fusion of three models: one LSTM with\nmultiple feature inputs, a second LSTM trained with speaker-adversarial\nmulti-task learning and a third residual net (ResNet) with 25 convolutional\nlayers and time-dilated convolutions. On the language modeling side, we use\nword and character LSTMs and convolutional WaveNet-style language models. \n\n"}
{"id": "1703.02914", "contents": "Title: Dropout Inference in Bayesian Neural Networks with Alpha-divergences Abstract: To obtain uncertainty estimates with real-world Bayesian deep learning\nmodels, practical inference approximations are needed. Dropout variational\ninference (VI) for example has been used for machine vision and medical\napplications, but VI can severely underestimates model uncertainty.\nAlpha-divergences are alternative divergences to VI's KL objective, which are\nable to avoid VI's uncertainty underestimation. But these are hard to use in\npractice: existing techniques can only use Gaussian approximating\ndistributions, and require existing models to be changed radically, thus are of\nlimited use for practitioners. We propose a re-parametrisation of the\nalpha-divergence objectives, deriving a simple inference technique which,\ntogether with dropout, can be easily implemented with existing models by simply\nchanging the loss of the model. We demonstrate improved uncertainty estimates\nand accuracy compared to VI in dropout networks. We study our model's epistemic\nuncertainty far away from the data using adversarial images, showing that these\ncan be distinguished from non-adversarial images by examining our model's\nuncertainty. \n\n"}
{"id": "1703.03900", "contents": "Title: Core Maintenance in Dynamic Graphs: A Parallel Approach based on\n  Matching Abstract: The core number of a vertex is a basic index depicting cohesiveness of a\ngraph, and has been widely used in large-scale graph analytics. In this paper,\nwe study the update of core numbers of vertices in dynamic graphs with edge\ninsertions/deletions, which is known as the core maintenance problem. Different\nfrom previous approaches that just focus on the case of single-edge\ninsertion/deletion and sequentially handle the edges when multiple edges are\ninserted/deleted, we investigate the parallelism in the core maintenance\nprocedure. Specifically, we show that if the inserted/deleted edges constitute\na matching, the core number update with respect to each inserted/deleted edge\ncan be handled in parallel. Based on this key observation, we propose parallel\nalgorithms for core maintenance in both cases of edge insertions and deletions.\nWe conduct extensive experiments to evaluate the efficiency, stability,\nparallelism and scalability of our algorithms on different types of real-world\nand synthetic graphs. Comparing with sequential approaches, our algorithms can\nimprove the core maintenance efficiency significantly. \n\n"}
{"id": "1703.03998", "contents": "Title: The Weighted Matching Approach to Maximum Cardinality Matching Abstract: Several papers have achieved time $O(\\sqrt n m)$ for cardinality matching,\nstarting from first principles. This results in a long derivation. We simplify\nthe task by employing well-known concepts for maximum weight matching. We use\nEdmonds' algorithm to derive the structure of shortest augmenting paths. We\nextend this to a complete algorithm for maximum cardinality matching in time\n$O(\\sqrt n m)$. \n\n"}
{"id": "1703.05881", "contents": "Title: Complexity of Correspondence Homomorphisms Abstract: Correspondence homomorphisms are both a generalization of standard\nhomomorphisms and a generalization of correspondence colourings. For a fixed\ntarget graph $H$, the problem is to decide whether an input graph $G$, with\neach edge labeled by a pair of permutations of $V(H)$, admits a homomorphism to\n$H$ `corresponding' to the labels, in a sense explained below.\n  We classify the complexity of this problem as a function of the fixed graph\n$H$. It turns out that there is dichotomy -- each of the problems is\npolynomial-time solvable or NP-complete. While most graphs $H$ yield\nNP-complete problems, there are interesting cases of graphs $H$ for which the\nproblem is solved by Gaussian elimination.\n  We also classify the complexity of the analogous correspondence {\\em list\nhomomorphism} problems, and also the complexity of a {\\em bipartite version} of\nboth problems. We emphasize the proofs for the case when $H$ is reflexive, but,\nfor the record, we include a rough sketch of the remaining proofs in an\nAppendix. \n\n"}
{"id": "1703.06040", "contents": "Title: Towards a Topology-Shape-Metrics Framework for Ortho-Radial Drawings Abstract: Ortho-Radial drawings are a generalization of orthogonal drawings to grids\nthat are formed by concentric circles and straight-line spokes emanating from\nthe circles' center. Such drawings have applications in schematic graph\nlayouts, e.g., for metro maps and destination maps.\n  A plane graph is a planar graph with a fixed planar embedding. We give a\ncombinatorial characterization of the plane graphs that admit a planar\northo-radial drawing without bends. Previously, such a characterization was\nonly known for paths, cycles, and theta graphs, and in the special case of\nrectangular drawings for cubic graphs, where the contour of each face is\nrequired to be a rectangle.\n  The characterization is expressed in terms of an ortho-radial representation\nthat, similar to Tamassia's orthogonal representations for orthogonal drawings\ndescribes such a drawing combinatorially in terms of angles around vertices and\nbends on the edges. In this sense our characterization can be seen as a first\nstep towards generalizing the Topology-Shape-Metrics framework of Tamassia to\northo-radial drawings. \n\n"}
{"id": "1703.07047", "contents": "Title: High-Resolution Breast Cancer Screening with Multi-View Deep\n  Convolutional Neural Networks Abstract: Advances in deep learning for natural images have prompted a surge of\ninterest in applying similar techniques to medical images. The majority of the\ninitial attempts focused on replacing the input of a deep convolutional neural\nnetwork with a medical image, which does not take into consideration the\nfundamental differences between these two types of images. Specifically, fine\ndetails are necessary for detection in medical images, unlike in natural images\nwhere coarse structures matter most. This difference makes it inadequate to use\nthe existing network architectures developed for natural images, because they\nwork on heavily downscaled images to reduce the memory requirements. This hides\ndetails necessary to make accurate predictions. Additionally, a single exam in\nmedical imaging often comes with a set of views which must be fused in order to\nreach a correct conclusion. In our work, we propose to use a multi-view deep\nconvolutional neural network that handles a set of high-resolution medical\nimages. We evaluate it on large-scale mammography-based breast cancer screening\n(BI-RADS prediction) using 886,000 images. We focus on investigating the impact\nof the training set size and image size on the prediction accuracy. Our results\nhighlight that performance increases with the size of training set, and that\nthe best performance can only be achieved using the original resolution. In the\nreader study, performed on a random subset of the test set, we confirmed the\nefficacy of our model, which achieved performance comparable to a committee of\nradiologists when presented with the same data. \n\n"}
{"id": "1703.07406", "contents": "Title: Subset sum problem in polycyclic groups Abstract: We consider a group-theoretic analogue of the classic subset sum problem. It\nis known that every virtually nilpotent group has polynomial time decidable\nsubset sum problem. In this paper we use subgroup distortion to show that every\npolycyclic non-virtually-nilpotent group has NP-complete subset sum problem. \n\n"}
{"id": "1703.07734", "contents": "Title: On the Probe Complexity of Local Computation Algorithms Abstract: The Local Computation Algorithms (LCA) model is a computational model aimed\nat problem instances with huge inputs and output. For graph problems, the input\ngraph is accessed using probes: strong probes (SP) specify a vertex $v$ and\nreceive as a reply a list of $v$'s neighbors, and weak probes (WP) specify a\nvertex $v$ and a port number $i$ and receive as a reply $v$'s $i^{th}$\nneighbor. Given a local query (e.g., \"is a certain vertex in the vertex cover\nof the input graph?\"), an LCA should compute the corresponding local output\n(e.g., \"yes\" or \"no\") while making only a small number of probes, with the\nrequirement that all local outputs form a single global solution (e.g., a legal\nvertex cover). We study the probe complexity of LCAs that are required to work\non graphs that may have arbitrarily large degrees. In particular, such LCAs are\nexpected to probe the graph a number of times that is significantly smaller\nthan the maximum, average, or even minimum degree.\n  For weak probes, we focus on the weak coloring problem. Among our results we\nshow a separation between weak 3-coloring and weak 2-coloring for deterministic\nLCAs: $\\log^* n + O(1)$ weak probes suffice for weak 3-coloring, but\n$\\Omega\\left(\\frac{\\log n}{\\log\\log n}\\right)$ weak probes are required for\nweak 2-coloring.\n  For strong probes, we consider randomized LCAs for vertex cover and\nmaximal/maximum matching. Our negative results include showing that there are\ngraphs for which finding a \\emph{maximal} matching requires $\\Omega(\\sqrt{n})$\nstrong probes. On the positive side, we design a randomized LCA that finds a\n$(1-\\epsilon)$ approximation to \\emph{maximum} matching in regular graphs, and\nuses $\\frac{1}{\\epsilon }^{O\\left( \\frac{1}{\\epsilon ^2}\\right)}$ probes,\nindependently of the number of vertices and of their degrees. \n\n"}
{"id": "1703.07891", "contents": "Title: Width Hierarchies for Quantum and Classical Ordered Binary Decision\n  Diagrams with Repeated Test Abstract: We consider quantum, nondterministic and probabilistic versions of known\ncomputational model Ordered Read-$k$-times Branching Programs or Ordered Binary\nDecision Diagrams with repeated test ($k$-QOBDD, $k$-NOBDD and $k$-POBDD). We\nshow width hierarchy for complexity classes of Boolean function computed by\nthese models and discuss relation between different variants of $k$-OBDD. \n\n"}
{"id": "1703.08940", "contents": "Title: Tree Edit Distance Cannot be Computed in Strongly Subcubic Time (unless\n  APSP can) Abstract: The edit distance between two rooted ordered trees with $n$ nodes labeled\nfrom an alphabet~$\\Sigma$ is the minimum cost of transforming one tree into the\nother by a sequence of elementary operations consisting of deleting and\nrelabeling existing nodes, as well as inserting new nodes. Tree edit distance\nis a well known generalization of string edit distance. The fastest known\nalgorithm for tree edit distance runs in cubic $O(n^3)$ time and is based on a\nsimilar dynamic programming solution as string edit distance. In this paper we\nshow that a truly subcubic $O(n^{3-\\varepsilon})$ time algorithm for tree edit\ndistance is unlikely: For $|\\Sigma| = \\Omega(n)$, a truly subcubic algorithm\nfor tree edit distance implies a truly subcubic algorithm for the all pairs\nshortest paths problem. For $|\\Sigma| = O(1)$, a truly subcubic algorithm for\ntree edit distance implies an $O(n^{k-\\varepsilon})$ algorithm for finding a\nmaximum weight $k$-clique.\n  Thus, while in terms of upper bounds string edit distance and tree edit\ndistance are highly related, in terms of lower bounds string edit distance\nexhibits the hardness of the strong exponential time hypothesis [Backurs, Indyk\nSTOC'15] whereas tree edit distance exhibits the hardness of all pairs shortest\npaths. Our result provides a matching conditional lower bound for one of the\nlast remaining classic dynamic programming problems. \n\n"}
{"id": "1704.00003", "contents": "Title: Spectral Methods for Nonparametric Models Abstract: Nonparametric models are versatile, albeit computationally expensive, tool\nfor modeling mixture models. In this paper, we introduce spectral methods for\nthe two most popular nonparametric models: the Indian Buffet Process (IBP) and\nthe Hierarchical Dirichlet Process (HDP). We show that using spectral methods\nfor the inference of nonparametric models are computationally and statistically\nefficient. In particular, we derive the lower-order moments of the IBP and the\nHDP, propose spectral algorithms for both models, and provide reconstruction\nguarantees for the algorithms. For the HDP, we further show that applying\nhierarchical models on dataset with hierarchical structure, which can be solved\nwith the generalized spectral HDP, produces better solutions to that of flat\nmodels regarding likelihood performance. \n\n"}
{"id": "1704.00899", "contents": "Title: Dynamic Rank Maximal Matchings Abstract: We consider the problem of matching applicants to posts where applicants have\npreferences over posts. Thus the input to our problem is a bipartite graph G =\n(A U P,E), where A denotes a set of applicants, P is a set of posts, and there\nare ranks on edges which denote the preferences of applicants over posts. A\nmatching M in G is called rank-maximal if it matches the maximum number of\napplicants to their rank 1 posts, subject to this the maximum number of\napplicants to their rank 2 posts, and so on.\n  We consider this problem in a dynamic setting, where vertices and edges can\nbe added and deleted at any point. Let n and m be the number of vertices and\nedges in an instance G, and r be the maximum rank used by any rank-maximal\nmatching in G. We give a simple O(r(m+n))-time algorithm to update an existing\nrank-maximal matching under each of these changes. When r = o(n), this is\nfaster than recomputing a rank-maximal matching completely using a known\nalgorithm like that of Irving et al., which takes time O(min((r + n,\nr*sqrt(n))m). \n\n"}
{"id": "1704.02227", "contents": "Title: Training Triplet Networks with GAN Abstract: Triplet networks are widely used models that are characterized by good\nperformance in classification and retrieval tasks. In this work we propose to\ntrain a triplet network by putting it as the discriminator in Generative\nAdversarial Nets (GANs). We make use of the good capability of representation\nlearning of the discriminator to increase the predictive quality of the model.\nWe evaluated our approach on Cifar10 and MNIST datasets and observed\nsignificant improvement on the classification performance using the simple k-nn\nmethod. \n\n"}
{"id": "1704.02700", "contents": "Title: 0/1/all CSPs, Half-Integral $A$-path Packing, and Linear-Time FPT\n  Algorithms Abstract: A recent trend in the design of FPT algorithms is exploiting the\nhalf-integrality of LP relaxations. In other words, starting with a\nhalf-integral optimal solution to an LP relaxation, we assign integral values\nto variables one-by-one by branch and bound. This technique is general and the\nresulting time complexity has a low dependency on the parameter. However, the\ntime complexity often becomes a large polynomial in the input size because we\nneed to compute half-integral optimal LP solutions.\n  In this paper, we address this issue by providing an $O(km)$-time algorithm\nfor solving the LPs arising from various FPT problems, where $k$ is the optimal\nvalue and $m$ is the number of edges/constraints. Our algorithm is based on\ninteresting connections among 0/1/all constraints, which has been studied in\nthe field of constraints satisfaction, $A$-path packing, which has been studied\nin the field of combinatorial optimization, and the LPs used in FPT algorithms.\nWith the aid of this algorithm, we obtain improved FPT algorithms for various\nproblems, including Group Feedback Vertex Set, Subset Feedback Vertex Set, Node\nMultiway Cut, Node Unique Label Cover, and Non-monochromatic Cycle Transversal.\nThe obtained running time for each of these problems is linear in the input\nsize and has the current smallest dependency on the parameter. In particular,\nthese algorithms are the first linear-time FPT algorithms for problems\nincluding Group Feedback Vertex Set and Non-monochromatic Cycle Transversal. \n\n"}
{"id": "1704.04370", "contents": "Title: Fast Similarity Sketching Abstract: We consider the $\\textit{Similarity Sketching}$ problem: Given a universe\n$[u] = \\{0,\\ldots, u-1\\}$ we want a random function $S$ mapping subsets\n$A\\subseteq [u]$ into vectors $S(A)$ of size $t$, such that the Jaccard\nsimilarity $J(A,B) = |A\\cap B|/|A\\cup B|$ between sets $A$ and $B$ is\npreserved. More precisely, define $X_i = [S(A)[i] =\n  S(B)[i]]$ and $X = \\sum_{i\\in [t]} X_i$. We want $E[X_i]=J(A,B)$, and we want\n$X$ to be strongly concentrated around $E[X] = t \\cdot J(A,B)$ (i.e.\nChernoff-style bounds). This is a fundamental problem which has found numerous\napplications in data mining, large-scale classification, computer vision,\nsimilarity search, etc. via the classic MinHash algorithm. The vectors $S(A)$\nare also called $\\textit{sketches}$. Strong concentration is critical, for\noften we want to sketch many sets $B_1,\\ldots,B_n$ so that we later, for a\nquery set $A$, can find (one of) the most similar $B_i$. It is then critical\nthat no $B_i$ looks much more similar to $A$ due to errors in the sketch.\n  The seminal $t\\times\\textit{MinHash}$ algorithm uses $t$ random hash\nfunctions $h_1,\\ldots, h_t$, and stores $\\left ( \\min_{a\\in A} h_1(A),\\ldots,\n\\min_{a\\in A} h_t(A) \\right )$ as the sketch of $A$. The main drawback of\nMinHash is, however, its $O(t\\cdot |A|)$ running time, and finding a sketch\nwith similar properties and faster running time has been the subject of several\npapers. (continued...) \n\n"}
{"id": "1704.04428", "contents": "Title: Parallel Multi Channel Convolution using General Matrix Multiplication Abstract: Convolutional neural networks (CNNs) have emerged as one of the most\nsuccessful machine learning technologies for image and video processing. The\nmost computationally intensive parts of CNNs are the convolutional layers,\nwhich convolve multi-channel images with multiple kernels. A common approach to\nimplementing convolutional layers is to expand the image into a column matrix\n(im2col) and perform Multiple Channel Multiple Kernel (MCMK) convolution using\nan existing parallel General Matrix Multiplication (GEMM) library. This im2col\nconversion greatly increases the memory footprint of the input matrix and\nreduces data locality.\n  In this paper we propose a new approach to MCMK convolution that is based on\nGeneral Matrix Multiplication (GEMM), but not on im2col. Our algorithm\neliminates the need for data replication on the input thereby enabling us to\napply the convolution kernels on the input images directly. We have implemented\nseveral variants of our algorithm on a CPU processor and an embedded ARM\nprocessor. On the CPU, our algorithm is faster than im2col in most cases. \n\n"}
{"id": "1704.04428", "contents": "Title: Parallel Multi Channel Convolution using General Matrix Multiplication Abstract: Convolutional neural networks (CNNs) have emerged as one of the most\nsuccessful machine learning technologies for image and video processing. The\nmost computationally intensive parts of CNNs are the convolutional layers,\nwhich convolve multi-channel images with multiple kernels. A common approach to\nimplementing convolutional layers is to expand the image into a column matrix\n(im2col) and perform Multiple Channel Multiple Kernel (MCMK) convolution using\nan existing parallel General Matrix Multiplication (GEMM) library. This im2col\nconversion greatly increases the memory footprint of the input matrix and\nreduces data locality.\n  In this paper we propose a new approach to MCMK convolution that is based on\nGeneral Matrix Multiplication (GEMM), but not on im2col. Our algorithm\neliminates the need for data replication on the input thereby enabling us to\napply the convolution kernels on the input images directly. We have implemented\nseveral variants of our algorithm on a CPU processor and an embedded ARM\nprocessor. On the CPU, our algorithm is faster than im2col in most cases. \n\n"}
{"id": "1704.04861", "contents": "Title: MobileNets: Efficient Convolutional Neural Networks for Mobile Vision\n  Applications Abstract: We present a class of efficient models called MobileNets for mobile and\nembedded vision applications. MobileNets are based on a streamlined\narchitecture that uses depth-wise separable convolutions to build light weight\ndeep neural networks. We introduce two simple global hyper-parameters that\nefficiently trade off between latency and accuracy. These hyper-parameters\nallow the model builder to choose the right sized model for their application\nbased on the constraints of the problem. We present extensive experiments on\nresource and accuracy tradeoffs and show strong performance compared to other\npopular models on ImageNet classification. We then demonstrate the\neffectiveness of MobileNets across a wide range of applications and use cases\nincluding object detection, finegrain classification, face attributes and large\nscale geo-localization. \n\n"}
{"id": "1704.05303", "contents": "Title: The Robot Routing Problem for Collecting Aggregate Stochastic Rewards Abstract: We propose a new model for formalizing reward collection problems on graphs\nwith dynamically generated rewards which may appear and disappear based on a\nstochastic model. The *robot routing problem* is modeled as a graph whose nodes\nare stochastic processes generating potential rewards over discrete time. The\nrewards are generated according to the stochastic process, but at each step, an\nexisting reward disappears with a given probability. The edges in the graph\nencode the (unit-distance) paths between the rewards' locations. On visiting a\nnode, the robot collects the accumulated reward at the node at that time, but\ntraveling between the nodes takes time. The optimization question asks to\ncompute an optimal (or epsilon-optimal) path that maximizes the expected\ncollected rewards.\n  We consider the finite and infinite-horizon robot routing problems. For\nfinite-horizon, the goal is to maximize the total expected reward, while for\ninfinite horizon we consider limit-average objectives. We study the\ncomputational and strategy complexity of these problems, establish NP-lower\nbounds and show that optimal strategies require memory in general. We also\nprovide an algorithm for computing epsilon-optimal infinite paths for arbitrary\nepsilon > 0. \n\n"}
{"id": "1704.05420", "contents": "Title: Diagonal RNNs in Symbolic Music Modeling Abstract: In this paper, we propose a new Recurrent Neural Network (RNN) architecture.\nThe novelty is simple: We use diagonal recurrent matrices instead of full. This\nresults in better test likelihood and faster convergence compared to regular\nfull RNNs in most of our experiments. We show the benefits of using diagonal\nrecurrent matrices with popularly used LSTM and GRU architectures as well as\nwith the vanilla RNN architecture, on four standard symbolic music datasets. \n\n"}
{"id": "1704.06084", "contents": "Title: Knowledge Fusion via Embeddings from Text, Knowledge Graphs, and Images Abstract: We present a baseline approach for cross-modal knowledge fusion. Different\nbasic fusion methods are evaluated on existing embedding approaches to show the\npotential of joining knowledge about certain concepts across modalities in a\nfused concept representation. \n\n"}
{"id": "1704.06176", "contents": "Title: Segmentation of the Proximal Femur from MR Images using Deep\n  Convolutional Neural Networks Abstract: Magnetic resonance imaging (MRI) has been proposed as a complimentary method\nto measure bone quality and assess fracture risk. However, manual segmentation\nof MR images of bone is time-consuming, limiting the use of MRI measurements in\nthe clinical practice. The purpose of this paper is to present an automatic\nproximal femur segmentation method that is based on deep convolutional neural\nnetworks (CNNs). This study had institutional review board approval and written\ninformed consent was obtained from all subjects. A dataset of volumetric\nstructural MR images of the proximal femur from 86 subject were\nmanually-segmented by an expert. We performed experiments by training two\ndifferent CNN architectures with multiple number of initial feature maps and\nlayers, and tested their segmentation performance against the gold standard of\nmanual segmentations using four-fold cross-validation. Automatic segmentation\nof the proximal femur achieved a high dice similarity score of 0.94$\\pm$0.05\nwith precision = 0.95$\\pm$0.02, and recall = 0.94$\\pm$0.08 using a CNN\narchitecture based on 3D convolution exceeding the performance of 2D CNNs. The\nhigh segmentation accuracy provided by CNNs has the potential to help bring the\nuse of structural MRI measurements of bone quality into clinical practice for\nmanagement of osteoporosis. \n\n"}
{"id": "1704.06361", "contents": "Title: Shared processor scheduling Abstract: We study the shared processor scheduling problem with a single shared\nprocessor where a unit time saving (weight) obtained by processing a job on the\nshared processor depends on the job. A polynomial-time optimization algorithm\nhas been given for the problem with equal weights in the literature. This paper\nextends that result by showing an $O(n \\log n)$ optimization algorithm for a\nclass of instances in which non-decreasing order of jobs with respect to\nprocessing times provides a non-increasing order with respect to weights ---\nthis instance generalizes the unweighted case of the problem. This algorithm\nalso leads to a $\\frac{1}{2}$-approximation algorithm for the general weighted\nproblem. The complexity of the weighted problem remains open. \n\n"}
{"id": "1704.07535", "contents": "Title: Abstract Syntax Networks for Code Generation and Semantic Parsing Abstract: Tasks like code generation and semantic parsing require mapping unstructured\n(or partially structured) inputs to well-formed, executable outputs. We\nintroduce abstract syntax networks, a modeling framework for these problems.\nThe outputs are represented as abstract syntax trees (ASTs) and constructed by\na decoder with a dynamically-determined modular structure paralleling the\nstructure of the output tree. On the benchmark Hearthstone dataset for code\ngeneration, our model obtains 79.2 BLEU and 22.7% exact match accuracy,\ncompared to previous state-of-the-art values of 67.1 and 6.1%. Furthermore, we\nperform competitively on the Atis, Jobs, and Geo semantic parsing datasets with\nno task-specific engineering. \n\n"}
{"id": "1704.08529", "contents": "Title: A polynomial-time randomized reduction from tournament isomorphism to\n  tournament asymmetry Abstract: The paper develops a new technique to extract a characteristic subset from a\nrandom source that repeatedly samples from a set of elements. Here a\ncharacteristic subset is a set that when containing an element contains all\nelements that have the same probability. With this technique at hand the paper\nlooks at the special case of the tournament isomorphism problem that stands in\nthe way towards a polynomial-time algorithm for the graph isomorphism problem.\nNoting that there is a reduction from the automorphism (asymmetry) problem to\nthe isomorphism problem, a reduction in the other direction is nevertheless not\nknown and remains a thorny open problem. Applying the new technique, we develop\na randomized polynomial-time Turing-reduction from the tournament isomorphism\nproblem to the tournament automorphism problem. This is the first such\nreduction for any kind of combinatorial object not known to have a\npolynomial-time solvable isomorphism problem. \n\n"}
{"id": "1705.00985", "contents": "Title: Determinant-Preserving Sparsification of SDDM Matrices with Applications\n  to Counting and Sampling Spanning Trees Abstract: We show variants of spectral sparsification routines can preserve the total\nspanning tree counts of graphs, which by Kirchhoff's matrix-tree theorem, is\nequivalent to determinant of a graph Laplacian minor, or equivalently, of any\nSDDM matrix. Our analyses utilizes this combinatorial connection to bridge\nbetween statistical leverage scores / effective resistances and the analysis of\nrandom graphs by [Janson, Combinatorics, Probability and Computing `94]. This\nleads to a routine that in quadratic time, sparsifies a graph down to about\n$n^{1.5}$ edges in ways that preserve both the determinant and the distribution\nof spanning trees (provided the sparsified graph is viewed as a random object).\nExtending this algorithm to work with Schur complements and approximate\nCholeksy factorizations leads to algorithms for counting and sampling spanning\ntrees which are nearly optimal for dense graphs.\n  We give an algorithm that computes a $(1 \\pm \\delta)$ approximation to the\ndeterminant of any SDDM matrix with constant probability in about $n^2\n\\delta^{-2}$ time. This is the first routine for graphs that outperforms\ngeneral-purpose routines for computing determinants of arbitrary matrices. We\nalso give an algorithm that generates in about $n^2 \\delta^{-2}$ time a\nspanning tree of a weighted undirected graph from a distribution with total\nvariation distance of $\\delta$ from the $w$-uniform distribution . \n\n"}
{"id": "1705.02944", "contents": "Title: Hardness Results for Structured Linear Systems Abstract: We show that if the nearly-linear time solvers for Laplacian matrices and\ntheir generalizations can be extended to solve just slightly larger families of\nlinear systems, then they can be used to quickly solve all systems of linear\nequations over the reals. This result can be viewed either positively or\nnegatively: either we will develop nearly-linear time algorithms for solving\nall systems of linear equations over the reals, or progress on the families we\ncan solve in nearly-linear time will soon halt. \n\n"}
{"id": "1705.03284", "contents": "Title: Towards a complexity theory for the congested clique Abstract: The congested clique model of distributed computing has been receiving\nattention as a model for densely connected distributed systems. While there has\nbeen significant progress on the side of upper bounds, we have very little in\nterms of lower bounds for the congested clique; indeed, it is now know that\nproving explicit congested clique lower bounds is as difficult as proving\ncircuit lower bounds.\n  In this work, we use various more traditional complexity-theoretic tools to\nbuild a clearer picture of the complexity landscape of the congested clique:\n  -- Nondeterminism and beyond: We introduce the nondeterministic congested\nclique model (analogous to NP) and show that there is a natural canonical\nproblem family that captures all problems solvable in constant time with\nnondeterministic algorithms. We further generalise these notions by introducing\nthe constant-round decision hierarchy (analogous to the polynomial hierarchy).\n  -- Non-constructive lower bounds: We lift the prior non-uniform counting\narguments to a general technique for proving non-constructive uniform lower\nbounds for the congested clique. In particular, we prove a time hierarchy\ntheorem for the congested clique, showing that there are decision problems of\nessentially all complexities, both in the deterministic and nondeterministic\nsettings.\n  -- Fine-grained complexity: We map out relationships between various natural\nproblems in the congested clique model, arguing that a reduction-based\ncomplexity theory currently gives us a fairly good picture of the complexity\nlandscape of the congested clique. \n\n"}
{"id": "1705.09379", "contents": "Title: Tensor rank is not multiplicative under the tensor product Abstract: The tensor rank of a tensor t is the smallest number r such that t can be\ndecomposed as a sum of r simple tensors. Let s be a k-tensor and let t be an\nl-tensor. The tensor product of s and t is a (k + l)-tensor. Tensor rank is\nsub-multiplicative under the tensor product. We revisit the connection between\nrestrictions and degenerations. A result of our study is that tensor rank is\nnot in general multiplicative under the tensor product. This answers a question\nof Draisma and Saptharishi. Specifically, if a tensor t has border rank\nstrictly smaller than its rank, then the tensor rank of t is not multiplicative\nunder taking a sufficiently hight tensor product power. The \"tensor Kronecker\nproduct\" from algebraic complexity theory is related to our tensor product but\ndifferent, namely it multiplies two k-tensors to get a k-tensor.\nNonmultiplicativity of the tensor Kronecker product has been known since the\nwork of Strassen.\n  It remains an open question whether border rank and asymptotic rank are\nmultiplicative under the tensor product. Interestingly, lower bounds on border\nrank obtained from generalised flattenings (including Young flattenings)\nmultiply under the tensor product. \n\n"}
{"id": "1706.01604", "contents": "Title: Hyperplane Clustering Via Dual Principal Component Pursuit Abstract: We extend the theoretical analysis of a recently proposed single subspace\nlearning algorithm, called Dual Principal Component Pursuit (DPCP), to the case\nwhere the data are drawn from of a union of hyperplanes. To gain insight into\nthe properties of the $\\ell_1$ non-convex problem associated with DPCP, we\ndevelop a geometric analysis of a closely related continuous optimization\nproblem. Then transferring this analysis to the discrete problem, our results\nstate that as long as the hyperplanes are sufficiently separated, the dominant\nhyperplane is sufficiently dominant and the points are uniformly distributed\ninside the associated hyperplanes, then the non-convex DPCP problem has a\nunique global solution, equal to the normal vector of the dominant hyperplane.\nThis suggests the correctness of a sequential hyperplane learning algorithm\nbased on DPCP. A thorough experimental evaluation reveals that hyperplane\nlearning schemes based on DPCP dramatically improve over the state-of-the-art\nmethods for the case of synthetic data, while are competitive to the\nstate-of-the-art in the case of 3D plane clustering for Kinect data. \n\n"}
{"id": "1706.02277", "contents": "Title: A Note on Multiparty Communication Complexity and the Hales-Jewett\n  Theorem Abstract: For integers $n$ and $k$, the density Hales-Jewett number $c_{n,k}$ is\ndefined as the maximal size of a subset of $[k]^n$ that contains no\ncombinatorial line. We show that for $k \\ge 3$ the density Hales-Jewett number\n$c_{n,k}$ is equal to the maximal size of a cylinder intersection in the\nproblem $Part_{n,k}$ of testing whether $k$ subsets of $[n]$ form a partition.\nIt follows that the communication complexity, in the Number On the Forehead\n(NOF) model, of $Part_{n,k}$, is equal to the minimal size of a partition of\n$[k]^n$ into subsets that do not contain a combinatorial line. Thus, the bound\nin \\cite{chattopadhyay2007languages} on $Part_{n,k}$ using the Hales-Jewett\ntheorem is in fact tight, and the density Hales-Jewett number can be thought of\nas a quantity in communication complexity. This gives a new angle to this well\nstudied quantity.\n  As a simple application we prove a lower bound on $c_{n,k}$, similar to the\nlower bound in \\cite{polymath2010moser} which is roughly $c_{n,k}/k^n \\ge\n\\exp(-O(\\log n)^{1/\\lceil \\log_2 k\\rceil})$. This lower bound follows from a\nprotocol for $Part_{n,k}$. It is interesting to better understand the\ncommunication complexity of $Part_{n,k}$ as this will also lead to the better\nunderstanding of the Hales-Jewett number. The main purpose of this note is to\nmotivate this study. \n\n"}
{"id": "1706.03177", "contents": "Title: Parameterized Algorithms for Power-Efficiently Connecting Wireless\n  Sensor Networks: Theory and Experiments Abstract: We study an NP-hard problem motivated by energy-efficiently maintaining the\nconnectivity of a symmetric wireless communication network: Given an\nedge-weighted $n$-vertex graph, find a connected spanning subgraph of minimum\ncost, where the cost is determined by letting each vertex pay the most\nexpensive edge incident to it in the subgraph. On the negative side, we show\nthat $o(\\log n)$-approximating the difference $d$ between the optimal solution\ncost and a natural lower bound is NP-hard and that, under the Exponential Time\nHypothesis, there are no exact algorithms running in $2^{o(n)}$ time or in\n$f(d)\\cdot n^{O(1)}$ time for any computable function $f$. Moreover, we show\nthat the special case of connecting $c$ network components with minimum\nadditional cost generally cannot be polynomial-time reduced to instances of\nsize $c^{O(1)}$ unless the polynomial-time hierarchy collapses. On the positive\nside, we provide an algorithm that reconnects $O(\\log n)$ connected components\nwith minimum additional cost in polynomial time. These algorithms are motivated\nby application scenarios of monitoring areas or where an existing sensor\nnetwork may fall apart into several connected components due to sensor faults.\nIn experiments, the algorithm outperforms CPLEX with known ILP formulations\nwhen $n$ is sufficiently large compared to $c$. \n\n"}
{"id": "1706.03607", "contents": "Title: Clustering Small Samples with Quality Guarantees: Adaptivity with\n  One2all pps Abstract: Clustering of data points is a fundamental tool in data analysis. We consider\npoints $X$ in a relaxed metric space, where the triangle inequality holds\nwithin a constant factor. The {\\em cost} of clustering $X$ by $Q$ is\n$V(Q)=\\sum_{x\\in X} d_{xQ}$. Two basic tasks, parametrized by $k \\geq 1$, are\n{\\em cost estimation}, which returns (approximate) $V(Q)$ for queries $Q$ such\nthat $|Q|=k$ and {\\em clustering}, which returns an (approximate) minimizer of\n$V(Q)$ of size $|Q|=k$. With very large data sets $X$, we seek efficient\nconstructions of small samples that act as surrogates to the full data for\nperforming these tasks. Existing constructions that provide quality guarantees\nare either worst-case, and unable to benefit from structure of real data sets,\nor make explicit strong assumptions on the structure. We show here how to avoid\nboth these pitfalls using adaptive designs.\n  At the core of our design is the {\\em one2all} construction of\nmulti-objective probability-proportional-to-size (pps) samples: Given a set $M$\nof centroids and $\\alpha \\geq 1$, one2all efficiently assigns probabilities to\npoints so that the clustering cost of {\\em each} $Q$ with cost $V(Q) \\geq\nV(M)/\\alpha$ can be estimated well from a sample of size $O(\\alpha\n|M|\\epsilon^{-2})$. For cost queries, we can obtain worst-case sample size\n$O(k\\epsilon^{-2})$ by applying one2all to a bicriteria approximation $M$, but\nwe adaptively balance $|M|$ and $\\alpha$ to further reduce sample size. For\nclustering, we design an adaptive wrapper that applies a base clustering\nalgorithm to a sample $S$. Our wrapper uses the smallest sample that provides\nstatistical guarantees that the quality of the clustering on the sample carries\nover to the full data set. We demonstrate experimentally the huge gains of\nusing our adaptive instead of worst-case methods. \n\n"}
{"id": "1706.04708", "contents": "Title: Experimental Study of Compressed Stack Algorithms in Limited Memory\n  Environments Abstract: The {\\em compressed stack} is a data structure designed by Barba {\\em et al.}\n(Algorithmica 2015) that allows to reduce the amount of memory needed by an\nalgorithm (at the cost of increasing its runtime). In this paper we introduce\nthe first implementation of this data structure and make its source code\npublicly available.\n  Together with the implementation we analyze the performance of the compressed\nstack. In our synthetic experiments, considering different test scenarios and\nusing data sizes ranging up to $2^{30}$ elements, we compare it with the\nclassic (uncompressed) stack, both in terms of runtime and memory used.\n  Our experiments show that the compressed stack needs significantly less\nmemory than the usual stack (this difference is significant for inputs\ncontaining $2000$ or more elements). Overall, with a proper choice of\nparameters, we can save a significant amount of space (from two to four orders\nof magnitude) with a small increase in the runtime ($2.32$ times slower on\naverage than the classic stack). These results holds even in test scenarios\nspecifically designed to be challenging for the compressed stack. \n\n"}
{"id": "1706.05599", "contents": "Title: Sample, computation vs storage tradeoffs for classification using tensor\n  subspace models Abstract: In this paper, we exhibit the tradeoffs between the (training) sample,\ncomputation and storage complexity for the problem of supervised classification\nusing signal subspace estimation. Our main tool is the use of tensor subspaces,\ni.e. subspaces with a Kronecker structure, for embedding the data into lower\ndimensions. Among the subspaces with a Kronecker structure, we show that using\nsubspaces with a hierarchical structure for representing data leads to improved\ntradeoffs. One of the main reasons for the improvement is that embedding data\ninto these hierarchical Kronecker structured subspaces prevents overfitting at\nhigher latent dimensions. \n\n"}
{"id": "1706.05761", "contents": "Title: Approximate Generalized Matching: $f$-Factors and $f$-Edge Covers Abstract: In this paper we present linear time approximation schemes for several\ngeneralized matching problems on nonbipartite graphs. Our results include\n$O_\\epsilon(m)$-time algorithms for $(1-\\epsilon)$-maximum weight $f$-factor\nand $(1+\\epsilon)$-approximate minimum weight $f$-edge cover. As a byproduct,\nwe also obtain direct algorithms for the exact cardinality versions of these\nproblems running in $O(m\\sqrt{f(V)})$ time.\n  The technical contributions of this work include an efficient method for\nmaintaining {\\em relaxed complementary slackness} in generalized matching\nproblems and approximation-preserving reductions between the $f$-factor and\n$f$-edge cover problems. \n\n"}
{"id": "1706.06084", "contents": "Title: Solving Integer Linear Programs with a Small Number of Global Variables\n  and Constraints Abstract: Integer Linear Programming (ILP) has a broad range of applications in various\nareas of artificial intelligence. Yet in spite of recent advances, we still\nlack a thorough understanding of which structural restrictions make ILP\ntractable. Here we study ILP instances consisting of a small number of \"global\"\nvariables and/or constraints such that the remaining part of the instance\nconsists of small and otherwise independent components; this is captured in\nterms of a structural measure we call fracture backdoors which generalizes, for\ninstance, the well-studied class of N -fold ILP instances.\n  Our main contributions can be divided into three parts. First, we formally\ndevelop fracture backdoors and obtain exact and approximation algorithms for\ncomputing these. Second, we exploit these backdoors to develop several new\nparameterized algorithms for ILP; the performance of these algorithms will\nnaturally scale based on the number of global variables or constraints in the\ninstance. Finally, we complement the developed algorithms with matching lower\nbounds. Altogether, our results paint a near-complete complexity landscape of\nILP with respect to fracture backdoors. \n\n"}
{"id": "1706.06467", "contents": "Title: Approximating the Volume of Tropical Polytopes is Difficult Abstract: We investigate the complexity of counting the number of integer points in\ntropical polytopes, and the complexity of calculating their volume. We study\nthe tropical analogue of the outer parallel body and establish bounds for its\nvolume. We deduce that there is no approximation algorithm of factor\n$\\alpha=2^{\\text{poly}(m,n)}$ for the volume of a tropical polytope given by\n$n$ vertices in a space of dimension $m$, unless P$=$NP. Neither is there such\nan approximation algorithm for counting the number of integer points in\ntropical polytopes described by vertices. If follows that approximating these\nvalues for tropical polytopes is more difficult than for classical polytopes.\nOur proofs use a reduction from the problem of calculating the tropical rank.\nFor tropical polytopes described by inequalities we prove that counting the\nnumber of integer points and calculating the volume are $\\#$P-hard. \n\n"}
{"id": "1706.06873", "contents": "Title: MEC: Memory-efficient Convolution for Deep Neural Network Abstract: Convolution is a critical component in modern deep neural networks, thus\nseveral algorithms for convolution have been developed. Direct convolution is\nsimple but suffers from poor performance. As an alternative, multiple indirect\nmethods have been proposed including im2col-based convolution, FFT-based\nconvolution, or Winograd-based algorithm. However, all these indirect methods\nhave high memory-overhead, which creates performance degradation and offers a\npoor trade-off between performance and memory consumption. In this work, we\npropose a memory-efficient convolution or MEC with compact lowering, which\nreduces memory-overhead substantially and accelerates convolution process. MEC\nlowers the input matrix in a simple yet efficient/compact way (i.e., much less\nmemory-overhead), and then executes multiple small matrix multiplications in\nparallel to get convolution completed. Additionally, the reduced memory\nfootprint improves memory sub-system efficiency, improving performance. Our\nexperimental results show that MEC reduces memory consumption significantly\nwith good speedup on both mobile and server platforms, compared with other\nindirect convolution algorithms. \n\n"}
{"id": "1706.08115", "contents": "Title: Steiner Point Removal with Distortion $O(\\log k)$ Abstract: In the Steiner point removal (SPR) problem, we are given a weighted graph\n$G=(V,E)$ and a set of terminals $K\\subset V$ of size $k$. The objective is to\nfind a minor $M$ of $G$ with only the terminals as its vertex set, such that\nthe distance between the terminals will be preserved up to a small\nmultiplicative distortion. Kamma, Krauthgamer and Nguyen [KKN15] used a\nball-growing algorithm with exponential distributions to show that the\ndistortion is at most $O(\\log^5 k)$. Cheung [Che17] improved the analysis of\nthe same algorithm, bounding the distortion by $O(\\log^2 k)$. We improve the\nanalysis of this ball-growing algorithm even further, bounding the distortion\nby $O(\\log k)$. \n\n"}
{"id": "1706.09370", "contents": "Title: DynASP2.5: Dynamic Programming on Tree Decompositions in Action Abstract: A vibrant theoretical research area are efficient exact parameterized\nalgorithms. Very recent solving competitions such as the PACE challenge show\nthat there is also increasing practical interest in the parameterized\nalgorithms community. An important research question is whether dedicated\nparameterized exact algorithms exhibit certain practical relevance and one can\neven beat well-established problem solvers. We consider the logic-based\ndeclarative modeling language and problem solving framework Answer Set\nProgramming (ASP). State-of-the-art ASP solvers rely considerably on Sat-based\nalgorithms. An ASP solver (DynASP2), which is based on a classical dynamic\nprogramming on tree decompositions, has been published very recently.\nUnfortunately, DynASP2 can outperform modern ASP solvers on programs of small\ntreewidth only if the question of interest is to count the number of solutions.\nIn this paper, we describe underlying concepts of our new implementation\n(DynASP2.5) that shows competitive behavior to state-of-the-art ASP solvers\neven for finding just one solution when solving problems as the Steiner tree\nproblem that have been modeled in ASP on graphs with low treewidth. Our\nimplementation is based on a novel approach that we call multi-pass dynamic\nprogramming (M-DPSINC). \n\n"}
{"id": "1706.10195", "contents": "Title: Agglomerative Clustering of Growing Squares Abstract: We study an agglomerative clustering problem motivated by interactive glyphs\nin geo-visualization. Consider a set of disjoint square glyphs on an\ninteractive map. When the user zooms out, the glyphs grow in size relative to\nthe map, possibly with different speeds. When two glyphs intersect, we wish to\nreplace them by a new glyph that captures the information of the intersecting\nglyphs.\n  We present a fully dynamic kinetic data structure that maintains a set of $n$\ndisjoint growing squares. Our data structure uses $O(n (\\log n \\log\\log n)^2)$\nspace, supports queries in worst case $O(\\log^3 n)$ time, and updates in\n$O(\\log^7 n)$ amortized time. This leads to an $O(n\\alpha(n)\\log^7 n)$ time\nalgorithm to solve the agglomerative clustering problem. This is a significant\nimprovement over the current best $O(n^2)$ time algorithms. \n\n"}
{"id": "1706.10209", "contents": "Title: Storage, Communication, and Load Balancing Trade-off in Distributed\n  Cache Networks Abstract: We consider load balancing in a network of caching servers delivering\ncontents to end users. Randomized load balancing via the so-called power of two\nchoices is a well-known approach in parallel and distributed systems. In this\nframework, we investigate the tension between storage resources, communication\ncost, and load balancing performance. To this end, we propose a randomized load\nbalancing scheme which simultaneously considers cache size limitation and\nproximity in the server redirection process.\n  In contrast to the classical power of two choices setup, since the memory\nlimitation and the proximity constraint cause correlation in the server\nselection process, we may not benefit from the power of two choices. However,\nwe prove that in certain regimes of problem parameters, our scheme results in\nthe maximum load of order $\\Theta(\\log\\log n)$ (here $n$ is the network size).\nThis is an exponential improvement compared to the scheme which assigns each\nrequest to the nearest available replica. Interestingly, the extra\ncommunication cost incurred by our proposed scheme, compared to the nearest\nreplica strategy, is small. Furthermore, our extensive simulations show that\nthe trade-off trend does not depend on the network topology and library\npopularity profile details. \n\n"}
{"id": "1706.10295", "contents": "Title: Noisy Networks for Exploration Abstract: We introduce NoisyNet, a deep reinforcement learning agent with parametric\nnoise added to its weights, and show that the induced stochasticity of the\nagent's policy can be used to aid efficient exploration. The parameters of the\nnoise are learned with gradient descent along with the remaining network\nweights. NoisyNet is straightforward to implement and adds little computational\noverhead. We find that replacing the conventional exploration heuristics for\nA3C, DQN and dueling agents (entropy reward and $\\epsilon$-greedy respectively)\nwith NoisyNet yields substantially higher scores for a wide range of Atari\ngames, in some cases advancing the agent from sub to super-human performance. \n\n"}
{"id": "1707.00943", "contents": "Title: The sample complexity of multi-reference alignment Abstract: The growing role of data-driven approaches to scientific discovery has\nunveiled a large class of models that involve latent transformations with a\nrigid algebraic constraint. Three-dimensional molecule reconstruction in\nCryo-Electron Microscopy (cryo-EM) is a central problem in this class. Despite\ndecades of algorithmic and software development, there is still little\ntheoretical understanding of the sample complexity of this problem, that is,\nnumber of images required for 3-D reconstruction. Here we consider\nmulti-reference alignment (MRA), a simple model that captures fundamental\naspects of the statistical and algorithmic challenges arising in cryo-EM and\nrelated problems. In MRA, an unknown signal is subject to two types of\ncorruption: a latent cyclic shift and the more traditional additive white\nnoise. The goal is to recover the signal at a certain precision from\nindependent samples. While at high signal-to-noise ratio (SNR), the number of\nobservations needed to recover a generic signal is proportional to\n$1/\\mathrm{SNR}$, we prove that it rises to a surprising $1/\\mathrm{SNR}^3$ in\nthe low SNR regime. This precise phenomenon was observed empirically more than\ntwenty years ago for cryo-EM but has remained unexplained to date. Furthermore,\nour techniques can easily be extended to the heterogeneous MRA model where the\nsamples come from a mixture of signals, as is often the case in applications\nsuch as cryo-EM, where molecules may have different conformations. This\nprovides a first step towards a statistical theory for heterogeneous cryo-EM. \n\n"}
{"id": "1707.03303", "contents": "Title: A characterization of testable hypergraph properties Abstract: We provide a combinatorial characterization of all testable properties of\n$k$-uniform hypergraphs ($k$-graphs for short). Here, a $k$-graph property $P$\nis testable if there is a randomized algorithm which makes a bounded number of\nedge queries and distinguishes with probability $2/3$ between $k$-graphs that\nsatisfy $P$ and those that are far from satisfying $P$. For the $2$-graph case,\nsuch a combinatorial characterization was obtained by Alon, Fischer, Newman and\nShapira. Our results for the $k$-graph setting are in contrast to those of\nAustin and Tao, who showed that for the somewhat stronger concept of local\nrepairability, the testability results for graphs do not extend to the\n$3$-graph setting. Our proof relies on a random subhypergraph sampling result\nproved in a companion paper. \n\n"}
{"id": "1707.03811", "contents": "Title: Computational complexity and 3-manifolds and zombies Abstract: We show the problem of counting homomorphisms from the fundamental group of a\nhomology $3$-sphere $M$ to a finite, non-abelian simple group $G$ is\n#P-complete, in the case that $G$ is fixed and $M$ is the computational input.\nSimilarly, deciding if there is a non-trivial homomorphism is NP-complete. In\nboth reductions, we can guarantee that every non-trivial homomorphism is a\nsurjection. As a corollary, for any fixed integer $m \\ge 5$, it is NP-complete\nto decide whether $M$ admits a connected $m$-sheeted covering.\n  Our construction is inspired by universality results in topological quantum\ncomputation. Given a classical reversible circuit $C$, we construct $M$ so that\nevaluations of $C$ with certain initialization and finalization conditions\ncorrespond to homomorphisms $\\pi_1(M) \\to G$. An intermediate state of $C$\nlikewise corresponds to a homomorphism $\\pi_1(\\Sigma_g) \\to G$, where\n$\\Sigma_g$ is a pointed Heegaard surface of $M$ of genus $g$. We analyze the\naction on these homomorphisms by the pointed mapping class group\n$\\text{MCG}_*(\\Sigma_g)$ and its Torelli subgroup $\\text{Tor}_*(\\Sigma_g)$. By\nresults of Dunfield-Thurston, the action of $\\text{MCG}_*(\\Sigma_g)$ is as\nlarge as possible when $g$ is sufficiently large; we can pass to the Torelli\ngroup using the congruence subgroup property of $\\text{Sp}(2g,\\mathbb{Z})$. Our\nresults can be interpreted as a sharp classical universality property of an\nassociated combinatorial $(2+1)$-dimensional TQFT. \n\n"}
{"id": "1707.04875", "contents": "Title: Coding sets with asymmetric information Abstract: We study the following one-way asymmetric transmission problem, also a\nvariant of model-based compressed sensing: a resource-limited encoder has to\nreport a small set $S$ from a universe of $N$ items to a more powerful decoder\n(server). The distinguishing feature is asymmetric information: the subset $S$\nis comprised of i.i.d. samples from a prior distribution $\\mu$, and $\\mu$ is\nonly known to the decoder. The goal for the encoder is to encode $S$\nobliviously, while achieving the information-theoretic bound of $|S| \\cdot\nH(\\mu)$, i.e., the Shannon entropy bound.\n  We first show that any such compression scheme must be {\\em randomized}, if\nit gains non-trivially from the prior $\\mu$. This stands in contrast to the\nsymmetric case (when both the encoder and decoder know $\\mu$), where the\nHuffman code provides a near-optimal deterministic solution. On the other hand,\na rather simple argument shows that, when $|S|=k$, a random linear code\nachieves near-optimal communication rate of about $k\\cdot H(\\mu)$ bits. Alas,\nthe resulting scheme has prohibitive decoding time: about ${N\\choose k} \\approx\n(N/k)^k$.\n  Our main result is a computationally efficient and linear coding scheme,\nwhich achieves an $O(\\lg\\lg N)$-competitive communication ratio compared to the\noptimal benchmark, and runs in $\\text{poly}(N,k)$ time. Our \"multi-level\"\ncoding scheme uses a combination of hashing and syndrome-decoding of\nReed-Solomon codes, and relies on viewing the (unknown) prior $\\mu$ as a rather\nsmall convex combination of uniform (\"flat\") distributions. \n\n"}
{"id": "1707.05808", "contents": "Title: A Note on Unconditional Subexponential-time Pseudo-deterministic\n  Algorithms for BPP Search Problems Abstract: We show the first unconditional pseudo-determinism result for all of\nsearch-BPP. Specifically, we show that every BPP search problem can be computed\npseudo-deterministically on average for infinitely many input lengths. In other\nwords, for infinitely many input lengths and for any polynomial-time samplable\ndistribution our algorithm succeeds in producing a unique answer (if one\nexists) with high probability over the distribution and the coins tossed. \n\n"}
{"id": "1707.06364", "contents": "Title: An Alon-Boppana Type Bound for Weighted Graphs and Lowerbounds for\n  Spectral Sparsification Abstract: We prove the following Alon-Boppana type theorem for general (not necessarily\nregular) weighted graphs: if $G$ is an $n$-node weighted undirected graph of\naverage combinatorial degree $d$ (that is, $G$ has $dn/2$ edges) and girth $g>\n2d^{1/8}+1$, and if $\\lambda_1 \\leq \\lambda_2 \\leq \\cdots \\lambda_n$ are the\neigenvalues of the (non-normalized) Laplacian of $G$, then \\[ \\frac\n{\\lambda_n}{\\lambda_2} \\geq 1 + \\frac 4{\\sqrt d} - O \\left( \\frac 1{d^{\\frac\n58} }\\right) \\] (The Alon-Boppana theorem implies that if $G$ is unweighted and\n$d$-regular, then $\\frac {\\lambda_n}{\\lambda_2} \\geq 1 + \\frac 4{\\sqrt d} -\nO\\left( \\frac 1 d \\right)$ if the diameter is at least $d^{1.5}$.)\n  Our result implies a lower bound for spectral sparsifiers. A graph $H$ is a\nspectral $\\epsilon$-sparsifier of a graph $G$ if \\[ L(G) \\preceq L(H) \\preceq\n(1+\\epsilon) L(G) \\] where $L(G)$ is the Laplacian matrix of $G$ and $L(H)$ is\nthe Laplacian matrix of $H$. Batson, Spielman and Srivastava proved that for\nevery $G$ there is an $\\epsilon$-sparsifier $H$ of average degree $d$ where\n$\\epsilon \\approx \\frac {4\\sqrt 2}{\\sqrt d}$ and the edges of $H$ are a\n(weighted) subset of the edges of $G$. Batson, Spielman and Srivastava also\nshow that the bound on $\\epsilon$ cannot be reduced below $\\approx \\frac\n2{\\sqrt d}$ when $G$ is a clique; our Alon-Boppana-type result implies that\n$\\epsilon$ cannot be reduced below $\\approx \\frac 4{\\sqrt d}$ when $G$ comes\nfrom a family of expanders of super-constant degree and super-constant girth.\n  The method of Batson, Spielman and Srivastava proves a more general result,\nabout sparsifying sums of rank-one matrices, and their method applies to an\n\"online\" setting. We show that for the online matrix setting the $4\\sqrt 2 /\n\\sqrt d$ bound is tight, up to lower order terms. \n\n"}
{"id": "1707.08484", "contents": "Title: MST in O(1) Rounds of the Congested Clique Abstract: We present a distributed randomized algorithm finding Minimum Spanning Tree\n(MST) of a given graph in O(1) rounds, with high probability, in the Congested\nClique model. The input graph in the Congested Clique model is a graph of n\nnodes, where each node initially knows only its incident edges. The\ncommunication graph is a clique with limited edge bandwidth: each two nodes\n(not necessarily neighbours in the input graph) can exchange $O(\\log n)$ bits.\n  As in previous works, the key part of the MST algorithm is an efficient\nConnected Components (CC) algorithm. However, unlike the former approaches, we\ndo not aim at simulating the standard Boruvka algorithm, at least at initial\nstages of the CC algorithm. Instead, we develop a new technique which combines\nconnected components of sample sparse subgraphs of the input graph in order to\naccelerate the process of uncovering connected components of the original input\ngraph. More specifically, we develop a sparsification technique which reduces\nan initial CC problem in $O(1)$ rounds to its two restricted instances. The\nformer instance has a graph with maximal degree $O(\\log \\log n)$ as the input\n-- here our sample-combining technique helps. In the latter instance, a\npartition of the input graph into $O(n/\\log \\log n)$ connected components is\nknown. This gives an opportunity to apply previous algorithms to determine\nconnected components in $O(1)$ rounds.\n  Our result addresses the problem from and the $O(\\log \\log n)$ algorithm of\nLotker et al. [SPAA 2003; SICOMP 2005], improves over previous $O(\\log* n)$\nalgorithm of Ghaffari et al. [PODC 2016] and $O(\\log \\log \\log n)$ algorithm of\nHegeman et al. [PODC 2015] . It also determines $\\Theta(1)$ round complexity in\nthe congested clique for MST, as well as other graph problems, including\nbipartiteness, cut verification, s-t connectivity and cycle containment. \n\n"}
{"id": "1707.08769", "contents": "Title: Ramsey Spanning Trees and their Applications Abstract: The metric Ramsey problem asks for the largest subset $S$ of a metric space\nthat can be embedded into an ultrametric (more generally into a Hilbert space)\nwith a given distortion. Study of this problem was motivated as a non-linear\nversion of Dvoretzky theorem. Mendel and Naor 2007 devised the so called Ramsey\nPartitions to address this problem, and showed the algorithmic applications of\ntheir techniques to approximate distance oracles and ranking problems.\n  In this paper we study the natural extension of the metric Ramsey problem to\ngraphs, and introduce the notion of Ramsey Spanning Trees. We ask for the\nlargest subset $S\\subseteq V$ of a given graph $G=(V,E)$, such that there\nexists a spanning tree of $G$ that has small stretch for $S$. Applied\niteratively, this provides a small collection of spanning trees, such that each\nvertex has a tree providing low stretch paths to all other vertices. The union\nof these trees serves as a special type of spanner, a tree-padding spanner. We\nuse this spanner to devise the first compact stateless routing scheme with\n$O(1)$ routing decision time, and labels which are much shorter than in all\ncurrently existing schemes.\n  We first revisit the metric Ramsey problem, and provide a new deterministic\nconstruction. We prove that for every $k$, any $n$-point metric space has a\nsubset $S$ of size at least $n^{1-1/k}$ which embeds into an ultrametric with\ndistortion $8k$. This results improves the best previous result of Mendel and\nNaor that obtained distortion $128k$ and required randomization. In addition,\nit provides the state-of-the-art deterministic construction of a distance\noracle. Building on this result, we prove that for every $k$, any $n$-vertex\ngraph $G=(V,E)$ has a subset $S$ of size at least $n^{1-1/k}$, and a spanning\ntree of $G$, that has stretch $O(k \\log \\log n)$ between any point in $S$ and\nany point in $V$. \n\n"}
{"id": "1707.08807", "contents": "Title: Nearest Common Ancestors: Universal Trees and Improved Labeling Schemes Abstract: We investigate the nearest common ancestor (NCA) function in rooted trees. As\nthe main conceptual contribution, the paper introduces universal trees for the\nNCA function: For a given family of rooted trees, an NCA-universal tree $S$ is\na rooted tree such that any tree $T$ of the family can be embedded into $S$\nsuch that the embedding of the NCA in $T$ of two nodes of $T$ is equal to the\nNCA in $S$ of the embeddings of the two nodes.\n  As the main technical result we give explicit constructions of NCA-universal\ntrees of size $n^{2.318}$ for the family of rooted $n$-vertex trees and of size\n$n^{1.894}$ for the family of rooted binary $n$-vertex trees. A direct\nconsequence is the explicit construction of NCA-labeling schemes with labels of\nsize $2.318\\log_2 n$ and $1.894\\log_2 n$ for the two families of rooted trees.\nThis improves on the best known such labeling schemes established by Alstrup,\nHalvorsen and Larsen [SODA 2014]. \n\n"}
{"id": "1707.09402", "contents": "Title: Independent Feedback Vertex Set for $P_5$-free Graphs Abstract: The NP-complete problem Feedback Vertex Set is that of deciding whether or\nnot it is possible, for a given integer $k\\geq 0$, to delete at most $k$\nvertices from a given graph so that what remains is a forest. The variant in\nwhich the deleted vertices must form an independent set is called Independent\nFeedback Vertex Set and is also NP-complete. In fact, even deciding if an\nindependent feedback vertex set exists is NP-complete and this problem is\nclosely related to the $3$-Colouring problem, or equivalently, to the problem\nof deciding whether or not a graph has an independent odd cycle transversal,\nthat is, an independent set of vertices whose deletion makes the graph\nbipartite. We initiate a systematic study of the complexity of Independent\nFeedback Vertex Set for $H$-free graphs. We prove that it is NP-complete if $H$\ncontains a claw or cycle. Tamura, Ito and Zhou proved that it is\npolynomial-time solvable for $P_4$-free graphs. We show that it remains\npolynomial-time solvable for $P_5$-free graphs. We prove analogous results for\nthe Independent Odd Cycle Transversal problem, which asks whether or not a\ngraph has an independent odd cycle transversal of size at most $k$ for a given\ninteger $k\\geq 0$. Finally, in line with our underlying research aim, we\ncompare the complexity of Independent Feedback Vertex Set for $H$-free graphs\nwith the complexity of $3$-Colouring, Independent Odd Cycle Transversal and\nother related problems. \n\n"}
{"id": "1708.00240", "contents": "Title: An Efficient Algorithm for Mixed Domination on Generalized\n  Series-Parallel Graphs Abstract: A mixed dominating set $S$ of a graph $G=(V,E)$ is a subset $ S \\subseteq V\n\\cup E$ such that each element $v\\in (V \\cup E) \\setminus S$ is adjacent or\nincident to at least one element in $S$. The mixed domination number\n$\\gamma_m(G)$ of a graph $G$ is the minimum cardinality among all mixed\ndominating sets in $G$. The problem of finding $\\gamma_{m}(G)$ is know to be\nNP-complete. In this paper, we present an explicit polynomial-time algorithm to\nconstruct a mixed dominating set of size $\\gamma_{m}(G)$ by a parse tree when\n$G$ is a generalized series-parallel graph. \n\n"}
{"id": "1708.01632", "contents": "Title: Localization of Electrical Flows Abstract: We show that in any graph, the average length of a flow path in an electrical\nflow between the endpoints of a random edge is $O(\\log^2 n)$. This is a\nconsequence of a more general result which shows that the spectral norm of the\nentrywise absolute value of the transfer impedance matrix of a graph is\n$O(\\log^2 n)$. This result implies a simple oblivious routing scheme based on\nelectrical flows in the case of transitive graphs. \n\n"}
{"id": "1708.03257", "contents": "Title: Robust polynomial regression up to the information theoretic limit Abstract: We consider the problem of robust polynomial regression, where one receives\nsamples $(x_i, y_i)$ that are usually within $\\sigma$ of a polynomial $y =\np(x)$, but have a $\\rho$ chance of being arbitrary adversarial outliers.\nPreviously, it was known how to efficiently estimate $p$ only when $\\rho <\n\\frac{1}{\\log d}$. We give an algorithm that works for the entire feasible\nrange of $\\rho < 1/2$, while simultaneously improving other parameters of the\nproblem. We complement our algorithm, which gives a factor 2 approximation,\nwith impossibility results that show, for example, that a $1.09$ approximation\nis impossible even with infinitely many samples. \n\n"}
{"id": "1708.03439", "contents": "Title: Combinatorial Optimization by Decomposition on Hybrid CPU--non-CPU\n  Solver Architectures Abstract: The advent of new special-purpose hardware such as FPGA or ASIC-based\nannealers and quantum processors has shown potential in solving certain\nfamilies of complex combinatorial optimization problems more efficiently than\nconventional CPUs. We show that to address an industrial optimization problem,\na hybrid architecture of CPUs and non-CPU devices is inevitable. In this paper,\nwe propose problem decomposition as an effective method for designing a hybrid\nCPU--non-CPU optimization solver. We introduce the required algorithmic\nelements for making problem decomposition a viable approach in meeting the\nreal-world constraints such as communication time and the potential higher cost\nof using non-CPU hardware. We then turn to the well-known maximum clique\nproblem, and propose a new method of decomposition for this problem. Our method\nenables us to solve the maximum clique problem on very large graphs using\nnon-CPU hardware that is considerably smaller than the size of the graph. As an\nexample, we show that the maximum clique problem on the com-Amazon graph, with\n334,863 vertices and 925,872 edges, can be solved with a single call to a\ndevice that can embed a fully connected graph of size at least 21 nodes, such\nas the D-Wave 2000Q. We also show that our proposed problem decomposition\napproach can improve the runtime of two of the best-known classical algorithms\nfor large, sparse graphs, namely PMC and BBMCSP, by orders of magnitude. In the\nlight of our study, we believe that new non-CPU hardware that is small in size\ncould become competitive with CPUs if it could be either mass produced and\nhighly parallelized, or able to provide high-quality solutions to specific,\nsmall-sized problems significantly faster than CPUs. \n\n"}
{"id": "1708.03903", "contents": "Title: Distributed Exact Weighted All-Pairs Shortest Paths in $\\tilde\n  O(n^{5/4})$ Rounds Abstract: We study computing {\\em all-pairs shortest paths} (APSP) on distributed\nnetworks (the CONGEST model). The goal is for every node in the (weighted)\nnetwork to know the distance from every other node using communication. The\nproblem admits $(1+o(1))$-approximation $\\tilde O(n)$-time algorithms\n~\\cite{LenzenP-podc15,Nanongkai-STOC14}, which are matched with $\\tilde\n\\Omega(n)$-time lower\nbounds~\\cite{Nanongkai-STOC14,LenzenP_stoc13,FrischknechtHW12}\\footnote{$\\tilde\n\\Theta$, $\\tilde O$ and $\\tilde \\Omega$ hide polylogarithmic factors. Note that\nthe lower bounds also hold even in the unweighted case and in the weighted case\nwith polynomial approximation ratios.}. No $\\omega(n)$ lower bound or $o(m)$\nupper bound were known for exact computation.\n  In this paper, we present an $\\tilde O(n^{5/4})$-time randomized (Las Vegas)\nalgorithm for exact weighted APSP; this provides the first improvement over the\nnaive $O(m)$-time algorithm when the network is not so sparse. Our result also\nholds for the case where edge weights are {\\em asymmetric} (a.k.a. the directed\ncase where communication is bidirectional). Our techniques also yield an\n$\\tilde O(n^{3/4}k^{1/2}+n)$-time algorithm for the {\\em $k$-source shortest\npaths} problem where we want every node to know distances from $k$ sources;\nthis improves Elkin's recent bound~\\cite{Elkin-STOC17} when $k=\\tilde\n\\omega(n^{1/4})$. \n\n"}
{"id": "1708.04044", "contents": "Title: Improved second-order evaluation complexity for unconstrained nonlinear\n  optimization using high-order regularized models Abstract: The unconstrained minimization of a sufficiently smooth objective function\n$f(x)$ is considered, for which derivatives up to order $p$, $p\\geq 2$, are\nassumed to be available. An adaptive regularization algorithm is proposed that\nuses Taylor models of the objective of order $p$ and that is guaranteed to find\na first- and second-order critical point in at most $O \\left(\\max\\left(\n\\epsilon_1^{-\\frac{p+1}{p}}, \\epsilon_2^{-\\frac{p+1}{p-1}} \\right) \\right)$\nfunction and derivatives evaluations, where $\\epsilon_1$ and $\\epsilon_2 >0$\nare prescribed first- and second-order optimality tolerances. Our approach\nextends the method in Birgin et al. (2016) to finding second-order critical\npoints, and establishes the novel complexity bound for second-order criticality\nunder identical problem assumptions as for first-order, namely, that the $p$-th\nderivative tensor is Lipschitz continuous and that $f(x)$ is bounded from\nbelow. The evaluation-complexity bound for second-order criticality improves on\nall such known existing results. \n\n"}
{"id": "1708.04692", "contents": "Title: GANs for Biological Image Synthesis Abstract: In this paper, we propose a novel application of Generative Adversarial\nNetworks (GAN) to the synthesis of cells imaged by fluorescence microscopy.\nCompared to natural images, cells tend to have a simpler and more geometric\nglobal structure that facilitates image generation. However, the correlation\nbetween the spatial pattern of different fluorescent proteins reflects\nimportant biological functions, and synthesized images have to capture these\nrelationships to be relevant for biological applications. We adapt GANs to the\ntask at hand and propose new models with casual dependencies between image\nchannels that can generate multi-channel images, which would be impossible to\nobtain experimentally. We evaluate our approach using two independent\ntechniques and compare it against sensible baselines. Finally, we demonstrate\nthat by interpolating across the latent space we can mimic the known changes in\nprotein localization that occur through time during the cell cycle, allowing us\nto predict temporal evolution from static images. \n\n"}
{"id": "1708.04788", "contents": "Title: BitNet: Bit-Regularized Deep Neural Networks Abstract: We present a novel optimization strategy for training neural networks which\nwe call \"BitNet\". The parameters of neural networks are usually unconstrained\nand have a dynamic range dispersed over all real values. Our key idea is to\nlimit the expressive power of the network by dynamically controlling the range\nand set of values that the parameters can take. We formulate this idea using a\nnovel end-to-end approach that circumvents the discrete parameter space by\noptimizing a relaxed continuous and differentiable upper bound of the typical\nclassification loss function. The approach can be interpreted as a\nregularization inspired by the Minimum Description Length (MDL) principle. For\neach layer of the network, our approach optimizes real-valued translation and\nscaling factors and arbitrary precision integer-valued parameters (weights). We\nempirically compare BitNet to an equivalent unregularized model on the MNIST\nand CIFAR-10 datasets. We show that BitNet converges faster to a superior\nquality solution. Additionally, the resulting model has significant savings in\nmemory due to the use of integer-valued parameters. \n\n"}
{"id": "1708.04908", "contents": "Title: The covertime of a biased random walk on $G_{n,p}$ Abstract: We analyze the covertime of a biased random walk on the random graph\n$G_{n,p}$. The walk is biased towards visiting vertices of low degree and this\nmakes the covertime less than in the unbiased case \n\n"}
{"id": "1708.05446", "contents": "Title: Robust Contextual Bandit via the Capped-$\\ell_{2}$ norm Abstract: This paper considers the actor-critic contextual bandit for the mobile health\n(mHealth) intervention. The state-of-the-art decision-making methods in mHealth\ngenerally assume that the noise in the dynamic system follows the Gaussian\ndistribution. Those methods use the least-square-based algorithm to estimate\nthe expected reward, which is prone to the existence of outliers. To deal with\nthe issue of outliers, we propose a novel robust actor-critic contextual bandit\nmethod for the mHealth intervention. In the critic updating, the\ncapped-$\\ell_{2}$ norm is used to measure the approximation error, which\nprevents outliers from dominating our objective. A set of weights could be\nachieved from the critic updating. Considering them gives a weighted objective\nfor the actor updating. It provides the badly noised sample in the critic\nupdating with zero weights for the actor updating. As a result, the robustness\nof both actor-critic updating is enhanced. There is a key parameter in the\ncapped-$\\ell_{2}$ norm. We provide a reliable method to properly set it by\nmaking use of one of the most fundamental definitions of outliers in\nstatistics. Extensive experiment results demonstrate that our method can\nachieve almost identical results compared with the state-of-the-art methods on\nthe dataset without outliers and dramatically outperform them on the datasets\nnoised by outliers. \n\n"}
{"id": "1708.08436", "contents": "Title: Spectral Sparsification of Simplicial Complexes for Clustering and Label\n  Propagation Abstract: As a generalization of the use of graphs to describe pairwise interactions,\nsimplicial complexes can be used to model higher-order interactions between\nthree or more objects in complex systems. There has been a recent surge in\nactivity for the development of data analysis methods applicable to simplicial\ncomplexes, including techniques based on computational topology, higher-order\nrandom processes, generalized Cheeger inequalities, isoperimetric inequalities,\nand spectral methods. In particular, spectral learning methods (e.g. label\npropagation and clustering) that directly operate on simplicial complexes\nrepresent a new direction for analyzing such complex datasets.\n  To apply spectral learning methods to massive datasets modeled as simplicial\ncomplexes, we develop a method for sparsifying simplicial complexes that\npreserves the spectrum of the associated Laplacian matrices. We show that the\ntheory of Spielman and Srivastava for the sparsification of graphs extends to\nsimplicial complexes via the up Laplacian. In particular, we introduce a\ngeneralized effective resistance for simplices, provide an algorithm for\nsparsifying simplicial complexes at a fixed dimension, and give a specific\nversion of the generalized Cheeger inequality for weighted simplicial\ncomplexes. Finally, we introduce higher-order generalizations of spectral\nclustering and label propagation for simplicial complexes and demonstrate via\nexperiments the utility of the proposed spectral sparsification method for\nthese applications. \n\n"}
{"id": "1708.09455", "contents": "Title: An algorithm to simulate alternating Turing machine in signal machine Abstract: Geometrical Computation as a new model of computation is the counterpart of\nCellular Automata that has Turing computing ability. In this paper we provide\nan algorithm to simulate Alternating Turing Machine in the context of Signal\nMachine using techniques adopted from the features of Signal Machine to set up\nand manage the copies/branches of Alternating Turing Machine. We show that our\nalgorithm can simulate Alternating Turing Machine in Signal Machine as same\nfunctionality as classic family of Turing Machines. Time complexity of the\nalgorithm is linear as ordinary simulated Turing Machines. Depending on the\ncomputation tree space complexity is exponential order of d, where d is the\ndepth of the computation tree. \n\n"}
{"id": "1708.09827", "contents": "Title: Walking Through Waypoints Abstract: We initiate the study of a fundamental combinatorial problem: Given a\ncapacitated graph $G=(V,E)$, find a shortest walk (\"route\") from a source $s\\in\nV$ to a destination $t\\in V$ that includes all vertices specified by a set\n$\\mathscr{W}\\subseteq V$: the \\emph{waypoints}. This waypoint routing problem\nfinds immediate applications in the context of modern networked distributed\nsystems. Our main contribution is an exact polynomial-time algorithm for graphs\nof bounded treewidth. We also show that if the number of waypoints is\nlogarithmically bounded, exact polynomial-time algorithms exist even for\ngeneral graphs. Our two algorithms provide an almost complete characterization\nof what can be solved exactly in polynomial-time: we show that more general\nproblems (e.g., on grid graphs of maximum degree 3, with slightly more\nwaypoints) are computationally intractable. \n\n"}
{"id": "1709.03395", "contents": "Title: Low-memory GEMM-based convolution algorithms for deep neural networks Abstract: Deep neural networks (DNNs) require very large amounts of computation both\nfor training and for inference when deployed in the field. A common approach to\nimplementing DNNs is to recast the most computationally expensive operations as\ngeneral matrix multiplication (GEMM). However, as we demonstrate in this paper,\nthere are a great many different ways to express DNN convolution operations\nusing GEMM. Although different approaches all perform the same number of\noperations, the size of temporary data structures differs significantly.\nConvolution of an input matrix with dimensions $C \\times H \\times W$, requires\n$O(K^2CHW)$ additional space using the classical im2col approach. More recently\nmemory-efficient approaches requiring just $O(KCHW)$ auxiliary space have been\nproposed.\n  We present two novel GEMM-based algorithms that require just $O(MHW)$ and\n$O(KW)$ additional space respectively, where $M$ is the number of channels in\nthe result of the convolution. These algorithms dramatically reduce the space\noverhead of DNN convolution, making it much more suitable for memory-limited\nembedded systems. Experimental evaluation shows that our low-memory algorithms\nare just as fast as the best patch-building approaches despite requiring just a\nfraction of the amount of additional memory. Our low-memory algorithms have\nexcellent data locality which gives them a further edge over patch-building\nalgorithms when multiple cores are used. As a result, our low memory algorithms\noften outperform the best patch-building algorithms using multiple threads. \n\n"}
{"id": "1709.04262", "contents": "Title: Lower Bounds for Approximating Graph Parameters via Communication\n  Complexity Abstract: In a celebrated work, Blais, Brody, and Matulef developed a technique for\nproving property testing lower bounds via reductions from communication\ncomplexity. Their work focused on testing properties of functions, and yielded\nnew lower bounds as well as simplified analyses of known lower bounds. Here, we\ntake a further step in generalizing the methodology of Blais et al. to analyze\nthe query complexity of graph parameter estimation problems. In particular, our\ntechnique decouples the lower bound arguments from the representation of the\ngraph, allowing it to work with any query type.\n  We illustrate our technique by providing new simpler proofs of previously\nknown tight lower bounds for the query complexity of several graph problems:\nestimating the number of edges in a graph, sampling edges from an\nalmost-uniform distribution, estimating the number of triangles (and more\ngenerally, $r$-cliques) in a graph, and estimating the moments of the degree\ndistribution of a graph. We also prove new lower bounds for estimating the edge\nconnectivity of a graph and estimating the number of instances of any fixed\nsubgraph in a graph. We show that the lower bounds for estimating the number of\ntriangles and edge connectivity also hold in a strictly stronger computational\nmodel that allows access to uniformly random edge samples. \n\n"}
{"id": "1709.05510", "contents": "Title: The Geometric Block Model Abstract: To capture the inherent geometric features of many community detection\nproblems, we propose to use a new random graph model of communities that we\ncall a Geometric Block Model. The geometric block model generalizes the random\ngeometric graphs in the same way that the well-studied stochastic block model\ngeneralizes the Erdos-Renyi random graphs. It is also a natural extension of\nrandom community models inspired by the recent theoretical and practical\nadvancement in community detection. While being a topic of fundamental\ntheoretical interest, our main contribution is to show that many practical\ncommunity structures are better explained by the geometric block model. We also\nshow that a simple triangle-counting algorithm to detect communities in the\ngeometric block model is near-optimal. Indeed, even in the regime where the\naverage degree of the graph grows only logarithmically with the number of\nvertices (sparse-graph), we show that this algorithm performs extremely well,\nboth theoretically and practically. In contrast, the triangle-counting\nalgorithm is far from being optimum for the stochastic block model. We simulate\nour results on both real and synthetic datasets to show superior performance of\nboth the new model as well as our algorithm. \n\n"}
{"id": "1709.07822", "contents": "Title: Planar Graph Perfect Matching is in NC Abstract: Is perfect matching in NC? That is, is there a deterministic fast parallel\nalgorithm for it? This has been an outstanding open question in theoretical\ncomputer science for over three decades, ever since the discovery of RNC\nmatching algorithms. Within this question, the case of planar graphs has\nremained an enigma: On the one hand, counting the number of perfect matchings\nis far harder than finding one (the former is #P-complete and the latter is in\nP), and on the other, for planar graphs, counting has long been known to be in\nNC whereas finding one has resisted a solution.\n  In this paper, we give an NC algorithm for finding a perfect matching in a\nplanar graph. Our algorithm uses the above-stated fact about counting matchings\nin a crucial way. Our main new idea is an NC algorithm for finding a face of\nthe perfect matching polytope at which $\\Omega(n)$ new conditions, involving\nconstraints of the polytope, are simultaneously satisfied. Several other ideas\nare also needed, such as finding a point in the interior of the minimum weight\nface of this polytope and finding a balanced tight odd set in NC. \n\n"}
{"id": "1709.07966", "contents": "Title: High Degree Sum of Squares Proofs, Bienstock-Zuckerberg hierarchy and\n  Chvatal-Gomory cuts Abstract: Chvatal-Gomory (CG) cuts and the Bienstock-Zuckerberg hierarchy capture\nuseful linear programs that the standard bounded degree Lasserre/Sum-of-Squares\nSOS hierarchy fails to capture.\n  In this paper we present a novel polynomial time SOS hierarchy for 0/1\nproblems with a custom subspace of high degree polynomials (not the standard\nsubspace of low-degree polynomials). We show that the new SOS hierarchy\nrecovers the Bienstock-Zuckerberg hierarchy. Our result implies a linear\nprogram that reproduces the Bienstock-Zuckerberg hierarchy as a polynomial\nsized, efficiently constructive extended formulation that satisfies all\nconstant pitch inequalities. The construction is also very simple, and it is\nfully defined by giving the supporting polynomials. Moreover, for a class of\npolytopes (e.g. set covering and packing problems), the resulting SOS hierarchy\noptimizes in polynomial time over the polytope resulting from any constant\nrounds of CG-cuts, up to an arbitrarily small error.\n  Arguably, this is the first example where different basis functions can be\nuseful in asymmetric situations to obtain a hierarchy of relaxations. \n\n"}
{"id": "1709.08139", "contents": "Title: Disabling External Influence in Social Networks via Edge Recommendation Abstract: Existing socio-psychological studies suggest that users of a social network\nform their opinions relying on the opinions of their neighbors. According to\nDeGroot opinion formation model, one value of particular importance is the\nasymptotic consensus value---the sum of user opinions weighted by the users'\neigenvector centralities. This value plays the role of an attractor for the\nopinions in the network and is a lucrative target for external influence.\nHowever, since any potentially malicious control of the opinion distribution in\na social network is clearly undesirable, it is important to design methods to\nprevent the external attempts to strategically change the asymptotic consensus\nvalue.\n  In this work, we assume that the adversary wants to maximize the asymptotic\nconsensus value by altering the opinions of some users in a network; we, then,\nstate DIVER---an NP-hard problem of disabling such external influence attempts\nby strategically adding a limited number of edges to the network. Relying on\nthe theory of Markov chains, we provide perturbation analysis that shows how\neigenvector centrality and, hence, DIVER's objective function change in\nresponse to an edge's addition to the network. The latter leads to the design\nof a pseudo-linear-time heuristic for DIVER, whose computation relies on\nefficient estimation of mean first passage times in a Markov chain. We confirm\nour theoretical findings in experiments. \n\n"}
{"id": "1709.08519", "contents": "Title: Enhanced Quantum Synchronization via Quantum Machine Learning Abstract: We study the quantum synchronization between a pair of two-level systems\ninside two coupled cavities. By using a digital-analog decomposition of the\nmaster equation that rules the system dynamics, we show that this approach\nleads to quantum synchronization between both two-level systems. Moreover, we\ncan identify in this digital-analog block decomposition the fundamental\nelements of a quantum machine learning protocol, in which the agent and the\nenvironment (learning units) interact through a mediating system, namely, the\nregister. If we can additionally equip this algorithm with a classical feedback\nmechanism, which consists of projective measurements in the register,\nreinitialization of the register state and local conditional operations on the\nagent and environment subspace, a powerful and flexible quantum machine\nlearning protocol emerges. Indeed, numerical simulations show that this\nprotocol enhances the synchronization process, even when every subsystem\nexperience different loss/decoherence mechanisms, and give us the flexibility\nto choose the synchronization state. Finally, we propose an implementation\nbased on current technologies in superconducting circuits. \n\n"}
{"id": "1709.09209", "contents": "Title: Recognizing Weak Embeddings of Graphs Abstract: We present an efficient algorithm for a problem in the interface between\nclustering and graph embeddings. An embedding $\\varphi:G\\rightarrow M$ of a\ngraph $G$ into a 2-manifold $M$ maps the vertices in $V(G)$ to distinct points\nand the edges in $E(G)$ to interior-disjoint Jordan arcs between the\ncorresponding vertices. In applications in clustering, cartography, and\nvisualization, nearby vertices and edges are often bundled to the same point or\noverlapping arcs, due to data compression or low resolution. This raises the\ncomputational problem of deciding whether a given map $\\varphi:G\\rightarrow M$\ncomes from an embedding. A map $\\varphi:G\\rightarrow M$ is a \\textbf{weak\nembedding} if it can be perturbed into an embedding\n$\\psi_\\varepsilon:G\\rightarrow M$ with\n$\\|\\varphi-\\psi_\\varepsilon\\|<\\varepsilon$ for every $\\varepsilon>0$, where\n$\\|.\\|$ is the unform norm. A polynomial-time algorithm for recognizing weak\nembeddings has recently been found by Fulek and Kyn\\v{c}l. It reduces the\nproblem to solving a system of linear equations over $\\mathbb{Z}_2$. It runs in\n$O(n^{2\\omega})\\leq O(n^{4.75})$ time, where $\\omega\\in [2,2.373)$ is the\nmatrix multiplication exponent and $n$ is the number of vertices and edges of\n$G$. We improve the running time to $O(n\\log n)$. Our algorithm is also\nconceptually simpler: We perform a sequence of \\emph{local operations} that\ngradually \"untangles\" the image $\\varphi(G)$ into an embedding $\\psi(G)$, or\nreports that $\\varphi$ is not a weak embedding. It combines local constraints\non the orientation of subgraphs directly, thereby eliminating the need for\nsolving large systems of linear equations. \n\n"}
{"id": "1709.10063", "contents": "Title: Finding Small Weight Isomorphisms with Additional Constraints is\n  Fixed-Parameter Tractable Abstract: Lubiw showed that several variants of Graph Isomorphism are NP-complete,\nwhere the solutions are required to satisfy certain additional constraints\n[SICOMP 10, 1981]. One of these, called Isomorphism With Restrictions, is to\ndecide for two given graphs $X_1=(V,E_1)$ and $X_2=(V,E_2)$ and a subset\n$R\\subseteq V\\times V$ of forbidden pairs whether there is an isomorphism $\\pi$\nfrom $X_1$ to $X_2$ such that $\\pi(i)\\neq j$ for all $(i,j)\\in R$. We prove\nthat this problem and several of its generalizations are in fact in FPT:\n  - The problem of deciding whether there is an isomorphism between two graphs\nthat moves k vertices and satisfies Lubiw-style constraints is in FPT, with k\nand the size of $R$ as parameters. The problem remains in FPT if a CNF of such\nconstraints is allowed. It follows that the problem to decide whether there is\nan isomorphism that moves exactly k vertices is in FPT. This solves a question\nleft open in our article on exact weight automorphisms [STACS 2017].\n  - When the weight and complexity are unrestricted, finding isomorphisms that\nsatisfy a CNF of Lubiw-style constraints can be solved in FPT with access to a\nGI oracle.\n  - Checking if there is an isomorphism $\\pi$ between two graphs with\ncomplexity t is also in FPT with t as parameter, where the complexity of a\npermutation is the Cayley measure defined as the minimum number t such that\n$\\pi$ can be expressed as a product of t transpositions.\n  - We consider a more general problem in which the vertex set of a graph X is\npartitioned into Red and Blue, and we are interested in an automorphism that\nstabilizes Red and Blue and moves exactly k vertices in Blue, where k is the\nparameter. This problem was introduced by [Downey and Fellows 1999], and we\nshowed [STACS 2017] that it is W[1]-hard even with color classes of size 4\ninside Red. Now, for color classes of size at most 3 inside Red, we show the\nproblem is in FPT. \n\n"}
{"id": "1710.03285", "contents": "Title: Coresets for Dependency Networks Abstract: Many applications infer the structure of a probabilistic graphical model from\ndata to elucidate the relationships between variables. But how can we train\ngraphical models on a massive data set? In this paper, we show how to construct\ncoresets -compressed data sets which can be used as proxy for the original data\nand have provably bounded worst case error- for Gaussian dependency networks\n(DNs), i.e., cyclic directed graphical models over Gaussians, where the parents\nof each variable are its Markov blanket. Specifically, we prove that Gaussian\nDNs admit coresets of size independent of the size of the data set.\nUnfortunately, this does not extend to DNs over members of the exponential\nfamily in general. As we will prove, Poisson DNs do not admit small coresets.\nDespite this worst-case result, we will provide an argument why our coreset\nconstruction for DNs can still work well in practice on count data. To\ncorroborate our theoretical results, we empirically evaluated the resulting\nCore DNs on real data sets. The results \n\n"}
{"id": "1710.03358", "contents": "Title: Balanced power diagrams for redistricting Abstract: We propose a method for redistricting, decomposing a geographical area into\nsubareas, called districts, so that the populations of the districts are as\nclose as possible and the districts are compact and contiguous. Each district\nis the intersection of a polygon with the geographical area. The polygons are\nconvex and the average number of sides per polygon is less than six. The\npolygons tend to be quite compact. With each polygon is associated a center.\nThe center is the centroid of the locations of the residents associated with\nthe polygon. The algorithm can be viewed as a heuristic for finding centers and\na balanced assignment of residents to centers so as to minimize the sum of\nsquared distances of residents to centers; hence the solution can be said to\nhave low dispersion. \n\n"}
{"id": "1710.04073", "contents": "Title: Stream Graphs and Link Streams for the Modeling of Interactions over\n  Time Abstract: Graph theory provides a language for studying the structure of relations, and\nit is often used to study interactions over time too. However, it poorly\ncaptures the both temporal and structural nature of interactions, that calls\nfor a dedicated formalism. In this paper, we generalize graph concepts in order\nto cope with both aspects in a consistent way. We start with elementary\nconcepts like density, clusters, or paths, and derive from them more advanced\nconcepts like cliques, degrees, clustering coefficients, or connected\ncomponents. We obtain a language to directly deal with interactions over time,\nsimilar to the language provided by graphs to deal with relations. This\nformalism is self-consistent: usual relations between different concepts are\npreserved. It is also consistent with graph theory: graph concepts are special\ncases of the ones we introduce. This makes it easy to generalize higher-level\nobjects such as quotient graphs, line graphs, k-cores, and centralities. This\npaper also considers discrete versus continuous time assumptions, instantaneous\nlinks, and extensions to more complex cases. \n\n"}
{"id": "1710.05241", "contents": "Title: Robust Decentralized Learning Using ADMM with Unreliable Agents Abstract: Many machine learning problems can be formulated as consensus optimization\nproblems which can be solved efficiently via a cooperative multi-agent system.\nHowever, the agents in the system can be unreliable due to a variety of\nreasons: noise, faults and attacks. Providing erroneous updates leads the\noptimization process in a wrong direction, and degrades the performance of\ndistributed machine learning algorithms. This paper considers the problem of\ndecentralized learning using ADMM in the presence of unreliable agents. First,\nwe rigorously analyze the effect of erroneous updates (in ADMM learning\niterations) on the convergence behavior of multi-agent system. We show that the\nalgorithm linearly converges to a neighborhood of the optimal solution under\ncertain conditions and characterize the neighborhood size analytically. Next,\nwe provide guidelines for network design to achieve a faster convergence. We\nalso provide conditions on the erroneous updates for exact convergence to the\noptimal solution. Finally, to mitigate the influence of unreliable agents, we\npropose \\textsf{ROAD}, a robust variant of ADMM, and show its resilience to\nunreliable agents with an exact convergence to the optimum. \n\n"}
{"id": "1710.05758", "contents": "Title: TensorQuant - A Simulation Toolbox for Deep Neural Network Quantization Abstract: Recent research implies that training and inference of deep neural networks\n(DNN) can be computed with low precision numerical representations of the\ntraining/test data, weights and gradients without a general loss in accuracy.\nThe benefit of such compact representations is twofold: they allow a\nsignificant reduction of the communication bottleneck in distributed DNN\ntraining and faster neural network implementations on hardware accelerators\nlike FPGAs. Several quantization methods have been proposed to map the original\n32-bit floating point problem to low-bit representations. While most related\npublications validate the proposed approach on a single DNN topology, it\nappears to be evident, that the optimal choice of the quantization method and\nnumber of coding bits is topology dependent. To this end, there is no general\ntheory available, which would allow users to derive the optimal quantization\nduring the design of a DNN topology. In this paper, we present a quantization\ntool box for the TensorFlow framework. TensorQuant allows a transparent\nquantization simulation of existing DNN topologies during training and\ninference. TensorQuant supports generic quantization methods and allows\nexperimental evaluation of the impact of the quantization on single layers as\nwell as on the full topology. In a first series of experiments with\nTensorQuant, we show an analysis of fix-point quantizations of popular CNN\ntopologies. \n\n"}
{"id": "1710.06273", "contents": "Title: Combinatorial Penalties: Which structures are preserved by convex\n  relaxations? Abstract: We consider the homogeneous and the non-homogeneous convex relaxations for\ncombinatorial penalty functions defined on support sets. Our study identifies\nkey differences in the tightness of the resulting relaxations through the\nnotion of the lower combinatorial envelope of a set-function along with new\nnecessary conditions for support identification. We then propose a general\nadaptive estimator for convex monotone regularizers, and derive new sufficient\nconditions for support recovery in the asymptotic setting. \n\n"}
{"id": "1710.07107", "contents": "Title: Discovering Patterns of Interest in IP Traffic Using Cliques in\n  Bipartite Link Streams Abstract: Studying IP traffic is crucial for many applications. We focus here on the\ndetection of (structurally and temporally) dense sequences of interactions,\nthat may indicate botnets or coordinated network scans. More precisely, we\nmodel a MAWI capture of IP traffic as a link streams, i.e. a sequence of\ninteractions $(t_1 , t_2 , u, v)$ meaning that devices $u$ and $v$ exchanged\npackets from time $t_1$ to time $t_2$ . This traffic is captured on a single\nrouter and so has a bipartite structure: links occur only between nodes in two\ndisjoint sets. We design a method for finding interesting bipartite cliques in\nsuch link streams, i.e. two sets of nodes and a time interval such that all\nnodes in the first set are linked to all nodes in the second set throughout the\ntime interval. We then explore the bipartite cliques present in the considered\ntrace. Comparison with the MAWILab classification of anomalous IP addresses\nshows that the found cliques succeed in detecting anomalous network activity. \n\n"}
{"id": "1710.08163", "contents": "Title: Satisfiability in multi-valued circuits Abstract: Satisfiability of Boolean circuits is among the most known and important\nproblems in theoretical computer science. This problem is NP-complete in\ngeneral but becomes polynomial time when restricted either to monotone gates or\nlinear gates. We go outside Boolean realm and consider circuits built of any\nfixed set of gates on an arbitrary large finite domain. From the complexity\npoint of view this is strictly connected with the problems of solving equations\n(or systems of equations) over finite algebras.\n  The research reported in this work was motivated by a desire to know for\nwhich finite algebras $\\mathbf A$ there is a polynomial time algorithm that\ndecides if an equation over $\\mathbf A$ has a solution. We are also looking for\npolynomial time algorithms that decide if two circuits over a finite algebra\ncompute the same function. Although we have not managed to solve these problems\nin the most general setting we have obtained such a characterization for a very\nbroad class of algebras from congruence modular varieties. This class includes\nmost known and well-studied algebras such as groups, rings, modules (and their\ngeneralizations like quasigroups, loops, near-rings, nonassociative rings, Lie\nalgebras), lattices (and their extensions like Boolean algebras, Heyting\nalgebras or other algebras connected with multi-valued logics including\nMV-algebras).\n  This paper seems to be the first systematic study of the computational\ncomplexity of satisfiability of non-Boolean circuits and solving equations over\nfinite algebras. The characterization results provided by the paper is given in\nterms of nice structural properties of algebras for which the problems are\nsolvable in polynomial time. \n\n"}
{"id": "1710.09605", "contents": "Title: Distributed Graph Clustering using Modularity and Map Equation Abstract: We study large-scale, distributed graph clustering. Given an undirected\ngraph, our objective is to partition the nodes into disjoint sets called\nclusters. A cluster should contain many internal edges while being sparsely\nconnected to other clusters. In the context of a social network, a cluster\ncould be a group of friends. Modularity and map equation are established\nformalizations of this internally-dense-externally-sparse principle. We present\ntwo versions of a simple distributed algorithm to optimize both measures. They\nare based on Thrill, a distributed big data processing framework that\nimplements an extended MapReduce model. The algorithms for the two measures,\nDSLM-Mod and DSLM-Map, differ only slightly. Adapting them for similar quality\nmeasures is straight-forward. We conduct an extensive experimental study on\nreal-world graphs and on synthetic benchmark graphs with up to 68 billion\nedges. Our algorithms are fast while detecting clusterings similar to those\ndetected by other sequential, parallel and distributed clustering algorithms.\nCompared to the distributed GossipMap algorithm, DSLM-Map needs less memory, is\nup to an order of magnitude faster and achieves better quality. \n\n"}
{"id": "1710.09780", "contents": "Title: Interactions of Computational Complexity Theory and Mathematics Abstract: $ $[This paper is a (self contained) chapter in a new book, Mathematics and\nComputation, whose draft is available on my homepage at\nhttps://www.math.ias.edu/avi/book ].\n  We survey some concrete interaction areas between computational complexity\ntheory and different fields of mathematics. We hope to demonstrate here that\nhardly any area of modern mathematics is untouched by the computational\nconnection (which in some cases is completely natural and in others may seem\nquite surprising). In my view, the breadth, depth, beauty and novelty of these\nconnections is inspiring, and speaks to a great potential of future\ninteractions (which indeed, are quickly expanding). We aim for variety. We give\nshort, simple descriptions (without proofs or much technical detail) of ideas,\nmotivations, results and connections; this will hopefully entice the reader to\ndig deeper. Each vignette focuses only on a single topic within a large\nmathematical filed. We cover the following:\n  $\\bullet$ Number Theory: Primality testing\n  $\\bullet$ Combinatorial Geometry: Point-line incidences\n  $\\bullet$ Operator Theory: The Kadison-Singer problem\n  $\\bullet$ Metric Geometry: Distortion of embeddings\n  $\\bullet$ Group Theory: Generation and random generation\n  $\\bullet$ Statistical Physics: Monte-Carlo Markov chains\n  $\\bullet$ Analysis and Probability: Noise stability\n  $\\bullet$ Lattice Theory: Short vectors\n  $\\bullet$ Invariant Theory: Actions on matrix tuples \n\n"}
{"id": "1710.10057", "contents": "Title: Multiwinner Voting with Fairness Constraints Abstract: Multiwinner voting rules are used to select a small representative subset of\ncandidates or items from a larger set given the preferences of voters. However,\nif candidates have sensitive attributes such as gender or ethnicity (when\nselecting a committee), or specified types such as political leaning (when\nselecting a subset of news items), an algorithm that chooses a subset by\noptimizing a multiwinner voting rule may be unbalanced in its selection -- it\nmay under or over represent a particular gender or political orientation in the\nexamples above. We introduce an algorithmic framework for multiwinner voting\nproblems when there is an additional requirement that the selected subset\nshould be \"fair\" with respect to a given set of attributes. Our framework\nprovides the flexibility to (1) specify fairness with respect to multiple,\nnon-disjoint attributes (e.g., ethnicity and gender) and (2) specify a score\nfunction. We study the computational complexity of this constrained multiwinner\nvoting problem for monotone and submodular score functions and present several\napproximation algorithms and matching hardness of approximation results for\nvarious attribute group structure and types of score functions. We also present\nsimulations that suggest that adding fairness constraints may not affect the\nscores significantly when compared to the unconstrained case. \n\n"}
{"id": "1710.10753", "contents": "Title: Computational Social Choice and Computational Complexity: BFFs? Abstract: We discuss the connection between computational social choice (comsoc) and\ncomputational complexity. We stress the work so far on, and urge continued\nfocus on, two less-recognized aspects of this connection. Firstly, this is very\nmuch a two-way street: Everyone knows complexity classification is used in\ncomsoc, but we also highlight benefits to complexity that have arisen from its\nuse in comsoc. Secondly, more subtle, less-known complexity tools often can be\nvery productively used in comsoc. \n\n"}
{"id": "1710.11004", "contents": "Title: Denoising random forests Abstract: This paper proposes a novel type of random forests called a denoising random\nforests that are robust against noises contained in test samples. Such\nnoise-corrupted samples cause serious damage to the estimation performances of\nrandom forests, since unexpected child nodes are often selected and the leaf\nnodes that the input sample reaches are sometimes far from those for a clean\nsample. Our main idea for tackling this problem originates from a binary\nindicator vector that encodes a traversal path of a sample in the forest. Our\nproposed method effectively employs this vector by introducing denoising\nautoencoders into random forests. A denoising autoencoder can be trained with\nindicator vectors produced from clean and noisy input samples, and non-leaf\nnodes where incorrect decisions are made can be identified by comparing the\ninput and output of the trained denoising autoencoder. Multiple traversal paths\nwith respect to the nodes with incorrect decisions caused by the noises can\nthen be considered for the estimation. \n\n"}
{"id": "1710.11253", "contents": "Title: Approximation Algorithms for $\\ell_0$-Low Rank Approximation Abstract: We study the $\\ell_0$-Low Rank Approximation Problem, where the goal is,\ngiven an $m \\times n$ matrix $A$, to output a rank-$k$ matrix $A'$ for which\n$\\|A'-A\\|_0$ is minimized. Here, for a matrix $B$, $\\|B\\|_0$ denotes the number\nof its non-zero entries. This NP-hard variant of low rank approximation is\nnatural for problems with no underlying metric, and its goal is to minimize the\nnumber of disagreeing data positions. We provide approximation algorithms which\nsignificantly improve the running time and approximation factor of previous\nwork. For $k > 1$, we show how to find, in poly$(mn)$ time for every $k$, a\nrank $O(k \\log(n/k))$ matrix $A'$ for which $\\|A'-A\\|_0 \\leq O(k^2 \\log(n/k))\n\\mathrm{OPT}$. To the best of our knowledge, this is the first algorithm with\nprovable guarantees for the $\\ell_0$-Low Rank Approximation Problem for $k >\n1$, even for bicriteria algorithms. For the well-studied case when $k = 1$, we\ngive a $(2+\\epsilon)$-approximation in {\\it sublinear time}, which is\nimpossible for other variants of low rank approximation such as for the\nFrobenius norm. We strengthen this for the well-studied case of binary matrices\nto obtain a $(1+O(\\psi))$-approximation in sublinear time, where $\\psi =\n\\mathrm{OPT}/\\lVert A\\rVert_0$. For small $\\psi$, our approximation factor is\n$1+o(1)$. \n\n"}
{"id": "1711.00963", "contents": "Title: The Complexity of Finding Small Separators in Temporal Graphs Abstract: Temporal graphs are graphs with time-stamped edges. We study the problem of\nfinding a small vertex set (the separator) with respect to two designated\nterminal vertices such that the removal of the set eliminates all temporal\npaths connecting one terminal to the other. Herein, we consider two models of\ntemporal paths: paths that pass through arbitrarily many edges per time step\n(non-strict) and paths that pass through at most one edge per time step\n(strict). Regarding the number of time steps of a temporal graph, we show a\ncomplexity dichotomy (NP-hardness versus polynomial-time solvability) for both\nproblem variants. Moreover we prove both problem variants to be NP-complete\neven on temporal graphs whose underlying graph is planar. We further show that,\non temporal graphs with planar underlying graph, if additionally the number of\ntime steps is constant, then the problem variant for strict paths is solvable\nin quasi-linear time. Finally, we introduce and motivate the notion of a\ntemporal core (vertices whose incident edges change over time). We prove that\nthe non-strict variant is fixed-parameter tractable when parameterized by the\nsize of the temporal core, while the strict variant remains NP-complete, even\nfor constant-size temporal cores. \n\n"}
{"id": "1711.01596", "contents": "Title: Is Input Sparsity Time Possible for Kernel Low-Rank Approximation? Abstract: Low-rank approximation is a common tool used to accelerate kernel methods:\nthe $n \\times n$ kernel matrix $K$ is approximated via a rank-$k$ matrix\n$\\tilde K$ which can be stored in much less space and processed more quickly.\nIn this work we study the limits of computationally efficient low-rank kernel\napproximation. We show that for a broad class of kernels, including the popular\nGaussian and polynomial kernels, computing a relative error $k$-rank\napproximation to $K$ is at least as difficult as multiplying the input data\nmatrix $A \\in \\mathbb{R}^{n \\times d}$ by an arbitrary matrix $C \\in\n\\mathbb{R}^{d \\times k}$. Barring a breakthrough in fast matrix multiplication,\nwhen $k$ is not too large, this requires $\\Omega(nnz(A)k)$ time where $nnz(A)$\nis the number of non-zeros in $A$. This lower bound matches, in many parameter\nregimes, recent work on subquadratic time algorithms for low-rank approximation\nof general kernels [MM16,MW17], demonstrating that these algorithms are\nunlikely to be significantly improved, in particular to $O(nnz(A))$ input\nsparsity runtimes. At the same time there is hope: we show for the first time\nthat $O(nnz(A))$ time approximation is possible for general radial basis\nfunction kernels (e.g., the Gaussian kernel) for the closely related problem of\nlow-rank approximation of the kernelized dataset. \n\n"}
{"id": "1711.04427", "contents": "Title: Grothendieck constant is norm of Strassen matrix multiplication tensor Abstract: We show that two important quantities from two disparate areas of complexity\ntheory --- Strassen's exponent of matrix multiplication $\\omega$ and\nGrothendieck's constant $K_G$ --- are intimately related. They are different\nmeasures of size for the same underlying object --- the matrix multiplication\ntensor, i.e., the $3$-tensor or bilinear operator $\\mu_{l,m,n} : \\mathbb{F}^{l\n\\times m} \\times \\mathbb{F}^{m \\times n} \\to \\mathbb{F}^{l \\times n}$, $(A,B)\n\\mapsto AB$ defined by matrix-matrix product over $\\mathbb{F} = \\mathbb{R}$ or\n$\\mathbb{C}$. It is well-known that Strassen's exponent of matrix\nmultiplication is the greatest lower bound on (the log of) a tensor rank of\n$\\mu_{l,m,n}$. We will show that Grothendieck's constant is the least upper\nbound on a tensor norm of $\\mu_{l,m,n}$, taken over all $l, m, n \\in\n\\mathbb{N}$. Aside from relating the two celebrated quantities, this insight\nallows us to rewrite Grothendieck's inequality as a norm inequality \\[\n\\lVert\\mu_{l,m,n}\\rVert_{1,2,\\infty}\n=\\max_{X,Y,M\\neq0}\\frac{|\\operatorname{tr}(XMY)|}{\\lVert X\\rVert_{1,2}\\lVert\nY\\rVert_{2,\\infty}\\lVert M\\rVert_{\\infty,1}}\\le K_G. \\] We prove that\nGrothendieck's inequality is unique: If we generalize the $(1,2,\\infty)$-norm\nto arbitrary $p,q, r \\in [1, \\infty]$, \\[\n\\lVert\\mu_{l,m,n}\\rVert_{p,q,r}=\\max_{X,Y,M\\neq0}\\frac{|\\operatorname{tr}(XMY)|}{\\|X\\|_{p,q}\\|Y\\|_{q,r}\\|M\\|_{r,p}},\n\\] then $(p,q,r )=(1,2,\\infty)$ is, up to cyclic permutations, the only choice\nfor which $\\lVert\\mu_{l,m,n}\\rVert_{p,q,r}$ is uniformly bounded by a constant\nindependent of $l,m,n$. \n\n"}
{"id": "1711.04712", "contents": "Title: Randomized Near Neighbor Graphs, Giant Components, and Applications in\n  Data Science Abstract: If we pick $n$ random points uniformly in $[0,1]^d$ and connect each point to\nits $k-$nearest neighbors, then it is well known that there exists a giant\nconnected component with high probability. We prove that in $[0,1]^d$ it\nsuffices to connect every point to $ c_{d,1} \\log{\\log{n}}$ points chosen\nrandomly among its $ c_{d,2} \\log{n}-$nearest neighbors to ensure a giant\ncomponent of size $n - o(n)$ with high probability. This construction yields a\nmuch sparser random graph with $\\sim n \\log\\log{n}$ instead of $\\sim n \\log{n}$\nedges that has comparable connectivity properties. This result has nontrivial\nimplications for problems in data science where an affinity matrix is\nconstructed: instead of picking the $k-$nearest neighbors, one can often pick\n$k' \\ll k$ random points out of the $k-$nearest neighbors without sacrificing\nefficiency. This can massively simplify and accelerate computation, we\nillustrate this with several numerical examples. \n\n"}
{"id": "1711.04740", "contents": "Title: Heavy Hitters and the Structure of Local Privacy Abstract: We present a new locally differentially private algorithm for the heavy\nhitters problem which achieves optimal worst-case error as a function of all\nstandardly considered parameters. Prior work obtained error rates which depend\noptimally on the number of users, the size of the domain, and the privacy\nparameter, but depend sub-optimally on the failure probability.\n  We strengthen existing lower bounds on the error to incorporate the failure\nprobability, and show that our new upper bound is tight with respect to this\nparameter as well. Our lower bound is based on a new understanding of the\nstructure of locally private protocols. We further develop these ideas to\nobtain the following general results beyond heavy hitters.\n  $\\bullet$ Advanced Grouposition: In the local model, group privacy for $k$\nusers degrades proportionally to $\\approx \\sqrt{k}$, instead of linearly in $k$\nas in the central model. Stronger group privacy yields improved max-information\nguarantees, as well as stronger lower bounds (via \"packing arguments\"), over\nthe central model.\n  $\\bullet$ Building on a transformation of Bassily and Smith (STOC 2015), we\ngive a generic transformation from any non-interactive approximate-private\nlocal protocol into a pure-private local protocol. Again in contrast with the\ncentral model, this shows that we cannot obtain more accurate algorithms by\nmoving from pure to approximate local privacy. \n\n"}
{"id": "1711.05376", "contents": "Title: Sliced Wasserstein Distance for Learning Gaussian Mixture Models Abstract: Gaussian mixture models (GMM) are powerful parametric tools with many\napplications in machine learning and computer vision. Expectation maximization\n(EM) is the most popular algorithm for estimating the GMM parameters. However,\nEM guarantees only convergence to a stationary point of the log-likelihood\nfunction, which could be arbitrarily worse than the optimal solution. Inspired\nby the relationship between the negative log-likelihood function and the\nKullback-Leibler (KL) divergence, we propose an alternative formulation for\nestimating the GMM parameters using the sliced Wasserstein distance, which\ngives rise to a new algorithm. Specifically, we propose minimizing the\nsliced-Wasserstein distance between the mixture model and the data distribution\nwith respect to the GMM parameters. In contrast to the KL-divergence, the\nenergy landscape for the sliced-Wasserstein distance is more well-behaved and\ntherefore more suitable for a stochastic gradient descent scheme to obtain the\noptimal GMM parameters. We show that our formulation results in parameter\nestimates that are more robust to random initializations and demonstrate that\nit can estimate high-dimensional data distributions more faithfully than the EM\nalgorithm. \n\n"}
{"id": "1711.06756", "contents": "Title: Deep supervised learning using local errors Abstract: Error backpropagation is a highly effective mechanism for learning\nhigh-quality hierarchical features in deep networks. Updating the features or\nweights in one layer, however, requires waiting for the propagation of error\nsignals from higher layers. Learning using delayed and non-local errors makes\nit hard to reconcile backpropagation with the learning mechanisms observed in\nbiological neural networks as it requires the neurons to maintain a memory of\nthe input long enough until the higher-layer errors arrive. In this paper, we\npropose an alternative learning mechanism where errors are generated locally in\neach layer using fixed, random auxiliary classifiers. Lower layers could thus\nbe trained independently of higher layers and training could either proceed\nlayer by layer, or simultaneously in all layers using local error information.\nWe address biological plausibility concerns such as weight symmetry\nrequirements and show that the proposed learning mechanism based on fixed,\nbroad, and random tuning of each neuron to the classification categories\noutperforms the biologically-motivated feedback alignment learning technique on\nthe MNIST, CIFAR10, and SVHN datasets, approaching the performance of standard\nbackpropagation. Our approach highlights a potential biological mechanism for\nthe supervised, or task-dependent, learning of feature hierarchies. In\naddition, we show that it is well suited for learning deep networks in custom\nhardware where it can drastically reduce memory traffic and data communication\noverheads. \n\n"}
{"id": "1711.07128", "contents": "Title: Hello Edge: Keyword Spotting on Microcontrollers Abstract: Keyword spotting (KWS) is a critical component for enabling speech based user\ninteractions on smart devices. It requires real-time response and high accuracy\nfor good user experience. Recently, neural networks have become an attractive\nchoice for KWS architecture because of their superior accuracy compared to\ntraditional speech processing algorithms. Due to its always-on nature, KWS\napplication has highly constrained power budget and typically runs on tiny\nmicrocontrollers with limited memory and compute capability. The design of\nneural network architecture for KWS must consider these constraints. In this\nwork, we perform neural network architecture evaluation and exploration for\nrunning KWS on resource-constrained microcontrollers. We train various neural\nnetwork architectures for keyword spotting published in literature to compare\ntheir accuracy and memory/compute requirements. We show that it is possible to\noptimize these neural network architectures to fit within the memory and\ncompute constraints of microcontrollers without sacrificing accuracy. We\nfurther explore the depthwise separable convolutional neural network (DS-CNN)\nand compare it against other neural network architectures. DS-CNN achieves an\naccuracy of 95.4%, which is ~10% higher than the DNN model with similar number\nof parameters. \n\n"}
{"id": "1711.07970", "contents": "Title: Deep Learning for Physical Processes: Incorporating Prior Scientific\n  Knowledge Abstract: We consider the use of Deep Learning methods for modeling complex phenomena\nlike those occurring in natural physical processes. With the large amount of\ndata gathered on these phenomena the data intensive paradigm could begin to\nchallenge more traditional approaches elaborated over the years in fields like\nmaths or physics. However, despite considerable successes in a variety of\napplication domains, the machine learning field is not yet ready to handle the\nlevel of complexity required by such problems. Using an example application,\nnamely Sea Surface Temperature Prediction, we show how general background\nknowledge gained from physics could be used as a guideline for designing\nefficient Deep Learning models. In order to motivate the approach and to assess\nits generality we demonstrate a formal link between the solution of a class of\ndifferential equations underlying a large family of physical phenomena and the\nproposed model. Experiments and comparison with series of baselines including a\nstate of the art numerical approach is then provided. \n\n"}
{"id": "1711.08421", "contents": "Title: Relief-Based Feature Selection: Introduction and Review Abstract: Feature selection plays a critical role in biomedical data mining, driven by\nincreasing feature dimensionality in target problems and growing interest in\nadvanced but computationally expensive methodologies able to model complex\nassociations. Specifically, there is a need for feature selection methods that\nare computationally efficient, yet sensitive to complex patterns of\nassociation, e.g. interactions, so that informative features are not mistakenly\neliminated prior to downstream modeling. This paper focuses on Relief-based\nalgorithms (RBAs), a unique family of filter-style feature selection algorithms\nthat have gained appeal by striking an effective balance between these\nobjectives while flexibly adapting to various data characteristics, e.g.\nclassification vs. regression. First, this work broadly examines types of\nfeature selection and defines RBAs within that context. Next, we introduce the\noriginal Relief algorithm and associated concepts, emphasizing the intuition\nbehind how it works, how feature weights generated by the algorithm can be\ninterpreted, and why it is sensitive to feature interactions without evaluating\ncombinations of features. Lastly, we include an expansive review of RBA\nmethodological research beyond Relief and its popular descendant, ReliefF. In\nparticular, we characterize branches of RBA research, and provide comparative\nsummaries of RBA algorithms including contributions, strategies, functionality,\ntime complexity, adaptation to key data characteristics, and software\navailability. \n\n"}
{"id": "1711.08841", "contents": "Title: Clustering Semi-Random Mixtures of Gaussians Abstract: Gaussian mixture models (GMM) are the most widely used statistical model for\nthe $k$-means clustering problem and form a popular framework for clustering in\nmachine learning and data analysis. In this paper, we propose a natural\nsemi-random model for $k$-means clustering that generalizes the Gaussian\nmixture model, and that we believe will be useful in identifying robust\nalgorithms. In our model, a semi-random adversary is allowed to make arbitrary\n\"monotone\" or helpful changes to the data generated from the Gaussian mixture\nmodel.\n  Our first contribution is a polynomial time algorithm that provably recovers\nthe ground-truth up to small classification error w.h.p., assuming certain\nseparation between the components. Perhaps surprisingly, the algorithm we\nanalyze is the popular Lloyd's algorithm for $k$-means clustering that is the\nmethod-of-choice in practice. Our second result complements the upper bound by\ngiving a nearly matching information-theoretic lower bound on the number of\nmisclassified points incurred by any $k$-means clustering algorithm on the\nsemi-random model. \n\n"}
{"id": "1711.10145", "contents": "Title: Lower Bounds for Approximating the Matching Polytope Abstract: We prove that any extended formulation that approximates the matching\npolytope on $n$-vertex graphs up to a factor of $(1+\\varepsilon)$ for any\n$\\frac2n \\le \\varepsilon \\le 1$ must have at least\n$\\binom{n}{{\\alpha}/{\\varepsilon}}$ defining inequalities where $0<\\alpha<1$ is\nan absolute constant. This is tight as exhibited by the $(1+\\varepsilon)$\napproximating linear program obtained by dropping the odd set constraints of\nsize larger than $({1+\\varepsilon})/{\\varepsilon}$ from the description of the\nmatching polytope. Previously, a tight lower bound of $2^{\\Omega(n)}$ was only\nknown for $\\varepsilon = O\\left(\\frac{1}{n}\\right)$ [Rothvoss, STOC '14; Braun\nand Pokutta, IEEE Trans. Information Theory '15] whereas for $\\frac2n \\le\n\\varepsilon \\le 1$, the best lower bound was\n$2^{\\Omega\\left({1}/{\\varepsilon}\\right)}$ [Rothvoss, STOC '14]. The key new\ningredient in our proof is a close connection to the non-negative rank of a\nlopsided version of the unique disjointness matrix. \n\n"}
{"id": "1711.10155", "contents": "Title: Adapting Local Sequential Algorithms to the Distributed Setting Abstract: It is a well known fact that sequential algorithms which exhibit a strong\n\"local\" nature can be adapted to the distributed setting given a legal graph\ncoloring. The running time of the distributed algorithm will then be at least\nthe number of colors. Surprisingly, this well known idea was never formally\nstated as a unified framework. In this paper we aim to define a robust family\nof local sequential algorithms which can be easily adapted to the distributed\nsetting. We then develop new tools to further enhance these algorithms,\nachieving state of the art results for fundamental problems.\n  We define a simple class of greedy-like algorithms which we call\n\\emph{orderless-local} algorithms. We show that given a legal $c$-coloring of\nthe graph, every algorithm in this family can be converted into a distributed\nalgorithm running in $O(c)$ communication rounds in the CONGEST model. We show\nthat this family is indeed robust as both the method of conditional\nexpectations and the unconstrained submodular maximization algorithm of\nBuchbinder \\etal \\cite{BuchbinderFNS15} can be expressed as orderless-local\nalgorithms for \\emph{local utility functions} --- Utility functions which have\na strong local nature to them.\n  We use the above algorithms as a base for new distributed approximation\nalgorithms for the weighted variants of some fundamental problems: Max $k$-Cut,\nMax-DiCut, Max 2-SAT and correlation clustering. We develop algorithms which\nhave the same approximation guarantees as their sequential counterparts, up to\na constant additive $\\epsilon$ factor, while achieving an $O(\\log^* n)$ running\ntime for deterministic algorithms and $O(\\epsilon^{-1})$ running time for\nrandomized ones. This improves exponentially upon the currently best known\nalgorithms. \n\n"}
{"id": "1711.11029", "contents": "Title: On the Parameterized Complexity of Approximating Dominating Set Abstract: We study the parameterized complexity of approximating the $k$-Dominating Set\n(DomSet) problem where an integer $k$ and a graph $G$ on $n$ vertices are given\nas input, and the goal is to find a dominating set of size at most $F(k) \\cdot\nk$ whenever the graph $G$ has a dominating set of size $k$. When such an\nalgorithm runs in time $T(k) \\cdot poly(n)$ (i.e., FPT-time) for some\ncomputable function $T$, it is said to be an $F(k)$-FPT-approximation algorithm\nfor $k$-DomSet. We prove the following for every computable functions $T, F$\nand every constant $\\varepsilon > 0$:\n  $\\bullet$ Assuming $W[1]\\neq FPT$, there is no $F(k)$-FPT-approximation\nalgorithm for $k$-DomSet.\n  $\\bullet$ Assuming the Exponential Time Hypothesis (ETH), there is no\n$F(k)$-approximation algorithm for $k$-DomSet that runs in $T(k) \\cdot\nn^{o(k)}$ time.\n  $\\bullet$ Assuming the Strong Exponential Time Hypothesis (SETH), for every\ninteger $k \\geq 2$, there is no $F(k)$-approximation algorithm for $k$-DomSet\nthat runs in $T(k) \\cdot n^{k - \\varepsilon}$ time.\n  $\\bullet$ Assuming the $k$-Sum Hypothesis, for every integer $k \\geq 3$,\nthere is no $F(k)$-approximation algorithm for $k$-DomSet that runs in $T(k)\n\\cdot n^{\\lceil k/2 \\rceil - \\varepsilon}$ time.\n  Our results are obtained by establishing a connection between communication\ncomplexity and hardness of approximation, generalizing the ideas from a recent\nbreakthrough work of Abboud et al. [FOCS 2017]. Specifically, we show that to\nprove hardness of approximation of a certain parameterized variant of the label\ncover problem, it suffices to devise a specific protocol for a communication\nproblem that depends on which hypothesis we rely on. Each of these\ncommunication problems turns out to be either a well studied problem or a\nvariant of one; this allows us to easily apply known techniques to solve them. \n\n"}
{"id": "1711.11560", "contents": "Title: Testing Conditional Independence of Discrete Distributions Abstract: We study the problem of testing \\emph{conditional independence} for discrete\ndistributions. Specifically, given samples from a discrete random variable $(X,\nY, Z)$ on domain $[\\ell_1]\\times[\\ell_2] \\times [n]$, we want to distinguish,\nwith probability at least $2/3$, between the case that $X$ and $Y$ are\nconditionally independent given $Z$ from the case that $(X, Y, Z)$ is\n$\\epsilon$-far, in $\\ell_1$-distance, from every distribution that has this\nproperty. Conditional independence is a concept of central importance in\nprobability and statistics with a range of applications in various scientific\ndomains. As such, the statistical task of testing conditional independence has\nbeen extensively studied in various forms within the statistics and\neconometrics communities for nearly a century. Perhaps surprisingly, this\nproblem has not been previously considered in the framework of distribution\nproperty testing and in particular no tester with sublinear sample complexity\nis known, even for the important special case that the domains of $X$ and $Y$\nare binary.\n  The main algorithmic result of this work is the first conditional\nindependence tester with {\\em sublinear} sample complexity for discrete\ndistributions over $[\\ell_1]\\times[\\ell_2] \\times [n]$. To complement our upper\nbounds, we prove information-theoretic lower bounds establishing that the\nsample complexity of our algorithm is optimal, up to constant factors, for a\nnumber of settings. Specifically, for the prototypical setting when $\\ell_1,\n\\ell_2 = O(1)$, we show that the sample complexity of testing conditional\nindependence (upper bound and matching lower bound) is\n  \\[\n  \\Theta\\left({\\max\\left(n^{1/2}/\\epsilon^2,\\min\\left(n^{7/8}/\\epsilon,n^{6/7}/\\epsilon^{8/7}\\right)\\right)}\\right)\\,.\n  \\] \n\n"}
{"id": "1712.00519", "contents": "Title: An Elementary Analysis of the Probability That a Binomial Random\n  Variable Exceeds Its Expectation Abstract: We give an elementary proof of the fact that a binomial random variable $X$\nwith parameters $n$ and $0.29/n \\le p < 1$ with probability at least $1/4$\nstrictly exceeds its expectation. We also show that for $1/n \\le p < 1 - 1/n$,\n$X$ exceeds its expectation by more than one with probability at least\n$0.0370$. Both probabilities approach $1/2$ when $np$ and $n(1-p)$ tend to\ninfinity. \n\n"}
{"id": "1712.00559", "contents": "Title: Progressive Neural Architecture Search Abstract: We propose a new method for learning the structure of convolutional neural\nnetworks (CNNs) that is more efficient than recent state-of-the-art methods\nbased on reinforcement learning and evolutionary algorithms. Our approach uses\na sequential model-based optimization (SMBO) strategy, in which we search for\nstructures in order of increasing complexity, while simultaneously learning a\nsurrogate model to guide the search through structure space. Direct comparison\nunder the same search space shows that our method is up to 5 times more\nefficient than the RL method of Zoph et al. (2018) in terms of number of models\nevaluated, and 8 times faster in terms of total compute. The structures we\ndiscover in this way achieve state of the art classification accuracies on\nCIFAR-10 and ImageNet. \n\n"}
{"id": "1712.01149", "contents": "Title: An Upper Bound on the GKS Game via Max Bipartite Matching Abstract: The sensitivity conjecture is a longstanding conjecture concerning the\nrelationship between the degree and sensitivity of a Boolean function. In 2015,\na communication game was formulated by Justin Gilmer, Michal Kouck\\'{y}, and\nMichael Saks to attempt to make progress on this conjecture. Andrew Drucker\nindependently formulated this game. Shortly after the creation of the GKS game,\nNisan Szegedy obtained a protocol for the game with a cost of $O(n^{.4732})$.\nWe improve Szegedy's result to a cost of $O(n^{.4696})$ by providing a\ntechnique to identify whether a set of codewords can be used as a viable\nstrategy in this game. \n\n"}
{"id": "1712.03563", "contents": "Title: DGCNN: Disordered Graph Convolutional Neural Network Based on the\n  Gaussian Mixture Model Abstract: Convolutional neural networks (CNNs) can be applied to graph similarity\nmatching, in which case they are called graph CNNs. Graph CNNs are attracting\nincreasing attention due to their effectiveness and efficiency. However, the\nexisting convolution approaches focus only on regular data forms and require\nthe transfer of the graph or key node neighborhoods of the graph into the same\nfixed form. During this transfer process, structural information of the graph\ncan be lost, and some redundant information can be incorporated. To overcome\nthis problem, we propose the disordered graph convolutional neural network\n(DGCNN) based on the mixed Gaussian model, which extends the CNN by adding a\npreprocessing layer called the disordered graph convolutional layer (DGCL). The\nDGCL uses a mixed Gaussian function to realize the mapping between the\nconvolution kernel and the nodes in the neighborhood of the graph. The output\nof the DGCL is the input of the CNN. We further implement a\nbackward-propagation optimization process of the convolutional layer by which\nwe incorporate the feature-learning model of the irregular node neighborhood\nstructure into the network. Thereafter, the optimization of the convolution\nkernel becomes part of the neural network learning process. The DGCNN can\naccept arbitrary scaled and disordered neighborhood graph structures as the\nreceptive fields of CNNs, which reduces information loss during graph\ntransformation. Finally, we perform experiments on multiple standard graph\ndatasets. The results show that the proposed method outperforms the\nstate-of-the-art methods in graph classification and retrieval. \n\n"}
{"id": "1712.04196", "contents": "Title: Empirical Evaluation of Kernel PCA Approximation Methods in\n  Classification Tasks Abstract: Kernel Principal Component Analysis (KPCA) is a popular dimensionality\nreduction technique with a wide range of applications. However, it suffers from\nthe problem of poor scalability. Various approximation methods have been\nproposed in the past to overcome this problem. The Nystr\\\"om method, Randomized\nNonlinear Component Analysis (RNCA) and Streaming Kernel Principal Component\nAnalysis (SKPCA) were proposed to deal with the scalability issue of KPCA.\nDespite having theoretical guarantees, their performance in real world learning\ntasks have not been explored previously. In this work the evaluation of SKPCA,\nRNCA and Nystr\\\"om method for the task of classification is done for several\nreal world datasets. The results obtained indicate that SKPCA based features\ngave much better classification accuracy when compared to the other methods for\na very large dataset. \n\n"}
{"id": "1712.04755", "contents": "Title: Exponential convergence of testing error for stochastic gradient methods Abstract: We consider binary classification problems with positive definite kernels and\nsquare loss, and study the convergence rates of stochastic gradient methods. We\nshow that while the excess testing loss (squared loss) converges slowly to zero\nas the number of observations (and thus iterations) goes to infinity, the\ntesting error (classification error) converges exponentially fast if low-noise\nconditions are assumed. \n\n"}
{"id": "1712.05134", "contents": "Title: Learning Compact Recurrent Neural Networks with Block-Term Tensor\n  Decomposition Abstract: Recurrent Neural Networks (RNNs) are powerful sequence modeling tools.\nHowever, when dealing with high dimensional inputs, the training of RNNs\nbecomes computational expensive due to the large number of model parameters.\nThis hinders RNNs from solving many important computer vision tasks, such as\nAction Recognition in Videos and Image Captioning. To overcome this problem, we\npropose a compact and flexible structure, namely Block-Term tensor\ndecomposition, which greatly reduces the parameters of RNNs and improves their\ntraining efficiency. Compared with alternative low-rank approximations, such as\ntensor-train RNN (TT-RNN), our method, Block-Term RNN (BT-RNN), is not only\nmore concise (when using the same rank), but also able to attain a better\napproximation to the original RNNs with much fewer parameters. On three\nchallenging tasks, including Action Recognition in Videos, Image Captioning and\nImage Generation, BT-RNN outperforms TT-RNN and the standard RNN in terms of\nboth prediction accuracy and convergence rate. Specifically, BT-LSTM utilizes\n17,388 times fewer parameters than the standard LSTM to achieve an accuracy\nimprovement over 15.6\\% in the Action Recognition task on the UCF11 dataset. \n\n"}
{"id": "1712.05735", "contents": "Title: Alternation, Sparsity and Sensitivity : Bounds and Exponential Gaps Abstract: $\\newcommand{\\sp}{\\mathsf{sparsity}}\\newcommand{\\s}{\\mathsf{s}}\\newcommand{\\al}{\\mathsf{alt}}$\nThe well-known Sensitivity Conjecture states that for any Boolean function $f$,\nblock sensitivity of $f$ is at most polynomial in sensitivity of $f$ (denoted\nby $\\s(f)$). The XOR Log-Rank Conjecture states that for any $n$ bit Boolean\nfunction, $f$ the communication complexity of a related function $f^{\\oplus}$\non $2n$ bits, (defined as $f^{\\oplus}(x,y)=f(x\\oplus y)$) is at most polynomial\nin logarithm of the sparsity of $f$ (denoted by $\\sp(f)$). A recent result of\nLin and Zhang (2017) implies that to confirm the above conjectures it suffices\nto upper bound alternation of $f$ (denoted $\\al(f)$) for all Boolean functions\n$f$ by polynomial in $\\s(f)$ and logarithm of $\\sp(f)$, respectively. In this\ncontext, we show the following :\n  * There exists a family of Boolean functions for which $\\al(f)$ is at least\nexponential in $\\s(f)$ and $\\al(f)$ is at least exponential in $\\log \\sp(f)$.\nEn route to the proof, we also show an exponential gap between $\\al(f)$ and the\ndecision tree complexity of $f$, which might be of independent interest.\n  * As our main result, we show that, despite the above gap between $\\al(f)$\nand $\\log \\sp(f)$, the XOR Log-Rank Conjecture is true for functions with the\nalternation upper bounded by $poly(\\log n)$. It is easy to observe that the\nSensitivity Conjecture is also true for this class of functions.\n  * The starting point for the above result is the observation (derived from\nLin and Zhang (2017)) that for any Boolean function $f$ and $m \\ge 2$,\n$deg(f)\\le \\al(f)deg_2(f)deg_m(f)$ where $deg(f)$, $deg_2(f)$ and $deg_m(f)$\nare the degrees of $f$ over $\\mathbb{R}$, $\\mathbb{F}_2$ and $\\mathbb{Z}_m$\nrespectively. We also show three further applications of this observation. \n\n"}
{"id": "1712.06424", "contents": "Title: Learning to Write Stylized Chinese Characters by Reading a Handful of\n  Examples Abstract: Automatically writing stylized Chinese characters is an attractive yet\nchallenging task due to its wide applicabilities. In this paper, we propose a\nnovel framework named Style-Aware Variational Auto-Encoder (SA-VAE) to flexibly\ngenerate Chinese characters. Specifically, we propose to capture the different\ncharacteristics of a Chinese character by disentangling the latent features\ninto content-related and style-related components. Considering of the complex\nshapes and structures, we incorporate the structure information as prior\nknowledge into our framework to guide the generation. Our framework shows a\npowerful one-shot/low-shot generalization ability by inferring the style\ncomponent given a character with unseen style. To the best of our knowledge,\nthis is the first attempt to learn to write new-style Chinese characters by\nobserving only one or a few examples. Extensive experiments demonstrate its\neffectiveness in generating different stylized Chinese characters by fusing the\nfeature vectors corresponding to different contents and styles, which is of\nsignificant importance in real-world applications. \n\n"}
{"id": "1712.06481", "contents": "Title: Inductive $k$-independent graphs and $c$-colorable subgraphs in\n  scheduling: A review Abstract: Inductive $k$-independent graphs generalize chordal graphs and have recently\nbeen advocated in the context of interference-avoiding wireless communication\nscheduling. The NP-hard problem of finding maximum-weight induced $c$-colorable\nsubgraphs, which is a generalization of finding maximum independent sets,\nnaturally occurs when selecting $c$ sets of pairwise non-conflicting jobs\n(modeled as graph vertices). We investigate the parameterized complexity of\nthis problem on inductive $k$-independent graphs. We show that the Independent\nSet problem is W[1]-hard even on 2-simplicial 3-minoes---a subclass of\ninductive 2-independent graphs. In contrast, we prove that the more general\nMaximum $c$-Colorable Subgraph problem is fixed-parameter tractable on\nedge-wise unions of cluster and chordal graphs, which are 2-simplicial. In both\ncases, the parameter is the solution size. Aside from this, we survey other\ngraph classes between inductive 1-inductive and inductive 2-inductive graphs\nwith applications in scheduling. \n\n"}
{"id": "1712.06865", "contents": "Title: Approximate Correlation Clustering Using Same-Cluster Queries Abstract: Ashtiani et al. (NIPS 2016) introduced a semi-supervised framework for\nclustering (SSAC) where a learner is allowed to make same-cluster queries. More\nspecifically, in their model, there is a query oracle that answers queries of\nthe form given any two vertices, do they belong to the same optimal cluster?.\nAshtiani et al. showed the usefulness of such a query framework by giving a\npolynomial time algorithm for the k-means clustering problem where the input\ndataset satisfies some separation condition. Ailon et al. extended the above\nwork to the approximation setting by giving an efficient (1+\\eps)-approximation\nalgorithm for k-means for any small \\eps > 0 and any dataset within the SSAC\nframework. In this work, we extend this line of study to the correlation\nclustering problem. Correlation clustering is a graph clustering problem where\npairwise similarity (or dissimilarity) information is given for every pair of\nvertices and the objective is to partition the vertices into clusters that\nminimise the disagreement (or maximises agreement) with the pairwise\ninformation given as input. These problems are popularly known as MinDisAgree\nand MaxAgree problems, and MinDisAgree[k] and MaxAgree[k] are versions of these\nproblems where the number of optimal clusters is at most k. There exist\nPolynomial Time Approximation Schemes (PTAS) for MinDisAgree[k] and MaxAgree[k]\nwhere the approximation guarantee is (1+\\eps) for any small \\eps and the\nrunning time is polynomial in the input parameters but exponential in k and\n1/\\eps. We obtain an (1+\\eps)-approximation algorithm for any small \\eps with\nrunning time that is polynomial in the input parameters and also in k and\n1/\\eps. We also give non-trivial upper and lower bounds on the number of\nsame-cluster queries, the lower bound being based on the Exponential Time\nHypothesis (ETH). \n\n"}
{"id": "1712.07027", "contents": "Title: Snake: a Stochastic Proximal Gradient Algorithm for Regularized Problems\n  over Large Graphs Abstract: A regularized optimization problem over a large unstructured graph is\nstudied, where the regularization term is tied to the graph geometry. Typical\nregularization examples include the total variation and the Laplacian\nregularizations over the graph. When applying the proximal gradient algorithm\nto solve this problem, there exist quite affordable methods to implement the\nproximity operator (backward step) in the special case where the graph is a\nsimple path without loops. In this paper, an algorithm, referred to as \"Snake\",\nis proposed to solve such regularized problems over general graphs, by taking\nbenefit of these fast methods. The algorithm consists in properly selecting\nrandom simple paths in the graph and performing the proximal gradient algorithm\nover these simple paths. This algorithm is an instance of a new general\nstochastic proximal gradient algorithm, whose convergence is proven.\nApplications to trend filtering and graph inpainting are provided among others.\nNumerical experiments are conducted over large graphs. \n\n"}
{"id": "1712.07276", "contents": "Title: Uniform Diagonalization Theorem for Complexity Classes of Promise\n  Problems including Randomized and Quantum Classes Abstract: Diagonalization in the spirit of Cantor's diagonal arguments is a widely used\ntool in theoretical computer sciences to obtain structural results about\ncomputational problems and complexity classes by indirect proofs. The Uniform\nDiagonalization Theorem allows the construction of problems outside complexity\nclasses while still being reducible to a specific decision problem. This paper\nprovides a generalization of the Uniform Diagonalization Theorem by extending\nit to promise problems and the complexity classes they form, e.g. randomized\nand quantum complexity classes. The theorem requires from the underlying\ncomputing model not only the decidability of its acceptance and rejection\nbehaviour but also of its promise-contradicting indifferent behaviour - a\nproperty that we will introduce as \"total decidability\" of promise problems.\n  Implications of the Uniform Diagonalization Theorem are mainly of two kinds:\n1. Existence of intermediate problems (e.g. between BQP and QMA) - also known\nas Ladner's Theorem - and 2. Undecidability if a problem of a complexity class\nis contained in a subclass (e.g. membership of a QMA-problem in BQP). Like the\noriginal Uniform Diagonalization Theorem the extension applies besides BQP and\nQMA to a large variety of complexity class pairs, including combinations from\ndeterministic, randomized and quantum classes. \n\n"}
{"id": "1712.07811", "contents": "Title: Multi-dimensional Graph Fourier Transform Abstract: Many signals on Cartesian product graphs appear in the real world, such as\ndigital images, sensor observation time series, and movie ratings on Netflix.\nThese signals are \"multi-dimensional\" and have directional characteristics\nalong each factor graph. However, the existing graph Fourier transform does not\ndistinguish these directions, and assigns 1-D spectra to signals on product\ngraphs. Further, these spectra are often multi-valued at some frequencies. Our\nmain result is a multi-dimensional graph Fourier transform that solves such\nproblems associated with the conventional GFT. Using algebraic properties of\nCartesian products, the proposed transform rearranges 1-D spectra obtained by\nthe conventional GFT into the multi-dimensional frequency domain, of which each\ndimension represents a directional frequency along each factor graph. Thus, the\nmulti-dimensional graph Fourier transform enables directional frequency\nanalysis, in addition to frequency analysis with the conventional GFT.\nMoreover, this rearrangement resolves the multi-valuedness of spectra in some\ncases. The multi-dimensional graph Fourier transform is a foundation of novel\nfilterings and stationarities that utilize dimensional information of graph\nsignals, which are also discussed in this study. The proposed methods are\napplicable to a wide variety of data that can be regarded as signals on\nCartesian product graphs. This study also notes that multivariate graph signals\ncan be regarded as 2-D univariate graph signals. This correspondence provides\nnatural definitions of the multivariate graph Fourier transform and the\nmultivariate stationarity based on their 2-D univariate versions. \n\n"}
{"id": "1712.08205", "contents": "Title: Practically-Self-Stabilizing Vector Clocks in the Absence of Execution\n  Fairness Abstract: Vector clock algorithms are basic wait-free building blocks that facilitate\ncausal ordering of events. As wait-free algorithms, they are guaranteed to\ncomplete their operations within a finite number of steps. Stabilizing\nalgorithms allow the system to recover after the occurrence of transient\nfaults, such as soft errors and arbitrary violations of the assumptions\naccording to which the system was designed to behave. We present the first, to\nthe best of our knowledge, stabilizing vector clock algorithm for asynchronous\ncrash-prone message-passing systems that can recover in a wait-free manner\nafter the occurrence of transient faults. In these settings, it is challenging\nto demonstrate a finite and wait-free recovery from (communication and crash\nfailures as well as) transient faults, bound the message and storage sizes,\ndeal with the removal of all stale information without blocking, and deal with\ncounter overflow events (which occur at different network nodes concurrently).\n  We present an algorithm that never violates safety in the absence of\ntransient faults and provides bounded time recovery during fair executions that\nfollow the last transient fault. The novelty is that in the absence of\nexecution fairness, the algorithm guarantees a bound on the number of times in\nwhich the system might violate safety (while existing algorithms might block\nforever due to the presence of both transient faults and crash failures).\n  Since vector clocks facilitate a number of elementary synchronization\nbuilding blocks (without requiring remote replica synchronization) in\nasynchronous systems, we believe that our analytical insights are useful for\nthe design of other systems that cannot guarantee execution fairness. \n\n"}
{"id": "1712.08373", "contents": "Title: Notes on complexity of packing coloring Abstract: A packing $k$-coloring for some integer $k$ of a graph $G=(V,E)$ is a mapping\n  $\\varphi:V\\to\\{1,\\ldots,k\\}$ such that any two vertices $u, v$ of color\n$\\varphi(u)=\\varphi(v)$ are in distance at least $\\varphi(u)+1$. This concept\nis motivated by frequency assignment problems. The \\emph{packing chromatic\nnumber} of $G$ is the smallest $k$ such that there exists a packing\n$k$-coloring of $G$.\n  Fiala and Golovach showed that determining the packing chromatic number for\nchordal graphs is \\NP-complete for diameter exactly 5. While the problem is\neasy to solve for diameter 2, we show \\NP-completeness for any diameter at\nleast 3. Our reduction also shows that the packing chromatic number is hard to\napproximate within $n^{{1/2}-\\varepsilon}$ for any $\\varepsilon > 0$.\n  In addition, we design an \\FPT algorithm for interval graphs of bounded\ndiameter. This leads us to exploring the problem of finding a partial coloring\nthat maximizes the number of colored vertices. \n\n"}
{"id": "1712.09617", "contents": "Title: On efficiently solvable cases of Quantum k-SAT Abstract: The constraint satisfaction problems k-SAT and Quantum k-SAT (k-QSAT) are\ncanonical NP-complete and QMA_1-complete problems (for k>=3), respectively,\nwhere QMA_1 is a quantum generalization of NP with one-sided error. Whereas\nk-SAT has been well-studied for special tractable cases, as well as from a\nparameterized complexity perspective, much less is known in similar settings\nfor k-QSAT. Here, we study the open problem of computing satisfying assignments\nto k-QSAT instances which have a \"matching\" or \"dimer covering\"; this is an NP\nproblem whose decision variant is trivial, but whose search complexity remains\nopen.\n  Our results fall into three directions, all of which relate to the \"matching\"\nsetting: (1) We give a polynomial-time classical algorithm for k-QSAT when all\nqubits occur in at most two clauses. (2) We give a parameterized algorithm for\nk-QSAT instances from a certain non-trivial class, which allows us to obtain\nexponential speedups over brute force methods in some cases. This is achieved\nby reducing the problem to solving for a single root of a single univariate\npolynomial. (3) We conduct a structural graph theoretic study of 3-QSAT\ninteraction graphs which have a \"matching\". We remark that the results of (2),\nin particular, introduce a number of new tools to the study of Quantum SAT,\nincluding graph theoretic concepts such as transfer filtrations and blow-ups\nfrom algebraic geometry. \n\n"}
{"id": "1712.09707", "contents": "Title: Deep learning for universal linear embeddings of nonlinear dynamics Abstract: Identifying coordinate transformations that make strongly nonlinear dynamics\napproximately linear is a central challenge in modern dynamical systems. These\ntransformations have the potential to enable prediction, estimation, and\ncontrol of nonlinear systems using standard linear theory. The Koopman operator\nhas emerged as a leading data-driven embedding, as eigenfunctions of this\noperator provide intrinsic coordinates that globally linearize the dynamics.\nHowever, identifying and representing these eigenfunctions has proven to be\nmathematically and computationally challenging. This work leverages the power\nof deep learning to discover representations of Koopman eigenfunctions from\ntrajectory data of dynamical systems. Our network is parsimonious and\ninterpretable by construction, embedding the dynamics on a low-dimensional\nmanifold that is of the intrinsic rank of the dynamics and parameterized by the\nKoopman eigenfunctions. In particular, we identify nonlinear coordinates on\nwhich the dynamics are globally linear using a modified auto-encoder. We also\ngeneralize Koopman representations to include a ubiquitous class of systems\nthat exhibit continuous spectra, ranging from the simple pendulum to nonlinear\noptics and broadband turbulence. Our framework parametrizes the continuous\nfrequency using an auxiliary network, enabling a compact and efficient\nembedding at the intrinsic rank, while connecting our models to half a century\nof asymptotics. In this way, we benefit from the power and generality of deep\nlearning, while retaining the physical interpretability of Koopman embeddings. \n\n"}
{"id": "1712.09948", "contents": "Title: Minimizing Polarization and Disagreement in Social Networks Abstract: The rise of social media and online social networks has been a disruptive\nforce in society. Opinions are increasingly shaped by interactions on online\nsocial media, and social phenomena including disagreement and polarization are\nnow tightly woven into everyday life. In this work we initiate the study of the\nfollowing question: given $n$ agents, each with its own initial opinion that\nreflects its core value on a topic, and an opinion dynamics model, what is the\nstructure of a social network that minimizes {\\em polarization} and {\\em\ndisagreement} simultaneously?\n  This question is central to recommender systems: should a recommender system\nprefer a link suggestion between two online users with similar mindsets in\norder to keep disagreement low, or between two users with different opinions in\norder to expose each to the other's viewpoint of the world, and decrease\noverall levels of polarization? Our contributions include a mathematical\nformalization of this question as an optimization problem and an exact,\ntime-efficient algorithm. We also prove that there always exists a network with\n$O(n/\\epsilon^2)$ edges that is a $(1+\\epsilon)$ approximation to the optimum.\nFor a fixed graph, we additionally show how to optimize our objective function\nover the agents' innate opinions in polynomial time.\n  We perform an empirical study of our proposed methods on synthetic and\nreal-world data that verify their value as mining tools to better understand\nthe trade-off between of disagreement and polarization. We find that there is a\nlot of space to reduce both polarization and disagreement in real-world\nnetworks; for instance, on a Reddit network where users exchange comments on\npolitics, our methods achieve a $\\sim 60\\,000$-fold reduction in polarization\nand disagreement. \n\n"}
{"id": "1801.00318", "contents": "Title: Towards Building an Intelligent Anti-Malware System: A Deep Learning\n  Approach using Support Vector Machine (SVM) for Malware Classification Abstract: Effective and efficient mitigation of malware is a long-time endeavor in the\ninformation security community. The development of an anti-malware system that\ncan counteract an unknown malware is a prolific activity that may benefit\nseveral sectors. We envision an intelligent anti-malware system that utilizes\nthe power of deep learning (DL) models. Using such models would enable the\ndetection of newly-released malware through mathematical generalization. That\nis, finding the relationship between a given malware $x$ and its corresponding\nmalware family $y$, $f: x \\mapsto y$. To accomplish this feat, we used the\nMalimg dataset (Nataraj et al., 2011) which consists of malware images that\nwere processed from malware binaries, and then we trained the following DL\nmodels 1 to classify each malware family: CNN-SVM (Tang, 2013), GRU-SVM\n(Agarap, 2017), and MLP-SVM. Empirical evidence has shown that the GRU-SVM\nstands out among the DL models with a predictive accuracy of ~84.92%. This\nstands to reason for the mentioned model had the relatively most sophisticated\narchitecture design among the presented models. The exploration of an even more\noptimal DL-SVM model is the next stage towards the engineering of an\nintelligent anti-malware system. \n\n"}
{"id": "1801.00496", "contents": "Title: On the tensor rank of $3\\times 3$ permanent and determinant Abstract: The tensor rank and border rank of the $3 \\times 3$ determinant tensor is\nknown to be $5$ if characteristic is not two. In this paper, we show that the\ntensor rank remains $5$ for fields of characteristic two as well. We also\ninclude an analysis of $5 \\times 5$ and $7 \\times 7$ determinant and permanent\ntensors, as well as the symmetric $3 \\times 3$ permanent and determinant\ntensors. We end with some remarks on binary tensors. \n\n"}
{"id": "1801.01105", "contents": "Title: Generalizing the Kawaguchi-Kyan bound to stochastic parallel machine\n  scheduling Abstract: Minimizing the sum of weighted completion times on $m$ identical parallel\nmachines is one of the most important and classical scheduling problems. For\nthe stochastic variant where processing times of jobs are random variables,\nM\\\"ohring, Schulz, and Uetz (1999) presented the first and still best known\napproximation result achieving, for arbitrarily many machines, performance\nratio $1+\\frac12(1+\\Delta)$, where $\\Delta$ is an upper bound on the squared\ncoefficient of variation of the processing times. We prove performance ratio\n$1+\\frac12(\\sqrt{2}-1)(1+\\Delta)$ for the same underlying algorithm---the\nWeighted Shortest Expected Processing Time (WSEPT) rule. For the special case\nof deterministic scheduling (i.e., $\\Delta=0$), our bound matches the tight\nperformance ratio $\\frac12(1+\\sqrt{2})$ of this algorithm (WSPT rule), derived\nby Kawaguchi and Kyan in a 1986 landmark paper. We present several further\nimprovements for WSEPT's performance ratio, one of them relying on a carefully\nrefined analysis of WSPT yielding, for every fixed number of machines $m$,\nWSPT's exact performance ratio of order $\\frac12(1+\\sqrt{2})-O(1/m^2)$. \n\n"}
{"id": "1801.02602", "contents": "Title: Three Puzzles on Mathematics, Computation, and Games Abstract: In this lecture I will talk about three mathematical puzzles involving\nmathematics and computation that have preoccupied me over the years. The first\npuzzle is to understand the amazing success of the simplex algorithm for linear\nprogramming. The second puzzle is about errors made when votes are counted\nduring elections. The third puzzle is: are quantum computers possible? \n\n"}
{"id": "1801.04062", "contents": "Title: MINE: Mutual Information Neural Estimation Abstract: We argue that the estimation of mutual information between high dimensional\ncontinuous random variables can be achieved by gradient descent over neural\nnetworks. We present a Mutual Information Neural Estimator (MINE) that is\nlinearly scalable in dimensionality as well as in sample size, trainable\nthrough back-prop, and strongly consistent. We present a handful of\napplications on which MINE can be used to minimize or maximize mutual\ninformation. We apply MINE to improve adversarially trained generative models.\nWe also use MINE to implement Information Bottleneck, applying it to supervised\nclassification; our results demonstrate substantial improvement in flexibility\nand performance in these settings. \n\n"}
{"id": "1801.05453", "contents": "Title: Beyond Word Importance: Contextual Decomposition to Extract Interactions\n  from LSTMs Abstract: The driving force behind the recent success of LSTMs has been their ability\nto learn complex and non-linear relationships. Consequently, our inability to\ndescribe these relationships has led to LSTMs being characterized as black\nboxes. To this end, we introduce contextual decomposition (CD), an\ninterpretation algorithm for analysing individual predictions made by standard\nLSTMs, without any changes to the underlying model. By decomposing the output\nof a LSTM, CD captures the contributions of combinations of words or variables\nto the final prediction of an LSTM. On the task of sentiment analysis with the\nYelp and SST data sets, we show that CD is able to reliably identify words and\nphrases of contrasting sentiment, and how they are combined to yield the LSTM's\nfinal prediction. Using the phrase-level labels in SST, we also demonstrate\nthat CD is able to successfully extract positive and negative negations from an\nLSTM, something which has not previously been done. \n\n"}
{"id": "1801.05856", "contents": "Title: Active Community Detection with Maximal Expected Model Change Abstract: We present a novel active learning algorithm for community detection on\nnetworks. Our proposed algorithm uses a Maximal Expected Model Change (MEMC)\ncriterion for querying network nodes label assignments. MEMC detects nodes that\nmaximally change the community assignment likelihood model following a query.\nOur method is inspired by detection in the benchmark Stochastic Block Model\n(SBM), where we provide sample complexity analysis and empirical study with SBM\nand real network data for binary as well as for the multi-class settings. The\nanalysis also covers the most challenging case of sparse degree and\nbelow-detection-threshold SBMs, where we observe a super-linear error\nreduction. MEMC is shown to be superior to the random selection baseline and\nother state-of-the-art active learners. \n\n"}
{"id": "1801.06237", "contents": "Title: Minor Excluded Network Families Admit Fast Distributed Algorithms Abstract: Distributed network optimization algorithms, such as minimum spanning tree,\nminimum cut, and shortest path, are an active research area in distributed\ncomputing. This paper presents a fast distributed algorithm for such problems\nin the CONGEST model, on networks that exclude a fixed minor.\n  On general graphs, many optimization problems, including the ones mentioned\nabove, require $\\tilde\\Omega(\\sqrt n)$ rounds of communication in the CONGEST\nmodel, even if the network graph has a much smaller diameter. Naturally, the\nnext step in algorithm design is to design efficient algorithms which bypass\nthis lower bound on a restricted class of graphs. Currently, the only known\nmethod of doing so uses the low-congestion shortcut framework of Ghaffari and\nHaeupler [SODA'16]. Building off of their work, this paper proves that excluded\nminor graphs admit high-quality shortcuts, leading to an $\\tilde O(D^2)$ round\nalgorithm for the aforementioned problems, where $D$ is the diameter of the\nnetwork graph. To work with excluded minor graph families, we utilize the Graph\nStructure Theorem of Robertson and Seymour. To the best of our knowledge, this\nis the first time the Graph Structure Theorem has been used for an algorithmic\nresult in the distributed setting.\n  Even though the proof is involved, merely showing the existence of good\nshortcuts is sufficient to obtain simple, efficient distributed algorithms. In\nparticular, the shortcut framework can efficiently construct near-optimal\nshortcuts and then use them to solve the optimization problems. This, combined\nwith the very general family of excluded minor graphs, which includes most\nother important graph classes, makes this result of significant interest. \n\n"}
{"id": "1801.06879", "contents": "Title: Bayesian Deep Convolutional Encoder-Decoder Networks for Surrogate\n  Modeling and Uncertainty Quantification Abstract: We are interested in the development of surrogate models for uncertainty\nquantification and propagation in problems governed by stochastic PDEs using a\ndeep convolutional encoder-decoder network in a similar fashion to approaches\nconsidered in deep learning for image-to-image regression tasks. Since normal\nneural networks are data intensive and cannot provide predictive uncertainty,\nwe propose a Bayesian approach to convolutional neural nets. A recently\nintroduced variational gradient descent algorithm based on Stein's method is\nscaled to deep convolutional networks to perform approximate Bayesian inference\non millions of uncertain network parameters. This approach achieves state of\nthe art performance in terms of predictive accuracy and uncertainty\nquantification in comparison to other approaches in Bayesian neural networks as\nwell as techniques that include Gaussian processes and ensemble methods even\nwhen the training data size is relatively small. To evaluate the performance of\nthis approach, we consider standard uncertainty quantification benchmark\nproblems including flow in heterogeneous media defined in terms of limited\ndata-driven permeability realizations. The performance of the surrogate model\ndeveloped is very good even though there is no underlying structure shared\nbetween the input (permeability) and output (flow/pressure) fields as is often\nthe case in the image-to-image regression models used in computer vision\nproblems. Studies are performed with an underlying stochastic input\ndimensionality up to $4,225$ where most other uncertainty quantification\nmethods fail. Uncertainty propagation tasks are considered and the predictive\noutput Bayesian statistics are compared to those obtained with Monte Carlo\nestimates. \n\n"}
{"id": "1801.07145", "contents": "Title: E-swish: Adjusting Activations to Different Network Depths Abstract: Activation functions have a notorious impact on neural networks on both\ntraining and testing the models against the desired problem. Currently, the\nmost used activation function is the Rectified Linear Unit (ReLU). This paper\nintroduces a new and novel activation function, closely related with the new\nactivation $Swish = x * sigmoid(x)$ (Ramachandran et al., 2017) which\ngeneralizes it. We call the new activation $E-swish = \\beta x * sigmoid(x)$. We\nshow that E-swish outperforms many other well-known activations including both\nReLU and Swish. For example, using E-swish provided 1.5% and 4.6% accuracy\nimprovements on Cifar10 and Cifar100 respectively for the WRN 10-2 when\ncompared to ReLU and 0.35% and 0.6% respectively when compared to Swish. The\ncode to reproduce all our experiments can be found at\nhttps://github.com/EricAlcaide/E-swish \n\n"}
{"id": "1801.07292", "contents": "Title: Convergence of Value Aggregation for Imitation Learning Abstract: Value aggregation is a general framework for solving imitation learning\nproblems. Based on the idea of data aggregation, it generates a policy sequence\nby iteratively interleaving policy optimization and evaluation in an online\nlearning setting. While the existence of a good policy in the policy sequence\ncan be guaranteed non-asymptotically, little is known about the convergence of\nthe sequence or the performance of the last policy. In this paper, we debunk\nthe common belief that value aggregation always produces a convergent policy\nsequence with improving performance. Moreover, we identify a critical stability\ncondition for convergence and provide a tight non-asymptotic bound on the\nperformance of the last policy. These new theoretical insights let us stabilize\nproblems with regularization, which removes the inconvenient process of\nidentifying the best policy in the policy sequence in stochastic problems. \n\n"}
{"id": "1801.07553", "contents": "Title: Stable gonality is computable Abstract: Stable gonality is a multigraph parameter that measures the complexity of a\ngraph. It is defined using maps to trees. Those maps, in some sense, divide the\nedges equally over the edges of the tree; stable gonality asks for the map with\nthe minimum number of edges mapped to each edge of the tree. This parameter is\nrelated to treewidth, but unlike treewidth, it distinguishes multigraphs from\ntheir underlying simple graphs. Stable gonality is relevant for problems in\nnumber theory. In this paper, we show that deciding whether the stable gonality\nof a given graph is at most a given integer $k$ belongs to the class NP, and we\ngive an algorithm that computes the stable gonality of a graph in\n$O((1.33n)^nm^m \\text{poly}(n,m))$ time. \n\n"}
{"id": "1801.08564", "contents": "Title: An Asymptotically Tight Bound on the Number of Relevant Variables in a\n  Bounded Degree Boolean Function Abstract: We prove that there is a constant $C\\leq 6.614$ such that every Boolean\nfunction of degree at most $d$ (as a polynomial over $\\mathbb{R}$) is a $C\\cdot\n2^d$-junta, i.e. it depends on at most $C\\cdot 2^d$ variables. This improves\nthe $d\\cdot 2^{d-1}$ upper bound of Nisan and Szegedy [Computational Complexity\n4 (1994)]. Our proof uses a new weighting scheme where we assign weights to\nvariables based on the highest degree monomial they appear on.\n  The bound of $C\\cdot 2^d$ is tight up to the constant $C$ as a lower bound of\n$2^d-1$ is achieved by a read-once decision tree of depth $d$. We slightly\nimprove the lower bound by constructing, for each positive integer $d$, a\nfunction of degree $d$ with $3\\cdot 2^{d-1}-2$ relevant variables. A similar\nconstruction was independently observed by Shinkar and Tal. \n\n"}
{"id": "1802.00459", "contents": "Title: Nearly Optimal Dynamic $k$-Means Clustering for High-Dimensional Data Abstract: We consider the $k$-means clustering problem in the dynamic streaming\nsetting, where points from a discrete Euclidean space $\\{1, 2, \\ldots,\n\\Delta\\}^d$ can be dynamically inserted to or deleted from the dataset. For\nthis problem, we provide a one-pass coreset construction algorithm using space\n$\\tilde{O}(k\\cdot \\mathrm{poly}(d, \\log\\Delta))$, where $k$ is the target\nnumber of centers. To our knowledge, this is the first dynamic geometric data\nstream algorithm for $k$-means using space polynomial in dimension and nearly\noptimal (linear) in $k$. \n\n"}
{"id": "1802.01284", "contents": "Title: Task-Aware Compressed Sensing with Generative Adversarial Networks Abstract: In recent years, neural network approaches have been widely adopted for\nmachine learning tasks, with applications in computer vision. More recently,\nunsupervised generative models based on neural networks have been successfully\napplied to model data distributions via low-dimensional latent spaces. In this\npaper, we use Generative Adversarial Networks (GANs) to impose structure in\ncompressed sensing problems, replacing the usual sparsity constraint. We\npropose to train the GANs in a task-aware fashion, specifically for\nreconstruction tasks. We also show that it is possible to train our model\nwithout using any (or much) non-compressed data. Finally, we show that the\nlatent space of the GAN carries discriminative information and can further be\nregularized to generate input features for general inference tasks. We\ndemonstrate the effectiveness of our method on a variety of reconstruction and\nclassification problems. \n\n"}
{"id": "1802.02270", "contents": "Title: Error correction in fast matrix multiplication and inverse Abstract: We present new algorithms to detect and correct errors in the product of two\nmatrices, or the inverse of a matrix, over an arbitrary field. Our algorithms\ndo not require any additional information or encoding other than the original\ninputs and the erroneous output. Their running time is softly linear in the\nnumber of nonzero entries in these matrices when the number of errors is\nsufficiently small, and they also incorporate fast matrix multiplication so\nthat the cost scales well when the number of errors is large. These algorithms\nbuild on the recent result of Gasieniec et al (2017) on correcting matrix\nproducts, as well as existing work on verification algorithms, sparse low-rank\nlinear algebra, and sparse polynomial interpolation. \n\n"}
{"id": "1802.02346", "contents": "Title: On Synthesis of Reversible Circuits with Small Number of Additional\n  Inputs Consisting of NOT, CNOT and 2-CNOT Gates Abstract: The paper discusses the gate complexity of reversible circuits with the small\nnumber of additional inputs consisting of NOT, CNOT and 2-CNOT gates. We study\nShannon's gate complexity function $L(n, q)$ for a reversible circuit\nimplementing a Boolean transformation $f\\colon \\mathbb Z_2^n \\to \\mathbb Z_2^n$\nwith $q \\leqslant O(n^2)$ additional inputs. The general bound $L(n,q) \\asymp\nn2^n \\mathop / \\log_2 n$ is proved for this case. \n\n"}
{"id": "1802.02498", "contents": "Title: Spectral Learning of Binomial HMMs for DNA Methylation Data Abstract: We consider learning parameters of Binomial Hidden Markov Models, which may\nbe used to model DNA methylation data. The standard algorithm for the problem\nis EM, which is computationally expensive for sequences of the scale of the\nmammalian genome. Recently developed spectral algorithms can learn parameters\nof latent variable models via tensor decomposition, and are highly efficient\nfor large data. However, these methods have only been applied to categorial\nHMMs, and the main challenge is how to extend them to Binomial HMMs while still\nretaining computational efficiency. We address this challenge by introducing a\nnew feature-map based approach that exploits specific properties of Binomial\nHMMs. We provide theoretical performance guarantees for our algorithm and\nevaluate it on real DNA methylation data. \n\n"}
{"id": "1802.03235", "contents": "Title: The $b$-bibranching Problem: TDI System, Packing, and Discrete Convexity Abstract: In this paper, we introduce the $b$-bibranching problem in digraphs, which is\na common generalization of the bibranching and $b$-branching problems. The\nbibranching problem, introduced by Schrijver (1982), is a common generalization\nof the branching and bipartite edge cover problems. Previous results on\nbibranchings include polynomial algorithms, a linear programming formulation\nwith total dual integrality, a packing theorem, and an M-convex submodular flow\nformulation. The $b$-branching problem, recently introduced by Kakimura,\nKamiyama, and Takazawa (2018), is a generalization of the branching problem\nadmitting higher indegree, i.e., each vertex $v$ can have indegree at most\n$b(v)$. For $b$-branchings, a combinatorial algorithm, a linear programming\nformulation with total dual integrality, and a packing theorem for branchings\nare extended. A main contribution of this paper is to extend those previous\nresults on bibranchings and $b$-branchings to $b$-bibranchings. That is, we\npresent a linear programming formulation with total dual integrality, a packing\ntheorem, and an M-convex submodular flow formulation for $b$-bibranchings. In\nparticular, the linear program and M-convex submodular flow formulations\nrespectively imply polynomial algorithms for finding a shortest\n$b$-bibranching. \n\n"}
{"id": "1802.03532", "contents": "Title: Bayesian Optimization Using Monotonicity Information and Its Application\n  in Machine Learning Hyperparameter Abstract: We propose an algorithm for a family of optimization problems where the\nobjective can be decomposed as a sum of functions with monotonicity properties.\nThe motivating problem is optimization of hyperparameters of machine learning\nalgorithms, where we argue that the objective, validation error, can be\ndecomposed as monotonic functions of the hyperparameters. Our proposed\nalgorithm adapts Bayesian optimization methods to incorporate the monotonicity\nconstraints. We illustrate the advantages of exploiting monotonicity using\nillustrative examples and demonstrate the improvements in optimization\nefficiency for some machine learning hyperparameter tuning applications. \n\n"}
{"id": "1802.04367", "contents": "Title: Computational Optimal Transport: Complexity by Accelerated Gradient\n  Descent Is Better Than by Sinkhorn's Algorithm Abstract: We analyze two algorithms for approximating the general optimal transport\n(OT) distance between two discrete distributions of size $n$, up to accuracy\n$\\varepsilon$. For the first algorithm, which is based on the celebrated\nSinkhorn's algorithm, we prove the complexity bound\n$\\widetilde{O}\\left({n^2/\\varepsilon^2}\\right)$ arithmetic operations. For the\nsecond one, which is based on our novel Adaptive Primal-Dual Accelerated\nGradient Descent (APDAGD) algorithm, we prove the complexity bound\n$\\widetilde{O}\\left(\\min\\left\\{n^{9/4}/\\varepsilon, n^{2}/\\varepsilon^2\n\\right\\}\\right)$ arithmetic operations. Both bounds have better dependence on\n$\\varepsilon$ than the state-of-the-art result given by\n$\\widetilde{O}\\left({n^2/\\varepsilon^3}\\right)$. Our second algorithm not only\nhas better dependence on $\\varepsilon$ in the complexity bound, but also is not\nspecific to entropic regularization and can solve the OT problem with different\nregularizers. \n\n"}
{"id": "1802.05411", "contents": "Title: Selecting the Best in GANs Family: a Post Selection Inference Framework Abstract: \"Which Generative Adversarial Networks (GANs) generates the most plausible\nimages?\" has been a frequently asked question among researchers. To address\nthis problem, we first propose an \\emph{incomplete} U-statistics estimate of\nmaximum mean discrepancy $\\mathrm{MMD}_{inc}$ to measure the distribution\ndiscrepancy between generated and real images. $\\mathrm{MMD}_{inc}$ enjoys the\nadvantages of asymptotic normality, computation efficiency, and model\nagnosticity. We then propose a GANs analysis framework to select and test the\n\"best\" member in GANs family using the Post Selection Inference (PSI) with\n$\\mathrm{MMD}_{inc}$. In the experiments, we adopt the proposed framework on 7\nGANs variants and compare their $\\mathrm{MMD}_{inc}$ scores. \n\n"}
{"id": "1802.05490", "contents": "Title: Grammar-based Compression of Unranked Trees Abstract: We introduce forest straight-line programs (FSLPs) as a compressed\nrepresentation of unranked ordered node-labelled trees. FSLPs are based on the\noperations of forest algebra and generalize tree straight-line programs. We\ncompare the succinctness of FSLPs with two other compression schemes for\nunranked trees: top dags and tree straight-line programs of first-child/next\nsibling encodings. Efficient translations between these formalisms are\nprovided. Finally, we show that equality of unranked trees in the setting where\ncertain symbols are associative or commutative can be tested in polynomial\ntime. This generalizes previous results for testing isomorphism of compressed\nunordered ranked trees. \n\n"}
{"id": "1802.05843", "contents": "Title: Minimal Algorithmic Information Loss Methods for Dimension Reduction,\n  Feature Selection and Network Sparsification Abstract: We present a novel, domain-agnostic, model-independent, unsupervised, and\nuniversally applicable approach for data summarization. Specifically, we focus\non addressing the challenge of reducing certain dimensionality aspects, such as\nthe number of edges in a network, while retaining essential features of\ninterest. These features include preserving crucial network properties like\ndegree distribution, clustering coefficient, edge betweenness, and degree and\neigenvector centralities. Our approach outperforms state-of-the-art network\nreduction techniques by achieving an average improvement in feature\npreservation. Previous methods grounded in statistics or classical information\ntheory have been limited in their ability to capture more intricate patterns\nand features, particularly nonlinear patterns stemming from deterministic\ncomputable processes. Moreover, these approaches heavily rely on a priori\nfeature selection, demanding constant supervision. Our findings demonstrate the\neffectiveness of the algorithms proposed in this study in overcoming these\nlimitations, all while maintaining a time-efficient computational profile. In\nmany instances, our approach not only matches but also surpasses the\nperformance of established network reduction algorithms. Furthermore, we extend\nthe applicability of our method to lossy compression tasks involving images or\nany bi-dimensional data. This highlights the versatility and broad utility of\nour approach in various domains. \n\n"}
{"id": "1802.05859", "contents": "Title: A Parameterized Strongly Polynomial Algorithm for Block Structured\n  Integer Programs Abstract: The theory of $n$-fold integer programming has been recently emerging as an\nimportant tool in parameterized complexity. The input to an $n$-fold integer\nprogram (IP) consists of parameter $A$, dimension $n$, and numerical data of\nbinary encoding length $L$. It was known for some time that such programs can\nbe solved in polynomial time using $O(n^{g(A)}L)$ arithmetic operations where\n$g$ is an exponential function of the parameter. In 2013 it was shown that it\ncan be solved in fixed-parameter tractable (FPT) time using $O(f(A)n^3L)$\narithmetic operations for a single-exponential function $f$. This, and a faster\nalgorithm for a special case of combinatorial $n$-fold IP, have led to several\nvery recent breakthroughs in the parameterized complexity of scheduling,\nstringology, and computational social choice. In 2015 it was shown that it can\nbe solved in strongly polynomial time using $O(n^{g(A)})$ arithmetic\noperations.\n  Here we establish a result which subsumes all three of the above results by\nshowing that $n$-fold IP can be solved in strongly polynomial FPT time using\n$O(f(A)n^3)$ arithmetic operations. In fact, our results are much more general,\nbriefly outlined as follows.\n  - There is a strongly polynomial algorithm for ILP whenever a so-called\nGraver-best oracle is realizable for it.\n  - Graver-best oracles for the large classes of multi-stage stochastic and\ntree-fold ILPs can be realized in FPT time. Together with the previous oracle\nalgorithm, this newly shows two large classes of ILP to be strongly polynomial;\nin contrast, only few classes of ILP were previously known to be strongly\npolynomial.\n  - We show that ILP is FPT parameterized by the largest coefficient\n$\\|A\\|_\\infty$ and the primal or dual treedepth of $A$, and that this\nparameterization cannot be relaxed, signifying substantial progress in\nunderstanding the parameterized complexity of ILP. \n\n"}
{"id": "1802.05905", "contents": "Title: Assigning times to minimise reachability in temporal graphs Abstract: Temporal graphs (in which edges are active at specified times) are of\nparticular relevance for spreading processes on graphs, e.g.~the spread of\ndisease or dissemination of information. Motivated by real-world applications,\nmodification of static graphs to control this spread has proven a rich topic\nfor previous research. Here, we introduce a new type of modification for\ntemporal graphs: the number of active times for each edge is fixed, but we can\nchange the relative order in which (sets of) edges are active. We investigate\nthe problem of determining an ordering of edges that minimises the maximum\nnumber of vertices reachable from any single starting vertex;\nepidemiologically, this corresponds to the worst-case number of vertices\ninfected in a single disease outbreak. We study two versions of this problem,\nboth of which we show to be $\\NP$-hard, and identify cases in which the problem\ncan be solved or approximated efficiently. \n\n"}
{"id": "1802.06212", "contents": "Title: Multi-Pass Streaming Algorithms for Monotone Submodular Function\n  Maximization Abstract: We consider maximizing a monotone submodular function under a cardinality\nconstraint or a knapsack constraint in the streaming setting. In particular,\nthe elements arrive sequentially and at any point of time, the algorithm has\naccess to only a small fraction of the data stored in primary memory. We\npropose the following streaming algorithms taking $O(\\varepsilon^{-1})$ passes:\n  ----a $(1-e^{-1}-\\varepsilon)$-approximation algorithm for the\ncardinality-constrained problem ---- a $(0.5-\\varepsilon)$-approximation\nalgorithm for the knapsack-constrained problem.\n  Both of our algorithms run in $O^\\ast(n)$ time, using $O^\\ast(K)$ space,\nwhere $n$ is the size of the ground set and $K$ is the size of the knapsack.\nHere the term $O^\\ast$ hides a polynomial of $\\log K$ and $\\varepsilon^{-1}$.\nOur streaming algorithms can also be used as fast approximation algorithms. In\nparticular, for the cardinality-constrained problem, our algorithm takes\n$O(n\\varepsilon^{-1} \\log (\\varepsilon^{-1}\\log K) )$ time, improving on the\nalgorithm of Badanidiyuru and Vondr\\'{a}k that takes $O(n \\varepsilon^{-1} \\log\n(\\varepsilon^{-1} K) )$ time. \n\n"}
{"id": "1802.06289", "contents": "Title: Faster Algorithms for Integer Programs with Block Structure Abstract: We consider integer programming problems $\\max \\{ c^T x : \\mathcal{A} x = b,\nl \\leq x \\leq u, x \\in \\mathbb{Z}^{nt}\\}$ where $\\mathcal{A}$ has a (recursive)\nblock-structure generalizing \"$n$-fold integer programs\" which recently\nreceived considerable attention in the literature. An $n$-fold IP is an integer\nprogram where $\\mathcal{A}$ consists of $n$ repetitions of submatrices $A \\in\n\\mathbb{Z}^{r \\times t}$ on the top horizontal part and $n$ repetitions of a\nmatrix $B \\in \\mathbb{Z}^{s \\times t}$ on the diagonal below the top part.\nInstead of allowing only two types of block matrices, one for the horizontal\nline and one for the diagonal, we generalize the $n$-fold setting to allow for\narbitrary matrices in every block. We show that such an integer program can be\nsolved in time $n^2 t^2 {\\phi} \\cdot (rs{\\Delta})^{\\mathcal{O}(rs^2+ sr^2)}$\n(ignoring logarithmic factors). Here ${\\Delta}$ is an upper bound on the\nlargest absolute value of an entry of $\\mathcal{A}$ and ${\\phi}$ is the largest\nbinary encoding length of a coefficient of $c$. This improves upon the\npreviously best algorithm of Hemmecke, Onn and Romanchuk that runs in time\n$n^3t^3 {\\phi} \\cdot {\\Delta}^{\\mathcal{O}(t^2s)}$. In particular, our\nalgorithm is not exponential in the number $t$ of columns of $A$ and $B$.\n  Our algorithm is based on a new upper bound on the $l_1$-norm of an element\nof the \"Graver basis\" of an integer matrix and on a proximity bound between the\nLP and IP optimal solutions tailored for IPs with block structure. These new\nbounds rely on the \"Steinitz Lemma\".\n  Furthermore, we extend our techniques to the recently introduced \"tree-fold\nIPs\", where we again present a more efficient algorithm in a generalized\nsetting. \n\n"}
{"id": "1802.06440", "contents": "Title: Capacitated Dynamic Programming: Faster Knapsack and Graph Algorithms Abstract: One of the most fundamental problems in Computer Science is the Knapsack\nproblem. Given a set of n items with different weights and values, it asks to\npick the most valuable subset whose total weight is below a capacity threshold\nT. Despite its wide applicability in various areas in Computer Science,\nOperations Research, and Finance, the best known running time for the problem\nis O(Tn). The main result of our work is an improved algorithm running in time\nO(TD), where D is the number of distinct weights. Previously, faster runtimes\nfor Knapsack were only possible when both weights and values are bounded by M\nand V respectively, running in time O(nMV) [Pisinger'99]. In comparison, our\nalgorithm implies a bound of O(nM^2) without any dependence on V, or O(nV^2)\nwithout any dependence on M. Additionally, for the unbounded Knapsack problem,\nwe provide an algorithm running in time O(M^2) or O(V^2). Both our algorithms\nmatch recent conditional lower bounds shown for the Knapsack problem [Cygan et\nal'17, K\\\"unnemann et al'17].\n  We also initiate a systematic study of general capacitated dynamic\nprogramming, of which Knapsack is a core problem. This problem asks to compute\nthe maximum weight path of length k in an edge- or node-weighted directed\nacyclic graph. In a graph with m edges, these problems are solvable by dynamic\nprogramming in time O(km), and we explore under which conditions the dependence\non k can be eliminated. We identify large classes of graphs where this is\npossible and apply our results to obtain linear time algorithms for the problem\nof k-sparse Delta-separated sequences. The main technical innovation behind our\nresults is identifying and exploiting concavity that appears in relaxations and\nsubproblems of the tasks we consider. \n\n"}
{"id": "1802.06686", "contents": "Title: On Local Distributed Sampling and Counting Abstract: In classic distributed graph problems, each instance on a graph specifies a\nspace of feasible solutions (e.g. all proper ($\\Delta+1$)-list-colorings of the\ngraph), and the task of distributed algorithm is to construct a feasible\nsolution using local information.\n  We study distributed sampling and counting problems, in which each instance\nspecifies a joint distribution of feasible solutions. The task of distributed\nalgorithm is to sample from this joint distribution, or to locally measure the\nvolume of the probability space via the marginal probabilities. The latter task\nis also known as inference, which is a local counterpart of counting.\n  For self-reducible classes of instances, the following equivalences are\nestablished in the LOCAL model up to polylogarithmic factors:\n  $\\bullet$ For all joint distributions, approximate inference and approximate\nsampling are computationally equivalent.\n  $\\bullet$ For all joint distributions defined by local constraints, exact\nsampling is reducible to either one of the above tasks.\n  $\\bullet$ If further, sequentially constructing a feasible solution is\ntrivial locally, then all above tasks are easy if and only if the joint\ndistribution exhibits strong spatial mixing.\n  Combining with the state of the arts of strong spatial mixing, we obtain\nefficient sampling algorithms in the LOCAL model for various important sampling\nproblems, including: an $O(\\sqrt{\\Delta}\\log^3n)$-round algorithm for exact\nsampling matchings in graphs with maximum degree $\\Delta$, and an\n$O(\\log^3n)$-round algorithm for sampling according to the hardcore model\n(weighted independent sets) in the uniqueness regime, which along with the\n$\\Omega(\\mathrm{diam})$ lower bound in arXiv:1702.00142 for sampling according\nto the hardcore model in the non-uniqueness regime, gives the first\ncomputational phase transition for distributed sampling. \n\n"}
{"id": "1802.06905", "contents": "Title: Communication-Optimal Convolutional Neural Nets Abstract: Efficiently executing convolutional neural nets (CNNs) is important in many\nmachine-learning tasks. Since the cost of moving a word of data, either between\nlevels of a memory hierarchy or between processors over a network, is much\nhigher than the cost of an arithmetic operation, minimizing data movement is\ncritical to performance optimization. In this paper, we present both new lower\nbounds on data movement needed for CNNs, and optimal sequential algorithms that\nattain these lower bounds. In most common cases, our optimal algorithms can\nattain significantly more data reuse than matrix multiplication. \n\n"}
{"id": "1802.06992", "contents": "Title: Sublinear Algorithms for MAXCUT and Correlation Clustering Abstract: We study sublinear algorithms for two fundamental graph problems, MAXCUT and\ncorrelation clustering. Our focus is on constructing core-sets as well as\ndeveloping streaming algorithms for these problems. Constant space algorithms\nare known for dense graphs for these problems, while $\\Omega(n)$ lower bounds\nexist (in the streaming setting) for sparse graphs.\n  Our goal in this paper is to bridge the gap between these extremes. Our first\nresult is to construct core-sets of size $\\tilde{O}(n^{1-\\delta})$ for both the\nproblems, on graphs with average degree $n^{\\delta}$ (for any $\\delta >0$).\nThis turns out to be optimal, under the exponential time hypothesis (ETH). Our\ncore-set analysis is based on studying random-induced sub-problems of\noptimization problems. To the best of our knowledge, all the known results in\nour parameter range rely crucially on near-regularity assumptions. We avoid\nthese by using a biased sampling approach, which we analyze using recent\nresults on concentration of quadratic functions. We then show that our\nconstruction yields a 2-pass streaming $(1+\\epsilon)$-approximation for both\nproblems; the algorithm uses $\\tilde{O}(n^{1-\\delta})$ space, for graphs of\naverage degree $n^\\delta$. \n\n"}
{"id": "1802.07244", "contents": "Title: Steering Social Activity: A Stochastic Optimal Control Point Of View Abstract: User engagement in online social networking depends critically on the level\nof social activity in the corresponding platform--the number of online actions,\nsuch as posts, shares or replies, taken by their users. Can we design\ndata-driven algorithms to increase social activity? At a user level, such\nalgorithms may increase activity by helping users decide when to take an action\nto be more likely to be noticed by their peers. At a network level, they may\nincrease activity by incentivizing a few influential users to take more\nactions, which in turn will trigger additional actions by other users. In this\npaper, we model social activity using the framework of marked temporal point\nprocesses, derive an alternate representation of these processes using\nstochastic differential equations (SDEs) with jumps and, exploiting this\nalternate representation, develop two efficient online algorithms with provable\nguarantees to steer social activity both at a user and at a network level. In\ndoing so, we establish a previously unexplored connection between optimal\ncontrol of jump SDEs and doubly stochastic marked temporal point processes,\nwhich is of independent interest. Finally, we experiment both with synthetic\nand real data gathered from Twitter and show that our algorithms consistently\nsteer social activity more effectively than the state of the art. \n\n"}
{"id": "1802.07401", "contents": "Title: A Study into the similarity in generator and discriminator in GAN\n  architecture Abstract: One popular generative model that has high-quality results is the Generative\nAdversarial Networks(GAN). This type of architecture consists of two separate\nnetworks that play against each other. The generator creates an output from the\ninput noise that is given to it. The discriminator has the task of determining\nif the input to it is real or fake. This takes place constantly eventually\nleads to the generator modeling the target distribution. This paper includes a\nstudy into the actual weights learned by the network and a study into the\nsimilarity of the discriminator and generator networks. The paper also tries to\nleverage the similarity between these networks and shows that indeed both the\nnetworks may have a similar structure with experimental evidence with a novel\nshared architecture. \n\n"}
{"id": "1802.08183", "contents": "Title: Projection-Free Online Optimization with Stochastic Gradient: From\n  Convexity to Submodularity Abstract: Online optimization has been a successful framework for solving large-scale\nproblems under computational constraints and partial information. Current\nmethods for online convex optimization require either a projection or exact\ngradient computation at each step, both of which can be prohibitively expensive\nfor large-scale applications. At the same time, there is a growing trend of\nnon-convex optimization in machine learning community and a need for online\nmethods. Continuous DR-submodular functions, which exhibit a natural\ndiminishing returns condition, have recently been proposed as a broad class of\nnon-convex functions which may be efficiently optimized. Although online\nmethods have been introduced, they suffer from similar problems. In this work,\nwe propose Meta-Frank-Wolfe, the first online projection-free algorithm that\nuses stochastic gradient estimates. The algorithm relies on a careful sampling\nof gradients in each round and achieves the optimal $O( \\sqrt{T})$ adversarial\nregret bounds for convex and continuous submodular optimization. We also\npropose One-Shot Frank-Wolfe, a simpler algorithm which requires only a single\nstochastic gradient estimate in each round and achieves an $O(T^{2/3})$\nstochastic regret bound for convex and continuous submodular optimization. We\napply our methods to develop a novel \"lifting\" framework for the online\ndiscrete submodular maximization and also see that they outperform current\nstate-of-the-art techniques on various experiments. \n\n"}
{"id": "1802.08252", "contents": "Title: The iisignature library: efficient calculation of iterated-integral\n  signatures and log signatures Abstract: Iterated-integral signatures and log signatures are vectors calculated from a\npath that characterise its shape. They come from the theory of differential\nequations driven by rough paths, and also have applications in statistics and\nmachine learning. We present algorithms for efficiently calculating these\nsignatures, and benchmark their performance. We release the methods as a Python\npackage. \n\n"}
{"id": "1802.08563", "contents": "Title: The Parameterized Hardness of the k-Center Problem in Transportation\n  Networks Abstract: In this paper we study the hardness of the $k$-Center problem on inputs that\nmodel transportation networks. For the problem, a graph $G=(V,E)$ with edge\nlengths and an integer $k$ are given and a center set $C\\subseteq V$ needs to\nbe chosen such that $|C|\\leq k$. The aim is to minimize the maximum distance of\nany vertex in the graph to the closest center. This problem arises in many\napplications of logistics, and thus it is natural to consider inputs that model\ntransportation networks. Such inputs are often assumed to be planar graphs, low\ndoubling metrics, or bounded highway dimension graphs. For each of these\nmodels, parameterized approximation algorithms have been shown to exist. We\ncomplement these results by proving that the $k$-Center problem is W[1]-hard on\nplanar graphs of constant doubling dimension, where the parameter is the\ncombination of the number of centers $k$, the highway dimension $h$, and the\npathwidth $p$. Moreover, under the Exponential Time Hypothesis there is no\n$f(k,p,h)\\cdot n^{o(p+\\sqrt{k+h})}$ time algorithm for any computable function\n$f$. Thus it is unlikely that the optimum solution to $k$-Center can be found\nefficiently, even when assuming that the input graph abides to all of the above\nmodels for transportation networks at once!\n  Additionally we give a simple parameterized $(1+\\varepsilon)$-approximation\nalgorithm for inputs of doubling dimension $d$ with runtime\n$(k^k/\\varepsilon^{O(kd)})\\cdot n^{O(1)}$. This generalizes a previous result,\nwhich considered inputs in $D$-dimensional $L_q$ metrics. \n\n"}
{"id": "1802.08663", "contents": "Title: Synchronization Strings: List Decoding for Insertions and Deletions Abstract: We study codes that are list-decodable under insertions and deletions.\nSpecifically, we consider the setting where a codeword over some finite\nalphabet of size $q$ may suffer from $\\delta$ fraction of adversarial deletions\nand $\\gamma$ fraction of adversarial insertions. A code is said to be\n$L$-list-decodable if there is an (efficient) algorithm that, given a received\nword, reports a list of $L$ codewords that include the original codeword.\n  Using the concept of synchronization strings, introduced by the first two\nauthors [STOC 2017], we show some surprising results. We show that for every\n$0\\leq\\delta<1$, every $0\\leq\\gamma<\\infty$ and every $\\epsilon>0$ there exist\nefficient codes of rate $1-\\delta-\\epsilon$ and constant alphabet (so\n$q=O_{\\delta,\\gamma,\\epsilon}(1)$) and sub-logarithmic list sizes. We stress\nthat the fraction of insertions can be arbitrarily large and the rate is\nindependent of this parameter. Our result sheds light on the remarkable\nasymmetry between the impact of insertions and deletions from the point of view\nof error-correction: Whereas deletions cost in the rate of the code, insertion\ncosts are borne by the adversary and not the code!\n  We also prove several tight bounds on the parameters of list-decodable insdel\ncodes. In particular, we show that the alphabet size of insdel codes needs to\nbe exponentially large in $\\epsilon^{-1}$, where $\\epsilon$ is the gap to\ncapacity above. Our result even applies to settings where the unique-decoding\ncapacity equals the list-decoding capacity and when it does so, it shows that\nthe alphabet size needs to be exponentially large in the gap to capacity. This\nis sharp contrast to the Hamming error model where alphabet size polynomial in\n$\\epsilon^{-1}$ suffices for unique decoding and also shows that the\nexponential dependence on the alphabet size in previous works that constructed\ninsdel codes is actually necessary! \n\n"}
{"id": "1802.09007", "contents": "Title: Evaluating and Tuning n-fold Integer Programming Abstract: In recent years, algorithmic breakthroughs in stringology, computational\nsocial choice, scheduling, etc., were achieved by applying the theory of\nso-called $n$-fold integer programming. An $n$-fold integer program (IP) has a\nhighly uniform block structured constraint matrix. Hemmecke, Onn, and Romanchuk\n[Math. Programming, 2013] showed an algorithm with runtime $a^{O(rst + r^2s)}\nn^3$, where $a$ is the largest coefficient, $r,s$, and $t$ are dimensions of\nblocks of the constraint matrix and $n$ is the total dimension of the IP; thus,\nan algorithm efficient if the blocks are of small size and with small\ncoefficients. The algorithm works by iteratively improving a feasible solution\nwith augmenting steps, and $n$-fold IPs have the special property that\naugmenting steps are guaranteed to exist in a not-too-large neighborhood.\n  We have implemented the algorithm and learned the following along the way.\nThe original algorithm is practically unusable, but we discover a series of\nimprovements which make its evaluation possible. Crucially, we observe that a\ncertain constant in the algorithm can be treated as a tuning parameter, which\nyields an efficient heuristic (essentially searching in a\nsmaller-than-guaranteed neighborhood). Furthermore, the algorithm uses an\noverly expensive strategy to find a \"best\" step, while finding only an\n\"approximatelly best\" step is much cheaper, yet sufficient for quick\nconvergence. Using this insight, we improve the asymptotic dependence on $n$\nfrom $n^3$ to $n^2 \\log n$.\n  We show that decreasing the tuning parameter initially leads to an increased\nnumber of iterations needed for convergence and eventually to getting stuck in\nlocal optima, as expected. However, surprisingly small values of the parameter\nalready exhibit good behavior. Second, our new strategy for finding\n\"approximatelly best\" steps wildly outperforms the original construction. \n\n"}
{"id": "1802.10172", "contents": "Title: Semi-Supervised Learning Enabled by Multiscale Deep Neural Network\n  Inversion Abstract: Deep Neural Networks (DNNs) provide state-of-the-art solutions in several\ndifficult machine perceptual tasks. However, their performance relies on the\navailability of a large set of labeled training data, which limits the breadth\nof their applicability. Hence, there is a need for new {\\em semi-supervised\nlearning} methods for DNNs that can leverage both (a small amount of) labeled\nand unlabeled training data. In this paper, we develop a general loss function\nenabling DNNs of any topology to be trained in a semi-supervised manner without\nextra hyper-parameters. As opposed to current semi-supervised techniques based\non topology-specific or unstable approaches, ours is both robust and general.\nWe demonstrate that our approach reaches state-of-the-art performance on the\nSVHN ($9.82\\%$ test error, with $500$ labels and wide Resnet) and CIFAR10\n(16.38% test error, with 8000 labels and sigmoid convolutional neural network)\ndata sets. \n\n"}
{"id": "1803.00606", "contents": "Title: On Oracle-Efficient PAC RL with Rich Observations Abstract: We study the computational tractability of PAC reinforcement learning with\nrich observations. We present new provably sample-efficient algorithms for\nenvironments with deterministic hidden state dynamics and stochastic rich\nobservations. These methods operate in an oracle model of computation --\naccessing policy and value function classes exclusively through standard\noptimization primitives -- and therefore represent computationally efficient\nalternatives to prior algorithms that require enumeration. With stochastic\nhidden state dynamics, we prove that the only known sample-efficient algorithm,\nOLIVE, cannot be implemented in the oracle model. We also present several\nexamples that illustrate fundamental challenges of tractable PAC reinforcement\nlearning in such general settings. \n\n"}
{"id": "1803.00904", "contents": "Title: Hardness of Approximate Nearest Neighbor Search Abstract: We prove conditional near-quadratic running time lower bounds for approximate\nBichromatic Closest Pair with Euclidean, Manhattan, Hamming, or edit distance.\nSpecifically, unless the Strong Exponential Time Hypothesis (SETH) is false,\nfor every $\\delta>0$ there exists a constant $\\epsilon>0$ such that computing a\n$(1+\\epsilon)$-approximation to the Bichromatic Closest Pair requires\n$n^{2-\\delta}$ time. In particular, this implies a near-linear query time for\nApproximate Nearest Neighbor search with polynomial preprocessing time.\n  Our reduction uses the Distributed PCP framework of [ARW'17], but obtains\nimproved efficiency using Algebraic Geometry (AG) codes. Efficient PCPs from AG\ncodes have been constructed in other settings before [BKKMS'16, BCGRS'17], but\nour construction is the first to yield new hardness results. \n\n"}
{"id": "1803.02750", "contents": "Title: Efficient Synchronization of State-based CRDTs Abstract: To ensure high availability in large scale distributed systems, Conflict-free\nReplicated Data Types (CRDTs) relax consistency by allowing immediate query and\nupdate operations at the local replica, with no need for remote\nsynchronization. State-based CRDTs synchronize replicas by periodically sending\ntheir full state to other replicas, which can become extremely costly as the\nCRDT state grows. Delta-based CRDTs address this problem by producing small\nincremental states (deltas) to be used in synchronization instead of the full\nstate. However, current synchronisation algorithms for Delta-based CRDTs induce\nredundant wasteful delta propagation, performing worse than expected, and\nsurprisingly, no better than State-based. In this paper we: 1) identify two\nsources of inefficiency in current synchronization algorithms for delta-based\nCRDTs; 2) bring the concept of join decomposition to state-based CRDTs; 3)\nexploit join decompositions to obtain optimal deltas and 4) improve the\nefficiency of synchronization algorithms; and finally, 5) evaluate the improved\nalgorithms. \n\n"}
{"id": "1803.03376", "contents": "Title: Learning Approximate Inference Networks for Structured Prediction Abstract: Structured prediction energy networks (SPENs; Belanger & McCallum 2016) use\nneural network architectures to define energy functions that can capture\narbitrary dependencies among parts of structured outputs. Prior work used\ngradient descent for inference, relaxing the structured output to a set of\ncontinuous variables and then optimizing the energy with respect to them. We\nreplace this use of gradient descent with a neural network trained to\napproximate structured argmax inference. This \"inference network\" outputs\ncontinuous values that we treat as the output structure. We develop\nlarge-margin training criteria for joint training of the structured energy\nfunction and inference network. On multi-label classification we report\nspeed-ups of 10-60x compared to (Belanger et al, 2017) while also improving\naccuracy. For sequence labeling with simple structured energies, our approach\nperforms comparably to exact inference while being much faster at test time. We\nthen demonstrate improved accuracy by augmenting the energy with a \"label\nlanguage model\" that scores entire output label sequences, showing it can\nimprove handling of long-distance dependencies in part-of-speech tagging.\nFinally, we show how inference networks can replace dynamic programming for\ntest-time inference in conditional random fields, suggestive for their general\nuse for fast inference in structured settings. \n\n"}
{"id": "1803.04051", "contents": "Title: Representation Learning over Dynamic Graphs Abstract: How can we effectively encode evolving information over dynamic graphs into\nlow-dimensional representations? In this paper, we propose DyRep, an inductive\ndeep representation learning framework that learns a set of functions to\nefficiently produce low-dimensional node embeddings that evolves over time. The\nlearned embeddings drive the dynamics of two key processes namely,\ncommunication and association between nodes in dynamic graphs. These processes\nexhibit complex nonlinear dynamics that evolve at different time scales and\nsubsequently contribute to the update of node embeddings. We employ a\ntime-scale dependent multivariate point process model to capture these\ndynamics. We devise an efficient unsupervised learning procedure and\ndemonstrate that our approach significantly outperforms representative\nbaselines on two real-world datasets for the problem of dynamic link prediction\nand event time prediction. \n\n"}
{"id": "1803.04282", "contents": "Title: Linear-Time In-Place DFS and BFS on the Word RAM Abstract: We present an in-place depth first search (DFS) and an in-place breadth first\nsearch (BFS) that runs on a word RAM in linear time such that, if the adjacency\narrays of the input graph are given in a sorted order, the input is restored\nafter running the algorithm. To obtain our results we use properties of the\nrepresentation used to store the given graph and show several linear-time\nin-place graph transformations from one representation into another. \n\n"}
{"id": "1803.04386", "contents": "Title: Flipout: Efficient Pseudo-Independent Weight Perturbations on\n  Mini-Batches Abstract: Stochastic neural net weights are used in a variety of contexts, including\nregularization, Bayesian neural nets, exploration in reinforcement learning,\nand evolution strategies. Unfortunately, due to the large number of weights,\nall the examples in a mini-batch typically share the same weight perturbation,\nthereby limiting the variance reduction effect of large mini-batches. We\nintroduce flipout, an efficient method for decorrelating the gradients within a\nmini-batch by implicitly sampling pseudo-independent weight perturbations for\neach example. Empirically, flipout achieves the ideal linear variance reduction\nfor fully connected networks, convolutional networks, and RNNs. We find\nsignificant speedups in training neural networks with multiplicative Gaussian\nperturbations. We show that flipout is effective at regularizing LSTMs, and\noutperforms previous methods. Flipout also enables us to vectorize evolution\nstrategies: in our experiments, a single GPU with flipout can handle the same\nthroughput as at least 40 CPU cores using existing methods, equivalent to a\nfactor-of-4 cost reduction on Amazon Web Services. \n\n"}
{"id": "1803.04940", "contents": "Title: Nearly Optimal Time Bounds for kPath in Hypergraphs Abstract: We give almost tight conditional lower bounds on the running time of the\nkHyperPath problem. Given an $r$-uniform hypergraph for some integer $r$,\nkHyperPath seeks a tight path of length $k$. That is, a sequence of $k$ nodes\nsuch that every consecutive $r$ of them constitute a hyperedge in the graph.\nThis problem is a natural generalization of the extensively-studied kPath\nproblem in graphs. We show that solving kHyperPath in time\n$O^*(2^{(1-\\gamma)k})$ where $\\gamma>0$ is independent of $r$ is probably\nimpossible. Specifically, it implies that Set Cover on $n$ elements can be\nsolved in time $O^*(2^{(1 - \\delta)n})$ for some $\\delta>0$. The only known\nlower bound for the kPath problem is $2^{\\Omega(k)} poly(n)$ where $n$ is the\nnumber of nodes assuming the Exponential Time Hypothesis (ETH), and finding any\nconditional lower bound with an explicit constant in the exponent has been an\nimportant open problem.\n  We complement our lower bound with an almost tight upper bound. Formally, for\nevery integer $r\\geq 3$ we give algorithms that solve kHyperPath and\nkHyperCycle on $r$-uniform hypergraphs with $n$ nodes and $m$ edges in time\n$2^k m \\cdot poly(n)$ and $2^k m^2 poly(n)$ respectively, and that is even for\nthe directed version of these problems. To the best of our knowledge, this is\nthe first algorithm for kHyperPath. The fastest algorithms known for kPath run\nin time $2^k poly(n)$ for directed graphs (Williams, 2009), and in time $1.66^k\npoly(n)$ for undirected graphs (Bj\\\"orklund \\etal, 2014). \n\n"}
{"id": "1803.05340", "contents": "Title: Measurement-based adaptation protocol with quantum reinforcement\n  learning Abstract: Machine learning employs dynamical algorithms that mimic the human capacity\nto learn, where the reinforcement learning ones are among the most similar to\nhumans in this respect. On the other hand, adaptability is an essential aspect\nto perform any task efficiently in a changing environment, and it is\nfundamental for many purposes, such as natural selection. Here, we propose an\nalgorithm based on successive measurements to adapt one quantum state to a\nreference unknown state, in the sense of achieving maximum overlap. The\nprotocol naturally provides many identical copies of the reference state, such\nthat in each measurement iteration more information about it is obtained. In\nour protocol, we consider a system composed of three parts, the \"environment\"\nsystem, which provides the reference state copies; the register, which is an\nauxiliary subsystem that interacts with the environment to acquire information\nfrom it; and the agent, which corresponds to the quantum state that is adapted\nby digital feedback with input corresponding to the outcome of the measurements\non the register. With this proposal we can achieve an average fidelity between\nthe environment and the agent of more than $90\\% $ with less than $30$\niterations of the protocol. In addition, we extend the formalism to $ d\n$-dimensional states, reaching an average fidelity of around $80\\% $ in less\nthan $400$ iterations for $d=$ 11, for a variety of genuinely quantum and\nsemiclassical states. This work paves the way for the development of quantum\nreinforcement learning protocols using quantum data and for the future\ndeployment of semi-autonomous quantum systems. \n\n"}
{"id": "1803.05825", "contents": "Title: A Generalized Matching Reconfiguration Problem Abstract: The goal in {\\em reconfiguration problems} is to compute a {\\em gradual\ntransformation} between two feasible solutions of a problem such that all\nintermediate solutions are also feasible. In the {\\em Matching Reconfiguration\nProblem} (MRP), proposed in a pioneering work by Ito et al.\\ from 2008, we are\ngiven a graph $G$ and two matchings $M$ and $M'$, and we are asked whether\nthere is a sequence of matchings in $G$ starting with $M$ and ending at $M'$,\neach resulting from the previous one by either adding or deleting a single edge\nin $G$, without ever going through a matching of size $< \\min\\{|M|,|M'|\\}-1$.\nIto et al.\\ gave a polynomial time algorithm for the problem.\n  In this paper we introduce a natural generalization of the MRP that depends\non an integer parameter $\\Delta \\ge 1$: here we are allowed to make $\\Delta$\nchanges to the current solution rather than 1 at each step of the\n{transformation procedure}. There is always a valid sequence of matchings\ntransforming $M$ to $M'$ if $\\Delta$ is sufficiently large, and naturally we\nwould like to minimize $\\Delta$. We first devise an optimal transformation\nprocedure for unweighted matching with $\\Delta = 3$, and then extend it to\nweighted matchings to achieve asymptotically optimal guarantees. The running\ntime of these procedures is linear.\n  We further demonstrate the applicability of this generalized problem to\ndynamic graph matchings. In this area, the number of changes to the maintained\nmatching per update step (the \\emph{recourse bound}) is an important quality\nmeasure. Nevertheless, the \\emph{worst-case} recourse bounds of almost all\nknown dynamic matching algorithms are prohibitively large, much larger than the\ncorresponding update times. We fill in this gap via a surprisingly simple\nblack-box reduction: Any dynamic algorithm for maintaining [...] \n\n"}
{"id": "1803.06521", "contents": "Title: Beyond the Low-Degree Algorithm: Mixtures of Subcubes and Their\n  Applications Abstract: We introduce the problem of learning mixtures of $k$ subcubes over\n$\\{0,1\\}^n$, which contains many classic learning theory problems as a special\ncase (and is itself a special case of others). We give a surprising $n^{O(\\log\nk)}$-time learning algorithm based on higher-order multilinear moments. It is\nnot possible to learn the parameters because the same distribution can be\nrepresented by quite different models. Instead, we develop a framework for\nreasoning about how multilinear moments can pinpoint essential features of the\nmixture, like the number of components.\n  We also give applications of our algorithm to learning decision trees with\nstochastic transitions (which also capture interesting scenarios where the\ntransitions are deterministic but there are latent variables). Using our\nalgorithm for learning mixtures of subcubes, we can approximate the Bayes\noptimal classifier within additive error $\\epsilon$ on $k$-leaf decision trees\nwith at most $s$ stochastic transitions on any root-to-leaf path in $n^{O(s +\n\\log k)}\\cdot\\text{poly}(1/\\epsilon)$ time. In this stochastic setting, the\nclassic Occam algorithms for learning decision trees with zero stochastic\ntransitions break down, while the low-degree algorithm of Linial et al.\ninherently has a quasipolynomial dependence on $1/\\epsilon$.\n  In contrast, as we will show, mixtures of $k$ subcubes are uniquely\ndetermined by their degree $2 \\log k$ moments and hence provide a useful\nabstraction for simultaneously achieving the polynomial dependence on\n$1/\\epsilon$ of the classic Occam algorithms for decision trees and the\nflexibility of the low-degree algorithm in being able to accommodate stochastic\ntransitions. Using our multilinear moment techniques, we also give the first\nimproved upper and lower bounds since the work of Feldman et al. for the\nrelated but harder problem of learning mixtures of binary product\ndistributions. \n\n"}
{"id": "1803.06878", "contents": "Title: Parameterized Complexity of Fair Vertex Evaluation Problems Abstract: A prototypical graph problem is centered around a graph-theoretic property\nfor a set of vertices and a solution to it is a set of vertices for which the\ndesired property holds. The task is to decide whether, in the given graph,\nthere exists a solution of a certain quality, where we use size as a quality\nmeasure. In this work, we are changing the measure to the fair measure\n[Lin&Sahni: Fair edge deletion problems. IEEE Trans. Comput. 89]. The measure\nis k if the number of solution neighbors does not exceed k for any vertex in\nthe graph. One possible way to study graph problems is by defining the property\nin a certain logic. For a given objective an evaluation problem is to find a\nset (of vertices) that simultaneously minimizes the assumed measure and\nsatisfies an appropriate formula.\n  In the presented paper we show that there is an FPT algorithm for the MSO\nFair Vertex Evaluation problem for formulas with one free variable\nparameterized by the twin cover number of the input graph. Here, the free\nvariable corresponds to the solution sought. One may define an extended variant\nof MSO Fair Vertex Evaluation for formulas with l free variables; here we\nmeasure a maximum number of neighbors in each of the l sets. However, such\nvariant is W[1]-hard for parameter l even on graphs with twin cover one.\nFurthermore, we study the Fair Vertex Cover (Fair VC) problem. Fair VC is among\nthe simplest problems with respect to the demanded property (i.e., the rest\nforms an edgeless graph). On the negative side, Fair VC is W[1]-hard when\nparameterized by both treedepth and feedback vertex set of the input graph. On\nthe positive side, we provide an FPT algorithm for the parameter modular width. \n\n"}
{"id": "1803.06978", "contents": "Title: Improving Transferability of Adversarial Examples with Input Diversity Abstract: Though CNNs have achieved the state-of-the-art performance on various vision\ntasks, they are vulnerable to adversarial examples --- crafted by adding\nhuman-imperceptible perturbations to clean images. However, most of the\nexisting adversarial attacks only achieve relatively low success rates under\nthe challenging black-box setting, where the attackers have no knowledge of the\nmodel structure and parameters. To this end, we propose to improve the\ntransferability of adversarial examples by creating diverse input patterns.\nInstead of only using the original images to generate adversarial examples, our\nmethod applies random transformations to the input images at each iteration.\nExtensive experiments on ImageNet show that the proposed attack method can\ngenerate adversarial examples that transfer much better to different networks\nthan existing baselines. By evaluating our method against top defense solutions\nand official baselines from NIPS 2017 adversarial competition, the enhanced\nattack reaches an average success rate of 73.0%, which outperforms the top-1\nattack submission in the NIPS competition by a large margin of 6.6%. We hope\nthat our proposed attack strategy can serve as a strong benchmark baseline for\nevaluating the robustness of networks to adversaries and the effectiveness of\ndifferent defense methods in the future. Code is available at\nhttps://github.com/cihangxie/DI-2-FGSM. \n\n"}
{"id": "1803.07465", "contents": "Title: A modification of the CSP algorithm for infinite languages Abstract: Constraint Satisfaction Problem on finite sets is known to be NP-complete in\ngeneral but certain restrictions on the constraint language can ensure\ntractability. It was proved that if a constraint language has a weak near\nunanimity polymorphism then the corresponding constraint satisfaction problem\nis tractable, otherwise it is NP-complete. In the paper we present a\nmodification of the algorithm that works in polynomial time even for infinite\nconstraint languages. \n\n"}
{"id": "1803.09947", "contents": "Title: Periodic Fourier representation of Boolean functions Abstract: In this work, we consider a new type of Fourier-like representation of\nBoolean function $f\\colon\\{+1,-1\\}^n\\to\\{+1,-1\\}$ \\[ f(x) =\n\\cos\\left(\\pi\\sum_{S\\subseteq[n]}\\phi_S \\prod_{i\\in S} x_i\\right). \\] This\nrepresentation, which we call the periodic Fourier representation, of Boolean\nfunction is closely related to a certain type of multipartite Bell inequalities\nand non-adaptive measurement-based quantum computation with linear\nside-processing ($\\mathrm{NMQC}_\\oplus$). The minimum number of non-zero\ncoefficients in the above representation, which we call the periodic Fourier\nsparsity, is equal to the required number of qubits for the exact computation\nof $f$ by $\\mathrm{NMQC}_\\oplus$. Periodic Fourier representations are not\nunique, and can be directly obtained both from the Fourier representation and\nthe $\\mathbb{F}_2$-polynomial representation. In this work, we first show that\nBoolean functions related to $\\mathbb{Z}/4\\mathbb{Z}$-polynomial have small\nperiodic Fourier sparsities. Second, we show that the periodic Fourier sparsity\nis at least $2^{\\mathrm{deg}_{\\mathbb{F}_2}(f)}-1$, which means that\n$\\mathrm{NMQC}_\\oplus$ efficiently computes a Boolean function $f$ if and only\nif $\\mathbb{F}_2$-degree of $f$ is small. Furthermore, we show that any\nsymmetric Boolean function, e.g., $\\mathsf{AND}_n$, $\\mathsf{Mod}^3_n$,\n$\\mathsf{Maj}_n$, etc, can be exactly computed by depth-2\n$\\mathrm{NMQC}_\\oplus$ using a polynomial number of qubits, that implies\nexponential gaps between $\\mathrm{NMQC}_\\oplus$ and depth-2\n$\\mathrm{NMQC}_\\oplus$. \n\n"}
{"id": "1803.10846", "contents": "Title: Non-Convex Matrix Completion Against a Semi-Random Adversary Abstract: Matrix completion is a well-studied problem with many machine learning\napplications. In practice, the problem is often solved by non-convex\noptimization algorithms. However, the current theoretical analysis for\nnon-convex algorithms relies heavily on the assumption that every entry is\nobserved with exactly the same probability $p$, which is not realistic in\npractice.\n  In this paper, we investigate a more realistic semi-random model, where the\nprobability of observing each entry is at least $p$. Even with this mild\nsemi-random perturbation, we can construct counter-examples where existing\nnon-convex algorithms get stuck in bad local optima.\n  In light of the negative results, we propose a pre-processing step that tries\nto re-weight the semi-random input, so that it becomes \"similar\" to a random\ninput. We give a nearly-linear time algorithm for this problem, and show that\nafter our pre-processing, all the local minima of the non-convex objective can\nbe used to approximately recover the underlying ground-truth matrix. \n\n"}
{"id": "1804.00141", "contents": "Title: The Popular Roommates problem Abstract: We consider the popular matching problem in a roommates instance with strict\npreference lists. While popular matchings always exist in a bipartite instance,\nthey need not exist in a roommates instance. The complexity of the popular\nmatching problem in a roommates instance has been an open problem for several\nyears and here we show it is NP-hard. A sub-class of max-size popular matchings\ncalled dominant matchings has been well-studied in bipartite graphs. We show\nthat the dominant matching problem in a roommates instance is also NP-hard and\nthis is the case even when the instance admits a stable matching. \n\n"}
{"id": "1804.01567", "contents": "Title: On-line Chain Partitioning Approach to Scheduling Abstract: An on-line chain partitioning algorithm receives the points of the poset from\nsome externally determined list. Being presented with a new point the algorithm\nlearns the comparability status of this new point to all previously presented\nones. As each point is received, the algorithm assigns this new point to a\nchain in an irrevocable manner and this assignment is made without knowledge of\nfuture points. Kierstead presented an algorithm using $(5^w-1)/4$ chains to\ncover each poset of width $w$. Felsner proved that width $2$ posets can be\npartitioned on-line into $5$ chains. We present an algorithm using $16$ chains\non posets of width $3$. This result significantly narrows down the previous\nbound of $31$. Moreover, we address the on-line chain partitioning problem for\ninterval orders. Kierstead and Trotter presented an algorithm using $3w-2$\nchains. We deal with an up-growing version of an on-line chain partition of\ninterval orders, i.e. we restrict possible inputs by the rule that each new\npoint is maximal at the moment of its arrival. We present an algorithm using\n$2w-1$ chains and show that there is no better one. These problems come from a\nneed for better algorithms that can be applied to scheduling. Each on-line\nchain partitioning algorithm schedules tasks in a multiprocessor environment,\nand therefore can be applied in order to minimize number of processors. \n\n"}
{"id": "1804.01710", "contents": "Title: Submodular Functions and Valued Constraint Satisfaction Problems over\n  Infinite Domains Abstract: Valued constraint satisfaction problems (VCSPs) are a large class of\ncombinatorial optimisation problems. It is desirable to classify the\ncomputational complexity of VCSPs depending on a fixed set of allowed cost\nfunctions in the input. Recently, the computational complexity of all VCSPs for\nfinite sets of cost functions over finite domains has been classified in this\nsense. Many natural optimisation problems, however, cannot be formulated as\nVCSPs over a finite domain. We initiate the systematic investigation of\ninfinite-domain VCSPs by studying the complexity of VCSPs for piecewise linear\nhomogeneous cost functions. We show that such VCSPs can be solved in polynomial\ntime when the cost functions are additionally submodular, and that this is\nindeed a maximally tractable class: adding any cost function that is not\nsubmodular leads to an NP-hard VCSP. \n\n"}
{"id": "1804.02246", "contents": "Title: Adaptive Cost-sensitive Online Classification Abstract: Cost-Sensitive Online Classification has drawn extensive attention in recent\nyears, where the main approach is to directly online optimize two well-known\ncost-sensitive metrics: (i) weighted sum of sensitivity and specificity; (ii)\nweighted misclassification cost. However, previous existing methods only\nconsidered first-order information of data stream. It is insufficient in\npractice, since many recent studies have proved that incorporating second-order\ninformation enhances the prediction performance of classification models. Thus,\nwe propose a family of cost-sensitive online classification algorithms with\nadaptive regularization in this paper. We theoretically analyze the proposed\nalgorithms and empirically validate their effectiveness and properties in\nextensive experiments. Then, for better trade off between the performance and\nefficiency, we further introduce the sketching technique into our algorithms,\nwhich significantly accelerates the computational speed with quite slight\nperformance loss. Finally, we apply our algorithms to tackle several online\nanomaly detection tasks from real world. Promising results prove that the\nproposed algorithms are effective and efficient in solving cost-sensitive\nonline classification problems in various real-world domains. \n\n"}
{"id": "1804.02476", "contents": "Title: Associative Compression Networks for Representation Learning Abstract: This paper introduces Associative Compression Networks (ACNs), a new\nframework for variational autoencoding with neural networks. The system differs\nfrom existing variational autoencoders (VAEs) in that the prior distribution\nused to model each code is conditioned on a similar code from the dataset. In\ncompression terms this equates to sequentially transmitting the dataset using\nan ordering determined by proximity in latent space. Since the prior need only\naccount for local, rather than global variations in the latent space, the\ncoding cost is greatly reduced, leading to rich, informative codes. Crucially,\nthe codes remain informative when powerful, autoregressive decoders are used,\nwhich we argue is fundamentally difficult with normal VAEs. Experimental\nresults on MNIST, CIFAR-10, ImageNet and CelebA show that ACNs discover\nhigh-level latent features such as object class, writing style, pose and facial\nexpression, which can be used to cluster and classify the data, as well as to\ngenerate diverse and convincing samples. We conclude that ACNs are a promising\nnew direction for representation learning: one that steps away from IID\nmodelling, and towards learning a structured description of the dataset as a\nwhole. \n\n"}
{"id": "1804.02854", "contents": "Title: Tight Hardness Results for Consensus Problems on Circular Strings and\n  Time Series Abstract: Consensus problems for strings and sequences appear in numerous application\ncontexts, ranging from bioinformatics over data mining to machine learning.\nClosing some gaps in the literature, we show that several fundamental problems\nin this context are NP- and W[1]-hard, and that the known (partially\nbrute-force) algorithms are close to optimality assuming the Exponential Time\nHypothesis. Among our main contributions is to settle the complexity status of\ncomputing a mean in dynamic time warping spaces which, as pointed out by Brill\net al. [DMKD 2019], suffered from many unproven or false assumptions in the\nliterature. We prove this problem to be NP-hard and additionally show that a\nrecent dynamic programming algorithm is essentially optimal. In this context,\nwe study a broad family of circular string alignment problems. This family also\nserves as a key for our hardness reductions, and it is of independent\n(practical) interest in molecular biology. In particular, we show tight\nhardness and running time lower bounds for Circular Consensus String; notably,\nthe corresponding non-circular version is easily linear-time solvable. \n\n"}
{"id": "1804.04025", "contents": "Title: Rapid mixing of Glauber dynamics for colorings below Vigoda's $11/6$\n  threshold Abstract: A well-known conjecture in computer science and statistical physics is that\nGlauber dynamics on the set of $k$-colorings of a graph $G$ on $n$ vertices\nwith maximum degree $\\Delta$ is rapidly mixing for $k \\geq \\Delta +2$. In FOCS\n1999, Vigoda showed rapid mixing of flip dynamics with certain flip parameters\non the set of proper $k$-colorings for $k > \\frac{11}{6}\\Delta$, implying rapid\nmixing for Glauber dynamics. In this paper, we obtain the first improvement\nbeyond the $\\frac{11}{6}\\Delta$ barrier for general graphs by showing rapid\nmixing for $k > (\\frac{11}{6} - \\eta)\\Delta$ for some positive constant $\\eta$.\nThe key to our proof is combining path coupling with a new kind of metric that\nincorporates a count of the extremal configurations of the chain. Additionally,\nour results extend to list coloring, a widely studied generalization of\ncoloring. Combined, these results answer two open questions from Frieze and\nVigoda's 2007 survey paper on Glauber dynamics for colorings. \n\n"}
{"id": "1804.05013", "contents": "Title: Connectivity in Random Annulus Graphs and the Geometric Block Model Abstract: We provide new connectivity results for {\\em vertex-random graphs} or {\\em\nrandom annulus graphs} which are significant generalizations of random\ngeometric graphs. Random geometric graphs (RGG) are one of the most basic\nmodels of random graphs for spatial networks proposed by Gilbert in 1961,\nshortly after the introduction of the Erd\\H{o}s-R\\'{en}yi random graphs. They\nresemble social networks in many ways (e.g. by spontaneously creating cluster\nof nodes with high modularity). The connectivity properties of RGG have been\nstudied since its introduction, and analyzing them has been significantly\nharder than their Erd\\H{o}s-R\\'{en}yi counterparts due to correlated edge\nformation.\n  Our next contribution is in using the connectivity of random annulus graphs\nto provide necessary and sufficient conditions for efficient recovery of\ncommunities for {\\em the geometric block model} (GBM). The GBM is a\nprobabilistic model for community detection defined over an RGG in a similar\nspirit as the popular {\\em stochastic block model}, which is defined over an\nErd\\H{o}s-R\\'{en}yi random graph. The geometric block model inherits the\ntransitivity properties of RGGs and thus models communities better than a\nstochastic block model. However, analyzing them requires fresh perspectives as\nall prior tools fail due to correlation in edge formation. We provide a simple\nand efficient algorithm that can recover communities in GBM exactly with high\nprobability in the regime of connectivity. \n\n"}
{"id": "1804.05230", "contents": "Title: The threshold for SDP-refutation of random regular NAE-3SAT Abstract: Unlike its cousin 3SAT, the NAE-3SAT (not-all-equal-3SAT) problem has the\nproperty that spectral/SDP algorithms can efficiently refute random instances\nwhen the constraint density is a large constant (with high probability). But do\nthese methods work immediately above the \"satisfiability threshold\", or is\nthere still a range of constraint densities for which random NAE-3SAT instances\nare unsatisfiable but hard to refute?\n  We show that the latter situation prevails, at least in the context of random\nregular instances and SDP-based refutation. More precisely, whereas a random\n$d$-regular instance of NAE-3SAT is easily shown to be unsatisfiable (whp) once\n$d \\geq 8$, we establish the following sharp threshold result regarding\nefficient refutation: If $d < 13.5$ then the basic SDP, even augmented with\ntriangle inequalities, fails to refute satisfiability (whp), if $d > 13.5$ then\neven the most basic spectral algorithm refutes satisfiability~(whp). \n\n"}
{"id": "1804.05436", "contents": "Title: Hidden Hamiltonian Cycle Recovery via Linear Programming Abstract: We introduce the problem of hidden Hamiltonian cycle recovery, where there is\nan unknown Hamiltonian cycle in an $n$-vertex complete graph that needs to be\ninferred from noisy edge measurements. The measurements are independent and\ndistributed according to $\\calP_n$ for edges in the cycle and $\\calQ_n$\notherwise. This formulation is motivated by a problem in genome assembly, where\nthe goal is to order a set of contigs (genome subsequences) according to their\npositions on the genome using long-range linking measurements between the\ncontigs. Computing the maximum likelihood estimate in this model reduces to a\nTraveling Salesman Problem (TSP). Despite the NP-hardness of TSP, we show that\na simple linear programming (LP) relaxation, namely the fractional $2$-factor\n(F2F) LP, recovers the hidden Hamiltonian cycle with high probability as $n \\to\n\\infty$ provided that $\\alpha_n - \\log n \\to \\infty$, where $\\alpha_n\n\\triangleq -2 \\log \\int \\sqrt{d P_n d Q_n}$ is the R\\'enyi divergence of order\n$\\frac{1}{2}$. This condition is information-theoretically optimal in the sense\nthat, under mild distributional assumptions, $\\alpha_n \\geq (1+o(1)) \\log n$ is\nnecessary for any algorithm to succeed regardless of the computational cost.\n  Departing from the usual proof techniques based on dual witness construction,\nthe analysis relies on the combinatorial characterization (in particular, the\nhalf-integrality) of the extreme points of the F2F polytope. Represented as\nbicolored multi-graphs, these extreme points are further decomposed into\nsimpler \"blossom-type\" structures for the large deviation analysis and counting\narguments. Evaluation of the algorithm on real data shows improvements over\nexisting approaches. \n\n"}
{"id": "1804.06515", "contents": "Title: Faster Evaluation of Subtraction Games Abstract: Subtraction games are played with one or more heaps of tokens, with players\ntaking turns removing from a single heap a number of tokens belonging to a\nspecified subtraction set; the last player to move wins. We describe how to\ncompute the set of winning heap sizes in single-heap subtraction games (for an\ninput consisting of the subtraction set and maximum heap size $n$), in time\n$\\tilde O(n)$, where the $\\tilde O$ elides logarithmic factors. For multi-heap\ngames, the optimal game play is determined by the nim-value of each heap; we\ndescribe how to compute the nim-values of all heaps of size up to~$n$ in time\n$\\tilde O(mn)$, where $m$ is the maximum nim-value occurring among these heap\nsizes. These time bounds improve naive dynamic programming algorithms with time\n$O(n|S|)$, because $m\\le|S|$ for all such games. We apply these results to the\ngame of subtract-a-square, whose set of winning positions is a maximal\nsquare-difference-free set of a type studied in number theory in connection\nwith the Furstenberg-S\\'ark\\\"ozy theorem. We provide experimental evidence\nthat, for this game, the set of winning positions has a density comparable to\nthat of the densest known square-difference-free sets, and has a modular\nstructure related to the known constructions for these dense sets.\nAdditionally, this game's nim-values are (experimentally) significantly smaller\nthan the size of its subtraction set, implying that our algorithm achieves a\npolynomial speedup over dynamic programming. \n\n"}
{"id": "1804.07353", "contents": "Title: Unsupervised Representation Adversarial Learning Network: from\n  Reconstruction to Generation Abstract: A good representation for arbitrarily complicated data should have the\ncapability of semantic generation, clustering and reconstruction. Previous\nresearch has already achieved impressive performance on either one. This paper\naims at learning a disentangled representation effective for all of them in an\nunsupervised way. To achieve all the three tasks together, we learn the forward\nand inverse mapping between data and representation on the basis of a symmetric\nadversarial process. In theory, we minimize the upper bound of the two\nconditional entropy loss between the latent variables and the observations\ntogether to achieve the cycle consistency. The newly proposed RepGAN is tested\non MNIST, fashionMNIST, CelebA, and SVHN datasets to perform unsupervised\nclassification, generation and reconstruction tasks. The result demonstrates\nthat RepGAN is able to learn a useful and competitive representation. To the\nauthor's knowledge, our work is the first one to achieve both a high\nunsupervised classification accuracy and low reconstruction error on MNIST.\nCodes are available at https://github.com/yzhouas/RepGAN-tensorflow. \n\n"}
{"id": "1804.07431", "contents": "Title: Finding Cliques in Social Networks: A New Distribution-Free Model Abstract: We propose a new distribution-free model of social networks. Our definitions\nare motivated by one of the most universal signatures of social networks,\ntriadic closure---the property that pairs of vertices with common neighbors\ntend to be adjacent. Our most basic definition is that of a \"$c$-closed\" graph,\nwhere for every pair of vertices $u,v$ with at least $c$ common neighbors, $u$\nand $v$ are adjacent. We study the classic problem of enumerating all maximal\ncliques, an important task in social network analysis. We prove that this\nproblem is fixed-parameter tractable with respect to $c$ on $c$-closed graphs.\nOur results carry over to \"weakly $c$-closed graphs\", which only require a\nvertex deletion ordering that avoids pairs of non-adjacent vertices with $c$\ncommon neighbors. Numerical experiments show that well-studied social networks\ntend to be weakly $c$-closed for modest values of $c$. \n\n"}
{"id": "1804.08111", "contents": "Title: Sampling in Uniqueness from the Potts and Random-Cluster Models on\n  Random Regular Graphs Abstract: We consider the problem of sampling from the Potts model on random regular\ngraphs. It is conjectured that sampling is possible when the temperature of the\nmodel is in the uniqueness regime of the regular tree, but positive algorithmic\nresults have been for the most part elusive. In this paper, for all integers\n$q\\geq 3$ and $\\Delta\\geq 3$, we develop algorithms that produce samples within\nerror $o(1)$ from the $q$-state Potts model on random $\\Delta$-regular graphs,\nwhenever the temperature is in uniqueness, for both the ferromagnetic and\nantiferromagnetic cases.\n  The algorithm for the antiferromagnetic Potts model is based on iteratively\nadding the edges of the graph and resampling a bichromatic class that contains\nthe endpoints of the newly added edge. Key to the algorithm is how to perform\nthe resampling step efficiently since bichromatic classes may induce\nlinear-sized components. To this end, we exploit the tree uniqueness to show\nthat the average growth of bichromatic components is typically small, which\nallows us to use correlation decay algorithms for the resampling step. While\nthe precise uniqueness threshold on the tree is not known for general values of\n$q$ and $\\Delta$ in the antiferromagnetic case, our algorithm works throughout\nuniqueness regardless of its value.\n  In the case of the ferromagnetic Potts model, we simplify the algorithm\nsignificantly by utilising the random-cluster representation of the model. In\nparticular, we show that a percolation-type algorithm succeeds in sampling from\nthe random-cluster model with parameters $p,q$ on random $\\Delta$-regular\ngraphs for all values of $q\\geq 1$ and $p<p_c(q,\\Delta)$, where $p_c(q,\\Delta)$\ncorresponds to a uniqueness threshold for the model on the $\\Delta$-regular\ntree. When restricted to integer values of $q$, this yields a simplified\nalgorithm for the ferromagnetic Potts model on random $\\Delta$-regular graphs. \n\n"}
{"id": "1804.08885", "contents": "Title: Polynomial Kernels for Hitting Forbidden Minors under Structural\n  Parameterizations Abstract: We investigate polynomial-time preprocessing for the problem of hitting\nforbidden minors in a graph, using the framework of kernelization. For a fixed\nfinite set of connected graphs F, the F-Deletion problem is the following:\ngiven a graph G and integer k, is it possible to delete k vertices from G to\nensure the resulting graph does not contain any graph from F as a minor?\nEarlier work by Fomin, Lokshtanov, Misra, and Saurabh [FOCS'12] showed that\nwhen F contains a planar graph, an instance (G,k) can be reduced in polynomial\ntime to an equivalent one of size $k^{O(1)}$. In this work we focus on\nstructural measures of the complexity of an instance, with the aim of giving\nnontrivial preprocessing guarantees for instances whose solutions are large.\nMotivated by several impossibility results, we parameterize the F-Deletion\nproblem by the size of a vertex modulator whose removal results in a graph of\nconstant treedepth $\\eta$.\n  We prove that for each set F of connected graphs and constant $\\eta$, the\nF-Deletion problem parameterized by the size of a treedepth-$\\eta$ modulator\nhas a polynomial kernel. Our kernelization is fully explicit and does not\ndepend on protrusion reduction or well-quasi-ordering, which are sources of\nalgorithmic non-constructivity in earlier works on F-Deletion. Our main\ntechnical contribution is to analyze how models of a forbidden minor in a graph\nG with modulator X, interact with the various connected components of G-X. By\nbounding the number of different types of behavior that can occur by a\npolynomial in |X|, we obtain a polynomial kernel using a recursive\npreprocessing strategy. Our results extend earlier work for specific instances\nof F-Deletion such as Vertex Cover and Feedback Vertex Set. It also generalizes\nearlier preprocessing results for F-Deletion parameterized by a vertex cover,\nwhich is a treedepth-one modulator. \n\n"}
{"id": "1804.09950", "contents": "Title: Quantum Algorithm for Dynamic Programming Approach for DAGs.\n  Applications for Zhegalkin Polynomial Evaluation and Some Problems on DAGs Abstract: In this paper, we present a quantum algorithm for dynamic programming\napproach for problems on directed acyclic graphs (DAGs). The running time of\nthe algorithm is $O(\\sqrt{\\hat{n}m}\\log \\hat{n})$, and the running time of the\nbest known deterministic algorithm is $O(n+m)$, where $n$ is the number of\nvertices, $\\hat{n}$ is the number of vertices with at least one outgoing edge;\n$m$ is the number of edges. We show that we can solve problems that use OR,\nAND, NAND, MAX and MIN functions as the main transition steps. The approach is\nuseful for a couple of problems. One of them is computing a Boolean formula\nthat is represented by Zhegalkin polynomial, a Boolean circuit with shared\ninput and non-constant depth evaluating. Another two are the single source\nlongest paths search for weighted DAGs and the diameter search problem for\nunweighted DAGs. \n\n"}
{"id": "1804.10470", "contents": "Title: Intersecting edge distinguishing colorings of hypergraphs Abstract: An edge labeling of a graph distinguishes neighbors by sets (multisets,\nresp.), if for any two adjacent vertices $u$ and $v$ the sets (multisets,\nresp.) of labels appearing on edges incident to $u$ and $v$ are different. In\nan analogous way we define total labelings distinguishing neighbors by sets or\nmultisets: for each vertex, we consider labels on incident edges and the label\nof the vertex itself.\n  In this paper we show that these problems, and also other problems of similar\nflavor, admit an elegant and natural generalization as a hypergraph coloring\nproblem. An ieds-coloring (iedm-coloring, resp.) of a hypergraph is a vertex\ncoloring, in which the sets (multisets, resp.) of colors, that appear on every\npair of intersecting edges are different. We show upper bounds on the size of\nlists, which guarantee the existence of an ieds- or iedm-coloring, respecting\nthese lists. The proof is essentially a randomized algorithm, whose expected\ntime complexity is polynomial. As corollaries, we derive new results concerning\nthe list variants of graph labeling problems, distinguishing neighbors by sets\nor multisets. We also show that our method is robust and can be easily extended\nfor different, related problems.\n  We also investigate a close connection between edge labelings of bipartite\ngraphs, distinguishing neighbors by sets, and the so-called property \\textbf{B}\nof hypergraphs. We discuss computational aspects of the problem and present\nsome classes of bipartite graphs, which admit such a labeling using two labels. \n\n"}
{"id": "1804.10696", "contents": "Title: Low Rank Approximation in the Presence of Outliers Abstract: We consider the problem of principal component analysis (PCA) in the presence\nof outliers. Given a matrix $A$ ($d \\times n$) and parameters $k, m$, the goal\nis to remove a set of at most $m$ columns of $A$ (known as outliers), so as to\nminimize the rank-$k$ approximation error of the remaining matrix. While much\nof the work on this problem has focused on recovery of the rank-$k$ subspace\nunder assumptions on the inliers and outliers, we focus on the approximation\nproblem above. Our main result shows that sampling-based methods developed in\nthe outlier-free case give non-trivial guarantees even in the presence of\noutliers. Using this insight, we develop a simple algorithm that has\nbi-criteria guarantees. Further, unlike similar formulations for clustering, we\nshow that bi-criteria guarantees are unavoidable for the problem, under\nappropriate complexity assumptions. \n\n"}
{"id": "1805.00060", "contents": "Title: On improving the approximation ratio of the r-shortest common\n  superstring problem Abstract: The Shortest Common Superstring problem (SCS) consists, for a set of strings\nS = {s_1,...,s_n}, in finding a minimum length string that contains all s_i,\n1<= i <= n, as substrings. While a 2+11/30 approximation ratio algorithm has\nrecently been published, the general objective is now to break the conceptual\nlower bound barrier of 2. This paper is a step ahead in this direction. Here we\nfocus on a particular instance of the SCS problem, meaning the r-SCS problem,\nwhich requires all input strings to be of the same length, r. Golonev et al.\nproved an approximation ratio which is better than the general one for r<= 6.\nHere we extend their approach and improve their approximation ratio, which is\nnow better than the general one for r<= 7, and less than or equal to 2 up to r\n= 6. \n\n"}
{"id": "1805.00181", "contents": "Title: Spectrally Robust Graph Isomorphism Abstract: We initiate the study of spectral generalizations of the graph isomorphism\nproblem.\n  (a)The Spectral Graph Dominance (SGD) problem: On input of two graphs $G$ and\n$H$ does there exist a permutation $\\pi$ such that $G\\preceq \\pi(H)$?\n  (b) The Spectrally Robust Graph Isomorphism (SRGI) problem: On input of two\ngraphs $G$ and $H$, find the smallest number $\\kappa$ over all permutations\n$\\pi$ such that $ \\pi(H) \\preceq G\\preceq \\kappa c \\pi(H)$ for some $c$. SRGI\nis a natural formulation of the network alignment problem that has various\napplications, most notably in computational biology.\n  Here $G\\preceq c H$ means that for all vectors $x$ we have $x^T L_G x \\leq c\nx^T L_H x$, where $L_G$ is the Laplacian $G$.\n  We prove NP-hardness for SGD. We also present a $\\kappa$-approximation\nalgorithm for SRGI for the case when both $G$ and $H$ are bounded-degree trees.\nThe algorithm runs in polynomial time when $\\kappa$ is a constant. \n\n"}
{"id": "1805.00327", "contents": "Title: A Taxonomy for Neural Memory Networks Abstract: In this paper, a taxonomy for memory networks is proposed based on their\nmemory organization. The taxonomy includes all the popular memory networks:\nvanilla recurrent neural network (RNN), long short term memory (LSTM ), neural\nstack and neural Turing machine and their variants. The taxonomy puts all these\nnetworks under a single umbrella and shows their relative expressive power ,\ni.e. vanilla RNN <=LSTM<=neural stack<=neural RAM. The differences and\ncommonality between these networks are analyzed. These differences are also\nconnected to the requirements of different tasks which can give the user\ninstructions of how to choose or design an appropriate memory network for a\nspecific task. As a conceptual simplified class of problems, four tasks of\nsynthetic symbol sequences: counting, counting with interference, reversing and\nrepeat counting are developed and tested to verify our arguments. And we use\ntwo natural language processing problems to discuss how this taxonomy helps\nchoosing the appropriate neural memory networks for real world problem. \n\n"}
{"id": "1805.00356", "contents": "Title: Deep Factorization Machines for Knowledge Tracing Abstract: This paper introduces our solution to the 2018 Duolingo Shared Task on Second\nLanguage Acquisition Modeling (SLAM). We used deep factorization machines, a\nwide and deep learning model of pairwise relationships between users, items,\nskills, and other entities considered. Our solution (AUC 0.815) hopefully\nmanaged to beat the logistic regression baseline (AUC 0.774) but not the top\nperforming model (AUC 0.861) and reveals interesting strategies to build upon\nitem response theory models. \n\n"}
{"id": "1805.00705", "contents": "Title: Investigating Audio, Visual, and Text Fusion Methods for End-to-End\n  Automatic Personality Prediction Abstract: We propose a tri-modal architecture to predict Big Five personality trait\nscores from video clips with different channels for audio, text, and video\ndata. For each channel, stacked Convolutional Neural Networks are employed. The\nchannels are fused both on decision-level and by concatenating their respective\nfully connected layers. It is shown that a multimodal fusion approach\noutperforms each single modality channel, with an improvement of 9.4\\% over the\nbest individual modality (video). Full backpropagation is also shown to be\nbetter than a linear combination of modalities, meaning complex interactions\nbetween modalities can be leveraged to build better models. Furthermore, we can\nsee the prediction relevance of each modality for each trait. The described\nmodel can be used to increase the emotional intelligence of virtual agents. \n\n"}
{"id": "1805.01209", "contents": "Title: Found Graph Data and Planted Vertex Covers Abstract: A typical way in which network data is recorded is to measure all the\ninteractions among a specified set of core nodes; this produces a graph\ncontaining this core together with a potentially larger set of fringe nodes\nthat have links to the core. Interactions between pairs of nodes in the fringe,\nhowever, are not recorded by this process, and hence not present in the\nresulting graph data. For example, a phone service provider may only have\nrecords of calls in which at least one of the participants is a customer; this\ncan include calls between a customer and a non-customer, but not between pairs\nof non-customers.\n  Knowledge of which nodes belong to the core is an important piece of metadata\nthat is crucial for interpreting the network dataset. But in many cases, this\nmetadata is not available, either because it has been lost due to difficulties\nin data provenance, or because the network consists of found data obtained in\nsettings such as counter-surveillance. This leads to a natural algorithmic\nproblem, namely the recovery of the core set. Since the core set forms a vertex\ncover of the graph, we essentially have a planted vertex cover problem, but\nwith an arbitrary underlying graph. We develop a theoretical framework for\nanalyzing this planted vertex cover problem, based on results in the theory of\nfixed-parameter tractability, together with algorithms for recovering the core.\nOur algorithms are fast, simple to implement, and out-perform several methods\nbased on network core-periphery structure on various real-world datasets. \n\n"}
{"id": "1805.03253", "contents": "Title: Efficient Shortest Paths in Scale-Free Networks with Underlying\n  Hyperbolic Geometry Abstract: A common way to accelerate shortest path algorithms on graphs is the use of a\nbidirectional search, which simultaneously explores the graph from the start\nand the destination. It has been observed recently that this strategy performs\nparticularly well on scale-free real-world networks. Such networks typically\nhave a heterogeneous degree distribution (e.g., a power-law distribution) and\nhigh clustering (i.e., vertices with a common neighbor are likely to be\nconnected themselves). These two properties can be obtained by assuming an\nunderlying hyperbolic geometry.\n  To explain the observed behavior of the bidirectional search, we analyze its\nrunning time on hyperbolic random graphs and prove that it is $\\mathcal {\\tilde\nO}(n^{2 - 1/\\alpha} + n^{1/(2\\alpha)} + \\delta_{\\max})$ with high probability,\nwhere $\\alpha \\in (0.5, 1)$ controls the power-law exponent of the degree\ndistribution, and $\\delta_{\\max}$ is the maximum degree. This bound is\nsublinear, improving the obvious worst-case linear bound. Although our analysis\ndepends on the underlying geometry, the algorithm itself is oblivious to it. \n\n"}
{"id": "1805.03405", "contents": "Title: Characterizing and decomposing classes of threshold, split, and\n  bipartite graphs via 1-Sperner hypergraphs Abstract: A hypergraph is said to be $1$-Sperner if for every two hyperedges the\nsmallest of their two set differences is of size one. We present several\napplications of $1$-Sperner hypergraphs and their structure to graphs. In\nparticular, we consider the classical characterizations of threshold and\ndomishold graphs and use them to obtain further characterizations of these\nclasses in terms of $1$-Spernerness, thresholdness, and $2$-asummability of\ntheir vertex cover, clique, dominating set, and closed neighborhood\nhypergraphs. Furthermore, we apply a decomposition property of $1$-Sperner\nhypergraphs to derive decomposition theorems for two classes of split graphs, a\nclass of bipartite graphs, and a class of cobipartite graphs. These\ndecomposition theorems are based on certain matrix partitions of the\ncorresponding graphs, giving rise to new classes of graphs of bounded\nclique-width and new polynomially solvable cases of several domination\nproblems. \n\n"}
{"id": "1805.04179", "contents": "Title: The Hidden Subgroup Problem and Post-quantum Group-based Cryptography Abstract: In this paper we discuss the Hidden Subgroup Problem (HSP) in relation to\npost-quantum group-based cryptography. We review the relationship between HSP\nand other computational problems discuss an optimal solution method, and review\nthe known results about the quantum complexity of HSP. We also overview some\nplatforms for group-based cryptosystems. Notably, efficient algorithms for\nsolving HSP in such infinite group platforms are not yet known. \n\n"}
{"id": "1805.04784", "contents": "Title: Nonlinear Metric Learning through Geodesic Interpolation within Lie\n  Groups Abstract: In this paper, we propose a nonlinear distance metric learning scheme based\non the fusion of component linear metrics. Instead of merging displacements at\neach data point, our model calculates the velocities induced by the component\ntransformations, via a geodesic interpolation on a Lie transfor- mation group.\nSuch velocities are later summed up to produce a global transformation that is\nguaranteed to be diffeomorphic. Consequently, pair-wise distances computed this\nway conform to a smooth and spatially varying metric, which can greatly benefit\nk-NN classification. Experiments on synthetic and real datasets demonstrate the\neffectiveness of our model. \n\n"}
{"id": "1805.05151", "contents": "Title: Domain Adaptation with Adversarial Training and Graph Embeddings Abstract: The success of deep neural networks (DNNs) is heavily dependent on the\navailability of labeled data. However, obtaining labeled data is a big\nchallenge in many real-world problems. In such scenarios, a DNN model can\nleverage labeled and unlabeled data from a related domain, but it has to deal\nwith the shift in data distributions between the source and the target domains.\nIn this paper, we study the problem of classifying social media posts during a\ncrisis event (e.g., Earthquake). For that, we use labeled and unlabeled data\nfrom past similar events (e.g., Flood) and unlabeled data for the current\nevent. We propose a novel model that performs adversarial learning based domain\nadaptation to deal with distribution drifts and graph based semi-supervised\nlearning to leverage unlabeled data within a single unified deep learning\nframework. Our experiments with two real-world crisis datasets collected from\nTwitter demonstrate significant improvements over several baselines. \n\n"}
{"id": "1805.05305", "contents": "Title: Transforming graph states using single-qubit operations Abstract: Stabilizer states form an important class of states in quantum information,\nand are of central importance in quantum error correction. Here, we provide an\nalgorithm for deciding whether one stabilizer (target) state can be obtained\nfrom another stabilizer (source) state by single-qubit Clifford operations\n(LC), single-qubit Pauli measurements (LPM), and classical communication (CC)\nbetween sites holding the individual qubits. What's more, we provide a recipe\nto obtain the sequence of LC+LPM+CC operations which prepare the desired target\nstate from the source state, and show how these operations can be applied in\nparallel to reach the target state in constant time. Our algorithm has\napplications in quantum networks, quantum computing, and can also serve as a\ndesign tool - for example, to find transformations between quantum error\ncorrecting codes. We provide a software implementation of our algorithm that\nmakes this tool easier to apply.\n  A key insight leading to our algorithm is to show that the problem is\nequivalent to one in graph theory, which is to decide whether some graph G' is\na vertex-minor of another graph G. Here we show that the vertex-minor problem\ncan be solved in time O(|G|^3) where |G| is the size of the graph G, whenever\nthe rank-width of G and the size of G' are bounded. Our algorithm is based on\ntechniques by Courcelle for solving fixed parameter tractable problems, where\nhere the relevant fixed parameter is the rank width. The second half of this\npaper serves as an accessible but far from exhausting introduction to these\nconcepts, that could be useful for many other problems in quantum information. \n\n"}
{"id": "1805.05592", "contents": "Title: Parallel Write-Efficient Algorithms and Data Structures for\n  Computational Geometry Abstract: In this paper, we design parallel write-efficient geometric algorithms that\nperform asymptotically fewer writes than standard algorithms for the same\nproblem. This is motivated by emerging non-volatile memory technologies with\nread performance being close to that of random access memory but writes being\nsignificantly more expensive in terms of energy and latency. We design\nalgorithms for planar Delaunay triangulation, $k$-d trees, and static and\ndynamic augmented trees. Our algorithms are designed in the recently introduced\nAsymmetric Nested-Parallel Model, which captures the parallel setting in which\nthere is a small symmetric memory where reads and writes are unit cost as well\nas a large asymmetric memory where writes are $\\omega$ times more expensive\nthan reads. In designing these algorithms, we introduce several techniques for\nobtaining write-efficiency, including DAG tracing, prefix doubling,\nreconstruction-based rebalancing and $\\alpha$-labeling, which we believe will\nbe useful for designing other parallel write-efficient algorithms. \n\n"}
{"id": "1805.06232", "contents": "Title: On Fair Division of Indivisible Items Abstract: We consider the task of assigning indivisible goods to a set of agents in a\nfair manner. Our notion of fairness is Nash social welfare, i.e., the goal is\nto maximize the geometric mean of the utilities of the agents. Each good comes\nin multiple items or copies, and the utility of an agent diminishes as it\nreceives more items of the same good. The utility of a bundle of items for an\nagent is the sum of the utilities of the items in the bundle. Each agent has a\nutility cap beyond which he does not value additional items. We give a\npolynomial time approximation algorithm that maximizes Nash social welfare up\nto a factor of $e^{1/{e}} \\approx 1.445$. The computed allocation is\nPareto-optimal and approximates envy-freeness up to one item up to a factor of\n$2 + \\eps$ \n\n"}
{"id": "1805.06862", "contents": "Title: Design Identification of Curve Patterns on Cultural Heritage Objects:\n  Combining Template Matching and CNN-based Re-Ranking Abstract: The surfaces of many cultural heritage objects were embellished with various\npatterns, especially curve patterns. In practice, most of the unearthed\ncultural heritage objects are highly fragmented, e.g., sherds of potteries or\nvessels, and each of them only shows a very small portion of the underlying\nfull design, with noise and deformations. The goal of this paper is to address\nthe challenging problem of automatically identifying the underlying full design\nof curve patterns from such a sherd. Specifically, we formulate this problem as\ntemplate matching: curve structure segmented from the sherd is matched to each\nlocation with each possible orientation of each known full design. In this\npaper, we propose a new two-stage matching algorithm, with a different matching\ncost in each stage. In Stage 1, we use a traditional template matching, which\nis highly computationally efficient, over the whole search space and identify a\nsmall set of candidate matchings. In Stage 2, we derive a new matching cost by\ntraining a dual-source Convolutional Neural Network (CNN) and apply it to\nre-rank the candidate matchings identified in Stage 1. We collect 600 pottery\nsherds with 98 full designs from the Woodland Period in Southeastern North\nAmerica for experiments and the performance of the proposed algorithm is very\ncompetitive. \n\n"}
{"id": "1805.07722", "contents": "Title: Task-Agnostic Meta-Learning for Few-shot Learning Abstract: Meta-learning approaches have been proposed to tackle the few-shot learning\nproblem.Typically, a meta-learner is trained on a variety of tasks in the hopes\nof being generalizable to new tasks. However, the generalizability on new tasks\nof a meta-learner could be fragile when it is over-trained on existing tasks\nduring meta-training phase. In other words, the initial model of a meta-learner\ncould be too biased towards existing tasks to adapt to new tasks, especially\nwhen only very few examples are available to update the model. To avoid a\nbiased meta-learner and improve its generalizability, we propose a novel\nparadigm of Task-Agnostic Meta-Learning (TAML) algorithms. Specifically, we\npresent an entropy-based approach that meta-learns an unbiased initial model\nwith the largest uncertainty over the output labels by preventing it from\nover-performing in classification tasks. Alternatively, a more general\ninequality-minimization TAML is presented for more ubiquitous scenarios by\ndirectly minimizing the inequality of initial losses beyond the classification\ntasks wherever a suitable loss can be defined.Experiments on benchmarked\ndatasets demonstrate that the proposed approaches outperform compared\nmeta-learning algorithms in both few-shot classification and reinforcement\nlearning tasks. \n\n"}
{"id": "1805.07914", "contents": "Title: Imitating Latent Policies from Observation Abstract: In this paper, we describe a novel approach to imitation learning that infers\nlatent policies directly from state observations. We introduce a method that\ncharacterizes the causal effects of latent actions on observations while\nsimultaneously predicting their likelihood. We then outline an action alignment\nprocedure that leverages a small amount of environment interactions to\ndetermine a mapping between the latent and real-world actions. We show that\nthis corrected labeling can be used for imitating the observed behavior, even\nthough no expert actions are given. We evaluate our approach within classic\ncontrol environments and a platform game and demonstrate that it performs\nbetter than standard approaches. Code for this work is available at\nhttps://github.com/ashedwards/ILPO. \n\n"}
{"id": "1805.08187", "contents": "Title: Finding forbidden minors in sublinear time: a $n^{1/2+o(1)}$-query\n  one-sided tester for minor closed properties on bounded degree graphs Abstract: Let $G$ be an undirected, bounded degree graph with $n$ vertices. Fix a\nfinite graph $H$, and suppose one must remove $\\varepsilon n$ edges from $G$ to\nmake it $H$-minor free (for some small constant $\\varepsilon > 0$). We give an\n$n^{1/2+o(1)}$-time randomized procedure that, with high probability, finds an\n$H$-minor in such a graph. As an application, suppose one must remove\n$\\varepsilon n$ edges from a bounded degree graph $G$ to make it planar. This\nresult implies an algorithm, with the same running time, that produces a\n$K_{3,3}$ or $K_5$ minor in $G$. No prior sublinear time bound was known for\nthis problem.\n  By the graph minor theorem, we get an analogous result for any minor-closed\nproperty. Up to $n^{o(1)}$ factors, this resolves a conjecture of\nBenjamini-Schramm-Shapira (STOC 2008) on the existence of one-sided property\ntesters for minor-closed properties. Furthermore, our algorithm is nearly\noptimal, by an $\\Omega(\\sqrt{n})$ lower bound of Czumaj et al (RSA 2014).\n  Prior to this work, the only graphs $H$ for which non-trivial one-sided\nproperty testers were known for $H$-minor freeness are the following: $H$ being\na forest or a cycle (Czumaj et al, RSA 2014), $K_{2,k}$, $(k\\times 2)$-grid,\nand the $k$-circus (Fichtenberger et al, Arxiv 2017). \n\n"}
{"id": "1805.09261", "contents": "Title: Online shortest paths with confidence intervals for routing in a time\n  varying random network Abstract: The increase in the world's population and rising standards of living is\nleading to an ever-increasing number of vehicles on the roads, and with it\never-increasing difficulties in traffic management. This traffic management in\ntransport networks can be clearly optimized by using information and\ncommunication technologies referred as Intelligent Transport Systems (ITS).\nThis management problem is usually reformulated as finding the shortest path in\na time varying random graph. In this article, an online shortest path\ncomputation using stochastic gradient descent is proposed. This routing\nalgorithm for ITS traffic management is based on the online Frank-Wolfe\napproach. Our improvement enables to find a confidence interval for the\nshortest path, by using the stochastic gradient algorithm for approximate\nBayesian inference. \n\n"}
{"id": "1805.10754", "contents": "Title: A note on block-and-bridge preserving maximum common subgraph algorithms\n  for outerplanar graphs Abstract: Schietgat, Ramon and Bruynooghe proposed a polynomial-time algorithm for\ncomputing a maximum common subgraph under the block-and-bridge preserving\nsubgraph isomorphism (BBP-MCS) for outerplanar graphs. We show that the article\ncontains the following errors: (i) The running time of the presented approach\nis claimed to be $\\mathcal{O}(n^{2.5})$ for two graphs of order $n$. We show\nthat the algorithm of the authors allows no better bound than\n$\\mathcal{O}(n^4)$ when using state-of-the-art general purpose methods to solve\nthe matching instances arising as subproblems. This is even true for the\nspecial case, where both input graphs are trees. (ii) The article suggests that\nthe dissimilarity measure derived from BBP-MCS is a metric. We show that the\ntriangle inequality is not always satisfied and, hence, it is not a metric.\nTherefore, the dissimilarity measure should not be used in combination with\ntechniques that rely on or exploit the triangle inequality in any way. Where\npossible, we give hints on techniques that are suitable to improve the\nalgorithm. \n\n"}
{"id": "1805.11016", "contents": "Title: Memory Augmented Self-Play Abstract: Self-play is an unsupervised training procedure which enables the\nreinforcement learning agents to explore the environment without requiring any\nexternal rewards. We augment the self-play setting by providing an external\nmemory where the agent can store experience from the previous tasks. This\nenables the agent to come up with more diverse self-play tasks resulting in\nfaster exploration of the environment. The agent pretrained in the memory\naugmented self-play setting easily outperforms the agent pretrained in\nno-memory self-play setting. \n\n"}
{"id": "1805.11233", "contents": "Title: Retraining-Based Iterative Weight Quantization for Deep Neural Networks Abstract: Model compression has gained a lot of attention due to its ability to reduce\nhardware resource requirements significantly while maintaining accuracy of\nDNNs. Model compression is especially useful for memory-intensive recurrent\nneural networks because smaller memory footprint is crucial not only for\nreducing storage requirement but also for fast inference operations.\nQuantization is known to be an effective model compression method and\nresearchers are interested in minimizing the number of bits to represent\nparameters. In this work, we introduce an iterative technique to apply\nquantization, presenting high compression ratio without any modifications to\nthe training algorithm. In the proposed technique, weight quantization is\nfollowed by retraining the model with full precision weights. We show that\niterative retraining generates new sets of weights which can be quantized with\ndecreasing quantization loss at each iteration. We also show that quantization\nis efficiently able to leverage pruning, another effective model compression\nmethod. Implementation issues on combining the two methods are also addressed.\nOur experimental results demonstrate that an LSTM model using 1-bit quantized\nweights is sufficient for PTB dataset without any accuracy degradation while\nprevious methods demand at least 2-4 bits for quantized weights. \n\n"}
{"id": "1805.11240", "contents": "Title: Truncated Horizon Policy Search: Combining Reinforcement Learning &\n  Imitation Learning Abstract: In this paper, we propose to combine imitation and reinforcement learning via\nthe idea of reward shaping using an oracle. We study the effectiveness of the\nnear-optimal cost-to-go oracle on the planning horizon and demonstrate that the\ncost-to-go oracle shortens the learner's planning horizon as function of its\naccuracy: a globally optimal oracle can shorten the planning horizon to one,\nleading to a one-step greedy Markov Decision Process which is much easier to\noptimize, while an oracle that is far away from the optimality requires\nplanning over a longer horizon to achieve near-optimal performance. Hence our\nnew insight bridges the gap and interpolates between imitation learning and\nreinforcement learning. Motivated by the above mentioned insights, we propose\nTruncated HORizon Policy Search (THOR), a method that focuses on searching for\npolicies that maximize the total reshaped reward over a finite planning horizon\nwhen the oracle is sub-optimal. We experimentally demonstrate that a\ngradient-based implementation of THOR can achieve superior performance compared\nto RL baselines and IL baselines even when the oracle is sub-optimal. \n\n"}
{"id": "1805.12233", "contents": "Title: How Important Is a Neuron? Abstract: The problem of attributing a deep network's prediction to its\n\\emph{input/base} features is well-studied. We introduce the notion of\n\\emph{conductance} to extend the notion of attribution to the understanding the\nimportance of \\emph{hidden} units.\n  Informally, the conductance of a hidden unit of a deep network is the\n\\emph{flow} of attribution via this hidden unit. We use conductance to\nunderstand the importance of a hidden unit to the prediction for a specific\ninput, or over a set of inputs. We evaluate the effectiveness of conductance in\nmultiple ways, including theoretical properties, ablation studies, and a\nfeature selection task. The empirical evaluations are done using the Inception\nnetwork over ImageNet data, and a sentiment analysis network over reviews. In\nboth cases, we demonstrate the effectiveness of conductance in identifying\ninteresting insights about the internal workings of these networks. \n\n"}
{"id": "1805.12573", "contents": "Title: Learning a Prior over Intent via Meta-Inverse Reinforcement Learning Abstract: A significant challenge for the practical application of reinforcement\nlearning in the real world is the need to specify an oracle reward function\nthat correctly defines a task. Inverse reinforcement learning (IRL) seeks to\navoid this challenge by instead inferring a reward function from expert\nbehavior. While appealing, it can be impractically expensive to collect\ndatasets of demonstrations that cover the variation common in the real world\n(e.g. opening any type of door). Thus in practice, IRL must commonly be\nperformed with only a limited set of demonstrations where it can be exceedingly\ndifficult to unambiguously recover a reward function. In this work, we exploit\nthe insight that demonstrations from other tasks can be used to constrain the\nset of possible reward functions by learning a \"prior\" that is specifically\noptimized for the ability to infer expressive reward functions from limited\nnumbers of demonstrations. We demonstrate that our method can efficiently\nrecover rewards from images for novel tasks and provide intuition as to how our\napproach is analogous to learning a prior. \n\n"}
{"id": "1806.00917", "contents": "Title: Principled Network Reliability Approximation: A Counting-Based Approach Abstract: As engineered systems expand, become more interdependent, and operate in\nreal-time, reliability assessment is indispensable to support investment and\ndecision making. However, network reliability problems are known to be\n#P-complete, a computational complexity class largely believed to be\nintractable. The computational intractability of network reliability motivates\nour quest for reliable approximations. Based on their theoretical foundations,\navailable methods can be grouped as follows: (i) exact or bounds, (ii)\nguarantee-less sampling, and (iii) probably approximately correct (PAC). Group\n(i) is well regarded due to its useful byproducts, but it does not scale in\npractice. Group (ii) scales well and verifies desirable properties, such as the\nbounded relative error, but it lacks error guarantees. Group (iii) is of great\ninterest when precision and scalability are required, as it harbors\ncomputationally feasible approximation schemes with PAC-guarantees. We give a\ncomprehensive review of classical methods before introducing modern techniques\nand our developments. We introduce K-RelNet, an extended counting-based\nestimation method that delivers PAC-guarantees for the K-terminal reliability\nproblem. Then, we test methods' performance using various benchmark systems. We\nhighlight the range of application of algorithms and provide the foundation for\nfuture resilience engineering as it increasingly necessitates methods for\nuncertainty quantification in complex systems. \n\n"}
{"id": "1806.01305", "contents": "Title: Towards the Practical Application of Near-Term Quantum Computers in\n  Quantum Chemistry Simulations: A Problem Decomposition Approach Abstract: With the aim of establishing a framework to efficiently perform the practical\napplication of quantum chemistry simulation on near-term quantum devices, we\nenvision a hybrid quantum--classical framework for leveraging problem\ndecomposition (PD) techniques in quantum chemistry. Specifically, we use PD\ntechniques to decompose a target molecular system into smaller subsystems\nrequiring fewer computational resources. In our framework, there are two levels\nof hybridization. At the first level, we use a classical algorithm to decompose\na target molecule into subsystems, and utilize a quantum algorithm to simulate\nthe quantum nature of the subsystems. The second level is in the quantum\nalgorithm. We consider the quantum--classical variational algorithm that\niterates between an expectation estimation using a quantum device and a\nparameter optimization using a classical device. We investigate three popular\nPD techniques for our hybrid approach: the fragment molecular-orbital (FMO)\nmethod, the divide-and-conquer (DC) technique, and the density matrix embedding\ntheory (DMET). We examine the efficacy of these techniques in correctly\ndifferentiating conformations of simple alkane molecules. In particular, we\nconsider the ratio between the number of qubits for PD and that of the full\nsystem; the mean absolute deviation; and the Pearson correlation coefficient\nand Spearman's rank correlation coefficient. Sampling error is introduced when\nexpectation values are measured on the quantum device. Therefore, we study how\nthis error affects the predictive performance of PD techniques. The present\nstudy is our first step to opening up the possibility of using quantum\nchemistry simulations at a scale close to the size of molecules relevant to\nindustry on near-term quantum hardware. \n\n"}
{"id": "1806.01804", "contents": "Title: Tree Path Majority Data Structures Abstract: We present the first solution to $\\tau$-majorities on tree paths. Given a\ntree of $n$ nodes, each with a label from $[1..\\sigma]$, and a fixed threshold\n$0<\\tau<1$, such a query gives two nodes $u$ and $v$ and asks for all the\nlabels that appear more than $\\tau \\cdot |P_{uv}|$ times in the path $P_{uv}$\nfrom $u$ to $v$, where $|P_{uv}|$ denotes the number of nodes in $P_{uv}$. Note\nthat the answer to any query is of size up to $1/\\tau$. On a $w$-bit RAM, we\nobtain a linear-space data structure with $O((1/\\tau)\\log^* n \\log\\log_w\n\\sigma)$ query time. For any $\\kappa > 1$, we can also build a structure that\nuses $O(n\\log^{[\\kappa]} n)$ space, where $\\log^{[\\kappa]} n$ denotes the\nfunction that applies logarithm $\\kappa$ times to $n$, and answers queries in\ntime $O((1/\\tau)\\log\\log_w \\sigma)$. The construction time of both structures\nis $O(n\\log n)$. We also describe two succinct-space solutions with the same\nquery time of the linear-space structure. One uses $2nH + 4n + o(n)(H+1)$ bits,\nwhere $H \\le \\lg\\sigma$ is the entropy of the label distribution, and can be\nbuilt in $O(n\\log n)$ time. The other uses $nH + O(n) + o(nH)$ bits and is\nbuilt in $O(n\\log n)$ time w.h.p. \n\n"}
{"id": "1806.02338", "contents": "Title: Towards Dependability Metrics for Neural Networks Abstract: Artificial neural networks (NN) are instrumental in realizing\nhighly-automated driving functionality. An overarching challenge is to identify\nbest safety engineering practices for NN and other learning-enabled components.\nIn particular, there is an urgent need for an adequate set of metrics for\nmeasuring all-important NN dependability attributes. We address this challenge\nby proposing a number of NN-specific and efficiently computable metrics for\nmeasuring NN dependability attributes including robustness, interpretability,\ncompleteness, and correctness. \n\n"}
{"id": "1806.04256", "contents": "Title: Pseudorandom Generators for Width-3 Branching Programs Abstract: We construct pseudorandom generators of seed length $\\tilde{O}(\\log(n)\\cdot\n\\log(1/\\epsilon))$ that $\\epsilon$-fool ordered read-once branching programs\n(ROBPs) of width $3$ and length $n$. For unordered ROBPs, we construct\npseudorandom generators with seed length $\\tilde{O}(\\log(n) \\cdot\n\\mathrm{poly}(1/\\epsilon))$. This is the first improvement for pseudorandom\ngenerators fooling width $3$ ROBPs since the work of Nisan [Combinatorica,\n1992].\n  Our constructions are based on the `iterated milder restrictions' approach of\nGopalan et al. [FOCS, 2012] (which further extends the Ajtai-Wigderson\nframework [FOCS, 1985]), combined with the INW-generator [STOC, 1994] at the\nlast step (as analyzed by Braverman et al. [SICOMP, 2014]). For the unordered\ncase, we combine iterated milder restrictions with the generator of\nChattopadhyay et al. [CCC, 2018].\n  Two conceptual ideas that play an important role in our analysis are: (1) A\nrelabeling technique allowing us to analyze a relabeled version of the given\nbranching program, which turns out to be much easier. (2) Treating the number\nof colliding layers in a branching program as a progress measure and showing\nthat it reduces significantly under pseudorandom restrictions.\n  In addition, we achieve nearly optimal seed-length\n$\\tilde{O}(\\log(n/\\epsilon))$ for the classes of: (1) read-once polynomials on\n$n$ variables, (2) locally-monotone ROBPs of length $n$ and width $3$\n(generalizing read-once CNFs and DNFs), and (3) constant-width ROBPs of length\n$n$ having a layer of width $2$ in every consecutive $\\mathrm{poly}\\log(n)$\nlayers. \n\n"}
{"id": "1806.04310", "contents": "Title: MISSION: Ultra Large-Scale Feature Selection using Count-Sketches Abstract: Feature selection is an important challenge in machine learning. It plays a\ncrucial role in the explainability of machine-driven decisions that are rapidly\npermeating throughout modern society. Unfortunately, the explosion in the size\nand dimensionality of real-world datasets poses a severe challenge to standard\nfeature selection algorithms. Today, it is not uncommon for datasets to have\nbillions of dimensions. At such scale, even storing the feature vector is\nimpossible, causing most existing feature selection methods to fail.\nWorkarounds like feature hashing, a standard approach to large-scale machine\nlearning, helps with the computational feasibility, but at the cost of losing\nthe interpretability of features. In this paper, we present MISSION, a novel\nframework for ultra large-scale feature selection that performs stochastic\ngradient descent while maintaining an efficient representation of the features\nin memory using a Count-Sketch data structure. MISSION retains the simplicity\nof feature hashing without sacrificing the interpretability of the features\nwhile using only O(log^2(p)) working memory. We demonstrate that MISSION\naccurately and efficiently performs feature selection on real-world,\nlarge-scale datasets with billions of dimensions. \n\n"}
{"id": "1806.04484", "contents": "Title: A Fourier-Analytic Approach for the Discrepancy of Random Set Systems Abstract: One of the prominent open problems in combinatorics is the discrepancy of set\nsystems where each element lies in at most $t$ sets. The Beck-Fiala conjecture\nsuggests that the right bound is $O(\\sqrt{t})$, but for three decades the only\nknown bound not depending on the size of the set system has been $O(t)$.\nArguably we currently lack techniques for breaking that barrier.\n  In this paper we introduce discrepancy bounds based on Fourier analysis. We\ndemonstrate our method on random set systems. Suppose one has $n$ elements and\n$m$ sets containing each element independently with probability $p$. We prove\nthat in the regime of $n \\geq \\Theta(m^2\\log(m))$, the discrepancy is at most\n$1$ with high probability. Previously, a result of Ezra and Lovett gave a bound\nof $O(1)$ under the stricter assumption that $n \\gg m^t$. \n\n"}
{"id": "1806.05105", "contents": "Title: Stability and complexity of mixed discriminants Abstract: We show that the mixed discriminant of $n$ positive semidefinite $n \\times n$\nreal symmetric matrices can be approximated within a relative error $\\epsilon\n>0$ in quasi-polynomial $n^{O(\\ln n -\\ln \\epsilon)}$ time, provided the\ndistance of each matrix to the identity matrix in the operator norm does not\nexceed some absolute constant $\\gamma_0 >0$. We deduce a similar result for the\nmixed discriminant of doubly stochastic $n$-tuples of matrices from the Marcus\n- Spielman - Srivastava bound on the roots of the mixed characteristic\npolynomial. Finally, we construct a quasi-polynomial algorithm for\napproximating the sum of $m$-th powers of principal minors of a matrix,\nprovided the operator norm of the matrix is strictly less than 1. As is shown\nby Gurvits, for $m=2$ the problem is $\\#P$-hard and covers the problem of\ncomputing the mixed discriminant of positive semidefinite matrices of rank 2. \n\n"}
{"id": "1806.05701", "contents": "Title: Computation-Aware Data Aggregation Abstract: Data aggregation is a fundamental primitive in distributed computing wherein\na network computes a function of every nodes' input. However, while compute\ntime is non-negligible in modern systems, standard models of distributed\ncomputing do not take compute time into account. Rather, most distributed\nmodels of computation only explicitly consider communication time.\n  In this paper, we introduce a model of distributed computation that considers\n\\emph{both} computation and communication so as to give a theoretical treatment\nof data aggregation. We study both the structure of and how to compute the\nfastest data aggregation schedule in this model. As our first result, we give a\npolynomial-time algorithm that computes the optimal schedule when the input\nnetwork is a complete graph. Moreover, since one may want to aggregate data\nover a pre-existing network, we also study data aggregation scheduling on\narbitrary graphs. We demonstrate that this problem on arbitrary graphs is hard\nto approximate within a multiplicative $1.5$ factor. Finally, we give an\n$O(\\log n \\cdot \\log \\frac{\\mathrm{OPT}}{t_m})$-approximation algorithm for\nthis problem on arbitrary graphs, where $n$ is the number of nodes and\n$\\mathrm{OPT}$ is the length of the optimal schedule. \n\n"}
{"id": "1806.06100", "contents": "Title: The Limits of Post-Selection Generalization Abstract: While statistics and machine learning offers numerous methods for ensuring\ngeneralization, these methods often fail in the presence of adaptivity---the\ncommon practice in which the choice of analysis depends on previous\ninteractions with the same dataset. A recent line of work has introduced\npowerful, general purpose algorithms that ensure post hoc generalization (also\ncalled robust or post-selection generalization), which says that, given the\noutput of the algorithm, it is hard to find any statistic for which the data\ndiffers significantly from the population it came from.\n  In this work we show several limitations on the power of algorithms\nsatisfying post hoc generalization. First, we show a tight lower bound on the\nerror of any algorithm that satisfies post hoc generalization and answers\nadaptively chosen statistical queries, showing a strong barrier to progress in\npost selection data analysis. Second, we show that post hoc generalization is\nnot closed under composition, despite many examples of such algorithms\nexhibiting strong composition properties. \n\n"}
{"id": "1806.06116", "contents": "Title: Stochastic WaveNet: A Generative Latent Variable Model for Sequential\n  Data Abstract: How to model distribution of sequential data, including but not limited to\nspeech and human motions, is an important ongoing research problem. It has been\ndemonstrated that model capacity can be significantly enhanced by introducing\nstochastic latent variables in the hidden states of recurrent neural networks.\nSimultaneously, WaveNet, equipped with dilated convolutions, achieves\nastonishing empirical performance in natural speech generation task. In this\npaper, we combine the ideas from both stochastic latent variables and dilated\nconvolutions, and propose a new architecture to model sequential data, termed\nas Stochastic WaveNet, where stochastic latent variables are injected into the\nWaveNet structure. We argue that Stochastic WaveNet enjoys powerful\ndistribution modeling capacity and the advantage of parallel training from\ndilated convolutions. In order to efficiently infer the posterior distribution\nof the latent variables, a novel inference network structure is designed based\non the characteristics of WaveNet architecture. State-of-the-art performances\non benchmark datasets are obtained by Stochastic WaveNet on natural speech\nmodeling and high quality human handwriting samples can be generated as well. \n\n"}
{"id": "1806.06173", "contents": "Title: On the Complexity of Detecting Convexity over a Box Abstract: It has recently been shown that the problem of testing global convexity of\npolynomials of degree four is {strongly} NP-hard, answering an open question of\nN.Z. Shor. This result is minimal in the degree of the polynomial when global\nconvexity is of concern. In a number of applications however, one is interested\nin testing convexity only over a compact region, most commonly a box (i.e.,\nhyper-rectangle). In this paper, we show that this problem is also strongly\nNP-hard, in fact for polynomials of degree as low as three. This result is\nminimal in the degree of the polynomial and in some sense justifies why\nconvexity detection in nonlinear optimization solvers is limited to quadratic\nfunctions or functions with special structure. As a byproduct, our proof shows\nthat the problem of testing whether all matrices in an interval family are\npositive semidefinite is strongly NP-hard. This problem, which was previously\nshown to be (weakly) NP-hard by Nemirovski, is of independent interest in the\ntheory of robust control. \n\n"}
{"id": "1806.06290", "contents": "Title: Average-Case Lower Bounds and Satisfiability Algorithms for Small\n  Threshold Circuits Abstract: $ \\newcommand{\\cclass}[1]{{\\normalfont\\textsf{##1}}} $We show average-case\nlower bounds for explicit Boolean functions against bounded-depth threshold\ncircuits with a superlinear number of wires. We show that for each integer $d >\n1$, there is a constant $\\varepsilon_d > 0$ such that the Parity function on\n$n$ bits has correlation at most $n^{-\\varepsilon_d}$ with depth-$d$ threshold\ncircuits which have at most $n^{1+\\varepsilon_d}$ wires, and the Generalized\nAndreev function on $n$ bits has correlation at most\n$\\exp(-{n^{\\varepsilon_d}})$ with depth-$d$ threshold circuits which have at\nmost $n^{1+\\varepsilon_d}$ wires. Previously, only worst-case lower bounds in\nthis setting were known (Impagliazzo, Paturi, and Saks (SICOMP 1997)).\n  We use our ideas to make progress on several related questions. We give\nsatisfiability algorithms beating brute force search for depth-$d$ threshold\ncircuits with a superlinear number of wires. These are the first such\nalgorithms for depth greater than 2. We also show that Parity on $n$ bits\ncannot be computed by polynomial-size $\\textsf{AC}^0$ circuits with $n^{o(1)}$\ngeneral threshold gates. Previously no lower bound for Parity in this setting\ncould handle more than $\\log(n)$ gates. This result also implies\nsubexponential-time learning algorithms for $\\textsf{AC}^0$ with $n^{o(1)}$\nthreshold gates under the uniform distribution. In addition, we give almost\noptimal bounds for the number of gates in a depth-$d$ threshold circuit\ncomputing Parity on average, and show average-case lower bounds for threshold\nformulas of any depth.\n  Our techniques include adaptive random restrictions, anti-concentration and\nthe structural theory of linear threshold functions, and bounded-read Chernoff\nbounds. \n\n"}
{"id": "1806.06365", "contents": "Title: How Could Polyhedral Theory Harness Deep Learning? Abstract: The holy grail of deep learning is to come up with an automatic method to\ndesign optimal architectures for different applications. In other words, how\ncan we effectively dimension and organize neurons along the network layers\nbased on the computational resources, input size, and amount of training data?\nWe outline promising research directions based on polyhedral theory and\nmixed-integer representability that may offer an analytical approach to this\nquestion, in contrast to the empirical techniques often employed. \n\n"}
{"id": "1806.06996", "contents": "Title: Optimization over Nonnegative and Convex Polynomials With and Without\n  Semidefinite Programming Abstract: The problem of optimizing over the cone of nonnegative polynomials is a\nfundamental problem in computational mathematics, with applications to\npolynomial optimization, control, machine learning, game theory, and\ncombinatorics, among others. A number of breakthrough papers in the early 2000s\nshowed that this problem, long thought to be out of reach, could be tackled by\nusing sum of squares programming. This technique however has proved to be\nexpensive for large-scale problems, as it involves solving large semidefinite\nprograms (SDPs).\n  In the first part of this thesis, we present two methods for approximately\nsolving large-scale sum of squares programs that dispense altogether with\nsemidefinite programming and only involve solving a sequence of linear or\nsecond order cone programs generated in an adaptive fashion. We then focus on\nthe problem of finding tight lower bounds on polynomial optimization problems\n(POPs), a fundamental task in this area that is most commonly handled through\nthe use of SDP-based sum of squares hierarchies (e.g., due to Lasserre and\nParrilo). In contrast to previous approaches, we provide the first theoretical\nframework for constructing converging hierarchies of lower bounds on POPs whose\ncomputation simply requires the ability to multiply certain fixed polynomials\ntogether and to check nonnegativity of the coefficients of their product.\n  In the second part of this thesis, we focus on the theory and applications of\nthe problem of optimizing over convex polynomials, a subcase of the problem of\noptimizing over nonnegative polynomials. (See manuscript for the rest of the\nabstract.) \n\n"}
{"id": "1806.07385", "contents": "Title: Detecting and interpreting myocardial infarction using fully\n  convolutional neural networks Abstract: Objective: We aim to provide an algorithm for the detection of myocardial\ninfarction that operates directly on ECG data without any preprocessing and to\ninvestigate its decision criteria. Approach: We train an ensemble of fully\nconvolutional neural networks on the PTB ECG dataset and apply state-of-the-art\nattribution methods. Main results: Our classifier reaches 93.3% sensitivity and\n89.7% specificity evaluated using 10-fold cross-validation with sampling based\non patients. The presented method outperforms state-of-the-art approaches and\nreaches the performance level of human cardiologists for detection of\nmyocardial infarction. We are able to discriminate channel-specific regions\nthat contribute most significantly to the neural network's decision.\nInterestingly, the network's decision is influenced by signs also recognized by\nhuman cardiologists as indicative of myocardial infarction. Significance: Our\nresults demonstrate the high prospects of algorithmic ECG analysis for future\nclinical applications considering both its quantitative performance as well as\nthe possibility of assessing decision criteria on a per-example basis, which\nenhances the comprehensibility of the approach. \n\n"}
{"id": "1806.08267", "contents": "Title: Complex Gated Recurrent Neural Networks Abstract: Complex numbers have long been favoured for digital signal processing, yet\ncomplex representations rarely appear in deep learning architectures. RNNs,\nwidely used to process time series and sequence information, could greatly\nbenefit from complex representations. We present a novel complex gated\nrecurrent cell, which is a hybrid cell combining complex-valued and\nnorm-preserving state transitions with a gating mechanism. The resulting RNN\nexhibits excellent stability and convergence properties and performs\ncompetitively on the synthetic memory and adding task, as well as on the\nreal-world tasks of human motion prediction. \n\n"}
{"id": "1806.08295", "contents": "Title: How Many Random Seeds? Statistical Power Analysis in Deep Reinforcement\n  Learning Experiments Abstract: Consistently checking the statistical significance of experimental results is\none of the mandatory methodological steps to address the so-called\n\"reproducibility crisis\" in deep reinforcement learning. In this tutorial\npaper, we explain how the number of random seeds relates to the probabilities\nof statistical errors. For both the t-test and the bootstrap confidence\ninterval test, we recall theoretical guidelines to determine the number of\nrandom seeds one should use to provide a statistically significant comparison\nof the performance of two algorithms. Finally, we discuss the influence of\ndeviations from the assumptions usually made by statistical tests. We show that\nthey can lead to inaccurate evaluations of statistical errors and provide\nguidelines to counter these negative effects. We make our code available to\nperform the tests. \n\n"}
{"id": "1806.09460", "contents": "Title: A Tour of Reinforcement Learning: The View from Continuous Control Abstract: This manuscript surveys reinforcement learning from the perspective of\noptimization and control with a focus on continuous control applications. It\nsurveys the general formulation, terminology, and typical experimental\nimplementations of reinforcement learning and reviews competing solution\nparadigms. In order to compare the relative merits of various techniques, this\nsurvey presents a case study of the Linear Quadratic Regulator (LQR) with\nunknown dynamics, perhaps the simplest and best-studied problem in optimal\ncontrol. The manuscript describes how merging techniques from learning theory\nand control can provide non-asymptotic characterizations of LQR performance and\nshows that these characterizations tend to match experimental behavior. In\nturn, when revisiting more complex applications, many of the observed phenomena\nin LQR persist. In particular, theory and experiment demonstrate the role and\nimportance of models and the cost of generality in reinforcement learning\nalgorithms. This survey concludes with a discussion of some of the challenges\nin designing learning systems that safely and reliably interact with complex\nand uncertain environments and how tools from reinforcement learning and\ncontrol might be combined to approach these challenges. \n\n"}
{"id": "1806.09730", "contents": "Title: Analysis of Invariance and Robustness via Invertibility of ReLU-Networks Abstract: Studying the invertibility of deep neural networks (DNNs) provides a\nprincipled approach to better understand the behavior of these powerful models.\nDespite being a promising diagnostic tool, a consistent theory on their\ninvertibility is still lacking. We derive a theoretically motivated approach to\nexplore the preimages of ReLU-layers and mechanisms affecting the stability of\nthe inverse. Using the developed theory, we numerically show how this approach\nuncovers characteristic properties of the network. \n\n"}
{"id": "1806.10586", "contents": "Title: Approximability of Discriminators Implies Diversity in GANs Abstract: While Generative Adversarial Networks (GANs) have empirically produced\nimpressive results on learning complex real-world distributions, recent works\nhave shown that they suffer from lack of diversity or mode collapse. The\ntheoretical work of Arora et al. suggests a dilemma about GANs' statistical\nproperties: powerful discriminators cause overfitting, whereas weak\ndiscriminators cannot detect mode collapse.\n  By contrast, we show in this paper that GANs can in principle learn\ndistributions in Wasserstein distance (or KL-divergence in many cases) with\npolynomial sample complexity, if the discriminator class has strong\ndistinguishing power against the particular generator class (instead of against\nall possible generators). For various generator classes such as mixture of\nGaussians, exponential families, and invertible and injective neural networks\ngenerators, we design corresponding discriminators (which are often neural nets\nof specific architectures) such that the Integral Probability Metric (IPM)\ninduced by the discriminators can provably approximate the Wasserstein distance\nand/or KL-divergence. This implies that if the training is successful, then the\nlearned distribution is close to the true distribution in Wasserstein distance\nor KL divergence, and thus cannot drop modes. Our preliminary experiments show\nthat on synthetic datasets the test IPM is well correlated with KL divergence\nor the Wasserstein distance, indicating that the lack of diversity in GANs may\nbe caused by the sub-optimality in optimization instead of statistical\ninefficiency. \n\n"}
{"id": "1807.01247", "contents": "Title: Ortho-polygon Visibility Representations of 3-connected 1-plane Graphs Abstract: An ortho-polygon visibility representation $\\Gamma$ of a $1$-plane graph $G$\n(OPVR of $G$) is an embedding preserving drawing that maps each vertex of $G$\nto a distinct orthogonal polygon and each edge of $G$ to a vertical or\nhorizontal visibility between its end-vertices. The representation $\\Gamma$ has\nvertex complexity $k$ if every polygon of $\\Gamma$ has at most $k$ reflex\ncorners. It is known that $3$-connected $1$-plane graphs admit an OPVR with\nvertex complexity at most twelve, while vertex complexity at least two may be\nrequired in some cases. In this paper, we reduce this gap by showing that\nvertex complexity five is always sufficient, while vertex complexity four may\nbe required in some cases. These results are based on the study of the\ncombinatorial properties of the B-, T-, and W-configurations in $3$-connected\n$1$-plane graphs. An implication of the upper bound is the existence of a\n$\\tilde{O}(n^\\frac{10}{7})$-time drawing algorithm that computes an OPVR of an\n$n$-vertex $3$-connected $1$-plane graph on an integer grid of size $O(n)\n\\times O(n)$ and with vertex complexity at most five. \n\n"}
{"id": "1807.02571", "contents": "Title: Leveraging Well-Conditioned Bases: Streaming \\& Distributed Summaries in\n  Minkowski $p$-Norms Abstract: Work on approximate linear algebra has led to efficient distributed and\nstreaming algorithms for problems such as approximate matrix multiplication,\nlow rank approximation, and regression, primarily for the Euclidean norm\n$\\ell_2$. We study other $\\ell_p$ norms, which are more robust for $p < 2$, and\ncan be used to find outliers for $p > 2$. Unlike previous algorithms for such\nnorms, we give algorithms that are (1) deterministic, (2) work simultaneously\nfor every $p \\geq 1$, including $p = \\infty$, and (3) can be implemented in\nboth distributed and streaming environments. We apply our results to\n$\\ell_p$-regression, entrywise $\\ell_1$-low rank approximation, and approximate\nmatrix multiplication. \n\n"}
{"id": "1807.02839", "contents": "Title: Hierarchical stochastic graphlet embedding for graph-based pattern\n  recognition Abstract: Despite being very successful within the pattern recognition and machine\nlearning community, graph-based methods are often unusable because of the lack\nof mathematical operations defined in graph domain. Graph embedding, which maps\ngraphs to a vectorial space, has been proposed as a way to tackle these\ndifficulties enabling the use of standard machine learning techniques. However,\nit is well known that graph embedding functions usually suffer from the loss of\nstructural information. In this paper, we consider the hierarchical structure\nof a graph as a way to mitigate this loss of information. The hierarchical\nstructure is constructed by topologically clustering the graph nodes, and\nconsidering each cluster as a node in the upper hierarchical level. Once this\nhierarchical structure is constructed, we consider several configurations to\ndefine the mapping into a vector space given a classical graph embedding, in\nparticular, we propose to make use of the Stochastic Graphlet Embedding (SGE).\nBroadly speaking, SGE produces a distribution of uniformly sampled low to high\norder graphlets as a way to embed graphs into the vector space. In what\nfollows, the coarse-to-fine structure of a graph hierarchy and the statistics\nfetched by the SGE complements each other and includes important structural\ninformation with varied contexts. Altogether, these two techniques\nsubstantially cope with the usual information loss involved in graph embedding\ntechniques, obtaining a more robust graph representation. This fact has been\ncorroborated through a detailed experimental evaluation on various benchmark\ngraph datasets, where we outperform the state-of-the-art methods. \n\n"}
{"id": "1807.03095", "contents": "Title: Mammography Assessment using Multi-Scale Deep Classifiers Abstract: Applying deep learning methods to mammography assessment has remained a\nchallenging topic. Dense noise with sparse expressions, mega-pixel raw data\nresolution, lack of diverse examples have all been factors affecting\nperformance. The lack of pixel-level ground truths have especially limited\nsegmentation methods in pushing beyond approximately bounding regions. We\npropose a classification approach grounded in high performance tissue\nassessment as an alternative to all-in-one localization and assessment models\nthat is also capable of pinpointing the causal pixels. First, the objective of\nthe mammography assessment task is formalized in the context of local tissue\nclassifiers. Then, the accuracy of a convolutional neural net is evaluated on\nclassifying patches of tissue with suspicious findings at varying scales, where\nhighest obtained AUC is above $0.9$. The local evaluations of one such expert\ntissue classifier is used to augment the results of a heatmap regression model\nand additionally recover the exact causal regions at high resolution as a\nsaliency image suitable for clinical settings. \n\n"}
{"id": "1807.03247", "contents": "Title: An Intriguing Failing of Convolutional Neural Networks and the CoordConv\n  Solution Abstract: Few ideas have enjoyed as large an impact on deep learning as convolution.\nFor any problem involving pixels or spatial representations, common intuition\nholds that convolutional neural networks may be appropriate. In this paper we\nshow a striking counterexample to this intuition via the seemingly trivial\ncoordinate transform problem, which simply requires learning a mapping between\ncoordinates in (x,y) Cartesian space and one-hot pixel space. Although\nconvolutional networks would seem appropriate for this task, we show that they\nfail spectacularly. We demonstrate and carefully analyze the failure first on a\ntoy problem, at which point a simple fix becomes obvious. We call this solution\nCoordConv, which works by giving convolution access to its own input\ncoordinates through the use of extra coordinate channels. Without sacrificing\nthe computational and parametric efficiency of ordinary convolution, CoordConv\nallows networks to learn either complete translation invariance or varying\ndegrees of translation dependence, as required by the end task. CoordConv\nsolves the coordinate transform problem with perfect generalization and 150\ntimes faster with 10--100 times fewer parameters than convolution. This stark\ncontrast raises the question: to what extent has this inability of convolution\npersisted insidiously inside other tasks, subtly hampering performance from\nwithin? A complete answer to this question will require further investigation,\nbut we show preliminary evidence that swapping convolution for CoordConv can\nimprove models on a diverse set of tasks. Using CoordConv in a GAN produced\nless mode collapse as the transform between high-level spatial latents and\npixels becomes easier to learn. A Faster R-CNN detection model trained on MNIST\nshowed 24% better IOU when using CoordConv, and in the RL domain agents playing\nAtari games benefit significantly from the use of CoordConv layers. \n\n"}
{"id": "1807.03465", "contents": "Title: The Kannan-Lov\\'asz-Simonovits Conjecture Abstract: The Kannan-Lov\\'asz-Simonovits conjecture says that the Cheeger constant of\nany logconcave density is achieved to within a universal, dimension-independent\nconstant factor by a hyperplane-induced subset. Here we survey the origin and\nconsequences of the conjecture (in geometry, probability, information theory\nand algorithms) as well as recent progress resulting in the current best\nbounds. The conjecture has lead to several techniques of general interest. \n\n"}
{"id": "1807.04801", "contents": "Title: Practical Obstacles to Deploying Active Learning Abstract: Active learning (AL) is a widely-used training strategy for maximizing\npredictive performance subject to a fixed annotation budget. In AL one\niteratively selects training examples for annotation, often those for which the\ncurrent model is most uncertain (by some measure). The hope is that active\nsampling leads to better performance than would be achieved under independent\nand identically distributed (i.i.d.) random samples. While AL has shown promise\nin retrospective evaluations, these studies often ignore practical obstacles to\nits use. In this paper we show that while AL may provide benefits when used\nwith specific models and for particular domains, the benefits of current\napproaches do not generalize reliably across models and tasks. This is\nproblematic because in practice one does not have the opportunity to explore\nand compare alternative AL strategies. Moreover, AL couples the training\ndataset with the model used to guide its acquisition. We find that subsequently\ntraining a successor model with an actively-acquired dataset does not\nconsistently outperform training on i.i.d. sampled data. Our findings raise the\nquestion of whether the downsides inherent to AL are worth the modest and\ninconsistent performance gains it tends to afford. \n\n"}
{"id": "1807.05076", "contents": "Title: Metalearning with Hebbian Fast Weights Abstract: We unify recent neural approaches to one-shot learning with older ideas of\nassociative memory in a model for metalearning. Our model learns jointly to\nrepresent data and to bind class labels to representations in a single shot. It\nbuilds representations via slow weights, learned across tasks through SGD,\nwhile fast weights constructed by a Hebbian learning rule implement one-shot\nbinding for each new task. On the Omniglot, Mini-ImageNet, and Penn Treebank\none-shot learning benchmarks, our model achieves state-of-the-art results. \n\n"}
{"id": "1807.05164", "contents": "Title: On the Number of Circuits in Regular Matroids (with Connections to\n  Lattices and Codes) Abstract: We show that for any regular matroid on $m$ elements and any $\\alpha \\geq 1$,\nthe number of $\\alpha$-minimum circuits, or circuits whose size is at most an\n$\\alpha$-multiple of the minimum size of a circuit in the matroid is bounded by\n$m^{O(\\alpha^2)}$. This generalizes a result of Karger for the number of\n$\\alpha$-minimum cuts in a graph. As a consequence, we obtain similar bounds on\nthe number of $\\alpha$-shortest vectors in \"totally unimodular\" lattices and on\nthe number of $\\alpha$-minimum weight codewords in \"regular\" codes. \n\n"}
{"id": "1807.05307", "contents": "Title: How Do Classifiers Induce Agents To Invest Effort Strategically? Abstract: Algorithms are often used to produce decision-making rules that classify or\nevaluate individuals. When these individuals have incentives to be classified a\ncertain way, they may behave strategically to influence their outcomes. We\ndevelop a model for how strategic agents can invest effort in order to change\nthe outcomes they receive, and we give a tight characterization of when such\nagents can be incentivized to invest specified forms of effort into improving\ntheir outcomes as opposed to \"gaming\" the classifier. We show that whenever any\n\"reasonable\" mechanism can do so, a simple linear mechanism suffices. \n\n"}
{"id": "1807.05377", "contents": "Title: SAT encodings for sorting networks, single-exception sorting networks\n  and $\\epsilon-$halvers Abstract: Sorting networks are oblivious sorting algorithms with many practical\napplications and rich theoretical properties. Propositional encodings of\nsorting networks are a key tool for proving concrete bounds on the minimum\nnumber of comparators or depth (number of parallel steps) of sorting networks.\nIn this paper, we present new SAT encodings that reduce the number of variables\nand clauses of the sorting constraint of optimality problems. Moreover, the\nproposed SAT encodings can be applied to a broader class of problems, such as\nthe search of optimal single-exception sorting networks and $\\epsilon-$halvers.\nWe obtain optimality results for single-exception sorting networks on $n \\le\n10$ inputs. \n\n"}
{"id": "1807.05532", "contents": "Title: Deterministic (1/2 + {\\epsilon})-Approximation for Submodular\n  Maximization over a Matroid Abstract: We study the problem of maximizing a monotone submodular function subject to\na matroid constraint and present a deterministic algorithm that achieves (1/2 +\n{\\epsilon})-approximation for the problem. This algorithm is the first\ndeterministic algorithm known to improve over the 1/2-approximation ratio of\nthe classical greedy algorithm proved by Nemhauser, Wolsely and Fisher in 1978. \n\n"}
{"id": "1807.05827", "contents": "Title: Remember and Forget for Experience Replay Abstract: Experience replay (ER) is a fundamental component of off-policy deep\nreinforcement learning (RL). ER recalls experiences from past iterations to\ncompute gradient estimates for the current policy, increasing data-efficiency.\nHowever, the accuracy of such updates may deteriorate when the policy diverges\nfrom past behaviors and can undermine the performance of ER. Many algorithms\nmitigate this issue by tuning hyper-parameters to slow down policy changes. An\nalternative is to actively enforce the similarity between policy and the\nexperiences in the replay memory. We introduce Remember and Forget Experience\nReplay (ReF-ER), a novel method that can enhance RL algorithms with\nparameterized policies. ReF-ER (1) skips gradients computed from experiences\nthat are too unlikely with the current policy and (2) regulates policy changes\nwithin a trust region of the replayed behaviors. We couple ReF-ER with\nQ-learning, deterministic policy gradient and off-policy gradient methods. We\nfind that ReF-ER consistently improves the performance of continuous-action,\noff-policy RL on fully observable benchmarks and partially observable flow\ncontrol problems. \n\n"}
{"id": "1807.05957", "contents": "Title: Finding a marked node on any graph by continuous-time quantum walk Abstract: Spatial search by discrete-time quantum walk can find a marked node on any\nergodic, reversible Markov chain $P$ quadratically faster than its classical\ncounterpart, i.e.\\ in a time that is in the square root of the hitting time of\n$P$. However, in the framework of continuous-time quantum walks, it was\npreviously unknown whether such general speed-up is possible. In fact, in this\nframework, the widely used quantum algorithm by Childs and Goldstone fails to\nachieve such a speedup. Furthermore, it is not clear how to apply this\nalgorithm for searching any Markov chain $P$. In this article, we aim to\nreconcile the apparent differences between the running times of spatial search\nalgorithms in these two frameworks. We first present a modified version of the\nChilds and Goldstone algorithm which can search for a marked element for any\nergodic, reversible $P$ by performing a quantum walk on its edges. Although\nthis approach improves the algorithmic running time for several instances, it\ncannot provide a generic quadratic speedup for any $P$. Secondly, using the\nframework of interpolated Markov chains, we provide a new spatial search\nalgorithm by continuous-time quantum walk which can find a marked node on any\n$P$ in the square root of the classical hitting time. In the scenario where\nmultiple nodes are marked, the algorithmic running time scales as the square\nroot of a quantity known as the extended hitting time. Our results establish a\nnovel connection between discrete-time and continuous-time quantum walks and\ncan be used to develop a number of Markov chain-based quantum algorithms. \n\n"}
{"id": "1807.06577", "contents": "Title: Fisher zeros and correlation decay in the Ising model Abstract: We study the complex zeros of the partition function of the Ising model,\nviewed as a polynomial in the \"interaction parameter\"; these are known as\nFisher zeros in light of their introduction by Fisher in 1965. While the zeros\nof the partition function as a polynomial in the \"field\" parameter have been\nextensively studied since the classical work of Lee and Yang, comparatively\nlittle is known about Fisher zeros for general graphs. Our main result shows\nthat the zero-field Ising model has no Fisher zeros in a complex neighborhood\nof the entire region of parameters where the model exhibits correlation decay.\nIn addition to shedding light on Fisher zeros themselves, this result also\nestablishes a formal connection between two distinct notions of phase\ntransition for the Ising model: the absence of complex zeros (analyticity of\nthe free energy, or the logarithm of the partition function) and decay of\ncorrelations with distance. We also discuss the consequences of our result for\nefficient deterministic approximation of the partition function. Our proof\nrelies heavily on algorithmic techniques, notably Weitz's self-avoiding walk\ntree, and as such belongs to a growing body of work that uses algorithmic\nmethods to resolve classical questions in statistical physics. \n\n"}
{"id": "1807.06686", "contents": "Title: Supermodular Locality Sensitive Hashes Abstract: In this work, we show deep connections between Locality Sensitive Hashability\nand submodular analysis. We show that the LSHablility of the most commonly\nanalyzed set similarities is in one-to-one correspondance with the\nsupermodularity of these similarities when taken with respect to the symmetric\ndifference of their arguments. We find that the supermodularity of equivalent\nLSHable similarities can be dependent on the set encoding. While monotonicity\nand supermodularity does not imply the metric condition necessary for\nsupermodularity, this condition is guaranteed for the more restricted class of\nsupermodular Hamming similarities that we introduce. We show moreover that LSH\npreserving transformations are also supermodular-preserving, yielding a way to\ngenerate families of similarities both LSHable and supermodular. Finally, we\nshow that even the more restricted family of cardinality-based supermodular\nHamming similarities presents promising aspects for the study of the link\nbetween LSHability and supermodularity. We hope that the several bridges that\nwe introduce between LSHability and supermodularity paves the way to a better\nunderstanding both of supermodular analysis and LSHability, notably in the\ncontext of large-scale supermodular optimization. \n\n"}
{"id": "1807.07282", "contents": "Title: Anomaly Detection for Water Treatment System based on Neural Network\n  with Automatic Architecture Optimization Abstract: We continue to develop our neural network (NN) based forecasting approach to\nanomaly detection (AD) using the Secure Water Treatment (SWaT) industrial\ncontrol system (ICS) testbed dataset. We propose genetic algorithms (GA) to\nfind the best NN architecture for a given dataset, using the NAB metric to\nassess the quality of different architectures. The drawbacks of the F1-metric\nare analyzed. Several techniques are proposed to improve the quality of AD:\nexponentially weighted smoothing, mean p-powered error measure, individual\nerror weight for each variable, disjoint prediction windows. Based on the\ntechniques used, an approach to anomaly interpretation is introduced. \n\n"}
{"id": "1807.07333", "contents": "Title: Sequence to Logic with Copy and Cache Abstract: Generating logical form equivalents of human language is a fresh way to\nemploy neural architectures where long short-term memory effectively captures\ndependencies in both encoder and decoder units.\n  The logical form of the sequence usually preserves information from the\nnatural language side in the form of similar tokens, and recently a copying\nmechanism has been proposed which increases the probability of outputting\ntokens from the source input through decoding.\n  In this paper we propose a caching mechanism as a more general form of the\ncopying mechanism which also weighs all the words from the source vocabulary\naccording to their relation to the current decoding context.\n  Our results confirm that the proposed method achieves improvements in\nsequence/token-level accuracy on sequence to logical form tasks. Further\nexperiments on cross-domain adversarial attacks show substantial improvements\nwhen using the most influential examples of other domains for training. \n\n"}
{"id": "1807.08678", "contents": "Title: Submodular Function Maximization in Parallel via the Multilinear\n  Relaxation Abstract: Balkanski and Singer [5] recently initiated the study of adaptivity (or\nparallelism) for constrained submodular function maximization, and studied the\nsetting of a cardinality constraint. Very recent improvements for this problem\nby Balkanski, Rubinstein, and Singer [6] and Ene and Nguyen [21] resulted in a\nnear-optimal $(1-1/e-\\epsilon)$-approximation in $O(\\log n/\\epsilon^2)$ rounds\nof adaptivity. Partly motivated by the goal of extending these results to more\ngeneral constraints, we describe parallel algorithms for approximately\nmaximizing the multilinear relaxation of a monotone submodular function subject\nto packing constraints. Formally our problem is to maximize $F(x)$ over $x \\in\n[0,1]^{n}$ subject to $Ax \\le 1$ where $F$ is the multilinear relaxation of a\nmonotone submodular function. Our algorithm achieves a near-optimal\n$(1-1/e-\\epsilon)$-approximation in $O(\\log^2 m \\log n/\\epsilon^4)$ rounds\nwhere $n$ is the cardinality of the ground set and $m$ is the number of packing\nconstraints. For many constraints of interest, the resulting fractional\nsolution can be rounded via known randomized rounding schemes that are\noblivious to the specific submodular function. We thus derive randomized\nalgorithms with poly-logarithmic adaptivity for a number of constraints\nincluding partition and laminar matroids, matchings, knapsack constraints, and\ntheir intersections. \n\n"}
{"id": "1807.09389", "contents": "Title: Shortest path queries, graph partitioning and covering problems in worst\n  and beyond worst case settings Abstract: In this thesis, we design algorithms for several NP-hard problems in both\nworst and beyond worst case settings. In the first part of the thesis, we apply\nthe traditional worst case methodology and design approximation algorithms for\nthe Hub Labeling problem; Hub Labeling is a preprocessing technique introduced\nto speed up shortest path queries. Before this work, Hub Labeling had been\nextensively studied mainly in the beyond worst case analysis setting, and in\nparticular on graphs with low highway dimension. In this work, we significantly\nimprove our theoretical understanding of the problem and design (worst-case)\nalgorithms for various classes of graphs, such as general graphs, graphs with\nunique shortest paths and trees, as well as provide matching inapproximability\nlower bounds for the problem in its most general settings. Finally, we\ndemonstrate a connection between computing a Hub Labeling on a tree and\nsearching for a node in a tree.\n  In the second part of the thesis, we turn to beyond worst case analysis and\nextensively study the stability model introduced by Bilu and Linial in an\nattempt to describe real-life instances of graph partitioning and clustering\nproblems. Informally, an instance of a combinatorial optimization problem is\nstable if it has a unique optimal solution that remains the unique optimum\nunder small multiplicative perturbations of the parameters of the input.\nUtilizing the power of convex relaxations for stable instances, we obtain\nseveral results for problems such as Edge/Node Multiway Cut, Independent Set\n(and its equivalent, in terms of exact solvability, Vertex Cover), clustering\nproblems such as $k$-center and $k$-median and the symmetric Traveling Salesman\nproblem. We also provide strong lower bounds for certain families of algorithms\nfor covering problems, thus exhibiting potential barriers towards the design of\nimproved algorithms in this framework. \n\n"}
{"id": "1807.09675", "contents": "Title: Toward an Optimal Quantum Algorithm for Polynomial Factorization over\n  Finite Fields Abstract: We present a randomized quantum algorithm for polynomial factorization over\nfinite fields. For polynomials of degree $n$ over a finite field $\\F_q$, the\naverage-case complexity of our algorithm is an expected $O(n^{1 + o(1)} \\log^{2\n+ o(1)}q)$ bit operations. Only for a negligible subset of polynomials of\ndegree $n$ our algorithm has a higher complexity of $O(n^{4 / 3 + o(1)} \\log^{2\n+ o(1)}q)$ bit operations. This breaks the classical $3/2$-exponent barrier for\npolynomial factorization over finite fields \\cite{guo2016alg}. \n\n"}
{"id": "1807.10262", "contents": "Title: Seeded Graph Matching via Large Neighborhood Statistics Abstract: We study a well known noisy model of the graph isomorphism problem. In this\nmodel, the goal is to perfectly recover the vertex correspondence between two\nedge-correlated Erd\\H{o}s-R\\'{e}nyi random graphs, with an initial seed set of\ncorrectly matched vertex pairs revealed as side information. For seeded\nproblems, our result provides a significant improvement over previously known\nresults. We show that it is possible to achieve the information-theoretic limit\nof graph sparsity in time polynomial in the number of vertices $n$. Moreover,\nwe show the number of seeds needed for exact recovery in polynomial-time can be\nas low as $n^{3\\epsilon}$ in the sparse graph regime (with the average degree\nsmaller than $n^{\\epsilon}$) and $\\Omega(\\log n)$ in the dense graph regime.\n  Our results also shed light on the unseeded problem. In particular, we give\nsub-exponential time algorithms for sparse models and an $n^{O(\\log n)}$\nalgorithm for dense models for some parameters, including some that are not\ncovered by recent results of Barak et al. \n\n"}
{"id": "1807.10422", "contents": "Title: Understanding V2V Driving Scenarios through Traffic Primitives Abstract: Semantically understanding complex drivers' encountering behavior, wherein\ntwo or multiple vehicles are spatially close to each other, does potentially\nbenefit autonomous car's decision-making design. This paper presents a\nframework of analyzing various encountering behaviors through decomposing\ndriving encounter data into small building blocks, called driving primitives,\nusing nonparametric Bayesian learning (NPBL) approaches, which offers a\nflexible way to gain an insight into the complex driving encounters without any\nprerequisite knowledge. The effectiveness of our proposed primitive-based\nframework is validated based on 976 naturalistic driving encounters, from which\nmore than 4000 driving primitives are learned using NPBL - a sticky HDP-HMM,\ncombined a hidden Markov model (HMM) with a hierarchical Dirichlet process\n(HDP). After that, a dynamic time warping method integrated with k-means\nclustering is then developed to cluster all these extracted driving primitives\ninto groups. Experimental results find that there exist 20 kinds of driving\nprimitives capable of representing the basic components of driving encounters\nin our database. This primitive-based analysis methodology potentially reveals\nunderlying information of vehicle-vehicle encounters for self-driving\napplications. \n\n"}
{"id": "1807.10789", "contents": "Title: Solving Target Set Selection with Bounded Thresholds Faster than $2^n$ Abstract: In this paper we consider the Target Set Selection problem. The problem\nnaturally arises in many fields like economy, sociology, medicine. In the\nTarget Set Selection problem one is given a graph $G$ with a function\n$\\operatorname{thr}: V(G) \\to \\mathbb{N} \\cup \\{0\\}$ and integers $k, \\ell$.\nThe goal of the problem is to activate at most $k$ vertices initially so that\nat the end of the activation process there is at least $\\ell$ activated\nvertices. The activation process occurs in the following way: (i) once\nactivated, a vertex stays activated forever; (ii) vertex $v$ becomes activated\nif at least $\\operatorname{thr}(v)$ of its neighbours are activated. The\nproblem and its different special cases were extensively studied from\napproximation and parameterized points of view. For example, parameterizations\nby the following parameters were studied: treewidth, feedback vertex set,\ndiameter, size of target set, vertex cover, cluster editing number and others.\n  Despite the extensive study of the problem it is still unknown whether the\nproblem can be solved in $\\mathcal{O}^*((2-\\epsilon)^n)$ time for some\n$\\epsilon >0$. We partially answer this question by presenting several\nfaster-than-trivial algorithms that work in cases of constant thresholds,\nconstant dual thresholds or when the threshold value of each vertex is bounded\nby one-third of its degree. Also, we show that the problem parameterized by\n$\\ell$ is W[1]-hard even when all thresholds are constant. \n\n"}
{"id": "1807.11135", "contents": "Title: A Hybrid Quantum-Classical Paradigm to Mitigate Embedding Costs in\n  Quantum Annealing---Abridged Version Abstract: Quantum annealing has shown significant potential as an approach to near-term\nquantum computing. Despite promising progress towards obtaining a quantum\nspeedup, quantum annealers are limited by the need to embed problem instances\nwithin the (often highly restricted) connectivity graph of the annealer. This\nembedding can be costly to perform and may destroy any computational speedup.\nHere we present a hybrid quantum-classical paradigm to help mitigate this\nlimitation, and show how a raw speedup that is negated by the embedding time\ncan nonetheless be exploited in certain circumstances. We illustrate this\napproach with initial results on a proof-of-concept implementation of an\nalgorithm for the dynamically weighted maximum independent set problem. \n\n"}
{"id": "1808.00838", "contents": "Title: Algorithms for Noisy Broadcast under Erasures Abstract: The noisy broadcast model was first studied in [Gallager, TranInf'88] where\nan $n$-character input is distributed among $n$ processors, so that each\nprocessor receives one input bit. Computation proceeds in rounds, where in each\nround each processor broadcasts a single character, and each reception is\ncorrupted independently at random with some probability $p$. [Gallager,\nTranInf'88] gave an algorithm for all processors to learn the input in\n$O(\\log\\log n)$ rounds with high probability. Later, a matching lower bound of\n$\\Omega(\\log\\log n)$ was given in [Goyal, Kindler, Saks; SICOMP'08].\n  We study a relaxed version of this model where each reception is erased and\nreplaced with a `?' independently with probability $p$. In this relaxed model,\nwe break past the lower bound of [Goyal, Kindler, Saks; SICOMP'08] and obtain\nan $O(\\log^* n)$-round algorithm for all processors to learn the input with\nhigh probability. We also show an $O(1)$-round algorithm for the same problem\nwhen the alphabet size is $\\Omega(\\mathrm{poly}(n))$. \n\n"}
{"id": "1808.01204", "contents": "Title: Learning Overparameterized Neural Networks via Stochastic Gradient\n  Descent on Structured Data Abstract: Neural networks have many successful applications, while much less\ntheoretical understanding has been gained. Towards bridging this gap, we study\nthe problem of learning a two-layer overparameterized ReLU neural network for\nmulti-class classification via stochastic gradient descent (SGD) from random\ninitialization. In the overparameterized setting, when the data comes from\nmixtures of well-separated distributions, we prove that SGD learns a network\nwith a small generalization error, albeit the network has enough capacity to\nfit arbitrary labels. Furthermore, the analysis provides interesting insights\ninto several aspects of learning neural networks and can be verified based on\nempirical studies on synthetic data and on the MNIST dataset. \n\n"}
{"id": "1808.01524", "contents": "Title: Learning disentangled representation from 12-lead electrograms:\n  application in localizing the origin of Ventricular Tachycardia Abstract: The increasing availability of electrocardiogram (ECG) data has motivated the\nuse of data-driven models for automating various clinical tasks based on ECG\ndata. The development of subject-specific models are limited by the cost and\ndifficulty of obtaining sufficient training data for each individual. The\nalternative of population model, however, faces challenges caused by the\nsignificant inter-subject variations within the ECG data. We address this\nchallenge by investigating for the first time the problem of learning\nrepresentations for clinically-informative variables while disentangling other\nfactors of variations within the ECG data. In this work, we present a\nconditional variational autoencoder (VAE) to extract the subject-specific\nadjustment to the ECG data, conditioned on task-specific representations\nlearned from a deterministic encoder. To encourage the representation for\ninter-subject variations to be independent from the task-specific\nrepresentation, maximum mean discrepancy is used to match all the moments\nbetween the distributions learned by the VAE conditioning on the code from the\ndeterministic encoder. The learning of the task-specific representation is\nregularized by a weak supervision in the form of contrastive regularization. We\napply the proposed method to a novel yet important clinical task of classifying\nthe origin of ventricular tachycardia (VT) into pre-defined segments,\ndemonstrating the efficacy of the proposed method against the standard VAE. \n\n"}
{"id": "1808.01975", "contents": "Title: A Survey on Surrogate Approaches to Non-negative Matrix Factorization Abstract: Motivated by applications in hyperspectral imaging we investigate methods for\napproximating a high-dimensional non-negative matrix $\\mathbf{\\mathit{Y}}$ by a\nproduct of two lower-dimensional, non-negative matrices $\\mathbf{\\mathit{K}}$\nand $\\mathbf{\\mathit{X}}.$ This so-called non-negative matrix factorization is\nbased on defining suitable Tikhonov functionals, which combine a discrepancy\nmeasure for $\\mathbf{\\mathit{Y}}\\approx\\mathbf{\\mathit{KX}}$ with penalty terms\nfor enforcing additional properties of $\\mathbf{\\mathit{K}}$ and\n$\\mathbf{\\mathit{X}}$. The minimization is based on alternating minimization\nwith respect to $\\mathbf{\\mathit{K}}$ or $\\mathbf{\\mathit{X}}$, where in each\niteration step one replaces the original Tikhonov functional by a locally\ndefined surrogate functional. The choice of surrogate functionals is crucial:\nIt should allow a comparatively simple minimization and simultaneously its\nfirst order optimality condition should lead to multiplicative update rules,\nwhich automatically preserve non-negativity of the iterates. We review the most\nstandard construction principles for surrogate functionals for Frobenius-norm\nand Kullback-Leibler discrepancy measures. We extend the known surrogate\nconstructions by a general framework, which allows to add a large variety of\npenalty terms. The paper finishes by deriving the corresponding alternating\nminimization schemes explicitely and by applying these methods to MALDI imaging\ndata. \n\n"}
{"id": "1808.04444", "contents": "Title: Character-Level Language Modeling with Deeper Self-Attention Abstract: LSTMs and other RNN variants have shown strong performance on character-level\nlanguage modeling. These models are typically trained using truncated\nbackpropagation through time, and it is common to assume that their success\nstems from their ability to remember long-term contexts. In this paper, we show\nthat a deep (64-layer) transformer model with fixed context outperforms RNN\nvariants by a large margin, achieving state of the art on two popular\nbenchmarks: 1.13 bits per character on text8 and 1.06 on enwik8. To get good\nresults at this depth, we show that it is important to add auxiliary losses,\nboth at intermediate network layers and intermediate sequence positions. \n\n"}
{"id": "1808.04572", "contents": "Title: Small Sample Learning in Big Data Era Abstract: As a promising area in artificial intelligence, a new learning paradigm,\ncalled Small Sample Learning (SSL), has been attracting prominent research\nattention in the recent years. In this paper, we aim to present a survey to\ncomprehensively introduce the current techniques proposed on this topic.\nSpecifically, current SSL techniques can be mainly divided into two categories.\nThe first category of SSL approaches can be called \"concept learning\", which\nemphasizes learning new concepts from only few related observations. The\npurpose is mainly to simulate human learning behaviors like recognition,\ngeneration, imagination, synthesis and analysis. The second category is called\n\"experience learning\", which usually co-exists with the large sample learning\nmanner of conventional machine learning. This category mainly focuses on\nlearning with insufficient samples, and can also be called small data learning\nin some literatures. More extensive surveys on both categories of SSL\ntechniques are introduced and some neuroscience evidences are provided to\nclarify the rationality of the entire SSL regime, and the relationship with\nhuman learning process. Some discussions on the main challenges and possible\nfuture research directions along this line are also presented. \n\n"}
{"id": "1808.05676", "contents": "Title: Why did the shape of your network change? (On detecting network\n  anomalies via non-local curvatures) Abstract: $Anomaly$ $detection$ problems (also called $change$-$point$ $detection$\nproblems) have been studied in data mining, statistics and computer science\nover the last several decades in applications such as medical condition\nmonitoring and weather change detection. In recent days, however, anomaly\ndetection problems have become increasing more relevant in the context of\n$network$ $science$ since useful insights for many complex systems in biology,\nfinance and social science are often obtained by representing them via\nnetworks. Notions of local and non-local curvatures of higher-dimensional\ngeometric shapes and topological spaces play a $fundamental$ role in physics\nand mathematics in characterizing anomalous behaviours of these higher\ndimensional entities. However, using curvature measures to detect anomalies in\nnetworks is not yet very common. To this end, a main goal in this paper to\nformulate and analyze curvature analysis methods to provide the foundations of\nsystematic approaches to find $critical$ $components$ and $detect$ $anomalies$\nin networks. For this purpose, we use two measures of network curvatures which\ndepend on non-trivial global properties, such as distributions of geodesics and\nhigher-order correlations among nodes, of the given network. Based on these\nmeasures, we precisely formulate several computational problems related to\nanomaly detection in static or dynamic networks, and provide non-trivial\ncomputational complexity results for these problems. This paper must $not$ be\nviewed as delivering the final word on appropriateness and suitability of\nspecific curvature measures. Instead, it is our hope that this paper will\nstimulate and motivate further theoretical or empirical research concerning the\nexciting interplay between notions of curvatures from network and non-network\ndomains, a $much$ desired goal in our opinion. \n\n"}
{"id": "1808.06394", "contents": "Title: Faster Support Vector Machines Abstract: The time complexity of support vector machines (SVMs) prohibits training on\nhuge data sets with millions of data points. Recently, multilevel approaches to\ntrain SVMs have been developed to allow for time-efficient training on huge\ndata sets. While regular SVMs perform the entire training in one -- time\nconsuming -- optimization step, multilevel SVMs first build a hierarchy of\nproblems decreasing in size that resemble the original problem and then train\nan SVM model for each hierarchy level, benefiting from the solved models of\nprevious levels. We present a faster multilevel support vector machine that\nuses a label propagation algorithm to construct the problem hierarchy.\nExtensive experiments indicate that our approach is up to orders of magnitude\nfaster than the previous fastest algorithm while having comparable\nclassification quality. For example, already one of our sequential solvers is\non average a factor 15 faster than the parallel ThunderSVM algorithm, while\nhaving similar classification quality. \n\n"}
{"id": "1808.09226", "contents": "Title: A Note on the Complexity of Manipulating Weighted Schulze Voting Abstract: We prove that the constructive weighted coalitional manipulation problem for\nthe Schulze voting rule can be solved in polynomial time for an unbounded\nnumber of candidates and an unbounded number of manipulators. \n\n"}
{"id": "1808.09931", "contents": "Title: Level Planarity: Transitivity vs. Even Crossings Abstract: Recently, Fulek et al. have presented Hanani-Tutte results for (radial) level\nplanarity, i.e., a graph is (radial) level planar if it admits a (radial) level\ndrawing where any two (independent) edges cross an even number of times. We\nshow that the 2-Sat formulation of level planarity testing due to Randerath et\nal. is equivalent to the strong Hanani-Tutte theorem for level planarity.\nFurther, we show that this relationship carries over to radial level planarity,\nwhich yields a novel polynomial-time algorithm for testing radial level\nplanarity. \n\n"}
{"id": "1808.10650", "contents": "Title: Graph reduction with spectral and cut guarantees Abstract: Can one reduce the size of a graph without significantly altering its basic\nproperties? The graph reduction problem is hereby approached from the\nperspective of restricted spectral approximation, a modification of the\nspectral similarity measure used for graph sparsification. This choice is\nmotivated by the observation that restricted approximation carries strong\nspectral and cut guarantees, and that it implies approximation results for\nunsupervised learning problems relying on spectral embeddings.\n  The paper then focuses on coarsening---the most common type of graph\nreduction. Sufficient conditions are derived for a small graph to approximate a\nlarger one in the sense of restricted similarity. These findings give rise to\nnearly-linear algorithms that, compared to both standard and advanced graph\nreduction methods, find coarse graphs of improved quality, often by a large\nmargin, without sacrificing speed. \n\n"}
{"id": "1809.00394", "contents": "Title: Mining Frequent Patterns in Evolving Graphs Abstract: Given a labeled graph, the frequent-subgraph mining (FSM) problem asks to\nfind all the $k$-vertex subgraphs that appear with frequency greater than a\ngiven threshold. FSM has numerous applications ranging from biology to network\nscience, as it provides a compact summary of the characteristics of the graph.\nHowever, the task is challenging, even more so for evolving graphs due to the\nstreaming nature of the input and the exponential time complexity of the\nproblem.\n  In this paper, we initiate the study of the approximate FSM problem in both\nincremental and fully-dynamic streaming settings, where arbitrary edges can be\nadded or removed from the graph. For each streaming setting, we propose\nalgorithms that can extract a high-quality approximation of the frequent\n$k$-vertex subgraphs for a given threshold, at any given time instance, with\nhigh probability. In contrast to the existing state-of-the-art solutions that\nrequire iterating over the entire set of subgraphs for any update, our\nalgorithms operate by maintaining a uniform sample of $k$-vertex subgraphs with\noptimized neighborhood-exploration procedures local to the updates. We provide\ntheoretical analysis of the proposed algorithms and empirically demonstrate\nthat the proposed algorithms generate high-quality results compared to\nbaselines. \n\n"}
{"id": "1809.00510", "contents": "Title: Flatland: a Lightweight First-Person 2-D Environment for Reinforcement\n  Learning Abstract: Flatland is a simple, lightweight environment for fast prototyping and\ntesting of reinforcement learning agents. It is of lower complexity compared to\nsimilar 3D platforms (e.g. DeepMind Lab or VizDoom), but emulates physical\nproperties of the real world, such as continuity, multi-modal\npartially-observable states with first-person view and coherent physics. We\npropose to use it as an intermediary benchmark for problems related to Lifelong\nLearning. Flatland is highly customizable and offers a wide range of task\ndifficulty to extensively evaluate the properties of artificial agents. We\nexperiment with three reinforcement learning baseline agents and show that they\ncan rapidly solve a navigation task in Flatland. A video of an agent acting in\nFlatland is available here: https://youtu.be/I5y6Y2ZypdA. \n\n"}
{"id": "1809.00862", "contents": "Title: Handwriting styles: benchmarks and evaluation metrics Abstract: Evaluating the style of handwriting generation is a challenging problem,\nsince it is not well defined. It is a key component in order to develop in\ndeveloping systems with more personalized experiences with humans. In this\npaper, we propose baseline benchmarks, in order to set anchors to estimate the\nrelative quality of different handwriting style methods. This will be done\nusing deep learning techniques, which have shown remarkable results in\ndifferent machine learning tasks, learning classification, regression, and most\nrelevant to our work, generating temporal sequences. We discuss the challenges\nassociated with evaluating our methods, which is related to evaluation of\ngenerative models in general. We then propose evaluation metrics, which we find\nrelevant to this problem, and we discuss how we evaluate the evaluation\nmetrics. In this study, we use IRON-OFF dataset. To the best of our knowledge,\nthere is no work done before in generating handwriting (either in terms of\nmethodology or the performance metrics), our in exploring styles using this\ndataset. \n\n"}
{"id": "1809.01077", "contents": "Title: Reasoning in Bayesian Opinion Exchange Networks Is PSPACE-Hard Abstract: We study the Bayesian model of opinion exchange of fully rational agents\narranged on a network. In this model, the agents receive private signals that\nare indicative of an unkown state of the world. Then, they repeatedly announce\nthe state of the world they consider most likely to their neighbors, at the\nsame time updating their beliefs based on their neighbors' announcements.\n  This model is extensively studied in economics since the work of Aumann\n(1976) and Geanakoplos and Polemarchakis (1982). It is known that the agents\neventually agree with high probability on any network. It is often argued that\nthe computations needed by agents in this model are difficult, but prior to our\nresults there was no rigorous work showing this hardness.\n  We show that it is PSPACE-hard for the agents to compute their actions in\nthis model. Furthermore, we show that it is equally difficult even to\napproximate an agent's posterior: It is PSPACE-hard to distinguish between the\nposterior being almost entirely concentrated on one state of the world or\nanother. \n\n"}
{"id": "1809.02016", "contents": "Title: Logarithmic regret in the dynamic and stochastic knapsack problem with\n  equal rewards Abstract: We study a dynamic and stochastic knapsack problem in which a decision maker\nis sequentially presented with items arriving according to a Bernoulli process\nover $n$ discrete time periods. Items have equal rewards and independent\nweights that are drawn from a known non-negative continuous distribution $F$.\nThe decision maker seeks to maximize the expected total reward of the items\nthat she includes in the knapsack while satisfying a capacity constraint and\nwhile making terminal decisions as soon as each item weight is revealed. Under\nmild regularity conditions on the weight distribution $F$, we prove that the\nregret---the expected difference between the performance of the best sequential\nalgorithm and that of a prophet who sees all of the weights before making any\ndecision---is, at most, logarithmic in $n$. Our proof is constructive. We\ndevise a reoptimized heuristic that achieves this regret bound. \n\n"}
{"id": "1809.02066", "contents": "Title: Two Dimensional Stochastic Configuration Networks for Image Data\n  Analytics Abstract: Stochastic configuration networks (SCNs) as a class of randomized learner\nmodel have been successfully employed in data analytics due to its universal\napproximation capability and fast modelling property. The technical essence\nlies in stochastically configuring hidden nodes (or basis functions) based on a\nsupervisory mechanism rather than data-independent randomization as usually\nadopted for building randomized neural networks. Given image data modelling\ntasks, the use of one-dimensional SCNs potentially demolishes the spatial\ninformation of images, and may result in undesirable performance. This paper\nextends the original SCNs to two-dimensional version, termed 2DSCNs, for fast\nbuilding randomized learners with matrix-inputs. Some theoretical analyses on\nthe goodness of 2DSCNs against SCNs, including the complexity of the random\nparameter space, and the superiority of generalization, are presented.\nEmpirical results over one regression, four benchmark handwritten digits\nclassification, and two human face recognition datasets demonstrate that the\nproposed 2DSCNs perform favourably and show good potential for image data\nanalytics. \n\n"}
{"id": "1809.02744", "contents": "Title: On the Calibration of Nested Dichotomies for Large Multiclass Tasks Abstract: Nested dichotomies are used as a method of transforming a multiclass\nclassification problem into a series of binary problems. A tree structure is\ninduced that recursively splits the set of classes into subsets, and a binary\nclassification model learns to discriminate between the two subsets of classes\nat each node. In this paper, we demonstrate that these nested dichotomies\ntypically exhibit poor probability calibration, even when the base binary\nmodels are well calibrated. We also show that this problem is exacerbated when\nthe binary models are poorly calibrated. We discuss the effectiveness of\ndifferent calibration strategies and show that accuracy and log-loss can be\nsignificantly improved by calibrating both the internal base models and the\nfull nested dichotomy structure, especially when the number of classes is high. \n\n"}
{"id": "1809.02995", "contents": "Title: On Solving Linear Systems in Sublinear Time Abstract: We study \\emph{sublinear} algorithms that solve linear systems locally. In\nthe classical version of this problem the input is a matrix $S\\in\n\\mathbb{R}^{n\\times n}$ and a vector $b\\in\\mathbb{R}^n$ in the range of $S$,\nand the goal is to output $x\\in \\mathbb{R}^n$ satisfying $Sx=b$. For the case\nwhen the matrix $S$ is symmetric diagonally dominant (SDD), the breakthrough\nalgorithm of Spielman and Teng [STOC 2004] approximately solves this problem in\nnear-linear time (in the input size which is the number of non-zeros in $S$),\nand subsequent papers have further simplified, improved, and generalized the\nalgorithms for this setting.\n  Here we focus on computing one (or a few) coordinates of $x$, which\npotentially allows for sublinear algorithms. Formally, given an index $u\\in\n[n]$ together with $S$ and $b$ as above, the goal is to output an approximation\n$\\hat{x}_u$ for $x^*_u$, where $x^*$ is a fixed solution to $Sx=b$.\n  Our results show that there is a qualitative gap between SDD matrices and the\nmore general class of positive semidefinite (PSD) matrices. For SDD matrices,\nwe develop an algorithm that approximates a single coordinate $x_{u}$ in time\nthat is polylogarithmic in $n$, provided that $S$ is sparse and has a small\ncondition number (e.g., Laplacian of an expander graph). The approximation\nguarantee is additive $| \\hat{x}_u-x^*_u | \\le \\epsilon \\| x^* \\|_\\infty$ for\naccuracy parameter $\\epsilon>0$. We further prove that the condition-number\nassumption is necessary and tight.\n  In contrast to the SDD matrices, we prove that for certain PSD matrices $S$,\nthe running time must be at least polynomial in $n$. This holds even when one\nwants to obtain the same additive approximation, and $S$ has bounded sparsity\nand condition number. \n\n"}
{"id": "1809.05183", "contents": "Title: Explainable time series tweaking via irreversible and reversible\n  temporal transformations Abstract: Time series classification has received great attention over the past decade\nwith a wide range of methods focusing on predictive performance by exploiting\nvarious types of temporal features. Nonetheless, little emphasis has been\nplaced on interpretability and explainability. In this paper, we formulate the\nnovel problem of explainable time series tweaking, where, given a time series\nand an opaque classifier that provides a particular classification decision for\nthe time series, we want to find the minimum number of changes to be performed\nto the given time series so that the classifier changes its decision to another\nclass. We show that the problem is NP-hard, and focus on two instantiations of\nthe problem, which we refer to as reversible and irreversible time series\ntweaking. The classifier under investigation is the random shapelet forest\nclassifier. Moreover, we propose two algorithmic solutions for the two problems\nalong with simple optimizations, as well as a baseline solution using the\nnearest neighbor classifier. An extensive experimental evaluation on a variety\nof real datasets demonstrates the usefulness and effectiveness of our problem\nformulation and solutions. \n\n"}
{"id": "1809.06498", "contents": "Title: HashTran-DNN: A Framework for Enhancing Robustness of Deep Neural\n  Networks against Adversarial Malware Samples Abstract: Adversarial machine learning in the context of image processing and related\napplications has received a large amount of attention. However, adversarial\nmachine learning, especially adversarial deep learning, in the context of\nmalware detection has received much less attention despite its apparent\nimportance. In this paper, we present a framework for enhancing the robustness\nof Deep Neural Networks (DNNs) against adversarial malware samples, dubbed\nHashing Transformation Deep Neural Networks} (HashTran-DNN). The core idea is\nto use hash functions with a certain locality-preserving property to transform\nsamples to enhance the robustness of DNNs in malware classification. The\nframework further uses a Denoising Auto-Encoder (DAE) regularizer to\nreconstruct the hash representations of samples, making the resulting DNN\nclassifiers capable of attaining the locality information in the latent space.\nWe experiment with two concrete instantiations of the HashTran-DNN framework to\nclassify Android malware. Experimental results show that four known attacks can\nrender standard DNNs useless in classifying Android malware, that known\ndefenses can at most defend three of the four attacks, and that HashTran-DNN\ncan effectively defend against all of the four attacks. \n\n"}
{"id": "1809.07425", "contents": "Title: Mean Estimation with Sub-Gaussian Rates in Polynomial Time Abstract: We study polynomial time algorithms for estimating the mean of a heavy-tailed\nmultivariate random vector. We assume only that the random vector $X$ has\nfinite mean and covariance. In this setting, the radius of confidence intervals\nachieved by the empirical mean are large compared to the case that $X$ is\nGaussian or sub-Gaussian.\n  We offer the first polynomial time algorithm to estimate the mean with\nsub-Gaussian-size confidence intervals under such mild assumptions. Our\nalgorithm is based on a new semidefinite programming relaxation of a\nhigh-dimensional median. Previous estimators which assumed only existence of\nfinitely-many moments of $X$ either sacrifice sub-Gaussian performance or are\nonly known to be computable via brute-force search procedures requiring time\nexponential in the dimension. \n\n"}
{"id": "1809.08151", "contents": "Title: SIC-MMAB: Synchronisation Involves Communication in Multiplayer\n  Multi-Armed Bandits Abstract: Motivated by cognitive radio networks, we consider the stochastic multiplayer\nmulti-armed bandit problem, where several players pull arms simultaneously and\ncollisions occur if one of them is pulled by several players at the same stage.\nWe present a decentralized algorithm that achieves the same performance as a\ncentralized one, contradicting the existing lower bounds for that problem. This\nis possible by \"hacking\" the standard model by constructing a communication\nprotocol between players that deliberately enforces collisions, allowing them\nto share their information at a negligible cost. This motivates the\nintroduction of a more appropriate dynamic setting without sensing, where\nsimilar communication protocols are no longer possible. However, we show that\nthe logarithmic growth of the regret is still achievable for this model with a\nnew algorithm. \n\n"}
{"id": "1809.08160", "contents": "Title: Data-compression for Parametrized Counting Problems on Sparse graphs Abstract: We study the concept of \\emph{compactor}, which may be seen as a\ncounting-analogue of kernelization in counting parameterized complexity. For a\nfunction $F:\\Sigma^*\\to \\Bbb{N}$ and a parameterization $\\kappa: \\Sigma^*\\to\n\\Bbb{N}$, a compactor $({\\sf P},{\\sf M})$ consists of a polynomial-time\ncomputable function ${\\sf P}$, called \\emph{condenser}, and a computable\nfunction ${\\sf M}$, called \\emph{extractor}, such that $F={\\sf M}\\circ {\\sf\nP}$, and the condensing ${\\sf P}(x)$ of $x$ has length at most $s(\\kappa(x))$,\nfor any input $x\\in \\Sigma^*.$ If $s$ is a polynomial function, then the\ncompactor is said to be of polynomial-size. Although the study on\ncounting-analogue of kernelization is not unprecedented, it has received little\nattention so far. We study a family of vertex-certified counting problems on\ngraphs that are MSOL-expressible; that is, for an MSOL-formula $\\phi$ with one\nfree set variable to be interpreted as a vertex subset, we want to count all\n$A\\subseteq V(G)$ where $|A|=k$ and $(G,A)\\models \\phi.$ In this paper, we\nprove that every vertex-certified counting problems on graphs that is\n\\emph{MSOL-expressible} and \\emph{treewidth modulable}, when parameterized by\n$k$, admits a polynomial-size compactor on $H$-topological-minor-free graphs\nwith condensing time $O(k^2n^2)$ and decoding time $2^{O(k)}.$ This implies the\nexistence of an {\\sf FPT}-algorithm of running time $O(n^2k^2)+2^{O(k)}.$ All\naforementioned complexities are under the Uniform Cost Measure (UCM) model\nwhere numbers can be stored in constant space and arithmetic operations can be\ndone in constant time. \n\n"}
{"id": "1809.09030", "contents": "Title: A Fairness-aware Hybrid Recommender System Abstract: Recommender systems are used in variety of domains affecting people's lives.\nThis has raised concerns about possible biases and discrimination that such\nsystems might exacerbate. There are two primary kinds of biases inherent in\nrecommender systems: observation bias and bias stemming from imbalanced data.\nObservation bias exists due to a feedback loop which causes the model to learn\nto only predict recommendations similar to previous ones. Imbalance in data\noccurs when systematic societal, historical, or other ambient bias is present\nin the data. In this paper, we address both biases by proposing a hybrid\nfairness-aware recommender system. Our model provides efficient and accurate\nrecommendations by incorporating multiple user-user and item-item similarity\nmeasures, content, and demographic information, while addressing recommendation\nbiases. We implement our model using a powerful and expressive probabilistic\nprogramming language called probabilistic soft logic. We experimentally\nevaluate our approach on a popular movie recommendation dataset, showing that\nour proposed model can provide more accurate and fairer recommendations,\ncompared to a state-of-the art fair recommender system. \n\n"}
{"id": "1809.09345", "contents": "Title: Subexponential algorithms for variants of homomorphism problem in string\n  graphs Abstract: We consider the complexity of finding weighted homomorphisms from\nintersection graphs of curves (string graphs) with $n$ vertices to a fixed\ngraph $H$. We provide a complete dichotomy for the problem: if $H$ has no two\nvertices sharing two common neighbors, then the problem can be solved in time\n$2^{O(n^{2/3} \\log n)}$, otherwise there is no algorithm working in time\n$2^{o(n)}$, even in intersection graphs of segments, unless the ETH fails. This\ngeneralizes several known results concerning the complexity of computatational\nproblems in geometric intersection graphs. Then we consider two variants of\ngraph homomorphism problem, called locally injective homomorphism and locally\nbijective homomorphism, where we require the homomorphism to be injective or\nbijective on the neighborhood of each vertex. We show that for each target\ngraph $H$, both problems can always be solved in time $2^{O(\\sqrt{n} \\log n)}$\nin string graphs. For the locally surjecive homomorphism, defined in an\nanalogous way, the situation seems more complicated. We show the dichotomy\ntheorem for simple connected graphs $H$ with maximum degree 2. If $H$ is\nisomorphic to $P_3$ or $C_4$, then the existence of a locally surjective\nhomomorphism from a string graph with $n$ vertices to $H$ can be decided in\ntime $2^{O(n^{2/3} \\log^{3/2} n)}$, otherwise the problem cannot be solved in\ntime $2^{o(n)}$, unless the ETH fails. As a byproduct, we obtain several\nresults concerning the complexity of variants of homomorphism problem in\n$P_t$-free graphs. In particular, we obtain the dichotomy theorem for weighted\nhomomorphism, analogous to the one for string graphs. \n\n"}
{"id": "1809.09350", "contents": "Title: Fully Implicit Online Learning Abstract: Regularized online learning is widely used in machine learning applications.\nIn online learning, performing exact minimization ($i.e.,$ implicit update) is\nknown to be beneficial to the numerical stability and structure of solution. In\nthis paper we study a class of regularized online algorithms without\nlinearizing the loss function or the regularizer, which we call \\emph{fully\nimplicit online learning} (FIOL). We show that for arbitrary Bregman\ndivergence, FIOL has the $O(\\sqrt{T})$ regret for general convex setting and\n$O(\\log T)$ regret for strongly convex setting, and the regret has an one-step\nimprovement effect because it avoids the approximation error of linearization.\nThen we propose efficient algorithms to solve the subproblem of FIOL. We show\nthat even if the solution of the subproblem has no closed form, it can be\nsolved with complexity comparable to the linearized online algoritms.\nExperiments validate the proposed approaches. \n\n"}
{"id": "1809.10241", "contents": "Title: Classifying Mammographic Breast Density by Residual Learning Abstract: Mammographic breast density, a parameter used to describe the proportion of\nbreast tissue fibrosis, is widely adopted as an evaluation characteristic of\nthe likelihood of breast cancer incidence. In this study, we present a\nradiomics approach based on residual learning for the classification of\nmammographic breast densities. Our method possesses several encouraging\nproperties such as being almost fully automatic, possessing big model capacity\nand flexibility. It can obtain outstanding classification results without the\nnecessity of result compensation using mammographs taken from different views.\nThe proposed method was instantiated with the INbreast dataset and\nclassification accuracies of 92.6% and 96.8% were obtained for the four BI-RADS\n(Breast Imaging and Reporting Data System) category task and the two BI-RADS\ncategory task,respectively. The superior performances achieved compared to the\nexisting state-of-the-art methods along with its encouraging properties\nindicate that our method has a great potential to be applied as a\ncomputer-aided diagnosis tool. \n\n"}
{"id": "1809.10508", "contents": "Title: Distance and routing labeling schemes for cube-free median graphs Abstract: Distance labeling schemes are schemes that label the vertices of a graph with\nshort labels in such a way that the distance between any two vertices $u$ and\n$v$ can be determined efficiently by merely inspecting the labels of $u$ and\n$v$, without using any other information. Similarly, routing labeling schemes\nlabel the vertices of a graph in a such a way that given the labels of a source\nnode and a destination node, it is possible to compute efficiently the port\nnumber of the edge from the source that heads in the direction of the\ndestination. One of important problems is finding natural classes of graphs\nadmitting distance and/or routing labeling schemes with labels of\npolylogarithmic size. In this paper, we show that the class of cube-free median\ngraphs on $n$ nodes enjoys distance and routing labeling schemes with labels of\n$O(\\log^3 n)$ bits. \n\n"}
{"id": "1810.00386", "contents": "Title: Harmonic Alignment Abstract: We propose a novel framework for combining datasets via alignment of their\nintrinsic geometry. This alignment can be used to fuse data originating from\ndisparate modalities, or to correct batch effects while preserving intrinsic\ndata structure. Importantly, we do not assume any pointwise correspondence\nbetween datasets, but instead rely on correspondence between a (possibly\nunknown) subset of data features. We leverage this assumption to construct an\nisometric alignment between the data. This alignment is obtained by relating\nthe expansion of data features in harmonics derived from diffusion operators\ndefined over each dataset. These expansions encode each feature as a function\nof the data geometry. We use this to relate the diffusion coordinates of each\ndataset through our assumption of partial feature correspondence. Then, a\nunified diffusion geometry is constructed over the aligned data, which can also\nbe used to correct the original data measurements. We demonstrate our method on\nseveral datasets, showing in particular its effectiveness in biological\napplications including fusion of single-cell RNA sequencing (scRNA-seq) and\nsingle-cell ATAC sequencing (scATAC-seq) data measured on the same population\nof cells, and removal of batch effect between biological samples. \n\n"}
{"id": "1810.00656", "contents": "Title: Perfect Match: A Simple Method for Learning Representations For\n  Counterfactual Inference With Neural Networks Abstract: Learning representations for counterfactual inference from observational data\nis of high practical relevance for many domains, such as healthcare, public\npolicy and economics. Counterfactual inference enables one to answer \"What\nif...?\" questions, such as \"What would be the outcome if we gave this patient\ntreatment $t_1$?\". However, current methods for training neural networks for\ncounterfactual inference on observational data are either overly complex,\nlimited to settings with only two available treatments, or both. Here, we\npresent Perfect Match (PM), a method for training neural networks for\ncounterfactual inference that is easy to implement, compatible with any\narchitecture, does not add computational complexity or hyperparameters, and\nextends to any number of treatments. PM is based on the idea of augmenting\nsamples within a minibatch with their propensity-matched nearest neighbours.\nOur experiments demonstrate that PM outperforms a number of more complex\nstate-of-the-art methods in inferring counterfactual outcomes across several\nbenchmarks, particularly in settings with many treatments. \n\n"}
{"id": "1810.00668", "contents": "Title: Wronging a Right: Generating Better Errors to Improve Grammatical Error\n  Detection Abstract: Grammatical error correction, like other machine learning tasks, greatly\nbenefits from large quantities of high quality training data, which is\ntypically expensive to produce. While writing a program to automatically\ngenerate realistic grammatical errors would be difficult, one could learn the\ndistribution of naturallyoccurring errors and attempt to introduce them into\nother datasets. Initial work on inducing errors in this way using statistical\nmachine translation has shown promise; we investigate cheaply constructing\nsynthetic samples, given a small corpus of human-annotated data, using an\noff-the-rack attentive sequence-to-sequence model and a straight-forward\npost-processing procedure. Our approach yields error-filled artificial data\nthat helps a vanilla bi-directional LSTM to outperform the previous state of\nthe art at grammatical error detection, and a previously introduced model to\ngain further improvements of over 5% $F_{0.5}$ score. When attempting to\ndetermine if a given sentence is synthetic, a human annotator at best achieves\n39.39 $F_1$ score, indicating that our model generates mostly human-like\ninstances. \n\n"}
{"id": "1810.01163", "contents": "Title: An Entropic Optimal Transport Loss for Learning Deep Neural Networks\n  under Label Noise in Remote Sensing Images Abstract: Deep neural networks have established as a powerful tool for large scale\nsupervised classification tasks. The state-of-the-art performances of deep\nneural networks are conditioned to the availability of large number of\naccurately labeled samples. In practice, collecting large scale accurately\nlabeled datasets is a challenging and tedious task in most scenarios of remote\nsensing image analysis, thus cheap surrogate procedures are employed to label\nthe dataset. Training deep neural networks on such datasets with inaccurate\nlabels easily overfits to the noisy training labels and degrades the\nperformance of the classification tasks drastically. To mitigate this effect,\nwe propose an original solution with entropic optimal transportation. It allows\nto learn in an end-to-end fashion deep neural networks that are, to some\nextent, robust to inaccurately labeled samples. We empirically demonstrate on\nseveral remote sensing datasets, where both scene and pixel-based hyperspectral\nimages are considered for classification. Our method proves to be highly\ntolerant to significant amounts of label noise and achieves favorable results\nagainst state-of-the-art methods. \n\n"}
{"id": "1810.02183", "contents": "Title: Revealing Network Structure, Confidentially: Improved Rates for\n  Node-Private Graphon Estimation Abstract: Motivated by growing concerns over ensuring privacy on social networks, we\ndevelop new algorithms and impossibility results for fitting complex\nstatistical models to network data subject to rigorous privacy guarantees. We\nconsider the so-called node-differentially private algorithms, which compute\ninformation about a graph or network while provably revealing almost no\ninformation about the presence or absence of a particular node in the graph.\n  We provide new algorithms for node-differentially private estimation for a\npopular and expressive family of network models: stochastic block models and\ntheir generalization, graphons. Our algorithms improve on prior work, reducing\ntheir error quadratically and matching, in many regimes, the optimal nonprivate\nalgorithm. We also show that for the simplest random graph models ($G(n,p)$ and\n$G(n,m)$), node-private algorithms can be qualitatively more accurate than for\nmore complex models---converging at a rate of $\\frac{1}{\\epsilon^2 n^{3}}$\ninstead of $\\frac{1}{\\epsilon^2 n^2}$. This result uses a new extension lemma\nfor differentially private algorithms that we hope will be broadly useful. \n\n"}
{"id": "1810.02304", "contents": "Title: Polynomial-time Recognition of 4-Steiner Powers Abstract: The $k^{th}$-power of a given graph $G=(V,E)$ is obtained from $G$ by adding\nan edge between every two distinct vertices at a distance at most $k$ in $G$.\nWe call $G$ a $k$-Steiner power if it is an induced subgraph of the\n$k^{th}$-power of some tree. Our main contribution is a polynomial-time\nrecognition algorithm of $4$-Steiner powers, thereby extending the\ndecade-year-old results of (Lin, Kearney and Jiang, ISAAC'00) for $k=1,2$ and\n(Chang and Ko, WG'07) for $k=3$.\n  A graph $G$ is termed $k$-leaf power if there is some tree $T$ such that: all\nvertices in $V(G)$ are leaf-nodes of $T$, and $G$ is an induced subgraph of the\n$k^{th}$-power of $T$. As a byproduct of our main result, we give the first\nknown polynomial-time recognition algorithm for $6$-leaf powers. \n\n"}
{"id": "1810.02797", "contents": "Title: RCCNet: An Efficient Convolutional Neural Network for Histological\n  Routine Colon Cancer Nuclei Classification Abstract: Efficient and precise classification of histological cell nuclei is of utmost\nimportance due to its potential applications in the field of medical image\nanalysis. It would facilitate the medical practitioners to better understand\nand explore various factors for cancer treatment. The classification of\nhistological cell nuclei is a challenging task due to the cellular\nheterogeneity. This paper proposes an efficient Convolutional Neural Network\n(CNN) based architecture for classification of histological routine colon\ncancer nuclei named as RCCNet. The main objective of this network is to keep\nthe CNN model as simple as possible. The proposed RCCNet model consists of only\n1,512,868 learnable parameters which are significantly less compared to the\npopular CNN models such as AlexNet, CIFARVGG, GoogLeNet, and WRN. The\nexperiments are conducted over publicly available routine colon cancer\nhistological dataset \"CRCHistoPhenotypes\". The results of the proposed RCCNet\nmodel are compared with five state-of-the-art CNN models in terms of the\naccuracy, weighted average F1 score and training time. The proposed method has\nachieved a classification accuracy of 80.61% and 0.7887 weighted average F1\nscore. The proposed RCCNet is more efficient and generalized terms of the\ntraining time and data over-fitting, respectively. \n\n"}
{"id": "1810.03742", "contents": "Title: Problem Solving at the Edge of Chaos: Entropy, Puzzles and the Sudoku\n  Freezing Transition Abstract: Sudoku is a widely popular $\\mathcal{NP}$-Complete combinatorial puzzle whose\nprospects for studying human computation have recently received attention, but\nthe algorithmic hardness of Sudoku solving is yet largely unexplored. In this\npaper, we study the statistical mechanical properties of random Sudoku grids,\nshowing that puzzles of varying sizes attain a hardness peak associated with a\ncritical behavior in the constrainedness of random instances. In doing so, we\nprovide the first description of a Sudoku \\emph{freezing} transition, showing\nthat the fraction of backbone variables undergoes a phase transition as the\ndensity of pre-filled cells is calibrated. We also uncover a variety of\ncritical phenomena in the applicability of Sudoku elimination strategies,\nproviding explanations as to why puzzles become boring outside the typical\nrange of clue densities adopted by Sudoku publishers. We further show that the\nconstrainedness of Sudoku puzzles can be understood in terms of the\ninformational (Shannon) entropy of their solutions, which only increases up to\nthe critical point where variables become frozen. Our findings shed light on\nthe nature of the $k$-coloring transition when the graph topology is fixed, and\nare an invitation to the study of phase transition phenomena in problems\ndefined over \\emph{alldifferent} constraints. They also suggest advantages to\nstudying the statistical mechanics of popular $\\mathcal{NP}$-Hard puzzles,\nwhich can both aid the design of hard instances and help understand the\ndifficulty of human problem solving. \n\n"}
{"id": "1810.04133", "contents": "Title: Learning One-hidden-layer Neural Networks under General Input\n  Distributions Abstract: Significant advances have been made recently on training neural networks,\nwhere the main challenge is in solving an optimization problem with abundant\ncritical points. However, existing approaches to address this issue crucially\nrely on a restrictive assumption: the training data is drawn from a Gaussian\ndistribution. In this paper, we provide a novel unified framework to design\nloss functions with desirable landscape properties for a wide range of general\ninput distributions. On these loss functions, remarkably, stochastic gradient\ndescent theoretically recovers the true parameters with global initializations\nand empirically outperforms the existing approaches. Our loss function design\nbridges the notion of score functions with the topic of neural network\noptimization. Central to our approach is the task of estimating the score\nfunction from samples, which is of basic and independent interest to\ntheoretical statistics. Traditional estimation methods (example: kernel based)\nfail right at the outset; we bring statistical methods of local likelihood to\ndesign a novel estimator of score functions, that provably adapts to the local\ngeometry of the unknown density. \n\n"}
{"id": "1810.04147", "contents": "Title: Entropic GANs meet VAEs: A Statistical Approach to Compute Sample\n  Likelihoods in GANs Abstract: Building on the success of deep learning, two modern approaches to learn a\nprobability model from the data are Generative Adversarial Networks (GANs) and\nVariational AutoEncoders (VAEs). VAEs consider an explicit probability model\nfor the data and compute a generative distribution by maximizing a variational\nlower-bound on the log-likelihood function. GANs, however, compute a generative\nmodel by minimizing a distance between observed and generated probability\ndistributions without considering an explicit model for the observed data. The\nlack of having explicit probability models in GANs prohibits computation of\nsample likelihoods in their frameworks and limits their use in statistical\ninference problems. In this work, we resolve this issue by constructing an\nexplicit probability model that can be used to compute sample likelihood\nstatistics in GANs. In particular, we prove that under this probability model,\na family of Wasserstein GANs with an entropy regularization can be viewed as a\ngenerative model that maximizes a variational lower-bound on average sample log\nlikelihoods, an approach that VAEs are based on. This result makes a principled\nconnection between two modern generative models, namely GANs and VAEs. In\naddition to the aforementioned theoretical results, we compute likelihood\nstatistics for GANs trained on Gaussian, MNIST, SVHN, CIFAR-10 and LSUN\ndatasets. Our numerical results validate the proposed theory. \n\n"}
{"id": "1810.04312", "contents": "Title: Using ACL2 in the Design of Efficient, Verifiable Data Structures for\n  High-Assurance Systems Abstract: Verification of algorithms and data structures utilized in modern autonomous\nand semi-autonomous vehicles for land, sea, air, and space presents a\nsignificant challenge. Autonomy algorithms, e.g., route planning, pattern\nmatching, and inference, are based on complex data structures such as directed\ngraphs and algebraic data types. Proof techniques for these data structures\nexist, but are oriented to unbounded, functional realizations, which are not\ntypically efficient in either space or time. Autonomous systems designers, on\nthe other hand, generally limit the space and time allocations for any given\nfunction, and require that algorithms deliver results within a finite time, or\nsuffer a watchdog timeout. Furthermore, high-assurance design rules frown on\ndynamic memory allocation, preferring simple array-based data structure\nimplementations.\n  In order to provide efficient implementations of high-level data structures\nused in autonomous systems with the high assurance needed for accreditation, we\nhave developed a verifying compilation technique that supports the \"natural\"\nfunctional proof style, but yet applies to more efficient data structure\nimplementations. Our toolchain features code generation to mainstream\nprogramming languages, as well as GPU-based and hardware-based realizations. We\nbase the Intermediate Verification Language for our toolchain upon higher-order\nlogic; however, we have used ACL2 to develop our efficient yet verifiable data\nstructure design. ACL2 is particularly well-suited for this work, with its\nsophisticated libraries for reasoning about aggregate data structures of\narbitrary size, efficient execution of formal specifications, as well as its\nsupport for \"single-threaded objects\" -- functional datatypes with imperative\n\"under the hood\" implementations.\n  In this paper, we detail our high-assurance data structure design approach,\nincluding examples in ACL2 of common algebraic data types implemented using\nthis design approach, proofs of correctness for those data types carried out in\nACL2, as well as sample ACL2 implementations of relevant algorithms utilizing\nthese efficient, high-assurance data structures. \n\n"}
{"id": "1810.04620", "contents": "Title: Parameterized Complexity of Independent Set in H-Free Graphs Abstract: In this paper, we investigate the complexity of Maximum Independent Set (MIS)\nin the class of $H$-free graphs, that is, graphs excluding a fixed graph as an\ninduced subgraph. Given that the problem remains $NP$-hard for most graphs $H$,\nwe study its fixed-parameter tractability and make progress towards a dichotomy\nbetween $FPT$ and $W[1]$-hard cases. We first show that MIS remains $W[1]$-hard\nin graphs forbidding simultaneously $K_{1, 4}$, any finite set of cycles of\nlength at least $4$, and any finite set of trees with at least two branching\nvertices. In particular, this answers an open question of Dabrowski et al.\nconcerning $C_4$-free graphs. Then we extend the polynomial algorithm of\nAlekseev when $H$ is a disjoint union of edges to an $FPT$ algorithm when $H$\nis a disjoint union of cliques. We also provide a framework for solving several\nother cases, which is a generalization of the concept of \\emph{iterative\nexpansion} accompanied by the extraction of a particular structure using\nRamsey's theorem. Iterative expansion is a maximization version of the\nso-called \\emph{iterative compression}. We believe that our framework can be of\nindependent interest for solving other similar graph problems. Finally, we\npresent positive and negative results on the existence of polynomial (Turing)\nkernels for several graphs $H$. \n\n"}
{"id": "1810.07900", "contents": "Title: Policy Gradient in Partially Observable Environments: Approximation and\n  Convergence Abstract: Policy gradient is a generic and flexible reinforcement learning approach\nthat generally enjoys simplicity in analysis, implementation, and deployment.\nIn the last few decades, this approach has been extensively advanced for fully\nobservable environments. In this paper, we generalize a variety of these\nadvances to partially observable settings, and similar to the fully observable\ncase, we keep our focus on the class of Markovian policies. We propose a series\nof technical tools, including a novel notion of advantage function, to develop\npolicy gradient algorithms and study their convergence properties in such\nenvironments. Deploying these tools, we generalize a variety of existing\ntheoretical guarantees, such as policy gradient and convergence theorems, to\npartially observable domains, those which also could be carried to more\nsettings of interest. This study also sheds light on the understanding of\npolicy gradient approaches in real-world applications which tend to be\npartially observable. \n\n"}
{"id": "1810.08054", "contents": "Title: Locally Private Mean Estimation: Z-test and Tight Confidence Intervals Abstract: This work provides tight upper- and lower-bounds for the problem of mean\nestimation under $\\epsilon$-differential privacy in the local model, when the\ninput is composed of $n$ i.i.d. drawn samples from a normal distribution with\nvariance $\\sigma$. Our algorithms result in a $(1-\\beta)$-confidence interval\nfor the underlying distribution's mean $\\mu$ of length $\\tilde O\\left(\n\\frac{\\sigma \\sqrt{\\log(\\frac 1 \\beta)}}{\\epsilon\\sqrt n} \\right)$. In\naddition, our algorithms leverage binary search using local differential\nprivacy for quantile estimation, a result which may be of separate interest.\nMoreover, we prove a matching lower-bound (up to poly-log factors), showing\nthat any one-shot (each individual is presented with a single query) local\ndifferentially private algorithm must return an interval of length\n$\\Omega\\left( \\frac{\\sigma\\sqrt{\\log(1/\\beta)}}{\\epsilon\\sqrt{n}}\\right)$. \n\n"}
{"id": "1810.09502", "contents": "Title: How to train your MAML Abstract: The field of few-shot learning has recently seen substantial advancements.\nMost of these advancements came from casting few-shot learning as a\nmeta-learning problem. Model Agnostic Meta Learning or MAML is currently one of\nthe best approaches for few-shot learning via meta-learning. MAML is simple,\nelegant and very powerful, however, it has a variety of issues, such as being\nvery sensitive to neural network architectures, often leading to instability\nduring training, requiring arduous hyperparameter searches to stabilize\ntraining and achieve high generalization and being very computationally\nexpensive at both training and inference times. In this paper, we propose\nvarious modifications to MAML that not only stabilize the system, but also\nsubstantially improve the generalization performance, convergence speed and\ncomputational overhead of MAML, which we call MAML++. \n\n"}
{"id": "1810.10938", "contents": "Title: Sample Efficient Algorithms for Learning Quantum Channels in PAC Model\n  and the Approximate State Discrimination Problem Abstract: We generalize the PAC (probably approximately correct) learning model to the\nquantum world by generalizing the concepts from classical functions to quantum\nprocesses, defining the problem of \\emph{PAC learning quantum process}, and\nstudy its sample complexity. In the problem of PAC learning quantum process, we\nwant to learn an $\\epsilon$-approximate of an unknown quantum process $c^*$\nfrom a known finite concept class $C$ with probability $1-\\delta$ using samples\n$\\{(x_1,c^*(x_1)),(x_2,c^*(x_2)),\\dots\\}$, where $\\{x_1,x_2, \\dots\\}$ are\ncomputational basis states sampled from an unknown distribution $D$ and\n$\\{c^*(x_1),c^*(x_2),\\dots\\}$ are the (possibly mixed) quantum states outputted\nby $c^*$. The special case of PAC-learning quantum process under constant input\nreduces to a natural problem which we named as approximate state\ndiscrimination, where we are given copies of an unknown quantum state $c^*$\nfrom an known finite set $C$, and we want to learn with probability $1-\\delta$\nan $\\epsilon$-approximate of $c^*$ with as few copies of $c^*$ as possible. We\nshow that the problem of PAC learning quantum process can be solved with\n$$O\\left(\\frac{\\log|C| + \\log(1/ \\delta)} { \\epsilon^2}\\right)$$ samples when\nthe outputs are pure states and $$O\\left(\\frac{\\log^3 |C|(\\log |C|+\\log(1/\n\\delta))} { \\epsilon^2}\\right)$$ samples if the outputs can be mixed. Some\nimplications of our results are that we can PAC-learn a polynomial sized\nquantum circuit in polynomial samples and approximate state discrimination can\nbe solved in polynomial samples even when concept class size $|C|$ is\nexponential in the number of qubits, an exponentially improvement over a full\nstate tomography. \n\n"}
{"id": "1810.11152", "contents": "Title: Efficient and High-Quality Seeded Graph Matching: Employing High Order\n  Structural Information Abstract: Driven by many real applications, we study the problem of seeded graph\nmatching. Given two graphs $G_1 = (V_1, E_1)$ and $G_2 = (V_2, E_2)$, and a\nsmall set $S$ of pre-matched node pairs $[u, v]$ where $u \\in V_1$ and $v \\in\nV_2$, the problem is to identify a matching between $V_1$ and $V_2$ growing\nfrom $S$, such that each pair in the matching corresponds to the same\nunderlying entity. Recent studies on efficient and effective seeded graph\nmatching have drawn a great deal of attention and many popular methods are\nlargely based on exploring the similarity between local structures to identify\nmatching pairs. While these recent techniques work well on random graphs, their\naccuracy is low over many real networks. Motivated by this, we propose to\nutilize high order neighboring information to improve the matching accuracy. As\na result, a new framework of seeded graph matching is proposed, which employs\nPersonalized PageRank (PPR) to quantify the matching score of each node pair.\nTo further boost the matching accuracy, we propose a novel postponing strategy,\nwhich postpones the selection of pairs that have competitors with similar\nmatching scores. We theoretically prove that the postpone strategy indeed\nsignificantly improves the matching accuracy. To improve the scalability of\nmatching large graphs, we also propose efficient approximation techniques based\non algorithms for computing PPR heavy hitters. Our comprehensive experimental\nstudies on large-scale real datasets demonstrate that, compared with state of\nthe art approaches, our framework not only increases the precision and recall\nboth by a significant margin but also achieves speed-up up to more than one\norder of magnitude. \n\n"}
{"id": "1810.11367", "contents": "Title: LAMVI-2: A Visual Tool for Comparing and Tuning Word Embedding Models Abstract: Tuning machine learning models, particularly deep learning architectures, is\na complex process. Automated hyperparameter tuning algorithms often depend on\nspecific optimization metrics. However, in many situations, a developer trades\none metric against another: accuracy versus overfitting, precision versus\nrecall, smaller models and accuracy, etc. With deep learning, not only are the\nmodel's representations opaque, the model's behavior when parameters \"knobs\"\nare changed may also be unpredictable. Thus, picking the \"best\" model often\nrequires time-consuming model comparison. In this work, we introduce LAMVI-2, a\nvisual analytics system to support a developer in comparing hyperparameter\nsettings and outcomes. By focusing on word-embedding models (\"deep learning for\ntext\") we integrate views to compare both high-level statistics as well as\ninternal model behaviors (e.g., comparing word 'distances'). We demonstrate how\ndevelopers can work with LAMVI-2 to more quickly and accurately narrow down an\nappropriate and effective application-specific model. \n\n"}
{"id": "1810.11829", "contents": "Title: On preserving non-discrimination when combining expert advice Abstract: We study the interplay between sequential decision making and avoiding\ndiscrimination against protected groups, when examples arrive online and do not\nfollow distributional assumptions. We consider the most basic extension of\nclassical online learning: \"Given a class of predictors that are individually\nnon-discriminatory with respect to a particular metric, how can we combine them\nto perform as well as the best predictor, while preserving non-discrimination?\"\nSurprisingly we show that this task is unachievable for the prevalent notion of\n\"equalized odds\" that requires equal false negative rates and equal false\npositive rates across groups. On the positive side, for another notion of\nnon-discrimination, \"equalized error rates\", we show that running separate\ninstances of the classical multiplicative weights algorithm for each group\nachieves this guarantee. Interestingly, even for this notion, we show that\nalgorithms with stronger performance guarantees than multiplicative weights\ncannot preserve non-discrimination. \n\n"}
{"id": "1810.12153", "contents": "Title: Deep learning long-range information in undirected graphs with wave\n  networks Abstract: Graph algorithms are key tools in many fields of science and technology. Some\nof these algorithms depend on propagating information between distant nodes in\na graph. Recently, there have been a number of deep learning architectures\nproposed to learn on undirected graphs. However, most of these architectures\naggregate information in the local neighborhood of a node, and therefore they\nmay not be capable of efficiently propagating long-range information. To solve\nthis problem we examine a recently proposed architecture, wave, which\npropagates information back and forth across an undirected graph in waves of\nnonlinear computation. We compare wave to graph convolution, an architecture\nbased on local aggregation, and find that wave learns three different\ngraph-based tasks with greater efficiency and accuracy. These three tasks\ninclude (1) labeling a path connecting two nodes in a graph, (2) solving a maze\npresented as an image, and (3) computing voltages in a circuit. These tasks\nrange from trivial to very difficult, but wave can extrapolate from small\ntraining examples to much larger testing examples. These results show that wave\nmay be able to efficiently solve a wide range of problems that require\nlong-range information propagation across undirected graphs. An implementation\nof the wave network, and example code for the maze problem are included in the\ntflon deep learning toolkit (https://bitbucket.org/mkmatlock/tflon). \n\n"}
{"id": "1810.12278", "contents": "Title: Towards Principled Uncertainty Estimation for Deep Neural Networks Abstract: When the cost of misclassifying a sample is high, it is useful to have an\naccurate estimate of uncertainty in the prediction for that sample. There are\nalso multiple types of uncertainty which are best estimated in different ways,\nfor example, uncertainty that is intrinsic to the training set may be\nwell-handled by a Bayesian approach, while uncertainty introduced by shifts\nbetween training and query distributions may be better-addressed by\ndensity/support estimation. In this paper, we examine three types of\nuncertainty: model capacity uncertainty, intrinsic data uncertainty, and open\nset uncertainty, and review techniques that have been derived to address each\none. We then introduce a unified hierarchical model, which combines methods\nfrom Bayesian inference, invertible latent density inference, and\ndiscriminative classification in a single end-to-end deep neural network\ntopology to yield efficient per-sample uncertainty estimation in a detection\ncontext. This approach addresses all three uncertainty types and can readily\naccommodate prior/base rates for binary detection. We then discuss how to\nextend this model to a more generic multiclass recognition context. \n\n"}
{"id": "1810.12856", "contents": "Title: Discovering state-parameter mappings in subsurface models using\n  generative adversarial networks Abstract: A fundamental problem in geophysical modeling is related to the\nidentification and approximation of causal structures among physical processes.\nHowever, resolving the bidirectional mappings between physical parameters and\nmodel state variables (i.e., solving the forward and inverse problems) is\nchallenging, especially when parameter dimensionality is high. Deep learning\nhas opened a new door toward knowledge representation and complex pattern\nidentification. In particular, the recently introduced generative adversarial\nnetworks (GANs) hold strong promises in learning cross-domain mappings for\nimage translation. This study presents a state-parameter identification GAN\n(SPID-GAN) for simultaneously learning bidirectional mappings between a\nhigh-dimensional parameter space and the corresponding model state space.\nSPID-GAN is demonstrated using a series of representative problems from\nsubsurface flow modeling. Results show that SPID-GAN achieves satisfactory\nperformance in identifying the bidirectional state-parameter mappings,\nproviding a new deep-learning-based, knowledge representation paradigm for a\nwide array of complex geophysical problems. \n\n"}
{"id": "1811.00178", "contents": "Title: Online learning using multiple times weight updating Abstract: Online learning makes sequence of decisions with partial data arrival where\nnext movement of data is unknown. In this paper, we have presented a new\ntechnique as multiple times weight updating that update the weight iteratively\nforsame instance. The proposed technique analyzed with popular state-of-art\nalgorithms from literature and experimented using established tool. The results\nindicates that mistake rate reduces to zero or close to zero for various\ndatasets and algorithms. The overhead running cost is not too expensive and\nachieving mistake rate close to zero further strengthen the proposed technique.\nThe present work include bound nature of weight updating for single instance\nand achieve optimal weight value. This proposed work could be extended to big\ndatasets problems to reduce mistake rate in online learning environment. Also,\nthe proposed technique could be helpful to meet real life challenges. \n\n"}
{"id": "1811.00539", "contents": "Title: Deep Structured Prediction with Nonlinear Output Transformations Abstract: Deep structured models are widely used for tasks like semantic segmentation,\nwhere explicit correlations between variables provide important prior\ninformation which generally helps to reduce the data needs of deep nets.\nHowever, current deep structured models are restricted by oftentimes very local\nneighborhood structure, which cannot be increased for computational complexity\nreasons, and by the fact that the output configuration, or a representation\nthereof, cannot be transformed further. Very recent approaches which address\nthose issues include graphical model inference inside deep nets so as to permit\nsubsequent non-linear output space transformations. However, optimization of\nthose formulations is challenging and not well understood. Here, we develop a\nnovel model which generalizes existing approaches, such as structured\nprediction energy networks, and discuss a formulation which maintains\napplicability of existing inference techniques. \n\n"}
{"id": "1811.00710", "contents": "Title: On subexponential running times for approximating directed Steiner tree\n  and related problems Abstract: This paper concerns proving almost tight (super-polynomial) running times,\nfor achieving desired approximation ratios for various problems. To illustrate,\nthe question we study, let us consider the Set-Cover problem with n elements\nand m sets. Now we specify our goal to approximate Set-Cover to a factor of\n(1-d)ln n, for a given parameter 0<d<1. What is the best possible running time\nfor achieving such approximation? This question was answered implicitly in the\nwork of Moshkovitz [Theory of Computing, 2015]: Assuming both the Projection\nGames Conjecture (PGC) and the Exponential-Time Hypothesis (ETH), any ((1-d) ln\nn)-approximation algorithm for Set-Cover must run in time >= 2^{n^{c d}}, for\nsome constant 0<d<1.\n  We study the questions along this line. First, we show that under ETH and PGC\nany ((1-d) \\ln n)-approximation for Set-Cover requires 2^{n^{d}}-time. This\n(almost) matches the running time of 2^{O(n^d)} for approximating Set-Cover to\na factor (1-d) ln n by Cygan et al. [IPL, 2009]. Our result is tight up to the\nconstant multiplying the n^{d} terms in the exponent. This lower bound applies\nto all of its generalizations, e.g., Group Steiner Tree (GST), Directed Steiner\n(DST), Covering Steiner Tree (CST), Connected Polymatroid (CP). We also show\nthat in almost exponential time, these problems reduce to Set-Cover: We show\n(1-d)ln n approximation algorithms for all these problems that run in time\n2^{n^{d \\log n } poly(m).\n  We also study log^{2-d}n approximation for GST. Chekuri-Pal [FOCS, 2005]\nshowed that GST admits (log^{2-d}n)-approximation in time\nexp(2^{log^{d+o(1)}n}), for any 0 < d < 1. We show the lower bound of GST: any\n(log^{2-d}n)-approximation for GST must run in time >=\nexp((1+o(1)){log^{d-c}n}), for any c>0, unless the ETH is false. Our result\nfollows by analyzing the work of Halperin and Krauthgamer [STOC, 2003]. The\nsame lower and upper bounds hold for CST. \n\n"}
{"id": "1811.00821", "contents": "Title: OrthoNet: Multilayer Network Data Clustering Abstract: Network data appears in very diverse applications, like biological, social,\nor sensor networks. Clustering of network nodes into categories or communities\nhas thus become a very common task in machine learning and data mining. Network\ndata comes with some information about the network edges. In some cases, this\nnetwork information can even be given with multiple views or multiple layers,\neach one representing a different type of relationship between the network\nnodes. Increasingly often, network nodes also carry a feature vector. We\npropose in this paper to extend the node clustering problem, that commonly\nconsiders only the network information, to a problem where both the network\ninformation and the node features are considered together for learning a\nclustering-friendly representation of the feature space. Specifically, we\ndesign a generic two-step algorithm for multilayer network data clustering. The\nfirst step aggregates the different layers of network information into a graph\nrepresentation given by the geometric mean of the network Laplacian matrices.\nThe second step uses a neural net to learn a feature embedding that is\nconsistent with the structure given by the network layers. We propose a novel\nalgorithm for efficiently training the neural net via stochastic gradient\ndescent, which encourages the neural net outputs to span the leading\neigenvectors of the aggregated Laplacian matrix, in order to capture the\npairwise interactions on the network, and provide a clustering-friendly\nrepresentation of the feature space. We demonstrate with an extensive set of\nexperiments on synthetic and real datasets that our method leads to a\nsignificant improvement w.r.t. state-of-the-art multilayer graph clustering\nalgorithms, as it judiciously combines nodes features and network information\nin the node embedding algorithms. \n\n"}
{"id": "1811.00944", "contents": "Title: Spectral Methods from Tensor Networks Abstract: A tensor network is a diagram that specifies a way to \"multiply\" a collection\nof tensors together to produce another tensor (or matrix). Many existing\nalgorithms for tensor problems (such as tensor decomposition and tensor PCA),\nalthough they are not presented this way, can be viewed as spectral methods on\nmatrices built from simple tensor networks. In this work we leverage the full\npower of this abstraction to design new algorithms for certain continuous\ntensor decomposition problems.\n  An important and challenging family of tensor problems comes from orbit\nrecovery, a class of inference problems involving group actions (inspired by\napplications such as cryo-electron microscopy). Orbit recovery problems over\nfinite groups can often be solved via standard tensor methods. However, for\ninfinite groups, no general algorithms are known. We give a new spectral\nalgorithm based on tensor networks for one such problem: continuous\nmulti-reference alignment over the infinite group SO(2). Our algorithm extends\nto the more general heterogeneous case. \n\n"}
{"id": "1811.00972", "contents": "Title: Clustering and Learning from Imbalanced Data Abstract: A learning classifier must outperform a trivial solution, in case of\nimbalanced data, this condition usually does not hold true. To overcome this\nproblem, we propose a novel data level resampling method - Clustering Based\nOversampling for improved learning from class imbalanced datasets. The\nessential idea behind the proposed method is to use the distance between a\nminority class sample and its respective cluster centroid to infer the number\nof new sample points to be generated for that minority class sample. The\nproposed algorithm has very less dependence on the technique used for finding\ncluster centroids and does not effect the majority class learning in any way.\nIt also improves learning from imbalanced data by incorporating the\ndistribution structure of minority class samples in generation of new data\nsamples. The newly generated minority class data is handled in a way as to\nprevent outlier production and overfitting. Implementation analysis on\ndifferent datasets using deep neural networks as the learning classifier shows\nthe effectiveness of this method as compared to other synthetic data resampling\ntechniques across several evaluation metrics. \n\n"}
{"id": "1811.01135", "contents": "Title: Content preserving text generation with attribute controls Abstract: In this work, we address the problem of modifying textual attributes of\nsentences. Given an input sentence and a set of attribute labels, we attempt to\ngenerate sentences that are compatible with the conditioning information. To\nensure that the model generates content compatible sentences, we introduce a\nreconstruction loss which interpolates between auto-encoding and\nback-translation loss components. We propose an adversarial loss to enforce\ngenerated samples to be attribute compatible and realistic. Through\nquantitative, qualitative and human evaluations we demonstrate that our model\nis capable of generating fluent sentences that better reflect the conditioning\ninformation compared to prior methods. We further demonstrate that the model is\ncapable of simultaneously controlling multiple attributes. \n\n"}
{"id": "1811.01177", "contents": "Title: Smoothed Analysis of the Art Gallery Problem Abstract: In the Art Gallery Problem we are given a polygon $P\\subset [0,L]^2$ on $n$\nvertices and a number $k$. We want to find a guard set $G$ of size $k$, such\nthat each point in $P$ is seen by a guard in $G$. Formally, a guard $g$ sees a\npoint $p \\in P$ if the line segment $pg$ is fully contained inside the polygon\n$P$. The history and practical findings indicate that irrational coordinates\nare a \"very rare\" phenomenon. We give a theoretical explanation. Next to worst\ncase analysis, Smoothed Analysis gained popularity to explain the practical\nperformance of algorithms, even if they perform badly in the worst case. The\nidea is to study the expected performance on small perturbations of the worst\ninput. The performance is measured in terms of the magnitude $\\delta$ of the\nperturbation and the input size. We consider four different models of\nperturbation. We show that the expected number of bits to describe optimal\nguard positions per guard is logarithmic in the input and the magnitude of the\nperturbation. This shows from a theoretical perspective that rational guards\nwith small bit-complexity are typical. Note that describing the guard position\nis the bottleneck to show NP-membership. The significance of our results is\nthat algebraic methods are not needed to solve the Art Gallery Problem in\ntypical instances. This is the first time an $\\exists\\mathbb{R}$-complete\nproblem was analyzed by Smoothed Analysis. \n\n"}
{"id": "1811.01903", "contents": "Title: Lower Bounds for Parallel and Randomized Convex Optimization Abstract: We study the question of whether parallelization in the exploration of the\nfeasible set can be used to speed up convex optimization, in the local oracle\nmodel of computation. We show that the answer is negative for both\ndeterministic and randomized algorithms applied to essentially any of the\ninteresting geometries and nonsmooth, weakly-smooth, or smooth objective\nfunctions. In particular, we show that it is not possible to obtain a\npolylogarithmic (in the sequential complexity of the problem) number of\nparallel rounds with a polynomial (in the dimension) number of queries per\nround. In the majority of these settings and when the dimension of the space is\npolynomial in the inverse target accuracy, our lower bounds match the oracle\ncomplexity of sequential convex optimization, up to at most a logarithmic\nfactor in the dimension, which makes them (nearly) tight. Prior to our work,\nlower bounds for parallel convex optimization algorithms were only known in a\nsmall fraction of the settings considered in this paper, mainly applying to\nEuclidean ($\\ell_2$) and $\\ell_\\infty$ spaces. Our work provides a more general\napproach for proving lower bounds in the setting of parallel convex\noptimization. \n\n"}
{"id": "1811.02023", "contents": "Title: Ordered Graph Limits and Their Applications Abstract: The emerging theory of graph limits exhibits an analytic perspective on\ngraphs, showing that many important concepts and tools in graph theory and its\napplications can be described more naturally (and sometimes proved more easily)\nin analytic language. We extend the theory of graph limits to the ordered\nsetting, presenting a limit object for dense vertex-ordered graphs, which we\ncall an orderon. As a special case, this yields limit objects for matrices\nwhose rows and columns are ordered, and for dynamic graphs that expand (via\nvertex insertions) over time. Along the way, we devise an ordered\nlocality-preserving variant of the cut distance between ordered graphs, showing\nthat two graphs are close with respect to this distance if and only if they are\nsimilar in terms of their ordered subgraph frequencies. We show that the space\nof orderons is compact with respect to this distance notion, which is key to a\nsuccessful analysis of combinatorial objects through their limits.\n  We derive several applications of the ordered limit theory in extremal\ncombinatorics, sampling, and property testing in ordered graphs. In particular,\nwe prove a new ordered analogue of the well-known result by Alon and Stav\n[RS\\&A'08] on the furthest graph from a hereditary property; this is the first\nknown result of this type in the ordered setting. Unlike the unordered regime,\nhere the random graph model $G(n, p)$ with an ordering over the vertices is not\nalways asymptotically the furthest from the property for some $p$. However,\nusing our ordered limit theory, we show that random graphs generated by a\nstochastic block model, where the blocks are consecutive in the vertex\nordering, are (approximately) the furthest. Additionally, we describe an\nalternative analytic proof of the ordered graph removal lemma [Alon et al.,\nFOCS'17]. \n\n"}
{"id": "1811.02471", "contents": "Title: Convolutional LSTMs for Cloud-Robust Segmentation of Remote Sensing\n  Imagery Abstract: Clouds frequently cover the Earth's surface and pose an omnipresent challenge\nto optical Earth observation methods. The vast majority of remote sensing\napproaches either selectively choose single cloud-free observations or employ a\npre-classification strategy to identify and mask cloudy pixels. We follow a\ndifferent strategy and treat cloud coverage as noise that is inherent to the\nobserved satellite data. In prior work, we directly employed a straightforward\n\\emph{convolutional long short-term memory} network for vegetation\nclassification without explicit cloud filtering and achieved state-of-the-art\nclassification accuracies. In this work, we investigate this cloud-robustness\nfurther by visualizing internal cell activations and performing an ablation\nexperiment on datasets of different cloud coverage. In the visualizations of\nnetwork states, we identified some cells in which modulation and input gates\nclosed on cloudy pixels. This indicates that the network has internalized a\ncloud-filtering mechanism without being specifically trained on cloud labels.\nOverall, our results question the necessity of sophisticated pre-processing\npipelines for multi-temporal deep learning approaches. \n\n"}
{"id": "1811.02564", "contents": "Title: On exponential convergence of SGD in non-convex over-parametrized\n  learning Abstract: Large over-parametrized models learned via stochastic gradient descent (SGD)\nmethods have become a key element in modern machine learning. Although SGD\nmethods are very effective in practice, most theoretical analyses of SGD\nsuggest slower convergence than what is empirically observed. In our recent\nwork [8] we analyzed how interpolation, common in modern over-parametrized\nlearning, results in exponential convergence of SGD with constant step size for\nconvex loss functions. In this note, we extend those results to a much broader\nnon-convex function class satisfying the Polyak-Lojasiewicz (PL) condition. A\nnumber of important non-convex problems in machine learning, including some\nclasses of neural networks, have been recently shown to satisfy the PL\ncondition. We argue that the PL condition provides a relevant and attractive\nsetting for many machine learning problems, particularly in the\nover-parametrized regime. \n\n"}
{"id": "1811.02659", "contents": "Title: Machine Learning Algorithms for Classification of Microcirculation\n  Images from Septic and Non-Septic Patients Abstract: Sepsis is a life-threatening disease and one of the major causes of death in\nhospitals. Imaging of microcirculatory dysfunction is a promising approach for\nautomated diagnosis of sepsis. We report a machine learning classifier capable\nof distinguishing non-septic and septic images from dark field microcirculation\nvideos of patients. The classifier achieves an accuracy of 89.45%. The area\nunder the receiver operating characteristics of the classifier was 0.92, the\nprecision was 0.92 and the recall was 0.84. Codes representing the learned\nfeature space of trained classifier were visualized using t-SNE embedding and\nwere separable and distinguished between images from critically ill and\nnon-septic patients. Using an unsupervised convolutional autoencoder,\nindependent of the clinical diagnosis, we also report clustering of learned\nfeatures from a compressed representation associated with healthy images and\nthose with microcirculatory dysfunction. The feature space used by our trained\nclassifier to distinguish between images from septic and non-septic patients\nhas potential diagnostic application. \n\n"}
{"id": "1811.02944", "contents": "Title: Connecting Knowledge Compilation Classes and Width Parameters Abstract: The field of knowledge compilation establishes the tractability of many tasks\nby studying how to compile them to Boolean circuit classes obeying some\nrequirements such as structuredness, decomposability, and determinism. However,\nin other settings such as intensional query evaluation on databases, we obtain\nBoolean circuits that satisfy some width bounds, e.g., they have bounded\ntreewidth or pathwidth. In this work, we give a systematic picture of many\ncircuit classes considered in knowledge compilation and show how they can be\nsystematically connected to width measures, through upper and lower bounds. Our\nupper bounds show that bounded-treewidth circuits can be constructively\nconverted to d-SDNNFs, in time linear in the circuit size and singly\nexponential in the treewidth; and that bounded-pathwidth circuits can similarly\nbe converted to uOBDDs. We show matching lower bounds on the compilation of\nmonotone DNF or CNF formulas to structured targets, assuming a constant bound\non the arity (size of clauses) and degree (number of occurrences of each\nvariable): any d-SDNNF (resp., SDNNF) for such a DNF (resp., CNF) must be of\nexponential size in its treewidth, and the same holds for uOBDDs (resp.,\nn-OBDDs) when considering pathwidth. Unlike most previous work, our bounds\napply to any formula of this class, not just a well-chosen family. Hence, we\nshow that pathwidth and treewidth respectively characterize the efficiency of\ncompiling monotone DNFs to uOBDDs and d-SDNNFs with compilation being singly\nexponential in the corresponding width parameter. We also show that our lower\nbounds on CNFs extend to unstructured compilation targets, with an exponential\nlower bound in the treewidth (resp., pathwidth) when compiling monotone CNFs of\nconstant arity and degree to DNNFs (resp., nFBDDs). \n\n"}
{"id": "1811.03402", "contents": "Title: A Survey on Data Collection for Machine Learning: a Big Data -- AI\n  Integration Perspective Abstract: Data collection is a major bottleneck in machine learning and an active\nresearch topic in multiple communities. There are largely two reasons data\ncollection has recently become a critical issue. First, as machine learning is\nbecoming more widely-used, we are seeing new applications that do not\nnecessarily have enough labeled data. Second, unlike traditional machine\nlearning, deep learning techniques automatically generate features, which saves\nfeature engineering costs, but in return may require larger amounts of labeled\ndata. Interestingly, recent research in data collection comes not only from the\nmachine learning, natural language, and computer vision communities, but also\nfrom the data management community due to the importance of handling large\namounts of data. In this survey, we perform a comprehensive study of data\ncollection from a data management point of view. Data collection largely\nconsists of data acquisition, data labeling, and improvement of existing data\nor models. We provide a research landscape of these operations, provide\nguidelines on which technique to use when, and identify interesting research\nchallenges. The integration of machine learning and data management for data\ncollection is part of a larger trend of Big data and Artificial Intelligence\n(AI) integration and opens many opportunities for new research. \n\n"}
{"id": "1811.03537", "contents": "Title: Iterative Classroom Teaching Abstract: We consider the machine teaching problem in a classroom-like setting wherein\nthe teacher has to deliver the same examples to a diverse group of students.\nTheir diversity stems from differences in their initial internal states as well\nas their learning rates. We prove that a teacher with full knowledge about the\nlearning dynamics of the students can teach a target concept to the entire\nclassroom using O(min{d,N} log(1/eps)) examples, where d is the ambient\ndimension of the problem, N is the number of learners, and eps is the accuracy\nparameter. We show the robustness of our teaching strategy when the teacher has\nlimited knowledge of the learners' internal dynamics as provided by a noisy\noracle. Further, we study the trade-off between the learners' workload and the\nteacher's cost in teaching the target concept. Our experiments validate our\ntheoretical results and suggest that appropriately partitioning the classroom\ninto homogenous groups provides a balance between these two objectives. \n\n"}
{"id": "1811.03831", "contents": "Title: Adaptive Regularization Algorithms with Inexact Evaluations for\n  Nonconvex Optimization Abstract: A regularization algorithm using inexact function values and inexact\nderivatives is proposed and its evaluation complexity analyzed. This algorithm\nis applicable to unconstrained problems and to problems with inexpensive\nconstraints (that is constraints whose evaluation and enforcement has\nnegligible cost) under the assumption that the derivative of highest degree is\n$\\beta$-H\\\"{o}lder continuous. It features a very flexible adaptive mechanism\nfor determining the inexactness which is allowed, at each iteration, when\ncomputing objective function values and derivatives. The complexity analysis\ncovers arbitrary optimality order and arbitrary degree of available approximate\nderivatives. It extends results of Cartis, Gould and Toint (2018) on the\nevaluation complexity to the inexact case: if a $q$th order minimizer is sought\nusing approximations to the first $p$ derivatives, it is proved that a suitable\napproximate minimizer within $\\epsilon$ is computed by the proposed algorithm\nin at most $O(\\epsilon^{-\\frac{p+\\beta}{p-q+\\beta}})$ iterations and at most\n$O(|\\log(\\epsilon)|\\epsilon^{-\\frac{p+\\beta}{p-q+\\beta}})$ approximate\nevaluations. An algorithmic variant, although more rigid in practice, can be\nproved to find such an approximate minimizer in\n$O(|\\log(\\epsilon)|+\\epsilon^{-\\frac{p+\\beta}{p-q+\\beta}})$ evaluations.While\nthe proposed framework remains so far conceptual for high degrees and orders,\nit is shown to yield simple and computationally realistic inexact methods when\nspecialized to the unconstrained and bound-constrained first- and second-order\ncases. The deterministic complexity results are finally extended to the\nstochastic context, yielding adaptive sample-size rules for subsampling methods\ntypical of machine learning. \n\n"}
{"id": "1811.03841", "contents": "Title: Unique End of Potential Line Abstract: This paper studies the complexity of problems in PPAD $\\cap$ PLS that have\nunique solutions. Three well-known examples of such problems are the problem of\nfinding a fixpoint of a contraction map, finding the unique sink of a Unique\nSink Orientation (USO), and solving the P-matrix Linear Complementarity Problem\n(P-LCP). Each of these are promise-problems, and when the promise holds, they\nalways possess unique solutions.\n  We define the complexity class UEOPL to capture problems of this type. We\nfirst define a class that we call EOPL, which consists of all problems that can\nbe reduced to End-of-Potential-Line. This problem merges the canonical\nPPAD-complete problem End-of-Line, with the canonical PLS-complete problem\nSink-of-Dag, and so EOPL captures problems that can be solved by a\nline-following algorithm that also simultaneously decreases a potential\nfunction.\n  Promise-UEOPL is a promise-subclass of EOPL in which the line in the\nEnd-of-Potential-Line instance is guaranteed to be unique via a promise. We\nturn this into a non-promise class UEOPL, by adding an extra solution type to\nEOPL that captures any pair of points that are provably on two different lines.\n  We show that UEOPL $\\subseteq$ EOPL $\\subseteq$ CLS, and that all of our\nmotivating problems are contained in UEOPL: specifically USO, P-LCP, and\nfinding a fixpoint of a Piecewise-Linear Contraction under an $\\ell_p$-norm all\nlie in UEOPL. Our results also imply that parity games, mean-payoff games,\ndiscounted games, and simple-stochastic games lie in UEOPL.\n  All of our containment results are proved via a reduction to a problem that\nwe call One-Permutation Discrete Contraction (OPDC). This problem is motivated\nby a discretized version of contraction, but it is also closely related to the\nUSO problem. We show that OPDC lies in UEOPL, and we are also able to show that\nOPDC is UEOPL-complete. \n\n"}
{"id": "1811.06787", "contents": "Title: Unifying lower bounds for algebraic machines, semantically Abstract: This paper presents a new abstract method for proving lower bounds in\ncomputational complexity. Based on the notion of topological and measurable\nentropy for dynamical systems, it is shown to generalise three previous lower\nbounds results from the literature in algebraic complexity. We use it to prove\nthat maxflow, a Ptime complete problem, is not computable in polylogarithmic\ntime on parallel random access machines (prams) working with real numbers. This\nimproves, albeit slightly, on a result of Mulmuley since the class of machines\nconsidered extends the class \"prams without bit operations\", making more\nprecise the relationship between Mulmuley's result and similar lower bounds on\nreal prams.\n  More importantly, we show our method captures previous lower bounds results\nfrom the literature, thus providing a unifying framework for \"topological\"\nproofs of lower bounds: Steele and Yao's lower bounds for algebraic decision\ntrees, Ben-Or's lower bounds for algebraic computation trees, Cucker's proof\nthat NC is not equal to Ptime in the real case, and Mulmuley's lower bounds for\n\"prams without bit operations\". \n\n"}
{"id": "1811.07315", "contents": "Title: Learning to infer: RL-based search for DNN primitive selection on\n  Heterogeneous Embedded Systems Abstract: Deep Learning is increasingly being adopted by industry for computer vision\napplications running on embedded devices. While Convolutional Neural Networks'\naccuracy has achieved a mature and remarkable state, inference latency and\nthroughput are a major concern especially when targeting low-cost and low-power\nembedded platforms. CNNs' inference latency may become a bottleneck for Deep\nLearning adoption by industry, as it is a crucial specification for many\nreal-time processes. Furthermore, deployment of CNNs across heterogeneous\nplatforms presents major compatibility issues due to vendor-specific technology\nand acceleration libraries. In this work, we present QS-DNN, a fully automatic\nsearch based on Reinforcement Learning which, combined with an inference engine\noptimizer, efficiently explores through the design space and empirically finds\nthe optimal combinations of libraries and primitives to speed up the inference\nof CNNs on heterogeneous embedded devices. We show that, an optimized\ncombination can achieve 45x speedup in inference latency on CPU compared to a\ndependency-free baseline and 2x on average on GPGPU compared to the best vendor\nlibrary. Further, we demonstrate that, the quality of results and time\n\"to-solution\" is much better than with Random Search and achieves up to 15x\nbetter results for a short-time search. \n\n"}
{"id": "1811.08568", "contents": "Title: High-Level Strategy Selection under Partial Observability in StarCraft:\n  Brood War Abstract: We consider the problem of high-level strategy selection in the adversarial\nsetting of real-time strategy games from a reinforcement learning perspective,\nwhere taking an action corresponds to switching to the respective strategy.\nHere, a good strategy successfully counters the opponent's current and possible\nfuture strategies which can only be estimated using partial observations. We\ninvestigate whether we can utilize the full game state information during\ntraining time (in the form of an auxiliary prediction task) to increase\nperformance. Experiments carried out within a StarCraft: Brood War bot against\nstrong community bots show substantial win rate improvements over a\nfixed-strategy baseline and encouraging results when learning with the\nauxiliary task. \n\n"}
{"id": "1811.08764", "contents": "Title: Regularizing by the Variance of the Activations' Sample-Variances Abstract: Normalization techniques play an important role in supporting efficient and\noften more effective training of deep neural networks. While conventional\nmethods explicitly normalize the activations, we suggest to add a loss term\ninstead. This new loss term encourages the variance of the activations to be\nstable and not vary from one random mini-batch to the next. As we prove, this\nencourages the activations to be distributed around a few distinct modes. We\nalso show that if the inputs are from a mixture of two Gaussians, the new loss\nwould either join the two together, or separate between them optimally in the\nLDA sense, depending on the prior probabilities. Finally, we are able to link\nthe new regularization term to the batchnorm method, which provides it with a\nregularization perspective. Our experiments demonstrate an improvement in\naccuracy over the batchnorm technique for both CNNs and fully connected\nnetworks. \n\n"}
{"id": "1811.08800", "contents": "Title: MGCN: Semi-supervised Classification in Multi-layer Graphs with Graph\n  Convolutional Networks Abstract: Graph embedding is an important approach for graph analysis tasks such as\nnode classification and link prediction. The goal of graph embedding is to find\na low dimensional representation of graph nodes that preserves the graph\ninformation. Recent methods like Graph Convolutional Network (GCN) try to\nconsider node attributes (if available) besides node relations and learn node\nembeddings for unsupervised and semi-supervised tasks on graphs. On the other\nhand, multi-layer graph analysis has been received attention recently. However,\nthe existing methods for multi-layer graph embedding cannot incorporate all\navailable information (like node attributes). Moreover, most of them consider\neither type of nodes or type of edges, and they do not treat within and between\nlayer edges differently. In this paper, we propose a method called MGCN that\nutilizes the GCN for multi-layer graphs. MGCN embeds nodes of multi-layer\ngraphs using both within and between layers relations and nodes attributes. We\nevaluate our method on the semi-supervised node classification task.\nExperimental results demonstrate the superiority of the proposed method to\nother multi-layer and single-layer competitors and also show the positive\neffect of using cross-layer edges. \n\n"}
{"id": "1811.08812", "contents": "Title: Adversarial Classifier for Imbalanced Problems Abstract: Adversarial approach has been widely used for data generation in the last few\nyears. However, this approach has not been extensively utilized for classifier\ntraining. In this paper, we propose an adversarial framework for classifier\ntraining that can also handle imbalanced data. Indeed, a network is trained via\nan adversarial approach to give weights to samples of the majority class such\nthat the obtained classification problem becomes more challenging for the\ndiscriminator and thus boosts its classification capability. In addition to the\ngeneral imbalanced classification problems, the proposed method can also be\nused for problems such as graph representation learning in which it is desired\nto discriminate similar nodes from dissimilar nodes. Experimental results on\nimbalanced data classification and on the tasks like graph link prediction show\nthe superiority of the proposed method compared to the state-of-the-art\nmethods. \n\n"}
{"id": "1811.08871", "contents": "Title: Efficient nonmyopic active search with applications in drug and\n  materials discovery Abstract: Active search is a learning paradigm for actively identifying as many members\nof a given class as possible. A critical target scenario is high-throughput\nscreening for scientific discovery, such as drug or materials discovery. In\nthis paper, we approach this problem in Bayesian decision framework. We first\nderive the Bayesian optimal policy under a natural utility, and establish a\ntheoretical hardness of active search, proving that the optimal policy can not\nbe approximated for any constant ratio. We also study the batch setting for the\nfirst time, where a batch of $b>1$ points can be queried at each iteration. We\ngive an asymptotic lower bound, linear in batch size, on the adaptivity gap:\nhow much we could lose if we query $b$ points at a time for $t$ iterations,\ninstead of one point at a time for $bt$ iterations. We then introduce a novel\napproach to nonmyopic approximations of the optimal policy that admits\nefficient computation. Our proposed policy can automatically trade off\nexploration and exploitation, without relying on any tuning parameters. We also\ngeneralize our policy to batch setting, and propose two approaches to tackle\nthe combinatorial search challenge. We evaluate our proposed policies on a\nlarge database of drug discovery and materials science. Results demonstrate the\nsuperior performance of our proposed policy in both sequential and batch\nsetting; the nonmyopic behavior is also illustrated in various aspects. \n\n"}
{"id": "1811.08996", "contents": "Title: HyperAdam: A Learnable Task-Adaptive Adam for Network Training Abstract: Deep neural networks are traditionally trained using human-designed\nstochastic optimization algorithms, such as SGD and Adam. Recently, the\napproach of learning to optimize network parameters has emerged as a promising\nresearch topic. However, these learned black-box optimizers sometimes do not\nfully utilize the experience in human-designed optimizers, therefore have\nlimitation in generalization ability. In this paper, a new optimizer, dubbed as\n\\textit{HyperAdam}, is proposed that combines the idea of \"learning to\noptimize\" and traditional Adam optimizer. Given a network for training, its\nparameter update in each iteration generated by HyperAdam is an adaptive\ncombination of multiple updates generated by Adam with varying decay rates. The\ncombination weights and decay rates in HyperAdam are adaptively learned\ndepending on the task. HyperAdam is modeled as a recurrent neural network with\nAdamCell, WeightCell and StateCell. It is justified to be state-of-the-art for\nvarious network training, such as multilayer perceptron, CNN and LSTM. \n\n"}
{"id": "1811.09906", "contents": "Title: Efficient constructions of convex combinations for 2-edge-connected\n  subgraphs on fundamental classes Abstract: Finding the exact integrality gap $\\alpha$ for the LP relaxation of the\n2-edge-connected spanning multigraph problem (2EC) is closely related to the\nsame problem for the Held-Karp relaxation of the metric traveling salesman\nproblem (TSP). While the former problem seems easier than the latter, since it\nis less constrained, currently the upper bounds on the respective integrality\ngaps for the two problems are the same.\n  An approach to proving integrality gaps for both of these problems is to\nconsider fundamental classes of extreme points. For 2EC, better bounds on the\nintegrality gap are known for certain important special cases of these\nfundamental points. For example, for half-integer square points, the\nintegrality gap is between $\\frac{6}{5}$ and $\\frac{4}{3}$. Our main result is\nto improve the approximation factor to $\\frac{9}{7}$ for 2EC for these points.\nOur approach is based on constructing convex combinations and our key tool is\nthe top-down coloring framework for tree augmentation, whose flexibility we\nemploy to exploit beneficial properties in both the initial spanning tree and\nin the input graph. We also show how these tools can be tailored to the closely\nrelated problem of uniform covers for which the proofs of the best-known bounds\ndo not yield polynomial-time algorithms. Another key ingredient is to use a\nrainbow spanning tree decomposition, which allows us to obtain a convex\ncombination of spanning trees with particular properties \n\n"}
{"id": "1811.10052", "contents": "Title: An overview of deep learning in medical imaging focusing on MRI Abstract: What has happened in machine learning lately, and what does it mean for the\nfuture of medical image analysis? Machine learning has witnessed a tremendous\namount of attention over the last few years. The current boom started around\n2009 when so-called deep artificial neural networks began outperforming other\nestablished models on a number of important benchmarks. Deep neural networks\nare now the state-of-the-art machine learning models across a variety of areas,\nfrom image analysis to natural language processing, and widely deployed in\nacademia and industry. These developments have a huge potential for medical\nimaging technology, medical data analysis, medical diagnostics and healthcare\nin general, slowly being realized. We provide a short overview of recent\nadvances and some associated challenges in machine learning applied to medical\nimage processing and image analysis. As this has become a very broad and fast\nexpanding field we will not survey the entire landscape of applications, but\nput particular focus on deep learning in MRI.\n  Our aim is threefold: (i) give a brief introduction to deep learning with\npointers to core references; (ii) indicate how deep learning has been applied\nto the entire MRI processing chain, from acquisition to image retrieval, from\nsegmentation to disease prediction; (iii) provide a starting point for people\ninterested in experimenting and perhaps contributing to the field of machine\nlearning for medical imaging by pointing out good educational resources,\nstate-of-the-art open-source code, and interesting sources of data and problems\nrelated medical imaging. \n\n"}
{"id": "1811.10501", "contents": "Title: Deep Ensemble Tensor Factorization for Longitudinal Patient Trajectories\n  Classification Abstract: We present a generative approach to classify scarcely observed longitudinal\npatient trajectories. The available time series are represented as tensors and\nfactorized using generative deep recurrent neural networks. The learned factors\nrepresent the patient data in a compact way and can then be used in a\ndownstream classification task. For more robustness and accuracy in the\npredictions, we used an ensemble of those deep generative models to mimic\nBayesian posterior sampling. We illustrate the performance of our architecture\non an intensive-care case study of in-hospital mortality prediction with 96\nlongitudinal measurement types measured across the first 48-hour from\nadmission. Our combination of generative and ensemble strategies achieves an\nAUC of over 0.85, and outperforms the SAPS-II mortality score and GRU\nbaselines. \n\n"}
{"id": "1811.10834", "contents": "Title: A Schur Complement Cheeger Inequality Abstract: Cheeger's inequality shows that any undirected graph $G$ with minimum nonzero\nnormalized Laplacian eigenvalue $\\lambda_G$ has a cut with conductance at most\n$O(\\sqrt{\\lambda_G})$. Qualitatively, Cheeger's inequality says that if the\nrelaxation time of a graph is high, there is a cut that certifies this.\nHowever, there is a gap in this relationship, as cuts can have conductance as\nlow as $\\Theta(\\lambda_G)$.\n  To better approximate the relaxation time of a graph, we consider a more\ngeneral object. Instead of bounding the mixing time with cuts, we bound it with\ncuts in graphs obtained by Schur complementing out vertices from the graph $G$.\nCombinatorially, these Schur complements describe random walks in $G$\nrestricted to a subset of its vertices. As a result, all Schur complement cuts\nhave conductance at least $\\Omega(\\lambda_G)$. We show that unlike with cuts,\nthis inequality is tight up to a constant factor. Specifically, there is a\nSchur complement cut with conductance at most $O(\\lambda_G)$. \n\n"}
{"id": "1811.12156", "contents": "Title: Improved Deep Embeddings for Inferencing with Multi-Layered Networks Abstract: Inferencing with network data necessitates the mapping of its nodes into a\nvector space, where the relationships are preserved. However, with\nmulti-layered networks, where multiple types of relationships exist for the\nsame set of nodes, it is crucial to exploit the information shared between\nlayers, in addition to the distinct aspects of each layer. In this paper, we\npropose a novel approach that first obtains node embeddings in all layers\njointly via DeepWalk on a \\textit{supra} graph, which allows interactions\nbetween layers, and then fine-tunes the embeddings to encourage cohesive\nstructure in the latent space. With empirical studies in node classification,\nlink prediction and multi-layered community detection, we show that the\nproposed approach outperforms existing single- and multi-layered network\nembedding algorithms on several benchmarks. In addition to effectively scaling\nto a large number of layers (tested up to $37$), our approach consistently\nproduces highly modular community structure, even when compared to methods that\ndirectly optimize for the modularity function. \n\n"}
{"id": "1811.12527", "contents": "Title: Algorithms and Hardness for Diameter in Dynamic Graphs Abstract: The diameter, radius and eccentricities are natural graph parameters. While\nthese problems have been studied extensively, there are no known dynamic\nalgorithms for them beyond the ones that follow from trivial recomputation\nafter each update or from solving dynamic All-Pairs Shortest Paths (APSP),\nwhich is very computationally intensive. This is the situation for dynamic\napproximation algorithms as well, and even if only edge insertions or edge\ndeletions need to be supported.\n  This paper provides a comprehensive study of the dynamic approximation of\nDiameter, Radius and Eccentricities, providing both conditional lower bounds,\nand new algorithms whose bounds are optimal under popular hypotheses in\nfine-grained complexity. Some of the highlights include:\n  - Under popular hardness hypotheses, there can be no significantly better\nfully dynamic approximation algorithms than recomputing the answer after each\nupdate, or maintaining full APSP.\n  - Nearly optimal partially dynamic (incremental/decremental) algorithms can\nbe achieved via efficient reductions to (incremental/decremental) maintenance\nof Single-Source Shortest Paths. For instance, a nearly\n$(3/2+\\epsilon)$-approximation to Diameter in directed or undirected graphs can\nbe maintained decrementally in total time $m^{1+o(1)}\\sqrt{n}/\\epsilon^2$. This\nnearly matches the static $3/2$-approximation algorithm for the problem that is\nknown to be conditionally optimal. \n\n"}
{"id": "1811.12547", "contents": "Title: The inverse Voronoi problem in graphs Abstract: We introduce the inverse Voronoi diagram problem in graphs: given a graph $G$\nwith positive edge-lengths and a collection $\\mathbb{U}$ of subsets of vertices\nof $V(G)$, decide whether $\\mathbb{U}$ is a Voronoi diagram in $G$ with respect\nto the shortest-path metric. We show that the problem is NP-hard, even for\nplanar graphs where all the edges have unit length. We also study the\nparameterized complexity of the problem and show that the problem is W[1]-hard\nwhen parameterized by the number of Voronoi cells or by the pathwidth of the\ngraph. For trees we show that the problem can be solved in $O(N+n \\log^2 n)$\ntime, where $n$ is the number of vertices in the tree and $N=n+\\sum_{U\\in\n\\mathbb{U}}|U|$ is the size of the description of the input. We also provide a\nlower bound of $\\Omega(n \\log n)$ time for trees with $n$ vertices. \n\n"}
{"id": "1811.12568", "contents": "Title: Parallelizing greedy for submodular set function maximization in\n  matroids and beyond Abstract: We consider parallel, or low adaptivity, algorithms for submodular function\nmaximization. This line of work was recently initiated by Balkanski and Singer\nand has already led to several interesting results on the cardinality\nconstraint and explicit packing constraints. An important open problem is the\nclassical setting of matroid constraint, which has been instrumental for\ndevelopments in submodular function maximization. In this paper we develop a\ngeneral strategy to parallelize the well-studied greedy algorithm and use it to\nobtain a randomized $\\left(\\frac{1}{2} - \\epsilon\\right)$-approximation in\n$\\operatorname{O}\\left( \\frac{\\log^2 n}{\\epsilon^2} \\right)$ rounds of\nadaptivity. We rely on this algorithm, and an elegant amplification approach\ndue to Badanidiyuru and Vondr\\'ak to obtain a fractional solution that yields a\nnear-optimal randomized $\\left( 1 - 1/e - \\epsilon \\right)$-approximation in\n$O\\left( {\\frac{\\log^2 n}{\\epsilon^3}} \\right) $ rounds of adaptivity. For\nnon-negative functions we obtain a $\\left( {3-2\\sqrt{2}}\\right)$-approximation\nand a fractional solution that yields a $\\left( {\\frac{1}{e} -\n\\epsilon}\\right)$-approximation. Our approach for parallelizing greedy yields\napproximations for intersections of matroids and matchoids, and the\napproximation ratios are comparable to those known for sequential greedy. \n\n"}
{"id": "1812.00172", "contents": "Title: Rank Projection Trees for Multilevel Neural Network Interpretation Abstract: A variety of methods have been proposed for interpreting nodes in deep neural\nnetworks, which typically involve scoring nodes at lower layers with respect to\ntheir effects on the output of higher-layer nodes (where lower and higher\nlayers are closer to the input and output layers, respectively). However, we\nmay be interested in picking out a prioritized collection of subsets of the\ninputs across a range of scales according to their importance for an output\nnode, and not simply a prioritized ranking across the inputs as singletons.\nSuch a situation may arise in biological applications, for instance, where we\nare interested in epistatic effects between groups of genes in determining a\ntrait of interest. Here, we outline a flexible framework which may be used to\ngenerate multiscale network interpretations, using any previously defined\nscoring function. We demonstrate the ability of our method to pick out\nbiologically important genes and gene sets in the domains of cancer and\npsychiatric genomics. \n\n"}
{"id": "1812.00181", "contents": "Title: Effects of Loss Functions And Target Representations on Adversarial\n  Robustness Abstract: Understanding and evaluating the robustness of neural networks under\nadversarial settings is a subject of growing interest. Attacks proposed in the\nliterature usually work with models trained to minimize cross-entropy loss and\noutput softmax probabilities. In this work, we present interesting experimental\nresults that suggest the importance of considering other loss functions and\ntarget representations, specifically, (1) training on mean-squared error and\n(2) representing targets as codewords generated from a random codebook. We\nevaluate the robustness of neural networks that implement these proposed\nmodifications using existing attacks, showing an increase in accuracy against\nuntargeted attacks of up to 98.7\\% and a decrease of targeted attack success\nrates of up to 99.8\\%. Our model demonstrates more robustness compared to its\nconventional counterpart even against attacks that are tailored to our\nmodifications. Furthermore, we find that the parameters of our modified model\nhave significantly smaller Lipschitz bounds, an important measure correlated\nwith a model's sensitivity to adversarial perturbations. \n\n"}
{"id": "1812.00338", "contents": "Title: Regularized Wasserstein Means for Aligning Distributional Data Abstract: We propose to align distributional data from the perspective of Wasserstein\nmeans. We raise the problem of regularizing Wasserstein means and propose\nseveral terms tailored to tackle different problems. Our formulation is based\non the variational transportation to distribute a sparse discrete measure into\nthe target domain. The resulting sparse representation well captures the\ndesired property of the domain while reducing the mapping cost. We demonstrate\nthe scalability and robustness of our method with examples in domain\nadaptation, point set registration, and skeleton layout. \n\n"}
{"id": "1812.00401", "contents": "Title: Investigating performance of neural networks and gradient boosting\n  models approximating microscopic traffic simulations in traffic optimization\n  tasks Abstract: We analyze the accuracy of traffic simulations metamodels based on neural\nnetworks and gradient boosting models (LightGBM), applied to traffic\noptimization as fitness functions of genetic algorithms. Our metamodels\napproximate outcomes of traffic simulations (the total time of waiting on a red\nsignal) taking as an input different traffic signal settings, in order to\nefficiently find (sub)optimal settings. Their accuracy was proven to be very\ngood on randomly selected test sets, but it turned out that the accuracy may\ndrop in case of settings expected (according to genetic algorithms) to be close\nto local optima, which makes the traffic optimization process more difficult.\nIn this work, we investigate 16 different metamodels and 20 settings of genetic\nalgorithms, in order to understand what are the reasons of this phenomenon,\nwhat is its scale, how it can be mitigated and what can be potentially done to\ndesign better real-time traffic optimization methods. \n\n"}
{"id": "1812.01097", "contents": "Title: LEAF: A Benchmark for Federated Settings Abstract: Modern federated networks, such as those comprised of wearable devices,\nmobile phones, or autonomous vehicles, generate massive amounts of data each\nday. This wealth of data can help to learn models that can improve the user\nexperience on each device. However, the scale and heterogeneity of federated\ndata presents new challenges in research areas such as federated learning,\nmeta-learning, and multi-task learning. As the machine learning community\nbegins to tackle these challenges, we are at a critical time to ensure that\ndevelopments made in these areas are grounded with realistic benchmarks. To\nthis end, we propose LEAF, a modular benchmarking framework for learning in\nfederated settings. LEAF includes a suite of open-source federated datasets, a\nrigorous evaluation framework, and a set of reference implementations, all\ngeared towards capturing the obstacles and intricacies of practical federated\nenvironments. \n\n"}
{"id": "1812.01105", "contents": "Title: Correspondence Analysis of Government Expenditure Patterns Abstract: We analyze expenditure patterns of discretionary funds by Brazilian congress\nmembers. This analysis is based on a large dataset containing over $7$ million\nexpenses made publicly available by the Brazilian government. This dataset has,\nup to now, remained widely untouched by machine learning methods. Our main\ncontributions are two-fold: (i) we provide a novel dataset benchmark for\nmachine learning-based efforts for government transparency to the broader\nresearch community, and (ii) introduce a neural network-based approach for\nanalyzing and visualizing outlying expense patterns. Our hope is that the\napproach presented here can inspire new machine learning methodologies for\ngovernment transparency applicable to other developing nations. \n\n"}
{"id": "1812.01164", "contents": "Title: Pre-Defined Sparse Neural Networks with Hardware Acceleration Abstract: Neural networks have proven to be extremely powerful tools for modern\nartificial intelligence applications, but computational and storage complexity\nremain limiting factors. This paper presents two compatible contributions\ntowards reducing the time, energy, computational, and storage complexities\nassociated with multilayer perceptrons. Pre-defined sparsity is proposed to\nreduce the complexity during both training and inference, regardless of the\nimplementation platform. Our results show that storage and computational\ncomplexity can be reduced by factors greater than 5X without significant\nperformance loss. The second contribution is an architecture for hardware\nacceleration that is compatible with pre-defined sparsity. This architecture\nsupports both training and inference modes and is flexible in the sense that it\nis not tied to a specific number of neurons. For example, this flexibility\nimplies that various sized neural networks can be supported on various sized\nField Programmable Gate Array (FPGA)s. \n\n"}
{"id": "1812.01484", "contents": "Title: Privacy-Preserving Distributed Deep Learning for Clinical Data Abstract: Deep learning with medical data often requires larger samples sizes than are\navailable at single providers. While data sharing among institutions is\ndesirable to train more accurate and sophisticated models, it can lead to\nsevere privacy concerns due the sensitive nature of the data. This problem has\nmotivated a number of studies on distributed training of neural networks that\ndo not require direct sharing of the training data. However, simple distributed\ntraining does not offer provable privacy guarantees to satisfy technical safe\nstandards and may reveal information about the underlying patients. We present\na method to train neural networks for clinical data in a distributed fashion\nunder differential privacy. We demonstrate these methods on two datasets that\ninclude information from multiple independent sites, the eICU collaborative\nResearch Database and The Cancer Genome Atlas. \n\n"}
{"id": "1812.01803", "contents": "Title: ECC: Platform-Independent Energy-Constrained Deep Neural Network\n  Compression via a Bilinear Regression Model Abstract: Many DNN-enabled vision applications constantly operate under severe energy\nconstraints such as unmanned aerial vehicles, Augmented Reality headsets, and\nsmartphones. Designing DNNs that can meet a stringent energy budget is becoming\nincreasingly important. This paper proposes ECC, a framework that compresses\nDNNs to meet a given energy constraint while minimizing accuracy loss. The key\nidea of ECC is to model the DNN energy consumption via a novel bilinear\nregression function. The energy estimate model allows us to formulate DNN\ncompression as a constrained optimization that minimizes the DNN loss function\nover the energy constraint. The optimization problem, however, has nontrivial\nconstraints. Therefore, existing deep learning solvers do not apply directly.\nWe propose an optimization algorithm that combines the essence of the\nAlternating Direction Method of Multipliers (ADMM) framework with\ngradient-based learning algorithms. The algorithm decomposes the original\nconstrained optimization into several subproblems that are solved iteratively\nand efficiently. ECC is also portable across different hardware platforms\nwithout requiring hardware knowledge. Experiments show that ECC achieves higher\naccuracy under the same or lower energy budget compared to state-of-the-art\nresource-constrained DNN compression techniques. \n\n"}
{"id": "1812.02171", "contents": "Title: Comparative Document Summarisation via Classification Abstract: This paper considers extractive summarisation in a comparative setting: given\ntwo or more document groups (e.g., separated by publication time), the goal is\nto select a small number of documents that are representative of each group,\nand also maximally distinguishable from other groups. We formulate a set of new\nobjective functions for this problem that connect recent literature on document\nsummarisation, interpretable machine learning, and data subset selection. In\nparticular, by casting the problem as a binary classification amongst different\ngroups, we derive objectives based on the notion of maximum mean discrepancy,\nas well as a simple yet effective gradient-based optimisation strategy. Our new\nformulation allows scalable evaluations of comparative summarisation as a\nclassification task, both automatically and via crowd-sourcing. To this end, we\nevaluate comparative summarisation methods on a newly curated collection of\ncontroversial news topics over 13 months. We observe that gradient-based\noptimisation outperforms discrete and baseline approaches in 14 out of 24\ndifferent automatic evaluation settings. In crowd-sourced evaluations,\nsummaries from gradient optimisation elicit 7% more accurate classification\nfrom human workers than discrete optimisation. Our result contrasts with recent\nliterature on submodular data subset selection that favours discrete\noptimisation. We posit that our formulation of comparative summarisation will\nprove useful in a diverse range of use cases such as comparing content sources,\nauthors, related topics, or distinct view points. \n\n"}
{"id": "1812.02216", "contents": "Title: Composing Entropic Policies using Divergence Correction Abstract: Composing previously mastered skills to solve novel tasks promises dramatic\nimprovements in the data efficiency of reinforcement learning. Here, we analyze\ntwo recent works composing behaviors represented in the form of action-value\nfunctions and show that they perform poorly in some situations. As part of this\nanalysis, we extend an important generalization of policy improvement to the\nmaximum entropy framework and introduce an algorithm for the practical\nimplementation of successor features in continuous action spaces. Then we\npropose a novel approach which addresses the failure cases of prior work and,\nin principle, recovers the optimal policy during transfer. This method works by\nexplicitly learning the (discounted, future) divergence between base policies.\nWe study this approach in the tabular case and on non-trivial continuous\ncontrol problems with compositional structure and show that it outperforms or\nmatches existing methods across all tasks considered. \n\n"}
{"id": "1812.02706", "contents": "Title: Coarse-Graining Auto-Encoders for Molecular Dynamics Abstract: Molecular dynamics simulations provide theoretical insight into the\nmicroscopic behavior of materials in condensed phase and, as a predictive tool,\nenable computational design of new compounds. However, because of the large\ntemporal and spatial scales involved in thermodynamic and kinetic phenomena in\nmaterials, atomistic simulations are often computationally unfeasible.\nCoarse-graining methods allow simulating larger systems, by reducing the\ndimensionality of the simulation, and propagating longer timesteps, by\naveraging out fast motions. Coarse-graining involves two coupled learning\nproblems; defining the mapping from an all-atom to a reduced representation,\nand the parametrization of a Hamiltonian over coarse-grained coordinates.\nMultiple statistical mechanics approaches have addressed the latter, but the\nformer is generally a hand-tuned process based on chemical intuition. Here we\npresent Autograin, an optimization framework based on auto-encoders to learn\nboth tasks simultaneously. Autograin is trained to learn the optimal mapping\nbetween all-atom and reduced representation, using the reconstruction loss to\nfacilitate the learning of coarse-grained variables. In addition, a\nforce-matching method is applied to variationally determine the coarse-grained\npotential energy function. This procedure is tested on a number of model\nsystems including single-molecule and bulk-phase periodic simulations. \n\n"}
{"id": "1812.02841", "contents": "Title: Hardy-Muckenhoupt Bounds for Laplacian Eigenvalues Abstract: We present two graph quantities Psi(G,S) and Psi_2(G) which give constant\nfactor estimates to the Dirichlet and Neumann eigenvalues, lambda(G,S) and\nlambda_2(G), respectively. Our techniques make use of a discrete Hardy-type\ninequality. \n\n"}
{"id": "1812.03224", "contents": "Title: A Hybrid Approach to Privacy-Preserving Federated Learning Abstract: Federated learning facilitates the collaborative training of models without\nthe sharing of raw data. However, recent attacks demonstrate that simply\nmaintaining data locality during training processes does not provide sufficient\nprivacy guarantees. Rather, we need a federated learning system capable of\npreventing inference over both the messages exchanged during training and the\nfinal trained model while ensuring the resulting model also has acceptable\npredictive accuracy. Existing federated learning approaches either use secure\nmultiparty computation (SMC) which is vulnerable to inference or differential\nprivacy which can lead to low accuracy given a large number of parties with\nrelatively small amounts of data each. In this paper, we present an alternative\napproach that utilizes both differential privacy and SMC to balance these\ntrade-offs. Combining differential privacy with secure multiparty computation\nenables us to reduce the growth of noise injection as the number of parties\nincreases without sacrificing privacy while maintaining a pre-defined rate of\ntrust. Our system is therefore a scalable approach that protects against\ninference threats and produces models with high accuracy. Additionally, our\nsystem can be used to train a variety of machine learning models, which we\nvalidate with experimental results on 3 different machine learning algorithms.\nOur experiments demonstrate that our approach out-performs state of the art\nsolutions. \n\n"}
{"id": "1812.03235", "contents": "Title: Improved Knowledge Graph Embedding using Background Taxonomic\n  Information Abstract: Knowledge graphs are used to represent relational information in terms of\ntriples. To enable learning about domains, embedding models, such as tensor\nfactorization models, can be used to make predictions of new triples. Often\nthere is background taxonomic information (in terms of subclasses and\nsubproperties) that should also be taken into account. We show that existing\nfully expressive (a.k.a. universal) models cannot provably respect subclass and\nsubproperty information. We show that minimal modifications to an existing\nknowledge graph completion method enables injection of taxonomic information.\nMoreover, we prove that our model is fully expressive, assuming a lower-bound\non the size of the embeddings. Experimental results on public knowledge graphs\nshow that despite its simplicity our approach is surprisingly effective. \n\n"}
{"id": "1812.03253", "contents": "Title: Counterfactuals uncover the modular structure of deep generative models Abstract: Deep generative models can emulate the perceptual properties of complex image\ndatasets, providing a latent representation of the data. However, manipulating\nsuch representation to perform meaningful and controllable transformations in\nthe data space remains challenging without some form of supervision. While\nprevious work has focused on exploiting statistical independence to disentangle\nlatent factors, we argue that such requirement is too restrictive and propose\ninstead a non-statistical framework that relies on counterfactual manipulations\nto uncover a modular structure of the network composed of disentangled groups\nof internal variables. Experiments with a variety of generative models trained\non complex image datasets show the obtained modules can be used to design\ntargeted interventions. This opens the way to applications such as\ncomputationally efficient style transfer and the automated assessment of\nrobustness to contextual changes in pattern recognition systems. \n\n"}
{"id": "1812.05316", "contents": "Title: Mind the Independence Gap Abstract: The independence gap of a graph was introduced by Ekim et al. (2018) as a\nmeasure of how far a graph is from being well-covered. It is defined as the\ndifference between the maximum and minimum size of a maximal independent set.\n  We investigate the independence gap of a graph from structural and\nalgorithmic points of view, with a focus on classes of perfect graphs.\nGeneralizing results on well-covered graphs due to Dean and Zito (1994) and\nHujdurovi\\'c et al. (2018), we express the independence gap of a perfect graph\nin terms of clique partitions and use this characterization to develop a\npolynomial-time algorithm for recognizing graphs of constant independence gap\nin any class of perfect graphs of bounded clique number. Next, we introduce a\nhereditary variant of the parameter, which we call hereditary independence gap\nand which measures the maximum independence gap over all induced subgraphs of\nthe graph. We show that determining whether a given graph has hereditary\nindependence gap at most $k$ is polynomial-time solvable if $k$ is fixed and\nco-NP-complete if $k$ is part of input. We also investigate the complexity of\nthe independent set problem in graph classes related to independence gap,\nshowing that the problem is NP-complete in the class of graphs of independence\ngap at most one and polynomial-time solvable in any class of graphs with\nbounded hereditary independence gap. Combined with some known results on\nclaw-free graphs, our results imply that the independent domination problem is\nsolvable in polynomial time in the class of $\\{$claw, 2$P_3\\}$-free graphs. \n\n"}
{"id": "1812.07793", "contents": "Title: The Computational Complexity of Angry Birds Abstract: The physics-based simulation game Angry Birds has been heavily researched by\nthe AI community over the past five years, and has been the subject of a\npopular AI competition that is currently held annually as part of a leading AI\nconference. Developing intelligent agents that can play this game effectively\nhas been an incredibly complex and challenging problem for traditional AI\ntechniques to solve, even though the game is simple enough that any human\nplayer could learn and master it within a short time. In this paper we analyse\nhow hard the problem really is, presenting several proofs for the computational\ncomplexity of Angry Birds. By using a combination of several gadgets within\nthis game's environment, we are able to demonstrate that the decision problem\nof solving general levels for different versions of Angry Birds is either\nNP-hard, PSPACE-hard, PSPACE-complete or EXPTIME-hard. Proof of NP-hardness is\nby reduction from 3-SAT, whilst proof of PSPACE-hardness is by reduction from\nTrue Quantified Boolean Formula (TQBF). Proof of EXPTIME-hardness is by\nreduction from G2, a known EXPTIME-complete problem similar to that used for\nmany previous games such as Chess, Go and Checkers. To the best of our\nknowledge, this is the first time that a single-player game has been proven\nEXPTIME-hard. This is achieved by using stochastic game engine dynamics to\neffectively model the real world, or in our case the physics simulator, as the\nopponent against which we are playing. These proofs can also be extended to\nother physics-based games with similar mechanics. \n\n"}
{"id": "1812.07826", "contents": "Title: Two-stage Combinatorial Optimization Problems under Risk Abstract: In this paper a class of combinatorial optimization problems is discussed. It\nis assumed that a solution can be constructed in two stages. The current\nfirst-stage costs are precisely known, while the future second-stage costs are\nonly known to belong to an uncertainty set, which contains a finite number of\nscenarios with known probability distribution. A partial solution, chosen in\nthe first stage, can be completed by performing an optimal recourse action,\nafter the true second-stage scenario is revealed. A solution minimizing the\nConditional Value at Risk (CVaR) measure is computed. Since expectation and\nmaximum are boundary cases of CVaR, the model generalizes the traditional\nstochastic and robust two-stage approaches, previously discussed in the\nexisting literature. In this paper some new negative and positive results are\nprovided for basic combinatorial optimization problems such as the selection or\nnetwork problems. \n\n"}
{"id": "1812.08731", "contents": "Title: Limits on the Universal Method for Matrix Multiplication Abstract: In this work, we prove limitations on the known methods for designing matrix\nmultiplication algorithms. Alman and Vassilevska Williams recently defined the\nUniversal Method, which substantially generalizes all the known approaches\nincluding Strassen's Laser Method and Cohn and Umans' Group Theoretic Method.\nWe prove concrete lower bounds on the algorithms one can design by applying the\nUniversal Method to many different tensors. Our proofs use new tools for upper\nbounding the asymptotic slice rank of a wide range of tensors. Our main result\nis that the Universal method applied to any Coppersmith-Winograd tensor $CW_q$\ncannot yield a bound on $\\omega$, the exponent of matrix multiplication, better\nthan $2.16805$. By comparison, it was previously only known that the weaker\n`Galactic Method' applied to $CW_q$ could not achieve an exponent of $2$.\n  We also study the Laser Method (which is, in principle, a highly special case\nof the Universal Method) and prove that it is \"complete\" for matrix\nmultiplication algorithms: when it applies to a tensor $T$, it achieves $\\omega\n= 2$ if and only if it is possible for the Universal method applied to $T$ to\nachieve $\\omega = 2$. Hence, the Laser Method, which was originally used as an\nalgorithmic tool, can also be seen as a lower bounding tool. For example, in\ntheir landmark paper, Coppersmith and Winograd achieved a bound of $\\omega \\leq\n2.376$, by applying the Laser Method to $CW_q$. By our result, the fact that\nthey did not achieve $\\omega=2$ implies a lower bound on the Universal Method\napplied to $CW_q$. Indeed, if it were possible for the Universal Method applied\nto $CW_q$ to achieve $\\omega=2$, then Coppersmith and Winograd's application of\nthe Laser Method would have achieved $\\omega=2$. \n\n"}
{"id": "1812.09094", "contents": "Title: A Simple Algorithm for Computing the Document Array Abstract: We present a simple algorithm for computing the document array given a string\ncollection and its suffix array as input. Our algorithm runs in linear time\nusing constant additional space for strings from constant alphabets. \n\n"}
{"id": "1812.10309", "contents": "Title: Efficiently list-edge coloring multigraphs asymptotically optimally Abstract: We give polynomial time algorithms for the seminal results of Kahn, who\nshowed that the Goldberg-Seymour and List-Coloring conjectures for (list-)edge\ncoloring multigraphs hold asymptotically. Kahn's arguments are based on the\nprobabilistic method and are non-constructive. Our key insight is to show that\nthe main result of Achlioptas, Iliopoulos and Kolmogorov for analyzing local\nsearch algorithms can be used to make constructive applications of a powerful\nversion of the so-called Lopsided Lovasz Local Lemma. In particular, we use it\nto design algorithms that exploit the fact that correlations in the probability\nspaces on matchings used by Kahn decay with distance. \n\n"}
{"id": "1812.10730", "contents": "Title: Neuromemrisitive Architecture of HTM with On-Device Learning and\n  Neurogenesis Abstract: Hierarchical temporal memory (HTM) is a biomimetic sequence memory algorithm\nthat holds promise for invariant representations of spatial and spatiotemporal\ninputs. This paper presents a comprehensive neuromemristive crossbar\narchitecture for the spatial pooler (SP) and the sparse distributed\nrepresentation classifier, which are fundamental to the algorithm. There are\nseveral unique features in the proposed architecture that tightly link with the\nHTM algorithm. A memristor that is suitable for emulating the HTM synapses is\nidentified and a new Z-window function is proposed. The architecture exploits\nthe concept of synthetic synapses to enable potential synapses in the HTM. The\ncrossbar for the SP avoids dark spots caused by unutilized crossbar regions and\nsupports rapid on-chip training within 2 clock cycles. This research also\nleverages plasticity mechanisms such as neurogenesis and homeostatic intrinsic\nplasticity to strengthen the robustness and performance of the SP. The proposed\ndesign is benchmarked for image recognition tasks using MNIST and Yale faces\ndatasets, and is evaluated using different metrics including entropy,\nsparseness, and noise robustness. Detailed power analysis at different stages\nof the SP operations is performed to demonstrate the suitability for mobile\nplatforms. \n\n"}
{"id": "1812.10771", "contents": "Title: Approximate counting and NP search problems Abstract: We study a new class of NP search problems, those which can be proved total\nusing standard combinatorial reasoning based on approximate counting. Our model\nfor this kind of reasoning is the bounded arithmetic theory $\\mathrm{APC}_2$ of\n[Je\\v{r}\\'abek 2009]. In particular, the Ramsey and weak pigeonhole search\nproblems lie in the new class. We give a purely computational characterization\nof this class and show that, relative to an oracle, it does not contain the\nproblem CPLS, a strengthening of PLS.\n  As CPLS is provably total in the theory $T^2_2$, this shows that\n$\\mathrm{APC}_2$ does not prove every $\\forall \\Sigma^b_1$ sentence which is\nprovable in bounded arithmetic. This answers the question posed in [Buss,\nKo{\\l}odziejczyk, Thapen 2014] and represents some progress in the programme of\nseparating the levels of the bounded arithmetic hierarchy by low-complexity\nsentences.\n  Our main technical tool is an extension of the \"fixing lemma\" from [Pudl\\'ak,\nThapen 2017], a form of switching lemma, which we use to show that a random\npartial oracle from a certain distribution will, with high probability,\ndetermine an entire computation of a $\\textrm{P}^{\\textrm{NP}}$ oracle machine.\nThe introduction to the paper is intended to make the statements and context of\nthe results accessible to someone unfamiliar with NP search problems or with\nbounded arithmetic. \n\n"}
{"id": "1812.11560", "contents": "Title: Monte-Carlo Sampling applied to Multiple Instance Learning for\n  Histological Image Classification Abstract: We propose a patch sampling strategy based on a sequential Monte-Carlo method\nfor high resolution image classification in the context of Multiple Instance\nLearning. When compared with grid sampling and uniform sampling techniques, it\nachieves higher generalization performance. We validate the strategy on two\nartificial datasets and two histological datasets for breast cancer and sun\nexposure classification. \n\n"}
{"id": "1901.00069", "contents": "Title: Recurrent Neural Networks for Time Series Forecasting Abstract: Time series forecasting is difficult. It is difficult even for recurrent\nneural networks with their inherent ability to learn sequentiality. This\narticle presents a recurrent neural network based time series forecasting\nframework covering feature engineering, feature importances, point and interval\npredictions, and forecast evaluation. The description of the method is followed\nby an empirical study using both LSTM and GRU networks. \n\n"}
{"id": "1901.01484", "contents": "Title: LanczosNet: Multi-Scale Deep Graph Convolutional Networks Abstract: We propose the Lanczos network (LanczosNet), which uses the Lanczos algorithm\nto construct low rank approximations of the graph Laplacian for graph\nconvolution. Relying on the tridiagonal decomposition of the Lanczos algorithm,\nwe not only efficiently exploit multi-scale information via fast approximated\ncomputation of matrix power but also design learnable spectral filters. Being\nfully differentiable, LanczosNet facilitates both graph kernel learning as well\nas learning node embeddings. We show the connection between our LanczosNet and\ngraph based manifold learning methods, especially the diffusion maps. We\nbenchmark our model against several recent deep graph networks on citation\nnetworks and QM8 quantum chemistry dataset. Experimental results show that our\nmodel achieves the state-of-the-art performance in most tasks. Code is released\nat: \\url{https://github.com/lrjconan/LanczosNetwork}. \n\n"}
{"id": "1901.01825", "contents": "Title: Multiple Set Matching and Pre-Filtering with Bloom Multifilters Abstract: Bloom filter is a space-efficient probabilistic data structure for checking\nelements' membership in a set. Given multiple sets, however, a standard Bloom\nfilter is not sufficient when looking for the items to which an element or a\nset of input elements belong to. In this article, we solve multiple set\nmatching problem by proposing two efficient Bloom Multifilters called Bloom\nMatrix and Bloom Vector. Both of them are space efficient and answer queries\nwith a set of identifiers for multiple set matching problems. We show that the\nspace efficiency can be optimized further according to the distribution of\nlabels among multiple sets: Uniform and Zipf. While both of them are space\nefficient, Bloom Vector can efficiently exploit Zipf distribution of data for\nfurther space reduction. Our results also highlight that basic ADD and LOOKUP\noperations on Bloom Matrix are faster than on Bloom Vector. However, Bloom\nMatrix does not meet the theoretical false positive rate of less than $10^{-2}$\nfor LOOKUP operations if the represented data or the labels are not uniformly\ndistributed among the multiple sets. Consequently, we introduce \\textit{Bloom\nTest} which uses Bloom Matrix as the pre-filter structure to determine which\nstructure is suitable for improved performance with an arbitrary input dataset. \n\n"}
{"id": "1901.02731", "contents": "Title: A Comprehensive guide to Bayesian Convolutional Neural Network with\n  Variational Inference Abstract: Artificial Neural Networks are connectionist systems that perform a given\ntask by learning on examples without having prior knowledge about the task.\nThis is done by finding an optimal point estimate for the weights in every\nnode. Generally, the network using point estimates as weights perform well with\nlarge datasets, but they fail to express uncertainty in regions with little or\nno data, leading to overconfident decisions.\n  In this paper, Bayesian Convolutional Neural Network (BayesCNN) using\nVariational Inference is proposed, that introduces probability distribution\nover the weights. Furthermore, the proposed BayesCNN architecture is applied to\ntasks like Image Classification, Image Super-Resolution and Generative\nAdversarial Networks. The results are compared to point-estimates based\narchitectures on MNIST, CIFAR-10 and CIFAR-100 datasets for Image\nCLassification task, on BSD300 dataset for Image Super Resolution task and on\nCIFAR10 dataset again for Generative Adversarial Network task.\n  BayesCNN is based on Bayes by Backprop which derives a variational\napproximation to the true posterior. We, therefore, introduce the idea of\napplying two convolutional operations, one for the mean and one for the\nvariance. Our proposed method not only achieves performances equivalent to\nfrequentist inference in identical architectures but also incorporate a\nmeasurement for uncertainties and regularisation. It further eliminates the use\nof dropout in the model. Moreover, we predict how certain the model prediction\nis based on the epistemic and aleatoric uncertainties and empirically show how\nthe uncertainty can decrease, allowing the decisions made by the network to\nbecome more deterministic as the training accuracy increases. Finally, we\npropose ways to prune the Bayesian architecture and to make it more\ncomputational and time effective. \n\n"}
{"id": "1901.02739", "contents": "Title: Dirichlet Variational Autoencoder Abstract: This paper proposes Dirichlet Variational Autoencoder (DirVAE) using a\nDirichlet prior for a continuous latent variable that exhibits the\ncharacteristic of the categorical probabilities. To infer the parameters of\nDirVAE, we utilize the stochastic gradient method by approximating the Gamma\ndistribution, which is a component of the Dirichlet distribution, with the\ninverse Gamma CDF approximation. Additionally, we reshape the component\ncollapsing issue by investigating two problem sources, which are decoder weight\ncollapsing and latent value collapsing, and we show that DirVAE has no\ncomponent collapsing; while Gaussian VAE exhibits the decoder weight collapsing\nand Stick-Breaking VAE shows the latent value collapsing. The experimental\nresults show that 1) DirVAE models the latent representation result with the\nbest log-likelihood compared to the baselines; and 2) DirVAE produces more\ninterpretable latent values with no collapsing issues which the baseline models\nsuffer from. Also, we show that the learned latent representation from the\nDirVAE achieves the best classification accuracy in the semi-supervised and the\nsupervised classification tasks on MNIST, OMNIGLOT, and SVHN compared to the\nbaseline VAEs. Finally, we demonstrated that the DirVAE augmented topic models\nshow better performances in most cases. \n\n"}
{"id": "1901.03364", "contents": "Title: On the Descriptive Complexity of Color Coding Abstract: Color coding is an algorithmic technique used in parameterized complexity\ntheory to detect \"small\" structures inside graphs. The idea is to derandomize\nalgorithms that first randomly color a graph and then search for an\neasily-detectable, small color pattern. We transfer color coding to the world\nof descriptive complexity theory by characterizing -- purely in terms of the\nsyntactic structure of describing formulas -- when the powerful second-order\nquantifiers representing a random coloring can be replaced by equivalent,\nsimple first-order formulas. Building on this result, we identify syntactic\nproperties of first-order quantifiers that can be eliminated from formulas\ndescribing parameterized problems. The result applies to many packing and\nembedding problems, but also to the long path problem. Together with a new\nresult on the parameterized complexity of formula families involving only a\nfixed number of variables, we get that many problems lie in FPT just because of\nthe way they are commonly described using logical formulas. \n\n"}
{"id": "1901.03415", "contents": "Title: Context Aware Machine Learning Abstract: We propose a principle for exploring context in machine learning models.\nStarting with a simple assumption that each observation may or may not depend\non its context, a conditional probability distribution is decomposed into two\nparts: context-free and context-sensitive. Then by employing the log-linear\nword production model for relating random variables to their embedding space\nrepresentation and making use of the convexity of natural exponential function,\nwe show that the embedding of an observation can also be decomposed into a\nweighted sum of two vectors, representing its context-free and\ncontext-sensitive parts, respectively. This simple treatment of context\nprovides a unified view of many existing deep learning models, leading to\nrevisions of these models able to achieve significant performance boost.\nSpecifically, our upgraded version of a recent sentence embedding model not\nonly outperforms the original one by a large margin, but also leads to a new,\nprincipled approach for compositing the embeddings of bag-of-words features, as\nwell as a new architecture for modeling attention in deep neural networks. More\nsurprisingly, our new principle provides a novel understanding of the gates and\nequations defined by the long short term memory model, which also leads to a\nnew model that is able to converge significantly faster and achieve much lower\nprediction errors. Furthermore, our principle also inspires a new type of\ngeneric neural network layer that better resembles real biological neurons than\nthe traditional linear mapping plus nonlinear activation based architecture.\nIts multi-layer extension provides a new principle for deep neural networks\nwhich subsumes residual network (ResNet) as its special case, and its extension\nto convolutional neutral network model accounts for irrelevant input (e.g.,\nbackground in an image) in addition to filtering. \n\n"}
{"id": "1901.05925", "contents": "Title: Resource-Aware Algorithms for Distributed Loop Closure Detection with\n  Provable Performance Guarantees Abstract: Inter-robot loop closure detection, e.g., for collaborative simultaneous\nlocalization and mapping (CSLAM), is a fundamental capability for many\nmultirobot applications in GPS-denied regimes. In real-world scenarios, this is\na resource-intensive process that involves exchanging observations and\nverifying potential matches. This poses severe challenges especially for\nsmall-size and low-cost robots with various operational and resource\nconstraints that limit, e.g., energy consumption, communication bandwidth, and\ncomputation capacity. This paper presents resource-aware algorithms for\ndistributed inter-robot loop closure detection. In particular, we seek to\nselect a subset of potential inter-robot loop closures that maximizes a\nmonotone submodular performance metric without exceeding computation and\ncommunication budgets. We demonstrate that this problem is in general NP-hard,\nand present efficient approximation algorithms with provable performance\nguarantees. A convex relaxation scheme is used to certify near-optimal\nperformance of the proposed framework in real and synthetic SLAM benchmarks. \n\n"}
{"id": "1901.06482", "contents": "Title: On Efficient Optimal Transport: An Analysis of Greedy and Accelerated\n  Mirror Descent Algorithms Abstract: We provide theoretical analyses for two algorithms that solve the regularized\noptimal transport (OT) problem between two discrete probability measures with\nat most $n$ atoms. We show that a greedy variant of the classical Sinkhorn\nalgorithm, known as the \\emph{Greenkhorn algorithm}, can be improved to\n$\\widetilde{\\mathcal{O}}(n^2\\varepsilon^{-2})$, improving on the best known\ncomplexity bound of $\\widetilde{\\mathcal{O}}(n^2\\varepsilon^{-3})$. Notably,\nthis matches the best known complexity bound for the Sinkhorn algorithm and\nhelps explain why the Greenkhorn algorithm can outperform the Sinkhorn\nalgorithm in practice. Our proof technique, which is based on a primal-dual\nformulation and a novel upper bound for the dual solution, also leads to a new\nclass of algorithms that we refer to as \\emph{adaptive primal-dual accelerated\nmirror descent} (APDAMD) algorithms. We prove that the complexity of these\nalgorithms is $\\widetilde{\\mathcal{O}}(n^2\\sqrt{\\delta}\\varepsilon^{-1})$,\nwhere $\\delta > 0$ refers to the inverse of the strong convexity module of\nBregman divergence with respect to $\\|\\cdot\\|_\\infty$. This implies that the\nAPDAMD algorithm is faster than the Sinkhorn and Greenkhorn algorithms in terms\nof $\\varepsilon$. Experimental results on synthetic and real datasets\ndemonstrate the favorable performance of the Greenkhorn and APDAMD algorithms\nin practice. \n\n"}
{"id": "1901.06523", "contents": "Title: Frequency Principle: Fourier Analysis Sheds Light on Deep Neural\n  Networks Abstract: We study the training process of Deep Neural Networks (DNNs) from the Fourier\nanalysis perspective. We demonstrate a very universal Frequency Principle\n(F-Principle) -- DNNs often fit target functions from low to high frequencies\n-- on high-dimensional benchmark datasets such as MNIST/CIFAR10 and deep neural\nnetworks such as VGG16. This F-Principle of DNNs is opposite to the behavior of\nmost conventional iterative numerical schemes (e.g., Jacobi method), which\nexhibit faster convergence for higher frequencies for various scientific\ncomputing problems. With a simple theory, we illustrate that this F-Principle\nresults from the regularity of the commonly used activation functions. The\nF-Principle implies an implicit bias that DNNs tend to fit training data by a\nlow-frequency function. This understanding provides an explanation of good\ngeneralization of DNNs on most real datasets and bad generalization of DNNs on\nparity function or randomized dataset. \n\n"}
{"id": "1901.06653", "contents": "Title: Fast algorithms at low temperatures via Markov chains Abstract: We define a discrete-time Markov chain for abstract polymer models and show\nthat under sufficient decay of the polymer weights, this chain mixes rapidly.\nWe apply this Markov chain to polymer models derived from the hard-core and\nferromagnetic Potts models on bounded-degree (bipartite) expander graphs. In\nthis setting, Jenssen, Keevash and Perkins (2019) recently gave an FPTAS and an\nefficient sampling algorithm at sufficiently high fugacity and low temperature\nrespectively. Their method is based on using the cluster expansion to obtain a\ncomplex zero-free region for the partition function of a polymer model, and\nthen approximating this partition function using the polynomial interpolation\nmethod of Barvinok.\n  Our approach via the polymer model Markov chain circumvents the zero-free\nanalysis and the generalization to complex parameters, and leads to a sampling\nalgorithm with a fast running time of $O(n \\log n)$ for the Potts model and\n$O(n^2 \\log n)$ for the hard-core model, in contrast to typical running times\nof $n^{O(\\log \\Delta)}$ for algorithms based on Barvinok's polynomial\ninterpolation method on graphs of maximum degree $\\Delta$. We finally combine\nour results for the hard-core and ferromagnetic Potts models with standard\nMarkov chain comparison tools to obtain polynomial mixing time for the usual\nspin Glauber dynamics restricted to even and odd or `red' dominant portions of\nthe respective state spaces. \n\n"}
{"id": "1901.08029", "contents": "Title: Learning to Collaborate in Markov Decision Processes Abstract: We consider a two-agent MDP framework where agents repeatedly solve a task in\na collaborative setting. We study the problem of designing a learning algorithm\nfor the first agent (A1) that facilitates a successful collaboration even in\ncases when the second agent (A2) is adapting its policy in an unknown way. The\nkey challenge in our setting is that the first agent faces non-stationarity in\nrewards and transitions because of the adaptive behavior of the second agent.\n  We design novel online learning algorithms for agent A1 whose regret decays\nas $O(T^{\\max\\{1-\\frac{3}{7} \\cdot \\alpha, \\frac{1}{4}\\}})$ with $T$ learning\nepisodes provided that the magnitude of agent A2's policy changes between any\ntwo consecutive episodes are upper bounded by $O(T^{-\\alpha})$. Here, the\nparameter $\\alpha$ is assumed to be strictly greater than $0$, and we show that\nthis assumption is necessary provided that the learning parity with noise\nproblem is computationally hard. We show that sub-linear regret of agent A1\nfurther implies near-optimality of the agents' joint return for MDPs that\nmanifest the properties of a smooth game. \n\n"}
{"id": "1901.08177", "contents": "Title: Generating and Aligning from Data Geometries with Generative Adversarial\n  Networks Abstract: Unsupervised domain mapping has attracted substantial attention in recent\nyears due to the success of models based on the cycle-consistency assumption.\nThese models map between two domains by fooling a probabilistic discriminator,\nthereby matching the probability distributions of the real and generated data.\nInstead of this probabilistic approach, we cast the problem in terms of\naligning the geometry of the manifolds of the two domains. We introduce the\nManifold Geometry Matching Generative Adversarial Network (MGM GAN), which adds\ntwo novel mechanisms to facilitate GANs sampling from the geometry of the\nmanifold rather than the density and then aligning two manifold geometries: (1)\nan importance sampling technique that reweights points based on their density\non the manifold, making the discriminator only able to discern geometry and (2)\na penalty adapted from traditional manifold alignment literature that\nexplicitly enforces the geometry to be preserved. The MGM GAN leverages the\nmanifolds arising from a pre-trained autoencoder to bridge the gap between\nformal manifold alignment literature and existing GAN work, and demonstrate the\nadvantages of modeling the manifold geometry over its density. \n\n"}
{"id": "1901.08386", "contents": "Title: PAC Identification of Many Good Arms in Stochastic Multi-Armed Bandits Abstract: We consider the problem of identifying any $k$ out of the best $m$ arms in an\n$n$-armed stochastic multi-armed bandit. Framed in the PAC setting, this\nparticular problem generalises both the problem of `best subset selection' and\nthat of selecting `one out of the best m' arms [arcsk 2017]. In applications\nsuch as crowd-sourcing and drug-designing, identifying a single good solution\nis often not sufficient. Moreover, finding the best subset might be hard due to\nthe presence of many indistinguishably close solutions. Our generalisation of\nidentifying exactly $k$ arms out of the best $m$, where $1 \\leq k \\leq m$,\nserves as a more effective alternative. We present a lower bound on the\nworst-case sample complexity for general $k$, and a fully sequential PAC\nalgorithm, \\GLUCB, which is more sample-efficient on easy instances. Also,\nextending our analysis to infinite-armed bandits, we present a PAC algorithm\nthat is independent of $n$, which identifies an arm from the best $\\rho$\nfraction of arms using at most an additive poly-log number of samples than\ncompared to the lower bound, thereby improving over [arcsk 2017] and\n[Aziz+AKA:2018]. The problem of identifying $k > 1$ distinct arms from the best\n$\\rho$ fraction is not always well-defined; for a special class of this\nproblem, we present lower and upper bounds. Finally, through a reduction, we\nestablish a relation between upper bounds for the `one out of the best $\\rho$'\nproblem for infinite instances and the `one out of the best $m$' problem for\nfinite instances. We conjecture that it is more efficient to solve `small'\nfinite instances using the latter formulation, rather than going through the\nformer. \n\n"}
{"id": "1901.09017", "contents": "Title: Finding a Mediocre Player Abstract: Consider a totally ordered set $S$ of $n$ elements; as an example, a set of\ntennis players and their rankings. Further assume that their ranking is a total\norder and thus satisfies transitivity and anti-symmetry. Following Frances Yao\n(1974), an element (player) is said to be $(i,j)$-\\emph{mediocre} if it is\nneither among the top $i$ nor among the bottom $j$ elements of $S$. Finding a\nmediocre element is closely related to finding the median element. More than\n$40$ years ago, Yao suggested a very simple and elegant algorithm for finding\nan $(i,j)$-mediocre element: Pick $i+j+1$ elements arbitrarily and select the\n$(i+1)$-th largest among them. She also asked: \"Is this the best algorithm?\" No\none seems to have found a better algorithm ever since. We first provide a\ndeterministic algorithm that beats the worst-case comparison bound in Yao's\nalgorithm for a large range of values of $i$ (and corresponding suitable\n$j=j(i)$) even if the current best selection algorithm is used. We then repeat\nthe exercise for randomized algorithms; the average number of comparisons of\nour algorithm beats the average comparison bound in Yao's algorithm for another\nlarge range of values of $i$ (and corresponding suitable $j=j(i)$) even if the\nbest selection algorithm is used; the improvement is most notable in the\nsymmetric case $i=j$. Moreover, the tight bound obtained in the analysis of\nYao's algorithm allows us to give a definite answer for this class of\nalgorithms. In summary, we answer Yao's question as follows: (i)~\"Presently\nnot\" for deterministic algorithms and (ii)~\"Definitely not\" for randomized\nalgorithms. (In fairness, it should be said however that Yao posed the question\nin the context of deterministic algorithms.) \n\n"}
{"id": "1901.09018", "contents": "Title: Provably efficient RL with Rich Observations via Latent State Decoding Abstract: We study the exploration problem in episodic MDPs with rich observations\ngenerated from a small number of latent states. Under certain identifiability\nassumptions, we demonstrate how to estimate a mapping from the observations to\nlatent states inductively through a sequence of regression and clustering steps\n-- where previously decoded latent states provide labels for later regression\nproblems -- and use it to construct good exploration policies. We provide\nfinite-sample guarantees on the quality of the learned state decoding function\nand exploration policies, and complement our theory with an empirical\nevaluation on a class of hard exploration problems. Our method exponentially\nimproves over $Q$-learning with na\\\"ive exploration, even when $Q$-learning has\ncheating access to latent states. \n\n"}
{"id": "1901.09203", "contents": "Title: ACNN: a Full Resolution DCNN for Medical Image Segmentation Abstract: Deep Convolutional Neural Networks (DCNNs) are used extensively in medical\nimage segmentation and hence 3D navigation for robot-assisted Minimally\nInvasive Surgeries (MISs). However, current DCNNs usually use down sampling\nlayers for increasing the receptive field and gaining abstract semantic\ninformation. These down sampling layers decrease the spatial dimension of\nfeature maps, which can be detrimental to image segmentation. Atrous\nconvolution is an alternative for the down sampling layer. It increases the\nreceptive field whilst maintains the spatial dimension of feature maps. In this\npaper, a method for effective atrous rate setting is proposed to achieve the\nlargest and fully-covered receptive field with a minimum number of atrous\nconvolutional layers. Furthermore, a new and full resolution DCNN - Atrous\nConvolutional Neural Network (ACNN), which incorporates cascaded atrous\nII-blocks, residual learning and Instance Normalization (IN) is proposed.\nApplication results of the proposed ACNN to Magnetic Resonance Imaging (MRI)\nand Computed Tomography (CT) image segmentation demonstrate that the proposed\nACNN can achieve higher segmentation Intersection over Unions (IoUs) than U-Net\nand Deeplabv3+, but with reduced trainable parameters. \n\n"}
{"id": "1901.09229", "contents": "Title: DELTA: DEep Learning Transfer using Feature Map with Attention for\n  Convolutional Networks Abstract: Transfer learning through fine-tuning a pre-trained neural network with an\nextremely large dataset, such as ImageNet, can significantly accelerate\ntraining while the accuracy is frequently bottlenecked by the limited dataset\nsize of the new target task. To solve the problem, some regularization methods,\nconstraining the outer layer weights of the target network using the starting\npoint as references (SPAR), have been studied. In this paper, we propose a\nnovel regularized transfer learning framework DELTA, namely DEep Learning\nTransfer using Feature Map with Attention. Instead of constraining the weights\nof neural network, DELTA aims to preserve the outer layer outputs of the target\nnetwork. Specifically, in addition to minimizing the empirical loss, DELTA\nintends to align the outer layer outputs of two networks, through constraining\na subset of feature maps that are precisely selected by attention that has been\nlearned in an supervised learning manner. We evaluate DELTA with the\nstate-of-the-art algorithms, including L2 and L2-SP. The experiment results\nshow that our proposed method outperforms these baselines with higher accuracy\nfor new tasks. \n\n"}
{"id": "1901.09342", "contents": "Title: On the Universality of Invariant Networks Abstract: Constraining linear layers in neural networks to respect symmetry\ntransformations from a group $G$ is a common design principle for invariant\nnetworks that has found many applications in machine learning.\n  In this paper, we consider a fundamental question that has received little\nattention to date: Can these networks approximate any (continuous) invariant\nfunction?\n  We tackle the rather general case where $G\\leq S_n$ (an arbitrary subgroup of\nthe symmetric group) that acts on $\\mathbb{R}^n$ by permuting coordinates. This\nsetting includes several recent popular invariant networks. We present two main\nresults: First, $G$-invariant networks are universal if high-order tensors are\nallowed. Second, there are groups $G$ for which higher-order tensors are\nunavoidable for obtaining universality.\n  $G$-invariant networks consisting of only first-order tensors are of special\ninterest due to their practical value. We conclude the paper by proving a\nnecessary condition for the universality of $G$-invariant networks that\nincorporate only first-order tensors. \n\n"}
{"id": "1901.09344", "contents": "Title: Stochastic Approximation of Smooth and Strongly Convex Functions: Beyond\n  the $O(1/T)$ Convergence Rate Abstract: Stochastic approximation (SA) is a classical approach for stochastic convex\noptimization. Previous studies have demonstrated that the convergence rate of\nSA can be improved by introducing either smoothness or strong convexity\ncondition. In this paper, we make use of smoothness and strong convexity\nsimultaneously to boost the convergence rate. Let $\\lambda$ be the modulus of\nstrong convexity, $\\kappa$ be the condition number, $F_*$ be the minimal risk,\nand $\\alpha>1$ be some small constant. First, we demonstrate that, in\nexpectation, an $O(1/[\\lambda T^\\alpha] + \\kappa F_*/T)$ risk bound is\nattainable when $T = \\Omega(\\kappa^\\alpha)$. Thus, when $F_*$ is small, the\nconvergence rate could be faster than $O(1/[\\lambda T])$ and approaches\n$O(1/[\\lambda T^\\alpha])$ in the ideal case. Second, to further benefit from\nsmall risk, we show that, in expectation, an $O(1/2^{T/\\kappa}+F_*)$ risk bound\nis achievable. Thus, the excess risk reduces exponentially until reaching\n$O(F_*)$, and if $F_*=0$, we obtain a global linear convergence. Finally, we\nemphasize that our proof is constructive and each risk bound is equipped with\nan efficient stochastic algorithm attaining that bound. \n\n"}
{"id": "1901.09392", "contents": "Title: On the (In)fidelity and Sensitivity for Explanations Abstract: We consider objective evaluation measures of saliency explanations for\ncomplex black-box machine learning models. We propose simple robust variants of\ntwo notions that have been considered in recent literature: (in)fidelity, and\nsensitivity. We analyze optimal explanations with respect to both these\nmeasures, and while the optimal explanation for sensitivity is a vacuous\nconstant explanation, the optimal explanation for infidelity is a novel\ncombination of two popular explanation methods. By varying the perturbation\ndistribution that defines infidelity, we obtain novel explanations by\noptimizing infidelity, which we show to out-perform existing explanations in\nboth quantitative and qualitative measurements. Another salient question given\nthese measures is how to modify any given explanation to have better values\nwith respect to these measures. We propose a simple modification based on\nlowering sensitivity, and moreover show that when done appropriately, we could\nsimultaneously improve both sensitivity as well as fidelity. \n\n"}
{"id": "1901.09423", "contents": "Title: Subspace arrangements, graph rigidity and derandomization through\n  submodular optimization Abstract: This paper presents a deterministic, strongly polynomial time algorithm for\ncomputing the matrix rank for a class of symbolic matrices (whose entries are\npolynomials over a field). This class was introduced, in a different language,\nby Lov\\'asz [Lov] in his study of flats in matroids, and proved a duality\ntheorem putting this problem in $NP \\cap coNP$. As such, our result is another\ndemonstration where ``good characterization'' in the sense of Edmonds leads to\nan efficient algorithm. In a different paper Lov\\'asz [Lov79] proved that all\nsuch symbolic rank problems have efficient probabilistic algorithms, namely are\nin $BPP$. As such, our algorithm may be interpreted as a derandomization\nresult, in the long sequence special cases of the PIT (Polynomial Identity\nTesting) problem. Finally, Lov\\'asz and Yemini [LoYe] showed how the same\nproblem generalizes the graph rigidity problem in two dimensions. As such, our\nalgorithm may be seen as a generalization of the well-known deterministic\nalgorithm for the latter problem.\n  There are two somewhat unusual technical features in this paper. The first is\nthe translation of Lov\\'asz' flats problem into a symbolic rank one. The second\nis the use of submodular optimization for derandomization. We hope that the\ntools developed for both will be useful for related problems, in particular for\nbetter understanding of graph rigidity in higher dimensions. \n\n"}
{"id": "1901.09557", "contents": "Title: Out-of-Sample Testing for GANs Abstract: We propose a new method to evaluate GANs, namely EvalGAN. EvalGAN relies on a\ntest set to directly measure the reconstruction quality in the original sample\nspace (no auxiliary networks are necessary), and it also computes the\n(log)likelihood for the reconstructed samples in the test set. Further, EvalGAN\nis agnostic to the GAN algorithm and the dataset. We decided to test it on\nthree state-of-the-art GANs over the well-known CIFAR-10 and CelebA datasets. \n\n"}
{"id": "1901.09712", "contents": "Title: Ising Models with Latent Conditional Gaussian Variables Abstract: Ising models describe the joint probability distribution of a vector of\nbinary feature variables. Typically, not all the variables interact with each\nother and one is interested in learning the presumably sparse network structure\nof the interacting variables. However, in the presence of latent variables, the\nconventional method of learning a sparse model might fail. This is because the\nlatent variables induce indirect interactions of the observed variables. In the\ncase of only a few latent conditional Gaussian variables these spurious\ninteractions contribute an additional low-rank component to the interaction\nparameters of the observed Ising model. Therefore, we propose to learn a sparse\n+ low-rank decomposition of the parameters of an Ising model using a convex\nregularized likelihood problem. We show that the same problem can be obtained\nas the dual of a maximum-entropy problem with a new type of relaxation, where\nthe sample means collectively need to match the expected values only up to a\ngiven tolerance. The solution to the convex optimization problem has\nconsistency properties in the high-dimensional setting, where the number of\nobserved binary variables and the number of latent conditional Gaussian\nvariables are allowed to grow with the number of training samples. \n\n"}
{"id": "1901.09892", "contents": "Title: A Black-box Attack on Neural Networks Based on Swarm Evolutionary\n  Algorithm Abstract: Neural networks play an increasingly important role in the field of machine\nlearning and are included in many applications in society. Unfortunately,\nneural networks suffer from adversarial samples generated to attack them.\nHowever, most of the generation approaches either assume that the attacker has\nfull knowledge of the neural network model or are limited by the type of\nattacked model. In this paper, we propose a new approach that generates a\nblack-box attack to neural networks based on the swarm evolutionary algorithm.\nBenefiting from the improvements in the technology and theoretical\ncharacteristics of evolutionary algorithms, our approach has the advantages of\neffectiveness, black-box attack, generality, and randomness. Our experimental\nresults show that both the MNIST images and the CIFAR-10 images can be\nperturbed to successful generate a black-box attack with 100\\% probability on\naverage. In addition, the proposed attack, which is successful on distilled\nneural networks with almost 100\\% probability, is resistant to defensive\ndistillation. The experimental results also indicate that the robustness of the\nartificial intelligence algorithm is related to the complexity of the model and\nthe data set. In addition, we find that the adversarial samples to some extent\nreproduce the characteristics of the sample data learned by the neural network\nmodel. \n\n"}
{"id": "1901.10026", "contents": "Title: Heterogeneous Network Motifs Abstract: Many real-world applications give rise to large heterogeneous networks where\nnodes and edges can be of any arbitrary type (e.g., user, web page, location).\nSpecial cases of such heterogeneous graphs include homogeneous graphs,\nbipartite, k-partite, signed, labeled graphs, among many others. In this work,\nwe generalize the notion of network motifs to heterogeneous networks. In\nparticular, small induced typed subgraphs called typed graphlets (heterogeneous\nnetwork motifs) are introduced and shown to be the fundamental building blocks\nof complex heterogeneous networks. Typed graphlets are a powerful\ngeneralization of the notion of graphlet (network motif) to heterogeneous\nnetworks as they capture both the induced subgraph of interest and the types\nassociated with the nodes in the induced subgraph. To address this problem, we\npropose a fast, parallel, and space-efficient framework for counting typed\ngraphlets in large networks. We discover the existence of non-trivial\ncombinatorial relationships between lower-order ($k-1$)-node typed graphlets\nand leverage them for deriving many of the $k$-node typed graphlets in $o(1)$\nconstant time. Thus, we avoid explicit enumeration of those typed graphlets.\nNotably, the time complexity matches the best untyped graphlet counting\nalgorithm. The experiments demonstrate the effectiveness of the proposed\nframework in terms of runtime, space-efficiency, parallel speedup, and\nscalability as it is able to handle large-scale networks. \n\n"}
{"id": "1901.10084", "contents": "Title: A Parallel Projection Method for Metric Constrained Optimization Abstract: Many clustering applications in machine learning and data mining rely on\nsolving metric-constrained optimization problems. These problems are\ncharacterized by $O(n^3)$ constraints that enforce triangle inequalities on\ndistance variables associated with $n$ objects in a large dataset. Despite its\nusefulness, metric-constrained optimization is challenging in practice due to\nthe cubic number of constraints and the high-memory requirements of standard\noptimization software. Recent work has shown that iterative projection methods\nare able to solve metric-constrained optimization problems on a much larger\nscale than was previously possible, thanks to their comparatively low memory\nrequirement. However, the major limitation of projection methods is their slow\nconvergence rate. In this paper we present a parallel projection method for\nmetric-constrained optimization which allows us to speed up the convergence\nrate in practice. The key to our approach is a new parallel execution schedule\nthat allows us to perform projections at multiple metric constraints\nsimultaneously without any conflicts or locking of variables. We illustrate the\neffectiveness of this execution schedule by implementing and testing a parallel\nprojection method for solving the metric-constrained linear programming\nrelaxation of correlation clustering. We show numerous experimental results on\nproblems involving up to 2.9 trillion constraints. \n\n"}
{"id": "1901.10310", "contents": "Title: Robust Learning from Untrusted Sources Abstract: Modern machine learning methods often require more data for training than a\nsingle expert can provide. Therefore, it has become a standard procedure to\ncollect data from external sources, e.g. via crowdsourcing. Unfortunately, the\nquality of these sources is not always guaranteed. As additional complications,\nthe data might be stored in a distributed way, or might even have to remain\nprivate. In this work, we address the question of how to learn robustly in such\nscenarios. Studying the problem through the lens of statistical learning\ntheory, we derive a procedure that allows for learning from all available\nsources, yet automatically suppresses irrelevant or corrupted data. We show by\nextensive experiments that our method provides significant improvements over\nalternative approaches from robust statistics and distributed optimization. \n\n"}
{"id": "1901.10387", "contents": "Title: Matching is as Easy as the Decision Problem, in the NC Model Abstract: Is matching in NC, i.e., is there a deterministic fast parallel algorithm for\nit? This has been an outstanding open question in TCS for over three decades,\never since the discovery of randomized NC matching algorithms [KUW85, MVV87].\nOver the last five years, the theoretical computer science community has\nlaunched a relentless attack on this question, leading to the discovery of\nseveral powerful ideas. We give what appears to be the culmination of this line\nof work: An NC algorithm for finding a minimum-weight perfect matching in a\ngeneral graph with polynomially bounded edge weights, provided it is given an\noracle for the decision problem. Consequently, for settling the main open\nproblem, it suffices to obtain an NC algorithm for the decision problem. We\nbelieve this new fact has qualitatively changed the nature of this open\nproblem.\n  All known efficient matching algorithms for general graphs follow one of two\napproaches: given by Edmonds [Edm65] and Lov\\'asz [Lov79]. Our oracle-based\nalgorithm follows a new approach and uses many of the ideas discovered in the\nlast five years.\n  The difficulty of obtaining an NC perfect matching algorithm led researchers\nto study matching vis-a-vis clever relaxations of the class NC. In this vein,\nrecently Goldwasser and Grossman [GG15] gave a pseudo-deterministic RNC\nalgorithm for finding a perfect matching in a bipartite graph, i.e., an RNC\nalgorithm with the additional requirement that on the same graph, it should\nreturn the same (i.e., unique) perfect matching for almost all choices of\nrandom bits. A corollary of our reduction is an analogous algorithm for general\ngraphs. \n\n"}
{"id": "cond-mat/0009417", "contents": "Title: Typical solution time for a vertex-covering algorithm on\n  finite-connectivity random graphs Abstract: In this letter, we analytically describe the typical solution time needed by\na backtracking algorithm to solve the vertex-cover problem on\nfinite-connectivity random graphs. We find two different transitions: The first\none is algorithm-dependent and marks the dynamical transition from linear to\nexponential solution times. The second one gives the maximum computational\ncomplexity, and is found exactly at the threshold where the system undergoes an\nalgorithm-independent phase transition in its solvability. Analytical results\nare corroborated by numerical simulations. \n\n"}
{"id": "cond-mat/0301271", "contents": "Title: Solving satisfiability problems by fluctuations: The dynamics of\n  stochastic local search algorithms Abstract: Stochastic local search algorithms are frequently used to numerically solve\nhard combinatorial optimization or decision problems. We give numerical and\napproximate analytical descriptions of the dynamics of such algorithms applied\nto random satisfiability problems. We find two different dynamical regimes,\ndepending on the number of constraints per variable: For low constraintness,\nthe problems are solved efficiently, i.e. in linear time. For higher\nconstraintness, the solution times become exponential. We observe that the\ndynamical behavior is characterized by a fast equilibration and fluctuations\naround this equilibrium. If the algorithm runs long enough, an exponentially\nrare fluctuation towards a solution appears. \n\n"}
{"id": "cond-mat/0301272", "contents": "Title: Relaxation and Metastability in the RandomWalkSAT search procedure Abstract: An analysis of the average properties of a local search resolution procedure\nfor the satisfaction of random Boolean constraints is presented. Depending on\nthe ratio alpha of constraints per variable, resolution takes a time T_res\ngrowing linearly (T_res \\sim tau(alpha) N, alpha < alpha_d) or exponentially\n(T_res \\sim exp(N zeta(alpha)), alpha > alpha_d) with the size N of the\ninstance. The relaxation time tau(alpha) in the linear phase is calculated\nthrough a systematic expansion scheme based on a quantum formulation of the\nevolution operator. For alpha > alpha_d, the system is trapped in some\nmetastable state, and resolution occurs from escape from this state through\ncrossing of a large barrier. An annealed calculation of the height zeta(alpha)\nof this barrier is proposed. The polynomial/exponentiel cross-over alpha_d is\nnot related to the onset of clustering among solutions. \n\n"}
{"id": "cond-mat/0501707", "contents": "Title: Focused Local Search for Random 3-Satisfiability Abstract: A local search algorithm solving an NP-complete optimisation problem can be\nviewed as a stochastic process moving in an 'energy landscape' towards\neventually finding an optimal solution. For the random 3-satisfiability\nproblem, the heuristic of focusing the local moves on the presently\nunsatisfiedclauses is known to be very effective: the time to solution has been\nobserved to grow only linearly in the number of variables, for a given\nclauses-to-variables ratio $\\alpha$ sufficiently far below the critical\nsatisfiability threshold $\\alpha_c \\approx 4.27$. We present numerical results\non the behaviour of three focused local search algorithms for this problem,\nconsidering in particular the characteristics of a focused variant of the\nsimple Metropolis dynamics. We estimate the optimal value for the\n``temperature'' parameter $\\eta$ for this algorithm, such that its linear-time\nregime extends as close to $\\alpha_c$ as possible. Similar parameter\noptimisation is performed also for the well-known WalkSAT algorithm and for the\nless studied, but very well performing Focused Record-to-Record Travel method.\nWe observe that with an appropriate choice of parameters, the linear time\nregime for each of these algorithms seems to extend well into ratios $\\alpha >\n4.2$ -- much further than has so far been generally assumed. We discuss the\nstatistics of solution times for the algorithms, relate their performance to\nthe process of ``whitening'', and present some conjectures on the shape of\ntheir computational phase diagrams. \n\n"}
{"id": "cond-mat/0603350", "contents": "Title: The number of matchings in random graphs Abstract: We study matchings on sparse random graphs by means of the cavity method. We\nfirst show how the method reproduces several known results about maximum and\nperfect matchings in regular and Erdos-Renyi random graphs. Our main new result\nis the computation of the entropy, i.e. the leading order of the logarithm of\nthe number of solutions, of matchings with a given size. We derive both an\nalgorithm to compute this entropy for an arbitrary graph with a girth that\ndiverges in the large size limit, and an analytic result for the entropy in\nregular and Erdos-Renyi random graph ensembles. \n\n"}
{"id": "cs/0012011", "contents": "Title: Towards a Universal Theory of Artificial Intelligence based on\n  Algorithmic Probability and Sequential Decision Theory Abstract: Decision theory formally solves the problem of rational agents in uncertain\nworlds if the true environmental probability distribution is known.\nSolomonoff's theory of universal induction formally solves the problem of\nsequence prediction for unknown distribution. We unify both theories and give\nstrong arguments that the resulting universal AIXI model behaves optimal in any\ncomputable environment. The major drawback of the AIXI model is that it is\nuncomputable. To overcome this problem, we construct a modified algorithm\nAIXI^tl, which is still superior to any other time t and space l bounded agent.\nThe computation time of AIXI^tl is of the order t x 2^l. \n\n"}
{"id": "cs/0106019", "contents": "Title: Playing Games with Algorithms: Algorithmic Combinatorial Game Theory Abstract: Combinatorial games lead to several interesting, clean problems in algorithms\nand complexity theory, many of which remain open. The purpose of this paper is\nto provide an overview of the area to encourage further research. In\nparticular, we begin with general background in Combinatorial Game Theory,\nwhich analyzes ideal play in perfect-information games, and Constraint Logic,\nwhich provides a framework for showing hardness. Then we survey results about\nthe complexity of determining ideal play in these games, and the related\nproblems of solving puzzles, in terms of both polynomial-time algorithms and\ncomputational intractability results. Our review of background and survey of\nalgorithmic results are by no means complete, but should serve as a useful\nprimer. \n\n"}
{"id": "cs/0106037", "contents": "Title: Using the No-Search Easy-Hard Technique for Downward Collapse Abstract: The top part of the preceding figure [figure appears in actual paper] shows\nsome classes from the (truth-table) bounded-query and boolean hierarchies. It\nis well-known that if either of these hierarchies collapses at a given level,\nthen all higher levels of that hierarchy collapse to that same level. This is a\nstandard ``upward translation of equality'' that has been known for over a\ndecade. The issue of whether these hierarchies can translate equality {\\em\ndownwards\\/} has proven vastly more challenging. In particular, with regard to\nthe figure above, consider the following claim:\n  $$P_{m-tt}^{\\Sigma_k^p} = P_{m+1-tt}^{\\Sigma_k^p} \\implies\n  DIFF_m(\\Sigma_k^p) coDIFF_m(\\Sigma_k^p) = BH(\\Sigma_k^p). (*) $$\n  This claim, if true, says that equality translates downwards between levels\nof the bounded-query hierarchy and the boolean hierarchy levels that (before\nthe fact) are immediately below them.\n  Until recently, it was not known whether (*) {\\em ever\\/} held, except for\nthe degenerate cases $m=0$ and $k=0$. Then Hemaspaandra, Hemaspaandra, and\nHempel \\cite{hem-hem-hem:j:downward-translation} proved that (*) holds for all\n$m$, for $k > 2$. Buhrman and Fortnow~\\cite{buh-for:j:two-queries} then showed\nthat, when $k=2$, (*) holds for the case $m = 1$. In this paper, we prove that\nfor the case $k=2$, (*) holds for all values of $m$. Since there is an oracle\nrelative to which ``for $k=1$, (*) holds for all $m$'' fails\n\\cite{buh-for:j:two-queries}, our achievement of the $k=2$ case cannot to be\nstrengthened to $k=1$ by any relativizable proof technique. The new downward\ntranslation we obtain also tightens the collapse in the polynomial hierarchy\nimplied by a collapse in the bounded-query hierarchy of the second level of the\npolynomial hierarchy. \n\n"}
{"id": "cs/0201007", "contents": "Title: Algorithm for generating orthogonal matrices with rational elements Abstract: Special orthogonal matrices with rational elements form the group SO(n,Q),\nwhere Q is the field of rational numbers. A theorem describing the structure of\nan arbitrary matrix from this group is proved. This theorem yields an algorithm\nfor generating such matrices by means of random number routines. \n\n"}
{"id": "cs/0207082", "contents": "Title: Dynamic Generators of Topologically Embedded Graphs Abstract: We provide a data structure for maintaining an embedding of a graph on a\nsurface (represented combinatorially by a permutation of edges around each\nvertex) and computing generators of the fundamental group of the surface, in\namortized time O(log n + log g(log log g)^3) per update on a surface of genus\ng; we can also test orientability of the surface in the same time, and maintain\nthe minimum and maximum spanning tree of the graph in time O(log n + log^4 g)\nper update. Our data structure allows edge insertion and deletion as well as\nthe dual operations; these operations may implicitly change the genus of the\nembedding surface. We apply similar ideas to improve the constant factor in a\nseparator theorem for low-genus graphs, and to find in linear time a\ntree-decomposition of low-genus low-diameter graphs. \n\n"}
{"id": "cs/0301015", "contents": "Title: Some remarks on the survey decimation algorithm for K-satisfiability Abstract: In this note we study the convergence of the survey decimation algorithm. An\nanalytic formula for the reduction of the complexity during the decimation is\nderived. The limit of the converge of the algorithm are estimated in the random\ncase: interesting phenomena appear near the boundary of convergence. \n\n"}
{"id": "cs/0301030", "contents": "Title: Bounds on the Number of Longest Common Subsequences Abstract: This paper performs the analysis necessary to bound the running time of\nknown, efficient algorithms for generating all longest common subsequences.\nThat is, we bound the running time as a function of input size for algorithms\nwith time essentially proportional to the output size. This paper considers\nboth the case of computing all distinct LCSs and the case of computing all LCS\nembeddings. Also included is an analysis of how much better the efficient\nalgorithms are than the standard method of generating LCS embeddings. A full\nanalysis is carried out with running times measured as a function of the total\nnumber of input characters, and much of the analysis is also provided for cases\nin which the two input sequences are of the same specified length or of two\nindependently specified lengths. \n\n"}
{"id": "cs/0302022", "contents": "Title: Fault-tolerant routing in peer-to-peer systems Abstract: We consider the problem of designing an overlay network and routing mechanism\nthat permits finding resources efficiently in a peer-to-peer system. We argue\nthat many existing approaches to this problem can be modeled as the\nconstruction of a random graph embedded in a metric space whose points\nrepresent resource identifiers, where the probability of a connection between\ntwo nodes depends only on the distance between them in the metric space. We\nstudy the performance of a peer-to-peer system where nodes are embedded at grid\npoints in a simple metric space: a one-dimensional real line. We prove upper\nand lower bounds on the message complexity of locating particular resources in\nsuch a system, under a variety of assumptions about failures of either nodes or\nthe connections between them. Our lower bounds in particular show that the use\nof inverse power-law distributions in routing, as suggested by Kleinberg\n(1999), is close to optimal. We also give efficient heuristics to dynamically\nmaintain such a system as new nodes arrive and old nodes depart. Finally, we\ngive experimental results that suggest promising directions for future work. \n\n"}
{"id": "cs/0306043", "contents": "Title: Skip Graphs Abstract: Skip graphs are a novel distributed data structure, based on skip lists, that\nprovide the full functionality of a balanced tree in a distributed system where\nresources are stored in separate nodes that may fail at any time. They are\ndesigned for use in searching peer-to-peer systems, and by providing the\nability to perform queries based on key ordering, they improve on existing\nsearch tools that provide only hash table functionality. Unlike skip lists or\nother tree data structures, skip graphs are highly resilient, tolerating a\nlarge fraction of failed nodes without losing connectivity. In addition,\nconstructing, inserting new nodes into, searching a skip graph, and detecting\nand repairing errors in the data structure introduced by node failures can be\ndone using simple and straightforward algorithms. \n\n"}
{"id": "cs/0309023", "contents": "Title: Efficient Algorithms for Citation Network Analysis Abstract: In the paper very efficient, linear in number of arcs, algorithms for\ndetermining Hummon and Doreian's arc weights SPLC and SPNP in citation network\nare proposed, and some theoretical properties of these weights are presented.\nThe nonacyclicity problem in citation networks is discussed. An approach to\nidentify on the basis of arc weights an important small subnetwork is proposed\nand illustrated on the citation networks of SOM (self organizing maps)\nliterature and US patents. \n\n"}
{"id": "cs/0309033", "contents": "Title: Lower bounds for predecessor searching in the cell probe model Abstract: We consider a fundamental problem in data structures, static predecessor\nsearching: Given a subset S of size n from the universe [m], store S so that\nqueries of the form \"What is the predecessor of x in S?\" can be answered\nefficiently. We study this problem in the cell probe model introduced by Yao.\nRecently, Beame and Fich obtained optimal bounds on the number of probes needed\nby any deterministic query scheme if the associated storage scheme uses only\nn^{O(1)} cells of word size (\\log m)^{O(1)} bits. We give a new lower bound\nproof for this problem that matches the bounds of Beame and Fich. Our lower\nbound proof has the following advantages: it works for randomised query schemes\ntoo, while Beame and Fich's proof works for deterministic query schemes only.\nIt also extends to `quantum address-only' query schemes that we define in this\npaper, and is simpler than Beame and Fich's proof. We prove our lower bound\nusing the round elimination approach of Miltersen, Nisan, Safra and Wigderson.\nUsing tools from information theory, we prove a strong round elimination lemma\nfor communication complexity that enables us to obtain a tight lower bound for\nthe predecessor problem. Our strong round elimination lemma also extends to\nquantum communication complexity. We also use our round elimination lemma to\nobtain a rounds versus communication tradeoff for the `greater-than' problem,\nimproving on the tradeoff in Miltersen et al. We believe that our round\nelimination lemma is of independent interest and should have other\napplications. \n\n"}
{"id": "cs/0411064", "contents": "Title: Lower-Stretch Spanning Trees Abstract: We prove that every weighted graph contains a spanning tree subgraph of\naverage stretch O((log n log log n)^2). Moreover, we show how to construct such\na tree in time O(m log^2 n). \n\n"}
{"id": "cs/0412102", "contents": "Title: Quantum Interactive Proofs with Competing Provers Abstract: This paper studies quantum refereed games, which are quantum interactive\nproof systems with two competing provers: one that tries to convince the\nverifier to accept and the other that tries to convince the verifier to reject.\nWe prove that every language having an ordinary quantum interactive proof\nsystem also has a quantum refereed game in which the verifier exchanges just\none round of messages with each prover. A key part of our proof is the fact\nthat there exists a single quantum measurement that reliably distinguishes\nbetween mixed states chosen arbitrarily from disjoint convex sets having large\nminimal trace distance from one another. We also show how to reduce the\nprobability of error for some classes of quantum refereed games. \n\n"}
{"id": "cs/0505061", "contents": "Title: EAH: A New Encoder based on Adaptive Variable-length Codes Abstract: Adaptive variable-length codes associate a variable-length codeword to the\nsymbol being encoded depending on the previous symbols in the input string.\nThis class of codes has been recently presented in [Dragos Trinca,\narXiv:cs.DS/0505007] as a new class of non-standard variable-length codes. New\nalgorithms for data compression, based on adaptive variable-length codes of\norder one and Huffman's algorithm, have been recently presented in [Dragos\nTrinca, ITCC 2004]. In this paper, we extend the work done so far by the\nfollowing contributions: first, we propose an improved generalization of these\nalgorithms, called EAHn. Second, we compute the entropy bounds for EAHn, using\nthe well-known bounds for Huffman's algorithm. Third, we discuss implementation\ndetails and give reports of experimental results obtained on some well-known\ncorpora. Finally, we describe a parallel version of EAHn using the PRAM model\nof computation. \n\n"}
{"id": "cs/0605013", "contents": "Title: Geometric representation of graphs in low dimension Abstract: We give an efficient randomized algorithm to construct a box representation\nof any graph G on n vertices in $1.5 (\\Delta + 2) \\ln n$ dimensions, where\n$\\Delta$ is the maximum degree of G. We also show that $\\boxi(G) \\le (\\Delta +\n2) \\ln n$ for any graph G. Our bound is tight up to a factor of $\\ln n$. We\nalso show that our randomized algorithm can be derandomized to get a polynomial\ntime deterministic algorithm. Though our general upper bound is in terms of\nmaximum degree $\\Delta$, we show that for almost all graphs on n vertices, its\nboxicity is upper bound by $c\\cdot(d_{av} + 1) \\ln n$ where d_{av} is the\naverage degree and c is a small constant. Also, we show that for any graph G,\n$\\boxi(G) \\le \\sqrt{8 n d_{av} \\ln n}$, which is tight up to a factor of $b\n\\sqrt{\\ln n}$ for a constant b. \n\n"}
{"id": "cs/0606116", "contents": "Title: New Algorithms for Regular Expression Matching Abstract: In this paper we revisit the classical regular expression matching problem,\nnamely, given a regular expression $R$ and a string $Q$, decide if $Q$ matches\none of the strings specified by $R$. Let $m$ and $n$ be the length of $R$ and\n$Q$, respectively. On a standard unit-cost RAM with word length $w \\geq \\log\nn$, we show that the problem can be solved in $O(m)$ space with the following\nrunning times: \\begin{equation*} \\begin{cases}\n  O(n\\frac{m \\log w}{w} + m \\log w) & \\text{if $m > w$} \\\\\n  O(n\\log m + m\\log m) & \\text{if $\\sqrt{w} < m \\leq w$} \\\\\n  O(\\min(n+ m^2, n\\log m + m\\log m)) & \\text{if $m \\leq \\sqrt{w}$.} \\end{cases}\n\\end{equation*} This improves the best known time bound among algorithms using\n$O(m)$ space. Whenever $w \\geq \\log^2 n$ it improves all known time bounds\nregardless of how much space is used. \n\n"}
{"id": "cs/0610042", "contents": "Title: A Polynomial Time Algorithm for The Traveling Salesman Problem Abstract: The ATSP polytope can be expressed by asymmetric polynomial size linear\nprogram. \n\n"}
{"id": "cs/0702025", "contents": "Title: Algebraic Signal Processing Theory: Cooley-Tukey Type Algorithms for\n  DCTs and DSTs Abstract: This paper presents a systematic methodology based on the algebraic theory of\nsignal processing to classify and derive fast algorithms for linear transforms.\nInstead of manipulating the entries of transform matrices, our approach derives\nthe algorithms by stepwise decomposition of the associated signal models, or\npolynomial algebras. This decomposition is based on two generic methods or\nalgebraic principles that generalize the well-known Cooley-Tukey FFT and make\nthe algorithms' derivations concise and transparent. Application to the 16\ndiscrete cosine and sine transforms yields a large class of fast algorithms,\nmany of which have not been found before. \n\n"}
{"id": "cs/0703001", "contents": "Title: Embedding Graphs into the Extended Grid Abstract: Let $G=(V,E)$ be an arbitrary undirected source graph to be embedded in a\ntarget graph $EM$, the extended grid with vertices on integer grid points and\nedges to nearest and next-nearest neighbours. We present an algorithm showing\nhow to embed $G$ into $EM$ in both time and space $O(|V|^2)$ using the new\nnotions of islands and bridges. An island is a connected subgraph in the target\ngraph which is mapped from exactly one vertex in the source graph while a\nbridge is an edge between two islands which is mapped from exactly one edge in\nthe source graph. This work is motivated by real industrial applications in the\nfield of quantum computing and a need to efficiently embed source graphs in the\nextended grid. \n\n"}
{"id": "cs/0703031", "contents": "Title: Sampling Eulerian orientations of triangular lattice graphs Abstract: We consider the problem of sampling from the uniform distribution on the set\nof Eulerian orientations of subgraphs of the triangular lattice. Although it is\nknown that this can be achieved in polynomial time for any graph, the algorithm\nstudied here is more natural in the context of planar Eulerian graphs. We\nanalyse the mixing time of a Markov chain on the Eulerian orientations of a\nplanar graph which moves between orientations by reversing the edges of\ndirected faces. Using path coupling and the comparison method we obtain a\npolynomial upper bound on the mixing time of this chain for any solid subgraph\nof the triangular lattice. By considering the conductance of the chain we show\nthat there exist subgraphs with holes for which the chain will always take an\nexponential amount of time to converge. Finally, as an additional justification\nfor studying a Markov chain on the set of Eulerian orientations of planar\ngraphs, we show that the problem of counting Eulerian orientations remains\n#P-complete when restricted to planar graphs.\n  A preliminary version of this work appeared as an extended abstract in the\n2nd Algorithms and Complexity in Durham workshop. \n\n"}
{"id": "cs/0703110", "contents": "Title: Geometric Complexity Theory IV: nonstandard quantum group for the\n  Kronecker problem Abstract: The Kronecker coefficient g_{\\lambda \\mu \\nu} is the multiplicity of the\nGL(V)\\times GL(W)-irreducible V_\\lambda \\otimes W_\\mu in the restriction of the\nGL(X)-irreducible X_\\nu via the natural map GL(V)\\times GL(W) \\to GL(V \\otimes\nW), where V, W are \\mathbb{C}-vector spaces and X = V \\otimes W. A fundamental\nopen problem in algebraic combinatorics is to find a positive combinatorial\nformula for these coefficients.\n  We construct two quantum objects for this problem, which we call the\nnonstandard quantum group and nonstandard Hecke algebra. We show that the\nnonstandard quantum group has a compact real form and its representations are\ncompletely reducible, that the nonstandard Hecke algebra is semisimple, and\nthat they satisfy an analog of quantum Schur-Weyl duality.\n  Using these nonstandard objects as a guide, we follow the approach of Adsul,\nSohoni, and Subrahmanyam to construct, in the case dim(V) = dim(W) =2, a\nrepresentation \\check{X}_\\nu of the nonstandard quantum group that specializes\nto Res_{GL(V) \\times GL(W)} X_\\nu at q=1. We then define a global crystal basis\n+HNSTC(\\nu) of \\check{X}_\\nu that solves the two-row Kronecker problem: the\nnumber of highest weight elements of +HNSTC(\\nu) of weight (\\lambda,\\mu) is the\nKronecker coefficient g_{\\lambda \\mu \\nu}. We go on to develop the beginnings\nof a graphical calculus for this basis, along the lines of the U_q(\\sl_2)\ngraphical calculus, and use this to organize the crystal components of\n+HNSTC(\\nu) into eight families. This yields a fairly simple, explicit and\npositive formula for two-row Kronecker coefficients, generalizing a formula of\nBrown, van Willigenburg, and Zabrocki. As a byproduct of the approach, we also\nobtain a rule for the decomposition of Res_{GL_2 \\times GL_2 \\rtimes \\S_2}\nX_\\nu into irreducibles. \n\n"}
{"id": "math-ph/0701043", "contents": "Title: Strong Spatial Mixing and Rapid Mixing with Five Colours for the Kagome\n  Lattice Abstract: We consider proper 5-colourings of the kagome lattice. Proper q-colourings\ncorrespond to configurations in the zero-temperature q-state anti-ferromagnetic\nPotts model. Salas and Sokal have given a computer assisted proof of strong\nspatial mixing on the kagome lattice for q>=6 under any temperature, including\nzero temperature. It is believed that there is strong spatial mixing for q>=4.\nHere we give a computer assisted proof of strong spatial mixing for q=5 and\nzero temperature. It is commonly known that strong spatial mixing implies that\nthere is a unique infinite-volume Gibbs measure and that the Glauber dynamics\nis rapidly mixing. We give a proof of rapid mixing of the Glauber dynamics on\nany finite subset of the vertices of the kagome lattice, provided that the\nboundary is free (not coloured). The Glauber dynamics is not necessarily\nirreducible if the boundary is chosen arbitrarily for q=5 colours. The Glauber\ndynamics can be used to uniformly sample proper 5-colourings. Thus, a\nconsequence of rapidly mixing Glauber dynamics is that there is fully\npolynomial randomised approximation scheme for counting the number of proper\n5-colourings. \n\n"}
{"id": "math/0209316", "contents": "Title: Cycle and Circle Tests of Balance in Gain Graphs: Forbidden Minors and\n  Their Groups Abstract: We examine two criteria for balance of a gain graph, one based on binary\ncycles and one on circles. The graphs for which each criterion is valid depend\non the set of allowed gain groups. The binary cycle test is invalid, except for\nforests, if any possible gain group has an element of odd order. Assuming all\ngroups are allowed, or all abelian groups, or merely the cyclic group of order\n3, we characterize, both constructively and by forbidden minors, the graphs for\nwhich the circle test is valid. It turns out that these three classes of groups\nhave the same set of forbidden minors. The exact reason for the importance of\nthe ternary cyclic group is not clear. \n\n"}
{"id": "math/0301274", "contents": "Title: On the existence of a new family of Diophantine equations for $\\bf\n  \\Omega$ Abstract: We show how to determine the $k$-th bit of Chaitin's algorithmically random\nreal number $\\Omega$ by solving $k$ instances of the halting problem. From this\nwe then reduce the problem of determining the $k$-th bit of $\\Omega$ to\ndetermining whether a certain Diophantine equation with two parameters, $k$ and\n$N$, has solutions for an odd or an even number of values of $N$. We also\ndemonstrate two further examples of $\\Omega$ in number theory: an exponential\nDiophantine equation with a parameter $k$ which has an odd number of solutions\niff the $k$-th bit of $\\Omega$ is 1, and a polynomial of positive integer\nvariables and a parameter $k$ that takes on an odd number of positive values\niff the $k$-th bit of $\\Omega$ is 1. \n\n"}
{"id": "math/0605472", "contents": "Title: An algebraic approach to Polya processes Abstract: P\\'olya processes are natural generalization of P\\'olya-Eggenberger urn\nmodels. This article presents a new approach of their asymptotic behaviour {\\it\nvia} moments, based on the spectral decomposition of a suitable finite\ndifference operator on polynomial functions. Especially, it provides new\nresults for {\\it large} processes (a P\\'olya process is called {\\it small} when\n1 is simple eigenvalue of its replacement matrix and when any other eigenvalue\nhas a real part $\\leq 1/2$; otherwise, it is called large). \n\n"}
{"id": "math/0606122", "contents": "Title: Diagonal Peg Solitaire Abstract: We study the classical game of peg solitaire when diagonal jumps are allowed.\nWe prove that on many boards, one can begin from a full board with one peg\nmissing, and finish with one peg anywhere on the board. We then consider the\nproblem of finding solutions that minimize the number of moves (where a move is\none or more jumps by the same peg), and find the shortest solution to the\n\"central game\", which begins and ends at the center. In some cases we can prove\nanalytically that our solutions are the shortest possible, in other cases we\napply A* or bidirectional search heuristics. \n\n"}
{"id": "quant-ph/0311001", "contents": "Title: Quantum walk algorithm for element distinctness Abstract: We use quantum walks to construct a new quantum algorithm for element\ndistinctness and its generalization. For element distinctness (the problem of\nfinding two equal items among N given items), we get an O(N^{2/3}) query\nquantum algorithm. This improves the previous O(N^{3/4}) query quantum\nalgorithm of Buhrman et.al. (quant-ph/0007016) and matches the lower bound by\nShi (quant-ph/0112086). The algorithm also solves the generalization of element\ndistinctness in which we have to find k equal items among N items. For this\nproblem, we get an O(N^{k/(k+1)}) query quantum algorithm. \n\n"}
{"id": "quant-ph/0402107", "contents": "Title: Coins Make Quantum Walks Faster Abstract: We show how to search N items arranged on a $\\sqrt{N}\\times\\sqrt{N}$ grid in\ntime $O(\\sqrt N \\log N)$, using a discrete time quantum walk. This result for\nthe first time exhibits a significant difference between discrete time and\ncontinuous time walks without coin degrees of freedom, since it has been shown\nrecently that such a continuous time walk needs time $\\Omega(N)$ to perform the\nsame task. Our result furthermore improves on a previous bound for quantum\nlocal search by Aaronson and Ambainis. We generalize our result to 3 and more\ndimensions where the walk yields the optimal performance of $O(\\sqrt{N})$ and\ngive several extensions of quantum walk search algorithms for general graphs.\nThe coin-flip operation needs to be chosen judiciously: we show that another\n``natural'' choice of coin gives a walk that takes $\\Omega(N)$ steps. We also\nshow that in 2 dimensions it is sufficient to have a two-dimensional coin-space\nto achieve the time $O(\\sqrt{N} \\log N)$. \n\n"}
