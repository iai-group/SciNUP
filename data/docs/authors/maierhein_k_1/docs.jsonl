{"id": "0711.3580", "contents": "Title: An evolutionary model with Turing machines Abstract: The development of a large non-coding fraction in eukaryotic DNA and the\nphenomenon of the code-bloat in the field of evolutionary computations show a\nstriking similarity. This seems to suggest that (in the presence of mechanisms\nof code growth) the evolution of a complex code can't be attained without\nmaintaining a large inactive fraction. To test this hypothesis we performed\ncomputer simulations of an evolutionary toy model for Turing machines, studying\nthe relations among fitness and coding/non-coding ratio while varying mutation\nand code growth rates. The results suggest that, in our model, having a large\nreservoir of non-coding states constitutes a great (long term) evolutionary\nadvantage. \n\n"}
{"id": "0712.4102", "contents": "Title: Digital Ecosystems: Evolving Service-Oriented Architectures Abstract: We view Digital Ecosystems to be the digital counterparts of biological\necosystems, exploiting the self-organising properties of biological ecosystems,\nwhich are considered to be robust, self-organising and scalable architectures\nthat can automatically solve complex, dynamic problems. Digital Ecosystems are\na novel optimisation technique where the optimisation works at two levels: a\nfirst optimisation, migration of agents (representing services) which are\ndistributed in a decentralised peer-to-peer network, operating continuously in\ntime; this process feeds a second optimisation based on evolutionary computing\nthat operates locally on single peers and is aimed at finding solutions to\nsatisfy locally relevant constraints. We created an Ecosystem-Oriented\nArchitecture of Digital Ecosystems by extending Service-Oriented Architectures\nwith distributed evolutionary computing, allowing services to recombine and\nevolve over time, constantly seeking to improve their effectiveness for the\nuser base. Individuals within our Digital Ecosystem will be applications\n(groups of services), created in response to user requests by using\nevolutionary optimisation to aggregate the services. These individuals will\nmigrate through the Digital Ecosystem and adapt to find niches where they are\nuseful in fulfilling other user requests for applications. Simulation results\nimply that the Digital Ecosystem performs better at large scales than a\ncomparable Service-Oriented Architecture, suggesting that incorporating ideas\nfrom theoretical ecology can contribute to useful self-organising properties in\ndigital ecosystems. \n\n"}
{"id": "0912.1310", "contents": "Title: Automatic creation of urban velocity fields from aerial video Abstract: In this paper, we present a system for modelling vehicle motion in an urban\nscene from low frame-rate aerial video. In particular, the scene is modelled as\na probability distribution over velocities at every pixel in the image.\n  We describe the complete system for acquiring this model. The video is\ncaptured from a helicopter and stabilized by warping the images to match an\northorectified image of the area. A pixel classifier is applied to the\nstabilized images, and the response is segmented to determine car locations and\norientations. The results are fed in to a tracking scheme which tracks cars for\nthree frames, creating tracklets. This allows the tracker to use a combination\nof velocity, direction, appearance, and acceleration cues to keep only tracks\nlikely to be correct. Each tracklet provides a measurement of the car velocity\nat every point along the tracklet's length, and these are then aggregated to\ncreate a histogram of vehicle velocities at every pixel in the image.\n  The results demonstrate that the velocity probability distribution prior can\nbe used to infer a variety of information about road lane directions, speed\nlimits, vehicle speeds and common trajectories, and traffic bottlenecks, as\nwell as providing a means of describing environmental knowledge about traffic\nrules that can be used in tracking. \n\n"}
{"id": "1003.0776", "contents": "Title: Properties of the Discrete Pulse Transform for Multi-Dimensional Arrays Abstract: This report presents properties of the Discrete Pulse Transform on\nmulti-dimensional arrays introduced by the authors two or so years ago. The\nmain result given here in Lemma 2.1 is also formulated in a paper to appear in\nIEEE Transactions on Image Processing. However, the proof, being too technical,\nwas omitted there and hence it appears in full in this publication. \n\n"}
{"id": "1011.1669", "contents": "Title: A \"missing\" family of classical orthogonal polynomials Abstract: We study a family of \"classical\" orthogonal polynomials which satisfy (apart\nfrom a 3-term recurrence relation) an eigenvalue problem with a differential\noperator of Dunkl-type. These polynomials can be obtained from the little\n$q$-Jacobi polynomials in the limit $q=-1$. We also show that these polynomials\nprovide a nontrivial realization of the Askey-Wilson algebra for $q=-1$. \n\n"}
{"id": "1106.3834", "contents": "Title: Dimensionally Constrained Symbolic Regression Abstract: We describe dimensionally constrained symbolic regression which has been\ndeveloped for mass measurement in certain classes of events in high-energy\nphysics (HEP). With symbolic regression, we can derive equations that are well\nknown in HEP. However, in problems with large number of variables, we find that\nby constraining the terms allowed in the symbolic regression, convergence\nbehavior is improved. Dimensionally constrained symbolic regression (DCSR)\nfinds solutions with much better fitness than is normally possible with\nsymbolic regression. In some cases, novel solutions are found. \n\n"}
{"id": "1109.4909", "contents": "Title: Sparse Online Low-Rank Projection and Outlier Rejection (SOLO) for 3-D\n  Rigid-Body Motion Registration Abstract: Motivated by an emerging theory of robust low-rank matrix representation, in\nthis paper, we introduce a novel solution for online rigid-body motion\nregistration. The goal is to develop algorithmic techniques that enable a\nrobust, real-time motion registration solution suitable for low-cost, portable\n3-D camera devices. Assuming 3-D image features are tracked via a standard\ntracker, the algorithm first utilizes Robust PCA to initialize a low-rank shape\nrepresentation of the rigid body. Robust PCA finds the global optimal solution\nof the initialization, while its complexity is comparable to singular value\ndecomposition. In the online update stage, we propose a more efficient\nalgorithm for sparse subspace projection to sequentially project new feature\nobservations onto the shape subspace. The lightweight update stage guarantees\nthe real-time performance of the solution while maintaining good registration\neven when the image sequence is contaminated by noise, gross data corruption,\noutlying features, and missing data. The state-of-the-art accuracy of the\nsolution is validated through extensive simulation and a real-world experiment,\nwhile the system enjoys one to two orders of magnitude speed-up compared to\nwell-established RANSAC solutions. The new algorithm will be released online to\naid peer evaluation. \n\n"}
{"id": "1110.0169", "contents": "Title: Robust artificial neural networks and outlier detection. Technical\n  report Abstract: Large outliers break down linear and nonlinear regression models. Robust\nregression methods allow one to filter out the outliers when building a model.\nBy replacing the traditional least squares criterion with the least trimmed\nsquares criterion, in which half of data is treated as potential outliers, one\ncan fit accurate regression models to strongly contaminated data.\nHigh-breakdown methods have become very well established in linear regression,\nbut have started being applied for non-linear regression only recently. In this\nwork, we examine the problem of fitting artificial neural networks to\ncontaminated data using least trimmed squares criterion. We introduce a\npenalized least trimmed squares criterion which prevents unnecessary removal of\nvalid data. Training of ANNs leads to a challenging non-smooth global\noptimization problem. We compare the efficiency of several derivative-free\noptimization methods in solving it, and show that our approach identifies the\noutliers correctly when ANNs are used for nonlinear regression. \n\n"}
{"id": "1203.0076", "contents": "Title: Using Barriers to Reduce the Sensitivity to Edge Miscalculations of\n  Casting-Based Object Projection Feature Estimation Abstract: 3D motion tracking is a critical task in many computer vision applications.\nUnsupervised markerless 3D motion tracking systems determine the most relevant\nobject in the screen and then track it by continuously estimating its\nprojection features (center and area) from the edge image and a point inside\nthe relevant object projection (namely, inner point), until the tracking fails.\nExisting reliable object projection feature estimation techniques are based on\nray-casting or grid-filling from the inner point. These techniques assume the\nedge image to be accurate. However, in real case scenarios, edge\nmiscalculations may arise from low contrast between the target object and its\nsurroundings or motion blur caused by low frame rates or fast moving target\nobjects. In this paper, we propose a barrier extension to casting-based\ntechniques that mitigates the effect of edge miscalculations. \n\n"}
{"id": "1203.1513", "contents": "Title: Invariant Scattering Convolution Networks Abstract: A wavelet scattering network computes a translation invariant image\nrepresentation, which is stable to deformations and preserves high frequency\ninformation for classification. It cascades wavelet transform convolutions with\nnon-linear modulus and averaging operators. The first network layer outputs\nSIFT-type descriptors whereas the next layers provide complementary invariant\ninformation which improves classification. The mathematical analysis of wavelet\nscattering networks explains important properties of deep convolution networks\nfor classification.\n  A scattering representation of stationary processes incorporates higher order\nmoments and can thus discriminate textures having the same Fourier power\nspectrum. State of the art classification results are obtained for handwritten\ndigits and texture discrimination, using a Gaussian kernel SVM and a generative\nPCA classifier. \n\n"}
{"id": "1207.4442", "contents": "Title: Complex-network analysis of combinatorial spaces: The NK landscape case Abstract: We propose a network characterization of combinatorial fitness landscapes by\nadapting the notion of inherent networks proposed for energy surfaces. We use\nthe well-known family of NK landscapes as an example. In our case the inherent\nnetwork is the graph whose vertices represent the local maxima in the\nlandscape, and the edges account for the transition probabilities between their\ncorresponding basins of attraction. We exhaustively extracted such networks on\nrepresentative NK landscape instances, and performed a statistical\ncharacterization of their properties. We found that most of these network\nproperties are related to the search difficulty on the underlying NK landscapes\nwith varying values of K. \n\n"}
{"id": "1208.5365", "contents": "Title: A Missing and Found Recognition System for Hajj and Umrah Abstract: This note describes an integrated recognition system for identifying missing\nand found objects as well as missing, dead, and found people during Hajj and\nUmrah seasons in the two Holy cities of Makkah and Madina in the Kingdom of\nSaudi Arabia. It is assumed that the total estimated number of pilgrims will\nreach 20 millions during the next decade. The ultimate goal of this system is\nto integrate facial recognition and object identification solutions into the\nHajj and Umrah rituals. The missing and found computerized system is part of\nthe CrowdSensing system for Hajj and Umrah crowd estimation, management and\nsafety. \n\n"}
{"id": "1209.0654", "contents": "Title: Compressive Optical Deflectometric Tomography: A Constrained\n  Total-Variation Minimization Approach Abstract: Optical Deflectometric Tomography (ODT) provides an accurate characterization\nof transparent materials whose complex surfaces present a real challenge for\nmanufacture and control. In ODT, the refractive index map (RIM) of a\ntransparent object is reconstructed by measuring light deflection under\nmultiple orientations. We show that this imaging modality can be made\n\"compressive\", i.e., a correct RIM reconstruction is achievable with far less\nobservations than required by traditional Filtered Back Projection (FBP)\nmethods. Assuming a cartoon-shape RIM model, this reconstruction is driven by\nminimizing the map Total-Variation under a fidelity constraint with the\navailable observations. Moreover, two other realistic assumptions are added to\nimprove the stability of our approach: the map positivity and a frontier\ncondition. Numerically, our method relies on an accurate ODT sensing model and\non a primal-dual minimization scheme, including easily the sensing operator and\nthe proposed RIM constraints. We conclude this paper by demonstrating the power\nof our method on synthetic and experimental data under various compressive\nscenarios. In particular, the compressiveness of the stabilized ODT problem is\ndemonstrated by observing a typical gain of 20 dB compared to FBP at only 5% of\n360 incident light angles for moderately noisy sensing. \n\n"}
{"id": "1211.7180", "contents": "Title: Multislice Modularity Optimization in Community Detection and Image\n  Segmentation Abstract: Because networks can be used to represent many complex systems, they have\nattracted considerable attention in physics, computer science, sociology, and\nmany other disciplines. One of the most important areas of network science is\nthe algorithmic detection of cohesive groups (i.e., \"communities\") of nodes. In\nthis paper, we algorithmically detect communities in social networks and image\ndata by optimizing multislice modularity. A key advantage of modularity\noptimization is that it does not require prior knowledge of the number or sizes\nof communities, and it is capable of finding network partitions that are\ncomposed of communities of different sizes. By optimizing multislice modularity\nand subsequently calculating diagnostics on the resulting network partitions,\nit is thereby possible to obtain information about network structure across\nmultiple system scales. We illustrate this method on data from both social\nnetworks and images, and we find that optimization of multislice modularity\nperforms well on these two tasks without the need for extensive\nproblem-specific adaptation. However, improving the computational speed of this\nmethod remains a challenging open problem. \n\n"}
{"id": "1212.3530", "contents": "Title: A Multi-Orientation Analysis Approach to Retinal Vessel Tracking Abstract: This paper presents a method for retinal vasculature extraction based on\nbiologically inspired multi-orientation analysis. We apply multi-orientation\nanalysis via so-called invertible orientation scores, modeling the cortical\ncolumns in the visual system of higher mammals. This allows us to generically\ndeal with many hitherto complex problems inherent to vessel tracking, such as\ncrossings, bifurcations, parallel vessels, vessels of varying widths and\nvessels with high curvature. Our approach applies tracking in invertible\norientation scores via a novel geometrical principle for curve optimization in\nthe Euclidean motion group SE(2). The method runs fully automatically and\nprovides a detailed model of the retinal vasculature, which is crucial as a\nsound basis for further quantitative analysis of the retina, especially in\nscreening applications. \n\n"}
{"id": "1212.4527", "contents": "Title: GMM-Based Hidden Markov Random Field for Color Image and 3D Volume\n  Segmentation Abstract: In this project, we first study the Gaussian-based hidden Markov random field\n(HMRF) model and its expectation-maximization (EM) algorithm. Then we\ngeneralize it to Gaussian mixture model-based hidden Markov random field. The\nalgorithm is implemented in MATLAB. We also apply this algorithm to color image\nsegmentation problems and 3D volume segmentation problems. \n\n"}
{"id": "1303.0018", "contents": "Title: Sparse Shape Reconstruction Abstract: This paper introduces a new shape-based image reconstruction technique\napplicable to a large class of imaging problems formulated in a variational\nsense. Given a collection of shape priors (a shape dictionary), we define our\nproblem as choosing the right elements and geometrically composing them through\nbasic set operations to characterize desired regions in the image. This\ncombinatorial problem can be relaxed and then solved using classical descent\nmethods. The main component of this relaxation is forming certain compactly\nsupported functions which we call \"knolls\", and reformulating the shape\nrepresentation as a basis expansion in terms of such functions. To select\nsuitable elements of the dictionary, our problem ultimately reduces to solving\na nonlinear program with sparsity constraints. We provide a new sparse\nnonlinear reconstruction technique to approach this problem. The performance of\nproposed technique is demonstrated with some standard imaging problems\nincluding image segmentation, X-ray tomography and diffusive tomography. \n\n"}
{"id": "1306.0152", "contents": "Title: An Analysis of the Connections Between Layers of Deep Neural Networks Abstract: We present an analysis of different techniques for selecting the connection\nbe- tween layers of deep neural networks. Traditional deep neural networks use\nran- dom connection tables between layers to keep the number of connections\nsmall and tune to different image features. This kind of connection performs\nadequately in supervised deep networks because their values are refined during\nthe training. On the other hand, in unsupervised learning, one cannot rely on\nback-propagation techniques to learn the connections between layers. In this\nwork, we tested four different techniques for connecting the first layer of the\nnetwork to the second layer on the CIFAR and SVHN datasets and showed that the\naccuracy can be im- proved up to 3% depending on the technique used. We also\nshowed that learning the connections based on the co-occurrences of the\nfeatures does not confer an advantage over a random connection table in small\nnetworks. This work is helpful to improve the efficiency of connections between\nthe layers of unsupervised deep neural networks. \n\n"}
{"id": "1306.1679", "contents": "Title: Clifford Fourier-Mellin transform with two real square roots of -1 in\n  Cl(p,q), p+q=2 Abstract: We describe a non-commutative generalization of the complex Fourier-Mellin\ntransform to Clifford algebra valued signal functions over the domain\n$\\R^{p,q}$ taking values in Cl(p,q), p+q=2.\n  Keywords: algebra, Fourier transforms; Logic, set theory, and algebra,\nFourier analysis, Integral transforms \n\n"}
{"id": "1307.3040", "contents": "Title: Between Sense and Sensibility: Declarative narrativisation of mental\n  models as a basis and benchmark for visuo-spatial cognition and computation\n  focussed collaborative cognitive systems Abstract: What lies between `\\emph{sensing}' and `\\emph{sensibility}'? In other words,\nwhat kind of cognitive processes mediate sensing capability, and the formation\nof sensible impressions ---e.g., abstractions, analogies, hypotheses and theory\nformation, beliefs and their revision, argument formation--- in domain-specific\nproblem solving, or in regular activities of everyday living, working and\nsimply going around in the environment? How can knowledge and reasoning about\nsuch capabilities, as exhibited by humans in particular problem contexts, be\nused as a model and benchmark for the development of collaborative cognitive\n(interaction) systems concerned with human assistance, assurance, and\nempowerment?\n  We pose these questions in the context of a range of assistive technologies\nconcerned with \\emph{visuo-spatial perception and cognition} tasks encompassing\naspects such as commonsense, creativity, and the application of specialist\ndomain knowledge and problem-solving thought processes. Assistive technologies\nbeing considered include: (a) human activity interpretation; (b) high-level\ncognitive rovotics; (c) people-centred creative design in domains such as\narchitecture & digital media creation, and (d) qualitative analyses geographic\ninformation systems. Computational narratives not only provide a rich cognitive\nbasis, but they also serve as a benchmark of functional performance in our\ndevelopment of computational cognitive assistance systems. We posit that\ncomputational narrativisation pertaining to space, actions, and change provides\na useful model of \\emph{visual} and \\emph{spatio-temporal thinking} within a\nwide-range of problem-solving tasks and application areas where collaborative\ncognitive systems could serve an assistive and empowering function. \n\n"}
{"id": "1308.0850", "contents": "Title: Generating Sequences With Recurrent Neural Networks Abstract: This paper shows how Long Short-term Memory recurrent neural networks can be\nused to generate complex sequences with long-range structure, simply by\npredicting one data point at a time. The approach is demonstrated for text\n(where the data are discrete) and online handwriting (where the data are\nreal-valued). It is then extended to handwriting synthesis by allowing the\nnetwork to condition its predictions on a text sequence. The resulting system\nis able to generate highly realistic cursive handwriting in a wide variety of\nstyles. \n\n"}
{"id": "1309.2074", "contents": "Title: Learning Transformations for Clustering and Classification Abstract: A low-rank transformation learning framework for subspace clustering and\nclassification is here proposed. Many high-dimensional data, such as face\nimages and motion sequences, approximately lie in a union of low-dimensional\nsubspaces. The corresponding subspace clustering problem has been extensively\nstudied in the literature to partition such high-dimensional data into clusters\ncorresponding to their underlying low-dimensional subspaces. However,\nlow-dimensional intrinsic structures are often violated for real-world\nobservations, as they can be corrupted by errors or deviate from ideal models.\nWe propose to address this by learning a linear transformation on subspaces\nusing matrix rank, via its convex surrogate nuclear norm, as the optimization\ncriteria. The learned linear transformation restores a low-rank structure for\ndata from the same subspace, and, at the same time, forces a a maximally\nseparated structure for data from different subspaces. In this way, we reduce\nvariations within subspaces, and increase separation between subspaces for a\nmore robust subspace clustering. This proposed learned robust subspace\nclustering framework significantly enhances the performance of existing\nsubspace clustering methods. Basic theoretical results here presented help to\nfurther support the underlying framework. To exploit the low-rank structures of\nthe transformed subspaces, we further introduce a fast subspace clustering\ntechnique, which efficiently combines robust PCA with sparse modeling. When\nclass labels are present at the training stage, we show this low-rank\ntransformation framework also significantly enhances classification\nperformance. Extensive experiments using public datasets are presented, showing\nthat the proposed approach significantly outperforms state-of-the-art methods\nfor subspace clustering and classification. \n\n"}
{"id": "1309.2094", "contents": "Title: The Linearized Bregman Method via Split Feasibility Problems: Analysis\n  and Generalizations Abstract: The linearized Bregman method is a method to calculate sparse solutions to\nsystems of linear equations. We formulate this problem as a split feasibility\nproblem, propose an algorithmic framework based on Bregman projections and\nprove a general convergence result for this framework. Convergence of the\nlinearized Bregman method will be obtained as a special case. Our approach also\nallows for several generalizations such as other objective functions,\nincremental iterations, incorporation of non-gaussian noise models or box\nconstraints. \n\n"}
{"id": "1310.4891", "contents": "Title: Dictionary Learning and Sparse Coding on Grassmann Manifolds: An\n  Extrinsic Solution Abstract: Recent advances in computer vision and machine learning suggest that a wide\nrange of problems can be addressed more appropriately by considering\nnon-Euclidean geometry. In this paper we explore sparse dictionary learning\nover the space of linear subspaces, which form Riemannian structures known as\nGrassmann manifolds. To this end, we propose to embed Grassmann manifolds into\nthe space of symmetric matrices by an isometric mapping, which enables us to\ndevise a closed-form solution for updating a Grassmann dictionary, atom by\natom. Furthermore, to handle non-linearity in data, we propose a kernelised\nversion of the dictionary learning algorithm. Experiments on several\nclassification tasks (face recognition, action recognition, dynamic texture\nclassification) show that the proposed approach achieves considerable\nimprovements in discrimination accuracy, in comparison to state-of-the-art\nmethods such as kernelised Affine Hull Method and graph-embedding Grassmann\ndiscriminant analysis. \n\n"}
{"id": "1311.6048", "contents": "Title: On the Design and Analysis of Multiple View Descriptors Abstract: We propose an extension of popular descriptors based on gradient orientation\nhistograms (HOG, computed in a single image) to multiple views. It hinges on\ninterpreting HOG as a conditional density in the space of sampled images, where\nthe effects of nuisance factors such as viewpoint and illumination are\nmarginalized. However, such marginalization is performed with respect to a very\ncoarse approximation of the underlying distribution. Our extension leverages on\nthe fact that multiple views of the same scene allow separating intrinsic from\nnuisance variability, and thus afford better marginalization of the latter. The\nresult is a descriptor that has the same complexity of single-view HOG, and can\nbe compared in the same manner, but exploits multiple views to better trade off\ninsensitivity to nuisance variability with specificity to intrinsic\nvariability. We also introduce a novel multi-view wide-baseline matching\ndataset, consisting of a mixture of real and synthetic objects with ground\ntruthed camera motion and dense three-dimensional geometry. \n\n"}
{"id": "1312.6082", "contents": "Title: Multi-digit Number Recognition from Street View Imagery using Deep\n  Convolutional Neural Networks Abstract: Recognizing arbitrary multi-character text in unconstrained natural\nphotographs is a hard problem. In this paper, we address an equally hard\nsub-problem in this domain viz. recognizing arbitrary multi-digit numbers from\nStreet View imagery. Traditional approaches to solve this problem typically\nseparate out the localization, segmentation, and recognition steps. In this\npaper we propose a unified approach that integrates these three steps via the\nuse of a deep convolutional neural network that operates directly on the image\npixels. We employ the DistBelief implementation of deep neural networks in\norder to train large, distributed neural networks on high quality images. We\nfind that the performance of this approach increases with the depth of the\nconvolutional network, with the best performance occurring in the deepest\narchitecture we trained, with eleven hidden layers. We evaluate this approach\non the publicly available SVHN dataset and achieve over $96\\%$ accuracy in\nrecognizing complete street numbers. We show that on a per-digit recognition\ntask, we improve upon the state-of-the-art, achieving $97.84\\%$ accuracy. We\nalso evaluate this approach on an even more challenging dataset generated from\nStreet View imagery containing several tens of millions of street number\nannotations and achieve over $90\\%$ accuracy. To further explore the\napplicability of the proposed system to broader text recognition tasks, we\napply it to synthetic distorted text from reCAPTCHA. reCAPTCHA is one of the\nmost secure reverse turing tests that uses distorted text to distinguish humans\nfrom bots. We report a $99.8\\%$ accuracy on the hardest category of reCAPTCHA.\nOur evaluations on both tasks indicate that at specific operating thresholds,\nthe performance of the proposed system is comparable to, and in some cases\nexceeds, that of human operators. \n\n"}
{"id": "1312.6199", "contents": "Title: Intriguing properties of neural networks Abstract: Deep neural networks are highly expressive models that have recently achieved\nstate of the art performance on speech and visual recognition tasks. While\ntheir expressiveness is the reason they succeed, it also causes them to learn\nuninterpretable solutions that could have counter-intuitive properties. In this\npaper we report two such properties.\n  First, we find that there is no distinction between individual high level\nunits and random linear combinations of high level units, according to various\nmethods of unit analysis. It suggests that it is the space, rather than the\nindividual units, that contains of the semantic information in the high layers\nof neural networks.\n  Second, we find that deep neural networks learn input-output mappings that\nare fairly discontinuous to a significant extend. We can cause the network to\nmisclassify an image by applying a certain imperceptible perturbation, which is\nfound by maximizing the network's prediction error. In addition, the specific\nnature of these perturbations is not a random artifact of learning: the same\nperturbation can cause a different network, that was trained on a different\nsubset of the dataset, to misclassify the same input. \n\n"}
{"id": "1401.7623", "contents": "Title: Graph matching: relax or not? Abstract: We consider the problem of exact and inexact matching of weighted undirected\ngraphs, in which a bijective correspondence is sought to minimize a quadratic\nweight disagreement. This computationally challenging problem is often relaxed\nas a convex quadratic program, in which the space of permutations is replaced\nby the space of doubly-stochastic matrices. However, the applicability of such\na relaxation is poorly understood. We define a broad class of friendly graphs\ncharacterized by an easily verifiable spectral property. We prove that for\nfriendly graphs, the convex relaxation is guaranteed to find the exact\nisomorphism or certify its inexistence. This result is further extended to\napproximately isomorphic graphs, for which we develop an explicit bound on the\namount of weight disagreement under which the relaxation is guaranteed to find\nthe globally optimal approximate isomorphism. We also show that in many cases,\nthe graph matching problem can be further harmlessly relaxed to a convex\nquadratic program with only n separable linear equality constraints, which is\nsubstantially more efficient than the standard relaxation involving 2n equality\nand n^2 inequality constraints. Finally, we show that our results are still\nvalid for unfriendly graphs if additional information in the form of seeds or\nattributes is allowed, with the latter satisfying an easy to verify spectral\ncharacteristic. \n\n"}
{"id": "1402.0170", "contents": "Title: Collaborative Receptive Field Learning Abstract: The challenge of object categorization in images is largely due to arbitrary\ntranslations and scales of the foreground objects. To attack this difficulty,\nwe propose a new approach called collaborative receptive field learning to\nextract specific receptive fields (RF's) or regions from multiple images, and\nthe selected RF's are supposed to focus on the foreground objects of a common\ncategory. To this end, we solve the problem by maximizing a submodular function\nover a similarity graph constructed by a pool of RF candidates. However,\nmeasuring pairwise distance of RF's for building the similarity graph is a\nnontrivial problem. Hence, we introduce a similarity metric called\npyramid-error distance (PED) to measure their pairwise distances through\nsumming up pyramid-like matching errors over a set of low-level features.\nBesides, in consistent with the proposed PED, we construct a simple\nnonparametric classifier for classification. Experimental results show that our\nmethod effectively discovers the foreground objects in images, and improves\nclassification performance. \n\n"}
{"id": "1402.0453", "contents": "Title: Fine-Grained Visual Categorization via Multi-stage Metric Learning Abstract: Fine-grained visual categorization (FGVC) is to categorize objects into\nsubordinate classes instead of basic classes. One major challenge in FGVC is\nthe co-occurrence of two issues: 1) many subordinate classes are highly\ncorrelated and are difficult to distinguish, and 2) there exists the large\nintra-class variation (e.g., due to object pose). This paper proposes to\nexplicitly address the above two issues via distance metric learning (DML). DML\naddresses the first issue by learning an embedding so that data points from the\nsame class will be pulled together while those from different classes should be\npushed apart from each other; and it addresses the second issue by allowing the\nflexibility that only a portion of the neighbors (not all data points) from the\nsame class need to be pulled together. However, feature representation of an\nimage is often high dimensional, and DML is known to have difficulty in dealing\nwith high dimensional feature vectors since it would require $\\mathcal{O}(d^2)$\nfor storage and $\\mathcal{O}(d^3)$ for optimization. To this end, we proposed a\nmulti-stage metric learning framework that divides the large-scale high\ndimensional learning problem to a series of simple subproblems, achieving\n$\\mathcal{O}(d)$ computational complexity. The empirical study with FVGC\nbenchmark datasets verifies that our method is both effective and efficient\ncompared to the state-of-the-art FGVC approaches. \n\n"}
{"id": "1402.5076", "contents": "Title: Robust Binary Fused Compressive Sensing using Adaptive Outlier Pursuit Abstract: We propose a new method, {\\it robust binary fused compressive sensing}\n(RoBFCS), to recover sparse piece-wise smooth signals from 1-bit compressive\nmeasurements. The proposed method is a modification of our previous {\\it binary\nfused compressive sensing} (BFCS) algorithm, which is based on the {\\it binary\niterative hard thresholding} (BIHT) algorithm. As in BIHT, the data term of the\nobjective function is a one-sided $\\ell_1$ (or $\\ell_2$) norm. Experiments show\nthat the proposed algorithm is able to take advantage of the piece-wise\nsmoothness of the original signal and detect sign flips and correct them,\nachieving more accurate recovery than BFCS and BIHT. \n\n"}
{"id": "1403.6706", "contents": "Title: Beyond L2-Loss Functions for Learning Sparse Models Abstract: Incorporating sparsity priors in learning tasks can give rise to simple, and\ninterpretable models for complex high dimensional data. Sparse models have\nfound widespread use in structure discovery, recovering data from corruptions,\nand a variety of large scale unsupervised and supervised learning problems.\nAssuming the availability of sufficient data, these methods infer dictionaries\nfor sparse representations by optimizing for high-fidelity reconstruction. In\nmost scenarios, the reconstruction quality is measured using the squared\nEuclidean distance, and efficient algorithms have been developed for both batch\nand online learning cases. However, new application domains motivate looking\nbeyond conventional loss functions. For example, robust loss functions such as\n$\\ell_1$ and Huber are useful in learning outlier-resilient models, and the\nquantile loss is beneficial in discovering structures that are the\nrepresentative of a particular quantile. These new applications motivate our\nwork in generalizing sparse learning to a broad class of convex loss functions.\nIn particular, we consider the class of piecewise linear quadratic (PLQ) cost\nfunctions that includes Huber, as well as $\\ell_1$, quantile, Vapnik, hinge\nloss, and smoothed variants of these penalties. We propose an algorithm to\nlearn dictionaries and obtain sparse codes when the data reconstruction\nfidelity is measured using any smooth PLQ cost function. We provide convergence\nguarantees for the proposed algorithm, and demonstrate the convergence behavior\nusing empirical experiments. Furthermore, we present three case studies that\nrequire the use of PLQ cost functions: (i) robust image modeling, (ii) tag\nrefinement for image annotation and retrieval and (iii) computing empirical\nconfidence limits for subspace clustering. \n\n"}
{"id": "1404.6351", "contents": "Title: Improving weather radar by fusion and classification Abstract: In air traffic management (ATM) all necessary operations (tactical planing,\nsector configuration, required staffing, runway configuration, routing of\napproaching aircrafts) rely on accurate measurements and predictions of the\ncurrent weather situation. An essential basis of information is delivered by\nweather radar images (WXR), which, unfortunately, exhibit a vast amount of\ndisturbances. Thus, the improvement of these datasets is the key factor for\nmore accurate predictions of weather phenomena and weather conditions. Image\nprocessing methods based on texture analysis and geometric operators allow to\nidentify regions including artefacts as well as zones of missing information.\nCorrection of these zones is implemented by exploiting multi-spectral satellite\ndata (Meteosat Second Generation). Results prove that the proposed system for\nartefact detection and data correction significantly improves the quality of\nWXR data and, thus, enables more reliable weather now- and forecast leading to\nincreased ATM safety. \n\n"}
{"id": "1406.1231", "contents": "Title: Multi-task Neural Networks for QSAR Predictions Abstract: Although artificial neural networks have occasionally been used for\nQuantitative Structure-Activity/Property Relationship (QSAR/QSPR) studies in\nthe past, the literature has of late been dominated by other machine learning\ntechniques such as random forests. However, a variety of new neural net\ntechniques along with successful applications in other domains have renewed\ninterest in network approaches. In this work, inspired by the winning team's\nuse of neural networks in a recent QSAR competition, we used an artificial\nneural network to learn a function that predicts activities of compounds for\nmultiple assays at the same time. We conducted experiments leveraging recent\nmethods for dealing with overfitting in neural networks as well as other tricks\nfrom the neural networks literature. We compared our methods to alternative\nmethods reported to perform well on these tasks and found that our neural net\nmethods provided superior performance. \n\n"}
{"id": "1406.1774", "contents": "Title: Small Sample Learning of Superpixel Classifiers for EM Segmentation-\n  Extended Version Abstract: Pixel and superpixel classifiers have become essential tools for EM\nsegmentation algorithms. Training these classifiers remains a major bottleneck\nprimarily due to the requirement of completely annotating the dataset which is\ntedious, error-prone and costly. In this paper, we propose an interactive\nlearning scheme for the superpixel classifier for EM segmentation. Our\nalgorithm is \"active semi-supervised\" because it requests the labels of a small\nnumber of examples from user and applies label propagation technique to\ngenerate these queries. Using only a small set ($<20\\%$) of all datapoints, the\nproposed algorithm consistently generates a classifier almost as accurate as\nthat estimated from a complete groundtruth. We provide segmentation results on\nmultiple datasets to show the strength of these classifiers. \n\n"}
{"id": "1406.2671", "contents": "Title: Conceptors: an easy introduction Abstract: Conceptors provide an elementary neuro-computational mechanism which sheds a\nfresh and unifying light on a diversity of cognitive phenomena. A number of\ndemanding learning and processing tasks can be solved with unprecedented ease,\nrobustness and accuracy. Some of these tasks were impossible to solve before.\nThis entirely informal paper introduces the basic principles of conceptors and\nhighlights some of their usages. \n\n"}
{"id": "1407.3501", "contents": "Title: Robots that can adapt like animals Abstract: As robots leave the controlled environments of factories to autonomously\nfunction in more complex, natural environments, they will have to respond to\nthe inevitable fact that they will become damaged. However, while animals can\nquickly adapt to a wide variety of injuries, current robots cannot \"think\noutside the box\" to find a compensatory behavior when damaged: they are limited\nto their pre-specified self-sensing abilities, can diagnose only anticipated\nfailure modes, and require a pre-programmed contingency plan for every type of\npotential damage, an impracticality for complex robots. Here we introduce an\nintelligent trial and error algorithm that allows robots to adapt to damage in\nless than two minutes, without requiring self-diagnosis or pre-specified\ncontingency plans. Before deployment, a robot exploits a novel algorithm to\ncreate a detailed map of the space of high-performing behaviors: This map\nrepresents the robot's intuitions about what behaviors it can perform and their\nvalue. If the robot is damaged, it uses these intuitions to guide a\ntrial-and-error learning algorithm that conducts intelligent experiments to\nrapidly discover a compensatory behavior that works in spite of the damage.\nExperiments reveal successful adaptations for a legged robot injured in five\ndifferent ways, including damaged, broken, and missing legs, and for a robotic\narm with joints broken in 14 different ways. This new technique will enable\nmore robust, effective, autonomous robots, and suggests principles that animals\nmay use to adapt to injury. \n\n"}
{"id": "1407.4764", "contents": "Title: Efficient On-the-fly Category Retrieval using ConvNets and GPUs Abstract: We investigate the gains in precision and speed, that can be obtained by\nusing Convolutional Networks (ConvNets) for on-the-fly retrieval - where\nclassifiers are learnt at run time for a textual query from downloaded images,\nand used to rank large image or video datasets.\n  We make three contributions: (i) we present an evaluation of state-of-the-art\nimage representations for object category retrieval over standard benchmark\ndatasets containing 1M+ images; (ii) we show that ConvNets can be used to\nobtain features which are incredibly performant, and yet much lower dimensional\nthan previous state-of-the-art image representations, and that their\ndimensionality can be reduced further without loss in performance by\ncompression using product quantization or binarization. Consequently, features\nwith the state-of-the-art performance on large-scale datasets of millions of\nimages can fit in the memory of even a commodity GPU card; (iii) we show that\nan SVM classifier can be learnt within a ConvNet framework on a GPU in parallel\nwith downloading the new training images, allowing for a continuous refinement\nof the model as more images become available, and simultaneous training and\nranking. The outcome is an on-the-fly system that significantly outperforms its\npredecessors in terms of: precision of retrieval, memory requirements, and\nspeed, facilitating accurate on-the-fly learning and ranking in under a second\non a single GPU. \n\n"}
{"id": "1409.0473", "contents": "Title: Neural Machine Translation by Jointly Learning to Align and Translate Abstract: Neural machine translation is a recently proposed approach to machine\ntranslation. Unlike the traditional statistical machine translation, the neural\nmachine translation aims at building a single neural network that can be\njointly tuned to maximize the translation performance. The models proposed\nrecently for neural machine translation often belong to a family of\nencoder-decoders and consists of an encoder that encodes a source sentence into\na fixed-length vector from which a decoder generates a translation. In this\npaper, we conjecture that the use of a fixed-length vector is a bottleneck in\nimproving the performance of this basic encoder-decoder architecture, and\npropose to extend this by allowing a model to automatically (soft-)search for\nparts of a source sentence that are relevant to predicting a target word,\nwithout having to form these parts as a hard segment explicitly. With this new\napproach, we achieve a translation performance comparable to the existing\nstate-of-the-art phrase-based system on the task of English-to-French\ntranslation. Furthermore, qualitative analysis reveals that the\n(soft-)alignments found by the model agree well with our intuition. \n\n"}
{"id": "1409.6041", "contents": "Title: Domain Adaptive Neural Networks for Object Recognition Abstract: We propose a simple neural network model to deal with the domain adaptation\nproblem in object recognition. Our model incorporates the Maximum Mean\nDiscrepancy (MMD) measure as a regularization in the supervised learning to\nreduce the distribution mismatch between the source and target domains in the\nlatent space. From experiments, we demonstrate that the MMD regularization is\nan effective tool to provide good domain adaptation models on both SURF\nfeatures and raw image pixels of a particular image data set. We also show that\nour proposed model, preceded by the denoising auto-encoder pretraining,\nachieves better performance than recent benchmark models on the same data sets.\nThis work represents the first study of MMD measure in the context of neural\nnetworks. \n\n"}
{"id": "1409.7307", "contents": "Title: Image Classification with A Deep Network Model based on Compressive\n  Sensing Abstract: To simplify the parameter of the deep learning network, a cascaded\ncompressive sensing model \"CSNet\" is implemented for image classification.\nFirstly, we use cascaded compressive sensing network to learn feature from the\ndata. Secondly, CSNet generates the feature by binary hashing and block-wise\nhistograms. Finally, a linear SVM classifier is used to classify these\nfeatures. The experiments on the MNIST dataset indicate that higher\nclassification accuracy can be obtained by this algorithm. \n\n"}
{"id": "1410.0781", "contents": "Title: SimNets: A Generalization of Convolutional Networks Abstract: We present a deep layered architecture that generalizes classical\nconvolutional neural networks (ConvNets). The architecture, called SimNets, is\ndriven by two operators, one being a similarity function whose family contains\nthe convolution operator used in ConvNets, and the other is a new soft\nmax-min-mean operator called MEX that realizes classical operators like ReLU\nand max pooling, but has additional capabilities that make SimNets a powerful\ngeneralization of ConvNets. Three interesting properties emerge from the\narchitecture: (i) the basic input to hidden layer to output machinery contains\nas special cases kernel machines with the Exponential and Generalized Gaussian\nkernels, the output units being \"neurons in feature space\" (ii) in its general\nform, the basic machinery has a higher abstraction level than kernel machines,\nand (iii) initializing networks using unsupervised learning is natural.\nExperiments demonstrate the capability of achieving state of the art accuracy\nwith networks that are an order of magnitude smaller than comparable ConvNets. \n\n"}
{"id": "1410.5224", "contents": "Title: Supervised mid-level features for word image representation Abstract: This paper addresses the problem of learning word image representations:\ngiven the cropped image of a word, we are interested in finding a descriptive,\nrobust, and compact fixed-length representation. Machine learning techniques\ncan then be supplied with these representations to produce models useful for\nword retrieval or recognition tasks. Although many works have focused on the\nmachine learning aspect once a global representation has been produced, little\nwork has been devoted to the construction of those base image representations:\nmost works use standard coding and aggregation techniques directly on top of\nstandard computer vision features such as SIFT or HOG.\n  We propose to learn local mid-level features suitable for building word image\nrepresentations. These features are learnt by leveraging character bounding box\nannotations on a small set of training images. However, contrary to other\napproaches that use character bounding box information, our approach does not\nrely on detecting the individual characters explicitly at testing time. Our\nlocal mid-level features can then be aggregated to produce a global word image\nsignature. When pairing these features with the recent word attributes\nframework of Almaz\\'an et al., we obtain results comparable with or better than\nthe state-of-the-art on matching and recognition tasks using global descriptors\nof only 96 dimensions. \n\n"}
{"id": "1410.5263", "contents": "Title: Building pattern recognition applications with the SPARE library Abstract: This paper presents the SPARE C++ library, an open source software tool\nconceived to build pattern recognition and soft computing systems. The library\nfollows the requirement of the generality: most of the implemented algorithms\nare able to process user-defined input data types transparently, such as\nlabeled graphs and sequences of objects, as well as standard numeric vectors.\nHere we present a high-level picture of the SPARE library characteristics,\nfocusing instead on the specific practical possibility of constructing pattern\nrecognition systems for different input data types. In particular, as a proof\nof concept, we discuss two application instances involving clustering of\nreal-valued multidimensional sequences and classification of labeled graphs. \n\n"}
{"id": "1410.8206", "contents": "Title: Addressing the Rare Word Problem in Neural Machine Translation Abstract: Neural Machine Translation (NMT) is a new approach to machine translation\nthat has shown promising results that are comparable to traditional approaches.\nA significant weakness in conventional NMT systems is their inability to\ncorrectly translate very rare words: end-to-end NMTs tend to have relatively\nsmall vocabularies with a single unk symbol that represents every possible\nout-of-vocabulary (OOV) word. In this paper, we propose and implement an\neffective technique to address this problem. We train an NMT system on data\nthat is augmented by the output of a word alignment algorithm, allowing the NMT\nsystem to emit, for each OOV word in the target sentence, the position of its\ncorresponding word in the source sentence. This information is later utilized\nin a post-processing step that translates every OOV word using a dictionary.\nOur experiments on the WMT14 English to French translation task show that this\nmethod provides a substantial improvement of up to 2.8 BLEU points over an\nequivalent NMT system that does not use this technique. With 37.5 BLEU points,\nour NMT system is the first to surpass the best result achieved on a WMT14\ncontest task. \n\n"}
{"id": "1411.0022", "contents": "Title: Generalized Adaptive Dictionary Learning via Domain Shift Minimization Abstract: Visual data driven dictionaries have been successfully employed for various\nobject recognition and classification tasks. However, the task becomes more\nchallenging if the training and test data are from contrasting domains. In this\npaper, we propose a novel and generalized approach towards learning an adaptive\nand common dictionary for multiple domains. Precisely, we project the data from\ndifferent domains onto a low dimensional space while preserving the intrinsic\nstructure of data from each domain. We also minimize the domain-shift among the\ndata from each pair of domains. Simultaneously, we learn a common adaptive\ndictionary. Our algorithm can also be modified to learn class-specific\ndictionaries which can be used for classification. We additionally propose a\ndiscriminative manifold regularization which imposes the intrinsic structure of\nclass specific features onto the sparse coefficients. Experiments on image\nclassification show that our approach fares better compared to the existing\nstate-of-the-art methods. \n\n"}
{"id": "1411.3229", "contents": "Title: Multi-modal Image Registration for Correlative Microscopy Abstract: Correlative microscopy is a methodology combining the functionality of light\nmicroscopy with the high resolution of electron microscopy and other microscopy\ntechnologies. Image registration for correlative microscopy is quite\nchallenging because it is a multi-modal, multi-scale and multi-dimensional\nregistration problem. In this report, I introduce two methods of image\nregistration for correlative microscopy. The first method is based on fiducials\n(beads). I generate landmarks from the fiducials and compute the similarity\ntransformation matrix based on three pairs of nearest corresponding landmarks.\nA least-squares matching process is applied afterwards to further refine the\nregistration. The second method is inspired by the image analogies approach. I\nintroduce the sparse representation model into image analogies. I first train\nrepresentative image patches (dictionaries) for pre-registered datasets from\ntwo different modalities, and then I use the sparse coding technique to\ntransfer a given image to a predicted image from one modality to another based\non the learned dictionaries. The final image registration is between the\npredicted image and the original image corresponding to the given image in the\ndifferent modality. The method transforms a multi-modal registration problem to\na mono-modal one. I test my approaches on Transmission Electron Microscopy\n(TEM) and confocal microscopy images. Experimental results of the methods are\nalso shown in this report. \n\n"}
{"id": "1411.4894", "contents": "Title: Low-level Vision by Consensus in a Spatial Hierarchy of Regions Abstract: We introduce a multi-scale framework for low-level vision, where the goal is\nestimating physical scene values from image data---such as depth from stereo\nimage pairs. The framework uses a dense, overlapping set of image regions at\nmultiple scales and a \"local model,\" such as a slanted-plane model for stereo\ndisparity, that is expected to be valid piecewise across the visual field.\nEstimation is cast as optimization over a dichotomous mixture of variables,\nsimultaneously determining which regions are inliers with respect to the local\nmodel (binary variables) and the correct co-ordinates in the local model space\nfor each inlying region (continuous variables). When the regions are organized\ninto a multi-scale hierarchy, optimization can occur in an efficient and\nparallel architecture, where distributed computational units iteratively\nperform calculations and share information through sparse connections between\nparents and children. The framework performs well on a standard benchmark for\nbinocular stereo, and it produces a distributional scene representation that is\nappropriate for combining with higher-level reasoning and other low-level cues. \n\n"}
{"id": "1411.5328", "contents": "Title: ConceptLearner: Discovering Visual Concepts from Weakly Labeled Image\n  Collections Abstract: Discovering visual knowledge from weakly labeled data is crucial to scale up\ncomputer vision recognition system, since it is expensive to obtain fully\nlabeled data for a large number of concept categories. In this paper, we\npropose ConceptLearner, which is a scalable approach to discover visual\nconcepts from weakly labeled image collections. Thousands of visual concept\ndetectors are learned automatically, without human in the loop for additional\nannotation. We show that these learned detectors could be applied to recognize\nconcepts at image-level and to detect concepts at image region-level\naccurately. Under domain-specific supervision, we further evaluate the learned\nconcepts for scene recognition on SUN database and for object detection on\nPascal VOC 2007. ConceptLearner shows promising performance compared to fully\nsupervised and weakly supervised methods. \n\n"}
{"id": "1411.7466", "contents": "Title: The Treasure beneath Convolutional Layers: Cross-convolutional-layer\n  Pooling for Image Classification Abstract: A number of recent studies have shown that a Deep Convolutional Neural\nNetwork (DCNN) pretrained on a large dataset can be adopted as a universal\nimage description which leads to astounding performance in many visual\nclassification tasks. Most of these studies, if not all, adopt activations of\nthe fully-connected layer of a DCNN as the image or region representation and\nit is believed that convolutional layer activations are less discriminative.\n  This paper, however, advocates that if used appropriately convolutional layer\nactivations can be turned into a powerful image representation which enjoys\nmany advantages over fully-connected layer activations. This is achieved by\nadopting a new technique proposed in this paper called\ncross-convolutional-layer pooling. More specifically, it extracts subarrays of\nfeature maps of one convolutional layer as local features and pools the\nextracted features with the guidance of feature maps of the successive\nconvolutional layer. Compared with exising methods that apply DCNNs in the\nlocal feature setting, the proposed method is significantly faster since it\nrequires much fewer times of DCNN forward computation. Moreover, it avoids the\ndomain mismatch issue which is usually encountered when applying fully\nconnected layer activations to describe local regions. By applying our method\nto four popular visual classification tasks, it is demonstrated that the\nproposed method can achieve comparable or in some cases significantly better\nperformance than existing fully-connected layer based image representations\nwhile incurring much lower computational cost. \n\n"}
{"id": "1411.7714", "contents": "Title: Features in Concert: Discriminative Feature Selection meets Unsupervised\n  Clustering Abstract: Feature selection is an essential problem in computer vision, important for\ncategory learning and recognition. Along with the rapid development of a wide\nvariety of visual features and classifiers, there is a growing need for\nefficient feature selection and combination methods, to construct powerful\nclassifiers for more complex and higher-level recognition tasks. We propose an\nalgorithm that efficiently discovers sparse, compact representations of input\nfeatures or classifiers, from a vast sea of candidates, with important\noptimality properties, low computational cost and excellent accuracy in\npractice. Different from boosting, we start with a discriminant linear\nclassification formulation that encourages sparse solutions. Then we obtain an\nequivalent unsupervised clustering problem that jointly discovers ensembles of\ndiverse features. They are independently valuable but even more powerful when\nunited in a cluster of classifiers. We evaluate our method on the task of\nlarge-scale recognition in video and show that it significantly outperforms\nclassical selection approaches, such as AdaBoost and greedy forward-backward\nselection, and powerful classifiers such as SVMs, in speed of training and\nperformance, especially in the case of limited training data. \n\n"}
{"id": "1412.3121", "contents": "Title: Multimodal Transfer Deep Learning with Applications in Audio-Visual\n  Recognition Abstract: We propose a transfer deep learning (TDL) framework that can transfer the\nknowledge obtained from a single-modal neural network to a network with a\ndifferent modality. Specifically, we show that we can leverage speech data to\nfine-tune the network trained for video recognition, given an initial set of\naudio-video parallel dataset within the same semantics. Our approach first\nlearns the analogy-preserving embeddings between the abstract representations\nlearned from intermediate layers of each network, allowing for semantics-level\ntransfer between the source and target modalities. We then apply our neural\nnetwork operation that fine-tunes the target network with the additional\nknowledge transferred from the source network, while keeping the topology of\nthe target network unchanged. While we present an audio-visual recognition task\nas an application of our approach, our framework is flexible and thus can work\nwith any multimodal dataset, or with any already-existing deep networks that\nshare the common underlying semantics. In this work in progress report, we aim\nto provide comprehensive results of different configurations of the proposed\napproach on two widely used audio-visual datasets, and we discuss potential\napplications of the proposed approach. \n\n"}
{"id": "1412.3161", "contents": "Title: Object-centric Sampling for Fine-grained Image Classification Abstract: This paper proposes to go beyond the state-of-the-art deep convolutional\nneural network (CNN) by incorporating the information from object detection,\nfocusing on dealing with fine-grained image classification. Unfortunately, CNN\nsuffers from over-fiting when it is trained on existing fine-grained image\nclassification benchmarks, which typically only consist of less than a few tens\nof thousands training images. Therefore, we first construct a large-scale\nfine-grained car recognition dataset that consists of 333 car classes with more\nthan 150 thousand training images. With this large-scale dataset, we are able\nto build a strong baseline for CNN with top-1 classification accuracy of 81.6%.\nOne major challenge in fine-grained image classification is that many classes\nare very similar to each other while having large within-class variation. One\ncontributing factor to the within-class variation is cluttered image\nbackground. However, the existing CNN training takes uniform window sampling\nover the image, acting as blind on the location of the object of interest. In\ncontrast, this paper proposes an \\emph{object-centric sampling} (OCS) scheme\nthat samples image windows based on the object location information. The\nchallenge in using the location information lies in how to design powerful\nobject detector and how to handle the imperfectness of detection results. To\nthat end, we design a saliency-aware object detection approach specific for the\nsetting of fine-grained image classification, and the uncertainty of detection\nresults are naturally handled in our OCS scheme. Our framework is demonstrated\nto be very effective, improving top-1 accuracy to 89.3% (from 81.6%) on the\nlarge-scale fine-grained car classification dataset. \n\n"}
{"id": "1412.3635", "contents": "Title: Simulating a perceptron on a quantum computer Abstract: Perceptrons are the basic computational unit of artificial neural networks,\nas they model the activation mechanism of an output neuron due to incoming\nsignals from its neighbours. As linear classifiers, they play an important role\nin the foundations of machine learning. In the context of the emerging field of\nquantum machine learning, several attempts have been made to develop a\ncorresponding unit using quantum information theory. Based on the quantum phase\nestimation algorithm, this paper introduces a quantum perceptron model\nimitating the step-activation function of a classical perceptron. This scheme\nrequires resources in $\\mathcal{O}(n)$ (where $n$ is the size of the input) and\npromises efficient applications for more complex structures such as trainable\nquantum neural networks. \n\n"}
{"id": "1412.4044", "contents": "Title: Adaptive Stochastic Gradient Descent on the Grassmannian for Robust\n  Low-Rank Subspace Recovery and Clustering Abstract: In this paper, we present GASG21 (Grassmannian Adaptive Stochastic Gradient\nfor $L_{2,1}$ norm minimization), an adaptive stochastic gradient algorithm to\nrobustly recover the low-rank subspace from a large matrix. In the presence of\ncolumn outliers, we reformulate the batch mode matrix $L_{2,1}$ norm\nminimization with rank constraint problem as a stochastic optimization approach\nconstrained on Grassmann manifold. For each observed data vector, the low-rank\nsubspace $\\mathcal{S}$ is updated by taking a gradient step along the geodesic\nof Grassmannian. In order to accelerate the convergence rate of the stochastic\ngradient method, we choose to adaptively tune the constant step-size by\nleveraging the consecutive gradients. Furthermore, we demonstrate that with\nproper initialization, the K-subspaces extension, K-GASG21, can robustly\ncluster a large number of corrupted data vectors into a union of subspaces.\nNumerical experiments on synthetic and real data demonstrate the efficiency and\naccuracy of the proposed algorithms even with heavy column outliers corruption. \n\n"}
{"id": "1412.4313", "contents": "Title: Combining the Best of Graphical Models and ConvNets for Semantic\n  Segmentation Abstract: We present a two-module approach to semantic segmentation that incorporates\nConvolutional Networks (CNNs) and Graphical Models. Graphical models are used\nto generate a small (5-30) set of diverse segmentations proposals, such that\nthis set has high recall. Since the number of required proposals is so low, we\ncan extract fairly complex features to rank them. Our complex feature of choice\nis a novel CNN called SegNet, which directly outputs a (coarse) semantic\nsegmentation. Importantly, SegNet is specifically trained to optimize the\ncorpus-level PASCAL IOU loss function. To the best of our knowledge, this is\nthe first CNN specifically designed for semantic segmentation. This two-module\napproach achieves $52.5\\%$ on the PASCAL 2012 segmentation challenge. \n\n"}
{"id": "1412.5661", "contents": "Title: DeepID-Net: Deformable Deep Convolutional Neural Networks for Object\n  Detection Abstract: In this paper, we propose deformable deep convolutional neural networks for\ngeneric object detection. This new deep learning object detection framework has\ninnovations in multiple aspects. In the proposed new deep architecture, a new\ndeformation constrained pooling (def-pooling) layer models the deformation of\nobject parts with geometric constraint and penalty. A new pre-training strategy\nis proposed to learn feature representations more suitable for the object\ndetection task and with good generalization capability. By changing the net\nstructures, training strategies, adding and removing some key components in the\ndetection pipeline, a set of models with large diversity are obtained, which\nsignificantly improves the effectiveness of model averaging. The proposed\napproach improves the mean averaged precision obtained by RCNN\n\\cite{girshick2014rich}, which was the state-of-the-art, from 31\\% to 50.3\\% on\nthe ILSVRC2014 detection test set. It also outperforms the winner of\nILSVRC2014, GoogLeNet, by 6.1\\%. Detailed component-wise analysis is also\nprovided through extensive experimental evaluation, which provide a global view\nfor people to understand the deep learning object detection pipeline. \n\n"}
{"id": "1412.5903", "contents": "Title: Deep Structured Output Learning for Unconstrained Text Recognition Abstract: We develop a representation suitable for the unconstrained recognition of\nwords in natural images: the general case of no fixed lexicon and unknown\nlength.\n  To this end we propose a convolutional neural network (CNN) based\narchitecture which incorporates a Conditional Random Field (CRF) graphical\nmodel, taking the whole word image as a single input. The unaries of the CRF\nare provided by a CNN that predicts characters at each position of the output,\nwhile higher order terms are provided by another CNN that detects the presence\nof N-grams. We show that this entire model (CRF, character predictor, N-gram\npredictor) can be jointly optimised by back-propagating the structured output\nloss, essentially requiring the system to perform multi-task learning, and\ntraining uses purely synthetically generated data. The resulting model is a\nmore accurate system on standard real-world text recognition benchmarks than\ncharacter prediction alone, setting a benchmark for systems that have not been\ntrained on a particular lexicon. In addition, our model achieves\nstate-of-the-art accuracy in lexicon-constrained scenarios, without being\nspecifically modelled for constrained recognition. To test the generalisation\nof our model, we also perform experiments with random alpha-numeric strings to\nevaluate the method when no visual language model is applicable. \n\n"}
{"id": "1412.7009", "contents": "Title: Generative Class-conditional Autoencoders Abstract: Recent work by Bengio et al. (2013) proposes a sampling procedure for\ndenoising autoencoders which involves learning the transition operator of a\nMarkov chain. The transition operator is typically unimodal, which limits its\ncapacity to model complex data. In order to perform efficient sampling from\nconditional distributions, we extend this work, both theoretically and\nalgorithmically, to gated autoencoders (Memisevic, 2013), The proposed model is\nable to generate convincing class-conditional samples when trained on both the\nMNIST and TFD datasets. \n\n"}
{"id": "1412.7091", "contents": "Title: Efficient Exact Gradient Update for training Deep Networks with Very\n  Large Sparse Targets Abstract: An important class of problems involves training deep neural networks with\nsparse prediction targets of very high dimension D. These occur naturally in\ne.g. neural language models or the learning of word-embeddings, often posed as\npredicting the probability of next words among a vocabulary of size D (e.g. 200\n000). Computing the equally large, but typically non-sparse D-dimensional\noutput vector from a last hidden layer of reasonable dimension d (e.g. 500)\nincurs a prohibitive O(Dd) computational cost for each example, as does\nupdating the D x d output weight matrix and computing the gradient needed for\nbackpropagation to previous layers. While efficient handling of large sparse\nnetwork inputs is trivial, the case of large sparse targets is not, and has\nthus so far been sidestepped with approximate alternatives such as hierarchical\nsoftmax or sampling-based approximations during training. In this work we\ndevelop an original algorithmic approach which, for a family of loss functions\nthat includes squared error and spherical softmax, can compute the exact loss,\ngradient update for the output weights, and gradient for backpropagation, all\nin O(d^2) per example instead of O(Dd), remarkably without ever computing the\nD-dimensional output. The proposed algorithm yields a speedup of D/4d , i.e.\ntwo orders of magnitude for typical sizes, for that critical part of the\ncomputations that often dominates the training time in this kind of network\narchitecture. \n\n"}
{"id": "1412.7210", "contents": "Title: Denoising autoencoder with modulated lateral connections learns\n  invariant representations of natural images Abstract: Suitable lateral connections between encoder and decoder are shown to allow\nhigher layers of a denoising autoencoder (dAE) to focus on invariant\nrepresentations. In regular autoencoders, detailed information needs to be\ncarried through the highest layers but lateral connections from encoder to\ndecoder relieve this pressure. It is shown that abstract invariant features can\nbe translated to detailed reconstructions when invariant features are allowed\nto modulate the strength of the lateral connection. Three dAE structures with\nmodulated and additive lateral connections, and without lateral connections\nwere compared in experiments using real-world images. The experiments verify\nthat adding modulated lateral connections to the model 1) improves the accuracy\nof the probability model for inputs, as measured by denoising performance; 2)\nresults in representations whose degree of invariance grows faster towards the\nhigher layers; and 3) supports the formation of diverse invariant poolings. \n\n"}
{"id": "1412.8341", "contents": "Title: Spectral classification using convolutional neural networks Abstract: There is a great need for accurate and autonomous spectral classification\nmethods in astrophysics. This thesis is about training a convolutional neural\nnetwork (ConvNet) to recognize an object class (quasar, star or galaxy) from\none-dimension spectra only. Author developed several scripts and C programs for\ndatasets preparation, preprocessing and postprocessing of the data. EBLearn\nlibrary (developed by Pierre Sermanet and Yann LeCun) was used to create\nConvNets. Application on dataset of more than 60000 spectra yielded success\nrate of nearly 95%. This thesis conclusively proved great potential of\nconvolutional neural networks and deep learning methods in astrophysics. \n\n"}
{"id": "1501.00102", "contents": "Title: ModDrop: adaptive multi-modal gesture recognition Abstract: We present a method for gesture detection and localisation based on\nmulti-scale and multi-modal deep learning. Each visual modality captures\nspatial information at a particular spatial scale (such as motion of the upper\nbody or a hand), and the whole system operates at three temporal scales. Key to\nour technique is a training strategy which exploits: i) careful initialization\nof individual modalities; and ii) gradual fusion involving random dropping of\nseparate channels (dubbed ModDrop) for learning cross-modality correlations\nwhile preserving uniqueness of each modality-specific representation. We\npresent experiments on the ChaLearn 2014 Looking at People Challenge gesture\nrecognition track, in which we placed first out of 17 teams. Fusing multiple\nmodalities at several spatial and temporal scales leads to a significant\nincrease in recognition rates, allowing the model to compensate for errors of\nthe individual classifiers as well as noise in the separate channels.\nFuthermore, the proposed ModDrop training technique ensures robustness of the\nclassifier to missing signals in one or several channels to produce meaningful\npredictions from any number of available modalities. In addition, we\ndemonstrate the applicability of the proposed fusion scheme to modalities of\narbitrary nature by experiments on the same dataset augmented with audio. \n\n"}
{"id": "1501.00299", "contents": "Title: Sequence Modeling using Gated Recurrent Neural Networks Abstract: In this paper, we have used Recurrent Neural Networks to capture and model\nhuman motion data and generate motions by prediction of the next immediate data\npoint at each time-step. Our RNN is armed with recently proposed Gated\nRecurrent Units which has shown promising results in some sequence modeling\nproblems such as Machine Translation and Speech Synthesis. We demonstrate that\nthis model is able to capture long-term dependencies in data and generate\nrealistic motions. \n\n"}
{"id": "1501.07844", "contents": "Title: A Proximal Bregman Projection Approach to Continuous Max-Flow Problems\n  Using Entropic Distances Abstract: One issue limiting the adaption of large-scale multi-region segmentation is\nthe sometimes prohibitive memory requirements. This is especially troubling\nconsidering advances in massively parallel computing and commercial graphics\nprocessing units because of their already limited memory compared to the\ncurrent random access memory used in more traditional computation. To address\nthis issue in the field of continuous max-flow segmentation, we have developed\na \\textit{pseudo-flow} framework using the theory of Bregman proximal\nprojections and entropic distances which implicitly represents flow variables\nbetween labels and designated source and sink nodes. This reduces the memory\nrequirements for max-flow segmentation by approximately 20\\% for Potts models\nand approximately 30\\% for hierarchical max-flow (HMF) and directed acyclic\ngraph max-flow (DAGMF) models. This represents a great improvement in the\nstate-of-the-art in max-flow segmentation, allowing for much larger problems to\nbe addressed and accelerated using commercially available graphics processing\nhardware. \n\n"}
{"id": "1502.02766", "contents": "Title: Multi-view Face Detection Using Deep Convolutional Neural Networks Abstract: In this paper we consider the problem of multi-view face detection. While\nthere has been significant research on this problem, current state-of-the-art\napproaches for this task require annotation of facial landmarks, e.g. TSM [25],\nor annotation of face poses [28, 22]. They also require training dozens of\nmodels to fully capture faces in all orientations, e.g. 22 models in HeadHunter\nmethod [22]. In this paper we propose Deep Dense Face Detector (DDFD), a method\nthat does not require pose/landmark annotation and is able to detect faces in a\nwide range of orientations using a single model based on deep convolutional\nneural networks. The proposed method has minimal complexity; unlike other\nrecent deep learning object detection methods [9], it does not require\nadditional components such as segmentation, bounding-box regression, or SVM\nclassifiers. Furthermore, we analyzed scores of the proposed face detector for\nfaces in different orientations and found that 1) the proposed method is able\nto detect faces from different angles and can handle occlusion to some extent,\n2) there seems to be a correlation between dis- tribution of positive examples\nin the training set and scores of the proposed face detector. The latter\nsuggests that the proposed methods performance can be further improved by using\nbetter sampling strategies and more sophisticated data augmentation techniques.\nEvaluations on popular face detection benchmark datasets show that our\nsingle-model face detector algorithm has similar or better performance compared\nto the previous methods, which are more complex and require annotations of\neither different poses or facial landmarks. \n\n"}
{"id": "1502.03167", "contents": "Title: Batch Normalization: Accelerating Deep Network Training by Reducing\n  Internal Covariate Shift Abstract: Training Deep Neural Networks is complicated by the fact that the\ndistribution of each layer's inputs changes during training, as the parameters\nof the previous layers change. This slows down the training by requiring lower\nlearning rates and careful parameter initialization, and makes it notoriously\nhard to train models with saturating nonlinearities. We refer to this\nphenomenon as internal covariate shift, and address the problem by normalizing\nlayer inputs. Our method draws its strength from making normalization a part of\nthe model architecture and performing the normalization for each training\nmini-batch. Batch Normalization allows us to use much higher learning rates and\nbe less careful about initialization. It also acts as a regularizer, in some\ncases eliminating the need for Dropout. Applied to a state-of-the-art image\nclassification model, Batch Normalization achieves the same accuracy with 14\ntimes fewer training steps, and beats the original model by a significant\nmargin. Using an ensemble of batch-normalized networks, we improve upon the\nbest published result on ImageNet classification: reaching 4.9% top-5\nvalidation error (and 4.8% test error), exceeding the accuracy of human raters. \n\n"}
{"id": "1502.04569", "contents": "Title: Image Specificity Abstract: For some images, descriptions written by multiple people are consistent with\neach other. But for other images, descriptions across people vary considerably.\nIn other words, some images are specific $-$ they elicit consistent\ndescriptions from different people $-$ while other images are ambiguous.\nApplications involving images and text can benefit from an understanding of\nwhich images are specific and which ones are ambiguous. For instance, consider\ntext-based image retrieval. If a query description is moderately similar to the\ncaption (or reference description) of an ambiguous image, that query may be\nconsidered a decent match to the image. But if the image is very specific, a\nmoderate similarity between the query and the reference description may not be\nsufficient to retrieve the image.\n  In this paper, we introduce the notion of image specificity. We present two\nmechanisms to measure specificity given multiple descriptions of an image: an\nautomated measure and a measure that relies on human judgement. We analyze\nimage specificity with respect to image content and properties to better\nunderstand what makes an image specific. We then train models to automatically\npredict the specificity of an image from image features alone without requiring\ntextual descriptions of the image. Finally, we show that modeling image\nspecificity leads to improvements in a text-based image retrieval application. \n\n"}
{"id": "1502.05243", "contents": "Title: SA-CNN: Dynamic Scene Classification using Convolutional Neural Networks Abstract: The task of classifying videos of natural dynamic scenes into appropriate\nclasses has gained lot of attention in recent years. The problem especially\nbecomes challenging when the camera used to capture the video is dynamic. In\nthis paper, we analyse the performance of statistical aggregation (SA)\ntechniques on various pre-trained convolutional neural network(CNN) models to\naddress this problem. The proposed approach works by extracting CNN activation\nfeatures for a number of frames in a video and then uses an aggregation scheme\nin order to obtain a robust feature descriptor for the video. We show through\nresults that the proposed approach performs better than the-state-of-the arts\nfor the Maryland and YUPenn dataset. The final descriptor obtained is powerful\nenough to distinguish among dynamic scenes and is even capable of addressing\nthe scenario where the camera motion is dominant and the scene dynamics are\ncomplex. Further, this paper shows an extensive study on the performance of\nvarious aggregation methods and their combinations. We compare the proposed\napproach with other dynamic scene classification algorithms on two publicly\navailable datasets - Maryland and YUPenn to demonstrate the superior\nperformance of the proposed approach. \n\n"}
{"id": "1502.05742", "contents": "Title: Application of Independent Component Analysis Techniques in Speckle\n  Noise Reduction of Retinal OCT Images Abstract: Optical Coherence Tomography (OCT) is an emerging technique in the field of\nbiomedical imaging, with applications in ophthalmology, dermatology, coronary\nimaging etc. OCT images usually suffer from a granular pattern, called speckle\nnoise, which restricts the process of interpretation. Therefore the need for\nspeckle noise reduction techniques is of high importance. To the best of our\nknowledge, use of Independent Component Analysis (ICA) techniques has never\nbeen explored for speckle reduction of OCT images. Here, a comparative study of\nseveral ICA techniques (InfoMax, JADE, FastICA and SOBI) is provided for noise\nreduction of retinal OCT images. Having multiple B-scans of the same location,\nthe eye movements are compensated using a rigid registration technique. Then,\ndifferent ICA techniques are applied to the aggregated set of B-scans for\nextracting the noise-free image. Signal-to-Noise-Ratio (SNR),\nContrast-to-Noise-Ratio (CNR) and Equivalent-Number-of-Looks (ENL), as well as\nanalysis on the computational complexity of the methods, are considered as\nmetrics for comparison. The results show that use of ICA can be beneficial,\nespecially in case of having fewer number of B-scans. \n\n"}
{"id": "1502.08039", "contents": "Title: Probabilistic Zero-shot Classification with Semantic Rankings Abstract: In this paper we propose a non-metric ranking-based representation of\nsemantic similarity that allows natural aggregation of semantic information\nfrom multiple heterogeneous sources. We apply the ranking-based representation\nto zero-shot learning problems, and present deterministic and probabilistic\nzero-shot classifiers which can be built from pre-trained classifiers without\nretraining. We demonstrate their the advantages on two large real-world image\ndatasets. In particular, we show that aggregating different sources of semantic\ninformation, including crowd-sourcing, leads to more accurate classification. \n\n"}
{"id": "1503.01224", "contents": "Title: Temporal Pyramid Pooling Based Convolutional Neural Networks for Action\n  Recognition Abstract: Encouraged by the success of Convolutional Neural Networks (CNNs) in image\nclassification, recently much effort is spent on applying CNNs to video based\naction recognition problems. One challenge is that video contains a varying\nnumber of frames which is incompatible to the standard input format of CNNs.\nExisting methods handle this issue either by directly sampling a fixed number\nof frames or bypassing this issue by introducing a 3D convolutional layer which\nconducts convolution in spatial-temporal domain.\n  To solve this issue, here we propose a novel network structure which allows\nan arbitrary number of frames as the network input. The key of our solution is\nto introduce a module consisting of an encoding layer and a temporal pyramid\npooling layer. The encoding layer maps the activation from previous layers to a\nfeature vector suitable for pooling while the temporal pyramid pooling layer\nconverts multiple frame-level activations into a fixed-length video-level\nrepresentation. In addition, we adopt a feature concatenation layer which\ncombines appearance information and motion information. Compared with the frame\nsampling strategy, our method avoids the risk of missing any important frames.\nCompared with the 3D convolutional method which requires a huge video dataset\nfor network training, our model can be learned on a small target dataset\nbecause we can leverage the off-the-shelf image-level CNN for model parameter\ninitialization. Experiments on two challenging datasets, Hollywood2 and HMDB51,\ndemonstrate that our method achieves superior performance over state-of-the-art\nmethods while requiring much fewer training data. \n\n"}
{"id": "1503.01228", "contents": "Title: Bethe Learning of Conditional Random Fields via MAP Decoding Abstract: Many machine learning tasks can be formulated in terms of predicting\nstructured outputs. In frameworks such as the structured support vector machine\n(SVM-Struct) and the structured perceptron, discriminative functions are\nlearned by iteratively applying efficient maximum a posteriori (MAP) decoding.\nHowever, maximum likelihood estimation (MLE) of probabilistic models over these\nsame structured spaces requires computing partition functions, which is\ngenerally intractable. This paper presents a method for learning discrete\nexponential family models using the Bethe approximation to the MLE. Remarkably,\nthis problem also reduces to iterative (MAP) decoding. This connection emerges\nby combining the Bethe approximation with a Frank-Wolfe (FW) algorithm on a\nconvex dual objective which circumvents the intractable partition function. The\nresult is a new single loop algorithm MLE-Struct, which is substantially more\nefficient than previous double-loop methods for approximate maximum likelihood\nestimation. Our algorithm outperforms existing methods in experiments involving\nimage segmentation, matching problems from vision, and a new dataset of\nuniversity roommate assignments. \n\n"}
{"id": "1503.01531", "contents": "Title: Spectral Clustering by Ellipsoid and Its Connection to Separable\n  Nonnegative Matrix Factorization Abstract: This paper proposes a variant of the normalized cut algorithm for spectral\nclustering. Although the normalized cut algorithm applies the K-means algorithm\nto the eigenvectors of a normalized graph Laplacian for finding clusters, our\nalgorithm instead uses a minimum volume enclosing ellipsoid for them. We show\nthat the algorithm shares similarity with the ellipsoidal rounding algorithm\nfor separable nonnegative matrix factorization. Our theoretical insight implies\nthat the algorithm can serve as a bridge between spectral clustering and\nseparable NMF. The K-means algorithm has the issues in that the choice of\ninitial points affects the construction of clusters and certain choices result\nin poor clustering performance. The normalized cut algorithm inherits these\nissues since K-means is incorporated in it, whereas the algorithm proposed here\ndoes not. An empirical study is presented to examine the performance of the\nalgorithm. \n\n"}
{"id": "1503.02302", "contents": "Title: DESAT: an SSW tool for SDO/AIA image de-saturation Abstract: Saturation affects a significant rate of images recorded by the Atmospheric\nImaging Assembly on the Solar Dynamics Observatory. This paper describes a\ncomputational method and a technological pipeline for the de-saturation of such\nimages, based on several mathematical ingredients like Expectation\nMaximization, image correlation and interpolation. An analysis of the\ncomputational properties and demands of the pipeline, together with an\nassessment of its reliability are performed against a set of data recorded from\nthe Feburary 25 2014 flaring event. \n\n"}
{"id": "1503.02427", "contents": "Title: Syntax-based Deep Matching of Short Texts Abstract: Many tasks in natural language processing, ranging from machine translation\nto question answering, can be reduced to the problem of matching two sentences\nor more generally two short texts. We propose a new approach to the problem,\ncalled Deep Match Tree (DeepMatch$_{tree}$), under a general setting. The\napproach consists of two components, 1) a mining algorithm to discover patterns\nfor matching two short-texts, defined in the product space of dependency trees,\nand 2) a deep neural network for matching short texts using the mined patterns,\nas well as a learning algorithm to build the network having a sparse structure.\nWe test our algorithm on the problem of matching a tweet and a response in\nsocial media, a hard matching problem proposed in [Wang et al., 2013], and show\nthat DeepMatch$_{tree}$ can outperform a number of competitor models including\none without using dependency trees and one based on word-embedding, all with\nlarge margins \n\n"}
{"id": "1503.07077", "contents": "Title: Rotation-invariant convolutional neural networks for galaxy morphology\n  prediction Abstract: Measuring the morphological parameters of galaxies is a key requirement for\nstudying their formation and evolution. Surveys such as the Sloan Digital Sky\nSurvey (SDSS) have resulted in the availability of very large collections of\nimages, which have permitted population-wide analyses of galaxy morphology.\nMorphological analysis has traditionally been carried out mostly via visual\ninspection by trained experts, which is time-consuming and does not scale to\nlarge ($\\gtrsim10^4$) numbers of images.\n  Although attempts have been made to build automated classification systems,\nthese have not been able to achieve the desired level of accuracy. The Galaxy\nZoo project successfully applied a crowdsourcing strategy, inviting online\nusers to classify images by answering a series of questions. Unfortunately,\neven this approach does not scale well enough to keep up with the increasing\navailability of galaxy images.\n  We present a deep neural network model for galaxy morphology classification\nwhich exploits translational and rotational symmetry. It was developed in the\ncontext of the Galaxy Challenge, an international competition to build the best\nmodel for morphology classification based on annotated images from the Galaxy\nZoo project.\n  For images with high agreement among the Galaxy Zoo participants, our model\nis able to reproduce their consensus with near-perfect accuracy ($> 99\\%$) for\nmost questions. Confident model predictions are highly accurate, which makes\nthe model suitable for filtering large collections of images and forwarding\nchallenging images to experts for manual annotation. This approach greatly\nreduces the experts' workload without affecting accuracy. The application of\nthese algorithms to larger sets of training data will be critical for analysing\nresults from future surveys such as the LSST. \n\n"}
{"id": "1503.08663", "contents": "Title: Visual Saliency Based on Multiscale Deep Features Abstract: Visual saliency is a fundamental problem in both cognitive and computational\nsciences, including computer vision. In this CVPR 2015 paper, we discover that\na high-quality visual saliency model can be trained with multiscale features\nextracted using a popular deep learning architecture, convolutional neural\nnetworks (CNNs), which have had many successes in visual recognition tasks. For\nlearning such saliency models, we introduce a neural network architecture,\nwhich has fully connected layers on top of CNNs responsible for extracting\nfeatures at three different scales. We then propose a refinement method to\nenhance the spatial coherence of our saliency results. Finally, aggregating\nmultiple saliency maps computed for different levels of image segmentation can\nfurther boost the performance, yielding saliency maps better than those\ngenerated from a single segmentation. To promote further research and\nevaluation of visual saliency models, we also construct a new large database of\n4447 challenging images and their pixelwise saliency annotation. Experimental\nresults demonstrate that our proposed method is capable of achieving\nstate-of-the-art performance on all public benchmarks, improving the F-Measure\nby 5.0% and 13.2% respectively on the MSRA-B dataset and our new dataset\n(HKU-IS), and lowering the mean absolute error by 5.7% and 35.1% respectively\non these two datasets. \n\n"}
{"id": "1504.05133", "contents": "Title: Exploiting Local Features from Deep Networks for Image Retrieval Abstract: Deep convolutional neural networks have been successfully applied to image\nclassification tasks. When these same networks have been applied to image\nretrieval, the assumption has been made that the last layers would give the\nbest performance, as they do in classification. We show that for instance-level\nimage retrieval, lower layers often perform better than the last layers in\nconvolutional neural networks. We present an approach for extracting\nconvolutional features from different layers of the networks, and adopt VLAD\nencoding to encode features into a single vector for each image. We investigate\nthe effect of different layers and scales of input images on the performance of\nconvolutional features using the recent deep networks OxfordNet and GoogLeNet.\nExperiments demonstrate that intermediate layers or higher layers with finer\nscales produce better results for image retrieval, compared to the last layer.\nWhen using compressed 128-D VLAD descriptors, our method obtains\nstate-of-the-art results and outperforms other VLAD and CNN based approaches on\ntwo out of three test datasets. Our work provides guidance for transferring\ndeep networks trained on image classification to image retrieval tasks. \n\n"}
{"id": "1504.05632", "contents": "Title: Self-Tuned Deep Super Resolution Abstract: Deep learning has been successfully applied to image super resolution (SR).\nIn this paper, we propose a deep joint super resolution (DJSR) model to exploit\nboth external and self similarities for SR. A Stacked Denoising Convolutional\nAuto Encoder (SDCAE) is first pre-trained on external examples with proper data\naugmentations. It is then fine-tuned with multi-scale self examples from each\ninput, where the reliability of self examples is explicitly taken into account.\nWe also enhance the model performance by sub-model training and selection. The\nDJSR model is extensively evaluated and compared with state-of-the-arts, and\nshow noticeable performance improvements both quantitatively and perceptually\non a wide range of images. \n\n"}
{"id": "1504.06787", "contents": "Title: Max-margin Deep Generative Models Abstract: Deep generative models (DGMs) are effective on learning multilayered\nrepresentations of complex data and performing inference of input data by\nexploring the generative ability. However, little work has been done on\nexamining or empowering the discriminative ability of DGMs on making accurate\npredictions. This paper presents max-margin deep generative models (mmDGMs),\nwhich explore the strongly discriminative principle of max-margin learning to\nimprove the discriminative power of DGMs, while retaining the generative\ncapability. We develop an efficient doubly stochastic subgradient algorithm for\nthe piecewise linear objective. Empirical results on MNIST and SVHN datasets\ndemonstrate that (1) max-margin learning can significantly improve the\nprediction performance of DGMs and meanwhile retain the generative ability; and\n(2) mmDGMs are competitive to the state-of-the-art fully discriminative\nnetworks by employing deep convolutional neural networks (CNNs) as both\nrecognition and generative models. \n\n"}
{"id": "1504.07590", "contents": "Title: A Robust Lane Detection and Departure Warning System Abstract: In this work, we have developed a robust lane detection and departure warning\ntechnique. Our system is based on single camera sensor. For lane detection a\nmodified Inverse Perspective Mapping using only a few extrinsic camera\nparameters and illuminant Invariant techniques is used. Lane markings are\nrepresented using a combination of 2nd and 4th order steerable filters, robust\nto shadowing. Effect of shadowing and extra sun light are removed using Lab\ncolor space, and illuminant invariant representation. Lanes are assumed to be\ncubic curves and fitted using robust RANSAC. This method can reliably detect\nlanes of the road and its boundary. This method has been experimented in Indian\nroad conditions under different challenging situations and the result obtained\nwere very good. For lane departure angle an optical flow based method were\nused. \n\n"}
{"id": "1504.07889", "contents": "Title: Bilinear CNNs for Fine-grained Visual Recognition Abstract: We present a simple and effective architecture for fine-grained visual\nrecognition called Bilinear Convolutional Neural Networks (B-CNNs). These\nnetworks represent an image as a pooled outer product of features derived from\ntwo CNNs and capture localized feature interactions in a translationally\ninvariant manner. B-CNNs belong to the class of orderless texture\nrepresentations but unlike prior work they can be trained in an end-to-end\nmanner. Our most accurate model obtains 84.1%, 79.4%, 86.9% and 91.3% per-image\naccuracy on the Caltech-UCSD birds [67], NABirds [64], FGVC aircraft [42], and\nStanford cars [33] dataset respectively and runs at 30 frames-per-second on a\nNVIDIA Titan X GPU. We then present a systematic analysis of these networks and\nshow that (1) the bilinear features are highly redundant and can be reduced by\nan order of magnitude in size without significant loss in accuracy, (2) are\nalso effective for other image classification tasks such as texture and scene\nrecognition, and (3) can be trained from scratch on the ImageNet dataset\noffering consistent improvements over the baseline architecture. Finally, we\npresent visualizations of these models on various datasets using top\nactivations of neural units and gradient-based inversion techniques. The source\ncode for the complete system is available at http://vis-www.cs.umass.edu/bcnn. \n\n"}
{"id": "1504.08083", "contents": "Title: Fast R-CNN Abstract: This paper proposes a Fast Region-based Convolutional Network method (Fast\nR-CNN) for object detection. Fast R-CNN builds on previous work to efficiently\nclassify object proposals using deep convolutional networks. Compared to\nprevious work, Fast R-CNN employs several innovations to improve training and\ntesting speed while also increasing detection accuracy. Fast R-CNN trains the\nvery deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and\nachieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains\nVGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is\nimplemented in Python and C++ (using Caffe) and is available under the\nopen-source MIT License at https://github.com/rbgirshick/fast-rcnn. \n\n"}
{"id": "1505.00670", "contents": "Title: Interleaved Text/Image Deep Mining on a Large-Scale Radiology Database\n  for Automated Image Interpretation Abstract: Despite tremendous progress in computer vision, there has not been an attempt\nfor machine learning on very large-scale medical image databases. We present an\ninterleaved text/image deep learning system to extract and mine the semantic\ninteractions of radiology images and reports from a national research\nhospital's Picture Archiving and Communication System. With natural language\nprocessing, we mine a collection of representative ~216K two-dimensional key\nimages selected by clinicians for diagnostic reference, and match the images\nwith their descriptions in an automated manner. Our system interleaves between\nunsupervised learning and supervised learning on document- and sentence-level\ntext collections, to generate semantic labels and to predict them given an\nimage. Given an image of a patient scan, semantic topics in radiology levels\nare predicted, and associated key-words are generated. Also, a number of\nfrequent disease types are detected as present or absent, to provide more\nspecific interpretation of a patient scan. This shows the potential of\nlarge-scale learning and prediction in electronic patient records available in\nmost modern clinical institutions. \n\n"}
{"id": "1505.00880", "contents": "Title: Multi-view Convolutional Neural Networks for 3D Shape Recognition Abstract: A longstanding question in computer vision concerns the representation of 3D\nshapes for recognition: should 3D shapes be represented with descriptors\noperating on their native 3D formats, such as voxel grid or polygon mesh, or\ncan they be effectively represented with view-based descriptors? We address\nthis question in the context of learning to recognize 3D shapes from a\ncollection of their rendered views on 2D images. We first present a standard\nCNN architecture trained to recognize the shapes' rendered views independently\nof each other, and show that a 3D shape can be recognized even from a single\nview at an accuracy far higher than using state-of-the-art 3D shape\ndescriptors. Recognition rates further increase when multiple views of the\nshapes are provided. In addition, we present a novel CNN architecture that\ncombines information from multiple views of a 3D shape into a single and\ncompact shape descriptor offering even better recognition performance. The same\narchitecture can be applied to accurately recognize human hand-drawn sketches\nof shapes. We conclude that a collection of 2D views can be highly informative\nfor 3D shape recognition and is amenable to emerging CNN architectures and\ntheir derivatives. \n\n"}
{"id": "1505.02074", "contents": "Title: Exploring Models and Data for Image Question Answering Abstract: This work aims to address the problem of image-based question-answering (QA)\nwith new models and datasets. In our work, we propose to use neural networks\nand visual semantic embeddings, without intermediate stages such as object\ndetection and image segmentation, to predict answers to simple questions about\nimages. Our model performs 1.8 times better than the only published results on\nan existing image QA dataset. We also present a question generation algorithm\nthat converts image descriptions, which are widely available, into QA form. We\nused this algorithm to produce an order-of-magnitude larger dataset, with more\nevenly distributed answers. A suite of baseline results on this new dataset are\nalso presented. \n\n"}
{"id": "1505.04585", "contents": "Title: Global Variational Method for Fingerprint Segmentation by Three-part\n  Decomposition Abstract: Verifying an identity claim by fingerprint recognition is a commonplace\nexperience for millions of people in their daily life, e.g. for unlocking a\ntablet computer or smartphone. The first processing step after fingerprint\nimage acquisition is segmentation, i.e. dividing a fingerprint image into a\nforeground region which contains the relevant features for the comparison\nalgorithm, and a background region. We propose a novel segmentation method by\nglobal three-part decomposition (G3PD). Based on global variational analysis,\nthe G3PD method decomposes a fingerprint image into cartoon, texture and noise\nparts. After decomposition, the foreground region is obtained from the non-zero\ncoefficients in the texture image using morphological processing. The\nsegmentation performance of the G3PD method is compared to five\nstate-of-the-art methods on a benchmark which comprises manually marked ground\ntruth segmentation for 10560 images. Performance evaluations show that the G3PD\nmethod consistently outperforms existing methods in terms of segmentation\naccuracy. \n\n"}
{"id": "1505.04597", "contents": "Title: U-Net: Convolutional Networks for Biomedical Image Segmentation Abstract: There is large consent that successful training of deep networks requires\nmany thousand annotated training samples. In this paper, we present a network\nand training strategy that relies on the strong use of data augmentation to use\nthe available annotated samples more efficiently. The architecture consists of\na contracting path to capture context and a symmetric expanding path that\nenables precise localization. We show that such a network can be trained\nend-to-end from very few images and outperforms the prior best method (a\nsliding-window convolutional network) on the ISBI challenge for segmentation of\nneuronal structures in electron microscopic stacks. Using the same network\ntrained on transmitted light microscopy images (phase contrast and DIC) we won\nthe ISBI cell tracking challenge 2015 in these categories by a large margin.\nMoreover, the network is fast. Segmentation of a 512x512 image takes less than\na second on a recent GPU. The full implementation (based on Caffe) and the\ntrained networks are available at\nhttp://lmb.informatik.uni-freiburg.de/people/ronneber/u-net . \n\n"}
{"id": "1505.05489", "contents": "Title: A Sparse Gaussian Process Framework for Photometric Redshift Estimation Abstract: Accurate photometric redshifts are a lynchpin for many future experiments to\npin down the cosmological model and for studies of galaxy evolution. In this\nstudy, a novel sparse regression framework for photometric redshift estimation\nis presented. Simulated and real data from SDSS DR12 were used to train and\ntest the proposed models. We show that approaches which include careful data\npreparation and model design offer a significant improvement in comparison with\nseveral competing machine learning algorithms. Standard implementations of most\nregression algorithms have as the objective the minimization of the sum of\nsquared errors. For redshift inference, however, this induces a bias in the\nposterior mean of the output distribution, which can be problematic. In this\npaper we directly target minimizing $\\Delta z = (z_\\textrm{s} -\nz_\\textrm{p})/(1+z_\\textrm{s})$ and address the bias problem via a\ndistribution-based weighting scheme, incorporated as part of the optimization\nobjective. The results are compared with other machine learning algorithms in\nthe field such as Artificial Neural Networks (ANN), Gaussian Processes (GPs)\nand sparse GPs. The proposed framework reaches a mean absolute $\\Delta z =\n0.0026(1+z_\\textrm{s})$, over the redshift range of $0 \\le z_\\textrm{s} \\le 2$\non the simulated data, and $\\Delta z = 0.0178(1+z_\\textrm{s})$ over the entire\nredshift range on the SDSS DR12 survey, outperforming the standard ANNz used in\nthe literature. We also investigate how the relative size of the training set\naffects the photometric redshift accuracy. We find that a training set of\n\\textgreater 30 per cent of total sample size, provides little additional\nconstraint on the photometric redshifts, and note that our GP formalism\nstrongly outperforms ANNz in the sparse data regime for the simulated data set. \n\n"}
{"id": "1505.05643", "contents": "Title: Object Modelling with a Handheld RGB-D Camera Abstract: This work presents a flexible system to reconstruct 3D models of objects\ncaptured with an RGB-D sensor. A major advantage of the method is that our\nreconstruction pipeline allows the user to acquire a full 3D model of the\nobject. This is achieved by acquiring several partial 3D models in different\nsessions that are automatically merged together to reconstruct a full model. In\naddition, the 3D models acquired by our system can be directly used by\nstate-of-the-art object instance recognition and object tracking modules,\nproviding object-perception capabilities for different applications, such as\nhuman-object interaction analysis or robot grasping. The system does not impose\nconstraints in the appearance of objects (textured, untextured) nor in the\nmodelling setup (moving camera with static object or a turn-table setup). The\nproposed reconstruction system has been used to model a large number of objects\nresulting in metrically accurate and visually appealing 3D models. \n\n"}
{"id": "1505.06250", "contents": "Title: Efficient Large Scale Video Classification Abstract: Video classification has advanced tremendously over the recent years. A large\npart of the improvements in video classification had to do with the work done\nby the image classification community and the use of deep convolutional\nnetworks (CNNs) which produce competitive results with hand- crafted motion\nfeatures. These networks were adapted to use video frames in various ways and\nhave yielded state of the art classification results. We present two methods\nthat build on this work, and scale it up to work with millions of videos and\nhundreds of thousands of classes while maintaining a low computational cost. In\nthe context of large scale video processing, training CNNs on video frames is\nextremely time consuming, due to the large number of frames involved. We\npropose to avoid this problem by training CNNs on either YouTube thumbnails or\nFlickr images, and then using these networks' outputs as features for other\nhigher level classifiers. We discuss the challenges of achieving this and\npropose two models for frame-level and video-level classification. The first is\na highly efficient mixture of experts while the latter is based on long short\nterm memory neural networks. We present results on the Sports-1M video dataset\n(1 million videos, 487 classes) and on a new dataset which has 12 million\nvideos and 150,000 labels. \n\n"}
{"id": "1505.06605", "contents": "Title: Expresso : A user-friendly GUI for Designing, Training and Exploring\n  Convolutional Neural Networks Abstract: With a view to provide a user-friendly interface for designing, training and\ndeveloping deep learning frameworks, we have developed Expresso, a GUI tool\nwritten in Python. Expresso is built atop Caffe, the open-source, prize-winning\nframework popularly used to develop Convolutional Neural Networks. Expresso\nprovides a convenient wizard-like graphical interface which guides the user\nthrough various common scenarios -- data import, construction and training of\ndeep networks, performing various experiments, analyzing and visualizing the\nresults of these experiments. The multi-threaded nature of Expresso enables\nconcurrent execution and notification of events related to the aforementioned\nscenarios. The GUI sub-components and inter-component interfaces in Expresso\nhave been designed with extensibility in mind. We believe Expresso's\nflexibility and ease of use will come in handy to researchers, newcomers and\nseasoned alike, in their explorations related to deep learning. \n\n"}
{"id": "1506.02142", "contents": "Title: Dropout as a Bayesian Approximation: Representing Model Uncertainty in\n  Deep Learning Abstract: Deep learning tools have gained tremendous attention in applied machine\nlearning. However such tools for regression and classification do not capture\nmodel uncertainty. In comparison, Bayesian models offer a mathematically\ngrounded framework to reason about model uncertainty, but usually come with a\nprohibitive computational cost. In this paper we develop a new theoretical\nframework casting dropout training in deep neural networks (NNs) as approximate\nBayesian inference in deep Gaussian processes. A direct result of this theory\ngives us tools to model uncertainty with dropout NNs -- extracting information\nfrom existing models that has been thrown away so far. This mitigates the\nproblem of representing uncertainty in deep learning without sacrificing either\ncomputational complexity or test accuracy. We perform an extensive study of the\nproperties of dropout's uncertainty. Various network architectures and\nnon-linearities are assessed on tasks of regression and classification, using\nMNIST as an example. We show a considerable improvement in predictive\nlog-likelihood and RMSE compared to existing state-of-the-art methods, and\nfinish by using dropout's uncertainty in deep reinforcement learning. \n\n"}
{"id": "1506.04191", "contents": "Title: Deep Structured Models For Group Activity Recognition Abstract: This paper presents a deep neural-network-based hierarchical graphical model\nfor individual and group activity recognition in surveillance scenes. Deep\nnetworks are used to recognize the actions of individual people in a scene.\nNext, a neural-network-based hierarchical graphical model refines the predicted\nlabels for each class by considering dependencies between the classes. This\nrefinement step mimics a message-passing step similar to inference in a\nprobabilistic graphical model. We show that this approach can be effective in\ngroup activity recognition, with the deep graphical model improving recognition\nrates over baseline methods. \n\n"}
{"id": "1506.05032", "contents": "Title: Histopathological Image Classification using Discriminative\n  Feature-oriented Dictionary Learning Abstract: In histopathological image analysis, feature extraction for classification is\na challenging task due to the diversity of histology features suitable for each\nproblem as well as presence of rich geometrical structures. In this paper, we\npropose an automatic feature discovery framework via learning class-specific\ndictionaries and present a low-complexity method for classification and disease\ngrading in histopathology. Essentially, our Discriminative Feature-oriented\nDictionary Learning (DFDL) method learns class-specific dictionaries such that\nunder a sparsity constraint, the learned dictionaries allow representing a new\nimage sample parsimoniously via the dictionary corresponding to the class\nidentity of the sample. At the same time, the dictionary is designed to be\npoorly capable of representing samples from other classes. Experiments on three\nchallenging real-world image databases: 1) histopathological images of\nintraductal breast lesions, 2) mammalian kidney, lung and spleen images\nprovided by the Animal Diagnostics Lab (ADL) at Pennsylvania State University,\nand 3) brain tumor images from The Cancer Genome Atlas (TCGA) database, reveal\nthe merits of our proposal over state-of-the-art alternatives. {Moreover, we\ndemonstrate that DFDL exhibits a more graceful decay in classification accuracy\nagainst the number of training images which is highly desirable in practice\nwhere generous training is often not available \n\n"}
{"id": "1506.05439", "contents": "Title: Learning with a Wasserstein Loss Abstract: Learning to predict multi-label outputs is challenging, but in many problems\nthere is a natural metric on the outputs that can be used to improve\npredictions. In this paper we develop a loss function for multi-label learning,\nbased on the Wasserstein distance. The Wasserstein distance provides a natural\nnotion of dissimilarity for probability measures. Although optimizing with\nrespect to the exact Wasserstein distance is costly, recent work has described\na regularized approximation that is efficiently computed. We describe an\nefficient learning algorithm based on this regularization, as well as a novel\nextension of the Wasserstein distance from probability measures to unnormalized\nmeasures. We also describe a statistical learning bound for the loss. The\nWasserstein loss can encourage smoothness of the predictions with respect to a\nchosen metric on the output space. We demonstrate this property on a real-data\ntag prediction problem, using the Yahoo Flickr Creative Commons dataset,\noutperforming a baseline that doesn't use the metric. \n\n"}
{"id": "1506.05603", "contents": "Title: Point-wise Map Recovery and Refinement from Functional Correspondence Abstract: Since their introduction in the shape analysis community, functional maps\nhave met with considerable success due to their ability to compactly represent\ndense correspondences between deformable shapes, with applications ranging from\nshape matching and image segmentation, to exploration of large shape\ncollections. Despite the numerous advantages of such representation, however,\nthe problem of converting a given functional map back to a point-to-point map\nhas received a surprisingly limited interest. In this paper we analyze the\ngeneral problem of point-wise map recovery from arbitrary functional maps. In\ndoing so, we rule out many of the assumptions required by the currently\nestablished approach -- most notably, the limiting requirement of the input\nshapes being nearly-isometric. We devise an efficient recovery process based on\na simple probabilistic model. Experiments confirm that this approach achieves\nremarkable accuracy improvements in very challenging cases. \n\n"}
{"id": "1506.07062", "contents": "Title: Improving Fiber Alignment in HARDI by Combining Contextual PDE Flow with\n  Constrained Spherical Deconvolution Abstract: We propose two strategies to improve the quality of tractography results\ncomputed from diffusion weighted magnetic resonance imaging (DW-MRI) data. Both\nmethods are based on the same PDE framework, defined in the coupled space of\npositions and orientations, associated with a stochastic process describing the\nenhancement of elongated structures while preserving crossing structures. In\nthe first method we use the enhancement PDE for contextual regularization of a\nfiber orientation distribution (FOD) that is obtained on individual voxels from\nhigh angular resolution diffusion imaging (HARDI) data via constrained\nspherical deconvolution (CSD). Thereby we improve the FOD as input for\nsubsequent tractography. Secondly, we introduce the fiber to bundle coherence\n(FBC), a measure for quantification of fiber alignment. The FBC is computed\nfrom a tractography result using the same PDE framework and provides a\ncriterion for removing the spurious fibers. We validate the proposed\ncombination of CSD and enhancement on phantom data and on human data, acquired\nwith different scanning protocols. On the phantom data we find that PDE\nenhancements improve both local metrics and global metrics of tractography\nresults, compared to CSD without enhancements. On the human data we show that\nthe enhancements allow for a better reconstruction of crossing fiber bundles\nand they reduce the variability of the tractography output with respect to the\nacquisition parameters. Finally, we show that both the enhancement of the FODs\nand the use of the FBC measure on the tractography improve the stability with\nrespect to different stochastic realizations of probabilistic tractography.\nThis is shown in a clinical application: the reconstruction of the optic\nradiation for epilepsy surgery planning. \n\n"}
{"id": "1506.08909", "contents": "Title: The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured\n  Multi-Turn Dialogue Systems Abstract: This paper introduces the Ubuntu Dialogue Corpus, a dataset containing almost\n1 million multi-turn dialogues, with a total of over 7 million utterances and\n100 million words. This provides a unique resource for research into building\ndialogue managers based on neural language models that can make use of large\namounts of unlabeled data. The dataset has both the multi-turn property of\nconversations in the Dialog State Tracking Challenge datasets, and the\nunstructured nature of interactions from microblog services such as Twitter. We\nalso describe two neural learning architectures suitable for analyzing this\ndataset, and provide benchmark performance on the task of selecting the best\nnext response. \n\n"}
{"id": "1507.02159", "contents": "Title: Towards Good Practices for Very Deep Two-Stream ConvNets Abstract: Deep convolutional networks have achieved great success for object\nrecognition in still images. However, for action recognition in videos, the\nimprovement of deep convolutional networks is not so evident. We argue that\nthere are two reasons that could probably explain this result. First the\ncurrent network architectures (e.g. Two-stream ConvNets) are relatively shallow\ncompared with those very deep models in image domain (e.g. VGGNet, GoogLeNet),\nand therefore their modeling capacity is constrained by their depth. Second,\nprobably more importantly, the training dataset of action recognition is\nextremely small compared with the ImageNet dataset, and thus it will be easy to\nover-fit on the training dataset.\n  To address these issues, this report presents very deep two-stream ConvNets\nfor action recognition, by adapting recent very deep architectures into video\ndomain. However, this extension is not easy as the size of action recognition\nis quite small. We design several good practices for the training of very deep\ntwo-stream ConvNets, namely (i) pre-training for both spatial and temporal\nnets, (ii) smaller learning rates, (iii) more data augmentation techniques,\n(iv) high drop out ratio. Meanwhile, we extend the Caffe toolbox into Multi-GPU\nimplementation with high computational efficiency and low memory consumption.\nWe verify the performance of very deep two-stream ConvNets on the dataset of\nUCF101 and it achieves the recognition accuracy of $91.4\\%$. \n\n"}
{"id": "1507.02313", "contents": "Title: Feature Representation in Convolutional Neural Networks Abstract: Convolutional Neural Networks (CNNs) are powerful models that achieve\nimpressive results for image classification. In addition, pre-trained CNNs are\nalso useful for other computer vision tasks as generic feature extractors. This\npaper aims to gain insight into the feature aspect of CNN and demonstrate other\nuses of CNN features. Our results show that CNN feature maps can be used with\nRandom Forests and SVM to yield classification results that outperforms the\noriginal CNN. A CNN that is less than optimal (e.g. not fully trained or\noverfitting) can also extract features for Random Forest/SVM that yield\ncompetitive classification accuracy. In contrast to the literature which uses\nthe top-layer activations as feature representation of images for other tasks,\nusing lower-layer features can yield better results for classification. \n\n"}
{"id": "1507.02879", "contents": "Title: Deep Perceptual Mapping for Thermal to Visible Face Recognition Abstract: Cross modal face matching between the thermal and visible spectrum is a much\nde- sired capability for night-time surveillance and security applications. Due\nto a very large modality gap, thermal-to-visible face recognition is one of the\nmost challenging face matching problem. In this paper, we present an approach\nto bridge this modality gap by a significant margin. Our approach captures the\nhighly non-linear relationship be- tween the two modalities by using a deep\nneural network. Our model attempts to learn a non-linear mapping from visible\nto thermal spectrum while preserving the identity in- formation. We show\nsubstantive performance improvement on a difficult thermal-visible face\ndataset. The presented approach improves the state-of-the-art by more than 10%\nin terms of Rank-1 identification and bridge the drop in performance due to the\nmodality gap by more than 40%. \n\n"}
{"id": "1507.04296", "contents": "Title: Massively Parallel Methods for Deep Reinforcement Learning Abstract: We present the first massively distributed architecture for deep\nreinforcement learning. This architecture uses four main components: parallel\nactors that generate new behaviour; parallel learners that are trained from\nstored experience; a distributed neural network to represent the value function\nor behaviour policy; and a distributed store of experience. We used our\narchitecture to implement the Deep Q-Network algorithm (DQN). Our distributed\nalgorithm was applied to 49 games from Atari 2600 games from the Arcade\nLearning Environment, using identical hyperparameters. Our performance\nsurpassed non-distributed DQN in 41 of the 49 games and also reduced the\nwall-time required to achieve these results by an order of magnitude on most\ngames. \n\n"}
{"id": "1508.00282", "contents": "Title: On Hyperspectral Classification in the Compressed Domain Abstract: In this paper, we study the problem of hyperspectral pixel classification\nbased on the recently proposed architectures for compressive whisk-broom\nhyperspectral imagers without the need to reconstruct the complete data cube. A\nclear advantage of classification in the compressed domain is its suitability\nfor real-time on-site processing of the sensed data. Moreover, it is assumed\nthat the training process also takes place in the compressed domain, thus,\nisolating the classification unit from the recovery unit at the receiver's\nside. We show that, perhaps surprisingly, using distinct measurement matrices\nfor different pixels results in more accuracy of the learned classifier and\nconsistent classification performance, supporting the role of information\ndiversity in learning. \n\n"}
{"id": "1508.01244", "contents": "Title: TabletGaze: Unconstrained Appearance-based Gaze Estimation in Mobile\n  Tablets Abstract: We study gaze estimation on tablets, our key design goal is uncalibrated gaze\nestimation using the front-facing camera during natural use of tablets, where\nthe posture and method of holding the tablet is not constrained. We collected\nthe first large unconstrained gaze dataset of tablet users, labeled Rice\nTabletGaze dataset. The dataset consists of 51 subjects, each with 4 different\npostures and 35 gaze locations. Subjects vary in race, gender and in their need\nfor prescription glasses, all of which might impact gaze estimation accuracy.\nDriven by our observations on the collected data, we present a TabletGaze\nalgorithm for automatic gaze estimation using multi-level HoG feature and\nRandom Forests regressor. The TabletGaze algorithm achieves a mean error of\n3.17 cm. We perform extensive evaluation on the impact of various factors such\nas dataset size, race, wearing glasses and user posture on the gaze estimation\naccuracy and make important observations about the impact of these factors. \n\n"}
{"id": "1508.01667", "contents": "Title: Places205-VGGNet Models for Scene Recognition Abstract: VGGNets have turned out to be effective for object recognition in still\nimages. However, it is unable to yield good performance by directly adapting\nthe VGGNet models trained on the ImageNet dataset for scene recognition. This\nreport describes our implementation of training the VGGNets on the large-scale\nPlaces205 dataset. Specifically, we train three VGGNet models, namely\nVGGNet-11, VGGNet-13, and VGGNet-16, by using a Multi-GPU extension of Caffe\ntoolbox with high computational efficiency. We verify the performance of\ntrained Places205-VGGNet models on three datasets: MIT67, SUN397, and\nPlaces205. Our trained models achieve the state-of-the-art performance on these\ndatasets and are made public available. \n\n"}
{"id": "1508.05508", "contents": "Title: Towards Neural Network-based Reasoning Abstract: We propose Neural Reasoner, a framework for neural network-based reasoning\nover natural language sentences. Given a question, Neural Reasoner can infer\nover multiple supporting facts and find an answer to the question in specific\nforms. Neural Reasoner has 1) a specific interaction-pooling mechanism,\nallowing it to examine multiple facts, and 2) a deep architecture, allowing it\nto model the complicated logical relations in reasoning tasks. Assuming no\nparticular structure exists in the question and facts, Neural Reasoner is able\nto accommodate different types of reasoning and different forms of language\nexpressions. Despite the model complexity, Neural Reasoner can still be trained\neffectively in an end-to-end manner. Our empirical studies show that Neural\nReasoner can outperform existing neural reasoning systems with remarkable\nmargins on two difficult artificial tasks (Positional Reasoning and Path\nFinding) proposed in [8]. For example, it improves the accuracy on Path\nFinding(10K) from 33.4% [6] to over 98%. \n\n"}
{"id": "1508.05581", "contents": "Title: Learning Sampling Distributions for Efficient Object Detection Abstract: Object detection is an important task in computer vision and learning\nsystems. Multistage particle windows (MPW), proposed by Gualdi et al., is an\nalgorithm of fast and accurate object detection. By sampling particle windows\nfrom a proposal distribution (PD), MPW avoids exhaustively scanning the image.\nDespite its success, it is unknown how to determine the number of stages and\nthe number of particle windows in each stage. Moreover, it has to generate too\nmany particle windows in the initialization step and it redraws unnecessary too\nmany particle windows around object-like regions. In this paper, we attempt to\nsolve the problems of MPW. An important fact we used is that there is large\nprobability for a randomly generated particle window not to contain the object\nbecause the object is a sparse event relevant to the huge number of candidate\nwindows. Therefore, we design the proposal distribution so as to efficiently\nreject the huge number of non-object windows. Specifically, we propose the\nconcepts of rejection, acceptance, and ambiguity windows and regions. This\ncontrasts to MPW which utilizes only on region of support. The PD of MPW is\nacceptance-oriented whereas the PD of our method (called iPW) is\nrejection-oriented. Experimental results on human and face detection\ndemonstrate the efficiency and effectiveness of the iPW algorithm. The source\ncode is publicly accessible. \n\n"}
{"id": "1508.06576", "contents": "Title: A Neural Algorithm of Artistic Style Abstract: In fine art, especially painting, humans have mastered the skill to create\nunique visual experiences through composing a complex interplay between the\ncontent and style of an image. Thus far the algorithmic basis of this process\nis unknown and there exists no artificial system with similar capabilities.\nHowever, in other key areas of visual perception such as object and face\nrecognition near-human performance was recently demonstrated by a class of\nbiologically inspired vision models called Deep Neural Networks. Here we\nintroduce an artificial system based on a Deep Neural Network that creates\nartistic images of high perceptual quality. The system uses neural\nrepresentations to separate and recombine content and style of arbitrary\nimages, providing a neural algorithm for the creation of artistic images.\nMoreover, in light of the striking similarities between performance-optimised\nartificial neural networks and biological vision, our work offers a path\nforward to an algorithmic understanding of how humans create and perceive\nartistic imagery. \n\n"}
{"id": "1508.06615", "contents": "Title: Character-Aware Neural Language Models Abstract: We describe a simple neural language model that relies only on\ncharacter-level inputs. Predictions are still made at the word-level. Our model\nemploys a convolutional neural network (CNN) and a highway network over\ncharacters, whose output is given to a long short-term memory (LSTM) recurrent\nneural network language model (RNN-LM). On the English Penn Treebank the model\nis on par with the existing state-of-the-art despite having 60% fewer\nparameters. On languages with rich morphology (Arabic, Czech, French, German,\nSpanish, Russian), the model outperforms word-level/morpheme-level LSTM\nbaselines, again with fewer parameters. The results suggest that on many\nlanguages, character inputs are sufficient for language modeling. Analysis of\nword representations obtained from the character composition part of the model\nreveals that the model is able to encode, from characters only, both semantic\nand orthographic information. \n\n"}
{"id": "1508.06904", "contents": "Title: Rapid Exact Signal Scanning with Deep Convolutional Neural Networks Abstract: A rigorous formulation of the dynamics of a signal processing scheme aimed at\ndense signal scanning without any loss in accuracy is introduced and analyzed.\nRelated methods proposed in the recent past lack a satisfactory analysis of\nwhether they actually fulfill any exactness constraints. This is improved\nthrough an exact characterization of the requirements for a sound sliding\nwindow approach. The tools developed in this paper are especially beneficial if\nConvolutional Neural Networks are employed, but can also be used as a more\ngeneral framework to validate related approaches to signal scanning. The\nproposed theory helps to eliminate redundant computations and renders special\ncase treatment unnecessary, resulting in a dramatic boost in efficiency\nparticularly on massively parallel processors. This is demonstrated both\ntheoretically in a computational complexity analysis and empirically on modern\nparallel processors. \n\n"}
{"id": "1509.01514", "contents": "Title: Conjugate Gradient Acceleration of Non-Linear Smoothing Filters Abstract: The most efficient signal edge-preserving smoothing filters, e.g., for\ndenoising, are non-linear. Thus, their acceleration is challenging and is often\nperformed in practice by tuning filter parameters, such as by increasing the\nwidth of the local smoothing neighborhood, resulting in more aggressive\nsmoothing of a single sweep at the cost of increased edge blurring. We propose\nan alternative technology, accelerating the original filters without tuning, by\nrunning them through a special conjugate gradient method, not affecting their\nquality. The filter non-linearity is dealt with by careful freezing and\nrestarting. Our initial numerical experiments on toy one-dimensional signals\ndemonstrate 20x acceleration of the classical bilateral filter and 3-5x\nacceleration of the recently developed guided filter. \n\n"}
{"id": "1509.01978", "contents": "Title: An Approach to the Analysis of the South Slavic Medieval Labels Using\n  Image Texture Abstract: The paper presents a new script classification method for the discrimination\nof the South Slavic medieval labels. It consists in the textural analysis of\nthe script types. In the first step, each letter is coded by the equivalent\nscript type, which is defined by its typographical features. Obtained coded\ntext is subjected to the run-length statistical analysis and to the adjacent\nlocal binary pattern analysis in order to extract the features. The result\nshows a diversity between the extracted features of the scripts, which makes\nthe feature classification more effective. It is the basis for the\nclassification process of the script identification by using an extension of a\nstate-of-the-art approach for document clustering. The proposed method is\nevaluated on an example of hand-engraved in stone and hand-printed in paper\nlabels in old Cyrillic, angular and round Glagolitic. Experiments demonstrate\nvery positive results, which prove the effectiveness of the proposed method. \n\n"}
{"id": "1509.02130", "contents": "Title: Structured Prediction with Output Embeddings for Semantic Image\n  Annotation Abstract: We address the task of annotating images with semantic tuples. Solving this\nproblem requires an algorithm which is able to deal with hundreds of classes\nfor each argument of the tuple. In such contexts, data sparsity becomes a key\nchallenge, as there will be a large number of classes for which only a few\nexamples are available. We propose handling this by incorporating feature\nrepresentations of both the inputs (images) and outputs (argument classes) into\na factorized log-linear model, and exploiting the flexibility of scoring\nfunctions based on bilinear forms. Experiments show that integrating feature\nrepresentations of the outputs in the structured prediction model leads to\nbetter overall predictions. We also conclude that the best output\nrepresentation is specific for each type of argument. \n\n"}
{"id": "1509.08147", "contents": "Title: Amodal Completion and Size Constancy in Natural Scenes Abstract: We consider the problem of enriching current object detection systems with\nveridical object sizes and relative depth estimates from a single image. There\nare several technical challenges to this, such as occlusions, lack of\ncalibration data and the scale ambiguity between object size and distance.\nThese have not been addressed in full generality in previous work. Here we\npropose to tackle these issues by building upon advances in object recognition\nand using recently created large-scale datasets. We first introduce the task of\namodal bounding box completion, which aims to infer the the full extent of the\nobject instances in the image. We then propose a probabilistic framework for\nlearning category-specific object size distributions from available annotations\nand leverage these in conjunction with amodal completion to infer veridical\nsizes in novel images. Finally, we introduce a focal length prediction approach\nthat exploits scene recognition to overcome inherent scaling ambiguities and we\ndemonstrate qualitative results on challenging real-world scenes. \n\n"}
{"id": "1509.08972", "contents": "Title: VLSI Implementation of Deep Neural Network Using Integral Stochastic\n  Computing Abstract: The hardware implementation of deep neural networks (DNNs) has recently\nreceived tremendous attention: many applications in fact require high-speed\noperations that suit a hardware implementation. However, numerous elements and\ncomplex interconnections are usually required, leading to a large area\noccupation and copious power consumption. Stochastic computing has shown\npromising results for low-power area-efficient hardware implementations, even\nthough existing stochastic algorithms require long streams that cause long\nlatencies. In this paper, we propose an integer form of stochastic computation\nand introduce some elementary circuits. We then propose an efficient\nimplementation of a DNN based on integral stochastic computing. The proposed\narchitecture has been implemented on a Virtex7 FPGA, resulting in 45% and 62%\naverage reductions in area and latency compared to the best reported\narchitecture in literature. We also synthesize the circuits in a 65 nm CMOS\ntechnology and we show that the proposed integral stochastic architecture\nresults in up to 21% reduction in energy consumption compared to the binary\nradix implementation at the same misclassification rate. Due to fault-tolerant\nnature of stochastic architectures, we also consider a quasi-synchronous\nimplementation which yields 33% reduction in energy consumption w.r.t. the\nbinary radix implementation without any compromise on performance. \n\n"}
{"id": "1510.00857", "contents": "Title: Approximate Fisher Kernels of non-iid Image Models for Image\n  Categorization Abstract: The bag-of-words (BoW) model treats images as sets of local descriptors and\nrepresents them by visual word histograms. The Fisher vector (FV)\nrepresentation extends BoW, by considering the first and second order\nstatistics of local descriptors. In both representations local descriptors are\nassumed to be identically and independently distributed (iid), which is a poor\nassumption from a modeling perspective. It has been experimentally observed\nthat the performance of BoW and FV representations can be improved by employing\ndiscounting transformations such as power normalization. In this paper, we\nintroduce non-iid models by treating the model parameters as latent variables\nwhich are integrated out, rendering all local regions dependent. Using the\nFisher kernel principle we encode an image by the gradient of the data\nlog-likelihood w.r.t. the model hyper-parameters. Our models naturally generate\ndiscounting effects in the representations; suggesting that such\ntransformations have proven successful because they closely correspond to the\nrepresentations obtained for non-iid models. To enable tractable computation,\nwe rely on variational free-energy bounds to learn the hyper-parameters and to\ncompute approximate Fisher kernels. Our experimental evaluation results\nvalidate that our models lead to performance improvements comparable to using\npower normalization, as employed in state-of-the-art feature aggregation\nmethods. \n\n"}
{"id": "1510.01257", "contents": "Title: Efficient Object Detection for High Resolution Images Abstract: Efficient generation of high-quality object proposals is an essential step in\nstate-of-the-art object detection systems based on deep convolutional neural\nnetworks (DCNN) features. Current object proposal algorithms are\ncomputationally inefficient in processing high resolution images containing\nsmall objects, which makes them the bottleneck in object detection systems. In\nthis paper we present effective methods to detect objects for high resolution\nimages. We combine two complementary strategies. The first approach is to\npredict bounding boxes based on adjacent visual features. The second approach\nuses high level image features to guide a two-step search process that\nadaptively focuses on regions that are likely to contain small objects. We\nextract features required for the two strategies by utilizing a pre-trained\nDCNN model known as AlexNet. We demonstrate the effectiveness of our algorithm\nby showing its performance on a high-resolution image subset of the SUN 2012\nobject detection dataset. \n\n"}
{"id": "1511.00561", "contents": "Title: SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image\n  Segmentation Abstract: We present a novel and practical deep fully convolutional neural network\narchitecture for semantic pixel-wise segmentation termed SegNet. This core\ntrainable segmentation engine consists of an encoder network, a corresponding\ndecoder network followed by a pixel-wise classification layer. The architecture\nof the encoder network is topologically identical to the 13 convolutional\nlayers in the VGG16 network. The role of the decoder network is to map the low\nresolution encoder feature maps to full input resolution feature maps for\npixel-wise classification. The novelty of SegNet lies is in the manner in which\nthe decoder upsamples its lower resolution input feature map(s). Specifically,\nthe decoder uses pooling indices computed in the max-pooling step of the\ncorresponding encoder to perform non-linear upsampling. This eliminates the\nneed for learning to upsample. The upsampled maps are sparse and are then\nconvolved with trainable filters to produce dense feature maps. We compare our\nproposed architecture with the widely adopted FCN and also with the well known\nDeepLab-LargeFOV, DeconvNet architectures. This comparison reveals the memory\nversus accuracy trade-off involved in achieving good segmentation performance.\n  SegNet was primarily motivated by scene understanding applications. Hence, it\nis designed to be efficient both in terms of memory and computational time\nduring inference. It is also significantly smaller in the number of trainable\nparameters than other competing architectures. We also performed a controlled\nbenchmark of SegNet and other architectures on both road scenes and SUN RGB-D\nindoor scene segmentation tasks. We show that SegNet provides good performance\nwith competitive inference time and more efficient inference memory-wise as\ncompared to other architectures. We also provide a Caffe implementation of\nSegNet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet/. \n\n"}
{"id": "1511.02274", "contents": "Title: Stacked Attention Networks for Image Question Answering Abstract: This paper presents stacked attention networks (SANs) that learn to answer\nnatural language questions from images. SANs use semantic representation of a\nquestion as query to search for the regions in an image that are related to the\nanswer. We argue that image question answering (QA) often requires multiple\nsteps of reasoning. Thus, we develop a multiple-layer SAN in which we query an\nimage multiple times to infer the answer progressively. Experiments conducted\non four image QA data sets demonstrate that the proposed SANs significantly\noutperform previous state-of-the-art approaches. The visualization of the\nattention layers illustrates the progress that the SAN locates the relevant\nvisual clues that lead to the answer of the question layer-by-layer. \n\n"}
{"id": "1511.02462", "contents": "Title: LOGO-Net: Large-scale Deep Logo Detection and Brand Recognition with\n  Deep Region-based Convolutional Networks Abstract: Logo detection from images has many applications, particularly for brand\nrecognition and intellectual property protection. Most existing studies for\nlogo recognition and detection are based on small-scale datasets which are not\ncomprehensive enough when exploring emerging deep learning techniques. In this\npaper, we introduce \"LOGO-Net\", a large-scale logo image database for logo\ndetection and brand recognition from real-world product images. To facilitate\nresearch, LOGO-Net has two datasets: (i)\"logos-18\" consists of 18 logo classes,\n10 brands, and 16,043 logo objects, and (ii) \"logos-160\" consists of 160 logo\nclasses, 100 brands, and 130,608 logo objects. We describe the ideas and\nchallenges for constructing such a large-scale database. Another key\ncontribution of this work is to apply emerging deep learning techniques for\nlogo detection and brand recognition tasks, and conduct extensive experiments\nby exploring several state-of-the-art deep region-based convolutional networks\ntechniques for object detection tasks. The LOGO-net will be released at\nhttp://logo-net.org/ \n\n"}
{"id": "1511.02680", "contents": "Title: Bayesian SegNet: Model Uncertainty in Deep Convolutional Encoder-Decoder\n  Architectures for Scene Understanding Abstract: We present a deep learning framework for probabilistic pixel-wise semantic\nsegmentation, which we term Bayesian SegNet. Semantic segmentation is an\nimportant tool for visual scene understanding and a meaningful measure of\nuncertainty is essential for decision making. Our contribution is a practical\nsystem which is able to predict pixel-wise class labels with a measure of model\nuncertainty. We achieve this by Monte Carlo sampling with dropout at test time\nto generate a posterior distribution of pixel class labels. In addition, we\nshow that modelling uncertainty improves segmentation performance by 2-3%\nacross a number of state of the art architectures such as SegNet, FCN and\nDilation Network, with no additional parametrisation. We also observe a\nsignificant improvement in performance for smaller datasets where modelling\nuncertainty is more effective. We benchmark Bayesian SegNet on the indoor SUN\nScene Understanding and outdoor CamVid driving scenes datasets. \n\n"}
{"id": "1511.03328", "contents": "Title: Semantic Image Segmentation with Task-Specific Edge Detection Using CNNs\n  and a Discriminatively Trained Domain Transform Abstract: Deep convolutional neural networks (CNNs) are the backbone of state-of-art\nsemantic image segmentation systems. Recent work has shown that complementing\nCNNs with fully-connected conditional random fields (CRFs) can significantly\nenhance their object localization accuracy, yet dense CRF inference is\ncomputationally expensive. We propose replacing the fully-connected CRF with\ndomain transform (DT), a modern edge-preserving filtering method in which the\namount of smoothing is controlled by a reference edge map. Domain transform\nfiltering is several times faster than dense CRF inference and we show that it\nyields comparable semantic segmentation results, accurately capturing object\nboundaries. Importantly, our formulation allows learning the reference edge map\nfrom intermediate CNN features instead of using the image gradient magnitude as\nin standard DT filtering. This produces task-specific edges in an end-to-end\ntrainable system optimizing the target semantic segmentation quality. \n\n"}
{"id": "1511.03745", "contents": "Title: Grounding of Textual Phrases in Images by Reconstruction Abstract: Grounding (i.e. localizing) arbitrary, free-form textual phrases in visual\ncontent is a challenging problem with many applications for human-computer\ninteraction and image-text reference resolution. Few datasets provide the\nground truth spatial localization of phrases, thus it is desirable to learn\nfrom data with no or little grounding supervision. We propose a novel approach\nwhich learns grounding by reconstructing a given phrase using an attention\nmechanism, which can be either latent or optimized directly. During training\nour approach encodes the phrase using a recurrent network language model and\nthen learns to attend to the relevant image region in order to reconstruct the\ninput phrase. At test time, the correct attention, i.e., the grounding, is\nevaluated. If grounding supervision is available it can be directly applied via\na loss over the attention mechanism. We demonstrate the effectiveness of our\napproach on the Flickr 30k Entities and ReferItGame datasets with different\nlevels of supervision, ranging from no supervision over partial supervision to\nfull supervision. Our supervised variant improves by a large margin over the\nstate-of-the-art on both datasets. \n\n"}
{"id": "1511.03979", "contents": "Title: Representational Distance Learning for Deep Neural Networks Abstract: Deep neural networks (DNNs) provide useful models of visual representational\ntransformations. We present a method that enables a DNN (student) to learn from\nthe internal representational spaces of a reference model (teacher), which\ncould be another DNN or, in the future, a biological brain. Representational\nspaces of the student and the teacher are characterized by representational\ndistance matrices (RDMs). We propose representational distance learning (RDL),\na stochastic gradient descent method that drives the RDMs of the student to\napproximate the RDMs of the teacher. We demonstrate that RDL is competitive\nwith other transfer learning techniques for two publicly available benchmark\ncomputer vision datasets (MNIST and CIFAR-100), while allowing for\narchitectural differences between student and teacher. By pulling the student's\nRDMs towards those of the teacher, RDL significantly improved visual\nclassification performance when compared to baseline networks that did not use\ntransfer learning. In the future, RDL may enable combined supervised training\nof deep neural networks using task constraints (e.g. images and category\nlabels) and constraints from brain-activity measurements, so as to build models\nthat replicate the internal representational spaces of biological brains. \n\n"}
{"id": "1511.04384", "contents": "Title: Deep Reflectance Maps Abstract: Undoing the image formation process and therefore decomposing appearance into\nits intrinsic properties is a challenging task due to the under-constraint\nnature of this inverse problem. While significant progress has been made on\ninferring shape, materials and illumination from images only, progress in an\nunconstrained setting is still limited. We propose a convolutional neural\narchitecture to estimate reflectance maps of specular materials in natural\nlighting conditions. We achieve this in an end-to-end learning formulation that\ndirectly predicts a reflectance map from the image itself. We show how to\nimprove estimates by facilitating additional supervision in an indirect scheme\nthat first predicts surface orientation and afterwards predicts the reflectance\nmap by a learning-based sparse data interpolation.\n  In order to analyze performance on this difficult task, we propose a new\nchallenge of Specular MAterials on SHapes with complex IllumiNation (SMASHINg)\nusing both synthetic and real images. Furthermore, we show the application of\nour method to a range of image-based editing tasks on real images. \n\n"}
{"id": "1511.05236", "contents": "Title: Reduced-Precision Strategies for Bounded Memory in Deep Neural Nets Abstract: This work investigates how using reduced precision data in Convolutional\nNeural Networks (CNNs) affects network accuracy during classification. More\nspecifically, this study considers networks where each layer may use different\nprecision data. Our key result is the observation that the tolerance of CNNs to\nreduced precision data not only varies across networks, a well established\nobservation, but also within networks. Tuning precision per layer is appealing\nas it could enable energy and performance improvements. In this paper we study\nhow error tolerance across layers varies and propose a method for finding a low\nprecision configuration for a network while maintaining high accuracy. A\ndiverse set of CNNs is analyzed showing that compared to a conventional\nimplementation using a 32-bit floating-point representation for all layers, and\nwith less than 1% loss in relative accuracy, the data footprint required by\nthese networks can be reduced by an average of 74% and up to 92%. \n\n"}
{"id": "1511.05432", "contents": "Title: Understanding Adversarial Training: Increasing Local Stability of Neural\n  Nets through Robust Optimization Abstract: We propose a general framework for increasing local stability of Artificial\nNeural Nets (ANNs) using Robust Optimization (RO). We achieve this through an\nalternating minimization-maximization procedure, in which the loss of the\nnetwork is minimized over perturbed examples that are generated at each\nparameter update. We show that adversarial training of ANNs is in fact\nrobustification of the network optimization, and that our proposed framework\ngeneralizes previous approaches for increasing local stability of ANNs.\nExperimental results reveal that our approach increases the robustness of the\nnetwork to existing adversarial examples, while making it harder to generate\nnew ones. Furthermore, our algorithm improves the accuracy of the network also\non the original test data. \n\n"}
{"id": "1511.05497", "contents": "Title: Learning Neural Network Architectures using Backpropagation Abstract: Deep neural networks with millions of parameters are at the heart of many\nstate of the art machine learning models today. However, recent works have\nshown that models with much smaller number of parameters can also perform just\nas well. In this work, we introduce the problem of architecture-learning, i.e;\nlearning the architecture of a neural network along with weights. We introduce\na new trainable parameter called tri-state ReLU, which helps in eliminating\nunnecessary neurons. We also propose a smooth regularizer which encourages the\ntotal number of neurons after elimination to be small. The resulting objective\nis differentiable and simple to optimize. We experimentally validate our method\non both small and large networks, and show that it can learn models with a\nconsiderably small number of parameters without affecting prediction accuracy. \n\n"}
{"id": "1511.05666", "contents": "Title: Super-Resolution with Deep Convolutional Sufficient Statistics Abstract: Inverse problems in image and audio, and super-resolution in particular, can\nbe seen as high-dimensional structured prediction problems, where the goal is\nto characterize the conditional distribution of a high-resolution output given\nits low-resolution corrupted observation. When the scaling ratio is small,\npoint estimates achieve impressive performance, but soon they suffer from the\nregression-to-the-mean problem, result of their inability to capture the\nmulti-modality of this conditional distribution. Modeling high-dimensional\nimage and audio distributions is a hard task, requiring both the ability to\nmodel complex geometrical structures and textured regions. In this paper, we\npropose to use as conditional model a Gibbs distribution, where its sufficient\nstatistics are given by deep convolutional neural networks. The features\ncomputed by the network are stable to local deformation, and have reduced\nvariance when the input is a stationary texture. These properties imply that\nthe resulting sufficient statistics minimize the uncertainty of the target\nsignals given the degraded observations, while being highly informative. The\nfilters of the CNN are initialized by multiscale complex wavelets, and then we\npropose an algorithm to fine-tune them by estimating the gradient of the\nconditional log-likelihood, which bears some similarities with Generative\nAdversarial Networks. We evaluate experimentally the proposed approach in the\nimage super-resolution task, but the approach is general and could be used in\nother challenging ill-posed problems such as audio bandwidth extension. \n\n"}
{"id": "1511.06062", "contents": "Title: Compact Bilinear Pooling Abstract: Bilinear models has been shown to achieve impressive performance on a wide\nrange of visual tasks, such as semantic segmentation, fine grained recognition\nand face recognition. However, bilinear features are high dimensional,\ntypically on the order of hundreds of thousands to a few million, which makes\nthem impractical for subsequent analysis. We propose two compact bilinear\nrepresentations with the same discriminative power as the full bilinear\nrepresentation but with only a few thousand dimensions. Our compact\nrepresentations allow back-propagation of classification errors enabling an\nend-to-end optimization of the visual recognition system. The compact bilinear\nrepresentations are derived through a novel kernelized analysis of bilinear\npooling which provide insights into the discriminative power of bilinear\npooling, and a platform for further research in compact pooling methods.\nExperimentation illustrate the utility of the proposed representations for\nimage classification and few-shot learning across several datasets. \n\n"}
{"id": "1511.06085", "contents": "Title: Variable Rate Image Compression with Recurrent Neural Networks Abstract: A large fraction of Internet traffic is now driven by requests from mobile\ndevices with relatively small screens and often stringent bandwidth\nrequirements. Due to these factors, it has become the norm for modern\ngraphics-heavy websites to transmit low-resolution, low-bytecount image\npreviews (thumbnails) as part of the initial page load process to improve\napparent page responsiveness. Increasing thumbnail compression beyond the\ncapabilities of existing codecs is therefore a current research focus, as any\nbyte savings will significantly enhance the experience of mobile device users.\nToward this end, we propose a general framework for variable-rate image\ncompression and a novel architecture based on convolutional and deconvolutional\nLSTM recurrent networks. Our models address the main issues that have prevented\nautoencoder neural networks from competing with existing image compression\nalgorithms: (1) our networks only need to be trained once (not per-image),\nregardless of input image dimensions and the desired compression rate; (2) our\nnetworks are progressive, meaning that the more bits are sent, the more\naccurate the image reconstruction; and (3) the proposed architecture is at\nleast as efficient as a standard purpose-trained autoencoder for a given number\nof bits. On a large-scale benchmark of 32$\\times$32 thumbnails, our LSTM-based\napproaches provide better visual quality than (headerless) JPEG, JPEG2000 and\nWebP, with a storage size that is reduced by 10% or more. \n\n"}
{"id": "1511.06233", "contents": "Title: Towards Open Set Deep Networks Abstract: Deep networks have produced significant gains for various visual recognition\nproblems, leading to high impact academic and commercial applications. Recent\nwork in deep networks highlighted that it is easy to generate images that\nhumans would never classify as a particular object class, yet networks classify\nsuch images high confidence as that given class - deep network are easily\nfooled with images humans do not consider meaningful. The closed set nature of\ndeep networks forces them to choose from one of the known classes leading to\nsuch artifacts. Recognition in the real world is open set, i.e. the recognition\nsystem should reject unknown/unseen classes at test time. We present a\nmethodology to adapt deep networks for open set recognition, by introducing a\nnew model layer, OpenMax, which estimates the probability of an input being\nfrom an unknown class. A key element of estimating the unknown probability is\nadapting Meta-Recognition concepts to the activation patterns in the\npenultimate layer of the network. OpenMax allows rejection of \"fooling\" and\nunrelated open set images presented to the system; OpenMax greatly reduces the\nnumber of obvious errors made by a deep network. We prove that the OpenMax\nconcept provides bounded open space risk, thereby formally providing an open\nset recognition solution. We evaluate the resulting open set deep networks\nusing pre-trained networks from the Caffe Model-zoo on ImageNet 2012 validation\ndata, and thousands of fooling and open set images. The proposed OpenMax model\nsignificantly outperforms open set recognition accuracy of basic deep networks\nas well as deep networks with thresholding of SoftMax probabilities. \n\n"}
{"id": "1511.06314", "contents": "Title: Why M Heads are Better than One: Training a Diverse Ensemble of Deep\n  Networks Abstract: Convolutional Neural Networks have achieved state-of-the-art performance on a\nwide range of tasks. Most benchmarks are led by ensembles of these powerful\nlearners, but ensembling is typically treated as a post-hoc procedure\nimplemented by averaging independently trained models with model variation\ninduced by bagging or random initialization. In this paper, we rigorously treat\nensembling as a first-class problem to explicitly address the question: what\nare the best strategies to create an ensemble? We first compare a large number\nof ensembling strategies, and then propose and evaluate novel strategies, such\nas parameter sharing (through a new family of models we call TreeNets) as well\nas training under ensemble-aware and diversity-encouraging losses. We\ndemonstrate that TreeNets can improve ensemble performance and that diverse\nensembles can be trained end-to-end under a unified loss, achieving\nsignificantly higher \"oracle\" accuracies than classical ensembles. \n\n"}
{"id": "1511.06381", "contents": "Title: Manifold Regularized Deep Neural Networks using Adversarial Examples Abstract: Learning meaningful representations using deep neural networks involves\ndesigning efficient training schemes and well-structured networks. Currently,\nthe method of stochastic gradient descent that has a momentum with dropout is\none of the most popular training protocols. Based on that, more advanced\nmethods (i.e., Maxout and Batch Normalization) have been proposed in recent\nyears, but most still suffer from performance degradation caused by small\nperturbations, also known as adversarial examples. To address this issue, we\npropose manifold regularized networks (MRnet) that utilize a novel training\nobjective function that minimizes the difference between multi-layer embedding\nresults of samples and those adversarial. Our experimental results demonstrated\nthat MRnet is more resilient to adversarial examples and helps us to generalize\nrepresentations on manifolds. Furthermore, combining MRnet and dropout allowed\nus to achieve competitive classification performances for three well-known\nbenchmarks: MNIST, CIFAR-10, and SVHN. \n\n"}
{"id": "1511.06408", "contents": "Title: Feature-based Attention in Convolutional Neural Networks Abstract: Convolutional neural networks (CNNs) have proven effective for image\nprocessing tasks, such as object recognition and classification. Recently, CNNs\nhave been enhanced with concepts of attention, similar to those found in\nbiology. Much of this work on attention has focused on effective serial spatial\nprocessing. In this paper, I introduce a simple procedure for applying\nfeature-based attention (FBA) to CNNs and compare multiple implementation\noptions. FBA is a top-down signal applied globally to an input image which\naides in detecting chosen objects in cluttered or noisy settings. The concept\nof FBA and the implementation details tested here were derived from what is\nknown (and debated) about biological object- and feature-based attention. The\nimplementations of FBA described here increase performance on challenging\nobject detection tasks using a procedure that is simple, fast, and does not\nrequire additional iterative training. Furthermore, the comparisons performed\nhere suggest that a proposed model of biological FBA (the \"feature similarity\ngain model\") is effective in increasing performance. \n\n"}
{"id": "1511.06457", "contents": "Title: DOC: Deep OCclusion Estimation From a Single Image Abstract: Recovering the occlusion relationships between objects is a fundamental human\nvisual ability which yields important information about the 3D world. In this\npaper we propose a deep network architecture, called DOC, which acts on a\nsingle image, detects object boundaries and estimates the border ownership\n(i.e. which side of the boundary is foreground and which is background). We\nrepresent occlusion relations by a binary edge map, to indicate the object\nboundary, and an occlusion orientation variable which is tangential to the\nboundary and whose direction specifies border ownership by a left-hand rule. We\ntrain two related deep convolutional neural networks, called DOC, which exploit\nlocal and non-local image cues to estimate this representation and hence\nrecover occlusion relations. In order to train and test DOC we construct a\nlarge-scale instance occlusion boundary dataset using PASCAL VOC images, which\nwe call the PASCAL instance occlusion dataset (PIOD). This contains 10,000\nimages and hence is two orders of magnitude larger than existing occlusion\ndatasets for outdoor images. We test two variants of DOC on PIOD and on the\nBSDS occlusion dataset and show they outperform state-of-the-art methods.\nFinally, we perform numerous experiments investigating multiple settings of DOC\nand transfer between BSDS and PIOD, which provides more insights for further\nstudy of occlusion estimation. \n\n"}
{"id": "1511.06489", "contents": "Title: A Simple Hierarchical Pooling Data Structure for Loop Closure Abstract: We propose a data structure obtained by hierarchically averaging bag-of-word\ndescriptors during a sequence of views that achieves average speedups in\nlarge-scale loop closure applications ranging from 4 to 20 times on benchmark\ndatasets. Although simple, the method works as well as sophisticated\nagglomerative schemes at a fraction of the cost with minimal loss of\nperformance. \n\n"}
{"id": "1511.06881", "contents": "Title: Zoom Better to See Clearer: Human and Object Parsing with Hierarchical\n  Auto-Zoom Net Abstract: Parsing articulated objects, e.g. humans and animals, into semantic parts\n(e.g. body, head and arms, etc.) from natural images is a challenging and\nfundamental problem for computer vision. A big difficulty is the large\nvariability of scale and location for objects and their corresponding parts.\nEven limited mistakes in estimating scale and location will degrade the parsing\noutput and cause errors in boundary details. To tackle these difficulties, we\npropose a \"Hierarchical Auto-Zoom Net\" (HAZN) for object part parsing which\nadapts to the local scales of objects and parts. HAZN is a sequence of two\n\"Auto-Zoom Net\" (AZNs), each employing fully convolutional networks that\nperform two tasks: (1) predict the locations and scales of object instances\n(the first AZN) or their parts (the second AZN); (2) estimate the part scores\nfor predicted object instance or part regions. Our model can adaptively \"zoom\"\n(resize) predicted image regions into their proper scales to refine the\nparsing.\n  We conduct extensive experiments over the PASCAL part datasets on humans,\nhorses, and cows. For humans, our approach significantly outperforms the\nstate-of-the-arts by 5% mIOU and is especially better at segmenting small\ninstances and small parts. We obtain similar improvements for parsing cows and\nhorses over alternative methods. In summary, our strategy of first zooming into\nobjects and then zooming into parts is very effective. It also enables us to\nprocess different regions of the image at different scales adaptively so that,\nfor example, we do not need to waste computational resources scaling the entire\nimage. \n\n"}
{"id": "1511.09249", "contents": "Title: On Learning to Think: Algorithmic Information Theory for Novel\n  Combinations of Reinforcement Learning Controllers and Recurrent Neural World\n  Models Abstract: This paper addresses the general problem of reinforcement learning (RL) in\npartially observable environments. In 2013, our large RL recurrent neural\nnetworks (RNNs) learned from scratch to drive simulated cars from\nhigh-dimensional video input. However, real brains are more powerful in many\nways. In particular, they learn a predictive model of their initially unknown\nenvironment, and somehow use it for abstract (e.g., hierarchical) planning and\nreasoning. Guided by algorithmic information theory, we describe RNN-based AIs\n(RNNAIs) designed to do the same. Such an RNNAI can be trained on never-ending\nsequences of tasks, some of them provided by the user, others invented by the\nRNNAI itself in a curious, playful fashion, to improve its RNN-based world\nmodel. Unlike our previous model-building RNN-based RL machines dating back to\n1990, the RNNAI learns to actively query its model for abstract reasoning and\nplanning and decision making, essentially \"learning to think.\" The basic ideas\nof this report can be applied to many other cases where one RNN-like system\nexploits the algorithmic information content of another. They are taken from a\ngrant proposal submitted in Fall 2014, and also explain concepts such as\n\"mirror neurons.\" Experimental results will be described in separate papers. \n\n"}
{"id": "1512.00596", "contents": "Title: The MegaFace Benchmark: 1 Million Faces for Recognition at Scale Abstract: Recent face recognition experiments on a major benchmark LFW show stunning\nperformance--a number of algorithms achieve near to perfect score, surpassing\nhuman recognition rates. In this paper, we advocate evaluations at the million\nscale (LFW includes only 13K photos of 5K people). To this end, we have\nassembled the MegaFace dataset and created the first MegaFace challenge. Our\ndataset includes One Million photos that capture more than 690K different\nindividuals. The challenge evaluates performance of algorithms with increasing\nnumbers of distractors (going from 10 to 1M) in the gallery set. We present\nboth identification and verification performance, evaluate performance with\nrespect to pose and a person's age, and compare as a function of training data\nsize (number of photos and people). We report results of state of the art and\nbaseline algorithms. Our key observations are that testing at the million scale\nreveals big performance differences (of algorithms that perform similarly well\non smaller scale) and that age invariant recognition as well as pose are still\nchallenging for most. The MegaFace dataset, baseline code, and evaluation\nscripts, are all publicly released for further experimentations at:\nmegaface.cs.washington.edu. \n\n"}
{"id": "1512.01274", "contents": "Title: MXNet: A Flexible and Efficient Machine Learning Library for\n  Heterogeneous Distributed Systems Abstract: MXNet is a multi-language machine learning (ML) library to ease the\ndevelopment of ML algorithms, especially for deep neural networks. Embedded in\nthe host language, it blends declarative symbolic expression with imperative\ntensor computation. It offers auto differentiation to derive gradients. MXNet\nis computation and memory efficient and runs on various heterogeneous systems,\nranging from mobile devices to distributed GPU clusters.\n  This paper describes both the API design and the system implementation of\nMXNet, and explains how embedding of both symbolic expression and tensor\noperation is handled in a unified fashion. Our preliminary experiments reveal\npromising results on large scale deep neural network applications using\nmultiple GPU machines. \n\n"}
{"id": "1512.01355", "contents": "Title: Staple: Complementary Learners for Real-Time Tracking Abstract: Correlation Filter-based trackers have recently achieved excellent\nperformance, showing great robustness to challenging situations exhibiting\nmotion blur and illumination changes. However, since the model that they learn\ndepends strongly on the spatial layout of the tracked object, they are\nnotoriously sensitive to deformation. Models based on colour statistics have\ncomplementary traits: they cope well with variation in shape, but suffer when\nillumination is not consistent throughout a sequence. Moreover, colour\ndistributions alone can be insufficiently discriminative. In this paper, we\nshow that a simple tracker combining complementary cues in a ridge regression\nframework can operate faster than 80 FPS and outperform not only all entries in\nthe popular VOT14 competition, but also recent and far more sophisticated\ntrackers according to multiple benchmarks. \n\n"}
{"id": "1512.02325", "contents": "Title: SSD: Single Shot MultiBox Detector Abstract: We present a method for detecting objects in images using a single deep\nneural network. Our approach, named SSD, discretizes the output space of\nbounding boxes into a set of default boxes over different aspect ratios and\nscales per feature map location. At prediction time, the network generates\nscores for the presence of each object category in each default box and\nproduces adjustments to the box to better match the object shape. Additionally,\nthe network combines predictions from multiple feature maps with different\nresolutions to naturally handle objects of various sizes. Our SSD model is\nsimple relative to methods that require object proposals because it completely\neliminates proposal generation and subsequent pixel or feature resampling stage\nand encapsulates all computation in a single network. This makes SSD easy to\ntrain and straightforward to integrate into systems that require a detection\ncomponent. Experimental results on the PASCAL VOC, MS COCO, and ILSVRC datasets\nconfirm that SSD has comparable accuracy to methods that utilize an additional\nobject proposal step and is much faster, while providing a unified framework\nfor both training and inference. Compared to other single stage methods, SSD\nhas much better accuracy, even with a smaller input image size. For $300\\times\n300$ input, SSD achieves 72.1% mAP on VOC2007 test at 58 FPS on a Nvidia Titan\nX and for $500\\times 500$ input, SSD achieves 75.1% mAP, outperforming a\ncomparable state of the art Faster R-CNN model. Code is available at\nhttps://github.com/weiliu89/caffe/tree/ssd . \n\n"}
{"id": "1512.02326", "contents": "Title: Learning to Point and Count Abstract: This paper proposes the problem of point-and-count as a test case to break\nthe what-and-where deadlock. Different from the traditional detection problem,\nthe goal is to discover key salient points as a way to localize and count the\nnumber of objects simultaneously. We propose two alternatives, one that counts\nfirst and then point, and another that works the other way around.\nFundamentally, they pivot around whether we solve \"what\" or \"where\" first. We\nevaluate their performance on dataset that contains multiple instances of the\nsame class, demonstrating the potentials and their synergies. The experiences\nderive a few important insights that explains why this is a much harder problem\nthan classification, including strong data bias and the inability to deal with\nobject scales robustly in state-of-art convolutional neural networks. \n\n"}
{"id": "1512.02767", "contents": "Title: Affinity CNN: Learning Pixel-Centric Pairwise Relations for\n  Figure/Ground Embedding Abstract: Spectral embedding provides a framework for solving perceptual organization\nproblems, including image segmentation and figure/ground organization. From an\naffinity matrix describing pairwise relationships between pixels, it clusters\npixels into regions, and, using a complex-valued extension, orders pixels\naccording to layer. We train a convolutional neural network (CNN) to directly\npredict the pairwise relationships that define this affinity matrix. Spectral\nembedding then resolves these predictions into a globally-consistent\nsegmentation and figure/ground organization of the scene. Experiments\ndemonstrate significant benefit to this direct coupling compared to prior works\nwhich use explicit intermediate stages, such as edge detection, on the pathway\nfrom image to affinities. Our results suggest spectral embedding as a powerful\nalternative to the conditional random field (CRF)-based globalization schemes\ntypically coupled to deep neural networks. \n\n"}
{"id": "1512.04150", "contents": "Title: Learning Deep Features for Discriminative Localization Abstract: In this work, we revisit the global average pooling layer proposed in [13],\nand shed light on how it explicitly enables the convolutional neural network to\nhave remarkable localization ability despite being trained on image-level\nlabels. While this technique was previously proposed as a means for\nregularizing training, we find that it actually builds a generic localizable\ndeep representation that can be applied to a variety of tasks. Despite the\napparent simplicity of global average pooling, we are able to achieve 37.1%\ntop-5 error for object localization on ILSVRC 2014, which is remarkably close\nto the 34.2% top-5 error achieved by a fully supervised CNN approach. We\ndemonstrate that our network is able to localize the discriminative image\nregions on a variety of tasks despite not being trained for them \n\n"}
{"id": "1512.05227", "contents": "Title: Fine-grained Categorization and Dataset Bootstrapping using Deep Metric\n  Learning with Humans in the Loop Abstract: Existing fine-grained visual categorization methods often suffer from three\nchallenges: lack of training data, large number of fine-grained categories, and\nhigh intraclass vs. low inter-class variance. In this work we propose a generic\niterative framework for fine-grained categorization and dataset bootstrapping\nthat handles these three challenges. Using deep metric learning with humans in\nthe loop, we learn a low dimensional feature embedding with anchor points on\nmanifolds for each category. These anchor points capture intra-class variances\nand remain discriminative between classes. In each round, images with high\nconfidence scores from our model are sent to humans for labeling. By comparing\nwith exemplar images, labelers mark each candidate image as either a \"true\npositive\" or a \"false positive\". True positives are added into our current\ndataset and false positives are regarded as \"hard negatives\" for our metric\nlearning model. Then the model is retrained with an expanded dataset and hard\nnegatives for the next round. To demonstrate the effectiveness of the proposed\nframework, we bootstrap a fine-grained flower dataset with 620 categories from\nInstagram images. The proposed deep metric learning scheme is evaluated on both\nour dataset and the CUB-200-2001 Birds dataset. Experimental evaluations show\nsignificant performance gain using dataset bootstrapping and demonstrate\nstate-of-the-art results achieved by the proposed deep metric learning methods. \n\n"}
{"id": "1512.05245", "contents": "Title: Symphony from Synapses: Neocortex as a Universal Dynamical Systems\n  Modeller using Hierarchical Temporal Memory Abstract: Reverse engineering the brain is proving difficult, perhaps impossible. While\nmany believe that this is just a matter of time and effort, a different\napproach might help. Here, we describe a very simple idea which explains the\npower of the brain as well as its structure, exploiting complex dynamics rather\nthan abstracting it away. Just as a Turing Machine is a Universal Digital\nComputer operating in a world of symbols, we propose that the brain is a\nUniversal Dynamical Systems Modeller, evolved bottom-up (itself using nested\nnetworks of interconnected, self-organised dynamical systems) to prosper in a\nworld of dynamical systems.\n  Recent progress in Applied Mathematics has produced startling evidence of\nwhat happens when abstract Dynamical Systems interact. Key latent information\ndescribing system A can be extracted by system B from very simple signals, and\nsignals can be used by one system to control and manipulate others. Using these\nfacts, we show how a region of the neocortex uses its dynamics to intrinsically\n\"compute\" about the external and internal world.\n  Building on an existing \"static\" model of cortical computation (Hawkins'\nHierarchical Temporal Memory - HTM), we describe how a region of neocortex can\nbe viewed as a network of components which together form a Dynamical Systems\nmodelling module, connected via sensory and motor pathways to the external\nworld, and forming part of a larger dynamical network in the brain.\n  Empirical modelling and simulations of Dynamical HTM are possible with simple\nextensions and combinations of currently existing open source software. We list\na number of relevant projects. \n\n"}
{"id": "1512.05509", "contents": "Title: An Empirical Comparison of Neural Architectures for Reinforcement\n  Learning in Partially Observable Environments Abstract: This paper explores the performance of fitted neural Q iteration for\nreinforcement learning in several partially observable environments, using\nthree recurrent neural network architectures: Long Short-Term Memory, Gated\nRecurrent Unit and MUT1, a recurrent neural architecture evolved from a pool of\nseveral thousands candidate architectures. A variant of fitted Q iteration,\nbased on Advantage values instead of Q values, is also explored. The results\nshow that GRU performs significantly better than LSTM and MUT1 for most of the\nproblems considered, requiring less training episodes and less CPU time before\nlearning a very good policy. Advantage learning also tends to produce better\nresults. \n\n"}
{"id": "1512.07679", "contents": "Title: Deep Reinforcement Learning in Large Discrete Action Spaces Abstract: Being able to reason in an environment with a large number of discrete\nactions is essential to bringing reinforcement learning to a larger class of\nproblems. Recommender systems, industrial plants and language models are only\nsome of the many real-world tasks involving large numbers of discrete actions\nfor which current methods are difficult or even often impossible to apply. An\nability to generalize over the set of actions as well as sub-linear complexity\nrelative to the size of the set are both necessary to handle such tasks.\nCurrent approaches are not able to provide both of these, which motivates the\nwork in this paper. Our proposed approach leverages prior information about the\nactions to embed them in a continuous space upon which it can generalize.\nAdditionally, approximate nearest-neighbor methods allow for logarithmic-time\nlookup complexity relative to the number of actions, which is necessary for\ntime-wise tractable training. This combined approach allows reinforcement\nlearning methods to be applied to large-scale learning problems previously\nintractable with current methods. We demonstrate our algorithm's abilities on a\nseries of tasks having up to one million actions. \n\n"}
{"id": "1601.02098", "contents": "Title: Supervised multiview learning based on simultaneous learning of\n  multiview intact and single view classifier Abstract: Multiview learning problem refers to the problem of learning a classifier\nfrom multiple view data. In this data set, each data points is presented by\nmultiple different views. In this paper, we propose a novel method for this\nproblem. This method is based on two assumptions. The first assumption is that\neach data point has an intact feature vector, and each view is obtained by a\nlinear transformation from the intact vector. The second assumption is that the\nintact vectors are discriminative, and in the intact space, we have a linear\nclassifier to separate the positive class from the negative class. We define an\nintact vector for each data point, and a view-conditional transformation matrix\nfor each view, and propose to reconstruct the multiple view feature vectors by\nthe product of the corresponding intact vectors and transformation matrices.\nMoreover, we also propose a linear classifier in the intact space, and learn it\njointly with the intact vectors. The learning problem is modeled by a\nminimization problem, and the objective function is composed of a Cauchy error\nestimator-based view-conditional reconstruction term over all data points and\nviews, and a classification error term measured by hinge loss over all the\nintact vectors of all the data points. Some regularization terms are also\nimposed to different variables in the objective function. The minimization\nproblem is solve by an iterative algorithm using alternate optimization\nstrategy and gradient descent algorithm. The proposed algorithm shows it\nadvantage in the compression to other multiview learning algorithms on\nbenchmark data sets. \n\n"}
{"id": "1601.03055", "contents": "Title: Subspace Clustering Based Tag Sharing for Inductive Tag Matrix\n  Refinement with Complex Errors Abstract: Annotating images with tags is useful for indexing and retrieving images.\nHowever, many available annotation data include missing or inaccurate\nannotations. In this paper, we propose an image annotation framework which\nsequentially performs tag completion and refinement. We utilize the subspace\nproperty of data via sparse subspace clustering for tag completion. Then we\npropose a novel matrix completion model for tag refinement, integrating visual\ncorrelation, semantic correlation and the novelly studied property of complex\nerrors. The proposed method outperforms the state-of-the-art approaches on\nmultiple benchmark datasets even when they contain certain levels of annotation\nnoise. \n\n"}
{"id": "1601.04183", "contents": "Title: TrueHappiness: Neuromorphic Emotion Recognition on TrueNorth Abstract: We present an approach to constructing a neuromorphic device that responds to\nlanguage input by producing neuron spikes in proportion to the strength of the\nappropriate positive or negative emotional response. Specifically, we perform a\nfine-grained sentiment analysis task with implementations on two different\nsystems: one using conventional spiking neural network (SNN) simulators and the\nother one using IBM's Neurosynaptic System TrueNorth. Input words are projected\ninto a high-dimensional semantic space and processed through a fully-connected\nneural network (FCNN) containing rectified linear units trained via\nbackpropagation. After training, this FCNN is converted to a SNN by\nsubstituting the ReLUs with integrate-and-fire neurons. We show that there is\npractically no performance loss due to conversion to a spiking network on a\nsentiment analysis test set, i.e. correlations between predictions and human\nannotations differ by less than 0.02 comparing the original DNN and its spiking\nequivalent. Additionally, we show that the SNN generated with this technique\ncan be mapped to existing neuromorphic hardware -- in our case, the TrueNorth\nchip. Mapping to the chip involves 4-bit synaptic weight discretization and\nadjustment of the neuron thresholds. The resulting end-to-end system can take a\nuser input, i.e. a word in a vocabulary of over 300,000 words, and estimate its\nsentiment on TrueNorth with a power consumption of approximately 50 uW. \n\n"}
{"id": "1601.05911", "contents": "Title: Orthogonal Echo State Networks and stochastic evaluations of likelihoods Abstract: We report about probabilistic likelihood estimates that are performed on time\nseries using an echo state network with orthogonal recurrent connectivity. The\nresults from tests using synthetic stochastic input time series with temporal\ninference indicate that the capability of the network to infer depends on the\nbalance between input strength and recurrent activity. This balance has an\ninfluence on the network with regard to the quality of inference from the short\nterm input history versus inference that accounts for influences that date back\na long time. Sensitivity of such networks against noise and the finite accuracy\nof network states in the recurrent layer are investigated. In addition, a\nmeasure based on mutual information between the output time series and the\nreservoir is introduced. Finally, different types of recurrent connectivity are\nevaluated. Orthogonal matrices show the best results of all investigated\nconnectivity types overall, but also in the way how the network performance\nscales with the size of the recurrent layer. \n\n"}
{"id": "1601.06608", "contents": "Title: An Unsupervised Method for Detection and Validation of The Optic Disc\n  and The Fovea Abstract: In this work, we have presented a novel method for detection of retinal image\nfeatures, the optic disc and the fovea, from colour fundus photographs of\ndilated eyes for Computer-aided Diagnosis(CAD) system. A saliency map based\nmethod was used to detect the optic disc followed by an unsupervised\nprobabilistic Latent Semantic Analysis for detection validation. The validation\nconcept is based on distinct vessels structures in the optic disc. By using the\nclinical information of standard location of the fovea with respect to the\noptic disc, the macula region is estimated. Accuracy of 100\\% detection is\nachieved for the optic disc and the macula on MESSIDOR and DIARETDB1 and 98.8\\%\ndetection accuracy on STARE dataset. \n\n"}
{"id": "1602.00134", "contents": "Title: Convolutional Pose Machines Abstract: Pose Machines provide a sequential prediction framework for learning rich\nimplicit spatial models. In this work we show a systematic design for how\nconvolutional networks can be incorporated into the pose machine framework for\nlearning image features and image-dependent spatial models for the task of pose\nestimation. The contribution of this paper is to implicitly model long-range\ndependencies between variables in structured prediction tasks such as\narticulated pose estimation. We achieve this by designing a sequential\narchitecture composed of convolutional networks that directly operate on belief\nmaps from previous stages, producing increasingly refined estimates for part\nlocations, without the need for explicit graphical model-style inference. Our\napproach addresses the characteristic difficulty of vanishing gradients during\ntraining by providing a natural learning objective function that enforces\nintermediate supervision, thereby replenishing back-propagated gradients and\nconditioning the learning procedure. We demonstrate state-of-the-art\nperformance and outperform competing methods on standard benchmarks including\nthe MPII, LSP, and FLIC datasets. \n\n"}
{"id": "1602.00709", "contents": "Title: Quantum perceptron over a field and neural network architecture\n  selection in a quantum computer Abstract: In this work, we propose a quantum neural network named quantum perceptron\nover a field (QPF). Quantum computers are not yet a reality and the models and\nalgorithms proposed in this work cannot be simulated in actual (or classical)\ncomputers. QPF is a direct generalization of a classical perceptron and solves\nsome drawbacks found in previous models of quantum perceptrons. We also present\na learning algorithm named Superposition based Architecture Learning algorithm\n(SAL) that optimizes the neural network weights and architectures. SAL searches\nfor the best architecture in a finite set of neural network architectures with\nlinear time over the number of patterns in the training set. SAL is the first\nlearning algorithm to determine neural network architectures in polynomial\ntime. This speedup is obtained by the use of quantum parallelism and a\nnon-linear quantum operator. \n\n"}
{"id": "1602.01255", "contents": "Title: Learning scale-variant and scale-invariant features for deep image\n  classification Abstract: Convolutional Neural Networks (CNNs) require large image corpora to be\ntrained on classification tasks. The variation in image resolutions, sizes of\nobjects and patterns depicted, and image scales, hampers CNN training and\nperformance, because the task-relevant information varies over spatial scales.\nPrevious work attempting to deal with such scale variations focused on\nencouraging scale-invariant CNN representations. However, scale-invariant\nrepresentations are incomplete representations of images, because images\ncontain scale-variant information as well. This paper addresses the combined\ndevelopment of scale-invariant and scale-variant representations. We propose a\nmulti- scale CNN method to encourage the recognition of both types of features\nand evaluate it on a challenging image classification task involving\ntask-relevant characteristics at multiple scales. The results show that our\nmulti-scale CNN outperforms single-scale CNN. This leads to the conclusion that\nencouraging the combined development of a scale-invariant and scale-variant\nrepresentation in CNNs is beneficial to image recognition performance. \n\n"}
{"id": "1602.02865", "contents": "Title: The Role of Typicality in Object Classification: Improving The\n  Generalization Capacity of Convolutional Neural Networks Abstract: Deep artificial neural networks have made remarkable progress in different\ntasks in the field of computer vision. However, the empirical analysis of these\nmodels and investigation of their failure cases has received attention\nrecently. In this work, we show that deep learning models cannot generalize to\natypical images that are substantially different from training images. This is\nin contrast to the superior generalization ability of the visual system in the\nhuman brain. We focus on Convolutional Neural Networks (CNN) as the\nstate-of-the-art models in object recognition and classification; investigate\nthis problem in more detail, and hypothesize that training CNN models suffer\nfrom unstructured loss minimization. We propose computational models to improve\nthe generalization capacity of CNNs by considering how typical a training image\nlooks like. By conducting an extensive set of experiments we show that\ninvolving a typicality measure can improve the classification results on a new\nset of images by a large margin. More importantly, this significant improvement\nis achieved without fine-tuning the CNN model on the target image set. \n\n"}
{"id": "1602.03930", "contents": "Title: Global Deconvolutional Networks for Semantic Segmentation Abstract: Semantic image segmentation is a principal problem in computer vision, where\nthe aim is to correctly classify each individual pixel of an image into a\nsemantic label. Its widespread use in many areas, including medical imaging and\nautonomous driving, has fostered extensive research in recent years. Empirical\nimprovements in tackling this task have primarily been motivated by successful\nexploitation of Convolutional Neural Networks (CNNs) pre-trained for image\nclassification and object recognition. However, the pixel-wise labelling with\nCNNs has its own unique challenges: (1) an accurate deconvolution, or\nupsampling, of low-resolution output into a higher-resolution segmentation mask\nand (2) an inclusion of global information, or context, within locally\nextracted features. To address these issues, we propose a novel architecture to\nconduct the equivalent of the deconvolution operation globally and acquire\ndense predictions. We demonstrate that it leads to improved performance of\nstate-of-the-art semantic segmentation models on the PASCAL VOC 2012 benchmark,\nreaching 74.0% mean IU accuracy on the test set. \n\n"}
{"id": "1602.07373", "contents": "Title: On Study of the Binarized Deep Neural Network for Image Classification Abstract: Recently, the deep neural network (derived from the artificial neural\nnetwork) has attracted many researchers' attention by its outstanding\nperformance. However, since this network requires high-performance GPUs and\nlarge storage, it is very hard to use it on individual devices. In order to\nimprove the deep neural network, many trials have been made by refining the\nnetwork structure or training strategy. Unlike those trials, in this paper, we\nfocused on the basic propagation function of the artificial neural network and\nproposed the binarized deep neural network. This network is a pure binary\nsystem, in which all the values and calculations are binarized. As a result,\nour network can save a lot of computational resource and storage. Therefore, it\nis possible to use it on various devices. Moreover, the experimental results\nproved the feasibility of the proposed network. \n\n"}
{"id": "1602.08405", "contents": "Title: We don't need no bounding-boxes: Training object class detectors using\n  only human verification Abstract: Training object class detectors typically requires a large set of images in\nwhich objects are annotated by bounding-boxes. However, manually drawing\nbounding-boxes is very time consuming. We propose a new scheme for training\nobject detectors which only requires annotators to verify bounding-boxes\nproduced automatically by the learning algorithm. Our scheme iterates between\nre-training the detector, re-localizing objects in the training images, and\nhuman verification. We use the verification signal both to improve re-training\nand to reduce the search space for re-localisation, which makes these steps\ndifferent to what is normally done in a weakly supervised setting. Extensive\nexperiments on PASCAL VOC 2007 show that (1) using human verification to update\ndetectors and reduce the search space leads to the rapid production of\nhigh-quality bounding-box annotations; (2) our scheme delivers detectors\nperforming almost as good as those trained in a fully supervised setting,\nwithout ever drawing any bounding-box; (3) as the verification task is very\nquick, our scheme substantially reduces total annotation time by a factor\n6x-9x. \n\n"}
{"id": "1602.08680", "contents": "Title: Measuring and Predicting Tag Importance for Image Retrieval Abstract: Textual data such as tags, sentence descriptions are combined with visual\ncues to reduce the semantic gap for image retrieval applications in today's\nMultimodal Image Retrieval (MIR) systems. However, all tags are treated as\nequally important in these systems, which may result in misalignment between\nvisual and textual modalities during MIR training. This will further lead to\ndegenerated retrieval performance at query time. To address this issue, we\ninvestigate the problem of tag importance prediction, where the goal is to\nautomatically predict the tag importance and use it in image retrieval. To\nachieve this, we first propose a method to measure the relative importance of\nobject and scene tags from image sentence descriptions. Using this as the\nground truth, we present a tag importance prediction model to jointly exploit\nvisual, semantic and context cues. The Structural Support Vector Machine (SSVM)\nformulation is adopted to ensure efficient training of the prediction model.\nThen, the Canonical Correlation Analysis (CCA) is employed to learn the\nrelation between the image visual feature and tag importance to obtain robust\nretrieval performance. Experimental results on three real-world datasets show a\nsignificant performance improvement of the proposed MIR with Tag Importance\nPrediction (MIR/TIP) system over other MIR systems. \n\n"}
{"id": "1603.00831", "contents": "Title: MOT16: A Benchmark for Multi-Object Tracking Abstract: Standardized benchmarks are crucial for the majority of computer vision\napplications. Although leaderboards and ranking tables should not be\nover-claimed, benchmarks often provide the most objective measure of\nperformance and are therefore important guides for reseach.\n  Recently, a new benchmark for Multiple Object Tracking, MOTChallenge, was\nlaunched with the goal of collecting existing and new data and creating a\nframework for the standardized evaluation of multiple object tracking methods.\nThe first release of the benchmark focuses on multiple people tracking, since\npedestrians are by far the most studied object in the tracking community. This\npaper accompanies a new release of the MOTChallenge benchmark. Unlike the\ninitial release, all videos of MOT16 have been carefully annotated following a\nconsistent protocol. Moreover, it not only offers a significant increase in the\nnumber of labeled boxes, but also provides multiple object classes beside\npedestrians and the level of visibility for every single object of interest. \n\n"}
{"id": "1603.01768", "contents": "Title: Semantic Style Transfer and Turning Two-Bit Doodles into Fine Artworks Abstract: Convolutional neural networks (CNNs) have proven highly effective at image\nsynthesis and style transfer. For most users, however, using them as tools can\nbe a challenging task due to their unpredictable behavior that goes against\ncommon intuitions. This paper introduces a novel concept to augment such\ngenerative architectures with semantic annotations, either by manually\nauthoring pixel labels or using existing solutions for semantic segmentation.\nThe result is a content-aware generative algorithm that offers meaningful\ncontrol over the outcome. Thus, we increase the quality of images generated by\navoiding common glitches, make the results look significantly more plausible,\nand extend the functional range of these algorithms---whether for portraits or\nlandscapes, etc. Applications include semantic style transfer and turning\ndoodles with few colors into masterful paintings! \n\n"}
{"id": "1603.02078", "contents": "Title: A novel learning-based frame pooling method for Event Detection Abstract: Detecting complex events in a large video collection crawled from video\nwebsites is a challenging task. When applying directly good image-based feature\nrepresentation, e.g., HOG, SIFT, to videos, we have to face the problem of how\nto pool multiple frame feature representations into one feature representation.\nIn this paper, we propose a novel learning-based frame pooling method. We\nformulate the pooling weight learning as an optimization problem and thus our\nmethod can automatically learn the best pooling weight configuration for each\nspecific event category. Experimental results conducted on TRECVID MED 2011\nreveal that our method outperforms the commonly used average pooling and max\npooling strategies on both high-level and low-level 2D image features. \n\n"}
{"id": "1603.03183", "contents": "Title: Exploring Context with Deep Structured models for Semantic Segmentation Abstract: State-of-the-art semantic image segmentation methods are mostly based on\ntraining deep convolutional neural networks (CNNs). In this work, we proffer to\nimprove semantic segmentation with the use of contextual information. In\nparticular, we explore `patch-patch' context and `patch-background' context in\ndeep CNNs. We formulate deep structured models by combining CNNs and\nConditional Random Fields (CRFs) for learning the patch-patch context between\nimage regions. Specifically, we formulate CNN-based pairwise potential\nfunctions to capture semantic correlations between neighboring patches.\nEfficient piecewise training of the proposed deep structured model is then\napplied in order to avoid repeated expensive CRF inference during the course of\nback propagation. For capturing the patch-background context, we show that a\nnetwork design with traditional multi-scale image inputs and sliding pyramid\npooling is very effective for improving performance. We perform comprehensive\nevaluation of the proposed method. We achieve new state-of-the-art performance\non a number of challenging semantic segmentation datasets including $NYUDv2$,\n$PASCAL$-$VOC2012$, $Cityscapes$, $PASCAL$-$Context$, $SUN$-$RGBD$,\n$SIFT$-$flow$, and $KITTI$ datasets. Particularly, we report an\nintersection-over-union score of $77.8$ on the $PASCAL$-$VOC2012$ dataset. \n\n"}
{"id": "1603.03657", "contents": "Title: Efficient forward propagation of time-sequences in convolutional neural\n  networks using Deep Shifting Abstract: When a Convolutional Neural Network is used for on-the-fly evaluation of\ncontinuously updating time-sequences, many redundant convolution operations are\nperformed. We propose the method of Deep Shifting, which remembers previously\ncalculated results of convolution operations in order to minimize the number of\ncalculations. The reduction in complexity is at least a constant and in the\nbest case quadratic. We demonstrate that this method does indeed save\nsignificant computation time in a practical implementation, especially when the\nnetworks receives a large number of time-frames. \n\n"}
{"id": "1603.03685", "contents": "Title: Determination of the edge of criticality in echo state networks through\n  Fisher information maximization Abstract: It is a widely accepted fact that the computational capability of recurrent\nneural networks is maximized on the so-called \"edge of criticality\". Once the\nnetwork operates in this configuration, it performs efficiently on a specific\napplication both in terms of (i) low prediction error and (ii) high short-term\nmemory capacity. Since the behavior of recurrent networks is strongly\ninfluenced by the particular input signal driving the dynamics, a universal,\napplication-independent method for determining the edge of criticality is still\nmissing. In this paper, we aim at addressing this issue by proposing a\ntheoretically motivated, unsupervised method based on Fisher information for\ndetermining the edge of criticality in recurrent neural networks. It is proven\nthat Fisher information is maximized for (finite-size) systems operating in\nsuch critical regions. However, Fisher information is notoriously difficult to\ncompute and either requires the probability density function or the conditional\ndependence of the system states with respect to the model parameters. The paper\ntakes advantage of a recently-developed non-parametric estimator of the Fisher\ninformation matrix and provides a method to determine the critical region of\necho state networks, a particular class of recurrent networks. The considered\ncontrol parameters, which indirectly affect the echo state network performance,\nare explored to identify those configurations lying on the edge of criticality\nand, as such, maximizing Fisher information and computational performance.\nExperimental results on benchmarks and real-world data demonstrate the\neffectiveness of the proposed method. \n\n"}
{"id": "1603.03827", "contents": "Title: Sequential Short-Text Classification with Recurrent and Convolutional\n  Neural Networks Abstract: Recent approaches based on artificial neural networks (ANNs) have shown\npromising results for short-text classification. However, many short texts\noccur in sequences (e.g., sentences in a document or utterances in a dialog),\nand most existing ANN-based systems do not leverage the preceding short texts\nwhen classifying a subsequent one. In this work, we present a model based on\nrecurrent neural networks and convolutional neural networks that incorporates\nthe preceding short texts. Our model achieves state-of-the-art results on three\ndifferent datasets for dialog act prediction. \n\n"}
{"id": "1603.04871", "contents": "Title: Combining the Best of Convolutional Layers and Recurrent Layers: A\n  Hybrid Network for Semantic Segmentation Abstract: State-of-the-art results of semantic segmentation are established by Fully\nConvolutional neural Networks (FCNs). FCNs rely on cascaded convolutional and\npooling layers to gradually enlarge the receptive fields of neurons, resulting\nin an indirect way of modeling the distant contextual dependence. In this work,\nwe advocate the use of spatially recurrent layers (i.e. ReNet layers) which\ndirectly capture global contexts and lead to improved feature representations.\nWe demonstrate the effectiveness of ReNet layers by building a Naive deep ReNet\n(N-ReNet), which achieves competitive performance on Stanford Background\ndataset. Furthermore, we integrate ReNet layers with FCNs, and develop a novel\nHybrid deep ReNet (H-ReNet). It enjoys a few remarkable properties, including\nfull-image receptive fields, end-to-end training, and efficient network\nexecution. On the PASCAL VOC 2012 benchmark, the H-ReNet improves the results\nof state-of-the-art approaches Piecewise, CRFasRNN and DeepParsing by 3.6%,\n2.3% and 0.2%, respectively, and achieves the highest IoUs for 13 out of the 20\nobject classes. \n\n"}
{"id": "1603.05835", "contents": "Title: A Flexible Primal-Dual Toolbox Abstract: \\textbf{FlexBox} is a flexible MATLAB toolbox for finite dimensional convex\nvariational problems in image processing and beyond. Such problems often\nconsist of non-differentiable parts and involve linear operators. The toolbox\nuses a primal-dual scheme to avoid (computationally) inefficient operator\ninversion and to get reliable error estimates. From the user-side,\n\\textbf{FlexBox} expects the primal formulation of the problem, automatically\ndecouples operators and dualizes the problem. For large-scale problems,\n\\textbf{FlexBox} also comes with a \\cpp-module, which can be used stand-alone\nor together with MATLAB via MEX-interfaces. Besides various pre-implemented\ndata-fidelities and regularization-terms, \\textbf{FlexBox} is able to handle\narbitrary operators while being easily extendable, due to its object-oriented\ndesign. The toolbox is available at\n\\href{http://www.flexbox.im}{http://www.flexbox.im} \n\n"}
{"id": "1603.06182", "contents": "Title: Modelling Temporal Information Using Discrete Fourier Transform for\n  Video Classification Abstract: Recently, video classification attracts intensive research efforts. However,\nmost existing works are based on framelevel visual features, which might fail\nto model the temporal information, e.g. characteristics accumulated along time.\nIn order to capture video temporal information, we propose to analyse features\nin frequency domain transformed by discrete Fourier transform (DFT features).\nFrame-level features are firstly extract by a pre-trained deep convolutional\nneural network (CNN). Then, time domain features are transformed and\ninterpolated into DFT features. CNN and DFT features are further encoded by\nusing different pooling methods and fused for video classification. In this\nway, static image features extracted from a pre-trained deep CNN and temporal\ninformation represented by DFT features are jointly considered for video\nclassification. We test our method for video emotion classification and action\nrecognition. Experimental results demonstrate that combining DFT features can\neffectively capture temporal information and therefore improve the performance\nof both video emotion classification and action recognition. Our approach has\nachieved a state-of-the-art performance on the largest video emotion dataset\n(VideoEmotion-8 dataset) and competitive results on UCF-101. \n\n"}
{"id": "1603.07285", "contents": "Title: A guide to convolution arithmetic for deep learning Abstract: We introduce a guide to help deep learning practitioners understand and\nmanipulate convolutional neural network architectures. The guide clarifies the\nrelationship between various properties (input shape, kernel shape, zero\npadding, strides and output shape) of convolutional, pooling and transposed\nconvolutional layers, as well as the relationship between convolutional and\ntransposed convolutional layers. Relationships are derived for various cases,\nand are illustrated in order to make them intuitive. \n\n"}
{"id": "1603.07475", "contents": "Title: Fine-scale Surface Normal Estimation using a Single NIR Image Abstract: We present surface normal estimation using a single near infrared (NIR)\nimage. We are focusing on fine-scale surface geometry captured with an\nuncalibrated light source. To tackle this ill-posed problem, we adopt a\ngenerative adversarial network which is effective in recovering a sharp output,\nwhich is also essential for fine-scale surface normal estimation. We\nincorporate angular error and integrability constraint into the objective\nfunction of the network to make estimated normals physically meaningful. We\ntrain and validate our network on a recent NIR dataset, and also evaluate the\ngenerality of our trained model by using new external datasets which are\ncaptured with a different camera under different environment. \n\n"}
{"id": "1603.07886", "contents": "Title: A Novel Biologically Mechanism-Based Visual Cognition Model--Automatic\n  Extraction of Semantics, Formation of Integrated Concepts and Re-selection\n  Features for Ambiguity Abstract: Integration between biology and information science benefits both fields.\nMany related models have been proposed, such as computational visual cognition\nmodels, computational motor control models, integrations of both and so on. In\ngeneral, the robustness and precision of recognition is one of the key problems\nfor object recognition models.\n  In this paper, inspired by features of human recognition process and their\nbiological mechanisms, a new integrated and dynamic framework is proposed to\nmimic the semantic extraction, concept formation and feature re-selection in\nhuman visual processing. The main contributions of the proposed model are as\nfollows:\n  (1) Semantic feature extraction: Local semantic features are learnt from\nepisodic features that are extracted from raw images through a deep neural\nnetwork;\n  (2) Integrated concept formation: Concepts are formed with local semantic\ninformation and structural information learnt through network.\n  (3) Feature re-selection: When ambiguity is detected during recognition\nprocess, distinctive features according to the difference between ambiguous\ncandidates are re-selected for recognition.\n  Experimental results on hand-written digits and facial shape dataset show\nthat, compared with other methods, the new proposed model exhibits higher\nrobustness and precision for visual recognition, especially in the condition\nwhen input samples are smantic ambiguous. Meanwhile, the introduced biological\nmechanisms further strengthen the interaction between neuroscience and\ninformation science. \n\n"}
{"id": "1603.08474", "contents": "Title: Deep Embedding for Spatial Role Labeling Abstract: This paper introduces the visually informed embedding of word (VIEW), a\ncontinuous vector representation for a word extracted from a deep neural model\ntrained using the Microsoft COCO data set to forecast the spatial arrangements\nbetween visual objects, given a textual description. The model is composed of a\ndeep multilayer perceptron (MLP) stacked on the top of a Long Short Term Memory\n(LSTM) network, the latter being preceded by an embedding layer. The VIEW is\napplied to transferring multimodal background knowledge to Spatial Role\nLabeling (SpRL) algorithms, which recognize spatial relations between objects\nmentioned in the text. This work also contributes with a new method to select\ncomplementary features and a fine-tuning method for MLP that improves the $F1$\nmeasure in classifying the words into spatial roles. The VIEW is evaluated with\nthe Task 3 of SemEval-2013 benchmark data set, SpaceEval. \n\n"}
{"id": "1603.08486", "contents": "Title: Learning to Read Chest X-Rays: Recurrent Neural Cascade Model for\n  Automated Image Annotation Abstract: Despite the recent advances in automatically describing image contents, their\napplications have been mostly limited to image caption datasets containing\nnatural images (e.g., Flickr 30k, MSCOCO). In this paper, we present a deep\nlearning model to efficiently detect a disease from an image and annotate its\ncontexts (e.g., location, severity and the affected organs). We employ a\npublicly available radiology dataset of chest x-rays and their reports, and use\nits image annotations to mine disease names to train convolutional neural\nnetworks (CNNs). In doing so, we adopt various regularization techniques to\ncircumvent the large normal-vs-diseased cases bias. Recurrent neural networks\n(RNNs) are then trained to describe the contexts of a detected disease, based\non the deep CNN features. Moreover, we introduce a novel approach to use the\nweights of the already trained pair of CNN/RNN on the domain-specific\nimage/text dataset, to infer the joint image/text contexts for composite image\nlabeling. Significantly improved image annotation results are demonstrated\nusing the recurrent neural cascade model by taking the joint image/text\ncontexts into account. \n\n"}
{"id": "1603.08511", "contents": "Title: Colorful Image Colorization Abstract: Given a grayscale photograph as input, this paper attacks the problem of\nhallucinating a plausible color version of the photograph. This problem is\nclearly underconstrained, so previous approaches have either relied on\nsignificant user interaction or resulted in desaturated colorizations. We\npropose a fully automatic approach that produces vibrant and realistic\ncolorizations. We embrace the underlying uncertainty of the problem by posing\nit as a classification task and use class-rebalancing at training time to\nincrease the diversity of colors in the result. The system is implemented as a\nfeed-forward pass in a CNN at test time and is trained on over a million color\nimages. We evaluate our algorithm using a \"colorization Turing test,\" asking\nhuman participants to choose between a generated and ground truth color image.\nOur method successfully fools humans on 32% of the trials, significantly higher\nthan previous methods. Moreover, we show that colorization can be a powerful\npretext task for self-supervised feature learning, acting as a cross-channel\nencoder. This approach results in state-of-the-art performance on several\nfeature learning benchmarks. \n\n"}
{"id": "1603.08907", "contents": "Title: Cross-modal Supervision for Learning Active Speaker Detection in Video Abstract: In this paper, we show how to use audio to supervise the learning of active\nspeaker detection in video. Voice Activity Detection (VAD) guides the learning\nof the vision-based classifier in a weakly supervised manner. The classifier\nuses spatio-temporal features to encode upper body motion - facial expressions\nand gesticulations associated with speaking. We further improve a generic model\nfor active speaker detection by learning person specific models. Finally, we\ndemonstrate the online adaptation of generic models learnt on one dataset, to\npreviously unseen people in a new dataset, again using audio (VAD) for weak\nsupervision. The use of temporal continuity overcomes the lack of clean\ntraining data. We are the first to present an active speaker detection system\nthat learns on one audio-visual dataset and automatically adapts to speakers in\na new dataset. This work can be seen as an example of how the availability of\nmulti-modal data allows us to learn a model without the need for supervision,\nby transferring knowledge from one modality to another. \n\n"}
{"id": "1603.09046", "contents": "Title: Dense Image Representation with Spatial Pyramid VLAD Coding of CNN for\n  Locally Robust Captioning Abstract: The workflow of extracting features from images using convolutional neural\nnetworks (CNN) and generating captions with recurrent neural networks (RNN) has\nbecome a de-facto standard for image captioning task. However, since CNN\nfeatures are originally designed for classification task, it is mostly\nconcerned with the main conspicuous element of the image, and often fails to\ncorrectly convey information on local, secondary elements. We propose to\nincorporate coding with vector of locally aggregated descriptors (VLAD) on\nspatial pyramid for CNN features of sub-regions in order to generate image\nrepresentations that better reflect the local information of the images. Our\nresults show that our method of compact VLAD coding can match CNN features with\nas little as 3% of dimensionality and, when combined with spatial pyramid, it\nresults in image captions that more accurately take local elements into\naccount. \n\n"}
{"id": "1603.09188", "contents": "Title: Unsupervised Visual Sense Disambiguation for Verbs using Multimodal\n  Embeddings Abstract: We introduce a new task, visual sense disambiguation for verbs: given an\nimage and a verb, assign the correct sense of the verb, i.e., the one that\ndescribes the action depicted in the image. Just as textual word sense\ndisambiguation is useful for a wide range of NLP tasks, visual sense\ndisambiguation can be useful for multimodal tasks such as image retrieval,\nimage description, and text illustration. We introduce VerSe, a new dataset\nthat augments existing multimodal datasets (COCO and TUHOI) with sense labels.\nWe propose an unsupervised algorithm based on Lesk which performs visual sense\ndisambiguation using textual, visual, or multimodal embeddings. We find that\ntextual embeddings perform well when gold-standard textual annotations (object\nlabels and image descriptions) are available, while multimodal embeddings\nperform well on unannotated images. We also verify our findings by using the\ntextual and multimodal embeddings as features in a supervised setting and\nanalyse the performance of visual sense disambiguation task. VerSe is made\npublicly available and can be downloaded at:\nhttps://github.com/spandanagella/verse. \n\n"}
{"id": "1603.09473", "contents": "Title: Learning Compatibility Across Categories for Heterogeneous Item\n  Recommendation Abstract: Identifying relationships between items is a key task of an online\nrecommender system, in order to help users discover items that are functionally\ncomplementary or visually compatible. In domains like clothing recommendation,\nthis task is particularly challenging since a successful system should be\ncapable of handling a large corpus of items, a huge amount of relationships\namong them, as well as the high-dimensional and semantically complicated\nfeatures involved. Furthermore, the human notion of \"compatibility\" to capture\ngoes beyond mere similarity: For two items to be compatible---whether jeans and\na t-shirt, or a laptop and a charger---they should be similar in some ways, but\nsystematically different in others.\n  In this paper we propose a novel method, Monomer, to learn complicated and\nheterogeneous relationships between items in product recommendation settings.\nRecently, scalable methods have been developed that address this task by\nlearning similarity metrics on top of the content of the products involved.\nHere our method relaxes the metricity assumption inherent in previous work and\nmodels multiple localized notions of 'relatedness,' so as to uncover ways in\nwhich related items should be systematically similar, and systematically\ndifferent. Quantitatively, we show that our system achieves state-of-the-art\nperformance on large-scale compatibility prediction tasks, especially in cases\nwhere there is substantial heterogeneity between related items. Qualitatively,\nwe demonstrate that richer notions of compatibility can be learned that go\nbeyond similarity, and that our model can make effective recommendations of\nheterogeneous content. \n\n"}
{"id": "1604.00990", "contents": "Title: Direct Visual Odometry using Bit-Planes Abstract: Feature descriptors, such as SIFT and ORB, are well-known for their\nrobustness to illumination changes, which has made them popular for\nfeature-based VSLAM\\@. However, in degraded imaging conditions such as low\nlight, low texture, blur and specular reflections, feature extraction is often\nunreliable. In contrast, direct VSLAM methods which estimate the camera pose by\nminimizing the photometric error using raw pixel intensities are often more\nrobust to low textured environments and blur. Nonetheless, at the core of\ndirect VSLAM is the reliance on a consistent photometric appearance across\nimages, otherwise known as the brightness constancy assumption. Unfortunately,\nbrightness constancy seldom holds in real world applications.\n  In this work, we overcome brightness constancy by incorporating feature\ndescriptors into a direct visual odometry framework. This combination results\nin an efficient algorithm that combines the strength of both feature-based\nalgorithms and direct methods. Namely, we achieve robustness to arbitrary\nphotometric variations while operating in low-textured and poorly lit\nenvironments. Our approach utilizes an efficient binary descriptor, which we\ncall Bit-Planes, and show how it can be used in the gradient-based optimization\nrequired by direct methods. Moreover, we show that the squared Euclidean\ndistance between Bit-Planes is equivalent to the Hamming distance. Hence, the\ndescriptor may be used in least squares optimization without sacrificing its\nphotometric invariance. Finally, we present empirical results that demonstrate\nthe robustness of the approach in poorly lit underground environments. \n\n"}
{"id": "1604.01444", "contents": "Title: A Convolutional Neural Network Neutrino Event Classifier Abstract: Convolutional neural networks (CNNs) have been widely applied in the computer\nvision community to solve complex problems in image recognition and analysis.\nWe describe an application of the CNN technology to the problem of identifying\nparticle interactions in sampling calorimeters used commonly in high energy\nphysics and high energy neutrino physics in particular. Following a discussion\nof the core concepts of CNNs and recent innovations in CNN architectures\nrelated to the field of deep learning, we outline a specific application to the\nNOvA neutrino detector. This algorithm, CVN (Convolutional Visual Network)\nidentifies neutrino interactions based on their topology without the need for\ndetailed reconstruction and outperforms algorithms currently in use by the NOvA\ncollaboration. \n\n"}
{"id": "1604.02646", "contents": "Title: Visualization Regularizers for Neural Network based Image Recognition Abstract: The success of deep neural networks is mostly due their ability to learn\nmeaningful features from the data. Features learned in the hidden layers of\ndeep neural networks trained in computer vision tasks have been shown to be\nsimilar to mid-level vision features. We leverage this fact in this work and\npropose the visualization regularizer for image tasks. The proposed\nregularization technique enforces smoothness of the features learned by hidden\nnodes and turns out to be a special case of Tikhonov regularization. We achieve\nhigher classification accuracy as compared to existing regularizers such as the\nL2 norm regularizer and dropout, on benchmark datasets without changing the\ntraining computational complexity. \n\n"}
{"id": "1604.02902", "contents": "Title: Statistics of RGBD Images Abstract: Cameras that can measure the depth of each pixel in addition to its color\nhave become easily available and are used in many consumer products worldwide.\nOften the depth channel is captured at lower quality compared to the RGB\nchannels and different algorithms have been proposed to improve the quality of\nthe D channel given the RGB channels. Typically these approaches work by\nassuming that edges in RGB are correlated with edges in D.\n  In this paper we approach this problem from the standpoint of natural image\nstatistics. We obtain examples of high quality RGBD images from a computer\ngraphics generated movie (MPI-Sintel) and we use these examples to compare\ndifferent probabilistic generative models of RGBD image patches. We then use\nthe generative models together with a degradation model and obtain a Bayes\nLeast Squares (BLS) estimator of the D channel given the RGB channels. Our\nresults show that learned generative models outperform the state-of-the-art in\nimproving the quality of depth channels given the color channels in natural\nimages even when training is performed on artificially generated images. \n\n"}
{"id": "1604.03247", "contents": "Title: Thesis: Multiple Kernel Learning for Object Categorization Abstract: Object Categorization is a challenging problem, especially when the images\nhave clutter background, occlusions or different lighting conditions. In the\npast, many descriptors have been proposed which aid object categorization even\nin such adverse conditions. Each descriptor has its own merits and de-merits.\nSome descriptors are invariant to transformations while the others are more\ndiscriminative. Past research has shown that, employing multiple descriptors\nrather than any single descriptor leads to better recognition. The problem of\nlearning the optimal combination of the available descriptors for a particular\nclassification task is studied. Multiple Kernel Learning (MKL) framework has\nbeen developed for learning an optimal combination of descriptors for object\ncategorization. Existing MKL formulations often employ block l-1 norm\nregularization which is equivalent to selecting a single kernel from a library\nof kernels. Since essentially a single descriptor is selected, the existing\nformulations maybe sub- optimal for object categorization. A MKL formulation\nbased on block l-infinity norm regularization has been developed, which chooses\nan optimal combination of kernels as opposed to selecting a single kernel. A\nComposite Multiple Kernel Learning(CKL) formulation based on mixed l-infinity\nand l-1 norm regularization has been developed. These formulations end in\nSecond Order Cone Programs(SOCP). Other efficient alter- native algorithms for\nthese formulation have been implemented. Empirical results on benchmark\ndatasets show significant improvement using these new MKL formulations. \n\n"}
{"id": "1604.04144", "contents": "Title: Self-taught learning of a deep invariant representation for visual\n  tracking via temporal slowness principle Abstract: Visual representation is crucial for a visual tracking method's performances.\nConventionally, visual representations adopted in visual tracking rely on\nhand-crafted computer vision descriptors. These descriptors were developed\ngenerically without considering tracking-specific information. In this paper,\nwe propose to learn complex-valued invariant representations from tracked\nsequential image patches, via strong temporal slowness constraint and stacked\nconvolutional autoencoders. The deep slow local representations are learned\noffline on unlabeled data and transferred to the observational model of our\nproposed tracker. The proposed observational model retains old training samples\nto alleviate drift, and collect negative samples which are coherent with\ntarget's motion pattern for better discriminative tracking. With the learned\nrepresentation and online training samples, a logistic regression classifier is\nadopted to distinguish target from background, and retrained online to adapt to\nappearance changes. Subsequently, the observational model is integrated into a\nparticle filter framework to peform visual tracking. Experimental results on\nvarious challenging benchmark sequences demonstrate that the proposed tracker\nperforms favourably against several state-of-the-art trackers. \n\n"}
{"id": "1604.04378", "contents": "Title: Match-SRNN: Modeling the Recursive Matching Structure with Spatial RNN Abstract: Semantic matching, which aims to determine the matching degree between two\ntexts, is a fundamental problem for many NLP applications. Recently, deep\nlearning approach has been applied to this problem and significant improvements\nhave been achieved. In this paper, we propose to view the generation of the\nglobal interaction between two texts as a recursive process: i.e. the\ninteraction of two texts at each position is a composition of the interactions\nbetween their prefixes as well as the word level interaction at the current\nposition. Based on this idea, we propose a novel deep architecture, namely\nMatch-SRNN, to model the recursive matching structure. Firstly, a tensor is\nconstructed to capture the word level interactions. Then a spatial RNN is\napplied to integrate the local interactions recursively, with importance\ndetermined by four types of gates. Finally, the matching score is calculated\nbased on the global interaction. We show that, after degenerated to the exact\nmatching scenario, Match-SRNN can approximate the dynamic programming process\nof longest common subsequence. Thus, there exists a clear interpretation for\nMatch-SRNN. Our experiments on two semantic matching tasks showed the\neffectiveness of Match-SRNN, and its ability of visualizing the learned\nmatching structure. \n\n"}
{"id": "1604.04473", "contents": "Title: Probing the Intra-Component Correlations within Fisher Vector for\n  Material Classification Abstract: Fisher vector (FV) has become a popular image representation. One notable\nunderlying assumption of the FV framework is that local descriptors are well\ndecorrelated within each cluster so that the covariance matrix for each\nGaussian can be simplified to be diagonal. Though the FV usually relies on the\nPrincipal Component Analysis (PCA) to decorrelate local features, the PCA is\napplied to the entire training data and hence it only diagonalizes the\n\\textit{universal} covariance matrix, rather than those w.r.t. the local\ncomponents. As a result, the local decorrelation assumption is usually not\nsupported in practice.\n  To relax this assumption, this paper proposes a completed model of the Fisher\nvector, which is termed as the Completed Fisher vector (CFV). The CFV is a more\ngeneral framework of the FV, since it encodes not only the variances but also\nthe correlations of the whitened local descriptors. The CFV thus leads to\nimproved discriminative power. We take the task of material categorization as\nan example and experimentally show that: 1) the CFV outperforms the FV under\nall parameter settings; 2) the CFV is robust to the changes in the number of\ncomponents in the mixture; 3) even with a relatively small visual vocabulary\nthe CFV still works well on two challenging datasets. \n\n"}
{"id": "1604.04808", "contents": "Title: Learning Models for Actions and Person-Object Interactions with Transfer\n  to Question Answering Abstract: This paper proposes deep convolutional network models that utilize local and\nglobal context to make human activity label predictions in still images,\nachieving state-of-the-art performance on two recent datasets with hundreds of\nlabels each. We use multiple instance learning to handle the lack of\nsupervision on the level of individual person instances, and weighted loss to\nhandle unbalanced training data. Further, we show how specialized features\ntrained on these datasets can be used to improve accuracy on the Visual\nQuestion Answering (VQA) task, in the form of multiple choice fill-in-the-blank\nquestions (Visual Madlibs). Specifically, we tackle two types of questions on\nperson activity and person-object relationship and show improvements over\ngeneric features trained on the ImageNet classification task. \n\n"}
{"id": "1604.06626", "contents": "Title: The Mean Partition Theorem of Consensus Clustering Abstract: To devise efficient solutions for approximating a mean partition in consensus\nclustering, Dimitriadou et al. [3] presented a necessary condition of\noptimality for a consensus function based on least square distances. We show\nthat their result is pivotal for deriving interesting properties of consensus\nclustering beyond optimization. For this, we present the necessary condition of\noptimality in a slightly stronger form in terms of the Mean Partition Theorem\nand extend it to the Expected Partition Theorem. To underpin its versatility,\nwe show three examples that apply the Mean Partition Theorem: (i) equivalence\nof the mean partition and optimal multiple alignment, (ii) construction of\nprofiles and motifs, and (iii) relationship between consensus clustering and\ncluster stability. \n\n"}
{"id": "1604.08220", "contents": "Title: Diving deeper into mentee networks Abstract: Modern computer vision is all about the possession of powerful image\nrepresentations. Deeper and deeper convolutional neural networks have been\nbuilt using larger and larger datasets and are made publicly available. A large\nswath of computer vision scientists use these pre-trained networks with varying\ndegrees of successes in various tasks. Even though there is tremendous success\nin copying these networks, the representational space is not learnt from the\ntarget dataset in a traditional manner. One of the reasons for opting to use a\npre-trained network over a network learnt from scratch is that small datasets\nprovide less supervision and require meticulous regularization, smaller and\ncareful tweaking of learning rates to even achieve stable learning without\nweight explosion. It is often the case that large deep networks are not\nportable, which necessitates the ability to learn mid-sized networks from\nscratch.\n  In this article, we dive deeper into training these mid-sized networks on\nsmall datasets from scratch by drawing additional supervision from a large\npre-trained network. Such learning also provides better generalization\naccuracies than networks trained with common regularization techniques such as\nl2, l1 and dropouts. We show that features learnt thus, are more general than\nthose learnt independently. We studied various characteristics of such networks\nand found some interesting behaviors. \n\n"}
{"id": "1605.00052", "contents": "Title: InterActive: Inter-Layer Activeness Propagation Abstract: An increasing number of computer vision tasks can be tackled with deep\nfeatures, which are the intermediate outputs of a pre-trained Convolutional\nNeural Network. Despite the astonishing performance, deep features extracted\nfrom low-level neurons are still below satisfaction, arguably because they\ncannot access the spatial context contained in the higher layers. In this\npaper, we present InterActive, a novel algorithm which computes the activeness\nof neurons and network connections. Activeness is propagated through a neural\nnetwork in a top-down manner, carrying high-level context and improving the\ndescriptive power of low-level and mid-level neurons. Visualization indicates\nthat neuron activeness can be interpreted as spatial-weighted neuron responses.\nWe achieve state-of-the-art classification performance on a wide range of image\ndatasets. \n\n"}
{"id": "1605.00743", "contents": "Title: Learning Attributes Equals Multi-Source Domain Generalization Abstract: Attributes possess appealing properties and benefit many computer vision\nproblems, such as object recognition, learning with humans in the loop, and\nimage retrieval. Whereas the existing work mainly pursues utilizing attributes\nfor various computer vision problems, we contend that the most basic\nproblem---how to accurately and robustly detect attributes from images---has\nbeen left under explored. Especially, the existing work rarely explicitly\ntackles the need that attribute detectors should generalize well across\ndifferent categories, including those previously unseen. Noting that this is\nanalogous to the objective of multi-source domain generalization, if we treat\neach category as a domain, we provide a novel perspective to attribute\ndetection and propose to gear the techniques in multi-source domain\ngeneralization for the purpose of learning cross-category generalizable\nattribute detectors. We validate our understanding and approach with extensive\nexperiments on four challenging datasets and three different problems. \n\n"}
{"id": "1605.01042", "contents": "Title: Hierarchical Bayesian Noise Inference for Robust Real-time Probabilistic\n  Object Classification Abstract: Robust environment perception is essential for decision-making on robots\noperating in complex domains. Principled treatment of uncertainty sources in a\nrobot's observation model is necessary for accurate mapping and object\ndetection. This is important not only for low-level observations (e.g.,\naccelerometer data), but for high-level observations such as semantic object\nlabels as well. This paper presents an approach for filtering sequences of\nobject classification probabilities using online modeling of the noise\ncharacteristics of the classifier outputs. A hierarchical Bayesian approach is\nused to model per-class noise distributions, while simultaneously allowing\nsharing of high-level noise characteristics between classes. The proposed\nfiltering scheme, called Hierarchical Bayesian Noise Inference (HBNI), is shown\nto outperform classification accuracy of existing methods. The paper also\npresents real-time filtered classification hardware experiments running fully\nonboard a moving quadrotor, where the proposed approach is demonstrated to work\nin a challenging domain where noise-agnostic filtering fails. \n\n"}
{"id": "1605.01988", "contents": "Title: LSTM with Working Memory Abstract: Previous RNN architectures have largely been superseded by LSTM, or \"Long\nShort-Term Memory\". Since its introduction, there have been many variations on\nthis simple design. However, it is still widely used and we are not aware of a\ngated-RNN architecture that outperforms LSTM in a broad sense while still being\nas simple and efficient. In this paper we propose a modified LSTM-like\narchitecture. Our architecture is still simple and achieves better performance\non the tasks that we tested on. We also introduce a new RNN performance\nbenchmark that uses the handwritten digits and stresses several important\nnetwork capabilities. \n\n"}
{"id": "1605.04068", "contents": "Title: Fast Semantic Image Segmentation with High Order Context and Guided\n  Filtering Abstract: This paper describes a fast and accurate semantic image segmentation approach\nthat encodes not only the discriminative features from deep neural networks,\nbut also the high-order context compatibility among adjacent objects as well as\nlow level image features. We formulate the underlying problem as the\nconditional random field that embeds local feature extraction, clique potential\nconstruction, and guided filtering within the same framework, and provide an\nefficient coarse-to-fine solver. At the coarse level, we combine local feature\nrepresentation and context interaction using a deep convolutional network, and\ndirectly learn the interaction from high order cliques with a message passing\nroutine, avoiding time-consuming explicit graph inference for joint probability\ndistribution. At the fine level, we introduce a guided filtering interpretation\nfor the mean field algorithm, and achieve accurate object boundaries with 100+\nfaster than classic learning methods. The two parts are connected and jointly\ntrained in an end-to-end fashion. Experimental results on Pascal VOC 2012\ndataset have shown that the proposed algorithm outperforms the\nstate-of-the-art, and that it achieves the rank 1 performance at the time of\nsubmission, both of which prove the effectiveness of this unified framework for\nsemantic image segmentation. \n\n"}
{"id": "1605.04711", "contents": "Title: Ternary Weight Networks Abstract: We present a memory and computation efficient ternary weight networks (TWNs)\n- with weights constrained to +1, 0 and -1. The Euclidian distance between full\n(float or double) precision weights and the ternary weights along with a\nscaling factor is minimized in training stage. Besides, a threshold-based\nternary function is optimized to get an approximated solution which can be fast\nand easily computed. TWNs have shown better expressive abilities than binary\nprecision counterparts. Meanwhile, TWNs achieve up to 16$\\times$ model\ncompression rate and need fewer multiplications compared with the float32\nprecision counterparts. Extensive experiments on MNIST, CIFAR-10, and ImageNet\ndatasets show that the TWNs achieve much better result than the\nBinary-Weight-Networks (BWNs) and the classification performance on MNIST and\nCIFAR-10 is very close to the full precision networks. We also verify our\nmethod on object detection task and show that TWNs significantly outperforms\nBWN by more than 10\\% mAP on PASCAL VOC dataset. The pytorch version of source\ncode is available at: https://github.com/Thinklab-SJTU/twns. \n\n"}
{"id": "1605.06465", "contents": "Title: Swapout: Learning an ensemble of deep architectures Abstract: We describe Swapout, a new stochastic training method, that outperforms\nResNets of identical network structure yielding impressive results on CIFAR-10\nand CIFAR-100. Swapout samples from a rich set of architectures including\ndropout, stochastic depth and residual architectures as special cases. When\nviewed as a regularization method swapout not only inhibits co-adaptation of\nunits in a layer, similar to dropout, but also across network layers. We\nconjecture that swapout achieves strong regularization by implicitly tying the\nparameters across layers. When viewed as an ensemble training method, it\nsamples a much richer set of architectures than existing methods such as\ndropout or stochastic depth. We propose a parameterization that reveals\nconnections to exiting architectures and suggests a much richer set of\narchitectures to be explored. We show that our formulation suggests an\nefficient training method and validate our conclusions on CIFAR-10 and\nCIFAR-100 matching state of the art accuracy. Remarkably, our 32 layer wider\nmodel performs similar to a 1001 layer ResNet model. \n\n"}
{"id": "1605.06560", "contents": "Title: Functional Hashing for Compressing Neural Networks Abstract: As the complexity of deep neural networks (DNNs) trend to grow to absorb the\nincreasing sizes of data, memory and energy consumption has been receiving more\nand more attentions for industrial applications, especially on mobile devices.\nThis paper presents a novel structure based on functional hashing to compress\nDNNs, namely FunHashNN. For each entry in a deep net, FunHashNN uses multiple\nlow-cost hash functions to fetch values in the compression space, and then\nemploys a small reconstruction network to recover that entry. The\nreconstruction network is plugged into the whole network and trained jointly.\nFunHashNN includes the recently proposed HashedNets as a degenerated case, and\nbenefits from larger value capacity and less reconstruction loss. We further\ndiscuss extensions with dual space hashing and multi-hops. On several benchmark\ndatasets, FunHashNN demonstrates high compression ratios with little loss on\nprediction accuracy. \n\n"}
{"id": "1605.06597", "contents": "Title: Adaptive Algorithm and Platform Selection for Visual Detection and\n  Tracking Abstract: Computer vision algorithms are known to be extremely sensitive to the\nenvironmental conditions in which the data is captured, e.g., lighting\nconditions and target density. Tuning of parameters or choosing a completely\nnew algorithm is often needed to achieve a certain performance level,\nespecially when there is a limitation of the computation source. In this paper,\nwe focus on this problem and propose a framework to adaptively select the\n\"best\" algorithm-parameter combination and the computation platform under\nperformance and cost constraints at design time, and adapt the algorithms at\nruntime based on real-time inputs. This necessitates developing a mechanism to\nswitch between different algorithms as the nature of the input video changes.\nOur proposed algorithm calculates a similarity function between a test video\nscenario and each training scenario, where the similarity calculation is based\non learning a manifold of image features that is shared by both the training\nand test datasets. Similarity between training and test dataset indicates the\nsame algorithm can be applied to both of them and achieve similar performance.\nWe design a cost function with this similarity measure to find the most similar\ntraining scenario to the test data. The \"best\" algorithm under a given platform\nis obtained by selecting the algorithm with a specific parameter combination\nthat performs the best on the corresponding training data. The proposed\nframework can be used first offline to choose the platform based on performance\nand cost constraints, and then online whereby the \"best\" algorithm is selected\nfor each new incoming video segment for a given platform. In the experiments,\nwe apply our algorithm to the problems of pedestrian detection and tracking. We\nshow how to adaptively select platforms and algorithm-parameter combinations.\nOur results provide optimal performance on 3 publicly available datasets. \n\n"}
{"id": "1605.07145", "contents": "Title: On Optimality Conditions for Auto-Encoder Signal Recovery Abstract: Auto-Encoders are unsupervised models that aim to learn patterns from\nobserved data by minimizing a reconstruction cost. The useful representations\nlearned are often found to be sparse and distributed. On the other hand,\ncompressed sensing and sparse coding assume a data generating process, where\nthe observed data is generated from some true latent signal source, and try to\nrecover the corresponding signal from measurements. Looking at auto-encoders\nfrom this \\textit{signal recovery perspective} enables us to have a more\ncoherent view of these techniques. In this paper, in particular, we show that\nthe \\textit{true} hidden representation can be approximately recovered if the\nweight matrices are highly incoherent with unit $ \\ell^{2} $ row length and the\nbias vectors takes the value (approximately) equal to the negative of the data\nmean. The recovery also becomes more and more accurate as the sparsity in\nhidden signals increases. Additionally, we empirically demonstrate that\nauto-encoders are capable of recovering the data generating dictionary when\nonly data samples are given. \n\n"}
{"id": "1605.08512", "contents": "Title: SNN: Stacked Neural Networks Abstract: It has been proven that transfer learning provides an easy way to achieve\nstate-of-the-art accuracies on several vision tasks by training a simple\nclassifier on top of features obtained from pre-trained neural networks. The\ngoal of this work is to generate better features for transfer learning from\nmultiple publicly available pre-trained neural networks. To this end, we\npropose a novel architecture called Stacked Neural Networks which leverages the\nfast training time of transfer learning while simultaneously being much more\naccurate. We show that using a stacked NN architecture can result in up to 8%\nimprovements in accuracy over state-of-the-art techniques using only one\npre-trained network for transfer learning. A second aim of this work is to make\nnetwork fine- tuning retain the generalizability of the base network to unseen\ntasks. To this end, we propose a new technique called \"joint fine-tuning\" that\nis able to give accuracies comparable to finetuning the same network\nindividually over two datasets. We also show that a jointly finetuned network\ngeneralizes better to unseen tasks when compared to a network finetuned over a\nsingle task. \n\n"}
{"id": "1605.09332", "contents": "Title: Parametric Exponential Linear Unit for Deep Convolutional Neural\n  Networks Abstract: Object recognition is an important task for improving the ability of visual\nsystems to perform complex scene understanding. Recently, the Exponential\nLinear Unit (ELU) has been proposed as a key component for managing bias shift\nin Convolutional Neural Networks (CNNs), but defines a parameter that must be\nset by hand. In this paper, we propose learning a parameterization of ELU in\norder to learn the proper activation shape at each layer in the CNNs. Our\nresults on the MNIST, CIFAR-10/100 and ImageNet datasets using the NiN,\nOverfeat, All-CNN and ResNet networks indicate that our proposed Parametric ELU\n(PELU) has better performances than the non-parametric ELU. We have observed as\nmuch as a 7.28% relative error improvement on ImageNet with the NiN network,\nwith only 0.0003% parameter increase. Our visual examination of the non-linear\nbehaviors adopted by Vgg using PELU shows that the network took advantage of\nthe added flexibility by learning different activations at different layers. \n\n"}
{"id": "1605.09759", "contents": "Title: Fast Zero-Shot Image Tagging Abstract: The well-known word analogy experiments show that the recent word vectors\ncapture fine-grained linguistic regularities in words by linear vector offsets,\nbut it is unclear how well the simple vector offsets can encode visual\nregularities over words. We study a particular image-word relevance relation in\nthis paper. Our results show that the word vectors of relevant tags for a given\nimage rank ahead of the irrelevant tags, along a principal direction in the\nword vector space. Inspired by this observation, we propose to solve image\ntagging by estimating the principal direction for an image. Particularly, we\nexploit linear mappings and nonlinear deep neural networks to approximate the\nprincipal direction from an input image. We arrive at a quite versatile tagging\nmodel. It runs fast given a test image, in constant time w.r.t.\\ the training\nset size. It not only gives superior performance for the conventional tagging\ntask on the NUS-WIDE dataset, but also outperforms competitive baselines on\nannotating images with previously unseen tags \n\n"}
{"id": "1606.00021", "contents": "Title: Texture Synthesis Using Shallow Convolutional Networks with Random\n  Filters Abstract: Here we demonstrate that the feature space of random shallow convolutional\nneural networks (CNNs) can serve as a surprisingly good model of natural\ntextures. Patches from the same texture are consistently classified as being\nmore similar then patches from different textures. Samples synthesized from the\nmodel capture spatial correlations on scales much larger then the receptive\nfield size, and sometimes even rival or surpass the perceptual quality of state\nof the art texture models (but show less variability). The current state of the\nart in parametric texture synthesis relies on the multi-layer feature space of\ndeep CNNs that were trained on natural images. Our finding suggests that such\noptimized multi-layer feature spaces are not imperative for texture modeling.\nInstead, much simpler shallow and convolutional networks can serve as the basis\nfor novel texture synthesis algorithms. \n\n"}
{"id": "1606.00487", "contents": "Title: Recurrent Fully Convolutional Networks for Video Segmentation Abstract: Image segmentation is an important step in most visual tasks. While\nconvolutional neural networks have shown to perform well on single image\nsegmentation, to our knowledge, no study has been been done on leveraging\nrecurrent gated architectures for video segmentation. Accordingly, we propose a\nnovel method for online segmentation of video sequences that incorporates\ntemporal data. The network is built from fully convolutional element and\nrecurrent unit that works on a sliding window over the temporal data. We also\nintroduce a novel convolutional gated recurrent unit that preserves the spatial\ninformation and reduces the parameters learned. Our method has the advantage\nthat it can work in an online fashion instead of operating over the whole input\nbatch of video frames. The network is tested on the change detection dataset,\nand proved to have 5.5\\% improvement in F-measure over a plain fully\nconvolutional network for per frame segmentation. It was also shown to have\nimprovement of 1.4\\% for the F-measure compared to our baseline network that we\ncall FCN 12s. \n\n"}
{"id": "1606.02280", "contents": "Title: Semi-Supervised Domain Adaptation for Weakly Labeled Semantic Video\n  Object Segmentation Abstract: Deep convolutional neural networks (CNNs) have been immensely successful in\nmany high-level computer vision tasks given large labeled datasets. However,\nfor video semantic object segmentation, a domain where labels are scarce,\neffectively exploiting the representation power of CNN with limited training\ndata remains a challenge. Simply borrowing the existing pretrained CNN image\nrecognition model for video segmentation task can severely hurt performance. We\npropose a semi-supervised approach to adapting CNN image recognition model\ntrained from labeled image data to the target domain exploiting both semantic\nevidence learned from CNN, and the intrinsic structures of video data. By\nexplicitly modeling and compensating for the domain shift from the source\ndomain to the target domain, this proposed approach underpins a robust semantic\nobject segmentation method against the changes in appearance, shape and\nocclusion in natural videos. We present extensive experiments on challenging\ndatasets that demonstrate the superior performance of our approach compared\nwith the state-of-the-art methods. \n\n"}
{"id": "1606.02894", "contents": "Title: A Comprehensive Analysis of Deep Learning Based Representation for Face\n  Recognition Abstract: Deep learning based approaches have been dominating the face recognition\nfield due to the significant performance improvement they have provided on the\nchallenging wild datasets. These approaches have been extensively tested on\nsuch unconstrained datasets, on the Labeled Faces in the Wild and YouTube\nFaces, to name a few. However, their capability to handle individual appearance\nvariations caused by factors such as head pose, illumination, occlusion, and\nmisalignment has not been thoroughly assessed till now. In this paper, we\npresent a comprehensive study to evaluate the performance of deep learning\nbased face representation under several conditions including the varying head\npose angles, upper and lower face occlusion, changing illumination of different\nstrengths, and misalignment due to erroneous facial feature localization. Two\nsuccessful and publicly available deep learning models, namely VGG-Face and\nLightened CNN have been utilized to extract face representations. The obtained\nresults show that although deep learning provides a powerful representation for\nface recognition, it can still benefit from preprocessing, for example, for\npose and illumination normalization in order to achieve better performance\nunder various conditions. Particularly, if these variations are not included in\nthe dataset used to train the deep learning model, the role of preprocessing\nbecomes more crucial. Experimental results also show that deep learning based\nrepresentation is robust to misalignment and can tolerate facial feature\nlocalization errors up to 10% of the interocular distance. \n\n"}
{"id": "1606.03498", "contents": "Title: Improved Techniques for Training GANs Abstract: We present a variety of new architectural features and training procedures\nthat we apply to the generative adversarial networks (GANs) framework. We focus\non two applications of GANs: semi-supervised learning, and the generation of\nimages that humans find visually realistic. Unlike most work on generative\nmodels, our primary goal is not to train a model that assigns high likelihood\nto test data, nor do we require the model to be able to learn well without\nusing any labels. Using our new techniques, we achieve state-of-the-art results\nin semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated\nimages are of high quality as confirmed by a visual Turing test: our model\ngenerates MNIST samples that humans cannot distinguish from real data, and\nCIFAR-10 samples that yield a human error rate of 21.3%. We also present\nImageNet samples with unprecedented resolution and show that our methods enable\nthe model to learn recognizable features of ImageNet classes. \n\n"}
{"id": "1606.03864", "contents": "Title: Neural Associative Memory for Dual-Sequence Modeling Abstract: Many important NLP problems can be posed as dual-sequence or\nsequence-to-sequence modeling tasks. Recent advances in building end-to-end\nneural architectures have been highly successful in solving such tasks. In this\nwork we propose a new architecture for dual-sequence modeling that is based on\nassociative memory. We derive AM-RNNs, a recurrent associative memory (AM)\nwhich augments generic recurrent neural networks (RNN). This architecture is\nextended to the Dual AM-RNN which operates on two AMs at once. Our models\nachieve very competitive results on textual entailment. A qualitative analysis\ndemonstrates that long range dependencies between source and target-sequence\ncan be bridged effectively using Dual AM-RNNs. However, an initial experiment\non auto-encoding reveals that these benefits are not exploited by the system\nwhen learning to solve sequence-to-sequence tasks which indicates that\nadditional supervision or regularization is needed. \n\n"}
{"id": "1606.04422", "contents": "Title: Logic Tensor Networks: Deep Learning and Logical Reasoning from Data and\n  Knowledge Abstract: We propose Logic Tensor Networks: a uniform framework for integrating\nautomatic learning and reasoning. A logic formalism called Real Logic is\ndefined on a first-order language whereby formulas have truth-value in the\ninterval [0,1] and semantics defined concretely on the domain of real numbers.\nLogical constants are interpreted as feature vectors of real numbers. Real\nLogic promotes a well-founded integration of deductive reasoning on a\nknowledge-base and efficient data-driven relational machine learning. We show\nhow Real Logic can be implemented in deep Tensor Neural Networks with the use\nof Google's tensorflow primitives. The paper concludes with experiments\napplying Logic Tensor Networks on a simple but representative example of\nknowledge completion. \n\n"}
{"id": "1606.05784", "contents": "Title: Hitting times of local and global optima in genetic algorithms with very\n  high selection pressure Abstract: The paper is devoted to upper bounds on the expected first hitting times of\nthe sets of local or global optima for non-elitist genetic algorithms with very\nhigh selection pressure. The results of this paper extend the range of\nsituations where the upper bounds on the expected runtime are known for genetic\nalgorithms and apply, in particular, to the Canonical Genetic Algorithm. The\nobtained bounds do not require the probability of fitness-decreasing mutation\nto be bounded by a constant less than one. \n\n"}
{"id": "1606.06160", "contents": "Title: DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low\n  Bitwidth Gradients Abstract: We propose DoReFa-Net, a method to train convolutional neural networks that\nhave low bitwidth weights and activations using low bitwidth parameter\ngradients. In particular, during backward pass, parameter gradients are\nstochastically quantized to low bitwidth numbers before being propagated to\nconvolutional layers. As convolutions during forward/backward passes can now\noperate on low bitwidth weights and activations/gradients respectively,\nDoReFa-Net can use bit convolution kernels to accelerate both training and\ninference. Moreover, as bit convolutions can be efficiently implemented on CPU,\nFPGA, ASIC and GPU, DoReFa-Net opens the way to accelerate training of low\nbitwidth neural network on these hardware. Our experiments on SVHN and ImageNet\ndatasets prove that DoReFa-Net can achieve comparable prediction accuracy as\n32-bit counterparts. For example, a DoReFa-Net derived from AlexNet that has\n1-bit weights, 2-bit activations, can be trained from scratch using 6-bit\ngradients to get 46.1\\% top-1 accuracy on ImageNet validation set. The\nDoReFa-Net AlexNet model is released publicly. \n\n"}
{"id": "1606.06871", "contents": "Title: A Comprehensive Study of Deep Bidirectional LSTM RNNs for Acoustic\n  Modeling in Speech Recognition Abstract: We present a comprehensive study of deep bidirectional long short-term memory\n(LSTM) recurrent neural network (RNN) based acoustic models for automatic\nspeech recognition (ASR). We study the effect of size and depth and train\nmodels of up to 8 layers. We investigate the training aspect and study\ndifferent variants of optimization methods, batching, truncated\nbackpropagation, different regularization techniques such as dropout and $L_2$\nregularization, and different gradient clipping variants.\n  The major part of the experimental analysis was performed on the Quaero\ncorpus. Additional experiments also were performed on the Switchboard corpus.\nOur best LSTM model has a relative improvement in word error rate of over 14\\%\ncompared to our best feed-forward neural network (FFNN) baseline on the Quaero\ntask. On this task, we get our best result with an 8 layer bidirectional LSTM\nand we show that a pretraining scheme with layer-wise construction helps for\ndeep LSTMs.\n  Finally we compare the training calculation time of many of the presented\nexperiments in relation with recognition performance.\n  All the experiments were done with RETURNN, the RWTH extensible training\nframework for universal recurrent neural networks in combination with RASR, the\nRWTH ASR toolkit. \n\n"}
{"id": "1607.02643", "contents": "Title: Hierarchical Deep Temporal Models for Group Activity Recognition Abstract: In this paper we present an approach for classifying the activity performed\nby a group of people in a video sequence. This problem of group activity\nrecognition can be addressed by examining individual person actions and their\nrelations. Temporal dynamics exist both at the level of individual person\nactions as well as at the level of group activity. Given a video sequence as\ninput, methods can be developed to capture these dynamics at both person-level\nand group-level detail. We build a deep model to capture these dynamics based\non LSTM (long short-term memory) models. In order to model both person-level\nand group-level dynamics, we present a 2-stage deep temporal model for the\ngroup activity recognition problem. In our approach, one LSTM model is designed\nto represent action dynamics of individual people in a video sequence and\nanother LSTM model is designed to aggregate person-level information for group\nactivity recognition. We collected a new dataset consisting of volleyball\nvideos labeled with individual and group activities in order to evaluate our\nmethod. Experimental results on this new Volleyball Dataset and the standard\nbenchmark Collective Activity Dataset demonstrate the efficacy of the proposed\nmodels. \n\n"}
{"id": "1607.03682", "contents": "Title: Hierarchical learning for DNN-based acoustic scene classification Abstract: In this paper, we present a deep neural network (DNN)-based acoustic scene\nclassification framework. Two hierarchical learning methods are proposed to\nimprove the DNN baseline performance by incorporating the hierarchical taxonomy\ninformation of environmental sounds. Firstly, the parameters of the DNN are\ninitialized by the proposed hierarchical pre-training. Multi-level objective\nfunction is then adopted to add more constraint on the cross-entropy based loss\nfunction. A series of experiments were conducted on the Task1 of the Detection\nand Classification of Acoustic Scenes and Events (DCASE) 2016 challenge. The\nfinal DNN-based system achieved a 22.9% relative improvement on average scene\nclassification error as compared with the Gaussian Mixture Model (GMM)-based\nbenchmark system across four standard folds. \n\n"}
{"id": "1607.04381", "contents": "Title: DSD: Dense-Sparse-Dense Training for Deep Neural Networks Abstract: Modern deep neural networks have a large number of parameters, making them\nvery hard to train. We propose DSD, a dense-sparse-dense training flow, for\nregularizing deep neural networks and achieving better optimization\nperformance. In the first D (Dense) step, we train a dense network to learn\nconnection weights and importance. In the S (Sparse) step, we regularize the\nnetwork by pruning the unimportant connections with small weights and\nretraining the network given the sparsity constraint. In the final D (re-Dense)\nstep, we increase the model capacity by removing the sparsity constraint,\nre-initialize the pruned parameters from zero and retrain the whole dense\nnetwork. Experiments show that DSD training can improve the performance for a\nwide range of CNNs, RNNs and LSTMs on the tasks of image classification,\ncaption generation and speech recognition. On ImageNet, DSD improved the Top1\naccuracy of GoogLeNet by 1.1%, VGG-16 by 4.3%, ResNet-18 by 1.2% and ResNet-50\nby 1.1%, respectively. On the WSJ'93 dataset, DSD improved DeepSpeech and\nDeepSpeech2 WER by 2.0% and 1.1%. On the Flickr-8K dataset, DSD improved the\nNeuralTalk BLEU score by over 1.7. DSD is easy to use in practice: at training\ntime, DSD incurs only one extra hyper-parameter: the sparsity ratio in the S\nstep. At testing time, DSD doesn't change the network architecture or incur any\ninference overhead. The consistent and significant performance gain of DSD\nexperiments shows the inadequacy of the current training methods for finding\nthe best local optimum, while DSD effectively achieves superior optimization\nperformance for finding a better solution. DSD models are available to download\nat https://songhan.github.io/DSD. \n\n"}
{"id": "1607.08539", "contents": "Title: Fine-To-Coarse Global Registration of RGB-D Scans Abstract: RGB-D scanning of indoor environments is important for many applications,\nincluding real estate, interior design, and virtual reality. However, it is\nstill challenging to register RGB-D images from a hand-held camera over a long\nvideo sequence into a globally consistent 3D model. Current methods often can\nlose tracking or drift and thus fail to reconstruct salient structures in large\nenvironments (e.g., parallel walls in different rooms). To address this\nproblem, we propose a \"fine-to-coarse\" global registration algorithm that\nleverages robust registrations at finer scales to seed detection and\nenforcement of new correspondence and structural constraints at coarser scales.\nTo test global registration algorithms, we provide a benchmark with 10,401\nmanually-clicked point correspondences in 25 scenes from the SUN3D dataset.\nDuring experiments with this benchmark, we find that our fine-to-coarse\nalgorithm registers long RGB-D sequences better than previous methods. \n\n"}
{"id": "1608.00641", "contents": "Title: Interactive Image Segmentation Using Constrained Dominant Sets Abstract: We propose a new approach to interactive image segmentation based on some\nproperties of a family of quadratic optimization problems related to dominant\nsets, a well-known graph-theoretic notion of a cluster which generalizes the\nconcept of a maximal clique to edge-weighted graphs. In particular, we show\nthat by properly controlling a regularization parameter which determines the\nstructure and the scale of the underlying problem, we are in a position to\nextract groups of dominant-set clusters which are constrained to contain\nuser-selected elements. The resulting algorithm can deal naturally with any\ntype of input modality, including scribbles, sloppy contours, and bounding\nboxes, and is able to robustly handle noisy annotations on the part of the\nuser. Experiments on standard benchmark datasets show the effectiveness of our\napproach as compared to state-of-the-art algorithms on a variety of natural\nimages under several input conditions. \n\n"}
{"id": "1608.02292", "contents": "Title: Online Adaptation of Deep Architectures with Reinforcement Learning Abstract: Online learning has become crucial to many problems in machine learning. As\nmore data is collected sequentially, quickly adapting to changes in the data\ndistribution can offer several competitive advantages such as avoiding loss of\nprior knowledge and more efficient learning. However, adaptation to changes in\nthe data distribution (also known as covariate shift) needs to be performed\nwithout compromising past knowledge already built in into the model to cope\nwith voluminous and dynamic data. In this paper, we propose an online stacked\nDenoising Autoencoder whose structure is adapted through reinforcement\nlearning. Our algorithm forces the network to exploit and explore favourable\narchitectures employing an estimated utility function that maximises the\naccuracy of an unseen validation sequence. Different actions, such as Pool,\nIncrement and Merge are available to modify the structure of the network. As we\nobserve through a series of experiments, our approach is more responsive,\nrobust, and principled than its counterparts for non-stationary as well as\nstationary data distributions. Experimental results indicate that our algorithm\nperforms better at preserving gained prior knowledge and responding to changes\nin the data distribution. \n\n"}
{"id": "1608.03667", "contents": "Title: Reasoning and Algorithm Selection Augmented Symbolic Segmentation Abstract: In this paper we present an alternative method to symbolic segmentation: we\napproach symbolic segmentation as an algorithm selection problem. That is, let\nthere be a set A of available algorithms for symbolic segmentation, a set of\ninput features $F$, a set of image attribute $\\mathbb{A}$ and a selection\nmechanism $S(F,\\mathbb{A},A)$ that selects on a case by case basis the best\nalgorithm. The semantic segmentation is then an optimization process that\ncombines best component segments from multiple results into a single optimal\nresult. The experiments compare three different algorithm selection mechanisms\nusing three selected semantic segmentation algorithms. The results show that\nusing the current state of art algorithms and relatively low accuracy of\nalgorithm selection the accuracy of the semantic segmentation can be improved\nby 2\\%. \n\n"}
{"id": "1608.03793", "contents": "Title: Applying Deep Learning to Basketball Trajectories Abstract: One of the emerging trends for sports analytics is the growing use of player\nand ball tracking data. A parallel development is deep learning predictive\napproaches that use vast quantities of data with less reliance on feature\nengineering. This paper applies recurrent neural networks in the form of\nsequence modeling to predict whether a three-point shot is successful. The\nmodels are capable of learning the trajectory of a basketball without any\nknowledge of physics. For comparison, a baseline static machine learning model\nwith a full set of features, such as angle and velocity, in addition to the\npositional data is also tested. Using a dataset of over 20,000 three pointers\nfrom NBA SportVu data, the models based simply on sequential positional data\noutperform a static feature rich machine learning model in predicting whether a\nthree-point shot is successful. This suggests deep learning models may offer an\nimprovement to traditional feature based machine learning methods for tracking\ndata. \n\n"}
{"id": "1608.03832", "contents": "Title: On Minimal Accuracy Algorithm Selection in Computer Vision and\n  Intelligent Systems Abstract: In this paper we discuss certain theoretical properties of algorithm\nselection approach to image processing and to intelligent system in general. We\nanalyze the theoretical limits of algorithm selection with respect to the\nalgorithm selection accuracy. We show the theoretical formulation of a crisp\nbound on the algorithm selector precision guaranteeing to always obtain better\nthan the best available algorithm result. \n\n"}
{"id": "1608.04064", "contents": "Title: About Pyramid Structure in Convolutional Neural Networks Abstract: Deep convolutional neural networks (CNN) brought revolution without any doubt\nto various challenging tasks, mainly in computer vision. However, their model\ndesigning still requires attention to reduce number of learnable parameters,\nwith no meaningful reduction in performance. In this paper we investigate to\nwhat extend CNN may take advantage of pyramid structure typical of biological\nneurons. A generalized statement over convolutional layers from input till\nfully connected layer is introduced that helps further in understanding and\ndesigning a successful deep network. It reduces ambiguity, number of\nparameters, and their size on disk without degrading overall accuracy.\nPerformance are shown on state-of-the-art models for MNIST, Cifar-10,\nCifar-100, and ImageNet-12 datasets. Despite more than 80% reduction in\nparameters for Caffe_LENET, challenging results are obtained. Further, despite\n10-20% reduction in training data along with 10-40% reduction in parameters for\nAlexNet model and its variations, competitive results are achieved when\ncompared to similar well-engineered deeper architectures. \n\n"}
{"id": "1608.05105", "contents": "Title: Evolutionary Approaches to Optimization Problems in Chimera Topologies Abstract: Chimera graphs define the topology of one of the first commercially available\nquantum computers. A variety of optimization problems have been mapped to this\ntopology to evaluate the behavior of quantum enhanced optimization heuristics\nin relation to other optimizers, being able to efficiently solve problems\nclassically to use them as benchmarks for quantum machines. In this paper we\ninvestigate for the first time the use of Evolutionary Algorithms (EAs) on\nIsing spin glass instances defined on the Chimera topology. Three genetic\nalgorithms (GAs) and three estimation of distribution algorithms (EDAs) are\nevaluated over $1000$ hard instances of the Ising spin glass constructed from\nSidon sets. We focus on determining whether the information about the topology\nof the graph can be used to improve the results of EAs and on identifying the\ncharacteristics of the Ising instances that influence the success rate of GAs\nand EDAs. \n\n"}
{"id": "1608.05203", "contents": "Title: Seeing with Humans: Gaze-Assisted Neural Image Captioning Abstract: Gaze reflects how humans process visual scenes and is therefore increasingly\nused in computer vision systems. Previous works demonstrated the potential of\ngaze for object-centric tasks, such as object localization and recognition, but\nit remains unclear if gaze can also be beneficial for scene-centric tasks, such\nas image captioning. We present a new perspective on gaze-assisted image\ncaptioning by studying the interplay between human gaze and the attention\nmechanism of deep neural networks. Using a public large-scale gaze dataset, we\nfirst assess the relationship between state-of-the-art object and scene\nrecognition models, bottom-up visual saliency, and human gaze. We then propose\na novel split attention model for image captioning. Our model integrates human\ngaze information into an attention-based long short-term memory architecture,\nand allows the algorithm to allocate attention selectively to both fixated and\nnon-fixated image regions. Through evaluation on the COCO/SALICON datasets we\nshow that our method improves image captioning performance and that gaze can\ncomplement machine attention for semantic scene understanding tasks. \n\n"}
{"id": "1608.06019", "contents": "Title: Domain Separation Networks Abstract: The cost of large scale data collection and annotation often makes the\napplication of machine learning algorithms to new tasks or datasets\nprohibitively expensive. One approach circumventing this cost is training\nmodels on synthetic data where annotations are provided automatically. Despite\ntheir appeal, such models often fail to generalize from synthetic to real\nimages, necessitating domain adaptation algorithms to manipulate these models\nbefore they can be successfully applied. Existing approaches focus either on\nmapping representations from one domain to the other, or on learning to extract\nfeatures that are invariant to the domain from which they were extracted.\nHowever, by focusing only on creating a mapping or shared representation\nbetween the two domains, they ignore the individual characteristics of each\ndomain. We suggest that explicitly modeling what is unique to each domain can\nimprove a model's ability to extract domain-invariant features. Inspired by\nwork on private-shared component analysis, we explicitly learn to extract image\nrepresentations that are partitioned into two subspaces: one component which is\nprivate to each domain and one which is shared across domains. Our model is\ntrained not only to perform the task we care about in the source domain, but\nalso to use the partitioned representation to reconstruct the images from both\ndomains. Our novel architecture results in a model that outperforms the\nstate-of-the-art on a range of unsupervised domain adaptation scenarios and\nadditionally produces visualizations of the private and shared representations\nenabling interpretation of the domain adaptation process. \n\n"}
{"id": "1608.07639", "contents": "Title: Learning to generalize to new compositions in image understanding Abstract: Recurrent neural networks have recently been used for learning to describe\nimages using natural language. However, it has been observed that these models\ngeneralize poorly to scenes that were not observed during training, possibly\ndepending too strongly on the statistics of the text in the training data. Here\nwe propose to describe images using short structured representations, aiming to\ncapture the crux of a description. These structured representations allow us to\ntease-out and evaluate separately two types of generalization: standard\ngeneralization to new images with similar scenes, and generalization to new\ncombinations of known entities. We compare two learning approaches on the\nMS-COCO dataset: a state-of-the-art recurrent network based on an LSTM (Show,\nAttend and Tell), and a simple structured prediction model on top of a deep\nnetwork. We find that the structured model generalizes to new compositions\nsubstantially better than the LSTM, ~7 times the accuracy of predicting\nstructured representations. By providing a concrete method to quantify\ngeneralization for unseen combinations, we argue that structured\nrepresentations and compositional splits are a useful benchmark for image\ncaptioning, and advocate compositional models that capture linguistic and\nvisual structure. \n\n"}
{"id": "1609.03396", "contents": "Title: FALCON: Feature Driven Selective Classification for Energy-Efficient\n  Image Recognition Abstract: Machine-learning algorithms have shown outstanding image recognition or\nclassification performance for computer vision applications. However, the\ncompute and energy requirement for implementing such classifier models for\nlarge-scale problems is quite high. In this paper, we propose Feature Driven\nSelective Classification (FALCON) inspired by the biological visual attention\nmechanism in the brain to optimize the energy-efficiency of machine-learning\nclassifiers. We use the consensus in the characteristic features\n(color/texture) across images in a dataset to decompose the original\nclassification problem and construct a tree of classifiers (nodes) with a\ngeneric-to-specific transition in the classification hierarchy. The initial\nnodes of the tree separate the instances based on feature information and\nselectively enable the latter nodes to perform object specific classification.\nThe proposed methodology allows selective activation of only those branches and\nnodes of the classification tree that are relevant to the input while keeping\nthe remaining nodes idle. Additionally, we propose a programmable and scalable\nNeuromorphic Engine (NeuE) that utilizes arrays of specialized neural\ncomputational elements to execute the FALCON based classifier models for\ndiverse datasets. The structure of FALCON facilitates the reuse of nodes while\nscaling up from small classification problems to larger ones thus allowing us\nto construct classifier implementations that are significantly more efficient.\nWe evaluate our approach for a 12-object classification task on the Caltech101\ndataset and 10-object task on CIFAR-10 dataset by constructing FALCON models on\nthe NeuE platform in 45nm technology. Our results demonstrate significant\nimprovement in energy-efficiency and training time for minimal loss in output\nquality. \n\n"}
{"id": "1609.03894", "contents": "Title: Crafting a multi-task CNN for viewpoint estimation Abstract: Convolutional Neural Networks (CNNs) were recently shown to provide\nstate-of-the-art results for object category viewpoint estimation. However\ndifferent ways of formulating this problem have been proposed and the competing\napproaches have been explored with very different design choices. This paper\npresents a comparison of these approaches in a unified setting as well as a\ndetailed analysis of the key factors that impact performance. Followingly, we\npresent a new joint training method with the detection task and demonstrate its\nbenefit. We also highlight the superiority of classification approaches over\nregression approaches, quantify the benefits of deeper architectures and\nextended training data, and demonstrate that synthetic data is beneficial even\nwhen using ImageNet training data. By combining all these elements, we\ndemonstrate an improvement of approximately 5% mAVP over previous\nstate-of-the-art results on the Pascal3D+ dataset. In particular for their most\nchallenging 24 view classification task we improve the results from 31.1% to\n36.1% mAVP. \n\n"}
{"id": "1609.04453", "contents": "Title: A Large Contextual Dataset for Classification, Detection and Counting of\n  Cars with Deep Learning Abstract: We have created a large diverse set of cars from overhead images, which are\nuseful for training a deep learner to binary classify, detect and count them.\nThe dataset and all related material will be made publically available. The set\ncontains contextual matter to aid in identification of difficult targets. We\ndemonstrate classification and detection on this dataset using a neural network\nwe call ResCeption. This network combines residual learning with\nInception-style layers and is used to count cars in one look. This is a new way\nto count objects rather than by localization or density estimation. It is\nfairly accurate, fast and easy to implement. Additionally, the counting method\nis not car or scene specific. It would be easy to train this method to count\nother kinds of objects and counting over new scenes requires no extra set up or\nassumptions about object locations. \n\n"}
{"id": "1609.05284", "contents": "Title: ReasoNet: Learning to Stop Reading in Machine Comprehension Abstract: Teaching a computer to read and answer general questions pertaining to a\ndocument is a challenging yet unsolved problem. In this paper, we describe a\nnovel neural network architecture called the Reasoning Network (ReasoNet) for\nmachine comprehension tasks. ReasoNets make use of multiple turns to\neffectively exploit and then reason over the relation among queries, documents,\nand answers. Different from previous approaches using a fixed number of turns\nduring inference, ReasoNets introduce a termination state to relax this\nconstraint on the reasoning depth. With the use of reinforcement learning,\nReasoNets can dynamically determine whether to continue the comprehension\nprocess after digesting intermediate results, or to terminate reading when it\nconcludes that existing information is adequate to produce an answer. ReasoNets\nhave achieved exceptional performance in machine comprehension datasets,\nincluding unstructured CNN and Daily Mail datasets, the Stanford SQuAD dataset,\nand a structured Graph Reachability dataset. \n\n"}
{"id": "1609.07093", "contents": "Title: Neural Photo Editing with Introspective Adversarial Networks Abstract: The increasingly photorealistic sample quality of generative image models\nsuggests their feasibility in applications beyond image generation. We present\nthe Neural Photo Editor, an interface that leverages the power of generative\nneural networks to make large, semantically coherent changes to existing\nimages. To tackle the challenge of achieving accurate reconstructions without\nloss of feature quality, we introduce the Introspective Adversarial Network, a\nnovel hybridization of the VAE and GAN. Our model efficiently captures\nlong-range dependencies through use of a computational block based on\nweight-shared dilated convolutions, and improves generalization performance\nwith Orthogonal Regularization, a novel weight regularization method. We\nvalidate our contributions on CelebA, SVHN, and CIFAR-100, and produce samples\nand reconstructions with high visual fidelity. \n\n"}
{"id": "1609.09018", "contents": "Title: Deep Architectures for Face Attributes Abstract: We train a deep convolutional neural network to perform identity\nclassification using a new dataset of public figures annotated with age,\ngender, ethnicity and emotion labels, and then fine-tune it for attribute\nclassification. An optimal sharing pattern of computational resources within\nthis network is determined by experiment, requiring only 1 G flops to produce\nall predictions. Rather than fine-tune by relearning weights in one additional\nlayer after the penultimate layer of the identity network, we try several\ndifferent depths for each attribute. We find that prediction of age and emotion\nis improved by fine-tuning from earlier layers onward, presumably because\ndeeper layers are progressively invariant to non-identity related changes in\nthe input. \n\n"}
{"id": "1609.09475", "contents": "Title: Multi-view Self-supervised Deep Learning for 6D Pose Estimation in the\n  Amazon Picking Challenge Abstract: Robot warehouse automation has attracted significant interest in recent\nyears, perhaps most visibly in the Amazon Picking Challenge (APC). A fully\nautonomous warehouse pick-and-place system requires robust vision that reliably\nrecognizes and locates objects amid cluttered environments, self-occlusions,\nsensor noise, and a large variety of objects. In this paper we present an\napproach that leverages multi-view RGB-D data and self-supervised, data-driven\nlearning to overcome those difficulties. The approach was part of the\nMIT-Princeton Team system that took 3rd- and 4th- place in the stowing and\npicking tasks, respectively at APC 2016. In the proposed approach, we segment\nand label multiple views of a scene with a fully convolutional neural network,\nand then fit pre-scanned 3D object models to the resulting segmentation to get\nthe 6D object pose. Training a deep neural network for segmentation typically\nrequires a large amount of training data. We propose a self-supervised method\nto generate a large labeled dataset without tedious manual segmentation. We\ndemonstrate that our system can reliably estimate the 6D pose of objects under\na variety of scenarios. All code, data, and benchmarks are available at\nhttp://apc.cs.princeton.edu/ \n\n"}
{"id": "1610.00824", "contents": "Title: Real Time Fine-Grained Categorization with Accuracy and Interpretability Abstract: A well-designed fine-grained categorization system usually has three\ncontradictory requirements: accuracy (the ability to identify objects among\nsubordinate categories); interpretability (the ability to provide\nhuman-understandable explanation of recognition system behavior); and\nefficiency (the speed of the system). To handle the trade-off between accuracy\nand interpretability, we propose a novel \"Deeper Part-Stacked CNN\" architecture\narmed with interpretability by modeling subtle differences between object\nparts. The proposed architecture consists of a part localization network, a\ntwo-stream classification network that simultaneously encodes object-level and\npart-level cues, and a feature vectors fusion component. Specifically, the part\nlocalization network is implemented by exploring a new paradigm for key point\nlocalization that first samples a small number of representable pixels and then\ndetermine their labels via a convolutional layer followed by a softmax layer.\nWe also use a cropping layer to extract part features and propose a scale\nmean-max layer for feature fusion learning. Experimentally, our proposed method\noutperform state-of-the-art approaches both in part localization task and\nclassification task on Caltech-UCSD Birds-200-2011. Moreover, by adopting a set\nof sharing strategies between the computation of multiple object parts, our\nsingle model is fairly efficient running at 32 frames/sec. \n\n"}
{"id": "1610.01206", "contents": "Title: A Survey of Multi-View Representation Learning Abstract: Recently, multi-view representation learning has become a rapidly growing\ndirection in machine learning and data mining areas. This paper introduces two\ncategories for multi-view representation learning: multi-view representation\nalignment and multi-view representation fusion. Consequently, we first review\nthe representative methods and theories of multi-view representation learning\nbased on the perspective of alignment, such as correlation-based alignment.\nRepresentative examples are canonical correlation analysis (CCA) and its\nseveral extensions. Then from the perspective of representation fusion we\ninvestigate the advancement of multi-view representation learning that ranges\nfrom generative methods including multi-modal topic learning, multi-view sparse\ncoding, and multi-view latent space Markov networks, to neural network-based\nmethods including multi-modal autoencoders, multi-view convolutional neural\nnetworks, and multi-modal recurrent neural networks. Further, we also\ninvestigate several important applications of multi-view representation\nlearning. Overall, this survey aims to provide an insightful overview of\ntheoretical foundation and state-of-the-art developments in the field of\nmulti-view representation learning and to help researchers find the most\nappropriate tools for particular applications. \n\n"}
{"id": "1610.02256", "contents": "Title: ILGNet: Inception Modules with Connected Local and Global Features for\n  Efficient Image Aesthetic Quality Classification using Domain Adaptation Abstract: In this paper, we address a challenging problem of aesthetic image\nclassification, which is to label an input image as high or low aesthetic\nquality. We take both the local and global features of images into\nconsideration. A novel deep convolutional neural network named ILGNet is\nproposed, which combines both the Inception modules and an connected layer of\nboth Local and Global features. The ILGnet is based on GoogLeNet. Thus, it is\neasy to use a pre-trained GoogLeNet for large-scale image classification\nproblem and fine tune our connected layers on an large scale database of\naesthetic related images: AVA, i.e. \\emph{domain adaptation}. The experiments\nreveal that our model achieves the state of the arts in AVA database. Both the\ntraining and testing speeds of our model are higher than those of the original\nGoogLeNet. \n\n"}
{"id": "1610.04574", "contents": "Title: Generalization Error of Invariant Classifiers Abstract: This paper studies the generalization error of invariant classifiers. In\nparticular, we consider the common scenario where the classification task is\ninvariant to certain transformations of the input, and that the classifier is\nconstructed (or learned) to be invariant to these transformations. Our approach\nrelies on factoring the input space into a product of a base space and a set of\ntransformations. We show that whereas the generalization error of a\nnon-invariant classifier is proportional to the complexity of the input space,\nthe generalization error of an invariant classifier is proportional to the\ncomplexity of the base space. We also derive a set of sufficient conditions on\nthe geometry of the base space and the set of transformations that ensure that\nthe complexity of the base space is much smaller than the complexity of the\ninput space. Our analysis applies to general classifiers such as convolutional\nneural networks. We demonstrate the implications of the developed theory for\nsuch classifiers with experiments on the MNIST and CIFAR-10 datasets. \n\n"}
{"id": "1610.05949", "contents": "Title: Visual-Inertial Monocular SLAM with Map Reuse Abstract: In recent years there have been excellent results in Visual-Inertial Odometry\ntechniques, which aim to compute the incremental motion of the sensor with high\naccuracy and robustness. However these approaches lack the capability to close\nloops, and trajectory estimation accumulates drift even if the sensor is\ncontinually revisiting the same place. In this work we present a novel\ntightly-coupled Visual-Inertial Simultaneous Localization and Mapping system\nthat is able to close loops and reuse its map to achieve zero-drift\nlocalization in already mapped areas. While our approach can be applied to any\ncamera configuration, we address here the most general problem of a monocular\ncamera, with its well-known scale ambiguity. We also propose a novel IMU\ninitialization method, which computes the scale, the gravity direction, the\nvelocity, and gyroscope and accelerometer biases, in a few seconds with high\naccuracy. We test our system in the 11 sequences of a recent micro-aerial\nvehicle public dataset achieving a typical scale factor error of 1% and\ncentimeter precision. We compare to the state-of-the-art in visual-inertial\nodometry in sequences with revisiting, proving the better accuracy of our\nmethod due to map reuse and no drift accumulation. \n\n"}
{"id": "1610.06402", "contents": "Title: A Growing Long-term Episodic & Semantic Memory Abstract: The long-term memory of most connectionist systems lies entirely in the\nweights of the system. Since the number of weights is typically fixed, this\nbounds the total amount of knowledge that can be learned and stored. Though\nthis is not normally a problem for a neural network designed for a specific\ntask, such a bound is undesirable for a system that continually learns over an\nopen range of domains. To address this, we describe a lifelong learning system\nthat leverages a fast, though non-differentiable, content-addressable memory\nwhich can be exploited to encode both a long history of sequential episodic\nknowledge and semantic knowledge over many episodes for an unbounded number of\ndomains. This opens the door for investigation into transfer learning, and\nleveraging prior knowledge that has been learned over a lifetime of experiences\nto new domains. \n\n"}
{"id": "1610.06421", "contents": "Title: Mixed Neural Network Approach for Temporal Sleep Stage Classification Abstract: This paper proposes a practical approach to addressing limitations posed by\nuse of single active electrodes in applications for sleep stage classification.\nElectroencephalography (EEG)-based characterizations of sleep stage progression\ncontribute the diagnosis and monitoring of the many pathologies of sleep.\nSeveral prior reports have explored ways of automating the analysis of sleep\nEEG and of reducing the complexity of the data needed for reliable\ndiscrimination of sleep stages in order to make it possible to perform sleep\nstudies at lower cost in the home (rather than only in specialized clinical\nfacilities). However, these reports have involved recordings from electrodes\nplaced on the cranial vertex or occiput, which can be uncomfortable or\ndifficult for subjects to position. Those that have utilized single EEG\nchannels which contain less sleep information, have showed poor classification\nperformance. We have taken advantage of Rectifier Neural Network for feature\ndetection and Long Short-Term Memory (LSTM) network for sequential data\nlearning to optimize classification performance with single electrode\nrecordings. After exploring alternative electrode placements, we found a\ncomfortable configuration of a single-channel EEG on the forehead and have\nshown that it can be integrated with additional electrodes for simultaneous\nrecording of the electroocuolgram (EOG). Evaluation of data from 62 people\n(with 494 hours sleep) demonstrated better performance of our analytical\nalgorithm for automated sleep classification than existing approaches using\nvertex or occipital electrode placements. Use of this recording configuration\nwith neural network deconvolution promises to make clinically indicated home\nsleep studies practical. \n\n"}
{"id": "1610.06453", "contents": "Title: Change-point Detection Methods for Body-Worn Video Abstract: Body-worn video (BWV) cameras are increasingly utilized by police departments\nto provide a record of police-public interactions. However, large-scale BWV\ndeployment produces terabytes of data per week, necessitating the development\nof effective computational methods to identify salient changes in video. In\nwork carried out at the 2016 RIPS program at IPAM, UCLA, we present a novel\ntwo-stage framework for video change-point detection. First, we employ\nstate-of-the-art machine learning methods including convolutional neural\nnetworks and support vector machines for scene classification. We then develop\nand compare change-point detection algorithms utilizing mean squared-error\nminimization, forecasting methods, hidden Markov models, and maximum likelihood\nestimation to identify noteworthy changes. We test our framework on detection\nof vehicle exits and entrances in a BWV data set provided by the Los Angeles\nPolice Department and achieve over 90% recall and nearly 70% precision --\ndemonstrating robustness to rapid scene changes, extreme luminance differences,\nand frequent camera occlusions. \n\n"}
{"id": "1610.06671", "contents": "Title: Multi-view metric learning for multi-instance image classification Abstract: It is critical and meaningful to make image classification since it can help\nhuman in image retrieval and recognition, object detection, etc. In this paper,\nthree-sides efforts are made to accomplish the task. First, visual features\nwith bag-of-words representation, not single vector, are extracted to\ncharacterize the image. To improve the performance, the idea of multi-view\nlearning is implemented and three kinds of features are provided, each one\ncorresponds to a single view. The information from three views is complementary\nto each other, which can be unified together. Then a new distance function is\ndesigned for bags by computing the weighted sum of the distances between\ninstances. The technique of metric learning is explored to construct a\ndata-dependent distance metric to measure the relationships between instances,\nmeanwhile between bags and images, more accurately. Last, a novel approach,\ncalled MVML, is proposed, which optimizes the joint probability that every\nimage is similar with its nearest image. MVML learns multiple distance metrics,\neach one models a single view, to unifies the information from multiple views.\nThe method can be solved by alternate optimization iteratively. Gradient ascent\nand positive semi-definite projection are utilized in the iterations. Distance\ncomparisons verified that the new bag distance function is prior to previous\nfunctions. In model evaluation, numerical experiments show that MVML with\nmultiple views performs better than single view condition, which demonstrates\nthat our model can assemble the complementary information efficiently and\nmeasure the distance between images more precisely. Experiments on influence of\nparameters and instance number validate the consistency of the method. \n\n"}
{"id": "1610.08851", "contents": "Title: Single- and Multi-Task Architectures for Tool Presence Detection\n  Challenge at M2CAI 2016 Abstract: The tool presence detection challenge at M2CAI 2016 consists of identifying\nthe presence/absence of seven surgical tools in the images of cholecystectomy\nvideos. Here, we propose to use deep architectures that are based on our\nprevious work where we presented several architectures to perform multiple\nrecognition tasks on laparoscopic videos. In this technical report, we present\nthe tool presence detection results using two architectures: (1) a single-task\narchitecture designed to perform solely the tool presence detection task and\n(2) a multi-task architecture designed to perform jointly phase recognition and\ntool presence detection. The results show that the multi-task network only\nslightly improves the tool presence detection results. In constrast, a\nsignificant improvement is obtained when there are more data available to train\nthe networks. This significant improvement can be regarded as a call for action\nfor other institutions to start working toward publishing more datasets into\nthe community, so that better models could be generated to perform the task. \n\n"}
{"id": "1610.09386", "contents": "Title: Detecting Breast Cancer using a Compressive Sensing Unmixing Algorithm Abstract: Traditional breast cancer imaging methods using microwave Nearfield Radar\nImaging (NRI) seek to recover the complex permittivity of the tissues at each\nvoxel in the imaging region. This approach is suboptimal, in that it does not\ndirectly consider the permittivity values that healthy and cancerous breast\ntissues typically have. In this paper, we describe a novel unmixing algorithm\nfor detecting breast cancer. In this approach, the breast tissue is separated\ninto three components, low water content (LWC), high water content (HWC), and\ncancerous tissues, and the goal of the optimization procedure is to recover the\nmixture proportions for each component. By utilizing this approach in a hybrid\nDBT / NRI system, the unmixing reconstruction process can be posed as a sparse\nrecovery problem, such that compressive sensing (CS) techniques can be\nemployed. A numerical analysis is performed, which demonstrates that cancerous\nlesions can be detected from their mixture proportion under the appropriate\nconditions. \n\n"}
{"id": "1610.09887", "contents": "Title: Depth-Width Tradeoffs in Approximating Natural Functions with Neural\n  Networks Abstract: We provide several new depth-based separation results for feed-forward neural\nnetworks, proving that various types of simple and natural functions can be\nbetter approximated using deeper networks than shallower ones, even if the\nshallower networks are much larger. This includes indicators of balls and\nellipses; non-linear functions which are radial with respect to the $L_1$ norm;\nand smooth non-linear functions. We also show that these gaps can be observed\nexperimentally: Increasing the depth indeed allows better learning than\nincreasing width, when training neural networks to learn an indicator of a unit\nball. \n\n"}
{"id": "1610.09908", "contents": "Title: Joint Large-Scale Motion Estimation and Image Reconstruction Abstract: This article describes the implementation of the joint motion estimation and\nimage reconstruction framework presented by Burger, Dirks and Sch\\\"onlieb and\nextends this framework to large-scale motion between consecutive image frames.\nThe variational framework uses displacements between consecutive frames based\non the optical flow approach to improve the image reconstruction quality on the\none hand and the motion estimation quality on the other. The energy functional\nconsists of a data-fidelity term with a general operator that connects the\ninput sequence to the solution, it has a total variation term for the image\nsequence and is connected to the underlying flow using an optical flow term.\nAdditional spatial regularity for the flow is modeled by a total variation\nregularizer for both components of the flow. The numerical minimization is\nperformed in an alternating manner using primal-dual techniques. The resulting\nschemes are presented as pseudo-code together with a short numerical\nevaluation. \n\n"}
{"id": "1611.01186", "contents": "Title: Demystifying ResNet Abstract: The Residual Network (ResNet), proposed in He et al. (2015), utilized\nshortcut connections to significantly reduce the difficulty of training, which\nresulted in great performance boosts in terms of both training and\ngeneralization error.\n  It was empirically observed in He et al. (2015) that stacking more layers of\nresidual blocks with shortcut 2 results in smaller training error, while it is\nnot true for shortcut of length 1 or 3. We provide a theoretical explanation\nfor the uniqueness of shortcut 2.\n  We show that with or without nonlinearities, by adding shortcuts that have\ndepth two, the condition number of the Hessian of the loss function at the zero\ninitial point is depth-invariant, which makes training very deep models no more\ndifficult than shallow ones. Shortcuts of higher depth result in an extremely\nflat (high-order) stationary point initially, from which the optimization\nalgorithm is hard to escape. The shortcut 1, however, is essentially equivalent\nto no shortcuts, which has a condition number exploding to infinity as the\nnumber of layers grows. We further argue that as the number of layers tends to\ninfinity, it suffices to only look at the loss function at the zero initial\npoint.\n  Extensive experiments are provided accompanying our theoretical results. We\nshow that initializing the network to small weights with shortcut 2 achieves\nsignificantly better results than random Gaussian (Xavier) initialization,\northogonal initialization, and shortcuts of deeper depth, from various\nperspectives ranging from final loss, learning dynamics and stability, to the\nbehavior of the Hessian along the learning process. \n\n"}
{"id": "1611.02049", "contents": "Title: Low-effort place recognition with WiFi fingerprints using deep learning Abstract: Using WiFi signals for indoor localization is the main localization modality\nof the existing personal indoor localization systems operating on mobile\ndevices. WiFi fingerprinting is also used for mobile robots, as WiFi signals\nare usually available indoors and can provide rough initial position estimate\nor can be used together with other positioning systems. Currently, the best\nsolutions rely on filtering, manual data analysis, and time-consuming parameter\ntuning to achieve reliable and accurate localization. In this work, we propose\nto use deep neural networks to significantly lower the work-force burden of the\nlocalization system design, while still achieving satisfactory results.\nAssuming the state-of-the-art hierarchical approach, we employ the DNN system\nfor building/floor classification. We show that stacked autoencoders allow to\nefficiently reduce the feature space in order to achieve robust and precise\nclassification. The proposed architecture is verified on the publicly available\nUJIIndoorLoc dataset and the results are compared with other solutions. \n\n"}
{"id": "1611.02345", "contents": "Title: Neural Taylor Approximations: Convergence and Exploration in Rectifier\n  Networks Abstract: Modern convolutional networks, incorporating rectifiers and max-pooling, are\nneither smooth nor convex; standard guarantees therefore do not apply.\nNevertheless, methods from convex optimization such as gradient descent and\nAdam are widely used as building blocks for deep learning algorithms. This\npaper provides the first convergence guarantee applicable to modern convnets,\nwhich furthermore matches a lower bound for convex nonsmooth functions. The key\ntechnical tool is the neural Taylor approximation -- a straightforward\napplication of Taylor expansions to neural networks -- and the associated\nTaylor loss. Experiments on a range of optimizers, layers, and tasks provide\nevidence that the analysis accurately captures the dynamics of neural\noptimization. The second half of the paper applies the Taylor approximation to\nisolate the main difficulty in training rectifier nets -- that gradients are\nshattered -- and investigates the hypothesis that, by exploring the space of\nactivation configurations more thoroughly, adaptive optimizers such as RMSProp\nand Adam are able to converge to better solutions. \n\n"}
{"id": "1611.03607", "contents": "Title: Deep Recurrent Neural Network for Mobile Human Activity Recognition with\n  High Throughput Abstract: In this paper, we propose a method of human activity recognition with high\nthroughput from raw accelerometer data applying a deep recurrent neural network\n(DRNN), and investigate various architectures and its combination to find the\nbest parameter values. The \"high throughput\" refers to short time at a time of\nrecognition. We investigated various parameters and architectures of the DRNN\nby using the training dataset of 432 trials with 6 activity classes from 7\npeople. The maximum recognition rate was 95.42% and 83.43% against the test\ndata of 108 segmented trials each of which has single activity class and 18\nmultiple sequential trials, respectively. Here, the maximum recognition rates\nby traditional methods were 71.65% and 54.97% for each. In addition, the\nefficiency of the found parameters was evaluated by using additional dataset.\nFurther, as for throughput of the recognition per unit time, the constructed\nDRNN was requiring only 1.347 [ms], while the best traditional method required\n11.031 [ms] which includes 11.027 [ms] for feature calculation. These\nadvantages are caused by the compact and small architecture of the constructed\nreal time oriented DRNN. \n\n"}
{"id": "1611.03673", "contents": "Title: Learning to Navigate in Complex Environments Abstract: Learning to navigate in complex environments with dynamic elements is an\nimportant milestone in developing AI agents. In this work we formulate the\nnavigation question as a reinforcement learning problem and show that data\nefficiency and task performance can be dramatically improved by relying on\nadditional auxiliary tasks leveraging multimodal sensory inputs. In particular\nwe consider jointly learning the goal-driven reinforcement learning problem\nwith auxiliary depth prediction and loop closure classification tasks. This\napproach can learn to navigate from raw sensory input in complicated 3D mazes,\napproaching human-level performance even under conditions where the goal\nlocation changes frequently. We provide detailed analysis of the agent\nbehaviour, its ability to localise, and its network activity dynamics, showing\nthat the agent implicitly learns key navigation abilities. \n\n"}
{"id": "1611.04413", "contents": "Title: Automatic discovery of discriminative parts as a quadratic assignment\n  problem Abstract: Part-based image classification consists in representing categories by small\nsets of discriminative parts upon which a representation of the images is\nbuilt. This paper addresses the question of how to automatically learn such\nparts from a set of labeled training images. The training of parts is cast as a\nquadratic assignment problem in which optimal correspondences between image\nregions and parts are automatically learned. The paper analyses different\nassignment strategies and thoroughly evaluates them on two public datasets:\nWillow actions and MIT 67 scenes. State-of-the art results are obtained on\nthese datasets. \n\n"}
{"id": "1611.04766", "contents": "Title: Differentiable Genetic Programming Abstract: We introduce the use of high order automatic differentiation, implemented via\nthe algebra of truncated Taylor polynomials, in genetic programming. Using the\nCartesian Genetic Programming encoding we obtain a high-order Taylor\nrepresentation of the program output that is then used to back-propagate errors\nduring learning. The resulting machine learning framework is called\ndifferentiable Cartesian Genetic Programming (dCGP). In the context of symbolic\nregression, dCGP offers a new approach to the long unsolved problem of constant\nrepresentation in GP expressions. On several problems of increasing complexity\nwe find that dCGP is able to find the exact form of the symbolic expression as\nwell as the constants values. We also demonstrate the use of dCGP to solve a\nlarge class of differential equations and to find prime integrals of dynamical\nsystems, presenting, in both cases, results that confirm the efficacy of our\napproach. \n\n"}
{"id": "1611.05203", "contents": "Title: Will People Like Your Image? Learning the Aesthetic Space Abstract: Rating how aesthetically pleasing an image appears is a highly complex matter\nand depends on a large number of different visual factors. Previous work has\ntackled the aesthetic rating problem by ranking on a 1-dimensional rating\nscale, e.g., incorporating handcrafted attributes. In this paper, we propose a\nrather general approach to automatically map aesthetic pleasingness with all\nits complexity into an \"aesthetic space\" to allow for a highly fine-grained\nresolution. In detail, making use of deep learning, our method directly learns\nan encoding of a given image into this high-dimensional feature space\nresembling visual aesthetics. Additionally to the mentioned visual factors,\ndifferences in personal judgments have a large impact on the likeableness of a\nphotograph. Nowadays, online platforms allow users to \"like\" or favor certain\ncontent with a single click. To incorporate a huge diversity of people, we make\nuse of such multi-user agreements and assemble a large data set of 380K images\n(AROD) with associated meta information and derive a score to rate how visually\npleasing a given photo is. We validate our derived model of aesthetics in a\nuser study. Further, without any extra data labeling or handcrafted features,\nwe achieve state-of-the art accuracy on the AVA benchmark data set. Finally, as\nour approach is able to predict the aesthetic quality of any arbitrary image or\nvideo, we demonstrate our results on applications for resorting photo\ncollections, capturing the best shot on mobile devices and aesthetic key-frame\nextraction from videos. \n\n"}
{"id": "1611.05267", "contents": "Title: Temporal Convolutional Networks for Action Segmentation and Detection Abstract: The ability to identify and temporally segment fine-grained human actions\nthroughout a video is crucial for robotics, surveillance, education, and\nbeyond. Typical approaches decouple this problem by first extracting local\nspatiotemporal features from video frames and then feeding them into a temporal\nclassifier that captures high-level temporal patterns. We introduce a new class\nof temporal models, which we call Temporal Convolutional Networks (TCNs), that\nuse a hierarchy of temporal convolutions to perform fine-grained action\nsegmentation or detection. Our Encoder-Decoder TCN uses pooling and upsampling\nto efficiently capture long-range temporal patterns whereas our Dilated TCN\nuses dilated convolutions. We show that TCNs are capable of capturing action\ncompositions, segment durations, and long-range dependencies, and are over a\nmagnitude faster to train than competing LSTM-based Recurrent Neural Networks.\nWe apply these models to three challenging fine-grained datasets and show large\nimprovements over the state of the art. \n\n"}
{"id": "1611.05335", "contents": "Title: Unsupervised Learning of Important Objects from First-Person Videos Abstract: A first-person camera, placed at a person's head, captures, which objects are\nimportant to the camera wearer. Most prior methods for this task learn to\ndetect such important objects from the manually labeled first-person data in a\nsupervised fashion. However, important objects are strongly related to the\ncamera wearer's internal state such as his intentions and attention, and thus,\nonly the person wearing the camera can provide the importance labels. Such a\nconstraint makes the annotation process costly and limited in scalability.\n  In this work, we show that we can detect important objects in first-person\nimages without the supervision by the camera wearer or even third-person\nlabelers. We formulate an important detection problem as an interplay between\nthe 1) segmentation and 2) recognition agents. The segmentation agent first\nproposes a possible important object segmentation mask for each image, and then\nfeeds it to the recognition agent, which learns to predict an important object\nmask using visual semantics and spatial features.\n  We implement such an interplay between both agents via an alternating\ncross-pathway supervision scheme inside our proposed Visual-Spatial Network\n(VSN). Our VSN consists of spatial (\"where\") and visual (\"what\") pathways, one\nof which learns common visual semantics while the other focuses on the spatial\nlocation cues. Our unsupervised learning is accomplished via a cross-pathway\nsupervision, where one pathway feeds its predictions to a segmentation agent,\nwhich proposes a candidate important object segmentation mask that is then used\nby the other pathway as a supervisory signal. We show our method's success on\ntwo different important object datasets, where our method achieves similar or\nbetter results as the supervised methods. \n\n"}
{"id": "1611.05397", "contents": "Title: Reinforcement Learning with Unsupervised Auxiliary Tasks Abstract: Deep reinforcement learning agents have achieved state-of-the-art results by\ndirectly maximising cumulative reward. However, environments contain a much\nwider variety of possible training signals. In this paper, we introduce an\nagent that also maximises many other pseudo-reward functions simultaneously by\nreinforcement learning. All of these tasks share a common representation that,\nlike unsupervised learning, continues to develop in the absence of extrinsic\nrewards. We also introduce a novel mechanism for focusing this representation\nupon extrinsic rewards, so that learning can rapidly adapt to the most relevant\naspects of the actual task. Our agent significantly outperforms the previous\nstate-of-the-art on Atari, averaging 880\\% expert human performance, and a\nchallenging suite of first-person, three-dimensional \\emph{Labyrinth} tasks\nleading to a mean speedup in learning of 10$\\times$ and averaging 87\\% expert\nhuman performance on Labyrinth. \n\n"}
{"id": "1611.05503", "contents": "Title: On the Exploration of Convolutional Fusion Networks for Visual\n  Recognition Abstract: Despite recent advances in multi-scale deep representations, their\nlimitations are attributed to expensive parameters and weak fusion modules.\nHence, we propose an efficient approach to fuse multi-scale deep\nrepresentations, called convolutional fusion networks (CFN). Owing to using\n1$\\times$1 convolution and global average pooling, CFN can efficiently generate\nthe side branches while adding few parameters. In addition, we present a\nlocally-connected fusion module, which can learn adaptive weights for the side\nbranches and form a discriminatively fused feature. CFN models trained on the\nCIFAR and ImageNet datasets demonstrate remarkable improvements over the plain\nCNNs. Furthermore, we generalize CFN to three new tasks, including scene\nrecognition, fine-grained recognition and image retrieval. Our experiments show\nthat it can obtain consistent improvements towards the transferring tasks. \n\n"}
{"id": "1611.05644", "contents": "Title: Inverting The Generator Of A Generative Adversarial Network Abstract: Generative adversarial networks (GANs) learn to synthesise new samples from a\nhigh-dimensional distribution by passing samples drawn from a latent space\nthrough a generative network. When the high-dimensional distribution describes\nimages of a particular data set, the network should learn to generate visually\nsimilar image samples for latent variables that are close to each other in the\nlatent space. For tasks such as image retrieval and image classification, it\nmay be useful to exploit the arrangement of the latent space by projecting\nimages into it, and using this as a representation for discriminative tasks.\nGANs often consist of multiple layers of non-linear computations, making them\nvery difficult to invert. This paper introduces techniques for projecting image\nsamples into the latent space using any pre-trained GAN, provided that the\ncomputational graph is available. We evaluate these techniques on both MNIST\ndigits and Omniglot handwritten characters. In the case of MNIST digits, we\nshow that projections into the latent space maintain information about the\nstyle and the identity of the digit. In the case of Omniglot characters, we\nshow that even characters from alphabets that have not been seen during\ntraining may be projected well into the latent space; this suggests that this\napproach may have applications in one-shot learning. \n\n"}
{"id": "1611.05689", "contents": "Title: End-to-end Learning of Cost-Volume Aggregation for Real-time Dense\n  Stereo Abstract: We present a new deep learning-based approach for dense stereo matching.\nCompared to previous works, our approach does not use deep learning of pixel\nappearance descriptors, employing very fast classical matching scores instead.\nAt the same time, our approach uses a deep convolutional network to predict the\nlocal parameters of cost volume aggregation process, which in this paper we\nimplement using differentiable domain transform. By treating such transform as\na recurrent neural network, we are able to train our whole system that includes\ncost volume computation, cost-volume aggregation (smoothing), and\nwinner-takes-all disparity selection end-to-end. The resulting method is highly\nefficient at test time, while achieving good matching accuracy. On the KITTI\n2015 benchmark, it achieves a result of 6.34\\% error rate while running at 29\nframes per second rate on a modern GPU. \n\n"}
{"id": "1611.05799", "contents": "Title: The Freiburg Groceries Dataset Abstract: With the increasing performance of machine learning techniques in the last\nfew years, the computer vision and robotics communities have created a large\nnumber of datasets for benchmarking object recognition tasks. These datasets\ncover a large spectrum of natural images and object categories, making them not\nonly useful as a testbed for comparing machine learning approaches, but also a\ngreat resource for bootstrapping different domain-specific perception and\nrobotic systems. One such domain is domestic environments, where an autonomous\nrobot has to recognize a large variety of everyday objects such as groceries.\nThis is a challenging task due to the large variety of objects and products,\nand where there is great need for real-world training data that goes beyond\nproduct images available online. In this paper, we address this issue and\npresent a dataset consisting of 5,000 images covering 25 different classes of\ngroceries, with at least 97 images per class. We collected all images from\nreal-world settings at different stores and apartments. In contrast to existing\ngroceries datasets, our dataset includes a large variety of perspectives,\nlighting conditions, and degrees of clutter. Overall, our images contain\nthousands of different object instances. It is our hope that machine learning\nand robotics researchers find this dataset of use for training, testing, and\nbootstrapping their approaches. As a baseline classifier to facilitate\ncomparison, we re-trained the CaffeNet architecture (an adaptation of the\nwell-known AlexNet) on our dataset and achieved a mean accuracy of 78.9%. We\nrelease this trained model along with the code and data splits we used in our\nexperiments. \n\n"}
{"id": "1611.06301", "contents": "Title: Inferring Restaurant Styles by Mining Crowd Sourced Photos from\n  User-Review Websites Abstract: When looking for a restaurant online, user uploaded photos often give people\nan immediate and tangible impression about a restaurant. Due to their\ninformativeness, such user contributed photos are leveraged by restaurant\nreview websites to provide their users an intuitive and effective search\nexperience. In this paper, we present a novel approach to inferring restaurant\ntypes or styles (ambiance, dish styles, suitability for different occasions)\nfrom user uploaded photos on user-review websites. To that end, we first\ncollect a novel restaurant photo dataset associating the user contributed\nphotos with the restaurant styles from TripAdvior. We then propose a deep\nmulti-instance multi-label learning (MIML) framework to deal with the unique\nproblem setting of the restaurant style classification task. We employ a\ntwo-step bootstrap strategy to train a multi-label convolutional neural network\n(CNN). The multi-label CNN is then used to compute the confidence scores of\nrestaurant styles for all the images associated with a restaurant. The computed\nconfidence scores are further used to train a final binary classifier for each\nrestaurant style tag. Upon training, the styles of a restaurant can be profiled\nby analyzing restaurant photos with the trained multi-label CNN and SVM models.\nExperimental evaluation has demonstrated that our crowd sourcing-based approach\ncan effectively infer the restaurant style when there are a sufficient number\nof user uploaded photos for a given restaurant. \n\n"}
{"id": "1611.06391", "contents": "Title: Deep Residual Learning for Compressed Sensing CT Reconstruction via\n  Persistent Homology Analysis Abstract: Recently, compressed sensing (CS) computed tomography (CT) using sparse\nprojection views has been extensively investigated to reduce the potential risk\nof radiation to patient. However, due to the insufficient number of projection\nviews, an analytic reconstruction approach results in severe streaking\nartifacts and CS-based iterative approach is computationally very expensive. To\naddress this issue, here we propose a novel deep residual learning approach for\nsparse view CT reconstruction. Specifically, based on a novel persistent\nhomology analysis showing that the manifold of streaking artifacts is\ntopologically simpler than original ones, a deep residual learning architecture\nthat estimates the streaking artifacts is developed. Once a streaking artifact\nimage is estimated, an artifact-free image can be obtained by subtracting the\nstreaking artifacts from the input image. Using extensive experiments with real\npatient data set, we confirm that the proposed residual learning provides\nsignificantly better image reconstruction performance with several orders of\nmagnitude faster computational speed. \n\n"}
{"id": "1611.06492", "contents": "Title: Recurrent Memory Addressing for describing videos Abstract: In this paper, we introduce Key-Value Memory Networks to a multimodal setting\nand a novel key-addressing mechanism to deal with sequence-to-sequence models.\nThe proposed model naturally decomposes the problem of video captioning into\nvision and language segments, dealing with them as key-value pairs. More\nspecifically, we learn a semantic embedding (v) corresponding to each frame (k)\nin the video, thereby creating (k, v) memory slots. We propose to find the next\nstep attention weights conditioned on the previous attention distributions for\nthe key-value memory slots in the memory addressing schema. Exploiting this\nflexibility of the framework, we additionally capture spatial dependencies\nwhile mapping from the visual to semantic embedding. Experiments done on the\nYoutube2Text dataset demonstrate usefulness of recurrent key-addressing, while\nachieving competitive scores on BLEU@4, METEOR metrics against state-of-the-art\nmodels. \n\n"}
{"id": "1611.06969", "contents": "Title: Kernel Cross-View Collaborative Representation based Classification for\n  Person Re-Identification Abstract: Person re-identification aims at the maintenance of a global identity as a\nperson moves among non-overlapping surveillance cameras. It is a hard task due\nto different illumination conditions, viewpoints and the small number of\nannotated individuals from each pair of cameras (small-sample-size problem).\nCollaborative Representation based Classification (CRC) has been employed\nsuccessfully to address the small-sample-size problem in computer vision.\nHowever, the original CRC formulation is not well-suited for person\nre-identification since it does not consider that probe and gallery samples are\nfrom different cameras. Furthermore, it is a linear model, while appearance\nchanges caused by different camera conditions indicate a strong nonlinear\ntransition between cameras. To overcome such limitations, we propose the Kernel\nCross-View Collaborative Representation based Classification (Kernel X-CRC)\nthat represents probe and gallery images by balancing representativeness and\nsimilarity nonlinearly. It assumes that a probe and its corresponding gallery\nimage are represented with similar coding vectors using individuals from the\ntraining set. Experimental results demonstrate that our assumption is true when\nusing a high-dimensional feature vector and becomes more compelling when\ndealing with a low-dimensional and discriminative representation computed using\na common subspace learning method. We achieve state-of-the-art for rank-1\nmatching rates in two person re-identification datasets (PRID450S and GRID) and\nthe second best results on VIPeR and CUHK01 datasets. \n\n"}
{"id": "1611.07819", "contents": "Title: dMath: Distributed Linear Algebra for DL Abstract: The paper presents a parallel math library, dMath, that demonstrates leading\nscaling when using intranode, internode, and hybrid-parallelism for deep\nlearning (DL). dMath provides easy-to-use distributed primitives and a variety\nof domain-specific algorithms including matrix multiplication, convolutions,\nand others allowing for rapid development of scalable applications like deep\nneural networks (DNNs). Persistent data stored in GPU memory and advanced\nmemory management techniques avoid costly transfers between host and device.\ndMath delivers performance, portability, and productivity to its specific\ndomain of support. \n\n"}
{"id": "1611.08036", "contents": "Title: Robotic Grasp Detection using Deep Convolutional Neural Networks Abstract: Deep learning has significantly advanced computer vision and natural language\nprocessing. While there have been some successes in robotics using deep\nlearning, it has not been widely adopted. In this paper, we present a novel\nrobotic grasp detection system that predicts the best grasping pose of a\nparallel-plate robotic gripper for novel objects using the RGB-D image of the\nscene. The proposed model uses a deep convolutional neural network to extract\nfeatures from the scene and then uses a shallow convolutional neural network to\npredict the grasp configuration for the object of interest. Our multi-modal\nmodel achieved an accuracy of 89.21% on the standard Cornell Grasp Dataset and\nruns at real-time speeds. This redefines the state-of-the-art for robotic grasp\ndetection. \n\n"}
{"id": "1611.08091", "contents": "Title: Deep Joint Face Hallucination and Recognition Abstract: Deep models have achieved impressive performance for face hallucination\ntasks. However, we observe that directly feeding the hallucinated facial images\ninto recog- nition models can even degrade the recognition performance despite\nthe much better visualization quality. In this paper, we address this problem\nby jointly learning a deep model for two tasks, i.e. face hallucination and\nrecognition. In particular, we design an end-to-end deep convolution network\nwith hallucination sub-network cascaded by recognition sub-network. The\nrecognition sub- network are responsible for producing discriminative feature\nrepresentations using the hallucinated images as inputs generated by\nhallucination sub-network. During training, we feed LR facial images into the\nnetwork and optimize the parameters by minimizing two loss items, i.e. 1) face\nhallucination loss measured by the pixel wise difference between the ground\ntruth HR images and network-generated images; and 2) verification loss which is\nmeasured by the classification error and intra-class distance. We extensively\nevaluate our method on LFW and YTF datasets. The experimental results show that\nour method can achieve recognition accuracy 97.95% on 4x down-sampled LFW\ntesting set, outperforming the accuracy 96.35% of conventional face recognition\nmodel. And on the more challenging YTF dataset, we achieve recognition accuracy\n90.65%, a margin over the recognition accuracy 89.45% obtained by conventional\nface recognition model on the 4x down-sampled version. \n\n"}
{"id": "1611.08272", "contents": "Title: InstanceCut: from Edges to Instances with MultiCut Abstract: This work addresses the task of instance-aware semantic segmentation. Our key\nmotivation is to design a simple method with a new modelling-paradigm, which\ntherefore has a different trade-off between advantages and disadvantages\ncompared to known approaches. Our approach, we term InstanceCut, represents the\nproblem by two output modalities: (i) an instance-agnostic semantic\nsegmentation and (ii) all instance-boundaries. The former is computed from a\nstandard convolutional neural network for semantic segmentation, and the latter\nis derived from a new instance-aware edge detection model. To reason globally\nabout the optimal partitioning of an image into instances, we combine these two\nmodalities into a novel MultiCut formulation. We evaluate our approach on the\nchallenging CityScapes dataset. Despite the conceptual simplicity of our\napproach, we achieve the best result among all published methods, and perform\nparticularly well for rare object classes. \n\n"}
{"id": "1611.08974", "contents": "Title: Semantic Scene Completion from a Single Depth Image Abstract: This paper focuses on semantic scene completion, a task for producing a\ncomplete 3D voxel representation of volumetric occupancy and semantic labels\nfor a scene from a single-view depth map observation. Previous work has\nconsidered scene completion and semantic labeling of depth maps separately.\nHowever, we observe that these two problems are tightly intertwined. To\nleverage the coupled nature of these two tasks, we introduce the semantic scene\ncompletion network (SSCNet), an end-to-end 3D convolutional network that takes\na single depth image as input and simultaneously outputs occupancy and semantic\nlabels for all voxels in the camera view frustum. Our network uses a\ndilation-based 3D context module to efficiently expand the receptive field and\nenable 3D context learning. To train our network, we construct SUNCG - a\nmanually created large-scale dataset of synthetic 3D scenes with dense\nvolumetric annotations. Our experiments demonstrate that the joint model\noutperforms methods addressing each task in isolation and outperforms\nalternative approaches on the semantic scene completion task. \n\n"}
{"id": "1611.09007", "contents": "Title: Hyperspectral CNN Classification with Limited Training Samples Abstract: Hyperspectral imaging sensors are becoming increasingly popular in robotics\napplications such as agriculture and mining, and allow per-pixel thematic\nclassification of materials in a scene based on their unique spectral\nsignatures. Recently, convolutional neural networks have shown remarkable\nperformance for classification tasks, but require substantial amounts of\nlabelled training data. This data must sufficiently cover the variability\nexpected to be encountered in the environment. For hyperspectral data, one of\nthe main variations encountered outdoors is due to incident illumination, which\ncan change in spectral shape and intensity depending on the scene geometry. For\nexample, regions occluded from the sun have a lower intensity and their\nincident irradiance skewed towards shorter wavelengths.\n  In this work, a data augmentation strategy based on relighting is used during\ntraining of a hyperspectral convolutional neural network. It allows training to\noccur in the outdoor environment given only a small labelled region, which does\nnot need to sufficiently represent the geometric variability of the entire\nscene. This is important for applications where obtaining large amounts of\ntraining data is labourious, hazardous or difficult, such as labelling pixels\nwithin shadows. Radiometric normalisation approaches for pre-processing the\nhyperspectral data are analysed and it is shown that methods based on the raw\npixel data are sufficient to be used as input for the classifier. This removes\nthe need for external hardware such as calibration boards, which can restrict\nthe application of hyperspectral sensors in robotics applications. Experiments\nto evaluate the classification system are carried out on two datasets captured\nfrom a field-based platform. \n\n"}
{"id": "1611.09232", "contents": "Title: Efficient Convolutional Auto-Encoding via Random Convexification and\n  Frequency-Domain Minimization Abstract: The omnipresence of deep learning architectures such as deep convolutional\nneural networks (CNN)s is fueled by the synergistic combination of\never-increasing labeled datasets and specialized hardware. Despite the\nindisputable success, the reliance on huge amounts of labeled data and\nspecialized hardware can be a limiting factor when approaching new\napplications. To help alleviating these limitations, we propose an efficient\nlearning strategy for layer-wise unsupervised training of deep CNNs on\nconventional hardware in acceptable time. Our proposed strategy consists of\nrandomly convexifying the reconstruction contractive auto-encoding (RCAE)\nlearning objective and solving the resulting large-scale convex minimization\nproblem in the frequency domain via coordinate descent (CD). The main\nadvantages of our proposed learning strategy are: (1) single tunable\noptimization parameter; (2) fast and guaranteed convergence; (3) possibilities\nfor full parallelization. Numerical experiments show that our proposed learning\nstrategy scales (in the worst case) linearly with image size, number of filters\nand filter size. \n\n"}
{"id": "1611.09394", "contents": "Title: Material Recognition from Local Appearance in Global Context Abstract: Recognition of materials has proven to be a challenging problem due to the\nwide variation in appearance within and between categories. Global image\ncontext, such as where the material is or what object it makes up, can be\ncrucial to recognizing the material. Existing methods, however, operate on an\nimplicit fusion of materials and context by using large receptive fields as\ninput (i.e., large image patches). Many recent material recognition methods\ntreat materials as yet another set of labels like objects. Materials are,\nhowever, fundamentally different from objects as they have no inherent shape or\ndefined spatial extent. Approaches that ignore this can only take advantage of\nlimited implicit context as it appears during training. We instead show that\nrecognizing materials purely from their local appearance and integrating\nseparately recognized global contextual cues including objects and places leads\nto superior dense, per-pixel, material recognition. We achieve this by training\na fully-convolutional material recognition network end-to-end with only\nmaterial category supervision. We integrate object and place estimates to this\nnetwork from independent CNNs. This approach avoids the necessity of preparing\nan impractically-large amount of training data to cover the product space of\nmaterials, objects, and scenes, while fully leveraging contextual cues for\ndense material recognition. Furthermore, we perform a detailed analysis of the\neffects of context granularity, spatial resolution, and the network level at\nwhich we introduce context. On a recently introduced comprehensive and diverse\nmaterial database \\cite{Schwartz2016}, we confirm that our method achieves\nstate-of-the-art accuracy with significantly less training data compared to\npast methods. \n\n"}
{"id": "1611.09430", "contents": "Title: Emergence of foveal image sampling from learning to attend in visual\n  scenes Abstract: We describe a neural attention model with a learnable retinal sampling\nlattice. The model is trained on a visual search task requiring the\nclassification of an object embedded in a visual scene amidst background\ndistractors using the smallest number of fixations. We explore the tiling\nproperties that emerge in the model's retinal sampling lattice after training.\nSpecifically, we show that this lattice resembles the eccentricity dependent\nsampling lattice of the primate retina, with a high resolution region in the\nfovea surrounded by a low resolution periphery. Furthermore, we find conditions\nwhere these emergent properties are amplified or eliminated providing clues to\ntheir function. \n\n"}
{"id": "1612.00603", "contents": "Title: A Point Set Generation Network for 3D Object Reconstruction from a\n  Single Image Abstract: Generation of 3D data by deep neural network has been attracting increasing\nattention in the research community. The majority of extant works resort to\nregular representations such as volumetric grids or collection of images;\nhowever, these representations obscure the natural invariance of 3D shapes\nunder geometric transformations and also suffer from a number of other issues.\nIn this paper we address the problem of 3D reconstruction from a single image,\ngenerating a straight-forward form of output -- point cloud coordinates. Along\nwith this problem arises a unique and interesting issue, that the groundtruth\nshape for an input image may be ambiguous. Driven by this unorthodox output\nform and the inherent ambiguity in groundtruth, we design architecture, loss\nfunction and learning paradigm that are novel and effective. Our final solution\nis a conditional shape sampler, capable of predicting multiple plausible 3D\npoint clouds from an input image. In experiments not only can our system\noutperform state-of-the-art methods on single image based 3d reconstruction\nbenchmarks; but it also shows a strong performance for 3d shape completion and\npromising ability in making multiple plausible predictions. \n\n"}
{"id": "1612.00686", "contents": "Title: Identifying and Categorizing Anomalies in Retinal Imaging Data Abstract: The identification and quantification of markers in medical images is\ncritical for diagnosis, prognosis and management of patients in clinical\npractice. Supervised- or weakly supervised training enables the detection of\nfindings that are known a priori. It does not scale well, and a priori\ndefinition limits the vocabulary of markers to known entities reducing the\naccuracy of diagnosis and prognosis. Here, we propose the identification of\nanomalies in large-scale medical imaging data using healthy examples as a\nreference. We detect and categorize candidates for anomaly findings untypical\nfor the observed data. A deep convolutional autoencoder is trained on healthy\nretinal images. The learned model generates a new feature representation, and\nthe distribution of healthy retinal patches is estimated by a one-class support\nvector machine. Results demonstrate that we can identify pathologic regions in\nimages without using expert annotations. A subsequent clustering categorizes\nfindings into clinically meaningful classes. In addition the learned features\noutperform standard embedding approaches in a classification task. \n\n"}
{"id": "1612.00835", "contents": "Title: Scribbler: Controlling Deep Image Synthesis with Sketch and Color Abstract: Recently, there have been several promising methods to generate realistic\nimagery from deep convolutional networks. These methods sidestep the\ntraditional computer graphics rendering pipeline and instead generate imagery\nat the pixel level by learning from large collections of photos (e.g. faces or\nbedrooms). However, these methods are of limited utility because it is\ndifficult for a user to control what the network produces. In this paper, we\npropose a deep adversarial image synthesis architecture that is conditioned on\nsketched boundaries and sparse color strokes to generate realistic cars,\nbedrooms, or faces. We demonstrate a sketch based image synthesis system which\nallows users to 'scribble' over the sketch to indicate preferred color for\nobjects. Our network can then generate convincing images that satisfy both the\ncolor and the sketch constraints of user. The network is feed-forward which\nallows users to see the effect of their edits in real time. We compare to\nrecent work on sketch to image synthesis and show that our approach can\ngenerate more realistic, more diverse, and more controllable outputs. The\narchitecture is also effective at user-guided colorization of grayscale images. \n\n"}
{"id": "1612.01079", "contents": "Title: End-to-end Learning of Driving Models from Large-scale Video Datasets Abstract: Robust perception-action models should be learned from training data with\ndiverse visual appearances and realistic behaviors, yet current approaches to\ndeep visuomotor policy learning have been generally limited to in-situ models\nlearned from a single vehicle or a simulation environment. We advocate learning\na generic vehicle motion model from large scale crowd-sourced video data, and\ndevelop an end-to-end trainable architecture for learning to predict a\ndistribution over future vehicle egomotion from instantaneous monocular camera\nobservations and previous vehicle state. Our model incorporates a novel\nFCN-LSTM architecture, which can be learned from large-scale crowd-sourced\nvehicle action data, and leverages available scene segmentation side tasks to\nimprove performance under a privileged learning paradigm. \n\n"}
{"id": "1612.01942", "contents": "Title: Semi-Supervised Learning with the Deep Rendering Mixture Model Abstract: Semi-supervised learning algorithms reduce the high cost of acquiring labeled\ntraining data by using both labeled and unlabeled data during learning. Deep\nConvolutional Networks (DCNs) have achieved great success in supervised tasks\nand as such have been widely employed in the semi-supervised learning. In this\npaper we leverage the recently developed Deep Rendering Mixture Model (DRMM), a\nprobabilistic generative model that models latent nuisance variation, and whose\ninference algorithm yields DCNs. We develop an EM algorithm for the DRMM to\nlearn from both labeled and unlabeled data. Guided by the theory of the DRMM,\nwe introduce a novel non-negativity constraint and a variational inference\nterm. We report state-of-the-art performance on MNIST and SVHN and competitive\nresults on CIFAR10. We also probe deeper into how a DRMM trained in a\nsemi-supervised setting represents latent nuisance variation using\nsynthetically rendered images. Taken together, our work provides a unified\nframework for supervised, unsupervised, and semi-supervised learning. \n\n"}
{"id": "1612.02184", "contents": "Title: Saliency Driven Image Manipulation Abstract: Have you ever taken a picture only to find out that an unimportant background\nobject ended up being overly salient? Or one of those team sports photos where\nyour favorite player blends with the rest? Wouldn't it be nice if you could\ntweak these pictures just a little bit so that the distractor would be\nattenuated and your favorite player will stand-out among her peers?\nManipulating images in order to control the saliency of objects is the goal of\nthis paper. We propose an approach that considers the internal color and\nsaliency properties of the image. It changes the saliency map via an\noptimization framework that relies on patch-based manipulation using only\npatches from within the same image to achieve realistic looking results.\nApplications include object enhancement, distractors attenuation and background\ndecluttering. Comparing our method to previous ones shows significant\nimprovement, both in the achieved saliency manipulation and in the realistic\nappearance of the resulting images. \n\n"}
{"id": "1612.02808", "contents": "Title: 3D Shape Segmentation with Projective Convolutional Networks Abstract: This paper introduces a deep architecture for segmenting 3D objects into\ntheir labeled semantic parts. Our architecture combines image-based Fully\nConvolutional Networks (FCNs) and surface-based Conditional Random Fields\n(CRFs) to yield coherent segmentations of 3D shapes. The image-based FCNs are\nused for efficient view-based reasoning about 3D object parts. Through a\nspecial projection layer, FCN outputs are effectively aggregated across\nmultiple views and scales, then are projected onto the 3D object surfaces.\nFinally, a surface-based CRF combines the projected outputs with geometric\nconsistency cues to yield coherent segmentations. The whole architecture\n(multi-view FCNs and CRF) is trained end-to-end. Our approach significantly\noutperforms the existing state-of-the-art methods in the currently largest\nsegmentation benchmark (ShapeNet). Finally, we demonstrate promising\nsegmentation results on noisy 3D shapes acquired from consumer-grade depth\ncameras. \n\n"}
{"id": "1612.03925", "contents": "Title: 3D fully convolutional networks for subcortical segmentation in MRI: A\n  large-scale study Abstract: This study investigates a 3D and fully convolutional neural network (CNN) for\nsubcortical brain structure segmentation in MRI. 3D CNN architectures have been\ngenerally avoided due to their computational and memory requirements during\ninference. We address the problem via small kernels, allowing deeper\narchitectures. We further model both local and global context by embedding\nintermediate-layer outputs in the final prediction, which encourages\nconsistency between features extracted at different scales and embeds\nfine-grained information directly in the segmentation process. Our model is\nefficiently trained end-to-end on a graphics processing unit (GPU), in a single\nstage, exploiting the dense inference capabilities of fully CNNs.\n  We performed comprehensive experiments over two publicly available datasets.\nFirst, we demonstrate a state-of-the-art performance on the ISBR dataset. Then,\nwe report a {\\em large-scale} multi-site evaluation over 1112 unregistered\nsubject datasets acquired from 17 different sites (ABIDE dataset), with ages\nranging from 7 to 64 years, showing that our method is robust to various\nacquisition protocols, demographics and clinical factors. Our method yielded\nsegmentations that are highly consistent with a standard atlas-based approach,\nwhile running in a fraction of the time needed by atlas-based methods and\navoiding registration/normalization steps. This makes it convenient for massive\nmulti-site neuroanatomical imaging studies. To the best of our knowledge, our\nwork is the first to study subcortical structure segmentation on such\nlarge-scale and heterogeneous data. \n\n"}
{"id": "1612.03928", "contents": "Title: Paying More Attention to Attention: Improving the Performance of\n  Convolutional Neural Networks via Attention Transfer Abstract: Attention plays a critical role in human visual experience. Furthermore, it\nhas recently been demonstrated that attention can also play an important role\nin the context of applying artificial neural networks to a variety of tasks\nfrom fields such as computer vision and NLP. In this work we show that, by\nproperly defining attention for convolutional neural networks, we can actually\nuse this type of information in order to significantly improve the performance\nof a student CNN network by forcing it to mimic the attention maps of a\npowerful teacher network. To that end, we propose several novel methods of\ntransferring attention, showing consistent improvement across a variety of\ndatasets and convolutional neural network architectures. Code and models for\nour experiments are available at\nhttps://github.com/szagoruyko/attention-transfer \n\n"}
{"id": "1612.05323", "contents": "Title: A Stochastic Large Deformation Model for Computational Anatomy Abstract: In the study of shapes of human organs using computational anatomy,\nvariations are found to arise from inter-subject anatomical differences,\ndisease-specific effects, and measurement noise. This paper introduces a\nstochastic model for incorporating random variations into the Large Deformation\nDiffeomorphic Metric Mapping (LDDMM) framework. By accounting for randomness in\na particular setup which is crafted to fit the geometrical properties of LDDMM,\nwe formulate the template estimation problem for landmarks with noise and give\ntwo methods for efficiently estimating the parameters of the noise fields from\na prescribed data set. One method directly approximates the time evolution of\nthe variance of each landmark by a finite set of differential equations, and\nthe other is based on an Expectation-Maximisation algorithm. In the second\nmethod, the evaluation of the data likelihood is achieved without registering\nthe landmarks, by applying bridge sampling using a stochastically perturbed\nversion of the large deformation gradient flow algorithm. The method and the\nestimation algorithms are experimentally validated on synthetic examples and\nshape data of human corpora callosa. \n\n"}
{"id": "1612.05571", "contents": "Title: Delta Networks for Optimized Recurrent Network Computation Abstract: Many neural networks exhibit stability in their activation patterns over time\nin response to inputs from sensors operating under real-world conditions. By\ncapitalizing on this property of natural signals, we propose a Recurrent Neural\nNetwork (RNN) architecture called a delta network in which each neuron\ntransmits its value only when the change in its activation exceeds a threshold.\nThe execution of RNNs as delta networks is attractive because their states must\nbe stored and fetched at every timestep, unlike in convolutional neural\nnetworks (CNNs). We show that a naive run-time delta network implementation\noffers modest improvements on the number of memory accesses and computes, but\noptimized training techniques confer higher accuracy at higher speedup. With\nthese optimizations, we demonstrate a 9X reduction in cost with negligible loss\nof accuracy for the TIDIGITS audio digit recognition benchmark. Similarly, on\nthe large Wall Street Journal speech recognition benchmark even existing\nnetworks can be greatly accelerated as delta networks, and a 5.7x improvement\nwith negligible loss of accuracy can be obtained through training. Finally, on\nan end-to-end CNN trained for steering angle prediction in a driving dataset,\nthe RNN cost can be reduced by a substantial 100X. \n\n"}
{"id": "1612.06341", "contents": "Title: Semantic Jitter: Dense Supervision for Visual Comparisons via Synthetic\n  Images Abstract: Distinguishing subtle differences in attributes is valuable, yet learning to\nmake visual comparisons remains non-trivial. Not only is the number of possible\ncomparisons quadratic in the number of training images, but also access to\nimages adequately spanning the space of fine-grained visual differences is\nlimited. We propose to overcome the sparsity of supervision problem via\nsynthetically generated images. Building on a state-of-the-art image generation\nengine, we sample pairs of training images exhibiting slight modifications of\nindividual attributes. Augmenting real training image pairs with these\nexamples, we then train attribute ranking models to predict the relative\nstrength of an attribute in novel pairs of real images. Our results on datasets\nof faces and fashion images show the great promise of bootstrapping imperfect\nimage generators to counteract sample sparsity for learning to rank. \n\n"}
{"id": "1612.08012", "contents": "Title: Validation, comparison, and combination of algorithms for automatic\n  detection of pulmonary nodules in computed tomography images: the LUNA16\n  challenge Abstract: Automatic detection of pulmonary nodules in thoracic computed tomography (CT)\nscans has been an active area of research for the last two decades. However,\nthere have only been few studies that provide a comparative performance\nevaluation of different systems on a common database. We have therefore set up\nthe LUNA16 challenge, an objective evaluation framework for automatic nodule\ndetection algorithms using the largest publicly available reference database of\nchest CT scans, the LIDC-IDRI data set. In LUNA16, participants develop their\nalgorithm and upload their predictions on 888 CT scans in one of the two\ntracks: 1) the complete nodule detection track where a complete CAD system\nshould be developed, or 2) the false positive reduction track where a provided\nset of nodule candidates should be classified. This paper describes the setup\nof LUNA16 and presents the results of the challenge so far. Moreover, the\nimpact of combining individual systems on the detection performance was also\ninvestigated. It was observed that the leading solutions employed convolutional\nnetworks and used the provided set of nodule candidates. The combination of\nthese solutions achieved an excellent sensitivity of over 95% at fewer than 1.0\nfalse positives per scan. This highlights the potential of combining algorithms\nto improve the detection performance. Our observer study with four expert\nreaders has shown that the best system detects nodules that were missed by\nexpert readers who originally annotated the LIDC-IDRI data. We released this\nset of additional nodules for further development of CAD systems. \n\n"}
{"id": "1612.09322", "contents": "Title: Deep Learning Logo Detection with Data Expansion by Synthesising Context Abstract: Logo detection in unconstrained images is challenging, particularly when only\nvery sparse labelled training images are accessible due to high labelling\ncosts. In this work, we describe a model training image synthesising method\ncapable of improving significantly logo detection performance when only a\nhandful of (e.g., 10) labelled training images captured in realistic context\nare available, avoiding extensive manual labelling costs. Specifically, we\ndesign a novel algorithm for generating Synthetic Context Logo (SCL) training\nimages to increase model robustness against unknown background clutters,\nresulting in superior logo detection performance. For benchmarking model\nperformance, we introduce a new logo detection dataset TopLogo-10 collected\nfrom top 10 most popular clothing/wearable brandname logos captured in rich\nvisual context. Extensive comparisons show the advantages of our proposed SCL\nmodel over the state-of-the-art alternatives for logo detection using two\nreal-world logo benchmark datasets: FlickrLogo-32 and our new TopLogo-10. \n\n"}
{"id": "1701.00160", "contents": "Title: NIPS 2016 Tutorial: Generative Adversarial Networks Abstract: This report summarizes the tutorial presented by the author at NIPS 2016 on\ngenerative adversarial networks (GANs). The tutorial describes: (1) Why\ngenerative modeling is a topic worth studying, (2) how generative models work,\nand how GANs compare to other generative models, (3) the details of how GANs\nwork, (4) research frontiers in GANs, and (5) state-of-the-art image models\nthat combine GANs with other methods. Finally, the tutorial contains three\nexercises for readers to complete, and the solutions to these exercises. \n\n"}
{"id": "1701.00794", "contents": "Title: Constrained Deep Weak Supervision for Histopathology Image Segmentation Abstract: In this paper, we develop a new weakly-supervised learning algorithm to learn\nto segment cancerous regions in histopathology images. Our work is under a\nmultiple instance learning framework (MIL) with a new formulation, deep weak\nsupervision (DWS); we also propose an effective way to introduce constraints to\nour neural networks to assist the learning process. The contributions of our\nalgorithm are threefold: (1) We build an end-to-end learning system that\nsegments cancerous regions with fully convolutional networks (FCN) in which\nimage-to-image weakly-supervised learning is performed. (2) We develop a deep\nweek supervision formulation to exploit multi-scale learning under weak\nsupervision within fully convolutional networks. (3) Constraints about positive\ninstances are introduced in our approach to effectively explore additional\nweakly-supervised information that is easy to obtain and enjoys a significant\nboost to the learning process. The proposed algorithm, abbreviated as DWS-MIL,\nis easy to implement and can be trained efficiently. Our system demonstrates\nstate-of-the-art results on large-scale histopathology image datasets and can\nbe applied to various applications in medical imaging beyond histopathology\nimages such as MRI, CT, and ultrasound images. \n\n"}
{"id": "1701.03153", "contents": "Title: Looking Beyond Appearances: Synthetic Training Data for Deep CNNs in\n  Re-identification Abstract: Re-identification is generally carried out by encoding the appearance of a\nsubject in terms of outfit, suggesting scenarios where people do not change\ntheir attire. In this paper we overcome this restriction, by proposing a\nframework based on a deep convolutional neural network, SOMAnet, that\nadditionally models other discriminative aspects, namely, structural attributes\nof the human figure (e.g. height, obesity, gender). Our method is unique in\nmany respects. First, SOMAnet is based on the Inception architecture, departing\nfrom the usual siamese framework. This spares expensive data preparation\n(pairing images across cameras) and allows the understanding of what the\nnetwork learned. Second, and most notably, the training data consists of a\nsynthetic 100K instance dataset, SOMAset, created by photorealistic human body\ngeneration software. Synthetic data represents a good compromise between\nrealistic imagery, usually not required in re-identification since surveillance\ncameras capture low-resolution silhouettes, and complete control of the\nsamples, which is useful in order to customize the data w.r.t. the surveillance\nscenario at-hand, e.g. ethnicity. SOMAnet, trained on SOMAset and fine-tuned on\nrecent re-identification benchmarks, outperforms all competitors, matching\nsubjects even with different apparel. The combination of synthetic data with\nInception architectures opens up new research avenues in re-identification. \n\n"}
{"id": "1701.05818", "contents": "Title: Fusion of Heterogeneous Data in Convolutional Networks for Urban\n  Semantic Labeling (Invited Paper) Abstract: In this work, we present a novel module to perform fusion of heterogeneous\ndata using fully convolutional networks for semantic labeling. We introduce\nresidual correction as a way to learn how to fuse predictions coming out of a\ndual stream architecture. Especially, we perform fusion of DSM and IRRG optical\ndata on the ISPRS Vaihingen dataset over a urban area and obtain new\nstate-of-the-art results. \n\n"}
{"id": "1702.00186", "contents": "Title: A Kinematic Chain Space for Monocular Motion Capture Abstract: This paper deals with motion capture of kinematic chains (e.g. human\nskeletons) from monocular image sequences taken by uncalibrated cameras. We\npresent a method based on projecting an observation into a kinematic chain\nspace (KCS). An optimization of the nuclear norm is proposed that implicitly\nenforces structural properties of the kinematic chain. Unlike other approaches\nour method does not require specific camera or object motion and is not relying\non training data or previously determined constraints such as particular body\nlengths. The proposed algorithm is able to reconstruct scenes with limited\ncamera motion and previously unseen motions. It is not only applicable to human\nskeletons but also to other kinematic chains for instance animals or industrial\nrobots. We achieve state-of-the-art results on different benchmark data bases\nand real world scenes. \n\n"}
{"id": "1702.01005", "contents": "Title: Intrinsic Grassmann Averages for Online Linear, Robust and Nonlinear\n  Subspace Learning Abstract: Principal Component Analysis (PCA) and Kernel Principal Component Analysis\n(KPCA) are fundamental methods in machine learning for dimensionality\nreduction. The former is a technique for finding this approximation in finite\ndimensions and the latter is often in an infinite dimensional Reproducing\nKernel Hilbert-space (RKHS). In this paper, we present a geometric framework\nfor computing the principal linear subspaces in both situations as well as for\nthe robust PCA case, that amounts to computing the intrinsic average on the\nspace of all subspaces: the Grassmann manifold. Points on this manifold are\ndefined as the subspaces spanned by $K$-tuples of observations. The intrinsic\nGrassmann average of these subspaces are shown to coincide with the principal\ncomponents of the observations when they are drawn from a Gaussian\ndistribution. We show similar results in the RKHS case and provide an efficient\nalgorithm for computing the projection onto the this average subspace. The\nresult is a method akin to KPCA which is substantially faster. Further, we\npresent a novel online version of the KPCA using our geometric framework.\nCompetitive performance of all our algorithms are demonstrated on a variety of\nreal and synthetic data sets. \n\n"}
{"id": "1702.02217", "contents": "Title: Multitask Evolution with Cartesian Genetic Programming Abstract: We introduce a genetic programming method for solving multiple Boolean\ncircuit synthesis tasks simultaneously. This allows us to solve a set of\nelementary logic functions twice as easily as with a direct, single-task\napproach. \n\n"}
{"id": "1702.04710", "contents": "Title: Multi-Task Convolutional Neural Network for Pose-Invariant Face\n  Recognition Abstract: This paper explores multi-task learning (MTL) for face recognition. We answer\nthe questions of how and why MTL can improve the face recognition performance.\nFirst, we propose a multi-task Convolutional Neural Network (CNN) for face\nrecognition where identity classification is the main task and pose,\nillumination, and expression estimations are the side tasks. Second, we develop\na dynamic-weighting scheme to automatically assign the loss weight to each side\ntask, which is a crucial problem in MTL. Third, we propose a pose-directed\nmulti-task CNN by grouping different poses to learn pose-specific identity\nfeatures, simultaneously across all poses. Last but not least, we propose an\nenergy-based weight analysis method to explore how CNN-based MTL works. We\nobserve that the side tasks serve as regularizations to disentangle the\nvariations from the learnt identity features. Extensive experiments on the\nentire Multi-PIE dataset demonstrate the effectiveness of the proposed\napproach. To the best of our knowledge, this is the first work using all data\nin Multi-PIE for face recognition. Our approach is also applicable to\nin-the-wild datasets for pose-invariant face recognition and achieves\ncomparable or better performance than state of the art on LFW, CFP, and IJB-A\ndatasets. \n\n"}
{"id": "1702.05068", "contents": "Title: Discovering objects and their relations from entangled scene\n  representations Abstract: Our world can be succinctly and compactly described as structured scenes of\nobjects and relations. A typical room, for example, contains salient objects\nsuch as tables, chairs and books, and these objects typically relate to each\nother by their underlying causes and semantics. This gives rise to correlated\nfeatures, such as position, function and shape. Humans exploit knowledge of\nobjects and their relations for learning a wide spectrum of tasks, and more\ngenerally when learning the structure underlying observed data. In this work,\nwe introduce relation networks (RNs) - a general purpose neural network\narchitecture for object-relation reasoning. We show that RNs are capable of\nlearning object relations from scene description data. Furthermore, we show\nthat RNs can act as a bottleneck that induces the factorization of objects from\nentangled scene description inputs, and from distributed deep representations\nof scene images provided by a variational autoencoder. The model can also be\nused in conjunction with differentiable memory mechanisms for implicit relation\ndiscovery in one-shot learning tasks. Our results suggest that relation\nnetworks are a potentially powerful architecture for solving a variety of\nproblems that require object relation reasoning. \n\n"}
{"id": "1702.05448", "contents": "Title: Learning to Detect Human-Object Interactions Abstract: We study the problem of detecting human-object interactions (HOI) in static\nimages, defined as predicting a human and an object bounding box with an\ninteraction class label that connects them. HOI detection is a fundamental\nproblem in computer vision as it provides semantic information about the\ninteractions among the detected objects. We introduce HICO-DET, a new large\nbenchmark for HOI detection, by augmenting the current HICO classification\nbenchmark with instance annotations. To solve the task, we propose Human-Object\nRegion-based Convolutional Neural Networks (HO-RCNN). At the core of our\nHO-RCNN is the Interaction Pattern, a novel DNN input that characterizes the\nspatial relations between two bounding boxes. Experiments on HICO-DET\ndemonstrate that our HO-RCNN, by exploiting human-object spatial relations\nthrough Interaction Patterns, significantly improves the performance of HOI\ndetection over baseline approaches. \n\n"}
{"id": "1702.05552", "contents": "Title: Soft + Hardwired Attention: An LSTM Framework for Human Trajectory\n  Prediction and Abnormal Event Detection Abstract: As humans we possess an intuitive ability for navigation which we master\nthrough years of practice; however existing approaches to model this trait for\ndiverse tasks including monitoring pedestrian flow and detecting abnormal\nevents have been limited by using a variety of hand-crafted features. Recent\nresearch in the area of deep-learning has demonstrated the power of learning\nfeatures directly from the data; and related research in recurrent neural\nnetworks has shown exemplary results in sequence-to-sequence problems such as\nneural machine translation and neural image caption generation. Motivated by\nthese approaches, we propose a novel method to predict the future motion of a\npedestrian given a short history of their, and their neighbours, past\nbehaviour. The novelty of the proposed method is the combined attention model\nwhich utilises both \"soft attention\" as well as \"hard-wired\" attention in order\nto map the trajectory information from the local neighbourhood to the future\npositions of the pedestrian of interest. We illustrate how a simple\napproximation of attention weights (i.e hard-wired) can be merged together with\nsoft attention weights in order to make our model applicable for challenging\nreal world scenarios with hundreds of neighbours. The navigational capability\nof the proposed method is tested on two challenging publicly available\nsurveillance databases where our model outperforms the current-state-of-the-art\nmethods. Additionally, we illustrate how the proposed architecture can be\ndirectly applied for the task of abnormal event detection without handcrafting\nthe features. \n\n"}
{"id": "1702.05729", "contents": "Title: Person Search with Natural Language Description Abstract: Searching persons in large-scale image databases with the query of natural\nlanguage description has important applications in video surveillance. Existing\nmethods mainly focused on searching persons with image-based or attribute-based\nqueries, which have major limitations for a practical usage. In this paper, we\nstudy the problem of person search with natural language description. Given the\ntextual description of a person, the algorithm of the person search is required\nto rank all the samples in the person database then retrieve the most relevant\nsample corresponding to the queried description. Since there is no person\ndataset or benchmark with textual description available, we collect a\nlarge-scale person description dataset with detailed natural language\nannotations and person samples from various sources, termed as CUHK Person\nDescription Dataset (CUHK-PEDES). A wide range of possible models and baselines\nhave been evaluated and compared on the person search benchmark. An Recurrent\nNeural Network with Gated Neural Attention mechanism (GNA-RNN) is proposed to\nestablish the state-of-the art performance on person search. \n\n"}
{"id": "1702.06506", "contents": "Title: PixelNet: Representation of the pixels, by the pixels, and for the\n  pixels Abstract: We explore design principles for general pixel-level prediction problems,\nfrom low-level edge detection to mid-level surface normal estimation to\nhigh-level semantic segmentation. Convolutional predictors, such as the\nfully-convolutional network (FCN), have achieved remarkable success by\nexploiting the spatial redundancy of neighboring pixels through convolutional\nprocessing. Though computationally efficient, we point out that such approaches\nare not statistically efficient during learning precisely because spatial\nredundancy limits the information learned from neighboring pixels. We\ndemonstrate that stratified sampling of pixels allows one to (1) add diversity\nduring batch updates, speeding up learning; (2) explore complex nonlinear\npredictors, improving accuracy; and (3) efficiently train state-of-the-art\nmodels tabula rasa (i.e., \"from scratch\") for diverse pixel-labeling tasks. Our\nsingle architecture produces state-of-the-art results for semantic segmentation\non PASCAL-Context dataset, surface normal estimation on NYUDv2 depth dataset,\nand edge detection on BSDS. \n\n"}
{"id": "1702.07097", "contents": "Title: Adaptive Bidirectional Backpropagation: Towards Biologically Plausible\n  Error Signal Transmission in Neural Networks Abstract: The back-propagation (BP) algorithm has been considered the de-facto method\nfor training deep neural networks. It back-propagates errors from the output\nlayer to the hidden layers in an exact manner using the transpose of the\nfeedforward weights. However, it has been argued that this is not biologically\nplausible because back-propagating error signals with the exact incoming\nweights are not considered possible in biological neural systems. In this work,\nwe propose a biologically plausible paradigm of neural architecture based on\nrelated literature in neuroscience and asymmetric BP-like methods.\nSpecifically, we propose two bidirectional learning algorithms with trainable\nfeedforward and feedback weights. The feedforward weights are used to relay\nactivations from the inputs to target outputs. The feedback weights pass the\nerror signals from the output layer to the hidden layers. Different from other\nasymmetric BP-like methods, the feedback weights are also plastic in our\nframework and are trained to approximate the forward activations. Preliminary\nresults show that our models outperform other asymmetric BP-like methods on the\nMNIST and the CIFAR-10 datasets. \n\n"}
{"id": "1702.07492", "contents": "Title: Robot gains Social Intelligence through Multimodal Deep Reinforcement\n  Learning Abstract: For robots to coexist with humans in a social world like ours, it is crucial\nthat they possess human-like social interaction skills. Programming a robot to\npossess such skills is a challenging task. In this paper, we propose a\nMultimodal Deep Q-Network (MDQN) to enable a robot to learn human-like\ninteraction skills through a trial and error method. This paper aims to develop\na robot that gathers data during its interaction with a human and learns human\ninteraction behaviour from the high-dimensional sensory information using\nend-to-end reinforcement learning. This paper demonstrates that the robot was\nable to learn basic interaction skills successfully, after 14 days of\ninteracting with people. \n\n"}
{"id": "1702.07811", "contents": "Title: Adaptive Neural Networks for Efficient Inference Abstract: We present an approach to adaptively utilize deep neural networks in order to\nreduce the evaluation time on new examples without loss of accuracy. Rather\nthan attempting to redesign or approximate existing networks, we propose two\nschemes that adaptively utilize networks. We first pose an adaptive network\nevaluation scheme, where we learn a system to adaptively choose the components\nof a deep network to be evaluated for each example. By allowing examples\ncorrectly classified using early layers of the system to exit, we avoid the\ncomputational time associated with full evaluation of the network. We extend\nthis to learn a network selection system that adaptively selects the network to\nbe evaluated for each example. We show that computational time can be\ndramatically reduced by exploiting the fact that many examples can be correctly\nclassified using relatively efficient networks and that complex,\ncomputationally costly networks are only necessary for a small fraction of\nexamples. We pose a global objective for learning an adaptive early exit or\nnetwork selection policy and solve it by reducing the policy learning problem\nto a layer-by-layer weighted binary classification problem. Empirically, these\napproaches yield dramatic reductions in computational cost, with up to a 2.8x\nspeedup on state-of-the-art networks from the ImageNet image recognition\nchallenge with minimal (<1%) loss of top5 accuracy. \n\n"}
{"id": "1702.08014", "contents": "Title: Adversarial Networks for the Detection of Aggressive Prostate Cancer Abstract: Semantic segmentation constitutes an integral part of medical image analyses\nfor which breakthroughs in the field of deep learning were of high relevance.\nThe large number of trainable parameters of deep neural networks however\nrenders them inherently data hungry, a characteristic that heavily challenges\nthe medical imaging community. Though interestingly, with the de facto standard\ntraining of fully convolutional networks (FCNs) for semantic segmentation being\nagnostic towards the `structure' of the predicted label maps, valuable\ncomplementary information about the global quality of the segmentation lies\nidle. In order to tap into this potential, we propose utilizing an adversarial\nnetwork which discriminates between expert and generated annotations in order\nto train FCNs for semantic segmentation. Because the adversary constitutes a\nlearned parametrization of what makes a good segmentation at a global level, we\nhypothesize that the method holds particular advantages for segmentation tasks\non complex structured, small datasets. This holds true in our experiments: We\nlearn to segment aggressive prostate cancer utilizing MRI images of 152\npatients and show that the proposed scheme is superior over the de facto\nstandard in terms of the detection sensitivity and the dice-score for\naggressive prostate cancer. The achieved relative gains are shown to be\nparticularly pronounced in the small dataset limit. \n\n"}
{"id": "1702.08727", "contents": "Title: Improving the Neural GPU Architecture for Algorithm Learning Abstract: Algorithm learning is a core problem in artificial intelligence with\nsignificant implications on automation level that can be achieved by machines.\nRecently deep learning methods are emerging for synthesizing an algorithm from\nits input-output examples, the most successful being the Neural GPU, capable of\nlearning multiplication. We present several improvements to the Neural GPU that\nsubstantially reduces training time and improves generalization. We introduce a\nnew technique - hard nonlinearities with saturation costs- that has general\napplicability. We also introduce a technique of diagonal gates that can be\napplied to active-memory models. The proposed architecture is the first capable\nof learning decimal multiplication end-to-end. \n\n"}
{"id": "1703.01101", "contents": "Title: Adversarial Examples for Semantic Image Segmentation Abstract: Machine learning methods in general and Deep Neural Networks in particular\nhave shown to be vulnerable to adversarial perturbations. So far this\nphenomenon has mainly been studied in the context of whole-image\nclassification. In this contribution, we analyse how adversarial perturbations\ncan affect the task of semantic segmentation. We show how existing adversarial\nattackers can be transferred to this task and that it is possible to create\nimperceptible adversarial perturbations that lead a deep network to misclassify\nalmost all pixels of a chosen class while leaving network prediction nearly\nunchanged outside this class. \n\n"}
{"id": "1703.02445", "contents": "Title: Object classification in images of Neoclassical furniture using Deep\n  Learning Abstract: This short paper outlines research results on object classification in images\nof Neoclassical furniture. The motivation was to provide an object recognition\nframework which is able to support the alignment of furniture images with a\nsymbolic level model. A data-driven bottom-up research routine in the\nNeoclassica research framework is the main use-case. It strives to deliver\ntools for analyzing the spread of aesthetic forms which are considered as a\ncultural transfer process. \n\n"}
{"id": "1703.03492", "contents": "Title: A New Representation of Skeleton Sequences for 3D Action Recognition Abstract: This paper presents a new method for 3D action recognition with skeleton\nsequences (i.e., 3D trajectories of human skeleton joints). The proposed method\nfirst transforms each skeleton sequence into three clips each consisting of\nseveral frames for spatial temporal feature learning using deep neural\nnetworks. Each clip is generated from one channel of the cylindrical\ncoordinates of the skeleton sequence. Each frame of the generated clips\nrepresents the temporal information of the entire skeleton sequence, and\nincorporates one particular spatial relationship between the joints. The entire\nclips include multiple frames with different spatial relationships, which\nprovide useful spatial structural information of the human skeleton. We propose\nto use deep convolutional neural networks to learn long-term temporal\ninformation of the skeleton sequence from the frames of the generated clips,\nand then use a Multi-Task Learning Network (MTLN) to jointly process all frames\nof the generated clips in parallel to incorporate spatial structural\ninformation for action recognition. Experimental results clearly show the\neffectiveness of the proposed new representation and feature learning method\nfor 3D action recognition. \n\n"}
{"id": "1703.04309", "contents": "Title: End-to-End Learning of Geometry and Context for Deep Stereo Regression Abstract: We propose a novel deep learning architecture for regressing disparity from a\nrectified pair of stereo images. We leverage knowledge of the problem's\ngeometry to form a cost volume using deep feature representations. We learn to\nincorporate contextual information using 3-D convolutions over this volume.\nDisparity values are regressed from the cost volume using a proposed\ndifferentiable soft argmin operation, which allows us to train our method\nend-to-end to sub-pixel accuracy without any additional post-processing or\nregularization. We evaluate our method on the Scene Flow and KITTI datasets and\non KITTI we set a new state-of-the-art benchmark, while being significantly\nfaster than competing approaches. \n\n"}
{"id": "1703.07255", "contents": "Title: ZM-Net: Real-time Zero-shot Image Manipulation Network Abstract: Many problems in image processing and computer vision (e.g. colorization,\nstyle transfer) can be posed as 'manipulating' an input image into a\ncorresponding output image given a user-specified guiding signal. A holy-grail\nsolution towards generic image manipulation should be able to efficiently alter\nan input image with any personalized signals (even signals unseen during\ntraining), such as diverse paintings and arbitrary descriptive attributes.\nHowever, existing methods are either inefficient to simultaneously process\nmultiple signals (let alone generalize to unseen signals), or unable to handle\nsignals from other modalities. In this paper, we make the first attempt to\naddress the zero-shot image manipulation task. We cast this problem as\nmanipulating an input image according to a parametric model whose key\nparameters can be conditionally generated from any guiding signal (even unseen\nones). To this end, we propose the Zero-shot Manipulation Net (ZM-Net), a\nfully-differentiable architecture that jointly optimizes an\nimage-transformation network (TNet) and a parameter network (PNet). The PNet\nlearns to generate key transformation parameters for the TNet given any guiding\nsignal while the TNet performs fast zero-shot image manipulation according to\nboth signal-dependent parameters from the PNet and signal-invariant parameters\nfrom the TNet itself. Extensive experiments show that our ZM-Net can perform\nhigh-quality image manipulation conditioned on different forms of guiding\nsignals (e.g. style images and attributes) in real-time (tens of milliseconds\nper image) even for unseen signals. Moreover, a large-scale style dataset with\nover 20,000 style images is also constructed to promote further research. \n\n"}
{"id": "1703.07402", "contents": "Title: Simple Online and Realtime Tracking with a Deep Association Metric Abstract: Simple Online and Realtime Tracking (SORT) is a pragmatic approach to\nmultiple object tracking with a focus on simple, effective algorithms. In this\npaper, we integrate appearance information to improve the performance of SORT.\nDue to this extension we are able to track objects through longer periods of\nocclusions, effectively reducing the number of identity switches. In spirit of\nthe original framework we place much of the computational complexity into an\noffline pre-training stage where we learn a deep association metric on a\nlarge-scale person re-identification dataset. During online application, we\nestablish measurement-to-track associations using nearest neighbor queries in\nvisual appearance space. Experimental evaluation shows that our extensions\nreduce the number of identity switches by 45%, achieving overall competitive\nperformance at high frame rates. \n\n"}
{"id": "1703.08378", "contents": "Title: Feature Fusion using Extended Jaccard Graph and Stochastic Gradient\n  Descent for Robot Abstract: Robot vision is a fundamental device for human-robot interaction and robot\ncomplex tasks. In this paper, we use Kinect and propose a feature graph fusion\n(FGF) for robot recognition. Our feature fusion utilizes RGB and depth\ninformation to construct fused feature from Kinect. FGF involves multi-Jaccard\nsimilarity to compute a robust graph and utilize word embedding method to\nenhance the recognition results. We also collect DUT RGB-D face dataset and a\nbenchmark datset to evaluate the effectiveness and efficiency of our method.\nThe experimental results illustrate FGF is robust and effective to face and\nobject datasets in robot applications. \n\n"}
{"id": "1703.08966", "contents": "Title: Mastering Sketching: Adversarial Augmentation for Structured Prediction Abstract: We present an integral framework for training sketch simplification networks\nthat convert challenging rough sketches into clean line drawings. Our approach\naugments a simplification network with a discriminator network, training both\nnetworks jointly so that the discriminator network discerns whether a line\ndrawing is a real training data or the output of the simplification network,\nwhich in turn tries to fool it. This approach has two major advantages. First,\nbecause the discriminator network learns the structure in line drawings, it\nencourages the output sketches of the simplification network to be more similar\nin appearance to the training sketches. Second, we can also train the\nsimplification network with additional unsupervised data, using the\ndiscriminator network as a substitute teacher. Thus, by adding only rough\nsketches without simplified line drawings, or only line drawings without the\noriginal rough sketches, we can improve the quality of the sketch\nsimplification. We show how our framework can be used to train models that\nsignificantly outperform the state of the art in the sketch simplification\ntask, despite using the same architecture for inference. We additionally\npresent an approach to optimize for a single image, which improves accuracy at\nthe cost of additional computation time. Finally, we show that, using the same\nframework, it is possible to train the network to perform the inverse problem,\ni.e., convert simple line sketches into pencil drawings, which is not possible\nusing the standard mean squared error loss. We validate our framework with two\nuser tests, where our approach is preferred to the state of the art in sketch\nsimplification 92.3% of the time and obtains 1.2 more points on a scale of 1 to\n5. \n\n"}
{"id": "1703.09393", "contents": "Title: Mixture of Counting CNNs: Adaptive Integration of CNNs Specialized to\n  Specific Appearance for Crowd Counting Abstract: This paper proposes a crowd counting method. Crowd counting is difficult\nbecause of large appearance changes of a target which caused by density and\nscale changes. Conventional crowd counting methods generally utilize one\npredictor (e,g., regression and multi-class classifier). However, such only one\npredictor can not count targets with large appearance changes well. In this\npaper, we propose to predict the number of targets using multiple CNNs\nspecialized to a specific appearance, and those CNNs are adaptively selected\naccording to the appearance of a test image. By integrating the selected CNNs,\nthe proposed method has the robustness to large appearance changes. In\nexperiments, we confirm that the proposed method can count crowd with lower\ncounting error than a CNN and integration of CNNs with fixed weights. Moreover,\nwe confirm that each predictor automatically specialized to a specific\nappearance. \n\n"}
{"id": "1703.09507", "contents": "Title: L2-constrained Softmax Loss for Discriminative Face Verification Abstract: In recent years, the performance of face verification systems has\nsignificantly improved using deep convolutional neural networks (DCNNs). A\ntypical pipeline for face verification includes training a deep network for\nsubject classification with softmax loss, using the penultimate layer output as\nthe feature descriptor, and generating a cosine similarity score given a pair\nof face images. The softmax loss function does not optimize the features to\nhave higher similarity score for positive pairs and lower similarity score for\nnegative pairs, which leads to a performance gap. In this paper, we add an\nL2-constraint to the feature descriptors which restricts them to lie on a\nhypersphere of a fixed radius. This module can be easily implemented using\nexisting deep learning frameworks. We show that integrating this simple step in\nthe training pipeline significantly boosts the performance of face\nverification. Specifically, we achieve state-of-the-art results on the\nchallenging IJB-A dataset, achieving True Accept Rate of 0.909 at False Accept\nRate 0.0001 on the face verification protocol. Additionally, we achieve\nstate-of-the-art performance on LFW dataset with an accuracy of 99.78%, and\ncompeting performance on YTF dataset with accuracy of 96.08%. \n\n"}
{"id": "1703.09684", "contents": "Title: An Analysis of Visual Question Answering Algorithms Abstract: In visual question answering (VQA), an algorithm must answer text-based\nquestions about images. While multiple datasets for VQA have been created since\nlate 2014, they all have flaws in both their content and the way algorithms are\nevaluated on them. As a result, evaluation scores are inflated and\npredominantly determined by answering easier questions, making it difficult to\ncompare different methods. In this paper, we analyze existing VQA algorithms\nusing a new dataset. It contains over 1.6 million questions organized into 12\ndifferent categories. We also introduce questions that are meaningless for a\ngiven image to force a VQA system to reason about image content. We propose new\nevaluation schemes that compensate for over-represented question-types and make\nit easier to study the strengths and weaknesses of algorithms. We analyze the\nperformance of both baseline and state-of-the-art VQA models, including\nmulti-modal compact bilinear pooling (MCB), neural module networks, and\nrecurrent answering units. Our experiments establish how attention helps\ncertain categories more than others, determine which models work better than\nothers, and explain how simple models (e.g. MLP) can surpass more complex\nmodels (MCB) by simply learning to answer large, easy question categories. \n\n"}
{"id": "1703.10553", "contents": "Title: Learning Convolutional Networks for Content-weighted Image Compression Abstract: Lossy image compression is generally formulated as a joint rate-distortion\noptimization to learn encoder, quantizer, and decoder. However, the quantizer\nis non-differentiable, and discrete entropy estimation usually is required for\nrate control. These make it very challenging to develop a convolutional network\n(CNN)-based image compression system. In this paper, motivated by that the\nlocal information content is spatially variant in an image, we suggest that the\nbit rate of the different parts of the image should be adapted to local\ncontent. And the content aware bit rate is allocated under the guidance of a\ncontent-weighted importance map. Thus, the sum of the importance map can serve\nas a continuous alternative of discrete entropy estimation to control\ncompression rate. And binarizer is adopted to quantize the output of encoder\ndue to the binarization scheme is also directly defined by the importance map.\nFurthermore, a proxy function is introduced for binary operation in backward\npropagation to make it differentiable. Therefore, the encoder, decoder,\nbinarizer and importance map can be jointly optimized in an end-to-end manner\nby using a subset of the ImageNet database. In low bit rate image compression,\nexperiments show that our system significantly outperforms JPEG and JPEG 2000\nby structural similarity (SSIM) index, and can produce the much better visual\nresult with sharp edges, rich textures, and fewer artifacts. \n\n"}
{"id": "1703.10580", "contents": "Title: MoFA: Model-based Deep Convolutional Face Autoencoder for Unsupervised\n  Monocular Reconstruction Abstract: In this work we propose a novel model-based deep convolutional autoencoder\nthat addresses the highly challenging problem of reconstructing a 3D human face\nfrom a single in-the-wild color image. To this end, we combine a convolutional\nencoder network with an expert-designed generative model that serves as\ndecoder. The core innovation is our new differentiable parametric decoder that\nencapsulates image formation analytically based on a generative model. Our\ndecoder takes as input a code vector with exactly defined semantic meaning that\nencodes detailed face pose, shape, expression, skin reflectance and scene\nillumination. Due to this new way of combining CNN-based with model-based face\nreconstruction, the CNN-based encoder learns to extract semantically meaningful\nparameters from a single monocular input image. For the first time, a CNN\nencoder and an expert-designed generative model can be trained end-to-end in an\nunsupervised manner, which renders training on very large (unlabeled) real\nworld data feasible. The obtained reconstructions compare favorably to current\nstate-of-the-art approaches in terms of quality and richness of representation. \n\n"}
{"id": "1704.00090", "contents": "Title: Learning to Predict Indoor Illumination from a Single Image Abstract: We propose an automatic method to infer high dynamic range illumination from\na single, limited field-of-view, low dynamic range photograph of an indoor\nscene. In contrast to previous work that relies on specialized image capture,\nuser input, and/or simple scene models, we train an end-to-end deep neural\nnetwork that directly regresses a limited field-of-view photo to HDR\nillumination, without strong assumptions on scene geometry, material\nproperties, or lighting. We show that this can be accomplished in a three step\nprocess: 1) we train a robust lighting classifier to automatically annotate the\nlocation of light sources in a large dataset of LDR environment maps, 2) we use\nthese annotations to train a deep neural network that predicts the location of\nlights in a scene from a single limited field-of-view photo, and 3) we\nfine-tune this network using a small dataset of HDR environment maps to predict\nlight intensities. This allows us to automatically recover high-quality HDR\nillumination estimates that significantly outperform previous state-of-the-art\nmethods. Consequently, using our illumination estimates for applications like\n3D object insertion, we can achieve results that are photo-realistic, which is\nvalidated via a perceptual user study. \n\n"}
{"id": "1704.00103", "contents": "Title: SafetyNet: Detecting and Rejecting Adversarial Examples Robustly Abstract: We describe a method to produce a network where current methods such as\nDeepFool have great difficulty producing adversarial samples. Our construction\nsuggests some insights into how deep networks work. We provide a reasonable\nanalyses that our construction is difficult to defeat, and show experimentally\nthat our method is hard to defeat with both Type I and Type II attacks using\nseveral standard networks and datasets. This SafetyNet architecture is used to\nan important and novel application SceneProof, which can reliably detect\nwhether an image is a picture of a real scene or not. SceneProof applies to\nimages captured with depth maps (RGBD images) and checks if a pair of image and\ndepth map is consistent. It relies on the relative difficulty of producing\nnaturalistic depth maps for images in post processing. We demonstrate that our\nSafetyNet is robust to adversarial examples built from currently known\nattacking approaches. \n\n"}
{"id": "1704.00275", "contents": "Title: SAR image despeckling through convolutional neural networks Abstract: In this paper we investigate the use of discriminative model learning through\nConvolutional Neural Networks (CNNs) for SAR image despeckling. The network\nuses a residual learning strategy, hence it does not recover the filtered\nimage, but the speckle component, which is then subtracted from the noisy one.\nTraining is carried out by considering a large multitemporal SAR image and its\nmultilook version, in order to approximate a clean image. Experimental results,\nboth on synthetic and real SAR data, show the method to achieve better\nperformance with respect to state-of-the-art techniques. \n\n"}
{"id": "1704.00498", "contents": "Title: Convolutional neural networks for segmentation and object detection of\n  human semen Abstract: We compare a set of convolutional neural network (CNN) architectures for the\ntask of segmenting and detecting human sperm cells in an image taken from a\nsemen sample. In contrast to previous work, samples are not stained or washed\nto allow for full sperm quality analysis, making analysis harder due to\nclutter. Our results indicate that training on full images is superior to\ntraining on patches when class-skew is properly handled. Full image training\nincluding up-sampling during training proves to be beneficial in deep CNNs for\npixel wise accuracy and detection performance. Predicted sperm cells are found\nby using connected components on the CNN predictions. We investigate\noptimization of a threshold parameter on the size of detected components. Our\nbest network achieves 93.87% precision and 91.89% recall on our test dataset\nafter thresholding outperforming a classical mage analysis approach. \n\n"}
{"id": "1704.00758", "contents": "Title: Unsupervised Action Proposal Ranking through Proposal Recombination Abstract: Recently, action proposal methods have played an important role in action\nrecognition tasks, as they reduce the search space dramatically. Most\nunsupervised action proposal methods tend to generate hundreds of action\nproposals which include many noisy, inconsistent, and unranked action\nproposals, while supervised action proposal methods take advantage of\npredefined object detectors (e.g., human detector) to refine and score the\naction proposals, but they require thousands of manual annotations to train.\n  Given the action proposals in a video, the goal of the proposed work is to\ngenerate a few better action proposals that are ranked properly. In our\napproach, we first divide action proposal into sub-proposal and then use\nDynamic Programming based graph optimization scheme to select the optimal\ncombinations of sub-proposals from different proposals and assign each new\nproposal a score. We propose a new unsupervised image-based actioness detector\nthat leverages web images and employs it as one of the node scores in our graph\nformulation. Moreover, we capture motion information by estimating the number\nof motion contours within each action proposal patch. The proposed method is an\nunsupervised method that neither needs bounding box annotations nor video level\nlabels, which is desirable with the current explosion of large-scale action\ndatasets. Our approach is generic and does not depend on a specific action\nproposal method. We evaluate our approach on several publicly available trimmed\nand un-trimmed datasets and obtain better performance compared to several\nproposal ranking methods. In addition, we demonstrate that properly ranked\nproposals produce significantly better action detection as compared to\nstate-of-the-art proposal based methods. \n\n"}
{"id": "1704.00763", "contents": "Title: AMC: Attention guided Multi-modal Correlation Learning for Image Search Abstract: Given a user's query, traditional image search systems rank images according\nto its relevance to a single modality (e.g., image content or surrounding\ntext). Nowadays, an increasing number of images on the Internet are available\nwith associated meta data in rich modalities (e.g., titles, keywords, tags,\netc.), which can be exploited for better similarity measure with queries. In\nthis paper, we leverage visual and textual modalities for image search by\nlearning their correlation with input query. According to the intent of query,\nattention mechanism can be introduced to adaptively balance the importance of\ndifferent modalities. We propose a novel Attention guided Multi-modal\nCorrelation (AMC) learning method which consists of a jointly learned hierarchy\nof intra and inter-attention networks. Conditioned on query's intent,\nintra-attention networks (i.e., visual intra-attention network and language\nintra-attention network) attend on informative parts within each modality; a\nmulti-modal inter-attention network promotes the importance of the most\nquery-relevant modalities. In experiments, we evaluate AMC models on the search\nlogs from two real world image search engines and show a significant boost on\nthe ranking of user-clicked images in search results. Additionally, we extend\nAMC models to caption ranking task on COCO dataset and achieve competitive\nresults compared with recent state-of-the-arts. \n\n"}
{"id": "1704.01137", "contents": "Title: DyVEDeep: Dynamic Variable Effort Deep Neural Networks Abstract: Deep Neural Networks (DNNs) have advanced the state-of-the-art in a variety\nof machine learning tasks and are deployed in increasing numbers of products\nand services. However, the computational requirements of training and\nevaluating large-scale DNNs are growing at a much faster pace than the\ncapabilities of the underlying hardware platforms that they are executed upon.\nIn this work, we propose Dynamic Variable Effort Deep Neural Networks\n(DyVEDeep) to reduce the computational requirements of DNNs during inference.\nPrevious efforts propose specialized hardware implementations for DNNs,\nstatically prune the network, or compress the weights. Complementary to these\napproaches, DyVEDeep is a dynamic approach that exploits the heterogeneity in\nthe inputs to DNNs to improve their compute efficiency with comparable\nclassification accuracy. DyVEDeep equips DNNs with dynamic effort mechanisms\nthat, in the course of processing an input, identify how critical a group of\ncomputations are to classify the input. DyVEDeep dynamically focuses its\ncompute effort only on the critical computa- tions, while skipping or\napproximating the rest. We propose 3 effort knobs that operate at different\nlevels of granularity viz. neuron, feature and layer levels. We build DyVEDeep\nversions for 5 popular image recognition benchmarks - one for CIFAR-10 and four\nfor ImageNet (AlexNet, OverFeat and VGG-16, weight-compressed AlexNet). Across\nall benchmarks, DyVEDeep achieves 2.1x-2.6x reduction in the number of scalar\noperations, which translates to 1.8x-2.3x performance improvement over a\nCaffe-based implementation, with < 0.5% loss in accuracy. \n\n"}
{"id": "1704.01464", "contents": "Title: Effect of Super Resolution on High Dimensional Features for Unsupervised\n  Face Recognition in the Wild Abstract: Majority of the face recognition algorithms use query faces captured from\nuncontrolled, in the wild, environment. Often caused by the cameras limited\ncapabilities, it is common for these captured facial images to be blurred or\nlow resolution. Super resolution algorithms are therefore crucial in improving\nthe resolution of such images especially when the image size is small requiring\nenlargement. This paper aims to demonstrate the effect of one of the\nstate-of-the-art algorithms in the field of image super resolution. To\ndemonstrate the functionality of the algorithm, various before and after 3D\nface alignment cases are provided using the images from the Labeled Faces in\nthe Wild (lfw). Resulting images are subject to testing on a closed set face\nrecognition protocol using unsupervised algorithms with high dimension\nextracted features. The inclusion of super resolution algorithm resulted in\nsignificant improved recognition rate over recently reported results obtained\nfrom unsupervised algorithms. \n\n"}
{"id": "1704.01705", "contents": "Title: Generate To Adapt: Aligning Domains using Generative Adversarial\n  Networks Abstract: Domain Adaptation is an actively researched problem in Computer Vision. In\nthis work, we propose an approach that leverages unsupervised data to bring the\nsource and target distributions closer in a learned joint feature space. We\naccomplish this by inducing a symbiotic relationship between the learned\nembedding and a generative adversarial network. This is in contrast to methods\nwhich use the adversarial framework for realistic data generation and\nretraining deep models with such data. We demonstrate the strength and\ngenerality of our approach by performing experiments on three different tasks\nwith varying levels of difficulty: (1) Digit classification (MNIST, SVHN and\nUSPS datasets) (2) Object recognition using OFFICE dataset and (3) Domain\nadaptation from synthetic to real data. Our method achieves state-of-the art\nperformance in most experimental settings and by far the only GAN-based method\nthat has been shown to work well across different datasets such as OFFICE and\nDIGITS. \n\n"}
{"id": "1704.02081", "contents": "Title: Evolution in Groups: A deeper look at synaptic cluster driven evolution\n  of deep neural networks Abstract: A promising paradigm for achieving highly efficient deep neural networks is\nthe idea of evolutionary deep intelligence, which mimics biological evolution\nprocesses to progressively synthesize more efficient networks. A crucial design\nfactor in evolutionary deep intelligence is the genetic encoding scheme used to\nsimulate heredity and determine the architectures of offspring networks. In\nthis study, we take a deeper look at the notion of synaptic cluster-driven\nevolution of deep neural networks which guides the evolution process towards\nthe formation of a highly sparse set of synaptic clusters in offspring\nnetworks. Utilizing a synaptic cluster-driven genetic encoding, the\nprobabilistic encoding of synaptic traits considers not only individual\nsynaptic properties but also inter-synaptic relationships within a deep neural\nnetwork. This process results in highly sparse offspring networks which are\nparticularly tailored for parallel computational devices such as GPUs and deep\nneural network accelerator chips. Comprehensive experimental results using four\nwell-known deep neural network architectures (LeNet-5, AlexNet, ResNet-56, and\nDetectNet) on two different tasks (object categorization and object detection)\ndemonstrate the efficiency of the proposed method. Cluster-driven genetic\nencoding scheme synthesizes networks that can achieve state-of-the-art\nperformance with significantly smaller number of synapses than that of the\noriginal ancestor network. ($\\sim$125-fold decrease in synapses for MNIST).\nFurthermore, the improved cluster efficiency in the generated offspring\nnetworks ($\\sim$9.71-fold decrease in clusters for MNIST and a $\\sim$8.16-fold\ndecrease in clusters for KITTI) is particularly useful for accelerated\nperformance on parallel computing hardware architectures such as those in GPUs\nand deep neural network accelerator chips. \n\n"}
{"id": "1704.02157", "contents": "Title: Multi-Scale Continuous CRFs as Sequential Deep Networks for Monocular\n  Depth Estimation Abstract: This paper addresses the problem of depth estimation from a single still\nimage. Inspired by recent works on multi- scale convolutional neural networks\n(CNN), we propose a deep model which fuses complementary information derived\nfrom multiple CNN side outputs. Different from previous methods, the\nintegration is obtained by means of continuous Conditional Random Fields\n(CRFs). In particular, we propose two different variations, one based on a\ncascade of multiple CRFs, the other on a unified graphical model. By designing\na novel CNN implementation of mean-field updates for continuous CRFs, we show\nthat both proposed models can be regarded as sequential deep networks and that\ntraining can be performed end-to-end. Through extensive experimental evaluation\nwe demonstrate the effective- ness of the proposed approach and establish new\nstate of the art results on publicly available datasets. \n\n"}
{"id": "1704.02161", "contents": "Title: ReLayNet: Retinal Layer and Fluid Segmentation of Macular Optical\n  Coherence Tomography using Fully Convolutional Network Abstract: Optical coherence tomography (OCT) is used for non-invasive diagnosis of\ndiabetic macular edema assessing the retinal layers. In this paper, we propose\na new fully convolutional deep architecture, termed ReLayNet, for end-to-end\nsegmentation of retinal layers and fluid masses in eye OCT scans. ReLayNet uses\na contracting path of convolutional blocks (encoders) to learn a hierarchy of\ncontextual features, followed by an expansive path of convolutional blocks\n(decoders) for semantic segmentation. ReLayNet is trained to optimize a joint\nloss function comprising of weighted logistic regression and Dice overlap loss.\nThe framework is validated on a publicly available benchmark dataset with\ncomparisons against five state-of-the-art segmentation methods including two\ndeep learning based approaches to substantiate its effectiveness. \n\n"}
{"id": "1704.02966", "contents": "Title: Loss Max-Pooling for Semantic Image Segmentation Abstract: We introduce a novel loss max-pooling concept for handling imbalanced\ntraining data distributions, applicable as alternative loss layer in the\ncontext of deep neural networks for semantic image segmentation. Most\nreal-world semantic segmentation datasets exhibit long tail distributions with\nfew object categories comprising the majority of data and consequently biasing\nthe classifiers towards them. Our method adaptively re-weights the\ncontributions of each pixel based on their observed losses, targeting\nunder-performing classification results as often encountered for\nunder-represented object classes. Our approach goes beyond conventional\ncost-sensitive learning attempts through adaptive considerations that allow us\nto indirectly address both, inter- and intra-class imbalances. We provide a\ntheoretical justification of our approach, complementary to experimental\nanalyses on benchmark datasets. In our experiments on the Cityscapes and Pascal\nVOC 2012 segmentation datasets we find consistently improved results,\ndemonstrating the efficacy of our approach. \n\n"}
{"id": "1704.03012", "contents": "Title: Stochastic Neural Networks for Hierarchical Reinforcement Learning Abstract: Deep reinforcement learning has achieved many impressive results in recent\nyears. However, tasks with sparse rewards or long horizons continue to pose\nsignificant challenges. To tackle these important problems, we propose a\ngeneral framework that first learns useful skills in a pre-training\nenvironment, and then leverages the acquired skills for learning faster in\ndownstream tasks. Our approach brings together some of the strengths of\nintrinsic motivation and hierarchical methods: the learning of useful skill is\nguided by a single proxy reward, the design of which requires very minimal\ndomain knowledge about the downstream tasks. Then a high-level policy is\ntrained on top of these skills, providing a significant improvement of the\nexploration and allowing to tackle sparse rewards in the downstream tasks. To\nefficiently pre-train a large span of skills, we use Stochastic Neural Networks\ncombined with an information-theoretic regularizer. Our experiments show that\nthis combination is effective in learning a wide span of interpretable skills\nin a sample-efficient way, and can significantly boost the learning performance\nuniformly across a wide range of downstream tasks. \n\n"}
{"id": "1704.04805", "contents": "Title: Replicator Equation: Applications Revisited Abstract: The replicator equation is a simple model of evolution that leads to stable\nform of Nash Equilibrium, Evolutionary Stable Strategy (ESS). It has been\nstudied in connection with Evolutionary Game Theory and was originally\ndeveloped for symmetric games. Beyond its first emphasis in biological use,\nevolutionary game theory has been expanded well beyond in social studies for\nbehavioral analysis, in machine learning, computer vision and others. Its\nseveral applications in the fields of machine learning and computer vision has\ndrawn my attention which is the reason to write this extended abstract \n\n"}
{"id": "1704.05310", "contents": "Title: Unsupervised Learning by Predicting Noise Abstract: Convolutional neural networks provide visual features that perform remarkably\nwell in many computer vision applications. However, training these networks\nrequires significant amounts of supervision. This paper introduces a generic\nframework to train deep networks, end-to-end, with no supervision. We propose\nto fix a set of target representations, called Noise As Targets (NAT), and to\nconstrain the deep features to align to them. This domain agnostic approach\navoids the standard unsupervised learning issues of trivial solutions and\ncollapsing of features. Thanks to a stochastic batch reassignment strategy and\na separable square loss function, it scales to millions of images. The proposed\napproach produces representations that perform on par with state-of-the-art\nunsupervised methods on ImageNet and Pascal VOC. \n\n"}
{"id": "1704.06065", "contents": "Title: End-to-End Unsupervised Deformable Image Registration with a\n  Convolutional Neural Network Abstract: In this work we propose a deep learning network for deformable image\nregistration (DIRNet). The DIRNet consists of a convolutional neural network\n(ConvNet) regressor, a spatial transformer, and a resampler. The ConvNet\nanalyzes a pair of fixed and moving images and outputs parameters for the\nspatial transformer, which generates the displacement vector field that enables\nthe resampler to warp the moving image to the fixed image. The DIRNet is\ntrained end-to-end by unsupervised optimization of a similarity metric between\ninput image pairs. A trained DIRNet can be applied to perform registration on\nunseen image pairs in one pass, thus non-iteratively. Evaluation was performed\nwith registration of images of handwritten digits (MNIST) and cardiac cine MR\nscans (Sunnybrook Cardiac Data). The results demonstrate that registration with\nDIRNet is as accurate as a conventional deformable image registration method\nwith substantially shorter execution times. \n\n"}
{"id": "1704.06228", "contents": "Title: Temporal Action Detection with Structured Segment Networks Abstract: Detecting actions in untrimmed videos is an important yet challenging task.\nIn this paper, we present the structured segment network (SSN), a novel\nframework which models the temporal structure of each action instance via a\nstructured temporal pyramid. On top of the pyramid, we further introduce a\ndecomposed discriminative model comprising two classifiers, respectively for\nclassifying actions and determining completeness. This allows the framework to\neffectively distinguish positive proposals from background or incomplete ones,\nthus leading to both accurate recognition and localization. These components\nare integrated into a unified network that can be efficiently trained in an\nend-to-end fashion. Additionally, a simple yet effective temporal action\nproposal scheme, dubbed temporal actionness grouping (TAG) is devised to\ngenerate high quality action proposals. On two challenging benchmarks, THUMOS14\nand ActivityNet, our method remarkably outperforms previous state-of-the-art\nmethods, demonstrating superior accuracy and strong adaptivity in handling\nactions with various temporal structures. \n\n"}
{"id": "1704.06410", "contents": "Title: Solar Power Plant Detection on Multi-Spectral Satellite Imagery using\n  Weakly-Supervised CNN with Feedback Features and m-PCNN Fusion Abstract: Most of the traditional convolutional neural networks (CNNs) implements\nbottom-up approach (feed-forward) for image classifications. However, many\nscientific studies demonstrate that visual perception in primates rely on both\nbottom-up and top-down connections. Therefore, in this work, we propose a CNN\nnetwork with feedback structure for Solar power plant detection on\nmiddle-resolution satellite images. To express the strength of the top-down\nconnections, we introduce feedback CNN network (FB-Net) to a baseline CNN model\nused for solar power plant classification on multi-spectral satellite data.\nMoreover, we introduce a method to improve class activation mapping (CAM) to\nour FB-Net, which takes advantage of multi-channel pulse coupled neural network\n(m-PCNN) for weakly-supervised localization of the solar power plants from the\nfeatures of proposed FB-Net. For the proposed FB-Net CAM with m-PCNN,\nexperimental results demonstrated promising results on both solar-power plant\nimage classification and detection task. \n\n"}
{"id": "1704.06485", "contents": "Title: Attend to You: Personalized Image Captioning with Context Sequence\n  Memory Networks Abstract: We address personalization issues of image captioning, which have not been\ndiscussed yet in previous research. For a query image, we aim to generate a\ndescriptive sentence, accounting for prior knowledge such as the user's active\nvocabularies in previous documents. As applications of personalized image\ncaptioning, we tackle two post automation tasks: hashtag prediction and post\ngeneration, on our newly collected Instagram dataset, consisting of 1.1M posts\nfrom 6.3K users. We propose a novel captioning model named Context Sequence\nMemory Network (CSMN). Its unique updates over previous memory network models\ninclude (i) exploiting memory as a repository for multiple types of context\ninformation, (ii) appending previously generated words into memory to capture\nlong-term information without suffering from the vanishing gradient problem,\nand (iii) adopting CNN memory structure to jointly represent nearby ordered\nmemory slots for better context understanding. With quantitative evaluation and\nuser studies via Amazon Mechanical Turk, we show the effectiveness of the three\nnovel features of CSMN and its performance enhancement for personalized image\ncaptioning over state-of-the-art captioning models. \n\n"}
{"id": "1704.06729", "contents": "Title: On Face Segmentation, Face Swapping, and Face Perception Abstract: We show that even when face images are unconstrained and arbitrarily paired,\nface swapping between them is actually quite simple. To this end, we make the\nfollowing contributions. (a) Instead of tailoring systems for face\nsegmentation, as others previously proposed, we show that a standard fully\nconvolutional network (FCN) can achieve remarkably fast and accurate\nsegmentations, provided that it is trained on a rich enough example set. For\nthis purpose, we describe novel data collection and generation routines which\nprovide challenging segmented face examples. (b) We use our segmentations to\nenable robust face swapping under unprecedented conditions. (c) Unlike previous\nwork, our swapping is robust enough to allow for extensive quantitative tests.\nTo this end, we use the Labeled Faces in the Wild (LFW) benchmark and measure\nthe effect of intra- and inter-subject face swapping on recognition. We show\nthat our intra-subject swapped faces remain as recognizable as their sources,\ntestifying to the effectiveness of our method. In line with well known\nperceptual studies, we show that better face swapping produces less\nrecognizable inter-subject results. This is the first time this effect was\nquantitatively demonstrated for machine vision systems. \n\n"}
{"id": "1704.06752", "contents": "Title: ScaleNet: Guiding Object Proposal Generation in Supermarkets and Beyond Abstract: Motivated by product detection in supermarkets, this paper studies the\nproblem of object proposal generation in supermarket images and other natural\nimages. We argue that estimation of object scales in images is helpful for\ngenerating object proposals, especially for supermarket images where object\nscales are usually within a small range. Therefore, we propose to estimate\nobject scales of images before generating object proposals. The proposed method\nfor predicting object scales is called ScaleNet. To validate the effectiveness\nof ScaleNet, we build three supermarket datasets, two of which are real-world\ndatasets used for testing and the other one is a synthetic dataset used for\ntraining. In short, we extend the previous state-of-the-art object proposal\nmethods by adding a scale prediction phase. The resulted method outperforms the\nprevious state-of-the-art on the supermarket datasets by a large margin. We\nalso show that the approach works for object proposal on other natural images\nand it outperforms the previous state-of-the-art object proposal methods on the\nMS COCO dataset. The supermarket datasets, the virtual supermarkets, and the\ntools for creating more synthetic datasets will be made public. \n\n"}
{"id": "1704.06847", "contents": "Title: A hybrid primal heuristic for Robust Multiperiod Network Design Abstract: We investigate the Robust Multiperiod Network Design Problem, a\ngeneralization of the classical Capacitated Network Design Problem that\nadditionally considers multiple design periods and provides solutions protected\nagainst traffic uncertainty. Given the intrinsic difficulty of the problem,\nwhich proves challenging even for state-of-the art commercial solvers, we\npropose a hybrid primal heuristic based on the combination of ant colony\noptimization and an exact large neighborhood search. Computational experiments\non a set of realistic instances from the SNDlib show that our heuristic can\nfind solutions of extremely good quality with low optimality gap. \n\n"}
{"id": "1704.06942", "contents": "Title: Population Seeding Techniques for Rolling Horizon Evolution in General\n  Video Game Playing Abstract: While Monte Carlo Tree Search and closely related methods have dominated\nGeneral Video Game Playing, recent research has demonstrated the promise of\nRolling Horizon Evolutionary Algorithms as an interesting alternative. However,\nthere is little attention paid to population initialization techniques in the\nsetting of general real-time video games. Therefore, this paper proposes the\nuse of population seeding to improve the performance of Rolling Horizon\nEvolution and presents the results of two methods, One Step Look Ahead and\nMonte Carlo Tree Search, tested on 20 games of the General Video Game AI corpus\nwith multiple evolution parameter values (population size and individual\nlength). An in-depth analysis is carried out between the results of the seeding\nmethods and the vanilla Rolling Horizon Evolution. In addition, the paper\npresents a comparison to a Monte Carlo Tree Search algorithm. The results are\npromising, with seeding able to boost performance significantly over baseline\nevolution and even match the high level of play obtained by the Monte Carlo\nTree Search. \n\n"}
{"id": "1704.07244", "contents": "Title: Fast PET reconstruction using Multi-scale Fully Convolutional Neural\n  Networks Abstract: Reconstruction of PET images is an ill-posed inverse problem and often\nrequires iterative algorithms to achieve good image quality for reliable\nclinical use in practice, at huge computational costs. In this paper, we\nconsider the PET reconstruction a dense prediction problem where the large\nscale contextual information is essential, and propose a novel architecture of\nmulti-scale fully convolutional neural networks (msfCNN) for fast PET image\nreconstruction. The proposed msfCNN gains large receptive fields with both\nmemory and computational efficiency, by using a downscaling-upscaling structure\nand dilated convolutions. Instead of pooling and deconvolution, we propose to\nuse the periodic shuffling operation from sub-pixel convolution and its inverse\nto scale the size of feature maps without losing resolution. Residual\nconnections were added to improve training. We trained the proposed msfCNN\nmodel with simulated data, and applied it to clinical PET data acquired on a\nSiemens mMR scanner. The results from real oncological and neurodegenerative\ncases show that the proposed msfCNN-based reconstruction outperforms the\niterative approaches in terms of computational time while achieving comparable\nimage quality for quantification. The proposed msfCNN model can be applied to\nother dense prediction tasks, and fast msfCNN-based PET reconstruction could\nfacilitate the potential use of molecular imaging in interventional/surgical\nprocedures, where cancer surgery can particularly benefit. \n\n"}
{"id": "1704.07754", "contents": "Title: Joint Sequence Learning and Cross-Modality Convolution for 3D Biomedical\n  Segmentation Abstract: Deep learning models such as convolutional neural net- work have been widely\nused in 3D biomedical segmentation and achieve state-of-the-art performance.\nHowever, most of them often adapt a single modality or stack multiple\nmodalities as different input channels. To better leverage the multi-\nmodalities, we propose a deep encoder-decoder structure with cross-modality\nconvolution layers to incorporate different modalities of MRI data. In\naddition, we exploit convolutional LSTM to model a sequence of 2D slices, and\njointly learn the multi-modalities and convolutional LSTM in an end-to-end\nmanner. To avoid converging to the certain labels, we adopt a re-weighting\nscheme and two-phase training to handle the label imbalance. Experimental\nresults on BRATS-2015 show that our method outperforms state-of-the-art\nbiomedical segmentation approaches. \n\n"}
{"id": "1704.07820", "contents": "Title: Introspective Generative Modeling: Decide Discriminatively Abstract: We study unsupervised learning by developing introspective generative\nmodeling (IGM) that attains a generator using progressively learned deep\nconvolutional neural networks. The generator is itself a discriminator, capable\nof introspection: being able to self-evaluate the difference between its\ngenerated samples and the given training data. When followed by repeated\ndiscriminative learning, desirable properties of modern discriminative\nclassifiers are directly inherited by the generator. IGM learns a cascade of\nCNN classifiers using a synthesis-by-classification algorithm. In the\nexperiments, we observe encouraging results on a number of applications\nincluding texture modeling, artistic style transferring, face modeling, and\nsemi-supervised learning. \n\n"}
{"id": "1704.08243", "contents": "Title: C-VQA: A Compositional Split of the Visual Question Answering (VQA) v1.0\n  Dataset Abstract: Visual Question Answering (VQA) has received a lot of attention over the past\ncouple of years. A number of deep learning models have been proposed for this\ntask. However, it has been shown that these models are heavily driven by\nsuperficial correlations in the training data and lack compositionality -- the\nability to answer questions about unseen compositions of seen concepts. This\ncompositionality is desirable and central to intelligence. In this paper, we\npropose a new setting for Visual Question Answering where the test\nquestion-answer pairs are compositionally novel compared to training\nquestion-answer pairs. To facilitate developing models under this setting, we\npresent a new compositional split of the VQA v1.0 dataset, which we call\nCompositional VQA (C-VQA). We analyze the distribution of questions and answers\nin the C-VQA splits. Finally, we evaluate several existing VQA models under\nthis new setting and show that the performances of these models degrade by a\nsignificant amount compared to the original VQA setting. \n\n"}
{"id": "1705.00727", "contents": "Title: Hyperspectral Image Classification with Markov Random Fields and a\n  Convolutional Neural Network Abstract: This paper presents a new supervised classification algorithm for remotely\nsensed hyperspectral image (HSI) which integrates spectral and spatial\ninformation in a unified Bayesian framework. First, we formulate the HSI\nclassification problem from a Bayesian perspective. Then, we adopt a\nconvolutional neural network (CNN) to learn the posterior class distributions\nusing a patch-wise training strategy to better use the spatial information.\nNext, spatial information is further considered by placing a spatial smoothness\nprior on the labels. Finally, we iteratively update the CNN parameters using\nstochastic gradient decent (SGD) and update the class labels of all pixel\nvectors using an alpha-expansion min-cut-based algorithm. Compared with other\nstate-of-the-art methods, the proposed classification method achieves better\nperformance on one synthetic dataset and two benchmark HSI datasets in a number\nof experimental settings. \n\n"}
{"id": "1705.01352", "contents": "Title: Optical Flow in Mostly Rigid Scenes Abstract: The optical flow of natural scenes is a combination of the motion of the\nobserver and the independent motion of objects. Existing algorithms typically\nfocus on either recovering motion and structure under the assumption of a\npurely static world or optical flow for general unconstrained scenes. We\ncombine these approaches in an optical flow algorithm that estimates an\nexplicit segmentation of moving objects from appearance and physical\nconstraints. In static regions we take advantage of strong constraints to\njointly estimate the camera motion and the 3D structure of the scene over\nmultiple frames. This allows us to also regularize the structure instead of the\nmotion. Our formulation uses a Plane+Parallax framework, which works even under\nsmall baselines, and reduces the motion estimation to a one-dimensional search\nproblem, resulting in more accurate estimation. In moving regions the flow is\ntreated as unconstrained, and computed with an existing optical flow method.\nThe resulting Mostly-Rigid Flow (MR-Flow) method achieves state-of-the-art\nresults on both the MPI-Sintel and KITTI-2015 benchmarks. \n\n"}
{"id": "1705.03428", "contents": "Title: Deep Projective 3D Semantic Segmentation Abstract: Semantic segmentation of 3D point clouds is a challenging problem with\nnumerous real-world applications. While deep learning has revolutionized the\nfield of image semantic segmentation, its impact on point cloud data has been\nlimited so far. Recent attempts, based on 3D deep learning approaches\n(3D-CNNs), have achieved below-expected results. Such methods require\nvoxelizations of the underlying point cloud data, leading to decreased spatial\nresolution and increased memory consumption. Additionally, 3D-CNNs greatly\nsuffer from the limited availability of annotated datasets.\n  In this paper, we propose an alternative framework that avoids the\nlimitations of 3D-CNNs. Instead of directly solving the problem in 3D, we first\nproject the point cloud onto a set of synthetic 2D-images. These images are\nthen used as input to a 2D-CNN, designed for semantic segmentation. Finally,\nthe obtained prediction scores are re-projected to the point cloud to obtain\nthe segmentation results. We further investigate the impact of multiple\nmodalities, such as color, depth and surface normals, in a multi-stream network\narchitecture. Experiments are performed on the recent Semantic3D dataset. Our\napproach sets a new state-of-the-art by achieving a relative gain of 7.9 %,\ncompared to the previous best approach. \n\n"}
{"id": "1705.03865", "contents": "Title: Survey of Visual Question Answering: Datasets and Techniques Abstract: Visual question answering (or VQA) is a new and exciting problem that\ncombines natural language processing and computer vision techniques. We present\na survey of the various datasets and models that have been used to tackle this\ntask. The first part of the survey details the various datasets for VQA and\ncompares them along some common factors. The second part of this survey details\nthe different approaches for VQA, classified into four types: non-deep learning\nmodels, deep learning models without attention, deep learning models with\nattention, and other models which do not fit into the first three. Finally, we\ncompare the performances of these approaches and provide some directions for\nfuture work. \n\n"}
{"id": "1705.05444", "contents": "Title: A Non-Rigid Map Fusion-Based RGB-Depth SLAM Method for Endoscopic\n  Capsule Robots Abstract: In the gastrointestinal (GI) tract endoscopy field, ingestible wireless\ncapsule endoscopy is considered as a minimally invasive novel diagnostic\ntechnology to inspect the entire GI tract and to diagnose various diseases and\npathologies. Since the development of this technology, medical device companies\nand many groups have made significant progress to turn such passive capsule\nendoscopes into robotic active capsule endoscopes to achieve almost all\nfunctions of current active flexible endoscopes. However, the use of robotic\ncapsule endoscopy still has some challenges. One such challenge is the precise\nlocalization of such active devices in 3D world, which is essential for a\nprecise three-dimensional (3D) mapping of the inner organ. A reliable 3D map of\nthe explored inner organ could assist the doctors to make more intuitive and\ncorrect diagnosis. In this paper, we propose to our knowledge for the first\ntime in literature a visual simultaneous localization and mapping (SLAM) method\nspecifically developed for endoscopic capsule robots. The proposed RGB-Depth\nSLAM method is capable of capturing comprehensive dense globally consistent\nsurfel-based maps of the inner organs explored by an endoscopic capsule robot\nin real time. This is achieved by using dense frame-to-model camera tracking\nand windowed surfelbased fusion coupled with frequent model refinement through\nnon-rigid surface deformations. \n\n"}
{"id": "1705.05502", "contents": "Title: The power of deeper networks for expressing natural functions Abstract: It is well-known that neural networks are universal approximators, but that\ndeeper networks tend in practice to be more powerful than shallower ones. We\nshed light on this by proving that the total number of neurons $m$ required to\napproximate natural classes of multivariate polynomials of $n$ variables grows\nonly linearly with $n$ for deep neural networks, but grows exponentially when\nmerely a single hidden layer is allowed. We also provide evidence that when the\nnumber of hidden layers is increased from $1$ to $k$, the neuron requirement\ngrows exponentially not with $n$ but with $n^{1/k}$, suggesting that the\nminimum number of layers required for practical expressibility grows only\nlogarithmically with $n$. \n\n"}
{"id": "1705.06260", "contents": "Title: A deep level set method for image segmentation Abstract: This paper proposes a novel image segmentation approachthat integrates fully\nconvolutional networks (FCNs) with a level setmodel. Compared with a FCN, the\nintegrated method can incorporatesmoothing and prior information to achieve an\naccurate segmentation.Furthermore, different than using the level set model as\na post-processingtool, we integrate it into the training phase to fine-tune the\nFCN. Thisallows the use of unlabeled data during training in a\nsemi-supervisedsetting. Using two types of medical imaging data (liver CT and\nleft ven-tricle MRI data), we show that the integrated method achieves\ngoodperformance even when little training data is available, outperformingthe\nFCN or the level set model alone. \n\n"}
{"id": "1705.07262", "contents": "Title: Batch Reinforcement Learning on the Industrial Benchmark: First\n  Experiences Abstract: The Particle Swarm Optimization Policy (PSO-P) has been recently introduced\nand proven to produce remarkable results on interacting with academic\nreinforcement learning benchmarks in an off-policy, batch-based setting. To\nfurther investigate the properties and feasibility on real-world applications,\nthis paper investigates PSO-P on the so-called Industrial Benchmark (IB), a\nnovel reinforcement learning (RL) benchmark that aims at being realistic by\nincluding a variety of aspects found in industrial applications, like\ncontinuous state and action spaces, a high dimensional, partially observable\nstate space, delayed effects, and complex stochasticity. The experimental\nresults of PSO-P on IB are compared to results of closed-form control policies\nderived from the model-based Recurrent Control Neural Network (RCNN) and the\nmodel-free Neural Fitted Q-Iteration (NFQ). Experiments show that PSO-P is not\nonly of interest for academic benchmarks, but also for real-world industrial\napplications, since it also yielded the best performing policy in our IB\nsetting. Compared to other well established RL techniques, PSO-P produced\noutstanding results in performance and robustness, requiring only a relatively\nlow amount of effort in finding adequate parameters or making complex design\ndecisions. \n\n"}
{"id": "1705.07831", "contents": "Title: Stabilizing GAN Training with Multiple Random Projections Abstract: Training generative adversarial networks is unstable in high-dimensions as\nthe true data distribution tends to be concentrated in a small fraction of the\nambient space. The discriminator is then quickly able to classify nearly all\ngenerated samples as fake, leaving the generator without meaningful gradients\nand causing it to deteriorate after a point in training. In this work, we\npropose training a single generator simultaneously against an array of\ndiscriminators, each of which looks at a different random low-dimensional\nprojection of the data. Individual discriminators, now provided with restricted\nviews of the input, are unable to reject generated samples perfectly and\ncontinue to provide meaningful gradients to the generator throughout training.\nMeanwhile, the generator learns to produce samples consistent with the full\ndata distribution to satisfy all discriminators simultaneously. We demonstrate\nthe practical utility of this approach experimentally, and show that it is able\nto produce image samples with higher quality than traditional training with a\nsingle discriminator. \n\n"}
{"id": "1705.08759", "contents": "Title: Bidirectional Beam Search: Forward-Backward Inference in Neural Sequence\n  Models for Fill-in-the-Blank Image Captioning Abstract: We develop the first approximate inference algorithm for 1-Best (and M-Best)\ndecoding in bidirectional neural sequence models by extending Beam Search (BS)\nto reason about both forward and backward time dependencies. Beam Search (BS)\nis a widely used approximate inference algorithm for decoding sequences from\nunidirectional neural sequence models. Interestingly, approximate inference in\nbidirectional models remains an open problem, despite their significant\nadvantage in modeling information from both the past and future. To enable the\nuse of bidirectional models, we present Bidirectional Beam Search (BiBS), an\nefficient algorithm for approximate bidirectional inference.To evaluate our\nmethod and as an interesting problem in its own right, we introduce a novel\nFill-in-the-Blank Image Captioning task which requires reasoning about both\npast and future sentence structure to reconstruct sensible image descriptions.\nWe use this task as well as the Visual Madlibs dataset to demonstrate the\neffectiveness of our approach, consistently outperforming all baseline methods. \n\n"}
{"id": "1705.09368", "contents": "Title: Pose Guided Person Image Generation Abstract: This paper proposes the novel Pose Guided Person Generation Network (PG$^2$)\nthat allows to synthesize person images in arbitrary poses, based on an image\nof that person and a novel pose. Our generation framework PG$^2$ utilizes the\npose information explicitly and consists of two key stages: pose integration\nand image refinement. In the first stage the condition image and the target\npose are fed into a U-Net-like network to generate an initial but coarse image\nof the person with the target pose. The second stage then refines the initial\nand blurry result by training a U-Net-like generator in an adversarial way.\nExtensive experimental results on both 128$\\times$64 re-identification images\nand 256$\\times$256 fashion photos show that our model generates high-quality\nperson images with convincing details. \n\n"}
{"id": "1705.10743", "contents": "Title: The Cramer Distance as a Solution to Biased Wasserstein Gradients Abstract: The Wasserstein probability metric has received much attention from the\nmachine learning community. Unlike the Kullback-Leibler divergence, which\nstrictly measures change in probability, the Wasserstein metric reflects the\nunderlying geometry between outcomes. The value of being sensitive to this\ngeometry has been demonstrated, among others, in ordinal regression and\ngenerative modelling. In this paper we describe three natural properties of\nprobability divergences that reflect requirements from machine learning: sum\ninvariance, scale sensitivity, and unbiased sample gradients. The Wasserstein\nmetric possesses the first two properties but, unlike the Kullback-Leibler\ndivergence, does not possess the third. We provide empirical evidence\nsuggesting that this is a serious issue in practice. Leveraging insights from\nprobabilistic forecasting we propose an alternative to the Wasserstein metric,\nthe Cram\\'er distance. We show that the Cram\\'er distance possesses all three\ndesired properties, combining the best of the Wasserstein and Kullback-Leibler\ndivergences. To illustrate the relevance of the Cram\\'er distance in practice\nwe design a new algorithm, the Cram\\'er Generative Adversarial Network (GAN),\nand show that it performs significantly better than the related Wasserstein\nGAN. \n\n"}
{"id": "1705.10823", "contents": "Title: Accelerating Neural Architecture Search using Performance Prediction Abstract: Methods for neural network hyperparameter optimization and meta-modeling are\ncomputationally expensive due to the need to train a large number of model\nconfigurations. In this paper, we show that standard frequentist regression\nmodels can predict the final performance of partially trained model\nconfigurations using features based on network architectures, hyperparameters,\nand time-series validation performance data. We empirically show that our\nperformance prediction models are much more effective than prominent Bayesian\ncounterparts, are simpler to implement, and are faster to train. Our models can\npredict final performance in both visual classification and language modeling\ndomains, are effective for predicting performance of drastically varying model\narchitectures, and can even generalize between model classes. Using these\nprediction models, we also propose an early stopping method for hyperparameter\noptimization and meta-modeling, which obtains a speedup of a factor up to 6x in\nboth hyperparameter optimization and meta-modeling. Finally, we empirically\nshow that our early stopping method can be seamlessly incorporated into both\nreinforcement learning-based architecture selection algorithms and bandit based\nsearch methods. Through extensive experimentation, we empirically show our\nperformance prediction models and early stopping algorithm are state-of-the-art\nin terms of prediction accuracy and speedup achieved while still identifying\nthe optimal model configurations. \n\n"}
{"id": "1705.10861", "contents": "Title: Generic Tubelet Proposals for Action Localization Abstract: We develop a novel framework for action localization in videos. We propose\nthe Tube Proposal Network (TPN), which can generate generic, class-independent,\nvideo-level tubelet proposals in videos. The generated tubelet proposals can be\nutilized in various video analysis tasks, including recognizing and localizing\nactions in videos. In particular, we integrate these generic tubelet proposals\ninto a unified temporal deep network for action classification. Compared with\nother methods, our generic tubelet proposal method is accurate, general, and is\nfully differentiable under a smoothL1 loss function. We demonstrate the\nperformance of our algorithm on the standard UCF-Sports, J-HMDB21, and UCF-101\ndatasets. Our class-independent TPN outperforms other tubelet generation\nmethods, and our unified temporal deep network achieves state-of-the-art\nlocalization results on all three datasets. \n\n"}
{"id": "1706.00082", "contents": "Title: Megapixel Size Image Creation using Generative Adversarial Networks Abstract: Since its appearance, Generative Adversarial Networks (GANs) have received a\nlot of interest in the AI community. In image generation several projects\nshowed how GANs are able to generate photorealistic images but the results so\nfar did not look adequate for the quality standard of visual media production\nindustry. We present an optimized image generation process based on a Deep\nConvolutional Generative Adversarial Networks (DCGANs), in order to create\nphotorealistic high-resolution images (up to 1024x1024 pixels). Furthermore,\nthe system was fed with a limited dataset of images, less than two thousand\nimages. All these results give more clue about future exploitation of GANs in\nComputer Graphics and Visual Effects. \n\n"}
{"id": "1706.00504", "contents": "Title: Dynamic Stripes: Exploiting the Dynamic Precision Requirements of\n  Activation Values in Neural Networks Abstract: Stripes is a Deep Neural Network (DNN) accelerator that uses bit-serial\ncomputation to offer performance that is proportional to the fixed-point\nprecision of the activation values. The fixed-point precisions are determined a\npriori using profiling and are selected at a per layer granularity. This paper\npresents Dynamic Stripes, an extension to Stripes that detects precision\nvariance at runtime and at a finer granularity. This extra level of precision\nreduction increases performance by 41% over Stripes. \n\n"}
{"id": "1706.00648", "contents": "Title: Dataflow Matrix Machines as a Model of Computations with Linear Streams Abstract: We overview dataflow matrix machines as a Turing complete generalization of\nrecurrent neural networks and as a programming platform. We describe vector\nspace of finite prefix trees with numerical leaves which allows us to combine\nexpressive power of dataflow matrix machines with simplicity of traditional\nrecurrent neural networks. \n\n"}
{"id": "1706.01021", "contents": "Title: Where and Who? Automatic Semantic-Aware Person Composition Abstract: Image compositing is a method used to generate realistic yet fake imagery by\ninserting contents from one image to another. Previous work in compositing has\nfocused on improving appearance compatibility of a user selected foreground\nsegment and a background image (i.e. color and illumination consistency). In\nthis work, we instead develop a fully automated compositing model that\nadditionally learns to select and transform compatible foreground segments from\na large collection given only an input image background. To simplify the task,\nwe restrict our problem by focusing on human instance composition, because\nhuman segments exhibit strong correlations with their background and because of\nthe availability of large annotated data. We develop a novel branching\nConvolutional Neural Network (CNN) that jointly predicts candidate person\nlocations given a background image. We then use pre-trained deep feature\nrepresentations to retrieve person instances from a large segment database.\nExperimental results show that our model can generate composite images that\nlook visually convincing. We also develop a user interface to demonstrate the\npotential application of our method. \n\n"}
{"id": "1706.03112", "contents": "Title: Unsupervised Adaptive Re-identification in Open World Dynamic Camera\n  Networks Abstract: Person re-identification is an open and challenging problem in computer\nvision. Existing approaches have concentrated on either designing the best\nfeature representation or learning optimal matching metrics in a static setting\nwhere the number of cameras are fixed in a network. Most approaches have\nneglected the dynamic and open world nature of the re-identification problem,\nwhere a new camera may be temporarily inserted into an existing system to get\nadditional information. To address such a novel and very practical problem, we\npropose an unsupervised adaptation scheme for re-identification models in a\ndynamic camera network. First, we formulate a domain perceptive\nre-identification method based on geodesic flow kernel that can effectively\nfind the best source camera (already installed) to adapt with a newly\nintroduced target camera, without requiring a very expensive training phase.\nSecond, we introduce a transitive inference algorithm for re-identification\nthat can exploit the information from best source camera to improve the\naccuracy across other camera pairs in a network of multiple cameras. Extensive\nexperiments on four benchmark datasets demonstrate that the proposed approach\nsignificantly outperforms the state-of-the-art unsupervised learning based\nalternatives whilst being extremely efficient to compute. \n\n"}
{"id": "1706.03581", "contents": "Title: Enriched Deep Recurrent Visual Attention Model for Multiple Object\n  Recognition Abstract: We design an Enriched Deep Recurrent Visual Attention Model (EDRAM) - an\nimproved attention-based architecture for multiple object recognition. The\nproposed model is a fully differentiable unit that can be optimized end-to-end\nby using Stochastic Gradient Descent (SGD). The Spatial Transformer (ST) was\nemployed as visual attention mechanism which allows to learn the geometric\ntransformation of objects within images. With the combination of the Spatial\nTransformer and the powerful recurrent architecture, the proposed EDRAM can\nlocalize and recognize objects simultaneously. EDRAM has been evaluated on two\npublicly available datasets including MNIST Cluttered (with 70K cluttered\ndigits) and SVHN (with up to 250k real world images of house numbers).\nExperiments show that it obtains superior performance as compared with the\nstate-of-the-art models. \n\n"}
{"id": "1706.03863", "contents": "Title: Criteria Sliders: Learning Continuous Database Criteria via Interactive\n  Ranking Abstract: Large databases are often organized by hand-labeled metadata, or criteria,\nwhich are expensive to collect. We can use unsupervised learning to model\ndatabase variation, but these models are often high dimensional, complex to\nparameterize, or require expert knowledge. We learn low-dimensional continuous\ncriteria via interactive ranking, so that the novice user need only describe\nthe relative ordering of examples. This is formed as semi-supervised label\npropagation in which we maximize the information gained from a limited number\nof examples. Further, we actively suggest data points to the user to rank in a\nmore informative way than existing work. Our efficient approach allows users to\ninteractively organize thousands of data points along 1D and 2D continuous\nsliders. We experiment with datasets of imagery and geometry to demonstrate\nthat our tool is useful for quickly assessing and organizing the content of\nlarge databases. \n\n"}
{"id": "1706.04264", "contents": "Title: von Mises-Fisher Mixture Model-based Deep learning: Application to Face\n  Verification Abstract: A number of pattern recognition tasks, \\textit{e.g.}, face verification, can\nbe boiled down to classification or clustering of unit length directional\nfeature vectors whose distance can be simply computed by their angle. In this\npaper, we propose the von Mises-Fisher (vMF) mixture model as the theoretical\nfoundation for an effective deep-learning of such directional features and\nderive a novel vMF Mixture Loss and its corresponding vMF deep features. The\nproposed vMF feature learning achieves the characteristics of discriminative\nlearning, \\textit{i.e.}, compacting the instances of the same class while\nincreasing the distance of instances from different classes. Moreover, it\nsubsumes a number of popular loss functions as well as an effective method in\ndeep learning, namely normalization. We conduct extensive experiments on face\nverification using 4 different challenging face datasets, \\textit{i.e.}, LFW,\nYouTube faces, CACD and IJB-A. Results show the effectiveness and excellent\ngeneralization ability of the proposed approach as it achieves state-of-the-art\nresults on the LFW, YouTube faces and CACD datasets and competitive results on\nthe IJB-A dataset. \n\n"}
{"id": "1706.04269", "contents": "Title: Action Search: Spotting Actions in Videos and Its Application to\n  Temporal Action Localization Abstract: State-of-the-art temporal action detectors inefficiently search the entire\nvideo for specific actions. Despite the encouraging progress these methods\nachieve, it is crucial to design automated approaches that only explore parts\nof the video which are the most relevant to the actions being searched for. To\naddress this need, we propose the new problem of action spotting in video,\nwhich we define as finding a specific action in a video while observing a small\nportion of that video. Inspired by the observation that humans are extremely\nefficient and accurate in spotting and finding action instances in video, we\npropose Action Search, a novel Recurrent Neural Network approach that mimics\nthe way humans spot actions. Moreover, to address the absence of data recording\nthe behavior of human annotators, we put forward the Human Searches dataset,\nwhich compiles the search sequences employed by human annotators spotting\nactions in the AVA and THUMOS14 datasets. We consider temporal action\nlocalization as an application of the action spotting problem. Experiments on\nthe THUMOS14 dataset reveal that our model is not only able to explore the\nvideo efficiently (observing on average 17.3% of the video) but it also\naccurately finds human activities with 30.8% mAP. \n\n"}
{"id": "1706.04496", "contents": "Title: Learning Local Shape Descriptors from Part Correspondences With\n  Multi-view Convolutional Networks Abstract: We present a new local descriptor for 3D shapes, directly applicable to a\nwide range of shape analysis problems such as point correspondences, semantic\nsegmentation, affordance prediction, and shape-to-scan matching. The descriptor\nis produced by a convolutional network that is trained to embed geometrically\nand semantically similar points close to one another in descriptor space. The\nnetwork processes surface neighborhoods around points on a shape that are\ncaptured at multiple scales by a succession of progressively zoomed out views,\ntaken from carefully selected camera positions. We leverage two extremely large\nsources of data to train our network. First, since our network processes\nrendered views in the form of 2D images, we repurpose architectures pre-trained\non massive image datasets. Second, we automatically generate a synthetic dense\npoint correspondence dataset by non-rigid alignment of corresponding shape\nparts in a large collection of segmented 3D models. As a result of these design\nchoices, our network effectively encodes multi-scale local context and\nfine-grained surface detail. Our network can be trained to produce either\ncategory-specific descriptors or more generic descriptors by learning from\nmultiple shape categories. Once trained, at test time, the network extracts\nlocal descriptors for shapes without requiring any part segmentation as input.\nOur method can produce effective local descriptors even for shapes whose\ncategory is unknown or different from the ones used while training. We\ndemonstrate through several experiments that our learned local descriptors are\nmore discriminative compared to state of the art alternatives, and are\neffective in a variety of shape analysis applications. \n\n"}
{"id": "1706.04698", "contents": "Title: Gradient Descent for Spiking Neural Networks Abstract: Much of studies on neural computation are based on network models of static\nneurons that produce analog output, despite the fact that information\nprocessing in the brain is predominantly carried out by dynamic neurons that\nproduce discrete pulses called spikes. Research in spike-based computation has\nbeen impeded by the lack of efficient supervised learning algorithm for spiking\nnetworks. Here, we present a gradient descent method for optimizing spiking\nnetwork models by introducing a differentiable formulation of spiking networks\nand deriving the exact gradient calculation. For demonstration, we trained\nrecurrent spiking networks on two dynamic tasks: one that requires optimizing\nfast (~millisecond) spike-based interactions for efficient encoding of\ninformation, and a delayed memory XOR task over extended duration (~second).\nThe results show that our method indeed optimizes the spiking network dynamics\non the time scale of individual spikes as well as behavioral time scales. In\nconclusion, our result offers a general purpose supervised learning algorithm\nfor spiking neural networks, thus advancing further investigations on\nspike-based computation. \n\n"}
{"id": "1706.05507", "contents": "Title: Variants of RMSProp and Adagrad with Logarithmic Regret Bounds Abstract: Adaptive gradient methods have become recently very popular, in particular as\nthey have been shown to be useful in the training of deep neural networks. In\nthis paper we have analyzed RMSProp, originally proposed for the training of\ndeep neural networks, in the context of online convex optimization and show\n$\\sqrt{T}$-type regret bounds. Moreover, we propose two variants SC-Adagrad and\nSC-RMSProp for which we show logarithmic regret bounds for strongly convex\nfunctions. Finally, we demonstrate in the experiments that these new variants\noutperform other adaptive gradient techniques or stochastic gradient descent in\nthe optimization of strongly convex functions as well as in training of deep\nneural networks. \n\n"}
{"id": "1706.06169", "contents": "Title: Satellite Imagery Feature Detection using Deep Convolutional Neural\n  Network: A Kaggle Competition Abstract: This paper describes our approach to the DSTL Satellite Imagery Feature\nDetection challenge run by Kaggle. The primary goal of this challenge is\naccurate semantic segmentation of different classes in satellite imagery. Our\napproach is based on an adaptation of fully convolutional neural network for\nmultispectral data processing. In addition, we defined several modifications to\nthe training objective and overall training pipeline, e.g. boundary effect\nestimation, also we discuss usage of data augmentation strategies and\nreflectance indices. Our solution scored third place out of 419 entries. Its\naccuracy is comparable to the first two places, but unlike those solutions, it\ndoesn't rely on complex ensembling techniques and thus can be easily scaled for\ndeployment in production as a part of automatic feature labeling systems for\nsatellite imagery analysis. \n\n"}
{"id": "1706.06873", "contents": "Title: MEC: Memory-efficient Convolution for Deep Neural Network Abstract: Convolution is a critical component in modern deep neural networks, thus\nseveral algorithms for convolution have been developed. Direct convolution is\nsimple but suffers from poor performance. As an alternative, multiple indirect\nmethods have been proposed including im2col-based convolution, FFT-based\nconvolution, or Winograd-based algorithm. However, all these indirect methods\nhave high memory-overhead, which creates performance degradation and offers a\npoor trade-off between performance and memory consumption. In this work, we\npropose a memory-efficient convolution or MEC with compact lowering, which\nreduces memory-overhead substantially and accelerates convolution process. MEC\nlowers the input matrix in a simple yet efficient/compact way (i.e., much less\nmemory-overhead), and then executes multiple small matrix multiplications in\nparallel to get convolution completed. Additionally, the reduced memory\nfootprint improves memory sub-system efficiency, improving performance. Our\nexperimental results show that MEC reduces memory consumption significantly\nwith good speedup on both mobile and server platforms, compared with other\nindirect convolution algorithms. \n\n"}
{"id": "1706.07842", "contents": "Title: Image Forgery Localization Based on Multi-Scale Convolutional Neural\n  Networks Abstract: In this paper, we propose to utilize Convolutional Neural Networks (CNNs) and\nthe segmentation-based multi-scale analysis to locate tampered areas in digital\nimages. First, to deal with color input sliding windows of different scales, a\nunified CNN architecture is designed. Then, we elaborately design the training\nprocedures of CNNs on sampled training patches. With a set of robust\nmulti-scale tampering detectors based on CNNs, complementary tampering\npossibility maps can be generated. Last but not least, a segmentation-based\nmethod is proposed to fuse the maps and generate the final decision map. By\nexploiting the benefits of both the small-scale and large-scale analyses, the\nsegmentation-based multi-scale analysis can lead to a performance leap in\nforgery localization of CNNs. Numerous experiments are conducted to demonstrate\nthe effectiveness and efficiency of our method. \n\n"}
{"id": "1706.07911", "contents": "Title: Large-Scale Mapping of Human Activity using Geo-Tagged Videos Abstract: This paper is the first work to perform spatio-temporal mapping of human\nactivity using the visual content of geo-tagged videos. We utilize a recent\ndeep-learning based video analysis framework, termed hidden two-stream\nnetworks, to recognize a range of activities in YouTube videos. This framework\nis efficient and can run in real time or faster which is important for\nrecognizing events as they occur in streaming video or for reducing latency in\nanalyzing already captured video. This is, in turn, important for using video\nin smart-city applications. We perform a series of experiments to show our\napproach is able to accurately map activities both spatially and temporally. We\nalso demonstrate the advantages of using the visual content over the\ntags/titles. \n\n"}
{"id": "1706.09274", "contents": "Title: The YouTube-8M Kaggle Competition: Challenges and Methods Abstract: We took part in the YouTube-8M Video Understanding Challenge hosted on\nKaggle, and achieved the 10th place within less than one month's time. In this\npaper, we present an extensive analysis and solution to the underlying\nmachine-learning problem based on frame-level data, where major challenges are\nidentified and corresponding preliminary methods are proposed. It's noteworthy\nthat, with merely the proposed strategies and uniformly-averaging multi-crop\nensemble was it sufficient for us to reach our ranking. We also report the\nmethods we believe to be promising but didn't have enough time to train to\nconvergence. We hope this paper could serve, to some extent, as a review and\nguideline of the YouTube-8M multi-label video classification benchmark,\ninspiring future attempts and research. \n\n"}
{"id": "1707.00243", "contents": "Title: Deep GrabCut for Object Selection Abstract: Most previous bounding-box-based segmentation methods assume the bounding box\ntightly covers the object of interest. However it is common that a rectangle\ninput could be too large or too small. In this paper, we propose a novel\nsegmentation approach that uses a rectangle as a soft constraint by\ntransforming it into an Euclidean distance map. A convolutional encoder-decoder\nnetwork is trained end-to-end by concatenating images with these distance maps\nas inputs and predicting the object masks as outputs. Our approach gets\naccurate segmentation results given sloppy rectangles while being general for\nboth interactive segmentation and instance segmentation. We show our network\nextends to curve-based input without retraining. We further apply our network\nto instance-level semantic segmentation and resolve any overlap using a\nconditional random field. Experiments on benchmark datasets demonstrate the\neffectiveness of the proposed approaches. \n\n"}
{"id": "1707.00684", "contents": "Title: Deep-learning-based data page classification for holographic memory Abstract: We propose a deep-learning-based classification of data pages used in\nholographic memory. We numerically investigated the classification performance\nof a conventional multi-layer perceptron (MLP) and a deep neural network, under\nthe condition that reconstructed page data are contaminated by some noise and\nare randomly laterally shifted. The MLP was found to have a classification\naccuracy of 91.58%, whereas the deep neural network was able to classify data\npages at an accuracy of 99.98%. The accuracy of the deep neural network is two\norders of magnitude better than the MLP. \n\n"}
{"id": "1707.00755", "contents": "Title: Appearance invariance in convolutional networks with neighborhood\n  similarity Abstract: We present a neighborhood similarity layer (NSL) which induces appearance\ninvariance in a network when used in conjunction with convolutional layers. We\nare motivated by the observation that, even though convolutional networks have\nlow generalization error, their generalization capability does not extend to\nsamples which are not represented by the training data. For instance, while\nnovel appearances of learned concepts pose no problem for the human visual\nsystem, feedforward convolutional networks are generally not successful in such\nsituations. Motivated by the Gestalt principle of grouping with respect to\nsimilarity, the proposed NSL transforms its input feature map using the feature\nvectors at each pixel as a frame of reference, i.e. center of attention, for\nits surrounding neighborhood. This transformation is spatially varying, hence\nnot a convolution. It is differentiable; therefore, networks including the\nproposed layer can be trained in an end-to-end manner. We analyze the\ninvariance of NSL to significant changes in appearance that are not represented\nin the training data. We also demonstrate its advantages for digit recognition,\nsemantic labeling and cell detection problems. \n\n"}
{"id": "1707.00809", "contents": "Title: Selective Deep Convolutional Features for Image Retrieval Abstract: Convolutional Neural Network (CNN) is a very powerful approach to extract\ndiscriminative local descriptors for effective image search. Recent work adopts\nfine-tuned strategies to further improve the discriminative power of the\ndescriptors. Taking a different approach, in this paper, we propose a novel\nframework to achieve competitive retrieval performance. Firstly, we propose\nvarious masking schemes, namely SIFT-mask, SUM-mask, and MAX-mask, to select a\nrepresentative subset of local convolutional features and remove a large number\nof redundant features. We demonstrate that this can effectively address the\nburstiness issue and improve retrieval accuracy. Secondly, we propose to employ\nrecent embedding and aggregating methods to further enhance feature\ndiscriminability. Extensive experiments demonstrate that our proposed framework\nachieves state-of-the-art retrieval accuracy. \n\n"}
{"id": "1707.01219", "contents": "Title: Like What You Like: Knowledge Distill via Neuron Selectivity Transfer Abstract: Despite deep neural networks have demonstrated extraordinary power in various\napplications, their superior performances are at expense of high storage and\ncomputational costs. Consequently, the acceleration and compression of neural\nnetworks have attracted much attention recently. Knowledge Transfer (KT), which\naims at training a smaller student network by transferring knowledge from a\nlarger teacher model, is one of the popular solutions. In this paper, we\npropose a novel knowledge transfer method by treating it as a distribution\nmatching problem. Particularly, we match the distributions of neuron\nselectivity patterns between teacher and student networks. To achieve this\ngoal, we devise a new KT loss function by minimizing the Maximum Mean\nDiscrepancy (MMD) metric between these distributions. Combined with the\noriginal loss function, our method can significantly improve the performance of\nstudent networks. We validate the effectiveness of our method across several\ndatasets, and further combine it with other KT methods to explore the best\npossible results. Last but not least, we fine-tune the model to other tasks\nsuch as object detection. The results are also encouraging, which confirm the\ntransferability of the learned features. \n\n"}
{"id": "1707.02683", "contents": "Title: Learning in High-Dimensional Multimedia Data: The State of the Art Abstract: During the last decade, the deluge of multimedia data has impacted a wide\nrange of research areas, including multimedia retrieval, 3D tracking, database\nmanagement, data mining, machine learning, social media analysis, medical\nimaging, and so on. Machine learning is largely involved in multimedia\napplications of building models for classification and regression tasks etc.,\nand the learning principle consists in designing the models based on the\ninformation contained in the multimedia dataset. While many paradigms exist and\nare widely used in the context of machine learning, most of them suffer from\nthe `curse of dimensionality', which means that some strange phenomena appears\nwhen data are represented in a high-dimensional space. Given the high\ndimensionality and the high complexity of multimedia data, it is important to\ninvestigate new machine learning algorithms to facilitate multimedia data\nanalysis. To deal with the impact of high dimensionality, an intuitive way is\nto reduce the dimensionality. On the other hand, some researchers devoted\nthemselves to designing some effective learning schemes for high-dimensional\ndata. In this survey, we cover feature transformation, feature selection and\nfeature encoding, three approaches fighting the consequences of the curse of\ndimensionality. Next, we briefly introduce some recent progress of effective\nlearning algorithms. Finally, promising future trends on multimedia learning\nare envisaged. \n\n"}
{"id": "1707.02749", "contents": "Title: Improving speaker turn embedding by crossmodal transfer learning from\n  face embedding Abstract: Learning speaker turn embeddings has shown considerable improvement in\nsituations where conventional speaker modeling approaches fail. However, this\nimprovement is relatively limited when compared to the gain observed in face\nembedding learning, which has been proven very successful for face verification\nand clustering tasks. Assuming that face and voices from the same identities\nshare some latent properties (like age, gender, ethnicity), we propose three\ntransfer learning approaches to leverage the knowledge from the face domain\n(learned from thousands of images and identities) for tasks in the speaker\ndomain. These approaches, namely target embedding transfer, relative distance\ntransfer, and clustering structure transfer, utilize the structure of the\nsource face embedding space at different granularities to regularize the target\nspeaker turn embedding space as optimizing terms. Our methods are evaluated on\ntwo public broadcast corpora and yield promising advances over competitive\nbaselines in verification and audio clustering tasks, especially when dealing\nwith short speaker utterances. The analysis of the results also gives insight\ninto characteristics of the embedding spaces and shows their potential\napplications. \n\n"}
{"id": "1707.02975", "contents": "Title: On Study of the Reliable Fully Convolutional Networks with Tree Arranged\n  Outputs (TAO-FCN) for Handwritten String Recognition Abstract: The handwritten string recognition is still a challengeable task, though the\npowerful deep learning tools were introduced. In this paper, based on TAO-FCN,\nwe proposed an end-to-end system for handwritten string recognition. Compared\nwith the conventional methods, there is no preprocess nor manually designed\nrules employed. With enough labelled data, it is easy to apply the proposed\nmethod to different applications. Although the performance of the proposed\nmethod may not be comparable with the state-of-the-art approaches, it's\nusability and robustness are more meaningful for practical applications. \n\n"}
{"id": "1707.04931", "contents": "Title: Pathological OCT Retinal Layer Segmentation using Branch Residual\n  U-shape Networks Abstract: The automatic segmentation of retinal layer structures enables\nclinically-relevant quantification and monitoring of eye disorders over time in\nOCT imaging. Eyes with late-stage diseases are particularly challenging to\nsegment, as their shape is highly warped due to pathological biomarkers. In\nthis context, we propose a novel fully Convolutional Neural Network (CNN)\narchitecture which combines dilated residual blocks in an asymmetric U-shape\nconfiguration, and can segment multiple layers of highly pathological eyes in\none shot. We validate our approach on a dataset of late-stage AMD patients and\ndemonstrate lower computational costs and higher performance compared to other\nstate-of-the-art methods. \n\n"}
{"id": "1707.05227", "contents": "Title: Auxiliary Objectives for Neural Error Detection Models Abstract: We investigate the utility of different auxiliary objectives and training\nstrategies within a neural sequence labeling approach to error detection in\nlearner writing. Auxiliary costs provide the model with additional linguistic\ninformation, allowing it to learn general-purpose compositional features that\ncan then be exploited for other objectives. Our experiments show that a joint\nlearning approach trained with parallel labels on in-domain data improves\nperformance over the previous best error detection system. While the resulting\nmodel has the same number of parameters, the additional objectives allow it to\nbe optimised more efficiently and achieve better performance. \n\n"}
{"id": "1707.05495", "contents": "Title: Order-Free RNN with Visual Attention for Multi-Label Classification Abstract: In this paper, we propose the joint learning attention and recurrent neural\nnetwork (RNN) models for multi-label classification. While approaches based on\nthe use of either model exist (e.g., for the task of image captioning),\ntraining such existing network architectures typically require pre-defined\nlabel sequences. For multi-label classification, it would be desirable to have\na robust inference process, so that the prediction error would not propagate\nand thus affect the performance. Our proposed model uniquely integrates\nattention and Long Short Term Memory (LSTM) models, which not only addresses\nthe above problem but also allows one to identify visual objects of interests\nwith varying sizes without the prior knowledge of particular label ordering.\nMore importantly, label co-occurrence information can be jointly exploited by\nour LSTM model. Finally, by advancing the technique of beam search, prediction\nof multiple labels can be efficiently achieved by our proposed network model. \n\n"}
{"id": "1707.05683", "contents": "Title: Exploiting Convolutional Representations for Multiscale Human Settlement\n  Detection Abstract: We test this premise and explore representation spaces from a single deep\nconvolutional network and their visualization to argue for a novel unified\nfeature extraction framework. The objective is to utilize and re-purpose\ntrained feature extractors without the need for network retraining on three\nremote sensing tasks i.e. superpixel mapping, pixel-level segmentation and\nsemantic based image visualization. By leveraging the same convolutional\nfeature extractors and viewing them as visual information extractors that\nencode different image representation spaces, we demonstrate a preliminary\ninductive transfer learning potential on multiscale experiments that\nincorporate edge-level details up to semantic-level information. \n\n"}
{"id": "1707.05974", "contents": "Title: Orthogonal and Idempotent Transformations for Learning Deep Neural\n  Networks Abstract: Identity transformations, used as skip-connections in residual networks,\ndirectly connect convolutional layers close to the input and those close to the\noutput in deep neural networks, improving information flow and thus easing the\ntraining. In this paper, we introduce two alternative linear transforms,\northogonal transformation and idempotent transformation. According to the\ndefinition and property of orthogonal and idempotent matrices, the product of\nmultiple orthogonal (same idempotent) matrices, used to form linear\ntransformations, is equal to a single orthogonal (idempotent) matrix, resulting\nin that information flow is improved and the training is eased. One interesting\npoint is that the success essentially stems from feature reuse and gradient\nreuse in forward and backward propagation for maintaining the information\nduring flow and eliminating the gradient vanishing problem because of the\nexpress way through skip-connections. We empirically demonstrate the\neffectiveness of the proposed two transformations: similar performance in\nsingle-branch networks and even superior in multi-branch networks in comparison\nto identity transformations. \n\n"}
{"id": "1707.06267", "contents": "Title: Shape Generation using Spatially Partitioned Point Clouds Abstract: We propose a method to generate 3D shapes using point clouds. Given a\npoint-cloud representation of a 3D shape, our method builds a kd-tree to\nspatially partition the points. This orders them consistently across all\nshapes, resulting in reasonably good correspondences across all shapes. We then\nuse PCA analysis to derive a linear shape basis across the spatially\npartitioned points, and optimize the point ordering by iteratively minimizing\nthe PCA reconstruction error. Even with the spatial sorting, the point clouds\nare inherently noisy and the resulting distribution over the shape coefficients\ncan be highly multi-modal. We propose to use the expressive power of neural\nnetworks to learn a distribution over the shape coefficients in a\ngenerative-adversarial framework. Compared to 3D shape generative models\ntrained on voxel-representations, our point-based method is considerably more\nlight-weight and scalable, with little loss of quality. It also outperforms\nsimpler linear factor models such as Probabilistic PCA, both qualitatively and\nquantitatively, on a number of categories from the ShapeNet dataset.\nFurthermore, our method can easily incorporate other point attributes such as\nnormal and color information, an additional advantage over voxel-based\nrepresentations. \n\n"}
{"id": "1707.07388", "contents": "Title: Semantic 3D Occupancy Mapping through Efficient High Order CRFs Abstract: Semantic 3D mapping can be used for many applications such as robot\nnavigation and virtual interaction. In recent years, there has been great\nprogress in semantic segmentation and geometric 3D mapping. However, it is\nstill challenging to combine these two tasks for accurate and large-scale\nsemantic mapping from images. In the paper, we propose an incremental and\n(near) real-time semantic mapping system. A 3D scrolling occupancy grid map is\nbuilt to represent the world, which is memory and computationally efficient and\nbounded for large scale environments. We utilize the CNN segmentation as prior\nprediction and further optimize 3D grid labels through a novel CRF model.\nSuperpixels are utilized to enforce smoothness and form robust P N high order\npotential. An efficient mean field inference is developed for the graph\noptimization. We evaluate our system on the KITTI dataset and improve the\nsegmentation accuracy by 10% over existing systems. \n\n"}
{"id": "1707.07410", "contents": "Title: Toward Geometric Deep SLAM Abstract: We present a point tracking system powered by two deep convolutional neural\nnetworks. The first network, MagicPoint, operates on single images and extracts\nsalient 2D points. The extracted points are \"SLAM-ready\" because they are by\ndesign isolated and well-distributed throughout the image. We compare this\nnetwork against classical point detectors and discover a significant\nperformance gap in the presence of image noise. As transformation estimation is\nmore simple when the detected points are geometrically stable, we designed a\nsecond network, MagicWarp, which operates on pairs of point images (outputs of\nMagicPoint), and estimates the homography that relates the inputs. This\ntransformation engine differs from traditional approaches because it does not\nuse local point descriptors, only point locations. Both networks are trained\nwith simple synthetic data, alleviating the requirement of expensive external\ncamera ground truthing and advanced graphics rendering pipelines. The system is\nfast and lean, easily running 30+ FPS on a single CPU. \n\n"}
{"id": "1707.07890", "contents": "Title: Spatiotemporal Modeling for Crowd Counting in Videos Abstract: Region of Interest (ROI) crowd counting can be formulated as a regression\nproblem of learning a mapping from an image or a video frame to a crowd density\nmap. Recently, convolutional neural network (CNN) models have achieved\npromising results for crowd counting. However, even when dealing with video\ndata, CNN-based methods still consider each video frame independently, ignoring\nthe strong temporal correlation between neighboring frames. To exploit the\notherwise very useful temporal information in video sequences, we propose a\nvariant of a recent deep learning model called convolutional LSTM (ConvLSTM)\nfor crowd counting. Unlike the previous CNN-based methods, our method fully\ncaptures both spatial and temporal dependencies. Furthermore, we extend the\nConvLSTM model to a bidirectional ConvLSTM model which can access long-range\ninformation in both directions. Extensive experiments using four publicly\navailable datasets demonstrate the reliability of our approach and the\neffectiveness of incorporating temporal information to boost the accuracy of\ncrowd counting. In addition, we also conduct some transfer learning experiments\nto show that once our model is trained on one dataset, its learning experience\ncan be transferred easily to a new dataset which consists of only very few\nvideo frames for model adaptation. \n\n"}
{"id": "1707.08559", "contents": "Title: Video Highlight Prediction Using Audience Chat Reactions Abstract: Sports channel video portals offer an exciting domain for research on\nmultimodal, multilingual analysis. We present methods addressing the problem of\nautomatic video highlight prediction based on joint visual features and textual\nanalysis of the real-world audience discourse with complex slang, in both\nEnglish and traditional Chinese. We present a novel dataset based on League of\nLegends championships recorded from North American and Taiwanese Twitch.tv\nchannels (will be released for further research), and demonstrate strong\nresults on these using multimodal, character-level CNN-RNN model architectures. \n\n"}
{"id": "1707.08718", "contents": "Title: Ultra-low-power Wireless Streaming Cameras Abstract: Wireless video streaming has traditionally been considered an extremely\npower-hungry operation. Existing approaches optimize the camera and\ncommunication modules individually to minimize their power consumption.\nHowever, the joint redesign and optimization of wireless communication as well\nas the camera is what that provides more power saving. We present an\nultra-low-power wireless video streaming camera. To achieve this, we present a\nnovel \"analog\" video backscatter technique that feeds analog pixels from the\nphoto-diodes directly to the backscatter hardware, thereby eliminating power\nconsuming hardware components such as ADCs and amplifiers. We prototype our\nwireless camera using off-the-shelf hardware and show that our design can\nstream video at up to 13 FPS and can operate up to a distance of 150 feet from\nthe access point. Our COTS prototype consumes 2.36mW. Finally, to demonstrate\nthe potential of our design, we built two proof-of-concept applications: video\nstreaming for micro-robots and security cameras for face detection. \n\n"}
{"id": "1707.08814", "contents": "Title: Representation-Aggregation Networks for Segmentation of Multi-Gigapixel\n  Histology Images Abstract: Convolutional Neural Network (CNN) models have become the state-of-the-art\nfor most computer vision tasks with natural images. However, these are not best\nsuited for multi-gigapixel resolution Whole Slide Images (WSIs) of histology\nslides due to large size of these images. Current approaches construct smaller\npatches from WSIs which results in the loss of contextual information. We\npropose to capture the spatial context using novel Representation-Aggregation\nNetwork (RAN) for segmentation purposes, wherein the first network learns\npatch-level representation and the second network aggregates context from a\ngrid of neighbouring patches. We can use any CNN for representation learning,\nand can utilize CNN or 2D-Long Short Term Memory (2D-LSTM) for\ncontext-aggregation. Our method significantly outperformed conventional\npatch-based CNN approaches on segmentation of tumour in WSIs of breast cancer\ntissue sections. \n\n"}
{"id": "1707.09143", "contents": "Title: Localizing Actions from Video Labels and Pseudo-Annotations Abstract: The goal of this paper is to determine the spatio-temporal location of\nactions in video. Where training from hard to obtain box annotations is the\nnorm, we propose an intuitive and effective algorithm that localizes actions\nfrom their class label only. We are inspired by recent work showing that\nunsupervised action proposals selected with human point-supervision perform as\nwell as using expensive box annotations. Rather than asking users to provide\npoint supervision, we propose fully automatic visual cues that replace manual\npoint annotations. We call the cues pseudo-annotations, introduce five of them,\nand propose a correlation metric for automatically selecting and combining\nthem. Thorough evaluation on challenging action localization datasets shows\nthat we reach results comparable to results with full box supervision. We also\nshow that pseudo-annotations can be leveraged during testing to improve weakly-\nand strongly-supervised localizers. \n\n"}
{"id": "1707.09585", "contents": "Title: Virtual PET Images from CT Data Using Deep Convolutional Networks:\n  Initial Results Abstract: In this work we present a novel system for PET estimation using CT scans. We\nexplore the use of fully convolutional networks (FCN) and conditional\ngenerative adversarial networks (GAN) to export PET data from CT data. Our\ndataset includes 25 pairs of PET and CT scans where 17 were used for training\nand 8 for testing. The system was tested for detection of malignant tumors in\nthe liver region. Initial results look promising showing high detection\nperformance with a TPR of 92.3% and FPR of 0.25 per case. Future work entails\nexpansion of the current system to the entire body using a much larger dataset.\nSuch a system can be used for tumor detection and drug treatment evaluation in\na CT-only environment instead of the expansive and radioactive PET-CT scan. \n\n"}
{"id": "1707.09593", "contents": "Title: Discover and Learn New Objects from Documentaries Abstract: Despite the remarkable progress in recent years, detecting objects in a new\ncontext remains a challenging task. Detectors learned from a public dataset can\nonly work with a fixed list of categories, while training from scratch usually\nrequires a large amount of training data with detailed annotations. This work\naims to explore a novel approach -- learning object detectors from documentary\nfilms in a weakly supervised manner. This is inspired by the observation that\ndocumentaries often provide dedicated exposition of certain object categories,\nwhere visual presentations are aligned with subtitles. We believe that object\ndetectors can be learned from such a rich source of information. Towards this\ngoal, we develop a joint probabilistic framework, where individual pieces of\ninformation, including video frames and subtitles, are brought together via\nboth visual and linguistic links. On top of this formulation, we further derive\na weakly supervised learning algorithm, where object model learning and\ntraining set mining are unified in an optimization procedure. Experimental\nresults on a real world dataset demonstrate that this is an effective approach\nto learning new object detectors. \n\n"}
{"id": "1707.09597", "contents": "Title: ScanNet: A Fast and Dense Scanning Framework for Metastatic Breast\n  Cancer Detection from Whole-Slide Images Abstract: Lymph node metastasis is one of the most significant diagnostic indicators in\nbreast cancer, which is traditionally observed under the microscope by\npathologists. In recent years, computerized histology diagnosis has become one\nof the most rapidly expanding fields in medical image computing, which\nalleviates pathologists' workload and reduces misdiagnosis rate. However,\nautomatic detection of lymph node metastases from whole slide images remains a\nchallenging problem, due to the large-scale data with enormous resolutions and\nexistence of hard mimics. In this paper, we propose a novel framework by\nleveraging fully convolutional networks for efficient inference to meet the\nspeed requirement for clinical practice, while reconstructing dense predictions\nunder different offsets for ensuring accurate detection on both micro- and\nmacro-metastases. Incorporating with the strategies of asynchronous sample\nprefetching and hard negative mining, the network can be effectively trained.\nExtensive experiments on the benchmark dataset of 2016 Camelyon Grand Challenge\ncorroborated the efficacy of our method. Compared with the state-of-the-art\nmethods, our method achieved superior performance with a faster speed on the\ntumor localization task and surpassed human performance on the WSI\nclassification task. \n\n"}
{"id": "1708.00042", "contents": "Title: Spatio-Temporal Action Detection with Cascade Proposal and Location\n  Anticipation Abstract: In this work, we address the problem of spatio-temporal action detection in\ntemporally untrimmed videos. It is an important and challenging task as finding\naccurate human actions in both temporal and spatial space is important for\nanalyzing large-scale video data. To tackle this problem, we propose a cascade\nproposal and location anticipation (CPLA) model for frame-level action\ndetection. There are several salient points of our model: (1) a cascade region\nproposal network (casRPN) is adopted for action proposal generation and shows\nbetter localization accuracy compared with single region proposal network\n(RPN); (2) action spatio-temporal consistencies are exploited via a location\nanticipation network (LAN) and thus frame-level action detection is not\nconducted independently. Frame-level detections are then linked by solving an\nlinking score maximization problem, and temporally trimmed into spatio-temporal\naction tubes. We demonstrate the effectiveness of our model on the challenging\nUCF101 and LIRIS-HARL datasets, both achieving state-of-the-art performance. \n\n"}
{"id": "1708.00197", "contents": "Title: Video Object Segmentation with Re-identification Abstract: Conventional video segmentation methods often rely on temporal continuity to\npropagate masks. Such an assumption suffers from issues like drifting and\ninability to handle large displacement. To overcome these issues, we formulate\nan effective mechanism to prevent the target from being lost via adaptive\nobject re-identification. Specifically, our Video Object Segmentation with\nRe-identification (VS-ReID) model includes a mask propagation module and a ReID\nmodule. The former module produces an initial probability map by flow warping\nwhile the latter module retrieves missing instances by adaptive matching. With\nthese two modules iteratively applied, our VS-ReID records a global mean\n(Region Jaccard and Boundary F measure) of 0.699, the best performance in 2017\nDAVIS Challenge. \n\n"}
{"id": "1708.00710", "contents": "Title: Accurate Lung Segmentation via Network-Wise Training of Convolutional\n  Networks Abstract: We introduce an accurate lung segmentation model for chest radiographs based\non deep convolutional neural networks. Our model is based on atrous\nconvolutional layers to increase the field-of-view of filters efficiently. To\nimprove segmentation performances further, we also propose a multi-stage\ntraining strategy, network-wise training, which the current stage network is\nfed with both input images and the outputs from pre-stage network. It is shown\nthat this strategy has an ability to reduce falsely predicted labels and\nproduce smooth boundaries of lung fields. We evaluate the proposed model on a\ncommon benchmark dataset, JSRT, and achieve the state-of-the-art segmentation\nperformances with much fewer model parameters. \n\n"}
{"id": "1708.00919", "contents": "Title: Learning Spherical Convolution for Fast Features from 360{\\deg} Imagery Abstract: While 360{\\deg} cameras offer tremendous new possibilities in vision,\ngraphics, and augmented reality, the spherical images they produce make core\nfeature extraction non-trivial. Convolutional neural networks (CNNs) trained on\nimages from perspective cameras yield \"flat\" filters, yet 360{\\deg} images\ncannot be projected to a single plane without significant distortion. A naive\nsolution that repeatedly projects the viewing sphere to all tangent planes is\naccurate, but much too computationally intensive for real problems. We propose\nto learn a spherical convolutional network that translates a planar CNN to\nprocess 360{\\deg} imagery directly in its equirectangular projection. Our\napproach learns to reproduce the flat filter outputs on 360{\\deg} data,\nsensitive to the varying distortion effects across the viewing sphere. The key\nbenefits are 1) efficient feature extraction for 360{\\deg} images and video,\nand 2) the ability to leverage powerful pre-trained networks researchers have\ncarefully honed (together with massive labeled image training sets) for\nperspective images. We validate our approach compared to several alternative\nmethods in terms of both raw CNN output accuracy as well as applying a\nstate-of-the-art \"flat\" object detector to 360{\\deg} data. Our method yields\nthe most accurate results while saving orders of magnitude in computation\nversus the existing exact reprojection solution. \n\n"}
{"id": "1708.01191", "contents": "Title: Unsupervised Video Understanding by Reconciliation of Posture\n  Similarities Abstract: Understanding human activity and being able to explain it in detail surpasses\nmere action classification by far in both complexity and value. The challenge\nis thus to describe an activity on the basis of its most fundamental\nconstituents, the individual postures and their distinctive transitions.\nSupervised learning of such a fine-grained representation based on elementary\nposes is very tedious and does not scale. Therefore, we propose a completely\nunsupervised deep learning procedure based solely on video sequences, which\nstarts from scratch without requiring pre-trained networks, predefined body\nmodels, or keypoints. A combinatorial sequence matching algorithm proposes\nrelations between frames from subsets of the training data, while a CNN is\nreconciling the transitivity conflicts of the different subsets to learn a\nsingle concerted pose embedding despite changes in appearance across sequences.\nWithout any manual annotation, the model learns a structured representation of\npostures and their temporal development. The model not only enables retrieval\nof similar postures but also temporal super-resolution. Additionally, based on\na recurrent formulation, next frames can be synthesized. \n\n"}
{"id": "1708.01565", "contents": "Title: Improving Speaker-Independent Lipreading with Domain-Adversarial\n  Training Abstract: We present a Lipreading system, i.e. a speech recognition system using only\nvisual features, which uses domain-adversarial training for speaker\nindependence. Domain-adversarial training is integrated into the optimization\nof a lipreader based on a stack of feedforward and LSTM (Long Short-Term\nMemory) recurrent neural networks, yielding an end-to-end trainable system\nwhich only requires a very small number of frames of untranscribed target data\nto substantially improve the recognition accuracy on the target speaker. On\npairs of different source and target speakers, we achieve a relative accuracy\nimprovement of around 40% with only 15 to 20 seconds of untranscribed target\nspeech data. On multi-speaker training setups, the accuracy improvements are\nsmaller but still substantial. \n\n"}
{"id": "1708.01806", "contents": "Title: Detecting Noteheads in Handwritten Scores with ConvNets and Bounding Box\n  Regression Abstract: Noteheads are the interface between the written score and music. Each\nnotehead on the page signifies one note to be played, and detecting noteheads\nis thus an unavoidable step for Optical Music Recognition. Noteheads are\nclearly distinct objects, however, the variety of music notation handwriting\nmakes noteheads harder to identify, and while handwritten music notation symbol\n{\\em classification} is a well-studied task, symbol {\\em detection} has usually\nbeen limited to heuristics and rule-based systems instead of machine learning\nmethods better suited to deal with the uncertainties in handwriting. We present\nongoing work on a simple notehead detector using convolutional neural networks\nfor pixel classification and bounding box regression that achieves a detection\nf-score of 0.97 on binary score images in the MUSCIMA++ dataset, does not\nrequire staff removal, and is applicable to a variety of handwriting styles and\nlevels of musical complexity. \n\n"}
{"id": "1708.02043", "contents": "Title: What is the Role of Recurrent Neural Networks (RNNs) in an Image Caption\n  Generator? Abstract: In neural image captioning systems, a recurrent neural network (RNN) is\ntypically viewed as the primary `generation' component. This view suggests that\nthe image features should be `injected' into the RNN. This is in fact the\ndominant view in the literature. Alternatively, the RNN can instead be viewed\nas only encoding the previously generated words. This view suggests that the\nRNN should only be used to encode linguistic features and that only the final\nrepresentation should be `merged' with the image features at a later stage.\nThis paper compares these two architectures. We find that, in general, late\nmerging outperforms injection, suggesting that RNNs are better viewed as\nencoders, rather than generators. \n\n"}
{"id": "1708.02313", "contents": "Title: GPLAC: Generalizing Vision-Based Robotic Skills using Weakly Labeled\n  Images Abstract: We tackle the problem of learning robotic sensorimotor control policies that\ncan generalize to visually diverse and unseen environments. Achieving broad\ngeneralization typically requires large datasets, which are difficult to obtain\nfor task-specific interactive processes such as reinforcement learning or\nlearning from demonstration. However, much of the visual diversity in the world\ncan be captured through passively collected datasets of images or videos. In\nour method, which we refer to as GPLAC (Generalized Policy Learning with\nAttentional Classifier), we use both interaction data and weakly labeled image\ndata to augment the generalization capacity of sensorimotor policies. Our\nmethod combines multitask learning on action selection and an auxiliary binary\nclassification objective, together with a convolutional neural network\narchitecture that uses an attentional mechanism to avoid distractors. We show\nthat pairing interaction data from just a single environment with a diverse\ndataset of weakly labeled data results in greatly improved generalization to\nunseen environments, and show that this generalization depends on both the\nauxiliary objective and the attentional architecture that we propose. We\ndemonstrate our results in both simulation and on a real robotic manipulator,\nand demonstrate substantial improvement over standard convolutional\narchitectures and domain adaptation methods. \n\n"}
{"id": "1708.02688", "contents": "Title: Statistics of Deep Generated Images Abstract: Here, we explore the low-level statistics of images generated by\nstate-of-the-art deep generative models. First, Variational auto-encoder\n(VAE~\\cite{kingma2013auto}), Wasserstein generative adversarial network\n(WGAN~\\cite{arjovsky2017wasserstein}) and deep convolutional generative\nadversarial network (DCGAN~\\cite{radford2015unsupervised}) are trained on the\nImageNet dataset and a large set of cartoon frames from animations. Then, for\nimages generated by these models as well as natural scenes and cartoons,\nstatistics including mean power spectrum, the number of connected components in\na given image area, distribution of random filter responses, and contrast\ndistribution are computed. Our analyses on training images support current\nfindings on scale invariance, non-Gaussianity, and Weibull contrast\ndistribution of natural scenes. We find that although similar results hold over\ncartoon images, there is still a significant difference between statistics of\nnatural scenes and images generated by VAE, DCGAN and WGAN models. In\nparticular, generated images do not have scale invariant mean power spectrum\nmagnitude, which indicates existence of extra structures in these images.\nInspecting how well the statistics of deep generated images match the known\nstatistical properties of natural images, such as scale invariance,\nnon-Gaussianity, and Weibull contrast distribution, can a) reveal the degree to\nwhich deep learning models capture the essence of the natural scenes, b)\nprovide a new dimension to evaluate models, and c) allow possible improvement\nof image generative models (e.g., via defining new loss functions). \n\n"}
{"id": "1708.02711", "contents": "Title: Tips and Tricks for Visual Question Answering: Learnings from the 2017\n  Challenge Abstract: This paper presents a state-of-the-art model for visual question answering\n(VQA), which won the first place in the 2017 VQA Challenge. VQA is a task of\nsignificant importance for research in artificial intelligence, given its\nmultimodal nature, clear evaluation protocol, and potential real-world\napplications. The performance of deep neural networks for VQA is very dependent\non choices of architectures and hyperparameters. To help further research in\nthe area, we describe in detail our high-performing, though relatively simple\nmodel. Through a massive exploration of architectures and hyperparameters\nrepresenting more than 3,000 GPU-hours, we identified tips and tricks that lead\nto its success, namely: sigmoid outputs, soft training targets, image features\nfrom bottom-up attention, gated tanh activations, output embeddings initialized\nusing GloVe and Google Images, large mini-batches, and smart shuffling of\ntraining data. We provide a detailed analysis of their impact on performance to\nassist others in making an appropriate selection. \n\n"}
{"id": "1708.02813", "contents": "Title: BlitzNet: A Real-Time Deep Network for Scene Understanding Abstract: Real-time scene understanding has become crucial in many applications such as\nautonomous driving. In this paper, we propose a deep architecture, called\nBlitzNet, that jointly performs object detection and semantic segmentation in\none forward pass, allowing real-time computations. Besides the computational\ngain of having a single network to perform several tasks, we show that object\ndetection and semantic segmentation benefit from each other in terms of\naccuracy. Experimental results for VOC and COCO datasets show state-of-the-art\nperformance for object detection and segmentation among real time systems. \n\n"}
{"id": "1708.02970", "contents": "Title: Personalized Cinemagraphs using Semantic Understanding and Collaborative\n  Learning Abstract: Cinemagraphs are a compelling way to convey dynamic aspects of a scene. In\nthese media, dynamic and still elements are juxtaposed to create an artistic\nand narrative experience. Creating a high-quality, aesthetically pleasing\ncinemagraph requires isolating objects in a semantically meaningful way and\nthen selecting good start times and looping periods for those objects to\nminimize visual artifacts (such a tearing). To achieve this, we present a new\ntechnique that uses object recognition and semantic segmentation as part of an\noptimization method to automatically create cinemagraphs from videos that are\nboth visually appealing and semantically meaningful. Given a scene with\nmultiple objects, there are many cinemagraphs one could create. Our method\nevaluates these multiple candidates and presents the best one, as determined by\na model trained to predict human preferences in a collaborative way. We\ndemonstrate the effectiveness of our approach with multiple results and a user\nstudy. \n\n"}
{"id": "1708.03888", "contents": "Title: Large Batch Training of Convolutional Networks Abstract: A common way to speed up training of large convolutional networks is to add\ncomputational units. Training is then performed using data-parallel synchronous\nStochastic Gradient Descent (SGD) with mini-batch divided between computational\nunits. With an increase in the number of nodes, the batch size grows. But\ntraining with large batch size often results in the lower model accuracy. We\nargue that the current recipe for large batch training (linear learning rate\nscaling with warm-up) is not general enough and training may diverge. To\novercome this optimization difficulties we propose a new training algorithm\nbased on Layer-wise Adaptive Rate Scaling (LARS). Using LARS, we scaled Alexnet\nup to a batch size of 8K, and Resnet-50 to a batch size of 32K without loss in\naccuracy. \n\n"}
{"id": "1708.04169", "contents": "Title: Divide and Fuse: A Re-ranking Approach for Person Re-identification Abstract: As re-ranking is a necessary procedure to boost person re-identification\n(re-ID) performance on large-scale datasets, the diversity of feature becomes\ncrucial to person reID for its importance both on designing pedestrian\ndescriptions and re-ranking based on feature fusion. However, in many\ncircumstances, only one type of pedestrian feature is available. In this paper,\nwe propose a \"Divide and use\" re-ranking framework for person re-ID. It\nexploits the diversity from different parts of a high-dimensional feature\nvector for fusion-based re-ranking, while no other features are accessible.\nSpecifically, given an image, the extracted feature is divided into\nsub-features. Then the contextual information of each sub-feature is\niteratively encoded into a new feature. Finally, the new features from the same\nimage are fused into one vector for re-ranking. Experimental results on two\nperson re-ID benchmarks demonstrate the effectiveness of the proposed\nframework. Especially, our method outperforms the state-of-the-art on the\nMarket-1501 dataset. \n\n"}
{"id": "1708.04370", "contents": "Title: Dockerface: an Easy to Install and Use Faster R-CNN Face Detector in a\n  Docker Container Abstract: Face detection is a very important task and a necessary pre-processing step\nfor many applications such as facial landmark detection, pose estimation,\nsentiment analysis and face recognition. Not only is face detection an\nimportant pre-processing step in computer vision applications but also in\ncomputational psychology, behavioral imaging and other fields where researchers\nmight not be initiated in computer vision frameworks and state-of-the-art\ndetection applications. A large part of existing research that includes face\ndetection as a pre-processing step uses existing out-of-the-box detectors such\nas the HoG-based dlib and the OpenCV Haar face detector which are no longer\nstate-of-the-art - they are primarily used because of their ease of use and\naccessibility. We introduce Dockerface, a very accurate Faster R-CNN face\ndetector in a Docker container which requires no training and is easy to\ninstall and use. \n\n"}
{"id": "1708.04680", "contents": "Title: Augmentor: An Image Augmentation Library for Machine Learning Abstract: The generation of artificial data based on existing observations, known as\ndata augmentation, is a technique used in machine learning to improve model\naccuracy, generalisation, and to control overfitting. Augmentor is a software\npackage, available in both Python and Julia versions, that provides a high\nlevel API for the expansion of image data using a stochastic, pipeline-based\napproach which effectively allows for images to be sampled from a distribution\nof augmented images at runtime. Augmentor provides methods for most standard\naugmentation practices as well as several advanced features such as\nlabel-preserving, randomised elastic distortions, and provides many helper\nfunctions for typical augmentation tasks used in machine learning. \n\n"}
{"id": "1708.06019", "contents": "Title: A Capacity Scaling Law for Artificial Neural Networks Abstract: We derive the calculation of two critical numbers predicting the behavior of\nperceptron networks. First, we derive the calculation of what we call the\nlossless memory (LM) dimension. The LM dimension is a generalization of the\nVapnik--Chervonenkis (VC) dimension that avoids structured data and therefore\nprovides an upper bound for perfectly fitting almost any training data. Second,\nwe derive what we call the MacKay (MK) dimension. This limit indicates a 50%\nchance of not being able to train a given function. Our derivations are\nperformed by embedding a neural network into Shannon's communication model\nwhich allows to interpret the two points as capacities measured in bits. We\npresent a proof and practical experiments that validate our upper bounds with\nrepeatable experiments using different network configurations, diverse\nimplementations, varying activation functions, and several learning algorithms.\nThe bottom line is that the two capacity points scale strictly linear with the\nnumber of weights. Among other practical applications, our result allows to\ncompare and benchmark different neural network implementations independent of a\nconcrete learning task. Our results provide insight into the capabilities and\nlimits of neural networks and generate valuable know how for experimental\ndesign decisions. \n\n"}
{"id": "1708.06250", "contents": "Title: Pillar Networks++: Distributed non-parametric deep and wide networks Abstract: In recent work, it was shown that combining multi-kernel based support vector\nmachines (SVMs) can lead to near state-of-the-art performance on an action\nrecognition dataset (HMDB-51 dataset). This was 0.4\\% lower than frameworks\nthat used hand-crafted features in addition to the deep convolutional feature\nextractors. In the present work, we show that combining distributed Gaussian\nProcesses with multi-stream deep convolutional neural networks (CNN) alleviate\nthe need to augment a neural network with hand-crafted features. In contrast to\nprior work, we treat each deep neural convolutional network as an expert\nwherein the individual predictions (and their respective uncertainties) are\ncombined into a Product of Experts (PoE) framework. \n\n"}
{"id": "1708.07089", "contents": "Title: Predicting Aesthetic Score Distribution through Cumulative\n  Jensen-Shannon Divergence Abstract: Aesthetic quality prediction is a challenging task in the computer vision\ncommunity because of the complex interplay with semantic contents and\nphotographic technologies. Recent studies on the powerful deep learning based\naesthetic quality assessment usually use a binary high-low label or a numerical\nscore to represent the aesthetic quality. However the scalar representation\ncannot describe well the underlying varieties of the human perception of\naesthetics. In this work, we propose to predict the aesthetic score\ndistribution (i.e., a score distribution vector of the ordinal basic human\nratings) using Deep Convolutional Neural Network (DCNN). Conventional DCNNs\nwhich aim to minimize the difference between the predicted scalar numbers or\nvectors and the ground truth cannot be directly used for the ordinal basic\nrating distribution. Thus, a novel CNN based on the Cumulative distribution\nwith Jensen-Shannon divergence (CJS-CNN) is presented to predict the aesthetic\nscore distribution of human ratings, with a new reliability-sensitive learning\nmethod based on the kurtosis of the score distribution, which eliminates the\nrequirement of the original full data of human ratings (without normalization).\nExperimental results on large scale aesthetic dataset demonstrate the\neffectiveness of our introduced CJS-CNN in this task. \n\n"}
{"id": "1708.07265", "contents": "Title: An Image Analysis Approach to the Calligraphy of Books Abstract: Text network analysis has received increasing attention as a consequence of\nits wide range of applications. In this work, we extend a previous work founded\non the study of topological features of mesoscopic networks. Here, the\ngeometrical properties of visualized networks are quantified in terms of\nseveral image analysis techniques and used as subsidies for authorship\nattribution. It was found that the visual features account for performance\nsimilar to that achieved by using topological measurements. In addition, the\ncombination of these two types of features improved the performance. \n\n"}
{"id": "1708.07549", "contents": "Title: Objective Classes for Micro-Facial Expression Recognition Abstract: Micro-expressions are brief spontaneous facial expressions that appear on a\nface when a person conceals an emotion, making them different to normal facial\nexpressions in subtlety and duration. Currently, emotion classes within the\nCASME II dataset are based on Action Units and self-reports, creating conflicts\nduring machine learning training. We will show that classifying expressions\nusing Action Units, instead of predicted emotion, removes the potential bias of\nhuman reporting. The proposed classes are tested using LBP-TOP, HOOF and HOG 3D\nfeature descriptors. The experiments are evaluated on two benchmark FACS coded\ndatasets: CASME II and SAMM. The best result achieves 86.35\\% accuracy when\nclassifying the proposed 5 classes on CASME II using HOG 3D, outperforming the\nresult of the state-of-the-art 5-class emotional-based classification in CASME\nII. Results indicate that classification based on Action Units provides an\nobjective method to improve micro-expression recognition. \n\n"}
{"id": "1708.08917", "contents": "Title: CirCNN: Accelerating and Compressing Deep Neural Networks Using\n  Block-CirculantWeight Matrices Abstract: Large-scale deep neural networks (DNNs) are both compute and memory\nintensive. As the size of DNNs continues to grow, it is critical to improve the\nenergy efficiency and performance while maintaining accuracy. For DNNs, the\nmodel size is an important factor affecting performance, scalability and energy\nefficiency. Weight pruning achieves good compression ratios but suffers from\nthree drawbacks: 1) the irregular network structure after pruning; 2) the\nincreased training complexity; and 3) the lack of rigorous guarantee of\ncompression ratio and inference accuracy. To overcome these limitations, this\npaper proposes CirCNN, a principled approach to represent weights and process\nneural networks using block-circulant matrices. CirCNN utilizes the Fast\nFourier Transform (FFT)-based fast multiplication, simultaneously reducing the\ncomputational complexity (both in inference and training) from O(n2) to\nO(nlogn) and the storage complexity from O(n2) to O(n), with negligible\naccuracy loss. Compared to other approaches, CirCNN is distinct due to its\nmathematical rigor: it can converge to the same effectiveness as DNNs without\ncompression. The CirCNN architecture, a universal DNN inference engine that can\nbe implemented on various hardware/software platforms with configurable network\narchitecture. To demonstrate the performance and energy efficiency, we test\nCirCNN in FPGA, ASIC and embedded processors. Our results show that CirCNN\narchitecture achieves very high energy efficiency and performance with a small\nhardware footprint. Based on the FPGA implementation and ASIC synthesis\nresults, CirCNN achieves 6-102X energy efficiency improvements compared with\nthe best state-of-the-art results. \n\n"}
{"id": "1708.08987", "contents": "Title: Deep Learning for Medical Image Analysis Abstract: This report describes my research activities in the Hasso Plattner Institute\nand summarizes my Ph.D. plan and several novels, end-to-end trainable\napproaches for analyzing medical images using deep learning algorithm. In this\nreport, as an example, we explore different novel methods based on deep\nlearning for brain abnormality detection, recognition, and segmentation. This\nreport prepared for the doctoral consortium in the AIME-2017 conference. \n\n"}
{"id": "1708.09011", "contents": "Title: Real-Time 6DOF Pose Relocalization for Event Cameras with Stacked\n  Spatial LSTM Networks Abstract: We present a new method to relocalize the 6DOF pose of an event camera solely\nbased on the event stream. Our method first creates the event image from a list\nof events that occurs in a very short time interval, then a Stacked Spatial\nLSTM Network (SP-LSTM) is used to learn the camera pose. Our SP-LSTM is\ncomposed of a CNN to learn deep features from the event images and a stack of\nLSTM to learn spatial dependencies in the image feature space. We show that the\nspatial dependency plays an important role in the relocalization task and the\nSP-LSTM can effectively learn this information. The experimental results on a\npublicly available dataset show that our approach generalizes well and\noutperforms recent methods by a substantial margin. Overall, our proposed\nmethod reduces by approx. 6 times the position error and 3 times the\norientation error compared to the current state of the art. The source code and\ntrained models will be released. \n\n"}
{"id": "1708.09083", "contents": "Title: Adaptive SVM+: Learning with Privileged Information for Domain\n  Adaptation Abstract: Incorporating additional knowledge in the learning process can be beneficial\nfor several computer vision and machine learning tasks. Whether privileged\ninformation originates from a source domain that is adapted to a target domain,\nor as additional features available at training time only, using such\nprivileged (i.e., auxiliary) information is of high importance as it improves\nthe recognition performance and generalization. However, both primary and\nprivileged information are rarely derived from the same distribution, which\nposes an additional challenge to the recognition task. To address these\nchallenges, we present a novel learning paradigm that leverages privileged\ninformation in a domain adaptation setup to perform visual recognition tasks.\nThe proposed framework, named Adaptive SVM+, combines the advantages of both\nthe learning using privileged information (LUPI) paradigm and the domain\nadaptation framework, which are naturally embedded in the objective function of\na regular SVM. We demonstrate the effectiveness of our approach on the publicly\navailable Animals with Attributes and INTERACT datasets and report\nstate-of-the-art results in both of them. \n\n"}
{"id": "1709.00575", "contents": "Title: Grasping the Finer Point: A Supervised Similarity Network for Metaphor\n  Detection Abstract: The ubiquity of metaphor in our everyday communication makes it an important\nproblem for natural language understanding. Yet, the majority of metaphor\nprocessing systems to date rely on hand-engineered features and there is still\nno consensus in the field as to which features are optimal for this task. In\nthis paper, we present the first deep learning architecture designed to capture\nmetaphorical composition. Our results demonstrate that it outperforms the\nexisting approaches in the metaphor identification task. \n\n"}
{"id": "1709.01424", "contents": "Title: Towards social pattern characterization in egocentric photo-streams Abstract: Following the increasingly popular trend of social interaction analysis in\negocentric vision, this manuscript presents a comprehensive study for automatic\nsocial pattern characterization of a wearable photo-camera user, by relying on\nthe visual analysis of egocentric photo-streams. The proposed framework\nconsists of three major steps. The first step is to detect social interactions\nof the user where the impact of several social signals on the task is explored.\nThe detected social events are inspected in the second step for categorization\ninto different social meetings. These two steps act at event-level where each\npotential social event is modeled as a multi-dimensional time-series, whose\ndimensions correspond to a set of relevant features for each task, and LSTM is\nemployed to classify the time-series. The last step of the framework is to\ncharacterize social patterns, which is essentially to infer the diversity and\nfrequency of the social relations of the user through discovery of recurrences\nof the same people across the whole set of social events of the user.\nExperimental evaluation over a dataset acquired by 9 users demonstrates\npromising results on the task of social pattern characterization from\negocentric photo-streams. \n\n"}
{"id": "1709.01472", "contents": "Title: Leveraging multiple datasets for deep leaf counting Abstract: The number of leaves a plant has is one of the key traits (phenotypes)\ndescribing its development and growth. Here, we propose an automated, deep\nlearning based approach for counting leaves in model rosette plants. While\nstate-of-the-art results on leaf counting with deep learning methods have\nrecently been reported, they obtain the count as a result of leaf segmentation\nand thus require per-leaf (instance) segmentation to train the models (a rather\nstrong annotation). Instead, our method treats leaf counting as a direct\nregression problem and thus only requires as annotation the total leaf count\nper plant. We argue that combining different datasets when training a deep\nneural network is beneficial and improves the results of the proposed approach.\nWe evaluate our method on the CVPPP 2017 Leaf Counting Challenge dataset, which\ncontains images of Arabidopsis and tobacco plants. Experimental results show\nthat the proposed method significantly outperforms the winner of the previous\nCVPPP challenge, improving the results by a minimum of ~50% on each of the test\ndatasets, and can achieve this performance without knowing the experimental\norigin of the data (i.e. in the wild setting of the challenge). We also compare\nthe counting accuracy of our model with that of per leaf segmentation\nalgorithms, achieving a 20% decrease in mean absolute difference in count\n(|DiC|). \n\n"}
{"id": "1709.01779", "contents": "Title: Deep learning from crowds Abstract: Over the last few years, deep learning has revolutionized the field of\nmachine learning by dramatically improving the state-of-the-art in various\ndomains. However, as the size of supervised artificial neural networks grows,\ntypically so does the need for larger labeled datasets. Recently, crowdsourcing\nhas established itself as an efficient and cost-effective solution for labeling\nlarge sets of data in a scalable manner, but it often requires aggregating\nlabels from multiple noisy contributors with different levels of expertise. In\nthis paper, we address the problem of learning deep neural networks from\ncrowds. We begin by describing an EM algorithm for jointly learning the\nparameters of the network and the reliabilities of the annotators. Then, a\nnovel general-purpose crowd layer is proposed, which allows us to train deep\nneural networks end-to-end, directly from the noisy labels of multiple\nannotators, using only backpropagation. We empirically show that the proposed\napproach is able to internally capture the reliability and biases of different\nannotators and achieve new state-of-the-art results for various crowdsourced\ndatasets across different settings, namely classification, regression and\nsequence labeling. \n\n"}
{"id": "1709.01784", "contents": "Title: Cross-Domain Image Retrieval with Attention Modeling Abstract: With the proliferation of e-commerce websites and the ubiquitousness of smart\nphones, cross-domain image retrieval using images taken by smart phones as\nqueries to search products on e-commerce websites is emerging as a popular\napplication. One challenge of this task is to locate the attention of both the\nquery and database images. In particular, database images, e.g. of fashion\nproducts, on e-commerce websites are typically displayed with other\naccessories, and the images taken by users contain noisy background and large\nvariations in orientation and lighting. Consequently, their attention is\ndifficult to locate. In this paper, we exploit the rich tag information\navailable on the e-commerce websites to locate the attention of database\nimages. For query images, we use each candidate image in the database as the\ncontext to locate the query attention. Novel deep convolutional neural network\narchitectures, namely TagYNet and CtxYNet, are proposed to learn the attention\nweights and then extract effective representations of the images. Experimental\nresults on public datasets confirm that our approaches have significant\nimprovement over the existing methods in terms of the retrieval accuracy and\nefficiency. \n\n"}
{"id": "1709.02249", "contents": "Title: Uncertainty-Aware Learning from Demonstration using Mixture Density\n  Networks with Sampling-Free Variance Modeling Abstract: In this paper, we propose an uncertainty-aware learning from demonstration\nmethod by presenting a novel uncertainty estimation method utilizing a mixture\ndensity network appropriate for modeling complex and noisy human behaviors. The\nproposed uncertainty acquisition can be done with a single forward path without\nMonte Carlo sampling and is suitable for real-time robotics applications. The\nproperties of the proposed uncertainty measure are analyzed through three\ndifferent synthetic examples, absence of data, heavy measurement noise, and\ncomposition of functions scenarios. We show that each case can be distinguished\nusing the proposed uncertainty measure and presented an uncertainty-aware\nlearn- ing from demonstration method of an autonomous driving using this\nproperty. The proposed uncertainty-aware learning from demonstration method\noutperforms other compared methods in terms of safety using a complex\nreal-world driving dataset. \n\n"}
{"id": "1709.02373", "contents": "Title: Adaptive PCA for Time-Varying Data Abstract: In this paper, we present an online adaptive PCA algorithm that is able to\ncompute the full dimensional eigenspace per new time-step of sequential data.\nThe algorithm is based on a one-step update rule that considers all second\norder correlations between previous samples and the new time-step. Our\nalgorithm has O(n) complexity per new time-step in its deterministic mode and\nO(1) complexity per new time-step in its stochastic mode. We test our algorithm\non a number of time-varying datasets of different physical phenomena. Explained\nvariance curves indicate that our technique provides an excellent approximation\nto the original eigenspace computed using standard PCA in batch mode. In\naddition, our experiments show that the stochastic mode, despite its much lower\ncomputational complexity, converges to the same eigenspace computed using the\ndeterministic mode. \n\n"}
{"id": "1709.02995", "contents": "Title: Optimal Transport for Deep Joint Transfer Learning Abstract: Training a Deep Neural Network (DNN) from scratch requires a large amount of\nlabeled data. For a classification task where only small amount of training\ndata is available, a common solution is to perform fine-tuning on a DNN which\nis pre-trained with related source data. This consecutive training process is\ntime consuming and does not consider explicitly the relatedness between\ndifferent source and target tasks.\n  In this paper, we propose a novel method to jointly fine-tune a Deep Neural\nNetwork with source data and target data. By adding an Optimal Transport loss\n(OT loss) between source and target classifier predictions as a constraint on\nthe source classifier, the proposed Joint Transfer Learning Network (JTLN) can\neffectively learn useful knowledge for target classification from source data.\nFurthermore, by using different kind of metric as cost matrix for the OT loss,\nJTLN can incorporate different prior knowledge about the relatedness between\ntarget categories and source categories.\n  We carried out experiments with JTLN based on Alexnet on image classification\ndatasets and the results verify the effectiveness of the proposed JTLN in\ncomparison with standard consecutive fine-tuning. This Joint Transfer Learning\nwith OT loss is general and can also be applied to other kind of Neural\nNetworks. \n\n"}
{"id": "1709.03247", "contents": "Title: Evolution of Convolutional Highway Networks Abstract: Convolutional highways are deep networks based on multiple stacked\nconvolutional layers for feature preprocessing. We introduce an evolutionary\nalgorithm (EA) for optimization of the structure and hyperparameters of\nconvolutional highways and demonstrate the potential of this optimization\nsetting on the well-known MNIST data set. The (1+1)-EA employs Rechenberg's\nmutation rate control and a niching mechanism to overcome local optima adapts\nthe optimization approach. An experimental study shows that the EA is capable\nof improving the state-of-the-art network contribution and of evolving highway\nnetworks from scratch. \n\n"}
{"id": "1709.03708", "contents": "Title: PQk-means: Billion-scale Clustering for Product-quantized Codes Abstract: Data clustering is a fundamental operation in data analysis. For handling\nlarge-scale data, the standard k-means clustering method is not only slow, but\nalso memory-inefficient. We propose an efficient clustering method for\nbillion-scale feature vectors, called PQk-means. By first compressing input\nvectors into short product-quantized (PQ) codes, PQk-means achieves fast and\nmemory-efficient clustering, even for high-dimensional vectors. Similar to\nk-means, PQk-means repeats the assignment and update steps, both of which can\nbe performed in the PQ-code domain. Experimental results show that even\nshort-length (32 bit) PQ-codes can produce competitive results compared with\nk-means. This result is of practical importance for clustering in\nmemory-restricted environments. Using the proposed PQk-means scheme, the\nclustering of one billion 128D SIFT features with K = 10^5 is achieved within\n14 hours, using just 32 GB of memory consumption on a single computer. \n\n"}
{"id": "1709.03946", "contents": "Title: Multimodal Content Analysis for Effective Advertisements on YouTube Abstract: The rapid advances in e-commerce and Web 2.0 technologies have greatly\nincreased the impact of commercial advertisements on the general public. As a\nkey enabling technology, a multitude of recommender systems exists which\nanalyzes user features and browsing patterns to recommend appealing\nadvertisements to users. In this work, we seek to study the characteristics or\nattributes that characterize an effective advertisement and recommend a useful\nset of features to aid the designing and production processes of commercial\nadvertisements. We analyze the temporal patterns from multimedia content of\nadvertisement videos including auditory, visual and textual components, and\nstudy their individual roles and synergies in the success of an advertisement.\nThe objective of this work is then to measure the effectiveness of an\nadvertisement, and to recommend a useful set of features to advertisement\ndesigners to make it more successful and approachable to users. Our proposed\nframework employs the signal processing technique of cross modality feature\nlearning where data streams from different components are employed to train\nseparate neural network models and are then fused together to learn a shared\nrepresentation. Subsequently, a neural network model trained on this joint\nfeature embedding representation is utilized as a classifier to predict\nadvertisement effectiveness. We validate our approach using subjective ratings\nfrom a dedicated user study, the sentiment strength of online viewer comments,\nand a viewer opinion metric of the ratio of the Likes and Views received by\neach advertisement from an online platform. \n\n"}
{"id": "1709.04329", "contents": "Title: GLAD: Global-Local-Alignment Descriptor for Pedestrian Retrieval Abstract: The huge variance of human pose and the misalignment of detected human images\nsignificantly increase the difficulty of person Re-Identification (Re-ID).\nMoreover, efficient Re-ID systems are required to cope with the massive visual\ndata being produced by video surveillance systems. Targeting to solve these\nproblems, this work proposes a Global-Local-Alignment Descriptor (GLAD) and an\nefficient indexing and retrieval framework, respectively. GLAD explicitly\nleverages the local and global cues in human body to generate a discriminative\nand robust representation. It consists of part extraction and descriptor\nlearning modules, where several part regions are first detected and then deep\nneural networks are designed for representation learning on both the local and\nglobal regions. A hierarchical indexing and retrieval framework is designed to\neliminate the huge redundancy in the gallery set, and accelerate the online\nRe-ID procedure. Extensive experimental results show GLAD achieves competitive\naccuracy compared to the state-of-the-art methods. Our retrieval framework\nsignificantly accelerates the online Re-ID procedure without loss of accuracy.\nTherefore, this work has potential to work better on person Re-ID tasks in real\nscenarios. \n\n"}
{"id": "1709.04905", "contents": "Title: One-Shot Visual Imitation Learning via Meta-Learning Abstract: In order for a robot to be a generalist that can perform a wide range of\njobs, it must be able to acquire a wide variety of skills quickly and\nefficiently in complex unstructured environments. High-capacity models such as\ndeep neural networks can enable a robot to represent complex skills, but\nlearning each skill from scratch then becomes infeasible. In this work, we\npresent a meta-imitation learning method that enables a robot to learn how to\nlearn more efficiently, allowing it to acquire new skills from just a single\ndemonstration. Unlike prior methods for one-shot imitation, our method can\nscale to raw pixel inputs and requires data from significantly fewer prior\ntasks for effective learning of new skills. Our experiments on both simulated\nand real robot platforms demonstrate the ability to learn new tasks,\nend-to-end, from a single visual demonstration. \n\n"}
{"id": "1709.05038", "contents": "Title: Self-Guiding Multimodal LSTM - when we do not have a perfect training\n  dataset for image captioning Abstract: In this paper, a self-guiding multimodal LSTM (sg-LSTM) image captioning\nmodel is proposed to handle uncontrolled imbalanced real-world image-sentence\ndataset. We collect FlickrNYC dataset from Flickr as our testbed with 306,165\nimages and the original text descriptions uploaded by the users are utilized as\nthe ground truth for training. Descriptions in FlickrNYC dataset vary\ndramatically ranging from short term-descriptions to long\nparagraph-descriptions and can describe any visual aspects, or even refer to\nobjects that are not depicted. To deal with the imbalanced and noisy situation\nand to fully explore the dataset itself, we propose a novel guiding textual\nfeature extracted utilizing a multimodal LSTM (m-LSTM) model. Training of\nm-LSTM is based on the portion of data in which the image content and the\ncorresponding descriptions are strongly bonded. Afterwards, during the training\nof sg-LSTM on the rest training data, this guiding information serves as\nadditional input to the network along with the image representations and the\nground-truth descriptions. By integrating these input components into a\nmultimodal block, we aim to form a training scheme with the textual information\ntightly coupled with the image content. The experimental results demonstrate\nthat the proposed sg-LSTM model outperforms the traditional state-of-the-art\nmultimodal RNN captioning framework in successfully describing the key\ncomponents of the input images. \n\n"}
{"id": "1709.05072", "contents": "Title: Joint Hierarchical Category Structure Learning and Large-Scale Image\n  Classification Abstract: We investigate the scalable image classification problem with a large number\nof categories. Hierarchical visual data structures are helpful for improving\nthe efficiency and performance of large-scale multi-class classification. We\npropose a novel image classification method based on learning hierarchical\ninter-class structures. Specifically, we first design a fast algorithm to\ncompute the similarity metric between categories, based on which a visual tree\nis constructed by hierarchical spectral clustering. Using the learned visual\ntree, a test sample label is efficiently predicted by searching for the best\npath over the entire tree. The proposed method is extensively evaluated on the\nILSVRC2010 and Caltech 256 benchmark datasets. Experimental results show that\nour method obtains significantly better category hierarchies than other\nstate-of-the-art visual tree-based methods and, therefore, much more accurate\nclassification. \n\n"}
{"id": "1709.05324", "contents": "Title: Cystoid macular edema segmentation of Optical Coherence Tomography\n  images using fully convolutional neural networks and fully connected CRFs Abstract: In this paper we present a new method for cystoid macular edema (CME)\nsegmentation in retinal Optical Coherence Tomography (OCT) images, using a\nfully convolutional neural network (FCN) and a fully connected conditional\nrandom fields (dense CRFs). As a first step, the framework trains the FCN model\nto extract features from retinal layers in OCT images, which exhibit CME, and\nthen segments CME regions using the trained model. Thereafter, dense CRFs are\nused to refine the segmentation according to the edema appearance. We have\ntrained and tested the framework with OCT images from 10 patients with diabetic\nmacular edema (DME). Our experimental results show that fluid and concrete\nmacular edema areas were segmented with good adherence to boundaries. A\nsegmentation accuracy of $0.61\\pm 0.21$ (Dice coefficient) was achieved, with\nrespect to the ground truth, which compares favourably with the previous\nstate-of-the-art that used a kernel regression based method ($0.51\\pm 0.34$).\nOur approach is versatile and we believe it can be easily adapted to detect\nother macular defects. \n\n"}
{"id": "1709.05397", "contents": "Title: Zero-Shot Learning to Manage a Large Number of Place-Specific\n  Compressive Change Classifiers Abstract: With recent progress in large-scale map maintenance and long-term map\nlearning, the task of change detection on a large-scale map from a visual image\ncaptured by a mobile robot has become a problem of increasing criticality.\nPrevious approaches for change detection are typically based on image\ndifferencing and require the memorization of a prohibitively large number of\nmapped images in the above context. In contrast, this study follows the recent,\nefficient paradigm of change-classifier-learning and specifically employs a\ncollection of place-specific change classifiers. Our change-classifier-learning\nalgorithm is based on zero-shot learning (ZSL) and represents a place-specific\nchange classifier by its training examples mined from an external knowledge\nbase (EKB). The proposed algorithm exhibits several advantages. First, we are\nrequired to memorize only training examples (rather than the classifier\nitself), which can be further compressed in the form of bag-of-words (BoW).\nSecondly, we can incorporate the most recent map into the classifiers by\nstraightforwardly adding or deleting a few training examples that correspond to\nthese classifiers. Thirdly, we can share the BoW vocabulary with other related\ntask scenarios (e.g., BoW-based self-localization), wherein the vocabulary is\ngenerally designed as a rich, continuously growing, and domain-adaptive\nknowledge base. In our contribution, the proposed algorithm is applied and\nevaluated on a practical long-term cross-season change detection system that\nconsists of a large number of place-specific object-level change classifiers. \n\n"}
{"id": "1709.05436", "contents": "Title: Scene-centric Joint Parsing of Cross-view Videos Abstract: Cross-view video understanding is an important yet under-explored area in\ncomputer vision. In this paper, we introduce a joint parsing framework that\nintegrates view-centric proposals into scene-centric parse graphs that\nrepresent a coherent scene-centric understanding of cross-view scenes. Our key\nobservations are that overlapping fields of views embed rich appearance and\ngeometry correlations and that knowledge fragments corresponding to individual\nvision tasks are governed by consistency constraints available in commonsense\nknowledge. The proposed joint parsing framework represents such correlations\nand constraints explicitly and generates semantic scene-centric parse graphs.\nQuantitative experiments show that scene-centric predictions in the parse graph\noutperform view-centric predictions. \n\n"}
{"id": "1709.05860", "contents": "Title: Microscopy Cell Segmentation via Adversarial Neural Networks Abstract: We present a novel method for cell segmentation in microscopy images which is\ninspired by the Generative Adversarial Neural Network (GAN) approach. Our\nframework is built on a pair of two competitive artificial neural networks,\nwith a unique architecture, termed Rib Cage, which are trained simultaneously\nand together define a min-max game resulting in an accurate segmentation of a\ngiven image. Our approach has two main strengths, similar to the GAN, the\nmethod does not require a formulation of a loss function for the optimization\nprocess. This allows training on a limited amount of annotated data in a weakly\nsupervised manner. Promising segmentation results on real fluorescent\nmicroscopy data are presented. The code is freely available at:\nhttps://github.com/arbellea/DeepCellSeg.git \n\n"}
{"id": "1709.05870", "contents": "Title: ZhuSuan: A Library for Bayesian Deep Learning Abstract: In this paper we introduce ZhuSuan, a python probabilistic programming\nlibrary for Bayesian deep learning, which conjoins the complimentary advantages\nof Bayesian methods and deep learning. ZhuSuan is built upon Tensorflow. Unlike\nexisting deep learning libraries, which are mainly designed for deterministic\nneural networks and supervised tasks, ZhuSuan is featured for its deep root\ninto Bayesian inference, thus supporting various kinds of probabilistic models,\nincluding both the traditional hierarchical Bayesian models and recent deep\ngenerative models. We use running examples to illustrate the probabilistic\nprogramming on ZhuSuan, including Bayesian logistic regression, variational\nauto-encoders, deep sigmoid belief networks and Bayesian recurrent neural\nnetworks. \n\n"}
{"id": "1709.05903", "contents": "Title: E$^2$BoWs: An End-to-End Bag-of-Words Model via Deep Convolutional\n  Neural Network Abstract: Traditional Bag-of-visual Words (BoWs) model is commonly generated with many\nsteps including local feature extraction, codebook generation, and feature\nquantization, etc. Those steps are relatively independent with each other and\nare hard to be jointly optimized. Moreover, the dependency on hand-crafted\nlocal feature makes BoWs model not effective in conveying high-level semantics.\nThese issues largely hinder the performance of BoWs model in large-scale image\napplications. To conquer these issues, we propose an End-to-End BoWs\n(E$^2$BoWs) model based on Deep Convolutional Neural Network (DCNN). Our model\ntakes an image as input, then identifies and separates the semantic objects in\nit, and finally outputs the visual words with high semantic discriminative\npower. Specifically, our model firstly generates Semantic Feature Maps (SFMs)\ncorresponding to different object categories through convolutional layers, then\nintroduces Bag-of-Words Layers (BoWL) to generate visual words for each\nindividual feature map. We also introduce a novel learning algorithm to\nreinforce the sparsity of the generated E$^2$BoWs model, which further ensures\nthe time and memory efficiency. We evaluate the proposed E$^2$BoWs model on\nseveral image search datasets including CIFAR-10, CIFAR-100, MIRFLICKR-25K and\nNUS-WIDE. Experimental results show that our method achieves promising accuracy\nand efficiency compared with recent deep learning based retrieval works. \n\n"}
{"id": "1709.05943", "contents": "Title: Fast YOLO: A Fast You Only Look Once System for Real-time Embedded\n  Object Detection in Video Abstract: Object detection is considered one of the most challenging problems in this\nfield of computer vision, as it involves the combination of object\nclassification and object localization within a scene. Recently, deep neural\nnetworks (DNNs) have been demonstrated to achieve superior object detection\nperformance compared to other approaches, with YOLOv2 (an improved You Only\nLook Once model) being one of the state-of-the-art in DNN-based object\ndetection methods in terms of both speed and accuracy. Although YOLOv2 can\nachieve real-time performance on a powerful GPU, it still remains very\nchallenging for leveraging this approach for real-time object detection in\nvideo on embedded computing devices with limited computational power and\nlimited memory. In this paper, we propose a new framework called Fast YOLO, a\nfast You Only Look Once framework which accelerates YOLOv2 to be able to\nperform object detection in video on embedded devices in a real-time manner.\nFirst, we leverage the evolutionary deep intelligence framework to evolve the\nYOLOv2 network architecture and produce an optimized architecture (referred to\nas O-YOLOv2 here) that has 2.8X fewer parameters with just a ~2% IOU drop. To\nfurther reduce power consumption on embedded devices while maintaining\nperformance, a motion-adaptive inference method is introduced into the proposed\nFast YOLO framework to reduce the frequency of deep inference with O-YOLOv2\nbased on temporal motion characteristics. Experimental results show that the\nproposed Fast YOLO framework can reduce the number of deep inferences by an\naverage of 38.13%, and an average speedup of ~3.3X for objection detection in\nvideo compared to the original YOLOv2, leading Fast YOLO to run an average of\n~18FPS on a Nvidia Jetson TX1 embedded system. \n\n"}
{"id": "1709.06019", "contents": "Title: LS-VO: Learning Dense Optical Subspace for Robust Visual Odometry\n  Estimation Abstract: This work proposes a novel deep network architecture to solve the camera\nEgo-Motion estimation problem. A motion estimation network generally learns\nfeatures similar to Optical Flow (OF) fields starting from sequences of images.\nThis OF can be described by a lower dimensional latent space. Previous research\nhas shown how to find linear approximations of this space. We propose to use an\nAuto-Encoder network to find a non-linear representation of the OF manifold. In\naddition, we propose to learn the latent space jointly with the estimation\ntask, so that the learned OF features become a more robust description of the\nOF input. We call this novel architecture LS-VO.\n  The experiments show that LS-VO achieves a considerable increase in\nperformances in respect to baselines, while the number of parameters of the\nestimation network only slightly increases. \n\n"}
{"id": "1709.06257", "contents": "Title: Deep-Learnt Classification of Light Curves Abstract: Astronomy light curves are sparse, gappy, and heteroscedastic. As a result\nstandard time series methods regularly used for financial and similar datasets\nare of little help and astronomers are usually left to their own instruments\nand techniques to classify light curves. A common approach is to derive\nstatistical features from the time series and to use machine learning methods,\ngenerally supervised, to separate objects into a few of the standard classes.\nIn this work, we transform the time series to two-dimensional light curve\nrepresentations in order to classify them using modern deep learning\ntechniques. In particular, we show that convolutional neural networks based\nclassifiers work well for broad characterization and classification. We use\nlabeled datasets of periodic variables from CRTS survey and show how this opens\ndoors for a quick classification of diverse classes with several possible\nexciting extensions. \n\n"}
{"id": "1709.07330", "contents": "Title: H-DenseUNet: Hybrid Densely Connected UNet for Liver and Tumor\n  Segmentation from CT Volumes Abstract: Liver cancer is one of the leading causes of cancer death. To assist doctors\nin hepatocellular carcinoma diagnosis and treatment planning, an accurate and\nautomatic liver and tumor segmentation method is highly demanded in the\nclinical practice. Recently, fully convolutional neural networks (FCNs),\nincluding 2D and 3D FCNs, serve as the back-bone in many volumetric image\nsegmentation. However, 2D convolutions can not fully leverage the spatial\ninformation along the third dimension while 3D convolutions suffer from high\ncomputational cost and GPU memory consumption. To address these issues, we\npropose a novel hybrid densely connected UNet (H-DenseUNet), which consists of\na 2D DenseUNet for efficiently extracting intra-slice features and a 3D\ncounterpart for hierarchically aggregating volumetric contexts under the spirit\nof the auto-context algorithm for liver and tumor segmentation. We formulate\nthe learning process of H-DenseUNet in an end-to-end manner, where the\nintra-slice representations and inter-slice features can be jointly optimized\nthrough a hybrid feature fusion (HFF) layer. We extensively evaluated our\nmethod on the dataset of MICCAI 2017 Liver Tumor Segmentation (LiTS) Challenge\nand 3DIRCADb Dataset. Our method outperformed other state-of-the-arts on the\nsegmentation results of tumors and achieved very competitive performance for\nliver segmentation even with a single model. \n\n"}
{"id": "1709.07368", "contents": "Title: Multi-label Pixelwise Classification for Reconstruction of Large-scale\n  Urban Areas Abstract: Object classification is one of the many holy grails in computer vision and\nas such has resulted in a very large number of algorithms being proposed\nalready. Specifically in recent years there has been considerable progress in\nthis area primarily due to the increased efficiency and accessibility of deep\nlearning techniques. In fact, for single-label object classification [i.e. only\none object present in the image] the state-of-the-art techniques employ deep\nneural networks and are reporting very close to human-like performance. There\nare specialized applications in which single-label object-level classification\nwill not suffice; for example in cases where the image contains multiple\nintertwined objects of different labels.\n  In this paper, we address the complex problem of multi-label pixelwise\nclassification. We present our distinct solution based on a convolutional\nneural network (CNN) for performing multi-label pixelwise classification and\nits application to large-scale urban reconstruction. A supervised learning\napproach is followed for training a 13-layer CNN using both LiDAR and satellite\nimages. An empirical study has been conducted to determine the hyperparameters\nwhich result in the optimal performance of the CNN. Scale invariance is\nintroduced by training the network on five different scales of the input and\nlabeled data. This results in six pixelwise classifications for each different\nscale. An SVM is then trained to map the six pixelwise classifications into a\nsingle-label. Lastly, we refine boundary pixel labels using graph-cuts for\nmaximum a-posteriori (MAP) estimation with Markov Random Field (MRF) priors.\nThe resulting pixelwise classification is then used to accurately extract and\nreconstruct the buildings in large-scale urban areas. The proposed approach has\nbeen extensively tested and the results are reported. \n\n"}
{"id": "1709.08524", "contents": "Title: Generative learning for deep networks Abstract: Learning, taking into account full distribution of the data, referred to as\ngenerative, is not feasible with deep neural networks (DNNs) because they model\nonly the conditional distribution of the outputs given the inputs. Current\nsolutions are either based on joint probability models facing difficult\nestimation problems or learn two separate networks, mapping inputs to outputs\n(recognition) and vice-versa (generation). We propose an intermediate approach.\nFirst, we show that forward computation in DNNs with logistic sigmoid\nactivations corresponds to a simplified approximate Bayesian inference in a\ndirected probabilistic multi-layer model. This connection allows to interpret\nDNN as a probabilistic model of the output and all hidden units given the\ninput. Second, we propose that in order for the recognition and generation\nnetworks to be more consistent with the joint model of the data, weights of the\nrecognition and generator network should be related by transposition. We\ndemonstrate in a tentative experiment that such a coupled pair can be learned\ngeneratively, modelling the full distribution of the data, and has enough\ncapacity to perform well in both recognition and generation. \n\n"}
{"id": "1709.08855", "contents": "Title: Learning to Inpaint for Image Compression Abstract: We study the design of deep architectures for lossy image compression. We\npresent two architectural recipes in the context of multi-stage progressive\nencoders and empirically demonstrate their importance on compression\nperformance. Specifically, we show that: (a) predicting the original image data\nfrom residuals in a multi-stage progressive architecture facilitates learning\nand leads to improved performance at approximating the original content and (b)\nlearning to inpaint (from neighboring image pixels) before performing\ncompression reduces the amount of information that must be stored to achieve a\nhigh-quality approximation. Incorporating these design choices in a baseline\nprogressive encoder yields an average reduction of over $60\\%$ in file size\nwith similar quality compared to the original residual encoder. \n\n"}
{"id": "1709.09354", "contents": "Title: Generative Adversarial Networks with Inverse Transformation Unit Abstract: In this paper we introduce a new structure to Generative Adversarial Networks\nby adding an inverse transformation unit behind the generator. We present two\ntheorems to claim the convergence of the model, and two conjectures to nonideal\nsituations when the transformation is not bijection. A general survey on models\nwith different transformations was done on the MNIST dataset and the\nFashion-MNIST dataset, which shows the transformation does not necessarily need\nto be bijection. Also, with certain transformations that blurs an image, our\nmodel successfully learned to sharpen the images and recover blurred images,\nwhich was additionally verified by our measurement of sharpness. \n\n"}
{"id": "1709.09641", "contents": "Title: Neural Multi-Atlas Label Fusion: Application to Cardiac MR Images Abstract: Multi-atlas segmentation approach is one of the most widely-used image\nsegmentation techniques in biomedical applications. There are two major\nchallenges in this category of methods, i.e., atlas selection and label fusion.\nIn this paper, we propose a novel multi-atlas segmentation method that\nformulates multi-atlas segmentation in a deep learning framework for better\nsolving these challenges. The proposed method, dubbed deep fusion net (DFN), is\na deep architecture that integrates a feature extraction subnet and a non-local\npatch-based label fusion (NL-PLF) subnet in a single network. The network\nparameters are learned by end-to-end training for automatically learning deep\nfeatures that enable optimal performance in a NL-PLF framework. The learned\ndeep features are further utilized in defining a similarity measure for atlas\nselection. By evaluating on two public cardiac MR datasets of SATA-13 and LV-09\nfor left ventricle segmentation, our approach achieved 0.833 in averaged Dice\nmetric (ADM) on SATA-13 dataset and 0.95 in ADM for epicardium segmentation on\nLV-09 dataset, comparing favorably with the other automatic left ventricle\nsegmentation methods. We also tested our approach on Cardiac Atlas Project\n(CAP) testing set of MICCAI 2013 SATA Segmentation Challenge, and our method\nachieved 0.815 in ADM, ranking highest at the time of writing. \n\n"}
{"id": "1709.09828", "contents": "Title: Photorealistic Style Transfer with Screened Poisson Equation Abstract: Recent work has shown impressive success in transferring painterly style to\nimages. These approaches, however, fall short of photorealistic style transfer.\nEven when both the input and reference images are photographs, the output still\nexhibits distortions reminiscent of a painting. In this paper we propose an\napproach that takes as input a stylized image and makes it more photorealistic.\nIt relies on the Screened Poisson Equation, maintaining the fidelity of the\nstylized image while constraining the gradients to those of the original input\nimage. Our method is fast, simple, fully automatic and shows positive progress\nin making a stylized image photorealistic. Our results exhibit finer details\nand are less prone to artifacts than the state-of-the-art. \n\n"}
{"id": "1709.10190", "contents": "Title: Unified Deep Supervised Domain Adaptation and Generalization Abstract: This work provides a unified framework for addressing the problem of visual\nsupervised domain adaptation and generalization with deep models. The main idea\nis to exploit the Siamese architecture to learn an embedding subspace that is\ndiscriminative, and where mapped visual domains are semantically aligned and\nyet maximally separated. The supervised setting becomes attractive especially\nwhen only few target data samples need to be labeled. In this scenario,\nalignment and separation of semantic probability distributions is difficult\nbecause of the lack of data. We found that by reverting to point-wise\nsurrogates of distribution distances and similarities provides an effective\nsolution. In addition, the approach has a high speed of adaptation, which\nrequires an extremely low number of labeled target training samples, even one\nper category can be effective. The approach is extended to domain\ngeneralization. For both applications the experiments show very promising\nresults. \n\n"}
{"id": "1710.01115", "contents": "Title: Detection of Inferior Myocardial Infarction using Shallow Convolutional\n  Neural Networks Abstract: Myocardial Infarction is one of the leading causes of death worldwide. This\npaper presents a Convolutional Neural Network (CNN) architecture which takes\nraw Electrocardiography (ECG) signal from lead II, III and AVF and\ndifferentiates between inferior myocardial infarction (IMI) and healthy\nsignals. The performance of the model is evaluated on IMI and healthy signals\nobtained from Physikalisch-Technische Bundesanstalt (PTB) database. A\nsubject-oriented approach is taken to comprehend the generalization capability\nof the model and compared with the current state of the art. In a\nsubject-oriented approach, the network is tested on one patient and trained on\nrest of the patients. Our model achieved a superior metrics scores (accuracy=\n84.54%, sensitivity= 85.33% and specificity= 84.09%) when compared to the\nbenchmark. We also analyzed the discriminating strength of the features\nextracted by the convolutional layers by means of geometric separability index\nand euclidean distance and compared it with the benchmark model. \n\n"}
{"id": "1710.01168", "contents": "Title: Fast Fine-grained Image Classification via Weakly Supervised\n  Discriminative Localization Abstract: Fine-grained image classification is to recognize hundreds of subcategories\nin each basic-level category. Existing methods employ discriminative\nlocalization to find the key distinctions among subcategories. However, they\ngenerally have two limitations: (1) Discriminative localization relies on\nregion proposal methods to hypothesize the locations of discriminative regions,\nwhich are time-consuming. (2) The training of discriminative localization\ndepends on object or part annotations, which are heavily labor-consuming. It is\nhighly challenging to address the two key limitations simultaneously, and\nexisting methods only focus on one of them. Therefore, we propose a weakly\nsupervised discriminative localization approach (WSDL) for fast fine-grained\nimage classification to address the two limitations at the same time, and its\nmain advantages are: (1) n-pathway end-to-end discriminative localization\nnetwork is designed to improve classification speed, which simultaneously\nlocalizes multiple different discriminative regions for one image to boost\nclassification accuracy, and shares full-image convolutional features generated\nby region proposal network to accelerate the process of generating region\nproposals as well as reduce the computation of convolutional operation. (2)\nMulti-level attention guided localization learning is proposed to localize\ndiscriminative regions with different focuses automatically, without using\nobject and part annotations, avoiding the labor consumption. Different level\nattentions focus on different characteristics of the image, which are\ncomplementary and boost the classification accuracy. Both are jointly employed\nto simultaneously improve classification speed and eliminate dependence on\nobject and part annotations. Compared with state-of-the-art methods on 2\nwidely-used fine-grained image classification datasets, our WSDL approach\nachieves the best performance. \n\n"}
{"id": "1710.01255", "contents": "Title: Variational Grid Setting Network Abstract: We propose a new neural network architecture for automatic generation of\nmissing characters in a Chinese font set. We call the neural network\narchitecture the Variational Grid Setting Network which is based on the\nvariational autoencoder (VAE) with some tweaks. The neural network model is\nable to generate missing characters relatively large in size ($256 \\times 256$\npixels). Moreover, we show that one can use very few samples for training data\nset, and get a satisfied result. \n\n"}
{"id": "1710.01691", "contents": "Title: Context Embedding Networks Abstract: Low dimensional embeddings that capture the main variations of interest in\ncollections of data are important for many applications. One way to construct\nthese embeddings is to acquire estimates of similarity from the crowd. However,\nsimilarity is a multi-dimensional concept that varies from individual to\nindividual. Existing models for learning embeddings from the crowd typically\nmake simplifying assumptions such as all individuals estimate similarity using\nthe same criteria, the list of criteria is known in advance, or that the crowd\nworkers are not influenced by the data that they see. To overcome these\nlimitations we introduce Context Embedding Networks (CENs). In addition to\nlearning interpretable embeddings from images, CENs also model worker biases\nfor different attributes along with the visual context i.e. the visual\nattributes highlighted by a set of images. Experiments on two noisy crowd\nannotated datasets show that modeling both worker bias and visual context\nresults in more interpretable embeddings compared to existing approaches. \n\n"}
{"id": "1710.01749", "contents": "Title: Semantic 3D Reconstruction with Finite Element Bases Abstract: We propose a novel framework for the discretisation of multi-label problems\non arbitrary, continuous domains. Our work bridges the gap between general FEM\ndiscretisations, and labeling problems that arise in a variety of computer\nvision tasks, including for instance those derived from the generalised Potts\nmodel. Starting from the popular formulation of labeling as a convex relaxation\nby functional lifting, we show that FEM discretisation is valid for the most\ngeneral case, where the regulariser is anisotropic and non-metric. While our\nfindings are generic and applicable to different vision problems, we\ndemonstrate their practical implementation in the context of semantic 3D\nreconstruction, where such regularisers have proved particularly beneficial.\nThe proposed FEM approach leads to a smaller memory footprint as well as faster\ncomputation, and it constitutes a very simple way to enable variable, adaptive\nresolution within the same model. \n\n"}
{"id": "1710.02310", "contents": "Title: Detecting the Moment of Completion: Temporal Models for Localising\n  Action Completion Abstract: Action completion detection is the problem of modelling the action's\nprogression towards localising the moment of completion - when the action's\ngoal is confidently considered achieved. In this work, we assess the ability of\ntwo temporal models, namely Hidden Markov Models (HMM) and Long-Short Term\nMemory (LSTM), to localise completion for six object interactions: switch,\nplug, open, pull, pick and drink. We use a supervised approach, where\nannotations of pre-completion and post-completion frames are available per\naction, and fine-tuned CNN features are used to train temporal models. Tested\non the Action-Completion-2016 dataset, we detect completion within 10 frames of\nannotations for ~75% of completed action sequences using both temporal models.\nResults show that fine-tuned CNN features outperform hand-crafted features for\nlocalisation, and that observing incomplete instances is necessary when\nincomplete sequences are also present in the test set. \n\n"}
{"id": "1710.02316", "contents": "Title: A Multiscale Patch Based Convolutional Network for Brain Tumor\n  Segmentation Abstract: This article presents a multiscale patch based convolutional neural network\nfor the automatic segmentation of brain tumors in multi-modality 3D MR images.\nWe use multiscale deep supervision and inputs to train a convolutional network.\nWe evaluate the effectiveness of the proposed approach on the BRATS 2017\nsegmentation challenge where we obtained dice scores of 0.755, 0.900, 0.782 and\n95% Hausdorff distance of 3.63mm, 4.10mm, and 6.81mm for enhanced tumor core,\nwhole tumor and tumor core respectively. \n\n"}
{"id": "1710.03344", "contents": "Title: Iterative PET Image Reconstruction Using Convolutional Neural Network\n  Representation Abstract: PET image reconstruction is challenging due to the ill-poseness of the\ninverse problem and limited number of detected photons. Recently deep neural\nnetworks have been widely and successfully used in computer vision tasks and\nattracted growing interests in medical imaging. In this work, we trained a deep\nresidual convolutional neural network to improve PET image quality by using the\nexisting inter-patient information. An innovative feature of the proposed\nmethod is that we embed the neural network in the iterative reconstruction\nframework for image representation, rather than using it as a post-processing\ntool. We formulate the objective function as a constraint optimization problem\nand solve it using the alternating direction method of multipliers (ADMM)\nalgorithm. Both simulation data and hybrid real data are used to evaluate the\nproposed method. Quantification results show that our proposed iterative neural\nnetwork method can outperform the neural network denoising and conventional\npenalized maximum likelihood methods. \n\n"}
{"id": "1710.04026", "contents": "Title: FFDNet: Toward a Fast and Flexible Solution for CNN based Image\n  Denoising Abstract: Due to the fast inference and good performance, discriminative learning\nmethods have been widely studied in image denoising. However, these methods\nmostly learn a specific model for each noise level, and require multiple models\nfor denoising images with different noise levels. They also lack flexibility to\ndeal with spatially variant noise, limiting their applications in practical\ndenoising. To address these issues, we present a fast and flexible denoising\nconvolutional neural network, namely FFDNet, with a tunable noise level map as\nthe input. The proposed FFDNet works on downsampled sub-images, achieving a\ngood trade-off between inference speed and denoising performance. In contrast\nto the existing discriminative denoisers, FFDNet enjoys several desirable\nproperties, including (i) the ability to handle a wide range of noise levels\n(i.e., [0, 75]) effectively with a single network, (ii) the ability to remove\nspatially variant noise by specifying a non-uniform noise level map, and (iii)\nfaster speed than benchmark BM3D even on CPU without sacrificing denoising\nperformance. Extensive experiments on synthetic and real noisy images are\nconducted to evaluate FFDNet in comparison with state-of-the-art denoisers. The\nresults show that FFDNet is effective and efficient, making it highly\nattractive for practical denoising applications. \n\n"}
{"id": "1710.04842", "contents": "Title: Dynamic texture recognition using time-causal and time-recursive\n  spatio-temporal receptive fields Abstract: This work presents a first evaluation of using spatio-temporal receptive\nfields from a recently proposed time-causal spatio-temporal scale-space\nframework as primitives for video analysis. We propose a new family of video\ndescriptors based on regional statistics of spatio-temporal receptive field\nresponses and evaluate this approach on the problem of dynamic texture\nrecognition. Our approach generalises a previously used method, based on joint\nhistograms of receptive field responses, from the spatial to the\nspatio-temporal domain and from object recognition to dynamic texture\nrecognition. The time-recursive formulation enables computationally efficient\ntime-causal recognition. The experimental evaluation demonstrates competitive\nperformance compared to state-of-the-art. Especially, it is shown that binary\nversions of our dynamic texture descriptors achieve improved performance\ncompared to a large range of similar methods using different primitives either\nhandcrafted or learned from data. Further, our qualitative and quantitative\ninvestigation into parameter choices and the use of different sets of receptive\nfields highlights the robustness and flexibility of our approach. Together,\nthese results support the descriptive power of this family of time-causal\nspatio-temporal receptive fields, validate our approach for dynamic texture\nrecognition and point towards the possibility of designing a range of video\nanalysis methods based on these new time-causal spatio-temporal primitives. \n\n"}
{"id": "1710.05285", "contents": "Title: CNNComparator: Comparative Analytics of Convolutional Neural Networks Abstract: Convolutional neural networks (CNNs) are widely used in many image\nrecognition tasks due to their extraordinary performance. However, training a\ngood CNN model can still be a challenging task. In a training process, a CNN\nmodel typically learns a large number of parameters over time, which usually\nresults in different performance. Often, it is difficult to explore the\nrelationships between the learned parameters and the model performance due to a\nlarge number of parameters and different random initializations. In this paper,\nwe present a visual analytics approach to compare two different snapshots of a\ntrained CNN model taken after different numbers of epochs, so as to provide\nsome insight into the design or the training of a better CNN model. Our system\ncompares snapshots by exploring the differences in operation parameters and the\ncorresponding blob data at different levels. A case study has been conducted to\ndemonstrate the effectiveness of our system. \n\n"}
{"id": "1710.05705", "contents": "Title: Blind Image Fusion for Hyperspectral Imaging with the Directional Total\n  Variation Abstract: Hyperspectral imaging is a cutting-edge type of remote sensing used for\nmapping vegetation properties, rock minerals and other materials. A major\ndrawback of hyperspectral imaging devices is their intrinsic low spatial\nresolution. In this paper, we propose a method for increasing the spatial\nresolution of a hyperspectral image by fusing it with an image of higher\nspatial resolution that was obtained with a different imaging modality. This is\naccomplished by solving a variational problem in which the regularization\nfunctional is the directional total variation. To accommodate for possible\nmis-registrations between the two images, we consider a non-convex blind\nsuper-resolution problem where both a fused image and the corresponding\nconvolution kernel are estimated. Using this approach, our model can realign\nthe given images if needed. Our experimental results indicate that the\nnon-convexity is negligible in practice and that reliable solutions can be\ncomputed using a variety of different optimization algorithms. Numerical\nresults on real remote sensing data from plant sciences and urban monitoring\nshow the potential of the proposed method and suggests that it is robust with\nrespect to the regularization parameters, mis-registration and the shape of the\nkernel. \n\n"}
{"id": "1710.05758", "contents": "Title: TensorQuant - A Simulation Toolbox for Deep Neural Network Quantization Abstract: Recent research implies that training and inference of deep neural networks\n(DNN) can be computed with low precision numerical representations of the\ntraining/test data, weights and gradients without a general loss in accuracy.\nThe benefit of such compact representations is twofold: they allow a\nsignificant reduction of the communication bottleneck in distributed DNN\ntraining and faster neural network implementations on hardware accelerators\nlike FPGAs. Several quantization methods have been proposed to map the original\n32-bit floating point problem to low-bit representations. While most related\npublications validate the proposed approach on a single DNN topology, it\nappears to be evident, that the optimal choice of the quantization method and\nnumber of coding bits is topology dependent. To this end, there is no general\ntheory available, which would allow users to derive the optimal quantization\nduring the design of a DNN topology. In this paper, we present a quantization\ntool box for the TensorFlow framework. TensorQuant allows a transparent\nquantization simulation of existing DNN topologies during training and\ninference. TensorQuant supports generic quantization methods and allows\nexperimental evaluation of the impact of the quantization on single layers as\nwell as on the full topology. In a first series of experiments with\nTensorQuant, we show an analysis of fix-point quantizations of popular CNN\ntopologies. \n\n"}
{"id": "1710.07477", "contents": "Title: Anticipating Daily Intention using On-Wrist Motion Triggered Sensing Abstract: Anticipating human intention by observing one's actions has many\napplications. For instance, picking up a cellphone, then a charger (actions)\nimplies that one wants to charge the cellphone (intention). By anticipating the\nintention, an intelligent system can guide the user to the closest power\noutlet. We propose an on-wrist motion triggered sensing system for anticipating\ndaily intentions, where the on-wrist sensors help us to persistently observe\none's actions. The core of the system is a novel Recurrent Neural Network (RNN)\nand Policy Network (PN), where the RNN encodes visual and motion observation to\nanticipate intention, and the PN parsimoniously triggers the process of visual\nobservation to reduce computation requirement. We jointly trained the whole\nnetwork using policy gradient and cross-entropy loss. To evaluate, we collect\nthe first daily \"intention\" dataset consisting of 2379 videos with 34\nintentions and 164 unique action sequences. Our method achieves 92.68%, 90.85%,\n97.56% accuracy on three users while processing only 29% of the visual\nobservation on average. \n\n"}
{"id": "1710.07965", "contents": "Title: Backtracking Regression Forests for Accurate Camera Relocalization Abstract: Camera relocalization plays a vital role in many robotics and computer vision\ntasks, such as global localization, recovery from tracking failure, and loop\nclosure detection. Recent random forests based methods directly predict 3D\nworld locations for 2D image locations to guide the camera pose optimization.\nDuring training, each tree greedily splits the samples to minimize the spatial\nvariance. However, these greedy splits often produce uneven sub-trees in\ntraining or incorrect 2D-3D correspondences in testing. To address these\nproblems, we propose a sample-balanced objective to encourage equal numbers of\nsamples in the left and right sub-trees, and a novel backtracking scheme to\nremedy the incorrect 2D-3D correspondence predictions. Furthermore, we extend\nthe regression forests based methods to use local features in both training and\ntesting stages for outdoor RGB-only applications. Experimental results on\npublicly available indoor and outdoor datasets demonstrate the efficacy of our\napproach, which shows superior or on-par accuracy with several state-of-the-art\nmethods. \n\n"}
{"id": "1710.08177", "contents": "Title: Progressive Learning for Systematic Design of Large Neural Networks Abstract: We develop an algorithm for systematic design of a large artificial neural\nnetwork using a progression property. We find that some non-linear functions,\nsuch as the rectifier linear unit and its derivatives, hold the property. The\nsystematic design addresses the choice of network size and regularization of\nparameters. The number of nodes and layers in network increases in progression\nwith the objective of consistently reducing an appropriate cost. Each layer is\noptimized at a time, where appropriate parameters are learned using convex\noptimization. Regularization parameters for convex optimization do not need a\nsignificant manual effort for tuning. We also use random instances for some\nweight matrices, and that helps to reduce the number of parameters we learn.\nThe developed network is expected to show good generalization power due to\nappropriate regularization and use of random weights in the layers. This\nexpectation is verified by extensive experiments for classification and\nregression problems, using standard databases. \n\n"}
{"id": "1710.08585", "contents": "Title: Max-Margin Invariant Features from Transformed Unlabeled Data Abstract: The study of representations invariant to common transformations of the data\nis important to learning. Most techniques have focused on local approximate\ninvariance implemented within expensive optimization frameworks lacking\nexplicit theoretical guarantees. In this paper, we study kernels that are\ninvariant to a unitary group while having theoretical guarantees in addressing\nthe important practical issue of unavailability of transformed versions of\nlabelled data. A problem we call the Unlabeled Transformation Problem which is\na special form of semi-supervised learning and one-shot learning. We present a\ntheoretically motivated alternate approach to the invariant kernel SVM based on\nwhich we propose Max-Margin Invariant Features (MMIF) to solve this problem. As\nan illustration, we design an framework for face recognition and demonstrate\nthe efficacy of our approach on a large scale semi-synthetic dataset with\n153,000 images and a new challenging protocol on Labelled Faces in the Wild\n(LFW) while out-performing strong baselines. \n\n"}
{"id": "1710.09282", "contents": "Title: A Survey of Model Compression and Acceleration for Deep Neural Networks Abstract: Deep neural networks (DNNs) have recently achieved great success in many\nvisual recognition tasks. However, existing deep neural network models are\ncomputationally expensive and memory intensive, hindering their deployment in\ndevices with low memory resources or in applications with strict latency\nrequirements. Therefore, a natural thought is to perform model compression and\nacceleration in deep networks without significantly decreasing the model\nperformance. During the past five years, tremendous progress has been made in\nthis area. In this paper, we review the recent techniques for compacting and\naccelerating DNN models. In general, these techniques are divided into four\ncategories: parameter pruning and quantization, low-rank factorization,\ntransferred/compact convolutional filters, and knowledge distillation. Methods\nof parameter pruning and quantization are described first, after that the other\ntechniques are introduced. For each category, we also provide insightful\nanalysis about the performance, related applications, advantages, and\ndrawbacks. Then we go through some very recent successful methods, for example,\ndynamic capacity networks and stochastic depths networks. After that, we survey\nthe evaluation matrices, the main datasets used for evaluating the model\nperformance, and recent benchmark efforts. Finally, we conclude this paper,\ndiscuss remaining the challenges and possible directions for future work. \n\n"}
{"id": "1710.09338", "contents": "Title: Real-Time Automatic Fetal Brain Extraction in Fetal MRI by Deep Learning Abstract: Brain segmentation is a fundamental first step in neuroimage analysis. In the\ncase of fetal MRI, it is particularly challenging and important due to the\narbitrary orientation of the fetus, organs that surround the fetal head, and\nintermittent fetal motion. Several promising methods have been proposed but are\nlimited in their performance in challenging cases and in real-time\nsegmentation. We aimed to develop a fully automatic segmentation method that\nindependently segments sections of the fetal brain in 2D fetal MRI slices in\nreal-time. To this end, we developed and evaluated a deep fully convolutional\nneural network based on 2D U-net and autocontext, and compared it to two\nalternative fast methods based on 1) a voxelwise fully convolutional network\nand 2) a method based on SIFT features, random forest and conditional random\nfield. We trained the networks with manual brain masks on 250 stacks of\ntraining images, and tested on 17 stacks of normal fetal brain images as well\nas 18 stacks of extremely challenging cases based on extreme motion, noise, and\nseverely abnormal brain shape. Experimental results show that our U-net\napproach outperformed the other methods and achieved average Dice metrics of\n96.52% and 78.83% in the normal and challenging test sets, respectively. With\nan unprecedented performance and a test run time of about 1 second, our network\ncan be used to segment the fetal brain in real-time while fetal MRI slices are\nbeing acquired. This can enable real-time motion tracking, motion detection,\nand 3D reconstruction of fetal brain MRI. \n\n"}
{"id": "1710.09829", "contents": "Title: Dynamic Routing Between Capsules Abstract: A capsule is a group of neurons whose activity vector represents the\ninstantiation parameters of a specific type of entity such as an object or an\nobject part. We use the length of the activity vector to represent the\nprobability that the entity exists and its orientation to represent the\ninstantiation parameters. Active capsules at one level make predictions, via\ntransformation matrices, for the instantiation parameters of higher-level\ncapsules. When multiple predictions agree, a higher level capsule becomes\nactive. We show that a discrimininatively trained, multi-layer capsule system\nachieves state-of-the-art performance on MNIST and is considerably better than\na convolutional net at recognizing highly overlapping digits. To achieve these\nresults we use an iterative routing-by-agreement mechanism: A lower-level\ncapsule prefers to send its output to higher level capsules whose activity\nvectors have a big scalar product with the prediction coming from the\nlower-level capsule. \n\n"}
{"id": "1710.10553", "contents": "Title: A Novel Approach to Artistic Textual Visualization via GAN Abstract: While the visualization of statistical data tends to a mature technology, the\nvisualization of textual data is still in its infancy, especially for the\nartistic text. Due to the fact that visualization of artistic text is valuable\nand attractive in both art and information science, we attempt to realize this\ntentative idea in this article. We propose the Generative Adversarial Network\nbased Artistic Textual Visualization (GAN-ATV) which can create paintings after\nanalyzing the semantic content of existing poems. Our GAN-ATV consists of two\nmain sections: natural language analysis section and visual information\nsynthesis section. In natural language analysis section, we use Bag-of-Word\n(BoW) feature descriptors and a two-layer network to mine and analyze the\nhigh-level semantic information from poems. In visual information synthesis\nsection, we design a cross-modal semantic understanding module and integrate it\nwith Generative Adversarial Network (GAN) to create paintings, whose content\nare corresponding to the original poems. Moreover, in order to train our\nGAN-ATV and verify its performance, we establish a cross-modal artistic dataset\nnamed \"Cross-Art\". In the Cross-Art dataset, there are six topics and each\ntopic has their corresponding paintings and poems. The experimental results on\nCross-Art dataset are shown in this article. \n\n"}
{"id": "1710.10564", "contents": "Title: A Bayesian Data Augmentation Approach for Learning Deep Models Abstract: Data augmentation is an essential part of the training process applied to\ndeep learning models. The motivation is that a robust training process for deep\nlearning models depends on large annotated datasets, which are expensive to be\nacquired, stored and processed. Therefore a reasonable alternative is to be\nable to automatically generate new annotated training samples using a process\nknown as data augmentation. The dominant data augmentation approach in the\nfield assumes that new training samples can be obtained via random geometric or\nappearance transformations applied to annotated training samples, but this is a\nstrong assumption because it is unclear if this is a reliable generative model\nfor producing new training samples. In this paper, we provide a novel Bayesian\nformulation to data augmentation, where new annotated training points are\ntreated as missing variables and generated based on the distribution learned\nfrom the training set. For learning, we introduce a theoretically sound\nalgorithm --- generalised Monte Carlo expectation maximisation, and demonstrate\none possible implementation via an extension of the Generative Adversarial\nNetwork (GAN). Classification results on MNIST, CIFAR-10 and CIFAR-100 show the\nbetter performance of our proposed method compared to the current dominant data\naugmentation approach mentioned above --- the results also show that our\napproach produces better classification results than similar GAN models. \n\n"}
{"id": "1710.10565", "contents": "Title: Synthetic Iris Presentation Attack using iDCGAN Abstract: Reliability and accuracy of iris biometric modality has prompted its\nlarge-scale deployment for critical applications such as border control and\nnational ID projects. The extensive growth of iris recognition systems has\nraised apprehensions about susceptibility of these systems to various attacks.\nIn the past, researchers have examined the impact of various iris presentation\nattacks such as textured contact lenses and print attacks. In this research, we\npresent a novel presentation attack using deep learning based synthetic iris\ngeneration. Utilizing the generative capability of deep convolutional\ngenerative adversarial networks and iris quality metrics, we propose a new\nframework, named as iDCGAN (iris deep convolutional generative adversarial\nnetwork) for generating realistic appearing synthetic iris images. We\ndemonstrate the effect of these synthetically generated iris images as\npresentation attack on iris recognition by using a commercial system. The\nstate-of-the-art presentation attack detection framework, DESIST is utilized to\nanalyze if it can discriminate these synthetically generated iris images from\nreal images. The experimental results illustrate that mitigating the proposed\nsynthetic presentation attack is of paramount importance. \n\n"}
{"id": "1710.10741", "contents": "Title: Evolving Deep Convolutional Neural Networks for Image Classification Abstract: Evolutionary computation methods have been successfully applied to neural\nnetworks since two decades ago, while those methods cannot scale well to the\nmodern deep neural networks due to the complicated architectures and large\nquantities of connection weights. In this paper, we propose a new method using\ngenetic algorithms for evolving the architectures and connection weight\ninitialization values of a deep convolutional neural network to address image\nclassification problems. In the proposed algorithm, an efficient\nvariable-length gene encoding strategy is designed to represent the different\nbuilding blocks and the unpredictable optimal depth in convolutional neural\nnetworks. In addition, a new representation scheme is developed for effectively\ninitializing connection weights of deep convolutional neural networks, which is\nexpected to avoid networks getting stuck into local minima which is typically a\nmajor issue in the backward gradient-based optimization. Furthermore, a novel\nfitness evaluation method is proposed to speed up the heuristic search with\nsubstantially less computational resource. The proposed algorithm is examined\nand compared with 22 existing algorithms on nine widely used image\nclassification tasks, including the state-of-the-art methods. The experimental\nresults demonstrate the remarkable superiority of the proposed algorithm over\nthe state-of-the-art algorithms in terms of classification error rate and the\nnumber of parameters (weights). \n\n"}
{"id": "1710.10779", "contents": "Title: Generative Adversarial Source Separation Abstract: Generative source separation methods such as non-negative matrix\nfactorization (NMF) or auto-encoders, rely on the assumption of an output\nprobability density. Generative Adversarial Networks (GANs) can learn data\ndistributions without needing a parametric assumption on the output density. We\nshow on a speech source separation experiment that, a multi-layer perceptron\ntrained with a Wasserstein-GAN formulation outperforms NMF, auto-encoders\ntrained with maximum likelihood, and variational auto-encoders in terms of\nsource to distortion ratio. \n\n"}
{"id": "1711.00436", "contents": "Title: Hierarchical Representations for Efficient Architecture Search Abstract: We explore efficient neural architecture search methods and show that a\nsimple yet powerful evolutionary algorithm can discover new architectures with\nexcellent performance. Our approach combines a novel hierarchical genetic\nrepresentation scheme that imitates the modularized design pattern commonly\nadopted by human experts, and an expressive search space that supports complex\ntopologies. Our algorithm efficiently discovers architectures that outperform a\nlarge number of manually designed models for image classification, obtaining\ntop-1 error of 3.6% on CIFAR-10 and 20.3% when transferred to ImageNet, which\nis competitive with the best existing neural architecture search approaches. We\nalso present results using random search, achieving 0.3% less top-1 accuracy on\nCIFAR-10 and 0.1% less on ImageNet whilst reducing the search time from 36\nhours down to 1 hour. \n\n"}
{"id": "1711.00549", "contents": "Title: Just ASK: Building an Architecture for Extensible Self-Service Spoken\n  Language Understanding Abstract: This paper presents the design of the machine learning architecture that\nunderlies the Alexa Skills Kit (ASK) a large scale Spoken Language\nUnderstanding (SLU) Software Development Kit (SDK) that enables developers to\nextend the capabilities of Amazon's virtual assistant, Alexa. At Amazon, the\ninfrastructure powers over 25,000 skills deployed through the ASK, as well as\nAWS's Amazon Lex SLU Service. The ASK emphasizes flexibility, predictability\nand a rapid iteration cycle for third party developers. It imposes inductive\nbiases that allow it to learn robust SLU models from extremely small and sparse\ndatasets and, in doing so, removes significant barriers to entry for software\ndevelopers and dialogue systems researchers. \n\n"}
{"id": "1711.00970", "contents": "Title: A Classification-Based Study of Covariate Shift in GAN Distributions Abstract: A basic, and still largely unanswered, question in the context of Generative\nAdversarial Networks (GANs) is whether they are truly able to capture all the\nfundamental characteristics of the distributions they are trained on. In\nparticular, evaluating the diversity of GAN distributions is challenging and\nexisting methods provide only a partial understanding of this issue. In this\npaper, we develop quantitative and scalable tools for assessing the diversity\nof GAN distributions. Specifically, we take a classification-based perspective\nand view loss of diversity as a form of covariate shift introduced by GANs. We\nexamine two specific forms of such shift: mode collapse and boundary\ndistortion. In contrast to prior work, our methods need only minimal human\nsupervision and can be readily applied to state-of-the-art GANs on large,\ncanonical datasets. Examining popular GANs using our tools indicates that these\nGANs have significant problems in reproducing the more distributional\nproperties of their training dataset. \n\n"}
{"id": "1711.01575", "contents": "Title: Adversarial Dropout Regularization Abstract: We present a method for transferring neural representations from label-rich\nsource domains to unlabeled target domains. Recent adversarial methods proposed\nfor this task learn to align features across domains by fooling a special\ndomain critic network. However, a drawback of this approach is that the critic\nsimply labels the generated features as in-domain or not, without considering\nthe boundaries between classes. This can lead to ambiguous features being\ngenerated near class boundaries, reducing target classification accuracy. We\npropose a novel approach, Adversarial Dropout Regularization (ADR), to\nencourage the generator to output more discriminative features for the target\ndomain. Our key idea is to replace the critic with one that detects\nnon-discriminative features, using dropout on the classifier network. The\ngenerator then learns to avoid these areas of the feature space and thus\ncreates better features. We apply our ADR approach to the problem of\nunsupervised domain adaptation for image classification and semantic\nsegmentation tasks, and demonstrate significant improvement over the state of\nthe art. We also show that our approach can be used to train Generative\nAdversarial Networks for semi-supervised learning. \n\n"}
{"id": "1711.01656", "contents": "Title: Spatial Pyramid Context-Aware Moving Object Detection and Tracking for\n  Full Motion Video and Wide Aerial Motion Imagery Abstract: A robust and fast automatic moving object detection and tracking system is\nessential to characterize target object and extract spatial and temporal\ninformation for different functionalities including video surveillance systems,\nurban traffic monitoring and navigation, robotic. In this dissertation, I\npresent a collaborative Spatial Pyramid Context-aware moving object detection\nand Tracking system. The proposed visual tracker is composed of one master\ntracker that usually relies on visual object features and two auxiliary\ntrackers based on object temporal motion information that will be called\ndynamically to assist master tracker. SPCT utilizes image spatial context at\ndifferent level to make the video tracking system resistant to occlusion,\nbackground noise and improve target localization accuracy and robustness. We\nchose a pre-selected seven-channel complementary features including RGB color,\nintensity and spatial pyramid of HoG to encode object color, shape and spatial\nlayout information. We exploit integral histogram as building block to meet the\ndemands of real-time performance. A novel fast algorithm is presented to\naccurately evaluate spatially weighted local histograms in constant time\ncomplexity using an extension of the integral histogram method. Different\ntechniques are explored to efficiently compute integral histogram on GPU\narchitecture and applied for fast spatio-temporal median computations and 3D\nface reconstruction texturing. We proposed a multi-component framework based on\nsemantic fusion of motion information with projected building footprint map to\nsignificantly reduce the false alarm rate in urban scenes with many tall\nstructures. The experiments on extensive VOTC2016 benchmark dataset and aerial\nvideo confirm that combining complementary tracking cues in an intelligent\nfusion framework enables persistent tracking for Full Motion Video and Wide\nAerial Motion Imagery. \n\n"}
{"id": "1711.02254", "contents": "Title: Doppler-Radar Based Hand Gesture Recognition System Using Convolutional\n  Neural Networks Abstract: Hand gesture recognition has long been a hot topic in human computer\ninteraction. Traditional camera-based hand gesture recognition systems cannot\nwork properly under dark circumstances. In this paper, a Doppler Radar based\nhand gesture recognition system using convolutional neural networks is\nproposed. A cost-effective Doppler radar sensor with dual receiving channels at\n5.8GHz is used to acquire a big database of four standard gestures. The\nreceived hand gesture signals are then processed with time-frequency analysis.\nConvolutional neural networks are used to classify different gestures.\nExperimental results verify the effectiveness of the system with an accuracy of\n98%. Besides, related factors such as recognition distance and gesture scale\nare investigated. \n\n"}
{"id": "1711.02638", "contents": "Title: Compression-aware Training of Deep Networks Abstract: In recent years, great progress has been made in a variety of application\ndomains thanks to the development of increasingly deeper neural networks.\nUnfortunately, the huge number of units of these networks makes them expensive\nboth computationally and memory-wise. To overcome this, exploiting the fact\nthat deep networks are over-parametrized, several compression strategies have\nbeen proposed. These methods, however, typically start from a network that has\nbeen trained in a standard manner, without considering such a future\ncompression. In this paper, we propose to explicitly account for compression in\nthe training process. To this end, we introduce a regularizer that encourages\nthe parameter matrix of each layer to have low rank during training. We show\nthat accounting for compression during training allows us to learn much more\ncompact, yet at least as effective, models than state-of-the-art compression\ntechniques. \n\n"}
{"id": "1711.02831", "contents": "Title: SIMILARnet: Simultaneous Intelligent Localization and Recognition\n  Network Abstract: Global Average Pooling (GAP) [4] has been used previously to generate class\nactivation for image classification tasks. The motivation behind SIMILARnet\ncomes from the fact that the convolutional filters possess position information\nof the essential features and hence, combination of the feature maps could help\nus locate the class instances in an image. We propose a biologically inspired\nmodel that is free of differential connections and doesn't require separate\ntraining thereby reducing computation overhead. Our novel architecture\ngenerates promising results and unlike existing methods, the model is not\nsensitive to the input image size, thus promising wider application. Codes for\nthe experiment and illustrations can be found at:\nhttps://github.com/brcsomnath/Advanced-GAP . \n\n"}
{"id": "1711.03473", "contents": "Title: Making a long story short: A Multi-Importance fast-forwarding egocentric\n  videos with the emphasis on relevant objects Abstract: The emergence of low-cost high-quality personal wearable cameras combined\nwith the increasing storage capacity of video-sharing websites have evoked a\ngrowing interest in first-person videos, since most videos are composed of\nlong-running unedited streams which are usually tedious and unpleasant to\nwatch. State-of-the-art semantic fast-forward methods currently face the\nchallenge of providing an adequate balance between smoothness in visual flow\nand the emphasis on the relevant parts. In this work, we present the\nMulti-Importance Fast-Forward (MIFF), a fully automatic methodology to\nfast-forward egocentric videos facing these challenges. The dilemma of defining\nwhat is the semantic information of a video is addressed by a learning process\nbased on the preferences of the user. Results show that the proposed method\nkeeps over $3$ times more semantic content than the state-of-the-art\nfast-forward. Finally, we discuss the need of a particular video stabilization\ntechnique for fast-forward egocentric videos. \n\n"}
{"id": "1711.03483", "contents": "Title: Learning Multi-Modal Word Representation Grounded in Visual Context Abstract: Representing the semantics of words is a long-standing problem for the\nnatural language processing community. Most methods compute word semantics\ngiven their textual context in large corpora. More recently, researchers\nattempted to integrate perceptual and visual features. Most of these works\nconsider the visual appearance of objects to enhance word representations but\nthey ignore the visual environment and context in which objects appear. We\npropose to unify text-based techniques with vision-based techniques by\nsimultaneously leveraging textual and visual context to learn multimodal word\nembeddings. We explore various choices for what can serve as a visual context\nand present an end-to-end method to integrate visual context elements in a\nmultimodal skip-gram model. We provide experiments and extensive analysis of\nthe obtained results. \n\n"}
{"id": "1711.04708", "contents": "Title: Machine Learning for the Geosciences: Challenges and Opportunities Abstract: Geosciences is a field of great societal relevance that requires solutions to\nseveral urgent problems facing our humanity and the planet. As geosciences\nenters the era of big data, machine learning (ML) -- that has been widely\nsuccessful in commercial domains -- offers immense potential to contribute to\nproblems in geosciences. However, problems in geosciences have several unique\nchallenges that are seldom found in traditional applications, requiring novel\nproblem formulations and methodologies in machine learning. This article\nintroduces researchers in the machine learning (ML) community to these\nchallenges offered by geoscience problems and the opportunities that exist for\nadvancing both machine learning and geosciences. We first highlight typical\nsources of geoscience data and describe their properties that make it\nchallenging to use traditional machine learning techniques. We then describe\nsome of the common categories of geoscience problems where machine learning can\nplay a role, and discuss some of the existing efforts and promising directions\nfor methodological development in machine learning. We conclude by discussing\nsome of the emerging research themes in machine learning that are applicable\nacross all problems in the geosciences, and the importance of a deep\ncollaboration between machine learning and geosciences for synergistic\nadvancements in both disciplines. \n\n"}
{"id": "1711.04710", "contents": "Title: Spatio-Temporal Data Mining: A Survey of Problems and Methods Abstract: Large volumes of spatio-temporal data are increasingly collected and studied\nin diverse domains including, climate science, social sciences, neuroscience,\nepidemiology, transportation, mobile health, and Earth sciences.\nSpatio-temporal data differs from relational data for which computational\napproaches are developed in the data mining community for multiple decades, in\nthat both spatial and temporal attributes are available in addition to the\nactual measurements/attributes. The presence of these attributes introduces\nadditional challenges that needs to be dealt with. Approaches for mining\nspatio-temporal data have been studied for over a decade in the data mining\ncommunity. In this article we present a broad survey of this relatively young\nfield of spatio-temporal data mining. We discuss different types of\nspatio-temporal data and the relevant data mining questions that arise in the\ncontext of analyzing each of these datasets. Based on the nature of the data\nmining problem studied, we classify literature on spatio-temporal data mining\ninto six major categories: clustering, predictive learning, change detection,\nfrequent pattern mining, anomaly detection, and relationship mining. We discuss\nthe various forms of spatio-temporal data mining problems in each of these\ncategories. \n\n"}
{"id": "1711.05175", "contents": "Title: Adversarial Information Factorization Abstract: We propose a novel generative model architecture designed to learn\nrepresentations for images that factor out a single attribute from the rest of\nthe representation. A single object may have many attributes which when altered\ndo not change the identity of the object itself. Consider the human face; the\nidentity of a particular person is independent of whether or not they happen to\nbe wearing glasses. The attribute of wearing glasses can be changed without\nchanging the identity of the person. However, the ability to manipulate and\nalter image attributes without altering the object identity is not a trivial\ntask. Here, we are interested in learning a representation of the image that\nseparates the identity of an object (such as a human face) from an attribute\n(such as 'wearing glasses'). We demonstrate the success of our factorization\napproach by using the learned representation to synthesize the same face with\nand without a chosen attribute. We refer to this specific synthesis process as\nimage attribute manipulation. We further demonstrate that our model achieves\ncompetitive scores, with state of the art, on a facial attribute classification\ntask. \n\n"}
{"id": "1711.05820", "contents": "Title: Zero-Shot Learning via Class-Conditioned Deep Generative Models Abstract: We present a deep generative model for learning to predict classes not seen\nat training time. Unlike most existing methods for this problem, that represent\neach class as a point (via a semantic embedding), we represent each seen/unseen\nclass using a class-specific latent-space distribution, conditioned on class\nattributes. We use these latent-space distributions as a prior for a supervised\nvariational autoencoder (VAE), which also facilitates learning highly\ndiscriminative feature representations for the inputs. The entire framework is\nlearned end-to-end using only the seen-class training data. The model infers\ncorresponding attributes of a test image by maximizing the VAE lower bound; the\ninferred attributes may be linked to labels not seen when training. We further\nextend our model to a (1) semi-supervised/transductive setting by leveraging\nunlabeled unseen-class data via an unsupervised learning module, and (2)\nfew-shot learning where we also have a small number of labeled inputs from the\nunseen classes. We compare our model with several state-of-the-art methods\nthrough a comprehensive set of experiments on a variety of benchmark data sets. \n\n"}
{"id": "1711.06666", "contents": "Title: ADVISE: Symbolism and External Knowledge for Decoding Advertisements Abstract: In order to convey the most content in their limited space, advertisements\nembed references to outside knowledge via symbolism. For example, a motorcycle\nstands for adventure (a positive property the ad wants associated with the\nproduct being sold), and a gun stands for danger (a negative property to\ndissuade viewers from undesirable behaviors). We show how to use symbolic\nreferences to better understand the meaning of an ad. We further show how\nanchoring ad understanding in general-purpose object recognition and image\ncaptioning improves results. We formulate the ad understanding task as matching\nthe ad image to human-generated statements that describe the action that the ad\nprompts, and the rationale it provides for taking this action. Our proposed\nmethod outperforms the state of the art on this task, and on an alternative\nformulation of question-answering on ads. We show additional applications of\nour learned representations for matching ads to slogans, and clustering ads\naccording to their topic, without extra training. \n\n"}
{"id": "1711.06704", "contents": "Title: Repeatability Is Not Enough: Learning Affine Regions via\n  Discriminability Abstract: A method for learning local affine-covariant regions is presented. We show\nthat maximizing geometric repeatability does not lead to local regions, a.k.a\nfeatures,that are reliably matched and this necessitates descriptor-based\nlearning. We explore factors that influence such learning and registration: the\nloss function, descriptor type, geometric parametrization and the trade-off\nbetween matchability and geometric accuracy and propose a novel hard\nnegative-constant loss function for learning of affine regions. The affine\nshape estimator -- AffNet -- trained with the hard negative-constant loss\noutperforms the state-of-the-art in bag-of-words image retrieval and wide\nbaseline stereo. The proposed training process does not require precisely\ngeometrically aligned patches.The source codes and trained weights are\navailable at https://github.com/ducha-aiki/affnet \n\n"}
{"id": "1711.06897", "contents": "Title: Single-Shot Refinement Neural Network for Object Detection Abstract: For object detection, the two-stage approach (e.g., Faster R-CNN) has been\nachieving the highest accuracy, whereas the one-stage approach (e.g., SSD) has\nthe advantage of high efficiency. To inherit the merits of both while\novercoming their disadvantages, in this paper, we propose a novel single-shot\nbased detector, called RefineDet, that achieves better accuracy than two-stage\nmethods and maintains comparable efficiency of one-stage methods. RefineDet\nconsists of two inter-connected modules, namely, the anchor refinement module\nand the object detection module. Specifically, the former aims to (1) filter\nout negative anchors to reduce search space for the classifier, and (2)\ncoarsely adjust the locations and sizes of anchors to provide better\ninitialization for the subsequent regressor. The latter module takes the\nrefined anchors as the input from the former to further improve the regression\nand predict multi-class label. Meanwhile, we design a transfer connection block\nto transfer the features in the anchor refinement module to predict locations,\nsizes and class labels of objects in the object detection module. The\nmulti-task loss function enables us to train the whole network in an end-to-end\nway. Extensive experiments on PASCAL VOC 2007, PASCAL VOC 2012, and MS COCO\ndemonstrate that RefineDet achieves state-of-the-art detection accuracy with\nhigh efficiency. Code is available at https://github.com/sfzhang15/RefineDet \n\n"}
{"id": "1711.06976", "contents": "Title: MIT Advanced Vehicle Technology Study: Large-Scale Naturalistic Driving\n  Study of Driver Behavior and Interaction with Automation Abstract: For the foreseeble future, human beings will likely remain an integral part\nof the driving task, monitoring the AI system as it performs anywhere from just\nover 0% to just under 100% of the driving. The governing objectives of the MIT\nAutonomous Vehicle Technology (MIT-AVT) study are to (1) undertake large-scale\nreal-world driving data collection that includes high-definition video to fuel\nthe development of deep learning based internal and external perception\nsystems, (2) gain a holistic understanding of how human beings interact with\nvehicle automation technology by integrating video data with vehicle state\ndata, driver characteristics, mental models, and self-reported experiences with\ntechnology, and (3) identify how technology and other factors related to\nautomation adoption and use can be improved in ways that save lives. In\npursuing these objectives, we have instrumented 23 Tesla Model S and Model X\nvehicles, 2 Volvo S90 vehicles, 2 Range Rover Evoque, and 2 Cadillac CT6\nvehicles for both long-term (over a year per driver) and medium term (one month\nper driver) naturalistic driving data collection. Furthermore, we are\ncontinually developing new methods for analysis of the massive-scale dataset\ncollected from the instrumented vehicle fleet. The recorded data streams\ninclude IMU, GPS, CAN messages, and high-definition video streams of the driver\nface, the driver cabin, the forward roadway, and the instrument cluster (on\nselect vehicles). The study is on-going and growing. To date, we have 122\nparticipants, 15,610 days of participation, 511,638 miles, and 7.1 billion\nvideo frames. This paper presents the design of the study, the data collection\nhardware, the processing of the data, and the computer vision algorithms\ncurrently being used to extract actionable knowledge from the data. \n\n"}
{"id": "1711.08000", "contents": "Title: Personalization of Saliency Estimation Abstract: Most existing saliency models use low-level features or task descriptions\nwhen generating attention predictions. However, the link between observer\ncharacteristics and gaze patterns is rarely investigated. We present a novel\nsaliency prediction technique which takes viewers' identities and personal\ntraits into consideration when modeling human attention. Instead of only\ncomputing image salience for average observers, we consider the interpersonal\nvariation in the viewing behaviors of observers with different personal traits\nand backgrounds. We present an enriched derivative of the GAN network, which is\nable to generate personalized saliency predictions when fed with image stimuli\nand specific information about the observer. Our model contains a generator\nwhich generates grayscale saliency heat maps based on the image and an observer\nlabel. The generator is paired with an adversarial discriminator which learns\nto distinguish generated salience from ground truth salience. The discriminator\nalso has the observer label as an input, which contributes to the\npersonalization ability of our approach. We evaluate the performance of our\npersonalized salience model by comparison with a benchmark model along with\nother un-personalized predictions, and illustrate improvements in prediction\naccuracy for all tested observer groups. \n\n"}
{"id": "1711.08565", "contents": "Title: Person Transfer GAN to Bridge Domain Gap for Person Re-Identification Abstract: Although the performance of person Re-Identification (ReID) has been\nsignificantly boosted, many challenging issues in real scenarios have not been\nfully investigated, e.g., the complex scenes and lighting variations, viewpoint\nand pose changes, and the large number of identities in a camera network. To\nfacilitate the research towards conquering those issues, this paper contributes\na new dataset called MSMT17 with many important features, e.g., 1) the raw\nvideos are taken by an 15-camera network deployed in both indoor and outdoor\nscenes, 2) the videos cover a long period of time and present complex lighting\nvariations, and 3) it contains currently the largest number of annotated\nidentities, i.e., 4,101 identities and 126,441 bounding boxes. We also observe\nthat, domain gap commonly exists between datasets, which essentially causes\nsevere performance drop when training and testing on different datasets. This\nresults in that available training data cannot be effectively leveraged for new\ntesting domains. To relieve the expensive costs of annotating new training\nsamples, we propose a Person Transfer Generative Adversarial Network (PTGAN) to\nbridge the domain gap. Comprehensive experiments show that the domain gap could\nbe substantially narrowed-down by the PTGAN. \n\n"}
{"id": "1711.08690", "contents": "Title: Attended End-to-end Architecture for Age Estimation from Facial\n  Expression Videos Abstract: The main challenges of age estimation from facial expression videos lie not\nonly in the modeling of the static facial appearance, but also in the capturing\nof the temporal facial dynamics. Traditional techniques to this problem focus\non constructing handcrafted features to explore the discriminative information\ncontained in facial appearance and dynamics separately. This relies on\nsophisticated feature-refinement and framework-design. In this paper, we\npresent an end-to-end architecture for age estimation, called Spatially-Indexed\nAttention Model (SIAM), which is able to simultaneously learn both the\nappearance and dynamics of age from raw videos of facial expressions.\nSpecifically, we employ convolutional neural networks to extract effective\nlatent appearance representations and feed them into recurrent networks to\nmodel the temporal dynamics. More importantly, we propose to leverage attention\nmodels for salience detection in both the spatial domain for each single image\nand the temporal domain for the whole video as well. We design a specific\nspatially-indexed attention mechanism among the convolutional layers to extract\nthe salient facial regions in each individual image, and a temporal attention\nlayer to assign attention weights to each frame. This two-pronged approach not\nonly improves the performance by allowing the model to focus on informative\nframes and facial areas, but it also offers an interpretable correspondence\nbetween the spatial facial regions as well as temporal frames, and the task of\nage estimation. We demonstrate the strong performance of our model in\nexperiments on a large, gender-balanced database with 400 subjects with ages\nspanning from 8 to 76 years. Experiments reveal that our model exhibits\nsignificant superiority over the state-of-the-art methods given sufficient\ntraining data. \n\n"}
{"id": "1711.08901", "contents": "Title: Supervised Hashing with End-to-End Binary Deep Neural Network Abstract: Image hashing is a popular technique applied to large scale content-based\nvisual retrieval due to its compact and efficient binary codes. Our work\nproposes a new end-to-end deep network architecture for supervised hashing\nwhich directly learns binary codes from input images and maintains good\nproperties over binary codes such as similarity preservation, independence, and\nbalancing. Furthermore, we also propose a new learning scheme that can cope\nwith the binary constrained loss function. The proposed algorithm not only is\nscalable for learning over large-scale datasets but also outperforms\nstate-of-the-art supervised hashing methods, which are illustrated throughout\nextensive experiments from various image retrieval benchmarks. \n\n"}
{"id": "1711.08904", "contents": "Title: Adversarial Transfer Learning for Cross-domain Visual Recognition Abstract: In many practical visual recognition scenarios, feature distribution in the\nsource domain is generally different from that of the target domain, which\nresults in the emergence of general cross-domain visual recognition problems.\nTo address the problems of visual domain mismatch, we propose a novel\nsemi-supervised adversarial transfer learning approach, which is called Coupled\nadversarial transfer Domain Adaptation (CatDA), for distribution alignment\nbetween two domains. The proposed CatDA approach is inspired by cycleGAN, but\nleveraging multiple shallow multilayer perceptrons (MLPs) instead of deep\nnetworks. Specifically, our CatDA comprises of two symmetric and slim\nsub-networks, such that the coupled adversarial learning framework is\nformulated. With such symmetry of two generators, the input data from\nsource/target domain can be fed into the MLP network for target/source domain\ngeneration, supervised by two confrontation oriented coupled discriminators.\nNotably, in order to avoid the critical flaw of high-capacity of the feature\nextraction function during domain adversarial training, domain specific loss\nand domain knowledge fidelity loss are proposed in each generator, such that\nthe effectiveness of the proposed transfer network is guaranteed. Additionally,\nthe essential difference from cycleGAN is that our method aims to generate\ndomain-agnostic and aligned features for domain adaptation and transfer\nlearning rather than synthesize realistic images. We show experimentally on a\nnumber of benchmark datasets and the proposed approach achieves competitive\nperformance over state-of-the-art domain adaptation and transfer learning\napproaches. \n\n"}
{"id": "1711.09404", "contents": "Title: Improving the Adversarial Robustness and Interpretability of Deep Neural\n  Networks by Regularizing their Input Gradients Abstract: Deep neural networks have proven remarkably effective at solving many\nclassification problems, but have been criticized recently for two major\nweaknesses: the reasons behind their predictions are uninterpretable, and the\npredictions themselves can often be fooled by small adversarial perturbations.\nThese problems pose major obstacles for the adoption of neural networks in\ndomains that require security or transparency. In this work, we evaluate the\neffectiveness of defenses that differentiably penalize the degree to which\nsmall changes in inputs can alter model predictions. Across multiple attacks,\narchitectures, defenses, and datasets, we find that neural networks trained\nwith this input gradient regularization exhibit robustness to transferred\nadversarial examples generated to fool all of the other models. We also find\nthat adversarial examples generated to fool gradient-regularized models fool\nall other models equally well, and actually lead to more \"legitimate,\"\ninterpretable misclassifications as rated by people (which we confirm in a\nhuman subject experiment). Finally, we demonstrate that regularizing input\ngradients makes them more naturally interpretable as rationales for model\npredictions. We conclude by discussing this relationship between\ninterpretability and robustness in deep neural networks. \n\n"}
{"id": "1711.10394", "contents": "Title: Exposing Computer Generated Images by Using Deep Convolutional Neural\n  Networks Abstract: The recent computer graphics developments have upraised the quality of the\ngenerated digital content, astonishing the most skeptical viewer. Games and\nmovies have taken advantage of this fact but, at the same time, these advances\nhave brought serious negative impacts like the ones yielded by fakeimages\nproduced with malicious intents. Digital artists can compose artificial images\ncapable of deceiving the great majority of people, turning this into a very\ndangerous weapon in a timespan currently know as Fake News/Post-Truth\" Era. In\nthis work, we propose a new approach for dealing with the problem of detecting\ncomputer generated images, through the application of deep convolutional\nnetworks and transfer learning techniques. We start from Residual Networks and\ndevelop different models adapted to the binary problem of identifying if an\nimage was or not computer generated. Differently from the current\nstate-of-the-art approaches, we don't rely on hand-crafted features, but\nprovide to the model the raw pixel information, achieving the same 0.97 of\nstate-of-the-art methods with two main advantages: our methods show more stable\nresults (depicted by lower variance) and eliminate the laborious and manual\nstep of specialized features extraction and selection. \n\n"}
{"id": "1711.10684", "contents": "Title: Road Extraction by Deep Residual U-Net Abstract: Road extraction from aerial images has been a hot research topic in the field\nof remote sensing image analysis. In this letter, a semantic segmentation\nneural network which combines the strengths of residual learning and U-Net is\nproposed for road area extraction. The network is built with residual units and\nhas similar architecture to that of U-Net. The benefits of this model is\ntwo-fold: first, residual units ease training of deep networks. Second, the\nrich skip connections within the network could facilitate information\npropagation, allowing us to design networks with fewer parameters however\nbetter performance. We test our network on a public road dataset and compare it\nwith U-Net and other two state of the art deep learning based road extraction\nmethods. The proposed approach outperforms all the comparing methods, which\ndemonstrates its superiority over recently developed state of the arts. \n\n"}
{"id": "1711.10761", "contents": "Title: Transfer Learning with Binary Neural Networks Abstract: Previous work has shown that it is possible to train deep neural networks\nwith low precision weights and activations. In the extreme case it is even\npossible to constrain the network to binary values. The costly floating point\nmultiplications are then reduced to fast logical operations. High end smart\nphones such as Google's Pixel 2 and Apple's iPhone X are already equipped with\nspecialised hardware for image processing and it is very likely that other\nfuture consumer hardware will also have dedicated accelerators for deep neural\nnetworks. Binary neural networks are attractive in this case because the\nlogical operations are very fast and efficient when implemented in hardware. We\npropose a transfer learning based architecture where we first train a binary\nnetwork on Imagenet and then retrain part of the network for different tasks\nwhile keeping most of the network fixed. The fixed binary part could be\nimplemented in a hardware accelerator while the last layers of the network are\nevaluated in software. We show that a single binary neural network trained on\nthe Imagenet dataset can indeed be used as a feature extractor for other\ndatasets. \n\n"}
{"id": "1711.10918", "contents": "Title: Joint Blind Motion Deblurring and Depth Estimation of Light Field Abstract: Removing camera motion blur from a single light field is a challenging task\nsince it is highly ill-posed inverse problem. The problem becomes even worse\nwhen blur kernel varies spatially due to scene depth variation and high-order\ncamera motion. In this paper, we propose a novel algorithm to estimate all blur\nmodel variables jointly, including latent sub-aperture image, camera motion,\nand scene depth from the blurred 4D light field. Exploiting multi-view nature\nof a light field relieves the inverse property of the optimization by utilizing\nstrong depth cues and multi-view blur observation. The proposed joint\nestimation achieves high quality light field deblurring and depth estimation\nsimultaneously under arbitrary 6-DOF camera motion and unconstrained scene\ndepth. Intensive experiment on real and synthetic blurred light field confirms\nthat the proposed algorithm outperforms the state-of-the-art light field\ndeblurring and depth estimation methods. \n\n"}
{"id": "1712.00080", "contents": "Title: Super SloMo: High Quality Estimation of Multiple Intermediate Frames for\n  Video Interpolation Abstract: Given two consecutive frames, video interpolation aims at generating\nintermediate frame(s) to form both spatially and temporally coherent video\nsequences. While most existing methods focus on single-frame interpolation, we\npropose an end-to-end convolutional neural network for variable-length\nmulti-frame video interpolation, where the motion interpretation and occlusion\nreasoning are jointly modeled. We start by computing bi-directional optical\nflow between the input images using a U-Net architecture. These flows are then\nlinearly combined at each time step to approximate the intermediate\nbi-directional optical flows. These approximate flows, however, only work well\nin locally smooth regions and produce artifacts around motion boundaries. To\naddress this shortcoming, we employ another U-Net to refine the approximated\nflow and also predict soft visibility maps. Finally, the two input images are\nwarped and linearly fused to form each intermediate frame. By applying the\nvisibility maps to the warped images before fusion, we exclude the contribution\nof occluded pixels to the interpolated intermediate frame to avoid artifacts.\nSince none of our learned network parameters are time-dependent, our approach\nis able to produce as many intermediate frames as needed. We use 1,132 video\nclips with 240-fps, containing 300K individual video frames, to train our\nnetwork. Experimental results on several datasets, predicting different numbers\nof interpolated frames, demonstrate that our approach performs consistently\nbetter than existing methods. \n\n"}
{"id": "1712.00282", "contents": "Title: Neural Signatures for Licence Plate Re-identification Abstract: The problem of vehicle licence plate re-identification is generally\nconsidered as a one-shot image retrieval problem. The objective of this task is\nto learn a feature representation (called a \"signature\") for licence plates.\nIncoming licence plate images are converted to signatures and matched to a\npreviously collected template database through a distance measure. Then, the\ninput image is recognized as the template whose signature is \"nearest\" to the\ninput signature. The template database is restricted to contain only a single\nsignature per unique licence plate for our problem.\n  We measure the performance of deep convolutional net-based features adapted\nfrom face recognition on this task. In addition, we also test a hybrid approach\ncombining the Fisher vector with a neural network-based embedding called \"f2nn\"\ntrained with the Triplet loss function. We find that the hybrid approach\nperforms comparably while providing computational benefits. The signature\ngenerated by the hybrid approach also shows higher generalizability to datasets\nmore dissimilar to the training corpus. \n\n"}
{"id": "1712.00557", "contents": "Title: Recurrent Neural Network Language Models for Open Vocabulary Event-Level\n  Cyber Anomaly Detection Abstract: Automated analysis methods are crucial aids for monitoring and defending a\nnetwork to protect the sensitive or confidential data it hosts. This work\nintroduces a flexible, powerful, and unsupervised approach to detecting\nanomalous behavior in computer and network logs, one that largely eliminates\ndomain-dependent feature engineering employed by existing methods. By treating\nsystem logs as threads of interleaved \"sentences\" (event log lines) to train\nonline unsupervised neural network language models, our approach provides an\nadaptive model of normal network behavior. We compare the effectiveness of both\nstandard and bidirectional recurrent neural network language models at\ndetecting malicious activity within network log data. Extending these models,\nwe introduce a tiered recurrent architecture, which provides context by\nmodeling sequences of users' actions over time. Compared to Isolation Forest\nand Principal Components Analysis, two popular anomaly detection algorithms, we\nobserve superior performance on the Los Alamos National Laboratory Cyber\nSecurity dataset. For log-line-level red team detection, our best performing\ncharacter-based model provides test set area under the receiver operator\ncharacteristic curve of 0.98, demonstrating the strong fine-grained anomaly\ndetection performance of this approach on open vocabulary logging sources. \n\n"}
{"id": "1712.00621", "contents": "Title: DR-Net: Transmission Steered Single Image Dehazing Network with Weakly\n  Supervised Refinement Abstract: Despite the recent progress in image dehazing, several problems remain\nlargely unsolved such as robustness for varying scenes, the visual quality of\nreconstructed images, and effectiveness and flexibility for applications. To\ntackle these problems, we propose a new deep network architecture for single\nimage dehazing called DR-Net. Our model consists of three main subnetworks: a\ntransmission prediction network that predicts transmission map for the input\nimage, a haze removal network that reconstructs latent image steered by the\ntransmission map, and a refinement network that enhances the details and color\nproperties of the dehazed result via weakly supervised learning. Compared to\nprevious methods, our method advances in three aspects: (i) pure data-driven\nmodel; (ii) the end-to-end system; (iii) superior robustness, accuracy, and\napplicability. Extensive experiments demonstrate that our DR-Net outperforms\nthe state-of-the-art methods on both synthetic and real images in qualitative\nand quantitative metrics. Additionally, the utility of DR-Net has been\nillustrated by its potential usage in several important computer vision tasks. \n\n"}
{"id": "1712.00733", "contents": "Title: Incorporating External Knowledge to Answer Open-Domain Visual Questions\n  with Dynamic Memory Networks Abstract: Visual Question Answering (VQA) has attracted much attention since it offers\ninsight into the relationships between the multi-modal analysis of images and\nnatural language. Most of the current algorithms are incapable of answering\nopen-domain questions that require to perform reasoning beyond the image\ncontents. To address this issue, we propose a novel framework which endows the\nmodel capabilities in answering more complex questions by leveraging massive\nexternal knowledge with dynamic memory networks. Specifically, the questions\nalong with the corresponding images trigger a process to retrieve the relevant\ninformation in external knowledge bases, which are embedded into a continuous\nvector space by preserving the entity-relation structures. Afterwards, we\nemploy dynamic memory networks to attend to the large body of facts in the\nknowledge graph and images, and then perform reasoning over these facts to\ngenerate corresponding answers. Extensive experiments demonstrate that our\nmodel not only achieves the state-of-the-art performance in the visual question\nanswering task, but can also answer open-domain questions effectively by\nleveraging the external knowledge. \n\n"}
{"id": "1712.01034", "contents": "Title: Towards Faster Training of Global Covariance Pooling Networks by\n  Iterative Matrix Square Root Normalization Abstract: Global covariance pooling in convolutional neural networks has achieved\nimpressive improvement over the classical first-order pooling. Recent works\nhave shown matrix square root normalization plays a central role in achieving\nstate-of-the-art performance. However, existing methods depend heavily on\neigendecomposition (EIG) or singular value decomposition (SVD), suffering from\ninefficient training due to limited support of EIG and SVD on GPU. Towards\naddressing this problem, we propose an iterative matrix square root\nnormalization method for fast end-to-end training of global covariance pooling\nnetworks. At the core of our method is a meta-layer designed with loop-embedded\ndirected graph structure. The meta-layer consists of three consecutive\nnonlinear structured layers, which perform pre-normalization, coupled matrix\niteration and post-compensation, respectively. Our method is much faster than\nEIG or SVD based ones, since it involves only matrix multiplications, suitable\nfor parallel implementation on GPU. Moreover, the proposed network with ResNet\narchitecture can converge in much less epochs, further accelerating network\ntraining. On large-scale ImageNet, we achieve competitive performance superior\nto existing counterparts. By finetuning our models pre-trained on ImageNet, we\nestablish state-of-the-art results on three challenging fine-grained\nbenchmarks. The source code and network models will be available at\nhttp://www.peihuali.org/iSQRT-COV \n\n"}
{"id": "1712.01039", "contents": "Title: Learning for Disparity Estimation through Feature Constancy Abstract: Stereo matching algorithms usually consist of four steps, including matching\ncost calculation, matching cost aggregation, disparity calculation, and\ndisparity refinement. Existing CNN-based methods only adopt CNN to solve parts\nof the four steps, or use different networks to deal with different steps,\nmaking them difficult to obtain the overall optimal solution. In this paper, we\npropose a network architecture to incorporate all steps of stereo matching. The\nnetwork consists of three parts. The first part calculates the multi-scale\nshared features. The second part performs matching cost calculation, matching\ncost aggregation and disparity calculation to estimate the initial disparity\nusing shared features. The initial disparity and the shared features are used\nto calculate the feature constancy that measures correctness of the\ncorrespondence between two input images. The initial disparity and the feature\nconstancy are then fed to a sub-network to refine the initial disparity. The\nproposed method has been evaluated on the Scene Flow and KITTI datasets. It\nachieves the state-of-the-art performance on the KITTI 2012 and KITTI 2015\nbenchmarks while maintaining a very fast running time. \n\n"}
{"id": "1712.01056", "contents": "Title: CNN based Learning using Reflection and Retinex Models for Intrinsic\n  Image Decomposition Abstract: Most of the traditional work on intrinsic image decomposition rely on\nderiving priors about scene characteristics. On the other hand, recent research\nuse deep learning models as in-and-out black box and do not consider the\nwell-established, traditional image formation process as the basis of their\nintrinsic learning process. As a consequence, although current deep learning\napproaches show superior performance when considering quantitative benchmark\nresults, traditional approaches are still dominant in achieving high\nqualitative results. In this paper, the aim is to exploit the best of the two\nworlds. A method is proposed that (1) is empowered by deep learning\ncapabilities, (2) considers a physics-based reflection model to steer the\nlearning process, and (3) exploits the traditional approach to obtain intrinsic\nimages by exploiting reflectance and shading gradient information. The proposed\nmodel is fast to compute and allows for the integration of all intrinsic\ncomponents. To train the new model, an object centered large-scale datasets\nwith intrinsic ground-truth images are created. The evaluation results\ndemonstrate that the new model outperforms existing methods. Visual inspection\nshows that the image formation loss function augments color reproduction and\nthe use of gradient information produces sharper edges. Datasets, models and\nhigher resolution images are available at https://ivi.fnwi.uva.nl/cv/retinet. \n\n"}
{"id": "1712.01238", "contents": "Title: Learning by Asking Questions Abstract: We introduce an interactive learning framework for the development and\ntesting of intelligent visual systems, called learning-by-asking (LBA). We\nexplore LBA in context of the Visual Question Answering (VQA) task. LBA differs\nfrom standard VQA training in that most questions are not observed during\ntraining time, and the learner must ask questions it wants answers to. Thus,\nLBA more closely mimics natural learning and has the potential to be more\ndata-efficient than the traditional VQA setting. We present a model that\nperforms LBA on the CLEVR dataset, and show that it automatically discovers an\neasy-to-hard curriculum when learning interactively from an oracle. Our LBA\ngenerated data consistently matches or outperforms the CLEVR train data and is\nmore sample efficient. We also show that our model asks questions that\ngeneralize to state-of-the-art VQA models and to novel test time distributions. \n\n"}
{"id": "1712.01636", "contents": "Title: Generalization of Deep Neural Networks for Chest Pathology\n  Classification in X-Rays Using Generative Adversarial Networks Abstract: Medical datasets are often highly imbalanced with over-representation of\ncommon medical problems and a paucity of data from rare conditions. We propose\nsimulation of pathology in images to overcome the above limitations. Using\nchest X-rays as a model medical image, we implement a generative adversarial\nnetwork (GAN) to create artificial images based upon a modest sized labeled\ndataset. We employ a combination of real and artificial images to train a deep\nconvolutional neural network (DCNN) to detect pathology across five classes of\nchest X-rays. Furthermore, we demonstrate that augmenting the original\nimbalanced dataset with GAN generated images improves performance of chest\npathology classification using the proposed DCNN in comparison to the same DCNN\ntrained with the original dataset alone. This improved performance is largely\nattributed to balancing of the dataset using GAN generated images, where image\nclasses that are lacking in example images are preferentially augmented. \n\n"}
{"id": "1712.01653", "contents": "Title: Context Augmentation for Convolutional Neural Networks Abstract: Recent enhancements of deep convolutional neural networks (ConvNets)\nempowered by enormous amounts of labeled data have closed the gap with human\nperformance for many object recognition tasks. These impressive results have\ngenerated interest in understanding and visualization of ConvNets. In this\nwork, we study the effect of background in the task of image classification.\nOur results show that changing the backgrounds of the training datasets can\nhave drastic effects on testing accuracies. Furthermore, we enhance existing\naugmentation techniques with the foreground segmented objects. The findings of\nthis work are important in increasing the accuracies when only a small dataset\nis available, in creating datasets, and creating synthetic images. \n\n"}
{"id": "1712.01751", "contents": "Title: Convolutional Recurrent Neural Networks for Dynamic MR Image\n  Reconstruction Abstract: Accelerating the data acquisition of dynamic magnetic resonance imaging (MRI)\nleads to a challenging ill-posed inverse problem, which has received great\ninterest from both the signal processing and machine learning community over\nthe last decades. The key ingredient to the problem is how to exploit the\ntemporal correlation of the MR sequence to resolve the aliasing artefact.\nTraditionally, such observation led to a formulation of a non-convex\noptimisation problem, which were solved using iterative algorithms. Recently,\nhowever, deep learning based-approaches have gained significant popularity due\nto its ability to solve general inversion problems. In this work, we propose a\nunique, novel convolutional recurrent neural network (CRNN) architecture which\nreconstructs high quality cardiac MR images from highly undersampled k-space\ndata by jointly exploiting the dependencies of the temporal sequences as well\nas the iterative nature of the traditional optimisation algorithms. In\nparticular, the proposed architecture embeds the structure of the traditional\niterative algorithms, efficiently modelling the recurrence of the iterative\nreconstruction stages by using recurrent hidden connections over such\niterations. In addition, spatiotemporal dependencies are simultaneously learnt\nby exploiting bidirectional recurrent hidden connections across time sequences.\nThe proposed algorithm is able to learn both the temporal dependency and the\niterative reconstruction process effectively with only a very small number of\nparameters, while outperforming current MR reconstruction methods in terms of\ncomputational complexity, reconstruction accuracy and speed. \n\n"}
{"id": "1712.02517", "contents": "Title: Broadcasting Convolutional Network for Visual Relational Reasoning Abstract: In this paper, we propose the Broadcasting Convolutional Network (BCN) that\nextracts key object features from the global field of an entire input image and\nrecognizes their relationship with local features. BCN is a simple network\nmodule that collects effective spatial features, embeds location information\nand broadcasts them to the entire feature maps. We further introduce the\nMulti-Relational Network (multiRN) that improves the existing Relation Network\n(RN) by utilizing the BCN module. In pixel-based relation reasoning problems,\nwith the help of BCN, multiRN extends the concept of `pairwise relations' in\nconventional RNs to `multiwise relations' by relating each object with multiple\nobjects at once. This yields in O(n) complexity for n objects, which is a vast\ncomputational gain from RNs that take O(n^2). Through experiments, multiRN has\nachieved a state-of-the-art performance on CLEVR dataset, which proves the\nusability of BCN on relation reasoning problems. \n\n"}
{"id": "1712.02861", "contents": "Title: Per-Pixel Feedback for improving Semantic Segmentation Abstract: Semantic segmentation is the task of assigning a label to each pixel in the\nimage.In recent years, deep convolutional neural networks have been driving\nadvances in multiple tasks related to cognition. Although, DCNNs have resulted\nin unprecedented visual recognition performances, they offer little\ntransparency. To understand how DCNN based models work at the task of semantic\nsegmentation, we try to analyze the DCNN models in semantic segmentation. We\ntry to find the importance of global image information for labeling pixels.\n  Based on the experiments on discriminative regions, and modeling of\nfixations, we propose a set of new training loss functions for fine-tuning DCNN\nbased models. The proposed training regime has shown improvement in performance\nof DeepLab Large FOV(VGG-16) Segmentation model for PASCAL VOC 2012 dataset.\nHowever, further test remains to conclusively evaluate the benefits due to the\nproposed loss functions across models, and data-sets.\n  Submitted in part fulfillment of the requirements for the degree of\nIntegrated Masters of Science in Applied Mathematics.\n  Update: Further Experiment showed minimal benefits.\n  Code Available [here](https://github.com/BardOfCodes/Seg-Unravel). \n\n"}
{"id": "1712.02874", "contents": "Title: Multi-Scale Video Frame-Synthesis Network with Transitive Consistency\n  Loss Abstract: Traditional approaches to interpolate/extrapolate frames in a video sequence\nrequire accurate pixel correspondences between images, e.g., using optical\nflow. Their results stem on the accuracy of optical flow estimation, and could\ngenerate heavy artifacts when flow estimation failed. Recently methods using\nauto-encoder has shown impressive progress, however they are usually trained\nfor specific interpolation/extrapolation settings and lack of flexibility and\nIn order to reduce these limitations, we propose a unified network to\nparameterize the interest frame position and therefore infer\ninterpolate/extrapolate frames within the same framework. To achieve this, we\nintroduce a transitive consistency loss to better regularize the network. We\nadopt a multi-scale structure for the network so that the parameters can be\nshared across multi-layers. Our approach avoids expensive global optimization\nof optical flow methods, and is efficient and flexible for video\ninterpolation/extrapolation applications. Experimental results have shown that\nour method performs favorably against state-of-the-art methods. \n\n"}
{"id": "1712.03660", "contents": "Title: Parallel Mapper Abstract: The construction of Mapper has emerged in the last decade as a powerful and\neffective topological data analysis tool that approximates and generalizes\nother topological summaries, such as the Reeb graph, the contour tree, split,\nand joint trees. In this paper, we study the parallel analysis of the\nconstruction of Mapper. We give a provably correct parallel algorithm to\nexecute Mapper on multiple processors and discuss the performance results that\ncompare our approach to a reference sequential Mapper implementation. We report\nthe performance experiments that demonstrate the efficiency of our method. \n\n"}
{"id": "1712.04248", "contents": "Title: Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box\n  Machine Learning Models Abstract: Many machine learning algorithms are vulnerable to almost imperceptible\nperturbations of their inputs. So far it was unclear how much risk adversarial\nperturbations carry for the safety of real-world machine learning applications\nbecause most methods used to generate such perturbations rely either on\ndetailed model information (gradient-based attacks) or on confidence scores\nsuch as class probabilities (score-based attacks), neither of which are\navailable in most real-world scenarios. In many such cases one currently needs\nto retreat to transfer-based attacks which rely on cumbersome substitute\nmodels, need access to the training data and can be defended against. Here we\nemphasise the importance of attacks which solely rely on the final model\ndecision. Such decision-based attacks are (1) applicable to real-world\nblack-box models such as autonomous cars, (2) need less knowledge and are\neasier to apply than transfer-based attacks and (3) are more robust to simple\ndefences than gradient- or score-based attacks. Previous attacks in this\ncategory were limited to simple models or simple datasets. Here we introduce\nthe Boundary Attack, a decision-based attack that starts from a large\nadversarial perturbation and then seeks to reduce the perturbation while\nstaying adversarial. The attack is conceptually simple, requires close to no\nhyperparameter tuning, does not rely on substitute models and is competitive\nwith the best gradient-based attacks in standard computer vision tasks like\nImageNet. We apply the attack on two black-box algorithms from Clarifai.com.\nThe Boundary Attack in particular and the class of decision-based attacks in\ngeneral open new avenues to study the robustness of machine learning models and\nraise new questions regarding the safety of deployed machine learning systems.\nAn implementation of the attack is available as part of Foolbox at\nhttps://github.com/bethgelab/foolbox . \n\n"}
{"id": "1712.04480", "contents": "Title: Learning a Complete Image Indexing Pipeline Abstract: To work at scale, a complete image indexing system comprises two components:\nAn inverted file index to restrict the actual search to only a subset that\nshould contain most of the items relevant to the query; An approximate distance\ncomputation mechanism to rapidly scan these lists. While supervised deep\nlearning has recently enabled improvements to the latter, the former continues\nto be based on unsupervised clustering in the literature. In this work, we\npropose a first system that learns both components within a unifying neural\nframework of structured binary encoding. \n\n"}
{"id": "1712.04621", "contents": "Title: The Effectiveness of Data Augmentation in Image Classification using\n  Deep Learning Abstract: In this paper, we explore and compare multiple solutions to the problem of\ndata augmentation in image classification. Previous work has demonstrated the\neffectiveness of data augmentation through simple techniques, such as cropping,\nrotating, and flipping input images. We artificially constrain our access to\ndata to a small subset of the ImageNet dataset, and compare each data\naugmentation technique in turn. One of the more successful data augmentations\nstrategies is the traditional transformations mentioned above. We also\nexperiment with GANs to generate images of different styles. Finally, we\npropose a method to allow a neural net to learn augmentations that best improve\nthe classifier, which we call neural augmentation. We discuss the successes and\nshortcomings of this method on various datasets. \n\n"}
{"id": "1712.06229", "contents": "Title: Panoramic Robust PCA for Foreground-Background Separation on Noisy,\n  Free-Motion Camera Video Abstract: This work presents a new robust PCA method for foreground-background\nseparation on freely moving camera video with possible dense and sparse\ncorruptions. Our proposed method registers the frames of the corrupted video\nand then encodes the varying perspective arising from camera motion as missing\ndata in a global model. This formulation allows our algorithm to produce a\npanoramic background component that automatically stitches together corrupted\ndata from partially overlapping frames to reconstruct the full field of view.\nWe model the registered video as the sum of a low-rank component that captures\nthe background, a smooth component that captures the dynamic foreground of the\nscene, and a sparse component that isolates possible outliers and other sparse\ncorruptions in the video. The low-rank portion of our model is based on a\nrecent low-rank matrix estimator (OptShrink) that has been shown to yield\nsuperior low-rank subspace estimates in practice. To estimate the smooth\nforeground component of our model, we use a weighted total variation framework\nthat enables our method to reliably decouple the true foreground of the video\nfrom sparse corruptions. We perform extensive numerical experiments on both\nstatic and moving camera video subject to a variety of dense and sparse\ncorruptions. Our experiments demonstrate the state-of-the-art performance of\nour proposed method compared to existing methods both in terms of foreground\nand background estimation accuracy. \n\n"}
{"id": "1712.06897", "contents": "Title: Learning Fixation Point Strategy for Object Detection and Classification Abstract: We propose a novel recurrent attentional structure to localize and recognize\nobjects jointly. The network can learn to extract a sequence of local\nobservations with detailed appearance and rough context, instead of sliding\nwindows or convolutions on the entire image. Meanwhile, those observations are\nfused to complete detection and classification tasks. On training, we present a\nhybrid loss function to learn the parameters of the multi-task network\nend-to-end. Particularly, the combination of stochastic and object-awareness\nstrategy, named SA, can select more abundant context and ensure the last\nfixation close to the object. In addition, we build a real-world dataset to\nverify the capacity of our method in detecting the object of interest including\nthose small ones. Our method can predict a precise bounding box on an image,\nand achieve high speed on large images without pooling operations. Experimental\nresults indicate that the proposed method can mine effective context by several\nlocal observations. Moreover, the precision and speed are easily improved by\nchanging the number of recurrent steps. Finally, we will open the source code\nof our proposed approach. \n\n"}
{"id": "1712.07881", "contents": "Title: Simulating Patho-realistic Ultrasound Images using Deep Generative\n  Networks with Adversarial Learning Abstract: Ultrasound imaging makes use of backscattering of waves during their\ninteraction with scatterers present in biological tissues. Simulation of\nsynthetic ultrasound images is a challenging problem on account of inability to\naccurately model various factors of which some include intra-/inter scanline\ninterference, transducer to surface coupling, artifacts on transducer elements,\ninhomogeneous shadowing and nonlinear attenuation. Current approaches typically\nsolve wave space equations making them computationally expensive and slow to\noperate. We propose a generative adversarial network (GAN) inspired approach\nfor fast simulation of patho-realistic ultrasound images. We apply the\nframework to intravascular ultrasound (IVUS) simulation. A stage 0 simulation\nperformed using pseudo B-mode ultrasound image simulator yields speckle mapping\nof a digitally defined phantom. The stage I GAN subsequently refines them to\npreserve tissue specific speckle intensities. The stage II GAN further refines\nthem to generate high resolution images with patho-realistic speckle profiles.\nWe evaluate patho-realism of simulated images with a visual Turing test\nindicating an equivocal confusion in discriminating simulated from real. We\nalso quantify the shift in tissue specific intensity distributions of the real\nand simulated images to prove their similarity. \n\n"}
{"id": "1712.08416", "contents": "Title: On the Integration of Optical Flow and Action Recognition Abstract: Most of the top performing action recognition methods use optical flow as a\n\"black box\" input. Here we take a deeper look at the combination of flow and\naction recognition, and investigate why optical flow is helpful, what makes a\nflow method good for action recognition, and how we can make it better. In\nparticular, we investigate the impact of different flow algorithms and input\ntransformations to better understand how these affect a state-of-the-art action\nrecognition method. Furthermore, we fine tune two neural-network flow methods\nend-to-end on the most widely used action recognition dataset (UCF101). Based\non these experiments, we make the following five observations: 1) optical flow\nis useful for action recognition because it is invariant to appearance, 2)\noptical flow methods are optimized to minimize end-point-error (EPE), but the\nEPE of current methods is not well correlated with action recognition\nperformance, 3) for the flow methods tested, accuracy at boundaries and at\nsmall displacements is most correlated with action recognition performance, 4)\ntraining optical flow to minimize classification error instead of minimizing\nEPE improves recognition performance, and 5) optical flow learned for the task\nof action recognition differs from traditional optical flow especially inside\nthe human body and at the boundary of the body. These observations may\nencourage optical flow researchers to look beyond EPE as a goal and guide\naction recognition researchers to seek better motion cues, leading to a tighter\nintegration of the optical flow and action recognition communities. \n\n"}
{"id": "1712.09867", "contents": "Title: Future Frame Prediction for Anomaly Detection -- A New Baseline Abstract: Anomaly detection in videos refers to the identification of events that do\nnot conform to expected behavior. However, almost all existing methods tackle\nthe problem by minimizing the reconstruction errors of training data, which\ncannot guarantee a larger reconstruction error for an abnormal event. In this\npaper, we propose to tackle the anomaly detection problem within a video\nprediction framework. To the best of our knowledge, this is the first work that\nleverages the difference between a predicted future frame and its ground truth\nto detect an abnormal event. To predict a future frame with higher quality for\nnormal events, other than the commonly used appearance (spatial) constraints on\nintensity and gradient, we also introduce a motion (temporal) constraint in\nvideo prediction by enforcing the optical flow between predicted frames and\nground truth frames to be consistent, and this is the first work that\nintroduces a temporal constraint into the video prediction task. Such spatial\nand motion constraints facilitate the future frame prediction for normal\nevents, and consequently facilitate to identify those abnormal events that do\nnot conform the expectation. Extensive experiments on both a toy dataset and\nsome publicly available datasets validate the effectiveness of our method in\nterms of robustness to the uncertainty in normal events and the sensitivity to\nabnormal events. \n\n"}
{"id": "1801.00318", "contents": "Title: Towards Building an Intelligent Anti-Malware System: A Deep Learning\n  Approach using Support Vector Machine (SVM) for Malware Classification Abstract: Effective and efficient mitigation of malware is a long-time endeavor in the\ninformation security community. The development of an anti-malware system that\ncan counteract an unknown malware is a prolific activity that may benefit\nseveral sectors. We envision an intelligent anti-malware system that utilizes\nthe power of deep learning (DL) models. Using such models would enable the\ndetection of newly-released malware through mathematical generalization. That\nis, finding the relationship between a given malware $x$ and its corresponding\nmalware family $y$, $f: x \\mapsto y$. To accomplish this feat, we used the\nMalimg dataset (Nataraj et al., 2011) which consists of malware images that\nwere processed from malware binaries, and then we trained the following DL\nmodels 1 to classify each malware family: CNN-SVM (Tang, 2013), GRU-SVM\n(Agarap, 2017), and MLP-SVM. Empirical evidence has shown that the GRU-SVM\nstands out among the DL models with a predictive accuracy of ~84.92%. This\nstands to reason for the mentioned model had the relatively most sophisticated\narchitecture design among the presented models. The exploration of an even more\noptimal DL-SVM model is the next stage towards the engineering of an\nintelligent anti-malware system. \n\n"}
{"id": "1801.01423", "contents": "Title: Overcoming catastrophic forgetting with hard attention to the task Abstract: Catastrophic forgetting occurs when a neural network loses the information\nlearned in a previous task after training on subsequent tasks. This problem\nremains a hurdle for artificial intelligence systems with sequential learning\ncapabilities. In this paper, we propose a task-based hard attention mechanism\nthat preserves previous tasks' information without affecting the current task's\nlearning. A hard attention mask is learned concurrently to every task, through\nstochastic gradient descent, and previous masks are exploited to condition such\nlearning. We show that the proposed mechanism is effective for reducing\ncatastrophic forgetting, cutting current rates by 45 to 80%. We also show that\nit is robust to different hyperparameter choices, and that it offers a number\nof monitoring capabilities. The approach features the possibility to control\nboth the stability and compactness of the learned knowledge, which we believe\nmakes it also attractive for online learning or network compression\napplications. \n\n"}
{"id": "1801.01442", "contents": "Title: ObamaNet: Photo-realistic lip-sync from text Abstract: We present ObamaNet, the first architecture that generates both audio and\nsynchronized photo-realistic lip-sync videos from any new text. Contrary to\nother published lip-sync approaches, ours is only composed of fully trainable\nneural modules and does not rely on any traditional computer graphics methods.\nMore precisely, we use three main modules: a text-to-speech network based on\nChar2Wav, a time-delayed LSTM to generate mouth-keypoints synced to the audio,\nand a network based on Pix2Pix to generate the video frames conditioned on the\nkeypoints. \n\n"}
{"id": "1801.02101", "contents": "Title: Improving utility of brain tumor confocal laser endomicroscopy:\n  objective value assessment and diagnostic frame detection with convolutional\n  neural networks Abstract: Confocal laser endomicroscopy (CLE), although capable of obtaining images at\ncellular resolution during surgery of brain tumors in real time, creates as\nmany non-diagnostic as diagnostic images. Non-useful images are often distorted\ndue to relative motion between probe and brain or blood artifacts. Many images,\nhowever, simply lack diagnostic features immediately informative to the\nphysician. Examining all the hundreds or thousands of images from a single case\nto discriminate diagnostic images from nondiagnostic ones can be tedious.\nProviding a real-time diagnostic value assessment of images (fast enough to be\nused during the surgical acquisition process and accurate enough for the\npathologist to rely on) to automatically detect diagnostic frames would\nstreamline the analysis of images and filter useful images for the\npathologist/surgeon. We sought to automatically classify images as diagnostic\nor non-diagnostic. AlexNet, a deep-learning architecture, was used in a 4-fold\ncross validation manner. Our dataset includes 16,795 images (8572 nondiagnostic\nand 8223 diagnostic) from 74 CLE-aided brain tumor surgery patients. The ground\ntruth for all the images is provided by the pathologist. Average model accuracy\non test data was 91% overall (90.79 % accuracy, 90.94 % sensitivity and 90.87 %\nspecificity). To evaluate the model reliability we also performed receiver\noperating characteristic (ROC) analysis yielding 0.958 average for the area\nunder ROC curve (AUC). These results demonstrate that a deeply trained AlexNet\nnetwork can achieve a model that reliably and quickly recognizes diagnostic CLE\nimages. \n\n"}
{"id": "1801.02251", "contents": "Title: Graph Autoencoder-Based Unsupervised Feature Selection with Broad and\n  Local Data Structure Preservation Abstract: Feature selection is a dimensionality reduction technique that selects a\nsubset of representative features from high dimensional data by eliminating\nirrelevant and redundant features. Recently, feature selection combined with\nsparse learning has attracted significant attention due to its outstanding\nperformance compared with traditional feature selection methods that ignores\ncorrelation between features. These works first map data onto a low-dimensional\nsubspace and then select features by posing a sparsity constraint on the\ntransformation matrix. However, they are restricted by design to linear data\ntransformation, a potential drawback given that the underlying correlation\nstructures of data are often non-linear. To leverage a more sophisticated\nembedding, we propose an autoencoder-based unsupervised feature selection\napproach that leverages a single-layer autoencoder for a joint framework of\nfeature selection and manifold learning. More specifically, we enforce column\nsparsity on the weight matrix connecting the input layer and the hidden layer,\nas in previous work. Additionally, we include spectral graph analysis on the\nprojected data into the learning process to achieve local data geometry\npreservation from the original data space to the low-dimensional feature space.\nExtensive experiments are conducted on image, audio, text, and biological data.\nThe promising experimental results validate the superiority of the proposed\nmethod. \n\n"}
{"id": "1801.02613", "contents": "Title: Characterizing Adversarial Subspaces Using Local Intrinsic\n  Dimensionality Abstract: Deep Neural Networks (DNNs) have recently been shown to be vulnerable against\nadversarial examples, which are carefully crafted instances that can mislead\nDNNs to make errors during prediction. To better understand such attacks, a\ncharacterization is needed of the properties of regions (the so-called\n'adversarial subspaces') in which adversarial examples lie. We tackle this\nchallenge by characterizing the dimensional properties of adversarial regions,\nvia the use of Local Intrinsic Dimensionality (LID). LID assesses the\nspace-filling capability of the region surrounding a reference example, based\non the distance distribution of the example to its neighbors. We first provide\nexplanations about how adversarial perturbation can affect the LID\ncharacteristic of adversarial regions, and then show empirically that LID\ncharacteristics can facilitate the distinction of adversarial examples\ngenerated using state-of-the-art attacks. As a proof-of-concept, we show that a\npotential application of LID is to distinguish adversarial examples, and the\npreliminary results show that it can outperform several state-of-the-art\ndetection measures by large margins for five attack strategies considered in\nthis paper across three benchmark datasets. Our analysis of the LID\ncharacteristic for adversarial regions not only motivates new directions of\neffective adversarial defense, but also opens up more challenges for developing\nnew attacks to better understand the vulnerabilities of DNNs. \n\n"}
{"id": "1801.02765", "contents": "Title: TextBoxes++: A Single-Shot Oriented Scene Text Detector Abstract: Scene text detection is an important step of scene text recognition system\nand also a challenging problem. Different from general object detection, the\nmain challenges of scene text detection lie on arbitrary orientations, small\nsizes, and significantly variant aspect ratios of text in natural images. In\nthis paper, we present an end-to-end trainable fast scene text detector, named\nTextBoxes++, which detects arbitrary-oriented scene text with both high\naccuracy and efficiency in a single network forward pass. No post-processing\nother than an efficient non-maximum suppression is involved. We have evaluated\nthe proposed TextBoxes++ on four public datasets. In all experiments,\nTextBoxes++ outperforms competing methods in terms of text localization\naccuracy and runtime. More specifically, TextBoxes++ achieves an f-measure of\n0.817 at 11.6fps for 1024*1024 ICDAR 2015 Incidental text images, and an\nf-measure of 0.5591 at 19.8fps for 768*768 COCO-Text images. Furthermore,\ncombined with a text recognizer, TextBoxes++ significantly outperforms the\nstate-of-the-art approaches for word spotting and end-to-end text recognition\ntasks on popular benchmarks. Code is available at:\nhttps://github.com/MhLiao/TextBoxes_plusplus \n\n"}
{"id": "1801.02929", "contents": "Title: Data Augmentation by Pairing Samples for Images Classification Abstract: Data augmentation is a widely used technique in many machine learning tasks,\nsuch as image classification, to virtually enlarge the training dataset size\nand avoid overfitting. Traditional data augmentation techniques for image\nclassification tasks create new samples from the original training data by, for\nexample, flipping, distorting, adding a small amount of noise to, or cropping a\npatch from an original image. In this paper, we introduce a simple but\nsurprisingly effective data augmentation technique for image classification\ntasks. With our technique, named SamplePairing, we synthesize a new sample from\none image by overlaying another image randomly chosen from the training data\n(i.e., taking an average of two images for each pixel). By using two images\nrandomly selected from the training set, we can generate $N^2$ new samples from\n$N$ training samples. This simple data augmentation technique significantly\nimproved classification accuracy for all the tested datasets; for example, the\ntop-1 error rate was reduced from 33.5% to 29.0% for the ILSVRC 2012 dataset\nwith GoogLeNet and from 8.22% to 6.93% in the CIFAR-10 dataset. We also show\nthat our SamplePairing technique largely improved accuracy when the number of\nsamples in the training set was very small. Therefore, our technique is more\nvaluable for tasks with a limited amount of training data, such as medical\nimaging tasks. \n\n"}
{"id": "1801.04065", "contents": "Title: Deep Stereo Matching with Explicit Cost Aggregation Sub-Architecture Abstract: Deep neural networks have shown excellent performance for stereo matching.\nMany efforts focus on the feature extraction and similarity measurement of the\nmatching cost computation step while less attention is paid on cost aggregation\nwhich is crucial for stereo matching. In this paper, we present a\nlearning-based cost aggregation method for stereo matching by a novel\nsub-architecture in the end-to-end trainable pipeline. We reformulate the cost\naggregation as a learning process of the generation and selection of cost\naggregation proposals which indicate the possible cost aggregation results. The\ncost aggregation sub-architecture is realized by a two-stream network: one for\nthe generation of cost aggregation proposals, the other for the selection of\nthe proposals. The criterion for the selection is determined by the low-level\nstructure information obtained from a light convolutional network. The\ntwo-stream network offers a global view guidance for the cost aggregation to\nrectify the mismatching value stemming from the limited view of the matching\ncost computation. The comprehensive experiments on challenge datasets such as\nKITTI and Scene Flow show that our method outperforms the state-of-the-art\nmethods. \n\n"}
{"id": "1801.05401", "contents": "Title: Low-Shot Learning from Imaginary Data Abstract: Humans can quickly learn new visual concepts, perhaps because they can easily\nvisualize or imagine what novel objects look like from different views.\nIncorporating this ability to hallucinate novel instances of new concepts might\nhelp machine vision systems perform better low-shot learning, i.e., learning\nconcepts from few examples. We present a novel approach to low-shot learning\nthat uses this idea. Our approach builds on recent progress in meta-learning\n(\"learning to learn\") by combining a meta-learner with a \"hallucinator\" that\nproduces additional training examples, and optimizing both models jointly. Our\nhallucinator can be incorporated into a variety of meta-learners and provides\nsignificant gains: up to a 6 point boost in classification accuracy when only a\nsingle training example is available, yielding state-of-the-art performance on\nthe challenging ImageNet low-shot classification benchmark. \n\n"}
{"id": "1801.05606", "contents": "Title: Multi-View Stereo 3D Edge Reconstruction Abstract: This paper presents a novel method for the reconstruction of 3D edges in\nmulti-view stereo scenarios. Previous research in the field typically relied on\nvideo sequences and limited the reconstruction process to either straight\nline-segments, or edge-points, i.e., 3D points that correspond to image edges.\nWe instead propose a system, denoted as EdgeGraph3D, able to recover both\nstraight and curved 3D edges from an unordered image sequence. A second\ncontribution of this work is a graph-based representation for 2D edges that\nallows the identification of the most structurally significant edges detected\nin an image. We integrate EdgeGraph3D in a multi-view stereo reconstruction\npipeline and analyze the benefits provided by 3D edges to the accuracy of the\nrecovered surfaces. We evaluate the effectiveness of our approach on multiple\ndatasets from two different collections in the multi-view stereo literature.\nExperimental results demonstrate the ability of EdgeGraph3D to work in presence\nof strong illumination changes and reflections, which are usually detrimental\nto the effectiveness of classical photometric reconstruction systems. \n\n"}
{"id": "1801.06761", "contents": "Title: PU-Net: Point Cloud Upsampling Network Abstract: Learning and analyzing 3D point clouds with deep networks is challenging due\nto the sparseness and irregularity of the data. In this paper, we present a\ndata-driven point cloud upsampling technique. The key idea is to learn\nmulti-level features per point and expand the point set via a multi-branch\nconvolution unit implicitly in feature space. The expanded feature is then\nsplit to a multitude of features, which are then reconstructed to an upsampled\npoint set. Our network is applied at a patch-level, with a joint loss function\nthat encourages the upsampled points to remain on the underlying surface with a\nuniform distribution. We conduct various experiments using synthesis and scan\ndata to evaluate our method and demonstrate its superiority over some baseline\nmethods and an optimization-based method. Results show that our upsampled\npoints have better uniformity and are located closer to the underlying\nsurfaces. \n\n"}
{"id": "1801.06801", "contents": "Title: Curvature-based Comparison of Two Neural Networks Abstract: In this paper we show the similarities and differences of two deep neural\nnetworks by comparing the manifolds composed of activation vectors in each\nfully connected layer of them. The main contribution of this paper includes 1)\na new data generating algorithm which is crucial for determining the dimension\nof manifolds; 2) a systematic strategy to compare manifolds. Especially, we\ntake Riemann curvature and sectional curvature as part of criterion, which can\nreflect the intrinsic geometric properties of manifolds. Some interesting\nresults and phenomenon are given, which help in specifying the similarities and\ndifferences between the features extracted by two networks and demystifying the\nintrinsic mechanism of deep neural networks. \n\n"}
{"id": "1801.07451", "contents": "Title: Novel digital tissue phenotypic signatures of distant metastasis in\n  colorectal cancer Abstract: Distant metastasis is the major cause of death in colorectal cancer (CRC).\nPatients at high risk of developing distant metastasis could benefit from\nappropriate adjuvant and follow-up treatments if stratified accurately at an\nearly stage of the disease. Studies have increasingly recognized the role of\ndiverse cellular components within the tumor microenvironment in the\ndevelopment and progression of CRC tumors. In this paper, we show that a new\nmethod of automated analysis of digitized images from colorectal cancer tissue\nslides can provide important estimates of distant metastasis-free survival\n(DMFS, the time before metastasis is first observed) on the basis of details of\nthe microenvironment. Specifically, we determine what cell types are found in\nthe vicinity of other cell types, and in what numbers, rather than\nconcentrating exclusively on the cancerous cells. We then extract novel tissue\nphenotypic signatures using statistical measurements about tissue composition.\nSuch signatures can underpin clinical decisions about the advisability of\nvarious types of adjuvant therapy. \n\n"}
{"id": "1801.07729", "contents": "Title: The Shape of Art History in the Eyes of the Machine Abstract: How does the machine classify styles in art? And how does it relate to art\nhistorians' methods for analyzing style? Several studies have shown the ability\nof the machine to learn and predict style categories, such as Renaissance,\nBaroque, Impressionism, etc., from images of paintings. This implies that the\nmachine can learn an internal representation encoding discriminative features\nthrough its visual analysis. However, such a representation is not necessarily\ninterpretable. We conducted a comprehensive study of several of the\nstate-of-the-art convolutional neural networks applied to the task of style\nclassification on 77K images of paintings, and analyzed the learned\nrepresentation through correlation analysis with concepts derived from art\nhistory. Surprisingly, the networks could place the works of art in a smooth\ntemporal arrangement mainly based on learning style labels, without any a\npriori knowledge of time of creation, the historical time and context of\nstyles, or relations between styles. The learned representations showed that\nthere are few underlying factors that explain the visual variations of style in\nart. Some of these factors were found to correlate with style patterns\nsuggested by Heinrich W\\\"olfflin (1846-1945). The learned representations also\nconsistently highlighted certain artists as the extreme distinctive\nrepresentative of their styles, which quantitatively confirms art historian\nobservations. \n\n"}
{"id": "1801.07848", "contents": "Title: Feeding Hand-Crafted Features for Enhancing the Performance of\n  Convolutional Neural Networks Abstract: Since the convolutional neural network (CNN) is be- lieved to find right\nfeatures for a given problem, the study of hand-crafted features is somewhat\nneglected these days. In this paper, we show that finding an appropriate\nfeature for the given problem may be still important as they can en- hance the\nperformance of CNN-based algorithms. Specif- ically, we show that feeding an\nappropriate feature to the CNN enhances its performance in some face related\nworks such as age/gender estimation, face detection and emotion recognition. We\nuse Gabor filter bank responses for these tasks, feeding them to the CNN along\nwith the input image. The stack of image and Gabor responses can be fed to the\nCNN as a tensor input, or as a fused image which is a weighted sum of image and\nGabor responses. The Gabor filter parameters can also be tuned depending on the\ngiven problem, for increasing the performance. From the extensive experiments,\nit is shown that the proposed methods provide better performance than the\nconventional CNN-based methods that use only the input images. \n\n"}
{"id": "1801.07853", "contents": "Title: Structured Triplet Learning with POS-tag Guided Attention for Visual\n  Question Answering Abstract: Visual question answering (VQA) is of significant interest due to its\npotential to be a strong test of image understanding systems and to probe the\nconnection between language and vision. Despite much recent progress, general\nVQA is far from a solved problem. In this paper, we focus on the VQA\nmultiple-choice task, and provide some good practices for designing an\neffective VQA model that can capture language-vision interactions and perform\njoint reasoning. We explore mechanisms of incorporating part-of-speech (POS)\ntag guided attention, convolutional n-grams, triplet attention interactions\nbetween the image, question and candidate answer, and structured learning for\ntriplets based on image-question pairs. We evaluate our models on two popular\ndatasets: Visual7W and VQA Real Multiple Choice. Our final model achieves the\nstate-of-the-art performance of 68.2% on Visual7W, and a very competitive\nperformance of 69.6% on the test-standard split of VQA Real Multiple Choice. \n\n"}
{"id": "1801.08925", "contents": "Title: Supersaliency: A Novel Pipeline for Predicting Smooth Pursuit-Based\n  Attention Improves Generalizability of Video Saliency Abstract: Predicting attention is a popular topic at the intersection of human and\ncomputer vision. However, even though most of the available video saliency data\nsets and models claim to target human observers' fixations, they fail to\ndifferentiate them from smooth pursuit (SP), a major eye movement type that is\nunique to perception of dynamic scenes. In this work, we highlight the\nimportance of SP and its prediction (which we call supersaliency, due to\ngreater selectivity compared to fixations), and aim to make its distinction\nfrom fixations explicit for computational models. To this end, we (i) use\nalgorithmic and manual annotations of SP and fixations for two well-established\nvideo saliency data sets, (ii) train Slicing Convolutional Neural Networks for\nsaliency prediction on either fixation- or SP-salient locations, and (iii)\nevaluate our and 26 publicly available dynamic saliency models on three data\nsets against traditional saliency and supersaliency ground truth. Overall, our\nmodels outperform the state of the art in both the new supersaliency and the\ntraditional saliency problem settings, for which literature models are\noptimized. Importantly, on two independent data sets, our supersaliency model\nshows greater generalization ability and outperforms all other models, even for\nfixation prediction. \n\n"}
{"id": "1801.09092", "contents": "Title: Interactive Generative Adversarial Networks for Facial Expression\n  Generation in Dyadic Interactions Abstract: A social interaction is a social exchange between two or more\nindividuals,where individuals modify and adjust their behaviors in response to\ntheir interaction partners. Our social interactions are one of most fundamental\naspects of our lives and can profoundly affect our mood, both positively and\nnegatively. With growing interest in virtual reality and avatar-mediated\ninteractions,it is desirable to make these interactions natural and human like\nto promote positive effect in the interactions and applications such as\nintelligent tutoring systems, automated interview systems and e-learning. In\nthis paper, we propose a method to generate facial behaviors for an agent.\nThese behaviors include facial expressions and head pose and they are generated\nconsidering the users affective state. Our models learn semantically meaningful\nrepresentations of the face and generate appropriate and temporally smooth\nfacial behaviors in dyadic interactions. \n\n"}
{"id": "1801.09468", "contents": "Title: DeepSIC: Deep Semantic Image Compression Abstract: Incorporating semantic information into the codecs during image compression\ncan significantly reduce the repetitive computation of fundamental semantic\nanalysis (such as object recognition) in client-side applications. The same\npractice also enable the compressed code to carry the image semantic\ninformation during storage and transmission. In this paper, we propose a\nconcept called Deep Semantic Image Compression (DeepSIC) and put forward two\nnovel architectures that aim to reconstruct the compressed image and generate\ncorresponding semantic representations at the same time. The first architecture\nperforms semantic analysis in the encoding process by reserving a portion of\nthe bits from the compressed code to store the semantic representations. The\nsecond performs semantic analysis in the decoding step with the feature maps\nthat are embedded in the compressed code. In both architectures, the feature\nmaps are shared by the compression and the semantic analytics modules. To\nvalidate our approaches, we conduct experiments on the publicly available\nbenchmarking datasets and achieve promising results. We also provide a thorough\nanalysis of the advantages and disadvantages of the proposed technique. \n\n"}
{"id": "1801.09571", "contents": "Title: End-to-End Fine-Grained Action Segmentation and Recognition Using\n  Conditional Random Field Models and Discriminative Sparse Coding Abstract: Fine-grained action segmentation and recognition is an important yet\nchallenging task. Given a long, untrimmed sequence of kinematic data, the task\nis to classify the action at each time frame and segment the time series into\nthe correct sequence of actions. In this paper, we propose a novel framework\nthat combines a temporal Conditional Random Field (CRF) model with a powerful\nframe-level representation based on discriminative sparse coding. We introduce\nan end-to-end algorithm for jointly learning the weights of the CRF model,\nwhich include action classification and action transition costs, as well as an\novercomplete dictionary of mid-level action primitives. This results in a CRF\nmodel that is driven by sparse coding features obtained using a discriminative\ndictionary that is shared among different actions and adapted to the task of\nstructured output learning. We evaluate our method on three surgical tasks\nusing kinematic data from the JIGSAWS dataset, as well as on a food preparation\ntask using accelerometer data from the 50 Salads dataset. Our results show that\nthe proposed method performs on par or better than state-of-the-art methods. \n\n"}
{"id": "1802.00168", "contents": "Title: Deep Neural Nets with Interpolating Function as Output Activation Abstract: We replace the output layer of deep neural nets, typically the softmax\nfunction, by a novel interpolating function. And we propose end-to-end training\nand testing algorithms for this new architecture. Compared to classical neural\nnets with softmax function as output activation, the surrogate with\ninterpolating function as output activation combines advantages of both deep\nand manifold learning. The new framework demonstrates the following major\nadvantages: First, it is better applicable to the case with insufficient\ntraining data. Second, it significantly improves the generalization accuracy on\na wide variety of networks. The algorithm is implemented in PyTorch, and code\nwill be made publicly available. \n\n"}
{"id": "1802.00209", "contents": "Title: Dual Recurrent Attention Units for Visual Question Answering Abstract: Visual Question Answering (VQA) requires AI models to comprehend data in two\ndomains, vision and text. Current state-of-the-art models use learned attention\nmechanisms to extract relevant information from the input domains to answer a\ncertain question. Thus, robust attention mechanisms are essential for powerful\nVQA models. In this paper, we propose a recurrent attention mechanism and show\nits benefits compared to the traditional convolutional approach. We perform two\nablation studies to evaluate recurrent attention. First, we introduce a\nbaseline VQA model with visual attention and test the performance difference\nbetween convolutional and recurrent attention on the VQA 2.0 dataset. Secondly,\nwe design an architecture for VQA which utilizes dual (textual and visual)\nRecurrent Attention Units (RAUs). Using this model, we show the effect of all\npossible combinations of recurrent and convolutional dual attention. Our single\nmodel outperforms the first place winner on the VQA 2016 challenge and to the\nbest of our knowledge, it is the second best performing single model on the VQA\n1.0 dataset. Furthermore, our model noticeably improves upon the winner of the\nVQA 2017 challenge. Moreover, we experiment replacing attention mechanisms in\nstate-of-the-art models with our RAUs and show increased performance. \n\n"}
{"id": "1802.00931", "contents": "Title: Deep Learning Framework for Multi-class Breast Cancer Histology Image\n  Classification Abstract: In this work, we present a deep learning framework for multi-class breast\ncancer image classification as our submission to the International Conference\non Image Analysis and Recognition (ICIAR) 2018 Grand Challenge on BreAst Cancer\nHistology images (BACH). As these histology images are too large to fit into\nGPU memory, we first propose using Inception V3 to perform patch level\nclassification. The patch level predictions are then passed through an ensemble\nfusion framework involving majority voting, gradient boosting machine (GBM),\nand logistic regression to obtain the image level prediction. We improve the\nsensitivity of the Normal and Benign predicted classes by designing a Dual Path\nNetwork (DPN) to be used as a feature extractor where these extracted features\nare further sent to a second layer of ensemble prediction fusion using GBM,\nlogistic regression, and support vector machine (SVM) to refine predictions.\nExperimental results demonstrate our framework shows a 12.5$\\%$ improvement\nover the state-of-the-art model. \n\n"}
{"id": "1802.00947", "contents": "Title: Ensembling Neural Networks for Digital Pathology Images Classification\n  and Segmentation Abstract: In the last years, neural networks have proven to be a powerful framework for\nvarious image analysis problems. However, some application domains have\nspecific limitations. Notably, digital pathology is an example of such fields\ndue to tremendous image sizes and quite limited number of training examples\navailable. In this paper, we adopt state-of-the-art convolutional neural\nnetworks (CNN) architectures for digital pathology images analysis. We propose\nto classify image patches to increase effective sample size and then to apply\nan ensembling technique to build prediction for the original images. To\nvalidate the developed approaches, we conducted experiments with \\textit{Breast\nCancer Histology Challenge} dataset and obtained 90\\% accuracy for the 4-class\ntissue classification task. \n\n"}
{"id": "1802.00948", "contents": "Title: Resset: A Recurrent Model for Sequence of Sets with Applications to\n  Electronic Medical Records Abstract: Modern healthcare is ripe for disruption by AI. A game changer would be\nautomatic understanding the latent processes from electronic medical records,\nwhich are being collected for billions of people worldwide. However, these\nhealthcare processes are complicated by the interaction between at least three\ndynamic components: the illness which involves multiple diseases, the care\nwhich involves multiple treatments, and the recording practice which is biased\nand erroneous. Existing methods are inadequate in capturing the dynamic\nstructure of care. We propose Resset, an end-to-end recurrent model that reads\nmedical record and predicts future risk. The model adopts the algebraic view in\nthat discrete medical objects are embedded into continuous vectors lying in the\nsame space. We formulate the problem as modeling sequences of sets, a novel\nsetting that have rarely, if not, been addressed. Within Resset, the bag of\ndiseases recorded at each clinic visit is modeled as function of sets. The same\nhold for the bag of treatments. The interaction between the disease bag and the\ntreatment bag at a visit is modeled in several, one of which as residual of\ndiseases minus the treatments. Finally, the health trajectory, which is a\nsequence of visits, is modeled using a recurrent neural network. We report\nresults on over a hundred thousand hospital visits by patients suffered from\ntwo costly chronic diseases -- diabetes and mental health. Resset shows\npromises in multiple predictive tasks such as readmission prediction,\ntreatments recommendation and diseases progression. \n\n"}
{"id": "1802.01666", "contents": "Title: Adviser Networks: Learning What Question to Ask for Human-In-The-Loop\n  Viewpoint Estimation Abstract: Humans have an unparalleled visual intelligence and can overcome visual\nambiguities that machines currently cannot. Recent works have shown that\nincorporating guidance from humans during inference for monocular\nviewpoint-estimation can help overcome difficult cases in which the\ncomputer-alone would have otherwise failed. These hybrid intelligence\napproaches are hence gaining traction. However, deciding what question to ask\nthe human at inference time remains an unknown for these problems.\n  We address this question by formulating it as an Adviser Problem: can we\nlearn a mapping from the input to a specific question to ask the human to\nmaximize the expected positive impact to the overall task? We formulate a\nsolution to the adviser problem for viewpoint estimation using a deep network\nwhere the question asks for the location of a keypoint in the input image. We\nshow that by using the Adviser Network's recommendations, the model and the\nhuman outperforms the previous hybrid-intelligence state-of-the-art by 3.7%,\nand the computer-only state-of-the-art by 5.28% absolute. \n\n"}
{"id": "1802.02213", "contents": "Title: A Multiresolution Convolutional Neural Network with Partial Label\n  Training for Annotating Reflectance Confocal Microscopy Images of Skin Abstract: We describe a new multiresolution \"nested encoder-decoder\" convolutional\nnetwork architecture and use it to annotate morphological patterns in\nreflectance confocal microscopy (RCM) images of human skin for aiding cancer\ndiagnosis. Skin cancers are the most common types of cancers, melanoma being\nthe deadliest among them. RCM is an effective, non-invasive pre-screening tool\nfor skin cancer diagnosis, with the required cellular resolution. However,\nimages are complex, low-contrast, and highly variable, so that clinicians\nrequire months to years of expert-level training to be able to make accurate\nassessments. In this paper, we address classifying 4 key clinically important\nstructural/textural patterns in RCM images. The occurrence and morphology of\nthese patterns are used by clinicians for diagnosis of melanomas. The large\nsize of RCM images, the large variance of pattern size, the large-scale range\nover which patterns appear, the class imbalance in collected images, and the\nlack of fully-labeled images all make this a challenging problem to address,\neven with automated machine learning tools. We designed a novel nested U-net\narchitecture to cope with these challenges, and a selective loss function to\nhandle partial labeling. Trained and tested on 56 melanoma-suspicious,\npartially labeled, 12k x 12k pixel images, our network automatically annotated\ndiagnostic patterns with high sensitivity and specificity, providing consistent\nlabels for unlabeled sections of the test images. Providing such annotation\nwill aid clinicians in achieving diagnostic accuracy, and perhaps more\nimportant, dramatically facilitate clinical training, thus enabling much more\nrapid adoption of RCM into widespread clinical use process. In addition, our\nadaptation of U-net architecture provides an intrinsically multiresolution deep\nnetwork that may be useful in other challenging biomedical image analysis\napplications. \n\n"}
{"id": "1802.02290", "contents": "Title: Spectral Image Visualization Using Generative Adversarial Networks Abstract: Spectral images captured by satellites and radio-telescopes are analyzed to\nobtain information about geological compositions distributions, distant asters\nas well as undersea terrain. Spectral images usually contain tens to hundreds\nof continuous narrow spectral bands and are widely used in various fields. But\nthe vast majority of those image signals are beyond the visible range, which\ncalls for special visualization technique. The visualizations of spectral\nimages shall convey as much information as possible from the original signal\nand facilitate image interpretation. However, most of the existing visualizatio\nmethods display spectral images in false colors, which contradict with human's\nexperience and expectation. In this paper, we present a novel visualization\ngenerative adversarial network (GAN) to display spectral images in natural\ncolors. To achieve our goal, we propose a loss function which consists of an\nadversarial loss and a structure loss. The adversarial loss pushes our solution\nto the natural image distribution using a discriminator network that is trained\nto differentiate between false-color images and natural-color images. We also\nuse a cycle loss as the structure constraint to guarantee structure\nconsistency. Experimental results show that our method is able to generate\nstructure-preserved and natural-looking visualizations. \n\n"}
{"id": "1802.02678", "contents": "Title: Biological Mechanisms for Learning: A Computational Model of Olfactory\n  Learning in the Manduca sexta Moth, with Applications to Neural Nets Abstract: The insect olfactory system, which includes the antennal lobe (AL), mushroom\nbody (MB), and ancillary structures, is a relatively simple neural system\ncapable of learning. Its structural features, which are widespread in\nbiological neural systems, process olfactory stimuli through a cascade of\nnetworks where large dimension shifts occur from stage to stage and where\nsparsity and randomness play a critical role in coding. Learning is partly\nenabled by a neuromodulatory reward mechanism of octopamine stimulation of the\nAL, whose increased activity induces rewiring of the MB through Hebbian\nplasticity. Enforced sparsity in the MB focuses Hebbian growth on neurons that\nare the most important for the representation of the learned odor. Based upon\ncurrent biophysical knowledge, we have constructed an end-to-end computational\nmodel of the Manduca sexta moth olfactory system which includes the interaction\nof the AL and MB under octopamine stimulation. Our model is able to robustly\nlearn new odors, and our simulations of integrate-and-fire neurons match the\nstatistical features of in-vivo firing rate data. From a biological\nperspective, the model provides a valuable tool for examining the role of\nneuromodulators, like octopamine, in learning, and gives insight into critical\ninteractions between sparsity, Hebbian growth, and stimulation during learning.\nOur simulations also inform predictions about structural details of the\nolfactory system that are not currently well-characterized. From a machine\nlearning perspective, the model yields bio-inspired mechanisms that are\npotentially useful in constructing neural nets for rapid learning from very few\nsamples. These mechanisms include high-noise layers, sparse layers as noise\nfilters, and a biologically-plausible optimization method to train the network\nbased on octopamine stimulation, sparse layers, and Hebbian growth. \n\n"}
{"id": "1802.02783", "contents": "Title: Saliency-Enhanced Robust Visual Tracking Abstract: Discrete correlation filter (DCF) based trackers have shown considerable\nsuccess in visual object tracking. These trackers often make use of low to mid\nlevel features such as histogram of gradients (HoG) and mid-layer activations\nfrom convolution neural networks (CNNs). We argue that including semantically\nhigher level information to the tracked features may provide further robustness\nto challenging cases such as viewpoint changes. Deep salient object detection\nis one example of such high level features, as it make use of semantic\ninformation to highlight the important regions in the given scene. In this\nwork, we propose an improvement over DCF based trackers by combining saliency\nbased and other features based filter responses. This combination is performed\nwith an adaptive weight on the saliency based filter responses, which is\nautomatically selected according to the temporal consistency of visual\nsaliency. We show that our method consistently improves a baseline DCF based\ntracker especially in challenging cases and performs superior to the\nstate-of-the-art. Our improved tracker operates at 9.3 fps, introducing a small\ncomputational burden over the baseline which operates at 11 fps. \n\n"}
{"id": "1802.02950", "contents": "Title: Rotate your Networks: Better Weight Consolidation and Less Catastrophic\n  Forgetting Abstract: In this paper we propose an approach to avoiding catastrophic forgetting in\nsequential task learning scenarios. Our technique is based on a network\nreparameterization that approximately diagonalizes the Fisher Information\nMatrix of the network parameters. This reparameterization takes the form of a\nfactorized rotation of parameter space which, when used in conjunction with\nElastic Weight Consolidation (which assumes a diagonal Fisher Information\nMatrix), leads to significantly better performance on lifelong learning of\nsequential tasks. Experimental results on the MNIST, CIFAR-100, CUB-200 and\nStanford-40 datasets demonstrate that we significantly improve the results of\nstandard elastic weight consolidation, and that we obtain competitive results\nwhen compared to other state-of-the-art in lifelong learning without\nforgetting. \n\n"}
{"id": "1802.03209", "contents": "Title: Drift Theory in Continuous Search Spaces: Expected Hitting Time of the\n  (1+1)-ES with 1/5 Success Rule Abstract: This paper explores the use of the standard approach for proving runtime\nbounds in discrete domains---often referred to as drift analysis---in the\ncontext of optimization on a continuous domain. Using this framework we analyze\nthe (1+1) Evolution Strategy with one-fifth success rule on the sphere\nfunction. To deal with potential functions that are not lower-bounded, we\nformulate novel drift theorems. We then use the theorems to prove bounds on the\nexpected hitting time to reach a certain target fitness in finite dimension\n$d$. The bounds are akin to linear convergence. We then study the dependency of\nthe different terms on $d$ proving a convergence rate dependency of\n$\\Theta(1/d)$. Our results constitute the first non-asymptotic analysis for the\nalgorithm considered as well as the first explicit application of drift\nanalysis to a randomized search heuristic with continuous domain. \n\n"}
{"id": "1802.03269", "contents": "Title: Unsupervised Deep Domain Adaptation for Pedestrian Detection Abstract: This paper addresses the problem of unsupervised domain adaptation on the\ntask of pedestrian detection in crowded scenes. First, we utilize an iterative\nalgorithm to iteratively select and auto-annotate positive pedestrian samples\nwith high confidence as the training samples for the target domain. Meanwhile,\nwe also reuse negative samples from the source domain to compensate for the\nimbalance between the amount of positive samples and negative samples. Second,\nbased on the deep network we also design an unsupervised regularizer to\nmitigate influence from data noise. More specifically, we transform the last\nfully connected layer into two sub-layers - an element-wise multiply layer and\na sum layer, and add the unsupervised regularizer to further improve the domain\nadaptation accuracy. In experiments for pedestrian detection, the proposed\nmethod boosts the recall value by nearly 30% while the precision stays almost\nthe same. Furthermore, we perform our method on standard domain adaptation\nbenchmarks on both supervised and unsupervised settings and also achieve\nstate-of-the-art results. \n\n"}
{"id": "1802.03494", "contents": "Title: AMC: AutoML for Model Compression and Acceleration on Mobile Devices Abstract: Model compression is a critical technique to efficiently deploy neural\nnetwork models on mobile devices which have limited computation resources and\ntight power budgets. Conventional model compression techniques rely on\nhand-crafted heuristics and rule-based policies that require domain experts to\nexplore the large design space trading off among model size, speed, and\naccuracy, which is usually sub-optimal and time-consuming. In this paper, we\npropose AutoML for Model Compression (AMC) which leverage reinforcement\nlearning to provide the model compression policy. This learning-based\ncompression policy outperforms conventional rule-based compression policy by\nhaving higher compression ratio, better preserving the accuracy and freeing\nhuman labor. Under 4x FLOPs reduction, we achieved 2.7% better accuracy than\nthe handcrafted model compression policy for VGG-16 on ImageNet. We applied\nthis automated, push-the-button compression pipeline to MobileNet and achieved\n1.81x speedup of measured inference latency on an Android phone and 1.43x\nspeedup on the Titan XP GPU, with only 0.1% loss of ImageNet Top-1 accuracy. \n\n"}
{"id": "1802.03528", "contents": "Title: Coverless information hiding based on Generative Model Abstract: A new coverless image information hiding method based on generative model is\nproposed, we feed the secret image to the generative model database, and\ngenerate a meaning-normal and independent image different from the secret\nimage, then, the generated image is transmitted to the receiver and is fed to\nthe generative model database to generate another image visually the same as\nthe secret image. So we only need to transmit the meaning-normal image which is\nnot related to the secret image, and we can achieve the same effect as the\ntransmission of the secret image. This is the first time to propose the\ncoverless image information hiding method based on generative model, compared\nwith the traditional image steganography, the transmitted image does not embed\nany information of the secret image in this method, therefore, can effectively\nresist steganalysis tools. Experimental results show that our method has high\ncapacity, safety and reliability. \n\n"}
{"id": "1802.03620", "contents": "Title: Optimal approximation of continuous functions by very deep ReLU networks Abstract: We consider approximations of general continuous functions on\nfinite-dimensional cubes by general deep ReLU neural networks and study the\napproximation rates with respect to the modulus of continuity of the function\nand the total number of weights $W$ in the network. We establish the complete\nphase diagram of feasible approximation rates and show that it includes two\ndistinct phases. One phase corresponds to slower approximations that can be\nachieved with constant-depth networks and continuous weight assignments. The\nother phase provides faster approximations at the cost of depths necessarily\ngrowing as a power law $L\\sim W^{\\alpha}, 0<\\alpha\\le 1,$ and with necessarily\ndiscontinuous weight assignments. In particular, we prove that constant-width\nfully-connected networks of depth $L\\sim W$ provide the fastest possible\napproximation rate $\\|f-\\widetilde f\\|_\\infty = O(\\omega_f(O(W^{-2/\\nu})))$\nthat cannot be achieved with less deep networks. \n\n"}
{"id": "1802.03750", "contents": "Title: FD-MobileNet: Improved MobileNet with a Fast Downsampling Strategy Abstract: We present Fast-Downsampling MobileNet (FD-MobileNet), an efficient and\naccurate network for very limited computational budgets (e.g., 10-140 MFLOPs).\nOur key idea is applying an aggressive downsampling strategy to MobileNet\nframework. In FD-MobileNet, we perform 32$\\times$ downsampling within 12\nlayers, only half the layers in the original MobileNet. This design brings\nthree advantages: (i) It remarkably reduces the computational cost. (ii) It\nincreases the information capacity and achieves significant performance\nimprovements. (iii) It is engineering-friendly and provides fast actual\ninference speed. Experiments on ILSVRC 2012 and PASCAL VOC 2007 datasets\ndemonstrate that FD-MobileNet consistently outperforms MobileNet and achieves\ncomparable results with ShuffleNet under different computational budgets, for\ninstance, surpassing MobileNet by 5.5% on the ILSVRC 2012 top-1 accuracy and\n3.6% on the VOC 2007 mAP under a complexity of 12 MFLOPs. On an ARM-based\ndevice, FD-MobileNet achieves 1.11$\\times$ inference speedup over MobileNet and\n1.82$\\times$ over ShuffleNet under the same complexity. \n\n"}
{"id": "1802.05668", "contents": "Title: Model compression via distillation and quantization Abstract: Deep neural networks (DNNs) continue to make significant advances, solving\ntasks from image classification to translation or reinforcement learning. One\naspect of the field receiving considerable attention is efficiently executing\ndeep models in resource-constrained environments, such as mobile or embedded\ndevices. This paper focuses on this problem, and proposes two new compression\nmethods, which jointly leverage weight quantization and distillation of larger\nteacher networks into smaller student networks. The first method we propose is\ncalled quantized distillation and leverages distillation during the training\nprocess, by incorporating distillation loss, expressed with respect to the\nteacher, into the training of a student network whose weights are quantized to\na limited set of levels. The second method, differentiable quantization,\noptimizes the location of quantization points through stochastic gradient\ndescent, to better fit the behavior of the teacher model. We validate both\nmethods through experiments on convolutional and recurrent architectures. We\nshow that quantized shallow students can reach similar accuracy levels to\nfull-precision teacher models, while providing order of magnitude compression,\nand inference speedup that is linear in the depth reduction. In sum, our\nresults enable DNNs for resource-constrained environments to leverage\narchitecture and accuracy advances developed on more powerful devices. \n\n"}
{"id": "1802.05766", "contents": "Title: Learning to Count Objects in Natural Images for Visual Question\n  Answering Abstract: Visual Question Answering (VQA) models have struggled with counting objects\nin natural images so far. We identify a fundamental problem due to soft\nattention in these models as a cause. To circumvent this problem, we propose a\nneural network component that allows robust counting from object proposals.\nExperiments on a toy task show the effectiveness of this component and we\nobtain state-of-the-art accuracy on the number category of the VQA v2 dataset\nwithout negatively affecting other categories, even outperforming ensemble\nmodels with our single model. On a difficult balanced pair metric, the\ncomponent gives a substantial improvement in counting over a strong baseline by\n6.6%. \n\n"}
{"id": "1802.05891", "contents": "Title: Training Deep Face Recognition Systems with Synthetic Data Abstract: Recent advances in deep learning have significantly increased the performance\nof face recognition systems. The performance and reliability of these models\ndepend heavily on the amount and quality of the training data. However, the\ncollection of annotated large datasets does not scale well and the control over\nthe quality of the data decreases with the size of the dataset. In this work,\nwe explore how synthetically generated data can be used to decrease the number\nof real-world images needed for training deep face recognition systems. In\nparticular, we make use of a 3D morphable face model for the generation of\nimages with arbitrary amounts of facial identities and with full control over\nimage variations, such as pose, illumination, and background. In our\nexperiments with an off-the-shelf face recognition software we observe the\nfollowing phenomena: 1) The amount of real training data needed to train\ncompetitive deep face recognition systems can be reduced significantly. 2)\nCombining large-scale real-world data with synthetic data leads to an increased\nperformance. 3) Models trained only on synthetic data with strong variations in\npose, illumination, and background perform very well across different datasets\neven without dataset adaptation. 4) The real-to-virtual performance gap can be\nclosed when using synthetic data for pre-training, followed by fine-tuning with\nreal-world images. 5) There are no observable negative effects of pre-training\nwith synthetic data. Thus, any face recognition system in our experiments\nbenefits from using synthetic face images. The synthetic data generator, as\nwell as all experiments, are publicly available. \n\n"}
{"id": "1802.06140", "contents": "Title: Real-Time 3D Shape of Micro-Details Abstract: Motivated by the growing demand for interactive environments, we propose an\naccurate real-time 3D shape reconstruction technique. To provide a reliable 3D\nreconstruction which is still a challenging task when dealing with real-world\napplications, we integrate several components including (i) Photometric Stereo\n(PS), (ii) perspective Cook-Torrance reflectance model that enables PS to deal\nwith a broad range of possible real-world object reflections, (iii) realistic\nlightening situation, (iv) a Recurrent Optimization Network (RON) and finally\n(v) heuristic Dijkstra Gaussian Mean Curvature (DGMC) initialization approach.\nWe demonstrate the potential benefits of our hybrid model by providing 3D shape\nwith highly-detailed information from micro-prints for the first time. All\nreal-world images are taken by a mobile phone camera under a simple setup as a\nconsumer-level equipment. In addition, complementary synthetic experiments\nconfirm the beneficial properties of our novel method and its superiority over\nthe state-of-the-art approaches. \n\n"}
{"id": "1802.06454", "contents": "Title: DA-GAN: Instance-level Image Translation by Deep Attention Generative\n  Adversarial Networks (with Supplementary Materials) Abstract: Unsupervised image translation, which aims in translating two independent\nsets of images, is challenging in discovering the correct correspondences\nwithout paired data. Existing works build upon Generative Adversarial Network\n(GAN) such that the distribution of the translated images are indistinguishable\nfrom the distribution of the target set. However, such set-level constraints\ncannot learn the instance-level correspondences (e.g. aligned semantic parts in\nobject configuration task). This limitation often results in false positives\n(e.g. geometric or semantic artifacts), and further leads to mode collapse\nproblem. To address the above issues, we propose a novel framework for\ninstance-level image translation by Deep Attention GAN (DA-GAN). Such a design\nenables DA-GAN to decompose the task of translating samples from two sets into\ntranslating instances in a highly-structured latent space. Specifically, we\njointly learn a deep attention encoder, and the instancelevel correspondences\ncould be consequently discovered through attending on the learned instance\npairs. Therefore, the constraints could be exploited on both set-level and\ninstance-level. Comparisons against several state-ofthe- arts demonstrate the\nsuperiority of our approach, and the broad application capability, e.g, pose\nmorphing, data augmentation, etc., pushes the margin of domain translation\nproblem. \n\n"}
{"id": "1802.07421", "contents": "Title: Conditional Adversarial Synthesis of 3D Facial Action Units Abstract: Employing deep learning-based approaches for fine-grained facial expression\nanalysis, such as those involving the estimation of Action Unit (AU)\nintensities, is difficult due to the lack of a large-scale dataset of real\nfaces with sufficiently diverse AU labels for training. In this paper, we\nconsider how AU-level facial image synthesis can be used to substantially\naugment such a dataset. We propose an AU synthesis framework that combines the\nwell-known 3D Morphable Model (3DMM), which intrinsically disentangles\nexpression parameters from other face attributes, with models that\nadversarially generate 3DMM expression parameters conditioned on given target\nAU labels, in contrast to the more conventional approach of generating facial\nimages directly. In this way, we are able to synthesize new combinations of\nexpression parameters and facial images from desired AU labels. Extensive\nquantitative and qualitative results on the benchmark DISFA dataset demonstrate\nthe effectiveness of our method on 3DMM facial expression parameter synthesis\nand data augmentation for deep learning-based AU intensity estimation. \n\n"}
{"id": "1802.07461", "contents": "Title: Emergence of Structured Behaviors from Curiosity-Based Intrinsic\n  Motivation Abstract: Infants are experts at playing, with an amazing ability to generate novel\nstructured behaviors in unstructured environments that lack clear extrinsic\nreward signals. We seek to replicate some of these abilities with a neural\nnetwork that implements curiosity-driven intrinsic motivation. Using a simple\nbut ecologically naturalistic simulated environment in which the agent can move\nand interact with objects it sees, the agent learns a world model predicting\nthe dynamic consequences of its actions. Simultaneously, the agent learns to\ntake actions that adversarially challenge the developing world model, pushing\nthe agent to explore novel and informative interactions with its environment.\nWe demonstrate that this policy leads to the self-supervised emergence of a\nspectrum of complex behaviors, including ego motion prediction, object\nattention, and object gathering. Moreover, the world model that the agent\nlearns supports improved performance on object dynamics prediction and\nlocalization tasks. Our results are a proof-of-principle that computational\nmodels of intrinsic motivation might account for key features of developmental\nvisuomotor learning in infants. \n\n"}
{"id": "1802.07856", "contents": "Title: xView: Objects in Context in Overhead Imagery Abstract: We introduce a new large-scale dataset for the advancement of object\ndetection techniques and overhead object detection research. This satellite\nimagery dataset enables research progress pertaining to four key computer\nvision frontiers. We utilize a novel process for geospatial category detection\nand bounding box annotation with three stages of quality control. Our data is\ncollected from WorldView-3 satellites at 0.3m ground sample distance, providing\nhigher resolution imagery than most public satellite imagery datasets. We\ncompare xView to other object detection datasets in both natural and overhead\nimagery domains and then provide a baseline analysis using the Single Shot\nMultiBox Detector. xView is one of the largest and most diverse publicly\navailable object-detection datasets to date, with over 1 million objects across\n60 classes in over 1,400 km^2 of imagery. \n\n"}
{"id": "1802.08686", "contents": "Title: Adversarial vulnerability for any classifier Abstract: Despite achieving impressive performance, state-of-the-art classifiers remain\nhighly vulnerable to small, imperceptible, adversarial perturbations. This\nvulnerability has proven empirically to be very intricate to address. In this\npaper, we study the phenomenon of adversarial perturbations under the\nassumption that the data is generated with a smooth generative model. We derive\nfundamental upper bounds on the robustness to perturbations of any\nclassification function, and prove the existence of adversarial perturbations\nthat transfer well across different classifiers with small risk. Our analysis\nof the robustness also provides insights onto key properties of generative\nmodels, such as their smoothness and dimensionality of latent space. We\nconclude with numerical experimental results showing that our bounds provide\ninformative baselines to the maximal achievable robustness on several datasets. \n\n"}
{"id": "1802.08701", "contents": "Title: Machine learning based hyperspectral image analysis: A survey Abstract: Hyperspectral sensors enable the study of the chemical properties of scene\nmaterials remotely for the purpose of identification, detection, and chemical\ncomposition analysis of objects in the environment. Hence, hyperspectral images\ncaptured from earth observing satellites and aircraft have been increasingly\nimportant in agriculture, environmental monitoring, urban planning, mining, and\ndefense. Machine learning algorithms due to their outstanding predictive power\nhave become a key tool for modern hyperspectral image analysis. Therefore, a\nsolid understanding of machine learning techniques have become essential for\nremote sensing researchers and practitioners. This paper reviews and compares\nrecent machine learning-based hyperspectral image analysis methods published in\nliterature. We organize the methods by the image analysis task and by the type\nof machine learning algorithm, and present a two-way mapping between the image\nanalysis tasks and the types of machine learning algorithms that can be applied\nto them. The paper is comprehensive in coverage of both hyperspectral image\nanalysis tasks and machine learning algorithms. The image analysis tasks\nconsidered are land cover classification, target detection, unmixing, and\nphysical parameter estimation. The machine learning algorithms covered are\nGaussian models, linear regression, logistic regression, support vector\nmachines, Gaussian mixture model, latent linear models, sparse linear models,\nGaussian mixture models, ensemble learning, directed graphical models,\nundirected graphical models, clustering, Gaussian processes, Dirichlet\nprocesses, and deep learning. We also discuss the open challenges in the field\nof hyperspectral image analysis and explore possible future directions. \n\n"}
{"id": "1802.08726", "contents": "Title: Longitudinal Face Aging in the Wild - Recent Deep Learning Approaches Abstract: Face Aging has raised considerable attentions and interest from the computer\nvision community in recent years. Numerous approaches ranging from purely image\nprocessing techniques to deep learning structures have been proposed in\nliterature. In this paper, we aim to give a review of recent developments of\nmodern deep learning based approaches, i.e. Deep Generative Models, for Face\nAging task. Their structures, formulation, learning algorithms as well as\nsynthesized results are also provided with systematic discussions. Moreover,\nthe aging databases used in most methods to learn the aging process are also\nreviewed. \n\n"}
{"id": "1802.08735", "contents": "Title: A DIRT-T Approach to Unsupervised Domain Adaptation Abstract: Domain adaptation refers to the problem of leveraging labeled data in a\nsource domain to learn an accurate model in a target domain where labels are\nscarce or unavailable. A recent approach for finding a common representation of\nthe two domains is via domain adversarial training (Ganin & Lempitsky, 2015),\nwhich attempts to induce a feature extractor that matches the source and target\nfeature distributions in some feature space. However, domain adversarial\ntraining faces two critical limitations: 1) if the feature extraction function\nhas high-capacity, then feature distribution matching is a weak constraint, 2)\nin non-conservative domain adaptation (where no single classifier can perform\nwell in both the source and target domains), training the model to do well on\nthe source domain hurts performance on the target domain. In this paper, we\naddress these issues through the lens of the cluster assumption, i.e., decision\nboundaries should not cross high-density data regions. We propose two novel and\nrelated models: 1) the Virtual Adversarial Domain Adaptation (VADA) model,\nwhich combines domain adversarial training with a penalty term that punishes\nthe violation the cluster assumption; 2) the Decision-boundary Iterative\nRefinement Training with a Teacher (DIRT-T) model, which takes the VADA model\nas initialization and employs natural gradient steps to further minimize the\ncluster assumption violation. Extensive empirical results demonstrate that the\ncombination of these two models significantly improve the state-of-the-art\nperformance on the digit, traffic sign, and Wi-Fi recognition domain adaptation\nbenchmarks. \n\n"}
{"id": "1802.08948", "contents": "Title: Multi-Oriented Scene Text Detection via Corner Localization and Region\n  Segmentation Abstract: Previous deep learning based state-of-the-art scene text detection methods\ncan be roughly classified into two categories. The first category treats scene\ntext as a type of general objects and follows general object detection paradigm\nto localize scene text by regressing the text box locations, but troubled by\nthe arbitrary-orientation and large aspect ratios of scene text. The second one\nsegments text regions directly, but mostly needs complex post processing. In\nthis paper, we present a method that combines the ideas of the two types of\nmethods while avoiding their shortcomings. We propose to detect scene text by\nlocalizing corner points of text bounding boxes and segmenting text regions in\nrelative positions. In inference stage, candidate boxes are generated by\nsampling and grouping corner points, which are further scored by segmentation\nmaps and suppressed by NMS. Compared with previous methods, our method can\nhandle long oriented text naturally and doesn't need complex post processing.\nThe experiments on ICDAR2013, ICDAR2015, MSRA-TD500, MLT and COCO-Text\ndemonstrate that the proposed algorithm achieves better or comparable results\nin both accuracy and efficiency. Based on VGG16, it achieves an F-measure of\n84.3% on ICDAR2015 and 81.5% on MSRA-TD500. \n\n"}
{"id": "1802.09431", "contents": "Title: Self Super-Resolution for Magnetic Resonance Images using Deep Networks Abstract: High resolution magnetic resonance~(MR) imaging~(MRI) is desirable in many\nclinical applications, however, there is a trade-off between resolution, speed\nof acquisition, and noise. It is common for MR images to have worse\nthrough-plane resolution~(slice thickness) than in-plane resolution. In these\nMRI images, high frequency information in the through-plane direction is not\nacquired, and cannot be resolved through interpolation. To address this issue,\nsuper-resolution methods have been developed to enhance spatial resolution. As\nan ill-posed problem, state-of-the-art super-resolution methods rely on the\npresence of external/training atlases to learn the transform from low\nresolution~(LR) images to high resolution~(HR) images. For several reasons,\nsuch HR atlas images are often not available for MRI sequences. This paper\npresents a self super-resolution~(SSR) algorithm, which does not use any\nexternal atlas images, yet can still resolve HR images only reliant on the\nacquired LR image. We use a blurred version of the input image to create\ntraining data for a state-of-the-art super-resolution deep network. The trained\nnetwork is applied to the original input image to estimate the HR image. Our\nSSR result shows a significant improvement on through-plane resolution compared\nto competing SSR methods. \n\n"}
{"id": "1802.09655", "contents": "Title: Translating and Segmenting Multimodal Medical Volumes with Cycle- and\n  Shape-Consistency Generative Adversarial Network Abstract: Synthesized medical images have several important applications, e.g., as an\nintermedium in cross-modality image registration and as supplementary training\nsamples to boost the generalization capability of a classifier. Especially,\nsynthesized computed tomography (CT) data can provide X-ray attenuation map for\nradiation therapy planning. In this work, we propose a generic cross-modality\nsynthesis approach with the following targets: 1) synthesizing realistic\nlooking 3D images using unpaired training data, 2) ensuring consistent\nanatomical structures, which could be changed by geometric distortion in\ncross-modality synthesis and 3) improving volume segmentation by using\nsynthetic data for modalities with limited training samples. We show that these\ngoals can be achieved with an end-to-end 3D convolutional neural network (CNN)\ncomposed of mutually-beneficial generators and segmentors for image synthesis\nand segmentation tasks. The generators are trained with an adversarial loss, a\ncycle-consistency loss, and also a shape-consistency loss, which is supervised\nby segmentors, to reduce the geometric distortion. From the segmentation view,\nthe segmentors are boosted by synthetic data from generators in an online\nmanner. Generators and segmentors prompt each other alternatively in an\nend-to-end training fashion. With extensive experiments on a dataset including\na total of 4,496 CT and magnetic resonance imaging (MRI) cardiovascular\nvolumes, we show both tasks are beneficial to each other and coupling these two\ntasks results in better performance than solving them exclusively. \n\n"}
{"id": "1802.10508", "contents": "Title: Brain Tumor Segmentation and Radiomics Survival Prediction: Contribution\n  to the BRATS 2017 Challenge Abstract: Quantitative analysis of brain tumors is critical for clinical decision\nmaking. While manual segmentation is tedious, time consuming and subjective,\nthis task is at the same time very challenging to solve for automatic\nsegmentation methods. In this paper we present our most recent effort on\ndeveloping a robust segmentation algorithm in the form of a convolutional\nneural network. Our network architecture was inspired by the popular U-Net and\nhas been carefully modified to maximize brain tumor segmentation performance.\nWe use a dice loss function to cope with class imbalances and use extensive\ndata augmentation to successfully prevent overfitting. Our method beats the\ncurrent state of the art on BraTS 2015, is one of the leading methods on the\nBraTS 2017 validation set (dice scores of 0.896, 0.797 and 0.732 for whole\ntumor, tumor core and enhancing tumor, respectively) and achieves very good\nDice scores on the test set (0.858 for whole, 0.775 for core and 0.647 for\nenhancing tumor). We furthermore take part in the survival prediction\nsubchallenge by training an ensemble of a random forest regressor and\nmultilayer perceptrons on shape features describing the tumor subregions. Our\napproach achieves 52.6% accuracy, a Spearman correlation coefficient of 0.496\nand a mean square error of 209607 on the test set. \n\n"}
{"id": "1803.00227", "contents": "Title: WRPN & Apprentice: Methods for Training and Inference using\n  Low-Precision Numerics Abstract: Today's high performance deep learning architectures involve large models\nwith numerous parameters. Low precision numerics has emerged as a popular\ntechnique to reduce both the compute and memory requirements of these large\nmodels. However, lowering precision often leads to accuracy degradation. We\ndescribe three schemes whereby one can both train and do efficient inference\nusing low precision numerics without hurting accuracy. Finally, we describe an\nefficient hardware accelerator that can take advantage of the proposed low\nprecision numerics. \n\n"}
{"id": "1803.00387", "contents": "Title: A General Pipeline for 3D Detection of Vehicles Abstract: Autonomous driving requires 3D perception of vehicles and other objects in\nthe in environment. Much of the current methods support 2D vehicle detection.\nThis paper proposes a flexible pipeline to adopt any 2D detection network and\nfuse it with a 3D point cloud to generate 3D information with minimum changes\nof the 2D detection networks. To identify the 3D box, an effective model\nfitting algorithm is developed based on generalised car models and score maps.\nA two-stage convolutional neural network (CNN) is proposed to refine the\ndetected 3D box. This pipeline is tested on the KITTI dataset using two\ndifferent 2D detection networks. The 3D detection results based on these two\nnetworks are similar, demonstrating the flexibility of the proposed pipeline.\nThe results rank second among the 3D detection algorithms, indicating its\ncompetencies in 3D detection. \n\n"}
{"id": "1803.00657", "contents": "Title: Evolutionary Generative Adversarial Networks Abstract: Generative adversarial networks (GAN) have been effective for learning\ngenerative models for real-world data. However, existing GANs (GAN and its\nvariants) tend to suffer from training problems such as instability and mode\ncollapse. In this paper, we propose a novel GAN framework called evolutionary\ngenerative adversarial networks (E-GAN) for stable GAN training and improved\ngenerative performance. Unlike existing GANs, which employ a pre-defined\nadversarial objective function alternately training a generator and a\ndiscriminator, we utilize different adversarial training objectives as mutation\noperations and evolve a population of generators to adapt to the environment\n(i.e., the discriminator). We also utilize an evaluation mechanism to measure\nthe quality and diversity of generated samples, such that only well-performing\ngenerator(s) are preserved and used for further training. In this way, E-GAN\novercomes the limitations of an individual adversarial training objective and\nalways preserves the best offspring, contributing to progress in and the\nsuccess of GANs. Experiments on several datasets demonstrate that E-GAN\nachieves convincing generative performance and reduces the training problems\ninherent in existing GANs. \n\n"}
{"id": "1803.02579", "contents": "Title: Concurrent Spatial and Channel Squeeze & Excitation in Fully\n  Convolutional Networks Abstract: Fully convolutional neural networks (F-CNNs) have set the state-of-the-art in\nimage segmentation for a plethora of applications. Architectural innovations\nwithin F-CNNs have mainly focused on improving spatial encoding or network\nconnectivity to aid gradient flow. In this paper, we explore an alternate\ndirection of recalibrating the feature maps adaptively, to boost meaningful\nfeatures, while suppressing weak ones. We draw inspiration from the recently\nproposed squeeze & excitation (SE) module for channel recalibration of feature\nmaps for image classification. Towards this end, we introduce three variants of\nSE modules for image segmentation, (i) squeezing spatially and exciting\nchannel-wise (cSE), (ii) squeezing channel-wise and exciting spatially (sSE)\nand (iii) concurrent spatial and channel squeeze & excitation (scSE). We\neffectively incorporate these SE modules within three different\nstate-of-the-art F-CNNs (DenseNet, SD-Net, U-Net) and observe consistent\nimprovement of performance across all architectures, while minimally effecting\nmodel complexity. Evaluations are performed on two challenging applications:\nwhole brain segmentation on MRI scans (Multi-Atlas Labelling Challenge Dataset)\nand organ segmentation on whole body contrast enhanced CT scans (Visceral\nDataset). \n\n"}
{"id": "1803.03341", "contents": "Title: Adversarial Training for Adverse Conditions: Robust Metric Localisation\n  using Appearance Transfer Abstract: We present a method of improving visual place recognition and metric\nlocalisation under very strong appear- ance change. We learn an invertable\ngenerator that can trans- form the conditions of images, e.g. from day to\nnight, summer to winter etc. This image transforming filter is explicitly\ndesigned to aid and abet feature-matching using a new loss based on SURF\ndetector and dense descriptor maps. A network is trained to output synthetic\nimages optimised for feature matching given only an input RGB image, and these\ngenerated images are used to localize the robot against a previously built map\nusing traditional sparse matching approaches. We benchmark our results using\nmultiple traversals of the Oxford RobotCar Dataset over a year-long period,\nusing one traversal as a map and the other to localise. We show that this\nmethod significantly improves place recognition and localisation under changing\nand adverse conditions, while reducing the number of mapping runs needed to\nsuccessfully achieve reliable localisation. \n\n"}
{"id": "1803.04228", "contents": "Title: Omnidirectional CNN for Visual Place Recognition and Navigation Abstract: $ $Visual place recognition is challenging, especially when only a few place\nexemplars are given. To mitigate the challenge, we consider place recognition\nmethod using omnidirectional cameras and propose a novel Omnidirectional\nConvolutional Neural Network (O-CNN) to handle severe camera pose variation.\nGiven a visual input, the task of the O-CNN is not to retrieve the matched\nplace exemplar, but to retrieve the closest place exemplar and estimate the\nrelative distance between the input and the closest place. With the ability to\nestimate relative distance, a heuristic policy is proposed to navigate a robot\nto the retrieved closest place. Note that the network is designed to take\nadvantage of the omnidirectional view by incorporating circular padding and\nrotation invariance. To train a powerful O-CNN, we build a virtual world for\ntraining on a large scale. We also propose a continuous lifted structured\nfeature embedding loss to learn the concept of distance efficiently. Finally,\nour experimental results confirm that our method achieves state-of-the-art\naccuracy and speed with both the virtual world and real-world datasets. \n\n"}
{"id": "1803.04523", "contents": "Title: Event-based Moving Object Detection and Tracking Abstract: Event-based vision sensors, such as the Dynamic Vision Sensor (DVS), are\nideally suited for real-time motion analysis. The unique properties encompassed\nin the readings of such sensors provide high temporal resolution, superior\nsensitivity to light and low latency. These properties provide the grounds to\nestimate motion extremely reliably in the most sophisticated scenarios but they\ncome at a price - modern event-based vision sensors have extremely low\nresolution and produce a lot of noise. Moreover, the asynchronous nature of the\nevent stream calls for novel algorithms.\n  This paper presents a new, efficient approach to object tracking with\nasynchronous cameras. We present a novel event stream representation which\nenables us to utilize information about the dynamic (temporal) component of the\nevent stream, and not only the spatial component, at every moment of time. This\nis done by approximating the 3D geometry of the event stream with a parametric\nmodel; as a result, the algorithm is capable of producing the\nmotion-compensated event stream (effectively approximating egomotion), and\nwithout using any form of external sensors in extremely low-light and noisy\nconditions without any form of feature tracking or explicit optical flow\ncomputation. We demonstrate our framework on the task of independent motion\ndetection and tracking, where we use the temporal model inconsistencies to\nlocate differently moving objects in challenging situations of very fast\nmotion. \n\n"}
{"id": "1803.04792", "contents": "Title: Testing Deep Neural Networks Abstract: Deep neural networks (DNNs) have a wide range of applications, and software\nemploying them must be thoroughly tested, especially in safety-critical\ndomains. However, traditional software test coverage metrics cannot be applied\ndirectly to DNNs. In this paper, inspired by the MC/DC coverage criterion, we\npropose a family of four novel test criteria that are tailored to structural\nfeatures of DNNs and their semantics. We validate the criteria by demonstrating\nthat the generated test inputs guided via our proposed coverage criteria are\nable to capture undesired behaviours in a DNN. Test cases are generated using a\nsymbolic approach and a gradient-based heuristic search. By comparing them with\nexisting methods, we show that our criteria achieve a balance between their\nability to find bugs (proxied using adversarial examples) and the computational\ncost of test case generation. Our experiments are conducted on state-of-the-art\nDNNs obtained using popular open source datasets, including MNIST, CIFAR-10 and\nImageNet. \n\n"}
{"id": "1803.05573", "contents": "Title: Improving GANs Using Optimal Transport Abstract: We present Optimal Transport GAN (OT-GAN), a variant of generative\nadversarial nets minimizing a new metric measuring the distance between the\ngenerator distribution and the data distribution. This metric, which we call\nmini-batch energy distance, combines optimal transport in primal form with an\nenergy distance defined in an adversarially learned feature space, resulting in\na highly discriminative distance function with unbiased mini-batch gradients.\nExperimentally we show OT-GAN to be highly stable when trained with large\nmini-batches, and we present state-of-the-art results on several popular\nbenchmark problems for image generation. \n\n"}
{"id": "1803.05759", "contents": "Title: Salient Region Segmentation Abstract: Saliency prediction is a well studied problem in computer vision. Early\nsaliency models were based on low-level hand-crafted feature derived from\ninsights gained in neuroscience and psychophysics. In the wake of deep learning\nbreakthrough, a new cohort of models were proposed based on neural network\narchitectures, allowing significantly higher gaze prediction than previous\nshallow models, on all metrics.\n  However, most models treat the saliency prediction as a \\textit{regression}\nproblem, and accurate regression of high-dimensional data is known to be a hard\nproblem. Furthermore, it is unclear that intermediate levels of saliency (ie,\nneither very high, nor very low) are meaningful: Something is either salient,\nor it is not.\n  Drawing from those two observations, we reformulate the saliency prediction\nproblem as a salient region \\textit{segmentation} problem. We demonstrate that\nthe reformulation allows for faster convergence than the classical regression\nproblem, while performance is comparable to state-of-the-art.\n  We also visualise the general features learned by the model, which are showed\nto be consistent with insights from psychophysics. \n\n"}
{"id": "1803.05788", "contents": "Title: DeepN-JPEG: A Deep Neural Network Favorable JPEG-based Image Compression\n  Framework Abstract: As one of most fascinating machine learning techniques, deep neural network\n(DNN) has demonstrated excellent performance in various intelligent tasks such\nas image classification. DNN achieves such performance, to a large extent, by\nperforming expensive training over huge volumes of training data. To reduce the\ndata storage and transfer overhead in smart resource-limited Internet-of-Thing\n(IoT) systems, effective data compression is a \"must-have\" feature before\ntransferring real-time produced dataset for training or classification. While\nthere have been many well-known image compression approaches (such as JPEG), we\nfor the first time find that a human-visual based image compression approach\nsuch as JPEG compression is not an optimized solution for DNN systems,\nespecially with high compression ratios. To this end, we develop an image\ncompression framework tailored for DNN applications, named \"DeepN-JPEG\", to\nembrace the nature of deep cascaded information process mechanism of DNN\narchitecture. Extensive experiments, based on \"ImageNet\" dataset with various\nstate-of-the-art DNNs, show that \"DeepN-JPEG\" can achieve ~3.5x higher\ncompression rate over the popular JPEG solution while maintaining the same\naccuracy level for image recognition, demonstrating its great potential of\nstorage and power efficiency in DNN-based smart IoT system design. \n\n"}
{"id": "1803.05796", "contents": "Title: Deep Architectures for Learning Context-dependent Ranking Functions Abstract: Object ranking is an important problem in the realm of preference learning.\nOn the basis of training data in the form of a set of rankings of objects,\nwhich are typically represented as feature vectors, the goal is to learn a\nranking function that predicts a linear order of any new set of objects.\nCurrent approaches commonly focus on ranking by scoring, i.e., on learning an\nunderlying latent utility function that seeks to capture the inherent utility\nof each object. These approaches, however, are not able to take possible\neffects of context-dependence into account, where context-dependence means that\nthe utility or usefulness of an object may also depend on what other objects\nare available as alternatives. In this paper, we formalize the problem of\ncontext-dependent ranking and present two general approaches based on two\nnatural representations of context-dependent ranking functions. Both approaches\nare instantiated by means of appropriate neural network architectures, which\nare evaluated on suitable benchmark task. \n\n"}
{"id": "1803.06049", "contents": "Title: Zero-Shot Object Detection: Learning to Simultaneously Recognize and\n  Localize Novel Concepts Abstract: Current Zero-Shot Learning (ZSL) approaches are restricted to recognition of\na single dominant unseen object category in a test image. We hypothesize that\nthis setting is ill-suited for real-world applications where unseen objects\nappear only as a part of a complex scene, warranting both the `recognition' and\n`localization' of an unseen category. To address this limitation, we introduce\na new \\emph{`Zero-Shot Detection'} (ZSD) problem setting, which aims at\nsimultaneously recognizing and locating object instances belonging to novel\ncategories without any training examples. We also propose a new experimental\nprotocol for ZSD based on the highly challenging ILSVRC dataset, adhering to\npractical issues, e.g., the rarity of unseen objects. To the best of our\nknowledge, this is the first end-to-end deep network for ZSD that jointly\nmodels the interplay between visual and semantic domain information. To\novercome the noise in the automatically derived semantic descriptions, we\nutilize the concept of meta-classes to design an original loss function that\nachieves synergy between max-margin class separation and semantic space\nclustering. Furthermore, we present a baseline approach extended from\nrecognition to detection setting. Our extensive experiments show significant\nperformance boost over the baseline on the imperative yet difficult ZSD\nproblem. \n\n"}
{"id": "1803.06091", "contents": "Title: Salient Objects in Clutter: Bringing Salient Object Detection to the\n  Foreground Abstract: We provide a comprehensive evaluation of salient object detection (SOD)\nmodels. Our analysis identifies a serious design bias of existing SOD datasets\nwhich assumes that each image contains at least one clearly outstanding salient\nobject in low clutter. The design bias has led to a saturated high performance\nfor state-of-the-art SOD models when evaluated on existing datasets. The\nmodels, however, still perform far from being satisfactory when applied to\nreal-world daily scenes. Based on our analyses, we first identify 7 crucial\naspects that a comprehensive and balanced dataset should fulfill. Then, we\npropose a new high quality dataset and update the previous saliency benchmark.\nSpecifically, our SOC (Salient Objects in Clutter) dataset, includes images\nwith salient and non-salient objects from daily object categories. Beyond\nobject category annotations, each salient image is accompanied by attributes\nthat reflect common challenges in real-world scenes. Finally, we report\nattribute-based performance assessment on our dataset. \n\n"}
{"id": "1803.06598", "contents": "Title: Facial Landmarks Detection by Self-Iterative Regression based\n  Landmarks-Attention Network Abstract: Cascaded Regression (CR) based methods have been proposed to solve facial\nlandmarks detection problem, which learn a series of descent directions by\nmultiple cascaded regressors separately trained in coarse and fine stages. They\noutperform the traditional gradient descent based methods in both accuracy and\nrunning speed. However, cascaded regression is not robust enough because each\nregressor's training data comes from the output of previous regressor.\nMoreover, training multiple regressors requires lots of computing resources,\nespecially for deep learning based methods. In this paper, we develop a\nSelf-Iterative Regression (SIR) framework to improve the model efficiency. Only\none self-iterative regressor is trained to learn the descent directions for\nsamples from coarse stages to fine stages, and parameters are iteratively\nupdated by the same regressor. Specifically, we proposed Landmarks-Attention\nNetwork (LAN) as our regressor, which concurrently learns features around each\nlandmark and obtains the holistic location increment. By doing so, not only the\nrest of regressors are removed to simplify the training process, but the number\nof model parameters is significantly decreased. The experiments demonstrate\nthat with only 3.72M model parameters, our proposed method achieves the\nstate-of-the-art performance. \n\n"}
{"id": "1803.06744", "contents": "Title: Fast Neural Architecture Construction using EnvelopeNets Abstract: Fast Neural Architecture Construction (NAC) is a method to construct deep\nnetwork architectures by pruning and expansion of a base network. In recent\nyears, several automated search methods for neural network architectures have\nbeen proposed using methods such as evolutionary algorithms and reinforcement\nlearning. These methods use a single scalar objective function (usually\naccuracy) that is evaluated after a full training and evaluation cycle. In\ncontrast NAC directly compares the utility of different filters using\nstatistics derived from filter featuremaps reach a state where the utility of\ndifferent filters within a network can be compared and hence can be used to\nconstruct networks. The training epochs needed for filters within a network to\nreach this state is much less than the training epochs needed for the accuracy\nof a network to stabilize. NAC exploits this finding to construct convolutional\nneural nets (CNNs) with close to state of the art accuracy, in < 1 GPU day,\nfaster than most of the current neural architecture search methods. The\nconstructed networks show close to state of the art performance on the image\nclassification problem on well known datasets (CIFAR-10, ImageNet) and\nconsistently show better performance than hand constructed and randomly\ngenerated networks of the same depth, operators and approximately the same\nnumber of parameters. \n\n"}
{"id": "1803.06813", "contents": "Title: Weakly Supervised Object Localization on grocery shelves using simple\n  FCN and Synthetic Dataset Abstract: We propose a weakly supervised method using two algorithms to predict object\nbounding boxes given only an image classification dataset. First algorithm is a\nsimple Fully Convolutional Network (FCN) trained to classify object instances.\nWe use the property of FCN to return a mask for images larger than training\nimages to get a primary output segmentation mask during test time by passing an\nimage pyramid to it. We enhance the FCN output mask into final output bounding\nboxes by a Convolutional Encoder-Decoder (ConvAE) viz. the second algorithm.\nConvAE is trained to localize objects on an artificially generated dataset of\noutput segmentation masks. We demonstrate the effectiveness of this method in\nlocalizing objects in grocery shelves where annotating data for object\ndetection is hard due to variety of objects. This method can be extended to any\nproblem domain where collecting images of objects is easy and annotating their\ncoordinates is hard. \n\n"}
{"id": "1803.06959", "contents": "Title: On the importance of single directions for generalization Abstract: Despite their ability to memorize large datasets, deep neural networks often\nachieve good generalization performance. However, the differences between the\nlearned solutions of networks which generalize and those which do not remain\nunclear. Additionally, the tuning properties of single directions (defined as\nthe activation of a single unit or some linear combination of units in response\nto some input) have been highlighted, but their importance has not been\nevaluated. Here, we connect these lines of inquiry to demonstrate that a\nnetwork's reliance on single directions is a good predictor of its\ngeneralization performance, across networks trained on datasets with different\nfractions of corrupted labels, across ensembles of networks trained on datasets\nwith unmodified labels, across different hyperparameters, and over the course\nof training. While dropout only regularizes this quantity up to a point, batch\nnormalization implicitly discourages single direction reliance, in part by\ndecreasing the class selectivity of individual units. Finally, we find that\nclass selectivity is a poor predictor of task importance, suggesting not only\nthat networks which generalize well minimize their dependence on individual\nunits by reducing their selectivity, but also that individually selective units\nmay not be necessary for strong network performance. \n\n"}
{"id": "1803.07702", "contents": "Title: Robust Depth Estimation from Auto Bracketed Images Abstract: As demand for advanced photographic applications on hand-held devices grows,\nthese electronics require the capture of high quality depth. However, under\nlow-light conditions, most devices still suffer from low imaging quality and\ninaccurate depth acquisition. To address the problem, we present a robust depth\nestimation method from a short burst shot with varied intensity (i.e., Auto\nBracketing) or strong noise (i.e., High ISO). We introduce a geometric\ntransformation between flow and depth tailored for burst images, enabling our\nlearning-based multi-view stereo matching to be performed effectively. We then\ndescribe our depth estimation pipeline that incorporates the geometric\ntransformation into our residual-flow network. It allows our framework to\nproduce an accurate depth map even with a bracketed image sequence. We\ndemonstrate that our method outperforms state-of-the-art methods for various\ndatasets captured by a smartphone and a DSLR camera. Moreover, we show that the\nestimated depth is applicable for image quality enhancement and photographic\nediting. \n\n"}
{"id": "1803.07703", "contents": "Title: Weakly Supervised Medical Diagnosis and Localization from Multiple\n  Resolutions Abstract: Diagnostic imaging often requires the simultaneous identification of a\nmultitude of findings of varied size and appearance. Beyond global indication\nof said findings, the prediction and display of localization information\nimproves trust in and understanding of results when augmenting clinical\nworkflow. Medical training data rarely includes more than global image-level\nlabels as segmentations are time-consuming and expensive to collect. We\nintroduce an approach to managing these practical constraints by applying a\nnovel architecture which learns at multiple resolutions while generating\nsaliency maps with weak supervision. Further, we parameterize the Log-Sum-Exp\npooling function with a learnable lower-bounded adaptation (LSE-LBA) to build\nin a sharpness prior and better handle localizing abnormalities of different\nsizes using only image-level labels. Applying this approach to interpreting\nchest x-rays, we set the state of the art on 9 abnormalities in the NIH's CXR14\ndataset while generating saliency maps with the highest resolution to date. \n\n"}
{"id": "1803.07781", "contents": "Title: Exploiting deep residual networks for human action recognition from\n  skeletal data Abstract: The computer vision community is currently focusing on solving action\nrecognition problems in real videos, which contain thousands of samples with\nmany challenges. In this process, Deep Convolutional Neural Networks (D-CNNs)\nhave played a significant role in advancing the state-of-the-art in various\nvision-based action recognition systems. Recently, the introduction of residual\nconnections in conjunction with a more traditional CNN model in a single\narchitecture called Residual Network (ResNet) has shown impressive performance\nand great potential for image recognition tasks. In this paper, we investigate\nand apply deep ResNets for human action recognition using skeletal data\nprovided by depth sensors. Firstly, the 3D coordinates of the human body joints\ncarried in skeleton sequences are transformed into image-based representations\nand stored as RGB images. These color images are able to capture the\nspatial-temporal evolutions of 3D motions from skeleton sequences and can be\nefficiently learned by D-CNNs. We then propose a novel deep learning\narchitecture based on ResNets to learn features from obtained color-based\nrepresentations and classify them into action classes. The proposed method is\nevaluated on three challenging benchmark datasets including MSR Action 3D,\nKARD, and NTU-RGB+D datasets. Experimental results demonstrate that our method\nachieves state-of-the-art performance for all these benchmarks whilst requiring\nless computation resource. In particular, the proposed method surpasses\nprevious approaches by a significant margin of 3.4% on MSR Action 3D dataset,\n0.67% on KARD dataset, and 2.5% on NTU-RGB+D dataset. \n\n"}
{"id": "1803.07870", "contents": "Title: Reservoir computing approaches for representation and classification of\n  multivariate time series Abstract: Classification of multivariate time series (MTS) has been tackled with a\nlarge variety of methodologies and applied to a wide range of scenarios.\nReservoir Computing (RC) provides efficient tools to generate a vectorial,\nfixed-size representation of the MTS that can be further processed by standard\nclassifiers. Despite their unrivaled training speed, MTS classifiers based on a\nstandard RC architecture fail to achieve the same accuracy of fully trainable\nneural networks. In this paper we introduce the reservoir model space, an\nunsupervised approach based on RC to learn vectorial representations of MTS.\nEach MTS is encoded within the parameters of a linear model trained to predict\na low-dimensional embedding of the reservoir dynamics. Compared to other RC\nmethods, our model space yields better representations and attains comparable\ncomputational performance, thanks to an intermediate dimensionality reduction\nprocedure. As a second contribution we propose a modular RC framework for MTS\nclassification, with an associated open-source Python library. The framework\nprovides different modules to seamlessly implement advanced RC architectures.\nThe architectures are compared to other MTS classifiers, including deep\nlearning models and time series kernels. Results obtained on benchmark and\nreal-world MTS datasets show that RC classifiers are dramatically faster and,\nwhen implemented using our proposed representation, also achieve superior\nclassification accuracy. \n\n"}
{"id": "1803.07913", "contents": "Title: HATS: Histograms of Averaged Time Surfaces for Robust Event-based Object\n  Classification Abstract: Event-based cameras have recently drawn the attention of the Computer Vision\ncommunity thanks to their advantages in terms of high temporal resolution, low\npower consumption and high dynamic range, compared to traditional frame-based\ncameras. These properties make event-based cameras an ideal choice for\nautonomous vehicles, robot navigation or UAV vision, among others. However, the\naccuracy of event-based object classification algorithms, which is of crucial\nimportance for any reliable system working in real-world conditions, is still\nfar behind their frame-based counterparts. Two main reasons for this\nperformance gap are: 1. The lack of effective low-level representations and\narchitectures for event-based object classification and 2. The absence of large\nreal-world event-based datasets. In this paper we address both problems. First,\nwe introduce a novel event-based feature representation together with a new\nmachine learning architecture. Compared to previous approaches, we use local\nmemory units to efficiently leverage past temporal information and build a\nrobust event-based representation. Second, we release the first large\nreal-world event-based dataset for object classification. We compare our method\nto the state-of-the-art with extensive experiments, showing better\nclassification performance and real-time computation. \n\n"}
{"id": "1803.08024", "contents": "Title: Stacked Cross Attention for Image-Text Matching Abstract: In this paper, we study the problem of image-text matching. Inferring the\nlatent semantic alignment between objects or other salient stuff (e.g. snow,\nsky, lawn) and the corresponding words in sentences allows to capture\nfine-grained interplay between vision and language, and makes image-text\nmatching more interpretable. Prior work either simply aggregates the similarity\nof all possible pairs of regions and words without attending differentially to\nmore and less important words or regions, or uses a multi-step attentional\nprocess to capture limited number of semantic alignments which is less\ninterpretable. In this paper, we present Stacked Cross Attention to discover\nthe full latent alignments using both image regions and words in a sentence as\ncontext and infer image-text similarity. Our approach achieves the\nstate-of-the-art results on the MS-COCO and Flickr30K datasets. On Flickr30K,\nour approach outperforms the current best methods by 22.1% relatively in text\nretrieval from image query, and 18.2% relatively in image retrieval with text\nquery (based on Recall@1). On MS-COCO, our approach improves sentence retrieval\nby 17.8% relatively and image retrieval by 16.6% relatively (based on Recall@1\nusing the 5K test set). Code has been made available at:\nhttps://github.com/kuanghuei/SCAN. \n\n"}
{"id": "1803.08203", "contents": "Title: Residual Networks: Lyapunov Stability and Convex Decomposition Abstract: While training error of most deep neural networks degrades as the depth of\nthe network increases, residual networks appear to be an exception. We show\nthat the main reason for this is the Lyapunov stability of the gradient descent\nalgorithm: for an arbitrarily chosen step size, the equilibria of the gradient\ndescent are most likely to remain stable for the parametrization of residual\nnetworks. We then present an architecture with a pair of residual networks to\napproximate a large class of functions by decomposing them into a convex and a\nconcave part. Some parameters of this model are shown to change little during\ntraining, and this imperfect optimization prevents overfitting the data and\nleads to solutions with small Lipschitz constants, while providing clues about\nthe generalization of other deep networks. \n\n"}
{"id": "1803.09453", "contents": "Title: CNN in MRF: Video Object Segmentation via Inference in A CNN-Based\n  Higher-Order Spatio-Temporal MRF Abstract: This paper addresses the problem of video object segmentation, where the\ninitial object mask is given in the first frame of an input video. We propose a\nnovel spatio-temporal Markov Random Field (MRF) model defined over pixels to\nhandle this problem. Unlike conventional MRF models, the spatial dependencies\namong pixels in our model are encoded by a Convolutional Neural Network (CNN).\nSpecifically, for a given object, the probability of a labeling to a set of\nspatially neighboring pixels can be predicted by a CNN trained for this\nspecific object. As a result, higher-order, richer dependencies among pixels in\nthe set can be implicitly modeled by the CNN. With temporal dependencies\nestablished by optical flow, the resulting MRF model combines both spatial and\ntemporal cues for tackling video object segmentation. However, performing\ninference in the MRF model is very difficult due to the very high-order\ndependencies. To this end, we propose a novel CNN-embedded algorithm to perform\napproximate inference in the MRF. This algorithm proceeds by alternating\nbetween a temporal fusion step and a feed-forward CNN step. When initialized\nwith an appearance-based one-shot segmentation CNN, our model outperforms the\nwinning entries of the DAVIS 2017 Challenge, without resorting to model\nensembling or any dedicated detectors. \n\n"}
{"id": "1803.09803", "contents": "Title: Generating Talking Face Landmarks from Speech Abstract: The presence of a corresponding talking face has been shown to significantly\nimprove speech intelligibility in noisy conditions and for hearing impaired\npopulation. In this paper, we present a system that can generate landmark\npoints of a talking face from an acoustic speech in real time. The system uses\na long short-term memory (LSTM) network and is trained on frontal videos of 27\ndifferent speakers with automatically extracted face landmarks. After training,\nit can produce talking face landmarks from the acoustic speech of unseen\nspeakers and utterances. The training phase contains three key steps. We first\ntransform landmarks of the first video frame to pin the two eye points into two\npredefined locations and apply the same transformation on all of the following\nvideo frames. We then remove the identity information by transforming the\nlandmarks into a mean face shape across the entire training dataset. Finally,\nwe train an LSTM network that takes the first- and second-order temporal\ndifferences of the log-mel spectrogram as input to predict face landmarks in\neach frame. We evaluate our system using the mean-squared error (MSE) loss of\nlandmarks of lips between predicted and ground-truth landmarks as well as their\nfirst- and second-order temporal differences. We further evaluate our system by\nconducting subjective tests, where the subjects try to distinguish the real and\nfake videos of talking face landmarks. Both tests show promising results. \n\n"}
{"id": "1803.09820", "contents": "Title: A disciplined approach to neural network hyper-parameters: Part 1 --\n  learning rate, batch size, momentum, and weight decay Abstract: Although deep learning has produced dazzling successes for applications of\nimage, speech, and video processing in the past few years, most trainings are\nwith suboptimal hyper-parameters, requiring unnecessarily long training times.\nSetting the hyper-parameters remains a black art that requires years of\nexperience to acquire. This report proposes several efficient ways to set the\nhyper-parameters that significantly reduce training time and improves\nperformance. Specifically, this report shows how to examine the training\nvalidation/test loss function for subtle clues of underfitting and overfitting\nand suggests guidelines for moving toward the optimal balance point. Then it\ndiscusses how to increase/decrease the learning rate/momentum to speed up\ntraining. Our experiments show that it is crucial to balance every manner of\nregularization for each dataset and architecture. Weight decay is used as a\nsample regularizer to show how its optimal value is tightly coupled with the\nlearning rates and momentums. Files to help replicate the results reported here\nare available. \n\n"}
{"id": "1803.09851", "contents": "Title: Attributes as Operators: Factorizing Unseen Attribute-Object\n  Compositions Abstract: We present a new approach to modeling visual attributes. Prior work casts\nattributes in a similar role as objects, learning a latent representation where\nproperties (e.g., sliced) are recognized by classifiers much in the way objects\n(e.g., apple) are. However, this common approach fails to separate the\nattributes observed during training from the objects with which they are\ncomposed, making it ineffectual when encountering new attribute-object\ncompositions. Instead, we propose to model attributes as operators. Our\napproach learns a semantic embedding that explicitly factors out attributes\nfrom their accompanying objects, and also benefits from novel regularizers\nexpressing attribute operators' effects (e.g., blunt should undo the effects of\nsharp). Not only does our approach align conceptually with the linguistic role\nof attributes as modifiers, but it also generalizes to recognize unseen\ncompositions of objects and attributes. We validate our approach on two\nchallenging datasets and demonstrate significant improvements over the\nstate-of-the-art. In addition, we show that not only can our model recognize\nunseen compositions robustly in an open-world setting, it can also generalize\nto compositions where objects themselves were unseen during training. \n\n"}
{"id": "1803.10158", "contents": "Title: End-to-End Learning of Driving Models with Surround-View Cameras and\n  Route Planners Abstract: For human drivers, having rear and side-view mirrors is vital for safe\ndriving. They deliver a more complete view of what is happening around the car.\nHuman drivers also heavily exploit their mental map for navigation.\nNonetheless, several methods have been published that learn driving models with\nonly a front-facing camera and without a route planner. This lack of\ninformation renders the self-driving task quite intractable. We investigate the\nproblem in a more realistic setting, which consists of a surround-view camera\nsystem with eight cameras, a route planner, and a CAN bus reader. In\nparticular, we develop a sensor setup that provides data for a 360-degree view\nof the area surrounding the vehicle, the driving route to the destination, and\nlow-level driving maneuvers (e.g. steering angle and speed) by human drivers.\nWith such a sensor setup we collect a new driving dataset, covering diverse\ndriving scenarios and varying weather/illumination conditions. Finally, we\nlearn a novel driving model by integrating information from the surround-view\ncameras and the route planner. Two route planners are exploited: 1) by\nrepresenting the planned routes on OpenStreetMap as a stack of GPS coordinates,\nand 2) by rendering the planned routes on TomTom Go Mobile and recording the\nprogression into a video. Our experiments show that: 1) 360-degree\nsurround-view cameras help avoid failures made with a single front-view camera,\nin particular for city driving and intersection scenarios; and 2) route\nplanners help the driving task significantly, especially for steering angle\nprediction. \n\n"}
{"id": "1803.10794", "contents": "Title: TrackingNet: A Large-Scale Dataset and Benchmark for Object Tracking in\n  the Wild Abstract: Despite the numerous developments in object tracking, further development of\ncurrent tracking algorithms is limited by small and mostly saturated datasets.\nAs a matter of fact, data-hungry trackers based on deep-learning currently rely\non object detection datasets due to the scarcity of dedicated large-scale\ntracking datasets. In this work, we present TrackingNet, the first large-scale\ndataset and benchmark for object tracking in the wild. We provide more than 30K\nvideos with more than 14 million dense bounding box annotations. Our dataset\ncovers a wide selection of object classes in broad and diverse context. By\nreleasing such a large-scale dataset, we expect deep trackers to further\nimprove and generalize. In addition, we introduce a new benchmark composed of\n500 novel videos, modeled with a distribution similar to our training dataset.\nBy sequestering the annotation of the test set and providing an online\nevaluation server, we provide a fair benchmark for future development of object\ntrackers. Deep trackers fine-tuned on a fraction of our dataset improve their\nperformance by up to 1.6% on OTB100 and up to 1.7% on TrackingNet Test. We\nprovide an extensive benchmark on TrackingNet by evaluating more than 20\ntrackers. Our results suggest that object tracking in the wild is far from\nbeing solved. \n\n"}
{"id": "1803.11217", "contents": "Title: Joint Person Segmentation and Identification in Synchronized First- and\n  Third-person Videos Abstract: In a world of pervasive cameras, public spaces are often captured from\nmultiple perspectives by cameras of different types, both fixed and mobile. An\nimportant problem is to organize these heterogeneous collections of videos by\nfinding connections between them, such as identifying correspondences between\nthe people appearing in the videos and the people holding or wearing the\ncameras. In this paper, we wish to solve two specific problems: (1) given two\nor more synchronized third-person videos of a scene, produce a pixel-level\nsegmentation of each visible person and identify corresponding people across\ndifferent views (i.e., determine who in camera A corresponds with whom in\ncamera B), and (2) given one or more synchronized third-person videos as well\nas a first-person video taken by a mobile or wearable camera, segment and\nidentify the camera wearer in the third-person videos. Unlike previous work\nwhich requires ground truth bounding boxes to estimate the correspondences, we\nperform person segmentation and identification jointly. We find that solving\nthese two problems simultaneously is mutually beneficial, because better\nfine-grained segmentation allows us to better perform matching across views,\nand information from multiple views helps us perform more accurate\nsegmentation. We evaluate our approach on two challenging datasets of\ninteracting people captured from multiple wearable cameras, and show that our\nproposed method performs significantly better than the state-of-the-art on both\nperson segmentation and identification. \n\n"}
{"id": "1803.11241", "contents": "Title: Improve the performance of transfer learning without fine-tuning using\n  dissimilarity-based multi-view learning for breast cancer histology images Abstract: Breast cancer is one of the most common types of cancer and leading\ncancer-related death causes for women. In the context of ICIAR 2018 Grand\nChallenge on Breast Cancer Histology Images, we compare one handcrafted feature\nextractor and five transfer learning feature extractors based on deep learning.\nWe find out that the deep learning networks pretrained on ImageNet have better\nperformance than the popular handcrafted features used for breast cancer\nhistology images. The best feature extractor achieves an average accuracy of\n79.30%. To improve the classification performance, a random forest\ndissimilarity based integration method is used to combine different feature\ngroups together. When the five deep learning feature groups are combined, the\naverage accuracy is improved to 82.90% (best accuracy 85.00%). When handcrafted\nfeatures are combined with the five deep learning feature groups, the average\naccuracy is improved to 87.10% (best accuracy 93.00%). \n\n"}
{"id": "1803.11410", "contents": "Title: The Resistance to Label Noise in K-NN and DNN Depends on its\n  Concentration Abstract: We investigate the classification performance of K-nearest neighbors (K-NN)\nand deep neural networks (DNNs) in the presence of label noise. We first show\nempirically that a DNN's prediction for a given test example depends on the\nlabels of the training examples in its local neighborhood. This motivates us to\nderive a realizable analytic expression that approximates the multi-class K-NN\nclassification error in the presence of label noise, which is of independent\nimportance. We then suggest that the expression for K-NN may serve as a\nfirst-order approximation for the DNN error. Finally, we demonstrate\nempirically the proximity of the developed expression to the observed\nperformance of K-NN and DNN classifiers. Our result may explain the already\nobserved surprising resistance of DNN to some types of label noise. It also\ncharacterizes an important factor of it showing that the more concentrated the\nnoise the greater is the degradation in performance. \n\n"}
{"id": "1804.00021", "contents": "Title: Hierarchical Transfer Convolutional Neural Networks for Image\n  Classification Abstract: In this paper, we address the issue of how to enhance the generalization\nperformance of convolutional neural networks (CNN) in the early learning stage\nfor image classification. This is motivated by real-time applications that\nrequire the generalization performance of CNN to be satisfactory within limited\ntraining time. In order to achieve this, a novel hierarchical transfer CNN\nframework is proposed. It consists of a group of shallow CNNs and a cloud CNN,\nwhere the shallow CNNs are trained firstly and then the first layers of the\ntrained shallow CNNs are used to initialize the first layer of the cloud CNN.\nThis method will boost the generalization performance of the cloud CNN\nsignificantly, especially during the early stage of training. Experiments using\nCIFAR-10 and ImageNet datasets are performed to examine the proposed method.\nResults demonstrate the improvement of testing accuracy is 12% on average and\nas much as 20% for the CIFAR-10 case while 5% testing accuracy improvement for\nthe ImageNet case during the early stage of learning. It is also shown that\nuniversal improvements of testing accuracy are obtained across different\nsettings of dropout and number of shallow CNNs. \n\n"}
{"id": "1804.00236", "contents": "Title: Recognizing Challenging Handwritten Annotations with Fully Convolutional\n  Networks Abstract: This paper introduces a very challenging dataset of historic German documents\nand evaluates Fully Convolutional Neural Network (FCNN) based methods to locate\nhandwritten annotations of any kind in these documents. The handwritten\nannotations can appear in form of underlines and text by using various writing\ninstruments, e.g., the use of pencils makes the data more challenging. We train\nand evaluate various end-to-end semantic segmentation approaches and report the\nresults. The task is to classify the pixels of documents into two classes:\nbackground and handwritten annotation. The best model achieves a mean\nIntersection over Union (IoU) score of 95.6% on the test documents of the\npresented dataset. We also present a comparison of different strategies used\nfor data augmentation and training on our presented dataset. For evaluation, we\nuse the Layout Analysis Evaluator for the ICDAR 2017 Competition on Layout\nAnalysis for Challenging Medieval Manuscripts. \n\n"}
{"id": "1804.00326", "contents": "Title: Seeing Voices and Hearing Faces: Cross-modal biometric matching Abstract: We introduce a seemingly impossible task: given only an audio clip of someone\nspeaking, decide which of two face images is the speaker. In this paper we\nstudy this, and a number of related cross-modal tasks, aimed at answering the\nquestion: how much can we infer from the voice about the face and vice versa?\nWe study this task \"in the wild\", employing the datasets that are now publicly\navailable for face recognition from static images (VGGFace) and speaker\nidentification from audio (VoxCeleb). These provide training and testing\nscenarios for both static and dynamic testing of cross-modal matching. We make\nthe following contributions: (i) we introduce CNN architectures for both binary\nand multi-way cross-modal face and audio matching, (ii) we compare dynamic\ntesting (where video information is available, but the audio is not from the\nsame video) with static testing (where only a single still image is available),\nand (iii) we use human testing as a baseline to calibrate the difficulty of the\ntask. We show that a CNN can indeed be trained to solve this task in both the\nstatic and dynamic scenarios, and is even well above chance on 10-way\nclassification of the face given the voice. The CNN matches human performance\non easy examples (e.g. different gender across faces) but exceeds human\nperformance on more challenging examples (e.g. faces with the same gender, age\nand nationality). \n\n"}
{"id": "1804.00382", "contents": "Title: Attention-based Ensemble for Deep Metric Learning Abstract: Deep metric learning aims to learn an embedding function, modeled as deep\nneural network. This embedding function usually puts semantically similar\nimages close while dissimilar images far from each other in the learned\nembedding space. Recently, ensemble has been applied to deep metric learning to\nyield state-of-the-art results. As one important aspect of ensemble, the\nlearners should be diverse in their feature embeddings. To this end, we propose\nan attention-based ensemble, which uses multiple attention masks, so that each\nlearner can attend to different parts of the object. We also propose a\ndivergence loss, which encourages diversity among the learners. The proposed\nmethod is applied to the standard benchmarks of deep metric learning and\nexperimental results show that it outperforms the state-of-the-art methods by a\nsignificant margin on image retrieval tasks. \n\n"}
{"id": "1804.00393", "contents": "Title: Generative Spatiotemporal Modeling Of Neutrophil Behavior Abstract: Cell motion and appearance have a strong correlation with cell cycle and\ndisease progression. Many contemporary efforts in machine learning utilize\nspatio-temporal models to predict a cell's physical state and, consequently,\nthe advancement of disease. Alternatively, generative models learn the\nunderlying distribution of the data, creating holistic representations that can\nbe used in learning. In this work, we propose an aggregate model that combine\nGenerative Adversarial Networks (GANs) and Autoregressive (AR) models to\npredict cell motion and appearance in human neutrophils imaged by differential\ninterference contrast (DIC) microscopy. We bifurcate the task of learning cell\nstatistics by leveraging GANs for the spatial component and AR models for the\ntemporal component. The aggregate model learned results offer a promising\ncomputational environment for studying changes in organellar shape, quantity,\nand spatial distribution over large sequences. \n\n"}
{"id": "1804.00433", "contents": "Title: SINet: A Scale-insensitive Convolutional Neural Network for Fast Vehicle\n  Detection Abstract: Vision-based vehicle detection approaches achieve incredible success in\nrecent years with the development of deep convolutional neural network (CNN).\nHowever, existing CNN based algorithms suffer from the problem that the\nconvolutional features are scale-sensitive in object detection task but it is\ncommon that traffic images and videos contain vehicles with a large variance of\nscales. In this paper, we delve into the source of scale sensitivity, and\nreveal two key issues: 1) existing RoI pooling destroys the structure of small\nscale objects, 2) the large intra-class distance for a large variance of scales\nexceeds the representation capability of a single network. Based on these\nfindings, we present a scale-insensitive convolutional neural network (SINet)\nfor fast detecting vehicles with a large variance of scales. First, we present\na context-aware RoI pooling to maintain the contextual information and original\nstructure of small scale objects. Second, we present a multi-branch decision\nnetwork to minimize the intra-class distance of features. These lightweight\ntechniques bring zero extra time complexity but prominent detection accuracy\nimprovement. The proposed techniques can be equipped with any deep network\narchitectures and keep them trained end-to-end. Our SINet achieves\nstate-of-the-art performance in terms of accuracy and speed (up to 37 FPS) on\nthe KITTI benchmark and a new highway dataset, which contains a large variance\nof scales and extremely small objects. \n\n"}
{"id": "1804.00607", "contents": "Title: MegaDepth: Learning Single-View Depth Prediction from Internet Photos Abstract: Single-view depth prediction is a fundamental problem in computer vision.\nRecently, deep learning methods have led to significant progress, but such\nmethods are limited by the available training data. Current datasets based on\n3D sensors have key limitations, including indoor-only images (NYU), small\nnumbers of training examples (Make3D), and sparse sampling (KITTI). We propose\nto use multi-view Internet photo collections, a virtually unlimited data\nsource, to generate training data via modern structure-from-motion and\nmulti-view stereo (MVS) methods, and present a large depth dataset called\nMegaDepth based on this idea. Data derived from MVS comes with its own\nchallenges, including noise and unreconstructable objects. We address these\nchallenges with new data cleaning methods, as well as automatically augmenting\nour data with ordinal depth relations generated using semantic segmentation. We\nvalidate the use of large amounts of Internet data by showing that models\ntrained on MegaDepth exhibit strong generalization-not only to novel scenes,\nbut also to other diverse datasets including Make3D, KITTI, and DIW, even when\nno images from those datasets are seen during training. \n\n"}
{"id": "1804.01174", "contents": "Title: Towards Deep Learning based Hand Keypoints Detection for Rapid\n  Sequential Movements from RGB Images Abstract: Hand keypoints detection and pose estimation has numerous applications in\ncomputer vision, but it is still an unsolved problem in many aspects. An\napplication of hand keypoints detection is in performing cognitive assessments\nof a subject by observing the performance of that subject in physical tasks\ninvolving rapid finger motion. As a part of this work, we introduce a novel\nhand key-points benchmark dataset that consists of hand gestures recorded\nspecifically for cognitive behavior monitoring. We explore the state of the art\nmethods in hand keypoint detection and we provide quantitative evaluations for\nthe performance of these methods on our dataset. In future, these results and\nour dataset can serve as a useful benchmark for hand keypoint recognition for\nrapid finger movements. \n\n"}
{"id": "1804.01296", "contents": "Title: Gaussian Process Uncertainty in Age Estimation as a Measure of Brain\n  Abnormality Abstract: Multivariate regression models for age estimation are a powerful tool for\nassessing abnormal brain morphology associated to neuropathology. Age\nprediction models are built on cohorts of healthy subjects and are built to\nreflect normal aging patterns. The application of these multivariate models to\ndiseased subjects usually results in high prediction errors, under the\nhypothesis that neuropathology presents a similar degenerative pattern as that\nof accelerated aging. In this work, we propose an alternative to the idea that\npathology follows a similar trajectory than normal aging. Instead, we propose\nthe use of metrics which measure deviations from the mean aging trajectory. We\npropose to measure these deviations using two different metrics: uncertainty in\na Gaussian process regression model and a newly proposed age weighted\nuncertainty measure. Consequently, our approach assumes that pathologic brain\npatterns are different to those of normal aging. We present results for\nsubjects with autism, mild cognitive impairment and Alzheimer's disease to\nhighlight the versatility of the approach to different diseases and age ranges.\nWe evaluate volume, thickness, and VBM features for quantifying brain\nmorphology. Our evaluations are performed on a large number of images obtained\nfrom a variety of publicly available neuroimaging databases. Across all\nfeatures, our uncertainty based measurements yield a better separation between\ndiseased subjects and healthy individuals than the prediction error. Finally,\nwe illustrate differences in the disease pattern to normal aging, supporting\nthe application of uncertainty as a measure of neuropathology. \n\n"}
{"id": "1804.01452", "contents": "Title: Jointly Discovering Visual Objects and Spoken Words from Raw Sensory\n  Input Abstract: In this paper, we explore neural network models that learn to associate\nsegments of spoken audio captions with the semantically relevant portions of\nnatural images that they refer to. We demonstrate that these audio-visual\nassociative localizations emerge from network-internal representations learned\nas a by-product of training to perform an image-audio retrieval task. Our\nmodels operate directly on the image pixels and speech waveform, and do not\nrely on any conventional supervision in the form of labels, segmentations, or\nalignments between the modalities during training. We perform analysis using\nthe Places 205 and ADE20k datasets demonstrating that our models implicitly\nlearn semantically-coupled object and word detectors. \n\n"}
{"id": "1804.01756", "contents": "Title: The Kanerva Machine: A Generative Distributed Memory Abstract: We present an end-to-end trained memory system that quickly adapts to new\ndata and generates samples like them. Inspired by Kanerva's sparse distributed\nmemory, it has a robust distributed reading and writing mechanism. The memory\nis analytically tractable, which enables optimal on-line compression via a\nBayesian update-rule. We formulate it as a hierarchical conditional generative\nmodel, where memory provides a rich data-dependent prior distribution.\nConsequently, the top-down memory and bottom-up perception are combined to\nproduce the code representing an observation. Empirically, we demonstrate that\nthe adaptive memory significantly improves generative models trained on both\nthe Omniglot and CIFAR datasets. Compared with the Differentiable Neural\nComputer (DNC) and its variants, our memory model has greater capacity and is\nsignificantly easier to train. \n\n"}
{"id": "1804.03286", "contents": "Title: On the Robustness of the CVPR 2018 White-Box Adversarial Example\n  Defenses Abstract: Neural networks are known to be vulnerable to adversarial examples. In this\nnote, we evaluate the two white-box defenses that appeared at CVPR 2018 and\nfind they are ineffective: when applying existing techniques, we can reduce the\naccuracy of the defended models to 0%. \n\n"}
{"id": "1804.03313", "contents": "Title: Cortex Neural Network: learning with Neural Network groups Abstract: Neural Network has been successfully applied to many real-world problems,\nsuch as image recognition and machine translation. However, for the current\narchitecture of neural networks, it is hard to perform complex cognitive tasks,\nfor example, to process the image and audio inputs together. Cortex, as an\nimportant architecture in the brain, is important for animals to perform the\ncomplex cognitive task. We view the architecture of Cortex in the brain as a\nmissing part in the design of the current artificial neural network. In this\npaper, we purpose Cortex Neural Network (CrtxNN). The Cortex Neural Network is\nan upper architecture of neural networks which motivated from cerebral cortex\nin the brain to handle different tasks in the same learning system. It is able\nto identify different tasks and solve them with different methods. In our\nimplementation, the Cortex Neural Network is able to process different\ncognitive tasks and perform reflection to get a higher accuracy. We provide a\nseries of experiments to examine the capability of the cortex architecture on\ntraditional neural networks. Our experiments proved its ability on the Cortex\nNeural Network can reach accuracy by 98.32% on MNIST and 62% on CIFAR10 at the\nsame time, which can promisingly reduce the loss by 40%. \n\n"}
{"id": "1804.03715", "contents": "Title: Graph Matching with Anchor Nodes: A Learning Approach Abstract: In this paper, we consider the weighted graph matching problem with partially\ndisclosed correspondences between a number of anchor nodes. Our construction\nexploits recently introduced node signatures based on graph Laplacians, namely\nthe Laplacian family signature (LFS) on the nodes, and the pairwise heat kernel\nmap on the edges. In this paper, without assuming an explicit form of\nparametric dependence nor a distance metric between node signatures, we\nformulate an optimization problem which incorporates the knowledge of anchor\nnodes. Solving this problem gives us an optimized proximity measure specific to\nthe graphs under consideration. Using this as a first order compatibility term,\nwe then set up an integer quadratic program (IQP) to solve for a near optimal\ngraph matching. Our experiments demonstrate the superior performance of our\napproach on randomly generated graphs and on two widely-used image sequences,\nwhen compared with other existing signature and adjacency matrix based graph\nmatching methods. \n\n"}
{"id": "1804.03809", "contents": "Title: Demoir\\'eing of Camera-Captured Screen Images Using Deep Convolutional\n  Neural Network Abstract: Taking photos of optoelectronic displays is a direct and spontaneous way of\ntransferring data and keeping records, which is widely practiced. However, due\nto the analog signal interference between the pixel grids of the display screen\nand camera sensor array, objectionable moir\\'e (alias) patterns appear in\ncaptured screen images. As the moir\\'e patterns are structured and highly\nvariant, they are difficult to be completely removed without affecting the\nunderneath latent image. In this paper, we propose an approach of deep\nconvolutional neural network for demoir\\'eing screen photos. The proposed DCNN\nconsists of a coarse-scale network and a fine-scale network. In the\ncoarse-scale network, the input image is first downsampled and then processed\nby stacked residual blocks to remove the moir\\'e artifacts. After that, the\nfine-scale network upsamples the demoir\\'ed low-resolution image back to the\noriginal resolution. Extensive experimental results have demonstrated that the\nproposed technique can efficiently remove the moir\\'e patterns for camera\nacquired screen images; the new technique outperforms the existing ones. \n\n"}
{"id": "1804.03999", "contents": "Title: Attention U-Net: Learning Where to Look for the Pancreas Abstract: We propose a novel attention gate (AG) model for medical imaging that\nautomatically learns to focus on target structures of varying shapes and sizes.\nModels trained with AGs implicitly learn to suppress irrelevant regions in an\ninput image while highlighting salient features useful for a specific task.\nThis enables us to eliminate the necessity of using explicit external\ntissue/organ localisation modules of cascaded convolutional neural networks\n(CNNs). AGs can be easily integrated into standard CNN architectures such as\nthe U-Net model with minimal computational overhead while increasing the model\nsensitivity and prediction accuracy. The proposed Attention U-Net architecture\nis evaluated on two large CT abdominal datasets for multi-class image\nsegmentation. Experimental results show that AGs consistently improve the\nprediction performance of U-Net across different datasets and training sizes\nwhile preserving computational efficiency. The code for the proposed\narchitecture is publicly available. \n\n"}
{"id": "1804.03999", "contents": "Title: Attention U-Net: Learning Where to Look for the Pancreas Abstract: We propose a novel attention gate (AG) model for medical imaging that\nautomatically learns to focus on target structures of varying shapes and sizes.\nModels trained with AGs implicitly learn to suppress irrelevant regions in an\ninput image while highlighting salient features useful for a specific task.\nThis enables us to eliminate the necessity of using explicit external\ntissue/organ localisation modules of cascaded convolutional neural networks\n(CNNs). AGs can be easily integrated into standard CNN architectures such as\nthe U-Net model with minimal computational overhead while increasing the model\nsensitivity and prediction accuracy. The proposed Attention U-Net architecture\nis evaluated on two large CT abdominal datasets for multi-class image\nsegmentation. Experimental results show that AGs consistently improve the\nprediction performance of U-Net across different datasets and training sizes\nwhile preserving computational efficiency. The code for the proposed\narchitecture is publicly available. \n\n"}
{"id": "1804.04326", "contents": "Title: STAIR Actions: A Video Dataset of Everyday Home Actions Abstract: A new large-scale video dataset for human action recognition, called STAIR\nActions is introduced. STAIR Actions contains 100 categories of action labels\nrepresenting fine-grained everyday home actions so that it can be applied to\nresearch in various home tasks such as nursing, caring, and security. In STAIR\nActions, each video has a single action label. Moreover, for each action\ncategory, there are around 1,000 videos that were obtained from YouTube or\nproduced by crowdsource workers. The duration of each video is mostly five to\nsix seconds. The total number of videos is 102,462. We explain how we\nconstructed STAIR Actions and show the characteristics of STAIR Actions\ncompared to existing datasets for human action recognition. Experiments with\nthree major models for action recognition show that STAIR Actions can train\nlarge models and achieve good performance. STAIR Actions can be downloaded from\nhttp://actions.stair.center \n\n"}
{"id": "1804.04395", "contents": "Title: Multi-Label Wireless Interference Identification with Convolutional\n  Neural Networks Abstract: The steadily growing use of license-free frequency bands require reliable\ncoexistence management and therefore proper wireless interference\nidentification (WII). In this work, we propose a WII approach based upon a deep\nconvolutional neural network (CNN) which classifies multiple IEEE 802.15.1,\nIEEE 802.11 b/g and IEEE 802.15.4 interfering signals in the presence of a\nutilized signal. The generated multi-label dataset contains frequency- and\ntime-limited sensing snapshots with the bandwidth of 10 MHz and duration of\n12.8 $\\mu$s, respectively. Each snapshot combines one utilized signal with up\nto multiple interfering signals. The approach shows promising results for\nsame-technology interference with a classification accuracy of approximately\n100 % for IEEE 802.15.1 and IEEE 802.15.4 signals. For IEEE 802.11 b/g signals\nthe accuracy increases for cross-technology interference with at least 90 %. \n\n"}
{"id": "1804.04563", "contents": "Title: Towards integrating spatial localization in convolutional neural\n  networks for brain image segmentation Abstract: Semantic segmentation is an established while rapidly evolving field in\nmedical imaging. In this paper we focus on the segmentation of brain Magnetic\nResonance Images (MRI) into cerebral structures using convolutional neural\nnetworks (CNN). CNNs achieve good performance by finding effective high\ndimensional image features describing the patch content only. In this work, we\npropose different ways to introduce spatial constraints into the network to\nfurther reduce prediction inconsistencies.\n  A patch based CNN architecture was trained, making use of multiple scales to\ngather contextual information. Spatial constraints were introduced within the\nCNN through a distance to landmarks feature or through the integration of a\nprobability atlas. We demonstrate experimentally that using spatial information\nhelps to reduce segmentation inconsistencies. \n\n"}
{"id": "1804.04694", "contents": "Title: A Variational U-Net for Conditional Appearance and Shape Generation Abstract: Deep generative models have demonstrated great performance in image\nsynthesis. However, results deteriorate in case of spatial deformations, since\nthey generate images of objects directly, rather than modeling the intricate\ninterplay of their inherent shape and appearance. We present a conditional\nU-Net for shape-guided image generation, conditioned on the output of a\nvariational autoencoder for appearance. The approach is trained end-to-end on\nimages, without requiring samples of the same object with varying pose or\nappearance. Experiments show that the model enables conditional image\ngeneration and transfer. Therefore, either shape or appearance can be retained\nfrom a query image, while freely altering the other. Moreover, appearance can\nbe sampled due to its stochastic latent representation, while preserving shape.\nIn quantitative and qualitative experiments on COCO, DeepFashion, shoes,\nMarket-1501 and handbags, the approach demonstrates significant improvements\nover the state-of-the-art. \n\n"}
{"id": "1804.04785", "contents": "Title: Deep Motion Boundary Detection Abstract: Motion boundary detection is a crucial yet challenging problem. Prior methods\nfocus on analyzing the gradients and distributions of optical flow fields, or\nuse hand-crafted features for motion boundary learning. In this paper, we\npropose the first dedicated end-to-end deep learning approach for motion\nboundary detection, which we term as MoBoNet. We introduce a refinement network\nstructure which takes source input images, initial forward and backward optical\nflows as well as corresponding warping errors as inputs and produces\nhigh-resolution motion boundaries. Furthermore, we show that the obtained\nmotion boundaries, through a fusion sub-network we design, can in turn guide\nthe optical flows for removing the artifacts. The proposed MoBoNet is generic\nand works with any optical flows. Our motion boundary detection and the refined\noptical flow estimation achieve results superior to the state of the art. \n\n"}
{"id": "1804.04876", "contents": "Title: Group Anomaly Detection using Deep Generative Models Abstract: Unlike conventional anomaly detection research that focuses on point\nanomalies, our goal is to detect anomalous collections of individual data\npoints. In particular, we perform group anomaly detection (GAD) with an\nemphasis on irregular group distributions (e.g. irregular mixtures of image\npixels). GAD is an important task in detecting unusual and anomalous phenomena\nin real-world applications such as high energy particle physics, social media,\nand medical imaging. In this paper, we take a generative approach by proposing\ndeep generative models: Adversarial autoencoder (AAE) and variational\nautoencoder (VAE) for group anomaly detection. Both AAE and VAE detect group\nanomalies using point-wise input data where group memberships are known a\npriori. We conduct extensive experiments to evaluate our models on real-world\ndatasets. The empirical results demonstrate that our approach is effective and\nrobust in detecting group anomalies. \n\n"}
{"id": "1804.04963", "contents": "Title: CNN-based Landmark Detection in Cardiac CTA Scans Abstract: Fast and accurate anatomical landmark detection can benefit many medical\nimage analysis methods. Here, we propose a method to automatically detect\nanatomical landmarks in medical images. Automatic landmark detection is\nperformed with a patch-based fully convolutional neural network (FCNN) that\ncombines regression and classification. For any given image patch, regression\nis used to predict the 3D displacement vector from the image patch to the\nlandmark. Simultaneously, classification is used to identify patches that\ncontain the landmark. Under the assumption that patches close to a landmark can\ndetermine the landmark location more precisely than patches farther from it,\nonly those patches that contain the landmark according to classification are\nused to determine the landmark location. The landmark location is obtained by\ncalculating the average landmark location using the computed 3D displacement\nvectors. The method is evaluated using detection of six clinically relevant\nlandmarks in coronary CT angiography (CCTA) scans: the right and left ostium,\nthe bifurcation of the left main coronary artery (LM) into the left anterior\ndescending and the left circumflex artery, and the origin of the right,\nnon-coronary, and left aortic valve commissure. The proposed method achieved an\naverage Euclidean distance error of 2.19 mm and 2.88 mm for the right and left\nostium respectively, 3.78 mm for the bifurcation of the LM, and 1.82 mm, 2.10\nmm and 1.89 mm for the origin of the right, non-coronary, and left aortic valve\ncommissure respectively, demonstrating accurate performance. The proposed\ncombination of regression and classification can be used to accurately detect\nlandmarks in CCTA scans. \n\n"}
{"id": "1804.05267", "contents": "Title: Low-Precision Floating-Point Schemes for Neural Network Training Abstract: The use of low-precision fixed-point arithmetic along with stochastic\nrounding has been proposed as a promising alternative to the commonly used\n32-bit floating point arithmetic to enhance training neural networks training\nin terms of performance and energy efficiency. In the first part of this paper,\nthe behaviour of the 12-bit fixed-point arithmetic when training a\nconvolutional neural network with the CIFAR-10 dataset is analysed, showing\nthat such arithmetic is not the most appropriate for the training phase. After\nthat, the paper presents and evaluates, under the same conditions, alternative\nlow-precision arithmetics, starting with the 12-bit floating-point arithmetic.\nThese two representations are then leveraged using local scaling in order to\nincrease accuracy and get closer to the baseline 32-bit floating-point\narithmetic. Finally, the paper introduces a simplified model in which both the\noutputs and the gradients of the neural networks are constrained to\npower-of-two values, just using 7 bits for their representation. The evaluation\ndemonstrates a minimal loss in accuracy for the proposed Power-of-Two neural\nnetwork, avoiding the use of multiplications and divisions and thereby,\nsignificantly reducing the training time as well as the energy consumption and\nmemory requirements during the training and inference phases. \n\n"}
{"id": "1804.05275", "contents": "Title: Horizontal Pyramid Matching for Person Re-identification Abstract: Despite the remarkable recent progress, person re-identification (Re-ID)\napproaches are still suffering from the failure cases where the discriminative\nbody parts are missing. To mitigate such cases, we propose a simple yet\neffective Horizontal Pyramid Matching (HPM) approach to fully exploit various\npartial information of a given person, so that correct person candidates can be\nstill identified even even some key parts are missing. Within the HPM, we make\nthe following contributions to produce a more robust feature representation for\nthe Re-ID task: 1) we learn to classify using partial feature representations\nat different horizontal pyramid scales, which successfully enhance the\ndiscriminative capabilities of various person parts; 2) we exploit average and\nmax pooling strategies to account for person-specific discriminative\ninformation in a global-local manner. To validate the effectiveness of the\nproposed HPM, extensive experiments are conducted on three popular benchmarks,\nincluding Market-1501, DukeMTMC-ReID and CUHK03. In particular, we achieve mAP\nscores of 83.1%, 74.5% and 59.7% on these benchmarks, which are the new\nstate-of-the-arts. Our code is available on Github \n\n"}
{"id": "1804.05374", "contents": "Title: Twin Regularization for online speech recognition Abstract: Online speech recognition is crucial for developing natural human-machine\ninterfaces. This modality, however, is significantly more challenging than\noff-line ASR, since real-time/low-latency constraints inevitably hinder the use\nof future information, that is known to be very helpful to perform robust\npredictions. A popular solution to mitigate this issue consists of feeding\nneural acoustic models with context windows that gather some future frames.\nThis introduces a latency which depends on the number of employed look-ahead\nfeatures. This paper explores a different approach, based on estimating the\nfuture rather than waiting for it. Our technique encourages the hidden\nrepresentations of a unidirectional recurrent network to embed some useful\ninformation about the future. Inspired by a recently proposed technique called\nTwin Networks, we add a regularization term that forces forward hidden states\nto be as close as possible to cotemporal backward ones, computed by a \"twin\"\nneural network running backwards in time. The experiments, conducted on a\nnumber of datasets, recurrent architectures, input features, and acoustic\nconditions, have shown the effectiveness of this approach. One important\nadvantage is that our method does not introduce any additional computation at\ntest time if compared to standard unidirectional recurrent networks. \n\n"}
{"id": "1804.06008", "contents": "Title: Geometry-aware Deep Network for Single-Image Novel View Synthesis Abstract: This paper tackles the problem of novel view synthesis from a single image.\nIn particular, we target real-world scenes with rich geometric structure, a\nchallenging task due to the large appearance variations of such scenes and the\nlack of simple 3D models to represent them. Modern, learning-based approaches\nmostly focus on appearance to synthesize novel views and thus tend to generate\npredictions that are inconsistent with the underlying scene structure. By\ncontrast, in this paper, we propose to exploit the 3D geometry of the scene to\nsynthesize a novel view. Specifically, we approximate a real-world scene by a\nfixed number of planes, and learn to predict a set of homographies and their\ncorresponding region masks to transform the input image into a novel view. To\nthis end, we develop a new region-aware geometric transform network that\nperforms these multiple tasks in a common framework. Our results on the outdoor\nKITTI and the indoor ScanNet datasets demonstrate the effectiveness of our\nnetwork in generating high quality synthetic views that respect the scene\ngeometry, thus outperforming the state-of-the-art methods. \n\n"}
{"id": "1804.06061", "contents": "Title: Improving Deep Binary Embedding Networks by Order-aware Reweighting of\n  Triplets Abstract: In this paper, we focus on triplet-based deep binary embedding networks for\nimage retrieval task. The triplet loss has been shown to be most effective for\nthe ranking problem. However, most of the previous works treat the triplets\nequally or select the hard triplets based on the loss. Such strategies do not\nconsider the order relations, which is important for retrieval task. To this\nend, we propose an order-aware reweighting method to effectively train the\ntriplet-based deep networks, which up-weights the important triplets and\ndown-weights the uninformative triplets. First, we present the order-aware\nweighting factors to indicate the importance of the triplets, which depend on\nthe rank order of binary codes. Then, we reshape the triplet loss to the\nsquared triplet loss such that the loss function will put more weights on the\nimportant triplets. Extensive evaluations on four benchmark datasets show that\nthe proposed method achieves significant performance compared with the\nstate-of-the-art baselines. \n\n"}
{"id": "1804.06078", "contents": "Title: Cross-Domain Adversarial Auto-Encoder Abstract: In this paper, we propose the Cross-Domain Adversarial Auto-Encoder (CDAAE)\nto address the problem of cross-domain image inference, generation and\ntransformation. We make the assumption that images from different domains share\nthe same latent code space for content, while having separate latent code space\nfor style. The proposed framework can map cross-domain data to a latent code\nvector consisting of a content part and a style part. The latent code vector is\nmatched with a prior distribution so that we can generate meaningful samples\nfrom any part of the prior space. Consequently, given a sample of one domain,\nour framework can generate various samples of the other domain with the same\ncontent of the input. This makes the proposed framework different from the\ncurrent work of cross-domain transformation. Besides, the proposed framework\ncan be trained with both labeled and unlabeled data, which makes it also\nsuitable for domain adaptation. Experimental results on data sets SVHN, MNIST\nand CASIA show the proposed framework achieved visually appealing performance\nfor image generation task. Besides, we also demonstrate the proposed method\nachieved superior results for domain adaptation. Code of our experiments is\navailable in https://github.com/luckycallor/CDAAE. \n\n"}
{"id": "1804.06173", "contents": "Title: Memetic Algorithms Beat Evolutionary Algorithms on the Class of Hurdle\n  Problems Abstract: Memetic algorithms are popular hybrid search heuristics that integrate local\nsearch into the search process of an evolutionary algorithm in order to combine\nthe advantages of rapid exploitation and global optimisation. However, these\nalgorithms are not well understood and the field is lacking a solid theoretical\nfoundation that explains when and why memetic algorithms are effective.\n  We provide a rigorous runtime analysis of a simple memetic algorithm, the\n$(1+1)$ MA, on the Hurdle problem class, a landscape class of tuneable\ndifficulty that shows a \"big valley structure\", a characteristic feature of\nmany hard problems from combinatorial optimisation. The only parameter of this\nclass is the hurdle width w, which describes the length of fitness valleys that\nhave to be overcome. We show that the $(1+1)$ EA requires $\\Theta(n^w)$\nexpected function evaluations to find the optimum, whereas the $(1+1)$ MA with\nbest-improvement and first-improvement local search can find the optimum in\n$\\Theta(n^2+n^3/w^2)$ and $\\Theta(n^3/w^2)$ function evaluations, respectively.\nSurprisingly, while increasing the hurdle width makes the problem harder for\nevolutionary algorithms, the problem becomes easier for memetic algorithms. We\ndiscuss how these findings can explain and illustrate the success of memetic\nalgorithms for problems with big valley structures. \n\n"}
{"id": "1804.06275", "contents": "Title: Network Signatures from Image Representation of Adjacency Matrices:\n  Deep/Transfer Learning for Subgraph Classification Abstract: We propose a novel subgraph image representation for classification of\nnetwork fragments with the targets being their parent networks. The graph image\nrepresentation is based on 2D image embeddings of adjacency matrices. We use\nthis image representation in two modes. First, as the input to a machine\nlearning algorithm. Second, as the input to a pure transfer learner. Our\nconclusions from several datasets are that (a) deep learning using our\nstructured image features performs the best compared to benchmark graph kernel\nand classical features based methods; and, (b) pure transfer learning works\neffectively with minimum interference from the user and is robust against small\ndata. \n\n"}
{"id": "1804.06579", "contents": "Title: Semi-Supervised Co-Analysis of 3D Shape Styles from Projected Lines Abstract: We present a semi-supervised co-analysis method for learning 3D shape styles\nfrom projected feature lines, achieving style patch localization with only weak\nsupervision. Given a collection of 3D shapes spanning multiple object\ncategories and styles, we perform style co-analysis over projected feature\nlines of each 3D shape and then backproject the learned style features onto the\n3D shapes. Our core analysis pipeline starts with mid-level patch sampling and\npre-selection of candidate style patches. Projective features are then encoded\nvia patch convolution. Multi-view feature integration and style clustering are\ncarried out under the framework of partially shared latent factor (PSLF)\nlearning, a multi-view feature learning scheme. PSLF achieves effective\nmulti-view feature fusion by distilling and exploiting consistent and\ncomplementary feature information from multiple views, while also selecting\nstyle patches from the candidates. Our style analysis approach supports both\nunsupervised and semi-supervised analysis. For the latter, our method accepts\nboth user-specified shape labels and style-ranked triplets as clustering\nconstraints.We demonstrate results from 3D shape style analysis and patch\nlocalization as well as improvements over state-of-the-art methods. We also\npresent several applications enabled by our style analysis. \n\n"}
{"id": "1804.06919", "contents": "Title: Video Compression through Image Interpolation Abstract: An ever increasing amount of our digital communication, media consumption,\nand content creation revolves around videos. We share, watch, and archive many\naspects of our lives through them, all of which are powered by strong video\ncompression. Traditional video compression is laboriously hand designed and\nhand optimized. This paper presents an alternative in an end-to-end deep\nlearning codec. Our codec builds on one simple idea: Video compression is\nrepeated image interpolation. It thus benefits from recent advances in deep\nimage interpolation and generation. Our deep video codec outperforms today's\nprevailing codecs, such as H.261, MPEG-4 Part 2, and performs on par with\nH.264. \n\n"}
{"id": "1804.08024", "contents": "Title: Angiodysplasia Detection and Localization Using Deep Convolutional\n  Neural Networks Abstract: Accurate detection and localization for angiodysplasia lesions is an\nimportant problem in early stage diagnostics of gastrointestinal bleeding and\nanemia. Gold-standard for angiodysplasia detection and localization is\nperformed using wireless capsule endoscopy. This pill-like device is able to\nproduce thousand of high enough resolution images during one passage through\ngastrointestinal tract. In this paper we present our winning solution for\nMICCAI 2017 Endoscopic Vision SubChallenge: Angiodysplasia Detection and\nLocalization its further improvements over the state-of-the-art results using\nseveral novel deep neural network architectures. It address the binary\nsegmentation problem, where every pixel in an image is labeled as an\nangiodysplasia lesions or background. Then, we analyze connected component of\neach predicted mask. Based on the analysis we developed a classifier that\npredict angiodysplasia lesions (binary variable) and a detector for their\nlocalization (center of a component). In this setting, our approach outperforms\nother methods in every task subcategory for angiodysplasia detection and\nlocalization thereby providing state-of-the-art results for these problems. The\nsource code for our solution is made publicly available at\nhttps://github.com/ternaus/angiodysplasia-segmentatio \n\n"}
{"id": "1804.08170", "contents": "Title: A Deep Convolutional Neural Network for Lung Cancer Diagnostic Abstract: In this paper, we examine the strength of deep learning technique for\ndiagnosing lung cancer on medical image analysis problem. Convolutional neural\nnetworks (CNNs) models become popular among the pattern recognition and\ncomputer vision research area because of their promising outcome on generating\nhigh-level image representations. We propose a new deep learning architecture\nfor learning high-level image representation to achieve high classification\naccuracy with low variance in medical image binary classification tasks. We aim\nto learn discriminant compact features at beginning of our deep convolutional\nneural network. We evaluate our model on Kaggle Data Science Bowl 2017 (KDSB17)\ndata set, and compare it with some related works proposed in the Kaggle\ncompetition. \n\n"}
{"id": "1804.08274", "contents": "Title: Jointly Localizing and Describing Events for Dense Video Captioning Abstract: Automatically describing a video with natural language is regarded as a\nfundamental challenge in computer vision. The problem nevertheless is not\ntrivial especially when a video contains multiple events to be worthy of\nmention, which often happens in real videos. A valid question is how to\ntemporally localize and then describe events, which is known as \"dense video\ncaptioning.\" In this paper, we present a novel framework for dense video\ncaptioning that unifies the localization of temporal event proposals and\nsentence generation of each proposal, by jointly training them in an end-to-end\nmanner. To combine these two worlds, we integrate a new design, namely\ndescriptiveness regression, into a single shot detection structure to infer the\ndescriptive complexity of each detected proposal via sentence generation. This\nin turn adjusts the temporal locations of each event proposal. Our model\ndiffers from existing dense video captioning methods since we propose a joint\nand global optimization of detection and captioning, and the framework uniquely\ncapitalizes on an attribute-augmented video captioning architecture. Extensive\nexperiments are conducted on ActivityNet Captions dataset and our framework\nshows clear improvements when compared to the state-of-the-art techniques. More\nremarkably, we obtain a new record: METEOR of 12.96% on ActivityNet Captions\nofficial test set. \n\n"}
{"id": "1804.08965", "contents": "Title: Correlation Tracking via Joint Discrimination and Reliability Learning Abstract: For visual tracking, an ideal filter learned by the correlation filter (CF)\nmethod should take both discrimination and reliability information. However,\nexisting attempts usually focus on the former one while pay less attention to\nreliability learning. This may make the learned filter be dominated by the\nunexpected salient regions on the feature map, thereby resulting in model\ndegradation. To address this issue, we propose a novel CF-based optimization\nproblem to jointly model the discrimination and reliability information. First,\nwe treat the filter as the element-wise product of a base filter and a\nreliability term. The base filter is aimed to learn the discrimination\ninformation between the target and backgrounds, and the reliability term\nencourages the final filter to focus on more reliable regions. Second, we\nintroduce a local response consistency regular term to emphasize equal\ncontributions of different regions and avoid the tracker being dominated by\nunreliable regions. The proposed optimization problem can be solved using the\nalternating direction method and speeded up in the Fourier domain. We conduct\nextensive experiments on the OTB-2013, OTB-2015 and VOT-2016 datasets to\nevaluate the proposed tracker. Experimental results show that our tracker\nperforms favorably against other state-of-the-art trackers. \n\n"}
{"id": "1804.09398", "contents": "Title: Learnable Histogram: Statistical Context Features for Deep Neural\n  Networks Abstract: Statistical features, such as histogram, Bag-of-Words (BoW) and Fisher\nVector, were commonly used with hand-crafted features in conventional\nclassification methods, but attract less attention since the popularity of deep\nlearning methods. In this paper, we propose a learnable histogram layer, which\nlearns histogram features within deep neural networks in end-to-end training.\nSuch a layer is able to back-propagate (BP) errors, learn optimal bin centers\nand bin widths, and be jointly optimized with other layers in deep networks\nduring training. Two vision problems, semantic segmentation and object\ndetection, are explored by integrating the learnable histogram layer into deep\nnetworks, which show that the proposed layer could be well generalized to\ndifferent applications. In-depth investigations are conducted to provide\ninsights on the newly introduced layer. \n\n"}
{"id": "1804.09699", "contents": "Title: Towards Fast Computation of Certified Robustness for ReLU Networks Abstract: Verifying the robustness property of a general Rectified Linear Unit (ReLU)\nnetwork is an NP-complete problem [Katz, Barrett, Dill, Julian and Kochenderfer\nCAV17]. Although finding the exact minimum adversarial distortion is hard,\ngiving a certified lower bound of the minimum distortion is possible. Current\navailable methods of computing such a bound are either time-consuming or\ndelivering low quality bounds that are too loose to be useful. In this paper,\nwe exploit the special structure of ReLU networks and provide two\ncomputationally efficient algorithms Fast-Lin and Fast-Lip that are able to\ncertify non-trivial lower bounds of minimum distortions, by bounding the ReLU\nunits with appropriate linear functions Fast-Lin, or by bounding the local\nLipschitz constant Fast-Lip. Experiments show that (1) our proposed methods\ndeliver bounds close to (the gap is 2-3X) exact minimum distortion found by\nReluplex in small MNIST networks while our algorithms are more than 10,000\ntimes faster; (2) our methods deliver similar quality of bounds (the gap is\nwithin 35% and usually around 10%; sometimes our bounds are even better) for\nlarger networks compared to the methods based on solving linear programming\nproblems but our algorithms are 33-14,000 times faster; (3) our method is\ncapable of solving large MNIST and CIFAR networks up to 7 layers with more than\n10,000 neurons within tens of seconds on a single CPU core.\n  In addition, we show that, in fact, there is no polynomial time algorithm\nthat can approximately find the minimum $\\ell_1$ adversarial distortion of a\nReLU network with a $0.99\\ln n$ approximation ratio unless\n$\\mathsf{NP}$=$\\mathsf{P}$, where $n$ is the number of neurons in the network. \n\n"}
{"id": "1804.09859", "contents": "Title: Competitive Learning Enriches Learning Representation and Accelerates\n  the Fine-tuning of CNNs Abstract: In this study, we propose the integration of competitive learning into\nconvolutional neural networks (CNNs) to improve the representation learning and\nefficiency of fine-tuning. Conventional CNNs use back propagation learning, and\nit enables powerful representation learning by a discrimination task. However,\nit requires huge amount of labeled data, and acquisition of labeled data is\nmuch harder than that of unlabeled data. Thus, efficient use of unlabeled data\nis getting crucial for DNNs. To address the problem, we introduce unsupervised\ncompetitive learning into the convolutional layer, and utilize unlabeled data\nfor effective representation learning. The results of validation experiments\nusing a toy model demonstrated that strong representation learning effectively\nextracted bases of images into convolutional filters using unlabeled data, and\naccelerated the speed of the fine-tuning of subsequent supervised back\npropagation learning. The leverage was more apparent when the number of filters\nwas sufficiently large, and, in such a case, the error rate steeply decreased\nin the initial phase of fine-tuning. Thus, the proposed method enlarged the\nnumber of filters in CNNs, and enabled a more detailed and generalized\nrepresentation. It could provide a possibility of not only deep but broad\nneural networks. \n\n"}
{"id": "1804.10172", "contents": "Title: Capsule networks for low-data transfer learning Abstract: We propose a capsule network-based architecture for generalizing learning to\nnew data with few examples. Using both generative and non-generative capsule\nnetworks with intermediate routing, we are able to generalize to new\ninformation over 25 times faster than a similar convolutional neural network.\nWe train the networks on the multiMNIST dataset lacking one digit. After the\nnetworks reach their maximum accuracy, we inject 1-100 examples of the missing\ndigit into the training set, and measure the number of batches needed to return\nto a comparable level of accuracy. We then discuss the improvement in low-data\ntransfer learning that capsule networks bring, and propose future directions\nfor capsule research. \n\n"}
{"id": "1804.10684", "contents": "Title: Joint Shape Representation and Classification for Detecting PDAC Abstract: We aim to detect pancreatic ductal adenocarcinoma (PDAC) in abdominal CT\nscans, which sheds light on early diagnosis of pancreatic cancer. This is a 3D\nvolume classification task with little training data. We propose a two-stage\nframework, which first segments the pancreas into a binary mask, then\ncompresses the mask into a shape vector and performs abnormality\nclassification. Shape representation and classification are performed in a\njoint manner, both to exploit the knowledge that PDAC often changes the shape\nof the pancreas and to prevent over-fitting. Experiments are performed on 300\nnormal scans and 136 PDAC cases. We achieve a specificity of 90.2% (false alarm\noccurs on less than 1/10 normal cases) at a sensitivity of 80.2% (less than 1/5\nPDAC cases are not detected), which show promise for clinical applications. \n\n"}
{"id": "1804.10743", "contents": "Title: Precise Box Score: Extract More Information from Datasets to Improve the\n  Performance of Face Detection Abstract: For the training of face detection network based on R-CNN framework, anchors\nare assigned to be positive samples if intersection-over-unions (IoUs) with\nground-truth are higher than the first threshold(such as 0.7); and to be\nnegative samples if their IoUs are lower than the second threshold(such as\n0.3). And the face detection model is trained by the above labels. However,\nanchors with IoU between first threshold and second threshold are not used. We\npropose a novel training strategy, Precise Box Score(PBS), to train object\ndetection models. The proposed training strategy uses the anchors with IoUs\nbetween the first and second threshold, which can consistently improve the\nperformance of face detection. Our proposed training strategy extracts more\ninformation from datasets, making better utilization of existing datasets.\nWhat's more, we also introduce a simple but effective model compression\nmethod(SEMCM), which can boost the performance of face detectors further.\nExperimental results show that the performance of face detection network can\nconsistently be improved based on our proposed scheme. \n\n"}
{"id": "1804.10992", "contents": "Title: Semi-parametric Image Synthesis Abstract: We present a semi-parametric approach to photographic image synthesis from\nsemantic layouts. The approach combines the complementary strengths of\nparametric and nonparametric techniques. The nonparametric component is a\nmemory bank of image segments constructed from a training set of images. Given\na novel semantic layout at test time, the memory bank is used to retrieve\nphotographic references that are provided as source material to a deep network.\nThe synthesis is performed by a deep network that draws on the provided\nphotographic material. Experiments on multiple semantic segmentation datasets\nshow that the presented approach yields considerably more realistic images than\nrecent purely parametric techniques. The results are shown in the supplementary\nvideo at https://youtu.be/U4Q98lenGLQ \n\n"}
{"id": "1805.00310", "contents": "Title: On the Limitation of MagNet Defense against $L_1$-based Adversarial\n  Examples Abstract: In recent years, defending adversarial perturbations to natural examples in\norder to build robust machine learning models trained by deep neural networks\n(DNNs) has become an emerging research field in the conjunction of deep\nlearning and security. In particular, MagNet consisting of an adversary\ndetector and a data reformer is by far one of the strongest defenses in the\nblack-box oblivious attack setting, where the attacker aims to craft\ntransferable adversarial examples from an undefended DNN model to bypass an\nunknown defense module deployed on the same DNN model. Under this setting,\nMagNet can successfully defend a variety of attacks in DNNs, including the\nhigh-confidence adversarial examples generated by the Carlini and Wagner's\nattack based on the $L_2$ distortion metric. However, in this paper, under the\nsame attack setting we show that adversarial examples crafted based on the\n$L_1$ distortion metric can easily bypass MagNet and mislead the target DNN\nimage classifiers on MNIST and CIFAR-10. We also provide explanations on why\nthe considered approach can yield adversarial examples with superior attack\nperformance and conduct extensive experiments on variants of MagNet to verify\nits lack of robustness to $L_1$ distortion based attacks. Notably, our results\nsubstantially weaken the assumption of effective threat models on MagNet that\nrequire knowing the deployed defense technique when attacking DNNs (i.e., the\ngray-box attack setting). \n\n"}
{"id": "1805.00329", "contents": "Title: DeepDIVA: A Highly-Functional Python Framework for Reproducible\n  Experiments Abstract: We introduce DeepDIVA: an infrastructure designed to enable quick and\nintuitive setup of reproducible experiments with a large range of useful\nanalysis functionality. Reproducing scientific results can be a frustrating\nexperience, not only in document image analysis but in machine learning in\ngeneral. Using DeepDIVA a researcher can either reproduce a given experiment\nwith a very limited amount of information or share their own experiments with\nothers. Moreover, the framework offers a large range of functions, such as\nboilerplate code, keeping track of experiments, hyper-parameter optimization,\nand visualization of data and results. To demonstrate the effectiveness of this\nframework, this paper presents case studies in the area of handwritten document\nanalysis where researchers benefit from the integrated functionality. DeepDIVA\nis implemented in Python and uses the deep learning framework PyTorch. It is\ncompletely open source, and accessible as Web Service through DIVAServices. \n\n"}
{"id": "1805.00330", "contents": "Title: Real-Time Human Detection as an Edge Service Enabled by a Lightweight\n  CNN Abstract: Edge computing allows more computing tasks to take place on the decentralized\nnodes at the edge of networks. Today many delay sensitive, mission-critical\napplications can leverage these edge devices to reduce the time delay or even\nto enable real time, online decision making thanks to their onsite presence.\nHuman objects detection, behavior recognition and prediction in smart\nsurveillance fall into that category, where a transition of a huge volume of\nvideo streaming data can take valuable time and place heavy pressure on\ncommunication networks. It is widely recognized that video processing and\nobject detection are computing intensive and too expensive to be handled by\nresource limited edge devices. Inspired by the depthwise separable convolution\nand Single Shot Multi-Box Detector (SSD), a lightweight Convolutional Neural\nNetwork (LCNN) is introduced in this paper. By narrowing down the classifier's\nsearching space to focus on human objects in surveillance video frames, the\nproposed LCNN algorithm is able to detect pedestrians with an affordable\ncomputation workload to an edge device. A prototype has been implemented on an\nedge node (Raspberry PI 3) using openCV libraries, and satisfactory performance\nis achieved using real world surveillance video streams. The experimental study\nhas validated the design of LCNN and shown it is a promising approach to\ncomputing intensive applications at the edge. \n\n"}
{"id": "1805.00342", "contents": "Title: A Feedback Neural Network for Small Target Motion Detection in Cluttered\n  Backgrounds Abstract: Small target motion detection is critical for insects to search for and track\nmates or prey which always appear as small dim speckles in the visual field. A\nclass of specific neurons, called small target motion detectors (STMDs), has\nbeen characterized by exquisite sensitivity for small target motion.\nUnderstanding and analyzing visual pathway of STMD neurons are beneficial to\ndesign artificial visual systems for small target motion detection. Feedback\nloops have been widely identified in visual neural circuits and play an\nimportant role in target detection. However, if there exists a feedback loop in\nthe STMD visual pathway or if a feedback loop could significantly improve the\ndetection performance of STMD neurons, is unclear. In this paper, we propose a\nfeedback neural network for small target motion detection against naturally\ncluttered backgrounds. In order to form a feedback loop, model output is\ntemporally delayed and relayed to previous neural layer as feedback signal.\nExtensive experiments showed that the significant improvement of the proposed\nfeedback neural network over the existing STMD-based models for small target\nmotion detection. \n\n"}
{"id": "1805.01328", "contents": "Title: Evaluation of CNN-based Single-Image Depth Estimation Methods Abstract: While an increasing interest in deep models for single-image depth estimation\nmethods can be observed, established schemes for their evaluation are still\nlimited. We propose a set of novel quality criteria, allowing for a more\ndetailed analysis by focusing on specific characteristics of depth maps. In\nparticular, we address the preservation of edges and planar regions, depth\nconsistency, and absolute distance accuracy. In order to employ these metrics\nto evaluate and compare state-of-the-art single-image depth estimation\napproaches, we provide a new high-quality RGB-D dataset. We used a DSLR camera\ntogether with a laser scanner to acquire high-resolution images and highly\naccurate depth maps. Experimental results show the validity of our proposed\nevaluation protocol. \n\n"}
{"id": "1805.01352", "contents": "Title: Spiking Deep Residual Network Abstract: Spiking neural networks (SNNs) have received significant attention for their\nbiological plausibility. SNNs theoretically have at least the same\ncomputational power as traditional artificial neural networks (ANNs). They\npossess potential of achieving energy-efficiency while keeping comparable\nperformance to deep neural networks (DNNs). However, it is still a big\nchallenge to train a very deep SNN. In this paper, we propose an efficient\napproach to build a spiking version of deep residual network (ResNet). ResNet\nis considered as a kind of the state-of-the-art convolutional neural networks\n(CNNs). We employ the idea of converting a trained ResNet to a network of\nspiking neurons, named Spiking ResNet (S-ResNet). We propose a shortcut\nconversion model to appropriately scale continuous-valued activations to match\nfiring rates in SNN, and a compensation mechanism to reduce the error caused by\ndiscretisation. Experimental results demonstrate that, compared with the\nstate-of-the-art SNN approaches, the proposed Spiking ResNet achieves the best\nperformance on CIFAR-10, CIFAR-100, and ImageNet 2012. Our work is the first\ntime to build a SNN deeper than 40, with comparable performance to ANNs on a\nlarge-scale dataset. \n\n"}
{"id": "1805.01972", "contents": "Title: Fast-converging Conditional Generative Adversarial Networks for Image\n  Synthesis Abstract: Building on top of the success of generative adversarial networks (GANs),\nconditional GANs attempt to better direct the data generation process by\nconditioning with certain additional information. Inspired by the most recent\nAC-GAN, in this paper we propose a fast-converging conditional GAN (FC-GAN). In\naddition to the real/fake classifier used in vanilla GANs, our discriminator\nhas an advanced auxiliary classifier which distinguishes each real class from\nan extra `fake' class. The `fake' class avoids mixing generated data with real\ndata, which can potentially confuse the classification of real data as AC-GAN\ndoes, and makes the advanced auxiliary classifier behave as another real/fake\nclassifier. As a result, FC-GAN can accelerate the process of differentiation\nof all classes, thus boost the convergence speed. Experimental results on image\nsynthesis demonstrate our model is competitive in the quality of images\ngenerated while achieving a faster convergence rate. \n\n"}
{"id": "1805.02214", "contents": "Title: Zero-shot Sequence Labeling: Transferring Knowledge from Sentences to\n  Tokens Abstract: Can attention- or gradient-based visualization techniques be used to infer\ntoken-level labels for binary sequence tagging problems, using networks trained\nonly on sentence-level labels? We construct a neural network architecture based\non soft attention, train it as a binary sentence classifier and evaluate\nagainst token-level annotation on four different datasets. Inferring token\nlabels from a network provides a method for quantitatively evaluating what the\nmodel is learning, along with generating useful feedback in assistance systems.\nOur results indicate that attention-based methods are able to predict\ntoken-level labels more accurately, compared to gradient-based methods,\nsometimes even rivaling the supervised oracle network. \n\n"}
{"id": "1805.02556", "contents": "Title: Relational Network for Skeleton-Based Action Recognition Abstract: With the fast development of effective and low-cost human skeleton capture\nsystems, skeleton-based action recognition has attracted much attention\nrecently. Most existing methods use Convolutional Neural Network (CNN) and\nRecurrent Neural Network (RNN) to extract spatio-temporal information embedded\nin the skeleton sequences for action recognition. However, these approaches are\nlimited in the ability of relational modeling in a single skeleton, due to the\nloss of important structural information when converting the raw skeleton data\nto adapt to the input format of CNN or RNN. In this paper, we propose an\nAttentional Recurrent Relational Network-LSTM (ARRN-LSTM) to simultaneously\nmodel spatial configurations and temporal dynamics in skeletons for action\nrecognition. We introduce the Recurrent Relational Network to learn the spatial\nfeatures in a single skeleton, followed by a multi-layer LSTM to learn the\ntemporal features in the skeleton sequences. Between the two modules, we design\nan adaptive attentional module to focus attention on the most discriminative\nparts in the single skeleton. To exploit the complementarity from different\ngeometries in the skeleton for sufficient relational modeling, we design a\ntwo-stream architecture to learn the structural features among joints and lines\nsimultaneously. Extensive experiments are conducted on several popular skeleton\ndatasets and the results show that the proposed approach achieves better\nresults than most mainstream methods. \n\n"}
{"id": "1805.02825", "contents": "Title: N2RPP: An Adversarial Network to Rebuild Plantar Pressure for ACLD\n  Patients Abstract: Foot is a vital part of human, and lots of valuable information is embedded.\nPlantar pressure is one of which contains this information and it describes\nhuman walking features. It is proved that once one has trouble with lower limb,\nthe distribution of plantar pressure will change to some degree. Plantar\npressure can be converted into images according to some simple standards. In\nthis paper, we take full advantage of these plantar pressure images for medical\nusage. We present N2RPP, a generative adversarial network (GAN) based method to\nrebuild plantar pressure images of anterior cruciate ligament deficiency (ACLD)\npatients from low dimension features, which are extracted from an autoencoder.\nThrough the result of experiments, the extracted features are a useful\nrepresentation to describe and rebuild plantar pressure images. According to\nN2RPP's results, we find out that there are several noteworthy differences\nbetween normal people and patients. This can provide doctors a rough direction\nof adjusting plantar pressure to a better distribution to reduce patients' sore\nand pain during the rehabilitation treatment for ACLD. \n\n"}
{"id": "1805.02850", "contents": "Title: Joint Cell Nuclei Detection and Segmentation in Microscopy Images Using\n  3D Convolutional Networks Abstract: We propose a 3D convolutional neural network to simultaneously segment and\ndetect cell nuclei in confocal microscopy images. Mirroring the co-dependency\nof these tasks, our proposed model consists of two serial components: the first\npart computes a segmentation of cell bodies, while the second module identifies\nthe centers of these cells. Our model is trained end-to-end from scratch on a\nmouse parotid salivary gland stem cell nuclei dataset comprising 107 image\nstacks from three independent cell preparations, each containing several\nhundred individual cell nuclei in 3D. In our experiments, we conduct a thorough\nevaluation of both detection accuracy and segmentation quality, on two\ndifferent datasets. The results show that the proposed method provides\nsignificantly improved detection and segmentation accuracy compared to\nstate-of-the-art and benchmark algorithms. Finally, we use a previously\ndescribed test-time drop-out strategy to obtain uncertainty estimates on our\npredictions and validate these estimates by demonstrating that they are\nstrongly correlated with accuracy. \n\n"}
{"id": "1805.03305", "contents": "Title: The Effectiveness of Instance Normalization: a Strong Baseline for\n  Single Image Dehazing Abstract: We propose a novel deep neural network architecture for the challenging\nproblem of single image dehazing, which aims to recover the clear image from a\ndegraded hazy image. Instead of relying on hand-crafted image priors or\nexplicitly estimating the components of the widely used atmospheric scattering\nmodel, our end-to-end system directly generates the clear image from an input\nhazy image. The proposed network has an encoder-decoder architecture with skip\nconnections and instance normalization. We adopt the convolutional layers of\nthe pre-trained VGG network as encoder to exploit the representation power of\ndeep features, and demonstrate the effectiveness of instance normalization for\nimage dehazing. Our simple yet effective network outperforms the\nstate-of-the-art methods by a large margin on the benchmark datasets. \n\n"}
{"id": "1805.04026", "contents": "Title: Towards an Unequivocal Representation of Actions Abstract: This work introduces verb-only representations for actions and interactions;\nthe problem of describing similar motions (e.g. 'open door', 'open cupboard'),\nand distinguish differing ones (e.g. 'open door' vs 'open bottle') using\nverb-only labels. Current approaches for action recognition neglect legitimate\nsemantic ambiguities and class overlaps between verbs (Fig. 1), relying on the\nobjects to disambiguate interactions. We deviate from single-verb labels and\nintroduce a mapping between observations and multiple verb labels - in order to\ncreate an Unequivocal Representation of Actions. The new representation\nbenefits from increased vocabulary and a soft assignment to an enriched space\nof verb labels. We learn these representations as multi-output regression,\nusing a two-stream fusion CNN. The proposed approach outperforms conventional\nsingle-verb labels (also known as majority voting) on three egocentric datasets\nfor both recognition and retrieval. \n\n"}
{"id": "1805.04487", "contents": "Title: Non-Stationary Texture Synthesis by Adversarial Expansion Abstract: The real world exhibits an abundance of non-stationary textures. Examples\ninclude textures with large-scale structures, as well as spatially variant and\ninhomogeneous textures. While existing example-based texture synthesis methods\ncan cope well with stationary textures, non-stationary textures still pose a\nconsiderable challenge, which remains unresolved. In this paper, we propose a\nnew approach for example-based non-stationary texture synthesis. Our approach\nuses a generative adversarial network (GAN), trained to double the spatial\nextent of texture blocks extracted from a specific texture exemplar. Once\ntrained, the fully convolutional generator is able to expand the size of the\nentire exemplar, as well as of any of its sub-blocks. We demonstrate that this\nconceptually simple approach is highly effective for capturing large-scale\nstructures, as well as other non-stationary attributes of the input exemplar.\nAs a result, it can cope with challenging textures, which, to our knowledge, no\nother existing method can handle. \n\n"}
{"id": "1805.05020", "contents": "Title: Learning Dual Convolutional Neural Networks for Low-Level Vision Abstract: In this paper, we propose a general dual convolutional neural network\n(DualCNN) for low-level vision problems, e.g., super-resolution,\nedge-preserving filtering, deraining and dehazing. These problems usually\ninvolve the estimation of two components of the target signals: structures and\ndetails. Motivated by this, our proposed DualCNN consists of two parallel\nbranches, which respectively recovers the structures and details in an\nend-to-end manner. The recovered structures and details can generate the target\nsignals according to the formation model for each particular application. The\nDualCNN is a flexible framework for low-level vision tasks and can be easily\nincorporated into existing CNNs. Experimental results show that the DualCNN can\nbe effectively applied to numerous low-level vision tasks with favorable\nperformance against the state-of-the-art methods. \n\n"}
{"id": "1805.06361", "contents": "Title: Object detection at 200 Frames Per Second Abstract: In this paper, we propose an efficient and fast object detector which can\nprocess hundreds of frames per second. To achieve this goal we investigate\nthree main aspects of the object detection framework: network architecture,\nloss function and training data (labeled and unlabeled). In order to obtain\ncompact network architecture, we introduce various improvements, based on\nrecent work, to develop an architecture which is computationally light-weight\nand achieves a reasonable performance. To further improve the performance,\nwhile keeping the complexity same, we utilize distillation loss function. Using\ndistillation loss we transfer the knowledge of a more accurate teacher network\nto proposed light-weight student network. We propose various innovations to\nmake distillation efficient for the proposed one stage detector pipeline:\nobjectness scaled distillation loss, feature map non-maximal suppression and a\nsingle unified distillation loss function for detection. Finally, building upon\nthe distillation loss, we explore how much can we push the performance by\nutilizing the unlabeled data. We train our model with unlabeled data using the\nsoft labels of the teacher network. Our final network consists of 10x fewer\nparameters than the VGG based object detection network and it achieves a speed\nof more than 200 FPS and proposed changes improve the detection accuracy by 14\nmAP over the baseline on Pascal dataset. \n\n"}
{"id": "1805.06771", "contents": "Title: Convolutional Social Pooling for Vehicle Trajectory Prediction Abstract: Forecasting the motion of surrounding vehicles is a critical ability for an\nautonomous vehicle deployed in complex traffic. Motion of all vehicles in a\nscene is governed by the traffic context, i.e., the motion and relative spatial\nconfiguration of neighboring vehicles. In this paper we propose an LSTM\nencoder-decoder model that uses convolutional social pooling as an improvement\nto social pooling layers for robustly learning interdependencies in vehicle\nmotion. Additionally, our model outputs a multi-modal predictive distribution\nover future trajectories based on maneuver classes. We evaluate our model using\nthe publicly available NGSIM US-101 and I-80 datasets. Our results show\nimprovement over the state of the art in terms of RMS values of prediction\nerror and negative log-likelihoods of true future trajectories under the\nmodel's predictive distribution. We also present a qualitative analysis of the\nmodel's predicted distributions for various traffic scenarios. \n\n"}
{"id": "1805.07588", "contents": "Title: Robust Optimization over Multiple Domains Abstract: In this work, we study the problem of learning a single model for multiple\ndomains. Unlike the conventional machine learning scenario where each domain\ncan have the corresponding model, multiple domains (i.e., applications/users)\nmay share the same machine learning model due to maintenance loads in cloud\ncomputing services. For example, a digit-recognition model should be applicable\nto hand-written digits, house numbers, car plates, etc. Therefore, an ideal\nmodel for cloud computing has to perform well at each applicable domain. To\naddress this new challenge from cloud computing, we develop a framework of\nrobust optimization over multiple domains. In lieu of minimizing the empirical\nrisk, we aim to learn a model optimized to the adversarial distribution over\nmultiple domains. Hence, we propose to learn the model and the adversarial\ndistribution simultaneously with the stochastic algorithm for efficiency.\nTheoretically, we analyze the convergence rate for convex and non-convex\nmodels. To our best knowledge, we first study the convergence rate of learning\na robust non-convex model with a practical algorithm. Furthermore, we\ndemonstrate that the robustness of the framework and the convergence rate can\nbe further enhanced by appropriate regularizers over the adversarial\ndistribution. The empirical study on real-world fine-grained visual\ncategorization and digits recognition tasks verifies the effectiveness and\nefficiency of the proposed framework. \n\n"}
{"id": "1805.07632", "contents": "Title: Latent Space Non-Linear Statistics Abstract: Given data, deep generative models, such as variational autoencoders (VAE)\nand generative adversarial networks (GAN), train a lower dimensional latent\nrepresentation of the data space. The linear Euclidean geometry of data space\npulls back to a nonlinear Riemannian geometry on the latent space. The latent\nspace thus provides a low-dimensional nonlinear representation of data and\nclassical linear statistical techniques are no longer applicable. In this paper\nwe show how statistics of data in their latent space representation can be\nperformed using techniques from the field of nonlinear manifold statistics.\nNonlinear manifold statistics provide generalizations of Euclidean statistical\nnotions including means, principal component analysis, and maximum likelihood\nfits of parametric probability distributions. We develop new techniques for\nmaximum likelihood inference in latent space, and adress the computational\ncomplexity of using geometric algorithms with high-dimensional data by training\na separate neural network to approximate the Riemannian metric and cometric\ntensor capturing the shape of the learned data manifold. \n\n"}
{"id": "1805.07777", "contents": "Title: DLBI: Deep learning guided Bayesian inference for structure\n  reconstruction of super-resolution fluorescence microscopy Abstract: Super-resolution fluorescence microscopy, with a resolution beyond the\ndiffraction limit of light, has become an indispensable tool to directly\nvisualize biological structures in living cells at a nanometer-scale\nresolution. Despite advances in high-density super-resolution fluorescent\ntechniques, existing methods still have bottlenecks, including extremely long\nexecution time, artificial thinning and thickening of structures, and lack of\nability to capture latent structures. Here we propose a novel deep learning\nguided Bayesian inference approach, DLBI, for the time-series analysis of\nhigh-density fluorescent images. Our method combines the strength of deep\nlearning and statistical inference, where deep learning captures the underlying\ndistribution of the fluorophores that are consistent with the observed\ntime-series fluorescent images by exploring local features and correlation\nalong time-axis, and statistical inference further refines the ultrastructure\nextracted by deep learning and endues physical meaning to the final image.\nComprehensive experimental results on both real and simulated datasets\ndemonstrate that our method provides more accurate and realistic local patch\nand large-field reconstruction than the state-of-the-art method, the 3B\nanalysis, while our method is more than two orders of magnitude faster. The\nmain program is available at https://github.com/lykaust15/DLBI \n\n"}
{"id": "1805.07888", "contents": "Title: DeepPhys: Video-Based Physiological Measurement Using Convolutional\n  Attention Networks Abstract: Non-contact video-based physiological measurement has many applications in\nhealth care and human-computer interaction. Practical applications require\nmeasurements to be accurate even in the presence of large head rotations. We\npropose the first end-to-end system for video-based measurement of heart and\nbreathing rate using a deep convolutional network. The system features a new\nmotion representation based on a skin reflection model and a new attention\nmechanism using appearance information to guide motion estimation, both of\nwhich enable robust measurement under heterogeneous lighting and major motions.\nOur approach significantly outperforms all current state-of-the-art methods on\nboth RGB and infrared video datasets. Furthermore, it allows spatial-temporal\ndistributions of physiological signals to be visualized via the attention\nmechanism. \n\n"}
{"id": "1805.08191", "contents": "Title: Hierarchically Structured Reinforcement Learning for Topically Coherent\n  Visual Story Generation Abstract: We propose a hierarchically structured reinforcement learning approach to\naddress the challenges of planning for generating coherent multi-sentence\nstories for the visual storytelling task. Within our framework, the task of\ngenerating a story given a sequence of images is divided across a two-level\nhierarchical decoder. The high-level decoder constructs a plan by generating a\nsemantic concept (i.e., topic) for each image in sequence. The low-level\ndecoder generates a sentence for each image using a semantic compositional\nnetwork, which effectively grounds the sentence generation conditioned on the\ntopic. The two decoders are jointly trained end-to-end using reinforcement\nlearning. We evaluate our model on the visual storytelling (VIST) dataset.\nEmpirical results from both automatic and human evaluations demonstrate that\nthe proposed hierarchically structured reinforced training achieves\nsignificantly better performance compared to a strong flat deep reinforcement\nlearning baseline. \n\n"}
{"id": "1805.08303", "contents": "Title: Compression of Deep Convolutional Neural Networks under Joint Sparsity\n  Constraints Abstract: We consider the optimization of deep convolutional neural networks (CNNs)\nsuch that they provide good performance while having reduced complexity if\ndeployed on either conventional systems utilizing spatial-domain convolution or\nlower complexity systems designed for Winograd convolution. Furthermore, we\nexplore the universal quantization and compression of these networks. In\nparticular, the proposed framework produces one compressed model whose\nconvolutional filters can be made sparse either in the spatial domain or in the\nWinograd domain. Hence, one compressed model can be deployed universally on any\nplatform, without need for re-training on the deployed platform, and the\nsparsity of its convolutional filters can be exploited for further complexity\nreduction in either domain. To get a better compression ratio, the sparse model\nis compressed in the spatial domain which has a less number of parameters. From\nour experiments, we obtain $24.2\\times$, $47.7\\times$ and $35.4\\times$\ncompressed models for ResNet-18, AlexNet and CT-SRCNN, while their\ncomputational cost is also reduced by $4.5\\times$, $5.1\\times$ and\n$23.5\\times$, respectively. \n\n"}
{"id": "1805.08365", "contents": "Title: Learning Markov Clustering Networks for Scene Text Detection Abstract: A novel framework named Markov Clustering Network (MCN) is proposed for fast\nand robust scene text detection. MCN predicts instance-level bounding boxes by\nfirstly converting an image into a Stochastic Flow Graph (SFG) and then\nperforming Markov Clustering on this graph. Our method can detect text objects\nwith arbitrary size and orientation without prior knowledge of object size. The\nstochastic flow graph encode objects' local correlation and semantic\ninformation. An object is modeled as strongly connected nodes, which allows\nflexible bottom-up detection for scale-varying and rotated objects. MCN\ngenerates bounding boxes without using Non-Maximum Suppression, and it can be\nfully parallelized on GPUs. The evaluation on public benchmarks shows that our\nmethod outperforms the existing methods by a large margin in detecting\nmultioriented text objects. MCN achieves new state-of-art performance on\nchallenging MSRA-TD500 dataset with precision of 0.88, recall of 0.79 and\nF-score of 0.83. Also, MCN achieves realtime inference with frame rate of 34\nFPS, which is $1.5\\times$ speedup when compared with the fastest scene text\ndetection algorithm. \n\n"}
{"id": "1805.08440", "contents": "Title: Classification Uncertainty of Deep Neural Networks Based on Gradient\n  Information Abstract: We study the quantification of uncertainty of Convolutional Neural Networks\n(CNNs) based on gradient metrics. Unlike the classical softmax entropy, such\nmetrics gather information from all layers of the CNN. We show for the EMNIST\ndigits data set that for several such metrics we achieve the same meta\nclassification accuracy -- i.e. the task of classifying predictions as correct\nor incorrect without knowing the actual label -- as for entropy thresholding.\nWe apply meta classification to unknown concepts (out-of-distribution samples)\n-- EMNIST/Omniglot letters, CIFAR10 and noise -- and demonstrate that meta\nclassification rates for unknown concepts can be increased when using entropy\ntogether with several gradient based metrics as input quantities for a meta\nclassifier. Meta classifiers only trained on the uncertainty metrics of known\nconcepts, i.e. EMNIST digits, usually do not perform equally well for all\nunknown concepts. If we however allow the meta classifier to be trained on\nuncertainty metrics for some out-of-distribution samples, meta classification\nfor concepts remote from EMNIST digits (then termed known unknowns) can be\nimproved considerably. \n\n"}
{"id": "1805.08657", "contents": "Title: Robust Conditional Generative Adversarial Networks Abstract: Conditional generative adversarial networks (cGAN) have led to large\nimprovements in the task of conditional image generation, which lies at the\nheart of computer vision. The major focus so far has been on performance\nimprovement, while there has been little effort in making cGAN more robust to\nnoise. The regression (of the generator) might lead to arbitrarily large errors\nin the output, which makes cGAN unreliable for real-world applications. In this\nwork, we introduce a novel conditional GAN model, called RoCGAN, which\nleverages structure in the target space of the model to address the issue. Our\nmodel augments the generator with an unsupervised pathway, which promotes the\noutputs of the generator to span the target manifold even in the presence of\nintense noise. We prove that RoCGAN share similar theoretical properties as GAN\nand experimentally verify that our model outperforms existing state-of-the-art\ncGAN architectures by a large margin in a variety of domains including images\nfrom natural scenes and faces. \n\n"}
{"id": "1805.08700", "contents": "Title: Evaluating ResNeXt Model Architecture for Image Classification Abstract: In recent years, deep learning methods have been successfully applied to\nimage classification tasks. Many such deep neural networks exist today that can\neasily differentiate cats from dogs. One such model is the ResNeXt model that\nuses a homogeneous, multi-branch architecture for image classification. This\npaper aims at implementing and evaluating the ResNeXt model architecture on\nsubsets of the CIFAR-10 dataset. It also tweaks the original ResNeXt\nhyper-parameters such as cardinality, depth and base-width and compares the\nperformance of the modified model with the original. Analysis of the\nexperiments performed in this paper show that a slight decrease in depth or\nbase-width does not affect the performance of the model much leading to\ncomparable results. \n\n"}
{"id": "1805.09097", "contents": "Title: Image Restoration by Estimating Frequency Distribution of Local Patches Abstract: In this paper, we propose a method to solve the image restoration problem,\nwhich tries to restore the details of a corrupted image, especially due to the\nloss caused by JPEG compression. We have treated an image in the frequency\ndomain to explicitly restore the frequency components lost during image\ncompression. In doing so, the distribution in the frequency domain is learned\nusing the cross entropy loss. Unlike recent approaches, we have reconstructed\nthe details of an image without using the scheme of adversarial training.\nRather, the image restoration problem is treated as a classification problem to\ndetermine the frequency coefficient for each frequency band in an image patch.\nIn this paper, we show that the proposed method effectively restores a\nJPEG-compressed image with more detailed high frequency components, making the\nrestored image more vivid. \n\n"}
{"id": "1805.09190", "contents": "Title: Towards the first adversarially robust neural network model on MNIST Abstract: Despite much effort, deep neural networks remain highly susceptible to tiny\ninput perturbations and even for MNIST, one of the most common toy datasets in\ncomputer vision, no neural network model exists for which adversarial\nperturbations are large and make semantic sense to humans. We show that even\nthe widely recognized and by far most successful defense by Madry et al. (1)\noverfits on the L-infinity metric (it's highly susceptible to L2 and L0\nperturbations), (2) classifies unrecognizable images with high certainty, (3)\nperforms not much better than simple input binarization and (4) features\nadversarial perturbations that make little sense to humans. These results\nsuggest that MNIST is far from being solved in terms of adversarial robustness.\nWe present a novel robust classification model that performs analysis by\nsynthesis using learned class-conditional data distributions. We derive bounds\non the robustness and go to great length to empirically evaluate our model\nusing maximally effective adversarial attacks by (a) applying decision-based,\nscore-based, gradient-based and transfer-based attacks for several different Lp\nnorms, (b) by designing a new attack that exploits the structure of our\ndefended model and (c) by devising a novel decision-based attack that seeks to\nminimize the number of perturbed pixels (L0). The results suggest that our\napproach yields state-of-the-art robustness on MNIST against L0, L2 and\nL-infinity perturbations and we demonstrate that most adversarial examples are\nstrongly perturbed towards the perceptual boundary between the original and the\nadversarial class. \n\n"}
{"id": "1805.09474", "contents": "Title: VisualBackProp for learning using privileged information with CNNs Abstract: In many machine learning applications, from medical diagnostics to autonomous\ndriving, the availability of prior knowledge can be used to improve the\npredictive performance of learning algorithms and incorporate `physical,'\n`domain knowledge,' or `common sense' concepts into training of machine\nlearning systems as well as verify constraints/properties of the systems. We\nexplore the learning using privileged information paradigm and show how to\nincorporate the privileged information, such as segmentation mask available\nalong with the classification label of each example, into the training stage of\nconvolutional neural networks. This is done by augmenting the CNN model with an\narchitectural component that effectively focuses model's attention on the\ndesired region of the input image during the training process and that is\ntransparent to the network's label prediction mechanism at testing. This\ncomponent effectively corresponds to the visualization strategy for identifying\nthe parts of the input, often referred to as visualization mask, that most\ncontribute to the prediction, yet uses this strategy in reverse to the\nclassical setting in order to enforce the desired visualization mask instead.\nWe verify our proposed algorithms through exhaustive experiments on benchmark\nImageNet and PASCAL VOC data sets and achieve improvements in the performance\nof $2.4\\%$ and $2.7\\%$ over standard single-supervision model training.\nFinally, we confirm the effectiveness of our approach on skin lesion\nclassification problem. \n\n"}
{"id": "1805.09749", "contents": "Title: MobiFace: A Novel Dataset for Mobile Face Tracking in the Wild Abstract: Face tracking serves as the crucial initial step in mobile applications\ntrying to analyse target faces over time in mobile settings. However, this\nproblem has received little attention, mainly due to the scarcity of dedicated\nface tracking benchmarks. In this work, we introduce MobiFace, the first\ndataset for single face tracking in mobile situations. It consists of 80\nunedited live-streaming mobile videos captured by 70 different smartphone users\nin fully unconstrained environments. Over $95K$ bounding boxes are manually\nlabelled. The videos are carefully selected to cover typical smartphone usage.\nThe videos are also annotated with 14 attributes, including 6 newly proposed\nattributes and 8 commonly seen in object tracking. 36 state-of-the-art\ntrackers, including facial landmark trackers, generic object trackers and\ntrackers that we have fine-tuned or improved, are evaluated. The results\nsuggest that mobile face tracking cannot be solved through existing approaches.\nIn addition, we show that fine-tuning on the MobiFace training data\nsignificantly boosts the performance of deep learning-based trackers,\nsuggesting that MobiFace captures the unique characteristics of mobile face\ntracking. Our goal is to offer the community a diverse dataset to enable the\ndesign and evaluation of mobile face trackers. The dataset, annotations and the\nevaluation server will be on \\url{https://mobiface.github.io/}. \n\n"}
{"id": "1805.09987", "contents": "Title: Learning from Multi-domain Artistic Images for Arbitrary Style Transfer Abstract: We propose a fast feed-forward network for arbitrary style transfer, which\ncan generate stylized image for previously unseen content and style image\npairs. Besides the traditional content and style representation based on deep\nfeatures and statistics for textures, we use adversarial networks to regularize\nthe generation of stylized images. Our adversarial network learns the intrinsic\nproperty of image styles from large-scale multi-domain artistic images. The\nadversarial training is challenging because both the input and output of our\ngenerator are diverse multi-domain images. We use a conditional generator that\nstylized content by shifting the statistics of deep features, and a conditional\ndiscriminator based on the coarse category of styles. Moreover, we propose a\nmask module to spatially decide the stylization level and stabilize adversarial\ntraining by avoiding mode collapse. As a side effect, our trained discriminator\ncan be applied to rank and select representative stylized images. We\nqualitatively and quantitatively evaluate the proposed method, and compare with\nrecent style transfer methods. We release our code and model at\nhttps://github.com/nightldj/behance_release. \n\n"}
{"id": "1805.10416", "contents": "Title: Human Action Generation with Generative Adversarial Networks Abstract: Inspired by the recent advances in generative models, we introduce a human\naction generation model in order to generate a consecutive sequence of human\nmotions to formulate novel actions. We propose a framework of an autoencoder\nand a generative adversarial network (GAN) to produce multiple and consecutive\nhuman actions conditioned on the initial state and the given class label. The\nproposed model is trained in an end-to-end fashion, where the autoencoder is\njointly trained with the GAN. The model is trained on the NTU RGB+D dataset and\nwe show that the proposed model can generate different styles of actions.\nMoreover, the model can successfully generate a sequence of novel actions given\ndifferent action labels as conditions. The conventional human action prediction\nand generation models lack those features, which are essential for practical\napplications. \n\n"}
{"id": "1805.10726", "contents": "Title: A Neurobiological Evaluation Metric for Neural Network Model Search Abstract: Neuroscience theory posits that the brain's visual system coarsely identifies\nbroad object categories via neural activation patterns, with similar objects\nproducing similar neural responses. Artificial neural networks also have\ninternal activation behavior in response to stimuli. We hypothesize that\nnetworks exhibiting brain-like activation behavior will demonstrate brain-like\ncharacteristics, e.g., stronger generalization capabilities. In this paper we\nintroduce a human-model similarity (HMS) metric, which quantifies the\nsimilarity of human fMRI and network activation behavior. To calculate HMS,\nrepresentational dissimilarity matrices (RDMs) are created as abstractions of\nactivation behavior, measured by the correlations of activations to stimulus\npairs. HMS is then the correlation between the fMRI RDM and the neural network\nRDM across all stimulus pairs. We test the metric on unsupervised predictive\ncoding networks, which specifically model visual perception, and assess the\nmetric for statistical significance over a large range of hyperparameters. Our\nexperiments show that networks with increased human-model similarity are\ncorrelated with better performance on two computer vision tasks: next frame\nprediction and object matching accuracy. Further, HMS identifies networks with\nhigh performance on both tasks. An unexpected secondary finding is that the\nmetric can be employed during training as an early-stopping mechanism. \n\n"}
{"id": "1805.10777", "contents": "Title: Object-Level Representation Learning for Few-Shot Image Classification Abstract: Few-shot learning that trains image classifiers over few labeled examples per\ncategory is a challenging task. In this paper, we propose to exploit an\nadditional big dataset with different categories to improve the accuracy of\nfew-shot learning over our target dataset. Our approach is based on the\nobservation that images can be decomposed into objects, which may appear in\nimages from both the additional dataset and our target dataset. We use the\nobject-level relation learned from the additional dataset to infer the\nsimilarity of images in our target dataset with unseen categories. Nearest\nneighbor search is applied to do image classification, which is a\nnon-parametric model and thus does not need fine-tuning. We evaluate our\nalgorithm on two popular datasets, namely Omniglot and MiniImagenet. We obtain\n8.5\\% and 2.7\\% absolute improvements for 5-way 1-shot and 5-way 5-shot\nexperiments on MiniImagenet, respectively. Source code will be published upon\nacceptance. \n\n"}
{"id": "1805.12589", "contents": "Title: Fast, Diverse and Accurate Image Captioning Guided By Part-of-Speech Abstract: Image captioning is an ambiguous problem, with many suitable captions for an\nimage. To address ambiguity, beam search is the de facto method for sampling\nmultiple captions. However, beam search is computationally expensive and known\nto produce generic captions. To address this concern, some variational\nauto-encoder (VAE) and generative adversarial net (GAN) based methods have been\nproposed. Though diverse, GAN and VAE are less accurate. In this paper, we\nfirst predict a meaningful summary of the image, then generate the caption\nbased on that summary. We use part-of-speech as summaries, since our summary\nshould drive caption generation. We achieve the trifecta: (1) High accuracy for\nthe diverse captions as evaluated by standard captioning metrics and user\nstudies; (2) Faster computation of diverse captions compared to beam search and\ndiverse beam search; and (3) High diversity as evaluated by counting novel\nsentences, distinct n-grams and mutual overlap (i.e., mBleu-4) scores. \n\n"}
{"id": "1806.00102", "contents": "Title: Respond-CAM: Analyzing Deep Models for 3D Imaging Data by Visualizations Abstract: The convolutional neural network (CNN) has become a powerful tool for various\nbiomedical image analysis tasks, but there is a lack of visual explanation for\nthe machinery of CNNs. In this paper, we present a novel algorithm,\nRespond-weighted Class Activation Mapping (Respond-CAM), for making CNN-based\nmodels interpretable by visualizing input regions that are important for\npredictions, especially for biomedical 3D imaging data inputs. Our method uses\nthe gradients of any target concept (e.g. the score of target class) that flows\ninto a convolutional layer. The weighted feature maps are combined to produce a\nheatmap that highlights the important regions in the image for predicting the\ntarget concept. We prove a preferable sum-to-score property of the Respond-CAM\nand verify its significant improvement on 3D images from the current\nstate-of-the-art approach. Our tests on Cellular Electron Cryo-Tomography 3D\nimages show that Respond-CAM achieves superior performance on visualizing the\nCNNs with 3D biomedical images inputs, and is able to get reasonably good\nresults on visualizing the CNNs with natural image inputs. The Respond-CAM is\nan efficient and reliable approach for visualizing the CNN machinery, and is\napplicable to a wide variety of CNN model families and image analysis tasks. \n\n"}
{"id": "1806.00672", "contents": "Title: Optimal Clustering under Uncertainty Abstract: Classical clustering algorithms typically either lack an underlying\nprobability framework to make them predictive or focus on parameter estimation\nrather than defining and minimizing a notion of error. Recent work addresses\nthese issues by developing a probabilistic framework based on the theory of\nrandom labeled point processes and characterizing a Bayes clusterer that\nminimizes the number of misclustered points. The Bayes clusterer is analogous\nto the Bayes classifier. Whereas determining a Bayes classifier requires full\nknowledge of the feature-label distribution, deriving a Bayes clusterer\nrequires full knowledge of the point process. When uncertain of the point\nprocess, one would like to find a robust clusterer that is optimal over the\nuncertainty, just as one may find optimal robust classifiers with uncertain\nfeature-label distributions. Herein, we derive an optimal robust clusterer by\nfirst finding an effective random point process that incorporates all\nrandomness within its own probabilistic structure and from which a Bayes\nclusterer can be derived that provides an optimal robust clusterer relative to\nthe uncertainty. This is analogous to the use of effective class-conditional\ndistributions in robust classification. After evaluating the performance of\nrobust clusterers in synthetic mixtures of Gaussians models, we apply the\nframework to granular imaging, where we make use of the asymptotic\ngranulometric moment theory for granular images to relate robust clustering\ntheory to the application. \n\n"}
{"id": "1806.01331", "contents": "Title: Precise Runtime Analysis for Plateau Functions Abstract: To gain a better theoretical understanding of how evolutionary algorithms\n(EAs) cope with plateaus of constant fitness, we propose the $n$-dimensional\nPlateau$_k$ function as natural benchmark and analyze how different variants of\nthe $(1 + 1)$ EA optimize it. The Plateau$_k$ function has a plateau of\nsecond-best fitness in a ball of radius $k$ around the optimum. As evolutionary\nalgorithm, we regard the $(1 + 1)$ EA using an arbitrary unbiased mutation\noperator. Denoting by $\\alpha$ the random number of bits flipped in an\napplication of this operator and assuming that $\\Pr[\\alpha = 1]$ has at least\nsome small sub-constant value, we show the surprising result that for all\nconstant $k \\ge 2$, the runtime $T$ follows a distribution close to the\ngeometric one with success probability equal to the probability to flip between\n$1$ and $k$ bits divided by the size of the plateau. Consequently, the expected\nruntime is the inverse of this number, and thus only depends on the probability\nto flip between $1$ and $k$ bits, but not on other characteristics of the\nmutation operator. Our result also implies that the optimal mutation rate for\nstandard bit mutation here is approximately $k/(en)$. Our main analysis tool is\na combined analysis of the Markov chains on the search point space and on the\nHamming level space, an approach that promises to be useful also for other\nplateau problems. \n\n"}
{"id": "1806.02400", "contents": "Title: A Comparative Study on Unsupervised Domain Adaptation Approaches for\n  Coffee Crop Mapping Abstract: In this work, we investigate the application of existing unsupervised domain\nadaptation (UDA) approaches to the task of transferring knowledge between crop\nregions having different coffee patterns. Given a geographical region with\nfully mapped coffee plantations, we observe that this knowledge can be used to\ntrain a classifier and to map a new county with no need of samples indicated in\nthe target region. Experimental results show that transferring knowledge via\nsome UDA strategies performs better than just applying a classifier trained in\na region to predict coffee crops in a new one. However, UDA methods may lead to\nnegative transfer, which may indicate that domains are too different that\ntransferring knowledge is not appropriate. We also verify that normalization\naffect significantly some UDA methods; we observe a meaningful complementary\ncontribution between coffee crops data; and a visual behavior suggests an\nexistent of a cluster of samples that are more likely to be drawn from a\nspecific data. \n\n"}
{"id": "1806.02583", "contents": "Title: Generative Adversarial Networks for Realistic Synthesis of Hyperspectral\n  Samples Abstract: This work addresses the scarcity of annotated hyperspectral data required to\ntrain deep neural networks. Especially, we investigate generative adversarial\nnetworks and their application to the synthesis of consistent labeled spectra.\nBy training such networks on public datasets, we show that these models are not\nonly able to capture the underlying distribution, but also to generate\ngenuine-looking and physically plausible spectra. Moreover, we experimentally\nvalidate that the synthetic samples can be used as an effective data\naugmentation strategy. We validate our approach on several public\nhyper-spectral datasets using a variety of deep classifiers. \n\n"}
{"id": "1806.03348", "contents": "Title: DSSLIC: Deep Semantic Segmentation-based Layered Image Compression Abstract: Deep learning has revolutionized many computer vision fields in the last few\nyears, including learning-based image compression. In this paper, we propose a\ndeep semantic segmentation-based layered image compression (DSSLIC) framework\nin which the semantic segmentation map of the input image is obtained and\nencoded as the base layer of the bit-stream. A compact representation of the\ninput image is also generated and encoded as the first enhancement layer. The\nsegmentation map and the compact version of the image are then employed to\nobtain a coarse reconstruction of the image. The residual between the input and\nthe coarse reconstruction is additionally encoded as another enhancement layer.\nExperimental results show that the proposed framework outperforms the\nH.265/HEVC-based BPG and other codecs in both PSNR and MS-SSIM metrics across a\nwide range of bit rates in RGB domain. Besides, since semantic segmentation map\nis included in the bit-stream, the proposed scheme can facilitate many other\ntasks such as image search and object-based adaptive image compression. \n\n"}
{"id": "1806.03361", "contents": "Title: A Content-Based Late Fusion Approach Applied to Pedestrian Detection Abstract: The variety of pedestrians detectors proposed in recent years has encouraged\nsome works to fuse pedestrian detectors to achieve a more accurate detection.\nThe intuition behind is to combine the detectors based on its spatial\nconsensus. We propose a novel method called Content-Based Spatial Consensus\n(CSBC), which, in addition to relying on spatial consensus, considers the\ncontent of the detection windows to learn a weighted-fusion of pedestrian\ndetectors. The result is a reduction in false alarms and an enhancement in the\ndetection. In this work, we also demonstrate that there is small influence of\nthe feature used to learn the contents of the windows of each detector, which\nenables our method to be efficient even employing simple features. The CSBC\novercomes state-of-the-art fusion methods in the ETH dataset and in the Caltech\ndataset. Particularly, our method is more efficient since fewer detectors are\nnecessary to achieve expressive results. \n\n"}
{"id": "1806.03583", "contents": "Title: IVUS-Net: An Intravascular Ultrasound Segmentation Network Abstract: IntraVascular UltraSound (IVUS) is one of the most effective imaging\nmodalities that provides assistance to experts in order to diagnose and treat\ncardiovascular diseases. We address a central problem in IVUS image analysis\nwith Fully Convolutional Network (FCN): automatically delineate the lumen and\nmedia-adventitia borders in IVUS images, which is crucial to shorten the\ndiagnosis process or benefits a faster and more accurate 3D reconstruction of\nthe artery. Particularly, we propose an FCN architecture, called IVUS-Net,\nfollowed by a post-processing contour extraction step, in order to\nautomatically segments the interior (lumen) and exterior (media-adventitia)\nregions of the human arteries. We evaluated our IVUS-Net on the test set of a\nstandard publicly available dataset containing 326 IVUS B-mode images with two\nmeasurements, namely Jaccard Measure (JM) and Hausdorff Distances (HD). The\nevaluation result shows that IVUS-Net outperforms the state-of-the-art lumen\nand media segmentation methods by 4% to 20% in terms of HD distance. IVUS-Net\nperforms well on images in the test set that contain a significant amount of\nmajor artifacts such as bifurcations, shadows, and side branches that are not\ncommon in the training set. Furthermore, using a modern GPU, IVUS-Net segments\neach IVUS frame only in 0.15 seconds. The proposed work, to the best of our\nknowledge, is the first deep learning based method for segmentation of both the\nlumen and the media vessel walls in 20 MHz IVUS B-mode images that achieves the\nbest results without any manual intervention. Code is available at\nhttps://github.com/Kulbear/ivus-segmentation-icsm2018 \n\n"}
{"id": "1806.03589", "contents": "Title: Free-Form Image Inpainting with Gated Convolution Abstract: We present a generative image inpainting system to complete images with\nfree-form mask and guidance. The system is based on gated convolutions learned\nfrom millions of images without additional labelling efforts. The proposed\ngated convolution solves the issue of vanilla convolution that treats all input\npixels as valid ones, generalizes partial convolution by providing a learnable\ndynamic feature selection mechanism for each channel at each spatial location\nacross all layers. Moreover, as free-form masks may appear anywhere in images\nwith any shape, global and local GANs designed for a single rectangular mask\nare not applicable. Thus, we also present a patch-based GAN loss, named\nSN-PatchGAN, by applying spectral-normalized discriminator on dense image\npatches. SN-PatchGAN is simple in formulation, fast and stable in training.\nResults on automatic image inpainting and user-guided extension demonstrate\nthat our system generates higher-quality and more flexible results than\nprevious methods. Our system helps user quickly remove distracting objects,\nmodify image layouts, clear watermarks and edit faces. Code, demo and models\nare available at: https://github.com/JiahuiYu/generative_inpainting \n\n"}
{"id": "1806.03772", "contents": "Title: DOOBNet: Deep Object Occlusion Boundary Detection from an Image Abstract: Object occlusion boundary detection is a fundamental and crucial research\nproblem in computer vision. This is challenging to solve as encountering the\nextreme boundary/non-boundary class imbalance during training an object\nocclusion boundary detector. In this paper, we propose to address this class\nimbalance by up-weighting the loss contribution of false negative and false\npositive examples with our novel Attention Loss function. We also propose a\nunified end-to-end multi-task deep object occlusion boundary detection network\n(DOOBNet) by sharing convolutional features to simultaneously predict object\nboundary and occlusion orientation. DOOBNet adopts an encoder-decoder structure\nwith skip connection in order to automatically learn multi-scale and\nmulti-level features. We significantly surpass the state-of-the-art on the PIOD\ndataset (ODS F-score of .702) and the BSDS ownership dataset (ODS F-score of\n.555), as well as improving the detecting speed to as 0.037s per image on the\nPIOD dataset. \n\n"}
{"id": "1806.03981", "contents": "Title: Rethinking Radiology: An Analysis of Different Approaches to BraTS Abstract: This paper discusses the deep learning architectures currently used for\npixel-wise segmentation of primary and secondary glioblastomas and low-grade\ngliomas. We implement various models such as the popular UNet architecture and\ncompare the performance of these implementations on the BRATS dataset. This\npaper will explore the different approaches and combinations, offering an in\ndepth discussion of how they perform and how we may improve upon them using\nmore recent advancements in deep learning architectures. \n\n"}
{"id": "1806.04009", "contents": "Title: Contextual Hourglass Networks for Segmentation and Density Estimation Abstract: Hourglass networks such as the U-Net and V-Net are popular neural\narchitectures for medical image segmentation and counting problems. Typical\ninstances of hourglass networks contain shortcut connections between mirroring\nlayers. These shortcut connections improve the performance and it is\nhypothesized that this is due to mitigating effects on the vanishing gradient\nproblem and the ability of the model to combine feature maps from earlier and\nlater layers. We propose a method for not only combining feature maps of\nmirroring layers but also feature maps of layers with different spatial\ndimensions. For instance, the method enables the integration of the bottleneck\nfeature map with those of the reconstruction layers. The proposed approach is\napplicable to any hourglass architecture. We evaluated the contextual hourglass\nnetworks on image segmentation and object counting problems in the medical\ndomain. We achieve competitive results outperforming popular hourglass networks\nby up to 17 percentage points. \n\n"}
{"id": "1806.04360", "contents": "Title: MSplit LBI: Realizing Feature Selection and Dense Estimation\n  Simultaneously in Few-shot and Zero-shot Learning Abstract: It is one typical and general topic of learning a good embedding model to\nefficiently learn the representation coefficients between two spaces/subspaces.\nTo solve this task, $L_{1}$ regularization is widely used for the pursuit of\nfeature selection and avoiding overfitting, and yet the sparse estimation of\nfeatures in $L_{1}$ regularization may cause the underfitting of training data.\n$L_{2}$ regularization is also frequently used, but it is a biased estimator.\nIn this paper, we propose the idea that the features consist of three\northogonal parts, \\emph{namely} sparse strong signals, dense weak signals and\nrandom noise, in which both strong and weak signals contribute to the fitting\nof data. To facilitate such novel decomposition, \\emph{MSplit} LBI is for the\nfirst time proposed to realize feature selection and dense estimation\nsimultaneously. We provide theoretical and simulational verification that our\nmethod exceeds $L_{1}$ and $L_{2}$ regularization, and extensive experimental\nresults show that our method achieves state-of-the-art performance in the\nfew-shot and zero-shot learning. \n\n"}
{"id": "1806.04391", "contents": "Title: Qiniu Submission to ActivityNet Challenge 2018 Abstract: In this paper, we introduce our submissions for the tasks of trimmed activity\nrecognition (Kinetics) and trimmed event recognition (Moments in Time) for\nActivitynet Challenge 2018. In the two tasks, non-local neural networks and\ntemporal segment networks are implemented as our base models. Multi-modal cues\nsuch as RGB image, optical flow and acoustic signal have also been used in our\nmethod. We also propose new non-local-based models for further improvement on\nthe recognition accuracy. The final submissions after ensembling the models\nachieve 83.5% top-1 accuracy and 96.8% top-5 accuracy on the Kinetics\nvalidation set, 35.81% top-1 accuracy and 62.59% top-5 accuracy on the MIT\nvalidation set. \n\n"}
{"id": "1806.04845", "contents": "Title: Interpretable Partitioned Embedding for Customized Fashion Outfit\n  Composition Abstract: Intelligent fashion outfit composition becomes more and more popular in these\nyears. Some deep learning based approaches reveal competitive composition\nrecently. However, the unexplainable characteristic makes such deep learning\nbased approach cannot meet the the designer, businesses and consumers' urge to\ncomprehend the importance of different attributes in an outfit composition. To\nrealize interpretable and customized fashion outfit compositions, we propose a\npartitioned embedding network to learn interpretable representations from\nclothing items. The overall network architecture consists of three components:\nan auto-encoder module, a supervised attributes module and a multi-independent\nmodule. The auto-encoder module serves to encode all useful information into\nthe embedding. In the supervised attributes module, multiple attributes labels\nare adopted to ensure that different parts of the overall embedding correspond\nto different attributes. In the multi-independent module, adversarial operation\nare adopted to fulfill the mutually independent constraint. With the\ninterpretable and partitioned embedding, we then construct an outfit\ncomposition graph and an attribute matching map. Given specified attributes\ndescription, our model can recommend a ranked list of outfit composition with\ninterpretable matching scores. Extensive experiments demonstrate that 1) the\npartitioned embedding have unmingled parts which corresponding to different\nattributes and 2) outfits recommended by our model are more desirable in\ncomparison with the existing methods. \n\n"}
{"id": "1806.05182", "contents": "Title: Fully Convolutional Network for Automatic Road Extraction from Satellite\n  Imagery Abstract: Analysis of high-resolution satellite images has been an important research\ntopic for traffic management, city planning, and road monitoring. One of the\nproblems here is automatic and precise road extraction. From an original image,\nit is difficult and computationally expensive to extract roads due to presences\nof other road-like features with straight edges. In this paper, we propose an\napproach for automatic road extraction based on a fully convolutional neural\nnetwork of U-net family. This network consists of ResNet-34 pre-trained on\nImageNet and decoder adapted from vanilla U-Net. Based on validation results,\nleaderboard and our own experience this network shows superior results for the\nDEEPGLOBE - CVPR 2018 road extraction sub-challenge. Moreover, this network\nuses moderate memory that allows using just one GTX 1080 or 1080ti video cards\nto perform whole training and makes pretty fast predictions. \n\n"}
{"id": "1806.05217", "contents": "Title: Impostor Networks for Fast Fine-Grained Recognition Abstract: In this work we introduce impostor networks, an architecture that allows to\nperform fine-grained recognition with high accuracy and using a light-weight\nconvolutional network, making it particularly suitable for fine-grained\napplications on low-power and non-GPU enabled platforms. Impostor networks\ncompensate for the lightness of its `backend' network by combining it with a\nlightweight non-parametric classifier. The combination of a convolutional\nnetwork and such non-parametric classifier is trained in an end-to-end fashion.\nSimilarly to convolutional neural networks, impostor networks can fit\nlarge-scale training datasets very well, while also being able to generalize to\nnew data points. At the same time, the bulk of computations within impostor\nnetworks happen through nearest neighbor search in high-dimensions. Such search\ncan be performed efficiently on a variety of architectures including standard\nCPUs, where deep convolutional networks are inefficient. In a series of\nexperiments with three fine-grained datasets, we show that impostor networks\nare able to boost the classification accuracy of a moderate-sized convolutional\nnetwork considerably at a very small computational cost. \n\n"}
{"id": "1806.05382", "contents": "Title: PCAS: Pruning Channels with Attention Statistics for Deep Network\n  Compression Abstract: Compression techniques for deep neural networks are important for\nimplementing them on small embedded devices. In particular, channel-pruning is\na useful technique for realizing compact networks. However, many conventional\nmethods require manual setting of compression ratios in each layer. It is\ndifficult to analyze the relationships between all layers, especially for\ndeeper models. To address these issues, we propose a simple channel-pruning\ntechnique based on attention statistics that enables to evaluate the importance\nof channels. We improved the method by means of a criterion for automatic\nchannel selection, using a single compression ratio for the entire model in\nplace of per-layer model analysis. The proposed approach achieved superior\nperformance over conventional methods with respect to accuracy and the\ncomputational costs for various models and datasets. We provide analysis\nresults for behavior of the proposed criterion on different datasets to\ndemonstrate its favorable properties for channel pruning. \n\n"}
{"id": "1806.06004", "contents": "Title: Partially-Supervised Image Captioning Abstract: Image captioning models are becoming increasingly successful at describing\nthe content of images in restricted domains. However, if these models are to\nfunction in the wild - for example, as assistants for people with impaired\nvision - a much larger number and variety of visual concepts must be\nunderstood. To address this problem, we teach image captioning models new\nvisual concepts from labeled images and object detection datasets. Since image\nlabels and object classes can be interpreted as partial captions, we formulate\nthis problem as learning from partially-specified sequence data. We then\npropose a novel algorithm for training sequence models, such as recurrent\nneural networks, on partially-specified sequences which we represent using\nfinite state automata. In the context of image captioning, our method lifts the\nrestriction that previously required image captioning models to be trained on\npaired image-sentence corpora only, or otherwise required specialized model\narchitectures to take advantage of alternative data modalities. Applying our\napproach to an existing neural captioning model, we achieve state of the art\nresults on the novel object captioning task using the COCO dataset. We further\nshow that we can train a captioning model to describe new visual concepts from\nthe Open Images dataset while maintaining competitive COCO evaluation scores. \n\n"}
{"id": "1806.06594", "contents": "Title: Deep Recurrent Neural Network for Multi-target Filtering Abstract: This paper addresses the problem of fixed motion and measurement models for\nmulti-target filtering using an adaptive learning framework. This is performed\nby defining target tuples with random finite set terminology and utilisation of\nrecurrent neural networks with a long short-term memory architecture. A novel\ndata association algorithm compatible with the predicted tracklet tuples is\nproposed, enabling the update of occluded targets, in addition to assigning\nbirth, survival and death of targets. The algorithm is evaluated over a\ncommonly used filtering simulation scenario, with highly promising results. \n\n"}
{"id": "1806.06927", "contents": "Title: Auto-Meta: Automated Gradient Based Meta Learner Search Abstract: Fully automating machine learning pipelines is one of the key challenges of\ncurrent artificial intelligence research, since practical machine learning\noften requires costly and time-consuming human-powered processes such as model\ndesign, algorithm development, and hyperparameter tuning. In this paper, we\nverify that automated architecture search synergizes with the effect of\ngradient-based meta learning. We adopt the progressive neural architecture\nsearch \\cite{liu:pnas_google:DBLP:journals/corr/abs-1712-00559} to find optimal\narchitectures for meta-learners. The gradient based meta-learner whose\narchitecture was automatically found achieved state-of-the-art results on the\n5-shot 5-way Mini-ImageNet classification problem with $74.65\\%$ accuracy,\nwhich is $11.54\\%$ improvement over the result obtained by the first\ngradient-based meta-learner called MAML\n\\cite{finn:maml:DBLP:conf/icml/FinnAL17}. To our best knowledge, this work is\nthe first successful neural architecture search implementation in the context\nof meta learning. \n\n"}
{"id": "1806.07073", "contents": "Title: Transfer Learning with Human Corneal Tissues: An Analysis of Optimal\n  Cut-Off Layer Abstract: Transfer learning is a powerful tool to adapt trained neural networks to new\ntasks. Depending on the similarity of the original task to the new task, the\nselection of the cut-off layer is critical. For medical applications like\ntissue classification, the last layers of an object classification network\nmight not be optimal. We found that on real data of human corneal tissues the\nbest feature representation can be found in the middle layers of the\nInception-v3 and in the rear layers of the VGG-19 architecture. \n\n"}
{"id": "1806.07822", "contents": "Title: Learning Neural Parsers with Deterministic Differentiable Imitation\n  Learning Abstract: We explore the problem of learning to decompose spatial tasks into segments,\nas exemplified by the problem of a painting robot covering a large object.\nInspired by the ability of classical decision tree algorithms to construct\nstructured partitions of their input spaces, we formulate the problem of\ndecomposing objects into segments as a parsing approach. We make the insight\nthat the derivation of a parse-tree that decomposes the object into segments\nclosely resembles a decision tree constructed by ID3, which can be done when\nthe ground-truth available. We learn to imitate an expert parsing oracle, such\nthat our neural parser can generalize to parse natural images without ground\ntruth. We introduce a novel deterministic policy gradient update, DRAG (i.e.,\nDeteRministically AGgrevate) in the form of a deterministic actor-critic\nvariant of AggreVaTeD, to train our neural parser. From another perspective,\nour approach is a variant of the Deterministic Policy Gradient suitable for the\nimitation learning setting. The deterministic policy representation offered by\ntraining our neural parser with DRAG allows it to outperform state of the art\nimitation and reinforcement learning approaches. \n\n"}
{"id": "1806.08047", "contents": "Title: Flexible Neural Representation for Physics Prediction Abstract: Humans have a remarkable capacity to understand the physical dynamics of\nobjects in their environment, flexibly capturing complex structures and\ninteractions at multiple levels of detail. Inspired by this ability, we propose\na hierarchical particle-based object representation that covers a wide variety\nof types of three-dimensional objects, including both arbitrary rigid\ngeometrical shapes and deformable materials. We then describe the Hierarchical\nRelation Network (HRN), an end-to-end differentiable neural network based on\nhierarchical graph convolution, that learns to predict physical dynamics in\nthis representation. Compared to other neural network baselines, the HRN\naccurately handles complex collisions and nonrigid deformations, generating\nplausible dynamics predictions at long time scales in novel settings, and\nscaling to large scene configurations. These results demonstrate an\narchitecture with the potential to form the basis of next-generation physics\npredictors for use in computer vision, robotics, and quantitative cognitive\nscience. \n\n"}
{"id": "1806.08251", "contents": "Title: Learning Multimodal Representations for Unseen Activities Abstract: We present a method to learn a joint multimodal representation space that\nenables recognition of unseen activities in videos. We first compare the effect\nof placing various constraints on the embedding space using paired text and\nvideo data. We also propose a method to improve the joint embedding space using\nan adversarial formulation, allowing it to benefit from unpaired text and video\ndata. By using unpaired text data, we show the ability to learn a\nrepresentation that better captures unseen activities.\n  In addition to testing on publicly available datasets, we introduce a new,\nlarge-scale text/video dataset.\n  We experimentally confirm that using paired and unpaired data to learn a\nshared embedding space benefits three difficult tasks (i) zero-shot activity\nclassification, (ii) unsupervised activity discovery, and (iii) unseen activity\ncaptioning, outperforming the state-of-the-arts. \n\n"}
{"id": "1806.08514", "contents": "Title: Virtual Codec Supervised Re-Sampling Network for Image Compression Abstract: In this paper, we propose an image re-sampling compression method by learning\nvirtual codec network (VCN) to resolve the non-differentiable problem of\nquantization function for image compression. Here, the image re-sampling not\nonly refers to image full-resolution re-sampling but also low-resolution\nre-sampling. We generalize this method for standard-compliant image compression\n(SCIC) framework and deep neural networks based compression (DNNC) framework.\nSpecifically, an input image is measured by re-sampling network (RSN) network\nto get re-sampled vectors. Then, these vectors are directly quantized in the\nfeature space in SCIC, or discrete cosine transform coefficients of these\nvectors are quantized to further improve coding efficiency in DNNC. At the\nencoder, the quantized vectors or coefficients are losslessly compressed by\narithmetic coding. At the receiver, the decoded vectors are utilized to restore\ninput image by image decoder network (IDN). In order to train RSN network and\nIDN network together in an end-to-end fashion, our VCN network intimates\nprojection from the re-sampled vectors to the IDN-decoded image. As a result,\ngradients from IDN network to RSN network can be approximated by VCN network's\ngradient. Because dimension reduction can be further achieved by quantization\nin some dimensional space after image re-sampling within auto-encoder\narchitecture, we can well initialize our networks from pre-trained auto-encoder\nnetworks. Through extensive experiments and analysis, it is verified that the\nproposed method has more effectiveness and versatility than many\nstate-of-the-art approaches. \n\n"}
{"id": "1806.08522", "contents": "Title: Efficient Semantic Segmentation using Gradual Grouping Abstract: Deep CNNs for semantic segmentation have high memory and run time\nrequirements. Various approaches have been proposed to make CNNs efficient like\ngrouped, shuffled, depth-wise separable convolutions. We study the\neffectiveness of these techniques on a real-time semantic segmentation\narchitecture like ERFNet for improving run time by over 5X. We apply these\ntechniques to CNN layers partially or fully and evaluate the testing accuracies\non Cityscapes dataset. We obtain accuracy vs parameters/FLOPs trade offs,\ngiving accuracy scores for models that can run under specified runtime budgets.\nWe further propose a novel training procedure which starts out with a dense\nconvolution but gradually evolves towards a grouped convolution. We show that\nour proposed training method and efficient architecture design can improve\naccuracies by over 8% with depth wise separable convolutions applied on the\nencoder of ERFNet and attaching a light weight decoder. This results in a model\nwhich has a 5X improvement in FLOPs while only suffering a 4% degradation in\naccuracy with respect to ERFNet. \n\n"}
{"id": "1806.08859", "contents": "Title: A deep learning framework for segmentation of retinal layers from OCT\n  images Abstract: Segmentation of retinal layers from Optical Coherence Tomography (OCT)\nvolumes is a fundamental problem for any computer aided diagnostic algorithm\ndevelopment. This requires preprocessing steps such as denoising, region of\ninterest extraction, flattening and edge detection all of which involve\nseparate parameter tuning. In this paper, we explore deep learning techniques\nto automate all these steps and handle the presence/absence of pathologies. A\nmodel is proposed consisting of a combination of Convolutional Neural Network\n(CNN) and Long Short Term Memory (LSTM). The CNN is used to extract layers of\ninterest image and extract the edges, while the LSTM is used to trace the layer\nboundary. This model is trained on a mixture of normal and AMD cases using\nminimal data. Validation results on three public datasets show that the\npixel-wise mean absolute error obtained with our system is 1.30 plus or minus\n0.48 which is lower than the inter-marker error of 1.79 plus or minus 0.76. Our\nmodel's performance is also on par with the existing methods. \n\n"}
{"id": "1806.09093", "contents": "Title: Analysis of Cellular Feature Differences of Astrocytomas with Distinct\n  Mutational Profiles Using Digitized Histopathology Images Abstract: Cellular phenotypic features derived from histopathology images are the basis\nof pathologic diagnosis and are thought to be related to underlying molecular\nprofiles. Due to overwhelming cell numbers and population heterogeneity, it\nremains challenging to quantitatively compute and compare features of cells\nwith distinct molecular signatures. In this study, we propose a self-reliant\nand efficient analysis framework that supports quantitative analysis of\ncellular phenotypic difference across distinct molecular groups. To demonstrate\nefficacy, we quantitatively analyze astrocytomas that are molecularly\ncharacterized as either Isocitrate Dehydrogenase (IDH) mutant (MUT) or wildtype\n(WT) using imaging data from The Cancer Genome Atlas database. Representative\ncell instances that are phenotypically different between these two groups are\nretrieved after segmentation, feature computation, data pruning, dimensionality\nreduction, and unsupervised clustering. Our analysis is generic and can be\napplied to a wide set of cell-based biomedical research. \n\n"}
{"id": "1806.09170", "contents": "Title: Fusion of complex networks and randomized neural networks for texture\n  analysis Abstract: This paper presents a high discriminative texture analysis method based on\nthe fusion of complex networks and randomized neural networks. In this\napproach, the input image is modeled as a complex networks and its topological\nproperties as well as the image pixels are used to train randomized neural\nnetworks in order to create a signature that represents the deep\ncharacteristics of the texture. The results obtained surpassed the accuracies\nof many methods available in the literature. This performance demonstrates that\nour proposed approach opens a promising source of research, which consists of\nexploring the synergy of neural networks and complex networks in the texture\nanalysis field. \n\n"}
{"id": "1806.09613", "contents": "Title: Attention-based Few-Shot Person Re-identification Using Meta Learning Abstract: In this paper, we investigate the challenging task of person\nre-identification from a new perspective and propose an end-to-end\nattention-based architecture for few-shot re-identification through\nmeta-learning. The motivation for this task lies in the fact that humans, can\nusually identify another person after just seeing that given person a few times\n(or even once) by attending to their memory. On the other hand, the unique\nnature of the person re-identification problem, i.e., only few examples exist\nper identity and new identities always appearing during testing, calls for a\nfew shot learning architecture with the capacity of handling new identities.\nHence, we frame the problem within a meta-learning setting, where a neural\nnetwork based meta-learner is trained to optimize a learner i.e., an\nattention-based matching function. Another challenge of the person\nre-identification problem is the small inter-class difference between different\nidentities and large intra-class difference of the same identity. In order to\nincrease the discriminative power of the model, we propose a new\nattention-based feature encoding scheme that takes into account the critical\nintra-view and cross-view relationship of images. We refer to the proposed\nAttention-based Re-identification Metalearning model as ARM. Extensive\nevaluations demonstrate the advantages of the ARM as compared to the\nstate-of-the-art on the challenging PRID2011, CUHK01, CUHK03 and Market1501\ndatasets. \n\n"}
{"id": "1806.09764", "contents": "Title: Deep Generative Models with Learnable Knowledge Constraints Abstract: The broad set of deep generative models (DGMs) has achieved remarkable\nadvances. However, it is often difficult to incorporate rich structured domain\nknowledge with the end-to-end DGMs. Posterior regularization (PR) offers a\nprincipled framework to impose structured constraints on probabilistic models,\nbut has limited applicability to the diverse DGMs that can lack a Bayesian\nformulation or even explicit density evaluation. PR also requires constraints\nto be fully specified a priori, which is impractical or suboptimal for complex\nknowledge with learnable uncertain parts. In this paper, we establish\nmathematical correspondence between PR and reinforcement learning (RL), and,\nbased on the connection, expand PR to learn constraints as the extrinsic reward\nin RL. The resulting algorithm is model-agnostic to apply to any DGMs, and is\nflexible to adapt arbitrary constraints with the model jointly. Experiments on\nhuman image generation and templated sentence generation show models with\nlearned knowledge constraints by our algorithm greatly improve over base\ngenerative models. \n\n"}
{"id": "1806.09789", "contents": "Title: On an Immuno-inspired Distributed, Embodied Action-Evolution cum\n  Selection Algorithm Abstract: Traditional Evolutionary Robotics (ER) employs evolutionary techniques to\nsearch for a single monolithic controller which can aid a robot to learn a\ndesired task. These techniques suffer from bootstrap and deception issues when\nthe tasks are complex for a single controller to learn. Behaviour-decomposition\ntechniques have been used to divide a task into multiple subtasks and evolve\nseparate subcontrollers for each subtask. However, these subcontrollers and the\nassociated subcontroller arbitrator(s) are all evolved off-line. A distributed,\nfully embodied and evolutionary version of such approaches will greatly aid\nonline learning and help reduce the reality gap. In this paper, we propose an\nimmunology-inspired embodied action-evolution cum selection algorithm that can\ncater to distributed ER. This algorithm evolves different subcontrollers for\ndifferent portions of the search space in a distributed manner just as\nantibodies are evolved and primed for different antigens in the antigenic\nspace. Experimentation on a collective of real robots embodied with the\nalgorithm showed that a repertoire of antibody-like subcontrollers was created,\nevolved and shared on-the-fly to cope up with different environmental\nconditions. In addition, instead of the conventionally used approach of\nbroadcasting for sharing, we present an Intelligent Packet Migration scheme\nthat reduces energy consumption. \n\n"}
{"id": "1806.10269", "contents": "Title: Collaborative Annotation of Semantic Objects in Images with\n  Multi-granularity Supervisions Abstract: Per-pixel masks of semantic objects are very useful in many applications,\nwhich, however, are tedious to be annotated. In this paper, we propose a\nhuman-agent collaborative annotation approach that can efficiently generate\nper-pixel masks of semantic objects in tagged images with multi-granularity\nsupervisions. Given a set of tagged image, a computer agent is first\ndynamically generated to roughly localize the semantic objects described by the\ntag. The agent first extracts massive object proposals from an image and then\ninfer the tag-related ones under the weak and strong supervisions from\nlinguistically and visually similar images and previously annotated object\nmasks. By representing such supervisions by over-complete dictionaries, the\ntag-related object proposals can pop-out according to their sparse coding\nlength, which are then converted to superpixels with binary labels. After that,\nhuman annotators participate in the annotation process by flipping labels and\ndividing superpixels with mouse clicks, which are used as click supervisions\nthat teach the agent to recover false positives/negatives in processing images\nwith the same tags. Experimental results show that our approach can facilitate\nthe annotation process and generate object masks that are highly consistent\nwith those generated by the LabelMe toolbox. \n\n"}
{"id": "1806.10319", "contents": "Title: Exploiting Spatial-Temporal Modelling and Multi-Modal Fusion for Human\n  Action Recognition Abstract: In this report, our approach to tackling the task of ActivityNet 2018\nKinetics-600 challenge is described in detail. Though spatial-temporal\nmodelling methods, which adopt either such end-to-end framework as I3D\n\\cite{i3d} or two-stage frameworks (i.e., CNN+RNN), have been proposed in\nexisting state-of-the-arts for this task, video modelling is far from being\nwell solved. In this challenge, we propose spatial-temporal network (StNet) for\nbetter joint spatial-temporal modelling and comprehensively video\nunderstanding. Besides, given that multi-modal information is contained in\nvideo source, we manage to integrate both early-fusion and later-fusion\nstrategy of multi-modal information via our proposed improved temporal Xception\nnetwork (iTXN) for video understanding. Our StNet RGB single model achieves\n78.99\\% top-1 precision in the Kinetics-600 validation set and that of our\nimproved temporal Xception network which integrates RGB, flow and audio\nmodalities is up to 82.35\\%. After model ensemble, we achieve top-1 precision\nas high as 85.0\\% on the validation set and rank No.1 among all submissions. \n\n"}
{"id": "1806.10556", "contents": "Title: Every Pixel Counts: Unsupervised Geometry Learning with Holistic 3D\n  Motion Understanding Abstract: Learning to estimate 3D geometry in a single image by watching unlabeled\nvideos via deep convolutional network has made significant process recently.\nCurrent state-of-the-art (SOTA) methods, are based on the learning framework of\nrigid structure-from-motion, where only 3D camera ego motion is modeled for\ngeometry estimation.However, moving objects also exist in many videos, e.g.\nmoving cars in a street scene. In this paper, we tackle such motion by\nadditionally incorporating per-pixel 3D object motion into the learning\nframework, which provides holistic 3D scene flow understanding and helps single\nimage geometry estimation. Specifically, given two consecutive frames from a\nvideo, we adopt a motion network to predict their relative 3D camera pose and a\nsegmentation mask distinguishing moving objects and rigid background. An\noptical flow network is used to estimate dense 2D per-pixel correspondence. A\nsingle image depth network predicts depth maps for both images. The four types\nof information, i.e. 2D flow, camera pose, segment mask and depth maps, are\nintegrated into a differentiable holistic 3D motion parser (HMP), where\nper-pixel 3D motion for rigid background and moving objects are recovered. We\ndesign various losses w.r.t. the two types of 3D motions for training the depth\nand motion networks, yielding further error reduction for estimated geometry.\nFinally, in order to solve the 3D motion confusion from monocular videos, we\ncombine stereo images into joint training. Experiments on KITTI 2015 dataset\nshow that our estimated geometry, 3D motion and moving object masks, not only\nare constrained to be consistent, but also significantly outperforms other SOTA\nalgorithms, demonstrating the benefits of our approach. \n\n"}
{"id": "1806.11306", "contents": "Title: Excavate Condition-invariant Space by Intrinsic Encoder Abstract: As the human, we can recognize the places across a wide range of changing\nenvironmental conditions such as those caused by weathers, seasons, and\nday-night cycles. We excavate and memorize the stable semantic structure of\ndifferent places and scenes. For example, we can recognize tree whether the\nbare tree in winter or lush tree in summer. Therefore, the intrinsic features\nthat are corresponding to specific semantic contents and condition-invariant of\nappearance changes can be employed to improve the performance of long-term\nplace recognition significantly.\n  In this paper, we propose a novel intrinsic encoder that excavates the\ncondition-invariant latent space of different places under drastic appearance\nchanges. Our method excavates the space of intrinsic structure and semantic\ninformation by proposed self-supervised encoder loss. Different from previous\nlearning based place recognition methods that need paired training data of each\nplace with appearance changes, we employ the weakly-supervised strategy to\nutilize unpaired set-based training data of different environmental conditions.\n  We conduct comprehensive experiments and show that our semi-supervised\nintrinsic encoder achieves remarkable performance for place recognition under\ndrastic appearance changes. The proposed intrinsic encoder outperforms the\nstate-of-the-art image-level place recognition methods on standard benchmark\nNordland. \n\n"}
{"id": "1807.00284", "contents": "Title: Autonomous Deep Learning: A Genetic DCNN Designer for Image\n  Classification Abstract: Recent years have witnessed the breakthrough success of deep convolutional\nneural networks (DCNNs) in image classification and other vision applications.\nAlthough freeing users from the troublesome handcrafted feature extraction by\nproviding a uniform feature extraction-classification framework, DCNNs still\nrequire a handcrafted design of their architectures. In this paper, we propose\nthe genetic DCNN designer, an autonomous learning algorithm can generate a DCNN\narchitecture automatically based on the data available for a specific image\nclassification problem. We first partition a DCNN into multiple stacked meta\nconvolutional blocks and fully connected blocks, each containing the operations\nof convolution, pooling, fully connection, batch normalization, activation and\ndrop out, and thus convert the architecture into an integer vector. Then, we\nuse refined evolutionary operations, including selection, mutation and\ncrossover to evolve a population of DCNN architectures. Our results on the\nMNIST, Fashion-MNIST, EMNISTDigit, EMNIST-Letter, CIFAR10 and CIFAR100 datasets\nsuggest that the proposed genetic DCNN designer is able to produce\nautomatically DCNN architectures, whose performance is comparable to, if not\nbetter than, that of stateof- the-art DCNN models \n\n"}
{"id": "1807.00578", "contents": "Title: Classifying neuromorphic data using a deep learning framework for image\n  classification Abstract: In the field of artificial intelligence, neuromorphic computing has been\naround for several decades. Deep learning has however made much recent progress\nsuch that it consistently outperforms neuromorphic learning algorithms in\nclassification tasks in terms of accuracy. Specifically in the field of image\nclassification, neuromorphic computing has been traditionally using either the\ntemporal or rate code for encoding static images in datasets into spike trains.\nIt is only till recently, that neuromorphic vision sensors are widely used by\nthe neuromorphic research community, and provides an alternative to such\nencoding methods. Since then, several neuromorphic datasets as obtained by\napplying such sensors on image datasets (e.g. the neuromorphic CALTECH 101)\nhave been introduced. These data are encoded in spike trains and hence seem\nideal for benchmarking of neuromorphic learning algorithms. Specifically, we\ntrain a deep learning framework used for image classification on the CALTECH\n101 and a collapsed version of the neuromorphic CALTECH 101 datasets. We\nobtained an accuracy of 91.66% and 78.01% for the CALTECH 101 and neuromorphic\nCALTECH 101 datasets respectively. For CALTECH 101, our accuracy is close to\nthe best reported accuracy, while for neuromorphic CALTECH 101, it outperforms\nthe last best reported accuracy by over 10%. This raises the question of the\nsuitability of such datasets as benchmarks for neuromorphic learning\nalgorithms. \n\n"}
{"id": "1807.02389", "contents": "Title: Accelerated physical emulation of Bayesian inference in spiking neural\n  networks Abstract: The massively parallel nature of biological information processing plays an\nimportant role for its superiority to human-engineered computing devices. In\nparticular, it may hold the key to overcoming the von Neumann bottleneck that\nlimits contemporary computer architectures. Physical-model neuromorphic devices\nseek to replicate not only this inherent parallelism, but also aspects of its\nmicroscopic dynamics in analog circuits emulating neurons and synapses.\nHowever, these machines require network models that are not only adept at\nsolving particular tasks, but that can also cope with the inherent\nimperfections of analog substrates. We present a spiking network model that\nperforms Bayesian inference through sampling on the BrainScaleS neuromorphic\nplatform, where we use it for generative and discriminative computations on\nvisual data. By illustrating its functionality on this platform, we implicitly\ndemonstrate its robustness to various substrate-specific distortive effects, as\nwell as its accelerated capability for computation. These results showcase the\nadvantages of brain-inspired physical computation and provide important\nbuilding blocks for large-scale neuromorphic applications. \n\n"}
{"id": "1807.02480", "contents": "Title: A Fully Convolutional Two-Stream Fusion Network for Interactive Image\n  Segmentation Abstract: In this paper, we propose a novel fully convolutional two-stream fusion\nnetwork (FCTSFN) for interactive image segmentation. The proposed network\nincludes two sub-networks: a two-stream late fusion network (TSLFN) that\npredicts the foreground at a reduced resolution, and a multi-scale refining\nnetwork (MSRN) that refines the foreground at full resolution. The TSLFN\nincludes two distinct deep streams followed by a fusion network. The intuition\nis that, since user interactions are more direct information on\nforeground/background than the image itself, the two-stream structure of the\nTSLFN reduces the number of layers between the pure user interaction features\nand the network output, allowing the user interactions to have a more direct\nimpact on the segmentation result. The MSRN fuses the features from different\nlayers of TSLFN with different scales, in order to seek the local to global\ninformation on the foreground to refine the segmentation result at full\nresolution. We conduct comprehensive experiments on four benchmark datasets.\nThe results show that the proposed network achieves competitive performance\ncompared to current state-of-the-art interactive image segmentation methods \n\n"}
{"id": "1807.02839", "contents": "Title: Hierarchical stochastic graphlet embedding for graph-based pattern\n  recognition Abstract: Despite being very successful within the pattern recognition and machine\nlearning community, graph-based methods are often unusable because of the lack\nof mathematical operations defined in graph domain. Graph embedding, which maps\ngraphs to a vectorial space, has been proposed as a way to tackle these\ndifficulties enabling the use of standard machine learning techniques. However,\nit is well known that graph embedding functions usually suffer from the loss of\nstructural information. In this paper, we consider the hierarchical structure\nof a graph as a way to mitigate this loss of information. The hierarchical\nstructure is constructed by topologically clustering the graph nodes, and\nconsidering each cluster as a node in the upper hierarchical level. Once this\nhierarchical structure is constructed, we consider several configurations to\ndefine the mapping into a vector space given a classical graph embedding, in\nparticular, we propose to make use of the Stochastic Graphlet Embedding (SGE).\nBroadly speaking, SGE produces a distribution of uniformly sampled low to high\norder graphlets as a way to embed graphs into the vector space. In what\nfollows, the coarse-to-fine structure of a graph hierarchy and the statistics\nfetched by the SGE complements each other and includes important structural\ninformation with varied contexts. Altogether, these two techniques\nsubstantially cope with the usual information loss involved in graph embedding\ntechniques, obtaining a more robust graph representation. This fact has been\ncorroborated through a detailed experimental evaluation on various benchmark\ngraph datasets, where we outperform the state-of-the-art methods. \n\n"}
{"id": "1807.02894", "contents": "Title: Automatic Classification of Defective Photovoltaic Module Cells in\n  Electroluminescence Images Abstract: Electroluminescence (EL) imaging is a useful modality for the inspection of\nphotovoltaic (PV) modules. EL images provide high spatial resolution, which\nmakes it possible to detect even finest defects on the surface of PV modules.\nHowever, the analysis of EL images is typically a manual process that is\nexpensive, time-consuming, and requires expert knowledge of many different\ntypes of defects. In this work, we investigate two approaches for automatic\ndetection of such defects in a single image of a PV cell. The approaches differ\nin their hardware requirements, which are dictated by their respective\napplication scenarios. The more hardware-efficient approach is based on\nhand-crafted features that are classified in a Support Vector Machine (SVM). To\nobtain a strong performance, we investigate and compare various processing\nvariants. The more hardware-demanding approach uses an end-to-end deep\nConvolutional Neural Network (CNN) that runs on a Graphics Processing Unit\n(GPU). Both approaches are trained on 1,968 cells extracted from high\nresolution EL intensity images of mono- and polycrystalline PV modules. The CNN\nis more accurate, and reaches an average accuracy of 88.42%. The SVM achieves a\nslightly lower average accuracy of 82.44%, but can run on arbitrary hardware.\nBoth automated approaches make continuous, highly accurate monitoring of PV\ncells feasible. \n\n"}
{"id": "1807.03136", "contents": "Title: G2C: A Generator-to-Classifier Framework Integrating Multi-Stained\n  Visual Cues for Pathological Glomerulus Classification Abstract: Pathological glomerulus classification plays a key role in the diagnosis of\nnephropathy. As the difference between different subcategories is subtle,\ndoctors often refer to slides from different staining methods to make\ndecisions. However, creating correspondence across various stains is\nlabor-intensive, bringing major difficulties in collecting data and training a\nvision-based algorithm to assist nephropathy diagnosis. This paper provides an\nalternative solution for integrating multi-stained visual cues for glomerulus\nclassification. Our approach, named generator-to-classifier (G2C), is a\ntwo-stage framework. Given an input image from a specified stain, several\ngenerators are first applied to estimate its appearances in other staining\nmethods, and a classifier follows to combine visual cues from different stains\nfor prediction (whether it is pathological, or which type of pathology it has).\nWe optimize these two stages in a joint manner. To provide a reasonable\ninitialization, we pre-train the generators in an unlabeled reference set under\nan unpaired image-to-image translation task, and then fine-tune them together\nwith the classifier. We conduct experiments on a glomerulus type classification\ndataset collected by ourselves (there are no publicly available datasets for\nthis purpose). Although joint optimization slightly harms the authenticity of\nthe generated patches, it boosts classification performance, suggesting more\neffective visual cues are extracted in an automatic way. We also transfer our\nmodel to a public dataset for breast cancer classification, and outperform the\nstate-of-the-arts significantly. \n\n"}
{"id": "1807.03284", "contents": "Title: Pooling Pyramid Network for Object Detection Abstract: We'd like to share a simple tweak of Single Shot Multibox Detector (SSD)\nfamily of detectors, which is effective in reducing model size while\nmaintaining the same quality. We share box predictors across all scales, and\nreplace convolution between scales with max pooling. This has two advantages\nover vanilla SSD: (1) it avoids score miscalibration across scales; (2) the\nshared predictor sees the training data over all scales. Since we reduce the\nnumber of predictors to one, and trim all convolutions between them, model size\nis significantly smaller. We empirically show that these changes do not hurt\nmodel quality compared to vanilla SSD. \n\n"}
{"id": "1807.03710", "contents": "Title: Recurrent Auto-Encoder Model for Large-Scale Industrial Sensor Signal\n  Analysis Abstract: Recurrent auto-encoder model summarises sequential data through an encoder\nstructure into a fixed-length vector and then reconstructs the original\nsequence through the decoder structure. The summarised vector can be used to\nrepresent time series features. In this paper, we propose relaxing the\ndimensionality of the decoder output so that it performs partial\nreconstruction. The fixed-length vector therefore represents features in the\nselected dimensions only. In addition, we propose using rolling fixed window\napproach to generate training samples from unbounded time series data. The\nchange of time series features over time can be summarised as a smooth\ntrajectory path. The fixed-length vectors are further analysed using additional\nvisualisation and unsupervised clustering techniques. The proposed method can\nbe applied in large-scale industrial processes for sensors signal analysis\npurpose, where clusters of the vector representations can reflect the operating\nstates of the industrial system. \n\n"}
{"id": "1807.04065", "contents": "Title: Recurrent Neural Networks with Flexible Gates using Kernel Activation\n  Functions Abstract: Gated recurrent neural networks have achieved remarkable results in the\nanalysis of sequential data. Inside these networks, gates are used to control\nthe flow of information, allowing to model even very long-term dependencies in\nthe data. In this paper, we investigate whether the original gate equation (a\nlinear projection followed by an element-wise sigmoid) can be improved. In\nparticular, we design a more flexible architecture, with a small number of\nadaptable parameters, which is able to model a wider range of gating functions\nthan the classical one. To this end, we replace the sigmoid function in the\nstandard gate with a non-parametric formulation extending the recently proposed\nkernel activation function (KAF), with the addition of a residual\nskip-connection. A set of experiments on sequential variants of the MNIST\ndataset shows that the adoption of this novel gate allows to improve accuracy\nwith a negligible cost in terms of computational power and with a large\nspeed-up in the number of training iterations. \n\n"}
{"id": "1807.04880", "contents": "Title: Effective Occlusion Handling for Fast Correlation Filter-based Trackers Abstract: Correlation filter-based trackers heavily suffer from the problem of multiple\npeaks in their response maps incurred by occlusions. Moreover, the whole\ntracking pipeline may break down due to the uncertainties brought by shifting\namong peaks, which will further lead to the degraded correlation filter model.\nTo alleviate the drift problem caused by occlusions, we propose a novel scheme\nto choose the specific filter model according to different scenarios.\nSpecifically, an effective measurement function is designed to evaluate the\nquality of filter response. A sophisticated strategy is employed to judge\nwhether occlusions occur, and then decide how to update the filter models. In\naddition, we take advantage of both log-polar method and pyramid-like approach\nto estimate the best scale of the target. We evaluate our proposed approach on\nVOT2018 challenge and OTB100 dataset, whose experimental result shows that the\nproposed tracker achieves the promising performance compared against the\nstate-of-the-art trackers. \n\n"}
{"id": "1807.05119", "contents": "Title: Learning-based Natural Geometric Matching with Homography Prior Abstract: Geometric matching is a key step in computer vision tasks. Previous\nlearning-based methods for geometric matching concentrate more on improving\nalignment quality, while we argue the importance of naturalness issue\nsimultaneously. To deal with this, firstly, Pearson correlation is applied to\nhandle large intra-class variations of features in feature matching stage.\nThen, we parametrize homography transformation with 9 parameters in full\nconnected layer of our network, to better characterize large viewpoint\nvariations compared with affine transformation. Furthermore, a novel loss\nfunction with Gaussian weights guarantees the model accuracy and efficiency in\ntraining procedure. Finally, we provide two choices for different purposes in\ngeometric matching. When compositing homography with affine transformation, the\nalignment accuracy improves and all lines are preserved, which results in a\nmore natural transformed image. When compositing homography with non-rigid\nthin-plate-spline transformation, the alignment accuracy further improves.\nExperimental results on Proposal Flow dataset show that our method outperforms\nstate-of-the-art methods, both in terms of alignment accuracy and naturalness. \n\n"}
{"id": "1807.05389", "contents": "Title: 3D human pose estimation from depth maps using a deep combination of\n  poses Abstract: Many real-world applications require the estimation of human body joints for\nhigher-level tasks as, for example, human behaviour understanding. In recent\nyears, depth sensors have become a popular approach to obtain three-dimensional\ninformation. The depth maps generated by these sensors provide information that\ncan be employed to disambiguate the poses observed in two-dimensional images.\nThis work addresses the problem of 3D human pose estimation from depth maps\nemploying a Deep Learning approach. We propose a model, named Deep Depth Pose\n(DDP), which receives a depth map containing a person and a set of predefined\n3D prototype poses and returns the 3D position of the body joints of the\nperson. In particular, DDP is defined as a ConvNet that computes the specific\nweights needed to linearly combine the prototypes for the given input. We have\nthoroughly evaluated DDP on the challenging 'ITOP' and 'UBC3V' datasets, which\nrespectively depict realistic and synthetic samples, defining a new\nstate-of-the-art on them. \n\n"}
{"id": "1807.05520", "contents": "Title: Deep Clustering for Unsupervised Learning of Visual Features Abstract: Clustering is a class of unsupervised learning methods that has been\nextensively applied and studied in computer vision. Little work has been done\nto adapt it to the end-to-end training of visual features on large scale\ndatasets. In this work, we present DeepCluster, a clustering method that\njointly learns the parameters of a neural network and the cluster assignments\nof the resulting features. DeepCluster iteratively groups the features with a\nstandard clustering algorithm, k-means, and uses the subsequent assignments as\nsupervision to update the weights of the network. We apply DeepCluster to the\nunsupervised training of convolutional neural networks on large datasets like\nImageNet and YFCC100M. The resulting model outperforms the current state of the\nart by a significant margin on all the standard benchmarks. \n\n"}
{"id": "1807.05618", "contents": "Title: Improved Person Re-Identification Based on Saliency and Semantic Parsing\n  with Deep Neural Network Models Abstract: Given a video or an image of a person acquired from a camera, person\nre-identification is the process of retrieving all instances of the same person\nfrom videos or images taken from a different camera with non-overlapping view.\nThis task has applications in various fields, such as surveillance, forensics,\nrobotics, multimedia. In this paper, we present a novel framework, named\nSaliency-Semantic Parsing Re-Identification (SSP-ReID), for taking advantage of\nthe capabilities of both clues: saliency and semantic parsing maps, to guide a\nbackbone convolutional neural network (CNN) to learn complementary\nrepresentations that improves the results over the original backbones. The\ninsight of fusing multiple clues is based on specific scenarios in which one\nresponse is better than another, thus favoring the combination of them to\nincrease performance. Due to its definition, our framework can be easily\napplied to a wide variety of networks and, in contrast to other competitive\nmethods, our training process follows simple and standard protocols. We present\nextensive evaluation of our approach through five backbones and three\nbenchmarks. Experimental results demonstrate the effectiveness of our person\nre-identification framework. In addition, we combine our framework with\nre-ranking techniques to achieve state-of-the-art results on three benchmarks. \n\n"}
{"id": "1807.05653", "contents": "Title: Learning and Matching Multi-View Descriptors for Registration of Point\n  Clouds Abstract: Critical to the registration of point clouds is the establishment of a set of\naccurate correspondences between points in 3D space. The correspondence problem\nis generally addressed by the design of discriminative 3D local descriptors on\nthe one hand, and the development of robust matching strategies on the other\nhand. In this work, we first propose a multi-view local descriptor, which is\nlearned from the images of multiple views, for the description of 3D keypoints.\nThen, we develop a robust matching approach, aiming at rejecting outlier\nmatches based on the efficient inference via belief propagation on the defined\ngraphical model. We have demonstrated the boost of our approaches to\nregistration on the public scanning and multi-view stereo datasets. The\nsuperior performance has been verified by the intensive comparisons against a\nvariety of descriptors and matching methods. \n\n"}
{"id": "1807.05959", "contents": "Title: A Multimodal Approach to Predict Social Media Popularity Abstract: Multiple modalities represent different aspects by which information is\nconveyed by a data source. Modern day social media platforms are one of the\nprimary sources of multimodal data, where users use different modes of\nexpression by posting textual as well as multimedia content such as images and\nvideos for sharing information. Multimodal information embedded in such posts\ncould be useful in predicting their popularity. To the best of our knowledge,\nno such multimodal dataset exists for the prediction of social media photos. In\nthis work, we propose a multimodal dataset consisiting of content, context, and\nsocial information for popularity prediction. Specifically, we augment the\nSMPT1 dataset for social media prediction in ACM Multimedia grand challenge\n2017 with image content, titles, descriptions, and tags. Next, in this paper,\nwe propose a multimodal approach which exploits visual features (i.e., content\ninformation), textual features (i.e., contextual information), and social\nfeatures (e.g., average views and group counts) to predict popularity of social\nmedia photos in terms of view counts. Experimental results confirm that despite\nour multimodal approach uses the half of the training dataset from SMP-T1, it\nachieves comparable performance with that of state-of-the-art. \n\n"}
{"id": "1807.06466", "contents": "Title: Automatic Skin Lesion Segmentation Using Deep Fully Convolutional\n  Networks Abstract: This paper summarizes our method and validation results for the ISIC\nChallenge 2018 - Skin Lesion Analysis Towards Melanoma Detection - Task 1:\nLesion Segmentation \n\n"}
{"id": "1807.07343", "contents": "Title: Automated Phenotyping of Epicuticular Waxes of Grapevine Berries Using\n  Light Separation and Convolutional Neural Networks Abstract: In viticulture the epicuticular wax as the outer layer of the berry skin is\nknown as trait which is correlated to resilience towards Botrytis bunch rot.\nTraditionally this trait is classified using the OIV descriptor 227 (berry\nbloom) in a time consuming way resulting in subjective and error-prone\nphenotypic data. In the present study an objective, fast and sensor-based\napproach was developed to monitor berry bloom. From the technical\npoint-of-view, it is known that the measurement of different illumination\ncomponents conveys important information about observed object surfaces. A\nMobile Light-Separation-Lab is proposed in order to capture\nillumination-separated images of grapevine berries for phenotyping the\ndistribution of epicuticular waxes (berry bloom). For image analysis, an\nefficient convolutional neural network approach is used to derive the\nuniformity and intactness of waxes on berries. Method validation over six\ngrapevine cultivars shows accuracies up to $97.3$%. In addition, electrical\nimpedance of the cuticle and its epicuticular waxes (described as an indicator\nfor the thickness of berry skin and its permeability) was correlated to the\ndetected proportion of waxes with $r=0.76$. This novel, fast and non-invasive\nphenotyping approach facilitates enlarged screenings within grapevine breeding\nmaterial and genetic repositories regarding berry bloom characteristics and its\nimpact on resilience towards Botrytis bunch rot. \n\n"}
{"id": "1807.07700", "contents": "Title: Editable Generative Adversarial Networks: Generating and Editing Faces\n  Simultaneously Abstract: We propose a novel framework for simultaneously generating and manipulating\nthe face images with desired attributes. While the state-of-the-art attribute\nediting technique has achieved the impressive performance for creating\nrealistic attribute effects, they only address the image editing problem, using\nthe input image as the condition of model. Recently, several studies attempt to\ntackle both novel face generation and attribute editing problem using a single\nsolution. However, their image quality is still unsatisfactory. Our goal is to\ndevelop a single unified model that can simultaneously create and edit high\nquality face images with desired attributes. A key idea of our work is that we\ndecompose the image into the latent and attribute vector in low dimensional\nrepresentation, and then utilize the GAN framework for mapping the low\ndimensional representation to the image. In this way, we can address both the\ngeneration and editing problem by learning the generator. For qualitative and\nquantitative evaluations, the proposed algorithm outperforms recent algorithms\naddressing the same problem. Also, we show that our model can achieve the\ncompetitive performance with the state-of-the-art attribute editing technique\nin terms of attribute editing quality. \n\n"}
{"id": "1807.07716", "contents": "Title: Brain Tumor Segmentation and Tractographic Feature Extraction from\n  Structural MR Images for Overall Survival Prediction Abstract: This paper introduces a novel methodology to integrate human brain\nconnectomics and parcellation for brain tumor segmentation and survival\nprediction. For segmentation, we utilize an existing brain parcellation atlas\nin the MNI152 1mm space and map this parcellation to each individual subject\ndata. We use deep neural network architectures together with hard negative\nmining to achieve the final voxel level classification. For survival\nprediction, we present a new method for combining features from connectomics\ndata, brain parcellation information, and the brain tumor mask. We leverage the\naverage connectome information from the Human Connectome Project and map each\nsubject brain volume onto this common connectome space. From this, we compute\ntractographic features that describe potential neural disruptions due to the\nbrain tumor. These features are then used to predict the overall survival of\nthe subjects. The main novelty in the proposed methods is the use of normalized\nbrain parcellation data and tractography data from the human connectome project\nfor analyzing MR images for segmentation and survival prediction. Experimental\nresults are reported on the BraTS2018 dataset. \n\n"}
{"id": "1807.08379", "contents": "Title: Towards Privacy-Preserving Visual Recognition via Adversarial Training:\n  A Pilot Study Abstract: This paper aims to improve privacy-preserving visual recognition, an\nincreasingly demanded feature in smart camera applications, by formulating a\nunique adversarial training framework. The proposed framework explicitly learns\na degradation transform for the original video inputs, in order to optimize the\ntrade-off between target task performance and the associated privacy budgets on\nthe degraded video. A notable challenge is that the privacy budget, often\ndefined and measured in task-driven contexts, cannot be reliably indicated\nusing any single model performance, because a strong protection of privacy has\nto sustain against any possible model that tries to hack privacy information.\nSuch an uncommon situation has motivated us to propose two strategies, i.e.,\nbudget model restarting and ensemble, to enhance the generalization of the\nlearned degradation on protecting privacy against unseen hacker models. Novel\ntraining strategies, evaluation protocols, and result visualization methods\nhave been designed accordingly. Two experiments on privacy-preserving action\nrecognition, with privacy budgets defined in various ways, manifest the\ncompelling effectiveness of the proposed framework in simultaneously\nmaintaining high target task (action recognition) performance while suppressing\nthe privacy breach risk. \n\n"}
{"id": "1807.08430", "contents": "Title: Actor-Action Semantic Segmentation with Region Masks Abstract: In this paper, we study the actor-action semantic segmentation problem, which\nrequires joint labeling of both actor and action categories in video frames.\nOne major challenge for this task is that when an actor performs an action,\ndifferent body parts of the actor provide different types of cues for the\naction category and may receive inconsistent action labeling when they are\nlabeled independently. To address this issue, we propose an end-to-end\nregion-based actor-action segmentation approach which relies on region masks\nfrom an instance segmentation algorithm. Our main novelty is to avoid labeling\npixels in a region mask independently - instead we assign a single action label\nto these pixels to achieve consistent action labeling. When a pixel belongs to\nmultiple region masks, max pooling is applied to resolve labeling conflicts.\nOur approach uses a two-stream network as the front-end (which learns features\ncapturing both appearance and motion information), and uses two region-based\nsegmentation networks as the back-end (which takes the fused features from the\ntwo-stream network as the input and predicts actor-action labeling).\nExperiments on the A2D dataset demonstrate that both the region-based\nsegmentation strategy and the fused features from the two-stream network\ncontribute to the performance improvements. The proposed approach outperforms\nthe state-of-the-art results by more than 8% in mean class accuracy, and more\nthan 5% in mean class IOU, which validates its effectiveness. \n\n"}
{"id": "1807.08512", "contents": "Title: Git Loss for Deep Face Recognition Abstract: Convolutional Neural Networks (CNNs) have been widely used in computer vision\ntasks, such as face recognition and verification, and have achieved\nstate-of-the-art results due to their ability to capture discriminative deep\nfeatures. Conventionally, CNNs have been trained with softmax as supervision\nsignal to penalize the classification loss. In order to further enhance the\ndiscriminative capability of deep features, we introduce a joint supervision\nsignal, Git loss, which leverages on softmax and center loss functions. The aim\nof our loss function is to minimize the intra-class variations as well as\nmaximize the inter-class distances. Such minimization and maximization of deep\nfeatures are considered ideal for face recognition task. We perform experiments\non two popular face recognition benchmarks datasets and show that our proposed\nloss function achieves maximum separability between deep face features of\ndifferent identities and achieves state-of-the-art accuracy on two major face\nrecognition benchmark datasets: Labeled Faces in the Wild (LFW) and YouTube\nFaces (YTF). However, it should be noted that the major objective of Git loss\nis to achieve maximum separability between deep features of divergent\nidentities. \n\n"}
{"id": "1807.08931", "contents": "Title: CReaM: Condensed Real-time Models for Depth Prediction using\n  Convolutional Neural Networks Abstract: Since the resurgence of CNNs the robotic vision community has developed a\nrange of algorithms that perform classification, semantic segmentation and\nstructure prediction (depths, normals, surface curvature) using neural\nnetworks. While some of these models achieve state-of-the art results and super\nhuman level performance, deploying these models in a time critical robotic\nenvironment remains an ongoing challenge. Real-time frameworks are of paramount\nimportance to build a robotic society where humans and robots integrate\nseamlessly. To this end, we present a novel real-time structure prediction\nframework that predicts depth at 30fps on an NVIDIA-TX2. At the time of\nwriting, this is the first piece of work to showcase such a capability on a\nmobile platform. We also demonstrate with extensive experiments that neural\nnetworks with very large model capacities can be leveraged in order to train\naccurate condensed model architectures in a \"from teacher to student\" style\nknowledge transfer. \n\n"}
{"id": "1807.09169", "contents": "Title: Convolutional Simplex Projection Network (CSPN) for Weakly Supervised\n  Semantic Segmentation Abstract: Weakly supervised semantic segmentation has been a subject of increased\ninterest due to the scarcity of fully annotated images. We introduce a new\napproach for solving weakly supervised semantic segmentation with deep\nConvolutional Neural Networks (CNNs). The method introduces a novel layer which\napplies simplex projection on the output of a neural network using area\nconstraints of class objects. The proposed method is general and can be\nseamlessly integrated into any CNN architecture. Moreover, the projection layer\nallows strongly supervised models to be adapted to weakly supervised models\neffortlessly by substituting ground truth labels. Our experiments have shown\nthat applying such an operation on the output of a CNN improves the accuracy of\nsemantic segmentation in a weakly supervised setting with image-level labels. \n\n"}
{"id": "1807.09372", "contents": "Title: A Synchronized Stereo and Plenoptic Visual Odometry Dataset Abstract: We present a new dataset to evaluate monocular, stereo, and plenoptic camera\nbased visual odometry algorithms. The dataset comprises a set of synchronized\nimage sequences recorded by a micro lens array (MLA) based plenoptic camera and\na stereo camera system. For this, the stereo cameras and the plenoptic camera\nwere assembled on a common hand-held platform. All sequences are recorded in a\nvery large loop, where beginning and end show the same scene. Therefore, the\ntracking accuracy of a visual odometry algorithm can be measured from the drift\nbetween beginning and end of the sequence. For both, the plenoptic camera and\nthe stereo system, we supply full intrinsic camera models, as well as\nvignetting data. The dataset consists of 11 sequences which were recorded in\nchallenging indoor and outdoor scenarios. We present, by way of example, the\nresults achieved by state-of-the-art algorithms. \n\n"}
{"id": "1807.09430", "contents": "Title: Semantics Meet Saliency: Exploring Domain Affinity and Models for\n  Dual-Task Prediction Abstract: Much research has examined models for prediction of semantic labels or\ninstances including dense pixel-wise prediction. The problem of predicting\nsalient objects or regions of an image has also been examined in a similar\nlight. With that said, there is an apparent relationship between these two\nproblem domains in that the composition of a scene and associated semantic\ncategories is certain to play into what is deemed salient. In this paper, we\nexplore the relationship between these two problem domains. This is carried out\nin constructing deep neural networks that perform both predictions together\nalbeit with different configurations for flow of conceptual information related\nto each distinct problem. This is accompanied by a detailed analysis of object\nco-occurrences that shed light on dataset bias and semantic precedence specific\nto individual categories. \n\n"}
{"id": "1807.10264", "contents": "Title: Layer-structured 3D Scene Inference via View Synthesis Abstract: We present an approach to infer a layer-structured 3D representation of a\nscene from a single input image. This allows us to infer not only the depth of\nthe visible pixels, but also to capture the texture and depth for content in\nthe scene that is not directly visible. We overcome the challenge posed by the\nlack of direct supervision by instead leveraging a more naturally available\nmulti-view supervisory signal. Our insight is to use view synthesis as a proxy\ntask: we enforce that our representation (inferred from a single image), when\nrendered from a novel perspective, matches the true observed image. We present\na learning framework that operationalizes this insight using a new,\ndifferentiable novel view renderer. We provide qualitative and quantitative\nvalidation of our approach in two different settings, and demonstrate that we\ncan learn to capture the hidden aspects of a scene. \n\n"}
{"id": "1807.10441", "contents": "Title: Synthetically Trained Icon Proposals for Parsing and Summarizing\n  Infographics Abstract: Widely used in news, business, and educational media, infographics are\nhandcrafted to effectively communicate messages about complex and often\nabstract topics including `ways to conserve the environment' and `understanding\nthe financial crisis'. Composed of stylistically and semantically diverse\nvisual and textual elements, infographics pose new challenges for computer\nvision. While automatic text extraction works well on infographics, computer\nvision approaches trained on natural images fail to identify the stand-alone\nvisual elements in infographics, or `icons'. To bridge this representation gap,\nwe propose a synthetic data generation strategy: we augment background patches\nin infographics from our Visually29K dataset with Internet-scraped icons which\nwe use as training data for an icon proposal mechanism. On a test set of 1K\nannotated infographics, icons are located with 38% precision and 34% recall\n(the best model trained with natural images achieves 14% precision and 7%\nrecall). Combining our icon proposals with icon classification and text\nextraction, we present a multi-modal summarization application. Our application\ntakes an infographic as input and automatically produces text tags and visual\nhashtags that are textually and visually representative of the infographic's\ntopics respectively. \n\n"}
{"id": "1807.10466", "contents": "Title: A Deep Learning Framework for Automatic Diagnosis in Lung Cancer Abstract: We developed a deep learning framework that helps to automatically identify\nand segment lung cancer areas in patients' tissue specimens. The study was\nbased on a cohort of lung cancer patients operated at the Uppsala University\nHospital. The tissues were reviewed by lung pathologists and then the cores\nwere compiled to tissue micro-arrays (TMAs). For experiments, hematoxylin-eosin\nstained slides from 712 patients were scanned and then manually annotated. Then\nthese scans and annotations were used to train segmentation models of the\ndeveloped framework. The performance of the developed deep learning framework\nwas evaluated on fully annotated TMA cores from 178 patients reaching\npixel-wise precision of 0.80 and recall of 0.86. Finally, publicly available\nStanford TMA cores were used to demonstrate high performance of the framework\nqualitatively. \n\n"}
{"id": "1807.10574", "contents": "Title: Deep Learning Hyperspectral Image Classification Using Multiple\n  Class-based Denoising Autoencoders, Mixed Pixel Training Augmentation, and\n  Morphological Operations Abstract: Herein, we present a system for hyperspectral image segmentation that\nutilizes multiple class--based denoising autoencoders which are efficiently\ntrained. Moreover, we present a novel hyperspectral data augmentation method\nfor labelled HSI data using linear mixtures of pixels from each class, which\nhelps the system with edge pixels which are almost always mixed pixels.\nFinally, we utilize a deep neural network and morphological hole-filling to\nprovide robust image classification. Results run on the Salinas dataset verify\nthe high performance of the proposed algorithm. \n\n"}
{"id": "1807.10576", "contents": "Title: Visual Attention driven by Convolutional Features Abstract: The understanding of where humans look in a scene is a problem of great\ninterest in visual perception and computer vision. When eye-tracking devices\nare not a viable option, models of human attention can be used to predict\nfixations. In this paper we give two contribution. First, we show a model of\nvisual attention that is simply based on deep convolutional neural networks\ntrained for object classification tasks. A method for visualizing saliency maps\nis defined which is evaluated in a saliency prediction task. Second, we\nintegrate the information of these maps with a bottom-up differential model of\neye-movements to simulate visual attention scanpaths. Results on saliency\nprediction and scores of similarity with human scanpaths demonstrate the\neffectiveness of this model. \n\n"}
{"id": "1807.10993", "contents": "Title: U-Finger: Multi-Scale Dilated Convolutional Network for Fingerprint\n  Image Denoising and Inpainting Abstract: This paper studies the challenging problem of fingerprint image denoising and\ninpainting. To tackle the challenge of suppressing complicated artifacts (blur,\nbrightness, contrast, elastic transformation, occlusion, scratch, resolution,\nrotation, and so on) while preserving fine textures, we develop a multi-scale\nconvolutional network, termed U- Finger. Based on the domain expertise, we show\nthat the usage of dilated convolutions as well as the removal of padding have\nimportant positive impacts on the final restoration performance, in addition to\nmulti-scale cascaded feature modules. Our model achieves the overall ranking of\nNo.2 in the ECCV 2018 Chalearn LAP Inpainting Competition Track 3 (Fingerprint\nDenoising and Inpainting). Among all participating teams, we obtain the MSE of\n0.0231 (rank 2), PSNR 16.9688 dB (rank 2), and SSIM 0.8093 (rank 3) on the\nhold-out testing set. \n\n"}
{"id": "1807.11091", "contents": "Title: StructADMM: A Systematic, High-Efficiency Framework of Structured Weight\n  Pruning for DNNs Abstract: Weight pruning methods of DNNs have been demonstrated to achieve a good model\npruning rate without loss of accuracy, thereby alleviating the significant\ncomputation/storage requirements of large-scale DNNs. Structured weight pruning\nmethods have been proposed to overcome the limitation of irregular network\nstructure and demonstrated actual GPU acceleration. However, in prior work the\npruning rate (degree of sparsity) and GPU acceleration are limited (to less\nthan 50%) when accuracy needs to be maintained. In this work,we overcome these\nlimitations by proposing a unified, systematic framework of structured weight\npruning for DNNs. It is a framework that can be used to induce different types\nof structured sparsity, such as filter-wise, channel-wise, and shape-wise\nsparsity, as well non-structured sparsity. The proposed framework incorporates\nstochastic gradient descent with ADMM, and can be understood as a dynamic\nregularization method in which the regularization target is analytically\nupdated in each iteration. Without loss of accuracy on the AlexNet model, we\nachieve 2.58X and 3.65X average measured speedup on two GPUs, clearly\noutperforming the prior work. The average speedups reach 3.15X and 8.52X when\nallowing a moderate ac-curacy loss of 2%. In this case the model compression\nfor convolutional layers is 15.0X, corresponding to 11.93X measured CPU\nspeedup. Our experiments on ResNet model and on other data sets like UCF101 and\nCIFAR-10 demonstrate the consistently higher performance of our framework. \n\n"}
{"id": "1807.11130", "contents": "Title: Geo-Supervised Visual Depth Prediction Abstract: We propose using global orientation from inertial measurements, and the bias\nit induces on the shape of objects populating the scene, to inform visual 3D\nreconstruction. We test the effect of using the resulting prior in depth\nprediction from a single image, where the normal vectors to surfaces of objects\nof certain classes tend to align with gravity or be orthogonal to it. Adding\nsuch a prior to baseline methods for monocular depth prediction yields\nimprovements beyond the state-of-the-art and illustrates the power of gravity\nas a supervisory signal. \n\n"}
{"id": "1807.11389", "contents": "Title: Multi-bin Trainable Linear Unit for Fast Image Restoration Networks Abstract: Tremendous advances in image restoration tasks such as denoising and\nsuper-resolution have been achieved using neural networks. Such approaches\ngenerally employ very deep architectures, large number of parameters, large\nreceptive fields and high nonlinear modeling capacity. In order to obtain\nefficient and fast image restoration networks one should improve upon the above\nmentioned requirements.\n  In this paper we propose a novel activation function, the multi-bin trainable\nlinear unit (MTLU), for increasing the nonlinear modeling capacity together\nwith lighter and shallower networks. We validate the proposed fast image\nrestoration networks for image denoising (FDnet) and super-resolution (FSRnet)\non standard benchmarks. We achieve large improvements in both memory and\nruntime over current state-of-the-art for comparable or better PSNR accuracies. \n\n"}
{"id": "1807.11436", "contents": "Title: Leveraging Motion Priors in Videos for Improving Human Segmentation Abstract: Despite many advances in deep-learning based semantic segmentation,\nperformance drop due to distribution mismatch is often encountered in the real\nworld. Recently, a few domain adaptation and active learning approaches have\nbeen proposed to mitigate the performance drop. However, very little attention\nhas been made toward leveraging information in videos which are naturally\ncaptured in most camera systems. In this work, we propose to leverage \"motion\nprior\" in videos for improving human segmentation in a weakly-supervised active\nlearning setting. By extracting motion information using optical flow in\nvideos, we can extract candidate foreground motion segments (referred to as\nmotion prior) potentially corresponding to human segments. We propose to learn\na memory-network-based policy model to select strong candidate segments\n(referred to as strong motion prior) through reinforcement learning. The\nselected segments have high precision and are directly used to finetune the\nmodel. In a newly collected surveillance camera dataset and a publicly\navailable UrbanStreet dataset, our proposed method improves the performance of\nhuman segmentation across multiple scenes and modalities (i.e., RGB to Infrared\n(IR)). Last but not least, our method is empirically complementary to existing\ndomain adaptation approaches such that additional performance gain is achieved\nby combining our weakly-supervised active learning approach with domain\nadaptation approaches. \n\n"}
{"id": "1807.11706", "contents": "Title: Learning Collaborative Generation Correction Modules for Blind Image\n  Deblurring and Beyond Abstract: Blind image deblurring plays a very important role in many vision and\nmultimedia applications. Most existing works tend to introduce complex priors\nto estimate the sharp image structures for blur kernel estimation. However, it\nhas been verified that directly optimizing these models is challenging and easy\nto fall into degenerate solutions. Although several experience-based heuristic\ninference strategies, including trained networks and designed iterations, have\nbeen developed, it is still hard to obtain theoretically guaranteed accurate\nsolutions. In this work, a collaborative learning framework is established to\naddress the above issues. Specifically, we first design two modules, named\nGenerator and Corrector, to extract the intrinsic image structures from the\ndata-driven and knowledge-based perspectives, respectively. By introducing a\ncollaborative methodology to cascade these modules, we can strictly prove the\nconvergence of our image propagations to a deblurring-related optimal solution.\nAs a nontrivial byproduct, we also apply the proposed method to address other\nrelated tasks, such as image interpolation and edge-preserved smoothing. Plenty\nof experiments demonstrate that our method can outperform the state-of-the-art\napproaches on both synthetic and real datasets. \n\n"}
{"id": "1807.11857", "contents": "Title: Joint Learning of Intrinsic Images and Semantic Segmentation Abstract: Semantic segmentation of outdoor scenes is problematic when there are\nvariations in imaging conditions. It is known that albedo (reflectance) is\ninvariant to all kinds of illumination effects. Thus, using reflectance images\nfor semantic segmentation task can be favorable. Additionally, not only\nsegmentation may benefit from reflectance, but also segmentation may be useful\nfor reflectance computation. Therefore, in this paper, the tasks of semantic\nsegmentation and intrinsic image decomposition are considered as a combined\nprocess by exploring their mutual relationship in a joint fashion. To that end,\nwe propose a supervised end-to-end CNN architecture to jointly learn intrinsic\nimage decomposition and semantic segmentation. We analyze the gains of\naddressing those two problems jointly. Moreover, new cascade CNN architectures\nfor intrinsic-for-segmentation and segmentation-for-intrinsic are proposed as\nsingle tasks. Furthermore, a dataset of 35K synthetic images of natural\nenvironments is created with corresponding albedo and shading (intrinsics), as\nwell as semantic labels (segmentation) assigned to each object/scene. The\nexperiments show that joint learning of intrinsic image decomposition and\nsemantic segmentation is beneficial for both tasks for natural scenes. Dataset\nand models are available at: https://ivi.fnwi.uva.nl/cv/intrinseg \n\n"}
{"id": "1808.00300", "contents": "Title: Learning Visual Question Answering by Bootstrapping Hard Attention Abstract: Attention mechanisms in biological perception are thought to select subsets\nof perceptual information for more sophisticated processing which would be\nprohibitive to perform on all sensory inputs. In computer vision, however,\nthere has been relatively little exploration of hard attention, where some\ninformation is selectively ignored, in spite of the success of soft attention,\nwhere information is re-weighted and aggregated, but never filtered out. Here,\nwe introduce a new approach for hard attention and find it achieves very\ncompetitive performance on a recently-released visual question answering\ndatasets, equalling and in some cases surpassing similar soft attention\narchitectures while entirely ignoring some features. Even though the hard\nattention mechanism is thought to be non-differentiable, we found that the\nfeature magnitudes correlate with semantic relevance, and provide a useful\nsignal for our mechanism's attentional selection criterion. Because hard\nattention selects important features of the input information, it can also be\nmore efficient than analogous soft attention mechanisms. This is especially\nimportant for recent approaches that use non-local pairwise operations, whereby\ncomputational and memory costs are quadratic in the size of the set of\nfeatures. \n\n"}
{"id": "1808.00313", "contents": "Title: A Network Structure to Explicitly Reduce Confusion Errors in Semantic\n  Segmentation Abstract: Confusing classes that are ubiquitous in real world often degrade performance\nfor many vision related applications like object detection, classification, and\nsegmentation. The confusion errors are not only caused by similar visual\npatterns but also amplified by various factors during the training of our\ndesigned models, such as reduced feature resolution in the encoding process or\nimbalanced data distributions. A large amount of deep learning based network\nstructures has been proposed in recent years to deal with these individual\nfactors and improve network performance. However, to our knowledge, no existing\nwork in semantic image segmentation is designed to tackle confusion errors\nexplicitly. In this paper, we present a novel and general network structure\nthat reduces confusion errors in more direct manner and apply the network for\nsemantic segmentation. There are two major contributions in our network\nstructure: 1) We ensemble subnets with heterogeneous output spaces based on the\ndiscriminative confusing groups. The training for each subnet can distinguish\nconfusing classes within the group without affecting unrelated classes outside\nthe group. 2) We propose an improved cross-entropy loss function that maximizes\nthe probability assigned to the correct class and penalizes the probabilities\nassigned to the confusing classes at the same time. Our network structure is a\ngeneral structure and can be easily adapted to any other networks to further\nreduce confusion errors. Without any changes in the feature encoder and\npost-processing steps, our experiments demonstrate consistent and significant\nimprovements on different baseline models on Cityscapes and PASCAL VOC datasets\n(e.g., 3.05% over ResNet-101 and 1.30% over ResNet-38). \n\n"}
{"id": "1808.00692", "contents": "Title: Online Temporal Calibration for Monocular Visual-Inertial Systems Abstract: Accurate state estimation is a fundamental module for various intelligent\napplications, such as robot navigation, autonomous driving, virtual and\naugmented reality. Visual and inertial fusion is a popular technology for 6-DOF\nstate estimation in recent years. Time instants at which different sensors'\nmeasurements are recorded are of crucial importance to the system's robustness\nand accuracy. In practice, timestamps of each sensor typically suffer from\ntriggering and transmission delays, leading to temporal misalignment (time\noffsets) among different sensors. Such temporal offset dramatically influences\nthe performance of sensor fusion. To this end, we propose an online approach\nfor calibrating temporal offset between visual and inertial measurements. Our\napproach achieves temporal offset calibration by jointly optimizing time\noffset, camera and IMU states, as well as feature locations in a SLAM system.\nFurthermore, the approach is a general model, which can be easily employed in\nseveral feature-based optimization frameworks. Simulation and experimental\nresults demonstrate the high accuracy of our calibration approach even compared\nwith other state-of-art offline tools. The VIO comparison against other methods\nproves that the online temporal calibration significantly benefits\nvisual-inertial systems. The source code of temporal calibration is integrated\ninto our public project, VINS-Mono. \n\n"}
{"id": "1808.01102", "contents": "Title: Hallucinating Agnostic Images to Generalize Across Domains Abstract: The ability to generalize across visual domains is crucial for the robustness\nof artificial recognition systems. Although many training sources may be\navailable in real contexts, the access to even unlabeled target samples cannot\nbe taken for granted, which makes standard unsupervised domain adaptation\nmethods inapplicable in the wild. In this work we investigate how to exploit\nmultiple sources by hallucinating a deep visual domain composed of images,\npossibly unrealistic, able to maintain categorical knowledge while discarding\nspecific source styles. The produced agnostic images are the result of a deep\narchitecture that applies pixel adaptation on the original source data guided\nby two adversarial domain classifier branches at image and feature level. Our\napproach is conceived to learn only from source data, but it seamlessly extends\nto the use of unlabeled target samples. Remarkable results for both\nmulti-source domain adaptation and domain generalization support the power of\nhallucinating agnostic images in this framework. \n\n"}
{"id": "1808.01358", "contents": "Title: Attributes' Importance for Zero-Shot Pose-Classification Based on\n  Wearable Sensors Abstract: This paper presents a simple yet effective method for improving the\nperformance of zero-shot learning (ZSL). ZSL classifies instances of unseen\nclasses, from which no training data is available, by utilizing the attributes\nof the classes. Conventional ZSL methods have equally dealt with all the\navailable attributes, but this sometimes causes misclassification. This is\nbecause an attribute that is effective for classifying instances of one class\nis not always effective for another class. In this case, a metric of\nclassifying the latter class can be undesirably influenced by the irrelevant\nattribute. This paper solves this problem by taking the importance of each\nattribute for each class into account when calculating the metric. In addition\nto the proposal of this new method, this paper also contributes by providing a\ndataset for pose classification based on wearable sensors, named HDPoseDS. It\ncontains 22 classes of poses performed by 10 subjects with 31 IMU sensors\nacross full body. To the best of our knowledge, it is the richest\nwearable-sensor dataset especially in terms of sensor density, and thus it is\nsuitable for studying zero-shot pose/action recognition. The presented method\nwas evaluated on HDPoseDS and outperformed relative improvement of 5.9% in\ncomparison to the best baseline method. \n\n"}
{"id": "1808.02016", "contents": "Title: MCRM: Mother Compact Recurrent Memory Abstract: LSTMs and GRUs are the most common recurrent neural network architectures\nused to solve temporal sequence problems. The two architectures have differing\ndata flows dealing with a common component called the cell state (also referred\nto as the memory). We attempt to enhance the memory by presenting a\nmodification that we call the Mother Compact Recurrent Memory (MCRM). MCRMs are\na type of a nested LSTM-GRU architecture where the cell state is the GRU hidden\nstate. The concatenation of the forget gate and input gate interactions from\nthe LSTM are considered an input to the GRU cell. Because MCRMs has this type\nof nesting, MCRMs have a compact memory pattern consisting of neurons that acts\nexplicitly in both long-term and short-term fashions. For some specific tasks,\nempirical results show that MCRMs outperform previously used architectures. \n\n"}
{"id": "1808.02201", "contents": "Title: Holistic 3D Scene Parsing and Reconstruction from a Single RGB Image Abstract: We propose a computational framework to jointly parse a single RGB image and\nreconstruct a holistic 3D configuration composed by a set of CAD models using a\nstochastic grammar model. Specifically, we introduce a Holistic Scene Grammar\n(HSG) to represent the 3D scene structure, which characterizes a joint\ndistribution over the functional and geometric space of indoor scenes. The\nproposed HSG captures three essential and often latent dimensions of the indoor\nscenes: i) latent human context, describing the affordance and the\nfunctionality of a room arrangement, ii) geometric constraints over the scene\nconfigurations, and iii) physical constraints that guarantee physically\nplausible parsing and reconstruction. We solve this joint parsing and\nreconstruction problem in an analysis-by-synthesis fashion, seeking to minimize\nthe differences between the input image and the rendered images generated by\nour 3D representation, over the space of depth, surface normal, and object\nsegmentation map. The optimal configuration, represented by a parse graph, is\ninferred using Markov chain Monte Carlo (MCMC), which efficiently traverses\nthrough the non-differentiable solution space, jointly optimizing object\nlocalization, 3D layout, and hidden human context. Experimental results\ndemonstrate that the proposed algorithm improves the generalization ability and\nsignificantly outperforms prior methods on 3D layout estimation, 3D object\ndetection, and holistic scene understanding. \n\n"}
{"id": "1808.02775", "contents": "Title: Omnidirectional DSO: Direct Sparse Odometry with Fisheye Cameras Abstract: We propose a novel real-time direct monocular visual odometry for\nomnidirectional cameras. Our method extends direct sparse odometry (DSO) by\nusing the unified omnidirectional model as a projection function, which can be\napplied to fisheye cameras with a field-of-view (FoV) well above 180 degrees.\nThis formulation allows for using the full area of the input image even with\nstrong distortion, while most existing visual odometry methods can only use a\nrectified and cropped part of it. Model parameters within an active keyframe\nwindow are jointly optimized, including the intrinsic/extrinsic camera\nparameters, 3D position of points, and affine brightness parameters. Thanks to\nthe wide FoV, image overlap between frames becomes bigger and points are more\nspatially distributed. Our results demonstrate that our method provides\nincreased accuracy and robustness over state-of-the-art visual odometry\nalgorithms. \n\n"}
{"id": "1808.04355", "contents": "Title: Large-Scale Study of Curiosity-Driven Learning Abstract: Reinforcement learning algorithms rely on carefully engineering environment\nrewards that are extrinsic to the agent. However, annotating each environment\nwith hand-designed, dense rewards is not scalable, motivating the need for\ndeveloping reward functions that are intrinsic to the agent. Curiosity is a\ntype of intrinsic reward function which uses prediction error as reward signal.\nIn this paper: (a) We perform the first large-scale study of purely\ncuriosity-driven learning, i.e. without any extrinsic rewards, across 54\nstandard benchmark environments, including the Atari game suite. Our results\nshow surprisingly good performance, and a high degree of alignment between the\nintrinsic curiosity objective and the hand-designed extrinsic rewards of many\ngame environments. (b) We investigate the effect of using different feature\nspaces for computing prediction error and show that random features are\nsufficient for many popular RL game benchmarks, but learned features appear to\ngeneralize better (e.g. to novel game levels in Super Mario Bros.). (c) We\ndemonstrate limitations of the prediction-based rewards in stochastic setups.\nGame-play videos and code are at\nhttps://pathak22.github.io/large-scale-curiosity/ \n\n"}
{"id": "1808.04503", "contents": "Title: Shared Multi-Task Imitation Learning for Indoor Self-Navigation Abstract: Deep imitation learning enables robots to learn from expert demonstrations to\nperform tasks such as lane following or obstacle avoidance. However, in the\ntraditional imitation learning framework, one model only learns one task, and\nthus it lacks of the capability to support a robot to perform various different\nnavigation tasks with one model in indoor environments. This paper proposes a\nnew framework, Shared Multi-headed Imitation Learning(SMIL), that allows a\nrobot to perform multiple tasks with one model without switching among\ndifferent models. We model each task as a sub-policy and design a multi-headed\npolicy to learn the shared information among related tasks by summing up\nactivations from all sub-policies. Compared to single or non-shared\nmulti-headed policies, this framework is able to leverage correlated\ninformation among tasks to increase performance.We have implemented this\nframework using a robot based on NVIDIA TX2 and performed extensive experiments\nin indoor environments with different baseline solutions. The results\ndemonstrate that SMIL has doubled the performance over nonshared multi-headed\npolicy. \n\n"}
{"id": "1808.04537", "contents": "Title: Learning Linear Transformations for Fast Arbitrary Style Transfer Abstract: Given a random pair of images, an arbitrary style transfer method extracts\nthe feel from the reference image to synthesize an output based on the look of\nthe other content image. Recent arbitrary style transfer methods transfer\nsecond order statistics from reference image onto content image via a\nmultiplication between content image features and a transformation matrix,\nwhich is computed from features with a pre-determined algorithm. These\nalgorithms either require computationally expensive operations, or fail to\nmodel the feature covariance and produce artifacts in synthesized images.\nGeneralized from these methods, in this work, we derive the form of\ntransformation matrix theoretically and present an arbitrary style transfer\napproach that learns the transformation matrix with a feed-forward network. Our\nalgorithm is highly efficient yet allows a flexible combination of multi-level\nstyles while preserving content affinity during style transfer process. We\ndemonstrate the effectiveness of our approach on four tasks: artistic style\ntransfer, video and photo-realistic style transfer as well as domain\nadaptation, including comparisons with the state-of-the-art methods. \n\n"}
{"id": "1808.04538", "contents": "Title: Text-to-Image-to-Text Translation using Cycle Consistent Adversarial\n  Networks Abstract: Text-to-Image translation has been an active area of research in the recent\npast. The ability for a network to learn the meaning of a sentence and generate\nan accurate image that depicts the sentence shows ability of the model to think\nmore like humans. Popular methods on text to image translation make use of\nGenerative Adversarial Networks (GANs) to generate high quality images based on\ntext input, but the generated images don't always reflect the meaning of the\nsentence given to the model as input. We address this issue by using a\ncaptioning network to caption on generated images and exploit the distance\nbetween ground truth captions and generated captions to improve the network\nfurther. We show extensive comparisons between our method and existing methods. \n\n"}
{"id": "1808.04848", "contents": "Title: URSA: A Neural Network for Unordered Point Clouds Using Constellations Abstract: This paper describes a neural network layer, named Ursa, that uses a\nconstellation of points to learn classification information from point cloud\ndata. Unlike other machine learning classification problems where the task is\nto classify an individual high-dimensional observation, in a point-cloud\nclassification problem the goal is to classify a set of d-dimensional\nobservations. Because a point cloud is a set, there is no ordering to the\ncollection of points in a point-cloud classification problem. Thus, the\nchallenge of classifying point clouds inputs is in building a classifier which\nis agnostic to the ordering of the observations, yet preserves the\nd-dimensional information of each point in the set. This research presents\nUrsa, a new layer type for an artificial neural network which achieves these\ntwo properties. Similar to new methods for this task, this architecture works\ndirectly on d-dimensional points rather than first converting the points to a\nd-dimensional volume. The Ursa layer is followed by a series of dense layers to\nclassify 2D and 3D objects from point clouds. Experiments on ModelNet40 and\nMNIST data show classification results comparable with current methods, while\nreducing the training parameters by over 50 percent. \n\n"}
{"id": "1808.05819", "contents": "Title: Uncertainty-aware Short-term Motion Prediction of Traffic Actors for\n  Autonomous Driving Abstract: We address one of the crucial aspects necessary for safe and efficient\noperations of autonomous vehicles, namely predicting future state of traffic\nactors in the autonomous vehicle's surroundings. We introduce a deep\nlearning-based approach that takes into account a current world state and\nproduces raster images of each actor's vicinity. The rasters are then used as\ninputs to deep convolutional models to infer future movement of actors while\nalso accounting for and capturing inherent uncertainty of the prediction task.\nExtensive experiments on real-world data strongly suggest benefits of the\nproposed approach. Moreover, following completion of the offline tests the\nsystem was successfully tested onboard self-driving vehicles. \n\n"}
{"id": "1808.06133", "contents": "Title: In Defense of Single-column Networks for Crowd Counting Abstract: Crowd counting usually addressed by density estimation becomes an\nincreasingly important topic in computer vision due to its widespread\napplications in video surveillance, urban planning, and intelligence gathering.\nHowever, it is essentially a challenging task because of the greatly varied\nsizes of objects, coupled with severe occlusions and vague appearance of\nextremely small individuals. Existing methods heavily rely on multi-column\nlearning architectures to extract multi-scale features, which however suffer\nfrom heavy computational cost, especially undesired for crowd counting. In this\npaper, we propose the single-column counting network (SCNet) for efficient\ncrowd counting without relying on multi-column networks. SCNet consists of\nresidual fusion modules (RFMs) for multi-scale feature extraction, a pyramid\npooling module (PPM) for information fusion, and a sub-pixel convolutional\nmodule (SPCM) followed by a bilinear upsampling layer for resolution recovery.\nThose proposed modules enable our SCNet to fully capture multi-scale features\nin a compact single-column architecture and estimate high-resolution density\nmap in an efficient way. In addition, we provide a principled paradigm for\ndensity map generation and data augmentation for training, which shows further\nimproved performance. Extensive experiments on three benchmark datasets show\nthat our SCNet delivers new state-of-the-art performance and surpasses previous\nmethods by large margins, which demonstrates the great effectiveness of SCNet\nas a single-column network for crowd counting. \n\n"}
{"id": "1808.07269", "contents": "Title: A Deep Neural Network for Pixel-Level Electromagnetic Particle\n  Identification in the MicroBooNE Liquid Argon Time Projection Chamber Abstract: We have developed a convolutional neural network (CNN) that can make a\npixel-level prediction of objects in image data recorded by a liquid argon time\nprojection chamber (LArTPC) for the first time. We describe the network design,\ntraining techniques, and software tools developed to train this network. The\ngoal of this work is to develop a complete deep neural network based data\nreconstruction chain for the MicroBooNE detector. We show the first\ndemonstration of a network's validity on real LArTPC data using MicroBooNE\ncollection plane images. The demonstration is performed for stopping muon and a\n$\\nu_\\mu$ charged current neutral pion data samples. \n\n"}
{"id": "1808.07659", "contents": "Title: PVNet: A Joint Convolutional Network of Point Cloud and Multi-View for\n  3D Shape Recognition Abstract: 3D object recognition has attracted wide research attention in the field of\nmultimedia and computer vision. With the recent proliferation of deep learning,\nvarious deep models with different representations have achieved the\nstate-of-the-art performance. Among them, point cloud and multi-view based 3D\nshape representations are promising recently, and their corresponding deep\nmodels have shown significant performance on 3D shape recognition. However,\nthere is little effort concentrating point cloud data and multi-view data for\n3D shape representation, which is, in our consideration, beneficial and\ncompensated to each other. In this paper, we propose the Point-View Network\n(PVNet), the first framework integrating both the point cloud and the\nmulti-view data towards joint 3D shape recognition. More specifically, an\nembedding attention fusion scheme is proposed that could employ high-level\nfeatures from the multi-view data to model the intrinsic correlation and\ndiscriminability of different structure features from the point cloud data. In\nparticular, the discriminative descriptions are quantified and leveraged as the\nsoft attention mask to further refine the structure feature of the 3D shape. We\nhave evaluated the proposed method on the ModelNet40 dataset for 3D shape\nclassification and retrieval tasks. Experimental results and comparisons with\nstate-of-the-art methods demonstrate that our framework can achieve superior\nperformance. \n\n"}
{"id": "1808.08210", "contents": "Title: Automatic Foreground Extraction from Imperfect Backgrounds using\n  Multi-Agent Consensus Equilibrium Abstract: Extracting accurate foreground objects from a scene is an essential step for\nmany video applications. Traditional background subtraction algorithms can\ngenerate coarse estimates, but generating high quality masks requires\nprofessional softwares with significant human interventions, e.g., providing\ntrimaps or labeling key frames. We propose an automatic foreground extraction\nmethod in applications where a static but imperfect background is available.\nExamples include filming and surveillance where the background can be captured\nbefore the objects enter the scene or after they leave the scene. Our proposed\nmethod is very robust and produces significantly better estimates than\nstate-of-the-art background subtraction, video segmentation and alpha matting\nmethods. The key innovation of our method is a novel information fusion\ntechnique. The fusion framework allows us to integrate the individual strengths\nof alpha matting, background subtraction and image denoising to produce an\noverall better estimate. Such integration is particularly important when\nhandling complex scenes with imperfect background. We show how the framework is\ndeveloped, and how the individual components are built. Extensive experiments\nand ablation studies are conducted to evaluate the proposed method. \n\n"}
{"id": "1808.08578", "contents": "Title: Automatic 3D bi-ventricular segmentation of cardiac images by a\n  shape-refined multi-task deep learning approach Abstract: Deep learning approaches have achieved state-of-the-art performance in\ncardiac magnetic resonance (CMR) image segmentation. However, most approaches\nhave focused on learning image intensity features for segmentation, whereas the\nincorporation of anatomical shape priors has received less attention. In this\npaper, we combine a multi-task deep learning approach with atlas propagation to\ndevelop a shape-constrained bi-ventricular segmentation pipeline for short-axis\nCMR volumetric images. The pipeline first employs a fully convolutional network\n(FCN) that learns segmentation and landmark localisation tasks simultaneously.\nThe architecture of the proposed FCN uses a 2.5D representation, thus combining\nthe computational advantage of 2D FCNs networks and the capability of\naddressing 3D spatial consistency without compromising segmentation accuracy.\nMoreover, the refinement step is designed to explicitly enforce a shape\nconstraint and improve segmentation quality. This step is effective for\novercoming image artefacts (e.g. due to different breath-hold positions and\nlarge slice thickness), which preclude the creation of anatomically meaningful\n3D cardiac shapes. The proposed pipeline is fully automated, due to network's\nability to infer landmarks, which are then used downstream in the pipeline to\ninitialise atlas propagation. We validate the pipeline on 1831 healthy subjects\nand 649 subjects with pulmonary hypertension. Extensive numerical experiments\non the two datasets demonstrate that our proposed method is robust and capable\nof producing accurate, high-resolution and anatomically smooth bi-ventricular\n3D models, despite the artefacts in input CMR volumes. \n\n"}
{"id": "1808.09916", "contents": "Title: Autoencoders, Kernels, and Multilayer Perceptrons for Electron\n  Micrograph Restoration and Compression Abstract: We present 14 autoencoders, 15 kernels and 14 multilayer perceptrons for\nelectron micrograph restoration and compression. These have been trained for\ntransmission electron microscopy (TEM), scanning transmission electron\nmicroscopy (STEM) and for both (TEM+STEM). TEM autoencoders have been trained\nfor 1$\\times$, 4$\\times$, 16$\\times$ and 64$\\times$ compression, STEM\nautoencoders for 1$\\times$, 4$\\times$ and 16$\\times$ compression and TEM+STEM\nautoencoders for 1$\\times$, 2$\\times$, 4$\\times$, 8$\\times$, 16$\\times$,\n32$\\times$ and 64$\\times$ compression. Kernels and multilayer perceptrons have\nbeen trained to approximate the denoising effect of the 4$\\times$ compression\nautoencoders. Kernels for input sizes of 3, 5, 7, 11 and 15 have been fitted\nfor TEM, STEM and TEM+STEM. TEM multilayer perceptrons have been trained with 1\nhidden layer for input sizes of 3, 5 and 7 and with 2 hidden layers for input\nsizes of 5 and 7. STEM multilayer perceptrons have been trained with 1 hidden\nlayer for input sizes of 3, 5 and 7. TEM+STEM multilayer perceptrons have been\ntrained with 1 hidden layer for input sizes of 3, 5, 7 and 11 and with 2 hidden\nlayers for input sizes of 3 and 7. Our code, example usage and pre-trained\nmodels are available at\nhttps://github.com/Jeffrey-Ede/Denoising-Kernels-MLPs-Autoencoders \n\n"}
{"id": "1808.10032", "contents": "Title: The Impact of Preprocessing on Deep Representations for Iris Recognition\n  on Unconstrained Environments Abstract: The use of iris as a biometric trait is widely used because of its high level\nof distinction and uniqueness. Nowadays, one of the major research challenges\nrelies on the recognition of iris images obtained in visible spectrum under\nunconstrained environments. In this scenario, the acquired iris are affected by\ncapture distance, rotation, blur, motion blur, low contrast and specular\nreflection, creating noises that disturb the iris recognition systems. Besides\ndelineating the iris region, usually preprocessing techniques such as\nnormalization and segmentation of noisy iris images are employed to minimize\nthese problems. But these techniques inevitably run into some errors. In this\ncontext, we propose the use of deep representations, more specifically,\narchitectures based on VGG and ResNet-50 networks, for dealing with the images\nusing (and not) iris segmentation and normalization. We use transfer learning\nfrom the face domain and also propose a specific data augmentation technique\nfor iris images. Our results show that the approach using non-normalized and\nonly circle-delimited iris images reaches a new state of the art in the\nofficial protocol of the NICE.II competition, a subset of the UBIRIS database,\none of the most challenging databases on unconstrained environments, reporting\nan average Equal Error Rate (EER) of 13.98% which represents an absolute\nreduction of about 5%. \n\n"}
{"id": "1808.10075", "contents": "Title: Towards Effective Deep Embedding for Zero-Shot Learning Abstract: Zero-shot learning (ZSL) can be formulated as a cross-domain matching\nproblem: after being projected into a joint embedding space, a visual sample\nwill match against all candidate class-level semantic descriptions and be\nassigned to the nearest class. In this process, the embedding space underpins\nthe success of such matching and is crucial for ZSL. In this paper, we conduct\nan in-depth study on the construction of embedding space for ZSL and posit that\nan ideal embedding space should satisfy two criteria: intra-class compactness\nand inter-class separability. While the former encourages the embeddings of\nvisual samples of one class to distribute tightly close to the semantic\ndescription embedding of this class, the latter requires embeddings from\ndifferent classes to be well separated from each other. Towards this goal, we\npresent a simple but effective two-branch network to simultaneously map\nsemantic descriptions and visual samples into a joint space, on which visual\nembeddings are forced to regress to their class-level semantic embeddings and\nthe embeddings crossing classes are required to be distinguishable by a\ntrainable classifier. Furthermore, we extend our method to a transductive\nsetting to better handle the model bias problem in ZSL (i.e., samples from\nunseen classes tend to be categorized into seen classes) with minimal extra\nsupervision. Specifically, we propose a pseudo labeling strategy to\nprogressively incorporate the testing samples into the training process and\nthus balance the model between seen and unseen classes. Experimental results on\nfive standard ZSL datasets show the superior performance of the proposed method\nand its transductive extension. \n\n"}
{"id": "1809.00095", "contents": "Title: Learning Sparse Low-Precision Neural Networks With Learnable\n  Regularization Abstract: We consider learning deep neural networks (DNNs) that consist of\nlow-precision weights and activations for efficient inference of fixed-point\noperations. In training low-precision networks, gradient descent in the\nbackward pass is performed with high-precision weights while quantized\nlow-precision weights and activations are used in the forward pass to calculate\nthe loss function for training. Thus, the gradient descent becomes suboptimal,\nand accuracy loss follows. In order to reduce the mismatch in the forward and\nbackward passes, we utilize mean squared quantization error (MSQE)\nregularization. In particular, we propose using a learnable regularization\ncoefficient with the MSQE regularizer to reinforce the convergence of\nhigh-precision weights to their quantized values. We also investigate how\npartial L2 regularization can be employed for weight pruning in a similar\nmanner. Finally, combining weight pruning, quantization, and entropy coding, we\nestablish a low-precision DNN compression pipeline. In our experiments, the\nproposed method yields low-precision MobileNet and ShuffleNet models on\nImageNet classification with the state-of-the-art compression ratios of 7.13\nand 6.79, respectively. Moreover, we examine our method for image super\nresolution networks to produce 8-bit low-precision models at negligible\nperformance loss. \n\n"}
{"id": "1809.00219", "contents": "Title: ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks Abstract: The Super-Resolution Generative Adversarial Network (SRGAN) is a seminal work\nthat is capable of generating realistic textures during single image\nsuper-resolution. However, the hallucinated details are often accompanied with\nunpleasant artifacts. To further enhance the visual quality, we thoroughly\nstudy three key components of SRGAN - network architecture, adversarial loss\nand perceptual loss, and improve each of them to derive an Enhanced SRGAN\n(ESRGAN). In particular, we introduce the Residual-in-Residual Dense Block\n(RRDB) without batch normalization as the basic network building unit.\nMoreover, we borrow the idea from relativistic GAN to let the discriminator\npredict relative realness instead of the absolute value. Finally, we improve\nthe perceptual loss by using the features before activation, which could\nprovide stronger supervision for brightness consistency and texture recovery.\nBenefiting from these improvements, the proposed ESRGAN achieves consistently\nbetter visual quality with more realistic and natural textures than SRGAN and\nwon the first place in the PIRM2018-SR Challenge. The code is available at\nhttps://github.com/xinntao/ESRGAN . \n\n"}
{"id": "1809.00681", "contents": "Title: Diverse and Coherent Paragraph Generation from Images Abstract: Paragraph generation from images, which has gained popularity recently, is an\nimportant task for video summarization, editing, and support of the disabled.\nTraditional image captioning methods fall short on this front, since they\naren't designed to generate long informative descriptions. Moreover, the\nvanilla approach of simply concatenating multiple short sentences, possibly\nsynthesized from a classical image captioning system, doesn't embrace the\nintricacies of paragraphs: coherent sentences, globally consistent structure,\nand diversity. To address those challenges, we propose to augment paragraph\ngeneration techniques with 'coherence vectors', 'global topic vectors', and\nmodeling of the inherent ambiguity of associating paragraphs with images, via a\nvariational auto-encoder formulation. We demonstrate the effectiveness of the\ndeveloped approach on two datasets, outperforming existing state-of-the-art\ntechniques on both. \n\n"}
{"id": "1809.01110", "contents": "Title: Text2Scene: Generating Compositional Scenes from Textual Descriptions Abstract: In this paper, we propose Text2Scene, a model that generates various forms of\ncompositional scene representations from natural language descriptions. Unlike\nrecent works, our method does NOT use Generative Adversarial Networks (GANs).\nText2Scene instead learns to sequentially generate objects and their attributes\n(location, size, appearance, etc) at every time step by attending to different\nparts of the input text and the current status of the generated scene. We show\nthat under minor modifications, the proposed framework can handle the\ngeneration of different forms of scene representations, including cartoon-like\nscenes, object layouts corresponding to real images, and synthetic images. Our\nmethod is not only competitive when compared with state-of-the-art GAN-based\nmethods using automatic metrics and superior based on human judgments but also\nhas the advantage of producing interpretable results. \n\n"}
{"id": "1809.01368", "contents": "Title: Towards a Better Match in Siamese Network Based Visual Object Tracker Abstract: Recently, Siamese network based trackers have received tremendous interest\nfor their fast tracking speed and high performance. Despite the great success,\nthis tracking framework still suffers from several limitations. First, it\ncannot properly handle large object rotation. Second, tracking gets easily\ndistracted when the background contains salient objects. In this paper, we\npropose two simple yet effective mechanisms, namely angle estimation and\nspatial masking, to address these issues. The objective is to extract more\nrepresentative features so that a better match can be obtained between the same\nobject from different frames. The resulting tracker, named Siam-BM, not only\nsignificantly improves the tracking performance, but more importantly maintains\nthe realtime capability. Evaluations on the VOT2017 dataset show that Siam-BM\nachieves an EAO of 0.335, which makes it the best-performing realtime tracker\nto date. \n\n"}
{"id": "1809.01810", "contents": "Title: Interpretable Visual Question Answering by Reasoning on Dependency Trees Abstract: Collaborative reasoning for understanding image-question pairs is a very\ncritical but underexplored topic in interpretable visual question answering\nsystems. Although very recent studies have attempted to use explicit\ncompositional processes to assemble multiple subtasks embedded in questions,\ntheir models heavily rely on annotations or handcrafted rules to obtain valid\nreasoning processes, which leads to either heavy workloads or poor performance\non compositional reasoning. In this paper, to better align image and language\ndomains in diverse and unrestricted cases, we propose a novel neural network\nmodel that performs global reasoning on a dependency tree parsed from the\nquestion; thus, our model is called a parse-tree-guided reasoning network\n(PTGRN). This network consists of three collaborative modules: i) an attention\nmodule that exploits the local visual evidence of each word parsed from the\nquestion, ii) a gated residual composition module that composes the previously\nmined evidence, and iii) a parse-tree-guided propagation module that passes the\nmined evidence along the parse tree. Thus, PTGRN is capable of building an\ninterpretable visual question answering (VQA) system that gradually derives\nimage cues following question-driven parse-tree reasoning. Experiments on\nrelational datasets demonstrate the superiority of PTGRN over current\nstate-of-the-art VQA methods, and the visualization results highlight the\nexplainable capability of our reasoning system. \n\n"}
{"id": "1809.02058", "contents": "Title: Memory Replay GANs: learning to generate images from new categories\n  without forgetting Abstract: Previous works on sequential learning address the problem of forgetting in\ndiscriminative models. In this paper we consider the case of generative models.\nIn particular, we investigate generative adversarial networks (GANs) in the\ntask of learning new categories in a sequential fashion. We first show that\nsequential fine tuning renders the network unable to properly generate images\nfrom previous categories (i.e. forgetting). Addressing this problem, we propose\nMemory Replay GANs (MeRGANs), a conditional GAN framework that integrates a\nmemory replay generator. We study two methods to prevent forgetting by\nleveraging these replays, namely joint training with replay and replay\nalignment. Qualitative and quantitative experimental results in MNIST, SVHN and\nLSUN datasets show that our memory replay approach can generate competitive\nimages while significantly mitigating the forgetting of previous categories. \n\n"}
{"id": "1809.02721", "contents": "Title: Learning to Solve NP-Complete Problems - A Graph Neural Network for\n  Decision TSP Abstract: Graph Neural Networks (GNN) are a promising technique for bridging\ndifferential programming and combinatorial domains. GNNs employ trainable\nmodules which can be assembled in different configurations that reflect the\nrelational structure of each problem instance. In this paper, we show that GNNs\ncan learn to solve, with very little supervision, the decision variant of the\nTraveling Salesperson Problem (TSP), a highly relevant $\\mathcal{NP}$-Complete\nproblem. Our model is trained to function as an effective message-passing\nalgorithm in which edges (embedded with their weights) communicate with\nvertices for a number of iterations after which the model is asked to decide\nwhether a route with cost $<C$ exists. We show that such a network can be\ntrained with sets of dual examples: given the optimal tour cost $C^{*}$, we\nproduce one decision instance with target cost $x\\%$ smaller and one with\ntarget cost $x\\%$ larger than $C^{*}$. We were able to obtain $80\\%$ accuracy\ntraining with $-2\\%,+2\\%$ deviations, and the same trained model can generalize\nfor more relaxed deviations with increasing performance. We also show that the\nmodel is capable of generalizing for larger problem sizes. Finally, we provide\na method for predicting the optimal route cost within $2\\%$ deviation from the\nground truth. In summary, our work shows that Graph Neural Networks are\npowerful enough to solve $\\mathcal{NP}$-Complete problems which combine\nsymbolic and numeric data. \n\n"}
{"id": "1809.02731", "contents": "Title: Exploiting Invertible Decoders for Unsupervised Sentence Representation\n  Learning Abstract: The encoder-decoder models for unsupervised sentence representation learning\ntend to discard the decoder after being trained on a large unlabelled corpus,\nsince only the encoder is needed to map the input sentence into a vector\nrepresentation. However, parameters learnt in the decoder also contain useful\ninformation about language. In order to utilise the decoder after learning, we\npresent two types of decoding functions whose inverse can be easily derived\nwithout expensive inverse calculation. Therefore, the inverse of the decoding\nfunction serves as another encoder that produces sentence representations. We\nshow that, with careful design of the decoding functions, the model learns good\nsentence representations, and the ensemble of the representations produced from\nthe encoder and the inverse of the decoder demonstrate even better\ngeneralisation ability and solid transferability. \n\n"}
{"id": "1809.02836", "contents": "Title: Context-Free Transductions with Neural Stacks Abstract: This paper analyzes the behavior of stack-augmented recurrent neural network\n(RNN) models. Due to the architectural similarity between stack RNNs and\npushdown transducers, we train stack RNN models on a number of tasks, including\nstring reversal, context-free language modelling, and cumulative XOR\nevaluation. Examining the behavior of our networks, we show that\nstack-augmented RNNs can discover intuitive stack-based strategies for solving\nour tasks. However, stack RNNs are more difficult to train than classical\narchitectures such as LSTMs. Rather than employ stack-based strategies, more\ncomplex networks often find approximate solutions by using the stack as\nunstructured memory. \n\n"}
{"id": "1809.02983", "contents": "Title: Dual Attention Network for Scene Segmentation Abstract: In this paper, we address the scene segmentation task by capturing rich\ncontextual dependencies based on the selfattention mechanism. Unlike previous\nworks that capture contexts by multi-scale features fusion, we propose a Dual\nAttention Networks (DANet) to adaptively integrate local features with their\nglobal dependencies. Specifically, we append two types of attention modules on\ntop of traditional dilated FCN, which model the semantic interdependencies in\nspatial and channel dimensions respectively. The position attention module\nselectively aggregates the features at each position by a weighted sum of the\nfeatures at all positions. Similar features would be related to each other\nregardless of their distances. Meanwhile, the channel attention module\nselectively emphasizes interdependent channel maps by integrating associated\nfeatures among all channel maps. We sum the outputs of the two attention\nmodules to further improve feature representation which contributes to more\nprecise segmentation results. We achieve new state-of-the-art segmentation\nperformance on three challenging scene segmentation datasets, i.e., Cityscapes,\nPASCAL Context and COCO Stuff dataset. In particular, a Mean IoU score of 81.5%\non Cityscapes test set is achieved without using coarse data. We make the code\nand trained model publicly available at https://github.com/junfu1115/DANet \n\n"}
{"id": "1809.03355", "contents": "Title: Learning to Zoom: a Saliency-Based Sampling Layer for Neural Networks Abstract: We introduce a saliency-based distortion layer for convolutional neural\nnetworks that helps to improve the spatial sampling of input data for a given\ntask. Our differentiable layer can be added as a preprocessing block to\nexisting task networks and trained altogether in an end-to-end fashion. The\neffect of the layer is to efficiently estimate how to sample from the original\ndata in order to boost task performance. For example, for an image\nclassification task in which the original data might range in size up to\nseveral megapixels, but where the desired input images to the task network are\nmuch smaller, our layer learns how best to sample from the underlying high\nresolution data in a manner which preserves task-relevant information better\nthan uniform downsampling. This has the effect of creating distorted,\ncaricature-like intermediate images, in which idiosyncratic elements of the\nimage that improve task performance are zoomed and exaggerated. Unlike\nalternative approaches such as spatial transformer networks, our proposed layer\nis inspired by image saliency, computed efficiently from uniformly downsampled\ndata, and degrades gracefully to a uniform sampling strategy under uncertainty.\nWe apply our layer to improve existing networks for the tasks of human gaze\nestimation and fine-grained object classification. Code for our method is\navailable in: http://github.com/recasens/Saliency-Sampler \n\n"}
{"id": "1809.03707", "contents": "Title: Answering Visual What-If Questions: From Actions to Predicted Scene\n  Descriptions Abstract: In-depth scene descriptions and question answering tasks have greatly\nincreased the scope of today's definition of scene understanding. While such\ntasks are in principle open ended, current formulations primarily focus on\ndescribing only the current state of the scenes under consideration. In\ncontrast, in this paper, we focus on the future states of the scenes which are\nalso conditioned on actions. We posit this as a question answering task, where\nan answer has to be given about a future scene state, given observations of the\ncurrent scene, and a question that includes a hypothetical action. Our solution\nis a hybrid model which integrates a physics engine into a question answering\narchitecture in order to anticipate future scene states resulting from\nobject-object interactions caused by an action. We demonstrate first results on\nthis challenging new problem and compare to baselines, where we outperform\nfully data-driven end-to-end learning approaches. \n\n"}
{"id": "1809.03864", "contents": "Title: Response Characterization for Auditing Cell Dynamics in Long Short-term\n  Memory Networks Abstract: In this paper, we introduce a novel method to interpret recurrent neural\nnetworks (RNNs), particularly long short-term memory networks (LSTMs) at the\ncellular level. We propose a systematic pipeline for interpreting individual\nhidden state dynamics within the network using response characterization\nmethods. The ranked contribution of individual cells to the network's output is\ncomputed by analyzing a set of interpretable metrics of their decoupled step\nand sinusoidal responses. As a result, our method is able to uniquely identify\nneurons with insightful dynamics, quantify relationships between dynamical\nproperties and test accuracy through ablation analysis, and interpret the\nimpact of network capacity on a network's dynamical distribution. Finally, we\ndemonstrate generalizability and scalability of our method by evaluating a\nseries of different benchmark sequential datasets. \n\n"}
{"id": "1809.04157", "contents": "Title: Heated-Up Softmax Embedding Abstract: Metric learning aims at learning a distance which is consistent with the\nsemantic meaning of the samples. The problem is generally solved by learning an\nembedding for each sample such that the embeddings of samples of the same\ncategory are compact while the embeddings of samples of different categories\nare spread-out in the feature space. We study the features extracted from the\nsecond last layer of a deep neural network based classifier trained with the\ncross entropy loss on top of the softmax layer. We show that training\nclassifiers with different temperature values of softmax function leads to\nfeatures with different levels of compactness. Leveraging these insights, we\npropose a \"heating-up\" strategy to train a classifier with increasing\ntemperatures, leading the corresponding embeddings to achieve state-of-the-art\nperformance on a variety of metric learning benchmarks. \n\n"}
{"id": "1809.04364", "contents": "Title: Thermal Features for Presentation Attack Detection in Hand Biometrics Abstract: This paper proposes a method for utilizing thermal features of the hand for\nthe purpose of presentation attack detection (PAD) that can be employed in a\nhand biometrics system's pipeline. By envisaging two different operational\nmodes of our system, and by employing a DCNN-based classifiers fine-tuned with\na dataset of real and fake hand representations captured in both visible and\nther- mal spectrum, we were able to bring two important deliverables. First, a\nPAD method operating in an open-set mode, capable of correctly discerning 100%\nof fake thermal samples, achieving Attack Presentation Classification Error\nRate (APCER) and Bona-Fide Presentation Classification Error Rate (BPCER) equal\nto 0%, which can be easily implemented into any existing system as a separate\ncomponent. Second, a hand biometrics system operating in a closed-set mode,\nthat has PAD built right into the recognition pipeline, and operating\nsimultaneously with the user-wise classification, achieving rank-1 recognition\naccuracy of up to 99.75%. We also show that thermal images of the human hand,\nin addition to liveness features they carry, can also improve classification\naccuracy of a biometric system, when coupled with visible light images. To\nfollow the reproducibility guidelines and to stimulate further research in this\narea, we share the trained model weights, source codes, and a newly created\ndataset of fake hand representations with interested researchers. \n\n"}
{"id": "1809.04983", "contents": "Title: Part-based Graph Convolutional Network for Action Recognition Abstract: Human actions comprise of joint motion of articulated body parts or\n`gestures'. Human skeleton is intuitively represented as a sparse graph with\njoints as nodes and natural connections between them as edges. Graph\nconvolutional networks have been used to recognize actions from skeletal\nvideos. We introduce a part-based graph convolutional network (PB-GCN) for this\ntask, inspired by Deformable Part-based Models (DPMs). We divide the skeleton\ngraph into four subgraphs with joints shared across them and learn a\nrecognition model using a part-based graph convolutional network. We show that\nsuch a model improves performance of recognition, compared to a model using\nentire skeleton graph. Instead of using 3D joint coordinates as node features,\nwe show that using relative coordinates and temporal displacements boosts\nperformance. Our model achieves state-of-the-art performance on two challenging\nbenchmark datasets NTURGB+D and HDM05, for skeletal action recognition. \n\n"}
{"id": "1809.06200", "contents": "Title: From Same Photo: Cheating on Visual Kinship Challenges Abstract: With the propensity for deep learning models to learn unintended signals from\ndata sets there is always the possibility that the network can `cheat' in order\nto solve a task. In the instance of data sets for visual kinship verification,\none such unintended signal could be that the faces are cropped from the same\nphotograph, since faces from the same photograph are more likely to be from the\nsame family. In this paper we investigate the influence of this artefactual\ndata inference in published data sets for kinship verification.\n  To this end, we obtain a large dataset, and train a CNN classifier to\ndetermine if two faces are from the same photograph or not. Using this\nclassifier alone as a naive classifier of kinship, we demonstrate near state of\nthe art results on five public benchmark data sets for kinship verification -\nachieving over 90% accuracy on one of them. Thus, we conclude that faces\nderived from the same photograph are a strong inadvertent signal in all the\ndata sets we examined, and it is likely that the fraction of kinship explained\nby existing kinship models is small. \n\n"}
{"id": "1809.06396", "contents": "Title: D\\'ej\\`a Vu: an empirical evaluation of the memorization properties of\n  ConvNets Abstract: Convolutional neural networks memorize part of their training data, which is\nwhy strategies such as data augmentation and drop-out are employed to mitigate\noverfitting. This paper considers the related question of \"membership\ninference\", where the goal is to determine if an image was used during\ntraining. We consider it under three complementary angles. We show how to\ndetect which dataset was used to train a model, and in particular whether some\nvalidation images were used at train time. We then analyze explicit\nmemorization and extend classical random label experiments to the problem of\nlearning a model that predicts if an image belongs to an arbitrary set.\nFinally, we propose a new approach to infer membership when a few of the top\nlayers are not available or have been fine-tuned, and show that lower layers\nstill carry information about the training samples. To support our findings, we\nconduct large-scale experiments on Imagenet and subsets of YFCC-100M with\nmodern architectures such as VGG and Resnet. \n\n"}
{"id": "1809.07999", "contents": "Title: Multimodal Dual Attention Memory for Video Story Question Answering Abstract: We propose a video story question-answering (QA) architecture, Multimodal\nDual Attention Memory (MDAM). The key idea is to use a dual attention mechanism\nwith late fusion. MDAM uses self-attention to learn the latent concepts in\nscene frames and captions. Given a question, MDAM uses the second attention\nover these latent concepts. Multimodal fusion is performed after the dual\nattention processes (late fusion). Using this processing pipeline, MDAM learns\nto infer a high-level vision-language joint representation from an abstraction\nof the full video content. We evaluate MDAM on PororoQA and MovieQA datasets\nwhich have large-scale QA annotations on cartoon videos and movies,\nrespectively. For both datasets, MDAM achieves new state-of-the-art results\nwith significant margins compared to the runner-up models. We confirm the best\nperformance of the dual attention mechanism combined with late fusion by\nablation studies. We also perform qualitative analysis by visualizing the\ninference mechanisms of MDAM. \n\n"}
{"id": "1809.08016", "contents": "Title: On-field player workload exposure and knee injury risk monitoring via\n  deep learning Abstract: In sports analytics, an understanding of accurate on-field 3D knee joint\nmoments (KJM) could provide an early warning system for athlete workload\nexposure and knee injury risk. Traditionally, this analysis has relied on\ncaptive laboratory force plates and associated downstream biomechanical\nmodeling, and many researchers have approached the problem of portability by\nextrapolating models built on linear statistics. An alternative approach would\nbe to capitalize on recent advances in deep learning. In this study, using the\npre-trained CaffeNet convolutional neural network (CNN) model, multivariate\nregression of marker-based motion capture to 3D KJM for three sports-related\nmovement types were compared. The strongest overall mean correlation to source\nmodeling of 0.8895 was achieved over the initial 33 % of stance phase for\nsidestepping. The accuracy of these mean predictions of the three critical KJM\nassociated with anterior cruciate ligament (ACL) injury demonstrate the\nfeasibility of on-field knee injury assessment using deep learning in lieu of\nlaboratory embedded force plates. This multidisciplinary research approach\nsignificantly advances machine representation of real-world physical models\nwith practical application for both community and professional level athletes. \n\n"}
{"id": "1809.08340", "contents": "Title: Unsupervised Image to Sequence Translation with Canvas-Drawer Networks Abstract: Encoding images as a series of high-level constructs, such as brush strokes\nor discrete shapes, can often be key to both human and machine understanding.\nIn many cases, however, data is only available in pixel form. We present a\nmethod for generating images directly in a high-level domain (e.g. brush\nstrokes), without the need for real pairwise data. Specifically, we train a\n\"canvas\" network to imitate the mapping of high-level constructs to pixels,\nfollowed by a high-level \"drawing\" network which is optimized through this\nmapping towards solving a desired image recreation or translation task. We\nsuccessfully discover sequential vector representations of symbols, large\nsketches, and 3D objects, utilizing only pixel data. We display applications of\nour method in image segmentation, and present several ablation studies\ncomparing various configurations. \n\n"}
{"id": "1809.08493", "contents": "Title: SelfKin: Self Adjusted Deep Model For Kinship Verification Abstract: One of the unsolved challenges in the field of biometrics and face\nrecognition is Kinship Verification. This problem aims to understand if two\npeople are family-related and how (sisters, brothers, etc.) Solving this\nproblem can give rise to varied tasks and applications. In the area of homeland\nsecurity (HLS) it is crucial to auto-detect if the person questioned is related\nto a wanted suspect, In the field of biometrics, kinship-verification can help\nto discriminate between families by photos and in the field of predicting or\nfashion it can help to predict an older or younger model of people faces.\nLately, and with the advanced deep learning technology, this problem has gained\nfocus from the research community in matters of data and research. In this\narticle, we propose using a Deep Learning approach for solving the\nKinship-Verification problem. Further, we offer a novel self-learning deep\nmodel, which learns the essential features from different faces. We show that\nour model wins the Recognize Families In the Wild(RFIW2018,FG2018) challenge\nand obtains state-of-the-art results. Moreover, we show that our proposed model\ncan reduce the size of the network by half without loss in performance. \n\n"}
{"id": "1809.09875", "contents": "Title: Active Learning for Deep Object Detection Abstract: The great success that deep models have achieved in the past is mainly owed\nto large amounts of labeled training data. However, the acquisition of labeled\ndata for new tasks aside from existing benchmarks is both challenging and\ncostly. Active learning can make the process of labeling new data more\nefficient by selecting unlabeled samples which, when labeled, are expected to\nimprove the model the most. In this paper, we combine a novel method of active\nlearning for object detection with an incremental learning scheme to enable\ncontinuous exploration of new unlabeled datasets. We propose a set of\nuncertainty-based active learning metrics suitable for most object detectors.\nFurthermore, we present an approach to leverage class imbalances during sample\nselection. All methods are evaluated systematically in a continuous exploration\ncontext on the PASCAL VOC 2012 dataset. \n\n"}
{"id": "1809.10200", "contents": "Title: Compressing the Input for CNNs with the First-Order Scattering Transform Abstract: We study the first-order scattering transform as a candidate for reducing the\nsignal processed by a convolutional neural network (CNN). We show theoretical\nand empirical evidence that in the case of natural images and sufficiently\nsmall translation invariance, this transform preserves most of the signal\ninformation needed for classification while substantially reducing the spatial\nresolution and total signal size. We demonstrate that cascading a CNN with this\nrepresentation performs on par with ImageNet classification models, commonly\nused in downstream tasks, such as the ResNet-50. We subsequently apply our\ntrained hybrid ImageNet model as a base model on a detection system, which has\ntypically larger image inputs. On Pascal VOC and COCO detection tasks we\ndemonstrate improvements in the inference speed and training memory consumption\ncompared to models trained directly on the input image. \n\n"}
{"id": "1809.10486", "contents": "Title: nnU-Net: Self-adapting Framework for U-Net-Based Medical Image\n  Segmentation Abstract: The U-Net was presented in 2015. With its straight-forward and successful\narchitecture it quickly evolved to a commonly used benchmark in medical image\nsegmentation. The adaptation of the U-Net to novel problems, however, comprises\nseveral degrees of freedom regarding the exact architecture, preprocessing,\ntraining and inference. These choices are not independent of each other and\nsubstantially impact the overall performance. The present paper introduces the\nnnU-Net ('no-new-Net'), which refers to a robust and self-adapting framework on\nthe basis of 2D and 3D vanilla U-Nets. We argue the strong case for taking away\nsuperfluous bells and whistles of many proposed network designs and instead\nfocus on the remaining aspects that make out the performance and\ngeneralizability of a method. We evaluate the nnU-Net in the context of the\nMedical Segmentation Decathlon challenge, which measures segmentation\nperformance in ten disciplines comprising distinct entities, image modalities,\nimage geometries and dataset sizes, with no manual adjustments between datasets\nallowed. At the time of manuscript submission, nnU-Net achieves the highest\nmean dice scores across all classes and seven phase 1 tasks (except class 1 in\nBrainTumour) in the online leaderboard of the challenge. \n\n"}
{"id": "1809.10635", "contents": "Title: Generative replay with feedback connections as a general strategy for\n  continual learning Abstract: A major obstacle to developing artificial intelligence applications capable\nof true lifelong learning is that artificial neural networks quickly or\ncatastrophically forget previously learned tasks when trained on a new one.\nNumerous methods for alleviating catastrophic forgetting are currently being\nproposed, but differences in evaluation protocols make it difficult to directly\ncompare their performance. To enable more meaningful comparisons, here we\nidentified three distinct scenarios for continual learning based on whether\ntask identity is known and, if it is not, whether it needs to be inferred.\nPerforming the split and permuted MNIST task protocols according to each of\nthese scenarios, we found that regularization-based approaches (e.g., elastic\nweight consolidation) failed when task identity needed to be inferred. In\ncontrast, generative replay combined with distillation (i.e., using class\nprobabilities as \"soft targets\") achieved superior performance in all three\nscenarios. Addressing the issue of efficiency, we reduced the computational\ncost of generative replay by integrating the generative model into the main\nmodel by equipping it with generative feedback or backward connections. This\nReplay-through-Feedback approach substantially shortened training time with no\nor negligible loss in performance. We believe this to be an important first\nstep towards making the powerful technique of generative replay scalable to\nreal-world continual learning applications. \n\n"}
{"id": "1809.11087", "contents": "Title: Learning to Remember, Forget and Ignore using Attention Control in\n  Memory Abstract: Typical neural networks with external memory do not effectively separate\ncapacity for episodic and working memory as is required for reasoning in\nhumans. Applying knowledge gained from psychological studies, we designed a new\nmodel called Differentiable Working Memory (DWM) in order to specifically\nemulate human working memory. As it shows the same functional characteristics\nas working memory, it robustly learns psychology inspired tasks and converges\nfaster than comparable state-of-the-art models. Moreover, the DWM model\nsuccessfully generalizes to sequences two orders of magnitude longer than the\nones used in training. Our in-depth analysis shows that the behavior of DWM is\ninterpretable and that it learns to have fine control over memory, allowing it\nto retain, ignore or forget information based on its relevance. \n\n"}
{"id": "1810.00403", "contents": "Title: Modelling local phase of images and textures with applications in phase\n  denoising and phase retrieval Abstract: The Fourier magnitude has been studied extensively, but less effort has been\ndevoted to the Fourier phase, despite its well-established importance in image\nrepresentation. Global phase was shown to be more important for image\nrepresentation than the magnitude, whereas local phase, exhibited in Gabor\nfilters, has been used for analysis purposes in detecting image contours and\nedges. Neither global nor local phase has been modelled in closed form,\nsuitable for Bayesian estimation.\n  In this work, we analyze the local phase of textured images and propose a\nlocal (Markovian) model for local phase coefficients. This model is\nGaussian-mixture-based, learned from the graph representation of images, based\non their complex wavelet decomposition. We demonstrate the applicability of the\nmodel in restoration of images with noisy local phase and in image retrieval,\nwhere we show superior performance to the well-known hybrid input-output (HIO)\nmethod. We also provide a framework for application of the model in a general\nsetup of image processing. \n\n"}
{"id": "1810.00740", "contents": "Title: Improving the Generalization of Adversarial Training with Domain\n  Adaptation Abstract: By injecting adversarial examples into training data, adversarial training is\npromising for improving the robustness of deep learning models. However, most\nexisting adversarial training approaches are based on a specific type of\nadversarial attack. It may not provide sufficiently representative samples from\nthe adversarial domain, leading to a weak generalization ability on adversarial\nexamples from other attacks. Moreover, during the adversarial training,\nadversarial perturbations on inputs are usually crafted by fast single-step\nadversaries so as to scale to large datasets. This work is mainly focused on\nthe adversarial training yet efficient FGSM adversary. In this scenario, it is\ndifficult to train a model with great generalization due to the lack of\nrepresentative adversarial samples, aka the samples are unable to accurately\nreflect the adversarial domain. To alleviate this problem, we propose a novel\nAdversarial Training with Domain Adaptation (ATDA) method. Our intuition is to\nregard the adversarial training on FGSM adversary as a domain adaption task\nwith limited number of target domain samples. The main idea is to learn a\nrepresentation that is semantically meaningful and domain invariant on the\nclean domain as well as the adversarial domain. Empirical evaluations on\nFashion-MNIST, SVHN, CIFAR-10 and CIFAR-100 demonstrate that ATDA can greatly\nimprove the generalization of adversarial training and the smoothness of the\nlearned models, and outperforms state-of-the-art methods on standard benchmark\ndatasets. To show the transfer ability of our method, we also extend ATDA to\nthe adversarial training on iterative attacks such as PGD-Adversial Training\n(PAT) and the defense performance is improved considerably. \n\n"}
{"id": "1810.01104", "contents": "Title: Target Aware Network Adaptation for Efficient Representation Learning Abstract: This paper presents an automatic network adaptation method that finds a\nConvNet structure well-suited to a given target task, e.g., image\nclassification, for efficiency as well as accuracy in transfer learning. We\ncall the concept target-aware transfer learning. Given only small-scale labeled\ndata, and starting from an ImageNet pre-trained network, we exploit a scheme of\nremoving its potential redundancy for the target task through iterative\noperations of filter-wise pruning and network optimization. The basic\nmotivation is that compact networks are on one hand more efficient and should\nalso be more tolerant, being less complex, against the risk of overfitting\nwhich would hinder the generalization of learned representations in the context\nof transfer learning. Further, unlike existing methods involving network\nsimplification, we also let the scheme identify redundant portions across the\nentire network, which automatically results in a network structure adapted to\nthe task at hand. We achieve this with a few novel ideas: (i) cumulative sum of\nactivation statistics for each layer, and (ii) a priority evaluation of pruning\nacross multiple layers. Experimental results by the method on five datasets\n(Flower102, CUB200-2011, Dog120, MIT67, and Stanford40) show favorable\naccuracies over the related state-of-the-art techniques while enhancing the\ncomputational and storage efficiency of the transferred model. \n\n"}
{"id": "1810.01406", "contents": "Title: Super-Resolution via Conditional Implicit Maximum Likelihood Estimation Abstract: Single-image super-resolution (SISR) is a canonical problem with diverse\napplications. Leading methods like SRGAN produce images that contain various\nartifacts, such as high-frequency noise, hallucinated colours and shape\ndistortions, which adversely affect the realism of the result. In this paper,\nwe propose an alternative approach based on an extension of the method of\nImplicit Maximum Likelihood Estimation (IMLE). We demonstrate greater\neffectiveness at noise reduction and preservation of the original colours and\nshapes, yielding more realistic super-resolved images. \n\n"}
{"id": "1810.01665", "contents": "Title: A Robot Localization Framework Using CNNs for Object Detection and Pose\n  Estimation Abstract: External localization is an essential part for the indoor operation of small\nor cost-efficient robots, as they are used, for example, in swarm robotics. We\nintroduce a two-stage localization and instance identification framework for\narbitrary robots based on convolutional neural networks. Object detection is\nperformed on an external camera image of the operation zone providing robot\nbounding boxes for an identification and orientation estimation convolutional\nneural network. Additionally, we propose a process to generate the necessary\ntraining data. The framework was evaluated with 3 different robot types and\nvarious identification patterns. We have analyzed the main framework\nhyperparameters providing recommendations for the framework operation settings.\nWe achieved up to 98% mAP@IOU0.5 and only 1.6{\\deg} orientation error, running\nwith a frame rate of 50 Hz on a GPU. \n\n"}
{"id": "1810.01898", "contents": "Title: A Multi-Face Challenging Dataset for Robust Face Recognition Abstract: Face recognition in images is an active area of interest among the computer\nvision researchers. However, recognizing human face in an unconstrained\nenvironment, is a relatively less-explored area of research. Multiple face\nrecognition in unconstrained environment is a challenging task, due to the\nvariation of view-point, scale, pose, illumination and expression of the face\nimages. Partial occlusion of faces makes the recognition task even more\nchallenging. The contribution of this paper is two-folds: introducing a\nchallenging multiface dataset (i.e., IIITS MFace Dataset) for face recognition\nin unconstrained environment and evaluating the performance of state-of-the-art\nhand-designed and deep learning based face descriptors on the dataset. The\nproposed IIITS MFace dataset contains faces with challenges like pose\nvariation, occlusion, mask, spectacle, expressions, change of illumination,\netc. We experiment with several state-of-the-art face descriptors, including\nrecent deep learning based face descriptors like VGGFace, and compare with the\nexisting benchmark face datasets. Results of the experiments clearly show that\nthe difficulty level of the proposed dataset is much higher compared to the\nbenchmark datasets. \n\n"}
{"id": "1810.03962", "contents": "Title: Densely Supervised Grasp Detector (DSGD) Abstract: This paper presents Densely Supervised Grasp Detector (DSGD), a deep learning\nframework which combines CNN structures with layer-wise feature fusion and\nproduces grasps and their confidence scores at different levels of the image\nhierarchy (i.e., global-, region-, and pixel-levels). % Specifically, at the\nglobal-level, DSGD uses the entire image information to predict a grasp. At the\nregion-level, DSGD uses a region proposal network to identify salient regions\nin the image and predicts a grasp for each salient region. At the pixel-level,\nDSGD uses a fully convolutional network and predicts a grasp and its confidence\nat every pixel. % During inference, DSGD selects the most confident grasp as\nthe output. This selection from hierarchically generated grasp candidates\novercomes limitations of the individual models. % DSGD outperforms\nstate-of-the-art methods on the Cornell grasp dataset in terms of grasp\naccuracy. % Evaluation on a multi-object dataset and real-world robotic\ngrasping experiments show that DSGD produces highly stable grasps on a set of\nunseen objects in new environments. It achieves 97% grasp detection accuracy\nand 90% robotic grasping success rate with real-time inference speed. \n\n"}
{"id": "1810.04452", "contents": "Title: AI Learns to Recognize Bengali Handwritten Digits: Bengali.AI Computer\n  Vision Challenge 2018 Abstract: Solving problems with Artificial intelligence in a competitive manner has\nlong been absent in Bangladesh and Bengali-speaking community. On the other\nhand, there has not been a well structured database for Bengali Handwritten\ndigits for mass public use. To bring out the best minds working in machine\nlearning and use their expertise to create a model which can easily recognize\nBengali Handwritten digits, we organized Bengali.AI Computer Vision\nChallenge.The challenge saw both local and international teams participating\nwith unprecedented efforts. \n\n"}
{"id": "1810.05045", "contents": "Title: Analysis of Noisy Evolutionary Optimization When Sampling Fails Abstract: In noisy evolutionary optimization, sampling is a common strategy to deal\nwith noise. By the sampling strategy, the fitness of a solution is evaluated\nmultiple times (called \\emph{sample size}) independently, and its true fitness\nis then approximated by the average of these evaluations. Most previous studies\non sampling are empirical, and the few theoretical studies mainly showed the\neffectiveness of sampling with a sufficiently large sample size. In this paper,\nwe theoretically examine what strategies can work when sampling with any fixed\nsample size fails. By constructing a family of artificial noisy examples, we\nprove that sampling is always ineffective, while using parent or offspring\npopulations can be helpful on some examples. We also construct an artificial\nnoisy example to show that when using neither sampling nor populations is\neffective, a tailored adaptive sampling (i.e., sampling with an adaptive sample\nsize) strategy can work. These findings may enhance our understanding of\nsampling to some extent, but future work is required to validate them in\nnatural situations. \n\n"}
{"id": "1810.05052", "contents": "Title: Deep Learning for Image Denoising: A Survey Abstract: Since the proposal of big data analysis and Graphic Processing Unit (GPU),\nthe deep learning technology has received a great deal of attention and has\nbeen widely applied in the field of imaging processing. In this paper, we have\nan aim to completely review and summarize the deep learning technologies for\nimage denoising proposed in recent years. Morever, we systematically analyze\nthe conventional machine learning methods for image denoising. Finally, we\npoint out some research directions for the deep learning technologies in image\ndenoising. \n\n"}
{"id": "1810.05475", "contents": "Title: Quantifying the amount of visual information used by neural caption\n  generators Abstract: This paper addresses the sensitivity of neural image caption generators to\ntheir visual input. A sensitivity analysis and omission analysis based on image\nfoils is reported, showing that the extent to which image captioning\narchitectures retain and are sensitive to visual information varies depending\non the type of word being generated and the position in the caption as a whole.\nWe motivate this work in the context of broader goals in the field to achieve\nmore explainability in AI. \n\n"}
{"id": "1810.05486", "contents": "Title: Training Deep Neural Network in Limited Precision Abstract: Energy and resource efficient training of DNNs will greatly extend the\napplications of deep learning. However, there are three major obstacles which\nmandate accurate calculation in high precision. In this paper, we tackle two of\nthem related to the loss of gradients during parameter update and\nbackpropagation through a softmax nonlinearity layer in low precision training.\nWe implemented SGD with Kahan summation by employing an additional parameter to\nvirtually extend the bit-width of the parameters for a reliable parameter\nupdate. We also proposed a simple guideline to help select the appropriate\nbit-width for the last FC layer followed by a softmax nonlinearity layer. It\ndetermines the lower bound of the required bit-width based on the class size of\nthe dataset. Extensive experiments on various network architectures and\nbenchmarks verifies the effectiveness of the proposed technique for low\nprecision training. \n\n"}
{"id": "1810.05723", "contents": "Title: Post-training 4-bit quantization of convolution networks for\n  rapid-deployment Abstract: Convolutional neural networks require significant memory bandwidth and\nstorage for intermediate computations, apart from substantial computing\nresources. Neural network quantization has significant benefits in reducing the\namount of intermediate results, but it often requires the full datasets and\ntime-consuming fine tuning to recover the accuracy lost after quantization.\nThis paper introduces the first practical 4-bit post training quantization\napproach: it does not involve training the quantized model (fine-tuning), nor\nit requires the availability of the full dataset. We target the quantization of\nboth activations and weights and suggest three complementary methods for\nminimizing quantization error at the tensor level, two of whom obtain a\nclosed-form analytical solution. Combining these methods, our approach achieves\naccuracy that is just a few percents less the state-of-the-art baseline across\na wide range of convolutional models. The source code to replicate all\nexperiments is available on GitHub:\n\\url{https://github.com/submission2019/cnn-quantization}. \n\n"}
{"id": "1810.05977", "contents": "Title: Learning to Sketch with Deep Q Networks and Demonstrated Strokes Abstract: Doodling is a useful and common intelligent skill that people can learn and\nmaster. In this work, we propose a two-stage learning framework to teach a\nmachine to doodle in a simulated painting environment via Stroke Demonstration\nand deep Q-learning (SDQ). The developed system, Doodle-SDQ, generates a\nsequence of pen actions to reproduce a reference drawing and mimics the\nbehavior of human painters. In the first stage, it learns to draw simple\nstrokes by imitating in supervised fashion from a set of strokeaction pairs\ncollected from artist paintings. In the second stage, it is challenged to draw\nreal and more complex doodles without ground truth actions; thus, it is trained\nwith Qlearning. Our experiments confirm that (1) doodling can be learned\nwithout direct stepby- step action supervision and (2) pretraining with stroke\ndemonstration via supervised learning is important to improve performance. We\nfurther show that Doodle-SDQ is effective at producing plausible drawings in\ndifferent media types, including sketch and watercolor. \n\n"}
{"id": "1810.07378", "contents": "Title: Progressive Weight Pruning of Deep Neural Networks using ADMM Abstract: Deep neural networks (DNNs) although achieving human-level performance in\nmany domains, have very large model size that hinders their broader\napplications on edge computing devices. Extensive research work have been\nconducted on DNN model compression or pruning. However, most of the previous\nwork took heuristic approaches. This work proposes a progressive weight pruning\napproach based on ADMM (Alternating Direction Method of Multipliers), a\npowerful technique to deal with non-convex optimization problems with\npotentially combinatorial constraints. Motivated by dynamic programming, the\nproposed method reaches extremely high pruning rate by using partial prunings\nwith moderate pruning rates. Therefore, it resolves the accuracy degradation\nand long convergence time problems when pursuing extremely high pruning ratios.\nIt achieves up to 34 times pruning rate for ImageNet dataset and 167 times\npruning rate for MNIST dataset, significantly higher than those reached by the\nliterature work. Under the same number of epochs, the proposed method also\nachieves faster convergence and higher compression rates. The codes and pruned\nDNN models are released in the link bit.ly/2zxdlss \n\n"}
{"id": "1810.07411", "contents": "Title: Continual Learning of Recurrent Neural Networks by Locally Aligning\n  Distributed Representations Abstract: Temporal models based on recurrent neural networks have proven to be quite\npowerful in a wide variety of applications. However, training these models\noften relies on back-propagation through time, which entails unfolding the\nnetwork over many time steps, making the process of conducting credit\nassignment considerably more challenging. Furthermore, the nature of\nback-propagation itself does not permit the use of non-differentiable\nactivation functions and is inherently sequential, making parallelization of\nthe underlying training process difficult. Here, we propose the Parallel\nTemporal Neural Coding Network (P-TNCN), a biologically inspired model trained\nby the learning algorithm we call Local Representation Alignment. It aims to\nresolve the difficulties and problems that plague recurrent networks trained by\nback-propagation through time. The architecture requires neither unrolling in\ntime nor the derivatives of its internal activation functions. We compare our\nmodel and learning procedure to other back-propagation through time\nalternatives (which also tend to be computationally expensive), including\nreal-time recurrent learning, echo state networks, and unbiased online\nrecurrent optimization. We show that it outperforms these on sequence modeling\nbenchmarks such as Bouncing MNIST, a new benchmark we denote as Bouncing\nNotMNIST, and Penn Treebank. Notably, our approach can in some instances\noutperform full back-propagation through time as well as variants such as\nsparse attentive back-tracking. Significantly, the hidden unit correction phase\nof P-TNCN allows it to adapt to new datasets even if its synaptic weights are\nheld fixed (zero-shot adaptation) and facilitates retention of prior generative\nknowledge when faced with a task sequence. We present results that show the\nP-TNCN's ability to conduct zero-shot adaptation and online continual sequence\nmodeling. \n\n"}
{"id": "1810.07901", "contents": "Title: Decoupling Semantic Context and Color Correlation with multi-class cross\n  branch regularization Abstract: This paper presents a novel design methodology for architecting a\nlight-weight and faster DNN architecture for vision applications. The\neffectiveness of the architecture is demonstrated on Color-Constancy use case\nan inherent block in camera and imaging pipelines. Specifically, we present a\nmulti-branch architecture that disassembles the contextual features and color\nproperties from an image, and later combines them to predict a global property\n(e.g. Global Illumination). We also propose an implicit regularization\ntechnique by designing cross-branch regularization block that enables the\nnetwork to retain high generalization accuracy. With a conservative use of best\ncomputational operators, the proposed architecture achieves state-of-the-art\naccuracy with 30X lesser model parameters and 70X faster inference time for\ncolor constancy. It is also shown that the proposed architecture is generic and\nachieves similar efficiency in other vision applications such as Low-Light\nphotography. \n\n"}
{"id": "1810.08103", "contents": "Title: Salience Biased Loss for Object Detection in Aerial Images Abstract: Object detection in remote sensing, especially in aerial images, remains a\nchallenging problem due to low image resolution, complex backgrounds, and\nvariation of scale and angles of objects in images. In current implementations,\nmulti-scale based and angle-based networks have been proposed and generate\npromising results with aerial image detection. In this paper, we propose a\nnovel loss function, called Salience Biased Loss (SBL), for deep neural\nnetworks, which uses salience information of the input image to achieve\nimproved performance for object detection. Our novel loss function treats\ntraining examples differently based on input complexity in order to avoid the\nover-contribution of easy cases in the training process. In our experiments,\nRetinaNet was trained with SBL to generate an one-stage detector,\nSBL-RetinaNet. SBL-RetinaNet is applied to the largest existing public aerial\nimage dataset, DOTA. Experimental results show our proposed loss function with\nthe RetinaNet architecture outperformed other state-of-art object detection\nmodels by at least 4.31 mAP, and RetinaNet by 2.26 mAP with the same inference\nspeed of RetinaNet. \n\n"}
{"id": "1810.08338", "contents": "Title: Multi-Domain Pose Network for Multi-Person Pose Estimation and Tracking Abstract: Multi-person human pose estimation and tracking in the wild is important and\nchallenging. For training a powerful model, large-scale training data are\ncrucial. While there are several datasets for human pose estimation, the best\npractice for training on multi-dataset has not been investigated. In this\npaper, we present a simple network called Multi-Domain Pose Network (MDPN) to\naddress this problem. By treating the task as multi-domain learning, our\nmethods can learn a better representation for pose prediction. Together with\nprediction heads fine-tuning and multi-branch combination, it shows significant\nimprovement over baselines and achieves the best performance on PoseTrack ECCV\n2018 Challenge without additional datasets other than MPII and COCO. \n\n"}
{"id": "1810.08476", "contents": "Title: Improving Fast Segmentation With Teacher-student Learning Abstract: Recently, segmentation neural networks have been significantly improved by\ndemonstrating very promising accuracies on public benchmarks. However, these\nmodels are very heavy and generally suffer from low inference speed, which\nlimits their application scenarios in practice. Meanwhile, existing fast\nsegmentation models usually fail to obtain satisfactory segmentation accuracies\non public benchmarks. In this paper, we propose a teacher-student learning\nframework that transfers the knowledge gained by a heavy and better performed\nsegmentation network (i.e. teacher) to guide the learning of fast segmentation\nnetworks (i.e. student). Specifically, both zero-order and first-order\nknowledge depicted in the fine annotated images and unlabeled auxiliary data\nare transferred to regularize our student learning. The proposed method can\nimprove existing fast segmentation models without incurring extra computational\noverhead, so it can still process images with the same fast speed. Extensive\nexperiments on the Pascal Context, Cityscape and VOC 2012 datasets demonstrate\nthat the proposed teacher-student learning framework is able to significantly\nboost the performance of student network. \n\n"}
{"id": "1810.08597", "contents": "Title: Detecting cities in aerial night-time images by learning structural\n  invariants using single reference augmentation Abstract: This paper examines, if it is possible to learn structural invariants of city\nimages by using only a single reference picture when producing transformations\nalong the variants in the dataset. Previous work explored the problem of\nlearning from only a few examples and showed that data augmentation techniques\nbenefit performance and generalization for machine learning approaches. First a\nprincipal component analysis in conjunction with a Fourier transform is trained\non a single reference augmentation training dataset using the city images.\nSecondly a convolutional neural network is trained on a similar dataset with\nmore samples. The findings are that the convolutional neural network is capable\nof finding images of the same category whereas the applied principal component\nanalysis in conjunction with a Fourier transform failed to solve this task. \n\n"}
{"id": "1810.08878", "contents": "Title: A Regressive Convolution Neural network and Support Vector Regression\n  Model for Electricity Consumption Forecasting Abstract: Electricity consumption forecasting has important implications for the\nmineral companies on guiding quarterly work, normal power system operation, and\nthe management. However, electricity consumption prediction for the mineral\ncompany is different from traditional electricity load prediction since mineral\ncompany electricity consumption can be affected by various factors (e.g., ore\ngrade, processing quantity of the crude ore, ball milling fill rate). The\nproblem is non-trivial due to three major challenges for traditional methods:\ninsufficient training data, high computational cost and low prediction\naccu-racy. To tackle these challenges, we firstly propose a Regressive\nConvolution Neural Network (RCNN) to predict the electricity consumption. While\nRCNN still suffers from high computation overhead, we utilize RCNN to extract\nfeatures from the history data and Regressive Support Vector Machine (SVR)\ntrained with the features to predict the electricity consumption. The\nexperimental results show that the proposed RCNN-SVR model achieves higher\naccuracy than using the traditional RNN or SVM alone. The MSE, MAPE, and\nCV-RMSE of RCNN-SVR model are 0.8564, 1.975%, and 0.0687% respectively, which\nillustrates the low predicting error rate of the proposed model. \n\n"}
{"id": "1810.09660", "contents": "Title: Large scale visual place recognition with sub-linear storage growth Abstract: Robotic and animal mapping systems share many of the same objectives and\nchallenges, but differ in one key aspect: where much of the research in robotic\nmapping has focused on solving the data association problem, the grid cell\nneurons underlying maps in the mammalian brain appear to intentionally break\ndata association by encoding many locations with a single grid cell neuron. One\npotential benefit of this intentional aliasing is both sub-linear map storage\nand computational requirements growth with environment size, which we\ndemonstrated in a previous proof-of-concept study that detected and encoded\nmutually complementary co-prime pattern frequencies in the visual map data. In\nthis research, we solve several of the key theoretical and practical\nlimitations of that prototype model and achieve significantly better sub-linear\nstorage growth, a factor reduction in storage requirements per map location,\nscalability to large datasets on standard compute equipment and improved\nrobustness to environments with visually challenging appearance change. These\nimprovements are achieved through several innovations including a flexible\nuser-driven choice mechanism for the periodic patterns underlying the new\nencoding method, a parallelized chunking technique that splits the map into\nsub-sections processed in parallel and a novel feature selection approach that\nselects only the image information most relevant to the encoded temporal\npatterns. We evaluate our techniques on two large benchmark datasets with the\ncomparison to the previous state-of-the-art system, as well as providing a\ndetailed analysis of system performance with respect to parameters such as\nrequired precision performance and the number of cyclic patterns encoded. \n\n"}
{"id": "1810.09726", "contents": "Title: CEREALS - Cost-Effective REgion-based Active Learning for Semantic\n  Segmentation Abstract: State of the art methods for semantic image segmentation are trained in a\nsupervised fashion using a large corpus of fully labeled training images.\nHowever, gathering such a corpus is expensive, due to human annotation effort,\nin contrast to gathering unlabeled data. We propose an active learning-based\nstrategy, called CEREALS, in which a human only has to hand-label a few,\nautomatically selected, regions within an unlabeled image corpus. This\nminimizes human annotation effort while maximizing the performance of a\nsemantic image segmentation method. The automatic selection procedure is\nachieved by: a) using a suitable information measure combined with an estimate\nabout human annotation effort, which is inferred from a learned cost model, and\nb) exploiting the spatial coherency of an image. The performance of CEREALS is\ndemonstrated on Cityscapes, where we are able to reduce the annotation effort\nto 17%, while keeping 95% of the mean Intersection over Union (mIoU) of a model\nthat was trained with the fully annotated training set of Cityscapes. \n\n"}
{"id": "1810.09805", "contents": "Title: Action and intention recognition of pedestrians in urban traffic Abstract: Action and intention recognition of pedestrians in urban settings are\nchallenging problems for Advanced Driver Assistance Systems as well as future\nautonomous vehicles to maintain smooth and safe traffic. This work investigates\na number of feature extraction methods in combination with several machine\nlearning algorithms to build knowledge on how to automatically detect the\naction and intention of pedestrians in urban traffic. We focus on the motion\nand head orientation to predict whether the pedestrian is about to cross the\nstreet or not. The work is based on the Joint Attention for Autonomous Driving\n(JAAD) dataset, which contains 346 videoclips of various traffic scenarios\ncaptured with cameras mounted in the windshield of a car. An accuracy of 72%\nfor head orientation estimation and 85% for motion detection is obtained in our\nexperiments. \n\n"}
{"id": "1810.10180", "contents": "Title: Understanding and correcting pathologies in the training of learned\n  optimizers Abstract: Deep learning has shown that learned functions can dramatically outperform\nhand-designed functions on perceptual tasks. Analogously, this suggests that\nlearned optimizers may similarly outperform current hand-designed optimizers,\nespecially for specific problems. However, learned optimizers are notoriously\ndifficult to train and have yet to demonstrate wall-clock speedups over\nhand-designed optimizers, and thus are rarely used in practice. Typically,\nlearned optimizers are trained by truncated backpropagation through an unrolled\noptimization process resulting in gradients that are either strongly biased\n(for short truncations) or have exploding norm (for long truncations). In this\nwork we propose a training scheme which overcomes both of these difficulties,\nby dynamically weighting two unbiased gradient estimators for a variational\nloss on optimizer performance, allowing us to train neural networks to perform\noptimization of a specific task faster than tuned first-order methods. We\ndemonstrate these results on problems where our learned optimizer trains\nconvolutional networks faster in wall-clock time compared to tuned first-order\nmethods and with an improvement in test loss. \n\n"}
{"id": "1810.11027", "contents": "Title: On the dissection of degenerate cosmologies with machine learning Abstract: Based on the DUSTGRAIN-pathfinder suite of simulations, we investigate\nobservational degeneracies between nine models of modified gravity and massive\nneutrinos. Three types of machine learning techniques are tested for their\nability to discriminate lensing convergence maps by extracting dimensional\nreduced representations of the data. Classical map descriptors such as the\npower spectrum, peak counts and Minkowski functionals are combined into a joint\nfeature vector and compared to the descriptors and statistics that are common\nto the field of digital image processing. To learn new features directly from\nthe data we use a Convolutional Neural Network (CNN). For the mapping between\nfeature vectors and the predictions of their underlying model, we implement two\ndifferent classifiers; one based on a nearest-neighbour search and one that is\nbased on a fully connected neural network. We find that the neural network\nprovides a much more robust classification than the nearest-neighbour approach\nand that the CNN provides the most discriminating representation of the data.\nIt achieves the cleanest separation between the different models and the\nhighest classification success rate of 59% for a single source redshift. Once\nwe perform a tomographic CNN analysis, the total classification accuracy\nincreases significantly to 76% with no observational degeneracies remaining.\nVisualising the filter responses of the CNN at different network depths\nprovides us with the unique opportunity to learn from very complex models and\nto understand better why they perform so well. \n\n"}
{"id": "1810.11137", "contents": "Title: Towards improved lossy image compression: Human image reconstruction\n  with public-domain images Abstract: Lossy image compression has been studied extensively in the context of\ntypical loss functions such as RMSE, MS-SSIM, etc. However, compression at low\nbitrates generally produces unsatisfying results. Furthermore, the availability\nof massive public image datasets appears to have hardly been exploited in image\ncompression. Here, we present a paradigm for eliciting human image\nreconstruction in order to perform lossy image compression. In this paradigm,\none human describes images to a second human, whose task is to reconstruct the\ntarget image using publicly available images and text instructions. The\nresulting reconstructions are then evaluated by human raters on the Amazon\nMechanical Turk platform and compared to reconstructions obtained using\nstate-of-the-art compressor WebP. Our results suggest that prioritizing\nsemantic visual elements may be key to achieving significant improvements in\nimage compression, and that our paradigm can be used to develop a more\nhuman-centric loss function.\n  The images, results and additional data are available at\nhttps://compression.stanford.edu/human-compression \n\n"}
{"id": "1810.11348", "contents": "Title: Security Event Recognition for Visual Surveillance Abstract: With rapidly increasing deployment of surveillance cameras, the reliable\nmethods for automatically analyzing the surveillance video and recognizing\nspecial events are demanded by different practical applications. This paper\nproposes a novel effective framework for security event analysis in\nsurveillance videos. First, convolutional neural network (CNN) framework is\nused to detect objects of interest in the given videos. Second, the owners of\nthe objects are recognized and monitored in real-time as well. If anyone moves\nany object, this person will be verified whether he/she is its owner. If not,\nthis event will be further analyzed and distinguished between two different\nscenes: moving the object away or stealing it. To validate the proposed\napproach, a new video dataset consisting of various scenarios is constructed\nfor more complex tasks. For comparison purpose, the experiments are also\ncarried out on the benchmark databases related to the task on abandoned luggage\ndetection. The experimental results show that the proposed approach outperforms\nthe state-of-the-art methods and effective in recognizing complex security\nevents. \n\n"}
{"id": "1810.11491", "contents": "Title: Empirical Evaluation of Contextual Policy Search with a Comparison-based\n  Surrogate Model and Active Covariance Matrix Adaptation Abstract: Contextual policy search (CPS) is a class of multi-task reinforcement\nlearning algorithms that is particularly useful for robotic applications. A\nrecent state-of-the-art method is Contextual Covariance Matrix Adaptation\nEvolution Strategies (C-CMA-ES). It is based on the standard black-box\noptimization algorithm CMA-ES. There are two useful extensions of CMA-ES that\nwe will transfer to C-CMA-ES and evaluate empirically: ACM-ES, which uses a\ncomparison-based surrogate model, and aCMA-ES, which uses an active update of\nthe covariance matrix. We will show that improvements with these methods can be\nimpressive in terms of sample-efficiency, although this is not relevant any\nmore for the robotic domain. \n\n"}
{"id": "1810.11598", "contents": "Title: Self-Supervised GAN to Counter Forgetting Abstract: GANs involve training two networks in an adversarial game, where each\nnetwork's task depends on its adversary. Recently, several works have framed\nGAN training as an online or continual learning problem. We focus on the\ndiscriminator, which must perform classification under an (adversarially)\nshifting data distribution. When trained on sequential tasks, neural networks\nexhibit \\emph{forgetting}. For GANs, discriminator forgetting leads to training\ninstability. To counter forgetting, we encourage the discriminator to maintain\nuseful representations by adding a self-supervision. Conditional GANs have a\nsimilar effect using labels. However, our self-supervised GAN does not require\nlabels, and closes the performance gap between conditional and unconditional\nmodels. We show that, in doing so, the self-supervised discriminator learns\nbetter representations than regular GANs. \n\n"}
{"id": "1810.11919", "contents": "Title: Text-Adaptive Generative Adversarial Networks: Manipulating Images with\n  Natural Language Abstract: This paper addresses the problem of manipulating images using natural\nlanguage description. Our task aims to semantically modify visual attributes of\nan object in an image according to the text describing the new visual\nappearance. Although existing methods synthesize images having new attributes,\nthey do not fully preserve text-irrelevant contents of the original image. In\nthis paper, we propose the text-adaptive generative adversarial network (TAGAN)\nto generate semantically manipulated images while preserving text-irrelevant\ncontents. The key to our method is the text-adaptive discriminator that creates\nword-level local discriminators according to input text to classify\nfine-grained attributes independently. With this discriminator, the generator\nlearns to generate images where only regions that correspond to the given text\nare modified. Experimental results show that our method outperforms existing\nmethods on CUB and Oxford-102 datasets, and our results were mostly preferred\non a user study. Extensive analysis shows that our method is able to\neffectively disentangle visual attributes and produce pleasing outputs. \n\n"}
{"id": "1810.12065", "contents": "Title: On the Convergence Rate of Training Recurrent Neural Networks Abstract: How can local-search methods such as stochastic gradient descent (SGD) avoid\nbad local minima in training multi-layer neural networks? Why can they fit\nrandom labels even given non-convex and non-smooth architectures? Most existing\ntheory only covers networks with one hidden layer, so can we go deeper?\n  In this paper, we focus on recurrent neural networks (RNNs) which are\nmulti-layer networks widely used in natural language processing. They are\nharder to analyze than feedforward neural networks, because the $\\textit{same}$\nrecurrent unit is repeatedly applied across the entire time horizon of length\n$L$, which is analogous to feedforward networks of depth $L$. We show when the\nnumber of neurons is sufficiently large, meaning polynomial in the training\ndata size and in $L$, then SGD is capable of minimizing the regression loss in\nthe linear convergence rate. This gives theoretical evidence of how RNNs can\nmemorize data.\n  More importantly, in this paper we build general toolkits to analyze\nmulti-layer networks with ReLU activations. For instance, we prove why ReLU\nactivations can prevent exponential gradient explosion or vanishing, and build\na perturbation theory to analyze first-order approximation of multi-layer\nnetworks. \n\n"}
{"id": "1810.12456", "contents": "Title: A Simple Recurrent Unit with Reduced Tensor Product Representations Abstract: idely used recurrent units, including Long-short Term Memory (LSTM) and the\nGated Recurrent Unit (GRU), perform well on natural language tasks, but their\nability to learn structured representations is still questionable. Exploiting\nreduced Tensor Product Representations (TPRs) --- distributed representations\nof symbolic structure in which vector-embedded symbols are bound to\nvector-embedded structural positions --- we propose the TPRU, a simple\nrecurrent unit that, at each time step, explicitly executes structural-role\nbinding and unbinding operations to incorporate structural information into\nlearning. A gradient analysis of our proposed TPRU is conducted to support our\nmodel design, and its performance on multiple datasets shows the effectiveness\nof our design choices. Furthermore, observations on a linguistically grounded\nstudy demonstrate the interpretability of our TPRU. \n\n"}
{"id": "1810.13273", "contents": "Title: Ionospheric activity prediction using convolutional recurrent neural\n  networks Abstract: The ionosphere electromagnetic activity is a major factor of the quality of\nsatellite telecommunications, Global Navigation Satellite Systems (GNSS) and\nother vital space applications. Being able to forecast globally the Total\nElectron Content (TEC) would enable a better anticipation of potential\nperformance degradations. A few studies have proposed models able to predict\nthe TEC locally, but not worldwide for most of them. Thanks to a large record\nof past TEC maps publicly available, we propose a method based on Deep Neural\nNetworks (DNN) to forecast a sequence of global TEC maps consecutive to an\ninput sequence of TEC maps, without introducing any prior knowledge other than\nEarth rotation periodicity. By combining several state-of-the-art\narchitectures, the proposed approach is competitive with previous works on TEC\nforecasting while predicting the TEC globally. \n\n"}
{"id": "1811.00656", "contents": "Title: Exposing DeepFake Videos By Detecting Face Warping Artifacts Abstract: In this work, we describe a new deep learning based method that can\neffectively distinguish AI-generated fake videos (referred to as {\\em DeepFake}\nvideos hereafter) from real videos. Our method is based on the observations\nthat current DeepFake algorithm can only generate images of limited\nresolutions, which need to be further warped to match the original faces in the\nsource video. Such transforms leave distinctive artifacts in the resulting\nDeepFake videos, and we show that they can be effectively captured by\nconvolutional neural networks (CNNs). Compared to previous methods which use a\nlarge amount of real and DeepFake generated images to train CNN classifier, our\nmethod does not need DeepFake generated images as negative training examples\nsince we target the artifacts in affine face warping as the distinctive feature\nto distinguish real and fake images. The advantages of our method are two-fold:\n(1) Such artifacts can be simulated directly using simple image processing\noperations on a image to make it as negative example. Since training a DeepFake\nmodel to generate negative examples is time-consuming and resource-demanding,\nour method saves a plenty of time and resources in training data collection;\n(2) Since such artifacts are general existed in DeepFake videos from different\nsources, our method is more robust compared to others. Our method is evaluated\non two sets of DeepFake video datasets for its effectiveness in practice. \n\n"}
{"id": "1811.00751", "contents": "Title: Show, Attend and Read: A Simple and Strong Baseline for Irregular Text\n  Recognition Abstract: Recognizing irregular text in natural scene images is challenging due to the\nlarge variance in text appearance, such as curvature, orientation and\ndistortion. Most existing approaches rely heavily on sophisticated model\ndesigns and/or extra fine-grained annotations, which, to some extent, increase\nthe difficulty in algorithm implementation and data collection. In this work,\nwe propose an easy-to-implement strong baseline for irregular scene text\nrecognition, using off-the-shelf neural network components and only word-level\nannotations. It is composed of a $31$-layer ResNet, an LSTM-based\nencoder-decoder framework and a 2-dimensional attention module. Despite its\nsimplicity, the proposed method is robust and achieves state-of-the-art\nperformance on both regular and irregular scene text recognition benchmarks.\nCode is available at: https://tinyurl.com/ShowAttendRead \n\n"}
{"id": "1811.01194", "contents": "Title: Pushing the boundaries of audiovisual word recognition using Residual\n  Networks and LSTMs Abstract: Visual and audiovisual speech recognition are witnessing a renaissance which\nis largely due to the advent of deep learning methods. In this paper, we\npresent a deep learning architecture for lipreading and audiovisual word\nrecognition, which combines Residual Networks equipped with spatiotemporal\ninput layers and Bidirectional LSTMs. The lipreading architecture attains\n11.92% misclassification rate on the challenging Lipreading-In-The-Wild\ndatabase, which is composed of excerpts from BBC-TV, each containing one of the\n500 target words. Audiovisual experiments are performed using both intermediate\nand late integration, as well as several types and levels of environmental\nnoise, and notable improvements over the audio-only network are reported, even\nin the case of clean speech. A further analysis on the utility of target word\nboundaries is provided, as well as on the capacity of the network in modeling\nthe linguistic context of the target word. Finally, we examine difficult word\npairs and discuss how visual information helps towards attaining higher\nrecognition accuracy. \n\n"}
{"id": "1811.01405", "contents": "Title: DeepKey: Towards End-to-End Physical Key Replication From a Single\n  Photograph Abstract: This paper describes DeepKey, an end-to-end deep neural architecture capable\nof taking a digital RGB image of an 'everyday' scene containing a pin tumbler\nkey (e.g. lying on a table or carpet) and fully automatically inferring a\nprintable 3D key model. We report on the key detection performance and describe\nhow candidates can be transformed into physical prints. We show an example\nopening a real-world lock. Our system is described in detail, providing a\nbreakdown of all components including key detection, pose normalisation,\nbitting segmentation and 3D model inference. We provide an in-depth evaluation\nand conclude by reflecting on limitations, applications, potential security\nrisks and societal impact. We contribute the DeepKey Datasets of 5, 300+ images\ncovering a few test keys with bounding boxes, pose and unaligned mask data. \n\n"}
{"id": "1811.01549", "contents": "Title: StNet: Local and Global Spatial-Temporal Modeling for Action Recognition Abstract: Despite the success of deep learning for static image understanding, it\nremains unclear what are the most effective network architectures for the\nspatial-temporal modeling in videos. In this paper, in contrast to the existing\nCNN+RNN or pure 3D convolution based approaches, we explore a novel spatial\ntemporal network (StNet) architecture for both local and global\nspatial-temporal modeling in videos. Particularly, StNet stacks N successive\nvideo frames into a \\emph{super-image} which has 3N channels and applies 2D\nconvolution on super-images to capture local spatial-temporal relationship. To\nmodel global spatial-temporal relationship, we apply temporal convolution on\nthe local spatial-temporal feature maps. Specifically, a novel temporal\nXception block is proposed in StNet. It employs a separate channel-wise and\ntemporal-wise convolution over the feature sequence of video. Extensive\nexperiments on the Kinetics dataset demonstrate that our framework outperforms\nseveral state-of-the-art approaches in action recognition and can strike a\nsatisfying trade-off between recognition accuracy and model complexity. We\nfurther demonstrate the generalization performance of the leaned video\nrepresentations on the UCF101 dataset. \n\n"}
{"id": "1811.02233", "contents": "Title: Weakly Supervised Scene Parsing with Point-based Distance Metric\n  Learning Abstract: Semantic scene parsing is suffering from the fact that pixel-level\nannotations are hard to be collected. To tackle this issue, we propose a\nPoint-based Distance Metric Learning (PDML) in this paper. PDML does not\nrequire dense annotated masks and only leverages several labeled points that\nare much easier to obtain to guide the training process. Concretely, we\nleverage semantic relationship among the annotated points by encouraging the\nfeature representations of the intra- and inter-category points to keep\nconsistent, i.e. points within the same category should have more similar\nfeature representations compared to those from different categories. We\nformulate such a characteristic into a simple distance metric loss, which\ncollaborates with the point-wise cross-entropy loss to optimize the deep neural\nnetworks. Furthermore, to fully exploit the limited annotations, distance\nmetric learning is conducted across different training images instead of simply\nadopting an image-dependent manner. We conduct extensive experiments on two\nchallenging scene parsing benchmarks of PASCAL-Context and ADE 20K to validate\nthe effectiveness of our PDML, and competitive mIoU scores are achieved. \n\n"}
{"id": "1811.02372", "contents": "Title: Identifica\\c{c}\\~ao autom\\'atica de picha\\c{c}\\~ao a partir de imagens\n  urbanas Abstract: Graffiti tagging is a common issue in great cities an local authorities are\non the move to combat it. The tagging map of a city can be a useful tool as it\nmay help to clean-up highly saturated regions and discourage future acts in the\nneighbourhood and currently there is no way of getting a tagging map of a\nregion in an automatic fashion and manual inspection or crowd participation are\nrequired. In this work, we describe a work in progress in creating an automatic\nway to get a tagging map of a city or region. It is based on the use of street\nview images and on the detection of graffiti tags in the images. \n\n"}
{"id": "1811.02413", "contents": "Title: Low-Rank Tensor Modeling for Hyperspectral Unmixing Accounting for\n  Spectral Variability Abstract: Traditional hyperspectral unmixing methods neglect the underlying variability\nof spectral signatures often observed in typical hyperspectral images (HI),\npropagating these missmodeling errors throughout the whole unmixing process.\nAttempts to model material spectra as members of sets or as random variables\ntend to lead to severely ill-posed unmixing problems. Although parametric\nmodels have been proposed to overcome this drawback by handling endmember\nvariability through generalizations of the mixing model, the success of these\ntechniques depend on employing appropriate regularization strategies. Moreover,\nthe existing approaches fail to adequately explore the natural multidimensinal\nrepresentation of HIs. Recently, tensor-based strategies considered low-rank\ndecompositions of hyperspectral images as an alternative to impose\nlow-dimensional structures on the solutions of standard and multitemporal\nunmixing problems. These strategies, however, present two main drawbacks: 1)\nthey confine the solutions to low-rank tensors, which often cannot represent\nthe complexity of real-world scenarios; and 2) they lack guarantees that\nendmembers and abundances will be correctly factorized in their respective\ntensors. In this work, we propose a more flexible approach, called ULTRA-V,\nthat imposes low-rank structures through regularizations whose strictness is\ncontrolled by scalar parameters. Simulations attest the superior accuracy of\nthe method when compared with state-of-the-art unmixing algorithms that account\nfor spectral variability. \n\n"}
{"id": "1811.02949", "contents": "Title: Instance Retrieval at Fine-grained Level Using Multi-Attribute\n  Recognition Abstract: In this paper, we present a method for instance ranking and retrieval at\nfine-grained level based on the global features extracted from a\nmulti-attribute recognition model which is not dependent on landmarks\ninformation or part-based annotations. Further, we make this architecture\nsuitable for mobile-device application by adopting the bilinear CNN to make the\nmulti-attribute recognition model smaller (in terms of the number of\nparameters). The experiments run on the Dress category of DeepFashion In-Shop\nClothes Retrieval and CUB200 datasets show that the results of instance\nretrieval at fine-grained level are promising for these datasets, specially in\nterms of texture and color. \n\n"}
{"id": "1811.03120", "contents": "Title: ColorUNet: A convolutional classification approach to colorization Abstract: This paper tackles the challenge of colorizing grayscale images. We take a\ndeep convolutional neural network approach, and choose to take the angle of\nclassification, working on a finite set of possible colors. Similarly to a\nrecent paper, we implement a loss and a prediction function that favor\nrealistic, colorful images rather than \"true\" ones.\n  We show that a rather lightweight architecture inspired by the U-Net, and\ntrained on a reasonable amount of pictures of landscapes, achieves satisfactory\nresults on this specific subset of pictures. We show that data augmentation\nsignificantly improves the performance and robustness of the model, and provide\nvisual analysis of the prediction confidence.\n  We show an application of our model, extending the task to video\ncolorization. We suggest a way to smooth color predictions across frames,\nwithout the need to train a recurrent network designed for sequential inputs. \n\n"}
{"id": "1811.03331", "contents": "Title: Improving Multi-Person Pose Estimation using Label Correction Abstract: Significant attention is being paid to multi-person pose estimation methods\nrecently, as there has been rapid progress in the field owing to convolutional\nneural networks. Especially, recent method which exploits part confidence maps\nand Part Affinity Fields (PAFs) has achieved accurate real-time prediction of\nmulti-person keypoints. However, human annotated labels are sometimes\ninappropriate for learning models. For example, if there is a limb that extends\noutside an image, a keypoint for the limb may not have annotations because it\nexists outside of the image, and thus the labels for the limb can not be\ngenerated. If a model is trained with data including such missing labels, the\noutput of the model for the location, even though it is correct, is penalized\nas a false positive, which is likely to cause negative effects on the\nperformance of the model. In this paper, we point out the existence of some\npatterns of inappropriate labels, and propose a novel method for correcting\nsuch labels with a teacher model trained on such incomplete data. Experiments\non the COCO dataset show that training with the corrected labels improves the\nperformance of the model and also speeds up training. \n\n"}
{"id": "1811.03529", "contents": "Title: Memorable Maps: A Framework for Re-defining Places in Visual Place\n  Recognition Abstract: This paper presents a cognition-inspired agnostic framework for building a\nmap for Visual Place Recognition. This framework draws inspiration from\nhuman-memorability, utilizes the traditional image entropy concept and computes\nthe static content in an image; thereby presenting a tri-folded criterion to\nassess the 'memorability' of an image for visual place recognition. A dataset\nnamely 'ESSEX3IN1' is created, composed of highly confusing images from indoor,\noutdoor and natural scenes for analysis. When used in conjunction with\nstate-of-the-art visual place recognition methods, the proposed framework\nprovides significant performance boost to these techniques, as evidenced by\nresults on ESSEX3IN1 and other public datasets. \n\n"}
{"id": "1811.03549", "contents": "Title: An End-to-end Approach to Semantic Segmentation with 3D CNN and\n  Posterior-CRF in Medical Images Abstract: Fully-connected Conditional Random Field (CRF) is often used as\npost-processing to refine voxel classification results by encouraging spatial\ncoherence. In this paper, we propose a new end-to-end training method called\nPosterior-CRF. In contrast with previous approaches which use the original\nimage intensity in the CRF, our approach applies 3D, fully connected CRF to the\nposterior probabilities from a CNN and optimizes both CNN and CRF together. The\nexperiments on white matter hyperintensities segmentation demonstrate that our\nmethod outperforms CNN, post-processing CRF and different end-to-end training\nCRF approaches. \n\n"}
{"id": "1811.05029", "contents": "Title: LookinGood: Enhancing Performance Capture with Real-time Neural\n  Re-Rendering Abstract: Motivated by augmented and virtual reality applications such as telepresence,\nthere has been a recent focus in real-time performance capture of humans under\nmotion. However, given the real-time constraint, these systems often suffer\nfrom artifacts in geometry and texture such as holes and noise in the final\nrendering, poor lighting, and low-resolution textures. We take the novel\napproach to augment such real-time performance capture systems with a deep\narchitecture that takes a rendering from an arbitrary viewpoint, and jointly\nperforms completion, super resolution, and denoising of the imagery in\nreal-time. We call this approach neural (re-)rendering, and our live system\n\"LookinGood\". Our deep architecture is trained to produce high resolution and\nhigh quality images from a coarse rendering in real-time. First, we propose a\nself-supervised training method that does not require manual ground-truth\nannotation. We contribute a specialized reconstruction error that uses semantic\ninformation to focus on relevant parts of the subject, e.g. the face. We also\nintroduce a salient reweighing scheme of the loss function that is able to\ndiscard outliers. We specifically design the system for virtual and augmented\nreality headsets where the consistency between the left and right eye plays a\ncrucial role in the final user experience. Finally, we generate temporally\nstable results by explicitly minimizing the difference between two consecutive\nframes. We tested the proposed system in two different scenarios: one involving\na single RGB-D sensor, and upper body reconstruction of an actor, the second\nconsisting of full body 360 degree capture. Through extensive experimentation,\nwe demonstrate how our system generalizes across unseen sequences and subjects.\nThe supplementary video is available at http://youtu.be/Md3tdAKoLGU. \n\n"}
{"id": "1811.05592", "contents": "Title: Controllability, Multiplexing, and Transfer Learning in Networks using\n  Evolutionary Learning Abstract: Networks are fundamental building blocks for representing data, and\ncomputations. Remarkable progress in learning in structurally defined (shallow\nor deep) networks has recently been achieved. Here we introduce evolutionary\nexploratory search and learning method of topologically flexible networks under\nthe constraint of producing elementary computational steady-state input-output\noperations.\n  Our results include; (1) the identification of networks, over four orders of\nmagnitude, implementing computation of steady-state input-output functions,\nsuch as a band-pass filter, a threshold function, and an inverse band-pass\nfunction. Next, (2) the learned networks are technically controllable as only a\nsmall number of driver nodes are required to move the system to a new state.\nFurthermore, we find that the fraction of required driver nodes is constant\nduring evolutionary learning, suggesting a stable system design. (3), our\nframework allows multiplexing of different computations using the same network.\nFor example, using a binary representation of the inputs, the network can\nreadily compute three different input-output functions. Finally, (4) the\nproposed evolutionary learning demonstrates transfer learning. If the system\nlearns one function A, then learning B requires on average less number of steps\nas compared to learning B from tabula rasa.\n  We conclude that the constrained evolutionary learning produces large robust\ncontrollable circuits, capable of multiplexing and transfer learning. Our study\nsuggests that network-based computations of steady-state functions,\nrepresenting either cellular modules of cell-to-cell communication networks or\ninternal molecular circuits communicating within a cell, could be a powerful\nmodel for biologically inspired computing. This complements conceptualizations\nsuch as attractor based models, or reservoir computing. \n\n"}
{"id": "1811.05819", "contents": "Title: Distortion Robust Image Classification using Deep Convolutional Neural\n  Network with Discrete Cosine Transform Abstract: Convolutional Neural Network is good at image classification. However, it is\nfound to be vulnerable to image quality degradation. Even a small amount of\ndistortion such as noise or blur can severely hamper the performance of these\nCNN architectures. Most of the work in the literature strives to mitigate this\nproblem simply by fine-tuning a pre-trained CNN on mutually exclusive or a\nunion set of distorted training data. This iterative fine-tuning process with\nall known types of distortion is exhaustive and the network struggles to handle\nunseen distortions. In this work, we propose distortion robust DCT-Net, a\nDiscrete Cosine Transform based module integrated into a deep network which is\nbuilt on top of VGG16. Unlike other works in the literature, DCT-Net is \"blind\"\nto the distortion type and level in an image both during training and testing.\nAs a part of the training process, the proposed DCT module discards input\ninformation which mostly represents the contribution of high frequencies. The\nDCT-Net is trained \"blindly\" only once and applied in generic situation without\nfurther retraining. We also extend the idea of traditional dropout and present\na training adaptive version of the same. We evaluate our proposed method\nagainst Gaussian blur, motion blur, salt and pepper noise, Gaussian noise and\nspeckle noise added to CIFAR-10/100 and ImageNet test sets. Experimental\nresults demonstrate that once trained, DCT-Net not only generalizes well to a\nvariety of unseen image distortions but also outperforms other methods in the\nliterature. \n\n"}
{"id": "1811.06047", "contents": "Title: Looking at the Driver/Rider in Autonomous Vehicles to Predict Take-Over\n  Readiness Abstract: Continuous estimation the driver's take-over readiness is critical for safe\nand timely transfer of control during the failure modes of autonomous vehicles.\nIn this paper, we propose a data-driven approach for estimating the driver's\ntake-over readiness based purely on observable cues from in-vehicle vision\nsensors. We present an extensive naturalistic drive dataset of drivers in a\nconditionally autonomous vehicle running on Californian freeways. We collect\nsubjective ratings for the driver's take-over readiness from multiple human\nobservers viewing the sensor feed. Analysis of the ratings in terms of\nintra-class correlation coefficients (ICCs) shows a high degree of consistency\nin the ratings across raters. We define a metric for the driver's take-over\nreadiness termed the 'Observable Readiness Index (ORI)' based on the ratings.\nFinally, we propose an LSTM model for continuous estimation of the driver's ORI\nbased on a holistic representation of the driver's state, capturing gaze, hand,\npose and foot activity. Our model estimates the ORI with a mean absolute error\nof 0.449 on a 5 point scale. \n\n"}
{"id": "1811.06152", "contents": "Title: Depth Prediction Without the Sensors: Leveraging Structure for\n  Unsupervised Learning from Monocular Videos Abstract: Learning to predict scene depth from RGB inputs is a challenging task both\nfor indoor and outdoor robot navigation. In this work we address unsupervised\nlearning of scene depth and robot ego-motion where supervision is provided by\nmonocular videos, as cameras are the cheapest, least restrictive and most\nubiquitous sensor for robotics.\n  Previous work in unsupervised image-to-depth learning has established strong\nbaselines in the domain. We propose a novel approach which produces higher\nquality results, is able to model moving objects and is shown to transfer\nacross data domains, e.g. from outdoors to indoor scenes. The main idea is to\nintroduce geometric structure in the learning process, by modeling the scene\nand the individual objects; camera ego-motion and object motions are learned\nfrom monocular videos as input. Furthermore an online refinement method is\nintroduced to adapt learning on the fly to unknown domains.\n  The proposed approach outperforms all state-of-the-art approaches, including\nthose that handle motion e.g. through learned flow. Our results are comparable\nin quality to the ones which used stereo as supervision and significantly\nimprove depth prediction on scenes and datasets which contain a lot of object\nmotion. The approach is of practical relevance, as it allows transfer across\nenvironments, by transferring models trained on data collected for robot\nnavigation in urban scenes to indoor navigation settings. The code associated\nwith this paper can be found at https://sites.google.com/view/struct2depth. \n\n"}
{"id": "1811.06186", "contents": "Title: GaitSet: Regarding Gait as a Set for Cross-View Gait Recognition Abstract: As a unique biometric feature that can be recognized at a distance, gait has\nbroad applications in crime prevention, forensic identification and social\nsecurity. To portray a gait, existing gait recognition methods utilize either a\ngait template, where temporal information is hard to preserve, or a gait\nsequence, which must keep unnecessary sequential constraints and thus loses the\nflexibility of gait recognition. In this paper we present a novel perspective,\nwhere a gait is regarded as a set consisting of independent frames. We propose\na new network named GaitSet to learn identity information from the set. Based\non the set perspective, our method is immune to permutation of frames, and can\nnaturally integrate frames from different videos which have been filmed under\ndifferent scenarios, such as diverse viewing angles, different clothes/carrying\nconditions. Experiments show that under normal walking conditions, our\nsingle-model method achieves an average rank-1 accuracy of 95.0% on the CASIA-B\ngait dataset and an 87.1% accuracy on the OU-MVLP gait dataset. These results\nrepresent new state-of-the-art recognition accuracy. On various complex\nscenarios, our model exhibits a significant level of robustness. It achieves\naccuracies of 87.2% and 70.4% on CASIA-B under bag-carrying and coat-wearing\nwalking conditions, respectively. These outperform the existing best methods by\na large margin. The method presented can also achieve a satisfactory accuracy\nwith a small number of frames in a test sample, e.g., 82.5% on CASIA-B with\nonly 7 frames. The source code has been released at\nhttps://github.com/AbnerHqC/GaitSet. \n\n"}
{"id": "1811.06666", "contents": "Title: Ground Plane Polling for 6DoF Pose Estimation of Objects on the Road Abstract: This paper introduces an approach to produce accurate 3D detection boxes for\nobjects on the ground using single monocular images. We do so by merging 2D\nvisual cues, 3D object dimensions, and ground plane constraints to produce\nboxes that are robust against small errors and incorrect predictions. First, we\ntrain a single-shot convolutional neural network (CNN) that produces multiple\nvisual and geometric cues of interest: 2D bounding boxes, 2D keypoints of\ninterest, coarse object orientations and object dimensions. Subsets of these\ncues are then used to poll probable ground planes from a pre-computed database\nof ground planes, to identify the \"best fit\" plane with highest consensus. Once\nidentified, the \"best fit\" plane provides enough constraints to successfully\nconstruct the desired 3D detection box, without directly predicting the 6DoF\npose of the object. The entire ground plane polling (GPP) procedure is\nconstructed as a non-parametrized layer of the CNN that outputs the desired\n\"best fit\" plane and the corresponding 3D keypoints, which together define the\nfinal 3D bounding box. Doing so allows us to poll thousands of different ground\nplane configurations without adding considerable overhead, while also creating\na single CNN that directly produces the desired output without the need for\npost processing. We evaluate our method on the 2D detection and orientation\nestimation benchmark from the challenging KITTI dataset, and provide additional\ncomparisons for 3D metrics of importance. This single-stage, single-pass CNN\nresults in superior localization and orientation estimation compared to more\ncomplex and computationally expensive monocular approaches. \n\n"}
{"id": "1811.07488", "contents": "Title: Quantifying Human Behavior on the Block Design Test Through Automated\n  Multi-Level Analysis of Overhead Video Abstract: The block design test is a standardized, widely used neuropsychological\nassessment of visuospatial reasoning that involves a person recreating a series\nof given designs out of a set of colored blocks. In current testing procedures,\nan expert neuropsychologist observes a person's accuracy and completion time as\nwell as overall impressions of the person's problem-solving procedures, errors,\netc., thus obtaining a holistic though subjective and often qualitative view of\nthe person's cognitive processes. We propose a new framework that combines room\nsensors and AI techniques to augment the information available to\nneuropsychologists from block design and similar tabletop assessments. In\nparticular, a ceiling-mounted camera captures an overhead view of the table\nsurface. From this video, we demonstrate how automated classification using\nmachine learning can produce a frame-level description of the state of the\nblock task and the person's actions over the course of each test problem. We\nalso show how a sequence-comparison algorithm can classify one individual's\nproblem-solving strategy relative to a database of simulated strategies, and\nhow these quantitative results can be visualized for use by neuropsychologists. \n\n"}
{"id": "1811.07630", "contents": "Title: SEIGAN: Towards Compositional Image Generation by Simultaneously\n  Learning to Segment, Enhance, and Inpaint Abstract: We present a novel approach to image manipulation and understanding by\nsimultaneously learning to segment object masks, paste objects to another\nbackground image, and remove them from original images. For this purpose, we\ndevelop a novel generative model for compositional image generation, SEIGAN\n(Segment-Enhance-Inpaint Generative Adversarial Network), which learns these\nthree operations together in an adversarial architecture with additional cycle\nconsistency losses. To train, SEIGAN needs only bounding box supervision and\ndoes not require pairing or ground truth masks. SEIGAN produces better\ngenerated images (evaluated by human assessors) than other approaches and\nproduces high-quality segmentation masks, improving over other adversarially\ntrained approaches and getting closer to the results of fully supervised\ntraining. \n\n"}
{"id": "1811.07672", "contents": "Title: When Conventional machine learning meets neuromorphic engineering: Deep\n  Temporal Networks (DTNets) a machine learning frawmework allowing to operate\n  on Events and Frames and implantable on Tensor Flow Like Hardware Abstract: We introduce in this paper the principle of Deep Temporal Networks that allow\nto add time to convolutional networks by allowing deep integration principles\nnot only using spatial information but also increasingly large temporal window.\nThe concept can be used for conventional image inputs but also event based\ndata. Although inspired by the architecture of brain that inegrates information\nover increasingly larger spatial but also temporal scales it can operate on\nconventional hardware using existing architectures. We introduce preliminary\nresults to show the efficiency of the method. More in-depth results and\nanalysis will be reported soon! \n\n"}
{"id": "1811.07753", "contents": "Title: Contextual Face Recognition with a Nested-Hierarchical Nonparametric\n  Identity Model Abstract: Current face recognition systems typically operate via classification into\nknown identities obtained from supervised identity annotations. There are two\nproblems with this paradigm: (1) current systems are unable to benefit from\noften abundant unlabelled data; and (2) they equate successful recognition with\nlabelling a given input image. Humans, on the other hand, regularly perform\nidentification of individuals completely unsupervised, recognising the identity\nof someone they have seen before even without being able to name that\nindividual. How can we go beyond the current classification paradigm towards a\nmore human understanding of identities? In previous work, we proposed an\nintegrated Bayesian model that coherently reasons about the observed images,\nidentities, partial knowledge about names, and the situational context of each\nobservation. Here, we propose extensions of the contextual component of this\nmodel, enabling unsupervised discovery of an unbounded number of contexts for\nimproved face recognition. \n\n"}
{"id": "1811.08264", "contents": "Title: Transferable Interactiveness Knowledge for Human-Object Interaction\n  Detection Abstract: Human-Object Interaction (HOI) Detection is an important problem to\nunderstand how humans interact with objects. In this paper, we explore\nInteractiveness Knowledge which indicates whether human and object interact\nwith each other or not. We found that interactiveness knowledge can be learned\nacross HOI datasets, regardless of HOI category settings. Our core idea is to\nexploit an Interactiveness Network to learn the general interactiveness\nknowledge from multiple HOI datasets and perform Non-Interaction Suppression\nbefore HOI classification in inference. On account of the generalization of\ninteractiveness, interactiveness network is a transferable knowledge learner\nand can be cooperated with any HOI detection models to achieve desirable\nresults. We extensively evaluate the proposed method on HICO-DET and V-COCO\ndatasets. Our framework outperforms state-of-the-art HOI detection results by a\ngreat margin, verifying its efficacy and flexibility. Code is available at\nhttps://github.com/DirtyHarryLYL/Transferable-Interactiveness-Network. \n\n"}
{"id": "1811.08321", "contents": "Title: Stability Based Filter Pruning for Accelerating Deep CNNs Abstract: Convolutional neural networks (CNN) have achieved impressive performance on\nthe wide variety of tasks (classification, detection, etc.) across multiple\ndomains at the cost of high computational and memory requirements. Thus,\nleveraging CNNs for real-time applications necessitates model compression\napproaches that not only reduce the total number of parameters but reduce the\noverall computation as well. In this work, we present a stability-based\napproach for filter-level pruning of CNNs. We evaluate our proposed approach on\ndifferent architectures (LeNet, VGG-16, ResNet, and Faster RCNN) and datasets\nand demonstrate its generalizability through extensive experiments. Moreover,\nour compressed models can be used at run-time without requiring any special\nlibraries or hardware. Our model compression method reduces the number of FLOPS\nby an impressive factor of 6.03X and GPU memory footprint by more than 17X,\nsignificantly outperforming other state-of-the-art filter pruning methods. \n\n"}
{"id": "1811.08560", "contents": "Title: Adjustable Real-time Style Transfer Abstract: Artistic style transfer is the problem of synthesizing an image with content\nsimilar to a given image and style similar to another. Although recent\nfeed-forward neural networks can generate stylized images in real-time, these\nmodels produce a single stylization given a pair of style/content images, and\nthe user doesn't have control over the synthesized output. Moreover, the style\ntransfer depends on the hyper-parameters of the model with varying \"optimum\"\nfor different input images. Therefore, if the stylized output is not appealing\nto the user, she/he has to try multiple models or retrain one with different\nhyper-parameters to get a favorite stylization. In this paper, we address these\nissues by proposing a novel method which allows adjustment of crucial\nhyper-parameters, after the training and in real-time, through a set of\nmanually adjustable parameters. These parameters enable the user to modify the\nsynthesized outputs from the same pair of style/content images, in search of a\nfavorite stylized image. Our quantitative and qualitative experiments indicate\nhow adjusting these parameters is comparable to retraining the model with\ndifferent hyper-parameters. We also demonstrate how these parameters can be\nrandomized to generate results which are diverse but still very similar in\nstyle and content. \n\n"}
{"id": "1811.08585", "contents": "Title: Progressive Feature Alignment for Unsupervised Domain Adaptation Abstract: Unsupervised domain adaptation (UDA) transfers knowledge from a label-rich\nsource domain to a fully-unlabeled target domain. To tackle this task, recent\napproaches resort to discriminative domain transfer in virtue of pseudo-labels\nto enforce the class-level distribution alignment across the source and target\ndomains. These methods, however, are vulnerable to the error accumulation and\nthus incapable of preserving cross-domain category consistency, as the\npseudo-labeling accuracy is not guaranteed explicitly. In this paper, we\npropose the Progressive Feature Alignment Network (PFAN) to align the\ndiscriminative features across domains progressively and effectively, via\nexploiting the intra-class variation in the target domain. To be specific, we\nfirst develop an Easy-to-Hard Transfer Strategy (EHTS) and an Adaptive\nPrototype Alignment (APA) step to train our model iteratively and\nalternatively. Moreover, upon observing that a good domain adaptation usually\nrequires a non-saturated source classifier, we consider a simple yet efficient\nway to retard the convergence speed of the source classification loss by\nfurther involving a temperature variate into the soft-max function. The\nextensive experimental results reveal that the proposed PFAN exceeds the\nstate-of-the-art performance on three UDA datasets. \n\n"}
{"id": "1811.09908", "contents": "Title: RGB-D Based Action Recognition with Light-weight 3D Convolutional\n  Networks Abstract: Different from RGB videos, depth data in RGB-D videos provide key\ncomplementary information for tristimulus visual data which potentially could\nachieve accuracy improvement for action recognition. However, most of the\nexisting action recognition models solely using RGB videos limit the\nperformance capacity. Additionally, the state-of-the-art action recognition\nmodels, namely 3D convolutional neural networks (3D-CNNs) contain tremendous\nparameters suffering from computational inefficiency. In this paper, we propose\na series of 3D light-weight architectures for action recognition based on RGB-D\ndata. Compared with conventional 3D-CNN models, the proposed light-weight\n3D-CNNs have considerably less parameters involving lower computation cost,\nwhile it results in favorable recognition performance. Experimental results on\ntwo public benchmark datasets show that our models can approximate or\noutperform the state-of-the-art approaches. Specifically, on the RGB+D-NTU\n(NTU) dataset, we achieve 93.2% and 97.6% for cross-subject and cross-view\nmeasurement, and on the Northwestern-UCLA Multiview Action 3D (N-UCLA) dataset,\nwe achieve 95.5% accuracy of cross-view. \n\n"}
{"id": "1811.09986", "contents": "Title: Learning Conditional Random Fields with Augmented Observations for\n  Partially Observed Action Recognition Abstract: This paper aims at recognizing partially observed human actions in videos.\nAction videos acquired in uncontrolled environments often contain corrupt\nframes, which make actions partially observed. Furthermore, these frames can\nlast for arbitrary lengths of time and appear irregularly. They are\ninconsistent with training data and degrade the performance of pre-trained\naction recognition systems. We present an approach to address this issue. For\neach training and testing actions, we divide it into segments and explore the\nmutual dependency between temporal segments. This property states that the\nsimilarity of two actions at one segment often implies their similarity at\nanother. We augment each segment with extra alternatives retrieved from\ntraining data. The augmentation algorithm is designed in a way where a few\nalternatives are good enough to replace the original segment where corrupt\nframes occur. Our approach is developed upon hidden conditional random fields\nand leverages the flexibility of hidden variables for uncertainty handling. It\nturns out that our approach integrates corrupt segment detection and\nalternative selection into the process of prediction, and can recognize\npartially observed actions more accurately. It is evaluated on both fully\nobserved actions and partially observed ones with either synthetic or real\ncorrupt frames. The experimental results manifest its general applicability and\nsuperior performance, especially when corrupt frames are present in the action\nvideos. \n\n"}
{"id": "1811.10352", "contents": "Title: EFANet: Exchangeable Feature Alignment Network for Arbitrary Style\n  Transfer Abstract: Style transfer has been an important topic both in computer vision and\ngraphics. Since the seminal work of Gatys et al. first demonstrates the power\nof stylization through optimization in the deep feature space, quite a few\napproaches have achieved real-time arbitrary style transfer with\nstraightforward statistic matching techniques. In this work, our key\nobservation is that only considering features in the input style image for the\nglobal deep feature statistic matching or local patch swap may not always\nensure a satisfactory style transfer; see e.g., Figure 1. Instead, we propose a\nnovel transfer framework, EFANet, that aims to jointly analyze and better align\nexchangeable features extracted from content and style image pair. In this way,\nthe style features from the style image seek for the best compatibility with\nthe content information in the content image, leading to more structured\nstylization results. In addition, a new whitening loss is developed for\npurifying the computed content features and better fusion with styles in\nfeature space. Qualitative and quantitative experiments demonstrate the\nadvantages of our approach. \n\n"}
{"id": "1811.10437", "contents": "Title: A Novel Learning-based Global Path Planning Algorithm for Planetary\n  Rovers Abstract: Autonomous path planning algorithms are significant to planetary exploration\nrovers, since relying on commands from Earth will heavily reduce their\nefficiency of executing exploration missions. This paper proposes a novel\nlearning-based algorithm to deal with global path planning problem for\nplanetary exploration rovers. Specifically, a novel deep convolutional neural\nnetwork with double branches (DB-CNN) is designed and trained, which can plan\npath directly from orbital images of planetary surfaces without implementing\nenvironment mapping. Moreover, the planning procedure requires no prior\nknowledge about planetary surface terrains. Finally, experimental results\ndemonstrate that DB-CNN achieves better performance on global path planning and\nfaster convergence during training compared with the existing Value Iteration\nNetwork (VIN). \n\n"}
{"id": "1811.10907", "contents": "Title: Efficient Image Retrieval via Decoupling Diffusion into Online and\n  Offline Processing Abstract: Diffusion is commonly used as a ranking or re-ranking method in retrieval\ntasks to achieve higher retrieval performance, and has attracted lots of\nattention in recent years. A downside to diffusion is that it performs slowly\nin comparison to the naive k-NN search, which causes a non-trivial online\ncomputational cost on large datasets. To overcome this weakness, we propose a\nnovel diffusion technique in this paper. In our work, instead of applying\ndiffusion to the query, we pre-compute the diffusion results of each element in\nthe database, making the online search a simple linear combination on top of\nthe k-NN search process. Our proposed method becomes 10~ times faster in terms\nof online search speed. Moreover, we propose to use late truncation instead of\nearly truncation in previous works to achieve better retrieval performance. \n\n"}
{"id": "1811.10946", "contents": "Title: Deep Learned Frame Prediction for Video Compression Abstract: Motion compensation is one of the most essential methods for any video\ncompression algorithm. Video frame prediction is a task analogous to motion\ncompensation. In recent years, the task of frame prediction is undertaken by\ndeep neural networks (DNNs). In this thesis we create a DNN to perform learned\nframe prediction and additionally implement a codec that contains our DNN. We\ntrain our network using two methods for two different goals. Firstly we train\nour network based on mean square error (MSE) only, aiming to obtain highest\nPSNR values at frame prediction and video compression. Secondly we use\nadversarial training to produce visually more realistic frame predictions. For\nframe prediction, we compare our method with the baseline methods of frame\ndifference and 16x16 block motion compensation. For video compression we\nfurther include x264 video codec in the comparison. We show that in frame\nprediction, adversarial training produces frames that look sharper and more\nrealistic, compared MSE based training, but in video compression it\nconsistently performs worse. This proves that even though adversarial training\nis useful for generating video frames that are more pleasing to the human eye,\nthey should not be employed for video compression. Moreover, our network\ntrained with MSE produces accurate frame predictions, and in quantitative\nresults, for both tasks, it produces comparable results in all videos and\noutperforms other methods on average. More specifically, learned frame\nprediction outperforms other methods in terms of rate-distortion performance in\ncase of high motion video, while the rate-distortion performance of our method\nis competitive with that of x264 in low motion video. \n\n"}
{"id": "1811.11168", "contents": "Title: Deformable ConvNets v2: More Deformable, Better Results Abstract: The superior performance of Deformable Convolutional Networks arises from its\nability to adapt to the geometric variations of objects. Through an examination\nof its adaptive behavior, we observe that while the spatial support for its\nneural features conforms more closely than regular ConvNets to object\nstructure, this support may nevertheless extend well beyond the region of\ninterest, causing features to be influenced by irrelevant image content. To\naddress this problem, we present a reformulation of Deformable ConvNets that\nimproves its ability to focus on pertinent image regions, through increased\nmodeling power and stronger training. The modeling power is enhanced through a\nmore comprehensive integration of deformable convolution within the network,\nand by introducing a modulation mechanism that expands the scope of deformation\nmodeling. To effectively harness this enriched modeling capability, we guide\nnetwork training via a proposed feature mimicking scheme that helps the network\nto learn features that reflect the object focus and classification power of\nR-CNN features. With the proposed contributions, this new version of Deformable\nConvNets yields significant performance gains over the original model and\nproduces leading results on the COCO benchmark for object detection and\ninstance segmentation. \n\n"}
{"id": "1811.11212", "contents": "Title: Self-Supervised GANs via Auxiliary Rotation Loss Abstract: Conditional GANs are at the forefront of natural image synthesis. The main\ndrawback of such models is the necessity for labeled data. In this work we\nexploit two popular unsupervised learning techniques, adversarial training and\nself-supervision, and take a step towards bridging the gap between conditional\nand unconditional GANs. In particular, we allow the networks to collaborate on\nthe task of representation learning, while being adversarial with respect to\nthe classic GAN game. The role of self-supervision is to encourage the\ndiscriminator to learn meaningful feature representations which are not\nforgotten during training. We test empirically both the quality of the learned\nimage representations, and the quality of the synthesized images. Under the\nsame conditions, the self-supervised GAN attains a similar performance to\nstate-of-the-art conditional counterparts. Finally, we show that this approach\nto fully unsupervised learning can be scaled to attain an FID of 23.4 on\nunconditional ImageNet generation. \n\n"}
{"id": "1811.11269", "contents": "Title: Generalizing semi-supervised generative adversarial networks to\n  regression using feature contrasting Abstract: In this work, we generalize semi-supervised generative adversarial networks\n(GANs) from classification problems to regression problems. In the last few\nyears, the importance of improving the training of neural networks using\nsemi-supervised training has been demonstrated for classification problems. We\npresent a novel loss function, called feature contrasting, resulting in a\ndiscriminator which can distinguish between fake and real data based on feature\nstatistics. This method avoids potential biases and limitations of alternative\napproaches. The generalization of semi-supervised GANs to the regime of\nregression problems of opens their use to countless applications as well as\nproviding an avenue for a deeper understanding of how GANs function. We first\ndemonstrate the capabilities of semi-supervised regression GANs on a toy\ndataset which allows for a detailed understanding of how they operate in\nvarious circumstances. This toy dataset is used to provide a theoretical basis\nof the semi-supervised regression GAN. We then apply the semi-supervised\nregression GANs to a number of real-world computer vision applications: age\nestimation, driving steering angle prediction, and crowd counting from single\nimages. We perform extensive tests of what accuracy can be achieved with\nsignificantly reduced annotated data. Through the combination of the\ntheoretical example and real-world scenarios, we demonstrate how\nsemi-supervised GANs can be generalized to regression problems. \n\n"}
{"id": "1811.11304", "contents": "Title: Universal Adversarial Training Abstract: Standard adversarial attacks change the predicted class label of a selected\nimage by adding specially tailored small perturbations to its pixels. In\ncontrast, a universal perturbation is an update that can be added to any image\nin a broad class of images, while still changing the predicted class label. We\nstudy the efficient generation of universal adversarial perturbations, and also\nefficient methods for hardening networks to these attacks. We propose a simple\noptimization-based universal attack that reduces the top-1 accuracy of various\nnetwork architectures on ImageNet to less than 20%, while learning the\nuniversal perturbation 13X faster than the standard method.\n  To defend against these perturbations, we propose universal adversarial\ntraining, which models the problem of robust classifier generation as a\ntwo-player min-max game, and produces robust models with only 2X the cost of\nnatural training. We also propose a simultaneous stochastic gradient method\nthat is almost free of extra computation, which allows us to do universal\nadversarial training on ImageNet. \n\n"}
{"id": "1811.11455", "contents": "Title: CrowdCam: Dynamic Region Segmentation Abstract: We consider the problem of segmenting dynamic regions in CrowdCam images,\nwhere a dynamic region is the projection of a moving 3D object on the image\nplane. Quite often, these regions are the most interesting parts of an image.\nCrowdCam images is a set of images of the same dynamic event, captured by a\ngroup of non-collaborating users. Almost every event of interest today is\ncaptured this way. This new type of images raises the need to develop new\nalgorithms tailored specifically for it. We propose a comprehensive solution to\nthe problem. Our solution combines cues that are based on geometry, appearance\nand proximity. First, geometric reasoning is used to produce rough score maps\nthat determine, for every pixel, how likely it is to be the projection of a\nstatic or dynamic scene point. These maps are noisy because CrowdCam images are\nusually few and far apart both in space and in time. Then, we use similarity in\nappearance space and proximity in the image plane to encourage neighboring\npixels to be labeled similarly as either static or dynamic. We collected a new,\nand challenging, data set to evaluate our algorithm. Results show that the\nsuccess score of our algorithm is nearly double that of the current state of\nthe art approach. \n\n"}
{"id": "1811.11985", "contents": "Title: Weakly Supervised Silhouette-based Semantic Scene Change Detection Abstract: This paper presents a novel semantic scene change detection scheme with only\nweak supervision. A straightforward approach for this task is to train a\nsemantic change detection network directly from a large-scale dataset in an\nend-to-end manner. However, a specific dataset for this task, which is usually\nlabor-intensive and time-consuming, becomes indispensable. To avoid this\nproblem, we propose to train this kind of network from existing datasets by\ndividing this task into change detection and semantic extraction. On the other\nhand, the difference in camera viewpoints, for example, images of the same\nscene captured from a vehicle-mounted camera at different time points, usually\nbrings a challenge to the change detection task. To address this challenge, we\npropose a new siamese network structure with the introduction of correlation\nlayer. In addition, we collect and annotate a publicly available dataset for\nsemantic change detection to evaluate the proposed method. The experimental\nresults verified both the robustness to viewpoint difference in change\ndetection task and the effectiveness for semantic change detection of the\nproposed networks. Our code and dataset are available at\nhttps://kensakurada.github.io/pscd. \n\n"}
{"id": "1811.12373", "contents": "Title: Diverse Image Synthesis from Semantic Layouts via Conditional IMLE Abstract: Most existing methods for conditional image synthesis are only able to\ngenerate a single plausible image for any given input, or at best a fixed\nnumber of plausible images. In this paper, we focus on the problem of\ngenerating images from semantic segmentation maps and present a simple new\nmethod that can generate an arbitrary number of images with diverse appearance\nfor the same semantic layout. Unlike most existing approaches which adopt the\nGAN framework, our method is based on the recently introduced Implicit Maximum\nLikelihood Estimation (IMLE) framework. Compared to the leading approach, our\nmethod is able to generate more diverse images while producing fewer artifacts\ndespite using the same architecture. The learned latent space also has sensible\nstructure despite the lack of supervision that encourages such behaviour.\nVideos and code are available at\nhttps://people.eecs.berkeley.edu/~ke.li/projects/imle/scene_layouts/. \n\n"}
{"id": "1811.12813", "contents": "Title: Real Time Bangladeshi Sign Language Detection using Faster R-CNN Abstract: Bangladeshi Sign Language (BdSL) is a commonly used medium of communication\nfor the hearing-impaired people in Bangladesh. Developing a real time system to\ndetect these signs from images is a great challenge. In this paper, we present\na technique to detect BdSL from images that performs in real time. Our method\nuses Convolutional Neural Network based object detection technique to detect\nthe presence of signs in the image region and to recognize its class. For this\npurpose, we adopted Faster Region-based Convolutional Network approach and\ndeveloped a dataset $-$ BdSLImset $-$ to train our system. Previous research\nworks in detecting BdSL generally depend on external devices while most of the\nother vision-based techniques do not perform efficiently in real time. Our\napproach, however, is free from such limitations and the experimental results\ndemonstrate that the proposed method successfully identifies and recognizes\nBangladeshi signs in real time. \n\n"}
{"id": "1812.00033", "contents": "Title: Learning from a tiny dataset of manual annotations: a teacher/student\n  approach for surgical phase recognition Abstract: Vision algorithms capable of interpreting scenes from a real-time video\nstream are necessary for computer-assisted surgery systems to achieve\ncontext-aware behavior. In laparoscopic procedures one particular algorithm\nneeded for such systems is the identification of surgical phases, for which the\ncurrent state of the art is a model based on a CNN-LSTM. A number of previous\nworks using models of this kind have trained them in a fully supervised manner,\nrequiring a fully annotated dataset. Instead, our work confronts the problem of\nlearning surgical phase recognition in scenarios presenting scarce amounts of\nannotated data (under 25% of all available video recordings). We propose a\nteacher/student type of approach, where a strong predictor called the teacher,\ntrained beforehand on a small dataset of ground truth-annotated videos,\ngenerates synthetic annotations for a larger dataset, which another model - the\nstudent - learns from. In our case, the teacher features a novel CNN-biLSTM-CRF\narchitecture, designed for offline inference only. The student, on the other\nhand, is a CNN-LSTM capable of making real-time predictions. Results for\nvarious amounts of manually annotated videos demonstrate the superiority of the\nnew CNN-biLSTM-CRF predictor as well as improved performance from the CNN-LSTM\ntrained using synthetic labels generated for unannotated videos. For both\noffline and online surgical phase recognition with very few annotated\nrecordings available, this new teacher/student strategy provides a valuable\nperformance improvement by efficiently leveraging the unannotated data. \n\n"}
{"id": "1812.00572", "contents": "Title: Practical Window Setting Optimization for Medical Image Deep Learning Abstract: The recent advancements in deep learning have allowed for numerous\napplications in computed tomography (CT), with potential to improve diagnostic\naccuracy, speed of interpretation, and clinical efficiency. However, the deep\nlearning community has to date neglected window display settings - a key\nfeature of clinical CT interpretation and opportunity for additional\noptimization. Here we propose a window setting optimization (WSO) module that\nis fully trainable with convolutional neural networks (CNNs) to find optimal\nwindow settings for clinical performance. Our approach was inspired by the\nmethod commonly used by practicing radiologists to interpret CT images by\nadjusting window settings to increase the visualization of certain pathologies.\nOur approach provides optimal window ranges to enhance the conspicuity of\nabnormalities, and was used to enable performance enhancement for intracranial\nhemorrhage and urinary stone detection. On each task, the WSO model\noutperformed models trained over the full range of Hounsfield unit values in CT\nimages, as well as images windowed with pre-defined settings. The WSO module\ncan be readily applied to any analysis of CT images, and can be further\ngeneralized to tasks on other medical imaging modalities. \n\n"}
{"id": "1812.00810", "contents": "Title: A Wasserstein GAN model with the total variational regularization Abstract: It is well known that the generative adversarial nets (GANs) are remarkably\ndifficult to train. The recently proposed Wasserstein GAN (WGAN) creates\nprincipled research directions towards addressing these issues. But we found in\npractice that gradient penalty WGANs (GP-WGANs) still suffer from training\ninstability. In this paper, we combine a Total Variational (TV) regularizing\nterm into the WGAN formulation instead of weight clipping or gradient penalty,\nwhich implies that the Lipschitz constraint is enforced on the critic network.\nOur proposed method is more stable at training than GP-WGANs and works well\nacross varied GAN architectures. We also present a method to control the\ntrade-off between image diversity and visual quality. It does not bring any\ncomputation burden. \n\n"}
{"id": "1812.00828", "contents": "Title: Novel Quality Metric for Duration Variability Compensation in Speaker\n  Verification using i-Vectors Abstract: Automatic speaker verification (ASV) is the process to recognize persons\nusing voice as biometric. The ASV systems show considerable recognition\nperformance with sufficient amount of speech from matched condition. One of the\ncrucial challenges of ASV technology is to improve recognition performance with\nspeech segments of short duration. In short duration condition, the model\nparameters are not properly estimated due to inadequate speech information, and\nthis results poor recognition accuracy even with the state-of-the-art i-vector\nbased ASV system. We hypothesize that considering the estimation quality during\nrecognition process would help to improve the ASV performance. This can be\nincorporated as a quality measure during fusion of ASV systems. This paper\ninvestigates a new quality measure for i-vector representation of speech\nutterances computed directly from Baum-Welch statistics. The proposed metric is\nsubsequently used as quality measure during fusion of ASV systems. In\nexperiments with the NIST SRE 2008 corpus, We have shown that inclusion of\nproposed quality metric exhibits considerable improvement in speaker\nverification performance. The results also indicate the potentiality of the\nproposed method in real-world scenario with short test utterances. \n\n"}
{"id": "1812.01192", "contents": "Title: Learning to Fuse Things and Stuff Abstract: We propose an end-to-end learning approach for panoptic segmentation, a novel\ntask unifying instance (things) and semantic (stuff) segmentation. Our model,\nTASCNet, uses feature maps from a shared backbone network to predict in a\nsingle feed-forward pass both things and stuff segmentations. We explicitly\nconstrain these two output distributions through a global things and stuff\nbinary mask to enforce cross-task consistency. Our proposed unified network is\ncompetitive with the state of the art on several benchmarks for panoptic\nsegmentation as well as on the individual semantic and instance segmentation\ntasks. \n\n"}
{"id": "1812.01687", "contents": "Title: PointCloud Saliency Maps Abstract: 3D point-cloud recognition with PointNet and its variants has received\nremarkable progress. A missing ingredient, however, is the ability to\nautomatically evaluate point-wise importance w.r.t.\\! classification\nperformance, which is usually reflected by a saliency map. A saliency map is an\nimportant tool as it allows one to perform further processes on point-cloud\ndata. In this paper, we propose a novel way of characterizing critical points\nand segments to build point-cloud saliency maps. Our method assigns each point\na score reflecting its contribution to the model-recognition loss. The saliency\nmap explicitly explains which points are the key for model recognition.\nFurthermore, aggregations of highly-scored points indicate important\nsegments/subsets in a point-cloud. Our motivation for constructing a saliency\nmap is by point dropping, which is a non-differentiable operator. To overcome\nthis issue, we approximate point-dropping with a differentiable procedure of\nshifting points towards the cloud centroid. Consequently, each saliency score\ncan be efficiently measured by the corresponding gradient of the loss w.r.t the\npoint under the spherical coordinates. Extensive evaluations on several\nstate-of-the-art point-cloud recognition models, including PointNet, PointNet++\nand DGCNN, demonstrate the veracity and generality of our proposed saliency\nmap. Code for experiments is released on\n\\url{https://github.com/tianzheng4/PointCloud-Saliency-Maps}. \n\n"}
{"id": "1812.02041", "contents": "Title: Learn to See by Events: Color Frame Synthesis from Event and RGB Cameras Abstract: Event cameras are biologically-inspired sensors that gather the temporal\nevolution of the scene. They capture pixel-wise brightness variations and\noutput a corresponding stream of asynchronous events. Despite having multiple\nadvantages with respect to traditional cameras, their use is partially\nprevented by the limited applicability of traditional data processing and\nvision algorithms. To this aim, we present a framework which exploits the\noutput stream of event cameras to synthesize RGB frames, relying on an initial\nor a periodic set of color key-frames and the sequence of intermediate events.\nDifferently from existing work, we propose a deep learning-based frame\nsynthesis method, consisting of an adversarial architecture combined with a\nrecurrent module. Qualitative results and quantitative per-pixel, perceptual,\nand semantic evaluation on four public datasets confirm the quality of the\nsynthesized images. \n\n"}
{"id": "1812.02068", "contents": "Title: Brain Segmentation from k-space with End-to-end Recurrent Attention\n  Network Abstract: The task of medical image segmentation commonly involves an image\nreconstruction step to convert acquired raw data to images before any analysis.\nHowever, noises, artifacts and loss of information due to the reconstruction\nprocess are almost inevitable, which compromises the final performance of\nsegmentation. We present a novel learning framework that performs magnetic\nresonance brain image segmentation directly from k-space data. The end-to-end\nframework consists of a unique task-driven attention module that recurrently\nutilizes intermediate segmentation estimation to facilitate image-domain\nfeature extraction from the raw data, thus closely bridging the reconstruction\nand the segmentation tasks. In addition, to address the challenge of manual\nlabeling, we introduce a novel workflow to generate labeled training data for\nsegmentation by exploiting imaging modality simulators and digital phantoms.\nExtensive experimental results show that the proposed method outperforms\nseveral state-of-the-art methods. \n\n"}
{"id": "1812.02415", "contents": "Title: Self-supervised Learning of Dense Shape Correspondence Abstract: We introduce the first completely unsupervised correspondence learning\napproach for deformable 3D shapes. Key to our model is the understanding that\nnatural deformations (such as changes in pose) approximately preserve the\nmetric structure of the surface, yielding a natural criterion to drive the\nlearning process toward distortion-minimizing predictions. On this basis, we\novercome the need for annotated data and replace it by a purely geometric\ncriterion. The resulting learning model is class-agnostic, and is able to\nleverage any type of deformable geometric data for the training phase. In\ncontrast to existing supervised approaches which specialize on the class seen\nat training time, we demonstrate stronger generalization as well as\napplicability to a variety of challenging settings. We showcase our method on a\nwide selection of correspondence benchmarks, where we outperform other methods\nin terms of accuracy, generalization, and efficiency. \n\n"}
{"id": "1812.02464", "contents": "Title: Pseudo-Rehearsal: Achieving Deep Reinforcement Learning without\n  Catastrophic Forgetting Abstract: Neural networks can achieve excellent results in a wide variety of\napplications. However, when they attempt to sequentially learn, they tend to\nlearn the new task while catastrophically forgetting previous ones. We propose\na model that overcomes catastrophic forgetting in sequential reinforcement\nlearning by combining ideas from continual learning in both the image\nclassification domain and the reinforcement learning domain. This model\nfeatures a dual memory system which separates continual learning from\nreinforcement learning and a pseudo-rehearsal system that \"recalls\" items\nrepresentative of previous tasks via a deep generative network. Our model\nsequentially learns Atari 2600 games without demonstrating catastrophic\nforgetting and continues to perform above human level on all three games. This\nresult is achieved without: demanding additional storage requirements as the\nnumber of tasks increases, storing raw data or revisiting past tasks. In\ncomparison, previous state-of-the-art solutions are substantially more\nvulnerable to forgetting on these complex deep reinforcement learning tasks. \n\n"}
{"id": "1812.02611", "contents": "Title: OMNIA Faster R-CNN: Detection in the wild through dataset merging and\n  soft distillation Abstract: Object detectors tend to perform poorly in new or open domains, and require\nexhaustive yet costly annotations from fully labeled datasets. We aim at\nbenefiting from several datasets with different categories but without\nadditional labelling, not only to increase the number of categories detected,\nbut also to take advantage from transfer learning and to enhance domain\nindependence.\n  Our dataset merging procedure starts with training several initial Faster\nR-CNN on the different datasets while considering the complementary datasets'\nimages for domain adaptation. Similarly to self-training methods, the\npredictions of these initial detectors mitigate the missing annotations on the\ncomplementary datasets. The final OMNIA Faster R-CNN is trained with all\ncategories on the union of the datasets enriched by predictions. The joint\ntraining handles unsafe targets with a new classification loss called SoftSig\nin a softly supervised way.\n  Experimental results show that in the case of fashion detection for images in\nthe wild, merging Modanet with COCO increases the final performance from 45.5%\nto 57.4% in mAP. Applying our soft distillation to the task of detection with\ndomain shift between GTA and Cityscapes enables to beat the state-of-the-art by\n5.3 points. Our methodology could unlock object detection for real-world\napplications without immense datasets. \n\n"}
{"id": "1812.02766", "contents": "Title: Knockoff Nets: Stealing Functionality of Black-Box Models Abstract: Machine Learning (ML) models are increasingly deployed in the wild to perform\na wide range of tasks. In this work, we ask to what extent can an adversary\nsteal functionality of such \"victim\" models based solely on blackbox\ninteractions: image in, predictions out. In contrast to prior work, we present\nan adversary lacking knowledge of train/test data used by the model, its\ninternals, and semantics over model outputs. We formulate model functionality\nstealing as a two-step approach: (i) querying a set of input images to the\nblackbox model to obtain predictions; and (ii) training a \"knockoff\" with\nqueried image-prediction pairs. We make multiple remarkable observations: (a)\nquerying random images from a different distribution than that of the blackbox\ntraining data results in a well-performing knockoff; (b) this is possible even\nwhen the knockoff is represented using a different architecture; and (c) our\nreinforcement learning approach additionally improves query sample efficiency\nin certain settings and provides performance gains. We validate model\nfunctionality stealing on a range of datasets and tasks, as well as on a\npopular image analysis API where we create a reasonable knockoff for as little\nas $30. \n\n"}
{"id": "1812.02781", "contents": "Title: ROI-10D: Monocular Lifting of 2D Detection to 6D Pose and Metric Shape Abstract: We present a deep learning method for end-to-end monocular 3D object\ndetection and metric shape retrieval. We propose a novel loss formulation by\nlifting 2D detection, orientation, and scale estimation into 3D space. Instead\nof optimizing these quantities separately, the 3D instantiation allows to\nproperly measure the metric misalignment of boxes. We experimentally show that\nour 10D lifting of sparse 2D Regions of Interests (RoIs) achieves great results\nboth for 6D pose and recovery of the textured metric geometry of instances.\nThis further enables 3D synthetic data augmentation via inpainting recovered\nmeshes directly onto the 2D scenes. We evaluate on KITTI3D against other strong\nmonocular methods and demonstrate that our approach doubles the AP on the 3D\npose metrics on the official test set, defining the new state of the art. \n\n"}
{"id": "1812.02863", "contents": "Title: Privacy Partitioning: Protecting User Data During the Deep Learning\n  Inference Phase Abstract: We present a practical method for protecting data during the inference phase\nof deep learning based on bipartite topology threat modeling and an interactive\nadversarial deep network construction. We term this approach \\emph{Privacy\nPartitioning}. In the proposed framework, we split the machine learning models\nand deploy a few layers into users' local devices, and the rest of the layers\ninto a remote server. We propose an approach to protect user's data during the\ninference phase, while still achieve good classification accuracy.\n  We conduct an experimental evaluation of this approach on benchmark datasets\nof three computer vision tasks. The experimental results indicate that this\napproach can be used to significantly attenuate the capacity for an adversary\nwith access to the state-of-the-art deep network's intermediate states to learn\nprivacy-sensitive inputs to the network. For example, we demonstrate that our\napproach can prevent attackers from inferring the private attributes such as\ngender from the Face image dataset without sacrificing the classification\naccuracy of the original machine learning task such as Face Identification. \n\n"}
{"id": "1812.02897", "contents": "Title: Improved Search Strategies with Application to Estimating Facial\n  Blendshape Parameters Abstract: It is well known that popular optimization techniques can lead to overfitting\nor even a lack of convergence altogether; thus, practitioners often utilize ad\nhoc regularization terms added to the energy functional. When carefully\ncrafted, these regularizations can produce compelling results. However,\nregularization changes both the energy landscape and the solution to the\noptimization problem, which can result in underfitting. Surprisingly, many\npractitioners both add regularization and claim that their model lacks the\nexpressivity to fit the data. Motivated by a geometric interpretation of the\nlinearized search space, we propose an approach that ameliorates overfitting\nwithout the need for regularization terms that restrict the expressiveness of\nthe underlying model. We illustrate the efficacy of our approach on\nminimization problems related to three-dimensional facial expression estimation\nwhere overfitting clouds semantic understanding and regularization may lead to\nunderfitting that misses or misinterprets subtle expressions. \n\n"}
{"id": "1812.03050", "contents": "Title: Graph Cut Segmentation Methods Revisited with a Quantum Algorithm Abstract: The design and performance of computer vision algorithms are greatly\ninfluenced by the hardware on which they are implemented. CPUs, multi-core\nCPUs, FPGAs and GPUs have inspired new algorithms and enabled existing ideas to\nbe realized. This is notably the case with GPUs, which has significantly\nchanged the landscape of computer vision research through deep learning. As the\nend of Moores law approaches, researchers and hardware manufacturers are\nexploring alternative hardware computing paradigms. Quantum computers are a\nvery promising alternative and offer polynomial or even exponential speed-ups\nover conventional computing for some problems. This paper presents a novel\napproach to image segmentation that uses new quantum computing hardware.\nSegmentation is formulated as a graph cut problem that can be mapped to the\nquantum approximate optimization algorithm (QAOA). This algorithm can be\nimplemented on current and near-term quantum computers. Encouraging results are\npresented on artificial and medical imaging data. This represents an important,\npractical step towards leveraging quantum computers for computer vision. \n\n"}
{"id": "1812.03368", "contents": "Title: Unsupervised Learning of Monocular Depth Estimation with Bundle\n  Adjustment, Super-Resolution and Clip Loss Abstract: We present a novel unsupervised learning framework for single view depth\nestimation using monocular videos. It is well known in 3D vision that enlarging\nthe baseline can increase the depth estimation accuracy, and jointly optimizing\na set of camera poses and landmarks is essential. In previous monocular\nunsupervised learning frameworks, only part of the photometric and geometric\nconstraints within a sequence are used as supervisory signals. This may result\nin a short baseline and overfitting. Besides, previous works generally estimate\na low resolution depth from a low resolution impute image. The low resolution\ndepth is then interpolated to recover the original resolution. This strategy\nmay generate large errors on object boundaries, as the depth of background and\nforeground are mixed to yield the high resolution depth. In this paper, we\nintroduce a bundle adjustment framework and a super-resolution network to solve\nthe above two problems. In bundle adjustment, depths and poses of an image\nsequence are jointly optimized, which increases the baseline by establishing\nthe relationship between farther frames. The super resolution network learns to\nestimate a high resolution depth from a low resolution image. Additionally, we\nintroduce the clip loss to deal with moving objects and occlusion. Experimental\nresults on the KITTI dataset show that the proposed algorithm outperforms the\nstate-of-the-art unsupervised methods using monocular sequences, and achieves\ncomparable or even better result compared to unsupervised methods using stereo\nsequences. \n\n"}
{"id": "1812.03451", "contents": "Title: A Comparison of Embedded Deep Learning Methods for Person Detection Abstract: Recent advancements in parallel computing, GPU technology and deep learning\nprovide a new platform for complex image processing tasks such as person\ndetection to flourish. Person detection is fundamental preliminary operation\nfor several high level computer vision tasks. One industry that can\nsignificantly benefit from person detection is retail. In recent years, various\nstudies attempt to find an optimal solution for person detection using neural\nnetworks and deep learning. This study conducts a comparison among the state of\nthe art deep learning base object detector with the focus on person detection\nperformance in indoor environments. Performance of various implementations of\nYOLO, SSD, RCNN, R-FCN and SqueezeDet have been assessed using our in-house\nproprietary dataset which consists of over 10 thousands indoor images captured\nform shopping malls, retails and stores. Experimental results indicate that,\nTiny YOLO-416 and SSD (VGG-300) are the fastest and Faster-RCNN (Inception\nResNet-v2) and R-FCN (ResNet-101) are the most accurate detectors investigated\nin this study. Further analysis shows that YOLO v3-416 delivers relatively\naccurate result in a reasonable amount of time, which makes it an ideal model\nfor person detection in embedded platforms. \n\n"}
{"id": "1812.03527", "contents": "Title: A Deep Multi-task Learning Approach to Skin Lesion Classification Abstract: Skin lesion identification is a key step toward dermatological diagnosis.\nWhen describing a skin lesion, it is very important to note its body site\ndistribution as many skin diseases commonly affect particular parts of the\nbody. To exploit the correlation between skin lesions and their body site\ndistributions, in this study, we investigate the possibility of improving skin\nlesion classification using the additional context information provided by body\nlocation. Specifically, we build a deep multi-task learning (MTL) framework to\njointly optimize skin lesion classification and body location classification\n(the latter is used as an inductive bias). Our MTL framework uses the\nstate-of-the-art ImageNet pretrained model with specialized loss functions for\nthe two related tasks. Our experiments show that the proposed MTL based method\nperforms more robustly than its standalone (single-task) counterpart. \n\n"}
{"id": "1812.04451", "contents": "Title: Coconditional Autoencoding Adversarial Networks for Chinese Font Feature\n  Learning Abstract: In this work, we propose a novel framework named Coconditional Autoencoding\nAdversarial Networks (CocoAAN) for Chinese font learning, which jointly learns\na generation network and two encoding networks of different feature domains\nusing an adversarial process. The encoding networks map the glyph images into\nstyle and content features respectively via the pairwise substitution\noptimization strategy, and the generation network maps these two kinds of\nfeatures to glyph samples. Together with a discriminative network conditioned\non the extracted features, our framework succeeds in producing\nrealistic-looking Chinese glyph images flexibly. Unlike previous models relying\non the complex segmentation of Chinese components or strokes, our model can\n\"parse\" structures in an unsupervised way, through which the content feature\nrepresentation of each character is captured. Experiments demonstrate our\nframework has a powerful generalization capacity to other unseen fonts and\ncharacters. \n\n"}
{"id": "1812.05050", "contents": "Title: Fast Online Object Tracking and Segmentation: A Unifying Approach Abstract: In this paper we illustrate how to perform both visual object tracking and\nsemi-supervised video object segmentation, in real-time, with a single simple\napproach. Our method, dubbed SiamMask, improves the offline training procedure\nof popular fully-convolutional Siamese approaches for object tracking by\naugmenting their loss with a binary segmentation task. Once trained, SiamMask\nsolely relies on a single bounding box initialisation and operates online,\nproducing class-agnostic object segmentation masks and rotated bounding boxes\nat 55 frames per second. Despite its simplicity, versatility and fast speed,\nour strategy allows us to establish a new state of the art among real-time\ntrackers on VOT-2018, while at the same time demonstrating competitive\nperformance and the best speed for the semi-supervised video object\nsegmentation task on DAVIS-2016 and DAVIS-2017. The project website is\nhttp://www.robots.ox.ac.uk/~qwang/SiamMask. \n\n"}
{"id": "1812.05252", "contents": "Title: Dynamic Fusion with Intra- and Inter- Modality Attention Flow for Visual\n  Question Answering Abstract: Learning effective fusion of multi-modality features is at the heart of\nvisual question answering. We propose a novel method of dynamically fusing\nmulti-modal features with intra- and inter-modality information flow, which\nalternatively pass dynamic information between and across the visual and\nlanguage modalities. It can robustly capture the high-level interactions\nbetween language and vision domains, thus significantly improves the\nperformance of visual question answering. We also show that the proposed\ndynamic intra-modality attention flow conditioned on the other modality can\ndynamically modulate the intra-modality attention of the target modality, which\nis vital for multimodality feature fusion. Experimental evaluations on the VQA\n2.0 dataset show that the proposed method achieves state-of-the-art VQA\nperformance. Extensive ablation studies are carried out for the comprehensive\nanalysis of the proposed method. \n\n"}
{"id": "1812.06417", "contents": "Title: Visual Dialogue without Vision or Dialogue Abstract: We characterise some of the quirks and shortcomings in the exploration of\nVisual Dialogue - a sequential question-answering task where the questions and\ncorresponding answers are related through given visual stimuli. To do so, we\ndevelop an embarrassingly simple method based on Canonical Correlation Analysis\n(CCA) that, on the standard dataset, achieves near state-of-the-art performance\non mean rank (MR). In direct contrast to current complex and over-parametrised\narchitectures that are both compute and time intensive, our method ignores the\nvisual stimuli, ignores the sequencing of dialogue, does not need gradients,\nuses off-the-shelf feature extractors, has at least an order of magnitude fewer\nparameters, and learns in practically no time. We argue that these results are\nindicative of issues in current approaches to Visual Dialogue and conduct\nanalyses to highlight implicit dataset biases and effects of over-constrained\nevaluation metrics. Our code is publicly available. \n\n"}
{"id": "1812.06589", "contents": "Title: Arbitrary Talking Face Generation via Attentional Audio-Visual Coherence\n  Learning Abstract: Talking face generation aims to synthesize a face video with precise lip\nsynchronization as well as a smooth transition of facial motion over the entire\nvideo via the given speech clip and facial image. Most existing methods mainly\nfocus on either disentangling the information in a single image or learning\ntemporal information between frames. However, cross-modality coherence between\naudio and video information has not been well addressed during synthesis. In\nthis paper, we propose a novel arbitrary talking face generation framework by\ndiscovering the audio-visual coherence via the proposed Asymmetric Mutual\nInformation Estimator (AMIE). In addition, we propose a Dynamic Attention (DA)\nblock by selectively focusing the lip area of the input image during the\ntraining stage, to further enhance lip synchronization. Experimental results on\nbenchmark LRW dataset and GRID dataset transcend the state-of-the-art methods\non prevalent metrics with robust high-resolution synthesizing on gender and\npose variations. \n\n"}
{"id": "1812.07248", "contents": "Title: Attend, Copy, Parse -- End-to-end information extraction from documents Abstract: Document information extraction tasks performed by humans create data\nconsisting of a PDF or document image input, and extracted string outputs. This\nend-to-end data is naturally consumed and produced when performing the task\nbecause it is valuable in and of itself. It is naturally available, at no\nadditional cost. Unfortunately, state-of-the-art word classification methods\nfor information extraction cannot use this data, instead requiring word-level\nlabels which are expensive to create and consequently not available for many\nreal life tasks. In this paper we propose the Attend, Copy, Parse architecture,\na deep neural network model that can be trained directly on end-to-end data,\nbypassing the need for word-level labels. We evaluate the proposed architecture\non a large diverse set of invoices, and outperform a state-of-the-art\nproduction system based on word classification. We believe our proposed\narchitecture can be used on many real life information extraction tasks where\nword classification cannot be used due to a lack of the required word-level\nlabels. \n\n"}
{"id": "1812.07567", "contents": "Title: Generative One-Shot Learning (GOL): A Semi-Parametric Approach to\n  One-Shot Learning in Autonomous Vision Abstract: Highly Autonomous Driving (HAD) systems rely on deep neural networks for the\nvisual perception of the driving environment. Such networks are trained on\nlarge manually annotated databases. In this work, a semi-parametric approach to\none-shot learning is proposed, with the aim of bypassing the manual annotation\nstep required for training perceptions systems used in autonomous driving. The\nproposed generative framework, coined Generative One-Shot Learning (GOL), takes\nas input single one-shot objects, or generic patterns, and a small set of\nso-called regularization samples used to drive the generative process. New\nsynthetic data is generated as Pareto optimal solutions from one-shot objects\nusing a set of generalization functions built into a generalization generator.\nGOL has been evaluated on environment perception challenges encountered in\nautonomous vision. \n\n"}
{"id": "1812.07760", "contents": "Title: Learning On-Road Visual Control for Self-Driving Vehicles with Auxiliary\n  Tasks Abstract: A safe and robust on-road navigation system is a crucial component of\nachieving fully automated vehicles. NVIDIA recently proposed an End-to-End\nalgorithm that can directly learn steering commands from raw pixels of a front\ncamera by using one convolutional neural network. In this paper, we leverage\nauxiliary information aside from raw images and design a novel network\nstructure, called Auxiliary Task Network (ATN), to help boost the driving\nperformance while maintaining the advantage of minimal training data and an\nEnd-to-End training method. In this network, we introduce human prior knowledge\ninto vehicle navigation by transferring features from image recognition tasks.\nImage semantic segmentation is applied as an auxiliary task for navigation. We\nconsider temporal information by introducing an LSTM module and optical flow to\nthe network. Finally, we combine vehicle kinematics with a sensor fusion step.\nWe discuss the benefits of our method over state-of-the-art visual navigation\nmethods both in the Udacity simulation environment and on the real-world\nComma.ai dataset. \n\n"}
{"id": "1812.07996", "contents": "Title: Mining Interpretable AOG Representations from Convolutional Networks via\n  Active Question Answering Abstract: In this paper, we present a method to mine object-part patterns from\nconv-layers of a pre-trained convolutional neural network (CNN). The mined\nobject-part patterns are organized by an And-Or graph (AOG). This interpretable\nAOG representation consists of a four-layer semantic hierarchy, i.e., semantic\nparts, part templates, latent patterns, and neural units. The AOG associates\neach object part with certain neural units in feature maps of conv-layers. The\nAOG is constructed in a weakly-supervised manner, i.e., very few annotations\n(e.g., 3-20) of object parts are used to guide the learning of AOGs. We develop\na question-answering (QA) method that uses active human-computer communications\nto mine patterns from a pre-trained CNN, in order to incrementally explain more\nfeatures in conv-layers. During the learning process, our QA method uses the\ncurrent AOG for part localization. The QA method actively identifies objects,\nwhose feature maps cannot be explained by the AOG. Then, our method asks people\nto annotate parts on the unexplained objects, and uses answers to discover CNN\npatterns corresponding to the newly labeled parts. In this way, our method\ngradually grows new branches and refines existing branches on the AOG to\nsemanticize CNN representations. In experiments, our method exhibited a high\nlearning efficiency. Our method used about 1/6-1/3 of the part annotations for\ntraining, but achieved similar or better part-localization performance than\nfast-RCNN methods. \n\n"}
{"id": "1812.08095", "contents": "Title: Window detection in aerial texture images of the Berlin 3D CityGML Model Abstract: This article explores the usage of the state-of-art neural network Mask R-CNN\nto be used for window detection of texture files from the CityGML model of\nBerlin. As texture files are very irregular in terms of size, exposure settings\nand orientation, we use several parameter optimisation methods to improve the\nprecision. Those textures are cropped from aerial photos, which implies that\nthe angle of the facade, the exposure as well as contrast are calibrated\ntowards the mean and not towards the single facade. The analysis of a single\ntexture image with the human eye itself is challenging: A combination of window\nand facade estimation and perspective analysis is necessary in order to\ndetermine the facades and windows. We train and detect bounding boxes and masks\nfrom two data sets with image size 128 and 256. We explore various\nconfiguration optimisation methods and the relation of the Region Proposal\nNetwork, detected ROIs and the mask output. Our final results shows that the we\ncan improve the average precision scores for both data set sizes, yet the\ninitial AP score varies and leads to different resulting scores. \n\n"}
{"id": "1812.08789", "contents": "Title: Steerable $e$PCA: Rotationally Invariant Exponential Family PCA Abstract: In photon-limited imaging, the pixel intensities are affected by photon count\nnoise. Many applications, such as 3-D reconstruction using correlation analysis\nin X-ray free electron laser (XFEL) single molecule imaging, require an\naccurate estimation of the covariance of the underlying 2-D clean images.\nAccurate estimation of the covariance from low-photon count images must take\ninto account that pixel intensities are Poisson distributed, hence the\nclassical sample covariance estimator is sub-optimal. Moreover, in single\nmolecule imaging, including in-plane rotated copies of all images could further\nimprove the accuracy of covariance estimation. In this paper we introduce an\nefficient and accurate algorithm for covariance matrix estimation of count\nnoise 2-D images, including their uniform planar rotations and possibly\nreflections. Our procedure, steerable $e$PCA, combines in a novel way two\nrecently introduced innovations. The first is a methodology for principal\ncomponent analysis (PCA) for Poisson distributions, and more generally,\nexponential family distributions, called $e$PCA. The second is steerable PCA, a\nfast and accurate procedure for including all planar rotations for PCA. The\nresulting principal components are invariant to the rotation and reflection of\nthe input images. We demonstrate the efficiency and accuracy of steerable\n$e$PCA in numerical experiments involving simulated XFEL datasets and rotated\nYale B face data. \n\n"}
{"id": "1812.09280", "contents": "Title: Canonical Correlation Analysis for Misaligned Satellite Image Change\n  Detection Abstract: Canonical correlation analysis (CCA) is a statistical learning method that\nseeks to build view-independent latent representations from multi-view data.\nThis method has been successfully applied to several pattern analysis tasks\nsuch as image-to-text mapping and view-invariant object/action recognition.\nHowever, this success is highly dependent on the quality of data pairing (i.e.,\nalignments) and mispairing adversely affects the generalization ability of the\nlearned CCA representations. In this paper, we address the issue of alignment\nerrors using a new variant of canonical correlation analysis referred to as\nalignment-agnostic (AA) CCA. Starting from erroneously paired data taken from\ndifferent views, this CCA finds transformation matrices by optimizing a\nconstrained maximization problem that mixes a data correlation term with\ncontext regularization; the particular design of these two terms mitigates the\neffect of alignment errors when learning the CCA transformations. Experiments\nconducted on multi-view tasks, including multi-temporal satellite image change\ndetection, show that our AA CCA method is highly effective and resilient to\nmispairing errors. \n\n"}
{"id": "1812.10157", "contents": "Title: Motion Selective Prediction for Video Frame Synthesis Abstract: Existing conditional video prediction approaches train a network from large\ndatabases and generalize to previously unseen data. We take the opposite\nstance, and introduce a model that learns from the first frames of a given\nvideo and extends its content and motion, to, eg, double its length. To this\nend, we propose a dual network that can use in a flexible way both dynamic and\nstatic convolutional motion kernels, to predict future frames. The construct of\nour model gives us the the means to efficiently analyze its functioning and\ninterpret its output. We demonstrate experimentally the robustness of our\napproach on challenging videos in-the-wild and show that it is competitive wrt\nrelated baselines. \n\n"}
{"id": "1812.10179", "contents": "Title: Deep Convolutional Generative Adversarial Network Based Food Recognition\n  Using Partially Labeled Data Abstract: Traditional machine learning algorithms using hand-crafted feature extraction\ntechniques (such as local binary pattern) have limited accuracy because of high\nvariation in images of the same class (or intra-class variation) for food\nrecognition task. In recent works, convolutional neural networks (CNN) have\nbeen applied to this task with better results than all previously reported\nmethods. However, they perform best when trained with large amount of annotated\n(labeled) food images. This is problematic when obtained in large volume,\nbecause they are expensive, laborious and impractical. Our work aims at\ndeveloping an efficient deep CNN learning-based method for food recognition\nalleviating these limitations by using partially labeled training data on\ngenerative adversarial networks (GANs). We make new enhancements to the\nunsupervised training architecture introduced by Goodfellow et al. (2014),\nwhich was originally aimed at generating new data by sampling a dataset. In\nthis work, we make modifications to deep convolutional GANs to make them robust\nand efficient for classifying food images. Experimental results on benchmarking\ndatasets show the superiority of our proposed method as compared to the\ncurrent-state-of-the-art methodologies even when trained with partially labeled\ntraining data. \n\n"}
{"id": "1812.10240", "contents": "Title: Studying the Plasticity in Deep Convolutional Neural Networks using\n  Random Pruning Abstract: Recently there has been a lot of work on pruning filters from deep\nconvolutional neural networks (CNNs) with the intention of reducing\ncomputations.The key idea is to rank the filters based on a certain criterion\n(say, l1-norm) and retain only the top ranked filters. Once the low scoring\nfilters are pruned away the remainder of the network is fine tuned and is shown\nto give performance comparable to the original unpruned network. In this work,\nwe report experiments which suggest that the comparable performance of the\npruned network is not due to the specific criterion chosen but due to the\ninherent plasticity of deep neural networks which allows them to recover from\nthe loss of pruned filters once the rest of the filters are fine-tuned.\nSpecifically we show counter-intuitive results wherein by randomly pruning\n25-50% filters from deep CNNs we are able to obtain the same performance as\nobtained by using state-of-the-art pruning methods. We empirically validate our\nclaims by doing an exhaustive evaluation with VGG-16 and ResNet-50. We also\nevaluate a real world scenario where a CNN trained on all 1000 ImageNet classes\nneeds to be tested on only a small set of classes at test time (say, only\nanimals). We create a new benchmark dataset from ImageNet to evaluate such\nclass specific pruning and show that even here a random pruning strategy gives\nclose to state-of-the-art performance. Unlike existing approaches which mainly\nfocus on the task of image classification, in this work we also report results\non object detection and image segmentation. We show that using a simple random\npruning strategy we can achieve significant speed up in object detection (74%\nimprovement in fps) while retaining the same accuracy as that of the original\nFaster RCNN model. Similarly we show that the performance of a pruned\nSegmentation Network (SegNet) is actually very similar to that of the original\nunpruned SegNet. \n\n"}
{"id": "1812.10325", "contents": "Title: Cluster Loss for Person Re-Identification Abstract: Person re-identification (ReID) is an important problem in computer vision,\nespecially for video surveillance applications. The problem focuses on\nidentifying people across different cameras or across different frames of the\nsame camera. The main challenge lies in identifying the similarity of the same\nperson against large appearance and structure variations, while differentiating\nbetween individuals. Recently, deep learning networks with triplet loss have\nbecome a common framework for person ReID. However, triplet loss focuses on\nobtaining correct orders on the training set. We demonstrate that it performs\ninferior in a clustering task. In this paper, we design a cluster loss, which\ncan lead to the model output with a larger inter-class variation and a smaller\nintra-class variation compared to the triplet loss. As a result, our model has\na better generalization ability and can achieve higher accuracy on the test set\nespecially for a clustering task. We also introduce a batch hard training\nmechanism for improving the results and faster convergence of training. \n\n"}
{"id": "1812.10766", "contents": "Title: SMPLR: Deep SMPL reverse for 3D human pose and shape recovery Abstract: Current state-of-the-art in 3D human pose and shape recovery relies on deep\nneural networks and statistical morphable body models, such as the Skinned\nMulti-Person Linear model (SMPL). However, regardless of the advantages of\nhaving both body pose and shape, SMPL-based solutions have shown difficulties\nto predict 3D bodies accurately. This is mainly due to the unconstrained nature\nof SMPL, which may generate unrealistic body meshes. Because of this,\nregression of SMPL parameters is a difficult task, often addressed with complex\nregularization terms. In this paper we propose to embed SMPL within a deep\nmodel to accurately estimate 3D pose and shape from a still RGB image. We use\nCNN-based 3D joint predictions as an intermediate representation to regress\nSMPL pose and shape parameters. Later, 3D joints are reconstructed again in the\nSMPL output. This module can be seen as an autoencoder where the encoder is a\ndeep neural network and the decoder is SMPL model. We refer to this as SMPL\nreverse (SMPLR). By implementing SMPLR as an encoder-decoder we avoid the need\nof complex constraints on pose and shape. Furthermore, given that in-the-wild\ndatasets usually lack accurate 3D annotations, it is desirable to lift 2D\njoints to 3D without pairing 3D annotations with RGB images. Therefore, we also\npropose a denoising autoencoder (DAE) module between CNN and SMPLR, able to\nlift 2D joints to 3D and partially recover from structured error. We evaluate\nour method on SURREAL and Human3.6M datasets, showing improvement over\nSMPL-based state-of-the-art alternatives by about 4 and 25 millimeters,\nrespectively. \n\n"}
{"id": "1812.10836", "contents": "Title: Adaptive Image Sampling using Deep Learning and its Application on X-Ray\n  Fluorescence Image Reconstruction Abstract: This paper presents an adaptive image sampling algorithm based on Deep\nLearning (DL). The adaptive sampling mask generation network is jointly trained\nwith an image inpainting network. The sampling rate is controlled in the mask\ngeneration network, and a binarization strategy is investigated to make the\nsampling mask binary. Besides the image sampling and reconstruction\napplication, we show that the proposed adaptive sampling algorithm is able to\nspeed up raster scan processes such as the X-Ray fluorescence (XRF) image\nscanning process. Recently XRF laboratory-based systems have evolved to\nlightweight and portable instruments thanks to technological advancements in\nboth X-Ray generation and detection. However, the scanning time of an XRF image\nis usually long due to the long exposures requires (e.g., $100 \\mu s-1ms$ per\npoint). We propose an XRF image inpainting approach to address the issue of\nlong scanning time, thus speeding up the scanning process while still\nmaintaining the possibility to reconstruct a high quality XRF image. The\nproposed adaptive image sampling algorithm is applied to the RGB image of the\nscanning target to generate the sampling mask. The XRF scanner is then driven\naccording to the sampling mask to scan a subset of the total image pixels.\nFinally, we inpaint the scanned XRF image by fusing the RGB image to\nreconstruct the full scan XRF image. The experiments show that the proposed\nadaptive sampling algorithm is able to effectively sample the image and achieve\na better reconstruction accuracy than that of the existing methods. \n\n"}
{"id": "1812.10915", "contents": "Title: Spatiotemporal Data Fusion for Precipitation Nowcasting Abstract: Precipitation nowcasting using neural networks and ground-based radars has\nbecome one of the key components of modern weather prediction services, but it\nis limited to the regions covered by ground-based radars. Truly global\nprecipitation nowcasting requires fusion of radar and satellite observations.\nWe propose the data fusion pipeline based on computer vision techniques,\nincluding novel inpainting algorithm with soft masking. \n\n"}
{"id": "1812.11042", "contents": "Title: Image Processing in Quantum Computers Abstract: Quantum Image Processing (QIP)is an exciting new field showing a lot of\npromise as a powerful addition to the arsenal of Image Processing techniques.\nRepresenting image pixel by pixel using classical information requires an\nenormous amount of computational resources. Hence, exploring methods to\nrepresent images in a different paradigm of information is important. In this\nwork, we study the representation of images in Quantum Information. The main\nmotivation for this pursuit is the ability of storing N bits of classical\ninformation in only log(2N) quantum bits (qubits). The promising first step was\nthe exponentially efficient implementation of the Fourier transform in quantum\ncomputers as compared to Fast Fourier Transform in classical computers. In\naddition, images encoded in quantum information could obey unique quantum\nproperties like superposition or entanglement. \n\n"}
{"id": "1812.11501", "contents": "Title: CoSpace: Common Subspace Learning from Hyperspectral-Multispectral\n  Correspondences Abstract: With a large amount of open satellite multispectral imagery (e.g., Sentinel-2\nand Landsat-8), considerable attention has been paid to global multispectral\nland cover classification. However, its limited spectral information hinders\nfurther improving the classification performance. Hyperspectral imaging enables\ndiscrimination between spectrally similar classes but its swath width from\nspace is narrow compared to multispectral ones. To achieve accurate land cover\nclassification over a large coverage, we propose a cross-modality feature\nlearning framework, called common subspace learning (CoSpace), by jointly\nconsidering subspace learning and supervised classification. By locally\naligning the manifold structure of the two modalities, CoSpace linearly learns\na shared latent subspace from hyperspectral-multispectral(HS-MS)\ncorrespondences. The multispectral out-of-samples can be then projected into\nthe subspace, which are expected to take advantages of rich spectral\ninformation of the corresponding hyperspectral data used for learning, and thus\nleads to a better classification. Extensive experiments on two simulated HSMS\ndatasets (University of Houston and Chikusei), where HS-MS data sets have\ntrade-offs between coverage and spectral resolution, are performed to\ndemonstrate the superiority and effectiveness of the proposed method in\ncomparison with previous state-of-the-art methods. \n\n"}
{"id": "1812.11560", "contents": "Title: Monte-Carlo Sampling applied to Multiple Instance Learning for\n  Histological Image Classification Abstract: We propose a patch sampling strategy based on a sequential Monte-Carlo method\nfor high resolution image classification in the context of Multiple Instance\nLearning. When compared with grid sampling and uniform sampling techniques, it\nachieves higher generalization performance. We validate the strategy on two\nartificial datasets and two histological datasets for breast cancer and sun\nexposure classification. \n\n"}
{"id": "1812.11737", "contents": "Title: The meaning of \"most\" for visual question answering models Abstract: The correct interpretation of quantifier statements in the context of a\nvisual scene requires non-trivial inference mechanisms. For the example of\n\"most\", we discuss two strategies which rely on fundamentally different\ncognitive concepts. Our aim is to identify what strategy deep learning models\nfor visual question answering learn when trained on such questions. To this\nend, we carefully design data to replicate experiments from psycholinguistics\nwhere the same question was investigated for humans. Focusing on the FiLM\nvisual question answering model, our experiments indicate that a form of\napproximate number system emerges whose performance declines with more\ndifficult scenes as predicted by Weber's law. Moreover, we identify confounding\nfactors, like spatial arrangement of the scene, which impede the effectiveness\nof this system. \n\n"}
{"id": "1812.11800", "contents": "Title: Regularized Binary Network Training Abstract: There is a significant performance gap between Binary Neural Networks (BNNs)\nand floating point Deep Neural Networks (DNNs). We propose to improve the\nbinary training method, by introducing a new regularization function that\nencourages training weights around binary values. In addition, we add trainable\nscaling factors to our regularization functions. Additionally, an improved\napproximation of the derivative of the sign activation function in the backward\ncomputation. These modifications are based on linear operations that are easily\nimplementable into the binary training framework. Experimental results on\nImageNet shows our method outperforms the traditional BNN method and XNOR-net. \n\n"}
{"id": "1812.11834", "contents": "Title: Sequential Gating Ensemble Network for Noise Robust Multi-Scale Face\n  Restoration Abstract: Face restoration from low resolution and noise is important for applications\nof face analysis recognition. However, most existing face restoration models\nomit the multiple scale issues in face restoration problem, which is still not\nwell-solved in research area. In this paper, we propose a Sequential Gating\nEnsemble Network (SGEN) for multi-scale noise robust face restoration issue. To\nendow the network with multi-scale representation ability, we first employ the\nprinciple of ensemble learning for SGEN network architecture designing. The\nSGEN aggregates multi-level base-encoders and base-decoders into the network,\nwhich enables the network to contain multiple scales of receptive field.\nInstead of combining these base-en/decoders directly with non-sequential\noperations, the SGEN takes base-en/decoders from different levels as sequential\ndata. Specifically, it is visualized that SGEN learns to sequentially extract\nhigh level information from base-encoders in bottom-up manner and restore low\nlevel information from base-decoders in top-down manner. Besides, we propose to\nrealize bottom-up and top-down information combination and selection with\nSequential Gating Unit (SGU). The SGU sequentially takes information from two\ndifferent levels as inputs and decides the output based on one active input.\nExperiment results on benchmark dataset demonstrate that our SGEN is more\neffective at multi-scale human face restoration with more image details and\nless noise than state-of-the-art image restoration models. Further utilizing\nadversarial training scheme, SGEN also produces more visually preferred results\nthan other models under subjective evaluation. \n\n"}
{"id": "1901.00054", "contents": "Title: A Noise-Sensitivity-Analysis-Based Test Prioritization Technique for\n  Deep Neural Networks Abstract: Deep neural networks (DNNs) have been widely used in the fields such as\nnatural language processing, computer vision and image recognition. But several\nstudies have been shown that deep neural networks can be easily fooled by\nartificial examples with some perturbations, which are widely known as\nadversarial examples. Adversarial examples can be used to attack deep neural\nnetworks or to improve the robustness of deep neural networks. A common way of\ngenerating adversarial examples is to first generate some noises and then add\nthem into original examples. In practice, different examples have different\nnoise-sensitive. To generate an effective adversarial example, it may be\nnecessary to add a lot of noise to low noise-sensitive example, which may make\nthe adversarial example meaningless. In this paper, we propose a\nnoise-sensitivity-analysis-based test prioritization technique to pick out\nexamples by their noise sensitivity. We construct an experiment to validate our\napproach on four image sets and two DNN models, which shows that examples are\nsensitive to noise and our method can effectively pick out examples by their\nnoise sensitivity. \n\n"}
{"id": "1901.00366", "contents": "Title: Learning Efficient Detector with Semi-supervised Adaptive Distillation Abstract: Knowledge Distillation (KD) has been used in image classification for model\ncompression. However, rare studies apply this technology on single-stage object\ndetectors. Focal loss shows that the accumulated errors of easily-classified\nsamples dominate the overall loss in the training process. This problem is also\nencountered when applying KD in the detection task. For KD, the teacher-defined\nhard samples are far more important than any others. We propose ADL to address\nthis issue by adaptively mimicking the teacher's logits, with more attention\npaid on two types of hard samples: hard-to-learn samples predicted by teacher\nwith low certainty and hard-to-mimic samples with a large gap between the\nteacher's and the student's prediction. ADL enlarges the distillation loss for\nhard-to-learn and hard-to-mimic samples and reduces distillation loss for the\ndominant easy samples, enabling distillation to work on the single-stage\ndetector first time, even if the student and the teacher are identical.\nBesides, ADL is effective in both the supervised setting and the\nsemi-supervised setting, even when the labeled data and unlabeled data are from\ndifferent distributions. For distillation on unlabeled data, ADL achieves\nbetter performance than existing data distillation which simply utilizes hard\ntargets, making the student detector surpass its teacher. On the COCO database,\nsemi-supervised adaptive distillation (SAD) makes a student detector with a\nbackbone of ResNet-50 surpasses its teacher with a backbone of ResNet-101,\nwhile the student has half of the teacher's computation complexity. The code is\navaiable at https://github.com/Tangshitao/Semi-supervised-Adaptive-Distillation \n\n"}
{"id": "1901.00534", "contents": "Title: Linear colour segmentation revisited Abstract: In this work we discuss the known algorithms for linear colour segmentation\nbased on a physical approach and propose a new modification of segmentation\nalgorithm. This algorithm is based on a region adjacency graph framework\nwithout a pre-segmentation stage. Proposed edge weight functions are defined\nfrom linear image model with normal noise. The colour space projective\ntransform is introduced as a novel pre-processing technique for better handling\nof shadow and highlight areas. The resulting algorithm is tested on a benchmark\ndataset consisting of the images of 19 natural scenes selected from the\nBarnard's DXC-930 SFU dataset and 12 natural scene images newly published for\ncommon use. The dataset is provided with pixel-by-pixel ground truth colour\nsegmentation for every image. Using this dataset, we show that the proposed\nalgorithm modifications lead to qualitative advantages over other model-based\nsegmentation algorithms, and also show the positive effect of each proposed\nmodification. The source code and datasets for this work are available for free\naccess at http://github.com/visillect/segmentation. \n\n"}
{"id": "1901.00600", "contents": "Title: A Remote Sensing Image Dataset for Cloud Removal Abstract: Cloud-based overlays are often present in optical remote sensing images, thus\nlimiting the application of acquired data. Removing clouds is an indispensable\npre-processing step in remote sensing image analysis. Deep learning has\nachieved great success in the field of remote sensing in recent years,\nincluding scene classification and change detection. However, deep learning is\nrarely applied in remote sensing image removal clouds. The reason is the lack\nof data sets for training neural networks. In order to solve this problem, this\npaper first proposed the Remote sensing Image Cloud rEmoving dataset (RICE).\nThe proposed dataset consists of two parts: RICE1 contains 500 pairs of images,\neach pair has images with cloud and cloudless size of 512*512; RICE2 contains\n450 sets of images, each set contains three 512*512 size images. ,\nrespectively, the reference picture without clouds, the picture of the cloud\nand the mask of its cloud. The dataset is freely available at\n\\url{https://github.com/BUPTLdy/RICE_DATASET}. \n\n"}
{"id": "1901.00889", "contents": "Title: Polarimetric Thermal to Visible Face Verification via Attribute\n  Preserved Synthesis Abstract: Thermal to visible face verification is a challenging problem due to the\nlarge domain discrepancy between the modalities. Existing approaches either\nattempt to synthesize visible faces from thermal faces or extract robust\nfeatures from these modalities for cross-modal matching. In this paper, we take\na different approach in which we make use of the attributes extracted from the\nvisible image to synthesize the attribute-preserved visible image from the\ninput thermal image for cross-modal matching. A pre-trained VGG-Face network is\nused to extract the attributes from the visible image. Then, a novel Attribute\nPreserved Generative Adversarial Network (AP-GAN) is proposed to synthesize the\nvisible image from the thermal image guided by the extracted attributes.\nFinally, a deep network is used to extract features from the synthesized image\nand the input visible image for verification. Extensive experiments on the ARL\nPolarimetric face dataset show that the proposed method achieves significant\nimprovements over the state-of-the-art methods. \n\n"}
{"id": "1901.01238", "contents": "Title: A Distance Map Regularized CNN for Cardiac Cine MR Image Segmentation Abstract: Cardiac image segmentation is a critical process for generating personalized\nmodels of the heart and for quantifying cardiac performance parameters. Several\nconvolutional neural network (CNN) architectures have been proposed to segment\nthe heart chambers from cardiac cine MR images. Here we propose a multi-task\nlearning (MTL)-based regularization framework for cardiac MR image\nsegmentation. The network is trained to perform the main task of semantic\nsegmentation, along with a simultaneous, auxiliary task of pixel-wise distance\nmap regression. The proposed distance map regularizer is a decoder network\nadded to the bottleneck layer of an existing CNN architecture, facilitating the\nnetwork to learn robust global features. The regularizer block is removed after\ntraining, so that the original number of network parameters does not change. We\nshow that the proposed regularization method improves both binary and\nmulti-class segmentation performance over the corresponding state-of-the-art\nCNN architectures on two publicly available cardiac cine MRI datasets,\nobtaining average dice coefficient of 0.84$\\pm$0.03 and 0.91$\\pm$0.04,\nrespectively. Furthermore, we also demonstrate improved generalization\nperformance of the distance map regularized network on cross-dataset\nsegmentation, showing as much as 42% improvement in myocardium Dice coefficient\nfrom 0.56$\\pm$0.28 to 0.80$\\pm$0.14. \n\n"}
{"id": "1901.01569", "contents": "Title: Segmentation Guided Image-to-Image Translation with Adversarial Networks Abstract: Recently image-to-image translation has received increasing attention, which\naims to map images in one domain to another specific one. Existing methods\nmainly solve this task via a deep generative model, and focus on exploring the\nrelationship between different domains. However, these methods neglect to\nutilize higher-level and instance-specific information to guide the training\nprocess, leading to a great deal of unrealistic generated images of low\nquality. Existing methods also lack of spatial controllability during\ntranslation. To address these challenge, we propose a novel Segmentation Guided\nGenerative Adversarial Networks (SGGAN), which leverages semantic segmentation\nto further boost the generation performance and provide spatial mapping. In\nparticular, a segmentor network is designed to impose semantic information on\nthe generated images. Experimental results on multi-domain face image\ntranslation task empirically demonstrate our ability of the spatial\nmodification and our superiority in image quality over several state-of-the-art\nmethods. \n\n"}
{"id": "1901.01708", "contents": "Title: Post-mortem Iris Recognition with Deep-Learning-based Image Segmentation Abstract: This paper proposes the first known to us iris recognition methodology\ndesigned specifically for post-mortem samples. We propose to use deep\nlearning-based iris segmentation models to extract highly irregular iris\ntexture areas in post-mortem iris images. We show how to use segmentation masks\npredicted by neural networks in conventional, Gabor-based iris recognition\nmethod, which employs circular approximations of the pupillary and limbic iris\nboundaries. As a whole, this method allows for a significant improvement in\npost-mortem iris recognition accuracy over the methods designed only for\nante-mortem irises, including the academic OSIRIS and commercial IriCore\nimplementations. The proposed method reaches the EER less than 1% for samples\ncollected up to 10 hours after death, when compared to 16.89% and 5.37% of EER\nobserved for OSIRIS and IriCore, respectively. For samples collected up to 369\nhours post-mortem, the proposed method achieves the EER 21.45%, while 33.59%\nand 25.38% are observed for OSIRIS and IriCore, respectively. Additionally, the\nmethod is tested on a database of iris images collected from ophthalmology\nclinic patients, for which it also offers an advantage over the two other\nalgorithms. This work is the first step towards post-mortem-specific iris\nrecognition, which increases the chances of identification of deceased subjects\nin forensic investigations. The new database of post-mortem iris images\nacquired from 42 subjects, as well as the deep learning-based segmentation\nmodels are made available along with the paper, to ensure all the results\npresented in this manuscript are reproducible. \n\n"}
{"id": "1901.01928", "contents": "Title: DSConv: Efficient Convolution Operator Abstract: Quantization is a popular way of increasing the speed and lowering the memory\nusage of Convolution Neural Networks (CNNs). When labelled training data is\navailable, network weights and activations have successfully been quantized\ndown to 1-bit. The same cannot be said about the scenario when labelled\ntraining data is not available, e.g. when quantizing a pre-trained model, where\ncurrent approaches show, at best, no loss of accuracy at 8-bit quantizations.\nWe introduce DSConv, a flexible quantized convolution operator that replaces\nsingle-precision operations with their far less expensive integer counterparts,\nwhile maintaining the probability distributions over both the kernel weights\nand the outputs. We test our model as a plug-and-play replacement for standard\nconvolution on most popular neural network architectures, ResNet, DenseNet,\nGoogLeNet, AlexNet and VGG-Net and demonstrate state-of-the-art results, with\nless than 1% loss of accuracy, without retraining, using only 4-bit\nquantization. We also show how a distillation-based adaptation stage with\nunlabelled data can improve results even further. \n\n"}
{"id": "1901.02132", "contents": "Title: Spatial-Winograd Pruning Enabling Sparse Winograd Convolution Abstract: Deep convolutional neural networks (CNNs) are deployed in various\napplications but demand immense computational requirements. Pruning techniques\nand Winograd convolution are two typical methods to reduce the CNN computation.\nHowever, they cannot be directly combined because Winograd transformation fills\nin the sparsity resulting from pruning. Li et al. (2017) propose sparse\nWinograd convolution in which weights are directly pruned in the Winograd\ndomain, but this technique is not very practical because Winograd-domain\nretraining requires low learning rates and hence significantly longer training\ntime. Besides, Liu et al. (2018) move the ReLU function into the Winograd\ndomain, which can help increase the weight sparsity but requires changes in the\nnetwork structure. To achieve a high Winograd-domain weight sparsity without\nchanging network structures, we propose a new pruning method, spatial-Winograd\npruning. As the first step, spatial-domain weights are pruned in a structured\nway, which efficiently transfers the spatial-domain sparsity into the Winograd\ndomain and avoids Winograd-domain retraining. For the next step, we also\nperform pruning and retraining directly in the Winograd domain but propose to\nuse an importance factor matrix to adjust weight importance and weight\ngradients. This adjustment makes it possible to effectively retrain the pruned\nWinograd-domain network without changing the network structure. For the three\nmodels on the datasets of CIFAR10, CIFAR-100, and ImageNet, our proposed method\ncan achieve the Winograd domain sparsities of 63%, 50%, and 74%, respectively. \n\n"}
{"id": "1901.02551", "contents": "Title: Thinking Outside the Pool: Active Training Image Creation for Relative\n  Attributes Abstract: Current wisdom suggests more labeled image data is always better, and\nobtaining labels is the bottleneck. Yet curating a pool of sufficiently diverse\nand informative images is itself a challenge. In particular, training image\ncuration is problematic for fine-grained attributes, where the subtle visual\ndifferences of interest may be rare within traditional image sources. We\npropose an active image generation approach to address this issue. The main\nidea is to jointly learn the attribute ranking task while also learning to\ngenerate novel realistic image samples that will benefit that task. We\nintroduce an end-to-end framework that dynamically \"imagines\" image pairs that\nwould confuse the current model, presents them to human annotators for\nlabeling, then improves the predictive model with the new examples. With\nresults on two datasets, we show that by thinking outside the pool of real\nimages, our approach gains generalization accuracy for challenging fine-grained\nattribute comparisons. \n\n"}
{"id": "1901.02579", "contents": "Title: Manipulation-skill Assessment from Videos with Spatial Attention Network Abstract: Recent advances in computer vision have made it possible to automatically\nassess from videos the manipulation skills of humans in performing a task,\nwhich breeds many important applications in domains such as health\nrehabilitation and manufacturing. Previous methods of video-based skill\nassessment did not consider the attention mechanism humans use in assessing\nvideos, limiting their performance as only a small part of video regions is\ninformative for skill assessment. Our motivation here is to estimate attention\nin videos that helps to focus on critically important video regions for better\nskill assessment. In particular, we propose a novel RNN-based spatial attention\nmodel that considers accumulated attention state from previous frames as well\nas high-level knowledge about the progress of an undergoing task. We evaluate\nour approach on a newly collected dataset of infant grasping task and four\nexisting datasets of hand manipulation tasks. Experiment results demonstrate\nthat state-of-the-art performance can be achieved by considering attention in\nautomatic skill assessment. \n\n"}
{"id": "1901.02596", "contents": "Title: MSR: Multi-Scale Shape Regression for Scene Text Detection Abstract: State-of-the-art scene text detection techniques predict quadrilateral boxes\nthat are prone to localization errors while dealing with straight or curved\ntext lines of different orientations and lengths in scenes. This paper presents\na novel multi-scale shape regression network (MSR) that is capable of locating\ntext lines of different lengths, shapes and curvatures in scenes. The proposed\nMSR detects scene texts by predicting dense text boundary points that\ninherently capture the location and shape of text lines accurately and are also\nmore tolerant to the variation of text line length as compared with the state\nof the arts using proposals or segmentation. Additionally, the multi-scale\nnetwork extracts and fuses features at different scales which demonstrates\nsuperb tolerance to the text scale variation. Extensive experiments over\nseveral public datasets show that the proposed MSR obtains superior detection\nperformance for both curved and straight text lines of different lengths and\norientations. \n\n"}
{"id": "1901.03037", "contents": "Title: Image Transformation can make Neural Networks more robust against\n  Adversarial Examples Abstract: Neural networks are being applied in many tasks related to IoT with\nencouraging results. For example, neural networks can precisely detect human,\nobjects and animal via surveillance camera for security purpose. However,\nneural networks have been recently found vulnerable to well-designed input\nsamples that called adversarial examples. Such issue causes neural networks to\nmisclassify adversarial examples that are imperceptible to humans. We found\ngiving a rotation to an adversarial example image can defeat the effect of\nadversarial examples. Using MNIST number images as the original images, we\nfirst generated adversarial examples to neural network recognizer, which was\ncompletely fooled by the forged examples. Then we rotated the adversarial image\nand gave them to the recognizer to find the recognizer to regain the correct\nrecognition. Thus, we empirically confirmed rotation to images can protect\npattern recognizer based on neural networks from adversarial example attacks. \n\n"}
{"id": "1901.03775", "contents": "Title: Creative AI Through Evolutionary Computation Abstract: The main power of artificial intelligence is not in modeling what we already\nknow, but in creating solutions that are new. Such solutions exist in extremely\nlarge, high-dimensional, and complex search spaces. Population-based search\ntechniques, i.e. variants of evolutionary computation, are well suited to\nfinding them. These techniques are also well positioned to take advantage of\nlarge-scale parallel computing resources, making creative AI through\nevolutionary computation the likely \"next deep learning\". \n\n"}
{"id": "1901.03781", "contents": "Title: DeepSpline: Data-Driven Reconstruction of Parametric Curves and Surfaces Abstract: Reconstruction of geometry based on different input modes, such as images or\npoint clouds, has been instrumental in the development of computer aided design\nand computer graphics. Optimal implementations of these applications have\ntraditionally involved the use of spline-based representations at their core.\nMost such methods attempt to solve optimization problems that minimize an\noutput-target mismatch. However, these optimization techniques require an\ninitialization that is close enough, as they are local methods by nature. We\npropose a deep learning architecture that adapts to perform spline fitting\ntasks accordingly, providing complementary results to the aforementioned\ntraditional methods. We showcase the performance of our approach, by\nreconstructing spline curves and surfaces based on input images or point\nclouds. \n\n"}
{"id": "1901.04584", "contents": "Title: Towards Personalized Management of Type B Aortic Dissection Using STENT:\n  a STandard cta database with annotation of the ENtire aorta and True-false\n  lumen Abstract: Type B Aortic Dissection(TBAD) is a rare aortic disease with a high 5-year\nmortality.Personalized and precise management of TBAD has been increasingly\ndesired in clinic which requires the geometric parameters of TBAD specific to\nthe patient be measured accurately.This remains to be a challenging task for\nvascular surgeons as manual measurement is highly subjective and imprecise. To\nsolve this problem,we introduce STENT-a STandard cta database with annotation\nof the ENtire aorta and True-false lumen. The database contains 274 CT\nangiography (CTA) scans from 274 unique TBAD patients and is split into a\ntraining set(254 cases including 210 preoperative and 44 postoperative scans )\nand a test set(20 cases).Based on STENT,we develop a series of methods\nincluding automated TBAD segmentation and automated measurement of TBAD\nparameters that facilitate personalized and precise management of the disease.\nIn this work, the database and the proposed methods are thoroughly introduced\nand evaluated and the results of our study shows the feasibility and\neffectiveness of our approach to easing the decision-making process for\nvascular surgeons during personalized TBAD management. \n\n"}
{"id": "1901.04780", "contents": "Title: DenseFusion: 6D Object Pose Estimation by Iterative Dense Fusion Abstract: A key technical challenge in performing 6D object pose estimation from RGB-D\nimage is to fully leverage the two complementary data sources. Prior works\neither extract information from the RGB image and depth separately or use\ncostly post-processing steps, limiting their performances in highly cluttered\nscenes and real-time applications. In this work, we present DenseFusion, a\ngeneric framework for estimating 6D pose of a set of known objects from RGB-D\nimages. DenseFusion is a heterogeneous architecture that processes the two data\nsources individually and uses a novel dense fusion network to extract\npixel-wise dense feature embedding, from which the pose is estimated.\nFurthermore, we integrate an end-to-end iterative pose refinement procedure\nthat further improves the pose estimation while achieving near real-time\ninference. Our experiments show that our method outperforms state-of-the-art\napproaches in two datasets, YCB-Video and LineMOD. We also deploy our proposed\nmethod to a real robot to grasp and manipulate objects based on the estimated\npose. \n\n"}
{"id": "1901.05031", "contents": "Title: Analysis and algorithms for $\\ell_p$-based semi-supervised learning on\n  graphs Abstract: This paper addresses theory and applications of $\\ell_p$-based Laplacian\nregularization in semi-supervised learning. The graph $p$-Laplacian for $p>2$\nhas been proposed recently as a replacement for the standard ($p=2$) graph\nLaplacian in semi-supervised learning problems with very few labels, where\nLaplacian learning is degenerate.\n  In the first part of the paper we prove new discrete to continuum convergence\nresults for $p$-Laplace problems on $k$-nearest neighbor ($k$-NN) graphs, which\nare more commonly used in practice than random geometric graphs. Our analysis\nshows that, on $k$-NN graphs, the $p$-Laplacian retains information about the\ndata distribution as $p\\to \\infty$ and Lipschitz learning ($p=\\infty$) is\nsensitive to the data distribution. This situation can be contrasted with\nrandom geometric graphs, where the $p$-Laplacian forgets the data distribution\nas $p\\to \\infty$. We also present a general framework for proving discrete to\ncontinuum convergence results in graph-based learning that only requires\npointwise consistency and monotonicity.\n  In the second part of the paper, we develop fast algorithms for solving the\nvariational and game-theoretic $p$-Laplace equations on weighted graphs for\n$p>2$. We present several efficient and scalable algorithms for both\nformulations, and present numerical results on synthetic data indicating their\nconvergence properties. Finally, we conduct extensive numerical experiments on\nthe MNIST, FashionMNIST and EMNIST datasets that illustrate the effectiveness\nof the $p$-Laplacian formulation for semi-supervised learning with few labels.\nIn particular, we find that Lipschitz learning ($p=\\infty$) performs well with\nvery few labels on $k$-NN graphs, which experimentally validates our\ntheoretical findings that Lipschitz learning retains information about the data\ndistribution (the unlabeled data) on $k$-NN graphs. \n\n"}
{"id": "1901.05362", "contents": "Title: Technical Report on Visual Quality Assessment for Frame Interpolation Abstract: Current benchmarks for optical flow algorithms evaluate the estimation\nquality by comparing their predicted flow field with the ground truth, and\nadditionally may compare interpolated frames, based on these predictions, with\nthe correct frames from the actual image sequences. For the latter comparisons,\nobjective measures such as mean square errors are applied. However, for\napplications like image interpolation, the expected user's quality of\nexperience cannot be fully deduced from such simple quality measures.\nTherefore, we conducted a subjective quality assessment study by crowdsourcing\nfor the interpolated images provided in one of the optical flow benchmarks, the\nMiddlebury benchmark. We used paired comparisons with forced choice and\nreconstructed absolute quality scale values according to Thurstone's model\nusing the classical least squares method. The results give rise to a re-ranking\nof 141 participating algorithms w.r.t. visual quality of interpolated frames\nmostly based on optical flow estimation. Our re-ranking result shows the\nnecessity of visual quality assessment as another evaluation metric for optical\nflow and frame interpolation benchmarks. \n\n"}
{"id": "1901.05742", "contents": "Title: A Temporal Attentive Approach for Video-Based Pedestrian Attribute\n  Recognition Abstract: In this paper, we first tackle the problem of pedestrian attribute\nrecognition by video-based approach. The challenge mainly lies in spatial and\ntemporal modeling and how to integrating them for effective and dynamic\npedestrian representation. To solve this problem, a novel multi-task model\nbased on the conventional neural network and temporal attention strategy is\nproposed. Since publicly available dataset is rare, two new large-scale video\ndatasets with expanded attribute definition are presented, on which the\neffectiveness of both video-based pedestrian attribute recognition methods and\nthe proposed new network architecture is well demonstrated. The two datasets\nare published on http://irip.buaa.edu.cn/mars_duke_attributes/index.html. \n\n"}
{"id": "1901.06514", "contents": "Title: The RobotriX: An eXtremely Photorealistic and Very-Large-Scale Indoor\n  Dataset of Sequences with Robot Trajectories and Interactions Abstract: Enter the RobotriX, an extremely photorealistic indoor dataset designed to\nenable the application of deep learning techniques to a wide variety of robotic\nvision problems. The RobotriX consists of hyperrealistic indoor scenes which\nare explored by robot agents which also interact with objects in a visually\nrealistic manner in that simulated world. Photorealistic scenes and robots are\nrendered by Unreal Engine into a virtual reality headset which captures gaze so\nthat a human operator can move the robot and use controllers for the robotic\nhands; scene information is dumped on a per-frame basis so that it can be\nreproduced offline to generate raw data and ground truth labels. By taking this\napproach, we were able to generate a dataset of 38 semantic classes totaling 8M\nstills recorded at +60 frames per second with full HD resolution. For each\nframe, RGB-D and 3D information is provided with full annotations in both\nspaces. Thanks to the high quality and quantity of both raw information and\nannotations, the RobotriX will serve as a new milestone for investigating 2D\nand 3D robotic vision tasks with large-scale data-driven techniques. \n\n"}
{"id": "1901.07273", "contents": "Title: Super-Trajectories: A Compact Yet Rich Video Representation Abstract: We propose a new video representation in terms of an over-segmentation of\ndense trajectories covering the whole video. Trajectories are often used to\nencode long-temporal information in several computer vision applications.\nSimilar to temporal superpixels, a temporal slice of super-trajectories are\nsuperpixels, but the later contains more information because it maintains the\nlong dense pixel-wise tracking information as well. The main challenge in using\ntrajectories for any application, is the accumulation of tracking error in the\ntrajectory construction. For our problem, this results in disconnected\nsuperpixels. We exploit constraints for edges in addition to trajectory based\ncolor and position similarity. Analogous to superpixels as a preprocessing tool\nfor images, the proposed representation has its applications for videos,\nespecially in trajectory based video analysis. \n\n"}
{"id": "1901.07295", "contents": "Title: Adversarial Pseudo Healthy Synthesis Needs Pathology Factorization Abstract: Pseudo healthy synthesis, i.e. the creation of a subject-specific `healthy'\nimage from a pathological one, could be helpful in tasks such as anomaly\ndetection, understanding changes induced by pathology and disease or even as\ndata augmentation. We treat this task as a factor decomposition problem: we aim\nto separate what appears to be healthy and where disease is (as a map). The two\nfactors are then recombined (by a network) to reconstruct the input disease\nimage. We train our models in an adversarial way using either paired or\nunpaired settings, where we pair disease images and maps (as segmentation\nmasks) when available. We quantitatively evaluate the quality of pseudo healthy\nimages. We show in a series of experiments, performed in ISLES and BraTS\ndatasets, that our method is better than conditional GAN and CycleGAN,\nhighlighting challenges in using adversarial methods in the image translation\ntask of pseudo healthy image generation. \n\n"}
{"id": "1901.07766", "contents": "Title: Programmable Neural Network Trojan for Pre-Trained Feature Extractor Abstract: Neural network (NN) trojaning attack is an emerging and important attack\nmodel that can broadly damage the system deployed with NN models. Existing\nstudies have explored the outsourced training attack scenario and transfer\nlearning attack scenario in some small datasets for specific domains, with\nlimited numbers of fixed target classes. In this paper, we propose a more\npowerful trojaning attack method for both outsourced training attack and\ntransfer learning attack, which outperforms existing studies in the capability,\ngenerality, and stealthiness. First, The attack is programmable that the\nmalicious misclassification target is not fixed and can be generated on demand\neven after the victim's deployment. Second, our trojan attack is not limited in\na small domain; one trojaned model on a large-scale dataset can affect\napplications of different domains that reuse its general features. Thirdly, our\ntrojan design is hard to be detected or eliminated even if the victims\nfine-tune the whole model. \n\n"}
{"id": "1901.07821", "contents": "Title: Rethinking Lossy Compression: The Rate-Distortion-Perception Tradeoff Abstract: Lossy compression algorithms are typically designed and analyzed through the\nlens of Shannon's rate-distortion theory, where the goal is to achieve the\nlowest possible distortion (e.g., low MSE or high SSIM) at any given bit rate.\nHowever, in recent years, it has become increasingly accepted that \"low\ndistortion\" is not a synonym for \"high perceptual quality\", and in fact\noptimization of one often comes at the expense of the other. In light of this\nunderstanding, it is natural to seek for a generalization of rate-distortion\ntheory which takes perceptual quality into account. In this paper, we adopt the\nmathematical definition of perceptual quality recently proposed by Blau &\nMichaeli (2018), and use it to study the three-way tradeoff between rate,\ndistortion, and perception. We show that restricting the perceptual quality to\nbe high, generally leads to an elevation of the rate-distortion curve, thus\nnecessitating a sacrifice in either rate or distortion. We prove several\nfundamental properties of this triple-tradeoff, calculate it in closed form for\na Bernoulli source, and illustrate it visually on a toy MNIST example. \n\n"}
{"id": "1901.08242", "contents": "Title: Unsupervised Image-to-Image Translation with Self-Attention Networks Abstract: Unsupervised image translation aims to learn the transformation from a source\ndomain to another target domain given unpaired training data. Several\nstate-of-the-art works have yielded impressive results in the GANs-based\nunsupervised image-to-image translation. It fails to capture strong geometric\nor structural changes between domains, or it produces unsatisfactory result for\ncomplex scenes, compared to local texture mapping tasks such as style transfer.\nRecently, SAGAN (Han Zhang, 2018) showed that the self-attention network\nproduces better results than the convolution-based GAN. However, the\neffectiveness of the self-attention network in unsupervised image-to-image\ntranslation tasks have not been verified. In this paper, we propose an\nunsupervised image-to-image translation with self-attention networks, in which\nlong range dependency helps to not only capture strong geometric change but\nalso generate details using cues from all feature locations. In experiments, we\nqualitatively and quantitatively show superiority of the proposed method\ncompared to existing state-of-the-art unsupervised image-to-image translation\ntask. The source code and our results are online:\nhttps://github.com/itsss/img2img_sa and\nhttp://itsc.kr/2019/01/24/2019_img2img_sa \n\n"}
{"id": "1901.08373", "contents": "Title: Three-dimensional Backbone Network for 3D Object Detection in Traffic\n  Scenes Abstract: The task of detecting 3D objects in traffic scenes has a pivotal role in many\nreal-world applications. However, the performance of 3D object detection is\nlower than that of 2D object detection due to the lack of powerful 3D feature\nextraction methods. To address this issue, this study proposes a 3D backbone\nnetwork to acquire comprehensive 3D feature maps for 3D object detection. It\nprimarily consists of sparse 3D convolutional neural network operations in the\npoint cloud. The 3D backbone network can inherently learn 3D features from the\nraw data without compressing the point cloud into multiple 2D images. The\nsparse 3D convolutional neural network takes full advantage of the sparsity in\nthe 3D point cloud to accelerate computation and save memory, which makes the\n3D backbone network feasible in a real-world application. Empirical experiments\nwere conducted on the KITTI benchmark and comparable results were obtained with\nrespect to the state-of-the-art performance for 3D object detection. \n\n"}
{"id": "1901.09005", "contents": "Title: Revisiting Self-Supervised Visual Representation Learning Abstract: Unsupervised visual representation learning remains a largely unsolved\nproblem in computer vision research. Among a big body of recently proposed\napproaches for unsupervised learning of visual representations, a class of\nself-supervised techniques achieves superior performance on many challenging\nbenchmarks. A large number of the pretext tasks for self-supervised learning\nhave been studied, but other important aspects, such as the choice of\nconvolutional neural networks (CNN), has not received equal attention.\nTherefore, we revisit numerous previously proposed self-supervised models,\nconduct a thorough large scale study and, as a result, uncover multiple crucial\ninsights. We challenge a number of common practices in selfsupervised visual\nrepresentation learning and observe that standard recipes for CNN design do not\nalways translate to self-supervised representation learning. As part of our\nstudy, we drastically boost the performance of previously proposed techniques\nand outperform previously published state-of-the-art results by a large margin. \n\n"}
{"id": "1901.09006", "contents": "Title: On the Limitations of Representing Functions on Sets Abstract: Recent work on the representation of functions on sets has considered the use\nof summation in a latent space to enforce permutation invariance. In\nparticular, it has been conjectured that the dimension of this latent space may\nremain fixed as the cardinality of the sets under consideration increases.\nHowever, we demonstrate that the analysis leading to this conjecture requires\nmappings which are highly discontinuous and argue that this is only of limited\npractical use. Motivated by this observation, we prove that an implementation\nof this model via continuous mappings (as provided by e.g. neural networks or\nGaussian processes) actually imposes a constraint on the dimensionality of the\nlatent space. Practical universal function representation for set inputs can\nonly be achieved with a latent dimension at least the size of the maximum\nnumber of input elements. \n\n"}
{"id": "1901.09270", "contents": "Title: Challenges in Designing Datasets and Validation for Autonomous Driving Abstract: Autonomous driving is getting a lot of attention in the last decade and will\nbe the hot topic at least until the first successful certification of a car\nwith Level 5 autonomy. There are many public datasets in the academic\ncommunity. However, they are far away from what a robust industrial production\nsystem needs. There is a large gap between academic and industrial setting and\na substantial way from a research prototype, built on public datasets, to a\ndeployable solution which is a challenging task. In this paper, we focus on bad\npractices that often happen in the autonomous driving from an industrial\ndeployment perspective. Data design deserves at least the same amount of\nattention as the model design. There is very little attention paid to these\nissues in the scientific community, and we hope this paper encourages better\nformalization of dataset design. More specifically, we focus on the datasets\ndesign and validation scheme for autonomous driving, where we would like to\nhighlight the common problems, wrong assumptions, and steps towards avoiding\nthem, as well as some open problems. \n\n"}
{"id": "1901.09366", "contents": "Title: 6D Object Pose Estimation Based on 2D Bounding Box Abstract: In this paper, we present a simple but powerful method to tackle the problem\nof estimating the 6D pose of objects from a single RGB image. Our system trains\na novel convolutional neural network to regress the unit quaternion, which\nrepresents the 3D rotation, from the partial image inside the bounding box\nreturned by 2D detection systems. Then we propose an algorithm we call Bounding\nBox Equation to efficiently and accurately obtain the 3D translation, using 3D\nrotation and 2D bounding box. Considering that the quadratic sum of the\nquaternion's four elements equals to one, we add a normalization layer to keep\nthe network's output on the unit sphere and put forward a special loss function\nfor unit quaternion regression. We evaluate our method on the LineMod dataset\nand experiment shows that our approach outperforms base-line and some state of\nthe art methods. \n\n"}
{"id": "1901.09575", "contents": "Title: Enhancing Quality for VVC Compressed Videos by Jointly Exploiting\n  Spatial Details and Temporal Structure Abstract: In this paper, we propose a quality enhancement network of versatile video\ncoding (VVC) compressed videos by jointly exploiting spatial details and\ntemporal structure (SDTS). The proposed network consists of a temporal\nstructure fusion subnet and a spatial detail enhancement subnet. The former\nsubnet is used to estimate and compensate the temporal motion across frames,\nand the latter subnet is used to reduce the compression artifacts and enhance\nthe reconstruction quality of compressed video. Experimental results\ndemonstrate the effectiveness of our SDTS-based method. \n\n"}
{"id": "1901.09819", "contents": "Title: Generalization of feature embeddings transferred from different video\n  anomaly detection domains Abstract: Detecting anomalous activity in video surveillance often involves using only\nnormal activity data in order to learn an accurate detector. Due to lack of\nannotated data for some specific target domain, one could employ existing data\nfrom a source domain to produce better predictions. Hence, transfer learning\npresents itself as an important tool. But how to analyze the resulting data\nspace? This paper investigates video anomaly detection, in particular feature\nembeddings of pre-trained CNN that can be used with non-fully supervised data.\nBy proposing novel cross-domain generalization measures, we study how source\nfeatures can generalize for different target video domains, as well as analyze\nunsupervised transfer learning. The proposed generalization measures are not\nonly a theorical approach, but show to be useful in practice as a way to\nunderstand which datasets can be used or transferred to describe video frames,\nwhich it is possible to better discriminate between normal and anomalous\nactivity. \n\n"}
{"id": "1901.09948", "contents": "Title: Surrogate Gradient Learning in Spiking Neural Networks Abstract: Spiking neural networks are nature's versatile solution to fault-tolerant and\nenergy efficient signal processing. To translate these benefits into hardware,\na growing number of neuromorphic spiking neural network processors attempt to\nemulate biological neural networks. These developments have created an imminent\nneed for methods and tools to enable such systems to solve real-world signal\nprocessing problems. Like conventional neural networks, spiking neural networks\ncan be trained on real, domain specific data. However, their training requires\novercoming a number of challenges linked to their binary and dynamical nature.\nThis article elucidates step-by-step the problems typically encountered when\ntraining spiking neural networks, and guides the reader through the key\nconcepts of synaptic plasticity and data-driven learning in the spiking\nsetting. To that end, it gives an overview of existing approaches and provides\nan introduction to surrogate gradient methods, specifically, as a particularly\nflexible and efficient method to overcome the aforementioned challenges. \n\n"}
{"id": "1901.09972", "contents": "Title: Heartbeat Anomaly Detection using Adversarial Oversampling Abstract: Cardiovascular diseases are one of the most common causes of death in the\nworld. Prevention, knowledge of previous cases in the family, and early\ndetection is the best strategy to reduce this fact. Different machine learning\napproaches to automatic diagnostic are being proposed to this task. As in most\nhealth problems, the imbalance between examples and classes is predominant in\nthis problem and affects the performance of the automated solution. In this\npaper, we address the classification of heartbeats images in different\ncardiovascular diseases. We propose a two-dimensional Convolutional Neural\nNetwork for classification after using a InfoGAN architecture for generating\nsynthetic images to unbalanced classes. We call this proposal Adversarial\nOversampling and compare it with the classical oversampling methods as SMOTE,\nADASYN, and RandomOversampling. The results show that the proposed approach\nimproves the classifier performance for the minority classes without harming\nthe performance in the balanced classes. \n\n"}
{"id": "1901.10124", "contents": "Title: Adversarial Adaptation of Scene Graph Models for Understanding Civic\n  Issues Abstract: Citizen engagement and technology usage are two emerging trends driven by\nsmart city initiatives. Governments around the world are adopting technology\nfor faster resolution of civic issues. Typically, citizens report issues, such\nas broken roads, garbage dumps, etc. through web portals and mobile apps, in\norder for the government authorities to take appropriate actions. Several\nmediums -- text, image, audio, video -- are used to report these issues.\nThrough a user study with 13 citizens and 3 authorities, we found that image is\nthe most preferred medium to report civic issues. However, analyzing civic\nissue related images is challenging for the authorities as it requires manual\neffort. Moreover, previous works have been limited to identifying a specific\nset of issues from images. In this work, given an image, we propose to generate\na Civic Issue Graph consisting of a set of objects and the semantic relations\nbetween them, which are representative of the underlying civic issue. We also\nrelease two multi-modal (text and images) datasets, that can help in further\nanalysis of civic issues from images. We present a novel approach for\nadversarial training of existing scene graph models that enables the use of\nscene graphs for new applications in the absence of any labelled training data.\nWe conduct several experiments to analyze the efficacy of our approach, and\nusing human evaluation, we establish the appropriateness of our model at\nrepresenting different civic issues. \n\n"}
{"id": "1901.11284", "contents": "Title: Capturing Object Detection Uncertainty in Multi-Layer Grid Maps Abstract: We propose a deep convolutional object detector for automated driving\napplications that also estimates classification, pose and shape uncertainty of\neach detected object. The input consists of a multi-layer grid map which is\nwell-suited for sensor fusion, free-space estimation and machine learning.\nBased on the estimated pose and shape uncertainty we approximate object hulls\nwith bounded collision probability which we find helpful for subsequent\ntrajectory planning tasks. We train our models based on the KITTI object\ndetection data set. In a quantitative and qualitative evaluation some models\nshow a similar performance and superior robustness compared to previously\ndeveloped object detectors. However, our evaluation also points to undesired\ndata set properties which should be addressed when training data-driven models\nor creating new data sets. \n\n"}
{"id": "cs/0602055", "contents": "Title: Revisiting Evolutionary Algorithms with On-the-Fly Population Size\n  Adjustment Abstract: In an evolutionary algorithm, the population has a very important role as its\nsize has direct implications regarding solution quality, speed, and\nreliability. Theoretical studies have been done in the past to investigate the\nrole of population sizing in evolutionary algorithms. In addition to those\nstudies, several self-adjusting population sizing mechanisms have been proposed\nin the literature. This paper revisits the latter topic and pays special\nattention to the genetic algorithm with adaptive population size (APGA), for\nwhich several researchers have claimed to be very effective at autonomously\n(re)sizing the population.\n  As opposed to those previous claims, this paper suggests a complete opposite\nview. Specifically, it shows that APGA is not capable of adapting the\npopulation size at all. This claim is supported on theoretical grounds and\nconfirmed by computer simulations. \n\n"}
